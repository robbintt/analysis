---
ver: rpa2
title: 'LLMs as Span Annotators: A Comparative Study of LLMs and Humans'
arxiv_id: '2504.08697'
source_url: https://arxiv.org/abs/2504.08697
tags:
- annotation
- annotations
- span
- human
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) can serve as span annotators, offering
  a scalable alternative to human annotators for tasks requiring fine-grained text
  evaluation. The study compared LLMs with human annotators across three tasks: data-to-text
  generation evaluation, machine translation error detection, and propaganda technique
  identification.'
---

# LLMs as Span Annotators: A Comparative Study of LLMs and Humans

## Quick Facts
- **arXiv ID:** 2504.08697
- **Source URL:** https://arxiv.org/abs/2504.08697
- **Reference count:** 40
- **Primary result:** LLMs achieved moderate agreement with human annotators on span annotation tasks, comparable to skilled crowdworkers, at a fraction of the cost and time.

## Executive Summary
This study investigates whether large language models can effectively serve as span annotators across three distinct tasks: data-to-text generation evaluation, machine translation error detection, and propaganda technique identification. The researchers found that LLMs, particularly reasoning models like DeepSeek-R1 and o3-mini, can generate relevant span annotations with performance levels comparable to skilled crowdworkers. While agreement with human annotators remains moderate (κ ≈ 0.4), LLMs offer significant advantages in speed (7.7s vs 10min per example) and cost ($0.005 vs $500 per 1k outputs). The study reveals that correct LLM annotations often differ from human annotations in span boundaries (24% character-level overlap), suggesting complementary strengths rather than wholesale replacement of human annotators.

## Method Summary
The researchers evaluated LLMs on three span annotation tasks using zero-shot prompting with detailed guidelines and structured JSON output formats. They tested five proprietary models (GPT-4o, Claude 3.7 Sonnet, DeepSeek-R1, o3-mini, Gemini 2.0 Flash Thinking) and two open models (Llama 3.3 70B). The evaluation used multiple metrics including F1 (hard/soft variants), Gamma γ for position+category alignment, Pearson correlation ρ over counts, and S∅ for empty cases. Human annotations were collected from 10 skilled crowdworkers and verified by in-house experts. The study also conducted manual analysis of a subset of examples to understand systematic differences between LLM and human annotations.

## Key Results
- LLMs achieved moderate agreement with human annotators (κ ≈ 0.4), comparable to skilled crowdworkers
- Reasoning models (DeepSeek-R1, o3-mini) outperformed instruction-tuned models in category discrimination
- Correct LLM annotations showed only 24% character-level overlap with human annotations, suggesting different but valid span boundary decisions
- LLMs confused related categories (e.g., Contradictory vs. Incoherent) despite correctly identifying problematic spans

## Why This Works (Mechanism)

### Mechanism 1: Structured Output with Guidelines Grounding
LLMs produce syntactically valid span annotations when provided with constrained output formats and detailed annotation guidelines. The JSON schema forces parseable span boundaries and categories, while guidelines reduce ambiguity in category assignment by specifying edge cases and conventions. This mechanism assumes the model's pre-training includes sufficient understanding of text structure and the task domain to map guidelines to specific span decisions.

### Mechanism 2: Reasoning Models Improve Category Discrimination
Models that generate explicit reasoning traces before annotation (DeepSeek-R1, o3-mini) achieve higher agreement with human annotators than instruction-tuned models without reasoning. Intermediate reasoning steps may allow the model to verify span boundaries, compare category definitions, and self-correct before committing to structured output. This mechanism assumes the reasoning trace reflects genuine decomposition of the annotation task rather than post-hoc rationalization.

### Mechanism 3: Partially Overlapping Attention Patterns Between LLMs and Humans
LLMs and humans identify valid but non-identical spans, with only 24% character-level overlap among correct annotations. Different priors lead to different boundary decisions: humans may prioritize semantic coherence while LLMs may attend to surface patterns or over-flag noise. This mechanism assumes both annotators can be "correct" while disagreeing, acknowledging inherent subjectivity in span annotation.

## Foundational Learning

- **Concept: Inter-annotator agreement metrics for spans** - Standard metrics (Cohen's κ) don't apply when spans vary in number/position. You need F1 with partial credit, γ for position+category alignment, and S∅ for empty-annotation cases. *Quick check:* If two annotators mark "wrong category" on the same text span but different categories, should this count as partial agreement?

- **Concept: Span annotation vs. sequence labeling** - Unlike token classification (NER, POS), span annotation allows overlapping spans, variable granularity, and requires boundary+category+reason. Fine-tuned encoder models struggle without extensive training data. *Quick check:* What's the difference between sequence labeling and span annotation for propaganda detection?

- **Concept: Reasoning models vs. instruction-tuned models** - Reasoning models (o3-mini, DeepSeek-R1) allocate inference compute to thinking traces, improving complex tasks at higher latency/cost. Instruction-tuned models respond directly. *Quick check:* When would you prefer an instruction-tuned model over a reasoning model for annotation?

## Architecture Onboarding

- **Component map:** Input layer (structured data X + text Y + categories C + guidelines G) -> Prompt construction (Pbase template with guidelines embedded) -> Model layer (constrained decoding or reasoning trace + JSON extraction) -> Output parsing (string matching to recover span positions) -> Evaluation layer (F1, γ, Pearson ρ, S∅)

- **Critical path:** Guideline quality → model understanding → constrained decoding → syntactically valid output → string matching accuracy → span boundary recovery (fails if model paraphrases span text)

- **Design tradeoffs:** Guidelines vs. examples (P5shot gave inconsistent results; detailed guidelines more reliable), reasoning vs. speed (DeepSeek-R1 takes 227.5s; Claude 3.7 Sonnet takes 9.0s), cost vs. accuracy (o3-mini costs $3.60/1k outputs vs. $500 for crowdworkers)

- **Failure signatures:** Low Pearson ρ (model over-/under-annotates), large F1∆ (soft − hard, model understands spans but misassigns categories), high w/o% (examples without annotations, model too conservative or misinterprets task)

- **First 3 experiments:** 1) Prompt ablation on dev set: Compare Pbase vs. Pnoguide on 50 examples; expect F1 drop without guidelines. 2) Model comparison on single domain: Run Llama 3.3 vs. DeepSeek-R1 on D2T-EVAL dev; measure F1, γ, latency. 3) Category confusion analysis: Generate confusion matrix for your task; identify systematically confused pairs; update guidelines with disambiguating examples.

## Open Questions the Paper Calls Out

### Open Question 1
Why do correct LLM annotations show only 24% overlap with human annotations despite similar overall error rates? The paper identifies this discrepancy but does not investigate whether LLMs and humans are identifying genuinely different valid issues or whether one group is systematically missing certain error types.

### Open Question 2
Can hybrid human-LLM annotation pipelines achieve higher quality than either annotators alone, and what is the optimal division of labor? While proposed as a direction, the paper does not experimentally validate any hybrid pipeline or compare different configurations.

### Open Question 3
Why does few-shot prompting degrade performance for reasoning models like DeepSeek-R1 in span annotation tasks? The paper observes this phenomenon but does not investigate whether this is due to example complexity, distraction from reasoning traces, or fundamental incompatibility between few-shot demonstrations and reasoning model architectures.

## Limitations
- Moderate agreement (κ ≈ 0.4) between LLMs and human annotators indicates systematic differences in annotation decisions
- String-matching-based span boundary recovery is brittle when models paraphrase rather than reproduce exact text
- Reasoning models that achieve best agreement are significantly slower and more expensive than instruction-tuned alternatives

## Confidence
- **High confidence:** LLMs can produce syntactically valid span annotations when provided with structured output constraints and detailed guidelines
- **Medium confidence:** Reasoning models achieve higher agreement with human annotators than instruction-tuned models
- **Medium confidence:** Human annotators and LLMs identify partially overlapping but non-identical spans (24% character-level overlap)

## Next Checks
1. **Cross-task generalizability test:** Apply the best-performing prompt (Pbase) and reasoning model (DeepSeek-R1) to a fourth, previously unseen span annotation task. Measure whether observed agreement patterns persist or performance degrades significantly with task novelty.

2. **Boundary precision validation:** For a subset of examples where LLMs identify correct categories but with different span boundaries than humans, conduct a blinded annotation study. Have human annotators rate which boundary placement is more appropriate, controlling for category knowledge to isolate boundary quality from category discrimination.

3. **Cost-benefit scaling analysis:** Calculate the break-even point where LLM annotation becomes more cost-effective than human annotation as task volume increases. Include both annotation time (7.7s vs. 10min) and cost ($0.005 vs. $500 per 1k outputs) while accounting for post-processing overhead and the need for human review of LLM outputs.