---
ver: rpa2
title: 'RiddleBench: A New Generative Reasoning Benchmark for LLMs'
arxiv_id: '2510.24932'
source_url: https://arxiv.org/abs/2510.24932
tags:
- reasoning
- riddlebench
- language
- performance
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RiddleBench, a new benchmark of 1,737 challenging
  puzzles designed to assess flexible, multi-faceted reasoning abilities in LLMs,
  focusing on logical deduction, spatial reasoning, and constraint satisfaction. Unlike
  existing benchmarks that emphasize structured skills like quantitative problem-solving,
  RiddleBench targets the synthesis of multiple reasoning capabilities central to
  human intelligence.
---

# RiddleBench: A New Generative Reasoning Benchmark for LLMs

## Quick Facts
- arXiv ID: 2510.24932
- Source URL: https://arxiv.org/abs/2510.24932
- Authors: Deepon Halder; Alan Saji; Thanmay Jayakumar; Ratish Puduppully; Anoop Kunchukuttan; Raj Dabre
- Reference count: 14
- Primary result: Introduces RiddleBench, a benchmark of 1,737 challenging puzzles revealing significant reasoning weaknesses in leading LLMs

## Executive Summary
RiddleBench is a new benchmark designed to assess flexible, multi-faceted reasoning abilities in large language models. Unlike existing benchmarks focused on structured skills like quantitative problem-solving, RiddleBench targets the synthesis of multiple reasoning capabilities central to human intelligence through 1,737 challenging puzzles from Indian competitive exams. The benchmark reveals that even top-tier models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve only around 60% accuracy, with significant weaknesses in logical deduction, spatial reasoning, and constraint satisfaction.

The evaluation uncovers critical failure modes including hallucination cascades where models uncritically accept flawed reasoning from other models, strong self-confirmation bias hindering self-correction, and fragile reasoning that degrades significantly when constraints are reordered or irrelevant information is introduced. These findings demonstrate that current LLMs struggle with the compositional integration of reasoning skills that humans handle naturally.

## Method Summary
RiddleBench was created by extracting puzzles from Indian competitive exam PDFs using OCR, followed by human verification to ensure accuracy. The benchmark contains 1,737 carefully curated puzzles designed to test logical deduction, spatial reasoning, and constraint satisfaction. Evaluation uses a zero-shot approach with a temperature of 0.7, requiring models to generate answers within \boxed{} notation. The benchmark tests three key hypotheses: models fail at compositional integration of reasoning skills (RQ1), exhibit poor self-correction due to self-confirmation bias (RQ2), and show fragile reasoning that degrades with perturbation (RQ3).

## Key Results
- Leading models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve only around 60% accuracy on RiddleBench
- Models exhibit "hallucination cascades" where they uncritically accept flawed reasoning from other models
- Strong self-confirmation bias prevents models from correcting their own errors, entrenching mistakes 67.7% of the time
- Performance degrades significantly (-6.70 percentage points on Blood Relations) when constraints are reordered or irrelevant information is introduced

## Why This Works (Mechanism)

### Mechanism 1: Diagnostic of Compositional Integration
The benchmark functions as a diagnostic tool by requiring models to satisfy multiple constraints simultaneously rather than retrieving single-hop solutions. Puzzles like Seating Arrangements require maintaining a mutable internal "mental model" of spatial or relational states. Models relying on sequential heuristics violate early constraints when integrating later ones, revealing failures in state-tracking. This mechanism assumes models that struggle with stateful representations will show specific failure modes in constraint-heavy tasks.

### Mechanism 2: Exposure of Error Entrenchment (Self-Confirmation)
The benchmark exposes a "self-confirmation bias" where models are statistically more likely to entrench errors than to correct them during self-reflection. When tasked with self-correction, the model validates its flawed logic 67.7% of the time, suggesting the reasoning process is a post-hoc justification rather than independent verification. This assumes the self-correction prompt does not significantly alter the model's internal confidence distribution for the initial answer.

### Mechanism 3: Detection of Fragile Heuristics via Perturbation
Reasoning capability is measured by robustness to surface-level changes, revealing that models use "brittle, sequential heuristics" rather than abstract logic. Shuffling constraint order or adding irrelevant "red herring" sentences disrupts the model's sequential processing path, causing significant performance drops. This fragility indicates a reliance on information presentation order rather than logical understanding of the problem space, assuming robust logical reasoning should be invariant to premise order or irrelevant data presence.

## Foundational Learning

- **Concept: Constraint Satisfaction Problems (CSP)**
  - Why needed here: To understand why RiddleBench puzzles are uniquely difficult—they require maintaining a consistent state across multiple variables (e.g., seating positions), unlike single-step math problems
  - Quick check question: Can you manually solve a 5-person seating arrangement by holding all constraints in working memory without writing them down?

- **Concept: Self-Consistency vs. Verification**
  - Why needed here: To distinguish between a model checking its own output (verification) and a model simply generating consistent-sounding text (self-confirmation)
  - Quick check question: If an LLM says "2+2=5" and you ask it to check its work, why might it try to justify why "5" is actually correct?

- **Concept: Invariance Testing**
  - Why needed here: To grasp the "Fragile Reasoning" finding—understanding that true logical reasoning should yield the same result regardless of the order in which facts are presented
  - Quick check question: If "A is taller than B" and "B is taller than C," does the conclusion change if I tell you "B is taller than C" first?

## Architecture Onboarding

- **Component map:** OCR extraction from competitive exam PDFs -> Human verification -> Structured JSON -> Zero-shot prompt (T=0.7) -> Model Inference -> Answer Extraction (`\boxed{}`) -> Accuracy Calc

- **Critical path:**
  1. Data Curation: Ensuring OCR quality is high; flawed source data compromises the "ground truth"
  2. Baseline Eval: Running the core 1,737 puzzles to establish the accuracy ceiling (~60-70%)
  3. Perturbation: Applying shuffling and "red herring" injections to measure robustness delta

- **Design tradeoffs:**
  - Zero-shot vs. Few-shot: Zero-shot chosen to test raw reasoning without "teaching" the puzzle format, potentially underestimating models that need examples
  - Generative vs. Multiple-Choice: Requiring generative answers (`\boxed{}`) avoids random guessing artifacts present in pure MCQ benchmarks

- **Failure signatures:**
  - Hallucination Cascade: A judge model validates flawed reasoning with phrases like "The reasoning follows a logical step-by-step deduction" despite the logic being wrong
  - Thinking Exhausted: The model hits the 8192-token "thinking budget" without concluding, indicating high computational uncertainty
  - Constraint Amnesia: Solving the first 3 of 5 seating constraints correctly but contradicting the first constraint when solving the 4th

- **First 3 experiments:**
  1. Reproduce Constraint Shuffle: Take 50 Seating Arrangement problems, shuffle the sentence order, and measure the accuracy drop to quantify brittleness
  2. Self-Correction Loop: Run a set of Blood Relations puzzles; feed the model's incorrect answers back to it asking for verification, and count how often it corrects itself vs. entrenches the error
  3. Visual vs. Textual Probing: Test if asking the model to generate a "state representation" (like the ASCII art mentioned in Appendix E) *before* answering improves accuracy on spatial tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot evaluation methodology may underestimate model capabilities compared to few-shot prompting
- "Hallucination cascade" analysis relies on human evaluation of model-to-model validation, introducing subjective judgment
- Focus on Indian competitive exam formats may limit generalizability to other reasoning domains and cultural contexts

## Confidence
- **Diagnostic Value of RiddleBench (High):** Well-supported by multiple controlled experiments and clear performance gaps between models
- **Self-Confirmation Bias Finding (Medium):** 67.7% entrenchment rate is compelling but depends on prompt formulation and evaluation criteria
- **Fragile Reasoning Detection (Medium):** Performance degradation is observable but interpretation requires further investigation

## Next Checks
1. **Prompt Sensitivity Analysis:** Systematically test different prompt formulations (including few-shot examples) to determine whether observed reasoning failures persist across prompting strategies

2. **Alternative Evaluation Protocols:** Implement automated consistency checking where models must justify answers using original constraints, reducing reliance on human evaluation

3. **Cross-Domain Transfer:** Evaluate whether models performing well on RiddleBench show improved performance on other compositional reasoning tasks (e.g., mathematical proof generation, multi-step planning)