---
ver: rpa2
title: 'BioArc: Discovering Optimal Neural Architectures for Biological Foundation
  Models'
arxiv_id: '2512.00283'
source_url: https://arxiv.org/abs/2512.00283
tags:
- architecture
- architectures
- task
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIOARC, a framework that automates neural
  architecture search for biological foundation models. It addresses the gap where
  existing models use generic architectures not optimized for the unique properties
  of biological data.
---

# BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models

## Quick Facts
- arXiv ID: 2512.00283
- Source URL: https://arxiv.org/abs/2512.00283
- Reference count: 40
- Primary result: BIOARC automates neural architecture search for biological foundation models, achieving state-of-the-art performance with models up to 25× smaller than pretrained baselines.

## Executive Summary
BIOARC introduces a framework for automating neural architecture search specifically for biological foundation models. The system addresses the gap where existing biological models use generic architectures not optimized for biological data properties. BIOARC employs a supernet-based neural architecture search approach that explores diverse combinations of CNN, Transformer, LSTM, Mamba, and Hyena blocks across DNA and protein modalities, while jointly optimizing tokenization and training strategies. The framework demonstrates that hybrid architectures consistently outperform single-module designs, with BIOARC-discovered models achieving superior performance on multiple downstream tasks while being significantly smaller than baseline models.

## Method Summary
BIOARC uses a neural architecture search framework with weight-sharing supernets to efficiently explore hybrid architectures combining CNN, Transformer, LSTM, Mamba, and Hyena blocks. The search space is pruned from 67 million to 360 candidates using rule-based, distance-based, and clustering methods. A supernet is constructed with shared unique blocks, and random path sampling during pretraining prevents co-adaptation. The system evaluates architectures using both pretraining-finetuning and from-scratch training paradigms, with Z-score normalization aggregating performance across tasks. BIOARC also includes an Agent system that predicts optimal architectures for new tasks using semantic retrieval and reasoning over historical task-architecture-performance pairs.

## Key Results
- BIOARC-discovered models outperform much larger pretrained baselines on multiple DNA and protein tasks
- Hybrid architectures (combining Hyena, Transformer, and CNN blocks) consistently outperform single-module baselines
- Models up to 25× smaller than baselines achieve state-of-the-art performance
- Architecture design should be tailored to task and modality, with optimal designs showing task-architecture similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid architectures combining Hyena, Transformer, and CNN blocks outperform single-module designs because each module type captures distinct structural properties.
- Mechanism: Hyena blocks capture long-range dependencies via implicit convolutions → Transformers model complex contextual relationships through attention → CNNs extract critical local features. The sequential composition enables progressive refinement from global context to local motifs.
- Core assumption: Biological sequences exhibit hierarchical structure with both local motifs and long-range dependencies that benefit from specialized processing stages.
- Evidence anchors: [abstract] "hybrid architectures consistently outperform single-module baselines"; [section 4.2] "optimal models typically initiate with Hyena block to first captures long-range dependencies"; [corpus] Weak direct support; related work "Composer" addresses hybrid search but not biological domains specifically.
- Break condition: If tasks require only local pattern matching without global context, early CNN layers may suffice and hybrid overhead becomes wasteful.

### Mechanism 2
- Claim: Supernet weight-sharing enables efficient architecture evaluation by decoupling path training while maintaining comparable ranking fidelity to full training.
- Mechanism: Unique blocks initialized once → random path sampling during training ensures equal operation exposure → prevents co-adaptation → supernet rankings correlate with scratch-trained performance (Spearman ρ=0.817).
- Core assumption: The relative performance ordering of architectures is preserved across weight-sharing and independent training regimes.
- Evidence anchors: [section 3.2] "Instead of creating and training a new instance of this block for each path, we initialize it only once and share its weights across all paths"; [appendix A.6.13] "Spearman Rank Correlation (ρ) of 0.8170 with a statistical significance of p < 1e−100"; [corpus] No direct corpus evidence for biological NAS supernet validation.
- Break condition: If co-adaptation between modules is task-critical, supernet initialization may underrepresent synergistic weight configurations.

### Mechanism 3
- Claim: Task-architecture similarity enables prediction of optimal designs for novel biological tasks via semantic retrieval and reasoning.
- Mechanism: Optimal architectures for functionally similar tasks exhibit topological similarity → encode task descriptions semantically → retrieve similar historical tasks → LLM reasons over retrieved architecture-performance pairs → predicts top candidates.
- Core assumption: Biological tasks cluster by functional properties that correlate with optimal architectural inductive biases.
- Evidence anchors: [section 3.5] "optimal architectures for functionally similar tasks exhibit significant topological similarity"; [section 4.5] "BIOARC Agent consistently outperforms other methods in both supervised and transfer settings"; [corpus] "TART" paper addresses token-based architecture prediction but lacks biological domain validation.
- Break condition: If novel tasks represent genuinely new functional categories without semantic analogs in the knowledge base, retrieval-based prediction degrades.

## Foundational Learning

- Concept: **Neural Architecture Search (NAS) with Weight Sharing**
  - Why needed here: BioArc relies on supernet construction where understanding how weight-sharing enables efficient search without training each architecture independently is essential.
  - Quick check question: Can you explain why random path sampling during supernet training prevents co-adaptation between modules?

- Concept: **Inductive Biases of Sequence Architectures (CNN, Transformer, Mamba/Hyena)**
  - Why needed here: The framework's hybrid design depends on understanding what structural properties each block type captures—local patterns, global attention, or efficient long-range modeling.
  - Quick check question: Why would Hyena be preferred over Transformer for processing very long biological sequences?

- Concept: **Self-Supervised Learning Objectives for Biological Sequences**
  - Why needed here: BioArc evaluates three pretraining strategies (masked modeling, contrastive learning, next token prediction) and their interaction with architecture choice.
  - Quick check question: How does masked modeling differ from contrastive learning in what representations they encourage?

## Architecture Onboarding

- Component map: Search Space Generator -> Supernet -> Training Loop -> Evaluation Protocol -> BIOARC Agent

- Critical path:
  1. Define search space with pruning (rule-based, distance-based, clustering) → 67M to 360 candidates
  2. Construct supernet with shared unique blocks
  3. Pretrain supernet with sampled paths (10 epochs, ~2h/epoch for MM)
  4. Evaluate candidates on downstream tasks with Z-score aggregation
  5. Deploy Agent for new task prediction

- Design tradeoffs:
  - Search space breadth vs. computational tractability (pruning reduces diversity)
  - Supernet pretraining investment vs. evaluation accuracy (rank correlation 0.817 suggests good fidelity)
  - Tokenizer complexity vs. architecture compatibility (Transformer prefers 6-mer, CNN prefers 1-mer)

- Failure signatures:
  - Low rank correlation between supernet and scratch training suggests supernet training instability
  - Agent precision@k = 0 indicates retrieval-reasoning mismatch for novel task types
  - Single-module baselines matching hybrid performance suggests search space may be over-constrained

- First 3 experiments:
  1. **Validate supernet ranking**: Train top-10 and bottom-10 architectures from scratch on 3 DNA tasks; verify ranking consistency with supernet scores.
  2. **Tokenizer-architecture interaction**: For fixed hybrid architecture (Hyena→Transformer→CNN), benchmark k-mer vs BPE tokenizers across all DNA tasks.
  3. **Agent transfer test**: Hold out one task family (e.g., all promoter detection tasks), train Agent on remaining tasks, measure Hit Rate@5 on held-out family.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the hybrid architectural design principles (e.g., Hyena-Transformer-CNN stacking) discovered for DNA and protein generalize to other biological modalities like RNA and single-cell data?
- Basis in paper: [explicit] The authors state in Section 3.1 that the framework "generalizes to other modalities such as RNA and single-cell data," but the experimental results (Section 4) are restricted to DNA and protein benchmarks.
- Why unresolved: Different biological modalities possess distinct structural properties (e.g., RNA secondary structure, single-cell sparsity) that may require different inductive biases than the sequence-optimized architectures found for DNA.
- What evidence would resolve it: Empirical evaluation of BioArc-discovered architectures on RNA (e.g., RNAErnie benchmarks) and single-cell transcriptomics (e.g., Geneformer benchmarks) datasets.

### Open Question 2
- Question: Why does large-scale pretraining frequently fail to outperform training from scratch on downstream biological tasks, and how can this be mitigated?
- Basis in paper: [inferred] Section 4.3 states that "pretraining does not guarantee gains" and "no pretraining strategy dominates," with training from scratch winning on specific tasks like CPD and PD.
- Why unresolved: This contradicts the standard foundation model paradigm. It is unclear if the failure is due to the specific SSL objectives (Masked Modeling vs. Contrastive), the scale of pretraining data, or a fundamental mismatch between pretraining representations and downstream task requirements.
- What evidence would resolve it: An ablation study analyzing the correlation between pretraining loss convergence and downstream transferability, alongside scaling laws for hybrid biological architectures.

### Open Question 3
- Question: Can the BioArc Agent predict optimal architectures for tasks with zero semantic overlap with its knowledge base (Zero-Shot Transfer)?
- Basis in paper: [inferred] Table 4 shows a significant performance drop in the "Transfer Setting" (e.g., Precision@5 drops from 0.30 to 0.30 but Recall drops, and Hit Rate is lower) compared to the "Supervised Setting," indicating limited generalization to novel tasks.
- Why unresolved: The agent relies on semantic retrieval of similar tasks; if a new task involves biological mechanisms (e.g., structural prediction) not represented in the historical knowledge base, the reasoning process may fail.
- What evidence would resolve it: Evaluation on a "held-out mechanism" benchmark where the agent must predict architectures for tasks requiring reasoning about biological properties (like binding affinity) not seen during training.

## Limitations

- Supernet-based evaluation introduces approximation error, with Spearman correlation of 0.817 between supernet rankings and independent training suggesting meaningful but not perfect fidelity
- Search space, though pruned from 67 million to 360 architectures, may still miss optimal configurations for novel task families
- BIOARC Agent relies on semantic similarity assumptions that may not hold for truly novel biological tasks outside the training distribution

## Confidence

- **High confidence**: Empirical performance claims showing BIOARC-discovered models outperforming baselines by 1.5-7.5% on held-out tasks; the methodology for supernet construction and one-shot evaluation is technically sound and well-documented
- **Medium confidence**: Claims about specific architectural patterns (Hyena→Transformer→CNN sequences) being optimal across modalities; while supported by experimental results, these may reflect search space constraints rather than fundamental architectural principles
- **Low confidence**: Generalization claims for the BIOARC Agent to truly novel task types; the semantic similarity assumption for architecture prediction lacks extensive validation across diverse biological domains

## Next Checks

1. **Supernet ranking fidelity validation**: Train the top-5 and bottom-5 architectures from scratch on 3 DNA and 3 protein tasks (different from evaluation set). Compute the Spearman correlation between these independent rankings and the original supernet rankings to verify that the 0.817 correlation generalizes beyond the evaluation protocol.

2. **Cross-modality architecture transfer**: Take the top-3 DNA architectures discovered by BIOARC and evaluate them directly on protein tasks without modification, and vice versa. Measure performance degradation to quantify how much architecture optimization is modality-specific versus task-specific.

3. **Novel task family generalization**: Hold out all promoter detection tasks from both search and Agent training phases. Use the remaining tasks to train the Agent, then test its ability to predict architectures for the held-out promoter detection tasks. Measure Hit Rate@5 to assess whether semantic retrieval can generalize to unseen but related task families.