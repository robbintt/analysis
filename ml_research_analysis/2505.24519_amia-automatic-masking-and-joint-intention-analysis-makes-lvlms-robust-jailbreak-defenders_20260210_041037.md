---
ver: rpa2
title: 'AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak
  Defenders'
arxiv_id: '2505.24519'
source_url: https://arxiv.org/abs/2505.24519
tags:
- safety
- amia
- image
- intention
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMIA is an inference-time defense method that enhances the safety
  of large vision-language models (LVLMs) against jailbreak attacks without retraining.
  It combines automatic masking of image patches least relevant to the input text
  with joint intention analysis to uncover hidden harmful intents.
---

# AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders

## Quick Facts
- arXiv ID: 2505.24519
- Source URL: https://arxiv.org/abs/2505.24519
- Authors: Yuqi Zhang; Yuchun Miao; Zuchao Li; Liang Ding
- Reference count: 23
- Primary result: 81.7% avg defense success rate with only 2% utility drop, no retraining

## Executive Summary
AMIA is an inference-time defense method that enhances the safety of large vision-language models (LVLMs) against jailbreak attacks without retraining. It combines automatic masking of image patches least relevant to the input text with joint intention analysis to uncover hidden harmful intents. Experiments on four jailbreak datasets show that AMIA improves defense success rates from 52.4% to 81.7% across diverse LVLMs, while preserving general utility with only a 2% average accuracy drop and modest inference overhead (~14%). Both components—masking and intention analysis—are essential for robust safety-utility trade-off.

## Method Summary
AMIA is a two-stage, training-free defense that operates entirely at inference time. First, it automatically masks a small set of image patches that have low semantic relevance to the input text, using a visual encoder to compute patch-text cosine similarity and masking the K lowest-scoring patches. Second, it applies a structured joint intention analysis prompt that forces the LVLM to articulate the essential intention before generating a final response. The method is designed to disrupt adversarial perturbations while preserving utility-critical information, and to expose harmful intent through explicit articulation. Experiments validate that both components are necessary and that the approach achieves strong defense gains with minimal utility degradation.

## Key Results
- Defense success rate improves from 52.4% to 81.7% across four jailbreak datasets
- General utility preserved with only 2% average accuracy drop on MMVP, AI2D, and MMStar benchmarks
- Both automatic masking and intention analysis are essential; ablation shows neither alone matches joint performance
- Single-pass inference adds only ~14% overhead compared to baseline LVLM inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective masking of text-irrelevant image patches disrupts adversarial perturbations while preserving utility-critical visual information.
- Mechanism: The image is divided into N patches; a visual encoder computes cosine similarity between each patch embedding and the text embedding; the K lowest-scoring patches are masked (set to black), breaking adversarial patterns that typically distribute perturbations across less semantically relevant regions.
- Core assumption: Adversarial perturbations are more fragile to masking of low-relevance regions than benign visual information is, and patch-level semantic relevance correlates with importance for general utility.
- Evidence anchors:
  - [abstract] "Automatically Masks a small set of text-irrelevant image patches to disrupt adversarial perturbations"
  - [section 2.1] "even when discarding a small portion of image information, AMIA significantly reduces the impact of adversarial image perturbation on LVLM safety, while largely preserving their general utility"
  - [corpus] CertMask (arXiv:2511.09834) addresses adversarial patches via mask coverage, providing related signal on masking as a defense class; corpus has limited direct replication of correlation-driven patch selection.
- Break condition: If attackers concentrate perturbations in text-relevant patches, or if K is set too high, utility degrades (Figure 4(a) shows MMVP accuracy drops at K=4 for N=16).

### Mechanism 2
- Claim: Structured joint intention analysis exposes harmful intent before final response generation, reactivating the LLM backbone's safety mechanisms.
- Mechanism: A single inference pass includes an instruction prompting the LVLM to first output "[INTENTION ANALYSIS]" describing the essential intention, then "[FINAL RESPONSE]". Explicitly articulating intent is assumed to make harmful goals more detectable to the base LLM's learned refusal behaviors.
- Core assumption: LVLMs retain usable safety from their LLM backbone, and forcing explicit intention articulation makes covert harmful intents more salient to these inherited refusal circuits.
- Evidence anchors:
  - [abstract] "conducts joint intention analysis to uncover and mitigate hidden harmful intents before response generation"
  - [section 1] "encourages LVLMs to identify and express potential harmful intention in text, thus reactivating the intrinsic safety of the LLM backbones within LVLMs"
  - [corpus] Bidirectional Intention Inference (arXiv:2509.22732) supports intention-based defense gains in LLMs, but direct LVLM evidence remains limited; ORCA (arXiv:2509.15435) signals reasoning/robustness links but does not validate intention analysis causality here.
- Break condition: If intention analysis outputs are gamed or suppressed, or if safety is largely visual rather than linguistic, this mechanism alone may not suffice (ablation shows masking is still needed for optimal defense).

### Mechanism 3
- Claim: The combination of masking and intention analysis is necessary for a robust safety-utility trade-off; neither component alone matches joint performance.
- Mechanism: Masking neutralizes visual adversarial structure; intention analysis compensates for multimodal joint attacks where harm emerges from image-text combinations; together they provide layered defense.
- Core assumption: Attack vectors span both perturbation-based visual exploits and joint semantic harms, requiring complementary mitigations.
- Evidence anchors:
  - [abstract] "Ablation confirms both masking and intention analysis are essential for a robust safety-utility trade-off"
  - [table 3] On Llava-v1.5-13B, intention analysis alone (63.7 MMVP Acc, 78.5 DSR) underperforms AMIA (63.0 MMVP Acc, 89.5 DSR); random masking also underperforms correlation-driven masking
  - [corpus] Zero-Shot Defense (arXiv:2503.00037) shows alignment-based visual defenses are possible, but corpus lacks direct ablation comparing masking vs intention vs both.
- Break condition: If runtime budget precludes both stages, or if joint analysis is poorly prompted, the trade-off degrades; hyperparameter sensitivity (K, N) may also impact robustness (Figure 4).

## Foundational Learning

- **Concept**: Vision-Language Joint Embeddings and Similarity
  - Why needed here: Understanding how images and text are projected into a shared space, and how cosine similarity identifies semantically relevant patches, is prerequisite to grasping why correlation-driven masking preserves utility.
  - Quick check question: Given a CLIP-style embedding space, what does a low cosine similarity between an image patch and a text prompt likely indicate about their semantic relationship?

- **Concept**: Adversarial Perturbations and Robustness
  - Why needed here: AMIA targets visually stealthy adversarial patterns; knowing how perturbations distribute and why masking can break their structure helps anticipate failure modes and attacker adaptations.
  - Quick check question: Why might localized masking of low-relevance patches disrupt an adversarial perturbation more than a benign image's semantics?

- **Concept**: Intent Recognition and Refusal Mechanisms in LLMs
  - Why needed here: The intention analysis stage relies on the assumption that LLMs have internal refusal circuits triggered by explicit intent articulation; understanding this helps calibrate expectations for the second mechanism.
  - Quick check question: What are two ways an LLM's safety training might cause it to refuse a request upon explicit articulation of harmful intent?

## Architecture Onboarding

- **Component map**: Input preprocessor -> Prompt constructor -> Single-pass LVLM inference -> Output parser
- **Critical path**: Image-to-patch splitting → encoder similarity computation → patch masking → prompt assembly with structured instruction → LVLM single-step inference → parse intention and response
- **Design tradeoffs**: Safety vs utility: higher K increases DSR but may reduce MMVP accuracy after a threshold (K=4 for N=16); N granularity affects masking precision. Overhead vs complexity: single-step intention analysis adds ~14% inference overhead; avoiding multi-turn or external LLM calls keeps it lightweight. Encoder dependency: reliance on VisRAG-Ret introduces an external model; alternative encoders not yet validated.
- **Failure signatures**: Utility drop when K is too large or when text-irrelevant patches contain task-critical visual cues. Attack success if adversarial perturbations concentrate in high-relevance patches or if intention analysis prompt is circumvented. Inconsistent outputs if LVLM fails to follow structured tag formatting.
- **First 3 experiments**:
  1. Replicate core defense: Implement N=16, K=3 masking with VisRAG-Ret on LLaVA-v1.5-7B; measure DSR on VisualAdv-Harmbench and accuracy on MMVP to verify baseline (52.4%→81.7% DSR, ~2% accuracy drop).
  2. Ablation of components: Run intention-only, masking-only, random masking, and full AMIA on the same model/dataset; confirm both components contribute and correlation-driven masking outperforms random.
  3. Sensitivity sweep: Vary K=1–4 and N across values while tracking DSR vs MMVP accuracy to reproduce the safety-utility trade-off curve and identify optimal K for your target LVLM.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive masking strategies outperform the fixed hyperparameter settings used in the current AMIA implementation?
  - Basis in paper: [explicit] The Limitations section states that "exploring globally optimal and adaptive masking strategies presents an exciting direction" for further improvement.
  - Why unresolved: The current method relies on empirically determined static values ($N=16, K=3$) for all inputs.
  - Evidence to resolve it: Benchmarks comparing the fixed $K=3$ baseline against a dynamic algorithm that adjusts masking ratios based on image content or adversarial detection scores.

- **Open Question 2**: How does AMIA's effectiveness scale with higher-resolution inputs or alternative visual encoders beyond VisRAG-Ret?
  - Basis in paper: [explicit] The Limitations section notes that the "applicability of our method to higher-resolution inputs and alternative encoders warrants further exploration."
  - Why unresolved: Experiments were restricted to standard resolutions (e.g., 336px) and a specific retrieval encoder.
  - Evidence to resolve it: Evaluation results on high-resolution benchmarks (e.g., 4K images) or LVLMs utilizing different vision backbones (e.g., SigLIP, EVA).

- **Open Question 3**: Is the image-text correlation-driven masking mechanism robust against adversaries who optimize perturbations to maximize semantic similarity with the text?
  - Basis in paper: [inferred] The masking relies on the VisRAG-Ret encoder to identify "least relevant" patches; an attacker could theoretically optimize noise to appear semantically relevant to the text to avoid being masked.
  - Why unresolved: The paper evaluates existing attacks but does not test against adaptive attacks designed specifically to evade the relevance filter.
  - Evidence to resolve it: Defense Success Rate (DSR) under attacks explicitly constrained to maintain high cosine similarity with the masking encoder.

## Limitations

- Reliance on external VisRAG-Ret encoder for patch-text similarity is a critical dependency with unspecified implementation details
- Claim that correlation-driven masking outperforms random masking assumes robustness of the patch-text similarity measure
- Intention analysis effectiveness depends on inherited safety mechanisms that may vary significantly across LVLM architectures
- Single-pass inference limits ability to handle complex multi-turn attacks
- Reported 2% utility drop and 14% overhead are model-averaged; individual model performance may deviate

## Confidence

**High confidence**: The general approach of combining selective masking with intention analysis is novel and technically sound; the reported defense success rate improvement from 52.4% to 81.7% is specific and measurable; the safety-utility trade-off claim is supported by ablation results showing both components are necessary.

**Medium confidence**: The claim that correlation-driven masking outperforms random masking assumes the patch-text similarity measure is robust and meaningful; the 2% average utility drop is model-averaged and may not hold for all LVLMs or tasks; the 14% inference overhead is specific but depends on the base model's inference cost.

**Low confidence**: The long-term robustness against adaptive attackers who may circumvent either masking or intention analysis; the effectiveness of intention analysis across all LVLM architectures with varying safety training; the generalizability of the safety-utility trade-off beyond the tested datasets and models.

## Next Checks

1. **Robustness to adaptive attacks**: Test AMIA against attackers who specifically target text-relevant patches or manipulate the intention analysis output format to evade detection. Measure how quickly DSR degrades as attackers adapt.

2. **Cross-model safety transfer**: Apply AMIA to LVLMs not in the original evaluation (e.g., different sizes or architectures) and verify that the safety-utility trade-off holds. Assess whether the VisRAG-Ret encoder's patch-text similarity remains meaningful across models.

3. **Ablation of encoder dependency**: Replace VisRAG-Ret with alternative vision-language encoders (e.g., CLIP, BLIP) for the masking stage and measure the impact on both DSR and utility preservation. This checks whether the method's effectiveness is tied to a specific encoder.