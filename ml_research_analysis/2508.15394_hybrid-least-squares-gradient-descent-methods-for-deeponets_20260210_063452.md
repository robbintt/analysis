---
ver: rpa2
title: Hybrid Least Squares/Gradient Descent Methods for DeepONets
arxiv_id: '2508.15394'
source_url: https://arxiv.org/abs/2508.15394
tags:
- adam
- deeponet
- training
- solution
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid least squares/gradient descent (LSGD)
  method to accelerate DeepONet training. DeepONets, neural operators that learn mappings
  between infinite-dimensional function spaces, are computationally expensive to train
  due to their complex structure involving two coupled neural networks.
---

# Hybrid Least Squares/Gradient Descent Methods for DeepONets

## Quick Facts
- **arXiv ID:** 2508.15394
- **Source URL:** https://arxiv.org/abs/2508.15394
- **Reference count:** 37
- **Primary result:** 10-20x speedup in DeepONet training with comparable accuracy using hybrid LS+Adam method

## Executive Summary
This paper introduces a hybrid least squares/gradient descent (LSGD) method to accelerate DeepONet training by exploiting the linearity of DeepONet's output with respect to the last layer parameters of the branch network. The method addresses the computational challenge of solving a prohibitively large LS system by decomposing it into two smaller subproblems using Kronecker factorization and Sylvester equations. Experiments on advection, diffusion-reaction, and Poisson equations demonstrate significant training acceleration while maintaining accuracy.

## Method Summary
The method exploits the fact that DeepONet outputs are linear with respect to the branch network's last layer parameters. By constraining this layer to be linear (no bias/activation), these parameters can be optimized via closed-form least squares rather than iterative gradient descent. The key innovation is decomposing the large LS system using Kronecker factorization into two smaller Sylvester equations solvable via spectral decomposition. The hybrid approach alternates between Adam updates for hidden layers and LS solves for the last layer, achieving 10-20x speedup in work units compared to Adam-only training.

## Key Results
- LS+Adam achieved 10-20x speedup in training DeepONets for various PDEs
- For Poisson equation with variable boundary conditions, LS+Adam reached 1.34e-3 mean relative L2 error at 10,000 work units vs. Adam-only's 4.50e-3 at 100,000 work units
- Method generalizes to L2 losses with regularization terms and physics-informed learning
- Validated on 1D advection, diffusion-reaction, and Poisson equations with Gaussian process-generated training data

## Why This Works (Mechanism)

### Mechanism 1: Linear Last Layer Optimization
- **Claim:** The DeepONet output's linearity with respect to branch last layer parameters enables analytical LS optimization
- **Mechanism:** DeepONet output $\langle b(u), t(y) \rangle$ becomes linear when branch last layer is constrained to be linear, transforming optimization into convex regression
- **Core assumption:** L2 loss function fits standard Least Squares form
- **Evidence:** Abstract explicitly states these parameters can be optimized using LS solve; method relies on linear output structure

### Mechanism 2: Kronecker Factorization
- **Claim:** Kronecker factorization decomposes large LS system into two smaller, solvable subproblems
- **Mechanism:** Direct LS requires inverting $IJ \times IJ$ matrix; factorization converts vector LS to matrix Sylvester equation solvable via spectral decomposition
- **Core assumption:** Data collection must be Cartesian product of branch and trunk inputs
- **Evidence:** Theorem 3 proves matrix $A_k$ can be factored as $K_{P,Q_k}(T_k \otimes B)$; reduces memory requirements

### Mechanism 3: Alternating Optimization
- **Claim:** Alternating LS and Adam steps accelerates convergence by frequently resolving optimal linear combination
- **Mechanism:** Adam shapes basis functions while LS instantly resets coefficients to global optimum for current basis
- **Core assumption:** Hidden layers provide valid approximation basis before convergence
- **Evidence:** Section 4 shows LS+Adam achieves comparable errors at 10% of training epochs; Algorithm 2 defines 5 Adam + 1 LS alternation

## Foundational Learning

- **Concept:** DeepONet Architecture (Branch and Trunk Networks)
  - **Why needed:** Method relies on factorization of branch (input function) and trunk (output coordinate) sub-networks
  - **Quick check:** Can you identify which network processes input sensor locations vs. query coordinates?

- **Concept:** Kronecker Product and Sylvester Equations
  - **Why needed:** Core speedup comes from replacing large vector operation with matrix equation $AXB = E$
  - **Quick check:** If matrix $A$ is $M \times N$ and $B$ is $P \times Q$, what are dimensions of $A \otimes B$?

- **Concept:** Tikhonov Regularization (Ridge Regression)
  - **Why needed:** Paper adds regularization term $\lambda \|C\|_F^2$ to ensure matrix system is full-rank
  - **Quick check:** What happens to eigenvalues of singular matrix $A^TA$ when you add $\lambda I$?

## Architecture Onboarding

- **Component map:**
  - Branch Network: Input $u$ → Hidden Layers ($\theta_B$) → **Linear Last Layer** ($\theta_L = C$) → Output $b(u)$
  - Trunk Network: Input $y$ → Hidden Layers ($\theta_T$) → Output $t(y)$
  - LS Solver: Takes pre-outputs $\tilde{b}(u)$ and trunk outputs $t(y)$, solves Sylvester equation to update $C$

- **Critical path:**
  1. Forward Pass: Compute hidden branch outputs $\tilde{B}$ and trunk outputs $T$
  2. Factorization: Compute $B^T B$ and $\sum \epsilon_k T_k^T T_k$
  3. Spectral Decomposition: Eigendecomposition of smaller matrices to solve for $C$ (Eq. 3.14)
  4. Backward Pass: Use Adam to update $\theta_B$ and $\theta_T$ only (freeze $\theta_L$ for gradient step)

- **Design tradeoffs:**
  - Batch Size: LS step requires full data batch vs. Adam's mini-batches, increasing memory requirements
  - Architecture: Branch network must end in linear layer, restricting flexibility

- **Failure signatures:**
  - Singular Matrix Error: Dead neurons or poor initialization cause near-zero eigenvalues; fix by increasing $\lambda$
  - Slow Convergence: Insufficient Adam pre-training causes LS step to overfit to noise immediately

- **First 3 experiments:**
  1. Unit Test: Implement Sylvester solver (Eq. 3.14) with random small matrices, verify against `numpy.linalg.lstsq`
  2. Supervised Baseline: Train on 1D Advection equation comparing pure Adam vs. LS+Adam, plot relative L2 error vs. Work Units
  3. Ablation on $\lambda$: Run Poisson equation with $\lambda = \{0, 10^{-9}, 10^{-6}\}$ to observe regularization impact

## Open Questions the Paper Calls Out

### Open Question 1: Nonlinear Operators
- **Question:** Can LSGD method be adapted for physics-informed losses with nonlinear or input-dependent operators?
- **Basis:** Paper states method cannot apply to unsupervised learning when physics term operator is nonlinear or depends on input function
- **Why unresolved:** Current formulation relies on linearity and separability of operator $L_k$ for Kronecker factorization
- **Evidence needed:** Theoretical extension accommodating non-separable terms or numerical experiments on nonlinear reaction terms

### Open Question 2: Non-Cartesian Data
- **Question:** How to modify factorization for training scenarios without Cartesian product data structure?
- **Basis:** Method efficiency depends on assumption that data can be represented as $\beta \times \tau_k$
- **Why unresolved:** Without Cartesian product, commutation matrix $K_{P,Q_k}$ cannot reduce large LS system, making LS step potentially infeasible
- **Evidence needed:** Reformulation avoiding Kronecker decomposition for sparse/irregular data or empirical results with random sampling

### Open Question 3: Other Neural Operators
- **Question:** Can hybrid strategy generalize to MIONet or Fourier Neural Operators?
- **Basis:** Paper mentions MIONet as DeepONet generalization but restricts methodology to "vanilla DeepONets"
- **Why unresolved:** MIONets involve tensor products, FNOs use spectral convolutions requiring different Sylvester equation forms
- **Evidence needed:** Derivation for MIONet inner product structure or benchmarks on non-DeepONet operator learning tasks

## Limitations
- Method requires branch network's last layer to be linear (no bias/activation), restricting architectural flexibility
- LS step requires full-batch computation, creating memory constraints for large datasets
- Cartesian product data structure assumption may not hold for problems with irregular input distributions

## Confidence
- **High Confidence:** Mathematical derivation of Kronecker factorization and Sylvester equation solver; experimental speedup results for advection and diffusion-reaction equations
- **Medium Confidence:** Comparison between LS+Adam and Adam-only training due to unspecified architectural details
- **Low Confidence:** Claims about general operator learning applicability beyond tested PDE examples, especially for high-dimensional or non-grid-based problems

## Next Checks
1. Implement factorized LS solver on simple test case (2x2 matrices) and verify identical results to standard Kronecker-based least squares
2. Reproduce 1D advection equation results comparing pure Adam vs. LS+Adam training, specifically verifying 10x speedup claim in work units
3. Conduct ablation study on regularization parameter λ across values (0, 10⁻⁹, 10⁻⁶) to determine impact on numerical stability and performance