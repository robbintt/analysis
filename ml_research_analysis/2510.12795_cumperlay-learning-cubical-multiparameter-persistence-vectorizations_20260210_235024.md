---
ver: rpa2
title: 'CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations'
arxiv_id: '2510.12795'
source_url: https://arxiv.org/abs/2510.12795
tags:
- slice
- persistence
- layer
- filtration
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CuMPerLay introduces a differentiable vectorization layer for Cubical
  Multiparameter Persistence (CMP), enabling end-to-end learning of topological features
  in deep learning pipelines. The method decomposes CMP into learnable single-parameter
  filtrations and integrates with architectures like Swin Transformers through differentiable
  vectorization.
---

# CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations

## Quick Facts
- **arXiv ID**: 2510.12795
- **Source URL**: https://arxiv.org/abs/2510.12795
- **Reference count**: 40
- **Primary result**: Introduces differentiable vectorization layer for Cubical Multiparameter Persistence (CMP) enabling end-to-end topological learning in deep networks

## Executive Summary
CuMPerLay introduces a novel differentiable vectorization layer for Cubical Multiparameter Persistence (CMP) that enables end-to-end learning of topological features in deep learning pipelines. The method decomposes CMP into learnable single-parameter filtrations and integrates with architectures like Swin Transformers through differentiable vectorization. Theoretical analysis proves stability under generalized Wasserstein metrics. Experiments on ISIC 2018, CBIS-DDSM, Glaucoma, and PASCAL VOC 2012 datasets show consistent performance improvements, with TopoSwin-MP achieving 84.63% accuracy on Glaucoma (vs 82.73% baseline) and 96.50% AUC on ISIC (vs 95.54% baseline). The approach particularly benefits limited-data scenarios, with topological features providing robust anchors for learning. The work also includes the first CUDA GPU implementation of Cubical Persistence and CMP, enabling efficient computation.

## Method Summary
CuMPerLay introduces a differentiable vectorization layer for Cubical Multiparameter Persistence (CMP) that enables end-to-end learning of topological features in deep learning pipelines. The method decomposes CMP into learnable single-parameter filtrations using a multifiltration decoder (CNN upsampling) that generates a filtration image, which is then thresholded through a staircase function to create individual filtrations. These are processed through a custom CUDA implementation of cubical persistence (computing H₀ and H₁) followed by PersLay vectorization with learnable weights and times. The resulting topological features are aggregated through an MLP and fused with intermediate Swin Transformer activations via a residual gated linear unit. The entire pipeline is trained end-to-end with a combined loss function incorporating classification, topological classification, bifiltration regularization, and local entropy terms. The approach is validated on four datasets (ISIC 2018, CBIS-DDSM, Glaucoma, PASCAL VOC 2012) showing consistent performance improvements, particularly in limited-data scenarios.

## Key Results
- TopoSwin-MP achieves 84.63% accuracy on Glaucoma dataset (vs 82.73% baseline)
- ISIC 2018: 96.50% AUC with CuMPerLay vs 95.54% baseline
- PASCAL VOC 2012: +0.5 mIoU improvement over standard Swin
- Performance gains are most pronounced in limited-data scenarios

## Why This Works (Mechanism)
CuMPerLay works by learning a continuous vectorization of cubical multiparameter persistence that can be integrated into deep learning pipelines. The key mechanism is the differentiable decomposition of CMP into learnable single-parameter filtrations, which are then processed through a custom CUDA implementation of cubical persistence. This allows the topological features to be learned end-to-end rather than being hand-crafted. The stability proof under generalized Wasserstein metrics ensures that small perturbations in the input lead to small changes in the learned representations, making the features robust to noise. The topological features provide additional constraints that act as anchors in the feature space, particularly beneficial when training data is limited.

## Foundational Learning
- **Cubical Persistence Theory**: Understanding of cubical complexes and homology groups H₀/H₁ is essential for implementing the persistence computation. Quick check: Verify H₀/H₁ barcodes match GUDHI on small test cases.
- **Differentiable Programming**: The staircase thresholding requires straight-through estimators to maintain gradient flow. Quick check: Confirm non-zero gradients propagate through persistence layer.
- **Wasserstein Stability**: The theoretical foundation ensuring learned representations are stable to input perturbations. Quick check: Measure distance between persistence diagrams under small input changes.
- **CUDA Programming**: Custom GPU implementation requires understanding of parallel union-find algorithms and memory management. Quick check: Benchmark against CPU implementation for correctness and speed.
- **Multi-Task Learning**: The combined loss formulation requires balancing classification and topological objectives. Quick check: Monitor both CE and topological losses during training.
- **Feature Fusion**: Integration of topological features with intermediate activations requires careful architectural design. Quick check: Verify feature dimensions match across fusion points.

## Architecture Onboarding

**Component Map**: Input Image -> SwinV2-B Backbone -> CuMPerLay Modules (after each block + input) -> Topological Classification Head -> Fusion Block -> Output

**Critical Path**: The critical path is the gradient flow from output through the topological classification head, back through the CuMPerLay vectorization layers, persistence computation, and multifiltration decoder to the frozen Swin backbone. The staircase thresholding requires straight-through estimation to maintain differentiability.

**Design Tradeoffs**: The method trades computational complexity (custom CUDA persistence) for improved generalization in limited-data scenarios. The 8×16 MP grid provides a balance between capturing sufficient topological structure and computational feasibility. The combined loss formulation requires careful weight tuning to balance semantic and topological objectives.

**Failure Signatures**: Non-differentiable persistence computation will result in zero gradients at the vectorization layer. Incorrect union-find implementation in CUDA will produce wrong persistence pairs or crash. Mismatched feature dimensions in the fusion block will cause runtime errors. Overly aggressive regularization weights will suppress learned topological features.

**Three First Experiments**:
1. **Unit test CUDA persistence** against GUDHI on synthetic cubical complexes with known H₀/H₁ barcodes to verify correctness of the union-find implementation and staircase thresholding.
2. **Ablation study on loss weights** (L_topo, L_reg, L_LE) across the four datasets to determine sensitivity and optimal configuration for different data regimes.
3. **Transfer learning validation** by freezing the learned CuMPerLay modules and testing on held-out pathology datasets to assess the generalization of topological feature extraction.

## Open Questions the Paper Calls Out
None

## Limitations
- CUDA persistence implementation details are not fully specified, making independent validation challenging
- Performance improvements, while consistent, are modest (1-2% gains), suggesting topological features capture subtle rather than dominant signals
- The optimal architectural choices for multifiltration decoding and topological classification heads remain unclear

## Confidence
- **High confidence**: CUDA implementation enables efficient CMP computation; theoretical stability under generalized Wasserstein metrics; experimental protocol and datasets are clearly specified
- **Medium confidence**: The topological features provide robust anchors in limited-data scenarios; the combined loss formulation effectively integrates topological and semantic objectives
- **Low confidence**: The specific architectural choices for multifiltration decoding and topological classification heads are optimal; the modest performance gains translate to meaningful practical improvements in real-world deployment

## Next Checks
1. **Unit test the CUDA persistence layer** against GUDHI on synthetic cubical complexes with known H₀/H₁ barcodes to verify correctness of the union-find implementation and staircase thresholding.
2. **Ablation study on loss weights** (L_topo, L_reg, L_LE) across the four datasets to determine sensitivity and optimal configuration for different data regimes.
3. **Transfer learning validation** by freezing the learned CuMPerLay modules and testing on held-out pathology datasets to assess the generalization of topological feature extraction.