---
ver: rpa2
title: Automated Meta Prompt Engineering for Alignment with the Theory of Mind
arxiv_id: '2505.09024'
source_url: https://arxiv.org/abs/2505.09024
tags:
- human
- content
- arxiv
- generative
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel method for aligning generative AI content
  with human expectations by using a Theory of Mind (ToM) approach. The core idea
  involves iteratively optimizing the similarity of neural states between human mental
  expectations and Large Language Model (LLM) outputs through agentic reinforcement
  learning.
---

# Automated Meta Prompt Engineering for Alignment with the Theory of Mind

## Quick Facts
- **arXiv ID:** 2505.09024
- **Source URL:** https://arxiv.org/abs/2505.09024
- **Reference count:** 0
- **Primary result:** 53.8% of generated tennis match reports fully aligned with human expectations using Theory of Mind optimization

## Executive Summary
This paper presents a novel method for aligning generative AI content with human expectations by iteratively optimizing the similarity of neural states between human mental expectations and Large Language Model (LLM) outputs. The core innovation uses a Theory of Mind (ToM) approach where an LLM-as-a-Judge (LLMaaJ) evaluates generated text across multiple dimensions (factualness, novelty, repetitiveness, relevance) and provides feedback to an LLM-as-an-Editor (LLMaaE) that refines prompts. Applied to generate tennis match reports during the US Open 2024, the system achieved full alignment with human expectations 53.8% of the time, with an average of 4.38 iterations needed for convergence. The method significantly improves content quality and reduces manual editing effort compared to baseline approaches.

## Method Summary
The method employs an agentic pipeline where IBM Granite 13B Chat generates factual bullets from match statistics while Llama 3 70B writes fluent paragraphs. An LLM-as-a-Judge (Llama 3 70B) evaluates output on four dimensions (Factualness, Novelty, Repetitiveness, Topic Alignment) using few-shot examples, producing score vectors. The geometric interpretation of content traits over Hilbert space combines spatial volume with vertices alignment, enabling the LLMaaJ to optimize on Human ToM. An LLM-as-an-Editor (Granite 13B) rewrites prompts based on delta explanations from the judge's evaluation. The system iterates until convergence (loss < 0.05) or timeout (2 minutes/21 iterations), with personalized user profiles tracking dimensional scores to enable faster convergence for subsequent content from the same editor.

## Key Results
- 53.8% of generated content fully aligned with human expectations (geometric loss < 0.05)
- Average of 4.38 iterations required for convergence across all dimensions
- 46.2% of content published without full convergence, but still preferred over initial generation
- Factualness degraded (15.1% worse) when convergence failed, while novelty and relevance improved

## Why This Works (Mechanism)

### Mechanism 1: Geometric ToM Optimization
Iterative prompt refinement through LLMaaJ feedback reduces the geometric distance between human expectation profiles and generated content. The judge evaluates orthogonal dimensions producing a score vector, while a covariance matrix captures inter-dimensional relationships. The difference between human-edited and generated polygons is quantified via Theory of Mind Area (tma) and Theory of Mind Distance (tmd). LLMaaE rewrites prompts based on delta explanations, guided by spatial judgment differences.

### Mechanism 2: Chain-of-Thought Prompt Refinement
CoT reasoning embedded in meta-prompts enables LLMaaE to interpret multi-dimensional feedback and generate targeted prompt modifications. After each LLMaaJ evaluation, a ToM CoT explanation describing the spatial judgment is appended to the next prompt, providing explicit reasoning steps that guide principled edits rather than random adjustments.

### Mechanism 3: Personalized Profile Convergence
Personalized user profiles that track average dimensional scores enable faster convergence on subsequent content for the same editor. Each human editor's modifications are captured as a profile model storing expected averages for novelty, repetitiveness, and factualness. The optimization problem uses stochastic gradient descent to adjust instructions, temperature, and top-k to maximize alignment probability for that profile.

## Foundational Learning

- **Theory of Mind (ToM) in AI Contexts**: Frames alignment as inferring human mental states (beliefs, expectations) from observable edits rather than simple preference learning. Quick check: Can you explain why ToM is more than just preference learning? (Hint: It involves modeling intentions and beliefs, not just reward signals.)

- **Reinforcement Learning with Human Feedback (RLHF)**: The iterative loop where LLMaaJ provides feedback and LLMaaE adjusts prompts is an RLHF variant. Understanding credit assignment and policy updates is essential for debugging non-convergence. Quick check: What's the difference between online RL, offline RL, and RLHF? Which does this paper most closely approximate?

- **Hilbert Space and Geometric Loss**: The paper represents content traits as vectors in Hilbert space and computes polygon areas/determinants as loss. Basic linear algebra (determinants, covariance matrices) is required to implement and debug the geometric calculations. Quick check: Given a 3x3 covariance matrix, how would you compute the "volume" of the corresponding polygon? What does a near-zero volume imply about dimension orthogonality?

## Architecture Onboarding

- **Component map**: Event Ingestion -> Feature Extractor + Generative AI Executor -> LLMaaJ (Judge) -> Profile Store -> LLMaaE (Editor) -> Optimization Loop -> Content Delivery

- **Critical path**: Match ends → Kafka message → Feature extraction → Bullet generation → Section writing → LLMaaJ evaluation → Compare to profile → Generate delta explanation → LLMaaE prompt rewrite → Repeat with new prompt → Check convergence → Publish or iterate

- **Design tradeoffs**: 4D evaluation (includes repetitiveness) vs 3D evaluation (omits repetitiveness) - 4D captures more nuance but increases optimization complexity. Timeout (2 min) vs convergence threshold balances editor patience against quality. Shared vs per-editor profiles reduce storage but ignore individual variance.

- **Failure signatures**: Non-convergence (46.2% of cases) where one dimension improves while another degrades. High variance across editors if profiles are noisy. Timeout exhaustion (21 iterations without convergence) indicating LLMaaE may be looping on similar instructions.

- **First 3 experiments**: 1) Baseline alignment test without RLHF to measure initial dimensional deltas. 2) Single-dimension perturbation test to verify LLMaaE generates correct instructions. 3) Convergence stress test with synthetic extreme profiles to measure iteration count vs paper's 4.38 average.

## Open Questions the Paper Calls Out

- **Custom dimension definition**: The authors would like to enable editors to define their own dimensions, definitions, and measurement criteria to align towards their priorities, moving beyond the fixed set of four dimensions currently used.

- **Condorcet Jury Theorem extension**: The authors propose extending their work to follow the Condorcet Jury Theorem by adding LLM jury members to the judge chamber to improve judgment accuracy through majority voting.

- **Shared ToM profile implications**: The authors want to study the implications of sharing and mixing ToM profiles to create emergent styles around each dimension of thought, exploring whether aggregated profiles can create synthetic editorial voices.

- **Mathematical vs human preference alignment**: The paper notes that editors preferred non-converged content despite the mathematical convergence threshold (loss < 0.05), suggesting the optimization target may not perfectly capture utility and may require re-weighting.

## Limitations

- Small sample size with only 254 reports and 4 human adjudicators limits generalizability
- Geometric ToM formulation may not capture full complexity of human mental states, reducing expectations to four scalar dimensions
- Assumes human edits are faithful representations of mental expectations, but editors may make inconsistent or context-dependent changes

## Confidence

- **High Confidence**: Iterative meta-prompting mechanism works as described (53.8% full alignment, 4.38 average iterations)
- **Medium Confidence**: ToM geometric interpretation meaningfully improves alignment over simpler methods
- **Low Confidence**: Generalizability to other domains and stability of editor profiles across sessions

## Next Checks

1. **Cross-adjudicator variance test**: Run the pipeline with multiple human editors editing the same initial content and measure variance in their edits to determine if LLMaaE can find prompts satisfying all editors simultaneously.

2. **Ablation on ToM geometry**: Implement simplified version optimizing dimensions independently (no covariance, no polygon areas) and compare convergence rates and content quality against full ToM method.

3. **Temporal preference drift analysis**: Generate content for the same editor across multiple matches separated by hours/days to track profile stability and measure re-convergence frequency.