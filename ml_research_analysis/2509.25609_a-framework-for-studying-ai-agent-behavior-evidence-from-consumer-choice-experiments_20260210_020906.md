---
ver: rpa2
title: 'A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice
  Experiments'
arxiv_id: '2509.25609'
source_url: https://arxiv.org/abs/2509.25609
tags:
- product
- agents
- gid00001
- agent
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ABxLAB, a framework for systematically studying
  AI agent decision-making by intercepting and modifying web content in real time.
  The authors apply it to consumer choice experiments in a realistic web-based shopping
  environment, varying prices, ratings, and psychological nudges.
---

# A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments

## Quick Facts
- arXiv ID: 2509.25609
- Source URL: https://arxiv.org/abs/2509.25609
- Reference count: 39
- Primary result: LLM agents exhibit systematic behavioral biases 3-10× stronger than humans in consumer choice experiments

## Executive Summary
This paper introduces ABxLAB, a framework for systematically studying AI agent decision-making by intercepting and modifying web content in real time. The authors apply it to consumer choice experiments in a realistic web-based shopping environment, varying prices, ratings, and psychological nudges. They find that LLM agents exhibit strong systematic biases in response to ratings, prices, order effects, and nudges, with effect sizes often 3-10× larger than human baselines. Agents are highly sensitive to ratings (30-80pp preference for higher-rated products), prices (15-65pp preference for cheaper options), and persuasive nudges (10-60pp shifts). Even simple textual cues like "best seller" or "Wirecutter's top pick" substantially alter agent choices. These biases persist across model families and are not driven by cognitive constraints as in humans, suggesting different underlying mechanisms.

## Method Summary
The ABxLAB framework extends Agent-Lab to intercept and modify web content in real time, enabling controlled behavioral experiments with AI agents in realistic web environments. The study uses OneStopMarket from WebArena as the shopping environment, conducting binary forced choice (2AFC) product selection tasks. Experiments vary three factors: price difference (50% cheaper vs. matched), rating difference (0.10 higher vs. matched), and 10 psychological nudges from behavioral economics. The framework tests 17 LLM models including GPT-5/4.1/4o variants, Claude, Gemini, Llama, and DeepSeek across 1,500 base configurations. Choice outcomes are analyzed using linear probability models with cluster-robust standard errors, comparing agent behavior to human baselines from previous literature.

## Key Results
- Agents show 30-80pp preference for higher-rated products versus 5-30pp in humans
- Agents exhibit 15-65pp preference for cheaper options versus 5-30pp in humans
- Persuasive nudges shift agent choices by 10-60pp, with authority cues being most effective
- Biases persist across model families and are not explained by cognitive constraints
- Simple textual cues like "best seller" or "Wirecutter's top pick" substantially alter agent choices

## Why This Works (Mechanism)

### Foundational Learning
- **ABxLAB interception framework**: Why needed - Enables controlled manipulation of agent environment for behavioral experiments; Quick check - Verify HTML modification occurs before agent observation
- **Binary forced choice design**: Why needed - Creates clear behavioral response measurement; Quick check - Confirm agents always select exactly one product
- **Cluster-robust regression analysis**: Why needed - Accounts for repeated measures across nudges and categories; Quick check - Verify two-way clustering structure in statistical output
- **Multi-condition experimental design**: Why needed - Isolates effects of price, rating, and nudge factors; Quick check - Confirm balanced randomization across all conditions
- **Cross-model comparison**: Why needed - Distinguishes systematic patterns from model-specific artifacts; Quick check - Verify consistent effect directions across model families
- **Human baseline comparison**: Why needed - Contextualizes agent behavior magnitude; Quick check - Confirm baseline effect sizes fall within literature ranges

### Architecture Onboarding

**Component Map**: OneStopMarket -> ABxLAB Interceptor -> Agent Environment -> Choice Outcome

**Critical Path**: Product catalog generation → Experimental condition assignment → HTML content modification → Agent observation → Action selection → Choice recording → Statistical analysis

**Design Tradeoffs**: Real-time content modification enables ecological validity but introduces latency; synthetic product data ensures control but may not reflect real-world complexity; temperature variation balances exploration and reproducibility across model types

**Failure Signatures**: Order effects dominating results (+90pp in some models); agents failing to visit both product tabs; rating/price matching violations in MR/MRaP conditions

**Three First Experiments**:
1. Run baseline condition (no nudges) to verify 50% choice probability
2. Test single high-effect nudge (authority) to confirm directional bias
3. Validate MR condition pairs satisfy exact price matching constraint

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the underlying mechanisms that drive systematic biases in LLM agent decision-making if not cognitive constraints?
- **Basis in paper**: [explicit] The authors state these responses occur "without the specific cognitive limitations that traditionally explain such biases... suggesting different underlying mechanisms (e.g. biased data) may drive artificial choice behavior."
- **Why unresolved**: The paper establishes existence and magnitude of biases but does not isolate whether they stem from training data correlations, reward modeling, or architectural properties.
- **What evidence would resolve it**: Ablation studies varying training data composition, comparing models trained on synthetic vs. human-generated data, or analyzing attention patterns during decision-making.

### Open Question 2
- **Question**: Do agent decision-making patterns generalize to consequential domains beyond consumer choice, such as medical or financial decisions?
- **Basis in paper**: [explicit] The authors acknowledge: "our evaluation focuses on one domain (consumer behavior)... the findings may differ in other domains."
- **Why unresolved**: Stakes, framing, and domain-specific norms may alter how agents weigh attributes; shopping nudges may not translate to clinical or investment contexts.
- **What evidence would resolve it**: Applying ABxLab-style interventions to medical treatment selection or financial product choice environments and comparing effect sizes.

### Open Question 3
- **Question**: Do agents with longer deliberation horizons exhibit different susceptibility to framing effects compared to faster-deciding agents?
- **Basis in paper**: [explicit] The appendix notes: "Future work should test whether these temporal patterns systematically condition sensitivity to interventions."
- **Why unresolved**: The paper documents heterogeneity in action steps before commitment but does not establish whether this mediates nudge effectiveness.
- **What evidence would resolve it**: Stratified analysis correlating per-trial deliberation length with intervention effect sizes, or controlled experiments manipulating deliberation prompts.

## Limitations
- Synthetic product data may not capture real-world complexity and consumer behavior nuances
- Post-hoc price matching in MRaP conditions introduces artificial constraints not present in natural shopping
- Temperature variation across models (0.1 vs 1) complicates direct cross-model comparisons
- Human baseline interface differs from agent environment, potentially explaining magnitude differences

## Confidence

**High confidence**: Systematic biases in agent responses to ratings, prices, and persuasive nudges are consistently observed across multiple model families. The directional effects (higher ratings → higher choice probability, lower prices → higher choice probability, persuasive cues → higher choice probability) are robust and replicable.

**Medium confidence**: The magnitude differences between agents and humans (3-10× larger effects) are well-documented but may be influenced by interface differences and experimental conditions. The persistence of biases across model families suggests systematic patterns, though the underlying mechanisms remain incompletely understood.

**Medium confidence**: The claim that these biases are not driven by cognitive constraints as in humans is supported by the data but requires further investigation into whether alternative computational constraints might be responsible.

## Next Checks
1. **Reproduce core findings with alternative product catalogs**: Run the ABxLAB framework using a different synthetic product dataset or real product data to verify that the observed biases persist beyond the specific OneStopMarket environment used in the study.

2. **Cross-interface validation**: Implement the exact human baseline interface and recruitment protocol to conduct head-to-head comparisons under identical conditions, controlling for potential interface artifacts that could explain the magnitude differences.

3. **Mechanism isolation tests**: Design controlled experiments to systematically vary individual components (memory instructions, observation pruning, action constraints) to identify which framework elements contribute most to the observed behavioral patterns, distinguishing between model-level biases and framework-induced effects.