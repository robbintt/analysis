---
ver: rpa2
title: 'MR. Judge: Multimodal Reasoner as a Judge'
arxiv_id: '2505.13403'
source_url: https://arxiv.org/abs/2505.13403
tags:
- response
- reasoning
- judge
- image
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MR. Judge, a multimodal reasoning-enhanced
  framework for training LLM judges.
---

# MR. Judge: Multimodal Reasoner as a Judge

## Quick Facts
- arXiv ID: 2505.13403
- Source URL: https://arxiv.org/abs/2505.13403
- Reference count: 29
- Outperforms GPT-4o by 9.9% on VL-RewardBench and improves inference-time scaling on MM-Vet by up to 7.7%

## Executive Summary
This paper introduces MR. Judge, a multimodal reasoning-enhanced framework for training LLM judges that formulates judgment as a multiple-choice problem with explicit reasoning traces. The core innovation addresses the scarcity of high-quality multimodal preference data through reverse response candidate synthesis from existing SFT datasets and reasoning distillation from text-based reasoners. Experiments demonstrate that MR. Judge-7B achieves state-of-the-art performance on VL-RewardBench while providing interpretable judgments, and shows significant improvements in inference-time scaling scenarios.

## Method Summary
MR. Judge trains MLLMs as judges through a two-stage process: (1) warm-up supervised fine-tuning using reasoning traces distilled from text-based reasoners like DeepSeek-R1, and (2) GRPO reinforcement learning with composite rewards balancing accuracy and format compliance. The method generates diverse, flawed alternatives from SFT datasets via controlled error injection, creating contrastive pairs where relative quality is known. Training uses a multiple-choice formulation with explicit reasoning traces, enabling interpretable judgments and rule-based reward assignment.

## Key Results
- MR. Judge-7B outperforms GPT-4o by 9.9% on VL-RewardBench macro accuracy
- Improves inference-time scaling performance on MM-Vet by up to 7.7%
- Shows better generalization than zero-RL baselines on VL-RewardBench (71.1% vs 66.4% macro accuracy)
- Reasoning distillation significantly improves complex reasoning behaviors compared to direct RL training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Formulating judgment as a multiple-choice problem with explicit reasoning traces improves both interpretability and accuracy over scalar scoring approaches.
- **Mechanism:** The multiple-choice structure forces comparative reasoning across candidates before selection, reducing reliance on spurious features (e.g., response length). The reasoning trace provides transparent justification and creates a rule-based supervision signal where correctness is exact label matching.
- **Core assumption:** Comparative evaluation elicits more reliable judgments than absolute scoring, and reasoning traces genuinely reflect the decision process rather than post-hoc rationalization.
- **Evidence anchors:**
  - [abstract]: "Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem."
  - [Section 1]: "the multiple-choice structure encourages comparative reasoning between multiple candidate responses, which has been shown to enhance judgment accuracy in preference-based tasks"
  - [Section 5]: "enables rule-based reinforcement learning by constraining the output space to discrete choices, simplifying both reward assignment and policy optimization"

### Mechanism 2
- **Claim:** Reverse synthesis of flawed candidates from SFT data provides scalable training signal without external annotation.
- **Mechanism:** Treat SFT responses as "best" candidates by default, then prompt an MLLM to inject controlled errors (hallucination, incompleteness, incorrect reasoning, incorrect knowledge). This creates contrastive pairs where relative quality is known, even if absolute quality varies.
- **Core assumption:** The original SFT annotations are sufficiently high-quality that deliberately degraded alternatives are reliably worse. Only relative ranking matters for training.
- **Evidence anchors:**
  - [abstract]: "reverse response candidate synthesis strategy to generate diverse, flawed alternatives from existing SFT datasets"
  - [Section 4]: "our method does not assume the reference responses are perfect. The only requirement is that the synthesized negatives be of relatively lower quality"
  - [Table 1]: Detailed prompt for error injection with four error categories

### Mechanism 3
- **Claim:** Warm-up SFT with reasoning traces distilled from text-based reasoners unlocks complex reasoning behaviors that RL alone cannot induce.
- **Mechanism:** Generate image descriptions → feed to text reasoner (e.g., DeepSeek-R1) with hinted prompts → clean hint references and align style → fine-tune MLLM. This primes the model before RL refinement.
- **Core assumption:** Text-based reasoning capabilities transfer to multimodal contexts via textual image descriptions, and MLLMs' prior SFT has suppressed exploratory reasoning diversity.
- **Evidence anchors:**
  - [Section 6]: "we observe limited emergence of complex reasoning behaviors such as re-evaluation. We hypothesize that this stems from MLLMs' thorough supervised fine-tuning (SFT) stage, which enhances output structure but reduces diversity"
  - [Section 6.1]: "Modality Bridging via Image Description... allows text-only models such as DeepSeek-R1 to reason about and evaluate visual content"

## Foundational Learning

- **Concept: RLHF (Reinforcement Learning from Human Feedback)**
  - **Why needed here:** MR. Judge is positioned as a reward model alternative for RLHF pipelines and inference-time scaling. Understanding how judges provide scalar or comparative signals to policy models is essential context.
  - **Quick check question:** Can you explain how a judge model's output would be used as a reward signal during PPO training of a policy model?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** The paper's core innovation is requiring explicit reasoning traces (`<think_think>` blocks) before selection. Understanding CoT's role in improving task decomposition and interpretability is prerequisite.
  - **Quick check question:** What is the difference between implicit reasoning (model internals) and explicit chain-of-thought, and why might the latter improve judgment reliability?

- **Concept: Knowledge/Capability Distillation**
  - **Why needed here:** Section 6.1 describes distilling reasoning from text-based models (DeepSeek-R1) to MLLMs. Understanding how teacher-student setups transfer capabilities is critical.
  - **Quick check question:** Why might distillation from a text-only reasoner to a multimodal model require "style alignment" and "hint removal" post-processing?

## Architecture Onboarding

- **Component map:** [SFT Seed Data] → [Negative Candidate Generator (MLLM)] → [MC Question Builder] → [Image + MC Question] → [Text Reasoner (DeepSeek-R1)] → [Reasoning Trace Cleaner] → [MR-Judge-8K Dataset] → [Stage 1: Warm-up SFT] → [Stage 2: RL with GRPO] → [MR. Judge Model] → [Inference: Reasoning → Selection → (Optional: Majority Voting)]

- **Critical path:** The quality of negative candidates and reasoning trace distillation directly determines training signal quality. If negatives are too easy or reasoning traces are incoherent, downstream RL cannot recover.

- **Design tradeoffs:**
  - **Response length vs. reasoning depth:** Truncated reward (L=1024) prevents runaway length but may cap complex reasoning. Figure 4 shows this is necessary for stable training.
  - **Candidate count variability:** Training with 2-4 candidates improves generalization but increases prompt complexity.
  - **SFT warmup vs. direct RL:** Paper shows SFT+RL (71.1 macro accuracy) outperforms zero-RL (66.4) for 7B model, but adds data curation cost.

- **Failure signatures:**
  - **Format reward collapse:** Model generates repeated outputs or never-ending reasoning. Fix: truncated reward assignment.
  - **Position bias:** Model learns to prefer certain labels (A/B/C/D). Fix: mandatory order shuffling (Section 4).
  - **Hint leakage:** Reasoning traces reference "as the hint suggests." Fix: LLM-based cleaning (Table 7).
  - **Hallucinated reasoning:** Model generates plausible-sounding but ungrounded analysis. Observed in Figure 20.

- **First 3 experiments:**
  1. **Ablate warm-up SFT:** Train MR. Judge-7B with zero-RL vs. SFT+RL on same data split. Compare VL-RewardBench macro accuracy to quantify reasoning distillation contribution.
  2. **Stress-test negative quality:** Generate negative candidates with varying error severity (controlled by temperature or error-type frequency). Plot judgment accuracy vs. negative difficulty to find minimum viable contrast.
  3. **Evaluate inference-time scaling efficiency:** On MM-Vet, vary candidate count (2, 4, 8) and measure accuracy gain per additional candidate. Identify compute-optimal sampling budget for different task MLLM capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reasoning traces be shortened through adaptive or hierarchical reasoning mechanisms without degrading MR. Judge's evaluation accuracy?
- **Basis in paper:** [explicit] The Limitation section states: "As a promising direction for future work, it is worth exploring strategies to shorten the output length without compromising judgment quality" to address increased evaluation time and computational costs from lengthy reasoning.
- **Why unresolved:** The current framework requires full deliberative reasoning traces for each judgment, creating a trade-off between interpretability/accuracy and efficiency that has not been explored.
- **What evidence would resolve it:** Experiments comparing accuracy-efficiency Pareto curves across different reasoning compression strategies (e.g., early stopping, hierarchical reasoning, adaptive depth) on VL-RewardBench and MM-Vet.

### Open Question 2
- **Question:** Does the reliance on text-based reasoning models for distillation impose a ceiling on multimodal reasoning capabilities specific to visual judgment tasks?
- **Basis in paper:** [inferred] The paper acknowledges that "current SOTA MLLMs are not able to produce reliable complex reasoning traces," necessitating distillation from text-based reasoners via image descriptions. This indirect transfer may not capture visual nuances that native multimodal reasoning could.
- **Why unresolved:** The modality bridging via textual image descriptions may lose fine-grained visual information, and the extent to which this limits reasoning quality in visual judgment scenarios remains unquantified.
- **What evidence would resolve it:** Ablation studies comparing judges trained with native multimodal reasoning traces (if obtainable) versus text-distilled traces on vision-centric tasks requiring subtle visual discrimination.

### Open Question 3
- **Question:** How robust is MR. Judge to distribution shift when evaluated on adversarial or out-of-domain multimodal queries not represented in the MR-Judge-8K training mixture?
- **Basis in paper:** [inferred] The training data is curated from specific SFT datasets (Allava, AI2D, ChartQA, etc.), and the negative synthesis prompts target four predefined error categories. Generalization to novel error types or domains is not systematically evaluated.
- **Why unresolved:** The multiple-choice formulation with fixed evaluation criteria (harmfulness > accuracy > detailedness) may not transfer to domains with different quality dimensions or adversarial manipulations.
- **What evidence would resolve it:** Evaluation on adversarially constructed multimodal benchmarks with error types excluded from synthesis prompts, or on domains (e.g., medical imaging, satellite imagery) absent from the training mixture.

## Limitations
- Generalization to absolute scoring: The multiple-choice formulation assumes relative ranking tasks; performance on tasks requiring absolute quality scores remains untested
- Quality dependence on SFT seeds: The method assumes SFT datasets contain "best" candidates, but many SFT datasets contain noisy or low-quality annotations that could propagate errors
- Transfer validity of text reasoning: While reasoning distillation shows promise, visual reasoning may require fundamentally different patterns than text reasoning, limiting distillation effectiveness

## Confidence
- **High confidence:** MR. Judge-7B outperforms GPT-4o on VL-RewardBench (9.9%) and improves inference-time scaling on MM-Vet (7.7%). These are direct empirical results with clear metrics.
- **Medium confidence:** The reverse synthesis strategy provides scalable training signal without external annotation. While the method is sound, the quality ceiling depends on SFT data quality, which varies.
- **Medium confidence:** Warm-up SFT with reasoning traces improves RL outcomes. The ablation shows 4.7% accuracy gain, but the mechanism (whether text reasoning transfers effectively to visual domains) needs further validation.

## Next Checks
1. **Ablate warm-up SFT:** Train MR. Judge-7B with zero-RL vs. SFT+RL on same data split. Compare VL-RewardBench macro accuracy to quantify reasoning distillation contribution.
2. **Stress-test negative quality:** Generate negative candidates with varying error severity (controlled by temperature or error-type frequency). Plot judgment accuracy vs. negative difficulty to find minimum viable contrast.
3. **Evaluate inference-time scaling efficiency:** On MM-Vet, vary candidate count (2, 4, 8) and measure accuracy gain per additional candidate. Identify compute-optimal sampling budget for different task MLLM capabilities.