---
ver: rpa2
title: Prototype-Guided Cross-Modal Knowledge Enhancement for Adaptive Survival Prediction
arxiv_id: '2503.10726'
source_url: https://arxiv.org/abs/2503.10726
tags:
- survival
- data
- prosurv
- features
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of survival prediction when
  multimodal data (whole-slide images and genomic data) is incomplete or unavailable
  in clinical practice. The proposed ProSurv framework introduces an intra-modal prototype
  updating mechanism to construct modality-specific prototype banks that encapsulate
  risk-relevant features across intervals.
---

# Prototype-Guided Cross-Modal Knowledge Enhancement for Adaptive Survival Prediction

## Quick Facts
- **arXiv ID**: 2503.10726
- **Source URL**: https://arxiv.org/abs/2503.10726
- **Reference count**: 30
- **Primary result**: ProSurv achieves 0.645 C-index, outperforming state-of-the-art multimodal survival prediction methods

## Executive Summary
This paper addresses the challenge of survival prediction when multimodal data (whole-slide images and genomic data) is incomplete or unavailable in clinical practice. The proposed ProSurv framework introduces an intra-modal prototype updating mechanism to construct modality-specific prototype banks that encapsulate risk-relevant features across intervals. These prototypes are then used in a cross-modal translation module to enhance knowledge representation for multimodal inputs and generate features for missing modalities. The method eliminates dependency on paired data and enables robust, adaptive survival prediction across diverse scenarios.

## Method Summary
ProSurv is a cross-modal knowledge enhancement framework for survival prediction that uses prototype-guided learning to handle missing modalities. The method constructs modality-specific prototype banks via intra-modal risk contrastive learning, then employs cross-attention to translate features between modalities. For multimodal inputs, it fuses original and translated features; for incomplete inputs, it uses the translated features as proxy representations. The framework is trained with a combined loss of survival prediction, prototype alignment, and cross-modal alignment objectives.

## Key Results
- ProSurv achieves an overall C-index of 0.645 on four cancer datasets (BRCA, BLCA, STAD, CRAD)
- Outperforms state-of-the-art methods using either unimodal or multimodal input
- Maintains strong performance even with high rates of unimodal training data (stable up to 50% unimodal rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality-specific prototype banks capture risk-relevant features that enable cross-modal knowledge transfer.
- Mechanism: During training, prototype banks are updated via risk contrastive learning with event-aware sampling. For uncensored patients, prototypes from the event bin are positive samples; for censored patients, prototypes after censoring time are positive. This binds prototypes to survival intervals.
- Core assumption: Survival-relevant features cluster by time interval within each modality, and these clusters transfer across modalities.
- Evidence anchors:
  - [abstract] "construct modality-specific prototype banks that encapsulate the statistics of the whole training set and preserve the modality-specific risk-relevant features/prototypes across intervals"
  - [section 2.3] Eq. 2 defines L_sim with event-aware sampling; ablation shows 3.8-4.5% C-index improvement with intra-modal update
  - [corpus] ProtoMol validates prototype-guided multimodal learning in molecular property prediction, but direct transfer to survival analysis is not independently verified
- Break condition: If prototypes converge to modality-specific noise rather than survival-relevant patterns, cross-modal translation will hallucinate irrelevant features.

### Mechanism 2
- Claim: Cross-attention between input features and prototype banks generates plausible missing-modality representations.
- Mechanism: Given pathology features as query, the cross-modal translation computes attention scores over genomic prototypes, which are then used to reconstruct genomic features. The attention patterns learned from paired data generalize to unimodal inference.
- Core assumption: The attention patterns learned from paired data generalize to unimodal inference.
- Evidence anchors:
  - [abstract] "cross-modal translation module utilizes the learned prototypes to enhance knowledge representation for multimodal inputs and generate features for missing modalities"
  - [section 2.4] Eq. 3 defines cross-modal translation; Fig. 3(a) shows t-SNE alignment between original and translated features
  - [corpus] Related work on missing modality handling exists (Distilled Prompt Learning, Handling Missing Modalities in NSCLC), but cross-modal reconstruction quality is not benchmarked externally
- Break condition: If attention captures spurious correlations, translated features will not generalize to unseen patients.

### Mechanism 3
- Claim: Knowledge-enhanced fusion of original and translated features improves robustness under incomplete data.
- Mechanism: For complete modalities, features are averaged and concatenated; for incomplete inputs, translated features serve as proxy representations. This ensures the model always has two feature streams.
- Core assumption: Translated features provide complementary information even when original modality is available.
- Evidence anchors:
  - [abstract] "enables robust, adaptive survival prediction across diverse scenarios"
  - [section 2.5] Eq. 5-6 define fusion strategies; Table 1 shows ProSurv achieves 0.640 C-index with pathology-only vs. 0.645 overall
  - [corpus] Weak external validation; corpus papers focus on different fusion strategies without direct comparison
- Break condition: If translated features are noisy, averaging may dilute original feature signal rather than enhance it.

## Foundational Learning

- Concept: **Discrete-time survival analysis**
  - Why needed here: ProSurv discretizes survival time into K bins and estimates hazard per bin; understanding NLL loss and censoring is essential to interpret L_surv.
  - Quick check question: Can you explain why censored patients contribute differently to the loss than uncensored patients?

- Concept: **Cross-attention mechanism**
  - Why needed here: The core translation module uses features to attend over prototypes; understanding Q/K/V interactions is required to debug attention patterns.
  - Quick check question: Given query from pathology and keys/values from genomic prototypes, what does a high attention weight imply about the relationship?

- Concept: **Prototype learning and metric spaces**
  - Why needed here: Prototypes are updated via cosine similarity and contrastive loss; understanding embedding space geometry helps diagnose prototype collapse.
  - Quick check question: If all prototypes converge to similar vectors, what happens to the cross-modal translation output?

## Architecture Onboarding

- Component map:
  1. **Feature extractors**: UNI (pretrained) for WSI patches → Transformer + GAP → pathology features; SNN for genomic data → genomic features
  2. **Prototype banks**: B_p, B_g with K=4 intervals × n=32 prototypes × d dimensions
  3. **Cross-modal translators**: Two cross-attention modules (pathology→genomics and genomics→pathology)
  4. **Fusion head**: Concatenate averaged features → FC + sigmoid → hazard prediction

- Critical path: WSI patches → pathology features → (a) update pathology prototype bank via L_sim, (b) attend over genomics prototypes → translated features → fuse with genomics features (if available) → survival loss

- Design tradeoffs:
  - K=4 intervals balances temporal granularity vs. prototype sparsity; fewer bins lose resolution, more bins require more data per bin
  - n=32 prototypes per bin: too few may miss subtypes, too many increases memory and risk of underutilization
  - α=0.2, β=0.2 weight prototype/alignment losses; higher values may over-regularize, lower values may under-constrain translation

- Failure signatures:
  - Prototype collapse: All prototypes in a bank converge to similar vectors → attention becomes uniform → translated features lose specificity
  - Modality dominance: One modality's features consistently outperform translated features → check alignment loss convergence
  - C-index degradation with high unimodal rate: Fig. 3(b) shows stability up to 50%; beyond this, prototype statistics may not represent full distribution

- First 3 experiments:
  1. **Prototype sanity check**: Visualize prototype distributions per bin using t-SNE; verify separation between bins and diversity within bins
  2. **Translation quality baseline**: On held-out paired data, compute L2 distance between original and translated features; compare to random baseline
  3. **Ablation by modality**: Train with 0%, 25%, 50%, 75% unimodal data; plot C-index curve to validate robustness claim and identify break point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the learned modality-specific prototypes correspond to distinct biological mechanisms or histological phenotypes?
- Basis: [inferred] The paper claims prototypes encapsulate "risk-relevant features" (Section 2.3) and demonstrates distribution alignment via t-SNE (Fig 3a), but does not provide evidence linking specific prototypes to known clinical biomarkers or morphological patterns.
- Why unresolved: Without semantic interpretation, the clinical utility of the prototype banks remains limited to performance metrics rather than explainable precision medicine.
- What evidence would resolve it: A correlation analysis between high-activation prototypes and ground-truth genetic mutations (e.g., specific gene expressions) or histological grades.

### Open Question 2
- Question: Can the cross-modal translation mechanism scale effectively to settings involving more than two modalities (e.g., incorporating radiology or clinical text)?
- Basis: [inferred] The framework is explicitly designed for a histo-genomic pair, utilizing pairwise cross-attention (Section 2.4) and specific translation paths.
- Why unresolved: It is unclear if the prototype guidance strategy remains efficient or if the architecture becomes combinatorially complex when expanding to N modalities.
- What evidence would resolve it: Extending the method to a dataset with 3+ modalities and evaluating the performance and computational cost of the resulting translation network.

### Open Question 3
- Question: How does the fixed number of discretized survival intervals (K) impact the model's ability to capture varying disease progression speeds?
- Basis: [inferred] The method fixes K=4 for all datasets (Section 3.1), forcing the prototype banks to divide the survival timeline into four rigid bins regardless of the specific cancer type's event density.
- Why unresolved: Aggressive cancers (e.g., BLCA) and indolent cancers (e.g., BRCA) may require different temporal granularities to accurately model risk, and a fixed K might obscure critical time-dependent features.
- What evidence would resolve it: An ablation study analyzing performance changes when K is varied (e.g., K ∈ {2, 4, 8, 12}) across datasets with different survival distributions.

## Limitations
- **Translation quality validation**: While t-SNE alignment is shown, quantitative reconstruction error is not reported, making it difficult to assess whether cross-modal translation produces meaningful representations.
- **Generalization to unseen intervals**: The method assumes survival-relevant features cluster by discretized intervals, but does not validate whether prototypes generalize to patients whose event times fall between training intervals.
- **Computational overhead**: The method introduces prototype banks and cross-attention modules, but memory and inference time requirements are not discussed.

## Confidence

- **High confidence**: The intra-modal prototype updating mechanism (Mechanism 1) is well-supported by ablation results showing 3.8-4.5% C-index improvement, and the risk-contrastive learning objective is clearly defined.
- **Medium confidence**: The cross-modal translation module (Mechanism 2) is theoretically sound, but without external benchmarks for missing modality reconstruction quality, the actual performance gain from translated features remains partially validated.
- **Medium confidence**: The knowledge-enhanced fusion strategy (Mechanism 3) shows robustness across unimodal rates, but the paper does not test extreme scenarios (e.g., 90%+ unimodal data) where prototype statistics may become unreliable.

## Next Checks

1. **Translation fidelity benchmark**: On held-out paired data, compute and report reconstruction error (e.g., L2 distance, cosine similarity) between original and translated features, and compare against baselines like nearest prototype copying or mean prototype translation.

2. **Interval generalization test**: Train ProSurv on patients with event times in early intervals, then evaluate on patients whose events fall in later intervals not well-represented in the training set, to assess prototype generalization.

3. **Robustness at extreme unimodal rates**: Extend the unimodal rate evaluation to 75%, 90%, and 95% to identify the break point where prototype banks no longer represent the full distribution and C-index degrades significantly.