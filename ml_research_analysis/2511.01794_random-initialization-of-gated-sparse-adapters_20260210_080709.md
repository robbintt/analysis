---
ver: rpa2
title: Random Initialization of Gated Sparse Adapters
arxiv_id: '2511.01794'
source_url: https://arxiv.org/abs/2511.01794
tags:
- sparse
- fine-tuning
- accuracy
- random
- mnist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in language model
  fine-tuning by introducing Random Initialization of Gated Sparse Adapters (RIGSA).
  The method starts with randomly-initialized full-rank adapters, gates them with
  a ReZero analog, and applies iterative magnitude pruning to find sparse sub-networks.
---

# Random Initialization of Gated Sparse Adapters

## Quick Facts
- arXiv ID: 2511.01794
- Source URL: https://arxiv.org/abs/2511.01794
- Reference count: 39
- Primary result: RIGSA shows less catastrophic forgetting than QLoRA on GSM8k, HellaSwag, and PIQA benchmarks while achieving 99.05% accuracy on Textual MNIST

## Executive Summary
This paper introduces Random Initialization of Gated Sparse Adapters (RIGSA) to address catastrophic forgetting during language model fine-tuning. The method combines randomly-initialized full-rank adapters with ReZero-style gating and iterative magnitude pruning to discover sparse sub-networks. The authors evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task called Textual MNIST, where they demonstrate the model's ability to learn the task from scratch. Despite having more trainable parameters than QLoRA, RIGSA shows superior resistance to forgetting on benchmark tasks while maintaining high performance on the target task.

## Method Summary
RIGSA employs a three-stage approach to adapter-based fine-tuning. First, full-rank adapters are randomly initialized rather than pretrained, allowing the model to learn task-specific representations from scratch. Second, these adapters are gated using a ReZero-style mechanism that applies a learnable scalar to modulate the adapter output, enabling smooth integration with the base model. Third, iterative magnitude pruning is applied to identify and retain only the most important adapter parameters, creating sparse sub-networks that are both efficient and effective. This combination of random initialization, gating, and pruning aims to balance plasticity for new tasks with stability for preserving existing capabilities.

## Key Results
- SmolLM2-1.7B-Instruct achieves 99.05% accuracy on Textual MNIST after one epoch of dense fine-tuning
- RIGSA maintains above 98% accuracy on Textual MNIST even after multiple pruning iterations
- RIGSA shows less catastrophic forgetting than QLoRA on GSM8k, HellaSwag, and PIQA benchmarks
- RIGSA performance is comparable to random masking despite having more trainable parameters

## Why This Works (Mechanism)
RIGSA mitigates catastrophic forgetting by leveraging the combined benefits of random initialization, gating, and sparsity. Random initialization allows adapters to learn task-specific representations without being constrained by pretrained weights that might conflict with new tasks. The ReZero gating mechanism provides a smooth transition between base model and adapter contributions, preventing abrupt changes that could destabilize learned behaviors. Iterative magnitude pruning creates sparse sub-networks that focus on the most critical parameters for the new task while leaving the base model largely untouched, preserving its existing capabilities. This architectural design enables effective learning of new tasks while maintaining performance on previously learned tasks.

## Foundational Learning
- **Catastrophic forgetting**: When neural networks overwrite previous knowledge while learning new tasks, leading to degraded performance on old tasks
  - Why needed: Understanding this phenomenon is crucial as it's the core problem RIGSA aims to solve
  - Quick check: Can you explain why fine-tuning often causes performance degradation on source tasks?

- **Adapter-based fine-tuning**: Inserting small trainable modules between layers of a pre-trained model instead of updating all parameters
  - Why needed: RIGSA builds upon adapter methodology to achieve parameter-efficient adaptation
  - Quick check: What are the advantages of adapters over full fine-tuning in terms of computational cost?

- **Magnitude pruning**: Removing network parameters with the smallest absolute values to create sparse models
  - Why needed: RIGSA uses this technique iteratively to discover sparse sub-networks
  - Quick check: How does magnitude pruning differ from other pruning methods like structured pruning?

- **ReZero initialization**: A technique where residual connections are scaled by a learnable parameter initialized to zero
  - Why needed: RIGSA employs ReZero-style gating to smoothly integrate adapter outputs
  - Quick check: What problem does ReZero solve in residual network training?

## Architecture Onboarding

**Component Map**: Base Model -> Adapter Layer -> ReZero Gate -> Magnitude Pruning -> Sparse Sub-network

**Critical Path**: The most important components for RIGSA's effectiveness are the random initialization of adapters, the ReZero gating mechanism, and the iterative magnitude pruning. The random initialization allows learning from scratch, the gating provides smooth integration, and pruning creates efficient sparse representations while preserving base model performance.

**Design Tradeoffs**: RIGSA trades increased parameter count (compared to QLoRA) for better forgetting resistance. The random initialization approach requires more training iterations but may discover more task-appropriate representations. The iterative pruning process adds computational overhead during training but results in more efficient inference models. The ReZero gating adds minimal parameter overhead while providing significant stability benefits.

**Failure Signatures**: RIGSA may fail when the random initialization cannot find suitable representations for the target task, when the pruning process removes critical parameters, or when the gating mechanism cannot properly balance base model and adapter contributions. Poor performance on the target task with good source task retention suggests initialization issues, while good target performance with catastrophic forgetting indicates pruning or gating problems.

**First Experiments**:
1. Verify that random initialization allows learning from scratch by comparing against pretrained adapter initialization
2. Test the impact of ReZero gating by comparing against no gating or alternative gating mechanisms
3. Evaluate different pruning strategies (magnitude vs. other methods) to assess their impact on forgetting and task performance

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is limited to a single base model (SmolLM2-1.7B-Instruct) and novel task (Textual MNIST), raising questions about generalizability
- The paper lacks detailed ablation studies to isolate the contributions of random initialization, gating, and iterative pruning
- The comparison with QLoRA is complicated by RIGSA having more trainable parameters, making it unclear whether benefits come from architecture or parameter count

## Confidence
- **High confidence**: The 99.05% accuracy on Textual MNIST after one epoch is straightforward and verifiable
- **Medium confidence**: Claims about forgetting resistance compared to QLoRA are supported but limited by narrow task diversity
- **Low confidence**: The assertion that RIGSA's performance is "comparable to random masking" lacks statistical rigor

## Next Checks
1. Replicate experiments across multiple base models (Llama, Mistral) and diverse task types to assess generalizability
2. Conduct ablation studies to disentangle effects of random initialization, gating, and iterative pruning
3. Perform statistical significance testing on benchmark results with confidence intervals to strengthen performance comparisons