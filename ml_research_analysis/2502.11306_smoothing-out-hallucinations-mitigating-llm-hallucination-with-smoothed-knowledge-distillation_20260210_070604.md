---
ver: rpa2
title: 'Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge
  Distillation'
arxiv_id: '2502.11306'
source_url: https://arxiv.org/abs/2502.11306
tags:
- hallucination
- labels
- language
- knowledge
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge distillation as a method to reduce
  hallucination in large language models (LLMs) by replacing hard-label training with
  smoothed soft labels from a teacher model. Experiments on summarization benchmarks
  show that models trained with knowledge distillation exhibit lower hallucination
  rates compared to standard supervised finetuning while maintaining strong performance
  on general NLP tasks.
---

# Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation

## Quick Facts
- arXiv ID: 2502.11306
- Source URL: https://arxiv.org/abs/2502.11306
- Reference count: 32
- Knowledge distillation reduces hallucination in LLMs by replacing hard labels with smoothed soft labels from teacher models

## Executive Summary
This paper investigates knowledge distillation (KD) as a method to reduce hallucination in large language models by replacing hard-label training with smoothed soft labels from a teacher model. Experiments on summarization benchmarks show that models trained with KD exhibit lower hallucination rates compared to standard supervised finetuning while maintaining strong performance on general NLP tasks. Across multiple model families (Llama-2, Llama-3.1, and Qwen-2.5), KD consistently improves factual consistency and reduces overconfident predictions, demonstrating that soft labels provide more effective training signals than hard labels for building reliable LLMs.

## Method Summary
The method replaces hard-label cross-entropy training with a combined loss: L = L_supervised + αL_KD, where L_KD is the KL divergence between student and teacher softmax distributions. Teacher models (Llama-2-13B, Llama-3.1-70B, Qwen-2.5-32B) are first finetuned on the Dolly dataset, then used to generate soft labels for student models (7B/8B) during supervised finetuning. The approach aims to reduce overconfidence-driven hallucination by providing context-dependent probability distributions rather than deterministic one-hot labels. Training uses LoRA for efficiency and is implemented in the MiniLLM framework.

## Key Results
- KD consistently improves factual consistency across Llama-2, Llama-3.1, and Qwen-2.5 model families
- Student models trained with KD show lower hallucination rates while maintaining strong ROUGE-L scores
- Optimal α values vary across model families, requiring hyperparameter tuning
- KD particularly effective at reducing faithfulness hallucination in summarization tasks

## Why This Works (Mechanism)

### Mechanism 1
Soft labels from teacher models reduce overconfidence-driven hallucination. Hard labels force the model to assign probability 1.0 to a single token and 0 to all alternatives, creating miscalibrated confidence. Soft labels distribute probability mass across multiple plausible continuations, encouraging the student to learn uncertainty-aware distributions rather than brittle point estimates. Core assumption: teacher models provide better-calibrated probability distributions than one-hot ground truth. Break condition: if the teacher model itself is miscalibrated or hallucinates, the student may inherit these weaknesses.

### Mechanism 2
Soft labels align training with the principle of maximum entropy, reducing arbitrary assumptions. Hard labels inject false certainty by enforcing a single correct token even when context permits multiple valid completions. Soft labels preserve entropy by maintaining non-zero probabilities for reasonable alternatives, preventing the model from learning spurious deterministic rules. Core assumption: hallucination partly stems from over-specified predictions that fail to generalize under ambiguity. Break condition: if tasks require sharp, deterministic outputs, excessive smoothing may degrade precision.

### Mechanism 3
Teacher-generated soft labels encode contextual dependencies that hard labels discard. Hard labels treat each token prediction independently, ignoring how valid completions vary with context. Teacher models produce context-conditioned distributions that transfer this sensitivity to students. Core assumption: faithfulness hallucination arises partly from models failing to condition predictions appropriately on prior context. Break condition: if teacher models have weak attention grounding themselves, the transferred distributions may not improve contextual faithfulness.

## Foundational Learning

- **Knowledge Distillation (KD) for Language Models**: Understanding standard KD (teacher-student logit matching, temperature scaling) is prerequisite. Quick check: Can you explain how KL divergence between teacher and student logits differs from cross-entropy with hard labels?

- **Calibration and Overconfidence in Neural Networks**: The paper's core hypothesis links miscalibration to hallucination. Quick check: Why does cross-entropy loss with hard labels encourage overconfident predictions even when accuracy is moderate?

- **Maximum Entropy Principle**: The theoretical justification for soft labels invokes Jaynes' principle. Quick check: Given partial information about a token distribution, why would a uniform distribution over plausible tokens be preferable to a point estimate?

## Architecture Onboarding

### Component map
Teacher Model (finely tuned) -> Soft Labels (logits) -> Student Model (KD training) -> Evaluation (hallucination metrics)

### Critical path
1. Finetune teacher on Dolly (or target instruction data) first using LoRA/QLoRA for large teachers
2. For each student training batch, run teacher inference to get logits for the same input
3. Compute L_KD between student and teacher logits; combine with L_supervised against ground-truth hard labels
4. Tune α via validation on faithfulness metrics (FR, factual consistency)

### Design tradeoffs
- α value: Low α (0.01-0.1) preserves hard-label signal; high α (1.0-10.0) risks underfitting ground truth
- Teacher size: Larger teachers may not be best; distillation scaling law research suggests slightly-more-capable teachers outperform much-larger ones
- Pretraining vs. finetuning KD: Paper only tests finetuning; pretraining with KD may have stronger effects

### Failure signatures
- Metric divergence: ROUGE-L improves but factual rate drops (observed in Llama-2 on XSUM)
- Student collapse: If α too high and teacher poor, student may regress to teacher's hallucinations
- Compute overhead: Teacher inference doubles forward passes

### First 3 experiments
1. Baseline sanity check: Replicate SFT vs. KD on a single model family (e.g., Llama-3.1-8B with Llama-3.1-70B teacher) on CNNDM; verify factual consistency improves per Table 1
2. α sensitivity sweep: Train student models with α ∈ {0.01, 0.1, 1.0, 10.0}; plot factual rate and factual consistency to identify Pareto frontier
3. Teacher quality ablation: Compare KD with a well-calibrated teacher vs. an intentionally miscalibrated teacher to test the core assumption that teacher calibration matters

## Open Questions the Paper Calls Out

### Open Question 1
Does applying knowledge distillation during pretraining (rather than only during instruction finetuning) provide greater hallucination reduction benefits? The paper notes this should ideally be integrated into pretraining rather than applied only during finetuning, but resource constraints limited experiments to instruction finetuning.

### Open Question 2
Does knowledge distillation also reduce factuality hallucination (contradictions with real-world knowledge), or only faithfulness hallucination? The study specifically targets faithfulness hallucination and acknowledges that factuality hallucination is another critical issue that requires different mitigation strategies.

### Open Question 3
How does teacher model quality and calibration affect the hallucination-reduction benefits of knowledge distillation? The paper notes that if the teacher itself exhibits hallucination or poor factual calibration, the student may inherit these weaknesses rather than mitigating them.

### Open Question 4
Can adaptive KD strategies that dynamically adjust soft-label smoothing based on context sensitivity outperform fixed α values? The paper suggests future work could explore adaptive KD strategies that dynamically adjust soft-label smoothing based on context sensitivity.

## Limitations

- Teacher model quality is critical: If the teacher itself hallucinates or is poorly calibrated, the student may inherit these weaknesses rather than overcoming them
- Experimental scope limited to supervised finetuning rather than pretraining, leaving open whether KD would be more effective at scale
- Optimal α values vary across model families without clear explanation, suggesting the method may require careful hyperparameter tuning per architecture

## Confidence

- **High Confidence**: The empirical observation that KD improves factual consistency on summarization tasks is well-supported by multiple experiments across different model families
- **Medium Confidence**: The mechanism explaining how soft labels reduce overconfidence-driven hallucination is plausible but relies on the untested assumption that teacher models are better calibrated than hard labels
- **Low Confidence**: The claim that KD would be more effective during pretraining than finetuning is speculative, as the paper only tests finetuning

## Next Checks

1. **Teacher Quality Ablation**: Systematically test KD with teachers of varying quality - compare a well-calibrated teacher against one with injected hallucinations or miscalibration to directly test whether teacher quality is the limiting factor

2. **Pretraining KD Experiment**: Implement knowledge distillation during pretraining (following Llama-3.2-1B approach) rather than just finetuning, and compare hallucination rates between pretraining-KD and finetuning-KD models

3. **Calibration Analysis**: Measure and compare expected calibration error (ECE) between student models trained with hard labels vs. soft labels, and between teacher models of different quality, to directly test the overconfidence hypothesis