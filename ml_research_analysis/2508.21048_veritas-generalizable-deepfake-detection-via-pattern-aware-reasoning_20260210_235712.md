---
ver: rpa2
title: 'Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning'
arxiv_id: '2508.21048'
source_url: https://arxiv.org/abs/2508.21048
tags:
- reasoning
- image
- arxiv
- images
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Veritas, a multi-modal large language model
  (MLLM)-based deepfake detector, addressing the challenge of generalizing to unseen
  deepfake techniques and data domains. Existing detectors struggle with practical
  deployment due to discrepancies between academic benchmarks and real-world scenarios.
---

# Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning

## Quick Facts
- arXiv ID: 2508.21048
- Source URL: https://arxiv.org/abs/2508.21048
- Authors: Hao Tan; Jun Lan; Zichang Tan; Ajian Liu; Chuanbiao Song; Senyuan Shi; Huijia Zhu; Weiqiang Wang; Jun Wan; Zhen Lei
- Reference count: 40
- Key outcome: Veritas achieves over 90% accuracy on unseen forgeries and in-the-wild data through pattern-aware reasoning and a two-stage training pipeline.

## Executive Summary
This paper introduces Veritas, a multi-modal large language model (MLLM)-based deepfake detector designed to generalize across unseen deepfake techniques and data domains. Existing detectors struggle with practical deployment due to discrepancies between academic benchmarks and real-world scenarios. To address this, the authors construct HydraFake, a dataset with hierarchical generalization testing covering unseen model architectures, emerging forgery techniques, and novel data domains. Veritas employs pattern-aware reasoning, incorporating human-like cognitive patterns such as "planning" and "self-reflection" to emulate forensic analysis. A two-stage training pipeline is proposed: pattern-guided cold-start with Mixed Preference Optimization (MiPO) for reasoning alignment, followed by Pattern-aware Group Relative Policy Optimization (P-GRPO) for adaptive reasoning. Experiments on HydraFake demonstrate that Veritas achieves significant gains over state-of-the-art detectors in cross-forgery and cross-domain scenarios, delivering transparent and faithful detection outputs.

## Method Summary
Veritas is built on InternVL3-8B and uses a two-stage training pipeline. First, a pattern-guided cold-start phase uses SFT (3 epochs) with format injection followed by Mixed Preference Optimization (MiPO, 2 epochs) on curated preference and non-preference data. Second, Pattern-aware Group Relative Policy Optimization (P-GRPO, 2 epochs) refines the model using rewards for appropriate reasoning patterns, reflection quality, and format adherence. The method is evaluated on HydraFake-100K, a dataset constructed for hierarchical generalization testing across four levels: In-Domain, Cross-Model, Cross-Forgery, and Cross-Domain.

## Key Results
- Veritas achieves 87.4% accuracy on Cross-Forgery vs 81.2% for flexible reasoning (+6.2%)
- P-GRPO with pattern-aware reward achieves 90.3% on Cross-Forgery vs 86.3% without it (+4.0%)
- Over 90% accuracy on unseen forgeries and in-the-wild data in Cross-Domain evaluation

## Why This Works (Mechanism)

### Mechanism 1: Pattern-Aware Reasoning Structure
- **Claim:** If the model internalizes structured reasoning patterns (fast judgement, planning, reasoning, self-reflection, conclusion), it may generalize better to out-of-distribution (OOD) deepfakes than vanilla chain-of-thought approaches.
- **Mechanism:** The structured patterns enforce a human-like forensic process—quick assessment, systematic planning, evidence gathering, critical reflection, and synthesis—rather than free-form reasoning that may hallucinate or miss artifacts.
- **Core assumption:** Deepfake detection benefits from explicit cognitive scaffolding rather than implicit pattern matching alone.
- **Evidence anchors:**
  - [abstract] "we introduce pattern-aware reasoning that involves critical reasoning patterns such as 'planning' and 'self-reflection' to emulate human forensic process"
  - [Table 2] Pattern-aware reasoning achieves 87.4% on Cross-Forgery vs 81.2% for flexible reasoning (+6.2%)
  - [corpus] Related work on generalizable detection confirms cross-domain degradation is a common failure mode; structured reasoning is a novel direction not explored in prior MLLM approaches
- **Break condition:** If reasoning patterns become too rigid or fail to adapt to novel forgery types, performance may degrade; the paper shows P-GRPO is needed to make patterns adaptive.

### Mechanism 2: Mixed Preference Optimization (MiPO) for Reasoning Alignment
- **Claim:** If the model learns from both human-annotated preferences and non-preference data (vague reasoning trajectories), it may produce more precise and faithful reasoning than SFT alone.
- **Mechanism:** MiPO exposes the model to its own failure modes (correct answer but weak reasoning; incorrect reasoning) and steers it away from memorization toward genuine forensic analysis.
- **Core assumption:** Non-preference data contains signal about what *not* to do, which shapes reasoning quality beyond simple imitation.
- **Evidence anchors:**
  - [Section 4.1] "we meticulously curate a mixed preference dataset... incorrect answers... answer is correct but the reasoning content is not precise or detailed enough"
  - [Table 5] MiPO achieves higher reasoning quality scores (4.6479) vs DPO (4.5077) and SFT-only (4.2538)
  - [corpus] Prior work on deepfake detection with MLLMs (e.g., M2F2-Det, DD-VQA) uses LLMs as post-hoc interpreters; MiPO is a novel alignment strategy for reasoning-based detection
- **Break condition:** If preference data is too sparse or non-preference data is noisy, alignment may fail; the paper uses 3K carefully curated pairs.

### Mechanism 3: Pattern-Aware GRPO (P-GRPO) for Adaptive Reasoning
- **Claim:** If the model is rewarded for *appropriate* use of planning and self-reflection (not just length), it may learn adaptive reasoning that scales with sample difficulty.
- **Mechanism:** P-GRPO uses a pattern-aware reward that gives higher rewards for correct answers involving planning/reflection, and penalizes overthinking on errors. This shapes reasoning depth conditionally.
- **Core assumption:** Reasoning should be dynamic—not all samples need deep reflection; adaptive patterns improve generalization.
- **Evidence anchors:**
  - [Section 4.2] "we incentivize appropriate thinking patterns through our pattern-aware reward mechanism"
  - [Table 3] P-GRPO with pattern-aware reward achieves 90.3% on Cross-Forgery vs 86.3% without it (+4.0%)
  - [corpus] Related work on adaptive reasoning in LLMs (s1, R1-style models) shows promise but has not been applied to deepfake detection; Veritas is first to use pattern-aware RL for this task
- **Break condition:** If reward shaping is misaligned (e.g., over-penalizing reflection on hard samples), the model may underthink; reflection quality reward helps mitigate this.

## Foundational Learning

- **Concept: Multi-Modal Large Language Models (MLLMs)**
  - **Why needed here:** Veritas builds on InternVL3-8B; understanding vision-language alignment and reasoning capabilities is essential.
  - **Quick check question:** Can you explain how an MLLM processes both image tokens and text tokens in a unified transformer?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** P-GRPO is a variant of GRPO with pattern-aware rewards; understanding policy optimization and reward shaping is critical.
  - **Quick check question:** What is the role of the KL divergence term in GRPO, and why might β be set to 0 in Veritas?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** Pattern-aware reasoning is a structured extension of CoT; knowing baseline CoT behavior helps appreciate the improvement.
  - **Quick check question:** How does vanilla CoT differ from structured CoT with explicit planning and reflection tags?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** HydraFake's hierarchical evaluation (cross-model, cross-forgery, cross-domain) tests OOD; understanding distribution shift is key.
  - **Quick check question:** Why might a detector trained on face-swapping fail on face relighting, even if both are "deepfakes"?

## Architecture Onboarding

- **Component map:** Base MLLM: InternVL3-8B (dynamic high-resolution strategy) -> SFT Data Pipeline: 3-step annotation (anomaly identification → visual facts → pattern injection) -> Cold-start: SFT (format injection) → MiPO (reasoning alignment) -> RL Stage: P-GRPO with pattern-aware reward, reflection quality reward, format reward -> HydraFake Dataset: 48K training (limited forgery types), 52K evaluation (hierarchical OOD)

- **Critical path:**
  1. Construct pattern-aware SFT data (36K samples) via multi-step annotation pipeline
  2. Cold-start: SFT (3 epochs, LoRA) → MiPO (2 epochs)
  3. P-GRPO (2 epochs) on 9K in-domain samples with pattern-aware rewards
  4. Evaluate on HydraFake's 4 OOD levels; monitor Cross-Forgery and Cross-Domain gaps

- **Design tradeoffs:**
  - **Reasoning depth vs. compute:** Longer reasoning (planning + reflection) improves hard samples but increases latency; P-GRPO learns to adapt depth.
  - **Cold-start quality vs. RL exploration:** Strong cold-start (via MiPO) yields stable RL but may reduce exploration; paper shows cold-start is essential (Figure 6).
  - **Dataset coverage vs. realism:** HydraFake limits training forgery types (FS, FR, EFG only) to test generalization; this simulates real-world deployment but may underfit known forgeries.

- **Failure signatures:**
  1. **Low Recall on Low-Resolution Data:** Table 11 shows DeepFaceLab (256×256) recall is poor for both Veritas and baselines; MLLMs struggle with low-res details.
  2. **Overthinking on Easy Samples:** Without P-GRPO, model may apply reflection unnecessarily; pattern-aware reward penalizes this.
  3. **Memorization without MiPO:** Figure 1 and Table 5 show SFT-only models produce vague reasoning; MiPO is needed for precise artifact localization.

- **First 3 experiments:**
  1. **Ablate pattern-aware reasoning:** Train with vanilla CoT vs. pattern-aware CoT on SFT + P-GRPO; expect +4–6% on Cross-Forgery (Table 2).
  2. **Ablate MiPO:** Compare SFT-only vs. SFT + MiPO cold-start; evaluate reasoning quality via score/ELO (Table 5) and OOD accuracy.
  3. **Test on Cross-Domain wild data:** Evaluate on Dreamina, GPT-4o, Hailuo AI subsets; expect >90% accuracy (Table 11) if P-GRPO is working.

## Open Questions the Paper Calls Out
None

## Limitations
- **Dataset Availability:** HydraFake-100K's accessibility is unclear, potentially blocking exact reproduction. While the paper lists a GitHub, the specific processed images and preference pairs may require reconstruction.
- **MiPO Hyperparameters:** The precise mixing ratio and loss weighting for non-preference data in MiPO are not fully detailed, which could impact reasoning alignment quality.
- **Low-Resolution Performance:** Both Veritas and baselines struggle with low-resolution data (e.g., DeepFaceLab at 256×256), indicating a fundamental limitation of MLLMs for fine-detail analysis.

## Confidence
- **High Confidence:** Veritas's superiority on HydraFake's hierarchical splits (Cross-Forgery +6.2%, Cross-Domain +4.0%) is well-supported by ablation studies and is the paper's central empirical claim.
- **Medium Confidence:** The claim that pattern-aware reasoning is novel and superior to vanilla CoT in deepfake detection is supported by comparison to related MLLM work, but the field is nascent with limited direct comparators.
- **Low Confidence:** The assertion that Veritas delivers "transparent and faithful detection outputs" is based on reasoning quality metrics (Score/ELO) which are subjective and may not fully capture faithfulness in complex scenarios.

## Next Checks
1. **Dataset Reconstruction Feasibility:** Attempt to recreate the HydraFake training split (48K images) using the specified public datasets and forgery techniques to test if pattern-aware SFT data can be generated.
2. **MiPO Implementation Test:** Implement MiPO with varying non-preference data ratios to find the optimal mixing strategy and validate its impact on reasoning quality.
3. **Cross-Domain Wild Data Evaluation:** Obtain a small sample of real-world deepfake data (e.g., from social media) to test Veritas's claimed >90% accuracy on unseen domains.