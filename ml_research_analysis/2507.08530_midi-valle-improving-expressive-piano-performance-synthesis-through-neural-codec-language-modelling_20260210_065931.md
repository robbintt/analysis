---
ver: rpa2
title: 'MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural
  Codec Language Modelling'
arxiv_id: '2507.08530'
source_url: https://arxiv.org/abs/2507.08530
tags:
- audio
- midi
- alle
- performance
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIDI-VALLE, a neural codec language model
  for expressive piano performance synthesis that overcomes generalization limitations
  of prior methods. By conditioning on reference audio and MIDI prompts, and using
  discrete tokenisation of both MIDI and audio via Octuple MIDI tokenisation and a
  high-fidelity audio codec (Piano-Encodec), the model achieves more consistent alignment
  between symbolic and acoustic representations.
---

# MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling
## Quick Facts
- arXiv ID: 2507.08530
- Source URL: https://arxiv.org/abs/2507.08530
- Reference count: 0
- Primary result: MIDI-VALLE achieves over 75% lower Fréchet Audio Distance than M2A baseline on ATEPP and Maestro datasets

## Executive Summary
MIDI-VALLE is a neural codec language model designed to synthesise expressive piano performances by conditioning on both reference audio and MIDI prompts. It overcomes generalisation limitations of prior methods by integrating discrete tokenisation of both MIDI and audio using Octuple MIDI tokenisation and a high-fidelity audio codec (Piano-Encodec). Trained on the large and diverse ATEPP dataset, MIDI-VALLE achieves significant improvements in synthesis quality and robustness, as demonstrated by objective metrics and subjective listening tests. The model's architecture ensures consistent alignment between symbolic and acoustic representations, advancing the state-of-the-art in music performance synthesis.

## Method Summary
MIDI-VALLE addresses the challenge of synthesising expressive piano performances by conditioning on both reference audio and MIDI prompts. The model employs a neural codec language model that uses discrete tokenisation for both MIDI and audio inputs, leveraging Octuple MIDI tokenisation and a high-fidelity audio codec (Piano-Encodec). This approach ensures consistent alignment between symbolic and acoustic representations. The model is trained on the large and diverse ATEPP dataset, which enhances its ability to generalise across different styles and expressions. By integrating reference audio conditioning, MIDI-VALLE captures nuanced performance characteristics, resulting in more expressive and realistic piano synthesis compared to previous methods.

## Key Results
- MIDI-VALLE achieves over 75% lower Fréchet Audio Distance than the M2A baseline on both ATEPP and Maestro datasets.
- The model demonstrates improved synthesis quality and robustness in subjective listening tests.
- Consistent alignment between symbolic and acoustic representations is achieved through discrete tokenisation.

## Why This Works (Mechanism)
MIDI-VALLE works by integrating reference audio conditioning with discrete tokenisation of both MIDI and audio. This approach ensures that the model captures nuanced performance characteristics and maintains consistent alignment between symbolic and acoustic representations. The use of a large and diverse dataset (ATEPP) further enhances the model's ability to generalise across different styles and expressions, resulting in more expressive and realistic piano synthesis.

## Foundational Learning
- **Discrete tokenisation of MIDI and audio**: Needed to convert continuous data into discrete tokens for language model processing. Quick check: Verify tokenisation preserves musical information and audio fidelity.
- **Neural codec language models**: Required for learning the relationship between MIDI and audio representations. Quick check: Ensure model can generate coherent audio from symbolic input.
- **Reference audio conditioning**: Essential for capturing expressive performance nuances. Quick check: Confirm conditioning improves expressiveness over unconditioned models.
- **Fréchet Audio Distance (FAD)**: Metric used to evaluate synthesis quality. Quick check: Validate FAD scores correlate with perceptual quality.
- **Octuple MIDI tokenisation**: Technique for efficient MIDI representation. Quick check: Test tokenisation efficiency and expressiveness preservation.

## Architecture Onboarding
- **Component map**: Reference audio -> MIDI tokeniser -> Audio codec -> Neural codec language model -> Synthesised audio
- **Critical path**: Reference audio and MIDI prompts are tokenised, processed by the neural codec language model, and decoded into expressive piano audio.
- **Design tradeoffs**: Discrete tokenisation improves alignment but may limit continuous expressiveness; reference audio conditioning enhances expressiveness but increases model complexity.
- **Failure signatures**: Poor generalisation to non-classical genres, reduced performance on smaller datasets, and potential loss of long-term musical coherence.
- **First experiments**: (1) Evaluate model on diverse musical genres; (2) Conduct ablation studies to isolate component contributions; (3) Test robustness on smaller or less-curated datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalisation to non-classical genres like jazz remains challenging.
- Reliance on a large, curated dataset (ATEPP) raises scalability and reproducibility concerns.
- Impact of discrete tokenisation on long-term musical coherence is not fully explored.

## Confidence
- Improved synthesis quality and robustness: High
- Broader genre adaptability: Medium

## Next Checks
1. Evaluate the model on a wider range of musical genres and styles to assess generalisation.
2. Conduct ablation studies to isolate the contributions of each proposed component (e.g., reference audio conditioning, tokenisation strategy).
3. Test the model's robustness and performance on smaller or less-curated datasets to determine scalability and practical applicability.