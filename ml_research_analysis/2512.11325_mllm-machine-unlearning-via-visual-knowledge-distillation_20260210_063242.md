---
ver: rpa2
title: MLLM Machine Unlearning via Visual Knowledge Distillation
arxiv_id: '2512.11325'
source_url: https://arxiv.org/abs/2512.11325
tags:
- knowledge
- visual
- unlearning
- mllm
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a machine unlearning method for multimodal
  large language models (MLLMs) that selectively removes visual knowledge about a
  target entity while preserving textual knowledge. The key innovation is a Visual
  Knowledge Distillation (VKD) scheme that leverages intermediate visual representations
  as supervision signals during fine-tuning.
---

# MLLM Machine Unlearning via Visual Knowledge Distillation

## Quick Facts
- arXiv ID: 2512.11325
- Source URL: https://arxiv.org/abs/2512.11325
- Reference count: 10
- Key outcome: Selective unlearning of visual knowledge about target entities while preserving textual knowledge, outperforming baselines on forgetting benchmarks

## Executive Summary
This paper introduces a machine unlearning method for multimodal large language models (MLLMs) that selectively removes visual knowledge about a target entity while preserving textual knowledge. The key innovation is a Visual Knowledge Distillation (VKD) scheme that leverages intermediate visual representations as supervision signals during fine-tuning. By only updating the visual module while keeping the LLM backbone frozen, the approach achieves better unlearning effectiveness and efficiency compared to state-of-the-art methods. On benchmarks like MLLMU-Bench and CLEAR, the method outperforms baselines in forgetting target visual knowledge while maintaining non-target visual and textual knowledge. The approach is also shown to be robust against relearning attacks.

## Method Summary
The method addresses MLLM unlearning by selectively erasing visual knowledge of target entities while preserving their textual knowledge and all knowledge of non-target entities. The approach uses a Visual Knowledge Distillation (VKD) scheme that freezes the LLM backbone and fine-tunes only the visual module. During fine-tuning, the method uses intermediate visual representations from the frozen teacher model (vanilla model) as supervision signals to preserve non-target visual knowledge. The process involves neuron pruning based on absolute importance scores, followed by FIM-based saliency masking to selectively update parameters. The combined loss function includes output loss and VKD loss, with the latter weighted by parameter β. The method is evaluated on MLLMU-Bench and CLEAR datasets using metrics like Forget VQA accuracy, Retain VQA accuracy, and robustness against relearning attacks.

## Key Results
- Outperforms state-of-the-art methods on MLLMU-Bench and CLEAR benchmarks
- Achieves 4× speedup compared to full fine-tuning approaches
- Demonstrates robustness against relearning attacks with lower Accuracy Gap (AG) metrics

## Why This Works (Mechanism)
The method works by selectively updating only the visual components of MLLMs while freezing the textual LLM backbone. By using intermediate visual representations from the frozen teacher model as supervision during fine-tuning, the approach preserves non-target visual knowledge while forgetting target visual knowledge. The neuron pruning and FIM-based masking allow for selective parameter updates that target only the visual knowledge related to the target entity. This selective approach is more efficient than full fine-tuning and more effective than previous methods at balancing forgetting and preservation.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): Models that process both visual and textual information. Why needed: The paper targets unlearning in these specific models. Quick check: Verify the model architecture processes both modalities.
- Visual Knowledge Distillation: Using intermediate visual representations as supervision during training. Why needed: Key mechanism for preserving non-target visual knowledge. Quick check: Confirm the teacher model's visual outputs are used as targets.
- FIM (Fisher Information Matrix): A technique for identifying important parameters based on gradient information. Why needed: Used for selective parameter masking during fine-tuning. Quick check: Verify gradients are squared and used for masking decisions.
- Neuron Pruning: Removing or deactivating neurons based on importance scores. Why needed: Initial step to reduce capacity for target knowledge. Quick check: Confirm neurons are pruned before fine-tuning.
- Selective Forgetting: The ability to remove specific knowledge while preserving others. Why needed: Core objective of the unlearning task. Quick check: Verify forgetting metrics for target knowledge and preservation metrics for non-target knowledge.

## Architecture Onboarding

### Component Map
Vanilla MLLM -> Neuron Pruning -> FIM Masking -> Fine-tuning with VKD

### Critical Path
The critical path is: Frozen LLM backbone + Fine-tuned Visual Module with VKD supervision. The visual module includes the vision encoder and projector MLPs, with deeper layers emphasized during fine-tuning. The VKD loss aligns student visual features with teacher visual features on retain set samples.

### Design Tradeoffs
- Freezing LLM backbone vs. full fine-tuning: Reduces computational cost but may limit unlearning effectiveness
- VKD vs. no distillation: Preserves non-target knowledge but adds complexity
- Neuron pruning threshold: Higher values increase forgetting but risk losing useful knowledge
- FIM mask threshold: Controls selectivity of parameter updates

### Failure Signatures
- Forget VQA accuracy doesn't decrease sufficiently: VKD too strong or thresholds too conservative
- Retain VQA accuracy drops significantly: VKD too weak or pruning too aggressive
- Forget QA accuracy drops: Unlearning is affecting textual knowledge, violating the preservation constraint
- Training instability: Learning rate too high or β value inappropriate

### Exactly 3 First Experiments
1. Verify neuron pruning correctly identifies and removes neurons with high importance scores for target knowledge
2. Confirm VKD loss properly aligns student visual features with teacher visual features on retain set samples
3. Test that freezing LLM backbone actually reduces fine-tuning time by approximately 4× compared to full fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Neuron pruning and FIM mask threshold values are not numerically specified
- Training hyperparameters (learning rate, epochs, batch size) are omitted
- Method assumes projector outputs capture sufficient semantic information for effective distillation
- Small margins of improvement over baselines suggest limited practical gains

## Confidence
- High confidence: Efficiency gains from freezing LLM backbone and effectiveness of VKD in preserving non-target knowledge
- Medium confidence: Superiority over state-of-the-art methods and robustness against relearning attacks
- Low confidence: Claim about preserving textual knowledge "as much as possible" lacks rigorous validation

## Next Checks
1. Systematically vary neuron pruning threshold d_I and FIM mask threshold d_F to identify optimal operating points and test sensitivity to hyperparameter choices
2. Replace projector-based VKD with distillation from alternative visual representations to confirm the projector is the optimal choice
3. Apply the trained unlearning model to a held-out dataset with different visual styles to verify forgetting generalizes beyond specific images rather than learning to ignore pixel patterns