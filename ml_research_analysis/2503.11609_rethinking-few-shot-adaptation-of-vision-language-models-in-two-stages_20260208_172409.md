---
ver: rpa2
title: Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages
arxiv_id: '2503.11609'
source_url: https://arxiv.org/abs/2503.11609
tags:
- base
- novel
- clip
- coop
- iterations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work analyzes the learning dynamics of parameter-efficient
  fine-tuning (PEFT) techniques for few-shot adaptation of vision-language models
  (VLMs). The authors observe that PEFT naturally splits into two phases: task-level
  feature extraction and specialization to available categories.'
---

# Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages

## Quick Facts
- **arXiv ID**: 2503.11609
- **Source URL**: https://arxiv.org/abs/2503.11609
- **Reference count**: 40
- **Primary result**: Two-stage adaptation (2SFS) outperforms state-of-the-art few-shot adaptation methods across multiple backbones, datasets, and settings by allocating computational budget between task-level feature extraction and linear classifier training.

## Executive Summary
This work analyzes the learning dynamics of parameter-efficient fine-tuning (PEFT) techniques for few-shot adaptation of vision-language models (VLMs). The authors observe that PEFT naturally splits into two phases: task-level feature extraction and specialization to available categories. Based on this insight, they propose Two-Stage Few-Shot Adaptation (2SFS), which allocates a fixed computational budget to first learn task-level features via PEFT, then train a linear classifier. This approach enables selective inference at the category level and outperforms state-of-the-art methods across multiple backbones, datasets, and settings.

## Method Summary
The method operates in two stages with a fixed computational budget. Stage 1 tunes all LayerNorm parameters in both visual and text encoders for α×m steps (α=0.6), learning task-level features. Stage 2 freezes the encoders and trains a linear classifier initialized with base category embeddings for (1-α)×m steps. The total steps m = M×k where M=300 and k is shots per class. Inference uses pre-computed base category embeddings, requiring text encoder computation only for novel categories.

## Key Results
- 2SFS with LayerNorm tuning achieves highest harmonic mean (HM) scores across all 11 datasets and k ∈ {4, 8, 16} settings
- LayerNorm outperforms LoRA and BitFit in novel category accuracy despite having fewer trainable parameters (61k vs 184k vs 125k)
- 2SFS reduces inference cost by pre-computing base category embeddings, requiring text encoder computation only for novel categories
- Fixed breakpoint timing α=0.6 works well on ImageNet but requires dataset-specific tuning for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
PEFT techniques exhibit a natural breakpoint during few-shot training where early iterations improve both base and novel category performance, but continued training degrades novel categories while specializing on base. During early optimization, LayerNorm parameters adapt to task-relevant feature distributions. After the breakpoint, gradients from base-only supervision pull representations toward base class centroids, disrupting features needed for unseen concepts. The breakpoint timing can be approximated by a fixed fraction α of the computational budget and transfers across datasets.

### Mechanism 2
LayerNorm tuning provides better novel-category generalization than LoRA or BitFit due to a more favorable data-to-parameter ratio in few-shot regimes. LayerNorm optimizes only scale γ and shift β vectors (~61k parameters total for ViT-B), whereas LoRA adds low-rank matrices (~184k) and BitFit tunes biases (~125k). Lower capacity constrains adaptation, acting as implicit regularization against overfitting to sparse base-class samples.

### Mechanism 3
Pre-computing base category embeddings as classifier weights enables O(1) retrieval at inference and separates feature learning from discrimination. After Stage 1, text embeddings φ_b = f^t_{ω*}(b) for base categories initialize classifier rows Φ_B. Stage 2 optimizes Φ_B via cross-entropy while freezing ω*. At inference, base categories bypass the text encoder entirely; only novel categories require encoding.

## Foundational Learning

- **Concept: Vision-Language Model Zero-Shot Classification**
  - Why needed here: The paper adapts CLIP's zero-shot mechanism to few-shot scenarios by modifying how embeddings are computed and compared.
  - Quick check question: Given an image x and class names {c_1, c_2, c_3}, how does CLIP determine the predicted class using cosine similarity?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) Categories**
  - Why needed here: The paper analyzes three PEFT families and identifies LayerNorm tuning as optimal for FSA.
  - Quick check question: What are the trainable parameters in each PEFT method: CoOp (prompt tuning), LayerNorm tuning (selective), and LoRA (reparameterization-based adapter)?

- **Concept: Base-to-Novel Generalization and Harmonic Mean**
  - Why needed here: The evaluation explicitly measures whether adaptation to base classes helps or harms unseen novel classes.
  - Quick check question: Why is harmonic mean preferred over arithmetic mean when one metric (novel accuracy) is significantly lower than the other (base accuracy)?

## Architecture Onboarding

- **Component map**:
CLIP VLM (ViT-B/16, ViT-B/32, or ViT-L/14 backbone)
├── Visual Encoder f^v: I → R^d (d=512 for ViT-B)
│   └── LayerNorm parameters: {(γ_l, β_l)} for L layers [Stage 1 tunable]
├── Text Encoder f^t: T → R^d
│   └── LayerNorm parameters [Stage 1 tunable]
└── Stage 2 Classifier Φ_B ∈ R^{|B|×d} [Stage 2 tunable]
    └── Initialization: row i = f^t_{ω*}(class_name_i)

- **Critical path**:
1. Budget allocation: Set m = M × k iterations (M=300, k=shots per class)
2. Stage 1 (α×m iterations): Tune all LayerNorm γ, β parameters via cross-entropy on base classes; optimizer: AdamW, lr=2e-4, weight_decay=0.01, batch_size=32
3. Stage 2 ((1-α)×m iterations): Freeze ω*, initialize Φ_B with text embeddings of base categories, train classifier via cross-entropy
4. Inference: Apply inference logic; pre-computed Φ*_B rows for base classes, text encoder only for novel classes

- **Design tradeoffs**:
- α selection: α=0.6 for LayerNorm (optimal on ImageNet validation); α=0.3 for LoRA (breaks earlier). Transferring α without validation data risks mismatch.
- PEFT choice: LayerNorm offers robustness and efficiency; LoRA yields higher base accuracy but lower novel accuracy.
- M scaling: M=100 under-trains (lowest novel accuracy); M=500 adds ~67% compute for marginal HM gain.

- **Failure signatures**:
- Early breakpoint (EuroSAT pattern): Very few base categories → breakpoint before α×m → HM drops. Fix: Reduce batch size or decrease α.
- Synchronized degradation (Food-101/Oxford Pets pattern): Base accuracy saturates or declines immediately after novel accuracy drops → classifier trains on disrupted features. Fix: Dataset-specific α tuning or validation-free early stopping.
- Underfitting signature: Novel accuracy plateaus but never degrades → increase α or M to allocate more compute to Stage 1.

- **First 3 experiments**:
1. Reproduce breakpoint visualization: Train CLIP ViT-B/16 with LayerNorm tuning on DTD (16-shot, base classes only), log base and novel accuracy every 50 iterations for 1250 iterations. Verify breakpoint occurs near iteration 600-900.
2. α sensitivity analysis: On ImageNet validation set with ViT-B/16 and 16-shot, sweep α ∈ {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, report Base, Novel, and HM. Confirm α=0.6 yields peak HM (~74.2%) or identify dataset-specific optimum.
3. LayerNorm vs LoRA comparison: Run 2SFS with LayerNorm (α=0.6) vs LoRA (α=0.3) on Stanford Cars and FGVC Aircraft. Compare: (a) trainable parameter count, (b) training time per iteration, (c) HM improvement over respective single-stage baselines.

## Open Questions the Paper Calls Out

- Can a validation-free stopping criterion be developed to dynamically determine the optimal point to switch between the feature learning and classifier training stages?
- Does the two-stage adaptation dynamics observed in classification tasks generalize to dense prediction tasks like semantic segmentation or temporal tasks like action recognition?
- How does the 2SFS framework interact with future PEFT strategies or non-Transformer backbones?

## Limitations

- Fixed breakpoint timing α=0.6 works well on ImageNet but requires dataset-specific tuning for optimal performance, particularly for small datasets with few base classes
- LayerNorm superiority assumption rests on data-to-parameter ratio arguments but lacks direct empirical validation beyond parameter count comparisons
- Inference speedup claim assumes fixed base categories, which may not hold in dynamic classification scenarios

## Confidence

- **High Confidence**: The breakpoint phenomenon is consistently observed across multiple datasets and PEFT methods in the provided visualizations
- **Medium Confidence**: The LayerNorm vs LoRA performance comparison is methodologically sound, but the claim about LayerNorm providing better regularization lacks direct empirical validation
- **Low Confidence**: The universal applicability of fixed α values across diverse datasets is questionable, particularly for small datasets with few base classes

## Next Checks

1. Implement an automated breakpoint detection algorithm that monitors the divergence point between base and novel accuracy curves during Stage 1 training, then validate whether this data-driven α selection improves HM scores compared to the fixed 0.6/0.3 heuristics

2. Extend the evaluation framework to include per-class base accuracy distributions and confusion matrices for both LayerNorm and LoRA variants, revealing whether LayerNorm's robustness comes at the cost of base-class discrimination quality

3. Conduct a head-to-head comparison measuring total training time, memory consumption, and inference latency for 2SFS-LayerNorm vs 2SFS-LoRA across all 11 datasets, quantifying the practical trade-offs between parameter efficiency and computational overhead