---
ver: rpa2
title: 'Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language
  Models'
arxiv_id: '2506.22950'
source_url: https://arxiv.org/abs/2506.22950
tags:
- sampling
- group
- memory
- micro
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the memory bottleneck in group-based reinforcement
  learning for large language models (LLMs), specifically when generating multiple
  responses per prompt for Group Reward Policy Optimization (GRPO). The proposed Infinite
  Sampling framework enables efficient and stable GRPO training by decoupling group
  size from GPU memory usage through three key components: (1) micro sampling groups
  that decompose large groups into memory-feasible decoding rounds; (2) continuous
  sampling that interleaves token-level generation across micro groups to maximize
  slot utilization; and (3) a length-aware scheduler combining prefix-based length
  prediction with a two-stage strategy (global grouping via FPTAS and runtime refill
  via SJF).'
---

# Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models

## Quick Facts
- arXiv ID: 2506.22950
- Source URL: https://arxiv.org/abs/2506.22950
- Authors: Liangyu Wang; Huanyi Xie; Xinhai Wang; Tianjin Huang; Mengdi Li; Di Wang
- Reference count: 40
- Key outcome: Reduces peak memory usage by over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on Qwen3-1.7B) and improves decoding throughput by over 25% compared to naive micro sampling group methods, while maintaining full-length completions and stable GRPO training under realistic GPU memory constraints.

## Executive Summary
This paper addresses the memory bottleneck in group-based reinforcement learning for large language models (LLMs), specifically when generating multiple responses per prompt for Group Reward Policy Optimization (GRPO). The proposed Infinite Sampling framework enables efficient and stable GRPO training by decoupling group size from GPU memory usage through three key components: (1) micro sampling groups that decompose large groups into memory-feasible decoding rounds; (2) continuous sampling that interleaves token-level generation across micro groups to maximize slot utilization; and (3) a length-aware scheduler combining prefix-based length prediction with a two-stage strategy (global grouping via FPTAS and runtime refill via SJF). Experimental results demonstrate that Infinite Sampling reduces peak memory usage by over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on Qwen3-1.7B) and improves decoding throughput by over 25% compared to naive micro sampling group methods, while maintaining full-length completions and stable GRPO training under realistic GPU memory constraints.

## Method Summary
Infinite Sampling introduces three components to enable memory-efficient GRPO training: (1) Micro Sampling Groups decompose large sample groups into smaller micro groups that can fit within GPU memory constraints, allowing KV cache pooling across decoding rounds while reusing the shared prompt prefill cache; (2) Continuous Sampling implements token-level interleaving across micro groups using fixed-slot mode to maximize GPU utilization by immediately reassigning completed sequence slots to new samples; and (3) Length-Aware Scheduling combines prefix-conditioned length prediction (using a BERT regressor on pseudo-prompts) with two-stage scheduling—FPTAS for global grouping based on predicted lengths and SJF for runtime slot refill—to minimize decoding steps while preventing memory spikes from concurrent long sequences. The framework maintains stable GRPO training by ensuring full-length completions while achieving significant memory and throughput improvements.

## Key Results
- Reduces peak decoding memory by over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on Qwen3-1.7B)
- Improves decoding throughput by over 25% compared to the naive micro sampling group method
- Achieves near-optimal scheduling performance, with Infinite (FPTAS+SJF) achieving 1770 steps vs oracle 1739 steps on GSM8K
- Maintains full-length completions and stable GRPO training while significantly reducing memory requirements

## Why This Works (Mechanism)

### Mechanism 1: Micro Sampling Groups for Memory Decoupling
- Decomposing large sample groups into smaller micro groups reduces peak KV cache memory by constraining the number of concurrently decoded sequences
- Instead of allocating KV cache for all G samples simultaneously, the system maintains a fixed-size memory pool for g active sequences (g << G)
- After each micro group completes decoding, its response KV cache is cleared and the memory is returned to the pool, while the shared prompt prefill KV cache is retained for reuse by subsequent micro groups
- Core assumption: All completions share the same prompt, enabling prompt KV cache reuse across micro groups without recomputation

### Mechanism 2: Continuous Sampling for Slot Utilization
- Token-level interleaving across micro groups improves GPU utilization by eliminating idle "bubble" periods between micro group barriers
- In Fixed-Slot mode, each decoding slot produces one sequence per micro-group round; when a sequence completes early, its slot is immediately reassigned to a new sample from the next micro group rather than remaining idle
- The shared prompt KV cache enables this interleaving without prompt recomputation
- Core assumption: The decoding scheduler can correctly manage KV cache lifecycle for each sample, recycling buffers upon completion while preserving the shared prompt cache

### Mechanism 3: Length-Aware Scheduling for Memory-Stable Throughput
- Combining prefix-conditioned length prediction with two-stage scheduling minimizes decoding steps while preventing memory spikes from concurrent long sequences
- Stage 1 samples k prefix tokens per sequence, concatenates them with the prompt to form a pseudo-prompt, and uses a BERT-based regressor to estimate final lengths; FPTAS partitions samples into balanced micro groups to minimize makespan
- Stage 2 applies SJF during decoding: when a slot frees, the shortest pending sample is dispatched, maximizing slot turnover
- Core assumption: Prefix-conditioned length predictions are sufficiently accurate to guide scheduling; the paper shows k=16 tokens achieves best accuracy

## Foundational Learning

- Concept: KV Cache in Autoregressive Decoding
  - Why needed here: Understanding that each generated token requires maintaining key-value tensors for all prior tokens, and that memory scales linearly with sequence length × number of concurrent sequences
  - Quick check question: If you decode 32 sequences of 512 tokens each with a 1.7B model, approximately how much KV cache memory is required compared to decoding 4 sequences?

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: GRPO estimates advantages using group-normalized rewards (A_i = r_i - mean(r)) / std(r)), eliminating the need for a critic network but requiring multiple samples per prompt
  - Quick check question: Why does GRPO's advantage estimation depend on group size G, and what happens to variance as G increases?

- Concept: Bin Packing and Scheduling Algorithms (FPTAS, SJF)
  - Why needed here: FPTAS provides near-optimal static grouping with formal guarantees; SJF minimizes average waiting time by prioritizing short jobs
  - Quick check question: If FPTAS produces a globally balanced plan but runtime sequence lengths deviate from predictions, why does SJF refill help correct the schedule?

## Architecture Onboarding

- Component map:
  - Prefill Engine: Computes shared prompt KV cache (reused across all samples)
  - Length Predictor: BERT-based regressor taking pseudo-prompts (prompt + k prefix tokens)
  - Scheduler: FPTAS global grouping → SJF slot-level refill
  - KV Cache Pool: Fixed-size memory region for g active sequences
  - Continuous Decoder: Token-level interleaved generation across slots
  - Reward Engine: Micro-batched reward computation + group-normalized advantages
  - Backprop Engine: Micro-batched gradient updates

- Critical path:
  1. Prompt enters → Prefill KV computed once
  2. For each of G samples: generate k prefix tokens → predict length
  3. FPTAS partitions G samples into N micro groups based on predicted lengths
  4. Continuous decoding: slots generate tokens interleaved; on completion, SJF selects next shortest pending sample
  5. Completed sequences → reward scoring → advantage normalization across full group
  6. Micro-batched backpropagation with accumulated gradients

- Design tradeoffs:
  - Micro group size g: Smaller g reduces memory but increases serialization; paper uses g=4 as default
  - Fixed-Slot vs Dynamic-Slot: Fixed preserves sequence length quality; Dynamic maximizes throughput but introduces bias
  - Prefix length k: Longer prefixes improve prediction accuracy but add overhead; k=16 is optimal
  - FPTAS vs SJF dominance: Table 2 shows SJF contributes most efficiency gains; FPTAS provides secondary load balancing

- Failure signatures:
  - Memory spike during continuous sampling: Likely caused by inaccurate length predictions grouping multiple long sequences; check predictor calibration