---
ver: rpa2
title: 'SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language
  Models on Audio-Video Understanding'
arxiv_id: '2601.21666'
source_url: https://arxiv.org/abs/2601.21666
tags:
- video
- score
- temporal
- across
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SONIC-O1 is a new benchmark for evaluating multimodal large language
  models on real-world audio-video understanding. It covers 13 conversational domains
  with 4,958 human-verified QA instances and demographic metadata, testing summarization,
  multiple-choice questions, and temporal localization.
---

# SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding

## Quick Facts
- arXiv ID: 2601.21666
- Source URL: https://arxiv.org/abs/2601.21666
- Reference count: 40
- SONIC-O1 is a new benchmark for evaluating multimodal large language models on real-world audio-video understanding, covering 13 conversational domains with 4,958 human-verified QA instances and demographic metadata.

## Executive Summary
SONIC-O1 is a comprehensive benchmark designed to evaluate multimodal large language models on audio-video understanding tasks in real-world conversational scenarios. The benchmark encompasses 13 diverse conversational domains and includes 4,958 human-verified question-answer pairs with associated demographic metadata. It tests models across three key tasks: summarization, multiple-choice questions, and temporal localization, providing a robust evaluation framework for assessing multimodal understanding capabilities.

The benchmark reveals significant performance disparities between closed-source and open-source models, with a 22.6% performance gap particularly evident in temporal localization tasks. Additionally, demographic disparities in model performance persist across all tasks, with the most pronounced effects observed in temporal localization. SONIC-O1 offers an open evaluation suite that enables reproducible and socially robust assessment of multimodal understanding in audio-video contexts.

## Method Summary
SONIC-O1 was constructed by curating audio-video data from 13 real-world conversational domains, followed by the creation of human-verified question-answer pairs and demographic metadata collection. The benchmark includes three evaluation tasks: summarization, multiple-choice questions (MCQs), and temporal localization. Each task is designed to test different aspects of multimodal understanding, from content synthesis to precise temporal event identification. The human-verified QA instances ensure high-quality ground truth, while demographic metadata allows for analysis of performance disparities across different user groups.

## Key Results
- Closed-source models outperform open-source models by 22.6%, especially on temporal localization tasks
- Demographic disparities persist across all tasks, with largest effects in temporal localization
- Benchmark includes 4,958 human-verified QA instances across 13 conversational domains

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of real-world conversational scenarios and its multi-task evaluation approach. By testing models on summarization, MCQs, and temporal localization, SONIC-O1 captures both high-level comprehension and fine-grained temporal understanding. The human-verified QA instances ensure reliable ground truth, while demographic metadata enables identification of socially relevant performance disparities.

## Foundational Learning
- **Multimodal Learning**: Why needed - enables models to process and integrate information from multiple input modalities; Quick check - verify model can handle audio and video inputs simultaneously
- **Temporal Localization**: Why needed - critical for identifying specific moments or events in video content; Quick check - assess model's ability to pinpoint time-stamped answers
- **Demographic Analysis**: Why needed - ensures fair and unbiased model performance across different user groups; Quick check - evaluate performance metrics across demographic categories

## Architecture Onboarding
**Component Map**: Audio-Video Data -> Feature Extraction -> Multimodal Fusion -> Task-Specific Heads (Summarization, MCQs, Temporal Localization) -> Output Generation

**Critical Path**: Audio-Video Data → Feature Extraction → Multimodal Fusion → Task-Specific Heads → Evaluation Metrics

**Design Tradeoffs**: The benchmark balances task complexity with real-world applicability, prioritizing comprehensive evaluation over simplified testing scenarios. This approach may increase computational requirements but provides more realistic assessment of model capabilities.

**Failure Signatures**: Performance degradation on temporal localization tasks, significant performance gaps across demographic groups, inability to handle diverse conversational domains effectively

**First 3 Experiments**:
1. Evaluate a closed-source model on all three tasks to establish baseline performance
2. Compare open-source model performance against closed-source baseline across all tasks
3. Analyze demographic disparities by running models on subsets of data categorized by demographic metadata

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap findings may not generalize across all model variants or future iterations
- Limited to 13 conversational domains, potentially missing broader real-world scenarios
- Demographic metadata analysis may not capture all potential biases in model performance

## Confidence
- High confidence in benchmark construction methodology and reported performance differences between model types
- Medium confidence in generalizability of demographic disparity findings due to sample representativeness
- Medium confidence in 22.6% performance gap, dependent on specific model versions and evaluation conditions

## Next Checks
1. Evaluate additional model versions and variants to verify stability of reported performance gap between closed- and open-source models
2. Expand demographic metadata collection to include additional demographic factors and test whether observed disparities persist or change
3. Conduct cross-dataset validation by testing models on SONIC-O1 and other multimodal benchmarks to assess consistency in performance patterns