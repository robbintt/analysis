---
ver: rpa2
title: LLM-based Text Simplification and its Effect on User Comprehension and Cognitive
  Load
arxiv_id: '2505.01980'
source_url: https://arxiv.org/abs/2505.01980
tags:
- text
- they
- more
- when
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the potential of large language models
  (LLMs) to enhance information accessibility through minimally lossy text simplification.
  The researchers developed a Gemini-based system using automated evaluation models
  and iterative prompt refinement to simplify complex texts across six domains.
---

# LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load

## Quick Facts
- arXiv ID: 2505.01980
- Source URL: https://arxiv.org/abs/2505.01980
- Reference count: 0
- Primary result: LLM-based text simplification improved comprehension accuracy by 3.9% (p<0.05) in a randomized controlled trial with 4,563 participants

## Executive Summary
This study demonstrates the potential of large language models (LLMs) to enhance information accessibility through minimally lossy text simplification. The researchers developed a Gemini-based system using automated evaluation models and iterative prompt refinement to simplify complex texts across six domains. In a randomized controlled trial with 4,563 participants and 31 texts, readers of simplified texts showed a 3.9% absolute improvement in comprehension (MCQs) compared to those reading original texts (p<0.05). The largest gains occurred in PubMed texts (14.6%), with more modest improvements in finance (5.5%), aerospace/computer science (3.8%), and legal domains (3.5%). Participants also reported greater ease of comprehension and confidence when reading simplified texts.

## Method Summary
The system uses Gemini 1.5 Flash for simplification with iterative prompt refinement guided by automated evaluation. Two autoeval models assess outputs: Gemini Ultra scores readability on a 1-10 scale, while Gemini 1.5 Pro evaluates fidelity through atomic claim mapping with weighted error scoring across eight error types. The refinement loop (824 iterations) optimizes combined score (readability minus fidelity errors) and terminates when improvements plateau. The final system was validated via randomized controlled trial with 4,563 participants reading 31 texts across six domains, measuring comprehension through multiple-choice questions under both open-book and closed-book conditions.

## Key Results
- 3.9% absolute improvement in comprehension accuracy (MCQs) for simplified vs. original texts (p<0.05)
- Largest gains in PubMed domain (14.6% improvement), with smaller but significant gains in finance (5.5%), aerospace/CS (3.8%), and legal (3.5%)
- Simplified texts rated easier to understand and increased reader confidence
- Effect persisted even in closed-book conditions (no text reference), suggesting improved encoding rather than just easier information retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based text simplification improves comprehension accuracy by reducing cognitive load while preserving information fidelity.
- Mechanism: The system transforms complex syntax and domain-specific jargon into accessible language. Readers allocate fewer cognitive resources to decoding, freeing capacity for comprehension and retention. The effect persists even without text reference (closed-book), suggesting improved encoding rather than merely easier information retrieval.
- Core assumption: Working memory capacity is the primary bottleneck for comprehending complex text; reducing linguistic complexity does not substantially alter the semantic content being tested.
- Evidence anchors:
  - [abstract] "participants who read simplified texts achieved 3.9% higher accuracy on comprehension MCQs (p<0.05), with the largest gains in PubMed (14.6%)"
  - [results] "The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted"
  - [corpus] Limited direct corpus evidence on cognitive load reduction mechanisms in LLM simplification; related work focuses on accessibility outcomes rather than mechanism validation.
- Break condition: If simplified text systematically omits information probed by comprehension questions, gains would reflect question-text misalignment rather than genuine comprehension improvement.

### Mechanism 2
- Claim: Iterative prompt refinement with automated evaluation produces higher-quality simplification than static prompts.
- Mechanism: The autoeval system scores candidate simplifications on readability (1-10 scale) and fidelity (weighted error scoring). A Gemini 1.5 Pro model analyzes misclassified examples and rewrites the prompt to address failures. This gradient-free optimization converged after 824 iterations.
- Core assumption: The autoeval metrics (readability score minus error score) correlate with human-perceived simplification quality and downstream comprehension outcomes.
- Evidence anchors:
  - [methods] "The prompt's score was defined as the averaged readability score minus the averaged error scores, and the best prompts were tracked based on this score"
  - [results] "The iterative process was terminated after 824 iterations, when the improvements plateaued"
  - [corpus] Related work "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss" addresses similar fidelity protection concerns, suggesting this is a recognized challenge.
- Break condition: If autoeval scores diverge from human judgment (e.g., high readability but actual incomprehensibility), the refinement loop optimizes the wrong objective.

### Mechanism 3
- Claim: Fidelity autoeval based on atomic claim mapping prevents information loss during simplification.
- Mechanism: The fidelity model decomposes original text into atomic claims, maps each to simplified text excerpts, and scores 8 error types across completeness (information loss) and entailment (distortion, hallucination) with differential weights (unfactual errors weighted 4× vs. minor fidelity loss at 1×).
- Core assumption: Atomic claim decomposition captures all information relevant for comprehension; the weighted error taxonomy reflects relative importance of different error types.
- Evidence anchors:
  - [methods] "comparing the original and rewritten texts, by first breaking up the original text into atomic claims and then mapping each of the claims to excerpts in the rewritten text"
  - [results] MCQ accuracy improvements suggest key information was preserved; however, "whether the poorer MCQ accuracies post-simplification (dots below the diagonal in Figure 6) reflect random variation or information distortion will need to be studied"
  - [corpus] Corpus evidence on atomic claim mapping efficacy is weak; related taxonomies exist but validation against comprehension is limited.
- Break condition: If atomic claims miss implicit information (e.g., causal relationships, contextual framing), the fidelity score may appear high while comprehension-relevant content is lost.

## Foundational Learning

- **Concept: Cognitive Load Theory**
  - Why needed here: Explains why simplification improves comprehension—readers have limited working memory, and complex syntax/jargon consumes capacity needed for meaning construction.
  - Quick check question: If a reader struggles with vocabulary, what cognitive resource is being depleted that could otherwise support comprehension?

- **Concept: Prompt Engineering with Few-Shot Examples**
  - Why needed here: The simplification model uses a custom prompt with 4 few-shot examples; understanding how examples shape output is essential for debugging and iteration.
  - Quick check question: How would you expect the model's output to change if all few-shot examples used very short sentences?

- **Concept: Randomized Controlled Trial (RCT) Design**
  - Why needed here: The study validates the system via RCT with 6 arms; understanding blocking, randomization, and control conditions is necessary to interpret results and design replication studies.
  - Quick check question: Why does the study include both open-book and closed-book conditions rather than just one?

## Architecture Onboarding

- **Component map:**
  - Simplification Model (Gemini 1.5 Flash + prompt + 4 few-shot examples) → produces simplified text
  - Readability Autoeval (Gemini Ultra) → scores 1-10 on single text input
  - Fidelity Autoeval (Gemini 1.5 Pro) → atomic claim mapping + weighted error scoring
  - Ranking Module → computes combined score (readability − errors), tracks best prompts
  - Prompt Refinement Model (Gemini 1.5 Pro) → analyzes failures, generates prompt variants

- **Critical path:** The autoeval scores determine which prompts survive. If either autoeval is miscalibrated, the entire refinement loop optimizes incorrectly. Start by validating autoeval against human judgment before trusting iterative refinement.

- **Design tradeoffs:**
  - Flash model for simplification (fast, low latency) vs. Ultra/Pro for evaluation (more capable reasoning)
  - Automated evaluation enables scale but may miss nuances human evaluators catch
  - Short texts (~60 words mean) enable controlled study but may not generalize to longer documents

- **Failure signatures:**
  - High readability + high error scores → simplification is too aggressive
  - Low readability + low error scores → simplification is too conservative
  - Divergence between autoeval scores and human comprehension test results → autoeval miscalibration
  - Domain-specific degradation (e.g., legal texts simplify poorly) → few-shot examples lack coverage

- **First 3 experiments:**
  1. **Validate autoeval on held-out texts:** Have humans rate readability and fidelity on 20-30 simplified texts; correlate with autoeval scores. Target r > 0.7 before trusting refinement loop.
  2. **Domain stress test:** Run simplification on 10 texts from your target domain; manually inspect for systematic errors (e.g., number truncation, negation loss, quantifier weakening).
  3. **Comprehension probe replication:** Run a small-scale (n=100) replication of the MCQ paradigm on your target content to verify the 3-4% accuracy improvement holds in your context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LLM-based text simplification yield comprehension gains for users who are intrinsically motivated to understand the material, as opposed to a general population incentivized by survey platforms?
- Basis in paper: [explicit] The Discussion states, "The specific comprehension impact on motivated users who want to understand the material, will also need to be studied in future work."
- Why unresolved: The current study utilized a survey platform where monetary incentives likely prioritized speed over deep reading, potentially depressing measured accuracies and masking effects that might appear in a learning-oriented context.
- Evidence: A randomized study recruiting participants based on their specific need or desire to learn the subject matter (e.g., patients reading about a specific diagnosis) rather than a general population sample.

### Open Question 2
- Question: Do the instances where simplified text resulted in lower question accuracy reflect random statistical variation or specific information distortion introduced by the model?
- Basis in paper: [explicit] The authors ask, "Whether the poorer MCQ accuracies post-simplification (dots below the diagonal in Figure 6) reflect random variation or information distortion will need to be studied with a larger sample size."
- Why unresolved: While the aggregate effect was positive, the study lacked the statistical power to determine if the specific cases of decreased accuracy were caused by hallucinations or meaning changes in the generated text.
- Evidence: A larger-scale study analyzing the specific "failures" (where simplified text scored lower) using human expert review to identify semantic errors or hallucinations in the generated outputs.

### Open Question 3
- Question: Can open-ended testing methodologies provide a more nuanced assessment of the depth of comprehension achieved through text simplification compared to multiple-choice questions (MCQs)?
- Basis in paper: [explicit] The paper suggests, "Future research could further explore alternative testing methodologies, such as open-ended questions ('explain what you learned') to provide a more nuanced assessment of comprehension."
- Why unresolved: The authors note that MCQs may not be challenging enough to capture the full depth of understanding, while their pilot studies found free-text answers unreliable due to low participant motivation on survey platforms.
- Evidence: A study design that couples the simplification intervention with open-ended response collection, specifically using a motivated participant pool to ensure high-quality textual feedback.

## Limitations
- The study used short texts (mean 61.7 words) that may not generalize to longer documents where coherence and narrative flow become critical
- Comprehension was measured only through multiple-choice questions, which may not capture all dimensions of understanding
- The autoeval models were not directly validated against human judgment in the final refinement loop
- Domain-specific effects were observed but not fully characterized across different reader populations

## Confidence

- **High Confidence**: The 3.9% absolute improvement in MCQ accuracy for simplified vs. original texts (p<0.05) across 4,563 participants is a robust finding supported by rigorous RCT methodology.
- **Medium Confidence**: The mechanism that simplification reduces cognitive load while preserving fidelity is plausible but not directly measured; evidence is indirect through comprehension outcomes.
- **Medium Confidence**: The iterative prompt refinement process produced measurable improvements, but the autoeval calibration against human judgment was limited to initial model training rather than ongoing validation.

## Next Checks
1. **Cross-domain validation**: Test the simplification system on 50+ texts from target domain(s) with human evaluation of both readability and fidelity, comparing autoeval scores against human ratings (target correlation r > 0.7).
2. **Longer document testing**: Evaluate the system on multi-paragraph texts (500+ words) to assess coherence preservation and identify any domain-specific degradation patterns.
3. **Comprehension depth assessment**: Supplement MCQ testing with open-ended questions or concept mapping tasks to verify that improvements reflect genuine understanding rather than improved information retrieval.