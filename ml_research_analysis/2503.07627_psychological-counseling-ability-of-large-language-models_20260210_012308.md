---
ver: rpa2
title: Psychological Counseling Ability of Large Language Models
arxiv_id: '2503.07627'
source_url: https://arxiv.org/abs/2503.07627
tags:
- questions
- llms
- psychological
- correctness
- counseling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically assessed the psychological counseling
  ability of mainstream Large Language Models (LLMs) using 1096 questions from the
  Chinese National Counselor Level 3 Examination. The results showed that LLMs achieved
  relatively low accuracy rates, with an average of 43.26% for Chinese questions and
  36.10% for English questions.
---

# Psychological Counseling Ability of Large Language Models
## Quick Facts
- arXiv ID: 2503.07627
- Source URL: https://arxiv.org/abs/2503.07627
- Reference count: 8
- Key result: Mainstream LLMs scored an average of 43.26% on Chinese counseling questions and 36.10% on English ones, with RAG boosting ERNIE-3.5's correctness from 45.8% to 59.6%.

## Executive Summary
This study systematically assessed the psychological counseling ability of mainstream LLMs using a 1096-question dataset from the Chinese National Counselor Level 3 Examination. Results showed overall low accuracy rates, with models performing better on analytical reasoning questions than on knowledge-recall questions. Notably, incorporating a Counselor's Guidebook via Retrieval-Augmented Generation (RAG) improved ERNIE-3.5's correctness by 14 percentage points, highlighting the potential of external knowledge retrieval to enhance counseling capabilities in LLMs.

## Method Summary
The authors evaluated four mainstream LLMs (GLM-3, GPT-4, ERNIE-3.5, Gemini) on a 1096-question dataset covering Chinese National Counselor Level 3 Examination content. Questions were categorized into Knowledge, Analytical, and Application types. For RAG testing, the ERNIE-3.5 model was provided with the Counselor's Guidebook as an external knowledge source during inference. Performance was measured by correctness rate and analyzed across languages (Chinese vs. English) and question types.

## Key Results
- Mainstream LLMs achieved average correctness rates of 43.26% (Chinese) and 36.10% (English) on counseling questions.
- Models performed significantly better on Analytical-based questions than Knowledge-based questions.
- RAG integration using the Counselor's Guidebook improved ERNIE-3.5's correctness rate from 45.8% to 59.6%.
- GLM-3, GPT-4, ERNIE-3.5, and Gemini outperformed GPT-3.5 on the counseling assessment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform significantly better on counseling tasks when their lack of domain-specific knowledge is addressed via Retrieval-Augmented Generation (RAG).
- Mechanism: A mainstream LLM's parametric knowledge is general and incomplete for specialized fields. The RAG system acts as a non-parametric memory module, retrieving relevant professional text from a guidebook and providing it as context, which allows the model to ground its reasoning in established facts it has not memorized.
- Core assumption: The primary bottleneck is a lack of access to information, not a fundamental failure in the model's reasoning or language processing capabilities.
- Evidence anchors:
  - [abstract] "...using the Counselor's Guidebook as a reference for ERNIE-3.5 improved its correctness rate from 45.8% to 59.6., highlighting the potential of Retrieval-Augmented Generation (RAG)..."
  - [section 3.3] "The correctness of the questions after referencing the document was significantly higher than the correctness of the questions previously done."
  - [corpus] Corpus evidence on this specific RAG mechanism is weak; neighbor papers focus on model fine-tuning and dialogue generation, not external retrieval.
- Break condition: Performance gains will plateau if the task requires synthesis or intuition beyond the retrieved text, or if the retrieval system fails to find the correct passage.

### Mechanism 2
- Claim: LLMs exhibit a performance asymmetry across different languages, reflecting the composition of their training data.
- Mechanism: Models develop stronger capabilities in languages that are more heavily represented in their pre-training corpora. Performance degrades in a secondary language due to less robust internal representations and potential information loss or bias introduced during translation.
- Core assumption: The model's underlying competence is not perfectly transferable across all languages.
- Evidence anchors:
  - [abstract] "...average of 43.26% for Chinese questions and 36.10% for English questions."
  - [section 3.1] "This may be attributed to the fact that some LLMs were more heavily optimized for the Chinese model... Meanwhile, it is possible that the questions, originally in Chinese, undergo some bias when translated into English..."
  - [corpus] One neighbor paper, "The Expressions of Depression and Anxiety in Chinese Psycho-counseling," highlights the importance of linguistic specificity in this domain, providing indirect support for language-dependent performance.
- Break condition: This mechanism would be mitigated in a model with a perfectly balanced, high-quality multilingual training set.

### Mechanism 3
- Claim: LLMs are relatively more proficient at analytical tasks (e.g., interpreting case details) than at recalling isolated professional knowledge.
- Mechanism: LLMs are trained on vast, diverse text that hones their ability to recognize patterns and analyze context. They are less reliable at recalling specific, rigid facts or definitions from a closed professional corpus without explicit prompting or retrieval.
- Core assumption: The model's strengths lie in language understanding and contextual analysis rather than in serving as a perfect knowledge base.
- Evidence anchors:
  - [abstract] "...LLMs demonstrated higher accuracy on Analytical-based questions compared to Knowledge-based questions."
  - [section 3.1] "This may imply that LLMs are more adept at understanding and analyzing information in known situations and have limitations in theoretical knowledge of counseling psychology."
  - [corpus] Corpus evidence for this specific task-type distinction is weak among neighbor papers.
- Break condition: This advantage would not hold if analytical questions required deep, non-obvious inferences dependent on missing expert knowledge.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: This is the primary intervention shown to improve model performance, addressing the core knowledge gap.
  - Quick check question: In this study, what was the specific document used as the external knowledge source for the RAG system? (Answer: The Psychological Counselor's Guidebook, Level 3).

- **Task-Based Evaluation (Knowledge vs. Analytical vs. Application)**
  - Why needed here: Aggregating performance into a single score masks specific weaknesses. Disaggregating by task type reveals that models are better at analyzing cases than recalling facts.
  - Quick check question: On which type of question did the LLMs perform best, and what does this indicate about their capabilities? (Answer: Analytical-based questions, indicating strength in processing context over recalling isolated facts).

- **Language & Translation Bias**
  - Why needed here: The study highlights that simply translating a dataset can introduce bias and lower model performance, a critical consideration for multilingual applications.
  - Quick check question: Why might an LLM perform worse on an English version of a test originally written in Chinese? (Answer: The model may be less optimized for English, and the translation process itself can introduce bias or information loss).

## Architecture Onboarding

- Component map: Question -> RAG Retrieval -> Augmented Prompt -> Model -> Answer
- Critical path: The path to improved performance is `Question -> RAG Retrieval -> Augmented Prompt -> Model -> Answer`. The RAG step is the critical lever for increasing accuracy on knowledge-based questions.
- Design tradeoffs:
  - **RAG vs. Fine-Tuning**: RAG is used here for its modularity and ease of updating knowledge without retraining. A tradeoff is that it doesn't fundamentally alter the model's internal reasoning patterns like fine-tuning would.
  - **Assessment vs. Real-World Dialogue**: The system is evaluated on multiple-choice questions. This provides objective, quantitative metrics but trades off the ecological validity of assessing a generative, open-ended counseling dialogue.
- Failure signatures:
  - **Knowledge Hallucination**: Confident but incorrect answers on factual questions (addressed by RAG).
  - **Reasoning/Application Failure**: Incorrectly applying a principle to a specific case scenario, even with context.
  - **Linguistic Degradation**: Lower accuracy on English questions indicates a failure to generalize across languages.
- First 3 experiments:
  1. **Baseline Assessment**: Run the full 1096-question dataset on a baseline model (e.g., GPT-3.5 or GPT-4) without RAG to establish initial accuracy.
  2. **RAG Integration**: Implement a RAG system with the Counselor's Guidebook and measure the uplift in accuracy, aiming to replicate the ~14% improvement seen with ERNIE-3.5.
  3. **Ablation by Question Type**: Isolate performance on Knowledge vs. Analytical questions. This will confirm if the RAG improvement is concentrated in the Knowledge domain, as hypothesized.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does performance on standardized counseling exams correlate with LLM effectiveness in real-time, multi-turn counseling interactions?
- Basis in paper: [explicit] The authors state that "Knowledge of counseling theory does not necessarily reflect the model's ability to practice in actual counseling" and note models "are not able to engage in real-life conversations and adjustments."
- Why unresolved: The study only assessed static multiple-choice questions; no real conversational counseling data was collected or evaluated.
- What evidence would resolve it: A study comparing LLM exam scores with human-rated outcomes in live or simulated counseling sessions.

### Open Question 2
- Question: Would RAG-enhanced LLMs outperform fine-tuned mental health-specific models (e.g., SoulChat, MentaLLaMA) on the same counseling competence benchmarks?
- Basis in paper: [explicit] The authors note that "many other LLMs exist, particularly the Mental Health Large Model" and "Future LLMs may perform better on counseling aptitude tests," but did not include them.
- Why unresolved: Only mainstream general-purpose LLMs were tested; domain-specific models were not evaluated under the same conditions.
- What evidence would resolve it: A direct comparison using the same 1096-question dataset with domain-specific counseling models.

### Open Question 3
- Question: Does prompt engineering or chain-of-thought prompting improve LLM accuracy on counseling assessment tasks compared to simple direct prompts?
- Basis in paper: [explicit] The authors acknowledge that "the prompts in this study used only simple direct questions and answers, which may affect the reliability and accuracy of the results."
- Why unresolved: Only one prompt format was tested; the effect of more sophisticated prompting strategies remains unknown.
- What evidence would resolve it: Controlled experiments comparing accuracy across multiple prompt strategies on the same question set.

## Limitations
- The use of static, multiple-choice questions does not capture the dynamic nature of real psychological counseling.
- Translation of questions from Chinese to English may have introduced subtle biases.
- The relatively small sample of four models limits generalizability across the broader LLM landscape.

## Confidence
- **High Confidence**: The RAG mechanism's ability to improve factual knowledge recall is well-supported by the 14% performance increase observed with ERNIE-3.5. The task-type performance asymmetry (analytical vs. knowledge) is also clearly demonstrated.
- **Medium Confidence**: The language performance difference is plausible given known training data biases, but the translation artifact introduces uncertainty about the true magnitude of this effect.
- **Low Confidence**: The claim that RAG addresses the "primary bottleneck" is asserted but not rigorously tested against alternatives like fine-tuning or domain-specific pre-training.

## Next Checks
1. **Replication with Diverse Models**: Test the RAG intervention across a broader range of LLMs (including open-source models) to verify the consistency of the performance gains.
2. **Dynamic Dialogue Evaluation**: Replace static multiple-choice questions with simulated counseling dialogues graded by domain experts to assess ecological validity.
3. **Alternative Knowledge Integration**: Compare RAG against fine-tuning and prompt engineering baselines to determine if retrieval is indeed the optimal approach for this domain.