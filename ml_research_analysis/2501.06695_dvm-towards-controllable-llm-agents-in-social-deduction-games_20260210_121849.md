---
ver: rpa2
title: 'DVM: Towards Controllable LLM Agents in Social Deduction Games'
arxiv_id: '2501.06695'
source_url: https://arxiv.org/abs/2501.06695
tags:
- agents
- game
- rate
- reward
- werewolf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DVM, a framework for developing controllable\
  \ LLM agents in social deduction games (SDGs) like Werewolf. The framework integrates\
  \ three components\u2014Predictor, Decider, and Discussor\u2014with reinforcement\
  \ learning to enable dynamic adjustment of gameplay proficiency based on specified\
  \ win rate constraints."
---

# DVM: Towards Controllable LLM Agents in Social Deduction Games

## Quick Facts
- arXiv ID: 2501.06695
- Source URL: https://arxiv.org/abs/2501.06695
- Reference count: 32
- Achieves up to 63.3% win rate for villagers and 66.6% for werewolves under unrestricted conditions

## Executive Summary
This paper introduces DVM, a framework for developing controllable LLM agents in social deduction games (SDGs) like Werewolf. The framework integrates three components—Predictor, Decider, and Discussor—with reinforcement learning to enable dynamic adjustment of gameplay proficiency based on specified win rate constraints. A novel decision chain reward mechanism and win rate-constrained reward function are proposed to guide the agent's performance toward target win rates. Experiments demonstrate that DVM outperforms existing methods and successfully modulates its win rates to meet predefined targets, achieving up to 63.3% win rate for villagers and 66.6% for werewolves under unrestricted conditions. This work advances adaptive and balanced gameplay in SDGs, providing new insights into controllable LLM agents.

## Method Summary
The DVM framework combines three core components: Predictor (forecasting game states and opponent behaviors), Decider (making strategic gameplay decisions), and Discussor (managing communication and persuasion). The system employs reinforcement learning with a novel decision chain reward mechanism that evaluates sequences of actions rather than individual moves. A win rate-constrained reward function guides the agent toward target performance levels, allowing dynamic adjustment of gameplay proficiency. The framework processes game state information, predicts opponent actions, generates strategic decisions, and manages discourse to achieve specified win rate constraints while maintaining coherent gameplay.

## Key Results
- Achieves up to 63.3% win rate for villagers and 66.6% for werewolves under unrestricted conditions
- Successfully modulates win rates to meet predefined target constraints through reinforcement learning
- Outperforms existing methods in social deduction game performance while maintaining controllability

## Why This Works (Mechanism)
The framework's success stems from its multi-component architecture that separates prediction, decision-making, and communication tasks. The decision chain reward mechanism captures the temporal dependencies in social deduction games, where sequences of actions matter more than isolated decisions. The win rate-constrained reward function provides explicit control over performance levels, allowing the agent to adapt its strategy based on specified constraints. By integrating these elements within a reinforcement learning framework, DVM can learn complex strategic behaviors while maintaining the ability to adjust its proficiency dynamically based on target win rates.

## Foundational Learning
- Reinforcement Learning: Why needed - Enables agents to learn optimal strategies through trial and error; Quick check - Verify learning curves show convergence toward target win rates
- Multi-Agent Systems: Why needed - Social deduction games involve multiple interacting players; Quick check - Test performance against varying numbers of opponents
- Natural Language Processing: Why needed - Communication and persuasion are critical game elements; Quick check - Evaluate coherence and effectiveness of generated dialogue

## Architecture Onboarding

**Component Map:**
Predictor -> Decider -> Discussor -> Game Environment

**Critical Path:**
Game State → Predictor → Decision Analysis → Decider → Action Selection → Discussor → Communication → Game Environment → Outcome Feedback → Reward Calculation

**Design Tradeoffs:**
The multi-component architecture increases flexibility and specialization but adds computational complexity. The decision chain reward mechanism captures temporal dependencies but requires longer training times. Win rate constraints provide control but may limit optimal strategy exploration. The separation of prediction, decision, and communication tasks enables modular development but introduces potential coordination challenges.

**Failure Signatures:**
Performance degradation when win rate constraints conflict with optimal strategies. Communication breakdowns when Discussor generates incoherent dialogue. Prediction errors leading to suboptimal decision chains. Computational bottlenecks in real-time gameplay scenarios. Loss of adaptability when facing novel opponent strategies outside training distribution.

**First 3 Experiments:**
1. Test win rate controllability by setting different target constraints and measuring achieved performance
2. Evaluate component isolation by disabling each module individually and measuring performance impact
3. Benchmark against baseline agents using standard evaluation metrics for social deduction games

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims based on specific baselines rather than human players, limiting real-world generalizability
- Computational costs and scalability concerns with multi-component architecture not addressed
- Win rate constraint mechanism may face practical challenges with diverse opponent strategies beyond tested scenarios
- Framework complexity may affect reproducibility in different game environments

## Confidence

**Confidence Labels:**
- High confidence: Technical implementation of DVM framework and core components is well-described and experimentally validated
- Medium confidence: Controllable win rates through reinforcement learning shows promise but needs broader validation
- Medium confidence: Performance comparisons with existing methods presented, but advantages need verification in diverse conditions

## Next Checks
1. Test DVM's performance against human players in controlled experiments to validate claims about realistic gameplay adaptability and win rate control
2. Evaluate the framework's scalability by testing with different game sizes and across multiple social deduction game variants
3. Conduct ablation studies to quantify individual contributions of each component and assess computational overhead under real-time gameplay conditions