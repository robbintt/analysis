---
ver: rpa2
title: 'Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded
  and Practical Framework with Synthetic Anomalies'
arxiv_id: '2506.13955'
source_url: https://arxiv.org/abs/2506.13955
tags:
- anomalies
- synthetic
- data
- anomaly
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper bridges unsupervised and semi-supervised anomaly detection\
  \ by introducing synthetic anomalies into the training process. The key innovation\
  \ is a mathematical framework showing that synthetic anomalies improve anomaly modeling\
  \ in low-density regions and enable optimal convergence guarantees for neural network\
  \ classifiers\u2014the first theoretical result for semi-supervised AD."
---

# Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies

## Quick Facts
- arXiv ID: 2506.13955
- Source URL: https://arxiv.org/abs/2506.13955
- Reference count: 40
- Key outcome: Introduces synthetic anomalies to bridge unsupervised and semi-supervised anomaly detection, providing theoretical convergence guarantees and empirical improvements across diverse datasets

## Executive Summary
This paper presents a novel framework that bridges unsupervised and semi-supervised anomaly detection by incorporating synthetic anomalies into the training process. The key innovation is a mathematical framework demonstrating that synthetic anomalies improve anomaly modeling in low-density regions and enable optimal convergence guarantees for neural network classifiersâ€”the first theoretical result for semi-supervised AD. The method involves adding synthetic anomalies sampled uniformly from the data support and training a binary classifier to distinguish normal data from combined known and synthetic anomalies.

## Method Summary
The framework adds synthetic anomalies sampled uniformly from the data support to the training process. A binary classifier is trained to distinguish normal data from the combination of known and synthetic anomalies. This approach provides theoretical guarantees for convergence in semi-supervised anomaly detection and shows consistent empirical improvements across five diverse datasets including tabular, image, and text data. The method generalizes beyond the theoretical model to other classification-based AD approaches like ES and DROCC, though with reduced effectiveness for autoencoder-based methods.

## Key Results
- Introduces the first theoretical convergence guarantees for semi-supervised anomaly detection
- Consistent performance improvements across five diverse datasets (tabular, image, and text)
- Benefits both known and unknown anomalies
- Generalizes to classification-based AD methods like ES and DROCC
- Validates the effectiveness of synthetic anomalies in AD beyond the theoretical framework

## Why This Works (Mechanism)
The framework works by addressing a fundamental limitation in semi-supervised anomaly detection: the lack of negative examples for training. By introducing synthetic anomalies that are uniformly sampled from the data support, the method provides the classifier with examples of what anomalies could look like, even if they're not present in the training data. This synthetic data fills in the low-density regions of the feature space where real anomalies might occur, enabling better discrimination between normal and anomalous instances. The uniform sampling strategy ensures that the synthetic anomalies are representative of the overall data distribution while still providing the negative examples needed for effective binary classification.

## Foundational Learning
- **Uniform sampling from data support**: Generating synthetic anomalies by uniformly sampling from the convex hull of normal data; needed to ensure synthetic anomalies are plausible yet distinct from normal instances; quick check: verify synthetic points lie within the data support boundaries
- **Binary classification for AD**: Treating anomaly detection as a binary classification problem between normal and (synthetic + known) anomalies; needed to leverage established classification theory and guarantees; quick check: ensure classifier architecture is appropriate for binary discrimination
- **Low-density region modeling**: Understanding that anomalies typically occur in low-density regions of the feature space; needed to justify why synthetic anomalies improve detection; quick check: analyze decision boundary density relative to data distribution
- **Semi-supervised learning theory**: The theoretical framework builds on existing semi-supervised learning concepts but adapts them to the anomaly detection setting; needed to provide convergence guarantees; quick check: verify assumptions about data distribution and classifier capacity
- **Synthetic data generation strategies**: The uniform sampling approach is one of many possible strategies; needed to establish a baseline method that's theoretically tractable; quick check: compare performance with alternative sampling strategies

## Architecture Onboarding

**Component Map**: Normal Data -> Synthetic Anomaly Generator -> Binary Classifier <- Known Anomalies

**Critical Path**: The core pipeline involves generating synthetic anomalies from the normal data distribution, combining them with any known anomalies, and training a binary classifier to distinguish between normal instances and the combined anomaly set.

**Design Tradeoffs**: The uniform sampling strategy for synthetic anomalies is simple and theoretically tractable but may not capture all possible anomaly patterns. More sophisticated sampling strategies could potentially improve performance but would sacrifice theoretical guarantees. The choice of binary classifier architecture represents another tradeoff between model complexity and generalization capability.

**Failure Signatures**: Poor performance may occur when the uniform sampling strategy fails to generate synthetic anomalies that are representative of potential real anomalies, particularly in cases with highly complex or non-uniform anomaly distributions. The method may also struggle with autoencoder-based approaches where the reconstruction error provides an alternative signal for anomaly detection.

**Three First Experiments**:
1. Generate synthetic anomalies using uniform sampling from the convex hull of normal data and visualize their distribution relative to real data points
2. Train a simple binary classifier (e.g., logistic regression or small neural network) on the combined dataset of normal data and synthetic anomalies
3. Evaluate the classifier's performance on a held-out test set containing known anomalies and assess improvement over baseline methods

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on specific assumptions about synthetic anomaly generation that may not hold in all real-world scenarios
- Reduced effectiveness for autoencoder-based anomaly detection methods
- Performance gains for truly unknown anomalies (never seen during training) remain less certain
- Uniform sampling strategy may not capture complex or non-uniform anomaly distributions effectively

## Confidence
- Theoretical framework and convergence guarantees: Medium
- Empirical improvements across datasets: High
- Generalizability to all AD methods: Low
- Effectiveness for unknown anomalies: Medium

## Next Checks
1. Test the framework on datasets with non-uniform anomaly distributions and varying degrees of class imbalance to assess robustness
2. Conduct ablation studies on different synthetic anomaly sampling strategies (e.g., density-based sampling) to determine optimal generation methods
3. Evaluate performance on real-world datasets with known anomaly types to validate the method's effectiveness for known anomaly detection in practical applications