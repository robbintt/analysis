---
ver: rpa2
title: Toward Revealing Nuanced Biases in Medical LLMs
arxiv_id: '2507.21176'
source_url: https://arxiv.org/abs/2507.21176
tags:
- bias
- llms
- questions
- perturbed
- multi-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a KG-LLM framework that uses knowledge graphs
  and auxiliary LLMs to systematically generate perturbed clinical questions and apply
  multi-hop reasoning to evaluate complex biases in medical LLMs. The method systematically
  perturbs patient attributes (age, gender, location) and combines them with multi-hop
  reasoning to uncover intersectional biases.
---

# Toward Revealing Nuanced Biases in Medical LLMs

## Quick Facts
- arXiv ID: 2507.21176
- Source URL: https://arxiv.org/abs/2507.21176
- Authors: Farzana Islam Adiba; Rahmatollah Beheshti
- Reference count: 40
- Primary result: KG-LLM framework achieves significantly higher bias detection scores than baselines across three datasets and six LLMs

## Executive Summary
This paper introduces a KG-LLM framework that uses knowledge graphs and auxiliary LLMs to systematically generate perturbed clinical questions and apply multi-hop reasoning to evaluate complex biases in medical LLMs. The method systematically perturbs patient attributes (age, gender, location) and combines them with multi-hop reasoning to uncover intersectional biases. Across three datasets, six LLMs, and five bias types, the framework achieved significantly higher bias detection scores than baselines, revealing up to 0.3–0.4 point increases in bias scores for intersectional attributes.

## Method Summary
The framework operates in two stages: first, a knowledge graph extraction pipeline uses spaCy to identify entities and relations from clinical text, then a Generator LLM creates patient scenarios and an Attacker LLM systematically perturbs demographic attributes while maintaining clinical features constant. Second, perturbed questions are fed to a Target LLM with three-step multi-hop reasoning (triplet extraction, knowledge expansion, inference) and Judge LLMs score bias across demographic dimensions. The method achieves higher bias detection scores than baseline approaches by forcing models to reveal implicit demographic-clinical associations through structured reasoning chains.

## Key Results
- Framework achieved significantly higher bias detection scores than baselines across three datasets
- Revealed up to 0.3–0.4 point increases in bias scores for intersectional attributes
- Human evaluation confirmed superior bias identification capability with statistically significant preference over non-multi-hop approaches in 3 out of 5 scenarios

## Why This Works (Mechanism)

### Mechanism 1
Systematic perturbation of patient attributes through knowledge graph-guided question generation reveals hidden biases that single-attribute or ad-hoc approaches miss. The framework extracts entity-relationship triplets from clinical text, uses a Generator LLM to synthesize these into patient scenarios, then an Attacker LLM systematically modifies specific attributes while holding clinical features constant. This controlled variation isolates the causal impact of demographic changes on model responses.

### Mechanism 2
A three-step multi-hop reasoning process exposes implicit demographic-clinical associations encoded in the Target LLM's weights that remain hidden under standard prompting. The Target LLM is prompted to extract KG triplets from the perturbed question, expand these triplets by linking to internal knowledge about disease ontologies and demographic risk associations, and synthesize a clinical response. Step 2 forces the model to reveal how it associates demographics with clinical concepts.

### Mechanism 3
Combinatorial perturbations (age + gender + location) reveal intersectional biases 0.3–0.4 points higher than single-attribute analyses, exposing non-additive bias interactions. The framework generates questions with systematic attribute combinations, measures response deviations across these combinations, and computes intersectional bias scores. The non-linear increase in bias scores for combined attributes indicates that biases compound rather than simply add.

## Foundational Learning

### Concept: Knowledge Graph Triplets in Clinical NLP
- Why needed here: The entire framework operates on structured triplets (head, relation, tail) extracted from clinical text
- Quick check question: Given the sentence "A 66-year-old male patient who presents with fever lives in Johannesburg," extract three valid triplets in the format `(head → relation → tail)`

### Concept: LLM-as-Judge Evaluation Paradigm
- Why needed here: The framework relies on Judge LLMs to score bias, but these judges introduce their own biases
- Quick check question: If GPT-4o as Judge consistently scores higher bias than Mistral-7B for the same responses, what are two possible explanations and how would you disambiguate them?

### Concept: Chain-of-Thought Prompting with Structured Outputs
- Why needed here: The multi-hop reasoning requires guiding the Target LLM to output intermediate KG triplets before synthesizing answers
- Quick check question: What is the key difference between standard CoT prompting and the three-step multi-hop reasoning used here, in terms of output structure?

## Architecture Onboarding

### Component map:
Extraction Module → spaCy + rule-based filtering → KG triplets from seed text → Generator LLM (GPT-4o) → sentences from KG → Attacker LLM (GPT-4o) → perturbed questions (4 variations per input) → Target LLM with 3-step CoT → KG triplets + expanded associations + clinical responses → Judge LLMs (GPT-4o, Mistral-7B, LLaMA-3.2-3B) → bias scores (0-1 scale) across 5 demographic dimensions → Validation Layer (BERTScore vs. ground truth + human evaluation)

### Critical path:
Seed dataset → KG triplet extraction → sentence generation → systematic perturbation → multi-hop reasoning → response generation → bias scoring

### Design tradeoffs:
- Rule-based filtering precision vs. recall: Table 4 shows filtering increases triplets for DiversityMedQA but decreases for EquityMedQA
- Perturbation complexity vs. interpretability: Full intersectional perturbations yield highest bias scores but complicate causal attribution
- Judge LLM scalability vs. alignment: Automated scoring enables scale but Table 5 shows only 75-77.5% agreement with physician ratings

### Failure signatures:
1. Low perturbed question quality: If factual consistency <3.5 or clinical relevance <3.5, discard the perturbation batch
2. High JS divergence for non-intersectional attributes: If single-attribute divergence >0.5, perturbations may be introducing noise
3. Physician-judge misalignment: If agreement drops below 70%, recalibrate judge prompts before scaling

### First 3 experiments:
1. Pipeline validation: Run 20 random QAs through all three configurations to confirm multi-hop reveals higher bias scores
2. Attribute ablation study: Test single vs. combined perturbations on the Nurse Bias dataset to replicate intersectional patterns
3. Judge calibration: Compare LLM judge scores against physician expert ratings to establish agreement baseline

## Open Questions the Paper Calls Out
- Can the framework effectively scale to reveal biases in sensitive attributes beyond age, gender, and location, such as race, socioeconomic status, or disability?
- To what extent do bias scores generated by "Judge LLMs" correlate with human clinical expert judgment in larger-scale evaluations?
- Does the structured multi-hop reasoning process inadvertently degrade clinical factual accuracy or increase hallucination rates compared to standard prompting?
- How effective is the framework when applied to generative medical tasks other than question answering, such as clinical note generation or discharge summarization?

## Limitations
- Reliance on Judge LLMs for automated scoring introduces potential circularity if judges share similar biases with target models
- Method's effectiveness depends heavily on quality of seed datasets and coverage of KG triplets, with inconsistent filtering effects across datasets
- Clinical significance of 0.3-0.4 point increases in intersectional bias scores remains unclear

## Confidence
- High: Systematic perturbation reveals higher bias scores than single-attribute analyses
- Medium: Multi-hop reasoning is necessary for bias detection
- Medium: 0.3-0.4 point increases in intersectional bias are meaningful

## Next Checks
1. Run the same perturbed questions through multiple Judge LLMs and compare bias scores across judges; if scores vary by >0.2 points, implement ensemble averaging
2. Test the framework on a held-out clinical dataset (e.g., MIMIC-IV clinical notes) not used in original validation
3. Expand human evaluation from 40 to 200+ questions across all three datasets to calculate inter-rater agreement and compare against LLM judge scores