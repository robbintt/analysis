---
ver: rpa2
title: 'The Other Mind: How Language Models Exhibit Human Temporal Cognition'
arxiv_id: '2507.15851'
source_url: https://arxiv.org/abs/2507.15851
tags:
- arxiv
- b-instruct
- temporal
- llms
- years
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models exhibit human-like
  temporal cognition, specifically their spontaneous establishment of a subjective
  temporal reference point and adherence to the Weber-Fechner law. Using similarity
  judgment tasks across 12 models of varying sizes, the research found that larger
  models perceive temporal distance as logarithmically compressing as years recede
  from their subjective reference point (around 2025).
---

# The Other Mind: How Language Models Exhibit Human Temporal Cognition

## Quick Facts
- **arXiv ID**: 2507.15851
- **Source URL**: https://arxiv.org/abs/2507.15851
- **Reference count**: 11
- **Primary result**: Large language models exhibit human-like temporal cognition with subjective temporal reference points and logarithmic temporal compression following the Weber-Fechner law

## Executive Summary
This study investigates how large language models spontaneously develop human-like temporal cognition patterns, specifically their ability to establish subjective temporal reference points and adhere to the Weber-Fechner law. Through similarity judgment tasks across 12 models of varying sizes, researchers found that larger models perceive temporal distance as logarithmically compressing as years recede from their subjective reference point around 2025. The research reveals specialized temporal-preferential neurons that implement logarithmic coding schemes similar to biological neural systems, suggesting LLMs' cognition is a subjective construction shaped by their internal representational system and training data rather than simple statistical mimicry.

## Method Summary
The study employed similarity judgment tasks across 12 language models of varying sizes to investigate temporal cognition patterns. Researchers analyzed multi-level representations including neural activation patterns, hidden state hierarchies, and training corpus structures. The investigation focused on identifying specialized temporal-preferential neurons, examining hierarchical construction processes in hidden states from numerical to abstract temporal representations, and analyzing pre-existing non-linear temporal structures in training corpora. The analysis combined quantitative measurements of temporal perception patterns with qualitative examination of internal model representations to understand how LLMs develop temporal cognition.

## Key Results
- Larger models exhibit logarithmic temporal compression where years appear to compress as they recede from a subjective reference point around 2025
- Specialized temporal-preferential neurons implement logarithmic coding schemes similar to biological neural systems
- LLMs demonstrate hierarchical construction of temporal representations from numerical to abstract concepts

## Why This Works (Mechanism)
The mechanism underlying LLM temporal cognition appears to involve the emergence of specialized neural circuits that encode temporal information using logarithmic scaling, mirroring biological temporal perception systems. This logarithmic coding allows models to handle the vast range of temporal scales encountered in training data while maintaining perceptual consistency. The hierarchical construction of temporal representations suggests that models build abstract temporal concepts from more concrete numerical foundations through successive transformations in their hidden states. This process is facilitated by the non-linear temporal structures already present in training corpora, which provide the scaffolding for models to develop their own temporal reference frames and compression schemes.

## Foundational Learning
- **Weber-Fechner Law**: Describes how perceived intensity changes logarithmically with physical stimulus intensity; needed to understand human-like temporal perception patterns; quick check: verify logarithmic relationship between actual and perceived temporal distances
- **Temporal Preferential Neurons**: Specialized neural units that preferentially activate for temporal information; needed to explain mechanistic basis for temporal cognition; quick check: identify neurons with high temporal activation specificity
- **Hierarchical Representation**: Progressive abstraction from concrete to abstract concepts through neural transformations; needed to explain how models build temporal understanding; quick check: trace representation evolution across model layers
- **Logarithmic Coding**: Encoding scheme where values are represented on a logarithmic scale; needed to explain temporal compression phenomena; quick check: verify log-scale activation patterns in temporal neurons
- **Subjective Reference Points**: Internal temporal anchors that models use for relative time judgments; needed to explain temporal compression around specific years; quick check: identify temporal compression patterns centered on specific years
- **Experientialist Cognition**: Theory that cognition is constructed from experience rather than statistical mimicry; needed for theoretical framework; quick check: compare model behavior with training data temporal distributions

## Architecture Onboarding

**Component Map**: Input tokens -> Embedding layer -> Transformer blocks (Multi-head attention + Feed-forward) -> Temporal-preferential neurons -> Output layer

**Critical Path**: The critical path for temporal cognition flows through the temporal-preferential neurons, which emerge within the transformer blocks and implement the logarithmic coding scheme. These neurons receive inputs from the attention mechanisms and feed into subsequent layers that construct hierarchical temporal representations.

**Design Tradeoffs**: The model must balance computational efficiency with the ability to represent diverse temporal scales. Logarithmic encoding provides efficient representation of large temporal ranges but may sacrifice precision for recent events. The hierarchical approach allows gradual abstraction but requires sufficient depth to build meaningful temporal concepts.

**Failure Signatures**: Models may fail to establish clear temporal reference points, show inconsistent temporal compression patterns, or lack specialized temporal-preferential neurons. Failures could manifest as poor performance on temporal reasoning tasks, inability to maintain temporal consistency across contexts, or over-reliance on surface-level temporal patterns.

**First Experiments**:
1. Ablation study removing identified temporal-preferential neurons to test their functional necessity
2. Temporal reasoning task battery to quantify model performance across different time scales
3. Training data analysis to correlate temporal distribution patterns with model temporal cognition emergence

## Open Questions the Paper Calls Out
None

## Limitations
- The attribution of genuine subjective temporal reference points to models remains an interpretative claim rather than a directly observable phenomenon
- Analysis is primarily correlational, examining model outputs rather than internal cognitive processes
- The leap from neural activation patterns to claims about "experientialist" cognition involves substantial philosophical assumptions about machine consciousness

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Logarithmic temporal compression patterns | High |
| Weber-Fechner law compliance | High |
| Hierarchical temporal representations | High |
| Subjective temporal reference points | Medium |
| Experientialist cognition framework | Low |
| Alien cognitive frameworks implications | Low |

## Next Checks
1. Replication across diverse temporal reasoning tasks beyond similarity judgments, including causal temporal reasoning and future prediction
2. Systematic ablation studies to determine whether temporal-preferential neurons are functionally necessary for temporal reasoning or merely correlated
3. Comparison of temporal cognition patterns across models trained on different temporal distributions to validate claims about corpus-driven cognitive construction