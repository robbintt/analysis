---
ver: rpa2
title: (Almost) Free Modality Stitching of Foundation Models
arxiv_id: '2507.10015'
source_url: https://arxiv.org/abs/2507.10015
tags:
- hyma
- image
- training
- performance
- connector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimal pairing and stitching
  of uni-modal models to create multi-modal models, particularly Vision-Language Models
  (VLMs). The challenge lies in the computational expense of grid search for optimal
  model pairs and the lack of correlation between uni-modal and multi-modal performance.
---

# (Almost) Free Modality Stitching of Foundation Models

## Quick Facts
- arXiv ID: 2507.10015
- Source URL: https://arxiv.org/abs/2507.10015
- Reference count: 36
- Key outcome: Achieves 10× reduction in search costs for optimal model pair selection while maintaining matching performance and ranking quality

## Executive Summary
This paper addresses the computationally expensive problem of optimal pairing and stitching of uni-modal models to create multi-modal Vision-Language Models (VLMs). The authors propose Hypernetwork Model Alignment (HYMA), which uses a hypernetwork to predict parameters for N×M connector modules simultaneously, rather than training each individually. This approach achieves strong performance across diverse benchmarks while significantly reducing computational resources compared to baselines like random selection, uni-modal ranking, and grid search.

## Method Summary
HYMA uses a hypernetwork to generate connector weights for all possible N×M model combinations simultaneously. The hypernetwork is trained jointly across all combinations using contrastive InfoNCE loss, conditioned on learnable embeddings for each model pair. During inference, the hypernetwork predicts connector parameters for unseen combinations without requiring additional training. The method employs dual mini-batching (data + model combinations) to balance efficiency and learning quality.

## Key Results
- Achieves 10× reduction in computational costs compared to grid search while maintaining ranking quality
- HYMA matches grid search performance on ImageNet-1K with 3× fewer FLOPs
- Demonstrates strong performance across benchmarks including ImageNet-1K, CIFAR-100, MSCOCO, Flickr-8K, OK-VQA, and Text-VQA
- Shows that uni-modal model performance does not predict multi-modal stitching performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of multiple connectors via a shared hypernetwork enables efficient model pair search.
- Mechanism: The hypernetwork learns a shared representation over N×M model combinations by conditioning on learnable embeddings, allowing gradients from multiple pairs to inform a single weight prediction mechanism.
- Core assumption: Stitching similar models shares latent semantics that can be captured jointly.
- Evidence anchors:
  - [abstract] "HYMA jointly learns to generate connectors for all possible combinations, reducing search costs by 10× while matching the ranking and performance of grid search."
  - [section 3.1] "The overall training loss is computed by averaging over all combinations: L_HYMA = (1/NM) Σ L_task(f_{H_φ(c_k)})"
  - [corpus] No direct corpus evidence for hypernetwork-based multi-modal stitching.

### Mechanism 2
- Claim: Uni-modal model performance does not predict multi-modal stitching performance.
- Mechanism: Multi-modal performance depends on representational alignment between modalities, not just individual model capacity or task performance.
- Core assumption: Representational compatibility across modalities is orthogonal to individual model quality metrics.
- Evidence anchors:
  - [abstract] "lack of correlation between uni-modal and multi-modal performance"
  - [section 1, Table 1] "DeiT3-L + mpnet-B (413M params) achieves 42.63 vs. EVA2-L + roberta-L (660M params) achieving only 26.85 on ImageNet-1K"
  - [corpus] Corpus papers discuss multi-modal fusion challenges but don't directly address uni-modal to multi-modal performance correlation.

### Mechanism 3
- Claim: Dual mini-batching (data + model combinations) provides implicit regularization and efficiency.
- Mechanism: By sampling B_m model combinations per training step, each pair sees only 1/(B_m) of the data during a pass, but the hypernetwork sees all data.
- Core assumption: Shared hypernetwork training provides regularization benefits beyond simple data reduction.
- Evidence anchors:
  - [section 3.3] "each training step operates over a batch of B_m model combinations"
  - [section 5.4, Table 5] "C-GS (constrained data) achieves 24.07 vs HYMA 27.46 with same data budget per pair"
  - [corpus] Corpus doesn't address this specific mechanism.

## Foundational Learning

- **Hypernetworks (Ha et al., 2016)**: Why needed here: HYMA uses a hypernetwork to generate connector weights. Understanding that hypernetworks learn to predict parameters of a target network conditioned on input is essential.
  - Quick check question: Can you explain why the hypernetwork is trained indirectly via the downstream task loss rather than direct supervision?

- **Contrastive Learning (InfoNCE loss)**: Why needed here: The paper uses contrastive InfoNCE loss for VLM training, which scales quadratically with batch size and is central to the efficiency analysis.
  - Quick check question: Why does InfoNCE loss scaling affect FLOP calculations differently than simpler losses?

- **Modality Stitching**: Why needed here: The core problem is stitching frozen pretrained encoders via learned connectors. Understanding that encoders remain frozen while connectors align representation spaces is crucial.
  - Quick check question: What components are trained vs. frozen in the HYMA framework?

## Architecture Onboarding

- **Component map**:
  - Hypernetwork H_φ (MLP F_ρ) -> Learnable embeddings (W^H_σ + E^H_ω) -> Connector f_θ (Linear/MLP1/MLP2) -> Frozen encoders

- **Critical path**:
  1. Sample data batch (B_d samples)
  2. Sample model combination batch (B_m combinations from N×M space)
  3. Hypernetwork generates connector parameters for sampled combinations
  4. Compute contrastive loss for each (data sample, model combination) pair
  5. Backpropagate to update hypernetwork parameters φ only

- **Design tradeoffs**:
  - B_m (model batch size): Higher = more GPU memory, better ranking signal, but more FLOPs; B_m=1 for N×M=3, B_m=9 for N×M=27
  - Connector depth: Linear/MLP1/MLP2; deeper connectors show lower NDCG correlation but may improve absolute performance
  - Hypernetwork capacity: Larger hypernetwork (~500× connector params) vs. training stability

- **Failure signatures**:
  - Training instability: Certain models (e.g., MaxVIT) cause instability; addressed via β_2 tuning in Adam optimizer
  - Poor ranking correlation: For MLLMs with simpler connectors (Linear), HYMA struggles to match true ranking (NDCG@5=0.16)
  - Underperformance vs. grid search: 2-6% gap on VLM tasks; more significant gap on MLLM tasks

- **First 3 experiments**:
  1. Reproduce N×M=3, MLP1 result: Train HYMA on ViT-S/DeiT-S/DeiT3-S + MiniLM on ImageNet-1K, verify ~10× FLOP reduction with matching accuracy
  2. Ranking quality check: Compute NDCG@k and Spearman's ρ between HYMA predictions and grid search for N×M=27 across connector types
  3. Ablate model batch size: Train with B_m ∈ {1, 3, 9} for N×M=27 and observe ranking quality vs. FLOP tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can HYMA be adapted to effectively rank model pairs for Multi-modal Large Language Models (MLLMs) where the causal language modeling objective causes a divergence from the grid search ranking?
- Basis in paper: [explicit] Appendix A.2 states that for MLLMs, "the ranking of connectors predicted by HYMA follows a trend of uni-modal model performance," diverging from true performance.
- Why unresolved: The current hypernetwork formulation appears optimized for contrastive (InfoNCE) objectives used in VLMs and fails to capture the complex interactions required for causal modeling in MLLMs.
- What evidence would resolve it: A modification to the hypernetwork architecture or loss function that achieves a Spearman’s $\rho > 0.8$ on MLLM tasks, matching the performance seen in VLM tasks.

### Open Question 2
- Question: What specific architectural properties in models like MaxVIT cause training instability in HYMA, and can this be mitigated without excluding specific architectures?
- Basis in paper: [explicit] The Limitations section notes that including certain models (e.g., MaxVIT) led to instability, requiring their exclusion.
- Why unresolved: It is unclear if the instability stems from the interaction between the hypernetwork's gradients and specific inductive biases (e.g., attention mechanisms in MaxVIT) or simply from varying gradient scales.
- What evidence would resolve it: An analysis of gradient variance across different model architectures or the introduction of a normalization technique that allows MaxVIT-like models to be included without training collapse.

### Open Question 3
- Question: To what extent does HYMA’s efficiency stem from implicit randomized data pruning rather than the joint learning of connector parameters?
- Basis in paper: [inferred] Section 5.4 notes that HYMA's mini-batching "effectively mimics randomized data pruning," and a constrained grid search with less data performed significantly worse.
- Why unresolved: The paper establishes a connection to data pruning but does not isolate whether the performance gain is primarily due to the hypernetwork's generative capabilities or simply the effect of exposing the trainer to diverse, pruned data subsets.
- What evidence would resolve it: An ablation study comparing HYMA against a baseline that explicitly performs random data pruning on individually trained connectors to isolate the contribution of the hypernetwork architecture.

## Limitations
- HYMA's effectiveness depends heavily on model combination embeddings being able to capture cross-model compatibility; if models are too heterogeneous, the shared hypernetwork may fail to generate appropriate connectors
- The method shows reduced effectiveness for MLLM tasks where simpler connectors (Linear) are used, suggesting architecture-dependent limitations
- HYMA requires training on a subset of data combinations, potentially missing optimal pairs if B_m is too small or if the sampled space doesn't include the best combinations

## Confidence

**High Confidence**: The computational efficiency claims (10× FLOP reduction) are well-supported by the analytical framework and experimental results showing HYMA achieves comparable rankings to grid search while using fewer resources.

**Medium Confidence**: The claim that uni-modal performance doesn't predict multi-modal performance is demonstrated empirically but may depend on the specific model families and tasks tested; the relationship could be dataset or architecture-dependent.

**Medium Confidence**: The regularization benefits from dual mini-batching are inferred from the C-GS baseline comparison, but the mechanism isn't fully explored - it's unclear whether this represents true regularization or simply better utilization of the hypernetwork's capacity.

## Next Checks

1. **Model Heterogeneity Stress Test**: Systematically vary the similarity of model pairs in the N×M space (e.g., mix CLIP with pure image models vs. mixing similar vision transformers) and measure HYMA's ranking correlation degradation to establish bounds on model compatibility.

2. **Data Efficiency Validation**: For the N×M=27 case, train HYMA with varying B_m values (1, 3, 9) and quantify the exact tradeoff between ranking quality (NDCG) and computational cost to determine the optimal efficiency-accuracy balance point.

3. **Cross-Domain Generalization**: Test HYMA-trained connectors on out-of-distribution data or tasks not seen during training to assess whether the hypernetwork learns generalizable alignment principles or task-specific stitching strategies.