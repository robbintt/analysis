---
ver: rpa2
title: A circuit for predicting hierarchical structure in-context in Large Language
  Models
arxiv_id: '2509.21534'
source_url: https://arxiv.org/abs/2509.21534
tags:
- heads
- induction
- tokens
- in-context
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) learn
  to predict structured sequences with hierarchical dependencies in-context, focusing
  on the role of induction heads and a supporting circuit. The authors design synthetic
  token sequences with first-, second-, and third-order hierarchical structures and
  evaluate several LLMs on these tasks.
---

# A circuit for predicting hierarchical structure in-context in Large Language Models

## Quick Facts
- arXiv ID: 2509.21534
- Source URL: https://arxiv.org/abs/2509.21534
- Reference count: 36
- This paper investigates how Large Language Models (LLMs) learn to predict structured sequences with hierarchical dependencies in-context, focusing on the role of induction heads and a supporting circuit.

## Executive Summary
This paper investigates how Large Language Models (LLMs) learn to predict structured sequences with hierarchical dependencies in-context, focusing on the role of induction heads and a supporting circuit. The authors design synthetic token sequences with first-, second-, and third-order hierarchical structures and evaluate several LLMs on these tasks. They discover that adaptive induction heads, which learn what successor tokens to attend to based on context, emerge in later layers and are crucial for accurate prediction. Additionally, they identify context matching heads that encode latent context identities (e.g., whether the current and previous chunks are the same) and show that these heads support the in-context learning ability of induction heads through controlled ablation experiments. The findings are replicated across multiple LLM families, suggesting a general mechanism for hierarchical in-context learning.

## Method Summary
The study uses synthetic token sequences with hierarchical dependencies to probe LLM in-context learning mechanisms. The authors generate sequences with controlled vocabularies and chunk structures, then analyze attention patterns and ablate specific heads to test causal relationships. They identify adaptive induction heads in later layers that condition successor attention on hierarchical context, and context matching heads in mid-layers that encode latent context identities. Linear probes are trained on head outputs to decode chunk identity, and ablation studies demonstrate the causal importance of context matching heads for the in-context learning ability of induction heads.

## Key Results
- Adaptive induction heads emerge in later layers that attend to successor tokens based on hierarchical context rather than uniformly to all past occurrences
- Context matching heads encode latent context identities (e.g., same/different from previous chunk) with >90% linear probe accuracy for 2nd-order structures
- Ablating context matching heads significantly reduces model prediction accuracy by disrupting the context-sensitivity of downstream adaptive induction heads
- The circuit mechanism is replicated across multiple LLM families including Qwen2.5, Llama, and Mistral

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Induction Heads
Induction heads in later layers appear to modify their attention patterns to select successor tokens based on hierarchical context, rather than attending uniformly to all past occurrences. Standard induction heads copy tokens following a repeated pattern (e.g., [A][B]...[A]->[B]). Adaptive heads condition this copying on a "latent context" (e.g., is this [A] inside chunk α or β?), ignoring successors from mismatched contexts. The model has developed distinct representations for different hierarchical contexts (chunks) which can be queried by the attention mechanism.

### Mechanism 2: Context Matching via N-gram Routing
A distinct set of attention heads, termed "context matching heads," encodes the identity of latent contexts (e.g., specific chunk types) by routing information from preceding tokens. These heads (often showing a "look-one-token-back" pattern) propagate contextual information forward. Linear probes can decode whether the current chunk matches the previous chunk from these heads' outputs, suggesting they build a representation of the hierarchical state. Representations of "chunk identity" are linearly decodable from the residual stream or attention head outputs.

### Mechanism 3: Causal Support Circuit
The in-context learning ability of induction heads is causally dependent on the output of context matching heads. Context matching heads provide the "state" information required by adaptive induction heads to disambiguate successor tokens. Ablating the support head degrades the induction head's ability to select the correct context, even if the induction mechanism itself (attending to successors) remains intact. The flow of information is directional from context matching heads (earlier/mid-layers) to adaptive induction heads (later layers).

## Foundational Learning
- **Induction Heads**: Standard transformer heads that copy tokens following repeated patterns. Why needed: The paper builds directly on the standard induction head definition to define a new "adaptive" variant. Quick check: Can you explain how a standard induction head copies the token "B" in the sequence [A][B] ... [A]?
- **Hierarchical/Markov Dependencies**: Structures where prediction requires memory of preceding context (states), not just the current token. Why needed: The study uses 2nd and 3rd-order sequences where the next state depends on the sequence of previous states. Quick check: In a 2nd-order Markov chain, does the next state depend only on the current state or the sequence of previous states?
- **Ablation Studies**: Lesioning specific components to observe performance degradation. Why needed: The causal claims rely on "lesioning" (zeroing out) specific heads to observe performance degradation. Quick check: If you remove a component and the system fails, does that prove the component is sufficient, necessary, or neither? (Answer: Suggests necessity).

## Architecture Onboarding
- **Component map**: Input -> Early Layers (Static Induction Heads) -> Mid Layers (Context Matching Heads) -> Late Layers (Adaptive Induction Heads) -> Output
- **Critical path**: The flow from Context Matching Heads → Adaptive Induction Heads. If the "state" is not encoded by the matching heads, the adaptive heads revert to static (uniform) behavior and accuracy collapses.
- **Design tradeoffs**: Synthetic vs. Natural: Synthetic data allows precise control over hierarchical order for probing; Natural language tests real-world validity but is messier to analyze. Probe Accuracy: 2nd-order context is highly decodable (>90%); 3rd-order is harder, suggesting capacity limits.
- **Failure signatures**: Uniform Attention: If attention maps look like static stripes (attending to all "B"s equally rather than context-appropriate "B"s), the adaptive circuit is broken or not initialized. Chance-level Accuracy: If ablation of identified matching heads drops model accuracy to random guessing, the intervention was likely too aggressive or the model relies exclusively on this circuit.
- **First 3 experiments**: 1) Run the "repeated random token" test to identify induction heads in your specific model checkpoint. 2) Generate 2nd-order synthetic sequences and train linear classifiers on mid-layer heads to decode "chunk ID." 3) Zero out the specific "context matching" heads identified in step 2 and visualize the attention shift in downstream adaptive induction heads.

## Open Questions the Paper Calls Out
- Can the proposed circuit support learning abstract relationships like those in ARC? The authors explicitly state uncertainty about whether the circuit is "powerful enough to induce abstract relationships" required for ARC tasks.
- Does explicit verbal reasoning (e.g., chain-of-thought) alter how induction heads allocate attention? The paper notes that encouraging in-context learning through reasoning may be a second mechanism for changing attention allocation.
- Does the efficacy of the context matching mechanism diminish as hierarchical depth increases beyond third-order structures? The paper observed lower context decodability for 3rd-order structures and did not test fourth-order or higher hierarchies.

## Limitations
- The study relies heavily on synthetic hierarchical sequences with controlled vocabularies, raising questions about generalization to natural language with richer hierarchical structures
- The analysis identifies specific head roles through attention patterns and ablation effects but could be strengthened by systematic verification of head contributions
- Linear probes are used to decode chunk identity from head outputs, but this does not prove the model uses this representation for in-context prediction in the hypothesized way

## Confidence
- **High Confidence**: The existence of adaptive induction heads in later layers that condition successor attention on hierarchical context
- **Medium Confidence**: The identification of context matching heads as a distinct circuit component that encodes latent chunk identity
- **Medium Confidence**: The causal claim that context matching heads are necessary for the in-context learning ability of adaptive induction heads

## Next Checks
1. Apply the linear probe and ablation methodology to natural language sequences with clear hierarchical structure to test if the same circuit components emerge and are necessary
2. Systematically ablate all heads in the identified context matching layers and measure the effect on both linear probe accuracy and model prediction accuracy to confirm the specificity of the identified circuit
3. Compare the attention patterns of the identified adaptive induction heads across different LLM families on the same synthetic task to verify consistency of the "context-sensitive successor attention" mechanism