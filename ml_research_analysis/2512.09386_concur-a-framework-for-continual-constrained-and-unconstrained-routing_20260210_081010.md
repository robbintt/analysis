---
ver: rpa2
title: 'CONCUR: A Framework for Continual Constrained and Unconstrained Routing'
arxiv_id: '2512.09386'
source_url: https://arxiv.org/abs/2512.09386
tags:
- routing
- accuracy
- strategies
- strategy
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CONCUR, a continual routing framework that supports
  both constrained and unconstrained routing settings. It addresses the inefficiency
  of retraining monolithic routing models whenever new computation strategies emerge.
---

# CONCUR: A Framework for Continual Constrained and Unconstrained Routing

## Quick Facts
- arXiv ID: 2512.09386
- Source URL: https://arxiv.org/abs/2512.09386
- Authors: Peter Baile Chen; Weiyue Li; Dan Roth; Michael Cafarella; Samuel Madden; Jacob Andreas
- Reference count: 13
- Primary result: Modular continual routing framework that outperforms best single strategy and existing methods while reducing training cost

## Executive Summary
CONCUR addresses the inefficiency of retraining monolithic routing models when new computation strategies emerge. It introduces a modular design with separate predictor models for each strategy, trained using dual input representations (general-purpose and task-specific) of both tasks and strategies. This allows easy incorporation of new strategies without retraining existing predictors and better captures problem complexity. Experiments show CONCUR achieves higher end-to-end accuracy and lower inference cost compared to best single strategies and existing routing methods across diverse in-distribution and out-of-distribution tasks.

## Method Summary
CONCUR uses a modular architecture where each computation strategy has its own predictor model. Predictors take concatenated inputs of frozen general-purpose embeddings (sentence-transformers/all-mpnet-base-v2) and learned task-specific representations. Each predictor has two MLP heads: one for accuracy classification (Cross-Entropy) and one for cost regression (MSE). In unconstrained settings, routing uses weighted sums of predictions, while constrained settings employ Dynamic Programming to maximize accuracy under budget constraints. The framework supports both continual learning (adding new strategies) and non-continual settings.

## Key Results
- Outperforms best single strategy across multi-hop QA, general reasoning, and math problems
- Achieves higher end-to-end accuracy and lower inference cost in both continual and non-continual settings
- Reduces training cost in continual settings by avoiding full retraining when extending to new strategies
- Demonstrates strong generalization from in-distribution (MMLU) to out-of-distribution (GPQA) tasks

## Why This Works (Mechanism)
CONCUR's modular design enables efficient adaptation to new strategies without retraining existing components. The dual input representation captures both general task characteristics and strategy-specific nuances, allowing accurate prediction of strategy-task compatibility. The DP-based constrained routing optimally allocates budget across tasks by considering both predicted accuracy and cost, while the weighted-sum approach in unconstrained settings provides efficient routing without complex optimization.

## Foundational Learning

**Dynamic Programming for Constrained Routing**: Used to optimally allocate limited computational budget across tasks by considering accuracy-cost trade-offs. Why needed: Enables optimal routing decisions under strict resource constraints. Quick check: Verify DP solver finds feasible solutions within budget for synthetic test cases.

**Modular Predictor Architecture**: Separate predictors for each strategy trained independently. Why needed: Allows adding new strategies without retraining existing components, reducing computational overhead. Quick check: Confirm adding a new strategy only requires training its predictor, not all existing ones.

**Dual Input Representations**: Concatenation of frozen general-purpose embeddings and learned task-specific representations. Why needed: Captures both universal task features and strategy-specific nuances for accurate predictions. Quick check: Validate that both embedding types contribute meaningfully to prediction accuracy.

## Architecture Onboarding

**Component Map**: Datasets -> Strategy Inference (ground truth collection) -> Predictor Training (accuracy + cost heads) -> Routing Evaluation (DP/weighted-sum) -> Final Accuracy/Cost Metrics

**Critical Path**: Ground truth collection -> Predictor training -> Routing decision making -> End-to-end evaluation

**Design Tradeoffs**: Modular predictors vs monolithic models (flexibility vs parameter efficiency), dual embeddings vs single representation (expressiveness vs complexity), DP routing vs greedy approaches (optimality vs speed)

**Failure Signatures**: 
- DP solver produces infeasible solutions (budget scaling issues)
- Routing accuracy degrades on out-of-distribution tasks (generalizability problems)
- Training time increases disproportionately with strategy count (scalability issues)

**Three First Experiments**:
1. Verify FLOP calculations match paper's reported values within order of magnitude
2. Test predictor accuracy on held-out strategy-task pairs to validate generalization
3. Run constrained routing on small synthetic dataset to verify DP solver behavior

## Open Questions the Paper Calls Out

**Open Question 1**: How does the global dynamic programming approach scale in latency for very large batch sizes?
- Basis: Paper claims efficiency for "reasonable number" of tasks without testing upper limits
- Unresolved: Unclear if routing decision time becomes bottleneck in high-throughput serving with massive batches
- Resolution evidence: Benchmarks measuring solve time and end-to-end latency as batch size scales to 10^4 or 10^5

**Open Question 2**: Can modular predictors generalize to complex strategies beyond standard decoding, such as tool use or RAG?
- Basis: Paper mentions broadening strategy definitions but current inputs may be insufficient for higher-dimensional strategy spaces
- Unresolved: Current feature inputs may not capture distinct cost-accuracy trade-offs of external tools or retrieval components
- Resolution evidence: Experiments integrating RAG configurations as distinct strategies within CONCUR

**Open Question 3**: Is reliance on FLOPs an effective proxy for real-world constraints like monetary cost or wall-clock latency?
- Basis: Authors use FLOPs as standardized efficiency proxy but don't address hardware utilization or API pricing variances
- Unresolved: Optimizing for FLOPs doesn't guarantee minimizing latency or dollar cost, which are primary constraints in production
- Resolution evidence: Comparative analysis of routing decisions when cost predictor trained on actual latency measurements vs theoretical FLOPs

## Limitations
- Exact FLOP calculation formula unspecified, affecting budget scaling and DP solver behavior
- Training/validation split sizes not provided, potentially impacting reproducibility
- Dual input representation effectiveness not fully validated across all task types

## Confidence

**High Confidence**: Core modular predictor architecture and training procedure clearly specified; claim of outperforming best single strategies and existing routing methods well-supported by experimental setup.

**Medium Confidence**: Claim of reduced training cost in continual settings supported by modular design, but exact computational savings depend on implementation specifics not fully detailed.

**Low Confidence**: Generalization capability across diverse out-of-distribution tasks relies on dual input representation effectiveness, which is not fully validated for all task types.

## Next Checks

1. **FLOP Calculation Validation**: Reproduce FLOP calculations for each strategy and verify they match the order of magnitude reported in Table 2 for "Best single strategy" baselines to ensure correct DP solver budget constraints.

2. **Generalization Gap Analysis**: Measure and compare accuracy gap between in-distribution (MMLU) and out-of-distribution (GPQA) tasks to identify if task-specific embeddings are overly dominating general-purpose features.

3. **Training Cost Comparison**: Quantify actual training time and resource usage for CONCUR in continual settings versus retraining monolithic models for each new strategy to validate claimed efficiency gains.