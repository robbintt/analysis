---
ver: rpa2
title: Textual Entailment and Token Probability as Bias Evaluation Metrics
arxiv_id: '2510.07662'
source_url: https://arxiv.org/abs/2510.07662
tags:
- bias
- metrics
- evaluation
- scores
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work compares two approaches to measuring social bias in
  language models: token probability (TP) metrics and natural language inference (NLI)
  metrics. While TP metrics are widely used, they are criticized for being distant
  from real-world use cases.'
---

# Textual Entailment and Token Probability as Bias Evaluation Metrics

## Quick Facts
- **arXiv ID**: 2510.07662
- **Source URL**: https://arxiv.org/abs/2510.07662
- **Reference count**: 16
- **Primary result**: NLI and TP bias evaluation metrics show very low correlation (R² ≤ 0.14) across 17 models, indicating they measure fundamentally different model behaviors.

## Executive Summary
This work compares two approaches to measuring social bias in language models: token probability (TP) metrics and natural language inference (NLI) metrics. While TP metrics are widely used, they are criticized for being distant from real-world use cases. The authors develop WinoQueer-NLI, a novel dataset that uses NLI for bias evaluation on exactly the same bias definitions as the original WinoQueer-TP dataset. They conduct extensive experiments across 17 models and find that NLI and TP metrics behave substantially differently, with very low correlation between them. NLI metrics are more brittle and sensitive to wording of counterstereotypical sentences, but slightly less sensitive to wording of tested stereotypes than TP approaches. The authors conclude that neither TP nor NLI is a "better" bias metric in all cases, and recommend a combination of TP, NLI, and downstream bias evaluations for comprehensive assessment.

## Method Summary
The authors convert the WinoQueer TP dataset (46,036 sentence pairs) into WQ-NLI (38,144 sentence triples) using predefined templates. They finetune 17 models from 7 families on MNLI for the NLI task, then evaluate each model on WQ-NLI triples to obtain probability scores for entailment, neutral, and contradiction given both stereotypical and counterstereotypical premises. Eleven different conversion metrics (M1-M11) are tested to aggregate these probabilities into percentile bias scores comparable to TP. The study finds maximum R² correlation of 0.14 between TP and NLI metrics, with NLI showing higher sensitivity to wording variations and unexpected behavior after debiasing interventions.

## Key Results
- NLI and TP metrics show very low correlation (R² ≤ 0.14) across 17 models, indicating they measure fundamentally different behaviors
- NLI metrics are more brittle and sensitive to wording variations than TP approaches, particularly for counterstereotypical sentences
- NLI bias scores often increase after debiasing interventions while TP scores decrease, suggesting NLI may detect underdebiased cases but responds unpredictably
- Neither TP nor NLI is universally "better" - the paper recommends combining both with downstream evaluations for comprehensive bias assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NLI metrics measure a fundamentally different model behavior than TP metrics, making them unsuitable as a drop-in replacement.
- Mechanism: TP metrics evaluate aggregated token probabilities for specific outputs, reflecting a model's latent associations. NLI metrics require a model to classify semantic relationships between sentences, which involves higher-level reasoning and is sensitive to sentence structure and wording.
- Core assumption: If two metrics measure the same underlying property (bias), their scores should be correlated. A lack of correlation indicates they measure different properties or that one is noisy.
- Evidence anchors:
  - [abstract] "We show that, curiously, NLI and TP bias evaluation behave substantially differently, with very low correlation among different NLI metrics and between NLI and TP metrics."
  - [section] "We conclude that token probability and NLI, when formulated as percentile bias scores, do not seem to be measuring the same model behavior."
- Break condition: This would be refuted if a strong correlation (e.g., R² > 0.5) were found between a well-formulated NLI metric and TP metrics across a diverse set of models.

### Mechanism 2
- Claim: NLI-based bias metrics are more brittle and unstable than TP metrics, leading to unpredictable results.
- Mechanism: The NLI task formulation is highly sensitive to the specific wording of stereotypes, counterfactual identities, and sentence templates. Small changes in input phrasing can cause large shifts in the probability distribution over [entailment, neutral, contradiction], making the aggregated percentile scores noisy. TP, which sums token probabilities, is less susceptible to these discrete shifts.
- Core assumption: A reliable metric should be reasonably robust to minor linguistic variations that do not change the core semantic meaning or bias being tested.
- Evidence anchors:
  - [abstract] "NLI metrics are more sensitive to wording of stereotypes and counterfactual identities, making them more brittle than TP approaches."
  - [section] "The overall behavior of the tested NLI metrics seems to be brittle and hard to predict."
- Break condition: This would be refuted if NLI metrics showed high internal consistency and low sensitivity to minor syntactic or lexical variations.

### Mechanism 3
- Claim: NLI metrics may capture different aspects of bias, such as underdebiasing, but their instability precludes their use as a standalone metric.
- Mechanism: The NLI task, which requires a model to judge if an identity entails a stereotype, may be better at surfacing latent stereotypical associations that persist after debiasing than TP metrics, which might be more easily gamed or show less sensitivity. However, this sensitivity comes at the cost of overall stability.
- Core assumption: Debiasing methods may affect TP and NLI performance differently, potentially masking residual bias in one metric but not the other.
- Evidence anchors:
  - [key outcome] "While NLI may detect underdebiased cases, its instability and poor correlation with established TP metrics mean it should be used alongside, not instead of, TP and downstream bias evaluations."
  - [section] "The concerning trend here is that NLI bias metrics often get higher after debiasing, while TP bias metrics uniformly move down."
- Break condition: This would be refuted if future work demonstrated that NLI scores consistently and reliably decreased after debiasing in tandem with TP scores.

## Foundational Learning

- Concept: **Token Probability (TP) Bias Metric**
  - Why needed here: This is the established baseline against which the paper compares NLI. Understanding its calculation (percentage of cases where stereotypical output has higher probability) is essential to interpret the results.
  - Quick check question: If a model assigns a higher aggregated token probability to "The doctor is a man" compared to "The doctor is a woman," what would be its likely TP bias score trend?

- Concept: **Natural Language Inference (NLI) / Textual Entailment**
  - Why needed here: This is the alternative bias evaluation task proposed and tested. It's crucial to understand its formulation (premise, hypothesis, labels: entail, contradict, neutral) and why "neutral" is considered the unbiased outcome.
  - Quick check question: In the paper's setup, if a premise is "Some people are LGBTQ+" and the hypothesis is "Some people are sinful," why is an "entail" prediction considered a sign of bias?

- Concept: **Percentile Bias Score**
  - Why needed here: The paper struggles to convert raw NLI probability tuples into an interpretable aggregate score comparable to TP's percentile. The choice of this conversion method is a central experimental challenge.
  - Quick check question: For an NLI metric, the bias score is the percentage of test instances where `p(Entailment|Stereotype) > p(Entailment|Counterstereotype)`. Why might this simple approach fail to capture the nuance of an "unbiased" model, which should ideally predict "neutral" for all instances?

## Architecture Onboarding

- Component map: WQ-TP Dataset -> WQ-NLI Dataset (conversion) -> MNLI Finetuning -> NLI Probability Extraction -> 11 Conversion Metrics (M1-M11) -> Percentile Bias Scores -> Correlation Analysis
- Critical path:
  1. **Data Preparation**: Convert WQ-TP sentence pairs into WQ-NLI triples using defined templates (plural and singular formats with 173 harm predicates and 19 templates)
  2. **Model Finetuning**: Finetune all subject models on the MNLI dataset for 4 epochs with linear classifier head (BERT/RoBERTa/GPT2: full finetuning; Llama/Qwen/Gemma/Mistral: LoRA)
  3. **Evaluation**: For each model and test triple, obtain probability scores for entailment, neutral, and contradiction given both stereotypical and counterstereotypical premises
  4. **Metric Calculation**: Apply each conversion metric (M1-M11) to the probabilities to get percentile bias scores
  5. **Correlation Analysis**: Calculate R² and Pearson correlation between TP and NLI scores
  6. **Sensitivity Analysis**: Use mutual information regression to determine which factors most influence scores

- Design tradeoffs:
  - **TP vs. NLI**: Tradeoff between TP's stability and broad applicability vs. NLI's potential for detecting underdebiasing and being more "task-like," but at the cost of high brittleness
  - **Choice of NLI Metric (M6 vs. M9)**: M6 penalizes entailment and rewards neutral; M9 rewards neutral and half-credits contradiction. The choice depends on whether one wants to strictly reward the "correct" answer or also give some credit for contradicting a stereotype

- Failure signatures:
  - **Low R² (R² < 0.14)**: Primary failure signal indicating metrics do not measure the same thing
  - **NLI Score Increase Post-Debiasing**: If an NLI score goes up after a debiasing intervention (which lowers TP score), this signals NLI's unpredictable response
  - **High Mutual Information on Wording**: If factors like "cis" or specific templates have high mutual information with bias scores, it signals that incidental wording choices are introducing noise

- First 3 experiments:
  1. **Replicate M6/M9 vs. TP Correlation**: Finetune a new model on MNLI, evaluate on WQ-NLI, and verify the weak linear relationship (R² ≈ 0.14) holds
  2. **Ablate Wording Sensitivity**: Paraphrase hypothesis sentences in a subset of test triples and measure variance in the model's output to quantify NLI's brittleness to wording
  3. **Test a New NLI Metric Formulation**: Design a new conversion metric using KL-divergence from an ideal "neutral" distribution and check its correlation with TP

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do NLI bias metrics often increase after debiasing via continued pretraining while TP metrics uniformly decrease?
  - Basis in paper: Section 3.3 notes the concerning trend where NLI metrics rise (indicating more bias) on debiased models like BERT Base Cased, contrary to the expected behavior seen in TP metrics.
  - Why unresolved: The paper identifies this divergent behavior but does not offer a mechanistic explanation for why continued pretraining affects the two evaluation methods in opposite directions.
  - What evidence would resolve it: Layer-wise analysis of model representations during debiasing to see if TP and NLI classifiers rely on distinct feature sets that are affected differently by new data.

- **Open Question 2**: How should practitioners optimally combine or weigh TP, NLI, and downstream metrics for a comprehensive bias assessment?
  - Basis in paper: The conclusion recommends using a "combination of TP, NLI, and downstream bias evaluations" but provides no specific framework for integrating these conflicting signals.
  - Why unresolved: Since the metrics correlate poorly (R² ≤ 0.14), they provide different information; simply averaging them may be misleading, yet no hierarchy or weighting strategy is proposed.
  - What evidence would resolve it: A meta-evaluation study correlating various weighted combinations of these metrics with human-annotated assessments of real-world model harms.

- **Open Question 3**: Do the low correlations between TP and NLI metrics persist across non-English languages and non-LGBTQ+ social dimensions?
  - Basis in paper: The limitations section notes the study is restricted to English and US-centric LGBTQ+ contexts, leaving the generalizability of the low-correlation finding untested.
  - Why unresolved: Grammatical gender in non-English languages or different stereotype structures in other social domains might alter how NLI and TP metrics interact.
  - What evidence would resolve it: Replicating the WQ-TP/WQ-NLI framework on multilingual models or datasets focusing on other protected groups (e.g., race, religion).

- **Open Question 4**: To what extent does the label distribution mismatch between MNLI training and the "always neutral" WQ-NLI evaluation contribute to the observed instability of NLI metrics?
  - Basis in paper: The limitations section highlights the "considerable difference in label distribution" between the balanced MNLI dataset and the neutral-only WQ-NLI test set as a potential factor in performance.
  - Why unresolved: It is unclear if the brittleness of NLI metrics is inherent to the task of bias detection or if it is an artifact of evaluating models on a label distribution they were not fine-tuned to expect.
  - What evidence would resolve it: Comparing metric stability when models are fine-tuned on neutral-heavy NLI datasets versus standard balanced datasets.

## Limitations

- **Dataset construction bias**: Converting WQ-TP to WQ-NLI introduces potential confounding factors that could influence the observed differences between TP and NLI metrics
- **MNLI label distribution mismatch**: The considerable difference between MNLI's balanced label distribution and WQ-NLI's neutral-only evaluation may contribute to NLI metric instability
- **Language and cultural scope**: Results are limited to English and US-centric LGBTQ+ contexts, leaving generalizability to other languages and social dimensions untested

## Confidence

- **Low**: On the broader claim that NLI metrics are fundamentally unsuited as a replacement for TP metrics, as the specific formulations tested may not represent all possible NLI-based approaches
- **Medium**: On the finding that NLI metrics are more sensitive to wording variations than TP metrics, as the extent of this sensitivity may vary with different metric formulations
- **High**: On the empirical observation that NLI and TP metrics behave substantially differently across 17 models, given the clear experimental setup and robust results

## Next Checks

1. **Test a new NLI metric formulation using KL-divergence**: Design and implement an NLI bias metric that measures the KL-divergence between the model's predicted distribution and an ideal "neutral" distribution (p(E)=p(N)=p(C)=1/3). Evaluate this metric on the same WQ-NLI dataset and compute its correlation with TP scores to determine if alternative NLI formulations might better align with TP behavior.

2. **Systematically ablate wording sensitivity**: Create a controlled experiment where specific wording variations (e.g., changing "are LGBTQ+" to "identify as LGBTQ+" or "have LGBTQ+ identity") are introduced in the hypothesis sentences. Measure the variance in NLI output probabilities across these variations and compare this variance to the variance observed when similar changes are made in TP-style sentence pairs.

3. **Test NLI metrics on a downstream bias task**: Select a downstream task known to be affected by social bias (e.g., coreference resolution with gendered pronouns or hate speech detection). Measure model performance on this task and correlate it with both TP and NLI bias scores to determine which metric better predicts real-world bias manifestations.