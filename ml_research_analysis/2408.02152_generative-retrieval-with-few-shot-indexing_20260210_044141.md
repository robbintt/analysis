---
ver: rpa2
title: Generative Retrieval with Few-shot Indexing
arxiv_id: '2408.02152'
source_url: https://arxiv.org/abs/2408.02152
tags:
- few-shot
- indexing
- retrieval
- document
- docids
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Few-Shot GR, a novel generative retrieval framework
  that uses few-shot prompting instead of training-based indexing. Existing generative
  retrieval methods rely on training a model to map queries to document identifiers
  (docids), which is resource-intensive and limits adaptability to dynamic corpora.
---

# Generative Retrieval with Few-shot Indexing

## Quick Facts
- arXiv ID: 2408.02152
- Source URL: https://arxiv.org/abs/2408.02152
- Authors: Arian Askari; Chuan Meng; Mohammad Aliannejadi; Zhaochun Ren; Evangelos Kanoulas; Suzan Verberne
- Reference count: 38
- Primary result: Few-Shot GR achieves superior recall and MRR compared to training-based generative retrieval methods while being significantly more efficient in indexing

## Executive Summary
The paper proposes Few-Shot GR, a novel generative retrieval framework that eliminates the need for training-based indexing by using few-shot prompting with large language models. Unlike existing generative retrieval methods that require fine-tuning models to map queries to document identifiers, Few-Shot GR uses in-context learning to generate a docid bank through demonstration examples. The approach employs one-to-many mapping where multiple docids are generated per document using pseudo queries, significantly improving recall. Experiments on Natural Questions and MS MARCO show that Few-Shot GR outperforms state-of-the-art generative retrieval methods while being more efficient for dynamic corpora.

## Method Summary
Few-Shot GR reframes information retrieval as a sequence-to-sequence generation task where documents are indexed by generating multiple document identifiers (docids) through few-shot prompting rather than training. The method uses a large language model with 3 demonstration examples to learn the pattern of mapping pseudo queries (generated by InPars) to meaningful docids. During retrieval, constrained beam search ensures generated docids match valid entries in the precomputed docid bank. The one-to-many mapping approach generates multiple docids per document to capture diverse query formulations, improving recall. The framework eliminates training requirements, making it particularly suitable for dynamic corpora where documents frequently change.

## Key Results
- Few-Shot GR achieves higher recall@10 and MRR@10 compared to training-based generative retrieval methods
- One-to-many mapping (n=10 docids per document) yields 27.2% improvement in Recall@10 over single docid mapping
- The method demonstrates significant indexing efficiency gains by avoiding training while maintaining competitive retrieval performance
- LLM selection significantly impacts performance, with Llama-3-8B and Zephyr-7B showing superior results compared to T5

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Indexing Without Training
- Claim: LLMs can create effective document identifiers through few-shot prompting alone, bypassing the need for fine-tuning.
- Mechanism: The system uses in-context learning with 3 demonstration examples (query→docid pairs) to teach the LLM the identifier generation pattern. The LLM leverages its pre-trained semantic understanding to map pseudo queries to meaningful docids (e.g., "minority-interest-accounting"), eliminating the objective mismatch that occurs when fine-tuning LLMs for GR tasks.
- Core assumption: The LLM's pre-trained knowledge contains sufficient semantic understanding to generate consistent, discriminative identifiers without task-specific training.
- Evidence anchors:
  - [abstract] "It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus"
  - [section 1] "Because there is a gap between the learning objectives of LLMs pre-training (text generation) and GR fine-tuning (query–docid mapping), fine-tuning an LLM with GR's objective may cause the LLM to forget its pre-trained knowledge"
  - [corpus] Related work on zero-shot GR (ZeroGR paper) supports the viability of training-free approaches
- Break condition: When documents contain domain-specific terminology or concepts outside the LLM's pre-training distribution

### Mechanism 2: One-to-Many Mapping via Pseudo Queries
- Claim: Generating multiple distinct docids per document improves recall by capturing diverse query formulations that could lead to the same document.
- Mechanism: For each document, n pseudo queries are first generated (using InPars), then each pseudo query produces a distinct docid. This creates multiple retrieval "entry points" to the same document. At retrieval time, different user queries can match different docid patterns, increasing the probability of successful retrieval.
- Core assumption: A document's relevance can be expressed through multiple semantically valid query formulations, and the LLM will generate different docids in response to different pseudo queries.
- Evidence anchors:
  - [abstract] "we devise few-shot indexing with one-to-many mapping to further enhance Few-shot GR"
  - [section 4] "increasing the number of generated docids from 1 to 10 yields a 27.2% improvement in Recall@10"
  - [corpus] Limited corpus evidence on optimal number of docids; paper uses 10 as saturation point
- Break condition: When pseudo queries lack diversity, producing redundant docids that don't expand retrieval coverage

### Mechanism 3: Constrained Decoding for Guaranteed Valid Retrieval
- Claim: Constrained beam search ensures generated docids always correspond to valid documents in the corpus.
- Mechanism: During retrieval, the LLM generates tokens autoregressively, but the beam search is constrained to only pursue paths that could complete to a valid docid in the pre-built bank. This transforms open-ended generation into a controlled search over the docid vocabulary, guaranteeing that every output maps to an actual document.
- Core assumption: The docid bank is complete (covers all documents) and the constraint mechanism doesn't exclude valid retrieval paths.
- Evidence anchors:
  - [abstract] "retrieves by constraining generated docids to match those in the bank"
  - [section 2] "we use constrained beam search [6] to the LLM's decoding, ensuring the generated docid matches a valid docid in the docid bank"
  - [corpus] Weak corpus evidence on constrained decoding efficiency at scale; assumes trie-based or prefix-tree constraints
- Break condition: When the docid bank grows too large (millions of documents), potentially causing constraint lookup latency issues

## Foundational Learning

- **Concept: Generative Retrieval Paradigm**
  - Why needed here: Few-shot GR fundamentally reframes IR as seq2seq generation rather than similarity matching
  - Quick check question: Can you explain why GR requires a docid bank instead of directly generating document text?

- **Concept: In-Context Learning / Few-Shot Prompting**
  - Why needed here: The entire indexing mechanism depends on the LLM learning the docid generation pattern from 3 examples
  - Quick check question: What would happen if the demonstration examples in Figure 1 used numeric docids instead of text docids?

- **Concept: Constrained Decoding (Beam Search with Constraints)**
  - Why needed here: Retrieval correctness depends on guaranteeing generated tokens form valid docids
  - Quick check question: How does constrained beam search differ from post-hoc filtering of generated outputs?

## Architecture Onboarding

**Component map:**
Documents -> Pseudo Query Gen (InPars) -> n queries -> LLM + Few-shot Prompt -> n docids per document -> Docid Bank (deduplicated)

Query -> Same LLM + Same Prompt -> Constrained Beam Search (docid bank as constraint) -> Generated docid -> Document lookup

**Critical path:**
1. Verify pseudo query quality (InPars generator must produce diverse, relevant queries)
2. Validate few-shot prompt format matches Figure 1 exactly
3. Confirm deduplication produces unique docid→document mapping
4. Test constrained decoding correctly limits generation to bank vocabulary

**Design tradeoffs:**
- **# docids per document (n=10 used):** Higher n improves recall but increases indexing time and storage linearly
- **Docid length (3-15 tokens):** Longer docids carry more semantics but slower constrained search; shorter risks collisions
- **LLM selection (Llama-3-8B vs Zephyr-7B vs T5):** Better LLMs improve performance (Table 2) but increase inference cost

**Failure signatures:**
- **Low Recall@1 but high Recall@10:** Indicates one-to-many mapping helps but ranking within constraints is weak
- **High docid redundancy:** Pseudo queries too similar; check InPars temperature/diversity settings
- **Retrieval latency spikes:** Docid bank too large for efficient constraint checking; consider prefix-tree indexing
- **Domain mismatch:** LLM generates plausible but incorrect docids for specialized corpora

**First 3 experiments:**
1. **Minimal validation:** Index 100 documents with n=3 docids each, manually inspect whether generated docids are semantically meaningful and distinct
2. **Retrieval sanity check:** Run retrieval on 20 known queries, verify constrained beam search produces valid docids and correct document mappings
3. **Ablation on n:** Compare n∈{1, 3, 5, 10} on a held-out subset to identify saturation point before full corpus indexing

## Open Questions the Paper Calls Out
None

## Limitations
- Dynamic corpus scalability: The docid bank generation requires re-prompting the LLM for every document addition, potentially becoming prohibitive for frequently updated corpora
- Pseudo query quality dependence: The one-to-many mapping relies entirely on InPars for generating diverse pseudo queries, with no analysis of semantic diversity or redundancy
- Constraint mechanism scalability: Constrained beam search efficiency at scale is untested, with potential lookup bottlenecks for large docid banks

## Confidence
**High confidence**: The core mechanism of few-shot indexing (avoiding training while maintaining performance) is well-supported by experimental results showing superior recall and MRR compared to training-based GR methods.

**Medium confidence**: The one-to-many mapping benefit (27.2% recall improvement from n=1 to n=10) is demonstrated, but lacks depth on query diversity and saturation points. The constrained decoding mechanism is theoretically sound but lacks empirical validation on scalability.

**Low confidence**: Claims about dynamic corpus advantages lack quantitative comparison with incremental training approaches. The robustness to domain shift and extreme corpus sizes remains untested.

## Next Checks
1. **Scalability analysis**: Measure indexing time and retrieval latency on corpora of increasing size (100K → 1M → 10M documents) to identify constraint search bottlenecks and quantify the trade-off between indexing efficiency and retrieval speed.

2. **Pseudo query diversity audit**: For a subset of documents, analyze the semantic similarity between generated pseudo queries and their corresponding docids. Measure whether n=10 provides diminishing returns in practice and identify the optimal number of docids per document based on query diversity metrics.

3. **Domain transfer experiment**: Test Few-Shot GR on a specialized corpus (e.g., legal documents or scientific papers) to evaluate performance degradation when LLM pre-training knowledge doesn't align with domain terminology. Compare against domain-adapted training-based approaches to quantify the generalization limits.