---
ver: rpa2
title: 'MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder
  Diagnosis'
arxiv_id: '2505.00032'
source_url: https://arxiv.org/abs/2505.00032
tags:
- data
- mdd-llm
- llms
- diagnosis
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MDD-LLM, a large language model-based framework
  for diagnosing major depressive disorder (MDD). The authors address the challenge
  of limited medical resources and complex diagnostic methods for MDD by fine-tuning
  LLMs on a large-scale UK Biobank dataset (274,348 records).
---

# MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis

## Quick Facts
- **arXiv ID**: 2505.00032
- **Source URL**: https://arxiv.org/abs/2505.00032
- **Reference count**: 0
- **Primary result**: 70B model achieves accuracy 0.8378 and AUC 0.8919 on UK Biobank MDD diagnosis task

## Executive Summary
This paper presents MDD-LLM, a large language model-based framework for diagnosing major depressive disorder (MDD). The authors address the challenge of limited medical resources and complex diagnostic methods for MDD by fine-tuning LLMs on a large-scale UK Biobank dataset (274,348 records). They propose a tabular data transformation method to convert structured medical data into LLM-compatible prompts and employ parameter-efficient fine-tuning techniques (LoRA and QLoRA). The framework demonstrates strong performance with the 70B model achieving an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming traditional machine learning and deep learning approaches. The model also shows robustness to missing data and provides interpretable explanations for its predictions.

## Method Summary
MDD-LLM fine-tunes Llama 3.1 models (8B and 70B parameters) on UK Biobank data using LoRA/QLoRA for parameter-efficient adaptation. The method transforms structured medical records into textual prompts using a "Text Template" format, then trains the LLM to predict binary MDD diagnosis with probability scores and explanations. Training uses 80% of data with 5-fold cross-validation, evaluating on 20% held-out test set. The framework converts tabular features like age, BMI, sleep duration, and medical history into natural language prompts that the LLM can process, then generates "Yes/No" predictions with associated confidence scores.

## Key Results
- 70B model achieves accuracy of 0.8378 and AUC of 0.8919 (95% CI: 0.8799 - 0.9040)
- Model demonstrates robustness to missing data with consistent performance across feature subsets
- Text Template outperforms List Template and GPT Generation Template in all metrics
- MDD-LLM significantly outperforms traditional machine learning and deep learning approaches

## Why This Works (Mechanism)
The approach works by leveraging LLMs' natural language reasoning capabilities through careful prompt engineering and parameter-efficient fine-tuning. By converting structured medical data into natural language prompts, the model can apply its pre-trained reasoning patterns to clinical features. LoRA/QLoRA adaptation allows the model to learn MDD-specific patterns without full fine-tuning, preserving the general reasoning capabilities while specializing for the diagnostic task. The textual format enables the model to generate both predictions and explanations, making the output interpretable for clinical use.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) with LoRA/QLoRA**
  - **Why needed here:** Fine-tuning a 70B parameter model (MDD-LLM) on a clinical dataset is computationally expensive. PEFT techniques allow adaptation of the model to the MDD task without retraining all parameters, making the project feasible on limited hardware (e.g., 4 H100 GPUs).
  - **Quick check question:** How does Low-Rank Adaptation (LoRA) reduce the memory footprint compared to full fine-tuning? (A: It freezes the pre-trained model weights and injects trainable low-rank decomposition matrices into the transformer layers, drastically reducing the number of trainable parameters.)

- **Concept: Structured Data to Prompt Transformation**
  - **Why needed here:** The foundational model (Llama 3.1) processes text, not database tables. The method's performance hinges on the "Text Template" or "GPT Generation Template" accurately encoding patient features (Age, BMI, etc.) into a natural language prompt the model can reason over.
  - **Quick check question:** Why might a simple "List Template" (e.g., "Age is 60, BMI is 24.5...") perform worse than a "Text Template"? (A: A simple list may lack semantic richness or contextual cues that the model relies on for reasoning, whereas a more narrative text template might better align with the language patterns seen during the LLM's pre-training.)

- **Concept: Imbalanced Classification in Medical Data**
  - **Why needed here:** The dataset is highly imbalanced (12,715 MDD cases vs. 261,633 controls). An unmodified model may achieve high accuracy by predicting only the majority class (healthy). The reported AUC of 0.8919 is a key metric, but understanding how class imbalance is managed is critical for clinical validity.
  - **Quick check question:** The model achieves 83.8% accuracy, but why is the F1 Score (0.8184) a more trustworthy metric for evaluating performance on the minority MDD class? (A: F1 Score is the harmonic mean of precision and recall, providing a better measure of the model's performance on the positive (MDD) class specifically, which is the critical clinical finding.)

## Architecture Onboarding

- **Component Map:** UK Biobank Data -> Tabular to Text Transformation -> Llama 3.1 Model with LoRA Adapters -> Yes/No Prediction + Probability + Explanation

- **Critical Path:**
  1. **Prompt Engineering:** Correct implementation of the `Text Template` is paramount. Garbled or ambiguous prompts will degrade model performance.
  2. **LoRA Configuration:** The rank (r=8) and alpha (Î±=16) must be correctly configured. A mistake here (e.g., too low rank) could under-capitalize on the clinical data.
  3. **Training Loop:** Ensure the loss is calculated correctly on the generated "Yes/No" token and the associated probability for proper gradient updates.

- **Design Tradeoffs:**
  - **Model Size (8B vs. 70B):** The 70B model has superior accuracy (83.8% vs 79.0%) and AUC, but the 8B model is much faster and requires less memory. The choice depends on the deployment environment's latency constraints.
  - **Fine-Tuning Method (LoRA vs. QLoRA):** LoRA is faster (40min) but uses more memory (245GB). QLoRA is slower (55min) but uses less memory (172GB) via quantization.
  - **Prompt Type:** "GPT Generation Template" is costly (API calls). "Text Template" offers a good balance of performance and cost.

- **Failure Signatures:**
  1. **Low AUC but High Accuracy:** A classic sign of failure on the imbalanced dataset, where the model is predicting "No" (healthy) for almost everyone.
  2. **Performance Drop on Missing Data:** If the model fails to generalize when a key feature (e.g., 'Sleep Duration') is missing, it indicates a lack of robustness.
  3. **Hallucinated Explanations:** The model provides a prediction ("Yes") with an explanation ("...because the patient is 25") but the patient's actual age in the prompt was 60. This is a critical failure mode for clinical trust.

- **First 3 Experiments:**
  1. **Baseline Validation:** Run the pre-trained Llama 3.1 8B model *without* fine-tuning on a small sample of transformed prompts. Expect near-random or poor performance, confirming the need for fine-tuning.
  2. **Ablation on Prompt Design:** Train three separate 8B models using the List, Text, and GPT Generation templates. Compare their F1 scores on a held-out validation set to validate the authors' finding that the Text Template is optimal.
  3. **Robustness Check:** Take the best-performing fine-tuned model and systematically mask random features in the test set (e.g., remove 20%, 40%, 60% of features). Plot the degradation in accuracy against the authors' reported robustness curve.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent would replacing the general-purpose Llama 3.1 backbone with a domain-specific medical LLM improve MDD diagnostic performance?
- **Open Question 2:** What specific mechanisms can effectively mitigate the risk of hallucinations in LLM-based MDD diagnoses to ensure clinical safety?
- **Open Question 3:** How do other parameter-efficient fine-tuning techniques (PEFT), such as Prefix Tuning or Adapter Tuning, compare to LoRA and QLoRA for this specific clinical application?
- **Open Question 4:** Can the MDD-LLM framework generalize its high diagnostic performance to external, multi-center cohorts with different demographic profiles?

## Limitations
- Clinical validity concerns due to potential demographic bias in UK Biobank cohort
- Unclear handling of missing data in prompt templates with 70% missing self-harm history
- Explanation quality unverified - could be post-hoc rationalizations rather than genuine reasoning
- No external validation on independent clinical populations

## Confidence
- **High Confidence**: Technical implementation of LoRA fine-tuning on Llama 3.1 is sound and reproducible
- **Medium Confidence**: Reported performance metrics are likely accurate for UK Biobank dataset
- **Low Confidence**: Claims about clinical utility and diagnostic superiority over established methods are not sufficiently supported

## Next Checks
1. **External Validation Study**: Apply the trained model to an independent clinical dataset from a different population (e.g., US-based medical records or Asian population cohort) to assess generalizability and real-world performance.
2. **Ablation Study on Missing Data**: Systematically remove different combinations of features (including those with high missing rates) from the test set to quantify the model's actual robustness and identify which features are most critical for accurate predictions.
3. **Explanation Quality Assessment**: Conduct a blinded study where psychiatrists evaluate the model's explanations against actual clinical reasoning. Compare agreement rates between model explanations and expert diagnoses to assess whether the explanations are clinically meaningful or merely plausible-sounding text.