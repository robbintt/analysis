---
ver: rpa2
title: Adaptable Symbolic Music Infilling with MIDI-RWKV
arxiv_id: '2506.13001'
source_url: https://arxiv.org/abs/2506.13001
tags:
- state
- music
- infilling
- each
- midi-rwkv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIDI-RWKV is a 35M-parameter symbolic music infilling model based
  on the RWKV-7 architecture, designed for controllable and adaptable computer-assisted
  composition. It uses single-section infilling with three attribute controls (note
  density, note duration, polyphony) and achieves state-of-the-art performance on
  multi-track music infilling tasks.
---

# Adaptable Symbolic Music Infilling with MIDI-RWKV

## Quick Facts
- arXiv ID: 2506.13001
- Source URL: https://arxiv.org/abs/2506.13001
- Reference count: 40
- Key outcome: MIDI-RWKV achieves state-of-the-art performance on multi-track symbolic music infilling with controllable attributes and effective style adaptation through state tuning in low-sample regimes.

## Executive Summary
MIDI-RWKV is a 35M-parameter symbolic music infilling model based on the RWKV-7 architecture, designed for controllable and adaptable computer-assisted composition. It uses single-section infilling with three attribute controls (note density, note duration, polyphony) and achieves state-of-the-art performance on multi-track music infilling tasks. The model can operate on arbitrarily long sequences and supports effective style adaptation through state tuning in the low-sample regime. Objective evaluations show MIDI-RWKV outperforms or matches comparable models across multiple metrics including content preservation, groove similarity, and pitch class histogram entropy difference for 4-bar infilling. Subjective listening tests demonstrate state-tuned models significantly improve over base and LoRA models (p<0.05). The model is available open-source with weights and code.

## Method Summary
MIDI-RWKV uses the RWKV-7 architecture with 12 layers, head size 64, and hidden dimension 384 (35M parameters total). The model is trained on GigaMIDI using single-section infilling, where masked content is moved to the sequence end. Three attribute controls per bar allow users to adjust note density (1-18+ notes), note duration (5 binary tokens), and polyphony (min/max notes). Style adaptation is achieved through state tuning, which optimizes only the initial hidden state vectors while freezing model weights. The model uses REMI+ tokenization with BPE encoding (16,000 vocab size) and achieves infinite context length through RWKV's linear attention mechanism.

## Key Results
- Achieves state-of-the-art performance on multi-track music infilling tasks with objective metrics: content preservation (0.605±0.243), groove similarity (0.939±0.069), and pitch class histogram entropy difference (0.405±0.399) for 4-bar infilling
- State-tuned models significantly outperform base and LoRA models in subjective listening tests (p<0.05) with only 294K parameters for adaptation
- Demonstrates ability to process sequences exceeding 8192 tokens during inference, showcasing RWKV's infinite context capability
- Outperforms or matches comparable models including MIDI-GPT, Bar Fill, and Composer's Assistant across multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Single-Section Infilling Objective
Simplifying the training objective to contiguous masked regions improves model performance compared to training on arbitrary sparse masking patterns. The authors argue that the space of arbitrary masking patterns increases exponentially with context length. By constraining training to single contiguous sections, the model learns a lower-dimensional mapping that aligns more closely with the structured requests typical in computer-assisted composition, reducing the complexity of the learned task.

### Mechanism 2: State Tuning for Style Adaptation
Fine-tuning only the initial hidden state vectors (state tuning) adapts the model to specific styles more effectively than LoRA in low-data regimes. By optimizing the initial state vectors while freezing model weights, the method biases the trajectory of the hidden state through the state space. The authors hypothesize this shifts the model's operation to a subspace corresponding to the target style, extracting information already stored in the pre-trained weights rather than learning new features.

### Mechanism 3: Linear Attention for Infinite Context
The RWKV-7 architecture allows the model to process arbitrarily long sequences during inference, unlike standard Transformers bounded by positional embeddings. RWKV-7 replaces quadratic softmax attention with a linear attention variant derived from RNNs. It maintains a fixed-size state vector that carries historical information, allowing the model to ingest context up to physical memory limits without the computational explosion of standard attention.

## Foundational Learning

- **Concept: REMI+ Tokenization**
  - **Why needed here:** The paper relies entirely on this representation (Bar, Position, Pitch, Duration, Velocity, Track/Program info) to convert polyphonic, multi-track MIDI into a linear sequence.
  - **Quick check question:** Can you explain how REMI+ handles multiple tracks compared to the standard REMI (which is single track)?

- **Concept: Infilling vs. Continuation**
  - **Why needed here:** The core value proposition is "computer-assisted composition" (infilling/editing) rather than "end-to-end generation" (continuation).
  - **Quick check question:** What specific token arrangement (Figure 2) allows a decoder-only model to perform infilling rather than just predicting the next token?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** This is the primary baseline against which "State Tuning" is compared. Understanding LoRA is necessary to interpret the claimed efficiency gains of the proposed method.
  - **Quick check question:** Why would updating only the initial state vector involve fewer parameters than updating low-rank decomposition matrices in attention layers?

## Architecture Onboarding

- **Component map:** MIDI file → MidiTok (REMI+ & BPE) → Token Sequence → RWKV-7 (12 layers, 384 hidden dim) → Control tokens injection → Single-section infilling objective → State Tuning adaptation
- **Critical path:**
  1. Data Prep: Implement the specific "Single-Section" masking logic where masked content is appended to the end of the sequence
  2. Training: Train base model on GigaMIDI using RWKV-LM library
  3. State Tuning: Implement the gradient update loop for initial states using RWKV-PEFT (or custom loop) with high learning rate (5e-2)
  4. Inference: Use rwkv.cpp with a custom sampling loop that injects control tokens dynamically
- **Design tradeoffs:**
  - Single-Section vs. Arbitrary Masking: Simplified training vs. increased inference calls for complex edits
  - RWKV vs. Transformer: Infinite context/inference speed vs. potential theoretical limitations of fixed-state expressiveness
  - State Tuning vs. LoRA: Extreme parameter efficiency (294k params) vs. inability to merge multiple adapters
- **Failure signatures:**
  - Mode Collapse: Generation of consecutive Bar_None tokens (empty measures)
  - State Tuning Instability: The paper uses a very high learning rate (5e-2) for state tuning; standard LR schedules might fail to shift the state sufficiently
  - Attribute Conflict: Rapidly switching attribute controls may be ignored to maintain musical coherence
- **First 3 experiments:**
  1. Sanity Check (Infilling Logic): Reproduce the "Single-Section" data preparation pipeline. Verify that masking bars 4-8 of a 16-bar track results in the correct token sequence
  2. State Tuning vs. Zero-State: Take the provided pre-trained weights. Optimize the initial state on a small set (e.g., 5 songs of a specific genre). Visualize the Frobenius distance between the tuned state and zero state
  3. Long-Context Stress Test: Feed the model a sequence exceeding 4096 tokens (the training length). Compare the content preservation (CP) metrics against a baseline Transformer that must chunk this input

## Open Questions the Paper Calls Out

- **Can truly arbitrary masking patterns over arbitrary contexts be achieved efficiently, without constraining the masking space at training or inference time?**
  - Basis: Appendix A states "the goal of truly arbitrary masking patterns over arbitrary contexts, which remains an open problem"
  - Why unresolved: Current approaches either limit masking patterns at inference or require saturating the exponentially growing space of masking patterns during training

- **Does state tuning's effectiveness generalize to highly experimental or unusual compositional styles that deviate significantly from pretraining data distributions?**
  - Basis: Limitations section states state tuning "may perform poorly on highly experimental or unusual compositional styles that deviate significantly from the distribution of the pretraining data"
  - Why unresolved: The rationale suggests state tuning extracts information already stored in model weights rather than teaching new information

- **Can token streaming and state caching be integrated into RWKV-based music models to enable real-time generative applications?**
  - Basis: Limitations section notes "The system cannot yet compose in real time due to inference latency and a lack of ability to 'stream' tokens"
  - Why unresolved: The current implementation does not reuse prefix states across inference calls, and the RWKV ecosystem lacks mature streaming infrastructure

## Limitations

- **Single-section constraint:** The model requires multiple inference calls for workflows needing simultaneous regeneration of non-contiguous bars, potentially increasing latency and disrupting musical coherence
- **Objective metric validity:** Heavy reliance on standard MIR metrics with limited correlation to perceived musical quality; small subjective test sample size (n=11)
- **Long-context generalization:** While RWKV-7 claims infinite context, there is no evidence that extremely long sequences maintain coherence, and the fixed-size state vector may still suffer from saturation

## Confidence

- **Base Model Performance (SOTA on multi-track infilling):** Medium - Strong objective metrics, but small subjective test sample size
- **State Tuning Superiority over LoRA:** High - Statistically significant subjective results (p<0.05) and objective metrics in low-sample regime
- **RWKV-7 Linear Attention for Infinite Context:** Medium - Demonstrated extrapolation, but no evidence for extreme lengths; theoretical limitations not fully addressed

## Next Checks

1. **Long-Context Coherence Test:** Feed the model a sequence exceeding 8192 tokens (double the training length) and evaluate Content Preservation and Groove Similarity against a chunked baseline (e.g., MIDI-GPT). This directly tests the claimed infinite context capability.

2. **Non-Contiguous Infilling Stress Test:** Design a test case requiring simultaneous regeneration of non-adjacent bars (e.g., bars 2, 5, and 8 of a 16-bar track). Compare the quality and coherence of MIDI-RWKV's multi-call approach against a baseline that natively supports arbitrary masking. This validates the single-section limitation.

3. **Style Transfer Boundary Test:** Take a pre-trained MIDI-RWKV model and attempt state tuning on a style completely out of distribution from GigaMIDI (e.g., a highly experimental microtonal genre). If the model fails to adapt, it confirms the assumption that the target style must be represented in the pre-trained weights.