---
ver: rpa2
title: Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling
  Based on Large Language Models
arxiv_id: '2503.13813'
source_url: https://arxiv.org/abs/2503.13813
tags:
- milp
- llms
- scheduling
- local
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a knowledge-augmented automated MILP formulation
  framework to address the challenge of multi-robot task allocation and scheduling
  in intelligent manufacturing systems while maintaining data privacy. The framework
  employs a local DeepSeek-R1-Distill-Qwen-32B model with domain-specific knowledge
  bases to extract spatiotemporal constraints with 82% accuracy, and a supervised
  fine-tuned Qwen2.5-Coder-7B-Instruct model for MILP code generation with 90% accuracy.
---

# Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models

## Quick Facts
- arXiv ID: 2503.13813
- Source URL: https://arxiv.org/abs/2503.13813
- Reference count: 23
- Knowledge-augmented automated MILP formulation framework for multi-robot task allocation and scheduling in intelligent manufacturing systems

## Executive Summary
This paper proposes a two-phase framework that leverages large language models to automatically construct Mixed-Integer Linear Programming (MILP) models for multi-robot task allocation and scheduling in manufacturing environments. The system addresses the critical challenge of maintaining data privacy while enabling non-experts to formulate complex scheduling problems. By combining a knowledge-augmented formulation phase with supervised fine-tuning for code generation, the framework achieves 82% accuracy in constraint extraction and 90% accuracy in generating executable Gurobi code, demonstrating successful application to aircraft skin manufacturing scenarios.

## Method Summary
The framework employs a two-phase approach to convert natural language task descriptions into executable scheduling solutions. Phase 1 uses a local DeepSeek-R1-Distill-Qwen-32B model augmented with a domain-specific knowledge base (FastGPT) to extract spatiotemporal constraints from natural language descriptions. Phase 2 employs a supervised fine-tuned Qwen2.5-Coder-7B-Instruct model to generate Python code compatible with the Gurobi solver. The knowledge base contains verified FJSP model formulations, while the SFT dataset consists of 494 verified instruction-output pairs covering six constraint types. The entire system runs on local hardware (4× RTX 4090D) to ensure data privacy in enterprise environments.

## Key Results
- 82% accuracy in spatiotemporal constraint extraction using knowledge-augmented formulation
- 90% accuracy in MILP code generation after supervised fine-tuning
- Successful automatic modeling for aircraft skin manufacturing cases
- Achieves data privacy compliance while maintaining computational efficiency

## Why This Works (Mechanism)
The framework works by addressing two fundamental challenges in automated MILP formulation: constraint extraction from natural language and reliable code generation. The knowledge base in Phase 1 provides domain-specific templates and formulations that ground the LLM's reasoning, dramatically improving constraint extraction accuracy from ~28% to 82%. The supervised fine-tuning in Phase 2 transforms a general-purpose code model into a specialist by training on verified MILP-to-code examples, achieving 90% accuracy compared to ~5% without fine-tuning. The two-phase design creates a clear separation between reasoning (formulation) and execution (coding), allowing each component to be optimized independently while maintaining data privacy through local model deployment.

## Foundational Learning
- **Concept:** Mixed-Integer Linear Programming (MILP) for Scheduling
  - **Why needed here:** This is the core mathematical representation the entire system aims to construct. Understanding how real-world constraints (e.g., a robot can only do one task at a time, a task must happen within a time window) are translated into linear equations with integer and continuous variables is essential to comprehend the system's output.
  - **Quick check question:** Can you formulate a simple constraint like "Task A must finish before Task B starts" using binary/integer variables and linear inequalities?

- **Concept:** Retrieval-Augmented Generation (RAG) / Knowledge Augmentation
  - **Why needed here:** The paper's first mechanism relies on a "knowledge base" to ground the LLM's reasoning. This concept explains how the LLM is provided with external, domain-specific information to improve its constraint extraction accuracy from 28% (without knowledge) to 82%.
  - **Quick check question:** What is the primary role of the "knowledge base" in the framework's first phase—is it to provide general knowledge or domain-specific constraint templates?

- **Concept:** Supervised Fine-Tuning (SFT)
  - **Why needed here:** This is the key technique used to transform a general-purpose code model into a specialist for MILP code generation. It explains the dramatic performance leap from ~5% to 90% accuracy in the second phase of the architecture.
  - **Quick check question:** According to the paper, what is the critical component of the SFT dataset construction pipeline that ensures the model learns from correct examples?

## Architecture Onboarding

- **Component map:** Phase 1 (Formulation): Natural Language → DeepSeek-R1-Distill-Qwen-32B + FastGPT Knowledge Base → MILP Model Vector. Phase 2 (Code Generation): MILP Model Vector → Qwen2.5-Coder-7B-Instruct (SFT) → Python Code → Gurobi Solver.

- **Critical path:** The most fragile part of the system is the handoff between Phase 1 and Phase 2. The paper reports 82% accuracy for constraint extraction in Phase 1. This means roughly 1 in 5 constraints may be missed or formulated incorrectly. The second phase, with 90% code generation accuracy, will still execute, but on a potentially incomplete or incorrect model, leading to a plausible but suboptimal or invalid schedule.

- **Design tradeoffs:** The primary tradeoff is between capability and deployability. The authors use smaller, local models (32B, 7B parameters) instead of more powerful cloud-based models (like GPT-4) to meet enterprise data privacy requirements. This necessitates compensating mechanisms like a knowledge base and SFT to achieve acceptable performance. Another tradeoff is specialization: the SFT dataset is built for FJSP-type problems, making the code generator highly accurate for that domain but potentially less general-purpose.

- **Failure signatures:**
  - Phase 1 Failure: The generated schedule will be syntactically correct and executable but will violate unstated constraints. For instance, a time-window constraint for adhesive application might be missed, leading to a structurally unsound product.
  - Phase 2 Failure: The generated Python code will fail to run or throw an error from the Gurobi solver. This is easier to detect. The paper notes lower accuracy (70%) for more complex constraints like "maximum time-interval constraints," indicating a likely failure point.
  - Systemic Failure: The two-phase nature can mask errors. If the final schedule is wrong, it's unclear if the error was in the extraction of the constraint (Phase 1) or the coding of the constraint (Phase 2).

- **First 3 experiments:**
  1. Validate Phase 1 in Isolation: Provide the formulation model with natural language descriptions of scenarios with known, single constraints (e.g., only a deadline). Manually inspect the generated MILP model to see if the constraint was correctly translated, bypassing the code generator.
  2. Validate Phase 2 in Isolation: Provide the SFT code generator model with pre-written, correct MILP formulations for various constraints. Verify if the generated Python code is both syntactically correct and passes a Gurobi syntax check, without running a full solve.
  3. End-to-End Test on a Known Scenario: Run the full pipeline on a simplified, classic scheduling problem (like a small Job Shop Scheduling instance) with a known optimal solution. Compare the output of the generated Gurobi code against the known optimal makespan.

## Open Questions the Paper Calls Out
None

## Limitations
- The 82% accuracy for constraint extraction means approximately 18% of constraints may be missed or incorrectly formulated, potentially leading to invalid schedules in real-world applications.
- The system's performance is heavily dependent on the quality and comprehensiveness of the knowledge base, which may not generalize well to novel or highly specialized manufacturing scenarios.
- The computational overhead of deploying two separate LLM models may impact real-time applicability in dynamic manufacturing environments.

## Confidence

- **High Confidence:** The two-phase architecture design and the general approach of using local models with knowledge augmentation for data privacy are well-founded and technically sound. The use of established tools like Gurobi for optimization validation is reliable.
- **Medium Confidence:** The reported accuracy metrics (82% for constraint extraction, 90% for code generation) are based on the specific dataset and evaluation methodology described. However, the paper lacks detailed information about the evaluation benchmarks and cross-validation procedures, making it difficult to assess generalizability.
- **Low Confidence:** The practical effectiveness in diverse, real-world manufacturing scenarios remains uncertain due to limited testing on varied case studies beyond aircraft skin manufacturing. The system's robustness to noisy or ambiguous natural language inputs is not thoroughly evaluated.

## Next Checks

1. **End-to-End Stress Testing:** Apply the framework to five diverse FJSP benchmark problems with known optimal solutions, measuring both accuracy of constraint extraction and final makespan performance against traditional MILP solvers.

2. **Knowledge Base Generalization Test:** Systematically remove or modify constraint types from the knowledge base and measure degradation in formulation accuracy to identify critical knowledge gaps and assess robustness.

3. **Cross-Scenario Validation:** Deploy the trained models on manufacturing scenarios from different industries (e.g., automotive assembly, electronics manufacturing) without retraining to evaluate domain transferability and identify potential overfitting to aircraft manufacturing contexts.