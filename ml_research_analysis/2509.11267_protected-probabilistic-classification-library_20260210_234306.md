---
ver: rpa2
title: Protected Probabilistic Classification Library
arxiv_id: '2509.11267'
source_url: https://arxiv.org/abs/2509.11267
tags:
- calibration
- protected
- classification
- base
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Python package for protected probabilistic
  classification, designed to improve calibration under dataset shift. The method
  extends earlier work by applying online calibration to adapt to distribution changes
  between training and test data.
---

# Protected Probabilistic Classification Library

## Quick Facts
- arXiv ID: 2509.11267
- Source URL: https://arxiv.org/abs/2509.11267
- Authors: Ivan Petej
- Reference count: 0
- Introduces Python package for protected probabilistic classification with improved calibration under dataset shift

## Executive Summary
This paper introduces a Python package for protected probabilistic classification that addresses the challenge of maintaining calibration under dataset shift. The method extends existing online calibration techniques by implementing a composite martingale framework with Cox calibration functions that dynamically adjust probabilistic outputs during inference. The approach is designed to handle various types of dataset shift including concept shift, feature shift, and label shift scenarios.

Extensive experiments across synthetic, MNIST, and real-world datasets demonstrate that protected classification consistently outperforms standard and post-hoc calibrated models in terms of expected calibration error, Brier loss, and log loss. When compared to recent online calibration approaches like Online Platt Scaling with Calibeating, the method achieves comparable or superior calibration error while providing additional benefits in streaming applications.

## Method Summary
The protected probabilistic classification method implements an online calibration framework that adapts to distribution changes between training and test data. It uses a composite martingale approach with Cox calibration functions to dynamically adjust probabilistic outputs during inference. The method continuously updates calibration parameters based on observed prediction errors, allowing it to maintain accuracy even as the data distribution shifts over time. This online approach differs from traditional post-hoc calibration methods by updating calibration parameters in real-time as new data arrives, making it particularly suitable for streaming applications and non-stationary environments.

## Key Results
- Achieves consistent improvements in expected calibration error across multiple dataset shift scenarios
- Outperforms standard and post-hoc calibrated models in Brier loss and log loss metrics
- Demonstrates superior or comparable performance to Online Platt Scaling with Calibeating under concept, feature, and label shift
- Reduces log loss while maintaining calibration in streaming algorithm applications

## Why This Works (Mechanism)
The method works by maintaining calibration under distribution shift through continuous online adjustment of probability estimates. The composite martingale framework monitors the reliability of predictions in real-time, while the Cox calibration function provides a flexible parametric form for adjusting confidence scores. This combination allows the system to detect and correct miscalibration as soon as it appears, rather than waiting until the end of inference or requiring batch processing.

## Foundational Learning
- **Martingale theory**: Essential for understanding the probabilistic guarantees and convergence properties of the online calibration process. Quick check: Verify that the constructed martingale has bounded increments and satisfies the necessary convergence conditions.
- **Cox proportional hazards model**: Provides the mathematical foundation for the calibration function that adjusts probability estimates. Quick check: Confirm that the hazard ratio assumptions hold for the calibration problem.
- **Online learning algorithms**: Critical for understanding how the method updates parameters incrementally without requiring full retraining. Quick check: Ensure that the regret bounds are acceptable for the application's latency requirements.
- **Calibration metrics**: Expected calibration error, Brier score, and log loss are the primary evaluation metrics. Quick check: Verify that the metric calculations are implemented correctly and consistently across experiments.
- **Dataset shift types**: Concept shift, feature shift, and label shift represent different ways the data distribution can change. Quick check: Confirm that the evaluation scenarios properly isolate each type of shift.

## Architecture Onboarding

Component map:
Base classifier -> Online calibration module -> Cox calibration function -> Martingale monitor -> Output predictions

Critical path:
The critical path flows from the base classifier through the online calibration module, where the Cox calibration function adjusts probability estimates based on martingale-based monitoring. The system continuously updates calibration parameters in real-time as predictions are made, with the martingale monitor providing feedback for adjustments.

Design tradeoffs:
- Real-time adaptation vs computational overhead: The online nature provides immediate correction but requires more computation per prediction
- Parametric flexibility vs overfitting risk: The Cox function provides good flexibility but may overfit on limited data
- Memory efficiency vs historical information: Limited memory for past observations reduces storage but may miss long-term patterns
- Simplicity vs expressiveness: The current calibration function balances ease of implementation with sufficient modeling power

Failure signatures:
- Degraded performance when calibration data is insufficient or of poor quality
- Potential overfitting to recent data patterns at the expense of long-term stability
- Computational bottlenecks in high-frequency prediction scenarios
- Reduced effectiveness when the underlying distribution changes too rapidly for online adaptation

First experiments:
1. Baseline calibration error comparison on a stationary dataset to verify correct implementation
2. Synthetic concept shift simulation to test adaptation capability
3. Streaming data evaluation with controlled feature drift to assess real-time performance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance under extreme or high-dimensional dataset shift scenarios remains untested
- Limited exploration of computational overhead in real-time streaming applications
- No explicit comparison with deep learning-based calibration methods

## Confidence
- High confidence: Calibration error improvements under controlled shift scenarios
- Medium confidence: Performance claims relative to specific competing methods
- Low confidence: Generalizability to all real-world streaming applications

## Next Checks
1. Evaluate performance on high-dimensional datasets (e.g., CIFAR-10, ImageNet) to test scalability
2. Conduct ablation studies on Cox calibration function hyperparameters to assess sensitivity
3. Test the method's robustness on highly imbalanced datasets with varying class distributions