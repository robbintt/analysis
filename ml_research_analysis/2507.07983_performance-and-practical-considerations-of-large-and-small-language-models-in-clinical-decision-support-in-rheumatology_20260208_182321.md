---
ver: rpa2
title: Performance and Practical Considerations of Large and Small Language Models
  in Clinical Decision Support in Rheumatology
arxiv_id: '2507.07983'
source_url: https://arxiv.org/abs/2507.07983
tags:
- language
- clinical
- rheumatology
- performance
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large and small language models for clinical
  decision support in rheumatology, focusing on diagnostic and therapeutic accuracy.
  Smaller language models (SLMs), particularly Mixtral-8x7b-32768 with retrieval-augmented
  generation (RAG), achieved higher performance than larger models, with F1 scores
  of 72% for diagnosis and 73% for treatment recommendations.
---

# Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology

## Quick Facts
- arXiv ID: 2507.07983
- Source URL: https://arxiv.org/abs/2507.07983
- Reference count: 0
- Smaller language models (SLMs), particularly Mixtral-8x7b-32768 with retrieval-augmented generation (RAG), achieved higher performance than larger models, with F1 scores of 72% for diagnosis and 73% for treatment recommendations.

## Executive Summary
This study evaluates large and small language models for clinical decision support in rheumatology, focusing on diagnostic and therapeutic accuracy. Smaller language models (SLMs), particularly Mixtral-8x7b-32768 with retrieval-augmented generation (RAG), achieved higher performance than larger models, with F1 scores of 72% for diagnosis and 73% for treatment recommendations. RAG integration improved performance across models, though benefits were model-dependent. While promising, no model reached specialist-level accuracy, underscoring the need for expert oversight. SLMs offer advantages in energy efficiency and cost-effective, local deployment, making them attractive for resource-limited healthcare settings.

## Method Summary
The study tested five language models (GPT-4o, Mixtral-8x7b-32768, Nemotron-70b, Qwen-Turbo 2.5, Claude-3.5-Sonnet) under four conditions (with/without RAG, with/without pre-specified diagnosis) on 10 standardized rheumatology patient cases. RAG used FAISS vector store with Sentence-BERT embeddings and FlashRank re-ranking to retrieve clinical guideline passages. Performance was evaluated using F1 scores for diagnosis (F1-Dx) and treatment recommendations (F1-Tx), along with RAGAS scores for retrieval quality.

## Key Results
- Mixtral-8x7b-32768 with RAG achieved highest F1 scores: 72% for diagnosis, 73% for treatment recommendations
- RAG integration improved performance across all models, with benefits varying by model architecture
- Nemotron achieved high diagnostic F1 (71%) without RAG, indicating strong internal knowledge representation
- No model reached specialist-level accuracy, highlighting need for expert oversight in clinical applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) improves clinical decision support by dynamically injecting current, domain-specific knowledge into language model outputs.
- Mechanism: RAG indexes clinical guidelines (e.g., EULAR, ACR) in a vector database, retrieves relevant passages via semantic similarity, and provides them as context during generation—reducing reliance on potentially outdated or incomplete training data.
- Core assumption: The base model can effectively integrate retrieved context into its reasoning process.
- Evidence anchors:
  - [abstract] "Smaller language models (SLMs), particularly Mixtral-8x7b-32768 with retrieval-augmented generation (RAG), achieved higher performance than larger models"
  - [section Page 3] "RAG has been proposed to address language models' limitations such as hallucination phenomena and data constraints by enabling language models to dynamically incorporate current external knowledge"
  - [corpus] MedCoT-RAG paper confirms RAG "offers a practical and privacy-preserving way to enhance LLMs with external knowledge" in medical QA tasks
- Break condition: If retrieved passages are irrelevant, poorly ranked, or exceed context window limits, RAG may degrade performance or introduce noise.

### Mechanism 2
- Claim: Mixture-of-Experts (MoE) architectures, such as Mixtral-8x7b, leverage external knowledge more effectively than dense architectures in RAG configurations.
- Mechanism: MoE models route inputs through specialized expert sub-networks, potentially allowing more nuanced integration of domain-specific retrieved content.
- Core assumption: Expert routing aligns with the semantic structure of retrieved clinical guidelines.
- Evidence anchors:
  - [section Page 3] "highlighting the effectiveness of its Mixture-of-Experts architecture in leveraging external knowledge"
  - [section Page 5, Table 1] Mixtral-8x7b-32768 with RAG achieved highest F1-Dx (72%) and F1-Tx (73%) among all tested models
  - [corpus] Weak direct evidence—no corpus papers specifically compare MoE vs. dense architectures in medical RAG
- Break condition: If expert routing fails to activate appropriately for clinical reasoning tasks, MoE advantages may not materialize.

### Mechanism 3
- Claim: RAG utility is model-dependent; models with stronger internal knowledge representations benefit less from retrieval augmentation.
- Mechanism: Models already encoding substantial domain knowledge may show marginal RAG gains, while models with weaker internal representations derive larger improvements from external context.
- Core assumption: RAG effectiveness correlates inversely with base model's pre-existing domain knowledge density.
- Evidence anchors:
  - [section Page 3-4] "Nemotron achieved a high diagnostic F1 score (71%) even without RAG, indicating strong internal knowledge representation"
  - [section Page 4] "Claude-3.5-Sonnet achieved a high RAGAS score without RAG (80%), suggesting a robust baseline performance that is less dependent on RAG integration"
  - [corpus] No corpus papers directly address model-dependent RAG utility variation
- Break condition: If retrieval quality is inconsistent across queries, even knowledge-weak models may not benefit reliably.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture component that enabled SLMs to outperform LLMs; requires understanding of retrieval, re-ranking, and context injection.
  - Quick check question: Can you explain why RAG might reduce hallucinations compared to vanilla LLM generation?

- Concept: **F1 Score and RAGAS Metrics**
  - Why needed here: Study's primary evaluation metrics; F1 measures precision-recall balance for diagnosis/treatment, RAGAS evaluates retrieval quality and faithfulness.
  - Quick check question: What does an F1 score of 72% tell you about the trade-off between false positives and false negatives in diagnostic suggestions?

- Concept: **Vector Embeddings and Similarity Search**
  - Why needed here: RAG pipeline uses Sentence-BERT embeddings and FAISS vector store for semantic retrieval of clinical guidelines.
  - Quick check question: Why would semantic similarity search outperform keyword matching for retrieving clinical guideline passages?

## Architecture Onboarding

- Component map:
  - **Input Layer**: Anonymized patient case data (diagnoses, medications, lab values)
  - **Retrieval System**: FAISS vector store (IndexFlatL2) → Sentence-BERT embeddings (768-dim) → FlashRank re-ranking
  - **Knowledge Base**: Clinical guidelines (EULAR, ACR, SIGN) indexed as vector segments
  - **Model Layer**: Five tested models (Mixtral-8x7b, GPT-4o, Nemotron-70b, Qwen-Turbo, Claude-3.5-Sonnet)
  - **Evaluation**: F1 scoring against guideline ground truths; RAGAS for response quality

- Critical path: Patient case → Prompt engineering (few-shot + chain-of-thought) → RAG retrieval → Context-augmented generation → Diagnosis/Treatment output → Metric evaluation

- Design tradeoffs:
  - SLMs offer local deployment and energy efficiency but require RAG to approach LLM performance
  - RAG adds latency and infrastructure complexity but reduces hallucination risk
  - Pre-specified diagnosis improves performance but reduces real-world applicability

- Failure signatures:
  - High RAGAS score with low F1: Model generates fluent but clinically incorrect recommendations
  - High variability across configurations (e.g., Nemotron 51%-67% range): Model sensitive to prompting/retrieval quality
  - Minor metric differences masking clinically dangerous errors (contraindications, red flags overlooked)

- First 3 experiments:
  1. Replicate Mixtral-8x7b + RAG pipeline on a held-out set of rheumatology cases to validate F1-Dx ~72% finding
  2. Ablate the FlashRank re-ranking component to measure retrieval quality impact on final F1 scores
  3. Test retrieval quality thresholds: systematically degrade retrieved context relevance to identify minimum viable retrieval performance for clinical safety

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SLMs with RAG match the diagnostic and triage performance of general practitioners or non-rheumatologist physicians in complex clinical cases?
- Basis in paper: [explicit] The authors state that "Future research should include comparisons with general practitioners and physicians from other domains... to better assess real-world applicability."
- Why unresolved: This study only benchmarked model outputs against evidence-based guideline ground truths, not against the clinical reasoning of human physicians.
- What evidence would resolve it: A comparative study measuring the concordance of SLM outputs with GP decisions on identical, complex patient cases.

### Open Question 2
- Question: Do the reported performance advantages of Mixtral-8x7b with RAG generalize to larger, multi-center clinical datasets?
- Basis in paper: [explicit] The authors identify the "small number of representative test cases" as a limitation and explicitly call for "clinical validation at scale" in future efforts.
- Why unresolved: The findings rely on only ten standardized patient cases derived from a single tertiary clinic in Germany.
- What evidence would resolve it: Validation results using a significantly larger dataset (e.g., hundreds of cases) sourced from diverse healthcare settings.

### Open Question 3
- Question: To what extent do high quantitative F1 scores mask clinically dangerous errors, such as overlooking contraindications or "red-flag" symptoms?
- Basis in paper: [inferred] The paper warns that "minor variations in quantitative metrics such as F1 or RAGAS may mask clinically relevant or potentially harmful errors" like missed contraindications.
- Why unresolved: The evaluation relied on F1 and RAGAS scores, which may not sufficiently penalize rare but dangerous hallucinations or omissions.
- What evidence would resolve it: A qualitative error analysis by rheumatologists specifically identifying "red-flag" failures in the high-scoring model outputs.

## Limitations

- Evaluation based on only 10 standardized patient cases, limiting generalizability to complex real-world clinical scenarios
- Requirement to pre-specify diagnoses before treatment recommendations reduces ecological validity
- No comparison with human expert performance for treatment recommendations, making it difficult to assess clinical acceptability
- Model-dependent RAG benefits observed but not fully explained, with no direct comparison of MoE vs. dense architectures

## Confidence

**High Confidence:** The finding that Mixtral-8x7b-32768 with RAG achieved the highest F1 scores (72% for diagnosis, 73% for treatment) is well-supported by the experimental results and consistent with the observed pattern across all tested models.

**Medium Confidence:** The assertion that SLMs offer advantages in energy efficiency and cost-effective local deployment is plausible but not directly measured in this study.

**Low Confidence:** The specific attribution of Mixtral-8x7b's superior performance to its Mixture-of-Experts architecture is weakly supported, as the study provides no direct comparison between MoE and dense architectures in this context.

## Next Checks

1. **External Validation on Independent Cases:** Replicate the Mixtral-8x7b + RAG pipeline on a held-out set of at least 50 rheumatology cases from different clinical settings to verify whether the F1-Dx ~72% finding holds across broader case diversity and complexity.

2. **Ablation of RAG Components:** Systematically remove FlashRank re-ranking and vary the retrieval relevance threshold to quantify the minimum viable retrieval quality required for clinically safe performance, identifying the point at which RAG integration ceases to provide net benefit.

3. **Human Expert Benchmarking:** Conduct a head-to-head comparison where board-certified rheumatologists evaluate the same patient cases and their recommendations are scored using the same F1 methodology, establishing whether any model approaches or exceeds human-level accuracy in either diagnosis or treatment recommendation.