---
ver: rpa2
title: Exploiting Primacy Effect To Improve Large Language Models
arxiv_id: '2507.13949'
source_url: https://arxiv.org/abs/2507.13949
tags:
- label
- bias
- options
- primacy
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses positional bias in Large Language Models (LLMs),
  particularly the primacy effect in multiple-choice question answering (MCQA). The
  authors demonstrate that fine-tuned LLMs exhibit stronger primacy bias than pre-trained
  versions, likely due to exposure to human-like patterns during fine-tuning.
---

# Exploiting Primacy Effect To Improve Large Language Models

## Quick Facts
- arXiv ID: 2507.13949
- Source URL: https://arxiv.org/abs/2507.13949
- Reference count: 4
- Primary result: Training-free method using semantic similarity reordering improves MCQ accuracy by exploiting LLM primacy bias

## Executive Summary
This study addresses positional bias in Large Language Models (LLMs), particularly the primacy effect in multiple-choice question answering (MCQA). The authors demonstrate that fine-tuned LLMs exhibit stronger primacy bias than pre-trained versions, likely due to exposure to human-like patterns during fine-tuning. They propose a training-free method that reorders answer options based on semantic similarity to the query using token-averaged cosine similarity, leveraging this bias to improve accuracy. Experimental results show significant performance gains across different models (Llama2, Llama3, Mistral) and datasets (CLINC, BANKING, HWU). The approach places the correct label in top positions more frequently, with models like Mistral-7B achieving up to 42% top-10 coverage on CLINC. The method is robust across architectures and demonstrates that positional biases can be strategically exploited rather than mitigated.

## Method Summary
The authors propose a training-free approach to improve LLM performance on multiple-choice questions by exploiting the primacy effect. The method reorders answer options based on their semantic similarity to the query, using token-averaged cosine similarity between query and option embeddings. This reordering places the most semantically similar option first, capitalizing on the model's tendency to favor earlier positions. The approach requires no additional training or fine-tuning and can be applied to any pre-existing LLM. The method was tested across three datasets (CLINC, BANKING, HWU) and multiple model architectures (Llama2, Llama3, Mistral), demonstrating consistent improvements in accuracy and correct answer positioning.

## Key Results
- Fine-tuned LLMs exhibit stronger primacy bias than pre-trained versions
- Semantic similarity reordering achieves 30-42% top-10 coverage for correct answers across tested models
- Performance gains are consistent across multiple architectures (Llama2, Llama3, Mistral) and datasets (CLINC, BANKING, HWU)
- The method requires no additional training and can be applied to any existing LLM

## Why This Works (Mechanism)
The primacy effect in LLMs manifests as a tendency to favor earlier positions in sequential inputs. Fine-tuned models develop stronger primacy bias through exposure to human-generated data patterns during training. By reordering answer options based on semantic similarity to the query, the correct answer is more likely to appear in earlier positions where the model's bias is strongest. The token-averaged cosine similarity metric effectively captures semantic relationships between queries and options, ensuring that semantically relevant answers are prioritized. This creates a virtuous cycle where the model's inherent bias is redirected toward better answers rather than arbitrary positional preferences.

## Foundational Learning
- **Primacy effect**: Cognitive bias favoring initial items in sequences; critical for understanding LLM positional preferences
  - Why needed: Explains the core mechanism being exploited
  - Quick check: Observe accuracy drop when correct answers are placed in later positions
- **Cosine similarity**: Metric for measuring semantic similarity between embeddings
  - Why needed: Provides the mathematical foundation for reordering strategy
  - Quick check: Verify that similar semantic content yields high cosine similarity scores
- **Token-averaged embeddings**: Method for creating fixed-size representations from variable-length text
  - Why needed: Enables consistent comparison between queries and options of different lengths
  - Quick check: Confirm that averaged embeddings preserve semantic relationships
- **Fine-tuning effects**: How model training on human data influences behavior patterns
  - Why needed: Explains why fine-tuned models show stronger primacy bias
  - Quick check: Compare primacy bias strength between pre-trained and fine-tuned models
- **MCQA format**: Multiple-choice question answering with explicit answer options
  - Why needed: Defines the problem domain and constraints
  - Quick check: Verify dataset follows standard MCQA formatting conventions
- **Positional bias exploitation**: Strategic use of known model biases to improve performance
  - Why needed: Framework for understanding the approach's novelty
  - Quick check: Test whether alternative positional biases (e.g., recency) can be similarly exploited

## Architecture Onboarding

**Component map**: Query -> Embedding Extraction -> Cosine Similarity Calculation -> Option Reordering -> LLM Inference

**Critical path**: The reordering step directly impacts LLM accuracy by placing correct answers in positions where primacy bias is strongest. The cosine similarity calculation must be efficient to avoid bottlenecks, and the embedding extraction must preserve semantic relationships.

**Design tradeoffs**: The method trades computational overhead (embedding calculations) for accuracy gains without requiring model retraining. It assumes semantic similarity correlates with correctness, which may not hold for all domains. The approach is limited to formats with explicit answer options and may not generalize to open-ended question answering.

**Failure signatures**: Poor performance occurs when answer options have high semantic overlap, when the correct answer is semantically distant from the query, or when the dataset uses non-standard formatting. The method may also fail if the LLM's positional bias is weak or if the embedding space doesn't capture relevant semantic relationships.

**First experiments**: 
1. Measure primacy bias strength in pre-trained vs. fine-tuned models using controlled option ordering
2. Test cosine similarity reordering on datasets with known semantic relationships between queries and answers
3. Evaluate performance degradation when answer options have high semantic similarity

## Open Questions the Paper Calls Out
None

## Limitations
- May not generalize to longer answer options beyond 4-6 tokens
- Performance uncertain when answer options contain highly similar semantic content
- Limited evaluation to datasets with standard sequential formatting conventions

## Confidence
- High: Existence of primacy bias in fine-tuned LLMs and effectiveness of reordering method on tested datasets
- Medium: Claims about semantic similarity being the optimal reordering criterion
- Low: Generalization to domains outside MCQA or non-sequential answer formats

## Next Checks
1. Test the method on datasets with longer answer options (10+ tokens) to evaluate scalability limits
2. Compare semantic similarity reordering against alternative ordering strategies (e.g., lexical ordering, frequency-based ordering)
3. Evaluate performance on datasets with answer options that have high semantic overlap to test method robustness