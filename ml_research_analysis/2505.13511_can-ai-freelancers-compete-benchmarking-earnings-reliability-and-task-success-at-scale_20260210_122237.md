---
ver: rpa2
title: Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success
  at Scale
arxiv_id: '2505.13511'
source_url: https://arxiv.org/abs/2505.13511
tags:
- tasks
- task
- benchmark
- test
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel benchmark for evaluating LLMs as
  freelance software engineers, using synthetic tasks derived from real-world freelance
  job postings. The benchmark includes 1,115 tasks with automated test cases and economic
  value annotations, enabling objective performance measurement.
---

# Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale

## Quick Facts
- **arXiv ID**: 2505.13511
- **Source URL**: https://arxiv.org/abs/2505.13511
- **Reference count**: 14
- **Primary result**: Claude 3.5 Haiku solved 78.7% of tasks, earning $1.52M in simulated value, closely followed by GPT-4o-mini at 77.3% ($1.49M).

## Executive Summary
This paper introduces a novel benchmark for evaluating LLMs as freelance software engineers using 1,115 synthetic tasks derived from real-world freelance job postings. The benchmark features automated test-case evaluation and economic value annotations, enabling objective performance measurement. Four modern LLMs were evaluated: Claude 3.5 Haiku, GPT-4o-mini, Qwen 2.5, and Mistral 7B. Results demonstrate that frontier models can handle well-defined programming tasks at near-human levels, with Claude 3.5 Haiku achieving the highest performance. The study highlights both the potential and limitations of current AI models in practical software development scenarios.

## Method Summary
The benchmark uses 1,115 synthetic programming tasks derived from Freelancer.com job postings, each with structured input-output test cases and estimated prices. A Random Forest regressor (R²=0.65) predicts task prices from skill tags. Models are evaluated using zero-shot prompting with temperature=0, executing generated code in sandboxed environments against test cases. Performance is measured through task success rate, test case accuracy, total earnings (sum of prices for fully-solved tasks), and error distributions. The synthetic task generation process remains partially automated and heuristic-based, while the evaluation framework enables scalable, objective measurement.

## Key Results
- Claude 3.5 Haiku achieved the highest performance with 78.7% task success rate and $1.52M in simulated earnings
- GPT-4o-mini closely followed with 77.3% success and $1.49M earnings
- Qwen 2.5 solved 68.5% of tasks ($1.33M) while Mistral 7B lagged at 42.5% ($0.70M)
- Frontier models typically failed exactly 1 test on unsuccessful tasks, suggesting near-complete capability with room for iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated test-case evaluation enables scalable, objective LLM performance measurement on freelance-style programming tasks.
- Mechanism: Each task includes structured input-output test cases (4 per task average). Model-generated code executes in sandboxed environment; outputs compared against expected results.
- Core assumption: Test cases adequately capture task requirements and correct outputs imply correct solutions.
- Evidence anchors: [abstract] "Each task is accompanied by structured input-output test cases and an estimated price tag, enabling automated correctness checking"

### Mechanism 2
- Claim: Economic value annotations translate technical accuracy into business-relevant metrics.
- Mechanism: Random Forest regressor (R²=0.65) predicts task prices from top 5 skill tags. "Total Value Earned" sums prices of fully-solved tasks.
- Core assumption: Tag-based pricing approximates real market valuation; full payment only on complete task success reflects freelance norms.
- Evidence anchors: [abstract] "economic value annotations, enabling objective performance measurement"

### Mechanism 3
- Claim: Zero-shot, single-turn evaluation isolates raw model capability but may underestimate real-world performance with iteration.
- Mechanism: Each model receives one prompt per task with no few-shot examples, clarifying questions, or retry attempts. Temperature=0 for determinism.
- Core assumption: Single-attempt success rate is a meaningful proxy for practical utility; models cannot self-debug or refine.
- Evidence anchors: [section 5] "We used a zero-shot prompting approach... without any few-shot examples"

## Foundational Learning

- **Unit test-based code evaluation**
  - Why needed here: The entire benchmark depends on understanding how input-output test cases validate code correctness automatically.
  - Quick check question: Given a function that should sort a list, what test cases would catch edge cases like empty input, duplicates, and already-sorted data?

- **Regression-based price prediction**
  - Why needed here: The pricing model uses Random Forest regression to map categorical skill tags to dollar values.
  - Quick check question: If a task has tags ["python", "sql", "machine learning"], would you expect higher or lower predicted price than ["data entry", "excel"]? Why?

- **Benchmark contamination concerns**
  - Why needed here: The paper cites Matton et al. (2024) on LLM training data leakage affecting benchmark reliability.
  - Quick check question: Why might a model perform suspiciously well on benchmark tasks that resemble its training data?

## Architecture Onboarding

- **Component map**: Task Dataset -> Pricing Model -> Evaluation Engine -> Metrics Aggregator
- **Critical path**:
  1. Load task JSON → extract prompt + test cases
  2. Query LLM with zero-shot prompt → capture code output
  3. Execute code against each test case → record pass/fail
  4. If all tests pass: add task price to earnings total
  5. Aggregate metrics across all 1,115 tasks
- **Design tradeoffs**:
  - Synthetic vs. real tasks: Synthetic enables automation but simplifies complexity (no ambiguous requirements, multi-file reasoning)
  - Single-turn vs. iterative: Single-turn ensures fairness but underestimates real capability with debugging
  - All-or-nothing pricing: Mirrors freelance contracts but ignores partial-work value
- **Failure signatures**:
  - Format mismatches: Model produces logically correct output but wrong formatting (units, precision)
  - Timeouts on large inputs: Solution correct on small tests but inefficient on scale
  - Complete failures (4/4 tests wrong): More common in smaller models (Mistral: 6 tasks) vs. frontier models (Claude/GPT: 0 tasks)
- **First 3 experiments**:
  1. Baseline replication: Run all four models on the published benchmark; verify earnings rankings match
  2. Error analysis on near-misses: For tasks where Claude/GPT fail exactly 1 test, identify whether issues are fixable via prompt refinement
  3. Multi-turn augmentation test: Allow models one retry with failing test feedback; measure improvement delta in task success rate

## Open Questions the Paper Calls Out

- **Would iterative self-debugging capabilities close the gap between partial and full task success for frontier models?**
  - Basis in paper: [explicit] Authors note Claude and GPT-4o-mini "typically were very close (usually just one failing test)" and suggest "if an interactive process were allowed" near-100% success might be achievable.
  - Why unresolved: The benchmark used strict single-shot evaluation with no opportunity for models to see test failures and correct them.

- **How much does performance degrade when task specifications include realistic ambiguity rather than fully-specified requirements?**
  - Basis in paper: [explicit] Authors state "real freelance projects involve additional challenges – ambiguity, complex context, creativity – that are not fully captured in our benchmark."
  - Why unresolved: All benchmark tasks were deliberately "self-contained" with "all necessary information" to remove ambiguity for automatic grading.

- **Can smaller open-source models (like Mistral 7B) approach frontier model performance through techniques like chain-of-thought prompting or tool integration?**
  - Basis in paper: [explicit] Authors state "Techniques like chain-of-thought prompting, self-debugging... or tool use (like calling a code compiler) might significantly help models like Mistral and Qwen on this benchmark."
  - Why unresolved: The study evaluated base model capabilities without augmentation techniques.

## Limitations
- Synthetic task generation process remains partially automated and heuristic-based, raising concerns about fidelity to real freelance work
- Pricing model's moderate R²=0.65 performance suggests substantial variance in price prediction affecting earnings comparisons
- Single-turn evaluation paradigm may systematically underestimate model capabilities in real-world scenarios where iteration and debugging are standard practice

## Confidence
- **High Confidence**: Claude 3.5 Haiku's superior performance (78.7% task success, $1.52M earnings) and general model rankings are well-supported by automated evaluation framework
- **Medium Confidence**: Benchmark's ability to measure practical software engineering capability at near-human levels is plausible but limited by synthetic nature of tasks and absence of iterative refinement
- **Low Confidence**: Claim that models can handle "well-defined programming tasks at near-human levels" requires scrutiny, as study doesn't directly compare against human freelancer performance

## Next Checks
1. **Human Benchmark Comparison**: Have human software engineers attempt identical synthetic tasks and compare success rates, task completion times, and communication requirements against LLM performance
2. **Iterative Evaluation**: Modify the benchmark to allow one iteration with failing test feedback and measure the improvement in task success rates to estimate the gap between single-turn and real-world performance
3. **Synthetic vs. Real Task Fidelity**: Select a subset of tasks and convert them back to realistic freelance job postings. Have actual freelancers bid on these tasks and compare their proposed approaches, pricing, and success criteria against the synthetic benchmark design