---
ver: rpa2
title: Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language
  Phenomena from Lexical Cues
arxiv_id: '2511.13658'
source_url: https://arxiv.org/abs/2511.13658
tags:
- phenomena
- reviews
- predictive
- deceptive
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using LLMs to translate machine-learned lexical
  cues into human-understandable language phenomena for detecting deceptive hotel
  reviews. The authors first identify unintuitive predictive words (like "Chicago")
  from logistic regression classifiers, then prompt LLMs to conjecture the underlying
  language phenomena these words reflect.
---

# Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues

## Quick Facts
- arXiv ID: 2511.13658
- Source URL: https://arxiv.org/abs/2511.13658
- Reference count: 35
- Primary result: LLMs can translate lexical cues into meaningful language phenomena that improve deception detection performance

## Executive Summary
This paper addresses the challenge of making machine-learned lexical cues interpretable by using large language models (LLMs) to conjecture the underlying language phenomena these cues represent. The authors focus on deceptive hotel review detection, where seemingly random words like "Chicago" emerge as predictive features from logistic regression models. By prompting LLMs to explain why certain words are predictive, the researchers generate conjectures about the language phenomena these words reflect. They validate these conjectures by testing whether incorporating the LLM-derived phenomena improves deception detection performance compared to zero-shot approaches, demonstrating that LLMs can reliably discover meaningful patterns from subtle lexical signals.

## Method Summary
The researchers employ a two-stage approach to bridge lexical cues and interpretable phenomena. First, they train logistic regression classifiers on hotel review datasets to identify predictive words, then use LLMs to generate conjectures about the language phenomena these words reflect through carefully crafted prompts. In the second stage, they validate these conjectures by testing whether the LLM-derived phenomena improve deception detection performance. The evaluation compares LLM-conjectured phenomena against zero-shot detection, phenomena from prior knowledge, and in-context examples across four different LLMs. The validation framework measures both task performance improvement and generalizability to similar domains, establishing that LLM-derived explanations are empirically grounded and more accurate than alternative approaches.

## Key Results
- LLM-conjectured phenomena improved deception detection performance over zero-shot baselines across all four tested LLMs
- The derived phenomena showed better generalizability to similar review domains compared to unigram features alone
- LLM-conjectured explanations outperformed phenomena derived from prior knowledge or in-context examples in accuracy

## Why This Works (Mechanism)
The approach works by leveraging LLMs' ability to infer contextual patterns from individual lexical cues. When presented with a predictive word like "Chicago," LLMs can draw on their training data to conjecture the broader linguistic or situational phenomena that would make such a word indicative of deception. This translation from isolated features to interpretable phenomena creates a bridge between black-box machine learning and human-understandable explanations. The validation process confirms that these conjectures capture genuine predictive patterns rather than spurious correlations, as evidenced by consistent performance improvements across multiple LLMs and domains.

## Foundational Learning
- **Logistic regression feature importance** - Understanding how predictive words are identified from model coefficients
  - Why needed: Provides the initial lexical cues that LLMs translate into phenomena
  - Quick check: Verify coefficient magnitudes and statistical significance
- **LLM prompting strategies** - Knowledge of how to structure queries to elicit meaningful explanations
  - Why needed: Determines the quality and relevance of conjectured phenomena
  - Quick check: Test prompt variations on sample predictive words
- **Zero-shot vs. few-shot learning** - Understanding the baseline performance without explicit phenomena guidance
  - Why needed: Establishes the performance improvement attributable to LLM-conjectured explanations
  - Quick check: Compare task performance with and without provided phenomena

## Architecture Onboarding

**Component Map:**
LR Model -> Feature Selection -> LLM Prompting -> Conjecture Generation -> Validation Framework -> Performance Assessment

**Critical Path:**
The critical path flows from logistic regression feature selection through LLM conjecture generation to downstream task validation. The quality of feature selection directly impacts the relevance of LLM conjectures, while the validation framework determines whether the conjectured phenomena genuinely improve detection performance.

**Design Tradeoffs:**
The approach trades computational cost of LLM inference against the benefit of interpretable explanations. Using multiple LLMs provides robustness but increases resource requirements. The validation method prioritizes empirical performance over direct interpretability assessment, potentially missing cases where explanations are statistically useful but cognitively opaque.

**Failure Signatures:**
- Poor feature selection leading to irrelevant LLM conjectures
- LLM hallucinations producing phenomena that don't improve task performance
- Overfitting to specific domains, reducing generalizability
- Prompts that elicit generic rather than specific phenomena explanations

**First Experiments:**
1. Test LLM conjecture generation on a small set of highly predictive words to assess initial quality
2. Validate conjectures on a held-out dataset to measure performance improvement
3. Compare LLM-derived phenomena against human-generated explanations for a subset of cases

## Open Questions the Paper Calls Out
None

## Limitations
- The approach depends on logistic regression feature importance, which may be sensitive to training data composition and preprocessing
- LLM-conjectured phenomena are validated primarily through task performance rather than direct human evaluation of interpretability
- Generalizability claims are limited to similar review domains without testing on more diverse text classification tasks

## Confidence
- High confidence: The empirical finding that LLM-conjectured phenomena improve deception detection performance over zero-shot baselines
- Medium confidence: The generalizability claims across domains, as the study only examined similar review domains
- Medium confidence: The assertion that LLM-derived phenomena are more accurate than those from prior knowledge, given the limited comparison methods

## Next Checks
1. Conduct human evaluation studies to assess whether LLM-conjectured explanations are genuinely interpretable and meaningful to domain experts, not just statistically useful
2. Test the approach on more diverse text classification tasks beyond review deception to establish broader generalizability
3. Compare LLM-conjectured phenomena against alternative feature importance methods (e.g., SHAP, LIME) to determine if logistic regression coefficients are optimal for this translation task