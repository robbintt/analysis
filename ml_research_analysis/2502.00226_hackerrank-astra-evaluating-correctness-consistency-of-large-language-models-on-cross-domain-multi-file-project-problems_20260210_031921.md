---
ver: rpa2
title: 'HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models
  on cross-domain multi-file project problems'
arxiv_id: '2502.00226'
source_url: https://arxiv.org/abs/2502.00226
tags:
- mean
- benchmark
- real-world
- development
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the HackerRank-ASTRA Benchmark to evaluate
  large language models (LLMs) on real-world multi-file, project-based software development
  tasks. Unlike existing benchmarks focused on single-file coding problems, ASTRA
  presents complex frontend development scenarios requiring modifications to multiple
  source files.
---

# HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems

## Quick Facts
- arXiv ID: 2502.00226
- Source URL: https://arxiv.org/abs/2502.00226
- Reference count: 16
- Primary result: New benchmark for evaluating LLMs on complex multi-file software development tasks shows top models achieving 75% average score with varying consistency

## Executive Summary
This paper introduces the HackerRank-ASTRA Benchmark, a novel evaluation framework for assessing large language models on real-world multi-file software development tasks. Unlike existing benchmarks focused on single-file coding problems, ASTRA presents complex frontend development scenarios requiring modifications to multiple source files with React/Redux patterns. The benchmark evaluates both correctness (mean score and pass@1 rate) and consistency (median standard deviation across 32 runs) of LLM-generated solutions. Initial evaluation of 65 problems reveals that while top models achieve comparable average scores of 75%, their consistency varies significantly, with Claude-3.5-Sonnet-1022 demonstrating the highest consistency (SD = 0.0497).

## Method Summary
The ASTRA benchmark consists of 65 multi-file software development problems targeting frontend development with React/Redux patterns. Each problem requires modifications across multiple source files to implement complete features. The evaluation framework assesses two key dimensions: correctness (measured through mean score and pass@1 rate) and consistency (measured through median standard deviation across 32 runs). Solutions are automatically scored based on predefined test cases, and models are compared across various sub-skills including component rendering, API handling, and UI interactions. The benchmark also analyzes error patterns and compares performance between different output formats (XML vs JSON).

## Key Results
- Top models (o1, o1-preview, Claude-3.5-Sonnet-1022) achieve comparable average scores of 75% on the ASTRA benchmark
- Claude-3.5-Sonnet-1022 demonstrates highest consistency with median standard deviation of 0.0497 across 32 runs
- XML consistently outperforms JSON in generated solutions across all evaluated models
- Performance varies significantly across different sub-skills, with UI interactions and logical operations showing higher error rates

## Why This Works (Mechanism)
The ASTRA benchmark effectively evaluates LLMs by simulating real-world software development scenarios where solutions require coordination across multiple files. The multi-file approach captures the complexity of actual development work, where components must integrate properly and maintain consistent patterns across the codebase. By requiring modifications to existing React/Redux codebases rather than creating solutions from scratch, the benchmark tests models' ability to understand and extend existing code patterns, which is crucial for practical software development. The automated scoring system ensures objective evaluation while the consistency measurement across multiple runs provides insight into model reliability for production deployment.

## Foundational Learning
- Multi-file project evaluation: Needed to assess real-world software development capabilities; quick check: can the model handle dependencies between files
- Consistency measurement: Required for reliable deployment in production environments; quick check: standard deviation across multiple runs remains below threshold
- Automated scoring systems: Essential for scalable evaluation of complex solutions; quick check: scoring matches human evaluation on sample problems
- React/Redux patterns: Industry-standard frontend architecture; quick check: models can generate syntactically correct and functionally appropriate code

## Architecture Onboarding
- Component map: Problem specification -> Model inference -> Solution generation -> Automated testing -> Scoring -> Analysis
- Critical path: Model receives problem context -> generates multi-file solution -> automated tests execute -> scores computed -> consistency measured across runs
- Design tradeoffs: Single-file vs multi-file evaluation (comprehensive but complex vs simple but limited); XML vs JSON output format (XML slightly better but both viable)
- Failure signatures: UI issues (incorrect rendering or event handling), logical errors (wrong algorithm implementation), data handling problems (incorrect API integration)
- First experiments: 1) Compare single vs multi-file evaluation accuracy; 2) Test different prompt engineering approaches; 3) Evaluate impact of context window size on performance

## Open Questions the Paper Calls Out
- How does model performance scale with increasing file complexity and interdependency?
- What is the impact of different prompt engineering strategies on multi-file solution quality?
- Can the benchmark framework be extended to evaluate backend development and other programming paradigms?
- How do different context window sizes affect the ability to maintain consistency across multiple files?
- What additional error categories might emerge as problem complexity increases?

## Limitations
- Relatively small benchmark size of 65 problems may not represent full diversity of software development challenges
- Focus on frontend development with React/Redux limits generalizability to other programming domains
- Automated scoring may miss subtle correctness issues that human reviewers would detect
- Limited exploration of different problem difficulty levels within the benchmark
- Potential bias toward specific coding patterns and architectures prevalent in React/Redux

## Confidence
- High Confidence: Benchmark design methodology and consistency evaluation framework
- Medium Confidence: Cross-model performance comparisons and average score calculations
- Medium Confidence: Error pattern analysis and categorization

## Next Checks
1. Expand benchmark coverage to include backend development tasks, different programming languages, and non-React frameworks
2. Conduct human evaluation studies to validate automated scoring accuracy and identify potential blind spots
3. Test model performance on progressively more complex multi-file dependencies to establish scalability boundaries
4. Investigate the impact of prompt engineering and context window size on solution quality
5. Develop additional error categories and detection mechanisms for emerging failure patterns