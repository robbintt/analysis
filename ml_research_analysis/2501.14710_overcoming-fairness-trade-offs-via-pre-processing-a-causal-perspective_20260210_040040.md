---
ver: rpa2
title: 'Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective'
arxiv_id: '2501.14710'
source_url: https://arxiv.org/abs/2501.14710
tags:
- world
- fairness
- find
- data
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper resolves two key fairness trade-offs in machine learning
  by leveraging causal pre-processing methods to approximate a "fair world" (FiND)
  where protected attributes have no causal effect on the target variable. The authors
  theoretically show that in this FiND world, multiple fairness metrics become naturally
  satisfied and align with high predictive performance.
---

# Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective

## Quick Facts
- arXiv ID: 2501.14710
- Source URL: https://arxiv.org/abs/2501.14710
- Authors: Charlotte Leininger; Simon Rittel; Ludwig Bothmann
- Reference count: 40
- Key outcome: This paper resolves two key fairness trade-offs in machine learning by leveraging causal pre-processing methods to approximate a "fair world" (FiND) where protected attributes have no causal effect on the target variable.

## Executive Summary
This paper presents a novel approach to resolving fundamental fairness trade-offs in machine learning by introducing the concept of a "FiND" (fictitious and normatively desired) world where protected attributes have no causal effect on the target variable. The authors demonstrate that in this FiND world, multiple fairness metrics that are typically incompatible become naturally satisfied and align with high predictive performance. By using causal pre-processing methods to transform real-world data to approximate the FiND world, the paper shows that both the fairness-accuracy trade-off and the trade-off between different fairness metrics can be resolved simultaneously. Empirical results using simulations and real-world mortgage data demonstrate that the approach successfully achieves this alignment, with fairness metrics showing differences under 3.6% compared to the FiND world while maintaining predictive performance.

## Method Summary
The method involves defining a causal DAG for the domain, then applying causal pre-processing techniques (fairadapt or warping) to transform the real-world data to approximate a FiND world where protected attributes have no causal effect on the target. The transformation methods project the descendants of the protected attribute to match the distribution of the unprotected group, conditioned on the causal graph. After pre-processing, models are trained on the transformed data using gradient boosted trees with demographic parity regularization. The approach is evaluated by measuring multiple fairness metrics alongside predictive performance, demonstrating that the pre-processing successfully resolves the traditional trade-offs between fairness and accuracy.

## Key Results
- In the FiND world, incompatible group fairness metrics (Demographic Parity, Equalized Odds, Predictive Parity) become mathematically compatible
- Pre-processed data achieved fairness metrics with differences under 3.6% compared to the FiND world
- Models trained on warped/adapted data satisfy multiple fairness metrics simultaneously with negligible performance loss
- Fairness and performance are in accord when measured on unbiased (FiND-aligned) data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In a "FiND" (fictitious and normatively desired) world where protected attributes ($A$) have no causal effect on the target ($Y$), incompatible group fairness metrics become mathematically compatible.
- **Mechanism:** The FiND world assumes statistical independence between $A$ and $Y$ ($Y \perp A$). This implies equal base rates across groups, which is the special case required to overcome the impossibility theorem.
- **Core assumption:** The primary drivers of group disparities in the real world are causal biases that should normatively be zero, rather than "natural" differences in qualification.
- **Evidence anchors:** Mentions that classical fairness metrics deemed incompatible are "naturally satisfied in the FiND world"; theoretically derives that the FiND world represents the "special case" of equal base rates required to overcome the impossibility theorem.
- **Break condition:** If real-world disparities are driven by legitimate causal pathways that the normative framework excludes, the model may underfit legitimate signal.

### Mechanism 2
- **Claim:** The fairness-accuracy trade-off is resolved because enforcing fairness constraints on biased real-world data improves accuracy when evaluated on unbiased (FiND) data.
- **Mechanism:** Standard models overfit to biases in real-world data to maximize accuracy. When a fairness constraint forces the model to ignore these spurious correlations, the model becomes more robust. If the test data is "unbiased" (FiND-aligned), the robust model outperforms the biased one.
- **Core assumption:** The observed real-world labels contain biased noise that degrades predictive performance regarding the "true" normatively desired outcome.
- **Evidence anchors:** States fairness and performance are in accord when measured on unbiased data; shows that for models trained on real data, increasing fairness constraints increases AUC when evaluated on FiND/adapted/warped test data.
- **Break condition:** If the test data does not reflect the FiND world, the trade-off reverts to its standard form where increasing fairness lowers accuracy.

### Mechanism 3
- **Claim:** Causal pre-processing methods (fairadapt, warping) can sufficiently approximate the FiND world to resolve trade-offs in practice.
- **Mechanism:** These methods transform the feature distributions of the protected group to match the distributions of the unprotected group, conditioned on the causal graph. This effectively "erases" the causal influence of the protected attribute from the feature space.
- **Core assumption:** The causal DAG is correctly specified and observed; there are no hidden confounders distorting the transformation.
- **Evidence anchors:** Describes fairadapt and warping techniques for projecting real data to the FiND representation; demonstrates that models trained on warped/adapted data satisfy multiple fairness metrics simultaneously with negligible performance loss.
- **Break condition:** If the causal graph is misspecified, the transformation may introduce new spurious correlations or fail to remove bias.

## Foundational Learning

- **Concept:** Directed Acyclic Graphs (DAGs) & Causal Do-Calculus
  - **Why needed here:** The entire FiND framework relies on distinguishing causal paths from correlations. Without understanding DAGs, one cannot define the "FiND world" or apply the transformation methods.
  - **Quick check question:** Can you distinguish between a confounder (common cause) and a mediator (intermediate step) in a credit scoring DAG?

- **Concept:** Group Fairness Metrics (Demographic Parity vs. Equalized Odds)
  - **Why needed here:** The paper claims to resolve the "impossibility theorem" between these metrics. Understanding their mathematical definitions is required to verify if the pre-processing actually aligned them.
  - **Quick check question:** Why is Equalized Odds generally considered harder to satisfy than Demographic Parity in imbalanced datasets?

- **Concept:** Counterfactual Fairness
  - **Why needed here:** The FiND world is a counterfactual construct. The mechanism relies on estimating "what would have happened" if the protected attribute had no effect.
  - **Quick check question:** How does counterfactual fairness differ from merely removing the protected attribute column from the dataset?

## Architecture Onboarding

- **Component map:** Real-world data + Expert-defined Causal DAG -> fairadapt/warping transformation -> Gradient Boosted Trees (xgboost) -> Fairness-Performance evaluation
- **Critical path:** The definition of the Causal DAG is the single point of failure. If the graph incorrectly identifies confounders or mediators, the pre-processing will transform the data based on false causal assumptions.
- **Design tradeoffs:**
  - **Fairadapt vs. Warping:** Fairadapt uses quantile regression forests (slower, potentially more accurate for complex non-linearities); Warping uses linear/rank-based residuals (faster, assumes simpler parametric forms)
  - **Strict vs. Relaxed FiND:** The paper defaults to "no causal effect" (strict). Practitioners must decide if some paths are "admissible" (e.g., gender affecting income is historically biased but potentially relevant to loan repayment capacity)
- **Failure signatures:**
  - **Metric Incompatibility Resurfaces:** If the pre-processing is poor, enforcing one metric will still hurt another
  - **Performance Collapse:** A massive drop in AUC on the transformed data suggests the DAG is wrong (removing legitimate signal)
  - **Distribution Mismatch:** Visual checks show transformed protected group data does not overlap with unprotected group data
- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Replicate the paper's simulation. Generate data with a known ground truth DAG. Apply warping. Verify that $A \perp Y$ in the transformed data.
  2. **Baseline Comparison (Real Data):** Train a model on raw HMDA data vs. Warped HMDA data. Plot the Fairness-Accuracy curve for both. The Warped model should show a flat or positive slope (fairness â‰ˆ accuracy), while raw data shows a negative slope.
  3. **Robustness Test:** Intentionally misspecify the DAG (e.g., flip a confounder to a mediator). Re-run the pre-processing. Measure the degradation in the fairness-accuracy alignment to quantify sensitivity to causal assumptions.

## Open Questions the Paper Calls Out

- **How can the FiND-world framework be extended to handle multiple protected attributes and intersectionality?**
  - The current framework considers only a single binary protected attribute; extending to multiple PAs requires theoretical and methodological development.

- **How sensitive are fairadapt and warping methods to DAG misspecification or partial causal structure learned from data?**
  - The paper assumes the true causal graph is known; practical applications often require learning DAGs from data, which introduces uncertainty.

- **How do pre-processing methods scale to structurally more complex DAGs with many variables?**
  - Experiments used relatively simple DAGs (5-6 variables); computational and statistical challenges may arise with higher-dimensional settings.

- **Can non-causal pre-processing methods approximate the FiND world as effectively as causal methods?**
  - The study focused on causal methods but many fairness pre-processing approaches are non-causal; their relative effectiveness is unknown.

## Limitations

- The method relies heavily on correct DAG specification, and misspecification can degrade effectiveness
- The FiND world assumption may not fully capture normative fairness in all real-world settings
- The approach requires expert knowledge to define the causal structure, limiting scalability
- Performance sensitivity to hyperparameter choices in the fairness-regularized training procedure

## Confidence

- **High confidence**: The theoretical mechanism that equal base rates resolve the impossibility theorem is sound and well-demonstrated through mathematical derivation
- **Medium confidence**: The empirical demonstration using synthetic data and HMDA shows the method works in controlled settings, but generalization to arbitrary domains requires further validation
- **Medium confidence**: The claim that fairness-accuracy trade-off disappears when evaluated on FiND-aligned data is theoretically justified but practically contingent on correctly approximating the FiND world

## Next Checks

1. **DAG Sensitivity Analysis**: Systematically test how incorrect DAG specifications affect the fairness-performance alignment. Quantify the degradation threshold where trade-offs reappear.

2. **Real-world Fairness Transfer**: Apply the method to multiple real datasets with different causal structures and protected attributes. Verify that the positive fairness-accuracy slope generalizes beyond HMDA mortgage data.

3. **Robustness to Imbalanced Protected Groups**: Test the method when the protected attribute has highly imbalanced group sizes (e.g., 95/5 split). Measure whether the pre-processing still produces meaningful fairness improvements or if statistical power becomes a limiting factor.