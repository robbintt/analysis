---
ver: rpa2
title: 'Gamayun''s Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter
  LLM'
arxiv_id: '2512.21580'
source_url: https://arxiv.org/abs/2512.21580
tags:
- language
- arxiv
- multilingual
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Gamayun, a 1.5B-parameter multilingual language
  model trained from scratch on 2.5 trillion tokens. To address the challenge of training
  small multilingual models, the authors introduce a two-stage pre-training strategy:
  balanced multilingual training for cross-lingual alignment, followed by high-quality
  English enrichment.'
---

# Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM

## Quick Facts
- arXiv ID: 2512.21580
- Source URL: https://arxiv.org/abs/2512.21580
- Authors: Alexander Podolskiy, Semen Molokov, Timofey Gerasin, Maksim Titov, Alexey Rukhovich, Artem Khrapov, Kirill Morozov, Evgeny Tetin, Constantine Korikov, Pavel Efimov, Polina Lazukova, Yuliya Skripkar, Nikita Okhotnikov, Irina Piontkovskaya, Meng Xiaojun, Zou Xueyi, Zhang Zhenhe
- Reference count: 40
- Gamayun outperforms LLaMA3.2-1B on all benchmarks and Qwen2.5-1.5B on most English and multilingual tasks

## Executive Summary
Gamayun is a 1.5B-parameter multilingual language model trained from scratch on 2.5 trillion tokens supporting 12 languages. The authors introduce a two-stage pre-training strategy that first establishes cross-lingual alignment through balanced multilingual training, then enriches with high-quality English data to transfer reasoning capabilities across all languages. Despite using significantly less compute than comparable models, Gamayun achieves state-of-the-art results on the MERA Russian benchmark among models of similar size and demonstrates strong performance across English and multilingual evaluations.

## Method Summary
Gamayun uses a LLaMA-like decoder-only transformer architecture (24 layers, 16 heads, hidden=2048) with SwiGLU activation, RMSNorm pre-normalization, and RoPE position encoding. The model employs a two-stage pre-training approach: Stage 1 uses proportional language sampling with ~37% English for cross-lingual alignment, followed by Stage 2 with ~70% English high-quality enrichment. The training corpus combines filtered mC4 with books, scientific papers, code, Wikipedia, and synthetic data. Post-training includes SFT (2.32M examples, 2 epochs) and DPO alignment (1 epoch).

## Key Results
- Outperforms LLaMA3.2-1B on all benchmarks and Qwen2.5-1.5B on most English and multilingual tasks
- Achieves state-of-the-art results on MERA Russian benchmark among models of comparable size
- Synthetic multiple-choice format (MCF) data closes 14.9-point gap between base and instructed model on MMLU MCF
- Two-stage pre-training strategy improves both multilingual and English performance compared to fixed-ratio training

## Why This Works (Mechanism)

### Mechanism 1
A two-stage pre-training strategy (balanced multilingual → English-enriched) improves both multilingual and English performance compared to fixed-ratio training. Stage 1 establishes cross-lingual representation alignment with balanced language mixing (~37% English). Stage 2 then leverages higher-quality English data to transfer reasoning and knowledge gains back to all languages via the already-aligned representations.

### Mechanism 2
Low-quality non-English data degrades training efficiency not just for that language but for all languages, including high-resource English. Poor-quality tokens introduce noise into gradient updates that propagates across shared model parameters, reducing the effective learning from high-quality data regardless of language.

### Mechanism 3
Synthetic multiple-choice format (MCF) data during post-training closes the gap between base and instructed model performance on benchmark formats, independent of underlying knowledge. Small models haven't learned the MCF task format during pre-training, so synthetic examples teach the response format (letter selection) using content already seen in pre-training, avoiding hallucination from novel knowledge.

## Foundational Learning

- **Curse of Multilinguality:**
  - Why needed here: Core problem the paper addresses; understanding why naive multilingual mixing degrades primary-language performance is essential to grasping why the two-stage solution works.
  - Quick check question: Given a fixed English token budget, would you expect a model trained on English-only or English+3-other-languages to have lower English perplexity? (Answer: English-only, per Fig. 3)

- **Tokenizer Fertility:**
  - Why needed here: Directly affects computational efficiency and effective context length; paper selects LLaMA 3 tokenizer based on fertility analysis across languages.
  - Quick check question: If tokenizer A produces 1.5 tokens/word and tokenizer B produces 3.0 tokens/word for the same text, which gives longer effective context for a 2048-token window? (Answer: Tokenizer A—fewer tokens per word means more words fit)

- **Cross-lingual Transfer:**
  - Why needed here: The theoretical basis for why English enrichment in Stage 2 helps non-English languages; assumes aligned representations allow capability transfer.
  - Quick check question: If you train a model on English math problems after multilingual alignment, would you expect improvement on Russian math problems? (Answer: Yes, per paper's findings—aligned representations transfer reasoning patterns)

## Architecture Onboarding

- **Component map:**
  Backbone: LLaMA-style decoder-only transformer (24 layers, 16 heads, hidden=2048, FFN intermediate=5440) → Normalization: RMSNorm (pre-normalization) → Activation: SwiGLU → Position encoding: RoPE (base 10,000 → 500,000 for long context) → Embeddings: Tied input-output (1.5B main + 0.2B embedding params) → Tokenizer: LLaMA 3 (128K vocab)

- **Critical path:**
  1. Data curation: mC4 base → quality filtering (perplexity, heuristics, classifier) → language-specific augmentation (books, STEM, code)
  2. Stage 1 pre-training: ~37% English, proportional sampling, ~2T tokens
  3. Stage 2 pre-training: ~70% English, high-quality sources, remaining tokens
  4. Annealing: 400B tokens, increased STEM/code/synthetic, context expansion to 16K
  5. Post-training: SFT (2.32M examples, 2 epochs) → DPO alignment (1 epoch)

- **Design tradeoffs:**
  - Deep-and-narrow vs. wide-and-shallow: Chose 24 layers for better performance at small scale
  - Vocabulary size vs. efficiency: 128K vocab balances multilingual coverage with embedding parameter overhead
  - Language count: Limited to 12 languages to reduce interference; more languages = more interference
  - No QK-norm: Simplified architecture; may affect training stability at scale

- **Failure signatures:**
  - English loss degrades during multilingual training → data quality gap in non-English sources
  - MCF accuracy stuck near random (25%) → insufficient format exposure; add synthetic MCF data
  - Russian morphology errors in generation → insufficient Russian token exposure; increase Russian proportion in Stage 1
  - Hallucination on post-training queries → synthetic data contains novel knowledge not in pre-training

- **First 3 experiments:**
  1. Replicate the two-stage vs. fixed-ratio ablation on a smaller model (~500M params, ~50B tokens) on Wikipedia to verify alignment→enrichment benefit before committing compute
  2. Test quality filtering impact: train two models with identical mix but one with mC4 raw, one with classifier-filtered non-English data; compare English and target-language loss curves
  3. Validate MCF format learning: take a base checkpoint, fine-tune with vs. without synthetic MCF data on a held-out task subset; measure MMLU-format vs. cloze-format gap

## Open Questions the Paper Calls Out

- **Optimal language mixing strategy:** What is the optimal strategy for mixing languages during multilingual pre-training of small LLMs, and does a single dominant language with others "attached" outperform balanced mixing?

- **Compute-efficient STEM reasoning:** Can small multilingual models close the performance gap in advanced STEM and mathematical reasoning without 10× more pre-training compute?

- **Optimal transition timing:** What determines the optimal transition point between Stage 1 (balanced multilingual) and Stage 2 (English-enriched) pre-training?

## Limitations

- Data quality classification relies on a FastText ensemble classifier with unspecified false-positive/false-negative rates, introducing uncertainty about excluded content
- Two-stage training dynamics lack ablation studies on different stage ratios or transition points
- Format learning vs. knowledge transfer claims remain unverified through alternative format teaching methods

## Confidence

**High Confidence:**
- Gamayun's benchmark performance superiority over LLaMA3.2-1B and Qwen2.5-1.5B is well-documented with multiple evaluation datasets
- The curse of multilinguality observation (Fig. 3) showing fixed-ratio multilingual training degrades primary language performance is directly measurable
- The tokenizer fertility analysis leading to LLaMA 3 tokenizer selection is based on reproducible metrics

**Medium Confidence:**
- The two-stage training strategy's superiority over alternatives, while demonstrated, lacks ablation studies on different stage ratios or transition points
- The cross-lingual transfer mechanism (English enrichment benefiting non-English languages) is theoretically sound but relies on unmeasured alignment quality metrics
- The claim that low-quality non-English data harms English performance is supported by loss curves but not by controlled experiments varying quality ratios

**Low Confidence:**
- The exact composition and quality thresholds of the corpus used for each language stage
- The generalizability of the synthetic MCF approach to other format-learning scenarios
- The long-term stability of the ABF context expansion technique beyond the reported 40B tokens

## Next Checks

1. **Ablation Study on Stage 1 Duration:** Train parallel models varying only the Stage 1 token count (e.g., 1T, 2T, 3T tokens) before switching to Stage 2 enrichment. Measure multilingual alignment quality (using parallel corpus retrieval accuracy) and final benchmark performance to identify the optimal alignment duration and test the "alignment prerequisite" hypothesis.

2. **Controlled Quality Filtering Experiment:** Create three training runs with identical language proportions but different quality profiles: (a) full mC4 without filtering, (b) classifier-filtered non-English data only, (c) classifier-filtered plus aggressive English filtering. Compare English and target-language loss curves throughout training to quantify cross-language interference from low-quality data.

3. **Format Learning Alternative Comparison:** Take a base Gamayun checkpoint and create three fine-tuned versions: (a) synthetic MCF data as described, (b) few-shot prompting on MMLU tasks without synthetic data, (c) instruction-tuned with format explanations but no synthetic examples. Evaluate all three on MMLU MCF to determine whether the improvement comes from format learning specifically or general instruction-following capabilities.