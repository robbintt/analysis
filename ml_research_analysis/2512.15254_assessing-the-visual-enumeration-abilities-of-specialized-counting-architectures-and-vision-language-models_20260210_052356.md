---
ver: rpa2
title: Assessing the Visual Enumeration Abilities of Specialized Counting Architectures
  and Vision-Language Models
arxiv_id: '2512.15254'
source_url: https://arxiv.org/abs/2512.15254
tags:
- counting
- count
- vlms
- object
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares specialized counting architectures with multimodal
  vision-language models (VLMs) on object counting tasks. It evaluates state-of-the-art
  VLMs (Claude 4.5 Sonnet, Gemini 2.5 Pro, GPT-5) against counting-specific models
  (PseCo, T2ICount, TFOC) across three datasets: FSC-147, FSCD-LVIS, and a novel synthetic
  benchmark (SolidCount).'
---

# Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models

## Quick Facts
- arXiv ID: 2512.15254
- Source URL: https://arxiv.org/abs/2512.15254
- Authors: Kuinan Hou; Jing Mi; Marco Zorzi; Lamberto Ballan; Alberto Testolin
- Reference count: 40
- Primary result: VLMs match or exceed specialized counting models on object counting benchmarks, especially when prompted to generate intermediate representations

## Executive Summary
This study compares specialized counting architectures with multimodal vision-language models (VLMs) on object counting tasks. The researchers evaluate state-of-the-art VLMs (Claude 4.5 Sonnet, Gemini 2.5 Pro, GPT-5) against counting-specific models (PseCo, T2ICount, TFOC) across three datasets: FSC-147, FSCD-LVIs, and a novel synthetic benchmark (SolidCount). The VLMs demonstrate remarkable zero-shot counting capability, with Gemini 2.5 Pro surpassing specialized models on most benchmarks. Notably, prompting VLMs to generate intermediate representations (locations and labels) significantly improves their accuracy, with Gemini achieving state-of-the-art performance across all datasets using the "point, label, and count" strategy. The study reveals that VLMs are more robust to visual complexity than specialized models, which struggle with background clutter and shape/color heterogeneity.

## Method Summary
The researchers conducted a comprehensive evaluation comparing specialized counting architectures (PseCo, T2ICount, TFOC) with multimodal vision-language models (VLMs) on object counting tasks. They tested three state-of-the-art VLMs (Claude 4.5 Sonnet, Gemini 2.5 Pro, GPT-5) across three datasets: FSC-147, FSCD-LVIs, and a novel synthetic benchmark called SolidCount. The evaluation included zero-shot testing where VLMs were prompted to count objects directly, as well as experiments where VLMs were prompted to first generate intermediate representations (locations and labels) before counting. The study specifically examined how different prompting strategies affected counting accuracy and compared performance across varying levels of visual complexity.

## Key Results
- VLMs achieved competitive zero-shot counting performance, with Gemini 2.5 Pro surpassing specialized models on most benchmarks
- Prompting VLMs to generate intermediate representations (locations and labels) significantly improved counting accuracy
- VLMs demonstrated greater robustness to visual complexity compared to specialized counting models, particularly with background clutter and shape/color heterogeneity

## Why This Works (Mechanism)
The study demonstrates that VLMs can leverage their general visual understanding and language capabilities to perform counting tasks without specialized training. By generating intermediate representations such as object locations and labels, VLMs can break down the counting task into more manageable subtasks that align with their existing capabilities. The success of the "point, label, and count" strategy suggests that VLMs can effectively use their spatial reasoning and object recognition abilities in a sequential manner to achieve accurate enumeration. This approach allows VLMs to handle complex visual scenes by first identifying and localizing objects before performing the counting operation, rather than trying to count directly from raw visual input.

## Foundational Learning

**Visual grounding** - Understanding the relationship between visual elements and their spatial locations in an image. Why needed: Essential for accurate object localization before counting. Quick check: Can the model correctly identify object positions in cluttered scenes.

**Zero-shot learning** - The ability to perform tasks without task-specific training. Why needed: Demonstrates the general capabilities of VLMs beyond their original training objectives. Quick check: Performance on tasks not explicitly included in training data.

**Multimodal reasoning** - Integrating visual and language understanding for complex tasks. Why needed: Enables VLMs to process both the visual scene and linguistic instructions simultaneously. Quick check: Accuracy on tasks requiring both visual perception and language comprehension.

**Intermediate representation generation** - Creating step-by-step outputs that break down complex tasks. Why needed: Allows models to decompose difficult problems into simpler subtasks. Quick check: Improvement in final task performance when using intermediate steps.

## Architecture Onboarding

**Component map**: Image input -> Visual encoder -> Feature extraction -> Language model -> Prompt processing -> Counting output

**Critical path**: Image processing → Visual feature extraction → Prompt interpretation → Intermediate representation generation → Final counting

**Design tradeoffs**: The study compares general-purpose VLMs against specialized counting architectures, highlighting the tradeoff between versatility and task-specific optimization. VLMs sacrifice some counting-specific accuracy for broader applicability, while specialized models excel at counting but lack general visual reasoning capabilities.

**Failure signatures**: Specialized models struggle with background clutter and heterogeneous object appearances, while VLMs may produce inconsistent counts when asked to count directly without intermediate steps. Both types of models can fail with heavily occluded objects or when objects blend into complex backgrounds.

**First experiments**:
1. Direct zero-shot counting on FSC-147 dataset to establish baseline VLM performance
2. Comparison of VLM performance with and without intermediate representation prompting on SolidCount
3. Stress testing both model types with varying levels of visual complexity and clutter

## Open Questions the Paper Calls Out
None

## Limitations

- The evaluation relies heavily on synthetic datasets (SolidCount) that may not fully capture real-world visual complexity and variability
- The "point, label, and count" prompting strategy requires multiple generations and additional computational overhead not thoroughly benchmarked for efficiency
- The study focuses primarily on counting accuracy without extensive examination of specific failure modes or error analysis for different object categories

## Confidence

**High confidence**: VLMs achieve competitive counting performance without specialized training, and Gemini 2.5 Pro specifically demonstrates state-of-the-art results across multiple benchmarks.

**Medium confidence**: The superiority of VLMs over specialized counting architectures in handling visual complexity, as this conclusion relies heavily on synthetic benchmark performance that may not generalize.

**Medium confidence**: The effectiveness of intermediate representation prompting strategies, as the evaluation shows clear improvements but lacks comprehensive efficiency analysis and scalability testing.

## Next Checks

1. **Real-world robustness testing**: Evaluate the same models on in-the-wild datasets with uncontrolled object distributions, varying lighting conditions, and natural scene clutter to validate whether synthetic benchmark performance translates to practical applications.

2. **Efficiency benchmarking**: Measure the computational cost (tokens, time, API calls) of the "point, label, and count" strategy compared to direct counting approaches across different scene complexities and object counts.

3. **Failure mode analysis**: Conduct systematic testing to identify specific visual scenarios (occlusion patterns, object overlap, unusual object arrangements) where each model type fails, providing more granular insights into model limitations beyond aggregate accuracy scores.