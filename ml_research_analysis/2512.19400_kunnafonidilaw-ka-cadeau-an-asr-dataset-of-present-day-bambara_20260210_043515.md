---
ver: rpa2
title: 'Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara'
arxiv_id: '2512.19400'
source_url: https://arxiv.org/abs/2512.19400
tags:
- speech
- data
- bambara
- dataset
- kunkado
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Kunkado, a 160-hour Bambara automatic speech
  recognition (ASR) dataset compiled from Malian radio archives to capture present-day
  spontaneous speech, including code-switching, disfluencies, background noise, and
  overlapping speakers. The authors developed pragmatic transcript normalization to
  reduce variability in number formatting, tags, and code-switching annotations, and
  finetuned Parakeet-based models on a 33.47-hour human-reviewed subset.
---

# Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara

## Quick Facts
- arXiv ID: 2512.19400
- Source URL: https://arxiv.org/abs/2512.19400
- Reference count: 6
- Key outcome: 160-hour Bambara ASR dataset from radio archives, finetuning reduces WER from 44.47% to 37.12% on one test set

## Executive Summary
This paper presents Kunkado, a 160-hour Bambara automatic speech recognition dataset compiled from Malian radio archives to capture present-day spontaneous speech, including code-switching, disfluencies, background noise, and overlapping speakers. The authors developed pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations, and finetuned Parakeet-based models on a 33.47-hour human-reviewed subset. Evaluation on two real-world test sets showed that finetuning with Kunkado reduced WER from 44.47% to 37.12% on one test set and from 36.07% to 32.33% on the other. Human evaluation indicated that the resulting model outperformed a comparable system trained on 98 hours of cleaner, less realistic speech, demonstrating the value of more authentic, challenging speech data for practical ASR deployment in predominantly oral languages.

## Method Summary
The authors compiled 160 hours of Bambara speech from Malian radio archives, segmented using energy-based silence detection, and created automatic transcripts using a pre-trained Parakeet model. Seven annotators corrected these transcripts with 13 acoustic tags and code-switching markers, producing 39.3 hours of reviewed data. A pragmatic normalization pipeline simplified number formats, reduced tag variability, and stripped code-switching markers. The authors then finetuned Parakeet-based models (FastConformer encoder + TDT/CTC decoders) on a 33.47-hour normalized subset using AdamW with Noam scheduler on 4× NVIDIA A100 80GB GPUs.

## Key Results
- Finetuning with Kunkado reduced WER from 44.47% to 37.12% on one test set
- WER improved from 36.07% to 32.33% on another test set
- Human evaluation showed the finetuned model outperformed a comparable system trained on 98 hours of cleaner speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on radio archive data with natural speech phenomena improves real-world ASR performance more than equivalent or larger amounts of curated, controlled recordings.
- Mechanism: Radio broadcasts contain the full distribution of deployment-time acoustic and linguistic challenges—code-switching, disfluencies, overlapping speakers, background noise—reducing train-test distribution mismatch. Models learn robust representations from this "in-the-wild" data rather than overfitting to artificial cleanliness.
- Core assumption: The acoustic-linguistic distribution of radio broadcasts approximates real-world Bambara speech use cases (phone calls, casual conversation, multi-speaker scenarios).
- Evidence anchors:
  - [abstract] "In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech."
  - [section 4] "soloni-v1 achieved the highest human evaluation score on Nyana-Eval (59.5 out of 135), outperforming soloni-v2, which was trained with an additional 98 hours from the African Next Voices Bambara dataset."
  - [corpus] Related work (Doumbouya et al., 2021) cited in paper confirms radio archives as viable data source for low-resource ASR; no direct corpus contradiction.
- Break condition: If deployment domain is fundamentally different from radio broadcasts (e.g., medical dictation, legal proceedings), domain mismatch may re-emerge.

### Mechanism 2
- Claim: Pragmatic transcript normalization (simplifying number formats, reducing tag variability, stripping code-switch markers) enables effective fine-tuning with limited (<35 hours) reviewed data.
- Mechanism: End-to-end ASR models require substantial data to learn consistent mappings when transcript conventions vary. Normalization collapses superficial annotation variance (e.g., "76" vs "bi wolonwula ni wOOrO"), allowing the model to allocate capacity toward acoustic-linguistic learning rather than orthographic disambiguation.
- Core assumption: The normalization procedure does not discard information critical to downstream applications; simplified representations remain sufficient for practical use.
- Evidence anchors:
  - [section 3] "We applied these normalization steps after noticing that, with only just over 30 hours of training data, the models were struggling with the human-annotation variability present in the reference transcripts. These models rarely improved beyond 70% WER due to inconsistencies in numbers, tags and code-switching."
  - [section 2] "Annotators were instructed to use 13 tags to capture as much acoustic and linguistic information as possible" but normalization "kept only a reduced set of acoustic event tags... modeled during training but ignored during evaluation."
  - [corpus] Limited direct corpus evidence on this specific normalization strategy for Bambara; similar approaches suggested in code-switching ASR literature but not validated for this language.
- Break condition: If downstream applications require precise number formatting or fine-grained acoustic event classification, this normalization will require post-processing or model retraining.

### Mechanism 3
- Claim: Re-annotator workflow (correcting automatic transcriptions rather than transcribing from scratch) with flexible orthography and numeric conventions accelerates dataset creation for predominantly oral languages.
- Mechanism: Starting from model-generated transcripts reduces cognitive load on annotators—they verify and correct rather than generate. Combined with pragmatic formatting (Hindu-Arabic numerals, no strict orthography enforcement), this reduces per-segment annotation time.
- Core assumption: The base ASR model (soloni-114m-tdt-ctc-v0) produces sufficiently accurate initial transcriptions that correction is faster than scratch transcription; annotator familiarity with Bambara orthography is limited enough that flexibility improves speed without degrading quality.
- Evidence anchors:
  - [section 2] "our team of seven annotators were able to correct 39.3 hours of segments in roughly 1260 hours of human labor, yielding a 32x ratio, a bit faster than the 36x reference datapoint reported in the transcription cost analysis study by (Diarra et al., 2025b)"
  - [section 2] "We speculate that more flexible orthography and the use of numerals rather than spelled-out numbers contributed to this 4-hour difference."
  - [corpus] Cost Analysis paper (Diarra et al., 2025b) provides the 36x reference point for traditional annotation approaches.
- Break condition: If base model quality is too low (e.g., WER >80%), correction overhead may approach scratch transcription time.

## Foundational Learning

- Concept: **Connectionist Temporal Classification (CTC) loss**
  - Why needed here: The Parakeet models use a hybrid architecture with CTC as one of two jointly-trained decoders. Understanding CTC helps explain why the model can learn from unaligned audio-text pairs.
  - Quick check question: Given an audio sequence of length T and a transcript of length N (N << T), how does CTC enable training without frame-level alignments?

- Concept: **Token-and-Duration Transducer (TDT)**
  - Why needed here: The second decoder in the architecture predicts both tokens and their durations. This hybrid approach combines autoregressive modeling with explicit duration prediction.
  - Quick check question: What advantage does jointly predicting token duration provide over standard autoregressive decoding?

- Concept: **Code-switching in ASR**
  - Why needed here: Bambara speech frequently mixes with French and Arabic. The dataset explicitly marks code-switched segments, and model performance depends on handling these transitions.
  - Quick check question: Why might intra-sentential code-switching (language change mid-sentence) be harder for ASR than inter-sentential switching?

## Architecture Onboarding

- Component map:
Radio Audio (raw)
    ↓ [pydub energy-based segmentation, 600ms min silence, -35dBFS threshold]
Audio Segments (0.6s–45s, mean 4.9s)
    ↓ [Initial transcription: soloni-114m-tdt-ctc-v0]
Automatic Transcripts
    ↓ [Human review with 13 tags + code-switch markers]
Reviewed Transcripts (39.3 hours)
    ↓ [bambara-normalizer: strip symbols/diacritics, normalize numbers, collapse tags]
Normalized Transcripts (33.47 hours train set)
    ↓ [Fine-tuning: FastConformer encoder + TDT decoder + CTC decoder]
Fine-tuned ASR Model (soloni-v1/v3)

- Critical path:
  1. Audio segmentation quality—over-segmentation cuts off speech; under-segmentation creates unmanageably long segments
  2. Base model transcription quality—determines annotation efficiency
  3. Normalization consistency—directly impacts model convergence with limited data
  4. Fine-tuning learning rate schedule—Noam scheduler with warmup prevents catastrophic forgetting

- Design tradeoffs:
  - **Silence-based vs VAD segmentation**: Paper uses energy threshold for speed and preservation of all content, but acknowledges rougher cuts and speech truncation
  - **Numeric representation**: Hindu-Arabic numerals (faster annotation, simpler downstream) vs spelled-out words (orthodox ASR practice)
  - **Tag retention during training**: Keeping some acoustic event tags provides signal for non-speech events but increases vocabulary and complexity
  - **Orthography enforcement**: Flexible orthography speeds annotation but may increase transcript variability

- Failure signatures:
  - WER plateauing at ~70% during fine-tuning → likely annotation inconsistency in numbers, tags, or code-switching markers
  - Model generating French words in Bambara-only contexts → insufficient code-switching boundary learning or training data skew
  - Poor performance on overlapping speech → <CHEVAUCHEMENT> tag may be underrepresented or model architecture lacks speaker separation

- First 3 experiments:
  1. **Baseline replication**: Load soloni-v2 from HuggingFace, evaluate on Nyana-Eval subset, confirm reported WER (36.07%) to validate evaluation pipeline.
  2. **Normalization ablation**: Train two models—one with full normalization pipeline, one with raw transcripts only—on the same 33.47-hour split to quantify normalization contribution (paper suggests >30% WER difference).
  3. **Data efficiency curve**: Fine-tune soloni-v2 on progressively larger subsets of Kunkado (10h, 20h, 33h) to establish the relationship between reviewed data quantity and WER reduction for this architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the use of Hindu-Arabic numerals in transcripts, as opposed to spelled-out words, improve or hinder ASR performance and downstream task accuracy in low-resource languages like Bambara?
- Basis in paper: [explicit] The authors state in Section 2 that they "have created data for comparison of number formatting approaches in end-to-end ASR—which remains a challenge even in high resource languages."
- Why unresolved: While the authors adopted numerals to speed up annotation and simplify validation, they acknowledge that the impact on model accuracy compared to traditional orthographic standards has not yet been evaluated.
- What evidence would resolve it: A comparative experiment training models on identical datasets differing only in number formatting (numerals vs. words) and evaluating on a test set with normalized output metrics.

### Open Question 2
- Question: What is the minimum volume of human-reviewed data required to train end-to-end ASR models to handle code-switching and linguistic variability without requiring aggressive pragmatic normalization?
- Basis in paper: [inferred] In Section 3, the authors note that with only 33 hours, models "struggled with the human-annotation variability" and required heavy normalization, leading to the conclusion that it would "require much more data for an end-to-end ASR model to learn the original task."
- Why unresolved: It is unclear if the pragmatic normalization used (removing tags and simplifying text) is a necessary preprocessing step for all low-resource datasets or a workaround for the specific 33-hour volume used in this study.
- What evidence would resolve it: Training scaling curves using larger subsets of the Kunkado dataset (e.g., 100h, 300h) with raw transcripts to see if the model naturally learns to handle tags and disfluencies without normalization.

### Open Question 3
- Question: How can evaluation metrics be adapted to specifically assess and incentivize improvements in code-switching detection and transcription within predominantly oral languages?
- Basis in paper: [explicit] The Conclusion states the authors release the dataset to encourage "future work on code-switching-aware evaluation and data collection."
- Why unresolved: Standard WER may penalize code-switched words (often French in this context) differently than native vocabulary, potentially masking the model's ability to handle multilingual spontaneous speech.
- What evidence would resolve it: The development and application of a specialized evaluation benchmark that segments and weighs code-switched tokens separately from monolingual Bambara speech.

## Limitations

- Limited human-reviewed training data (33.47 hours) required pragmatic normalization that may have removed linguistically meaningful distinctions
- Evaluation based on only two modest-sized test sets without detailed human evaluation methodology
- Findings may not generalize to other predominantly oral languages with different orthographies or code-switching patterns

## Confidence

- **High Confidence**: The core finding that radio archive data with natural speech phenomena improves ASR performance over cleaner, curated data is well-supported by the WER comparisons (37.12% vs 44.47% on one test set, 32.33% vs 36.07% on another) and the human evaluation results showing soloni-v1 outperforming soloni-v2 on Nyana-Eval.
- **Medium Confidence**: The mechanism by which transcript normalization enables effective fine-tuning with limited data is plausible but not directly validated through ablation studies in this paper. The claim about improved annotation efficiency (32x ratio vs 36x reference) is supported by comparison to published work but lacks detailed methodology description.
- **Low Confidence**: The generalization of these findings to other predominantly oral languages is speculative. The specific normalization rules, annotation workflows, and even the base model choice may not transfer directly to languages with different orthographies, code-switching patterns, or acoustic characteristics.

## Next Checks

1. **Normalization Ablation Study**: Train two identical models on the same 33.47-hour subset—one with full normalization applied, one with raw transcripts including all tags, code-switching markers, and original number formats. This would quantify the exact contribution of normalization to WER reduction and reveal whether information loss occurred.

2. **Base Model Quality Impact**: Conduct a controlled experiment varying the quality of initial automatic transcriptions by using different base models (e.g., soloni-114m-tdt-ctc-v0 vs a weaker or stronger model). This would isolate the impact of base model quality on annotation efficiency and final WER, testing the 32x annotation ratio claim more rigorously.

3. **Cross-Domain Generalization Test**: Evaluate the fine-tuned soloni-v1 model on a completely different Bambara speech corpus (e.g., telephone conversations, if available, or data from a different geographic region). This would test whether the radio-based training generalizes beyond the broadcast domain to truly represent "present-day" Bambara speech as claimed.