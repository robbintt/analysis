---
ver: rpa2
title: 'A2AS: Agentic AI Runtime Security and Self-Defense'
arxiv_id: '2510.13825'
source_url: https://arxiv.org/abs/2510.13825
tags:
- security
- a2as
- prompt
- arxiv
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces A2AS, a runtime security layer for AI agents
  and LLM-powered applications designed to address the inherent vulnerability of prompt
  injection attacks. A2AS implements the BASIC security model through five core controls:
  behavior certificates for action permissions, authenticated prompts for integrity
  verification, security boundaries for input isolation, in-context defenses for secure
  model reasoning, and codified policies for application-specific rules.'
---

# A2AS: Agentic AI Runtime Security and Self-Defense

## Quick Facts
- arXiv ID: 2510.13825
- Source URL: https://arxiv.org/abs/2510.13825
- Authors: Eugene Neelou, Ivan Novikov, Max Moroz, Om Narayan, Tiffany Saade, Mika Ayenson, Ilya Kabanov, Jen Ozmen, Edward Lee, Vineeth Sai Narajala, Emmanuel Guilherme Junior, Ken Huang, Huseyin Gulsin, Jason Ross, Marat Vyshegorodtsev, Adelin Travers, Idan Habler, Rahul Jadav
- Reference count: 38
- Primary result: Runtime security layer preventing prompt injection attacks in AI agents through five core controls

## Executive Summary
A2AS introduces a runtime security layer for AI agents and LLM-powered applications that addresses the critical vulnerability of prompt injection attacks. The framework implements the BASIC security model through five core controls: behavior certificates, authenticated prompts, security boundaries, in-context defenses, and codified policies. Operating natively within the context window, A2AS provides context-level controls through prompt instrumentation without requiring external dependencies, architectural changes, model retraining, or operational complexity.

The approach was evaluated through three attack scenarios demonstrating effective protection against prompt injection, data exfiltration, and malicious behavior propagation while maintaining system performance and usability. The framework's design focuses on providing function-level and context-level security enforcement through prompt instrumentation, making it a practical solution for securing agentic AI systems in production environments.

## Method Summary
A2AS operates as a runtime security layer that implements the BASIC security model through five core controls: behavior certificates for action permissions, authenticated prompts for integrity verification, security boundaries for input isolation, in-context defenses for secure model reasoning, and codified policies for application-specific rules. The framework works natively within the context window, avoiding latency overhead and external dependencies while providing context-level controls through prompt instrumentation. This approach enables function-level and context-level security enforcement without requiring custom encoders or extensive policy specification, making it operationally transparent for production deployment.

## Key Results
- Effectively prevents prompt injection attacks through authenticated prompt verification and behavior certificates
- Maintains zero-latency overhead by operating natively within the context window
- Provides comprehensive protection against user-to-agent, agent-to-tool, and agent-to-agent attack vectors
- Demonstrates operational transparency without requiring external dependencies or architectural changes

## Why This Works (Mechanism)
A2AS works by implementing a comprehensive security model that operates at the runtime level within AI agents. The framework uses behavior certificates to define and enforce action permissions, ensuring agents only perform authorized operations. Authenticated prompts verify the integrity of incoming instructions, preventing malicious actors from injecting unauthorized commands. Security boundaries create isolation between different input sources, while in-context defenses provide secure reasoning mechanisms within the model itself. Codified policies allow for application-specific rule enforcement, creating a multi-layered defense system that addresses the unique challenges of prompt injection attacks.

## Foundational Learning
**Behavior Certificates**: Define allowed actions for agents - needed to prevent unauthorized operations, check by verifying certificate enforcement in test scenarios
**Authenticated Prompts**: Verify prompt integrity and origin - needed to prevent malicious instruction injection, check by testing with forged prompt attempts
**Security Boundaries**: Isolate different input sources - needed to prevent cross-contamination of malicious content, check by testing boundary violations
**In-Context Defenses**: Provide secure reasoning within model context - needed to maintain security without external dependencies, check by evaluating reasoning under attack
**Codified Policies**: Application-specific rule enforcement - needed for customizable security requirements, check by testing policy violation detection

## Architecture Onboarding

Component Map: Agent -> A2AS Runtime Layer -> BASIC Security Controls -> Protected Operations

Critical Path: Input Reception -> Authentication Verification -> Boundary Enforcement -> Defense Application -> Policy Enforcement -> Action Execution

Design Tradeoffs: Zero-latency operation vs. comprehensive security coverage; Native context operation vs. potential context window limitations; Operational transparency vs. security granularity

Failure Signatures: Authentication bypass attempts; Boundary crossing violations; Policy enforcement failures; Defense mechanism evasion

First Experiments:
1. Test authenticated prompt verification with controlled injection attempts
2. Evaluate behavior certificate enforcement across different action types
3. Validate security boundary isolation between multiple input sources

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focuses on simulated attack scenarios rather than real-world deployment testing
- Framework's effectiveness against sophisticated, multi-vector attacks remains unclear
- Lacks empirical validation of claimed zero-latency overhead across different agent complexities

## Confidence

**High Confidence**: The BASIC security model framework and its five core components (behavior certificates, authenticated prompts, security boundaries, in-context defenses, codified policies) are well-defined and theoretically sound

**Medium Confidence**: The runtime security layer's ability to prevent prompt injection attacks, as demonstrated through controlled attack scenarios

**Low Confidence**: Claims regarding zero-latency overhead and complete operational transparency in production environments without external dependencies

## Next Checks
1. Deploy A2AS in a production environment with real user interactions to validate performance claims and identify any operational constraints not apparent in controlled testing
2. Conduct stress testing with advanced, multi-vector attack scenarios to assess framework resilience against sophisticated adversarial techniques
3. Benchmark the actual latency impact across different agent architectures and deployment scales to verify the claimed zero-overhead performance