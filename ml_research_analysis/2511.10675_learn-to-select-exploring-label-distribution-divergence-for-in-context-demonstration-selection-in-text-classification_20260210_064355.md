---
ver: rpa2
title: 'Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration
  Selection in Text Classification'
arxiv_id: '2511.10675'
source_url: https://arxiv.org/abs/2511.10675
tags:
- label
- test
- performance
- demonstrations
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of selecting effective in-context
  demonstrations for text classification using large language models (LLMs). Most
  existing methods focus on semantic similarity but often neglect label distribution
  alignment, which can lead to poor performance, especially in ambiguous or noisy
  scenarios.
---

# Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification

## Quick Facts
- arXiv ID: 2511.10675
- Source URL: https://arxiv.org/abs/2511.10675
- Reference count: 12
- The paper proposes TopK + Label Distribution Divergence (L2D), a two-stage demonstration selection method that achieves an average accuracy improvement of 2.11% over the best prior method on seven text classification benchmarks.

## Executive Summary
This paper addresses the problem of selecting effective in-context demonstrations for text classification using large language models (LLMs). Most existing methods focus on semantic similarity but often neglect label distribution alignment, which can lead to poor performance, especially in ambiguous or noisy scenarios. The authors propose TopK + Label Distribution Divergence (L2D), a two-stage demonstration selection method. First, it retrieves the TopK semantically similar demonstrations using a retrieval model. Then, it fine-tunes a small language model (SLM) to estimate label probability distributions for both the test input and candidate demonstrations. The method calculates label distribution divergence using Jensen-Shannon divergence to select demonstrations that are both semantically similar and label-aligned. Extensive experiments on seven text classification benchmarks show that L2D consistently outperforms existing baselines, achieving an average accuracy improvement of 2.11% over the best prior method.

## Method Summary
The method uses a two-stage approach: (1) Semantic Retrieval: retrieves K=30 semantically similar demonstrations using gte-base-en-v1.5 embeddings, and (2) Label Divergence Estimation: fine-tunes an SLM (e.g., RoBERTa-base) on training data to predict label probability distributions for both test inputs and candidates. Jensen-Shannon divergence is computed between these distributions, and demonstrations are re-ranked using a hybrid score combining semantic similarity and label alignment (α=0.5 by default). The top 8 demonstrations are then used for 8-shot in-context learning with LLMs.

## Key Results
- L2D consistently outperforms existing baselines across seven text classification benchmarks
- Average accuracy improvement of 2.11% over the best prior method
- Optimal α range is 0.4-0.6, balancing semantic similarity and label alignment
- Higher SLM classification accuracy correlates with improved LLM ICL performance
- Candidate pool size K=30 is optimal; larger pools degrade performance

## Why This Works (Mechanism)

### Mechanism 1: Label Distribution Alignment via SLM-Guided Re-ranking
Demonstrations with label distributions aligned to the test input improve ICL performance beyond semantic similarity alone. A fine-tuned SLM predicts soft label probability distributions for both test inputs and candidate demonstrations. Jensen-Shannon divergence quantifies distributional mismatch; demonstrations with lower divergence (higher alignment) are prioritized in re-ranking. Core assumption: labels are not entirely independent of instances; semantically similar texts can carry contradictory or ambiguous label signals, especially in noisy data.

### Mechanism 2: Hybrid Scoring Balancing Semantic and Distributional Signals
Jointly scoring semantic similarity and label alignment (via tunable α) yields more robust demonstration selection than either signal alone. The final hybrid score S^hybrid = α · S^text + (1−α) · S^label combines cosine similarity from the retriever with JS-based label alignment. Empirically, α ∈ [0.4, 0.6] (default 0.5) provides optimal balance. Core assumption: neither semantic similarity nor label alignment is sufficient alone; both contribute orthogonal information for effective demonstrations.

### Mechanism 3: SLM Quality Transfer to LLM Performance
Higher SLM classification accuracy correlates with improved LLM ICL performance when using L2D. More accurate SLMs produce better-calibrated label distributions, which in turn yield more informative divergence scores for re-ranking. The SLM acts as a label-aware filter that transfers its task understanding to the LLM via demonstration selection. Core assumption: the SLM's label distribution estimates meaningfully capture task-relevant structure that the LLM can exploit.

## Foundational Learning

- **Jensen-Shannon Divergence**: Quantifies symmetric, bounded divergence between probability distributions; enables comparing test input and demonstration label distributions on a [0,1] scale. Quick check: Can you explain why JSD is preferred over raw KL divergence when both distributions have unknown or sparse support?

- **In-Context Learning (ICL) Sensitivity**: Motivates the entire problem—ICL performance varies significantly with demonstration choice and ordering; understanding this sensitivity is prerequisite. Quick check: What prior findings (Min et al., 2022) suggest about the role of correct vs. random labels in demonstrations, and how does L2D's robustness to label noise interact with those findings?

- **Retrieval-Augmented Demonstration Selection**: TopK semantic retrieval is the first stage; understanding embedding-based similarity (e.g., gte-base-en-v1.5) is required. Quick check: How does cosine similarity between embeddings relate to semantic similarity, and what are its known failure modes for adversarial or ambiguous inputs?

## Architecture Onboarding

- **Component map**: Semantic Retriever -> SLM Fine-tuning Module -> Label Divergence Calculator -> Hybrid Scoring Engine -> Prompt Assembler
- **Critical path**: SLM fine-tuning quality → accurate label distributions → meaningful JSD scores → effective re-ranking. If the SLM is poorly calibrated or undertrained, the entire reranking signal degrades.
- **Design tradeoffs**:
  - Candidate pool size (K=30): Larger pools introduce semantic noise that even L2D cannot fully filter; smaller pools may miss aligned demonstrations. Paper finds K=30 optimal; K=50 degrades performance.
  - α parameter (0.5 default): Balances semantic vs. label signals. Extreme α (0 or 1) ablates one component and hurts performance.
  - SLM choice: Larger/more capable SLMs (DeBERTa-v3-base) improve results but add fine-tuning cost.
- **Failure signatures**:
  - SLM accuracy < ~84% on validation set correlates with diminished LLM gains (Table 3).
  - Excessively large candidate pools (K=50+) introduce noise, slightly reducing accuracy and increasing latency.
  - Out-of-domain (OOD) demonstration pools: L2D still helps but with smaller margins (~3-4% gain over baselines).
- **First 3 experiments**:
  1. Replicate on SST-2 with Qwen2.5-7B-Instruct: Use RoBERTa-base as SLM, K=30, α=0.5, 8-shot. Verify ~96.5% accuracy vs. TopK baseline (~96.0%).
  2. Ablate α on Subj dataset: Run α ∈ {0.0, 0.3, 0.5, 0.7, 1.0}. Confirm performance degradation at extremes and optimal zone near 0.5.
  3. Swap SLM for DeBERTa-v3-base: Fine-tune DeBERTa on same training split; compare LLM accuracy gains vs. RoBERTa baseline on CR and SST-5 to validate positive correlation claim.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Label Distribution Divergence (L2D) framework be effectively adapted for generative tasks, such as text summarization or machine translation? The authors state in the "Limitations and future work" section: "Future work will explore the applicability of our approach to other tasks, such as text generation, to further examine its generalizability." This remains unresolved because the current L2D method relies on calculating Jensen-Shannon divergence between fixed label probability distributions generated by a classifier, while generative tasks typically lack the discrete, finite label spaces required for this specific divergence calculation.

### Open Question 2
Does the performance improvement provided by L2D persist or diminish when applied to LLMs with significantly larger parameter scales (e.g., >70B parameters)? The authors acknowledge: "The selection of LLMs is constrained to model scales between 2B and 14B due to limited computational resources." It is unclear if the guidance from a small, fine-tuned language model (SLM) remains beneficial as the inference model's intrinsic reasoning capabilities and general knowledge far exceed those of the guide model.

### Open Question 3
Is the standard TopK semantic retrieval method the optimal initial filter for the L2D re-ranking stage? The authors note: "While our study primarily investigates the impact of label distribution divergence... part of the strong performance can also be attributed to the underlying TopK retrieval mechanism... A more thorough evaluation of the retrieval component is left for future work." The two-stage process might miss optimal demonstrations that are semantically dissimilar but distributionally aligned if they are filtered out by the TopK stage before the L2D calculation occurs.

### Open Question 4
How sensitive is L2D to the size and quality of the training data available for fine-tuning the Small Language Model (SLM)? The paper demonstrates a positive correlation between SLM accuracy and LLM performance, but does not analyze the lower bound of data required to achieve a useful SLM or how noisy SLM predictions impact the final selection quality.

## Limitations
- Method validated only on seven English text classification benchmarks; performance on multilingual, low-resource, or long-document domains remains untested
- Effectiveness tightly coupled to quality of semantic retriever (gte-base-en-v1.5); poor retrieval embeddings could undermine even perfect label distribution estimation
- Fixed α=0.5 and K=30 settings were empirically optimal on these datasets but may not generalize; cross-dataset hyperparameter tuning is not explored

## Confidence
- **High Confidence**: The core two-stage retrieval + reranking pipeline is well-specified and reproducible. The positive correlation between SLM accuracy and LLM gains is empirically supported within tested settings.
- **Medium Confidence**: The hybrid scoring mechanism and optimal α range (0.4-0.6) are empirically justified but may not hold in out-of-distribution or highly ambiguous domains.
- **Low Confidence**: Claims about robustness to label noise and out-of-domain (OOD) performance are supported by limited ablation; systematic OOD evaluation is absent.

## Next Checks
1. **Cross-Lingual Transfer Test**: Apply L2D to a multilingual text classification dataset (e.g., XNLI) using the same retriever and SLM setup. Measure whether label distribution divergence remains effective across languages.

2. **Retrieval Quality Ablation**: Replace gte-base-en-v1.5 with a weaker or domain-specific retriever (e.g., sentence-BERT trained on niche data). Quantify degradation in L2D performance to isolate retrieval vs. reranking contributions.

3. **SLM-Calibration Stress Test**: Intentionally miscalibrate the SLM (e.g., temperature scaling >1.0) to produce overconfident or underconfident label distributions. Measure impact on JSD-based reranking quality and final LLM accuracy.