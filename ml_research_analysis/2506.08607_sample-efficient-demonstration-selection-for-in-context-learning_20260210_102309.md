---
ver: rpa2
title: Sample Efficient Demonstration Selection for In-Context Learning
arxiv_id: '2506.08607'
source_url: https://arxiv.org/abs/2506.08607
tags:
- case
- selection
- arms
- answer
- exemplars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CASE proposes a sample-efficient exemplar selection method for
  in-context learning that formulates the task as a top-m arms identification problem
  in stochastic linear bandits. It maintains a shortlist of challenger arms and selectively
  samples from them to reduce the number of LLM evaluations.
---

# Sample Efficient Demonstration Selection for In-Context Learning

## Quick Facts
- arXiv ID: 2506.08607
- Source URL: https://arxiv.org/abs/2506.08607
- Authors: Kiran Purohit; V Venktesh; Sourangshu Bhattacharya; Avishek Anand
- Reference count: 40
- Key outcome: CASE achieves up to 7× speedup and 87% reduction in LLM calls while maintaining/improving task performance

## Executive Summary
This paper introduces CASE, a sample-efficient method for exemplar selection in in-context learning that formulates the problem as a top-m arms identification task in stochastic linear bandits. The approach maintains a shortlist of challenger arms and selectively samples from them to minimize LLM evaluations while identifying high-quality demonstration subsets. By modeling exemplar subset rewards as linear functions of BERT-based similarity scores, CASE significantly reduces computational overhead while maintaining or improving task performance across multiple reasoning datasets.

## Method Summary
CASE transforms exemplar selection into a top-m arms identification problem in stochastic linear bandits, where each arm represents a candidate subset of demonstrations. The method maintains a shortlist of challenger arms and uses a selective exploration strategy to minimize LLM evaluations. It models the reward of each exemplar subset as a linear function of BERT-based similarity scores between demonstrations, creating a stochastic linear bandit setting. During the exploration phase, the algorithm samples from the shortlist to identify the top-performing arms, then proceeds to exploit the best-performing exemplar subset for task completion.

## Key Results
- Achieves up to 7× speedup in runtime compared to state-of-the-art exemplar selection methods
- Reduces LLM calls by up to 87% (7× fewer evaluations) while maintaining or improving performance
- Maintains or improves task performance across multiple reasoning datasets including GSM8K, MATH, and StrategyQA

## Why This Works (Mechanism)
CASE works by framing exemplar selection as a bandit problem where the agent must efficiently identify the best demonstration subsets (arms) with minimal evaluations. The key insight is that by modeling rewards as linear functions of BERT-based similarity scores, the algorithm can leverage the structure of the problem to make informed decisions about which arms to explore. The selective exploration strategy maintains a shortlist of promising candidates and only evaluates a subset of these, dramatically reducing the number of expensive LLM calls needed while still identifying high-quality exemplars.

## Foundational Learning
- Stochastic Linear Bandits: A bandit framework where rewards are modeled as linear functions of feature vectors; needed to handle the exploration-exploitation tradeoff efficiently
- Top-m Arms Identification: A variant of the best arm identification problem where the goal is to find the m best arms rather than just the single best; needed to handle scenarios where multiple high-quality exemplar subsets may exist
- BERT-based Similarity Embeddings: Dense vector representations capturing semantic similarity between demonstrations; needed to model the relationship between exemplar similarity and task performance

Quick check: Verify that the linear reward assumption holds by comparing performance when using non-linear similarity metrics or different embedding models.

## Architecture Onboarding

Component Map: Demonstration Pool -> BERT Similarity Scores -> Linear Reward Model -> Stochastic Linear Bandit -> Selective Exploration -> Best Exemplar Subset

Critical Path: The algorithm starts with a pool of demonstrations, computes BERT-based similarity scores between them, models rewards as linear functions of these scores, runs the stochastic linear bandit algorithm with selective exploration, and outputs the identified best exemplar subset for in-context learning.

Design Tradeoffs: The approach trades off between exploration (finding potentially better exemplar subsets) and exploitation (using known good subsets) through the bandit framework. The linear reward assumption simplifies the problem but may not capture all nuances of exemplar quality. The shortlist mechanism reduces computational cost but may miss some potentially good arms.

Failure Signatures: Performance degradation when the linear reward assumption breaks down (e.g., non-linear relationships between similarity and performance). Poor results when demonstration diversity is crucial but the algorithm favors similarity-based selection. Computational inefficiency if the shortlist becomes too large relative to the full arm set.

Exactly 3 first experiments:
1. Test with varying numbers of demonstrations in the pool to determine scaling behavior
2. Evaluate performance using different embedding models (e.g., sentence-BERT, CLIP) to test robustness
3. Run ablation studies removing the selective exploration mechanism to quantify its contribution to efficiency gains

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the formal upper bound on the regret for the selective exploration strategy used in CASE?
- Basis in paper: Section 3.4 states, "While we postpone a rigorous derivation of the regret bound for CASE to a later study," and the conclusion lists deriving this bound as future work.
- Why unresolved: While the paper adopts the sample complexity bound from the GIFA framework (Theorem 1), it relies on the assumption that the selective exploration strategy achieves low average regret without providing a formal proof for this specific sampling mechanism.
- What evidence would resolve it: A formal proof derivation establishing the regret bound for the Challenger Arm Sampling algorithm, specifically accounting for the dynamic reduction of the search space from the full arm set to the shortlist N_t.

### Open Question 2
- Question: How robust is the linear reward assumption when using different embedding models or facing non-linear reward structures?
- Basis in paper: Section 3.2 assumes the reward ρ(a) can be modeled as a linear function of BERT-based similarity scores (α^T x_a). Section 3.4 notes this rests on the assumption that the average regret is upper bounded, which relies on the linear model accurately representing the exemplar subset quality.
- Why unresolved: The paper relies on a stochastic linear bandit setting, but if the relationship between exemplar similarity and LLM performance is non-linear or dependent on the specific embedding model used, the algorithm's theoretical guarantees and empirical efficiency may degrade.
- What evidence would resolve it: An empirical analysis measuring the performance divergence of CASE when using non-linear feature mappings or distinct embedding architectures to verify if the linear approximation holds across diverse semantic spaces.

### Open Question 3
- Question: Can CASE be effectively adapted for adaptive retrieval scenarios and re-ranking tasks?
- Basis in paper: The conclusion explicitly states, "In the future, we also plan to extend and apply CASE to adaptive retrieval methods... Specifically, using the challenger arm sampling techniques we could carefully choose relevant documents to re-rank..."
- Why unresolved: The current work focuses strictly on static exemplar selection for reasoning tasks. It has not been tested in retrieval-augmented generation (RAG) or search scenarios where the "arms" are documents retrieved dynamically for a query rather than pre-defined exemplar subsets.
- What evidence would resolve it: Experimental results applying the challenger arm sampling framework to standard information retrieval or RAG benchmarks, demonstrating efficiency gains in document re-ranking compared to standard retrieval heuristics.

## Limitations
- Assumes linear relationship between BERT similarity and exemplar subset rewards, which may not hold across all task types or domains
- Focuses on finding the single best exemplar subset rather than leveraging diverse demonstration types
- Does not extensively explore performance with different embedding models or non-linear reward structures

## Confidence
High confidence: Experimental results showing 7× speedup and 87% reduction in LLM calls while maintaining/improving performance; well-defined mathematical formulation of the stochastic linear bandit problem
Medium confidence: Generalizability to different types of reasoning tasks beyond those tested; linear relationship between BERT similarity and task performance may not hold universally
Medium confidence: Scalability to very large demonstration pools or different types of language models; effectiveness on different model scales or architectures not extensively explored

## Next Checks
1. Test the approach on non-reasoning tasks (e.g., classification, summarization) to validate whether the linear relationship between BERT similarity and performance holds across different task types
2. Evaluate the method's performance when using different embedding models beyond BERT (e.g., sentence-BERT, CLIP) to assess the robustness of the similarity-based reward modeling
3. Conduct ablation studies varying the size of the demonstration pool to determine how the approach scales with larger numbers of potential exemplars and whether the computational benefits remain proportional