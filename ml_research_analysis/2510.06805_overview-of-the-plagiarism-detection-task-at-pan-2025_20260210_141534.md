---
ver: rpa2
title: Overview of the Plagiarism Detection Task at PAN 2025
arxiv_id: '2510.06805'
source_url: https://arxiv.org/abs/2510.06805
tags:
- plagiarism
- dataset
- detection
- approaches
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the 2025 PAN plagiarism detection task, which
  aims to identify automatically generated textual plagiarism in scientific articles.
  A large-scale dataset of 78,038 document pairs was created using three large language
  models (LLaMA, DeepSeek-R1, Mistral) to generate paraphrased plagiarism from arXiv
  articles.
---

# Overview of the Plagiarism Detection Task at PAN 2025

## Quick Facts
- **arXiv ID**: 2510.06805
- **Source URL**: https://arxiv.org/abs/2510.06805
- **Reference count**: 35
- **Primary result**: All approaches using semantic similarity achieved moderate performance (up to 0.8 recall, 0.5 precision) on LLM-paraphrased plagiarism but failed to generalize to traditional plagiarism datasets.

## Executive Summary
This paper presents the 2025 PAN plagiarism detection task, which aims to identify automatically generated textual plagiarism in scientific articles. A large-scale dataset of 78,038 document pairs was created using three large language models (LLaMA, DeepSeek-R1, Mistral) to generate paraphrased plagiarism from arXiv articles. The task required participants to detect and align plagiarized paragraphs with their sources. Four submissions and four baselines participated, with all approaches using semantic similarity based on embedding vectors. The best-performing approach achieved up to 0.8 recall and 0.5 precision, though most methods showed poor generalizability when tested on the 2015 dataset. The results suggest that while semantic similarity approaches work well on the new dataset, they struggle with robustness across different data distributions.

## Method Summary
The 2025 PAN plagiarism detection task focused on identifying LLM-generated paraphrased plagiarism in scientific articles. The dataset was constructed by selecting arXiv articles, generating synthetic plagiarized versions using three different LLMs, and creating document pairs with labeled plagiarism annotations. Participants were tasked with detecting plagiarized paragraphs and aligning them with their source documents. The evaluation used the plagdet metric, which combines precision, recall, and granularity. All submitted approaches and baselines relied on semantic similarity through embedding vectors, with variations in embedding models (SPECTER, Linq-Embed-Mistral, E5, GloVe) and processing strategies (paragraph vs. sentence level, single-stage vs. cascaded filtering).

## Key Results
- Semantic similarity approaches achieved up to 0.8 recall and 0.5 precision on the PAN25 dataset
- Linq-Embed-Mistral baseline outperformed all submissions, indicating specialized retrieval models suit plagiarism detection well
- Most approaches failed to generalize to the 2015 PAN dataset, showing severe performance degradation
- High precision methods (0.67 micro) achieved very low recall (0.16 micro) due to aggressive filtering
- Some approaches showed higher recall on "altered" (non-plagiarized LLM content) than on actual plagiarism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity via embedding cosine similarity can detect LLM-paraphrased plagiarism with moderate effectiveness on matched distributions.
- Mechanism: Documents are segmented into paragraphs or sentences → each segment encoded as a dense embedding vector → pairwise cosine similarity computed between source and suspicious document segments → threshold-based classification identifies plagiarized alignments → adjacent detections merged into contiguous spans.
- Core assumption: LLM paraphrasing preserves semantic content detectable through vector space proximity even when surface lexical features are obscured.
- Evidence anchors:
  - [abstract] "naive semantic similarity approaches based on embedding vectors provide promising results of up to 0.8 recall and 0.5 precision"
  - [section 3.1] Baselines use "cosine similarity based on Linq, Qwen2 7B instruct, and Llama-3.3 70B Instruct" with thresholds "determined by calculating the ideal cut-offs on the training split"
  - [corpus] Insufficient external validation—corpus neighbors address AI text detection generally, not embedding-based plagiarism specifically
- Break condition: Distribution shift between training and deployment data causes significant degradation (PAN25→PAN12 evaluation shows near-random performance for most systems).

### Mechanism 2
- Claim: Embedding models optimized for text retrieval tasks may outperform general-purpose LLM embeddings for plagiarism alignment.
- Mechanism: Use Linq-Embed-Mistral (fine-tuned for retrieval tasks) rather than general instruction-tuned models → compute same cosine similarity pipeline → higher alignment accuracy observed.
- Core assumption: Plagiarism detection and text retrieval share similar requirements for semantic matching under paraphrase variation.
- Evidence anchors:
  - [section 3.1] "Linq outperforms all submissions, indicating that specialized models for the text retrieval task might suit the task for plagiarism detection particularly well"
  - [Table 2] Linq achieves micro-plagdet 0.61 vs. Qwen2 (0.39) and Llama (0.28)
  - [corpus] No direct corpus evidence on retrieval-optimized embeddings for plagiarism detection
- Break condition: Assumption: performance advantage may not transfer to non-scientific domains or different paraphrase strategies.

### Mechanism 3
- Claim: Two-stage cascaded filtering can preserve high precision but risks aggressive recall reduction.
- Mechanism: Stage 1 filters candidate pairs using TF-IDF and Jaccard similarity → Stage 2 applies BERT classifier fine-tuned on training data → only pairs passing both stages flagged as plagiarism.
- Core assumption: Lexical similarity provides computationally efficient coarse filter before expensive neural classification.
- Evidence anchors:
  - [section 3.2.3] jrluo approach: "first aligned pairs by using TF-IDF vector similarities. For each pair, he calculated the word-based Jaccard similarity and discarded all pairs below a given threshold. All remaining sentence pairs were classified as plagiarism or genuine by a BERT classifier fine-tuned on the training data"
  - [Table 2] jrluo achieves highest precision (0.67 micro) but lowest recall (0.16 micro)
  - [corpus] Weak corpus support—no neighbor papers validate cascaded architectures for plagiarism specifically
- Break condition: Aggressive first-stage filtering discards true positives before classification stage.

## Foundational Learning

- **Embedding vectors and semantic representation**
  - Why needed here: All submitted systems and baselines operate by encoding text segments into dense vectors; understanding what these vectors capture is essential for debugging alignment failures.
  - Quick check question: Given two paraphrased paragraphs with low token overlap, would you expect their SPECTER embeddings to have high or low cosine similarity, and why?

- **Cosine similarity vs. lexical overlap metrics**
  - Why needed here: The dataset construction uses weighted combinations (50% SPECTER semantic, 40% TF-IDF lexical, 10% section title); knowing when each fails informs threshold tuning.
  - Quick check question: If a plagiarized paragraph replaces technical terminology with synonyms, which metric (cosine on embeddings or TF-IDF overlap) would degrade more?

- **Precision-recall tradeoff and plagdet metric**
  - Why needed here: High precision (0.67) with low recall (0.16) versus balanced 0.5/0.5 represents different operational risks; plagdet combines F1 with granularity penalty.
  - Quick check question: For academic integrity screening, would you prioritize high precision or high recall, and what are the consequences of each error type?

## Architecture Onboarding

- **Component map:**
  - Document parser (ar5iv HTML5 format → structured paragraphs) -> Segment splitter (paragraph-level or sentence-level tokenization) -> Embedding encoder (SPECTER, Linq-Embed-Mistral, E5, GloVe, etc.) -> Similarity matrix computer (pairwise cosine similarity) -> Threshold optimizer (grid search on training split) -> Span merger (combine adjacent detections based on proximity/semantic coherence) -> Evaluation layer (plagdet, recall, precision, granularity)

- **Critical path:**
  1. Load document pair (S, P) from dataset
  2. Split both documents into segments
  3. Encode all segments as embedding vectors
  4. Compute N×M similarity matrix between P-segments and S-segments
  5. For each P-segment, identify highest-similarity S-segment; apply threshold
  6. Merge consecutive positive detections into contiguous spans
  7. Output character-level alignments for evaluation

- **Design tradeoffs:**
  - Paragraph vs. sentence splitting: Paragraphs simpler and match dataset construction; sentences finer-grained but require post-hoc merging
  - General vs. specialized embeddings: Specialized (Linq) shows higher performance but may overfit to scientific domain
  - Threshold placement: Conservative thresholds yield high precision/low recall; permissive thresholds increase recall but false positives on "altered" cases
  - Single-stage vs. cascaded: Cascaded filtering reduces computation but risks discarding true positives early

- **Failure signatures:**
  - High recall on "altered" (non-plagiarized LLM content) cases: model conflates LLM-generation with plagiarism (Llama baseline shows 2× higher recall on altered vs. plagiarized)
  - High granularity score (>1.5): duplicate detections for same source paragraph, indicates merging logic failure
  - Sharp cross-dataset degradation: performance collapse on PAN12 indicates threshold overfitting or embedding distribution mismatch

- **First 3 experiments:**
  1. **Baseline replication:** Implement paragraph-level Linq-Embed-Mistral encoding with threshold optimized on training split; target micro-plagdet ≈0.6
  2. **Cross-dataset generalization test:** Train threshold on PAN25 training set, evaluate on held-out PAN25 test and PAN12 corpus; quantify generalization gap
  3. **False positive profiling:** Measure recall separately on plagiarized vs. altered cases; if altered recall > plagiarized recall, threshold or embedding is detecting LLM-ness rather than plagiarism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can plagiarism detection methods be developed that generalize effectively across both modern LLM-generated plagiarism and traditional human-paraphrased plagiarism?
- Basis in paper: [explicit] The authors note that most approaches "underperform significantly on the 2015 dataset, indicating a lack in generalizability" and identify the need for approaches that are robust against changes in data distribution.
- Why unresolved: Current semantic similarity approaches are highly tuned to the specific characteristics of the new LLM-generated dataset and fail to maintain performance on the older PAN-12 benchmark.
- Evidence: A model that achieves comparable PlagDet scores on both the 2025 dataset and the 2015 dataset without specific fine-tuning for each.

### Open Question 2
- Question: How can detection systems distinguish between unauthorized plagiarism and authorized "altered" text (legitimate LLM-assisted writing) to minimize false accusations?
- Basis in paper: [inferred] The authors highlight that baselines like Llama and Qwen2 exhibit higher recall on "altered" cases than on actual plagiarism, making a detected case "significantly more likely to be a wrong accusation."
- Why unresolved: Current semantic similarity metrics cannot differentiate between legitimate paraphrasing (with citation) and illegitimate plagiarism when the textual similarity is high in both scenarios.
- Evidence: A system that maintains high recall on plagiarized segments while achieving near-zero recall on the "altered" (non-plagiarized but LLM-generated) subset of the dataset.

### Open Question 3
- Question: Does generating entire synthetic articles from source texts, rather than replacing paragraphs in existing documents, promote a greater diversity of detection methodologies?
- Basis in paper: [explicit] The authors state that in future iterations, they will "generate a new article by paraphrasing... [which] should also promote a larger variety of detection approaches" to overcome the limitations of the current paragraph-swapping setup.
- Why unresolved: The current dataset construction allows "naive semantic similarity" to work too well, potentially discouraging the development of more sophisticated structural or reasoning-based detectors.
- Evidence: Future task submissions that utilize structural analysis or reasoning chain detection rather than relying primarily on embedding vector similarity.

### Open Question 4
- Question: How can plagiarism detection tasks be framed to capture "ideological reuse" or "copying of reasoning chains" rather than just textual alignment?
- Basis in paper: [explicit] The authors argue that future iterations "must therefore focus more on proper citations and the actual case of idealogical reuse or copying of reasoning-chains to stay relevant."
- Why unresolved: As LLMs generate novel text that passes peer review, simple text alignment becomes less indicative of misconduct, requiring new metrics for idea theft.
- Evidence: A dataset and evaluation protocol that successfully identifies plagiarism even when the text is completely rewritten but the logical structure is copied without attribution.

## Limitations
- **Distribution shift vulnerability**: All approaches exhibit severe performance degradation when applied to the 2015 PAN dataset, suggesting the current methods are overfitting to the specific LLM-paraphrasing style used in the 2025 corpus rather than learning generalizable plagiarism detection patterns.
- **False positive correlation with AI detection**: The Llama baseline's significantly higher recall on "altered" cases compared to actual plagiarized cases indicates the embeddings may be detecting LLM-generated text in general rather than identifying specific source-target relationships.
- **Precision-recall tradeoff limitations**: The best-performing approaches achieve at most 0.5 precision, meaning half of all detected alignments are false positives, requiring substantial manual review overhead.

## Confidence
- **High confidence**: Semantic similarity approaches work on matched distributions (PAN25 training→test), Linq-Embed-Mistral shows strong performance, cascaded filtering preserves precision
- **Medium confidence**: Embedding models optimized for retrieval tasks outperform general-purpose embeddings for plagiarism detection
- **Low confidence**: Methods generalize across different data distributions and paraphrasing strategies

## Next Checks
1. **Cross-paraphrase strategy validation**: Test the same approaches on the same arXiv source documents but paraphrased using different LLMs (GPT-4, Claude) or paraphrasing strategies (back-translation, controlled generation) to assess robustness to paraphrase variation.

2. **Domain transfer experiment**: Apply the PAN25-trained models to plagiarism detection in non-scientific domains (news articles, Wikipedia, student essays) to evaluate whether scientific embedding models transfer beyond their training domain.

3. **Human evaluation of false positives**: Conduct expert annotation of top false positive detections to determine whether embeddings are identifying LLM-generated text in general or actual plagiarism, separating the AI detection problem from source-alignment detection.