---
ver: rpa2
title: Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled
  Gradient Descent
arxiv_id: '2502.11740'
source_url: https://arxiv.org/abs/2502.11740
tags:
- visual
- arxiv
- mdgd
- forgetting
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses visual knowledge forgetting in multimodal
  large language models (MLLMs) during instruction tuning, where text-driven training
  leads to degradation of pre-trained visual understanding. The authors propose Modality-Decoupled
  Gradient Descent (MDGD), which uses effective rank to quantify visual representation
  richness and frames forgetting as excessive compression under the information bottleneck
  principle.
---

# Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled Gradient Descent

## Quick Facts
- arXiv ID: 2502.11740
- Source URL: https://arxiv.org/abs/2502.11740
- Reference count: 28
- This paper addresses visual knowledge forgetting in multimodal large language models (MLLMs) during instruction tuning, where text-driven training leads to degradation of pre-trained visual understanding.

## Executive Summary
This paper addresses visual knowledge forgetting in multimodal large language models (MLLMs) during instruction tuning, where text-driven training leads to degradation of pre-trained visual understanding. The authors propose Modality-Decoupled Gradient Descent (MDGD), which uses effective rank to quantify visual representation richness and frames forgetting as excessive compression under the information bottleneck principle. MDGD disentangles visual learning from task-specific alignment by regulating gradient updates to preserve visual representation richness while enabling task adaptation. The method also includes a memory-efficient fine-tuning variant using gradient masking for parameter-efficient fine-tuning. Experiments on LLaVA-1.5 and MiniCPM across various downstream tasks demonstrate that MDGD effectively mitigates visual forgetting while maintaining strong task performance, with the gradient masking variant achieving comparable results while fine-tuning only 10% of parameters.

## Method Summary
MDGD addresses visual knowledge forgetting by quantifying representation richness through effective rank and decoupling visual preservation from task adaptation. The method computes an auxiliary alignment loss between pre-trained and current visual encodings, derives orthogonal task gradients by projecting away visual drift directions, and applies gradient masking to enable parameter-efficient fine-tuning. The visual alignment loss L_v measures the L1 distance between pre-trained and current visual representations. Orthogonal gradients are computed by projecting task gradients away from the visual drift direction. The method can be applied in full form or with gradient masking (MDGD-GM) to fine-tune only the top α% of parameters by gradient alignment similarity. Experiments use LLaVA-1.5-7B and MiniCPM-V-2.8B models, fine-tuning last layers plus adapters while maintaining a frozen pre-trained reference model for visual alignment.

## Key Results
- MDGD effectively mitigates visual forgetting in MLLMs while maintaining strong task performance
- MDGD-GM variant achieves comparable results while fine-tuning only 10% of parameters (124M vs 1.2B total)
- Effective rank reduction correlates with visual knowledge degradation during standard fine-tuning
- Both LLaVA-1.5 and MiniCPM benefit from MDGD across various downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Effective Rank Quantification of Visual Forgetting
Visual forgetting manifests as a reduction in the effective rank of visual representations during text-driven instruction tuning. Effective rank measures the "richness" of representations via the entropy of singular value distributions. Lower erank indicates aggressive compression into fewer dimensions. The paper empirically observes erank(Z^v_θ) < erank(Z^v_0) after standard fine-tuning, meaning visual representations span a narrower subspace. Visual knowledge is encoded in the diversity of representation dimensions; compressing this space degrades visual capabilities even if task accuracy appears maintained.

### Mechanism 2: Modality-Decoupled Gradient Orthogonalization
Decoupling task optimization gradients from visual understanding drift prevents over-compression of visual representations. The method computes auxiliary loss L_v(φ,θ) = ||μ(X^v|φ) - π(X^v|θ)||_1 between pre-trained and current visual encodings. It derives orthogonal task gradients by projecting away the visual drift direction: ḡ_θ = ∇_θ L^vl(θ) - [(∇_θ L^vl(θ)^T h_θ) / ||h_θ||²] · h_θ. A regularization term projects current gradients toward the pre-trained model's orthogonal direction. Gradient components aligned with visual drift harm retention; orthogonal components preserve visual knowledge while enabling task adaptation.

### Mechanism 3: Gradient Masking for Parameter-Efficient Fine-Tuning
Selective parameter updates based on gradient alignment achieve comparable results while fine-tuning only ~10% of parameters. Creates binary mask M_g̃θ = 1[ḡ_θ^T ḡ_φ / (||ḡ_φ|| ||ḡ_θ||) ≥ T_α] selecting top α% parameters with highest cosine similarity between current and pre-trained orthogonal gradients. Only these parameters receive gradient updates. Parameters with high gradient alignment between pre-trained and current models contribute to both task adaptation and visual preservation simultaneously.

## Foundational Learning

- **Information Bottleneck Principle**
  - Why needed here: The paper frames visual forgetting as the IB compression trade-off: optimizing for I(y;Z) (task prediction) while inadvertently minimizing I(X^v;Z) (visual information retention).
  - Quick check question: Can you explain why text-driven supervision tends to compress visual representations more than multimodal pre-training?

- **Gradient Surgery / Orthogonal Projection**
  - Why needed here: The core mechanism requires projecting gradients to be orthogonal to visual drift directions, a technique derived from multi-task gradient surgery literature.
  - Quick check question: Given two gradient vectors g_task and g_drift, how would you compute the component of g_task orthogonal to g_drift?

- **Effective Rank (Matrix Dimensionality)**
  - Why needed here: This is the primary metric for detecting and monitoring visual forgetting throughout training.
  - Quick check question: Why is effective rank (based on singular value entropy) preferred over simple matrix rank for measuring representation richness?

## Architecture Onboarding

**Component map:**
Pre-trained MLLM μ_φ (frozen reference) ──┐
                                          │ L_v comparison
Current MLLM π_θ (trainable) ─────────────┘
     │
     ├── Visual Encoder f → Visual Tokens X^v
     ├── LLM Backbone → Hidden States
     └── Projection Layers (fine-tuned)

**Critical path:**
1. Forward pass through both pre-trained (frozen) and current MLLM
2. Extract visual encodings from image token positions
3. Compute auxiliary alignment loss L_v between encodings
4. Compute task loss L^vl and derive orthogonal gradients via Eq. 6-7
5. Apply gradient masking (MDGD-GM) or projection (full MDGD)
6. Update only masked/projected parameters

**Design tradeoffs:**
- Full MDGD vs. MDGD-GM: Full version provides stronger regularization but requires 10× more parameters and incompatible with LoRA; MDGD-GM enables PEFT but relies on heuristic thresholding
- Masking ratio α: Lower values (10%) more memory-efficient but risk underfitting; higher values (50%) retain more expressiveness

**Failure signatures:**
- Task performance plateaus early → α too aggressive, critical parameters masked
- Visual metrics still degrading → L_v weight too low or gradient projection not applied
- T-SNE shows visual drift similar to baseline → pre-trained reference not being used correctly

**First 3 experiments:**
1. **Baseline comparison:** Fine-tune LLaVA-1.5 on Flickr30K with standard FT vs. MDGD vs. MDGD-GM; measure effective rank and pre-trained task retention (GQA, POPE)
2. **Ablation on visual alignment:** Run MDGD w/o L_v term to isolate contribution of explicit representation alignment vs. gradient orthogonalization alone
3. **Masking ratio sweep:** Compare α ∈ {5%, 10%, 30%, 50%} on OKVQA task; plot training loss curves and final H-score to find efficiency-performance sweet spot

## Open Questions the Paper Calls Out
The paper explicitly notes that while the approach is theoretically generalizable, extending it to "more diverse, free-form multimodal inputs remains an avenue for future research." The experiments are limited to visual and textual inputs, optimizing specifically for visual representation preservation.

## Limitations
- Unknown methodological parameters: Key hyperparameters including learning rate, batch size, or training epochs for fine-tuning experiments are not specified
- Sign function ambiguity: The formulation of λ(φ,θ) = sign(μ(X^v|φ) - π(X^v|θ)) is not fully specified
- Gradient masking heuristic: The threshold T_α for selecting the top α% parameters is described as heuristic without empirical justification

## Confidence
**High confidence**: The fundamental mechanism of using effective rank to quantify visual representation degradation is well-founded theoretically and empirically supported.

**Medium confidence**: The specific implementation details of MDGD-GM (gradient masking threshold, parameter selection) are described clearly but lack extensive ablation studies.

**Low confidence**: The theoretical connection between effective rank reduction and actual visual capability degradation, while plausible, lacks direct causal validation.

## Next Validation Checks
**Validation 1: Sensitivity analysis on masking threshold** - Systematically vary α from 5% to 50% on a single task (e.g., OKVQA) and measure both task performance and visual representation preservation metrics to identify the optimal trade-off point.

**Validation 2: Ablation on visual alignment loss** - Run experiments with MDGD but remove the L_v term to isolate whether gradient orthogonalization alone provides sufficient forgetting mitigation, or whether explicit visual alignment is necessary.

**Validation 3: Transfer to different visual encoder** - Apply MDGD to an MLLM using a different pre-trained visual encoder (e.g., CLIP-ViT-B/32 instead of CLIP-ViT-B/16) to verify the method's robustness across visual backbone variations.