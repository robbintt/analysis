---
ver: rpa2
title: 'Menta: A Small Language Model for On-Device Mental Health Prediction'
arxiv_id: '2512.02716'
source_url: https://arxiv.org/abs/2512.02716
tags:
- health
- mental
- task
- tasks
- menta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Menta, a small language model optimized for
  multi-task mental health prediction from social media text. The model is designed
  to overcome the computational and privacy limitations of large language models while
  maintaining high predictive accuracy.
---

# Menta: A Small Language Model for On-Device Mental Health Prediction

## Quick Facts
- arXiv ID: 2512.02716
- Source URL: https://arxiv.org/abs/2512.02716
- Authors: Tianyi Zhang; Xiangyuan Xue; Lingyan Ruan; Shiya Fu; Feng Xia; Simon D'Alfonso; Vassilis Kostakos; Ting Dang; Hong Jia
- Reference count: 33
- Primary result: 4B parameter model matches or exceeds 13B models on depression/stress classification while enabling on-device deployment

## Executive Summary
This paper presents Menta, a small language model optimized for multi-task mental health prediction from social media text. The model addresses computational and privacy limitations of large language models through LoRA-based fine-tuning on a 4B parameter backbone. Trained across six mental health classification tasks using a cross-dataset strategy and balanced accuracy-oriented loss function, Menta achieves 15.2% average accuracy improvement over non-fine-tuned baselines while being 3.25× smaller than 13B models. The model is successfully deployed on iPhone 15 Pro Max for real-time, privacy-preserving mental health monitoring with approximately 3GB RAM usage.

## Method Summary
Menta employs a LoRA-based fine-tuning framework on Qwen-3 (4B) using cross-dataset multi-task training across six mental health classification tasks. The model uses a combined loss function of cross-entropy and balanced accuracy surrogate, with task-specific sampling weights and class balancing. Training data comes from four datasets (Dreaddit, DepSeverity, SDCNL, CSSRS-Suicide) with a 72/8/20 train/val/test split. The model is quantized to GGUF format (Q4_K_M) and deployed via llama.cpp inference engine on iOS, achieving real-time performance with 4096-token context window and ~3GB RAM usage.

## Key Results
- Menta achieves 15.2% average accuracy improvement over best non-fine-tuned baselines
- 4B parameter Menta matches 13B Mental-Alpaca on stress classification (0.82 accuracy) while being 3.25× smaller
- Outperforms 13B models on depression and stress tasks with lower variance indicating greater robustness
- Successfully deployed on iPhone 15 Pro Max with ~3GB RAM usage and real-time inference capabilities
- Achieves highest balanced accuracy (0.564) across tasks compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA-based parameter-efficient fine-tuning enables a 4B parameter model to match or exceed 13B models on specific mental health tasks.
- Mechanism: Low-rank adaptation matrices are injected into transformer attention layers (query and value projections), allowing domain adaptation with ~0.1% trainable parameters while preserving the pretrained backbone's reasoning.
- Core assumption: Mental health language tasks rely on localized lexical, affective, and syntactic cues that can be captured through targeted weight modifications without requiring broader world knowledge.
- Evidence anchors: [Section 3.2] specifies LoRA configuration (r=16, α=32, dropout=0.05); [Table 1] shows Menta matching Mental-Alpaca (13B) on stress classification; related work supports SLM+LoRA capability on mental health tasks.

### Mechanism 2
- Claim: The balanced accuracy (BACC) surrogate loss explicitly penalizes class imbalance, improving minority-class performance across heterogeneous mental health tasks.
- Mechanism: A differentiable approximation of balanced accuracy uses margin-based sigmoid functions to compute soft true positive rates, with combined loss directly optimizing for fairness across classes rather than aggregate accuracy.
- Core assumption: The surrogate BACC loss gradient provides meaningful signals that generalize across tasks with different label distributions (e.g., 72.8% minimal vs. 7.9% severe imbalance).
- Evidence anchors: [Section 3.2] defines the BACC loss formulation; [Figure 6b] shows Menta achieving highest BACC (0.564) across tasks; novel contribution requiring independent validation.

### Mechanism 3
- Claim: Cross-dataset multi-task training with weighted sampling creates a unified model that generalizes better than task-specific single-task models.
- Mechanism: Six tasks from four datasets are combined with task-specific sampling weights (λt), allowing the shared backbone to learn transferable representations across stress, depression severity, and suicide risk classification.
- Core assumption: Mental health conditions share latent linguistic features (e.g., first-person pronouns, negative affect) that benefit from joint optimization despite different label schemas.
- Evidence anchors: [Section 3.2] describes unified multi-task dataset construction; [Figure 7] shows general Menta outperforming single-task variants with lower variance; Benton et al. (2017) provides indirect support for multi-task benefits.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables fine-tuning a 4B parameter model with minimal memory overhead, making on-device deployment feasible. Understanding rank (r=16), scaling factor (α=32), and target modules (query/value projections) is essential for reproducing results.
  - Quick check question: If LoRA rank were increased to 64, would you expect better or worse performance on Task 6? What tradeoff would this create for on-device deployment?

- **Balanced Accuracy and Class Imbalance**
  - Why needed here: Mental health datasets are inherently imbalanced (e.g., 72.8% minimal depression vs. 7.9% severe). Standard cross-entropy loss would bias predictions toward majority classes, making the BACC surrogate loss critical for clinical utility.
  - Quick check question: Why would a model achieving 85% accuracy on Task 3 (depression severity) potentially be clinically useless if evaluated only on accuracy?

- **On-Device Inference Constraints**
  - Why needed here: The deployment section requires understanding quantization (Q4_K_M), memory mapping, and latency metrics (TTFT, ITPS). These directly determine whether the model can run on a smartphone in real-time.
  - Quick check question: If RAM usage increases from 3GB to 4GB after changing quantization levels, what downstream effects would you expect on battery life and user experience?

## Architecture Onboarding

- **Component map:**
  Qwen-3 (4B) backbone -> LoRA adapters (r=16, α=32, dropout=0.05) on Q/V projections -> Combined CE + BACC surrogate loss -> Unified multi-task corpus -> GGUF quantization (Q4_K_M) -> llama.cpp iOS deployment

- **Critical path:**
  1. Dataset preparation and label alignment (Task 1-6 definitions)
  2. LoRA adapter initialization and training (multi-task weighted sampling)
  3. Model export to GGUF format with 4-bit quantization
  4. iOS integration via llama.cpp C++ bridge with Metal acceleration
  5. Runtime inference with 4096-token context window and KV-cache management

- **Design tradeoffs:**
  - Model size vs. task coverage: 4B parameters limit capacity for complex reasoning (Task 6 struggles) but enable on-device deployment; 13B models perform better on suicide tasks but require cloud inference
  - Quantization level vs. accuracy: Q4_K_M reduces RAM to ~3GB but may degrade performance on nuanced classifications; higher precision (Q8_0) improves accuracy but exceeds device constraints
  - Single unified model vs. task-specific ensemble: Unified model simplifies deployment and reduces storage (one 2.33GB file) but sacrifices specialization; ensembles would improve accuracy but multiply resource requirements

- **Failure signatures:**
  - Long input truncation: Posts >6000 characters are truncated; multi-speaker threads with formatting noise (Case 3) cause misclassification
  - Format noise sensitivity: HTML artifacts ("&gt;") and placeholder tokens ("Hyperactive behavior") act as spurious cues, biasing predictions
  - Capacity ceiling on fine-grained tasks: Task 6 (5-class suicide risk) achieves only 0.35 accuracy, suggesting the model cannot reliably distinguish subtle risk levels
  - Token budget exceeded: GGUF quantization with 4096-token context may drop critical context for long user histories (average 1783 tokens for Tasks 5-6)

- **First 3 experiments:**
  1. Ablate BACC loss: Train Menta with cross-entropy only to quantify the BACC contribution (compare Figure 6 results against CE-only baseline)
  2. Vary LoRA rank: Test r∈{8, 16, 32, 64} to find the efficiency-accuracy frontier for on-device deployment (monitor trainable parameter count and RAM usage)
  3. Test input length limits: Systematically vary truncation thresholds (2000, 4000, 6000, 8000 characters) on Task 6 to identify where long-context breakdown occurs and whether chunking strategies help

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating multimodal signals—such as linguistic style dynamics, user metadata, and interaction patterns—improve the robustness and ecological validity of Menta compared to text-only analysis?
- Basis in paper: [explicit] The authors state in the Limitations section that the study focused exclusively on textual data, excluding multimodal features that are critical for real-world context.
- Why unresolved: The current model architecture and training data were restricted to textual social media posts.
- What evidence would resolve it: A comparative study benchmarking Menta's performance against a multimodal variant trained on datasets containing metadata and interaction logs.

### Open Question 2
- Question: Can on-device SLMs like Menta operate effectively and ethically as assistive tools in clinical settings under clinician supervision?
- Basis in paper: [explicit] The authors note that the use of publicly available, anonymized datasets "limits conclusions about clinical safety, ethical compliance, and longitudinal reliability."
- Why unresolved: The study evaluated the model on public social media data rather than in live clinical environments.
- What evidence would resolve it: Results from deployment trials where the model supports clinical workflows, specifically measuring safety, reliability, and clinician acceptance.

### Open Question 3
- Question: How does the model perform on nuanced or overlapping mental health conditions, such as anxiety and bipolar disorder, which were underrepresented in the training data?
- Basis in paper: [explicit] The authors identify a "restricted diversity of datasets," noting that conditions like anxiety and bipolar disorder are underrepresented in their current corpus.
- Why unresolved: The model was fine-tuned primarily on depression, stress, and suicidal ideation tasks.
- What evidence would resolve it: Performance metrics (Accuracy/BACC) derived from evaluating the model on datasets specifically annotated for anxiety and bipolar disorder.

## Limitations
- Model capacity ceiling for fine-grained classification tasks, particularly 5-class suicide risk detection (0.35 accuracy)
- Sensitivity to long input truncation (>6000 characters) and formatting noise affecting clinical reliability
- Novel balanced accuracy surrogate loss requires independent validation without direct corpus evidence
- Limited generalizability to conditions underrepresented in training data (anxiety, bipolar disorder)

## Confidence
- **High Confidence**: LoRA-based fine-tuning mechanism and on-device deployment pipeline are well-established with clear specifications
- **Medium Confidence**: Cross-dataset multi-task benefits and specific hyperparameter choices (LoRA rank r=16, α=32, task weights λt) are supported by internal comparisons but lack external validation
- **Low Confidence**: Clinical utility and real-world mental health monitoring effectiveness cannot be validated without user studies or clinical trials

## Next Checks
1. Ablation study on BACC loss contribution: Train baseline Menta with only cross-entropy loss, then compare both accuracy and balanced accuracy across all six tasks to quantify the specific contribution of the novel BACC surrogate.

2. Cross-device deployment testing: Implement the GGUF model on at least three different device classes (high-end smartphone, mid-range smartphone, tablet) to measure how TTFT, ITPS, and RAM usage scale with hardware capabilities and document minimum viable device specifications.

3. Long-context and noise robustness evaluation: Systematically test Task 6 performance with varying input lengths (2000, 4000, 6000, 8000 characters) and with synthetic formatting noise added to evaluate whether chunking strategies or input preprocessing improve accuracy on long, noisy posts.