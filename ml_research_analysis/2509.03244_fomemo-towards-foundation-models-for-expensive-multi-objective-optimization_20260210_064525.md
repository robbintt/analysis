---
ver: rpa2
title: 'FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization'
arxiv_id: '2509.03244'
source_url: https://arxiv.org/abs/2509.03244
tags:
- e-01
- e-03
- e-02
- optimization
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel paradigm called FoMEMO (Foundation Models
  for Expensive Multi-objective Optimization) to address the challenge of efficient
  multi-objective optimization in scenarios with limited evaluations. Unlike traditional
  methods that rebuild Gaussian process surrogates for each new problem, FoMEMO pre-trains
  a foundation model using hundreds of millions of synthetic data, enabling fast in-context
  optimization without subsequent model updates.
---

# FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization

## Quick Facts
- **arXiv ID**: 2509.03244
- **Source URL**: https://arxiv.org/abs/2509.03244
- **Reference count**: 40
- **Key outcome**: FoMEMO uses foundation models pre-trained on synthetic data to perform efficient multi-objective optimization without retraining, achieving superior performance and speed compared to traditional GP-based methods.

## Executive Summary
This paper introduces FoMEMO, a novel foundation model approach for expensive multi-objective optimization that eliminates the need for retraining Gaussian process surrogates for each new problem. By pre-training on hundreds of millions of synthetic data samples, FoMEMO enables fast in-context optimization through preference-conditioned aggregated posterior prediction. The model predicts preference-wise aggregated posteriors using Tchebycheff scalarization, facilitating both preference-based and preference-free acquisition functions. Experimental results demonstrate consistent outperformance on synthetic and real-world problems, particularly in high-dimensional scenarios where traditional methods struggle with computational efficiency.

## Method Summary
FoMEMO is a foundation model for expensive multi-objective optimization that pre-trains a Transformer architecture on synthetic data generated from Gaussian processes with RBF kernels. The model learns to predict preference-wise aggregated posteriors by mapping context trajectories and query points to posterior distributions conditioned on user preference vectors. During inference, the frozen model performs in-context optimization by processing the optimization trajectory as input, eliminating the need for subsequent model updates or retraining. The approach uses a variable-dimensional encoder to handle changing input sizes and implements an objective-aware regression head to manage distributional shifts across different objective dimensions.

## Key Results
- FoMEMO consistently outperforms traditional GP-based methods (qNEHVI, MOEA/D-EGO) on both synthetic and real-world optimization problems
- Query times are orders of magnitude lower than GP-based methods, particularly as objective dimensions increase
- The model successfully handles problems with up to 30 decision variables and 6 objectives
- Zero-shot optimization achieves competitive results without any subsequent model training

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Prior Fitting for In-Context Generalization
The model acts as a Prior-Data Fitted Network (PFN) by pre-training on synthetic data from Gaussian Processes with RBF kernels, learning to approximate Bayesian inference without parameter updates during deployment. The core assumption is that real-world functions are sufficiently smooth to be approximated by the RBF kernel regime used in training.

### Mechanism 2: Preference-Conditioned Aggregated Posterior Prediction
Instead of modeling independent posteriors for each objective, FoMEMO predicts a single preference-wise aggregated posterior via Tchebycheff scalarization, allowing dynamic modeling of the trade-off surface defined by user preferences in a single forward pass.

### Mechanism 3: Direct Uncertainty-Aware Acquisition
By bypassing GP surrogate modeling, FoMEMO reduces query latency through fixed neural network inference, with the Transformer output providing calibrated uncertainty estimates for exploration-exploitation trade-offs.

## Foundational Learning

- **Gaussian Process (GP) Priors & Kernels**: Understanding that GPs define distributions over functions and kernels encode smoothness assumptions is critical for interpreting the synthetic data generation and "Universal Approximator" claims.
  - *Quick check*: Why is the RBF kernel choice critical for claiming the model can generalize to "unknown" functions?

- **Scalarization (Tchebycheff Method)**: The model predicts scalarized values based on preference vectors rather than raw objectives, requiring understanding of how Tchebycheff maps multi-objective vectors to single scalars.
  - *Quick check*: How does the model handle changing user preferences during inference without retraining?

- **In-Context Learning (ICL) & PFNs**: FoMEMO uses frozen model parameters with trajectory as context rather than standard train-then-test paradigms.
  - *Quick check*: What's the difference between updating model weights during optimization vs. providing a trajectory as context to a frozen model?

## Architecture Onboarding

- **Component map**: Synthetic Data Generator -> Encoder -> Transformer Backbone -> Objective-Aware Regression Head -> Acquisition Function Layer
- **Critical path**: The Synthetic Data Generator is highest risk; if generated functions don't cover real test case diversity, the Foundation Model will fail to generalize.
- **Design tradeoffs**: RBF kernels for synthetic data balance prior coverage with tractability but may miss discontinuous real-world dynamics; the hybrid variable-dimensional encoder with objective-aware head balances flexibility with precision.
- **Failure signatures**: High-Dimensional Collapse occurs beyond 30 dimensions; Preference Misalignment causes noisy UHVI estimates with low preference sampling density.
- **First 3 experiments**:
  1. Sanity Check (1D/2D): Run FoMEMO on "ToyRobust" problem to verify predicted mean and uncertainty bands align with ground truth given 5-7 context points.
  2. Ablation on Priors: Train with Matern kernels instead of RBF and test on non-smooth synthetic functions to verify RBF "Universal Approximator" capability.
  3. Scalability Stress Test: Benchmark FoMEMO vs. qNEHVI query times on 10-objective problem to confirm efficiency gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning the pre-trained foundation model on domain-specific data improve performance compared to the current zero-shot approach?
- Basis in paper: The Conclusion explicitly asks to "Investigate whether fine-tuning can help improve the performance in domain-specific tasks."
- Why unresolved: The current study focuses exclusively on "freezing" model parameters for in-context optimization, leaving potential benefits of lightweight adaptation unexplored.
- What evidence would resolve it: Comparative analysis showing hypervolume performance of fine-tuned vs. frozen FoMEMO models on data-rich problem domains.

### Open Question 2
- Question: How can model architectures and embedding techniques be developed to efficiently handle optimization problems with significantly higher feature or objective dimensions?
- Basis in paper: The Conclusion lists developing "architectures that allow for datasets with larger sizes... to efficiently handle higher-dimensional feature and objective spaces" as a key future direction.
- Why unresolved: Current synthetic training faces sample complexity issues in high dimensions, and the model was tested only up to 30 features and 6 objectives.
- What evidence would resolve it: New architecture or training scheme maintaining efficiency and accuracy when scaling beyond d=30 or m=6.

### Open Question 3
- Question: How can the model generalize to real-world problems involving function structures or constraints completely outside standard Gaussian Process priors?
- Basis in paper: Appendix E.1 notes the method may struggle with problems "completely beyond GP priors" and suggests incorporating "expert knowledge to design specialized priors" as a potential solution.
- Why unresolved: Pre-training relies entirely on synthetic data from GP distributions, which may not capture non-smooth or highly structured constraints in specialized engineering tasks.
- What evidence would resolve it: Experiments integrating expert-designed priors into synthetic data generation, demonstrating successful optimization on problems violating GP smoothness assumptions.

## Limitations

- The synthetic RBF-kernel GP prior may not capture all real-world function characteristics, particularly discontinuities or conditional dependencies
- The preference-conditioned architecture may struggle with non-convex or disconnected Pareto fronts that cannot be adequately represented through Tchebycheff scalarization
- The Transformer backbone's fixed architecture may limit scalability beyond the 30-dimensional training scope

## Confidence

- **High**: Computational efficiency advantages (demonstrated through empirical timing comparisons)
- **Medium**: Optimization performance claims (though consistently outperforming baselines, comparison set is limited)
- **Low**: Generalization claims across all possible real-world scenarios (synthetic prior coverage is assumed rather than empirically validated)

## Next Checks

1. **Prior Distribution Coverage Test**: Systematically evaluate FoMEMO on benchmark suites containing both smooth (RBF-compatible) and discontinuous objective functions to quantify performance degradation when function characteristics fall outside synthetic training distribution.

2. **Preference Space Density Analysis**: Measure optimization performance as a function of sampled preference vector density during inference, particularly for UHVI calculations, to establish minimum sampling requirements for reliable hypervolume estimation.

3. **Architecture Scalability Validation**: Test FoMEMO on optimization problems with dimensionality exceeding 30 (the stated training limit) to empirically determine when positional encoding and model capacity limitations begin degrading performance.