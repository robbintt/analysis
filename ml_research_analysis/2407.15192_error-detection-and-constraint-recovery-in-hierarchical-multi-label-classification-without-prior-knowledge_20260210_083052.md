---
ver: rpa2
title: Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification
  without Prior Knowledge
arxiv_id: '2407.15192'
source_url: https://arxiv.org/abs/2407.15192
tags:
- error
- learning
- constraints
- class
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neurosymbolic approach to hierarchical
  multi-label classification (HMC) that learns to detect model errors and recover
  hierarchical constraints without prior knowledge of the hierarchy. The authors extend
  the Error Detection Rules (EDR) framework to the HMC setting and propose Focused
  EDR, which optimizes for error F1-score rather than precision by leveraging the
  ratio of submodular functions.
---

# Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge

## Quick Facts
- arXiv ID: 2407.15192
- Source URL: https://arxiv.org/abs/2407.15192
- Reference count: 39
- Primary result: Learned error detection rules achieve 83.17% F1 on military vehicle dataset without prior hierarchy knowledge

## Executive Summary
This paper introduces Focused EDR, a neurosymbolic approach that learns to detect errors and recover hierarchical constraints in hierarchical multi-label classification without prior knowledge of the hierarchy. The method extends Error Detection Rules (EDR) by optimizing for error F1-score rather than precision using submodular ratio maximization. Focused EDR learns rules that both detect model errors and recover underlying hierarchical constraints from prediction patterns. Experiments on three datasets (including a new military vehicle recognition dataset) show the method significantly outperforms black-box neural detectors and the original EDR approach in error detection accuracy.

## Method Summary
The approach trains a main HMC model and auxiliary models (secondary architecture and binary classifiers) to generate prediction conditions across granularities. For each class, Algorithm 1 greedily optimizes POS/(BOD+FP) ratio to learn condition sets that detect errors. The method then extracts cross-granularity conditions from these rules as hierarchical constraints. These constraints can be integrated with neurosymbolic models like Logic Tensor Networks using an adjusted loss that combines prediction loss with constraint consistency.

## Key Results
- Focused EDR achieves 83.17%, 77.78%, and 65.83% error F1 on military vehicles, ImageNet50, and OpenImage36 datasets respectively
- The method successfully recovers hierarchical constraints even when fine-grained labels are missing from training data
- When integrated with LTN, learned constraints improve fine-grain accuracy from 54.35% to 62.43% on military vehicles dataset
- Constraint recovery degrades gracefully with incomplete data, maintaining reasonable performance when 10-50% of fine-grained labels are omitted

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-granularity prediction inconsistencies serve as reliable error signals in hierarchical classification.
- **Mechanism**: For each class y at granularity g, the method builds condition sets from predictions at all other granularities, plus a secondary model and binary classifiers. When the base model assigns label y but conditions from other granularities indicate incompatible labels, the rule fires an error prediction.
- **Core assumption**: Hierarchical violations correlate with prediction errors in the ground truth distribution.
- **Evidence anchors:**
  - [abstract]: "these rules are not only effective in detecting when a machine learning classifier has made an error but also can be leveraged as constraints for HMC"
  - [Section 3]: "for any granularity g and label y ∈ Y_g, the conditions in DC_y ∩ C_{f̂θ}_g are specifying hierarchy constraints"
  - [corpus]: Weak direct evidence; corpus papers focus on constraint-aware learning with known constraints rather than constraint recovery from errors.
- **Break condition**: If hierarchical relationships in data are weak or labels are nearly independent across granularities, cross-granularity conditions will not provide discriminative error signal.

### Mechanism 2
- **Claim**: Optimizing for F1-score via submodular ratio optimization yields better error detection than precision-only optimization.
- **Mechanism**: The original EDR maximized precision (POS/BOD), which ignores false negatives. Focused EDR maximizes POS/(BOD + FP^y_T), which is mathematically equivalent to F1-score of the error class e_y. Since POS and BOD are both submodular functions, the ratio optimization uses the submodular ratio approximation algorithm.
- **Core assumption**: The submodular approximation provides sufficiently good solutions for the F1 objective in practice.
- **Evidence anchors:**
  - [Section 3]: "We note that the quantities of POS^{DC_y}_T and BOD^{DC_y}_T are both submodular by Claim 2 of Theorem 4 of [31]"
  - [Section 3, Theorem 3.1]: "a solution DC*_y for the maximization problem in Equation (2) of a class y ∈ Y_G also maximizes the error F1-score for class y"
  - [Table 3]: Focused EDR achieves 83.17%, 77.78%, 65.83% error F1 vs. DetRuleLearn's 82.62%, 72.78%, 46.46% across three datasets.
- **Break condition**: If greedy submodular approximation gets trapped in poor local optima, or if the condition space is too large relative to training samples, F1 gains may not materialize.

### Mechanism 3
- **Claim**: Learned error rules can be repurposed as hierarchical constraints for neurosymbolic training.
- **Mechanism**: After learning DC_y sets for all classes, the subset of conditions that reference other granularities encode "if y is predicted AND y' from another granularity is predicted, this is an error"—which is equivalent to "y and y' should not co-occur." These extracted constraints are then fed to Logic Tensor Networks with an adjusted loss combining prediction loss and constraint consistency.
- **Core assumption**: Rules learned from error patterns generalize to valid domain constraints rather than overfitting to spurious correlations.
- **Evidence anchors:**
  - [Section 4, Table 2]: "f-EDR + LTN (ours)" improves fine-grain accuracy from 54.35% to 62.43% on Military Vehicles, reduces inconsistencies from 7.77% to 5.37%
  - [Section 4]: "we can successfully recover the majority of HMC constraints with our approach... the recovery of constraints degrades gracefully with incomplete data"
  - [corpus]: Related work [12, 32] assumes constraints are known a priori; this paper relaxes that assumption.
- **Break condition**: If training data has systematic label noise that creates false constraint patterns, recovered rules will encode incorrect constraints and harm neurosymbolic model performance.

## Foundational Learning

- **Concept: Submodular functions and greedy optimization**
  - Why needed here: The Focused EDR optimization relies on submodularity to provide approximation guarantees. Understanding why greedy selection works for submodular maximization (diminishing returns property) helps diagnose when the algorithm may fail.
  - Quick check question: Can you explain why adding elements greedily to a submodular function provides a (1 - 1/e) approximation for cardinality-constrained maximization?

- **Concept: Hierarchical Multi-Label Classification (HMC)**
  - Why needed here: The entire method is built on exploiting the structure of HMC problems—specifically that predictions at different granularities should be consistent. Understanding label hierarchies (e.g., "T-14" is-a "Tank") is essential for interpreting recovered constraints.
  - Quick check question: Given a two-level hierarchy with coarse labels {Animal, Vehicle} and fine labels {Dog, Cat, Car, Truck}, what constraint violations would you expect a model to make?

- **Concept: Logic Tensor Networks (LTN) and semantic loss**
  - Why needed here: The paper uses LTN to integrate recovered constraints into neural training. Understanding how logical formulas are converted to differentiable losses (grounding, satisfiability as continuous objective) is necessary to reproduce the neurosymbolic integration.
  - Quick check question: How would you convert a constraint "IF predicted_fine = T-14 THEN predicted_coarse = Tank" into a differentiable loss term?

## Architecture Onboarding

- **Component map**: Main model (ViT-b16) -> Condition extractor -> Focused EDR learner -> Constraint extractor -> (Optional) LTN integrator
- **Critical path**:
  1. Train base model f̂θ on labeled HMC data → obtain predictions Ŷ_T
  2. Train secondary model and binary classifiers → build condition sets C_g for each granularity
  3. For each class y: Run Algorithm 1 to maximize POS/(BOD + FP) → obtain DC_y
  4. Extract cross-granularity conditions from DC_y as recovered constraints
  5. (Optional) Train LTN with recovered constraints

- **Design tradeoffs**:
  - **Condition set size vs. computational cost**: More conditions (larger C_g) enable finer-grained rules but increase Algorithm 1 runtime to O(|C_g|²)
  - **Secondary model investment**: A well-chosen secondary architecture can provide complementary signals, but training additional models increases overhead
  - **Binary classifier subset B**: Including more labels in B provides more conditions but requires training more binary models
  - **Precision vs. recall in error detection**: F1 optimization balances both, but applications may prefer high precision (conservative error flagging) or high recall (catch all errors)

- **Failure signatures**:
  - **Low error F1 with high precision**: Likely optimizing for precision (original EDR) rather than F1—verify Algorithm 1 implementation
  - **Recovered constraints contradict known domain knowledge**: Training data may have systematic label noise; inspect samples triggering contradictory rules
  - **No cross-granularity conditions selected**: Granularities may be nearly independent in the data, or base model may already be highly consistent
  - **LTN performance degrades with constraints**: Extracted constraints may be spurious; filter by support count or confidence before integration

- **First 3 experiments**:
  1. **Reproduce error detection baseline**: Train ViT-b16 on Military Vehicles dataset, implement Focused EDR with conditions from secondary model, compare error F1 against Table 3 values (target: ~83%)
  2. **Ablate condition sources**: Run Focused EDR with only cross-granularity conditions (remove secondary model and binary classifiers) to isolate the contribution of hierarchical structure to error detection
  3. **Noise tolerance sweep**: Follow Figure 1 protocol—remove increasing fractions of fine-grained labels from training, measure constraint recovery F1 and error detection F1 degradation curves

## Open Questions the Paper Calls Out
- **Open Question 1**: How does Focused EDR perform on hierarchical multi-label classification tasks with more than two levels of granularity (G > 2)?
  - **Basis in paper**: [explicit] The paper states, "The framework can extend for G > 2, but we leave evaluation of the approach beyond two levels to future work."
  - **Why unresolved**: The current experimental design and evaluation were restricted to a two-level hierarchy (fine and coarse), leaving the interaction of error detection rules in deeper hierarchies untested.
  - **What evidence would resolve it**: Experimental results on datasets with three or more hierarchical levels showing error F1-scores and constraint recovery rates comparable to the two-level results.

## Limitations
- Secondary model architecture and binary classifier training procedures are underspecified in the paper
- The method assumes consistent ground-truth labels across granularities, which may not hold in real-world datasets
- Neurosymbolic integration via LTN lacks implementation details (constraint loss weighting, grounding specifics)

## Confidence
- **High**: Error detection F1 improvements via F1-optimized Focused EDR (supported by quantitative comparisons in Table 3)
- **Medium**: Constraint recovery capabilities without prior knowledge (graceful degradation shown but recovery quality depends on training data patterns)
- **Medium**: LTN integration benefits (improvements demonstrated but implementation details incomplete)

## Next Checks
1. Implement and validate Algorithm 1 with synthetic hierarchical data to verify F1 optimization and submodular approximation behavior
2. Test constraint recovery with varying levels of label noise and hierarchical violations to characterize robustness bounds
3. Replicate LTN integration with controlled constraint sets to verify that learned constraints provide measurable performance improvements