---
ver: rpa2
title: 'DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets'
arxiv_id: '2511.03911'
source_url: https://arxiv.org/abs/2511.03911
tags:
- memory
- decohd
- accuracy
- class
- channels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DECOHD addresses the memory bottleneck in hyperdimensional computing
  (HDC) classifiers, which traditionally store one dense prototype hypervector per
  class. Instead of compressing dimensionality, DECOHD reduces memory along the class
  axis by parameterizing class prototypes as compositions of a small, shared set of
  per-layer channels.
---

# DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets

## Quick Facts
- arXiv ID: 2511.03911
- Source URL: https://arxiv.org/abs/2511.03911
- Authors: Sanggeon Yun; Hyunwoo Oh; Ryozo Masukawa; Mohsen Imani
- Reference count: 40
- Key outcome: DECOHD achieves accuracy within 0.1-0.15% of baseline under tight memory budgets, with up to 97% fewer trainable parameters and 277× energy efficiency in ASIC

## Executive Summary
DECOHD introduces a memory-efficient hyperdimensional computing classifier that reduces storage requirements by parameterizing class prototypes as compositions of shared per-layer channels rather than storing dense hypervectors per class. Unlike prior approaches that compress dimensionality, DECOHD preserves the full ambient dimensionality and its robustness benefits while reducing memory along the class axis. The method uses HDC-native binding and bundling operations to compose channel representations during inference, enabling compatibility with existing HDC hardware. Evaluation shows DECOHD maintains accuracy within 0.1-0.15% of strong non-reduced HDC baselines while reducing trainable parameters by up to 97% and achieving significant hardware efficiency gains.

## Method Summary
DECOHD addresses the memory bottleneck in hyperdimensional computing classifiers by decomposing class prototypes into compositions of a small, shared set of per-layer channels. Instead of storing one dense prototype hypervector per class, DECOHD learns low-dimensional latent factors that control how channels are combined through binding and bundling operations. The training process updates only these latents and bundling weights in a decomposed space, while inference remains purely HDC-compatible by composing channels using binding (XOR for binary, element-wise multiplication for real-valued) and bundling (majority vote for binary, mean for real-valued). This design preserves the full ambient dimensionality D and its associated robustness benefits while significantly reducing the memory footprint along the class axis. The method supports both binary and real-valued HDC, with flexibility to trade off between memory, computation, and accuracy through depth tuning.

## Key Results
- Achieves accuracy within 0.1-0.15% of strong non-reduced HDC baseline (worst case 5.7%) under tight memory budgets
- Reduces trainable parameters by up to 97% while maintaining competitive accuracy
- ASIC implementation delivers up to 277× energy efficiency and 35× speedups over CPU, with only 0.38× memory usage

## Why This Works (Mechanism)
DECOHD works by exploiting the compositional nature of hyperdimensional computing while reducing redundancy in class representation. Traditional HDC stores C full-dimensional prototypes, but many classes may share similar feature patterns that can be represented more efficiently. By learning a small set of per-layer channels and composing them through binding and bundling, DECOHD captures class-specific information through learned combination weights rather than independent prototypes. This decomposition preserves the holographic properties and robustness of full-dimensional HDC while dramatically reducing storage requirements. The method maintains accuracy because the binding and bundling operations in HDC are inherently flexible and can reconstruct class-specific representations from shared components when combined appropriately.

## Foundational Learning

**Hyperdimensional Computing (HDC)**: A brain-inspired computing paradigm using high-dimensional (thousands of dimensions) vectors for robust, error-tolerant computation. Why needed: Provides the foundation for DECOHD's approach to classification. Quick check: Verify understanding of hypervectors, binding (XOR/multiplication), and bundling (majority vote/mean).

**Prototype-based Classification**: Storing one representative hypervector per class and classifying by similarity (typically Hamming distance for binary). Why needed: DECOHD modifies this paradigm to reduce memory while preserving functionality. Quick check: Understand how classification works in standard HDC.

**Binding and Bundling Operations**: Core HDC operations where binding creates association between vectors and bundling aggregates multiple vectors. Why needed: DECOHD uses these operations to compose class representations from shared channels. Quick check: Know the difference between binary (XOR) and real-valued (element-wise multiplication) binding.

**Parameter Decomposition**: Technique of representing high-dimensional parameters through lower-dimensional latent factors. Why needed: Enables DECOHD to reduce memory by learning how to compose rather than storing full prototypes. Quick check: Understand the trade-off between decomposition and reconstruction accuracy.

## Architecture Onboarding

**Component Map**: Input features -> Per-layer channels (shared across classes) -> Binding operations -> Bundling with class-specific weights -> Output classification

**Critical Path**: Feature extraction -> Channel composition through N binding layers -> Final bundling with W weights -> Similarity comparison for classification

**Design Tradeoffs**: Depth vs. accuracy (deeper = more memory/computation, potentially lower accuracy), binary vs. real-valued HDC (binary more robust, real-valued potentially more expressive), number of channels M vs. memory efficiency

**Failure Signatures**: Severe accuracy degradation in deep (3+ layer) designs (up to 28.7% drop), optimization challenges with large class counts (C > 1000), potential loss of robustness if combined with aggressive feature-axis compression

**First 3 Experiments to Run**:
1. Compare accuracy of 1-layer vs. 3-layer DECOHD designs on ISOLET to observe depth trade-off
2. Measure memory usage of DECOHD vs. standard HDC across different channel counts M
3. Test robustness to random bit-flip noise comparing DECOHD with SparseHD baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DecoHD's accuracy and memory efficiency scale to datasets with significantly larger output spaces (e.g., C > 1000 classes)?
- Basis in paper: [inferred] The evaluation is limited to datasets with very low class counts (ISOLET C=26; others ≤ 12). While the method is theoretically motivated for multi-class problems where O(CD) is costly, the empirical validation does not cover high-cardinality classification scenarios where the bundling head W ∈ R^{C×M} might face optimization challenges.
- Why unresolved: The paper does not provide experimental evidence or theoretical bounds for performance when the number of classes scales by orders of magnitude.
- What evidence would resolve it: Evaluation results on standard large-scale classification benchmarks (e.g., ImageNet-1k) analyzing the relationship between class count, bundling weights, and accuracy.

### Open Question 2
- Question: Can DecoHD be combined with aggressive feature-axis compression (e.g., binary/ternary quantization) without negating its robustness benefits?
- Basis in paper: [inferred] The authors explicitly argue against prior feature-axis compression methods (like SparseHD) because they "erode concentration" and robustness. However, DecoHD preserves full dimensionality D, leaving the potential synergy of applying both class-axis (DecoHD) and feature-axis compression simultaneously unexplored.
- Why unresolved: The paper treats feature-axis and class-axis compression as alternatives rather than potential complements, so the interaction effects are unknown.
- What evidence would resolve it: Experiments applying binary or low-precision quantization to the decomposed channel factors to measure the resulting accuracy-robustness Pareto frontier.

### Open Question 3
- Question: What is the theoretical mechanism causing the severe accuracy degradation in deep (3+ layer) decomposed architectures?
- Basis in paper: [inferred] Table III shows a drastic 28.7% accuracy drop when moving from 1-layer to 3-layer designs, attributed loosely to "per-channel interference." This suggests the optimization landscape or the holographic properties of repeated binding become unstable, but the paper does not offer a formal explanation.
- Why unresolved: The paper observes the trade-off empirically but does not derive the theoretical limits of binding depth in this specific decomposed learning context.
- What evidence would resolve it: A theoretical analysis or ablation study isolating gradient flow and hypervector orthogonality as functions of binding depth N.

## Limitations
- Increased computational overhead during inference, particularly for deeper designs, creating trade-offs with real-time applications
- Limited generalizability to sequential or temporal data domains, as evaluation focuses on static classification tasks
- Hardware efficiency claims based on specific ASIC design that may not translate directly to other platforms

## Confidence

**High**: Memory reduction claims and accuracy preservation are directly supported by experimental results across multiple datasets.

**Medium**: Scalability claims to larger class counts and robustness benefits lack empirical validation on diverse benchmarks.

**Medium**: Hardware efficiency measurements are platform-specific and may not generalize to all implementations.

## Next Checks

1. Test DECOHD on sequential or temporal datasets to assess applicability beyond static classification tasks.

2. Evaluate the impact of deeper designs on inference latency in real-time scenarios to better understand practical trade-offs.

3. Conduct stress tests with more diverse noise patterns (e.g., structured or correlated noise) to validate robustness claims under broader conditions.