---
ver: rpa2
title: Using Large Language Models for education managements in Vietnamese with low
  resources
arxiv_id: '2501.15022'
source_url: https://arxiv.org/abs/2501.15022
tags:
- data
- language
- training
- question
- educational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes VietEduFrame, a framework for applying large
  language models (LLMs) to educational management tasks in Vietnamese institutions.
  The authors developed a tailored dataset from student education documents at Hanoi
  VNU, addressing resource constraints.
---

# Using Large Language Models for education managements in Vietnamese with low resources

## Quick Facts
- arXiv ID: 2501.15022
- Source URL: https://arxiv.org/abs/2501.15022
- Reference count: 18
- Key outcome: VietEduFrame fine-tuned Vistral-7B with LoRA achieved 43.23% EM and 81.24% F1 on Vietnamese educational QA

## Executive Summary
This paper introduces VietEduFrame, a framework for applying large language models to educational management tasks in Vietnamese institutions. The authors address the challenge of limited Vietnamese educational datasets by creating a synthetic QA dataset from student education documents at Hanoi VNU. Through fine-tuning both Bloom and Vistral models with and without LoRA, they demonstrate that resource-efficient approaches can achieve reasonable performance on Vietnamese educational QA tasks. The Vistral model with LoRA achieved the best results, showing that LLMs can be effectively applied to Vietnamese educational management despite resource constraints.

## Method Summary
The authors developed VietEduFrame by first creating a synthetic QA dataset from VNU student regulations using GPT-3.5 Turbo with Chain-of-Thought, Self-Consistency CoT, and Tree of Thought prompting. The generated data underwent human validation with 54.92% rated "very good." They then fine-tuned both Vistral-7B and BLOOM models with and without LoRA (rank 128) on this dataset. The fine-tuning used standard configurations including warmup_ratio=0.05, weight_decay=0.01, and max_length=1024 over 10 epochs. Performance was evaluated using Exact Match and F1-score metrics.

## Key Results
- Vistral with LoRA achieved best performance: 43.23% EM and 81.24% F1
- LoRA reduced training time from 14 hours to 6 hours per epoch and GPU memory from 61.2GB to 32GB
- Full fine-tuning of Vistral outperformed LoRA by only +0.49 EM and +0.33 F1
- Vistral consistently outperformed BLOOM by approximately 9 EM points and 8.88 F1 points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation using LLMs with structured prompting can produce viable training data when real annotated data is unavailable
- Mechanism: GPT-3.5 Turbo generates context-question-answer triplets from regulatory documents using Chain-of-Thought, Self-Consistency CoT, and Tree of Thought prompting. Quality is validated through ROUGE/BLEU scores and human expert review
- Core assumption: The LLM-generated QA pairs accurately reflect the distribution of real student queries about university regulations
- Evidence anchors: "framework leverages synthetic data to supplement real-world examples" [abstract], "Using technique prompting Chain of Thought, Self-Consistency Chain of Thought, and Tree of Thought" [section 3.1]
- Break condition: If synthetic data distribution diverges significantly from actual student query patterns, fine-tuned models will fail to generalize to real deployments

### Mechanism 2
- Claim: LoRA (Low-Rank Adaptation) enables fine-tuning large models under severe computational constraints with minimal performance degradation
- Mechanism: LoRA decomposes weight updates into low-rank matrices (rank r=128), training only W_up and W_down while freezing base model. This reduces trainable parameters by d×k/(d+k)/r times
- Core assumption: The rank-128 decomposition captures sufficient domain-specific knowledge for educational QA without catastrophic forgetting
- Evidence anchors: "fine-tuning and deploying LLMs remain computationally expensive, especially in resource-constrained environments" [abstract], "The number of parameters training is reduced dk/(d+k)/r times" [section 4.2.1]
- Break condition: If rank is too low or base model lacks sufficient Vietnamese pre-training, LoRA adapters cannot inject necessary domain knowledge

### Mechanism 3
- Claim: Language-specific pre-training (Vistral) provides stronger foundation for Vietnamese educational QA than multilingual models (BLOOM)
- Mechanism: Vistral's pre-training on Vietnamese corpus and architecture optimizations (rolling buffer cache, sliding-window attention) better preserve Vietnamese language patterns during fine-tuning
- Core assumption: Vietnamese-specific pre-training captures linguistic structures (tone marks, word segmentation) that multilingual models miss
- Evidence anchors: "Vistral model with LoRA achieved the best performance" [abstract], "Vistral consistently outperforms Bloom in both Exact and F1-score" [section 6.1]
- Break condition: If educational domain vocabulary diverges significantly from general Vietnamese corpus used in Vistral pre-training, the advantage may diminish

## Foundational Learning

- **Fine-tuning vs. Full Training**
  - Why needed here: Paper compares full parameter training (61.2GB GPU, 14 hours/epoch for Vistral) against LoRA (32GB, 6 hours/epoch) with only ~1% performance drop
  - Quick check question: Can you explain why freezing most parameters and only training low-rank adapters would reduce GPU memory requirements?

- **Question Answering Metrics (EM vs. F1)**
  - Why needed here: The paper reports 43.23% Exact Match but 81.24% F1—understanding this gap is critical for interpreting results
  - Quick check question: Why might F1 score be significantly higher than Exact Match in educational QA contexts?

- **Vietnamese Language Challenges in NLP**
  - Why needed here: Paper addresses Vietnamese-specific constraints (low resources, institutional variability, lack of standardized datasets)
  - Quick check question: What are the key linguistic features of Vietnamese that make it challenging for models primarily trained on English data?

## Architecture Onboarding

- **Component map:** Raw regulatory PDFs → Preprocessing (cleaning, segmentation, KATEX conversion) → LLM-based QA generation (GPT-3.5 Turbo with CoT prompting) → Human validation (54.92% very good, 28.28% good) → Fine-tuning (Vistral/BLOOM with/without LoRA) → Context adaptation with instructional prompts → Evaluation (EM, F1)

- **Critical path:** Data preprocessing quality directly determines synthetic QA quality → Prompt engineering for data generation controls coverage of regulatory topics → LoRA rank selection (128 chosen) balances capacity vs. resource constraints

- **Design tradeoffs:** Full fine-tuning: +0.49 EM, +0.33 F1 vs. LoRA, but 2x GPU RAM (61.2GB vs 32GB) and 2.3x training time; Vistral vs. BLOOM: +9 EM points, +8.88 F1 points (with LoRA), but requires Vietnamese-specific infrastructure; Synthetic vs. real data: Faster iteration, but human validation reveals 6.78% "bad" + 1.05% "very bad" samples

- **Failure signatures:** Low EM but high F1: Model captures keywords but not exact phrasing (see Table 8 example—"suy nghĩ từng bước" artifact in output); Training loss plateaus early: LoRA rank too low or learning rate issues; Context length errors: Vistral sliding window (max 4446 tokens in dataset, need to verify window size compatibility)

- **First 3 experiments:**
  1. **Baseline replication**: Fine-tune Vistral-7B with LoRA (rank 128) on the provided dataset, target >40 EM / >80 F1 on validation set. Compare resource usage against paper's reported 32GB GPU / 6 hours per epoch
  2. **Ablation on data quality**: Train on only "Very Good" + "Good" samples (83.2% of data) vs. full dataset. Hypothesis: removing 15.8% lower-quality samples improves EM without significant coverage loss
  3. **Context window analysis**: Test model on questions requiring multi-hop reasoning across regulatory sections (e.g., combining Article 33 lecturer responsibilities with graduation requirements). Paper acknowledges reasoning limitations—identify specific failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be enhanced to improve logical reasoning capabilities for complex, multi-step educational management queries?
- Basis in paper: Section 6.5 states that the models "may produce answers that lack coherent logical structure" and fail to follow clear reasoning for multi-step problems
- Why unresolved: The current study focused on resource-efficient fine-tuning (LoRA) which improved metrics but did not fundamentally resolve the models' inherent struggles with deep logical deduction
- What evidence would resolve it: Successful integration and evaluation of reasoning-enhancing techniques (e.g., Chain-of-Thought fine-tuning) that show improved performance on a dedicated dataset of complex, multi-step questions

### Open Question 2
- Question: To what extent does the Vistral-based framework generalize to regulations of educational institutions other than Hanoi VNU?
- Basis in paper: Section 3 notes "Institutional Variability" as a challenge, and Section 6.5 highlights the limitation that models "might not fully grasp the specific regulations... unique to different educational institutions"
- Why unresolved: The dataset was constructed exclusively from documents provided by Vietnam National University (VNU), potentially introducing institutional bias
- What evidence would resolve it: Zero-shot or few-shot evaluation results of the current model on a held-out dataset derived from different Vietnamese universities

### Open Question 3
- Question: Can the integration of external knowledge graphs or retrieval-augmented generation (RAG) mitigate the model's lack of specialized domain knowledge?
- Basis in paper: Section 6.5 identifies "Lack of Specialized Knowledge" as a key limitation, noting the model's difficulty in handling specific regulations without domain-specific expertise
- Why unresolved: The current methodology relies solely on parameter-efficient fine-tuning (PEFT) to internalize knowledge, which may be insufficient for strict regulatory accuracy
- What evidence would resolve it: A comparative study showing that a RAG-enhanced version of the framework yields higher Exact Match (EM) scores on specific regulatory queries than the fine-tuned-only baseline

## Limitations

- The study relies on synthetic data generation, which may not fully capture real student query complexity, with 15.8% of samples rated as "bad" or "very bad"
- The framework shows limitations in reasoning capabilities, particularly for multi-step problems requiring synthesis across regulatory sections
- The dataset was constructed exclusively from VNU documents, potentially limiting generalization to other Vietnamese institutions with different regulatory structures

## Confidence

- **High Confidence**: The effectiveness of LoRA in reducing computational requirements while maintaining performance (supported by direct comparisons showing only ~1% performance drop with 2x reduction in GPU RAM and training time)
- **Medium Confidence**: The superiority of Vistral over BLOOM for Vietnamese educational QA (supported by performance metrics but limited by single dataset and institutional context)
- **Low Confidence**: The scalability of the framework to other Vietnamese institutions with different regulatory structures (extrapolated from single institution dataset without cross-validation)

## Next Checks

1. **Cross-Institutional Validation**: Test the fine-tuned models on regulatory documents from other Vietnamese universities to assess generalization across institutional variations. This would validate whether the framework's effectiveness extends beyond VNU's specific context.

2. **Real Data Comparison**: Collect and annotate a small dataset of actual student queries about university regulations, then compare model performance on synthetic vs. real data. This would quantify the impact of synthetic data limitations on practical deployment.

3. **Reasoning Capability Assessment**: Design a test set specifically targeting multi-hop reasoning requirements (e.g., questions requiring synthesis of information across multiple regulatory articles). This would provide concrete evidence for or against the paper's acknowledged reasoning limitations.