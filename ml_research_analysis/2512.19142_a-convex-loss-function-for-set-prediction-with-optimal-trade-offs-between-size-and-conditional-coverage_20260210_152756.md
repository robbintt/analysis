---
ver: rpa2
title: A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size
  and Conditional Coverage
arxiv_id: '2512.19142'
source_url: https://arxiv.org/abs/2512.19142
tags:
- function
- loss
- which
- functions
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a convex loss function for set prediction\
  \ that enables optimal trade-offs between conditional coverage and set size, using\
  \ Choquet integrals and Lov\xE1sz extensions of submodular functions. The method\
  \ learns non-decreasing subset-valued functions via level sets of a real-valued\
  \ function, allowing efficient optimization and guaranteed conditional coverage."
---

# A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage

## Quick Facts
- arXiv ID: 2512.19142
- Source URL: https://arxiv.org/abs/2512.19142
- Reference count: 40
- Primary result: Convex loss function enabling optimal trade-offs between conditional coverage and set size via level sets of real-valued functions

## Executive Summary
This paper introduces a convex loss function for set prediction that achieves optimal trade-offs between conditional coverage and set size. The method learns non-decreasing subset-valued functions through level sets of a real-valued function, enabling efficient gradient-based optimization. The approach uses Choquet integrals and Lovász extensions of submodular functions to convert discrete set selection into continuous convex optimization, providing theoretical guarantees on conditional coverage while maintaining computational tractability.

## Method Summary
The method learns a real-valued function g: X × Y → R and defines prediction sets as level sets A(λ, x) = {y ∈ Y : g(x, y) ≥ -λ}. The loss function ℓ(y, g) = v(g(x, ·)) + ½g(x, y)² combines the Lovász extension v of a submodular size function V with a quadratic penalty. This proper scoring rule simultaneously optimizes conditional coverage at all levels λ. The framework supports both deterministic and randomized predictions, with efficient optimization via SGD or iteratively reweighted least-squares for kernel methods. Label smoothing and post-clustering techniques improve empirical performance.

## Key Results
- The proposed convex loss achieves optimal trade-offs between set size and conditional coverage for all coverage levels simultaneously
- Experiments on synthetic classification and regression datasets show improved performance over marginal coverage approaches, particularly with concave size penalties
- The method learns non-monotonic level sets that better adapt to heterogeneous conditional distributions compared to fixed-threshold methods

## Why This Works (Mechanism)

### Mechanism 1
Level set parameterization enables efficient optimization over all possible prediction sets simultaneously by converting discrete set selection into continuous function optimization. The method learns g: X × Y → R instead of directly optimizing set-valued functions, making gradient-based methods applicable.

### Mechanism 2
The Choquet integral (Lovász extension) converts discrete submodular set functions into convex continuous functions, enabling tractable optimization. For submodular V, its Lovász extension v is convex, transforming combinatorial optimization into convex optimization.

### Mechanism 3
Integrating the Lagrangian loss over all trade-off parameters λ yields a proper scoring rule that simultaneously optimizes conditional coverage at all levels. This integral equals v(g(x,·)) + ½∫g(x,y)²dp(y|x), and minimizing E[ℓ(Y,g(X,·))] yields optimal sets for every λ.

## Foundational Learning

- **Submodular functions**: Why needed: The entire convexity of the loss depends on V being submodular. Quick check: Given V(A) = min(|A|, r), is V submodular? Verify using V(A∩B) + V(A∪B) ≤ V(A) + V(B).

- **Choquet integral and layer-cake representation**: Why needed: The loss function derivation relies on expressing integrals over level sets as the Choquet integral. Quick check: For V(A) = |A|/k and h: Y → R⁻, compute ∫₀^∞ V({h ≥ -λ})dλ and show it equals (1/k)∑ᵢ hᵢ.

- **Conditional vs. marginal coverage in conformal prediction**: Why needed: The paper targets conditional coverage (P(Y ∈ A(X)|X=x) ≥ 1-α for all x), which is harder than marginal coverage. Quick check: If sets A(x) achieve 90% marginal coverage, could there exist some x values with 50% conditional coverage?

## Architecture Onboarding

- Component map: Input (x,y) → Function g_θ(x,y) → Level sets A(λ,x) = {y: g_θ(x,y) ≥ -λ} → Loss: ℓ(y,g) = v(g(x,·)) + ½g(x,y)² → Optimizer → Updated θ → At inference: Input x → g_θ(x,·) → Estimate p̂(y|x) from subgradient → Threshold λ(α,x) → Output set A(α,x)

- Critical path: 1. Choose submodular size function V (cardinality for classification, set-cover for connected sets in regression) 2. Implement Lovász extension v and its subgradient oracle 3. Implement loss ℓ(y,g) = v(g(x,·)) + ½g(x,y)² with label smoothing 4. Train model via SGD with unbiased subgradient estimates 5. At test time, extract conditional probability estimate from subgradient and compute threshold λ(α,x)

- Design tradeoffs:
  - **Concave vs. linear V**: Concave penalties encourage clustered prediction values but lose information about posterior probabilities. Use concave when model capacity is limited.
  - **Deterministic vs. randomized predictions**: Randomized predictions achieve any coverage level α exactly but require sampling at inference. Deterministic predictions are simpler but may over-cover.
  - **SGD vs. IRLS**: SGD scales to large datasets and arbitrary architectures. IRLS gives more precise solutions for kernel methods but requires O(kd²n) per iteration.

- Failure signatures:
  - **Collapsed predictions**: If V(A) = min(|A|, 1), all predictions collapse to a constant. Diagnose by checking if all g(x, ·) values are equal.
  - **No conditional coverage improvement**: If λ is input-independent, the method reduces to marginal coverage. Compare learned λ(α, x) vs. input-independent λ.
  - **Disconnected sets in regression**: Modular V produces sets with holes. Use set-cover V with appropriate structuring element radius.

- First 3 experiments:
  1. **Synthetic classification with k=3-5 classes**: Compare new quadratic loss vs. softmax vs. standard square loss on Gaussian mixture data. Measure area-loss as primary metric.
  2. **Ablation on concave penalties**: Train with V(A)=|A|/k vs. V(A)=log(1+|A|) on underparameterized models. Confirm concave penalties help when model capacity is limited.
  3. **Regression with set-cover penalty**: Generate 1D regression data with heteroscedastic noise. Compare modular V vs. set-cover V. Visualize prediction sets at α=0.1.

## Open Questions the Paper Calls Out

### Open Question 1
Can the convex loss framework be extended to non-submodular size functions while preserving tractability? The authors state that several relevant notions of size are not submodular, breaking the convexity guarantees. Evidence would require reformulation using alternative convex relaxations or approximation algorithms with theoretical guarantees.

### Open Question 2
What are the provable benefits of label smoothing and post-clustering regularizations when integrated with conformal prediction? The conclusion mentions these techniques lack theoretical analysis of their effects on coverage guarantees. Evidence would require theoretical bounds on coverage after conformalization with regularized estimators.

### Open Question 3
How does the method perform on large-scale real-world datasets compared to density estimation and marginal coverage baselines? All experiments use synthetic datasets, and computational properties for large-scale setups could be explored further. Evidence would require benchmarking on standard real-world datasets comparing area-losses and coverage calibration.

## Limitations

- Submodularity requirement excludes many natural size measures like minimal enclosing ball volume or shortest path length
- Level set parameterization assumes optimal sets at different coverage levels are nested, which may not hold in all problem domains
- Theoretical guarantees on conditional coverage are asymptotic and may not hold in finite samples with misspecified models

## Confidence

- **High confidence**: The convexity of the loss function when V is submodular, and the correctness of the proper scoring rule derivation
- **Medium confidence**: The empirical performance improvements over marginal coverage methods based on synthetic experiments
- **Low confidence**: The practical utility of asymmetric loss functions and label smoothing techniques due to limited experimental validation

## Next Checks

1. **Submodularity verification**: Formally verify submodularity by checking V(A∩B) + V(A∪B) ≤ V(A) + V(B) for all relevant subsets. Test with V(A) = min(|A|, 2) and V(A) = log(1+|A|) on small synthetic problems.

2. **Coverage calibration analysis**: Beyond average coverage, analyze the distribution of coverage errors across different x values. Compute P(Y ∈ A(α,X)|X=x) for multiple x regions to verify conditional coverage is maintained uniformly.

3. **Scalability stress test**: Evaluate the method on larger-scale problems (thousands of training points, higher-dimensional Y) using SGD. Monitor convergence behavior and compare wall-clock time to baseline methods while tracking coverage guarantees.