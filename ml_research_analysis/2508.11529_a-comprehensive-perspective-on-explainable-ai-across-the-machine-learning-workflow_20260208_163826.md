---
ver: rpa2
title: A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow
arxiv_id: '2508.11529'
source_url: https://arxiv.org/abs/2508.11529
tags:
- data
- learning
- explanations
- https
- explainability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Holistic Explainable AI (HXAI), a novel framework
  that broadens the scope of AI explainability beyond model outputs to encompass the
  entire data analysis workflow. HXAI addresses the critical gap in current explainable
  AI (XAI) methods, which primarily focus on individual predictions while overlooking
  upstream decisions and downstream quality checks that impact trust and usability.
---

# A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow

## Quick Facts
- arXiv ID: 2508.11529
- Source URL: https://arxiv.org/abs/2508.11529
- Reference count: 40
- One-line primary result: Introduces Holistic Explainable AI (HXAI), a framework that expands AI explainability beyond model outputs to cover the entire data analysis workflow.

## Executive Summary
This paper addresses the critical gap in current explainable AI (XAI) methods by proposing Holistic Explainable AI (HXAI), a framework that broadens explainability beyond individual predictions to encompass the entire machine learning workflow. HXAI integrates six components—data, analysis setup, learning process, model output, model quality, and communication channel—and aligns them with diverse user needs. The framework leverages insights from human sciences and user studies to define effective explanations that are contrastive, causal, truthful, user-centered, and interactive. A comprehensive 112-item question bank supports rigorous coverage analysis of existing tools, and an AI agent powered by large language models (LLMs) is proposed to tailor explanations to individual users, bridging the gap between AI developers and domain experts.

## Method Summary
The HXAI framework is conceptualized as a comprehensive system covering six components of the ML workflow, evaluated using a 112-item question bank mapped across components and user types. The methodology involves taxonomizing explainability per component, mapping user needs, and aggregating explanations via an LLM-based HXAI Agent. Tools are assessed for coverage by manually checking which questions they can answer, with no code or training procedure provided. The framework remains theoretical, with the HXAI Agent described at a high level but lacking implementation details.

## Key Results
- HXAI identifies significant gaps in current XAI tools' ability to explain upstream (data, setup) and downstream (quality) workflow components.
- A 112-item question bank operationalizes holistic explainability, revealing that existing tools primarily focus on model output rather than the full analysis process.
- The proposed LLM-based HXAI Agent can tailor explanations to user expertise levels, though empirical validation of its effectiveness is pending.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Broadening explainability to cover the entire ML workflow addresses trust gaps better than focusing solely on model output.
- **Mechanism:** Traditional XAI explains predictions in isolation, leaving users to guess about upstream data or downstream quality validity. HXAI's six components ensure the "why" of a result is traced through the analysis pipeline.
- **Core assumption:** Users require visibility into the entire provenance of a decision—not just the final calculation—to accurately calibrate trust.
- **Evidence anchors:**
  - [abstract] "HXAI addresses the critical gap... which primarily focus on individual predictions while overlooking upstream decisions and downstream quality checks."
  - [section 2.1] "Effective explanations must therefore... translate statistical relationships into actionable, counterfactual narratives, and expose data provenance alongside model-quality diagnostics."
- **Break condition:** If the user ignores upstream/downstream information and relies solely on the final prediction probability, the mechanism fails to improve trust calibration.

### Mechanism 2
- **Claim:** An LLM-based HXAI Agent can bridge the semantic gap between technical metrics and non-expert understanding by acting as a translation layer.
- **Mechanism:** The agent acts as the "communication channel," ingesting technical artifacts and user context, then using natural language generation to tailor the explanation format to the specific user type.
- **Core assumption:** LLMs can accurately interpret visual and statistical artifacts and ground their natural language outputs in these specific data points without hallucination.
- **Evidence anchors:**
  - [abstract] "We further demonstrate how AI agents that embed large-language models can orchestrate diverse explanation techniques, translating technical artifacts into stakeholder-specific narratives."
  - [section 5.2.2] "LLMs have the potential to act as effective substitutes for data analysts in explainability tasks... tailoring multiple explanations to different users."
- **Break condition:** If the LLM misinterprets a metric or hallucinates a causal link, the explanation degrades trust rather than building it.

### Mechanism 3
- **Claim:** A structured question bank (112 items) operationalizes "Holisticness" by defining explicit coverage requirements for each workflow stage.
- **Mechanism:** The framework provides specific questions that force the system to instrument the ML pipeline to capture metadata that answers these questions.
- **Core assumption:** The metrics and logs captured during the ML workflow are sufficient to answer the 112 questions meaningfully.
- **Evidence anchors:**
  - [abstract] "A comprehensive 112-item question bank supports the framework, enabling rigorous coverage analysis of existing tools."
  - [section 5.3] "A practical way to illustrate how HXAI can help users develop a mental model... is through a structured question bank."
- **Break condition:** If the ML pipeline lacks observability, the system cannot answer "Learning Process" questions, breaking the holistic loop.

## Foundational Learning

- **Concept: Workflow Scope (XAI vs. HXAI)**
  - **Why needed here:** To distinguish between explaining a single prediction (traditional XAI) and explaining the entire analysis process (HXAI). The paper argues the latter is required for true trustworthiness.
  - **Quick check question:** If a user asks "Why was this transaction flagged as fraud?", is that a Model Output question or an Analysis Setup question?

- **Concept: Stakeholder Segmentation**
  - **Why needed here:** The paper explicitly defines three user types (Data Scientist, Data Analyst, Domain Expert) with different needs. Understanding this segmentation is critical for the "User-Centered" component of HXAI.
  - **Quick check question:** Which user type primarily needs to know if the selected optimization metric aligns with business goals?

- **Concept: Post-hoc vs. Intrinsic Explainability**
  - **Why needed here:** While Model Output often relies on post-hoc methods (LIME/SHAP), HXAI's "Analysis Setup" and "Learning Process" components rely on intrinsic transparency (logging decisions as they happen). You must distinguish these to implement the system.
  - **Quick check question:** Can you use SHAP values to explain the "Learning Process" component?

## Architecture Onboarding

- **Component map:**
  Raw Data + User Profile -> Standard ML Pipeline -> HXAI Layer (6 Parallel Components) -> HXAI Agent -> Natural Language Explanation + Visual Artifacts

- **Critical path:**
  1. **User Identification:** System identifies the user as a "Domain Expert."
  2. **Question Mapping:** User asks "Is the model good?"; Agent maps this to "Model Quality" and "Performance Summary" questions.
  3. **Artifact Retrieval:** Agent retrieves relevant metrics (Accuracy, Calibration curves) rather than raw weights.
  4. **Translation:** Agent generates a contrastive explanation ("The model is 90% accurate, which is 15% better than the baseline...").

- **Design tradeoffs:**
  - **Completeness vs. Overload:** The system *can* explain everything (112 questions), but the Agent must filter to prevent cognitive overload.
  - **Provenance vs. Performance:** Instrumenting the pipeline for "Learning Process" explainability adds storage and compute overhead.

- **Failure signatures:**
  - **The "Jargon Leak":** The Agent fails to translate technical terms for a Domain Expert.
  - **The "Silent Pipeline":** The system cannot answer "Analysis Setup" questions because the initial configuration logic was not logged.
  - **The "Hallucinated Cause":** The Agent attributes a model failure to "data imbalance" when the logs show the data was balanced.

- **First 3 experiments:**
  1. **Question Bank Audit:** Run the 112-item question bank against your current ML platform to identify which workflow components are completely "dark" (unobservable).
  2. **Simple Agent Wrapper:** Build a prototype RAG agent that answers just the "Model Quality" questions for a Domain Expert using a pre-computed validation report.
  3. **User Persona A/B Test:** Provide the same explanation artifacts to a Data Scientist and a Domain Expert. Measure if the "User-Centered" adaptation actually improves task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the diverse dimensions of trustworthiness (e.g., stability, fairness, explainability) be mathematically combined into a single, standardized quantitative metric?
- **Basis in paper:** [explicit] Section 7.2 states that "Selecting the appropriate metrics for each factor and developing a comprehensive method for measuring overall system trustworthiness remains an open problem."
- **Why unresolved:** Current evaluation methods treat these dimensions in isolation, lacking a unified framework to aggregate them into a holistic score.
- **What evidence would resolve it:** A formulated index or scoring framework that successfully weights and combines disparate metrics and is validated to correlate with user trust.

### Open Question 2
- **Question:** To what extent does a modular, multi-agentic architecture improve the explanation quality of HXAI systems compared to a single monolithic agent?
- **Basis in paper:** [explicit] Section 7.2 suggests the current "single monolithic agent" approach "can be enhanced by adopting a modular, multi-agentic architecture" with specialized agents for each HXAI component.
- **Why unresolved:** The paper proposes the multi-agent structure as a future direction but does not implement or test it against the monolithic baseline.
- **What evidence would resolve it:** A comparative study measuring explanation fidelity, latency, and user satisfaction between a modular system and a single-LLM agent.

### Open Question 3
- **Question:** How can the effectiveness of the proposed HXAI agent in bridging the expert-non-expert gap be empirically validated through user studies?
- **Basis in paper:** [explicit] Section 7.3 lists the limitation that the proposed approach "has not yet been validated through user studies," and Section 7.2 calls for "comprehensive user studies to assess the robustness of AI agents."
- **Why unresolved:** The framework is theoretical; it is unknown if the AI agent's translations actually improve mental models or trust calibration for non-experts.
- **What evidence would resolve it:** Results from controlled experiments showing that users interacting with the HXAI agent achieve higher task performance or better trust calibration compared to users viewing static XAI dashboards.

## Limitations
- The 112-item question bank's operational validity—whether these questions truly capture stakeholder needs—remains untested in real-world user studies.
- The HXAI Agent's architecture lacks implementation details, prompt engineering strategies, and hallucination mitigation techniques, making replication challenging.
- The claim that HXAI significantly improves trust calibration compared to traditional XAI methods is currently speculative, as no user studies measuring trust or decision-making outcomes are presented.

## Confidence

- **High Confidence:** The identification of gaps in current XAI tools' coverage of upstream/downstream workflow components is well-supported by the tool audit results and aligns with established HCI literature on transparency.
- **Medium Confidence:** The LLM agent's ability to translate technical artifacts for different user types is theoretically sound but lacks empirical validation of translation accuracy or user comprehension improvements.
- **Low Confidence:** The claim that HXAI significantly improves trust calibration compared to traditional XAI methods is currently speculative, as no user studies measuring trust or decision-making outcomes are presented.

## Next Checks

1. **User Comprehension Study:** Conduct a controlled experiment where participants (data scientists vs. domain experts) receive either traditional XAI explanations or HXAI-generated explanations for the same model, then measure comprehension accuracy and time-to-understanding.
2. **Hallucination Audit:** Implement the HXAI Agent and systematically test its responses against ground-truth pipeline logs to quantify hallucination rates, particularly when explaining causal relationships in the learning process.
3. **Coverage Gap Analysis:** Apply the 112-item question bank to a modern AutoML platform with enhanced observability to identify whether the current "dark" components (Data, Setup, Learning) can be illuminated with existing instrumentation.