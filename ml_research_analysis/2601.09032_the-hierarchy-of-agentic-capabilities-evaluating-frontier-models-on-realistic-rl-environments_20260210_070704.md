---
ver: rpa2
title: 'The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic
  RL Environments'
arxiv_id: '2601.09032'
source_url: https://arxiv.org/abs/2601.09032
tags:
- tasks
- agents
- evaluation
- tool
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates frontier AI models on 150 realistic workplace
  tasks within an e-commerce RL environment, identifying a hierarchy of five agentic
  capabilities: tool use, planning, adaptability, groundedness, and common-sense reasoning.
  Even the best-performing models fail approximately 40% of tasks, with weaker models
  failing at basic tool use and planning, while stronger models primarily struggle
  with common-sense reasoning requiring contextual inference.'
---

# The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments

## Quick Facts
- **arXiv ID:** 2601.09032
- **Source URL:** https://arxiv.org/abs/2601.09032
- **Reference count:** 2
- **Primary result:** Evaluates 12 frontier AI models on 150 e-commerce customer support tasks, identifying a five-level hierarchy of agentic capabilities from basic tool use to complex common-sense reasoning.

## Executive Summary
This paper presents a comprehensive evaluation framework for measuring the practical capabilities of frontier language models in realistic workplace scenarios. The researchers created CORECRAFT, Inc., a simulated e-commerce environment where models act as customer support agents handling 150 diverse tasks ranging from simple order lookups to complex technical support issues. The study reveals a clear hierarchy of five agentic capabilities - tool use, planning, adaptability, groundedness, and common-sense reasoning - with even the best-performing models failing approximately 40% of tasks. The research introduces a task-centric environment design methodology emphasizing domain expert contributions and provides detailed failure analysis to guide future agent development.

## Method Summary
The evaluation framework uses a sandboxed e-commerce environment where models interact through a Model Context Protocol (MCP) interface with tools for searching, retrieving, creating, and updating customer, product, order, and ticket data. Twelve frontier models are tested on 150 fully autonomous tasks, receiving system prompts, tool schemas, and task descriptions before executing unlimited tool calls until providing final responses. An LLM judge verifies task completion using human-written rubrics, and failure analysis categorizes errors into five capability levels. The environment includes realistic constraints like loyalty tiers, product compatibility issues, and multi-step reasoning requirements.

## Key Results
- All models fail approximately 40% of tasks, even the strongest frontier models
- A clear hierarchy of agentic capabilities emerges: tool use (Level 1), planning (Level 2), adaptability (Level 3), groundedness (Level 4), and common-sense reasoning (Level 5)
- Weaker models primarily fail at basic tool use and planning, while stronger models struggle with contextual inference requiring common-sense reasoning
- Task-centric environment design with domain expert contributions proves effective for creating realistic evaluation scenarios

## Why This Works (Mechanism)
The framework works by creating a realistic, constrained environment that forces models to demonstrate practical reasoning capabilities rather than just pattern matching. The MCP interface standardizes tool interaction while the diverse task suite exposes different levels of cognitive complexity. The LLM judge provides scalable, consistent evaluation, and the hierarchical failure analysis reveals specific capability gaps that would be invisible in traditional benchmark tests.

## Foundational Learning
- **Model Context Protocol (MCP):** Standardized interface for AI models to interact with external tools and data sources; needed for consistent tool access across different models
- **Task-centric environment design:** Creating evaluation environments focused on specific task types rather than general-purpose benchmarks; needed to measure practical workplace capabilities
- **Hierarchical capability assessment:** Breaking down complex reasoning into progressive skill levels; needed to identify specific failure points and development priorities
- **LLM-as-judge methodology:** Using strong language models to evaluate task completion against human-defined rubrics; needed for scalable and consistent evaluation
- **Domain expert contribution:** Involving subject matter experts in task creation; needed to ensure tasks reflect realistic workplace challenges
- **Failure signature analysis:** Categorizing errors by capability level to identify specific weaknesses; needed to guide targeted model improvements

## Architecture Onboarding

**Component Map:** Task Input -> System Prompt + MCP Tools -> Model Execution Loop -> Tool Calls -> Response Generation -> LLM Judge Evaluation -> Capability Level Classification

**Critical Path:** Task specification → Model receives context → Tool call execution → Result processing → Final response → Judge verification

**Design Tradeoffs:** 
- Unlimited tool calls vs. efficiency (trading thoroughness for resource usage)
- LLM judge vs. human evaluation (trading scalability for potential subjectivity)
- Sandboxed environment vs. real-world deployment (trading safety for ecological validity)

**Failure Signatures:** 
- Level 1: Incorrect tool parameter mapping
- Level 2: Failure to sequence multi-step operations
- Level 3: Inability to adjust queries after empty results
- Level 4: Loss of conversation context across tool calls
- Level 5: Missing contextual inference requiring real-world knowledge

**3 First Experiments:**
1. Validate the MCP tool schemas by testing each tool individually with known inputs and expected outputs
2. Run a small subset of simple tasks (5-10) through the complete evaluation pipeline to verify the judge's rubric application
3. Test model adaptability by deliberately providing empty results for one query and observing if the model attempts alternative approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on LLM judge which may introduce subjectivity and depends on rubric quality
- Full task suite and evaluation rubrics are proprietary, limiting reproducibility
- Does not measure execution efficiency (tool call count, time) which is critical for real-world deployment
- Performance metrics focus on completion rates without accounting for resource utilization

## Confidence
- **High Confidence:** Hierarchy of agentic capabilities is well-supported by systematic failure analysis across multiple models
- **Medium Confidence:** Task-centric environment design methodology is effective but lacks direct comparative evidence
- **Low Confidence:** Specific numerical rankings between individual models are difficult to verify without full access to tasks and rubrics

## Next Checks
1. Request and release complete evaluation rubrics and task specifications to enable independent verification of performance metrics
2. Extend evaluation to measure not just task completion but also tool call efficiency and execution time
3. Validate the capability hierarchy framework on a disjoint task set from a different domain (e.g., healthcare or finance) to test generalizability