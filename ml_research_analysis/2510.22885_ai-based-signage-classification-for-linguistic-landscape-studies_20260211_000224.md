---
ver: rpa2
title: AI based signage classification for linguistic landscape studies
arxiv_id: '2510.22885'
source_url: https://arxiv.org/abs/2510.22885
tags:
- language
- chinese
- figure
- chinatown
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied AI-based optical character recognition (OCR)
  and language detection to automate linguistic landscape analysis in Honolulu Chinatown,
  analyzing a georeferenced dataset of 1,449 photos. The AI model achieved 79% overall
  accuracy in detecting signage languages, demonstrating potential for large-scale
  analysis while highlighting limitations including distorted signs, reflections,
  degraded surfaces, graffiti, and AI hallucination.
---

# AI based signage classification for linguistic landscape studies

## Quick Facts
- arXiv ID: 2510.22885
- Source URL: https://arxiv.org/abs/2510.22885
- Reference count: 0
- Primary result: 79% accuracy in AI-based signage language classification

## Executive Summary
This study applied AI-based optical character recognition (OCR) and language detection to automate linguistic landscape analysis in Honolulu Chinatown, analyzing a georeferenced dataset of 1,449 photos. The AI model achieved 79% overall accuracy in detecting signage languages, demonstrating potential for large-scale analysis while highlighting limitations including distorted signs, reflections, degraded surfaces, graffiti, and AI hallucination. Five types of mislabeling were identified, with errors particularly common in multilingual contexts and visually complex scenes. The research concludes that while AI can significantly reduce manual labor in LL studies, fully automated analysis is not yet reliable and a hybrid approach combining AI automation with human validation is recommended for accurate results.

## Method Summary
The study used the Google Cloud Vision AI library to extract text from street-level photos of Honolulu Chinatown signage through OCR, then applied the API's built-in language detection service to classify detected text. The system ranks languages by proportional text presence in each image, handling multiple languages without requiring separate models. Manual human validation served as ground truth for accuracy measurement, with errors categorized into five types: distortion, reflection, degraded surfaces, graffiti, and hallucination. The dataset consisted of 1,449 georeferenced photos collected by researchers, primarily featuring English, Chinese, Japanese, and Vietnamese text.

## Key Results
- AI model achieved 79% accuracy in signage language classification
- Five distinct error types identified: distortion, reflection, degraded surfaces, graffiti, and hallucination
- Chinese-Japanese confusion occurred due to shared kanji characters without contextual kana
- Model often detected peripheral or background text with equal importance as focal signage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCR-based text extraction combined with statistical language detection enables automated signage classification at scale.
- Mechanism: The Google Cloud Vision API first extracts text regions via OCR, then passes text strings through built-in language detection that classifies each segment based on character patterns and orthographic features. The system ranks languages by proportional text presence.
- Core assumption: Signage contains machine-readable text with sufficient orthographic distinctiveness for language classification.
- Evidence anchors:
  - [abstract] "we constructed a georeferenced photo dataset of 1,449 images collected by researchers and applied AI for optical character recognition (OCR) and language classification"
  - [section 2.2] "We used the API's Optical Character Recognition (OCR) function to detect and extract text from signs in the photos. The extracted text strings were then passed through API's built-in language detection services"
  - [corpus] Limited direct corpus support; neighboring papers address medical imaging and LLM-based automation rather than OCR-language pipelines.
- Break condition: Images without text, highly stylized fonts, or scripts the model wasn't trained on will fail extraction.

### Mechanism 2
- Claim: Proportional language ranking allows multi-language detection within single images without requiring separate models per language.
- Mechanism: The API detects all recognizable text segments, calculates the relative proportion of each detected language, and outputs a ranked list with the dominant language first.
- Core assumption: Dominance by text quantity correlates with research-relevance in linguistic landscape analysis.
- Evidence anchors:
  - [section 2.2] "Based on the proportion of each language appearing in the image, the library ranks the detected languages, with the top-ranked language representing the most dominant in the photo"
  - [section 2.2] "The Google Cloud Vision AI library can handle multiple languages in one image without requiring separate models"
  - [corpus] No corpus evidence for this specific ranking mechanism.
- Break condition: Equal-length multilingual signs may produce arbitrary rankings; peripheral text may dominate if larger than focal signage.

### Mechanism 3
- Claim: Systematic error patterns emerge from visual ambiguity and equal-area attention weighting, enabling targeted mitigation strategies.
- Mechanism: The model applies uniform attention across all image regions, detecting peripheral or background text that human interpreters ignore. Errors cluster around distortion, reflection, degradation, graffiti, and hallucination—each traceable to specific visual degradation types.
- Core assumption: Error patterns are consistent enough to be categorized and potentially pre-filtered.
- Evidence anchors:
  - [abstract] "Five recurring types of mislabeling were identified, including distortion, reflection, degraded surface, graffiti, and hallucination"
  - [section 4] "However, the AI models process all areas of the image with an equal importance and may pick up textual or pattern-like elements at the edges, which are not the intended focus"
  - [section 11] "This example highlights a key consideration for AI-powered language detection and ranking systems, as the model may prioritize languages based on the length, regardless whether they are the focus of the picture or not"
  - [corpus] Weak corpus support; neighboring papers discuss LLM automation but not vision-specific attention mechanisms.
- Break condition: Novel degradation types (e.g., new graffiti styles, unusual lighting) may produce uncategorized errors.

## Foundational Learning

- Concept: **Optical Character Recognition (OCR)**
  - Why needed here: OCR is the upstream process that converts visual text in photos into machine-readable strings before language detection can occur. Without understanding OCR limitations (angle sensitivity, contrast requirements), you cannot diagnose downstream classification failures.
  - Quick check question: Given a photo taken at 45° angle to a reflective glass sign, would you expect OCR to extract complete or partial text?

- Concept: **Language Detection via Orthographic Features**
  - Why needed here: The API classifies language by recognizing script-specific character patterns (e.g., hiragana for Japanese, hanzi for Chinese). Understanding this explains why Chinese-Japanese confusion occurs when kanji overlap without surrounding kana context.
  - Quick check question: If a sign contains only shared kanji characters without hiragana or pinyin, can statistical language detection reliably distinguish Chinese from Japanese?

- Concept: **Linguistic Landscape (LL) Methodology**
  - Why needed here: LL research has specific conventions about what counts as a "sign," which languages are relevant to code, and how to handle multilingual contexts. These disciplinary norms define what "accuracy" means beyond raw technical performance.

## Architecture Onboarding

### Component Map
Google Cloud Vision API -> OCR Text Extraction -> Language Detection -> Ranked Language Output

### Critical Path
OCR text extraction -> Language detection classification -> Proportional ranking -> Top-language output

### Design Tradeoffs
- **Proprietary API vs Open Source**: Uses Google Cloud Vision API for convenience but lacks transparency in internal algorithms
- **Uniform Attention vs Selective Focus**: Equal processing of all image regions captures peripheral text but introduces noise
- **Single Top-Rank vs Multi-Language**: Outputs only top-ranked language, potentially missing equally important secondary languages
- **Pre-trained Models vs Fine-tuning**: Relies on general-purpose models without domain-specific adaptation to signage characteristics

### Failure Signatures
- Chinese-Japanese confusion on kanji-only text
- Peripheral text detection (e.g., bucket labels, background signs)
- Text extraction failures on distorted, reflective, or degraded surfaces
- "Hallucination" of languages from texture patterns or random character-like elements

### 3 First Experiments
1. Test OCR extraction on controlled set of distorted/angled signage images
2. Compare language detection accuracy on kanji-only vs kanji-with-kana signs
3. Evaluate peripheral text detection by cropping images to focal signage only

## Open Questions the Paper Calls Out
None

## Limitations
- 79% accuracy metric only considers top-ranked language prediction, potentially missing important secondary languages
- Error analysis based on subset validation rather than comprehensive double-blind coding
- Proprietary Google Cloud Vision API prevents full understanding of failure modes
- Performance specific to Honolulu Chinatown's multilingual context may not generalize to other linguistic environments

## Confidence
- **High confidence**: The AI pipeline mechanism (OCR → language detection → ranking) functions as described and produces measurable results
- **Medium confidence**: The 79% accuracy figure and five error categories are valid for this dataset but may not generalize across different urban contexts or signage types
- **Low confidence**: Claims about "hallucination" as a distinct error type require further validation, as this could overlap with existing OCR limitations

## Next Checks
1. Replicate the study in a different linguistic landscape (e.g., European city with Latin-script languages) to test cross-context performance
2. Implement human-in-the-loop validation where ambiguous predictions trigger expert review rather than automatic classification
3. Test alternative open-source OCR and language detection pipelines to benchmark against the proprietary Google Cloud Vision API performance