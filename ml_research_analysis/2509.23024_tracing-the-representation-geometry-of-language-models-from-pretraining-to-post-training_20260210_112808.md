---
ver: rpa2
title: Tracing the Representation Geometry of Language Models from Pretraining to
  Post-training
arxiv_id: '2509.23024'
source_url: https://arxiv.org/abs/2509.23024
tags:
- pretraining
- representation
- language
- geometry
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces spectral analysis to quantify representation\
  \ geometry during LLM training, using metrics RankMe (effective rank) and \u03B1\
  ReQ (eigenvalue decay rate) to track geometric evolution across pretraining and\
  \ post-training. During autoregressive pretraining, models exhibit three distinct\
  \ geometric phases: a rapid \"warmup\" collapse, an \"entropy-seeking\" expansion\
  \ coinciding with n-gram memorization, and a \"compression-seeking\" anisotropic\
  \ consolidation enabling long-context understanding."
---

# Tracing the Representation Geometry of Language Models from Pretraining to Post-training

## Quick Facts
- **arXiv ID**: 2509.23024
- **Source URL**: https://arxiv.org/abs/2509.23024
- **Reference count**: 40
- **Key outcome**: Introduces spectral analysis metrics (RankMe and αReQ) to quantify representation geometry during LLM training, revealing three distinct geometric phases during pretraining and differential effects of post-training methods

## Executive Summary
This study introduces a quantitative framework for analyzing representation geometry in language models using spectral analysis metrics - RankMe (effective rank) and αReQ (eigenvalue decay rate). The authors track geometric evolution across both pretraining and post-training phases, revealing that autoregressive pretraining exhibits three distinct geometric phases: a rapid warmup collapse, an entropy-seeking expansion coinciding with n-gram memorization, and a compression-seeking anisotropic consolidation that enables long-context understanding. Post-training methods show differential geometric effects: SFT/DPO drive entropy-seeking dynamics while RLVR induces compression-seeking patterns, with implications for downstream capabilities and robustness.

## Method Summary
The study employs spectral analysis to quantify representation geometry during LLM training, introducing two metrics: RankMe (effective rank) to measure dimensionality and αReQ (eigenvalue decay rate) to capture anisotropy. These metrics track geometric evolution across pretraining and post-training phases. The authors analyze how cross-entropy optimization under skewed token frequencies and representational bottlenecks drives geometric transformations, examining the relationship between geometric phases and emergent capabilities like long-context understanding. They also compare the geometric effects of different post-training methods including SFT, DPO, and RLVR.

## Key Results
- Autoregressive pretraining exhibits three distinct geometric phases: warmup collapse, entropy-seeking expansion (linked to n-gram memorization), and compression-seeking consolidation enabling long-context understanding
- Cross-entropy optimization under skewed token frequencies and representational bottlenecks drives these geometric transformations
- Post-training methods differentially transform geometry: SFT/DPO drive entropy-seeking dynamics reducing OOD robustness, while RLVR induces compression-seeking dynamics enhancing reward alignment but reducing generation diversity

## Why This Works (Mechanism)
The geometric phases emerge from fundamental optimization dynamics under specific conditions. During pretraining, cross-entropy loss drives the model to minimize uncertainty across the output distribution. Under skewed token frequencies (Zipfian distributions common in natural language), this creates a tension between fitting frequent patterns (driving entropy-seeking expansion) and maintaining representational efficiency (driving compression-seeking consolidation). The warmup collapse occurs as initial random representations rapidly adapt to the data distribution. The entropy-seeking phase corresponds to the model memorizing frequent n-grams and local patterns, expanding its representational capacity to capture these statistics. The compression-seeking phase emerges as the model develops more efficient, anisotropic representations that can capture longer-range dependencies and support capabilities like long-context understanding. Post-training methods impose additional optimization pressures that differentially affect this geometry based on their objectives and training dynamics.

## Foundational Learning
- **Spectral analysis of neural representations** - Why needed: Provides quantitative tools to measure representation geometry beyond simple performance metrics; Quick check: Verify RankMe and αReQ capture meaningful differences in known geometric regimes
- **Geometric phases in optimization** - Why needed: Understanding how optimization dynamics manifest as geometric transformations helps predict and control model behavior; Quick check: Test whether phase transitions align with specific training milestones
- **Representation bottleneck theory** - Why needed: Explains why compression-seeking phases emerge as models balance capacity and efficiency; Quick check: Verify geometric consolidation correlates with improved sample efficiency
- **Zipfian token distributions** - Why needed: Skewed frequencies create specific optimization pressures that drive geometric evolution; Quick check: Test how varying frequency skew affects phase transitions
- **Cross-entropy optimization dynamics** - Why needed: The primary pretraining objective directly shapes representation geometry through gradient flow; Quick check: Verify geometric patterns emerge consistently across different cross-entropy implementations
- **Emergent capabilities and geometry** - Why needed: Links abstract geometric measures to concrete model behaviors and performance characteristics; Quick check: Test whether geometric consolidation predicts capability emergence better than performance metrics alone

## Architecture Onboarding

**Component Map**
Input tokens -> Embedding layer -> Transformer layers -> Output projection -> Loss function -> Geometric metrics (RankMe, αReQ)

**Critical Path**
Token embedding → Transformer processing → Representation formation → Output distribution → Loss computation → Gradient update → Geometric evolution

**Design Tradeoffs**
- Spectral analysis provides geometric insights but may miss functional aspects of representations
- Simple metrics (RankMe, αReQ) enable tractable analysis but may oversimplify complex geometric structures
- Focus on autoregressive models limits generalizability to other architectures
- Correlation between geometry and capabilities doesn't establish causation

**Failure Signatures**
- Geometric metrics fail to distinguish functionally different representations
- Phase transitions don't align with known capability emergence
- Post-training geometric effects contradict intended capability changes
- Geometric patterns inconsistent across model scales or architectures

**First Experiments**
1. Measure RankMe and αReQ trajectories during standard pretraining to verify three-phase pattern
2. Compare geometric evolution across different token frequency distributions to test skew dependency
3. Test whether geometric consolidation at phase III correlates with improved long-context performance

## Open Questions the Paper Calls Out
None

## Limitations
- Findings rely on specific model architectures with unknown generalizability to sparse models or different attention mechanisms
- Spectral metrics may miss important nonlinear functional aspects of representations within layers
- Connection between geometric phases and capabilities like long-context understanding remains correlational rather than causal
- Post-training analysis doesn't fully account for dataset composition differences or hyperparameter variations

## Confidence

**High confidence**: The existence of three distinct geometric phases during autoregressive pretraining and their general characterization (warmup collapse, entropy-seeking expansion, compression-seeking consolidation) - supported by consistent patterns across experiments and clear mechanistic explanations through cross-entropy optimization dynamics.

**Medium confidence**: The specific claims about RLVR inducing "compression-seeking" dynamics and SFT/DPO driving "entropy-seeking" patterns during post-training - while patterns are observed, confounding factors like dataset differences and hyperparameter choices limit definitive attribution.

**Medium confidence**: The connection between phase III geometric consolidation and long-context understanding - correlation is demonstrated but causal mechanisms remain incompletely explained.

## Next Checks
1. Conduct ablation studies on the pretraining dataset composition (token frequency skew, vocabulary size) to determine which factors most strongly drive the transition between geometric phases, particularly testing whether phase II (entropy-seeking) is primarily driven by n-gram memorization versus other phenomena.

2. Design controlled experiments comparing representation geometry across different model families (transformers, state-space models, sparse architectures) to test the generalizability of the three-phase geometric evolution pattern beyond standard autoregressive transformers.

3. Implement targeted interventions at each geometric phase boundary during pretraining (e.g., early stopping at phase transitions, gradient modifications) to empirically test whether phase III consolidation is necessary and sufficient for long-context understanding versus merely correlated with it.