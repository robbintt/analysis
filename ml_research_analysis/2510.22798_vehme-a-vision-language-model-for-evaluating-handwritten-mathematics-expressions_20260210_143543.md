---
ver: rpa2
title: 'VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions'
arxiv_id: '2510.22798'
source_url: https://arxiv.org/abs/2510.22798
tags:
- student
- answer
- error
- localization
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VEHME is a vision-language model for grading handwritten mathematical\
  \ expressions. It combines a two-phase training pipeline\u2014supervised fine-tuning\
  \ on QwQ-32B-generated data followed by reinforcement learning with Group Relative\
  \ Policy Optimization\u2014and introduces an Expression-Aware Visual Prompting Module\
  \ to improve spatial understanding."
---

# VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions

## Quick Facts
- arXiv ID: 2510.22798
- Source URL: https://arxiv.org/abs/2510.22798
- Authors: Thu Phuong Nguyen; Duc M. Nguyen; Hyotaek Jeon; Hyunwook Lee; Hyunmin Song; Sungahn Ko; Taehwan Kim
- Reference count: 40
- Primary result: VEHME achieves 73.01% error detection and 61.13% error localization accuracy on AIHub, approaching proprietary model performance

## Executive Summary
VEHME addresses the challenge of evaluating handwritten mathematical expressions by combining vision-language modeling with a two-phase training pipeline. The system first uses supervised fine-tuning on synthesized data from QwQ-32B, then refines performance through reinforcement learning with Group Relative Policy Optimization. An Expression-Aware Visual Prompting Module enhances spatial understanding of multi-line expressions. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art performance among open-source models while operating with only 7 billion parameters.

## Method Summary
VEHME employs a two-phase training pipeline on Qwen2.5-VL-7B-Instruct. Phase one uses supervised fine-tuning on QwQ-32B-generated data with grammar-constrained repair to create structured reasoning traces. Phase two applies reinforcement learning with Group Relative Policy Optimization using a composite reward function balancing correctness, localization accuracy, and output quality. An Expression-Aware Visual Prompting Module, trained separately on synthetic multi-line expressions, provides oriented bounding box guidance to improve spatial understanding. LoRA fine-tuning (rank=8) is applied during both phases, with image resolution optimized at 224×224 for efficiency.

## Key Results
- 73.01% error detection accuracy and 61.13% error localization F1 on AIHub dataset
- 75% error detection accuracy on rotated samples with EVPM (62% without)
- 69.40% error detection accuracy on FERMAT dataset
- Approaches proprietary model performance while using only 7 billion parameters

## Why This Works (Mechanism)

### Mechanism 1
Dual-phase training (SFT followed by GRPO) enables both format alignment and reasoning refinement for grading tasks. Supervised fine-tuning on QwQ-32B-generated data establishes structured output format and foundational reasoning patterns. GRPO then optimizes relative response rankings via group-based advantage estimation, improving correctness, reasoning depth, and error localization without requiring a value function.

**Core assumption:** SFT provides sufficient initialization for GRPO to refine; the composite reward accurately signals task objectives.

**Evidence anchors:**
- [abstract] "VEHME integrates a two-phase training pipeline: (i) supervised fine-tuning using structured reasoning data, and (ii) reinforcement learning that aligns model outputs with multi-dimensional grading objectives"
- [section 4.4, Table 2] Ablation shows removing SFT drops ED accuracy from 73.01% to 63.24%; removing RL collapses EL F1 from 58.18% to 29.59%
- [corpus] Related work on process reward models (Wang et al., 2025; Khalifa et al., 2025) supports multi-step reasoning supervision, but corpus evidence specific to GRPO for handwriting is limited

**Break condition:** If SFT data quality is poor (incorrect reasoning traces), GRPO may amplify errors rather than correct them.

### Mechanism 2
Expression-Aware Visual Prompting Module (EVPM) improves spatial understanding of multi-line handwritten expressions through oriented bounding box guidance. EVPM is trained on synthetically generated multi-line expressions with rotation and offset augmentation to predict expression-level bounding boxes. These boxes are overlaid as visual prompts during inference, helping the VLM attend to individual expressions in unstructured layouts.

**Core assumption:** Synthetic training data with augmentation transfers to real handwritten student work; bounding box prompts effectively guide VLM attention.

**Evidence anchors:**
- [abstract] "To enhance spatial understanding, we propose an Expression-Aware Visual Prompting Module, trained on our synthesized multi-line math expressions dataset"
- [section 4.4, Table 3] On heavily rotated samples (mean 21.81°), removing EVPM drops ED from 75% to 62% and EL from 66% to 62%
- [corpus] Corpus does not contain direct comparisons of visual prompting for handwritten math; mechanism remains paper-specific

**Break condition:** If handwriting is extremely illegible or expressions overlap significantly, bounding box detection may fail, providing misleading prompts.

### Mechanism 3
Data synthesis from QwQ-32B with grammar-constrained repair generates high-quality training pairs where direct annotation is unavailable. QwQ-32B produces structured reasoning traces (thinking, correctness, localization) from LaTeX-rendered inputs. Truncated outputs are repaired using grammar-constrained decoding to ensure valid format. This addresses the scarcity of annotated handwritten math evaluation data (C1).

**Core assumption:** QwQ-32B's reasoning quality transfers when distilled to smaller VLM; grammar-based repair preserves reasoning integrity.

**Evidence anchors:**
- [section 3.2.1] "Zheng et al. (2024) demonstrates that QwQ-32B achieves state-of-the-art performance among open-source LLMs on step-level mathematical evaluation tasks"
- [section 3.2.1, Algorithm 1] Details post-processing with grammar-constrained decoding for format repair
- [corpus] Zheng et al. (2024) in corpus confirms QwQ-32B strength on process evaluation; direct evidence for VLM distillation is limited

**Break condition:** If QwQ-32B exhibits systematic biases (length preference, anchoring), these propagate through distillation.

## Foundational Learning

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** Core RL method for refining grading without value function estimation; relies on intra-group reward comparisons.
  - **Quick check question:** How does GRPO estimate advantage differently from PPO?

- **Concept:** Vision-Language Model architectures (encoder-LLM fusion)
  - **Why needed here:** VEHME builds on Qwen2.5-VL-7B; understanding visual token processing is essential for debugging EVPM integration.
  - **Quick check question:** How are image patches converted to tokens the LLM can process?

- **Concept:** Composite reward design for multi-objective tasks
  - **Why needed here:** Grading requires balancing correctness, localization accuracy, and output quality; poorly weighted rewards cause mode collapse.
  - **Quick check question:** What happens if localization reward dominates correctness reward?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B-Instruct -> SFT (QwQ-32B synthesized data) -> GRPO (composite reward) -> EVPM (oriented bounding box prompts)

- **Critical path:**
  1. Generate SFT data via QwQ-32B with grammar-constrained repair
  2. Fine-tune VLM on synthesized data (LoRA on MLP layers, rank 8)
  3. Train EVPM on synthetic multi-line expressions (10K samples)
  4. Run GRPO with EVPM-augmented inputs and composite reward

- **Design tradeoffs:**
  - Image resolution: 224×224 is efficient but 448×448 improves accuracy (Table 4); plateau at 768×768
  - EVPM overhead: Modest gain on standard data (~1% ED), significant on rotated samples (~13% ED)
  - SFT necessity: Skipping SFT causes 10% ED drop; required for format alignment

- **Failure signatures:**
  - Low EL F1 with reasonable ED: GRPO may not be converging on localization reward
  - Poor performance on rotated images: EVPM not integrated or trained insufficiently
  - Format errors in output: Grammar-constrained repair may be skipping; verify SFT data quality

- **First 3 experiments:**
  1. **Baseline sanity check:** Run Qwen2.5-VL-7B-Instruct zero-shot on AIHub subset to confirm performance gap
  2. **SFT-only ablation:** Train with SFT data only, skip GRPO; expect ~46.9% ED (matching Table 2 "-RL")
  3. **EVPM rotation test:** Evaluate on challenging subset with/without EVPM to verify spatial prompting contribution

## Open Questions the Paper Calls Out
- **Question:** How can bias propagation from teacher models (QwQ-32B) be mitigated in the evaluation pipeline when using LLM-as-judge for reward modeling?
- **Question:** Can the Expression-Aware Visual Prompting Module be improved to handle spatial variations beyond rotation, such as non-linear layouts or overlapping expressions?
- **Question:** What alternative reward formulations could reduce RL training instability while maintaining or improving error detection and localization accuracy?

## Limitations
- GRPO implementation details remain underspecified, with critical hyperparameters not provided
- EVPM shows modest gains on standard data (~1% improvement) despite added complexity
- FERMAT dataset evaluation lacks direct comparison with proprietary models

## Confidence
**High Confidence:** The two-phase training pipeline (SFT + GRPO) demonstrates measurable impact through ablation studies showing 10% ED drop when removing SFT and collapse to 29.59% EL F1 without RL.

**Medium Confidence:** The Expression-Aware Visual Prompting Module shows clear benefits on rotated samples but limited impact on standard data, with generalization of synthetic training to real handwriting uncertain.

**Medium Confidence:** Data synthesis approach using QwQ-32B with grammar-constrained repair addresses annotation bottlenecks, but quality transfer depends on assumptions about reasoning pattern generalization.

## Next Checks
1. **FERMAT competitive benchmark:** Run GPT-4V and Gemini on FERMAT dataset to establish true performance baselines and verify "comparable to proprietary models" claim.

2. **GRPO hyperparameter sensitivity:** Systematically vary group size, KL penalty, and reward weights to determine which components drive performance gains versus potential overfitting.

3. **EVPM ablation on multi-line complexity:** Test EVPM performance across different expression densities and overlap levels to quantify whether spatial prompting benefits scale with handwriting complexity.