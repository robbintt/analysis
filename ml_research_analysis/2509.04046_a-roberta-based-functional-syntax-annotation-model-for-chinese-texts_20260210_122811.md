---
ver: rpa2
title: A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts
arxiv_id: '2509.04046'
source_url: https://arxiv.org/abs/2509.04046
tags:
- functional
- annotation
- chinese
- grammar
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed the first RoBERTa-based functional syntax
  annotation model for Chinese texts, achieving an F1 score of 0.852 on named entity
  recognition tasks. The researchers created a dataset of 4,100 sentences from the
  People's Daily 2014 corpus, annotated according to Cardiff Grammar functional syntax
  theory.
---

# A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts

## Quick Facts
- arXiv ID: 2509.04046
- Source URL: https://arxiv.org/abs/2509.04046
- Reference count: 26
- First RoBERTa-based functional syntax annotation model for Chinese texts, achieving F1 score of 0.852 on NER tasks

## Executive Summary
This study developed the first RoBERTa-based functional syntax annotation model for Chinese texts, addressing the gap between functional syntax theory and modern attention-based NLP models. The researchers created a dataset of 4,100 sentences from the People's Daily 2014 corpus, annotated according to Cardiff Grammar functional syntax theory. By fine-tuning RoBERTa-Chinese-wwm-ext with BIO tagging for named entity recognition, the model successfully identified core syntactic elements including Subject, Main Verb, and Complement, achieving strong performance on primary elements while showing room for improvement on imbalanced label samples. This work integrates functional syntax with pre-trained language models, providing a new method for automated Chinese functional syntax analysis.

## Method Summary
The researchers fine-tuned RoBERTa-Chinese-wwm-ext for token classification on a dataset of 4,100 Chinese sentences from People's Daily 2014, annotated with 8 structural elements (Subject, Main Verb, Complement, Adjunct, Operator, Auxiliary, Binder, Negator) using BIO format. The model added a classification head to the pre-trained encoder, froze the first 6 of 12 layers to reduce overfitting, and employed class-weighted loss and oversampling for rare labels. Training used AdamW optimizer with linear LR scheduling and warmup, Optuna for hyperparameter search, and seqeval for evaluation. The approach reframed functional syntax annotation as an NER task to leverage mature sequence labeling architectures.

## Key Results
- Achieved F1 score of 0.852 on named entity recognition tasks
- Successfully identified core syntactic elements (Subject, Main Verb, Complement) with excellent performance
- Outperformed comparative approaches including CRF+BiLSTM, vanilla Transformer, and BERT variants
- Demonstrated 0.129 overfitting gap between train (F1=0.98) and test performance

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning RoBERTa-Chinese-wwm-ext enables functional syntax annotation with limited labeled data (4,100 sentences) by transferring contextual representations learned during pre-training. Pre-trained language models encode token relationships via self-attention; these representations transfer to syntax tasks through a token classification head. Whole word masking (WWM) addresses Chinese tokenization where words split into characters. Core assumption: Contextual patterns from pre-training capture syntactic relationships generalizing to functional syntax annotation. Evidence anchors: [abstract] "fine-tuned a RoBERTa-Chinese-wwm-ext model to identify core syntactic elements... achieving an F1 score of 0.852"; [section 4.1] "selected RoBERTa-Chinese-wwm as the foundation model... adding a classification head for token classification"; [corpus] Weak direct evidence; related work (Cui et al.) suggests WWM helps Chinese NER, but not tested against non-WWM baseline here. Break condition: If functional syntax categories don't align with distributional patterns from pre-training (e.g., highly abstract semantic roles), transfer degrades. Paper notes room for improvement on imbalanced labels.

### Mechanism 2
Reframing functional syntax annotation as an NER task with BIO tagging enables use of mature sequence labeling architectures. Each token is classified as Beginning (B-), Inside (I-), or Outside (O-) of a functional element span, converting hierarchical syntax into linear token-level prediction. Core assumption: Functional syntax elements can be represented as non-overlapping, linear spans without losing essential structural information. Evidence anchors: [abstract] "implement the named entity recognition task, achieving an F1 score of 0.852"; [section 3.2] "annotation was carried out in a BIO format with... Subject, Main Verb, Complement, Adjunct, Operator, Auxiliary, Binder, Negator"; [corpus] No corpus evidence for this specific formulation; standard practice in NER literature. Break condition: When syntax allows conflation (same token serving multiple roles), BIO forces single predictions. Paper documents 5% of Operators misclassified as Main Verbs due to this.

### Mechanism 3
Class-weighted loss and oversampling for rare labels partially mitigates label imbalance effects. Inverse-frequency class weights penalize errors on rare classes more heavily; oversampling increases minority class exposure during training. Core assumption: Rare labels follow similar patterns to common labels but need more training signal; structural complexity isn't the primary bottleneck. Evidence anchors: [section 3.1] "For the O and N label categories... the study implemented oversampling techniques"; [section 4.1] "calculating class weights based on inverse label frequency to address label imbalance"; [section 4.3] Despite interventions, "Adjunct (A) and Negator (N) showed relatively weaker recognition performance" (Negators: 40 samples, 7% misclassified as Operators). Break condition: If rare labels have fundamentally different linguistic patterns, weighting won't help. Confusion patterns suggest possible structural ambiguity beyond sample size.

## Foundational Learning

- Concept: Cardiff Grammar / Systemic Functional Linguistics
  - Why needed here: Annotation scheme uses functional syntax categories based on semantic roles, not grammatical function. Understanding conflation (one token serving multiple roles) is essential for interpreting errors like O/M confusion.
  - Quick check question: In "They are journalists" (他们 是 记者), why might "are" (是) be annotated as both Operator AND Main Verb?

- Concept: BIO Tagging for Sequence Labeling
  - Why needed here: The architecture and evaluation depend on BIO format. You must know B-X starts a span, I-X continues it, O means outside.
  - Quick check question: Given [B-S, I-S, B-M, B-C, I-C, O], what entities and spans are detected?

- Concept: Fine-tuning Pre-trained Language Models
  - Why needed here: Method builds on RoBERTa-Chinese-wwm-ext by adding a classification head and freezing lower layers to reduce overfitting.
  - Quick check question: Paper freezes first 6 of 12 layers. What is the intended effect, and what tradeoff does this introduce?

## Architecture Onboarding

- Component map:
  Raw Chinese text -> RoBERTa tokenizer (character-level with WWM) -> RoBERTa-Chinese-wwm-ext (12 layers; 1-6 frozen, 7-12 fine-tuned) -> Linear classification head -> 17 labels (8 entity types × 2 [B/I] + O) -> Cross-entropy with inverse-frequency class weights

- Critical path:
  1. Data prep -> BIO annotation -> 80/10/10 split -> oversample minority classes
  2. Initialize from RoBERTa checkpoint -> add classification head -> freeze layers 1-6
  3. Train with AdamW, linear warmup, early stopping (patience=3)
  4. Optuna search over learning rate, batch size, weight decay, dropout
  5. Evaluate on test set with confusion matrix

- Design tradeoffs:
  - **Freezing 6/12 layers**: Reduces overfitting (gap=0.129) but may limit syntax adaptation.
  - **BIO for functional syntax**: Enables standard NER tools but conflicts with conflation (see O/M confusion).
  - **Excluding rare tags (concession, infinitives)**: Improves stability but sacrifices linguistic completeness.

- Failure signatures:
  - **High train F1 + low test F1**: Overfitting. Check early stopping; add regularization.
  - **Rare labels misclassified as common ones**: Imbalance insufficiently addressed. Verify oversampling and class weights.
  - **O/M confusion (~5%)**: Inherent linguistic ambiguity; may need multi-label prediction.
  - **Boundary errors on long spans**: Model struggles with complex NPs; consider constituency features.

- First 3 experiments:
  1. **Baseline**: Train with defaults (lr=2e-5, batch=32, 10 epochs). Measure train/val/test F1. Compare to reported F1=0.852.
  2. **Ablate freezing**: Test (a) no layers frozen vs. (b) 8 layers frozen. Compare overfitting gaps.
  3. **Rare label inspection**: Extract all Negator (N) and Operator (O) test predictions. Manually classify errors as annotation ambiguity vs. insufficient signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model be extended to map the identified functional syntactic elements to their corresponding semantic functions (e.g., transitivity, mood, theme-rheme)?
- Basis in paper: [explicit] The conclusion states that future research should "investigate the correspondence between different structural components and semantic functions such as transitivity, mood, voice, polarity, and theme-rheme structure."
- Why unresolved: The current study focused strictly on identifying structural elements (Subject, Main Verb, etc.) without integrating the semantic layers required for full functional analysis.
- What evidence would resolve it: A multi-task model outputting both syntactic labels and semantic tags for the same input text.

### Open Question 2
- Question: Does expanding the training data beyond the 4,100-sentence news corpus significantly reduce the observed overfitting gap and improve generalization to other text types?
- Basis in paper: [explicit] Section 4.2 notes the model "approached its performance ceiling under the current data scale" and lists "corpus expansion" and "increasing the diversity of annotated data" as necessary future work.
- Why unresolved: The current dataset is small and domain-specific (People's Daily), resulting in a non-negligible overfitting gap (0.129).
- What evidence would resolve it: Improved F1 scores and a reduced overfitting gap when training on a larger, multi-genre corpus.

### Open Question 3
- Question: Can architectural modifications resolve boundary errors in complex nested constituents (e.g., distinguishing a relative clause from its head noun)?
- Basis in paper: [inferred] The error analysis (Section 4.3) highlights that the model incorrectly splits complex constituents (e.g., "people who hide in the bathroom to smoke"), failing to treat the nested structure as a single Complement.
- Why unresolved: The current token-classification approach appears to struggle with the semantic unity of complex phrases containing multiple internal syntactic elements.
- What evidence would resolve it: Successful identification of correct entity boundaries in nested clauses similar to the error example in Figure 4.

## Limitations
- Limited annotation guidelines with only one example provided, making it difficult to assess inter-annotator agreement
- Partial effectiveness of class imbalance mitigation, with rare labels (Negator: 40 samples, Adjunct: 121 samples) still showing weaker performance
- 5% misclassification rate between Operator and Main Verb categories suggesting either annotation ambiguity or model limitations

## Confidence
- **High Confidence**: The F1 score of 0.852 is well-supported by the methodology and evaluation setup. The architectural choices (RoBERTa-Chinese-wwm-ext with BIO tagging) are standard and appropriately applied.
- **Medium Confidence**: The claim that this is the "first RoBERTa-based functional syntax annotation model" is plausible but difficult to verify without exhaustive literature review. The comparative analysis against other models is reasonable but could benefit from additional baselines.
- **Medium Confidence**: The effectiveness of the class imbalance mitigation strategies is demonstrated but incomplete - rare labels still underperform, suggesting the interventions may be insufficient for the structural complexity involved.

## Next Checks
1. **Inter-annotator Agreement Study**: Have two additional linguists independently annotate 100 sentences from the dataset. Calculate Cohen's kappa to quantify annotation consistency, particularly for boundary cases between Operator and Main Verb categories. This would reveal whether the 5% confusion rate reflects true linguistic ambiguity or inconsistent annotation.

2. **Ablation of Class Imbalance Interventions**: Train two additional models - one without class weighting and one without oversampling. Compare per-class F1 scores to isolate which intervention (weighting vs oversampling) contributes more to rare label performance. This would validate whether the current combination is optimal or if one approach dominates.

3. **Multi-label Prediction Experiment**: Modify the model to output multiple functional labels per token instead of single BIO predictions. Retrain and evaluate on the same test set, measuring whether this resolves the Operator/Main Verb conflation while introducing new challenges (e.g., increased computational complexity or label dependencies).