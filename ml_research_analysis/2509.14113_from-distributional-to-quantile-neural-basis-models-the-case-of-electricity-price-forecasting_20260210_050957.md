---
ver: rpa2
title: 'From Distributional to Quantile Neural Basis Models: the case of Electricity
  Price Forecasting'
arxiv_id: '2509.14113'
source_url: https://arxiv.org/abs/2509.14113
tags:
- neural
- forecasting
- electricity
- quantile
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Quantile Neural Basis Model (QNBM), a
  probabilistic forecasting method that incorporates interpretability principles from
  Quantile Generalized Additive Models into an end-to-end neural network training
  framework. The approach leverages shared basis decomposition and weight factorization
  to avoid parametric distributional assumptions while maintaining competitive performance.
---

# From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting

## Quick Facts
- **arXiv ID**: 2509.14113
- **Source URL**: https://arxiv.org/abs/2509.14113
- **Reference count**: 35
- **Primary result**: QNBM achieves probabilistic forecasting performance comparable to distributional and quantile regression neural networks while providing interpretable shape function mappings for electricity price prediction.

## Executive Summary
This work introduces the Quantile Neural Basis Model (QNBM), which extends Neural Basis Models from distributional to quantile regression by directly predicting conditional quantiles without parametric distributional assumptions. The approach leverages shared basis decomposition and weight factorization to maintain interpretability while achieving competitive probabilistic forecasting performance. Experimental evaluation on day-ahead electricity price forecasting in German and Belgian markets demonstrates that QNBM provides valuable insights into model behavior through learned nonlinear mappings from input features to output predictions across the forecasting horizon.

## Method Summary
QNBM implements a shared neural network that produces basis functions, which are then linearly combined through factorized weight matrices to create feature-specific shape functions. These shape functions are further combined via a low-rank tensor factorization to predict conditional quantiles directly. The model is trained using Pinball loss across 99 percentiles and employs weekly recalibration with ensemble averaging over multiple random initializations. The architecture maintains the interpretability principles of Generalized Additive Models while operating within an end-to-end neural network training framework.

## Key Results
- QNBM achieves CRPS performance aligned with distributional NBMLSS architecture on German and Belgian electricity markets
- The model avoids excessive negative price spikes seen in distributional models, particularly near zero and negative price settlements
- QNBM provides interpretable shape functions that reveal nonlinear relationships between input features and forecasted quantiles across the 24-hour horizon

## Why This Works (Mechanism)

### Mechanism 1: Shared Basis Decomposition with Linear Projections
- **Claim**: Decomposing features through shared neural basis functions combined via learned linear projections enables interpretable shape function extraction without sacrificing predictive accuracy
- **Mechanism**: A shared neural network produces basis functions that are aggregated into feature-specific shape functions, which are then combined into horizon-specific quantile estimates through factorized weight tensors
- **Core assumption**: Features can be meaningfully represented through additive combinations of shared nonlinear basis functions
- **Evidence anchors**: Figures 5-6 show extracted shape functions; Equations 1-3 formalize the decomposition; performance comparable to NBMLSS architecture
- **Break condition**: High concurvity among features may cause unstable shape function interpretations across recalibration runs

### Mechanism 2: Nonparametric Quantile Direct Mapping
- **Claim**: Directly predicting conditional quantiles bypasses parametric distributional constraints, enabling more flexible tail behavior
- **Mechanism**: Final layer maps directly to output-conditioned quantiles trained via Pinball loss, avoiding estimation of distribution parameters
- **Core assumption**: Individual quantiles can be sufficiently regularized via shared basis structure to avoid quantile crossing
- **Evidence anchors**: Avoids excessively large tail spikes seen in distributional models; trained directly on 99 percentiles
- **Break condition**: Quantile crossing occurs if predicted quantiles violate ordering constraints

### Mechanism 3: Low-Rank Tensor Factorization for Scalability
- **Claim**: Approximating large weight tensors via low-rank factorization reduces memory and computational overhead while improving generalization
- **Mechanism**: Weight tensor is reshaped and factorized as M ≈ AB^⊤ with rank r << m, n to compress the mapping from shape functions to quantile outputs
- **Core assumption**: True weight tensors lie approximately on a low-dimensional manifold
- **Evidence anchors**: Rank parameter set to r=16; computational benefits demonstrated through factorization
- **Break condition**: Under-expression occurs if true tensor rank exceeds factorization rank, degrading quantile accuracy

## Foundational Learning

- **Concept: Generalized Additive Models (GAMs)**
  - **Why needed here**: QNBM inherits GAM structure where output is additive combination of feature shape functions
  - **Quick check question**: Can you explain why additive models trade interaction modeling for interpretability?

- **Concept: Quantile Regression and Pinball Loss**
  - **Why needed here**: Model is trained by minimizing Pinball loss across 99 percentiles
  - **Quick check question**: For γ=0.95, does Pinball loss penalize over-prediction or under-prediction more heavily?

- **Concept: Concurvity in Additive Models**
  - **Why needed here**: Paper explicitly flags concurvity as a limitation affecting shape function stability
  - **Quick check question**: If two features are nonlinearly correlated, what might happen to their respective shape functions across different model initializations?

## Architecture Onboarding

- **Component map**: Input -> RevIN normalization -> Shared basis network -> Factorized projections -> Quantile outputs -> Pinball loss -> Backprop through factorized weights
- **Critical path**: Flattened conditioning vector passes through RevIN normalization, shared MLP with ReLU activations and dropout, then through low-rank factorized matrices to produce quantile predictions for each horizon step
- **Design tradeoffs**: Rank r balances memory/regularization vs capacity (r=16 used); basis function count n_z balances expressivity vs overfitting (32-128 range); quantile set size affects CRPS approximation quality
- **Failure signatures**: Quantile crossing (check if q̂_γ1 > q̂_γ2 for γ1 < γ2); unstable shape functions across recalibrations; overconfident intervals (low PICP)
- **First 3 experiments**:
  1. Reproduce Table II results on German market with r=16; verify CRPS within ±0.1 of reported values
  2. Sweep r ∈ {4, 8, 16, 32} and plot CRPS vs rank to identify performance cliffs
  3. Audit test set for quantile crossing rate; investigate monotonicity regularization if >0.1% crossing occurs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can automatic feature selection methods or Rashomon set approximations successfully resolve concurvity issues in QNBM's additive structure?
- **Basis**: Authors plan to investigate concurvity using Rashomon set approximation and automatic feature selection
- **Why unresolved**: Concurvity (nonlinear multicollinearity) complicates isolation of individual feature effects
- **What evidence would resolve it**: Demonstration of feature selection pipeline reducing redundancy without degrading CRPS scores

### Open Question 2
- **Question**: How can nonlinear interactions among shape functions be integrated without compromising interpretability?
- **Basis**: Nonlinear interactions among shape functions remain unexplored in literature
- **Why unresolved**: Current model assumes additive effects; interaction terms increase complexity and risk obscuring transparent mappings
- **What evidence would resolve it**: Extension including interaction terms with clear visualization or explanation metrics

### Open Question 3
- **Question**: To what extent do additional regularization techniques mitigate quantile overfitting and overconfidence?
- **Basis**: Overconfident intervals in baseline experiments warrant exploration of additional regularization
- **Why unresolved**: Quantile regression models tend to overfit specific distribution areas despite Dropout
- **What evidence would resolve it**: Ablation studies showing improved PICP scores with specific regularization methods

## Limitations

- **Quantile crossing uncertainty**: Paper does not report quantile crossing rates, making it difficult to assess nonparametric quantile assumption validity
- **Implementation details missing**: Exact RevIN layer placement and precise feature composition remain unspecified, affecting reproducibility
- **Rank sensitivity unexplored**: Rank parameter (r=16) chosen empirically without systematic analysis of capacity limitations in heterogeneous markets

## Confidence

- **High Confidence**: Mechanism 1 (Shared Basis Decomposition) - well-formalized with supporting visual evidence
- **Medium Confidence**: Mechanism 2 (Nonparametric Quantile Mapping) - theoretical justification sound but empirical validation of quantile crossing avoidance missing
- **Medium Confidence**: Mechanism 3 (Low-Rank Factorization) - computational benefits demonstrated but rank sensitivity not thoroughly explored

## Next Checks

1. **Quantile Crossing Audit**: Compute percentage of test samples where quantile predictions cross across all horizons and quantile pairs
2. **Rank Sensitivity Analysis**: Systematically sweep r ∈ {4, 8, 16, 32} and report CRPS, PICP, and memory usage to identify performance cliffs
3. **Concurvity Impact Study**: Train models with increasingly correlated feature sets and measure variance in shape functions across random seeds to quantify stability degradation