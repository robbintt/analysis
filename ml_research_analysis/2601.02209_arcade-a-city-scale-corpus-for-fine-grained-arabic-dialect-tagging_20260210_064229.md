---
ver: rpa2
title: 'ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging'
arxiv_id: '2601.02209'
source_url: https://arxiv.org/abs/2601.02209
tags:
- dialect
- arabic
- speech
- audio
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARCADE, the first Arabic speech corpus annotated
  with city-level dialect granularity. The dataset contains 3,790 unique 30-second
  audio segments from 58 cities across 19 countries, with 6,907 annotations covering
  MSA and dialectal speech.
---

# ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging

## Quick Facts
- arXiv ID: 2601.02209
- Source URL: https://arxiv.org/abs/2601.02209
- Reference count: 26
- First Arabic speech corpus annotated with city-level dialect granularity

## Executive Summary
This paper introduces ARCADE, a comprehensive Arabic speech corpus designed to address the critical need for fine-grained dialect identification resources. The dataset comprises 3,790 unique 30-second audio segments from 58 cities across 19 countries, with 6,907 annotations covering Modern Standard Arabic and dialectal speech. Each clip was meticulously annotated by native Arabic speakers for multiple attributes including emotion, speech type, dialect category, and confidence levels. The corpus enables city-level modeling of Arabic dialects and supports multi-task learning approaches for speech analysis.

## Method Summary
The dataset was constructed from radio speech recordings, with each 30-second segment annotated by multiple native Arabic speakers. Annotators provided labels for emotion, speech type, dialect category, and confidence levels. The corpus covers 58 cities across 19 countries, with dialect annotations at city-level granularity. Audio quality was assessed through signal-to-noise ratio measurements, revealing geographic variations in recording quality. The dataset is publicly available and includes comprehensive metadata for each segment.

## Key Results
- 3,790 unique 30-second audio segments from 58 cities across 19 countries
- 6,907 annotations covering MSA and dialectal speech with high inter-annotator agreement (91.9% "sure" confidence)
- Geographic audio quality variations: North African and Gulf cities showed higher signal-to-noise ratios

## Why This Works (Mechanism)
The corpus addresses the gap in fine-grained Arabic dialect identification by providing city-level annotations that capture regional linguistic variations. The multi-task annotation approach enables comprehensive speech analysis beyond dialect identification alone. Radio speech as source material provides professionally produced content with good audio quality, while the 30-second segment length balances standardization with sufficient context for dialect recognition.

## Foundational Learning
1. **Arabic Dialect Continuum** - why needed: Understanding continuous regional variations rather than discrete categories; quick check: Compare adjacent city annotations for gradual vs abrupt changes
2. **Speech Signal Processing** - why needed: Audio quality assessment and feature extraction; quick check: Verify SNR measurements correlate with perceived audio clarity
3. **Multi-task Learning** - why needed: Joint modeling of emotion, speech type, and dialect improves overall performance; quick check: Compare single-task vs multi-task model performance

## Architecture Onboarding
Component map: Radio Source -> Audio Segmentation -> Multi-Annotator Labeling -> Quality Assessment -> Public Release

Critical path: Raw radio recordings → 30-second segmentation → triple annotation pipeline → dialect classification model training

Design tradeoffs: Radio speech provides quality control but may not represent spontaneous conversation; city-level granularity increases complexity but captures finer dialectal features

Failure signatures: Inconsistent annotations across annotators; poor audio quality affecting feature extraction; dialect ambiguity in border regions

First experiments:
1. Train baseline dialect classifier on MSA vs dialect distinction
2. Evaluate inter-annotator agreement metrics across different dialect regions
3. Assess audio quality impact on dialect classification accuracy

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Geographic representativeness may be uneven across 19 countries
- Radio speech may not capture spontaneous conversational dialects
- 30-second segments might miss longer dialectal features and code-switching patterns
- Limited annotator diversity could affect reliability for less distinct dialect regions

## Confidence
- Corpus creation methodology: High confidence - clearly described procedures following established practices
- Dialect annotation quality: Medium confidence - high inter-annotator agreement but potential annotator familiarity bias not fully explored
- Audio quality analysis: Medium confidence - SNR measurements provided but performance validation lacking

## Next Checks
1. Conduct speaker diversity analysis to quantify demographic representation across the 58 cities, including age, gender, and socioeconomic factors
2. Perform ablation studies on dialect classification accuracy using different segment lengths (15s, 30s, 60s) to determine optimal annotation duration
3. Validate the dataset's utility by training and evaluating baseline dialect identification models, comparing performance across geographic regions and speech types (MSA vs dialectal)