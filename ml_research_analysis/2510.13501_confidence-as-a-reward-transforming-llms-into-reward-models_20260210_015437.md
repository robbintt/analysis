---
ver: rpa2
title: 'Confidence as a Reward: Transforming LLMs into Reward Models'
arxiv_id: '2510.13501'
source_url: https://arxiv.org/abs/2510.13501
tags:
- reward
- confidence
- arxiv
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Confidence-as-a-Reward (CRew), a training-free
  approach that uses the mean token-level probability of a model's final answer as
  a reward signal for evaluating solutions, particularly for close-ended tasks. CRew
  is based on the observation that a model's confidence in its final answer correlates
  with its correctness.
---

# Confidence as a Reward: Transforming LLMs into Reward Models

## Quick Facts
- arXiv ID: 2510.13501
- Source URL: https://arxiv.org/abs/2510.13501
- Authors: He Du; Bowen Li; Chengxing Xie; Chang Gao; Kai Chen; Dacheng Tao
- Reference count: 21
- Primary result: Introduces a training-free reward model using LLM confidence that outperforms other training-free methods and rivals trained reward models on mathematical tasks

## Executive Summary
This paper introduces Confidence-as-a-Reward (CRew), a training-free approach that uses the mean token-level probability of a model's final answer as a reward signal for evaluating solutions, particularly for close-ended tasks. The method is based on the observation that a model's confidence in its final answer correlates with its correctness. The authors also propose CRew-DPO, a training method that constructs preference data from confidence scores and correctness signals to further enhance the model's judging capabilities. Experimental results on mathematical reasoning tasks (MATH500 and RewardMATH benchmarks) show that CRew outperforms other training-free reward methods and even surpasses most trained reward models. Additionally, CRew demonstrates strong correlation with model reasoning performance and can effectively filter high-quality training data. Finetuning with CRew-DPO further improves judging capabilities and outperforms existing self-training methods.

## Method Summary
CRew leverages the mean token-level probability of a model's final answer as a proxy for confidence, which serves as a reward signal for evaluating solutions. For close-ended problems, the method extracts the final answer token (e.g., using delimiters like `\boxed{}`) and computes the average probability across all tokens in the answer sequence. This confidence score correlates with correctness, enabling CRew to function as a training-free reward model. The authors further introduce CRew-DPO, which constructs preference data by comparing confidence scores between correct and incorrect answers, then fine-tunes the model using this synthetic preference data. This approach enhances the model's ability to judge solution quality and generalizes well across mathematical domains.

## Key Results
- CRew outperforms other training-free reward methods and rivals trained reward models on MATH500 and RewardMATH benchmarks
- CRew demonstrates strong correlation (0.83) between reasoning performance and evaluation capability
- Fine-tuning with CRew-DPO improves judging capabilities and outperforms existing self-training methods
- Counter-intuitively, training on low-confidence correct solutions yields better reasoning performance than high-confidence solutions

## Why This Works (Mechanism)
The mechanism relies on the correlation between a model's confidence (measured as mean token probability) in its final answer and the actual correctness of that answer. For close-ended problems, this confidence serves as a reliable proxy for solution quality. By using this confidence signal both as a direct reward and as a basis for constructing preference pairs, the model learns to better distinguish between high and low quality solutions. The strong correlation between reasoning ability and evaluation capability suggests that models that can generate good solutions are also capable of judging them, enabling the transfer of evaluation skills through fine-tuning.

## Foundational Learning
- **Token-level probability calculation**: Understanding how to extract and average token probabilities from LLM outputs is essential for computing confidence scores
- **Close-ended problem identification**: The method requires identifying problems with discrete, verifiable final answers to extract confidence signals
- **Preference learning**: Constructing preference pairs from confidence scores enables the use of DPO-style fine-tuning to enhance judging capabilities
- **Correlation analysis**: Understanding the relationship between confidence scores and correctness is crucial for validating the reward signal
- **Data filtering techniques**: The ability to use confidence scores to filter high-quality training data improves overall model performance
- **Self-training methodology**: The CRew-DPO approach demonstrates how self-training can enhance evaluation capabilities without external labels

## Architecture Onboarding

**Component Map**: LLM -> Token Probability Extraction -> Confidence Score Calculation -> Reward Signal -> Preference Pair Construction -> Fine-tuning

**Critical Path**: The critical path involves extracting the final answer, computing token probabilities, averaging them to obtain confidence scores, and using these scores as rewards or for constructing preference data. This path directly impacts the quality of the reward signal and subsequent model improvements.

**Design Tradeoffs**: The method trades generality for simplicity by focusing on close-ended tasks where final answers are easily identifiable. While this limits applicability to open-ended tasks, it enables a training-free approach that leverages the model's existing capabilities without requiring additional labeled data.

**Failure Signatures**: The approach may fail when models are poorly calibrated, producing high-confidence but incorrect answers. It may also struggle with out-of-distribution data where the correlation between confidence and correctness breaks down.

**First Experiments**:
1. Compute CRew scores on a held-out set of mathematical problems and correlate with ground truth correctness
2. Compare CRew rankings against human-annotated quality scores on sample solutions
3. Test CRew-DPO fine-tuning on a small mathematical dataset and evaluate performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CRew mechanism be effectively adapted for open-ended generation tasks where a discrete "final answer" token does not exist?
- **Basis in paper:** [Explicit] The paper explicitly restricts its methodology to "close-ended problems where the final answer can be effortless identified," relying on extracting specific tokens (e.g., `\boxed{}`) to calculate the mean probability.
- **Why unresolved:** The current definition of confidence depends entirely on the probability of a concise, verifiable final token sequence. Open-ended tasks (e.g., summarization, creative writing) lack these distinct targets, making the proposed extraction and averaging method inapplicable.
- **What evidence would resolve it:** A modified formulation of CRew that utilizes token-level or segment-level confidence over reasoning chains or semantic embeddings for open-ended benchmarks, showing comparable performance to close-ended results.

### Open Question 2
- **Question:** Why does fine-tuning on correct but *low-confidence* data yield better reasoning performance than using high-confidence data?
- **Basis in paper:** [Explicit] Section 4.4 (Data Filtering) notes the counter-intuitive finding that models trained on low-confidence correct solutions significantly outperform those trained on high-confidence solutions, suggesting only that they "contain more potential variability."
- **Why unresolved:** The paper observes this phenomenon but does not provide a theoretical justification for why "less certain" correct reasoning paths are more effective for learning than the model's "best" (highest confidence) reasoning paths.
- **What evidence would resolve it:** An analysis of the gradient diversity or information content within low-confidence vs. high-confidence reasoning trajectories, linking higher "variability" to improved model generalization.

### Open Question 3
- **Question:** Does the evaluation capability acquired through CRew-DPO generalize beyond the mathematical domain?
- **Basis in paper:** [Inferred] Section 5.3 (Generalization) tests transfer learning between MATH and GSM8K. While the authors claim this demonstrates generalization, both are mathematical reasoning tasks, leaving cross-domain transfer unexplored.
- **Why unresolved:** It is unclear if the strong correlation (0.83) between reasoning and evaluation is specific to the formal logic of mathematics or if CRew-DPO teaches a universal verification skill applicable to other domains like coding or logic.
- **What evidence would resolve it:** Experiments training CRew-DPO on mathematical data and evaluating the reward model's performance on out-of-distribution domains such as code generation (HumanEval) or logical entailment.

## Limitations
- CRew is primarily designed for close-ended tasks and its effectiveness on open-ended tasks or tasks requiring subjective judgment is unclear
- The approach assumes that a model's confidence in its final answer correlates with its correctness, which may not always hold true, especially for models with poor calibration or when dealing with out-of-distribution data
- The use of confidence scores as a reward signal may introduce bias towards overconfident models or models that tend to provide high-confidence but incorrect answers

## Confidence

| Claim | Confidence |
|-------|------------|
| CRew as an effective training-free reward method | High |
| CRew outperforming trained reward models | Medium |
| CRew's effectiveness on close-ended tasks | High |
| CRew's generalization to other domains | Low |

## Next Checks
1. Evaluate CRew on open-ended tasks and tasks requiring subjective judgment to assess its effectiveness beyond close-ended tasks
2. Conduct experiments with models of varying calibration levels to determine the robustness of CRew to model calibration issues
3. Test CRew on tasks from diverse domains, such as natural language understanding, commonsense reasoning, and knowledge-based question answering, to evaluate its generalizability