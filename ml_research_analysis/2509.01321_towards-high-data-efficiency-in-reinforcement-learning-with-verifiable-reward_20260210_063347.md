---
ver: rpa2
title: Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward
arxiv_id: '2509.01321'
source_url: https://arxiv.org/abs/2509.01321
tags:
- training
- samples
- data
- selection
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEPO is a data-efficient reinforcement learning framework for large
  reasoning models that combines optimized offline and online data selection strategies.
  The offline phase uses PageRank-weighted Determinantal Point Process pruning and
  difficulty-aware normal distribution sampling to curate high-quality subsets emphasizing
  diversity, influence, and appropriate difficulty.
---

# Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward

## Quick Facts
- **arXiv ID:** 2509.01321
- **Source URL:** https://arxiv.org/abs/2509.01321
- **Reference count:** 33
- **Primary result:** DEPO achieves 1.85× speedup on AIME24 and 1.66× on AIME25 using only 20% of training data.

## Executive Summary
DEPO is a data-efficient reinforcement learning framework for large reasoning models that combines optimized offline and online data selection strategies. The offline phase uses PageRank-weighted Determinantal Point Processes pruning and difficulty-aware normal distribution sampling to curate high-quality subsets emphasizing diversity, influence, and appropriate difficulty. The online phase employs sample-level explorability metrics to dynamically filter rollouts and incorporates dynamic replay for under-explored samples. Experiments across five reasoning benchmarks with three different LLMs show DEPO consistently outperforms competitive baselines.

## Method Summary
DEPO is a two-stage pipeline for training reasoning models with verifiable rewards. The offline phase constructs a similarity graph from sample embeddings, applies PageRank-weighted Determinantal Point Processes to prune 50% of samples while preserving diversity and influence, then performs difficulty-aware sampling using a normal distribution centered at μ=0.5 to select 20% of the original dataset. The online phase calculates an explorability metric based on historical entropy and advantage for each sample, filtering rollouts below threshold λ=1.5 while maintaining a replay buffer for under-explored samples. The framework is implemented with Verl and vLLM, using GRPO algorithm with 8 rollouts per prompt and batch size 256.

## Key Results
- DEPO achieves 1.85× speedup on AIME24 and 1.66× on AIME25 using only 20% of training data
- The framework consistently outperforms competitive baselines across five reasoning benchmarks
- Dynamic replay and explorability filtering reduce rollout computational costs significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pruning data via PageRank-weighted Determinantal Point Processes (DPP) prior to training may improve sample efficiency by eliminating redundancy while retaining influential examples.
- **Mechanism:** The framework constructs a similarity graph of sample embeddings. It then maximizes the determinant of a kernel matrix $L_Y$ that combines sample similarity (diversity) and PageRank scores (influence). A greedy algorithm selects a subset that spans the feature space.
- **Core assumption:** Embedding similarity acts as a proxy for semantic redundancy in reasoning tasks, and "influence" (centrality in the graph) correlates with pedagogical value.
- **Evidence anchors:**
  - [section 2.2.2]: Describes maximizing $\det(L_Y)$ to unify diversity and influence.
  - [corpus]: Related work like *Online Difficulty Filtering* discusses difficulty, but the specific use of DPP for diversity is distinct to this paper's offline phase.
- **Break condition:** If the embedding space fails to capture reasoning complexity (e.g., distinct syntax but identical logic), the DPP may retain semantically redundant samples.

### Mechanism 2
- **Claim:** Sampling training data with a difficulty distribution approximating a normal curve ($\mu \approx 0.5$) accelerates convergence compared to sampling uniformly or focusing solely on hard examples.
- **Mechanism:** After initial pruning, the model performs rollouts to calculate accuracy (difficulty). Samples are selected with probability proportional to the normal density function centered on the mean difficulty, filtering out trivial (accuracy $\approx 1$) or impossible (accuracy $\approx 0$) cases.
- **Core assumption:** Samples of "medium" difficulty provide the highest information gain for the current policy, aligning with the model's "Zone of Proximal Development."
- **Evidence anchors:**
  - [section 2.2.3]: Eq. 3 defines the probability $p_i$ using the standard normal density $\phi$.
  - [figure 6a]: Shows "Normal Distribution" sampling outperforms "Random" and "Stratified" sampling.
- **Break condition:** If the model's capability shifts rapidly during training, the static offline difficulty estimates become obsolete, potentially starving the model of necessary "hard" examples later in training.

### Mechanism 3
- **Claim:** Dynamic rollout filtering based on an "explorability" metric significantly reduces computational costs by skipping inference for samples with low learning potential.
- **Mechanism:** The system calculates a sample-level score based on historical entropy (sliding window) and normalized advantage. If a sample has consistently low entropy (policy is confident/stable) or provides little advantage signal, it is skipped. A replay buffer ensures under-explored samples are eventually revisited.
- **Core assumption:** High entropy correlates with exploration potential, and low entropy indicates the policy has stabilized (or overfitted) on that specific sample, yielding diminishing returns for further rollouts.
- **Evidence anchors:**
  - [section 2.3.1]: Eq. 4 defines the explorability metric $E$ using entropy and advantage.
  - [abstract]: Mentions "reducing substantial rollout computational costs."
- **Break condition:** If "confident wrong" answers (low entropy but high error) are not filtered out by the verifiable reward signal $V(o, a)$, the model might ignore samples requiring correction.

## Foundational Learning

- **Concept:** **Determinantal Point Processes (DPPs)**
  - **Why needed here:** DPPs are the mathematical engine for the offline selection phase. Understanding them is required to modify the diversity/influence trade-off ($L_Y$ kernel).
  - **Quick check question:** How does maximizing the determinant of a subset's kernel matrix ensure diversity better than simply clustering?

- **Concept:** **Entropy in Reinforcement Learning**
  - **Why needed here:** Entropy is the primary signal for the "Explorability" metric. You must understand the trade-off between exploitation (low entropy) and exploration (high entropy) to tune the filtering thresholds.
  - **Quick check question:** Why might high entropy in a "negative" (incorrect) rollout still be considered valuable for exploration in this framework?

- **Concept:** **Verifiable Rewards (RLVR)**
  - **Why needed here:** The entire framework relies on binary signals (correct/incorrect) from verifiers rather than dense reward models. This constrains how "difficulty" and "advantage" are calculated.
  - **Quick check question:** How does the reliance on a binary verifier affect the stability of the "advantage" estimation in the explorability metric?

## Architecture Onboarding

- **Component map:** Embedding extraction -> PageRank-weighted DPP -> Difficulty Sampler -> RL Training Loop (Policy Model -> Rollout Generator -> Online Filter -> Optimizer) -> Replay Buffer
- **Critical path:** The **Explorability Calculation** (Eq. 4) is the runtime bottleneck. It requires maintaining a sliding window of entropy/reward history per sample. Efficient implementation of this history lookup is crucial for the 1.85x speedup claim.
- **Design tradeoffs:**
  - **Offline Static vs. Online Dynamic:** DEPO relies on offline difficulty estimation to bootstrap. If the initial model is very weak, these estimates may be noisy.
  - **Replay Ratio ($\rho$):** Increasing $\rho$ ensures coverage but reduces the "skip rate" of rollouts, diminishing efficiency gains.
  - **Window Size ($w$):** A larger window smooths explorability estimates but delays the response to the model's changing capabilities.
- **Failure signatures:**
  - **Early Plateauing:** Accuracy stops improving early → Likely μ (difficulty mean) is set too high (too hard) or replay ratio ρ is too low.
  - **Collapse of Reasoning:** Output length decreases drastically without accuracy gain → Explorability threshold λ may be too aggressive, filtering out necessary high-variance samples.
  - **Slow Convergence:** Training time increases rather than decreases → The overhead of computing DPP or Explorability exceeds the cost of the saved rollouts (likely due to implementation inefficiency in the history tracking).
- **First 3 experiments:**
  1. **Sanity Check (Offline):** Train on a small subset (e.g., 1k samples) selected via DPP vs. Random vs. Top-k difficulty. Verify that DPP provides a better "accuracy per sample" ratio.
  2. **Ablation (Online):** Run the full pipeline but fix the "Explorability" threshold to include 100% of samples (emulating GRPO). Measure the theoretical speedup gap vs. the actual performance drop.
  3. **Hyperparameter Sensitivity:** Sweep the decay rate d (Eq. 6). If d is too high, the model stops exploring too early. Plot "Rollouts Skipped" vs. "Final Accuracy" to find the knee of the curve.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the entropy threshold (λ) for filtering high-entropy negative rollouts be derived adaptively during training instead of being set as a static hyperparameter?
- **Basis:** [inferred] Appendix D.1 demonstrates that performance is sensitive to the threshold λ; setting it too low excludes useful samples, while setting it too high introduces noise, yet the paper relies on a fixed value (1.5).

### Open Question 2
- **Question:** Would a dynamic curriculum strategy that adjusts the difficulty mean (μ) over time outperform the fixed normal distribution sampling used in the offline phase?
- **Basis:** [inferred] Section 3.4 notes that easier samples accelerate early training while harder samples improve final convergence. The current method fixes μ to balance these, leaving the potential of a time-varying μ unexplored.

### Open Question 3
- **Question:** Does the relationship between data efficiency (20% subset) and performance hold when scaling to significantly larger models (e.g., 70B+ parameters) or different architectures?
- **Basis:** [inferred] Experiments are limited to 7B and 8B models (Table 1). It is unclear if the "explorability" dynamics of smaller models transfer to larger models with different emergent reasoning capabilities.

## Limitations
- The empirical claims rely on proprietary benchmarks (AIME25) where independent verification is difficult
- The offline PageRank-weighted DPP relies on sample embeddings that are not standardized
- The "explorability" metric assumes that high entropy correlates with learning potential, but this conflates uncertainty from exploration vs. uncertainty from confusion

## Confidence
- **High confidence**: The offline pruning mechanism (DPP + difficulty sampling) is technically sound and the theoretical justification is clear
- **Medium confidence**: The online explorability filtering provides measurable rollout savings, but the claim that it directly causes the accuracy improvements is less certain
- **Low confidence**: The generalization claim across different LLMs (DeepSeek, Qwen, Llama) is based on only three models

## Next Checks
1. **Offline selector ablation**: Train DEPO with random sampling instead of difficulty-aware normal distribution. Measure if the 1.85× speedup on AIME24 disappears, isolating the contribution of the offline phase.
2. **Explorability threshold sweep**: Run the full pipeline with λ ∈ [0.5, 2.5]. Plot "Rollouts Skipped" vs. "Final Accuracy" to confirm there is a non-trivial optimum and the reported λ=1.5 is not an arbitrary choice.
3. **Out-of-distribution test**: Apply DEPO to a non-mathematical reasoning task (e.g., commonsense QA). If the method relies on the verifiable reward signal being a perfect oracle, performance may degrade significantly on tasks with ambiguous answers.