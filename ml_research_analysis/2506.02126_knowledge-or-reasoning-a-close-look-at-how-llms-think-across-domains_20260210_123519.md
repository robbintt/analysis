---
ver: rpa2
title: Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains
arxiv_id: '2506.02126'
source_url: https://arxiv.org/abs/2506.02126
tags:
- reasoning
- knowledge
- medical
- arxiv
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the roles of knowledge and reasoning in
  the reasoning processes of Large Language Models (LLMs) by introducing a fine-grained
  evaluation framework that decomposes reasoning into two distinct components: knowledge
  correctness and reasoning quality. The framework employs two novel metrics: Knowledge
  Index (KI), which assesses the factual accuracy of knowledge presented in each reasoning
  step, and Information Gain (InfoGain), which measures the informativeness of reasoning
  steps by quantifying the reduction in uncertainty towards the final answer.'
---

# Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains

## Quick Facts
- arXiv ID: 2506.02126
- Source URL: https://arxiv.org/abs/2506.02126
- Reference count: 40
- Key outcome: This work investigates the roles of knowledge and reasoning in the reasoning processes of Large Language Models (LLMs) by introducing a fine-grained evaluation framework that decomposes reasoning into two distinct components: knowledge correctness and reasoning quality.

## Executive Summary
This paper introduces a novel evaluation framework to dissect LLM reasoning into knowledge correctness and reasoning quality, revealing critical trade-offs between different training methods. Through systematic experiments on medical and mathematical domains, the study finds that supervised fine-tuning (SFT) improves domain knowledge but reduces reasoning efficiency, while reinforcement learning (RL) enhances both by pruning incorrect knowledge paths. The research highlights that domain-specific knowledge is crucial in medical tasks, whereas reasoning is more important in mathematics, and that RL is effective in improving both aspects in medical reasoning.

## Method Summary
The study employs a decomposition-based evaluation framework using two key metrics: Knowledge Index (KI) for factual accuracy and Information Gain (InfoGain) for reasoning quality. The framework uses GPT-4o to decompose model reasoning into discrete steps, then evaluates each step's knowledge consistency (via KI) and its contribution to answer certainty (via InfoGain using perplexity reduction). The authors train Qwen2.5-7B and DeepSeek-R1-Distill-Qwen-7B models with SFT on medical-o1 reasoning data and RL on RLHFlow math data, using specific hyperparameters for both training phases.

## Key Results
- SFT improves domain knowledge (KI) but reduces reasoning quality (InfoGain drops 38.9% on average)
- RL enhances both reasoning accuracy and knowledge correctness by pruning inaccurate or irrelevant knowledge
- General reasoning abilities from R1-distilled models do not transfer effectively to medical domains
- Domain-specific knowledge is crucial in medical tasks, while reasoning is more important in mathematics

## Why This Works (Mechanism)

### Mechanism 1: RL-Driven Knowledge Pruning
- **Claim:** Reinforcement Learning improves reasoning correctness not by injecting new facts, but by reinforcing trajectories that avoid factual errors.
- **Mechanism:** RL optimizes the policy to maximize rewards (correct final answers). The paper suggests this optimization process inherently penalizes reasoning paths containing hallucinated or irrelevant knowledge, effectively "pruning" them. This leads to higher Knowledge Index (KI) scores because the surviving paths are factually cleaner, rather than factually richer.
- **Core assumption:** The reward signal (final answer correctness) is sufficient to penalize intermediate factual errors without explicit process supervision.
- **Evidence anchors:**
  - [Abstract] "RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths..."
  - [Page 9, Figure 9] "RL enhances knowledge metrics by guiding the model to discard reasoning paths containing incorrect knowledge, rather than adding new facts."
  - [Corpus] The neighbor paper *SFT memorizes, RL generalizes* (Chu et al., 2025) supports the generalization capability of RL over memorization in SFT.

### Mechanism 2: SFT-Induced Reasoning Verbosement
- **Claim:** Supervised Fine-Tuning (SFT) improves domain knowledge (KI) at the expense of reasoning efficiency (InfoGain).
- **Mechanism:** SFT forces the model to mimic expert chains-of-thought. While this grounds the model in domain facts (raising KI), the paper argues it causes the model to adopt "verbose or suboptimal reasoning" patterns, such as redundant clarifications or unnecessary steps. This redundancy dilutes the informational contribution of each step, causing a drop in Information Gain.
- **Core assumption:** The drop in InfoGain (perplexity reduction per step) accurately reflects lower reasoning quality/efficiency rather than just a stylistic change.
- **Evidence anchors:**
  - [Abstract] "SFT raises final-answer accuracy... but often at the cost of reasoning quality: InfoGain drops by 38.9% on average."
  - [Page 7, Figure 7] Shows SFT adds "redundant clarification" steps that map to previous logic without adding new information.
  - [Corpus] *FineMedLM-o1* (arXiv:2501.09213) discusses enhancing reasoning from SFT to test-time training, implicitly acknowledging SFT limitations in complex reasoning.

### Mechanism 3: Domain-Reasoning Decoupling
- **Claim:** General reasoning capabilities (e.g., those distilled from math-heavy corpora) do not transfer effectively to knowledge-intensive domains like medicine.
- **Mechanism:** The "reasoning" learned by R1-distilled models is heavily biased toward symbolic manipulation (math/code). When applied to medicine, this reasoning structure conflicts with the need for dense, external factual retrieval. The model attempts to apply mathematical inference patterns to domain knowledge where they do not fit, leading to lower performance compared to a base model trained on domain data.
- **Core assumption:** The performance gap is due to a fundamental incompatibility in reasoning styles (symbolic vs. factual) rather than just a lack of medical data.
- **Evidence anchors:**
  - [Abstract] "The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain..."
  - [Page 6] "Since Qwen-R1 was primarily trained on R1-generated texts focused on math and code... medical domain knowledge... could conflict with its prior representations."

## Foundational Learning

- **Concept: Information Gain (InfoGain) via Perplexity**
  - **Why needed here:** This is the primary metric for "reasoning quality." It quantifies how much a single step reduces uncertainty about the final answer.
  - **Quick check question:** If a model takes 10 steps to solve a problem but the perplexity of the correct answer only drops in the last step, is the InfoGain high or low?

- **Concept: Knowledge Index (KI)**
  - **Why needed here:** This isolates "factuality" from "logic." It requires verifying extracted claims against a ground truth (e.g., textbooks), not just checking if the final answer is right.
  - **Quick check question:** Can a model have a high KI (correct facts) but low accuracy? (Hint: See Figure 1, Step 4).

- **Concept: Process Supervision vs. Outcome Supervision**
  - **Why needed here:** The paper analyzes SFT (process mimicry) vs. RL (outcome optimization). Understanding this tradeoff is key to interpreting why SFT boosts knowledge (mimicry of facts) while RL boosts efficiency (optimization of outcome).
  - **Quick check question:** Which training method is more likely to "hallucinate" a correct answer using wrong logic, SFT or RL?

## Architecture Onboarding

- **Component map:** Input (Q) -> Decomposer (GPT-4o) -> Reasoning Steps [s1, s2...] -> Evaluator Branch 1 (InfoGain: Qwen2.5-7B PPL) and Evaluator Branch 2 (KI: GPT-4o extraction + Retriever + GPT-4o judgment)
- **Critical path:** The **Decomposition** stage. The entire framework depends on GPT-4o's ability to accurately segment the reasoning into discrete steps and extract the specific knowledge claim within them. If decomposition fails (e.g., merging two logical leaps), InfoGain and KI metrics become noisy.
- **Design tradeoffs:**
  - **Cost vs. Automation:** Using GPT-4o for decomposition and judgment enables fine-grained analysis but creates a dependency on a proprietary model and introduces latency/cost.
  - **Metric Sensitivity:** InfoGain relies on an external "untrained" model for PPL calculation. If that model has different tokenization or knowledge than the model being tested, the uncertainty estimation may be skewed.
- **Failure signatures:**
  - **High KI, Low InfoGain:** The model is "babbling" correct facts that are irrelevant to the specific question derivation (common in SFT models).
  - **High InfoGain, Low KI:** The model is making logical leaps (high info) based on hallucinated premises (low KI).
- **First 3 experiments:**
  1. **Verify the SFT Penalty:** Take a base model and an SFT-finetuned variant. Run the evaluation pipeline on a subset of MedQA. Confirm that KI rises while InfoGain drops significantly (replicating the 38.9% drop finding).
  2. **RL Pruning Validation:** Compare an SFT model against an SFT+RL model on a case study (e.g., the ototoxicity example in Figure 9). Specifically, count the number of reasoning steps containing factually incorrect claims (should decrease with RL).
  3. **Domain Transfer Test:** Evaluate a math-specialized model (e.g., DeepSeek-R1-Distill) on MedMCQA. Check if the InfoGain is high (good reasoning structure) but KI is low (poor domain knowledge), confirming the decoupling mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the observed trade-offs between knowledge and reasoning quality generalize to larger model scales (e.g., 70B+ parameters) and different model architectures beyond the Qwen family?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "all experiments were conducted on the Qwen family of 7B-parameter models. ... We acknowledge that evaluating a broader range of models is necessary to assess the generality of our conclusions."
- **Why unresolved:** The study is deliberately scoped to 7B Qwen variants. It is unknown if SFT's tendency to reduce reasoning quality (measured by InfoGain) and RL's ability to prune incorrect knowledge are fundamental phenomena that persist at scale, or if they are artifacts of model capacity.
- **What evidence would resolve it:** Replicating the same evaluation framework (KI and InfoGain metrics) across a diverse set of large-scale models (e.g., Llama 3.1 70B/405B, DeepSeek-V3) during SFT and RL training would demonstrate the scalability and architectural dependence of the findings.

### Open Question 2
- **Question:** Can the Knowledge Index (KI) and Information Gain (InfoGain) metrics be adapted to evaluate structured, multi-faceted reasoning in domains like law or finance where knowledge sources and reasoning patterns differ significantly from medicine and math?
- **Basis in paper:** [explicit] In the Broader Impact discussion, the authors suggest: "Looking ahead, we believe our evaluation framework can be extended to other structured reasoning domains. In legal tasks, for instance, the IRAC structure offers a natural alignment... Adapting our knowledge-vs.-reasoning framework to these domains... offers a promising path."
- **Why unresolved:** The current framework relies on domain-specific decomposition prompts and knowledge retrieval (e.g., medical textbooks for KI). The effectiveness of this pipeline for domains with more ambiguous knowledge (e.g., legal precedents) or different reasoning structures (e.g., financial forecasting) is untested.
- **What evidence would resolve it:** Applying the metrics to benchmarks in a new domain (e.g., LegalBench) would be necessary. This would require defining domain-appropriate knowledge bases for KI and validating whether InfoGain correlates with expert assessments of reasoning quality in that domain.

### Open Question 3
- **Question:** What are the precise mechanisms by which Reinforcement Learning (RL) improves knowledge correctness (KI) without explicitly introducing new factual knowledge?
- **Basis in paper:** [explicit] The authors note this counter-intuitive finding: "This is particularly interesting given that RL introduces little new knowledge to the model... To further investigate, we conduct a case study revealing that RL enhances knowledge metrics by guiding the model to discard reasoning paths containing incorrect knowledge, rather than adding new facts."
- **Why unresolved:** The case study provides a qualitative example, but a mechanistic understanding is lacking. It is unclear whether RL is simply up-weighting existing correct knowledge retrieval, learning to better avoid known hallucinations, or restructuring internal representations to favor accurate knowledge recall.
- **What evidence would resolve it:** A causal analysis of model attention heads or internal states during RL training could reveal how the policy gradient signal modifies the model's propensity to retrieve or rely on correct vs. incorrect knowledge. Probing experiments before and after RL could quantify shifts in factual recall probability.

### Open Question 4
- **Question:** Is the failure of R1-distilled reasoning abilities to transfer to the medical domain via SFT/RL primarily due to domain-specific knowledge conflicts, or does the distilled reasoning pattern itself lack adaptability?
- **Basis in paper:** [explicit] The authors find: "The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL" and suggest this "may stem from a domain shift... [where] medical domain knowledge introduced during our post-training could conflict with its prior representations."
- **Why unresolved:** This is a hypothesis for the observed performance drop. It is not determined whether the issue is a fundamental brittleness of the distilled "reasoning style" when faced with novel knowledge, or if it is a solvable problem of negative transfer from the math/code-centric pre-training.
- **What evidence would resolve it:** An ablation study where the R1-distilled model is further pre-trained on a large, general medical corpus (before SFT/RL) could test if aligning the base knowledge mitigates the transfer issue. Alternatively, analyzing the entropy and attention patterns of the distilled model during medical tasks could reveal if the reasoning process itself becomes more rigid.

## Limitations
- The study is limited to 7B-parameter Qwen models, so scalability and architecture dependence remain untested.
- The knowledge base and exact retrieval procedure for KI are not fully specified, limiting reproducibility.
- The framework's dependency on GPT-4o for decomposition and judgment introduces potential biases and costs.

## Confidence
- **High:** The core finding that SFT improves domain knowledge (KI) while RL improves both knowledge and reasoning quality (InfoGain) is well-supported by multiple experiments and ablation studies.
- **Medium:** The mechanism of RL "pruning" incorrect knowledge is supported by case studies but lacks deeper causal analysis.
- **Medium:** The domain transfer failure of R1-distilled models is demonstrated but the exact cause (reasoning style vs. knowledge conflict) is speculative.

## Next Checks
1. **Replicate the SFT penalty finding:** Train a base model and SFT-finetuned variant on MedQA, then verify KI increases while InfoGain drops significantly (38.9% target).
2. **Validate RL pruning mechanism:** Compare SFT vs. SFT+RL on the ototoxicity case study, counting reasoning steps with factually incorrect claims.
3. **Test domain transfer hypothesis:** Evaluate DeepSeek-R1-Distill on MedMCQA to confirm high InfoGain but low KI, validating the reasoning-knowledge decoupling.