---
ver: rpa2
title: 'Heimdall: test-time scaling on the generative verification'
arxiv_id: '2504.10337'
source_url: https://arxiv.org/abs/2504.10337
tags:
- verification
- solution
- heimdall
- problems
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Heimdall, a long Chain-of-Thought verification
  LLM trained via reinforcement learning to accurately judge the correctness of solutions
  to competitive math problems. The key innovation is filtering out extreme problem
  cases during training to avoid bias toward problem difficulty rather than solution
  correctness.
---

# Heimdall: test-time scaling on the generative verification

## Quick Facts
- arXiv ID: 2504.10337
- Source URL: https://arxiv.org/abs/2504.10337
- Authors: Wenlei Shi; Xing Jin
- Reference count: 35
- Primary result: Heimdall achieves 94.5% verification accuracy on AIME2024, scaling to 97.5% with repeated sampling, and improves problem-solving accuracy from 54.2% to 70.0% (up to 83.3%) via Pessimistic Verification.

## Executive Summary
This paper introduces Heimdall, a long Chain-of-Thought verification LLM trained via reinforcement learning to accurately judge the correctness of solutions to competitive math problems. The key innovation is filtering out extreme problem cases during training to avoid bias toward problem difficulty rather than solution correctness. Heimdall achieves a 94.5% verification accuracy on AIME2024, scaling up to 97.5% with repeated sampling. The paper also proposes Pessimistic Verification, a unified algorithm for inference-time scaling that improves problem-solving accuracy from 54.2% to 70.0% (and up to 83.3%) on AIME2025 when combined with solver models. Human evaluations confirm Heimdall's strong generalization to out-of-domain tasks like math proofs. Finally, a prototype automatic knowledge discovery system demonstrates Heimdall's ability to detect flaws in synthetic datasets, revealing that nearly half of the data is flawed.

## Method Summary
Heimdall is trained as a long Chain-of-Thought verifier using reinforcement learning with PPO on math competition problems. The training pipeline generates 16 solutions per problem using DeepSeek-R1-Distill-Qwen-32B, applies rule-based labeling for correctness, and critically filters out problems where all solutions are correct or all are incorrect to avoid difficulty bias. The verifier is trained to output binary correctness judgments after reasoning chains. For inference, Pessimistic Verification combines multiple verifications per solution using a lower-confidence-bound selection formula that balances solver frequency bias against verification signal strength.

## Key Results
- Heimdall achieves 94.5% verification accuracy on AIME2024, scaling to 97.5% with 64 verifications
- Pessimistic Verification improves problem-solving accuracy from 54.2% to 70.0% on AIME2025
- Human evaluations confirm strong generalization to math proofs and out-of-domain tasks
- Prototype knowledge discovery system identifies flaws in nearly half of synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Data Filtering Prevents Difficulty Bias
Filtering extreme problem cases enables the verifier to learn solution correctness rather than problem difficulty. The authors identify two failure modes—easy problems with only correct solutions, and hard problems with only wrong solutions. Both lack contrastive examples, causing the model to associate problem features with difficulty rather than learning to identify reasoning errors. Removing these cases forces the model to attend to step-level correctness.

### Mechanism 2: Pessimistic Verification Balances Solver Bias and Verifier Signal
A lower-confidence-bound selection formula unifies majority voting and verification-based selection, outperforming both extremes. The algorithm treats solution selection as a multi-arm bandit where each answer is an arm. The formula `argmax(r(a_i) - α·ln(NM)/(N_i·M + 1))` adds an uncertainty penalty. When N_i (visits to an answer) is small, the penalty dominates and behavior collapses to majority voting. When N_i is large, verification scores stabilize and dominate selection.

### Mechanism 3: Verification Scales with Compute in Two Dimensions
Verification accuracy improves with both response length (training-time) and repeated sampling (inference-time). During RL training, longer CoT responses correlate with higher accuracy as the model tackles harder cases. At inference, sampling multiple verification trajectories and voting reduces variance. The paper shows accuracy rising from 94.5% to 97.5% on AIME2024 as verifications increase from 2 to 64.

## Foundational Learning

- **Proximal Policy Optimization (PPO) for LLMs**
  - Why needed here: Heimdall is trained with PPO; understanding reward shaping and policy gradients helps debug training instability.
  - Quick check question: Can you explain why PPO uses a clipped surrogate objective rather than vanilla policy gradients?

- **Multi-Armed Bandit Theory and Lower Confidence Bounds**
  - Why needed here: Pessimistic Verification is derived from bandit theory; the uncertainty penalty is a lower-confidence-bound term.
  - Quick check question: In a bandit setting, why does an optimism-based exploration strategy (UCB) differ from pessimism-based selection?

- **Chain-of-Thought Reasoning and Test-Time Scaling**
  - Why needed here: The entire approach depends on CoT verification and compute scaling laws.
  - Quick check question: Why does increasing response token count during RL correlate with accuracy gains on harder problems?

## Architecture Onboarding

- **Component map:** Problem dataset -> Solver generates 16 solutions -> Rule-based labeler -> Filter extreme cases -> PPO training -> Heimdall verifier -> Pessimistic Verification selector -> Final answer

- **Critical path:** Data filtering is the highest-leverage step; unfiltered data causes divergent training (Figure 2). Prompt template must preserve solution summary only; including raw CoT with reflections degrades performance. Hyperparameter α controls pessimism strength; paper uses α=0.1 but does not provide sensitivity analysis.

- **Design tradeoffs:** N vs. M allocation: Figure 4 (right) shows compute-optimal configurations favor increasing N (solutions) more frequently than M (verifications), because correct answers are sparse. Verification budget: More verifications reduce variance but hit diminishing returns; convergence observed around N=64 verifications. Solver-verifier coupling: Heimdall generalizes to unseen solvers (90.1% on DeepSeek-R1, 89.9% on Gemini 2.5 Pro) but performance gains are smaller for stronger solvers.

- **Failure signatures:** Spatial reasoning problems: Section 4.3 notes 3/4 single-correct-solution problems Heimdall failed involved spatial reasoning—base model limitation. Implicit assumptions in proofs: Section 5 identifies that subtle assumptions not stated as propositions may escape detection. False negatives vs. false positives: Figure 3 (top-right) shows FPR and FNR both decrease with sampling, but asymmetric rates may bias selection.

- **First 3 experiments:**
  1. Replicate data filtering ablation: Train two verifiers (filtered vs. unfiltered) on identical base model; plot accuracy gap over training steps to confirm mechanism.
  2. Sensitivity analysis on α: Sweep α ∈ {0.01, 0.05, 0.1, 0.5, 1.0} on validation set; identify phase transition where behavior shifts from majority-voting-dominated to verification-dominated.
  3. Cross-domain generalization test: Apply Heimdall to coding problems with unit tests as ground truth; measure verification accuracy and compare to math-domain performance to assess domain transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-processing solutions with a summarization model to expand brief reasoning steps improve Heimdall's verification accuracy?
- Basis in paper: [explicit] The authors note in Section 7 that the summary parts of reasoning models are "sometimes overly brief" and suggest that "A more detailed explanation would make the verification easier."
- Why unresolved: The current dataset uses raw summaries which can be sparse (e.g., "driving to an answer with only one sentence"), but the effect of specifically augmenting them for verification has not been tested.
- What evidence would resolve it: A comparison of verification accuracy on original sparse summaries versus augmented, detailed summaries generated by an external LLM.

### Open Question 2
- Question: Does training Heimdall on non-math domains (e.g., coding or formal proofs) significantly enhance its general verification capabilities or cross-domain transfer?
- Basis in paper: [explicit] Section 7 states, "we expect it beneficial to train with data in other domains. For example, in the context of coding problems, backward checking may take the alternative form of designing test cases."
- Why unresolved: The current model is trained only on math problems with explicit answers, and performance on other domains relies solely on zero-shot generalization.
- What evidence would resolve it: Experiments training separate verifiers on coding/proof data and evaluating on cross-domain tasks compared to the baseline model.

### Open Question 3
- Question: How can LLMs be effectively trained to autonomously pose valuable, novel scientific questions rather than just solving or verifying existing ones?
- Basis in paper: [explicit] The authors argue in Section 7 that "posing valuable questions is a challenging task... Such ability is often the critical part of the scientific discovery, which however is seldom investigated."
- Why unresolved: The paper's prototype relies on NuminaMath for question posing, treating the question generator as a fixed component rather than a capability to be optimized.
- What evidence would resolve it: A study measuring the novelty or utility of questions generated by an LLM within a closed-loop discovery system.

## Limitations
- Base model dependency: The paper doesn't specify the exact reasoning model used as Heimdall's base, affecting reproducibility
- Training data scale: Filtering criteria and dataset size are not specified, potentially limiting contrastive learning
- Hyperparameter sensitivity: α=0.1 is used without sensitivity analysis or exploration of optimal settings

## Confidence
- **High Confidence**: Verification accuracy scales with repeated sampling; Pessimistic Verification outperforms both pure majority voting and pure verification selection; Heimdall generalizes to unseen solvers
- **Medium Confidence**: Data filtering prevents difficulty bias; Two-dimensional scaling improves accuracy; Pessimistic Verification captures benefits of both approaches
- **Low Confidence**: Nearly half of synthetic datasets are flawed (single dataset examined); Automatic knowledge discovery system is "surprisingly effective" (qualitative assessment)

## Next Checks
1. **Cross-Domain Transfer Validation**: Apply Heimdall to coding problems where correctness can be automatically verified (unit tests, compiler checks). Compare verification accuracy and problem-solving improvements to math-domain performance to quantify domain transfer capabilities.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary α in Pessimistic Verification across {0.01, 0.05, 0.1, 0.5, 1.0} on validation sets from multiple problem distributions. Identify phase transitions and optimal settings for different solver strengths.

3. **Base Model Ablation Study**: Train Heimdall from scratch using three different reasoning-capable base models (e.g., DeepSeek-R1-Distill-Qwen-32B, Qwen-2.5-32B, Llama-3.1-Nemotron-70B-Instruct). Compare final verification accuracy and problem-solving improvements to isolate base model effects.