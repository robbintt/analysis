---
ver: rpa2
title: 'ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning
  Models'
arxiv_id: '2505.17250'
source_url: https://arxiv.org/abs/2505.17250
tags:
- reasoning
- conciserl
- accuracy
- length
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConciseRL introduces a hyperparameter-free conciseness score as
  a reward signal within reinforcement learning to guide models toward generating
  correct and concise reasoning traces. Unlike prior methods that penalize token count,
  ConciseRL uses an LLM as a judge to evaluate the semantic compactness of reasoning,
  enabling dynamic, context-aware feedback.
---

# ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models

## Quick Facts
- **arXiv ID:** 2505.17250
- **Source URL:** https://arxiv.org/abs/2505.17250
- **Reference count:** 34
- **Primary result:** Achieves 7% accuracy improvement with 31× token reduction on simple MATH problems using semantic conciseness scoring via LLM judge

## Executive Summary
ConciseRL introduces a novel approach to making reasoning models more efficient by using an LLM as a judge to evaluate the semantic conciseness of reasoning traces, rather than simply penalizing token count. The method employs reinforcement learning with a hyperparameter-free conciseness score that rewards models for generating correct yet compact reasoning. Unlike prior methods that use static length penalties, ConciseRL's dynamic, context-aware feedback enables models to adaptively allocate reasoning length based on problem difficulty. On the MATH dataset, the method reduces token usage by up to 31× on simple problems while improving accuracy by 7%, and on the hardest problems, it outperforms full reasoning by 7.5% accuracy with 3.6× fewer tokens.

## Method Summary
ConciseRL uses an LLM judge to score the semantic conciseness of reasoning traces, providing context-aware rewards rather than static token penalties. The method employs reinforcement learning with accuracy-gated rewards that multiply conciseness scores by correctness signals, preventing short-but-wrong responses. During training, the model samples multiple reasoning traces per prompt, evaluates their correctness, and queries the judge only for correct traces to compute rewards. The policy is updated using PPO with leave-one-out advantage estimation. The approach enables emergent adaptive reasoning allocation, where models naturally generate longer traces for harder problems and shorter ones for easier problems based on the judge's semantic evaluation.

## Key Results
- Achieves 7% accuracy improvement with 31× token reduction on simple MATH problems
- On hardest problems, outperforms full reasoning by 7.5% accuracy with 3.6× fewer tokens
- Improves TheoremQA accuracy by 2.2% using 12.5× fewer tokens
- Outperforms static length penalties and demonstrates adaptive reasoning length based on difficulty

## Why This Works (Mechanism)

### Mechanism 1: Semantic Conciseness Scoring via LLM-as-Judge
Using an LLM to evaluate reasoning conciseness produces richer, context-aware reward signals than static token-count penalties. The judge LLM receives reasoning traces and assigns discrete scores based on semantic compactness—logical step efficiency, redundancy, and clarity—rather than surface length. This enables traces of identical token length to receive different scores based on their semantic quality. The method assumes the judge can reliably discriminate semantic conciseness across diverse problem types, with evidence showing improved conciseness scores during training and superior performance compared to token-level baselines.

### Mechanism 2: Accuracy-Gated Reward Structure
Multiplying conciseness reward by correctness signal prevents the model from learning to generate short-but-wrong responses. The gated variant only queries the judge when answers are correct, reducing API costs while ensuring the policy only receives positive gradient signal when both conditions hold. This structure assumes deterministic correctness evaluation and sufficient baseline reasoning capability. Evidence shows the gated variant achieves better stability and efficiency trade-offs compared to separated reward approaches.

### Mechanism 3: Adaptive Reasoning Allocation via Difficulty-Correlated Length
The semantic conciseness reward enables emergent difficulty-adaptive reasoning, where the policy learns that harder problems require more elaboration to remain "concise yet correct." Because conciseness is evaluated per-trace relative to problem context, the model automatically allocates more tokens to complex problems. The judge's scoring captures this nuance, with evidence showing strong correlation between problem difficulty and generated trace length across different difficulty levels.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** ConciseRL uses PPO with leave-one-out advantage estimation for stable policy gradient updates on sequence-level rewards
  - **Quick check question:** Can you explain why PPO's clipping mechanism prevents destructive policy updates compared to vanilla REINFORCE?

- **Concept: Chain-of-Thought Reasoning in LLMs**
  - **Why needed here:** The method targets "overthinking" in reasoning models that generate verbose CoT traces; understanding CoT baseline behavior is essential
  - **Quick check question:** What is the "overthinking problem" in reasoning models, and why does it occur?

- **Concept: LLM-as-a-Judge Evaluation**
  - **Why needed here:** The core novelty is using an LLM judge for conciseness scoring; understanding prior judge applications provides context
  - **Quick check question:** What are known failure modes of LLM-as-judge setups (bias, inconsistency), and how might they affect RL training?

## Architecture Onboarding

- **Component map:** Policy model (reasoning LLM) -> Generates traces -> Correctness evaluator (deterministic checker) -> Judge model (external LLM) -> Reward composer (accuracy-gated or separated) -> PPO optimizer

- **Critical path:** 1) Sample 8 traces per prompt from current policy 2) Extract answers and compute correctness 3) For correct traces only, query judge for conciseness score 4) Compose rewards and compute leave-one-out advantages 5) Update policy via clipped PPO objective 6) Monitor accuracy, length, KL divergence, reward trend

- **Design tradeoffs:** Judge capability vs. cost (GPT-4.1 nano cheap but noisy vs. mini/o mini better signal at higher cost), Gated vs. separated reward (gated cheaper and more stable vs. separated enables maximum compression), KL penalty magnitude (too high constrains learning vs. too low risks collapse)

- **Failure signatures:** No length reduction + high accuracy (judge too noisy, switch to stronger judge), Aggressive length reduction + accuracy collapse (over-emphasizes conciseness, increase accuracy weight), KL divergence spikes (reduce learning rate or increase KL coefficient), Judge API costs explode (verify accuracy gating is working)

- **First 3 experiments:**
  1. Reproduce baseline comparison: Train ConciseRL (gated, GPT-4.1 mini judge) on MATH500 subset; compare to Efficient Reasoning α∈{0.1,0.4,0.6} on accuracy vs. token length trade-off curve
  2. Ablate judge quality: Run identical training with GPT-4.1 mini vs. nano judges on held-out set
  3. Verify adaptive behavior: Evaluate trained model on MATH500 difficulty levels 1–5; plot tokens vs. difficulty

## Open Questions the Paper Calls Out
None

## Limitations
- Judge reliability and bias across diverse reasoning domains remains uncertain, with no established reliability metrics
- Long-tail behavior and potential catastrophic forgetting during training are not thoroughly examined
- Scalability and computational cost implications of judge API calls during training are not fully analyzed

## Confidence

**High confidence claims:**
- ConciseRL achieves strong accuracy-token trade-offs on MATH and TheoremQA datasets
- The accuracy-gated reward structure prevents short-but-wrong responses
- Semantic conciseness scoring differs meaningfully from static token penalties

**Medium confidence claims:**
- Adaptive reasoning allocation based on problem difficulty emerges naturally from the reward structure
- The method generalizes to reasoning problems beyond the tested domains
- Judge quality directly correlates with learning efficiency

**Low confidence claims:**
- ConciseRL is universally compatible with existing RL training pipelines
- The conciseness scoring methodology extends robustly to non-mathematical reasoning domains
- The 31× token reduction on simple problems is representative of general performance

## Next Checks
1. **Cross-domain generalization test:** Apply ConciseRL to non-mathematical reasoning datasets (strategy games, commonsense reasoning, code generation) to verify judge can reliably evaluate semantic compactness across diverse domains.

2. **Judge bias and reliability analysis:** Conduct systematic experiments varying judge models, prompts, and scoring criteria to establish robustness of conciseness evaluation, including human evaluation of judge consistency.

3. **Long-tail and robustness evaluation:** Test trained model on rare problem types, adversarial examples, and out-of-distribution reasoning tasks to assess catastrophic forgetting and robustness across difficulty distributions.