---
ver: rpa2
title: Skill-Targeted Adaptive Training
arxiv_id: '2510.10023'
source_url: https://arxiv.org/abs/2510.10023
tags:
- skills
- training
- questions
- data
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model saturation in supervised
  fine-tuning, where models show little to no improvement on data similar to their
  pre-training set. The proposed solution, STAT, uses a stronger LLM as a teacher
  to identify each model's missing skills and adapt the training data accordingly.
---

# Skill-Targeted Adaptive Training

## Quick Facts
- **arXiv ID:** 2510.10023
- **Source URL:** https://arxiv.org/abs/2510.10023
- **Reference count:** 40
- **Primary result:** Up to 7.5% improvement on MATH benchmark over standard fine-tuning

## Executive Summary
This paper addresses model saturation in supervised fine-tuning where models show minimal improvement on data similar to pre-training. The proposed STAT framework uses a stronger LLM teacher to identify each student model's missing skills and adapt training data accordingly. By analyzing incorrect responses on difficult questions, the teacher creates a Missing-Skill-Profile that guides targeted training through either reweighted existing examples (STAT-Sel) or synthesized questions (STAT-Syn). Across experiments on Llama and Qwen models, STAT achieves significant improvements on MATH (up to 7.5%) and strong out-of-distribution generalization (4.6% average gain), while also complementing reinforcement learning approaches.

## Method Summary
STAT operates in three stages: (1) Reward filtering identifies difficult questions using process reward model thresholds, (2) Missing-Skill-Profile construction analyzes student errors with teacher LLM to map failures to specific skills, and (3) Skill-targeted training either reweights existing data (STAT-Sel) or synthesizes new questions (STAT-Syn) based on the skill profile. The framework uses a predefined skill vocabulary of 128 skills across 7 subjects, with skill labels provided by the teacher. Training employs QLoRA fine-tuning with 3 epochs and learning rate selection based on MATH validation accuracy.

## Key Results
- 7.5% improvement on MATH benchmark over standard fine-tuning baselines
- 4.6% average gain on out-of-distribution tests (GSM8K, MATH2, MATH-perturb, AMC23, AIME24/25)
- Strong complementarity with RL approaches (6.5% gain over SFT-RLHF)
- Effective remediation of basic skill deficiencies like algebraic computations that standard methods miss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Identifying missing skills from student responses enables targeted remediation that average-loss optimization misses.
- **Mechanism:** A stronger teacher LLM analyzes incorrect responses on difficult validation questions, extracting which skills from a predefined skill vocabulary the student failed to apply. This creates a model-specific Missing-Skill-Profile that maps each question to its associated skill gaps.
- **Core assumption:** The teacher's skill-labeling accuracy is sufficient for downstream selection; cross-teacher agreement (~43% with Claude-3.5-Sonnet) suggests noise tolerance.
- **Evidence anchors:**
  - [abstract]: "By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill."
  - [Section 4, Figure 2]: Algebra-centric skills (solving equations, basic arithmetic operations) dominate the Top 10 missing skills across Llama and Qwen models.
  - [corpus]: Neighbor work "Skill-Aware Data Selection and Fine-Tuning" (arXiv:2601.10109) shows related skill-based data selection improves reasoning distillation, supporting skill-awareness as a general principle.

### Mechanism 2
- **Claim:** Reward-filtered question selection identifies training-relevant failures without ground-truth labels.
- **Mechanism:** A process reward model scores each step of the student's response. Questions receive a "difficult" label if: (a) final-step reward ≤ τ1, (b) average step reward ≤ τ1, or (c) any intermediate step ≤ τ2. This filters the validation set to those questions where the student's reasoning process breaks down.
- **Core assumption:** Low process rewards correlate with recoverable skill gaps rather than fundamental capability ceilings.
- **Evidence anchors:**
  - [Section 2.1, Equation 1]: Formal definition of threshold filtering function R(q).
  - [Table 11]: Reward filtering achieves 78.0% classification accuracy on difficult questions, comparable to consistency heuristic (79.8%) and length heuristic (74.2%).
  - [corpus]: No direct corpus evidence for reward-filtering specifically; related work focuses on outcome-based KD.

### Mechanism 3
- **Claim:** Reweighting training data by skill frequency in the Missing-Skill-Profile addresses distribution mismatch between training loss and generation errors.
- **Mechanism:** STAT-Sel samples training questions proportionally to their associated skills' frequency in the Missing-Skill-Profile. STAT-Syn generates synthetic questions conditioned on seed examples and missing-skill labels, then filters by teacher consistency (≥2 of 3 responses agreeing).
- **Core assumption:** Skill-level alignment between training and evaluation is sufficient for transfer; embedding similarity is not.
- **Evidence anchors:**
  - [Section 3.2]: Embed-Sel (embedding-based selection) shows only marginal gains over MATH-Train, while STAT-Sel achieves 6.7% average improvement on Llama-3.2-3B.
  - [Figure 5]: Case study shows Embed-Syn generates questions matching the problem topic (Ellipse Properties) while STAT-Syn targets the actual failure mode (Solving Equations).
  - [corpus]: "Distill Not Only Data but Also Rewards" (arXiv:2502.19557) highlights limitations of data-only distillation, consistent with STAT's finding that surface similarity is insufficient.

## Foundational Learning

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - Why needed here: Stage 1 relies on PRM scoring; understanding the difference is critical for threshold selection and debugging.
  - Quick check question: Given a 5-step solution with step rewards [0.9, 0.8, 0.3, 0.9, 0.9], would τ1=0.85, τ2=0.7 classify this as "difficult"?

- **Concept: Skill decomposition and skill-maps**
  - Why needed here: The entire pipeline depends on a predefined skill vocabulary S and Skill-Map: S → P. Understanding how these are constructed (via LLM labeling and clustering) is necessary for domain adaptation.
  - Quick check question: If adapting STAT to coding tasks, what process would you use to derive the skill vocabulary?

- **Concept: Model saturation in SFT**
  - Why needed here: The paper's motivation is addressing saturation where average loss optimization no longer improves generation quality. Understanding why this occurs (loss-generativity mismatch) clarifies why skill-targeting works.
  - Quick check question: Why might a model with low perplexity on MATH training data still fail on basic algebraic manipulation?

## Architecture Onboarding

- **Component map:** Stage 1 (Reward Filtering): Process Reward Model → Threshold Classifier → Q_difficult validation/test split; Stage 2 (Skill Profiling): Teacher LLM + Skill Vocabulary → Missing-Skill-Profile: Q_val_difficult → S; Stage 3a (STAT-Sel): Missing-Skill-Profile + Skill-Map → Weighted sampling from P_targeted; Stage 3b (STAT-Syn): Missing-Skill-Profile + Skill-Map + Teacher LLM → Synthetic question generation → Consistency filtering → P_targeted

- **Critical path:** Stage 1 → Stage 2 → Stage 3a/3b. Errors in Stage 1 (misclassifying easy questions as difficult) waste teacher capacity; errors in Stage 2 (incorrect skill labels) misdirect all training.

- **Design tradeoffs:**
  - STAT-Sel vs. STAT-Syn: STAT-Syn is ~2-3x more expensive (requires teacher generation + consistency filtering) but performs better on hard questions (MATHD-hard: 31.7% vs. 26.6% for Llama-3.2-3B).
  - PRM thresholds: Higher τ1/τ2 → more questions flagged as difficult → higher coverage but more noise.
  - Teacher model choice: GPT-4o-mini used for cost; GPT-4o may improve skill-labeling accuracy but was not strictly necessary.

- **Failure signatures:**
  - Stage 1: If PRM is miscalibrated for your domain, Q_difficult will not correlate with actual student errors. Check: compare PRM classification accuracy against ground-truth correctness on a held-out sample.
  - Stage 2: If skill vocabulary S is too coarse or too fine-grained, Missing-Skill-Profile becomes uninformative. Check: analyze skill frequency distribution; excessive sparsity suggests vocabulary issues.
  - Stage 3: If STAT-Syn questions are too similar to validation set, you risk memorization without skill transfer. Check: compare performance on MATHD (difficult subset) vs. standard MATH.

- **First 3 experiments:**
  1. **Baseline replication:** Train Llama-3.2-3B-Instruct with MATH-Augment and verify minimal gain (~1-2%). This establishes your saturation point.
  2. **Ablation on reward thresholds:** Compare τ1 ∈ {0.8, 0.85, 0.9}, τ2 ∈ {0.6, 0.7, 0.8} on classification accuracy and downstream MATH performance. Use Table 8/9 as reference.
  3. **Skill-vocabulary sensitivity:** Run STAT-Sel with a reduced skill vocabulary (e.g., 50 skills instead of 128) to assess robustness. If performance drops sharply, skill granularity is critical.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the skills identified by the teacher LLM correspond to specific mechanistic circuits within the student model?
- **Basis in paper:** [explicit] The conclusion states it would be valuable to investigate whether these skills correspond to specific mechanistic circuits within the model.
- **Why unresolved:** The paper focuses entirely on external performance metrics (accuracy on benchmarks) rather than internal interpretability or circuit-level analysis.
- **What evidence would resolve it:** Causal tracing or ablation studies mapping specific "missing skills" (e.g., "Algebraic Manipulation") to localized neural sub-networks or attention heads.

### Open Question 2
- **Question:** Can the STAT framework be effectively applied to non-reasoning dimensions such as safety and interpretability?
- **Basis in paper:** [explicit] The conclusion identifies exploring whether STAT can improve dimensions such as safety and interpretability as an interesting avenue for further study.
- **Why unresolved:** Experiments were restricted to mathematical reasoning benchmarks (MATH, GSM8K, AIME, etc.).
- **What evidence would resolve it:** Applying STAT to safety datasets (e.g., AdvBench) to determine if a "Missing-Skill-Profile" for safety can be constructed and used to reduce harmful outputs.

### Open Question 3
- **Question:** How sensitive is the model's performance to disagreement or bias in the teacher's skill labeling?
- **Basis in paper:** [inferred] Section E.3 reveals only 43% agreement between GPT-4o-mini and Claude-3.5-Sonnet on skill labels, indicating potential instability in the ground truth.
- **Why unresolved:** The method relies on a single teacher (GPT-4o-mini) and does not test robustness against the labeling noise implied by the low agreement rate.
- **What evidence would resolve it:** Ablation studies analyzing performance changes when using differing teacher models or artificially injecting noise into the Missing-Skill-Profile.

### Open Question 4
- **Question:** Can Reinforcement Learning (GRPO) be improved by incorporating skill-based feedback directly into the reward signal?
- **Basis in paper:** [explicit] Section 4 suggests a future direction is to develop a GRPO variant that incorporates skill-based feedback into the reward, as standard GRPO provides only coarse feedback.
- **Why unresolved:** The current paper applies STAT and GRPO sequentially rather than integrating them into a single optimization loop.
- **What evidence would resolve it:** Implementation of a unified "Skill-GRPO" algorithm where the reward is weighted by the Missing-Skill-Profile, compared against the sequential baseline.

## Limitations

- **Skill Vocabulary Dependence:** STAT's effectiveness relies heavily on the predefined skill vocabulary, with uncertainty about optimal granularity across different domains and model scales.
- **Teacher Error Propagation:** The method depends on teacher correctness despite consistency filtering, with potential systematic biases in skill labeling that could misdirect training.
- **Generalization Uncertainty:** Strong OOD performance on mathematical reasoning may not transfer to domains where skill decomposition is less well-defined or the skill vocabulary doesn't generalize.

## Confidence

- **High Confidence:** Core mechanism of identifying missing skills and targeting training accordingly is well-supported by ablation studies and substantial MATH improvement (7.5%).
- **Medium Confidence:** Process reward model thresholds are somewhat arbitrary and may not generalize to other domains; 78% accuracy on difficult question classification could vary significantly.
- **Medium Confidence:** Complementarity with RL approaches is demonstrated but interaction between skill-targeted training and reward shaping in RL remains underexplored.

## Next Checks

1. **Domain Transfer Validation:** Apply STAT to non-mathematical domains (e.g., coding, commonsense reasoning) and evaluate whether skill vocabulary construction and missing-skill identification remain effective. Test if embedding-based selection becomes more competitive when skill decomposition is less clear.

2. **Teacher-Student Gap Analysis:** Systematically vary the teacher-student capability gap (e.g., use Claude-3.5-Sonnet vs. GPT-4o-mini as teacher for the same student model) and measure the impact on missing-skill identification accuracy and downstream performance.

3. **Skill Vocabulary Sensitivity:** Conduct controlled experiments varying skill granularity (e.g., 50 vs. 200 skills) across different model sizes and domains. Measure the trade-off between vocabulary precision and coverage, and determine if there's an optimal granularity that generalizes across tasks.