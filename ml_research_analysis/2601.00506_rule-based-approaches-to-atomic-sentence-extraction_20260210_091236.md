---
ver: rpa2
title: Rule-Based Approaches to Atomic Sentence Extraction
arxiv_id: '2601.00506'
source_url: https://arxiv.org/abs/2601.00506
tags:
- atomic
- sentences
- sentence
- extraction
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated which linguistic structures most challenge
  rule-based atomic sentence extraction systems. Using the WikiSplit dataset and spaCy's
  dependency parser, a rule-based system was developed to decompose complex sentences
  into single-idea units.
---

# Rule-Based Approaches to Atomic Sentence Extraction

## Quick Facts
- arXiv ID: 2601.00506
- Source URL: https://arxiv.org/abs/2601.00506
- Authors: Lineesha Kamana; Akshita Ananda Subramanian; Mehuli Ghosh; Suman Saha
- Reference count: 6
- Primary result: Rule-based system using spaCy dependency parser achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, BERTScore F1 = 0.5898 on WikiSplit dataset

## Executive Summary
This study investigated which linguistic structures most challenge rule-based atomic sentence extraction systems. Using the WikiSplit dataset and spaCy's dependency parser, a rule-based system was developed to decompose complex sentences into single-idea units. The system achieved moderate-to-high lexical and structural alignment but weaker phrasal and semantic preservation. Error analysis revealed that missing objects (44%), coordination errors (5%), and missing subjects (4%) were the most frequent issues. Complex structures like relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions caused the majority of failures.

## Method Summary
The method uses spaCy's dependency parser to identify clause boundaries and split complex sentences at coordinating conjunctions ("and"/"or", POS "conj") and subordinate clauses ("advcl", "relcl"). The system attempts subject propagation across coordinated predicates to maintain completeness. Sentences are preprocessed by deduplication and lowercasing. The approach relies on handcrafted rules rather than machine learning, enabling interpretable decision boundaries. The 100-sentence evaluation set was manually annotated into 252 gold-standard atomic pairs.

## Key Results
- ROUGE-1 F1 = 0.6714 indicates moderate lexical overlap
- ROUGE-2 F1 = 0.478 shows weak phrasal preservation
- BERTScore F1 = 0.5898 reveals semantic meaning loss
- Missing Object errors account for 44.4% of total failures

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Based Clause Boundary Detection
The system parses sentences using spaCy's dependency parser, identifies split points by detecting coordinating conjunctions and subordinate clause markers, then segments at these boundaries. This works because clause-level semantic units map predictably to dependency graph substructures when syntactic cues are unambiguous.

### Mechanism 2: Subject Propagation for Predicate Completion
After splitting at conjunctions, the system attempts to reassign the original subject to each newly created predicate, converting compound structures into complete propositions. This succeeds when the dependency parser correctly identifies subject-head relations and the subject is recoverable from the governing clause.

### Mechanism 3: Handcrafted Rules for Syntactic Simplification
A finite set of linguistically-motivated rules decompose complex sentences while preserving interpretability of extraction decisions. This approach works when the rule inventory covers encountered syntactic patterns, though it struggles with multiple interacting phenomena.

## Foundational Learning

- **Concept: Dependency Parsing (nsubj, dobj, advcl, relcl relations)** - Why needed: The entire extraction system operates on spaCy's dependency output; understanding these labels is essential for debugging rule triggers.
  - Quick check: Given "The cat that the dog chased meowed," which token has the "relcl" dependency, and what is its head?

- **Concept: Clause Types (relative, adverbial, coordinated)** - Why needed: Error analysis is organized by clause structure; identifying which type caused a failure requires recognizing these patterns.
  - Quick check: In "She left before he arrived," which clause is adverbial, and what subordinator marks it?

- **Concept: ROUGE and BERTScore Metrics** - Why needed: System evaluation relies on interpreting what these scores measure (lexical overlap vs. semantic similarity).
  - Quick check: If ROUGE-2 is low but ROUGE-L is high, what does this indicate about phrasal vs. structural similarity?

## Architecture Onboarding

- **Component map:** Input sentences → Lowercase normalization → spaCy dependency parse → Rule engine (conj/advcl/relcl detection) → Subject/object propagation → Atomic sentence output → ROUGE/BERTScore evaluation
- **Critical path:** Sentence enters → lowercase normalization → spaCy dependency parse generates labeled graph → Rule engine scans for split triggers → Subject/object propagation attempts completion → Atomic sentences emitted and aligned to gold pairs → Metrics computed
- **Design tradeoffs:** `en_core_web_sm` chosen for speed over accuracy; larger spaCy models may improve parse quality. Rule-based approach trades coverage for interpretability vs. neural methods. Manual gold standard limits statistical power but enables fine-grained error analysis.
- **Failure signatures:** Missing Object (44.4%): Predicate extracted without required argument. Missing Subject (4.4%): Verb phrase with no subject, common in passives. Coordination errors (5.2%): Lost predicates or incorrect splits. Semantic drift: BERTScore (0.59) lower than ROUGE-L (0.65) suggests meaning loss.
- **First 3 experiments:** 1) Parse quality audit: Run 20 failure cases through larger spaCy model to determine if errors originate from parser inaccuracies vs. rule gaps. 2) Object recovery rule: Implement explicit object propagation for transitive verbs with missing direct objects; evaluate impact on Missing Object error rate. 3) Nested clause stress test: Construct 30 synthetic sentences with 2+ embedded clause types; measure which rule combinations fail and document interaction patterns.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does improving atomic sentence extraction directly enhance performance in downstream tasks such as question answering and fact verification? The conclusion suggests exploring this, but the current study focused solely on linguistic extraction accuracy without testing utility in external NLP pipelines.

- **Open Question 2:** Can a hybrid approach combining rule-based splitting with argument recovery components effectively reduce the high rate of "missing object" errors? The authors suggest this could improve accuracy, but the current implementation strictly relies on dependency parsing which failed to include objects in 44% of errors.

- **Open Question 3:** Does the identified distribution of error types (specifically the prevalence of missing objects) generalize to the full WikiSplit dataset or other domains? The methodology relied on a small sample (100 sentences), leaving the system's robustness across the complete dataset unverified.

## Limitations

- The 44% Missing Object error rate and 39% "Multiple/Other" errors indicate the rule inventory fails to capture essential syntactic dependencies for argument completion.
- ROUGE-L (0.65) shows reasonable structural preservation but BERTScore (0.59) reveals semantic meaning loss.
- The dependency parser choice (en_core_web_sm) may introduce systematic errors that compound rule-based failures, particularly in complex nested constructions.

## Confidence

- **High confidence:** The overall ROUGE and BERTScore values accurately reflect moderate-to-high lexical/structural alignment with weaker semantic preservation. The error distribution is reliably measured against the gold standard.
- **Medium confidence:** The attribution of specific failure types to particular syntactic structures is plausible but could benefit from more systematic error categorization.
- **Low confidence:** The scalability and generalizability of handcrafted rules to other domains or languages, given the limited 100-sentence evaluation set and English-specific spaCy dependencies.

## Next Checks

1. **Parser dependency audit:** Evaluate the same 100 sentences through both spaCy en_core_web_sm and the larger en_core_web_trf model to quantify how much error stems from parser accuracy versus rule design.

2. **Argument completion validation:** Implement explicit object recovery rules for transitive verbs with missing direct objects, then measure the reduction in Missing Object errors across the full error corpus.

3. **Nested clause interaction study:** Construct a test suite of 50 sentences containing 2+ interacting clause types to systematically document which rule combinations fail and why.