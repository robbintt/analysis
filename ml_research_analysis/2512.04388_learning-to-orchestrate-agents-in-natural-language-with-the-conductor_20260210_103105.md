---
ver: rpa2
title: Learning to Orchestrate Agents in Natural Language with the Conductor
arxiv_id: '2512.04388'
source_url: https://arxiv.org/abs/2512.04388
tags:
- conductor
- performance
- arxiv
- reasoning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Conductor, a 7B language model trained\
  \ via reinforcement learning to automatically coordinate and prompt-engineer a pool\
  \ of powerful LLM workers. By outputting natural-language workflows that specify\
  \ subtasks, worker assignments, and information access, the Conductor learns to\
  \ combine diverse models\u2019 strengths through emergent strategies like verification\
  \ and refinement."
---

# Learning to Orchestrate Agents in Natural Language with the Conductor

## Quick Facts
- arXiv ID: 2512.04388
- Source URL: https://arxiv.org/abs/2512.04388
- Reference count: 40
- Key outcome: 7B Conductor achieves state-of-the-art results on LiveCodeBench, MATH, and GPQA by automatically coordinating diverse LLM workers through natural language workflows

## Executive Summary
This paper introduces the Conductor, a 7B language model trained via reinforcement learning to automatically coordinate and prompt-engineer a pool of powerful LLM workers. By outputting natural-language workflows that specify subtasks, worker assignments, and information access, the Conductor learns to combine diverse models' strengths through emergent strategies like verification and refinement. On challenging reasoning benchmarks (LiveCodeBench, GPQA), the Conductor achieves state-of-the-art results, surpassing both individual workers and prior multi-agent baselines at a fraction of the cost. Extensions allow the Conductor to adapt to arbitrary agent pools and employ recursive topologies for test-time scaling. The work demonstrates that small models can effectively orchestrate collective intelligence, opening new avenues for collaborative AI systems.

## Method Summary
The Conductor is a 7B Qwen2.5 model trained via GRPO (Grouped Relative Policy Optimization) to output natural-language workflows specifying subtasks, worker assignments, and context access patterns. Given a question, it generates three Python lists: model_id, subtasks, and access_list, which the executor uses to sequentially prompt workers. The training reward is binary—0 for unparseable format, 1 for correct final answer, 0.5 for incorrect but parseable—computed across 960 training problems from MATH, MMLU, RLPR, and LiveCodeBench V1. The model learns to discover effective coordination strategies (verification loops, planner-executor patterns) through end-to-end RL without explicit programming of these strategies.

## Key Results
- Achieves 87.9% on LiveCodeBench V6, surpassing individual workers (GPT-5: 85.1%, Gemini 2.5: 83.8%) and prior multi-agent baselines (Tree of Agents: 77.2%, FACT: 80.6%)
- Matches or exceeds GPT-4-level performance on GPQA-Diamond (74.2%) and MATH500 (80.4%)
- Demonstrates emergent coordination strategies like verification and refinement discovered through pure RL
- Shows recursive self-calling enables adaptive test-time scaling, improving performance on BigCodeBench by redistributing agent selection based on observed failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end RL enables emergent discovery of coordination strategies that outperform hand-designed scaffolds.
- Mechanism: The Conductor receives only a binary reward signal (format compliance + task correctness). Through GRPO optimization, it discovers effective topologies—verification loops, planner-executor patterns, parallel-then-aggregate structures—without explicit programming of these strategies.
- Core assumption: The reward signal is sufficiently dense and the action space (natural language workflow specification) is expressive enough for gradient-driven discovery.
- Evidence anchors:
  - [abstract] "powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization"
  - [section 3.1] "we observe the emergence of problem breakdowns and prompt-engineered subtasks...together with communication strategies that combine independent attempts with final debate rounds"
  - [corpus] Related work HCPO (arXiv:2511.12123) shows hierarchical conductor-based policy optimization improves multi-agent RL, suggesting the conductor pattern transfers across domains.
- Break condition: If worker models are too weak to provide meaningful signal, or if the task space lacks verifiable solutions, the reward signal becomes uninformative and emergent strategies may not develop.

### Mechanism 2
- Claim: Natural language subtask specification enables finer-grained capability matching than fixed routing.
- Mechanism: Unlike prior routers that select from pre-defined topologies, the Conductor generates arbitrary natural language instructions per worker. This allows task decomposition tailored to each worker's demonstrated strengths (e.g., using Gemini for planning, GPT-5 for code implementation).
- Core assumption: Workers respond differentially to prompt variations, and the Conductor can learn these response patterns through training.
- Evidence anchors:
  - [section 4.5] "while both [3B and 7B] models still display performance well beyond all of our baselines, our larger 7B variant maintains a clear edge...we trace this performance gap to the larger model's superior prompt engineering skills"
  - [appendix B.3] "GPT-5's strong performance in math and competitive coding...while Gemini excels in scientific reasoning...Claude Sonnet 4 struggles at competitive coding, but is one of the dominant models at code generation with diverse function calling"
  - [corpus] Limited direct corpus evidence on natural language vs. fixed routing; related work focuses on different aspects.
- Break condition: If workers do not exhibit meaningful performance variance across prompt formulations, or if the Conductor lacks sufficient capacity to model prompt-response relationships, the advantage over fixed routing diminishes.

### Mechanism 3
- Claim: Recursive self-calling enables adaptive test-time scaling and online strategy revision.
- Mechanism: When the Conductor can select itself as a worker, it receives the previous workflow's output and can devise a new coordination strategy. This allows redistribution of agent selection based on observed failures (e.g., shifting from GPT-5 to Claude/Gemini on BigCodeBench after detecting suboptimal performance).
- Core assumption: The Conductor can diagnose failure modes from worker outputs and has learned alternative strategies during training.
- Evidence anchors:
  - [section 3.2] "allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling"
  - [section 4.4] "we find the Conductor effectively redistributes its agent selection towards Claude 4 and Gemini 2.5 during its BigCodeBench recursive calls after observing the unexpectedly suboptimal behavior of GPT5"
  - [corpus] "Self-Resource Allocation in Multi-Agent LLM Systems" (arXiv:2504.02051) explores related adaptive allocation, but does not address recursion specifically.
- Break condition: If recursion depth is too shallow, or if the Conductor hasn't encountered similar failure patterns during training, revision may not improve over the initial strategy.

## Foundational Learning

- Concept: **GRPO (Grouped Relative Policy Optimization)**
  - Why needed here: This is the core RL algorithm training the Conductor. It generates multiple completions per question, computes advantages via group normalization, and applies clipped policy updates with optional KL penalties.
  - Quick check question: Can you explain why GRPO uses group-relative advantages rather than absolute rewards, and what problem this solves?

- Concept: **Verifiable reward signals**
  - Why needed here: The Conductor's training relies on tasks with ground-truth solutions (math, code, multiple-choice). Without verifiable outputs, the correctness reward cannot be computed.
  - Quick check question: For a new task domain, how would you determine whether it supports the verifiable reward paradigm?

- Concept: **Agent pool randomization for generalization**
  - Why needed here: The adaptive worker selection extension trains with randomly sampled subsets of the worker pool, forcing the Conductor to learn general coordination rather than memorizing specific model combinations.
  - Quick check question: Why does training with full access to all workers potentially harm generalization to constrained subsets?

## Architecture Onboarding

- Component map: Question → Conductor (generates workflow) → Executor (prompts workers) → Workers (generate outputs) → Reward Computer (computes correctness) → GRPO Trainer (updates policy)
- Critical path: Question → Conductor generates workflow (max 5 steps, 1024 tokens) → Executor parses lists, runs workers sequentially with specified context → Final worker output → reward computation → Batch of (question, completions, rewards) → GRPO update
- Design tradeoffs:
  - **Workflow length (5 steps)**: Longer workflows increase compute cost; paper finds average ~3 steps sufficient
  - **Worker context window (4096 tokens, minimal reasoning budgets)**: Constrains worker capability but reduces cost; unconstrained evaluation uses higher limits
  - **OOD vs. in-distribution few-shot examples**: Paper finds OOD examples improve performance by preventing exploitation (Appendix B.2)
  - **Recursion depth**: Unlimited risks infinite loops; paper uses small discount factor (0.25) on initial round rewards
- Failure signatures:
  - **Format failures**: Conductor outputs unparseable lists → reward=0, no gradient signal from task performance
  - **Worker mismatch**: Conductor assigns tasks to inappropriate workers (e.g., weak open-source models as validators) → low correctness reward
  - **Over-reliance patterns**: Pre-trained Conductors may favor certain workers regardless of task; requires randomization training to break
  - **Recursion loops**: Without proper termination, self-calling can cycle without improvement
- First 3 experiments:
  1. **Cold-start validation**: Train Conductor from scratch on a small subset (100 problems) with full logging of emergent strategies. Verify format compliance reaches >95% before scaling.
  2. **Worker ablation**: Fix all agent selections to single powerful worker (e.g., GPT-5) and measure performance gap. This isolates coordination benefit from prompt-engineering benefit (see Table 10).
  3. **Few-shot condition comparison**: Compare in-distribution vs. OOD few-shot examples on held-out tasks. Replicate Appendix B.2 finding that OOD examples improve exploration.

## Open Questions the Paper Calls Out

- **Can the Conductor framework effectively coordinate multi-modal workers** such as vision models, robotics controllers, or protein structure predictors? The paper explicitly states this as an exciting extension but current work only coordinates text-based LLM workers.
- **How does Conductor performance scale with model size** beyond 7B parameters? While 3B vs 7B comparison shows improved prompt engineering with scale, whether scaling benefits continue to 30B, 70B, or frontier-scale models remains untested.
- **What are the optimal trade-offs between recursive depth, compute cost, and performance gains?** The paper introduces recursive topologies but evaluates only with limited recursion (<2× original calls), leaving the Pareto frontier for test-time compute characterization incomplete.

## Limitations

- Reliance on verifiable reward signals restricts applicability to math, coding, and multiple-choice domains, limiting scalability to open-ended generation tasks
- Analysis of emergent coordination strategies is largely qualitative, with more systematic ablation studies needed on specific coordination patterns
- Cost analysis focuses on API calls rather than total energy consumption or environmental impact, which becomes increasingly relevant as these systems scale

## Confidence

- **High confidence**: Claims about the Conductor achieving state-of-the-art results on LiveCodeBench, MATH, and GPQA benchmarks through natural language workflow orchestration
- **Medium confidence**: Claims about emergent coordination strategies (verification loops, planner-executor patterns) being discovered through end-to-end RL
- **Medium confidence**: Claims about recursive self-calling enabling adaptive test-time scaling

## Next Checks

1. **Mechanism isolation experiment**: Fix all agent selections to single powerful workers (e.g., GPT-5) while keeping natural language subtask generation. Compare performance to full orchestration to quantify the relative contribution of coordination versus prompt engineering.

2. **Strategy emergence analysis**: Log and categorize the first 50 distinct workflow patterns that emerge during training. Measure how strategy diversity evolves with training iterations and correlate with performance improvements.

3. **Recursion depth sensitivity**: Systematically vary recursion depth limits (1, 2, 3, unlimited) and measure the trade-off between performance gains and computational cost. Identify the optimal depth threshold where diminishing returns begin.