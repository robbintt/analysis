---
ver: rpa2
title: 'Beyond Shapley Values: Cooperative Games for the Interpretation of Machine
  Learning Models'
arxiv_id: '2506.13900'
source_url: https://arxiv.org/abs/2506.13900
tags:
- shapley
- values
- value
- game
- cooperative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that while Shapley values are widely used in\
  \ post-hoc interpretability of machine learning models, their axiomatic justifications\
  \ are not necessarily relevant for feature attribution. The authors propose a broader\
  \ framework using cooperative game theory, introducing two general families of efficient\
  \ allocations\u2014the Weber and Harsanyi sets\u2014that extend beyond Shapley values."
---

# Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models

## Quick Facts
- arXiv ID: 2506.13900
- Source URL: https://arxiv.org/abs/2506.13900
- Authors: Marouane Il Idrissi; Agathe Fernandes Machado; Arthur Charpentier
- Reference count: 9
- Primary result: The paper proposes a broader framework using cooperative game theory for feature attribution, introducing Weber and Harsanyi sets that extend beyond Shapley values, and argues that value function selection is more critical than allocation method.

## Executive Summary
This paper challenges the dominance of Shapley values in post-hoc interpretability by arguing that their axiomatic justifications are not necessarily relevant for feature attribution. The authors propose a broader framework using cooperative game theory, introducing two general families of efficient allocations—the Weber and Harsanyi sets—that extend beyond Shapley values. They clarify the distinction between value functions and aggregation rules, and present a three-step blueprint for constructing theoretically-grounded feature attributions: choosing a quantity of interest, selecting a value function, and picking an allocation. The paper demonstrates that the choice of value function is more critical than the allocation method, and that recent theoretical developments in oblique projections can yield purer, more faithful attributions.

## Method Summary
The paper presents a three-step blueprint for constructing feature attributions: (1) choose a quantity of interest to decompose (e.g., model output, variance), (2) select a value function where v(D) equals that quantity, and (3) pick an efficient allocation from the Weber or Harsanyi sets. The Weber set parameterizes allocations via probability distributions over player orderings, while the Harsanyi set redistributes incremental coalition values (dividends) according to weight systems. The authors argue that value function selection dominates allocation choice for attribution quality, as efficiency is the only axiom directly linking attributions to the target quantity.

## Key Results
- Shapley values are special cases of efficient allocations within the Weber set (uniform permutation distribution) and Harsanyi set (uniform weight systems).
- Value function selection is more critical than allocation method for attribution quality, as axioms governing aggregation do not constrain the choice of value function.
- Recent theoretical developments in oblique projections can yield purer, more faithful attributions compared to standard conditional-expectation value functions.
- Proportional marginal effects (PME) allocations can outperform Shapley values in cases with correlated features by better handling spurious feature attributions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley values can be understood as the uniform distribution over player orderings, and non-uniform distributions yield alternative efficient allocations.
- Mechanism: The Weber set parameterizes allocations via a probability mass function p over permutations. For each ordering π, player j receives their marginal contribution to preceding players: v(π_j) - v(π_j \ {j}). Aggregating over all orderings weighted by p yields efficient allocations. Shapley values emerge when p(π) = 1/d! (uniform).
- Core assumption: Players join coalitions sequentially, and the chosen ordering distribution reflects meaningful inductive bias for the interpretability task.
- Evidence anchors:
  - [abstract] "We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values"
  - [Section 2.1] Theorem 1: "ϕ_v = Shap_v if and only if p(π) = 1/d! for every π ∈ S_D"
  - [corpus] Weak corpus support; related papers focus on Shapley approximations rather than Weber set alternatives.
- Break condition: If permutation ordering lacks semantic connection to feature relationships (e.g., no causal or temporal structure), non-uniform p provides no interpretive benefit.

### Mechanism 2
- Claim: Allocations can be constructed by redistributing Harsanyi dividends—incremental coalition values—according to arbitrary weight systems.
- Mechanism: Dividends φ_v(A) = v(A) - Σ_{B⊂A} φ_v(B) capture each coalition's incremental worth beyond subcoalitions. A weight system λ_j(A) ≥ 0 with Σ_j λ_j(A) = 1 distributes each coalition's dividend among members. Shapley values correspond to egalitarian weights λ_j(A) = 1/|A|.
- Core assumption: Dividends represent meaningful "credit" that can be fairly apportioned; the weighting scheme reflects desired fairness or interpretability properties.
- Evidence anchors:
  - [Section 2.2] "Allocations in the Harsanyi set redistribute the dividends among the coalition members according to a weight system"
  - [Section 2.2] Theorem 2: Shapley values iff weights are uniform across coalition members
  - [corpus] Weak direct support; corpus papers reference Shapley fairness axioms but not Harsanyi dividend alternatives.
- Break condition: If coalition dividends have no intrinsic meaning for the ML task (e.g., interaction effects are artifacts of correlation), dividend redistribution cannot yield faithful attribution.

### Mechanism 3
- Claim: Value function selection dominates allocation choice for attribution quality; efficiency is the only axiom directly linking attributions to the target quantity.
- Mechanism: An allocation ϕ_v is efficient if Σ_j ϕ_v(j) = v(D) - v(∅). This guarantees the attribution decomposes the quantity of interest. However, axioms governing aggregation (symmetry, linearity, etc.) do not constrain v. Different v mapping to the same v(D) can yield substantively different attributions—even with identical allocations.
- Core assumption: The "correct" attribution depends on a well-specified quantity of interest and a value function faithfully representing model structure.
- Evidence anchors:
  - [abstract] "the choice of value function is more critical than the allocation method"
  - [Section 2.3] "an allocation cannot correct a poorly chosen value function... the axioms typically used to justify feature attributions theoretically govern only the aggregation process, not the choice of value function"
  - [Section 3.2] Analytical example showing oblique vs. conditional-expectation value functions yield different Shapley values with different fidelity to model structure
  - [corpus] [2505.13118] extends Shapley to uncertainty attribution via novel value functions, supporting value function primacy.
- Break condition: If no theoretical criterion validates value function choice (no ground truth for decomposition), empirical comparisons remain circular.

## Foundational Learning

- Concept: **Transferable-utility cooperative games**
  - Why needed here: The entire framework models features as "players" in a game where coalition values v(A) are real numbers and total payoff v(D) is redistributed.
  - Quick check question: Given 3 features with v({1})=2, v({2})=3, v({1,2})=6, what is the dividend φ_v({1,2})? (Answer: 6 - 2 - 3 = 1)

- Concept: **Möbius inversion on posets**
  - Why needed here: Harsanyi dividends are computed via Möbius inversion; understanding this clarifies how value functions encode interaction structure.
  - Quick check question: For a set function v, the Möbius transform φ_v(A) = Σ_{B⊆A} (-1)^{|A|-|B|} v(B). If v is additive (v(A∪B)=v(A)+v(B) for disjoint A,B), what are the dividends for |A|>1? (Answer: Zero—no synergistic value)

- Concept: **Orthogonal vs. oblique projections**
  - Why needed here: Standard conditional-expectation value functions use orthogonal projections; oblique projections incorporate dependency structure explicitly, yielding "purer" attributions.
  - Quick check question: When features X_1, X_2 are correlated, does E[f(X)|X_1=x_1] project onto the space spanned by X_1 alone, or onto a space modified by correlation structure? (Answer: Orthogonal projection onto σ(X_1)-measurable functions, which implicitly marginalizes X_2 using conditional distributions affected by correlation)

## Architecture Onboarding

- Component map: Quantity of Interest -> Value Function -> Allocation Method -> Attribution Vector
- Critical path: Quantity of Interest → Value Function Design → Efficiency Verification → Allocation Selection → Attribution Computation. The value function is the dominant decision point.
- Design tradeoffs:
  - Uniform permutation distribution (Shapley) vs. structured distributions: Uniform is maximum-entropy but ignores causal/temporal priors; structured distributions (e.g., Asymmetric Shapley) incorporate domain knowledge but risk bias
  - Conditional-expectation vs. oblique-projection value functions: Conditional expectations are standard but may conflate correlation with importance; oblique projections are theoretically purer but require explicit dependency modeling
  - Shapley vs. Proportional Marginal Effects (PME): Shapley satisfies symmetry; PME better handles correlated/spurious features by adapting to the dual game
- Failure signatures:
  - **Spurious feature attribution**: Shapley values assign non-zero importance to features uncorrelated with Y but correlated with predictive features (see Section 3.3 analytical example with X_3)
  - **Impure decomposition**: Conditional-expectation value functions produce attributions that don't reflect model structure (e.g., interaction term attribution split incorrectly)
  - **Efficiency violation**: If v(D) ≠ quantity of interest, attributions won't decompose the target—check v(D) explicitly before computing allocations
- First 3 experiments:
  1. **Validate value function primacy**: On a simple model f(x) = x_1 + x_2 + x_1·x_2 with correlated features (Corr(X_1, X_2)=ρ), compute Shapley values with (a) conditional-expectation v and (b) oblique-projection v. Compare attribution fidelity to the analytical decomposition.
  2. **Test PME on spurious features**: Construct data with X_3 spurious (correlated with X_1 but not in true model). Compare Shapley vs. PME attributions; verify PME assigns zero importance to X_3 while Shapley does not.
  3. **Explore Weber set beyond uniform**: Implement a non-uniform permutation distribution encoding causal ordering (e.g., causes before effects). Compare resulting attributions to Shapley on a synthetic dataset with known causal structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can optimal transportation theory be formalized to derive new weighting systems within the Harsanyi set that are optimized for specific interpretability-driven costs?
- Basis in paper: [explicit] The authors state that "optimal transportation theory could be employed to derive new weighting systems, and thus new allocations optimized for various interpretability-driven costs. Such approaches have yet to be explored in the literature."
- Why unresolved: The paper identifies the potential connection to optimal transport but does not develop the mathematical formulation or specific cost functions required to implement it.
- What evidence would resolve it: A formalized allocation method derived from an optimal transport problem between feature coalitions and players, accompanied by theoretical guarantees on the resulting interpretation properties.

### Open Question 2
- Question: What specific optimization objectives or constraints—beyond incorporating causal structure—can be used to define non-uniform random order distributions within the Weber set that yield superior feature attributions?
- Basis in paper: [explicit] The authors note that "by altering optimization objectives or incorporating constraints, one can define new optimal distributions and, thus, novel allocations," but admit that fully exploring these possibilities remains an open direction.
- Why unresolved: While the paper frames allocation as a choice of distribution, it does not prescribe which specific objectives (e.g., stability, sparsity) should replace the maximum-entropy uniform distribution used by Shapley values.
- What evidence would resolve it: A comparative analysis of Weber set allocations derived from distinct non-uniform distributions, demonstrating improved fidelity or robustness on benchmark tasks.

### Open Question 3
- Question: How can the "purity" of an attribution be empirically validated or compared against other methods when a ground truth for feature importance is unavailable?
- Basis in paper: [inferred] The paper argues that the "absence of ground truth limits empirical comparisons" and that current methods can yield "misleading insights." It relies on theoretical justifications (like oblique projections) because empirical validation is difficult.
- Why unresolved: The authors highlight that the problem is one of "representation," where different decompositions are valid mathematically but potentially misleading practically, lacking a standardized empirical test for fidelity.
- What evidence would resolve it: The development of synthetic benchmark datasets with known, immutable structural properties (ground truth) that can rigorously test whether specific value functions and allocations recover the correct attributions.

## Limitations
- The claim that value function selection dominates allocation choice lacks comprehensive empirical validation across diverse model architectures and real-world datasets.
- The oblique-projection value functions and PME allocations are referenced but not fully specified, requiring external citations for complete implementation.
- The practical advantages of PME allocations for spurious features and Weber set alternatives beyond uniform distributions lack comprehensive empirical demonstration.

## Confidence
- **High Confidence**: The mechanism showing Shapley values as uniform distributions over permutations (Mechanism 1) and the role of efficiency in allocations (Mechanism 3) are well-supported by mathematical proofs and established theory.
- **Medium Confidence**: The claim about value function primacy (Mechanism 3) is analytically demonstrated but would benefit from broader empirical validation. The distinction between conditional-expectation and oblique-projection value functions is theoretically sound but incompletely specified.
- **Low Confidence**: The practical advantages of PME allocations for spurious features and Weber set alternatives beyond uniform distributions lack comprehensive empirical demonstration.

## Next Checks
1. **Empirical validation of value function primacy**: Compare attributions from multiple value functions (conditional expectation, oblique projection, variance-based) on diverse ML models (tree ensembles, neural networks) using synthetic and real datasets with known ground truth.
2. **Reproduce oblique-projection implementation**: Implement the oblique-projection value functions as described in the referenced work (Il Idrissi et al., 2025) and validate on correlated feature scenarios where conditional expectations fail.
3. **Benchmark PME against Shapley variants**: Systematically evaluate PME allocations on datasets with controlled spurious features and correlated predictors, comparing against Shapley values, Asymmetric Shapley, and other correlation-aware methods using multiple attribution fidelity metrics.