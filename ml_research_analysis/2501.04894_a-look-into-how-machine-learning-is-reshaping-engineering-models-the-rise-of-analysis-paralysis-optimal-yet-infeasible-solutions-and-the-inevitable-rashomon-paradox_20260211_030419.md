---
ver: rpa2
title: 'A Look into How Machine Learning is Reshaping Engineering Models: the Rise
  of Analysis Paralysis, Optimal yet Infeasible Solutions, and the Inevitable Rashomon
  Paradox'
arxiv_id: '2501.04894'
source_url: https://arxiv.org/abs/2501.04894
tags:
- engineering
- learning
- https
- machine
- paradox
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how machine learning (ML) is reshaping traditional
  engineering models and identifies three core paradoxes that arise when adopting
  ML in engineering practice. The author compares traditional physics-based, causal,
  semi-empirical, and empirical models with their ML counterparts, demonstrating ML's
  application through induction (using symbolic regression for concrete strength prediction),
  deduction (ML-enhanced buckling analysis), and abduction (ML-driven design optimization
  and clustering).
---

# A Look into How Machine Learning is Reshaping Engineering Models: the Rise of Analysis Paralysis, Optimal yet Infeasible Solutions, and the Inevitable Rashomon Paradox

## Quick Facts
- arXiv ID: 2501.04894
- Source URL: https://arxiv.org/abs/2501.04894
- Reference count: 40
- Demonstrates ML-induced paradoxes in engineering: analysis paralysis, infeasible optimization, and contradictory explainability

## Executive Summary
This paper examines how machine learning adoption in engineering creates three fundamental paradoxes that challenge traditional practice. Through concrete case studies in structural engineering, the author demonstrates how ML's statistical power comes at the cost of physical interpretability, how optimization can produce technically optimal but practically unusable designs, and how different explainability methods can yield contradictory interpretations. The work provides both theoretical framework and practical examples of these challenges, along with proposed mitigation strategies including hybrid physics-informed models and multi-tiered constraint hierarchies.

## Method Summary
The study applies ML algorithms (PySR for symbolic regression, Random Forest, SPINEX) to three engineering problems: concrete compressive strength prediction, RC column fire resistance analysis, and CFST axial capacity optimization. Models are trained using standard random splits into training/validation/test sets with 10-fold cross-validation. The analysis compares ML predictions against traditional code-based equations and examines interpretability through dual SHAP and LIME explanations. Physical plausibility and constructability constraints are used to evaluate the practical viability of ML-optimized solutions.

## Key Results
- Analysis paralysis occurs when ML models achieve high accuracy but lose physical interpretability compared to traditional physics-based models
- Optimal yet infeasible solutions arise when ML optimization produces designs that violate practical constraints like standard section availability
- The Rashomon paradox manifests when different explainability methods (SHAP vs. LIME) provide contradictory interpretations of feature importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid physics-informed models can reduce analysis paralysis by enforcing physical consistency during training.
- Mechanism: Physics-informed neural networks (PINNs) integrate governing equations (e.g., Euler's buckling formula) as soft constraints into the loss function. The network learns to minimize prediction error while remaining within physically plausible boundaries, preserving some interpretability through equation residuals.
- Core assumption: The governing physics are known and differentiable; violations indicate model extrapolation beyond valid domains.
- Evidence anchors:
  - [abstract] "hybrid models that incorporate physical constraints" as mitigation for analysis paralysis
  - [section 3.0, Table 1] PINNs listed as ML equivalent to physics-based models, noting they "strive to retain interpretability of physics-based models"
  - [corpus] HILL paper mentions "latent space representations...often remain obscure," supporting that unguided latent learning leads to interpretability loss
- Break condition: When physical laws are incomplete or the phenomenon is fundamentally empirical (e.g., concrete bond strength), PINNs provide no advantage over data-driven approaches.

### Mechanism 2
- Claim: Multi-tiered constraint hierarchies prevent infeasible solutions by separating hard code-compliance constraints from soft optimization objectives.
- Mechanism: Optimization operates with penalty-weighted layers: (1) Primary constraints enforce building code minimums (e.g., fire resistance R ≥ 120 min), (2) Secondary constraints enforce constructability (e.g., standard section availability), (3) Tertiary objectives optimize material volume. Violations at higher tiers multiply penalty terms exponentially.
- Core assumption: Constraint boundaries are discrete and enumerable; continuous optimization can be mapped to discrete commercial catalogs.
- Evidence anchors:
  - [section 5.2] CFST optimization returned D = 380 mm—no standard section exists, making the solution "optimal yet infeasible"
  - [section 5.2] Paper recommends "discrete variable constraints that align with commercially available structural sections"
  - [corpus] Weak direct evidence; neighbor papers focus on software engineering, not structural constraint handling
- Break condition: When design space has high-dimensional categorical dependencies (e.g., multi-material composites with manufacturing sequencing), simple penalty hierarchies fail to capture feasibility interdependencies.

### Mechanism 3
- Claim: Cross-validation across multiple explainability methods (SHAP + LIME) with physics-based verification reduces Rashomon paradox risks.
- Mechanism: SHAP computes feature contributions via Shapley values across all feature coalitions (globally consistent), while LIME fits local linear approximations around individual predictions. Disagreements flag regions where model behavior diverges from physical expectations; these become candidate points for domain expert review or model retraining.
- Core assumption: Physical laws provide ground truth for feature directionality; systematic sign inversions indicate model pathology rather than legitimate complexity.
- Evidence anchors:
  - [section 5.3] SHAP and LIME showed opposite signs for eccentricity's effect on fire resistance; higher eccentricity reducing FR is physically implausible
  - [section 5.3] "This paradox arises due to differences in methodology"—SHAP considers all coalitions, LIME approximates locally
  - [corpus] Weak direct evidence on explainability disagreement quantification
- Break condition: When the underlying phenomenon is genuinely multicausal with context-dependent feature roles (e.g., soil-structure interaction), sign disagreements may reflect reality rather than model error.

## Foundational Learning

- Concept: **Induction vs. Deduction vs. Abduction in engineering reasoning**
  - Why needed here: The paper structures ML applications by reasoning type; understanding this taxonomy clarifies when ML replaces vs. augments traditional methods.
  - Quick check question: Given a new structural failure with limited sensor data, would you apply inductive, deductive, or abductive ML approaches?

- Concept: **Physics-informed loss functions**
  - Why needed here: PINNs and hybrid models require encoding governing equations as differentiable constraints; misunderstanding this leads to incorrect penalty formulations.
  - Quick check question: How would you incorporate Euler's buckling equation into a neural network training loop for column capacity prediction?

- Concept: **Feature attribution method limitations (SHAP vs. LIME)**
  - Why needed here: The Rashomon paradox emerges from methodological differences; practitioners must understand when each method is reliable.
  - Quick check question: For a highly non-linear interaction between concrete cover and fire resistance, which method would you trust more—why?

## Architecture Onboarding

- Component map: Data layer -> Model layer -> Explainability layer -> Constraint layer -> Validation layer
- Critical path:
  1. Define physical boundaries and code constraints before model training
  2. Train with default hyperparameters first (per paper's methodology), validate via 10-fold CV
  3. Generate dual explainability reports; flag any sign disagreements with physics
  4. Map continuous optimization outputs to discrete commercial catalogs

- Design tradeoffs:
  - Accuracy vs. interpretability: Symbolic regression yields equations but lower R² than black-box ensembles
  - Generality vs. specificity: Models trained on narrow datasets (e.g., only circular CFST) may fail on rectangular sections
  - Automation vs. oversight: Fully automated optimization produces infeasible solutions without constructability constraints

- Failure signatures:
  - Symbolic expressions with non-physical dimensional combinations (e.g., D·t·fy term without proper coefficient normalization)
  - Optimization returning non-standard dimensions (D = 380 mm when no such tube exists)
  - SHAP/LIME feature importance contradicting known thermomechanical principles

- First 3 experiments:
  1. Replicate the concrete strength symbolic regression on Yeh dataset; compare PySR equation vs. Abrams Law on held-out test set using R² and physical plausibility of coefficients.
  2. Train Random Forest on RC column fire resistance; generate SHAP and LIME explanations for 10 random samples; count and categorize disagreements.
  3. Run CFST fire resistance optimization with and without discrete section constraints; measure infeasibility rate and distance to nearest standard section.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What standardized metrics can be developed to evaluate machine learning interpretability specifically from an engineering lens?
- **Basis in paper:** [explicit] Section 6.0 states that future research must focus on "establishing standardized metrics for evaluating interpretability from an engineering lens."
- **Why unresolved:** Current evaluation relies heavily on statistical performance (e.g., R², MAE) rather than the physical consistency or "engineering intuition" required for professional acceptance.
- **What evidence would resolve it:** A validated framework of benchmarks that scores models on their adherence to physical laws alongside predictive accuracy.

### Open Question 2
- **Question:** How can thresholds be established to quantify acceptable levels of disagreement between different explainability methods (e.g., SHAP vs. LIME) relative to physical laws?
- **Basis in paper:** [explicit] Section 5.3 suggests a solution involving "establishing clear thresholds and metrics for acceptable levels of disagreement between different explainability methods."
- **Why unresolved:** The Rashomon paradox causes methods like SHAP and LIME to yield contradictory feature importances, and currently no protocol exists to determine which explanation is physically valid.
- **What evidence would resolve it:** A quantitative methodology that flags when divergence between XAI methods violates known mechanical principles (e.g., thermomechanical behaviors).

### Open Question 3
- **Question:** What architectural frameworks are required to enable continuous learning and knowledge transfer in engineering ML models?
- **Basis in paper:** [explicit] Section 6.0 notes the necessity for systems that "possess the capability for continuous learning and knowledge transfer," as static systems quickly become obsolete.
- **Why unresolved:** Structural engineering domains evolve with new materials and codes; current static models cannot adapt dynamically without complete retraining.
- **What evidence would resolve it:** The demonstration of a model that incrementally learns from new design codes or material data without losing previously acquired domain knowledge.

## Limitations
- The paper relies on specific dataset splits and default hyperparameter settings that are not fully specified
- Symbolic regression equations and explainability method implementations depend on software versions that may have changed
- The Rashomon paradox demonstration depends on finding specific edge cases where SHAP and LIME disagree with physical intuition

## Confidence
- High confidence: The core mechanisms of analysis paralysis and infeasible optimization solutions are well-supported
- Medium confidence: The Rashomon paradox mechanism is theoretically sound but reproducibility depends on implementation details
- Low confidence: The hybrid physics-informed model mitigation strategy lacks empirical validation beyond theoretical description

## Next Checks
1. Reproduce the concrete strength symbolic regression using the exact Yeh dataset and verify whether PySR with default settings generates equations with R² > 0.92 that also satisfy physical plausibility criteria
2. Generate 100 random RC column fire resistance predictions and systematically count SHAP/LIME sign disagreements, comparing frequency and magnitude against physical expectations
3. Implement the multi-tiered constraint hierarchy on the CFST optimization problem and measure the reduction in infeasible (non-standard section) solutions versus unconstrained optimization