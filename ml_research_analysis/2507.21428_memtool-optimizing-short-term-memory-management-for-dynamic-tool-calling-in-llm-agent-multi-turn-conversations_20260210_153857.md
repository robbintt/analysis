---
ver: rpa2
title: 'MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling
  in LLM Agent Multi-Turn Conversations'
arxiv_id: '2507.21428'
source_url: https://arxiv.org/abs/2507.21428
tags:
- tool
- tools
- agent
- mode
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MemTool introduces three architectures to dynamically manage tool\
  \ context in LLM agents during multi-turn conversations: Autonomous Agent Mode (full\
  \ autonomy to add/remove tools), Workflow Mode (deterministic prune-then-search),\
  \ and Hybrid Mode (structured pruning with autonomous tool retrieval). Evaluated\
  \ on 13+ models using the ScaleMCP benchmark, Autonomous Agent Mode achieved 90\u2013\
  94% tool-removal efficiency with reasoning LLMs, while smaller models showed 0\u2013\
  60% efficiency."
---

# MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations

## Quick Facts
- arXiv ID: 2507.21428
- Source URL: https://arxiv.org/abs/2507.21428
- Authors: Elias Lumer; Anmol Gulati; Vamse Kumar Subbiah; Pradeep Honaganahalli Basavaraju; James A. Burke
- Reference count: 10
- Primary result: Three architectures dynamically manage tool context in LLM agents during multi-turn conversations, with Autonomous Agent Mode achieving 90–94% tool-removal efficiency with reasoning LLMs.

## Executive Summary
MemTool addresses the critical challenge of managing tool context in LLM agents during multi-turn conversations, where tool definitions can exceed API limits (128–512 tools). The paper introduces three architectures: Autonomous Agent Mode (full autonomy to add/remove tools), Workflow Mode (deterministic prune-then-search), and Hybrid Mode (structured pruning with autonomous tool retrieval). Evaluated on 13+ models using the ScaleMCP benchmark, the study demonstrates that reasoning-capable LLMs can autonomously manage tool context with 90–94% removal efficiency, while smaller models struggle without deterministic workflows. The research highlights the critical role of model capability and prompt design in effective short-term tool memory management.

## Method Summary
MemTool introduces three architectures for dynamic tool context management in LLM agents. The approach uses the ScaleMCP benchmark (5,000 MCP servers) with 100 sequential user queries. The three modes are: Autonomous Agent Mode (LLM equipped with Search_Tools and Remove_Tools functions), Workflow Mode (deterministic prune-then-search with separate LLM calls), and Hybrid Mode (deterministic pruning + autonomous tool addition). Evaluation metrics include Removal Ratio, Avg Removal Ratio 3T, Avg Residual 3T, Task Completion, and Tool Correctness. The system uses text-embedding-ada-002 for semantic search with top-k=5 retrieval, and tool limit L=128.

## Key Results
- Reasoning LLMs (GPT-o3, Gemini 2.5 Pro/Flash) achieve 90–94% tool-removal efficiency in Autonomous Agent Mode
- Smaller models show 0–60% removal efficiency in Autonomous mode but >90% in Workflow/Hybrid modes
- Hybrid Mode consistently maintains >90% removal ratios across all models while preserving adaptive retrieval
- Task completion is highest in Autonomous and Hybrid modes due to adaptive tool retrieval capabilities

## Why This Works (Mechanism)

### Mechanism 1
Reasoning-capable LLMs can autonomously manage tool context when equipped with explicit memory operations. The agent receives `Search_Tools` and `Remove_Tools` meta-tools, enabling it to query a tool knowledge base and prune irrelevant tools between turns. The system prompt includes the current tool count as a dynamic variable, allowing the model to track context occupancy. When tool count approaches API limits (128), error messages signal the agent to prune. Core assumption: Models with large reinforcement learning reasoning budgets can simultaneously manage tool context while solving user queries. Break condition: Non-reasoning models fail to invoke `Remove_Tools` proactively, causing tool accumulation to API limits.

### Mechanism 2
Deterministic workflow architectures decouple memory management from task execution, enabling reliable pruning across all model sizes. Two sequential LLM calls precede task execution: (1) a pruning call identifies irrelevant tools based on query-tool relevance; (2) a search call retrieves new tools. The agent receives only the final managed tool set, eliminating autonomy-dependent failures. Core assumption: Separate LLM calls focused solely on tool relevance judgment outperform simultaneous task-and-memory reasoning. Break condition: Once the agent initializes with the tool list, it cannot request additional tools mid-task if initial retrieval fails.

### Mechanism 3
Hybrid architectures retain adaptive retrieval benefits while outsourcing the pruning task that models struggle with. Deterministic pruning removes irrelevant tools before each turn; the agent retains `Search_Tools` autonomy for retrieval. This asymmetric design leverages observed model strengths (searching) while mitigating weaknesses (removal). Core assumption: Models are better at recognizing when tools are needed than when they are no longer needed. Break condition: If agents aggressively add tools without triggering pruning, context can still overflow before the next turn's deterministic prune.

## Foundational Learning

- **Concept: Context window as working memory**
  - Why needed: The paper analogizes LLM context to RAM; understanding token budgets and API tool limits (128–512) is prerequisite to designing memory management.
  - Quick check: What happens when tool definitions exceed the API's tool-count limit during a session?

- **Concept: Retrieval-augmented tool selection**
  - Why needed: MemTool relies on vector search over 5,000 MCP servers; understanding embedding-based retrieval explains how tools are discovered dynamically.
  - Quick check: How does semantic search differ from fixed tool registries in multi-turn settings?

- **Concept: Agentic autonomy gradients**
  - Why needed: The three modes represent a spectrum from full autonomy to deterministic control; understanding trade-offs between adaptability and reliability informs mode selection.
  - Quick check: Which mode would you choose for a cost-sensitive production system with a mid-tier model?

## Architecture Onboarding

- **Component map:** Tool Knowledge Base -> Memory Manager -> Agent Loop -> History Manager
- **Critical path:** 1) User query arrives with prior messages and prior tool set; 2) History manager prunes message context if needed; 3) Memory manager executes mode-specific logic (autonomous, workflow, or hybrid); 4) Agent receives managed tool list and executes task; 5) Tool state persists to next turn
- **Design tradeoffs:** Autonomous Agent: Highest task completion but requires reasoning models; prompt design is critical. Workflow: Most reliable across models but no mid-task recovery from poor retrieval. Hybrid: Balanced adaptability with stable memory; requires two LLM calls per turn.
- **Failure signatures:** Tool count monotonic increase → Model not invoking `Remove_Tools` (Autonomous mode with non-reasoning model). Low task completion despite high removal ratio → Poor retrieval or tool selection quality. Sudden API errors → Tool limit exceeded; check if pruning step executed.
- **First 3 experiments:** 1) Replicate Autonomous Agent Mode with GPT-4o vs. GPT-o3 on 20-turn conversation; compare tool count trajectories to validate reasoning-model dependency. 2) Implement Workflow Mode with a small model (e.g., GPT-4.1 Mini) as memory controller and larger model for task execution; measure cost-per-turn. 3) Stress-test Hybrid Mode with queries requiring 3+ sequential tool searches; verify pruning keeps tool count below 128 across 50 turns.

## Open Questions the Paper Calls Out

### Open Question 1
Can short-term tool memory management approaches like MemTool be effectively extended to long-term memory for tool-using agents across multiple sessions? Basis: The conclusion states "Looking forward, MemTool can be paired with long-term memory advancements in tool-using LLM agents" and Section 2.1.2 notes that "MemTool's three modes can be partially extrapolated to how LLM agents manage their long-term memory as well." Unresolved because MemTool was evaluated only within single-session multi-turn conversations; no experiments tested cross-session persistence.

### Open Question 2
What specific prompting techniques or training methods could improve smaller models' tool-removal efficiency in Autonomous Agent Mode? Basis: The paper notes smaller models "struggle to efficiently manage its short-term tool memory" and that "prompting the system message of the LLM agent can provide detailed guidelines on when to remove tools and when to add them" but "the system prompt, when it did not include the current tool count, caused the LLM agent to not remove tools." Unresolved because the study identifies the problem (0–60% removal efficiency for smaller models) but offers only preliminary prompt suggestions without systematic evaluation.

### Open Question 3
How does MemTool performance scale beyond 100 sequential turns with larger tool repositories (e.g., 10,000+ tools)? Basis: The evaluation used 100 sequential queries with 5,000 MCP servers, but Section 3.2 notes the sampling was stratified based on an average of 5 tool calls per turn, leaving scalability unexplored. Unresolved because production deployments may involve longer sessions and larger tool ecosystems than tested.

## Limitations

- Prompt template sensitivity creates uncertainty about reproducibility, as prompt design appears to be the critical differentiator between reasoning and non-reasoning model performance.
- Benchmark generality is questionable with only 100 sequential queries, raising concerns about whether observed performance differences reflect fundamental architectural advantages or dataset-specific artifacts.
- Cost-performance tradeoffs are not analyzed, despite Workflow and Hybrid modes requiring multiple LLM calls per turn versus one in Autonomous mode.

## Confidence

**High Confidence:** The core architectural designs and their implementation are clearly specified. The three-mode spectrum from autonomy to determinism is well-justified by observed model capability differences. Task completion results showing Autonomous and Hybrid modes outperforming Workflow are consistent across multiple metrics.

**Medium Confidence:** The claim that reasoning models can autonomously manage tool context (90-94% removal efficiency) is supported by results but depends heavily on prompt engineering not fully disclosed. The mechanism by which non-reasoning models fail at memory management is observed but the prompt modifications needed for improvement are not systematically explored.

**Low Confidence:** The assertion that models are inherently better at recognizing when tools are needed versus when they are no longer needed (underpinning Hybrid Mode design) is reasonable but not directly tested. No comparison is provided between models' retrieval accuracy versus removal accuracy to validate this asymmetry claim.

## Next Checks

1. **Prompt Ablation Study:** Systematically remove the dynamic tool count variable from the Autonomous Agent Mode prompt and measure the impact on removal efficiency across all tested models.

2. **Cross-Benchmark Validation:** Implement the three architectures on an independent tool-calling benchmark (e.g., ToolBench or Gorilla) with at least 50 queries. Compare relative mode performance to ScaleMCP results to assess generality.

3. **Cost-Per-Turn Analysis:** Calculate total token consumption per turn for each mode across all model sizes, including tool descriptions, history, and intermediate LLM calls. Determine the breakeven point where Workflow/Hybrid costs exceed their performance benefits compared to Autonomous mode.