---
ver: rpa2
title: Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding
  in Large Language Models
arxiv_id: '2512.13194'
source_url: https://arxiv.org/abs/2512.13194
tags:
- ears
- rejection
- speculative
- sampling
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Efficient Adaptive Rejection Sampling (EARS) improves speculative
  decoding by dynamically adjusting the acceptance threshold based on the target model's
  predictive uncertainty. It reduces random rejections in high-uncertainty scenarios,
  increasing token throughput by up to 18.12% while maintaining accuracy (only 0.84%
  drop on GSM8K).
---

# Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models

## Quick Facts
- arXiv ID: 2512.13194
- Source URL: https://arxiv.org/abs/2512.13194
- Reference count: 7
- Primary result: 18.12% throughput improvement with only 0.84% accuracy drop on GSM8K

## Executive Summary
Efficient Adaptive Rejection Sampling (EARS) improves speculative decoding by dynamically adjusting the acceptance threshold based on the target model's predictive uncertainty. It reduces random rejections in high-uncertainty scenarios, increasing token throughput by up to 18.12% while maintaining accuracy (only 0.84% drop on GSM8K). The method is simple, architecture-agnostic, and integrates seamlessly into existing frameworks.

## Method Summary
EARS modifies the rejection sampling rule in speculative decoding by introducing an uncertainty-aware tolerance term. For each token position, it computes predictive uncertainty as 1 - max(P_target), then relaxes the acceptance threshold by β times this uncertainty. This creates a "pardon path" that accepts candidates rejected due to random threshold variance rather than genuine distributional mismatch. The method pre-computes and caches uncertainty metrics to avoid memory bandwidth bottlenecks during verification.

## Key Results
- 18.12% token throughput improvement on Open-domain QA tasks
- 0.84% accuracy drop on GSM8K reasoning benchmark (96.44% → 95.60%)
- β=0.1 hyperparameter provides optimal balance between throughput and accuracy
- Pre-computation optimization avoids memory bandwidth bottlenecks

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Aware Threshold Relaxation
Makes the rejection threshold proportional to model uncertainty, reducing "random rejections" without degrading output quality. When target model's distribution is flat (multiple plausible tokens), uncertainty is high, automatically lowering the effective acceptance threshold. This prevents borderline candidates from being rejected purely due to random variance.

### Mechanism 2: Two-Path Acceptance with "Pardon" Logic
Implements a secondary acceptance path for near-miss candidates, preserving throughput gains while maintaining distributional fidelity. The algorithm first checks standard condition R_i ≥ U_i, then checks R_i ≥ U_i - Tolerance_i for borderline cases. This specifically rescues candidates rejected due to random threshold variance rather than genuine distributional mismatch.

### Mechanism 3: Pre-Computation with Cached Uncertainty
Computes and caches uncertainty metrics during the forward pass to avoid memory bandwidth bottlenecks during verification. After target model computes P_t in parallel for all draft positions, max(P_t) is extracted immediately and cached. During verification, EARS reads this scalar rather than re-scanning full probability vector.

## Foundational Learning

- **Speculative Decoding Pipeline**: Understanding the draft-then-verify loop is prerequisite to understanding where EARS fits. Quick check: Can you explain why speculative decoding improves latency even though it requires running both a draft and target model?

- **Rejection Sampling for Distribution Matching**: EARS directly modifies the rejection sampling rule. Quick check: In standard speculative decoding, what does it mean probabilistically when R_i < U_i causes a rejection?

- **Predictive Uncertainty Measures (Entropy vs. Confidence)**: EARS uses 1 - max(P_t) as uncertainty proxy. Quick check: Why might 1 - max(P_t) be a cheaper uncertainty estimate than computing full Shannon entropy over vocabulary?

## Architecture Onboarding

- **Component map**:
  [Draft Model Md] → generates candidate tokens {x1...xγ}
  ↓
  [Target Model Mt] → parallel forward pass computes Pt(xi), Pt(full vocab)
  ↓
  [Uncertainty Cache] → extracts and stores max(Pt) per position
  ↓
  [EARS Verifier] → for each position i:
    1. Compute Ri = Pt(xi) / Pd(xi)
    2. Sample Ui ~ Uniform(0,1)
    3. Compute Tolerance_i = β · (1 - max(Pt))
    4. Check: Ri ≥ Ui? → Accept
    5. Else check: Ri ≥ Ui - Tolerance_i? → Accept (pardon)
    6. Else → Reject, fallback to Mt autoregressive generation

- **Critical path**: The verification loop (steps 4-6 above) is latency-critical. Any overhead in computing Tolerance_i or second comparison directly adds to per-token verification time.

- **Design tradeoffs**:
  - β hyperparameter: Lower β (0.05) is conservative, higher β (0.2) increases throughput but risks quality degradation
  - Uncertainty measure choice: Using 1 - max(P_t) is computationally cheap but less informative than full entropy
  - Temperature interaction: Higher temperatures flatten P_t, automatically increasing tolerance

- **Failure signatures**:
  - Sudden accuracy drop on reasoning benchmarks: likely indicates β too high or miscalibrated uncertainty
  - No throughput improvement: check uncertainty cache is populated correctly
  - Increased latency: second comparison adds branching overhead, profile GPU SIMD utilization

- **First 3 experiments**:
  1. Ablation on β: Run speculative decoding with EARS at β ∈ {0.0, 0.05, 0.1, 0.15, 0.2} on creative writing and GSM8K
  2. Latency breakdown profiling: Instrument verification loop to measure time in standard path, pardon path, rejection
  3. Uncertainty calibration check: Compute correlation between 1 - max(P_t) and actual token-level diversity

## Open Questions the Paper Calls Out

- **Open Question 1**: Can distribution entropy or variance provide a more optimal tolerance signal than the 1-max(P_target) approximation? The authors list exploring more refined uncertainty measures as future work.

- **Open Question 2**: Does the pardon path induce a distribution shift that violates theoretical guarantee of sampling strictly from target model's distribution? The algorithm accepts tokens when P_target/P_draft < U, deviating from standard rejection sampling proofs.

- **Open Question 3**: How does EARS perform in chain-of-thought scenarios where adaptive tolerance might incorrectly accept logically incoherent tokens? The authors propose investigating performance in complex reasoning scenarios like chain-of-thought and tool calling.

## Limitations

- Uncertainty calibration relies on 1 - max(P_t) as coarse measure that may not accurately reflect genuine model ambiguity in all contexts
- Effectiveness likely tied to choice of speculative length γ, but paper does not analyze performance scaling with different draft sequence lengths
- Pardon path explicitly accepts candidates that would be rejected under standard rejection sampling, with no formal proof or extensive distributional analysis of the impact

## Confidence

- **High Confidence**: Core mechanism is clearly described and implementable; GSM8K accuracy result provides empirical validation; pre-computation optimization is straightforward engineering improvement
- **Medium Confidence**: Throughput improvement claim relies on unnamed benchmark; β=0.1 as default balance is heuristic without systematic tuning method
- **Low Confidence**: Claim of seamless integration lacks detail on specific speculative decoding implementations; architecture-agnostic assertion untested beyond Qwen3-32B

## Next Checks

1. **Beta hyperparameter sweep on diverse tasks**: Systematically evaluate EARS with β ∈ {0.0, 0.05, 0.1, 0.15, 0.2} on creative writing, reasoning (GSM8K-style), and factoid QA; plot throughput vs. accuracy to identify task-specific Pareto frontiers

2. **Speculative length scaling study**: Fix β=0.1 and evaluate EARS on GSM8K with γ ∈ {1, 3, 5, 10, 20}; measure both throughput and accuracy to reveal whether uncertainty-aware mechanism compounds benefits with longer drafts

3. **Distributional analysis of pardoned tokens**: For held-out dataset, compare distribution of tokens accepted via standard path vs. pardon path; compute KL divergence from target model's distribution for each set to test distributional impact