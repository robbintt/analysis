---
ver: rpa2
title: 'Downsized and Compromised?: Assessing the Faithfulness of Model Compression'
arxiv_id: '2510.06125'
source_url: https://arxiv.org/abs/2510.06125
tags:
- bias
- compression
- dataset
- accuracy
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for evaluating the faithfulness
  of compressed machine learning models beyond traditional size and accuracy metrics.
  The authors propose using model agreement and chi-squared statistical tests to detect
  whether compression techniques cause significant shifts in predictive behavior and
  fairness across demographic subgroups.
---

# Downsized and Compromised?: Assessing the Faithfulness of Model Compression

## Quick Facts
- arXiv ID: 2510.06125
- Source URL: https://arxiv.org/abs/2510.06125
- Reference count: 40
- Authors: Moumita Kamal; Douglas A. Talbert
- Key outcome: Introduces framework using model agreement and chi-squared tests to detect compression-induced shifts in predictive behavior and fairness

## Executive Summary
This paper introduces a novel framework for evaluating the faithfulness of compressed machine learning models beyond traditional size and accuracy metrics. The authors propose using model agreement and chi-squared statistical tests to detect whether compression techniques cause significant shifts in predictive behavior and fairness across demographic subgroups. They demonstrate their approach by applying quantization and pruning to neural networks trained on three socially meaningful datasets: COMPAS, Trauma, and Employment. Their findings show that while quantization preserves both accuracy and agreement with baseline models, pruning often leads to significant decreases in both. Importantly, the statistical tests reveal that even models with high agreement can exhibit non-random shifts in predictive distributions, particularly affecting specific subgroups.

## Method Summary
The framework evaluates compressed models by comparing their predictions to baseline models using model agreement metrics (accuracy, precision, recall, F1) and chi-squared statistical tests on contingency tables. The authors train baseline neural networks on three datasets (COMPAS, Trauma, Employment) then apply quantization-aware training and magnitude-based pruning. For each compressed model, they compute agreement statistics with the baseline and run chi-squared tests to detect statistically significant changes in prediction distributions, both overall and within demographic subgroups. The approach uses 70/15/15 train/validation/test splits and evaluates both predictive consistency and fairness preservation.

## Key Results
- Quantization preserves model accuracy and agreement with baseline models, maintaining faithfulness
- Pruning often causes significant accuracy drops and agreement decreases, indicating unfaithful compression
- Chi-squared tests reveal non-random shifts in prediction distributions even when agreement metrics appear high
- Subgroup analysis shows compression can disproportionately affect specific demographic groups despite stable aggregate fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chi-squared tests on prediction distributions detect statistically significant behavioral shifts in compressed models that accuracy metrics miss.
- Mechanism: The method constructs contingency tables comparing predicted class distributions between baseline and compressed models, then computes χ² statistics with p-values. A significant p-value (≤0.05) indicates the compression introduced non-random changes in predictive behavior, even when overall accuracy remains similar.
- Core assumption: The baseline model's predictions represent the "ground truth" behavior that compressed models should replicate; statistical independence between compression and prediction changes implies faithfulness.
- Evidence anchors:
  - [abstract]: "apply chi-squared tests to detect statistically significant changes in predictive patterns across both the overall dataset and demographic subgroups"
  - [Section 6.2]: Describes contingency table construction and p-value calculation for agreement statistics
  - [corpus]: Weak corpus support—neighbor papers focus on compression effects but not statistical faithfulness testing
- Break condition: When baseline model itself has systematic errors or bias, faithfulness to it perpetuates problems rather than detecting them.

### Mechanism 2
- Claim: Model agreement metrics directly measure instance-level alignment between baseline and compressed models, revealing disagreements that aggregate accuracy obscures.
- Mechanism: Computes agreement accuracy, precision, recall, and F1-score by treating baseline predictions as ground truth. This captures whether both models make the same errors on the same instances, not just whether they achieve similar overall accuracy.
- Core assumption: Two models achieving similar accuracy may differ systematically in which specific instances they classify correctly; measuring agreement directly is more sensitive to behavioral changes.
- Evidence anchors:
  - [abstract]: "assess predictive consistency between the original and compressed models using model agreement"
  - [Section 6]: Shows quantized COMPAS model with 0.820 accuracy but 0.896 agreement accuracy, revealing behavioral consistency beyond raw performance
  - [corpus]: Minimal direct support—neighbor papers do not use agreement-based faithfulness metrics
- Break condition: When validation set is not representative of test distribution, agreement predictions become unreliable.

### Mechanism 3
- Claim: Extending statistical testing to demographic subgroups detects fairness shifts that aggregate equalized odds metrics hide.
- Mechanism: Constructs separate 2×2 contingency tables for each demographic subgroup (e.g., male vs. female), combines into multi-dimensional tables, and computes chi-squared statistics. This identifies whether compression systematically alters behavior for specific groups even when overall bias metrics appear stable.
- Core assumption: Aggregate fairness metrics like equalized odds can show similar values before and after compression while individual subgroup treatment changes significantly.
- Evidence anchors:
  - [abstract]: "exposing shifts that aggregate fairness metrics may obscure"
  - [Section 7.2]: Shows combined contingency table approach (Table 9) and subgroup-specific p-value analysis (Figure 10)
  - [corpus]: Weak support—neighbor papers discuss compression effects on robustness/reasoning but not subgroup-specific fairness shifts
- Break condition: When subgroup sample sizes are too small, chi-squared tests lack power to detect real differences.

## Foundational Learning

- Concept: Chi-squared test for independence
  - Why needed here: Core statistical method for detecting whether compression causes non-random changes in prediction distributions
  - Quick check question: Given a 2×2 contingency table of baseline vs. compressed predictions with O observed and E expected counts, what does χ² = Σ(O-E)²/E represent?

- Concept: Contingency table construction for model comparison
  - Why needed here: Required to format prediction comparisons for statistical testing across overall and subgroup analyses
  - Quick check question: How would you structure a contingency table comparing baseline and compressed model predictions for a binary classification task?

- Concept: Equalized odds fairness metric
  - Why needed here: Baseline fairness comparison; the paper shows this metric can miss compression-induced bias shifts
  - Quick check question: If a model has equal TPR and FPR across demographic groups but agreement patterns with its compressed version differ by group, is it "faithful" regarding fairness?

## Architecture Onboarding

- Component map: Dataset → Train/Val/Test Split (70/15/15) → Baseline ANN (tuned) → Quantization (QAT) or Pruning → Compressed Models → Agreement Statistics (vs. Baseline) → Contingency Tables (overall + subgroup) → Chi-squared Tests → p-values → Faithfulness Decision

- Critical path: Baseline model quality → Compression technique selection → Agreement computation → Statistical significance testing. If baseline is poorly tuned, all downstream faithfulness assessments are compromised.

- Design tradeoffs:
  - Quantization: ~88% size reduction with minimal accuracy loss; higher faithfulness but less computational sparsity benefit
  - Pruning: Creates sparse models but may not reduce stored size without specialized handling; higher risk of unfaithful behavior (COMPAS accuracy dropped from 0.826 to 0.708)
  - Validation set size: Smaller validation sets reduce predictability of test set faithfulness

- Failure signatures:
  - High accuracy but low agreement accuracy → compression systematically changes which instances are correct, not overall performance
  - Non-significant overall chi-squared but significant subgroup chi-squared → aggregate metrics hiding group-specific problems (see Figure 10, where "AfAm" subgroup drove race differences)
  - Validation-test agreement gap widening across iterations → unstable compression process

- First 3 experiments:
  1. Replicate baseline model training with 70/15/15 split, verify validation accuracy predicts test accuracy (RMSE < 0.02, MAPE < 2% per Table 5).
  2. Apply quantization-aware training, compute agreement statistics and chi-squared p-values; confirm p > 0.05 for most runs indicates faithful compression.
  3. Apply magnitude pruning to same baseline, run subgroup-level chi-squared tests; expect higher rate of significant p-values (≤0.05) indicating unfaithful compression, especially for pruned COMPAS model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do knowledge distillation and low-rank factorization introduce the same statistically significant shifts in subgroup predictions observed in pruning and quantization?
- Basis in paper: [explicit] Section 9.1 states future work will "expand this framework to include additional compression techniques, such as knowledge distillation and low-rank factorization."
- Why unresolved: The current study restricted experiments to magnitude-based pruning and quantization-aware training on Artificial Neural Networks (ANNs).
- What evidence would resolve it: Application of the proposed chi-squared agreement metrics and bias tests to models compressed via distillation and low-rank approximation on the same datasets.

### Open Question 2
- Question: Does tightening the statistical significance threshold on validation sets effectively prevent the deployment of unfaithful models without excessively rejecting faithful ones?
- Basis in paper: [explicit] Section 9.1 proposes evaluating whether "making the threshold for validation set statistical significance more sensitive... can better prevent acceptance of unfaithful compressions."
- Why unresolved: While validation sets predicted outcomes well (80-100%), they were not perfect; the optimal sensitivity for a diagnostic threshold remains undetermined.
- What evidence would resolve it: A comparative study measuring false acceptance rates of unfaithful compressions under varying p-value thresholds (e.g., $\alpha=0.01$ vs $0.05$) on validation data.

### Open Question 3
- Question: Can consistency of model explanations serve as a reliable metric for faithfulness, detecting shifts that prediction-based agreement metrics might miss?
- Basis in paper: [explicit] Section 2.3.3 and Section 9.1 identify "consistency of explanations" and "explain[ing] the model differences" as necessary expansions beyond prediction agreement.
- Why unresolved: A model may retain prediction accuracy while altering internal reasoning; current metrics focus solely on output labels and fairness statistics.
- What evidence would resolve it: Correlation analysis between the proposed statistical agreement metrics and explanation similarity scores (e.g., SHAP values) across compressed and baseline models.

## Limitations
- The framework depends heavily on baseline model quality, potentially preserving rather than detecting inherent biases
- Small subgroup sample sizes reduce statistical power for detecting fairness shifts
- Focuses on prediction distribution shifts rather than reasoning process changes

## Confidence

- **High Confidence**: Model agreement metrics reliably detect instance-level behavioral shifts that accuracy metrics miss; this follows directly from established classification evaluation methodology.
- **Medium Confidence**: Chi-squared tests successfully identify statistically significant compression-induced changes; while the statistical methodology is sound, the assumption that baseline predictions represent "faithful" behavior is problematic when baselines have inherent biases.
- **Low Confidence**: Subgroup-specific fairness preservation claims; small subgroup sample sizes create unstable statistical estimates, and the paper does not validate whether detected shifts actually harm downstream fairness outcomes.

## Next Checks
1. **Baseline Bias Validation**: Test whether the faithfulness framework correctly identifies compression-induced changes when applied to intentionally biased baseline models—does it detect preservation of bias or just accuracy maintenance?
2. **Sample Size Sensitivity**: Systematically evaluate chi-squared test power across different subgroup sizes using synthetic data to determine minimum detectable effect sizes and false negative rates.
3. **Cross-Dataset Generalization**: Apply the framework to non-social datasets (e.g., medical imaging, natural language) to verify that statistical significance in prediction distribution shifts correlates with meaningful behavioral changes beyond social bias contexts.