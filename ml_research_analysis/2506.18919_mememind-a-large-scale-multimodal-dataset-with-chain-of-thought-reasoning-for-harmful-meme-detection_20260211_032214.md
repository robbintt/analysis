---
ver: rpa2
title: 'MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning
  for Harmful Meme Detection'
arxiv_id: '2506.18919'
source_url: https://arxiv.org/abs/2506.18919
tags:
- harmful
- meme
- dataset
- reasoning
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemeMind, a large-scale dataset for harmful
  meme detection featuring over 40,000 annotated samples across five harmful categories.
  Each meme includes Chain-of-Thought reasoning annotations that simulate human cognitive
  processes for harmfulness assessment.
---

# MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection

## Quick Facts
- **arXiv ID**: 2506.18919
- **Source URL**: https://arxiv.org/abs/2506.18919
- **Reference count**: 15
- **Primary result**: Introduces MemeMind dataset (43,223 memes) with CoT reasoning annotations; proposes MemeGuard model achieving 86.25% accuracy and 84.26% F1-score on harmful meme detection

## Executive Summary
This paper introduces MemeMind, a large-scale dataset for harmful meme detection featuring over 40,000 annotated samples across five harmful categories. Each meme includes Chain-of-Thought reasoning annotations that simulate human cognitive processes for harmfulness assessment. Building on this dataset, the authors propose MemeGuard, a reasoning-enhanced multimodal detection model that leverages visual enhancement, reasoning alignment, and reinforcement learning to improve both detection accuracy and interpretability. Experimental results show MemeGuard achieves 86.25% accuracy and 84.26% F1-score, significantly outperforming state-of-the-art baselines on the MemeMind benchmark.

## Method Summary
The authors present a three-stage training pipeline using Qwen2.5-VL-7B-Instruct as backbone. Stage 1 performs full fine-tuning on caption data to establish visual-textual alignment. Stage 2 applies LoRA fine-tuning on Chain-of-Thought annotations to learn reasoning patterns. Stage 3 uses GRPO reinforcement learning with a composite gated reward to optimize reasoning quality and detection accuracy. The model generates binary harmful/non-harmful judgments along with multi-label subcategory predictions and interpretable reasoning chains explaining its decisions.

## Key Results
- MemeGuard achieves 86.25% accuracy and 84.26% F1-score on the MemeMind benchmark
- Model outperforms state-of-the-art baselines including Qwen2.5-VL-7B-Instruct and ViT-LLM
- Reasoning enhancement stage provides significant performance gains (83.48% → 86.25% accuracy)
- Model demonstrates strong interpretability through generated Chain-of-Thought explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) supervision bridges the gap between explicit multimodal features and implicit harmful intent.
- Mechanism: By forcing the model to generate intermediate reasoning steps (captioning, sub-category analysis) before a final judgment, the system learns to deconstruct metaphors and cultural references rather than relying on surface-level correlations. This mimics the "human cognitive process" defined in the dataset annotations.
- Core assumption: The provided CoT annotations accurately decompose the causal logic of harmfulness, and the model has sufficient capacity to learn this decomposition rather than hallucinating rationalizations.
- Evidence anchors:
  - [abstract] "MemeMind provides detailed Chain-of-Thought (CoT) reasoning annotations to support fine-grained analysis of implicit intentions."
  - [section 4.2] "CoT annotations provide an auxiliary supervision signal to regularize the reasoning process, thereby ensuring logical consistency and rigor."
  - [corpus] "Read as You See" supports the efficacy of guiding LLMs with structured reasoning for low-resource explainable detection, while "Demystifying Hateful Content" highlights the difficulty of implicit hate messages that CoT aims to solve.
- Break condition: If the reasoning chains in the training data contain logical leaps or hallucinations, the model may learn to generate persuasive but unfaithful explanations (faithfulness gap).

### Mechanism 2
- Claim: Decoupled "Visual Enhancement" establishes a robust semantic foundation before high-level reasoning is attempted.
- Mechanism: The architecture explicitly separates visual grounding (Stage 1) from reasoning (Stage 2). By first fine-tuning the visual encoder and LLM decoder using caption data (minimizing cross-entropy loss), the model creates precise visual-textual correspondences. This prevents the reasoning module from operating on noisy or misaligned visual features.
- Core assumption: Caption generation is a valid proxy task for learning the visual semantics necessary for harmfulness detection.
- Evidence anchors:
  - [abstract] "MemeGuard... leverages visual enhancement... to improve both detection accuracy and interpretability."
  - [section 4.1] "This alignment process facilitates the establishment of precise semantic correspondences, enabling a fine-grained interpretation of visual cues."
  - [table 4] Shows a performance drop (86.25% -> 84.81% Acc) when Stage 1 is removed, evidencing the value of dedicated visual alignment.
  - [corpus] Corpus evidence is weak regarding this specific decoupled stage; most related works (e.g., "AdamMeme") focus on end-to-end probing rather than staged alignment.
- Break condition: If the visual encoder overfits to the caption distribution but fails to capture the nuanced visual irony or specific offensive imagery required for the downstream task, the reasoning stage will receive insufficient signals.

### Mechanism 3
- Claim: Gated Reinforcement Learning (GRPO) with composite rewards shifts the model from mimicking patterns to optimizing decision boundaries.
- Mechanism: The "Reasoning Enhancement" stage (Stage 3) uses a specific reward function ($r_{total}$) that includes semantic similarity, sub-category accuracy, and final judgment. Crucially, the reward is "gated" (set to 0) if the final judgment is wrong. This forces the model to explore reasoning paths that are logically consistent *and* result in the correct label, rather than just maximizing token probability.
- Core assumption: The defined reward components accurately capture the constituents of a "good" explanation and detection, and the gated reward signal is dense enough to avoid convergence issues.
- Evidence anchors:
  - [section 4.3] "Reinforcement Learning facilitates the autonomous exploration of superior logic paths... This constraint forces the model to align its reasoning logic with accurate classification outcomes."
  - [table 4] Shows a performance drop (86.25% -> 83.48% Acc) without Stage 3, suggesting that supervised fine-tuning alone is suboptimal.
  - [corpus] "Learning from Mistakes" conceptually aligns with optimizing based on misjudgment patterns, similar to how RL optimizes against a reward signal.
- Break condition: If the reward for semantic similarity ($r_{sem}$) dominates the reward for judgment accuracy ($r_{fin}$), the model might generate fluent but incorrect reasoning. The gate is intended to prevent this, but reward hacking remains a risk.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Harmful memes rely on implicit intent (irony, metaphor). Direct classification (Image/Text -> Label) fails because the semantic gap is too wide. CoT provides the "intermediate steps" to bridge this gap.
  - Quick check question: Can you distinguish between a model outputting a label ("Harmful") vs. a model outputting a reasoning chain ("The image uses a historical event to mock a specific group, therefore it is harmful")?

- Concept: **Multimodal Alignment (Vision-Language Models)**
  - Why needed here: Memes are a fusion of text and image. The model must understand that text "Nice weather" over an image of a nuclear explosion is sarcastic/harmful, not a literal weather report.
  - Quick check question: How does the model handle text embedded *inside* the image (OCR) vs. text provided as a separate caption?

- Concept: **Reinforcement Learning from Human Feedback (RLHF) / GRPO**
  - Why needed here: Supervised learning (teacher forcing) only teaches the model to predict the next token based on the training data. It doesn't explicitly optimize for the *goal* (correctness of the logic). RL is used here to align the reasoning process with the desired outcome.
  - Quick check question: Why is a "gated" reward (reward = 0 if the final answer is wrong) stricter than a standard cross-entropy loss?

## Architecture Onboarding

- Component map: Qwen2.5-VL-7B-Instruct (Vision Encoder + LLM) -> Visual Enhancement (Stage 1) -> Reasoning Alignment (Stage 2) -> Reasoning Enhancement (Stage 3)

- Critical path:
  1. **Data Prep:** Format MemeMind data into (Image, Caption) for Stage 1 and (Image, CoT) for Stage 2/3.
  2. **Stage 1 Execution:** Train the base model to describe memes accurately. *Checkpoint required.*
  3. **Stage 2 Execution:** Load Stage 1 checkpoint. Train with LoRA to generate the full CoT (Question -> Reasoning -> Judgment). *Checkpoint required.*
  4. **Stage 3 Execution:** Load Stage 2 checkpoint. Initialize RL environment with the composite reward function ($r_{sem}, r_{sub}, r_{fin}$). Train using GRPO.

- Design tradeoffs:
  - **Full Fine-tuning (Stage 1) vs. LoRA (Stage 2):** The authors use full fine-tuning for visual grounding to maximize adaptation to meme visual styles, but switch to LoRA for reasoning to preserve the LLM's general reasoning capabilities while efficiently learning the specific task logic.
  - **Gated Reward:** The design choice $r_{total}=0$ if $r_{fin}=0$ is aggressive. It prioritizes accuracy over partial credit for "almost correct" reasoning, which may speed up convergence to accurate detectors but could reduce the diversity of generated reasoning chains.

- Failure signatures:
  - **Visual Hallucination:** In Stage 1, if the model describes generic image content but misses the embedded text or specific visual meme format, Stage 2 reasoning will fail.
  - **Reasoning Collapse:** In Stage 2, the model might learn to output the correct label but generate generic/gibberish reasoning if the CoT supervision is not weighted correctly.
  - **Reward Hacking:** In Stage 3, the model might artificially inflate the length of reasoning to maximize $r_{sem}$ (BLEU/ROUGE metrics often favor length) while ignoring the semantic validity, or it might learn to output the correct sub-category labels without justifying them in text.

- First 3 experiments:
  1. **Visual Grounding Validation:** Run the Stage 1 model on a held-out set of 50 memes. Manually check if the generated captions capture the *irony* or *metaphor*, not just the objects.
  2. **Ablation on CoT:** Train a variant of MemeGuard using only binary labels (Table 5 configuration) on a small subset. Compare accuracy against the CoT-trained model to quantify the value added by the reasoning annotations.
  3. **Reward Sensitivity:** In Stage 3, adjust the weights ($\alpha, \beta, \gamma$) of the composite reward. Specifically, try removing the "gate" (allowing partial reward even for wrong final judgment) to observe if the model converges faster but to a lower accuracy plateau.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the generalization capability of reasoning-enhanced models to out-of-distribution (OOD) memes be rigorously optimized and validated?
- Basis in paper: [explicit] The "Limitations" section states that "the interpretability of the model’s reasoning process and its generalization capability to out-of-distribution (OOD) memes require more rigorous optimization and validation."
- Why unresolved: The current study focuses on performance within the MemeMind benchmark, leaving the model's stability and reasoning consistency on unseen, structurally different, or evolving meme formats unverified.
- What evidence would resolve it: Benchmarking results showing MemeGuard's performance degradation (or stability) when tested on external datasets like FHM or HarMeme without fine-tuning, or against adversarially generated meme variations.

### Open Question 2
- Question: To what extent can the detection framework be secured against adversarial attacks where actors reverse-engineer the reasoning logic to evade safety filters?
- Basis in paper: [explicit] The "Ethics Statement" identifies the risk that "adversarial actors could exploit our detection framework to reverse-engineer sequences that evade safety filters" as a limitation of open-sourcing the method.
- Why unresolved: While the benefits of open-sourcing are deemed to outweigh the risks, the paper does not provide an analysis of the model's robustness against such specific reverse-engineering attacks or "dual use" scenarios.
- What evidence would resolve it: A robustness analysis measuring the attack success rate of adversarially modified memes designed specifically to trick the Chain-of-Thought reasoning into a "Nonharmful" judgment.

### Open Question 3
- Question: How does detection performance scale with the inclusion of a broader spectrum of cultural contexts and implicit harmful categories beyond the current taxonomy?
- Basis in paper: [explicit] The "Limitations" section notes that "Further expansion is required to encompass a broader spectrum of harmful categories and cultural contexts to enhance the dataset’s depth and breadth."
- Why unresolved: The current dataset, while large, may not fully capture the "vast and rapidly evolving landscape of online content" or nuanced cultural references that differ significantly from the English and Chinese samples currently annotated.
- What evidence would resolve it: Performance metrics on a test set specifically curated from underrepresented cultures or novel harmful categories (e.g., emerging forms of cyberbullying) to see if the CoT reasoning generalizes or fails.

### Open Question 4
- Question: Does the reliance on GPT-4o for generating initial Chain-of-Thought annotations introduce systematic reasoning biases or hallucinations that persist after human review?
- Basis in paper: [inferred] The methodology uses GPT-4o as the primary annotator to "emulate this cognitive process" before manual correction. This relies on the assumption that the model's initial reasoning trajectory is sound and aligns with human cognitive standards.
- Why unresolved: The paper checks for consistency (Fleiss’ Kappa) and does manual correction, but does not analyze if the *structure* of the model-generated reasoning limits the scope of human corrections (e.g., confirmation bias towards the model's initial interpretation).
- What evidence would resolve it: A comparative analysis of annotation quality and reasoning accuracy between samples generated via GPT-4o pre-annotation versus a fully human-annotated control group.

## Limitations

- **Faithfulness concerns**: The Chain-of-Thought annotations may introduce post-hoc rationalizations rather than faithful reasoning, with no validation that generated chains reflect actual decision processes
- **Dataset availability**: MemeMind dataset is not publicly available, requiring reconstruction from component datasets or direct author contact for reproduction
- **Generalization uncertainty**: Model performance on out-of-distribution memes and novel cultural contexts remains unverified beyond the benchmark test set

## Confidence

- **High**: The staged training architecture (Visual Enhancement → Reasoning Alignment → Reasoning Enhancement) is technically sound and the reported accuracy/F1 improvements over baselines are substantial and reproducible given the dataset
- **Medium**: The claim that Chain-of-Thought supervision significantly improves detection performance rests on the assumption that the provided reasoning chains are both accurate and complete representations of harmfulness assessment logic
- **Low**: The assertion that the gated reinforcement learning objective prevents reward hacking while maintaining reasoning quality requires more empirical validation, as the gating mechanism could potentially create optimization plateaus or encourage conservative reasoning

## Next Checks

1. **Faithfulness Validation**: Run a human evaluation study where annotators assess whether the generated CoT reasoning chains actually justify the final judgment decisions, measuring the faithfulness gap between stated reasoning and actual decision process

2. **Reward Function Sensitivity**: Systematically vary the weights (α, β, γ) and gating threshold in the composite reward function to determine the sensitivity of final detection performance to these hyperparameters and identify potential reward hacking behaviors

3. **Dataset Independence Test**: Evaluate MemeGuard on memes from social media platforms not represented in the training data (outside the five source datasets) to assess generalization to real-world meme distributions and cultural contexts