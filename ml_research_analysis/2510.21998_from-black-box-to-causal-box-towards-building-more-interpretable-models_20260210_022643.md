---
ver: rpa2
title: 'From Black-box to Causal-box: Towards Building More Interpretable Models'
arxiv_id: '2510.21998'
source_url: https://arxiv.org/abs/2510.21998
tags:
- counterfactual
- causal
- features
- interpretable
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the notion of causal interpretability, which\
  \ addresses whether a prediction model can consistently answer counterfactual queries\
  \ from observational data. The authors analyze two common model classes\u2014blackbox\
  \ and concept-based models\u2014and show that neither is causally interpretable\
  \ in general."
---

# From Black-box to Causal-box: Towards Building More Interpretable Models

## Quick Facts
- arXiv ID: 2510.21998
- Source URL: https://arxiv.org/abs/2510.21998
- Authors: Inwoo Hwang; Yushu Pan; Elias Bareinboim
- Reference count: 40
- Key outcome: This paper introduces causal interpretability as the property that models in a class yield consistent counterfactual predictions, shows neither blackbox nor concept-based models are causally interpretable in general, and provides a graphical criterion for determining whether a model architecture supports specific counterfactual queries.

## Executive Summary
This paper addresses the fundamental question of whether prediction models can consistently answer counterfactual queries from observational data. The authors introduce the notion of causal interpretability, which requires that all models in a class yield consistent counterfactual predictions for a given query. They analyze common model classes - blackbox models that predict from raw data and concept-based models that predict from extracted features - and prove that neither is causally interpretable in general. To address this gap, they propose a framework for building models that are causally interpretable by design, characterized by a complete graphical criterion determining whether a given model architecture supports a given counterfactual query. The key contribution is identifying the unique maximal set of features that yields an interpretable model with maximal predictive expressiveness, establishing a fundamental tradeoff between causal interpretability and predictive accuracy.

## Method Summary
The paper proposes Generalized Concept-based Prediction (GCP) models that use a subset T of features V to predict labels. A feature extractor estimates P(T|X) from raw data X, and a classifier predicts the label from these features. Models are trained end-to-end using binary cross-entropy loss. For synthetic BarMNIST data, the feature extractor is a 3-layer MLP (hidden=32, leakyrelu) and the classifier is another 3-layer MLP. For CelebA, the feature extractor is a ResNet34 and the classifier is linear. Counterfactual queries are computed using a do-calculus formula: P(Ŷ_w'|x) = Σ_t P(Ŷ|w'∩T, t\W)P(t|x). The causal interpretability of a model using features T for query Q(W) is determined by a graphical criterion: T is interpretable for Q(W) iff T ⊆ W ∪ ND(W), where ND(W) are the non-descendants of W in the causal graph.

## Key Results
- Neither blackbox models nor concept-based models are causally interpretable in general
- A complete graphical criterion determines whether a model architecture supports a given counterfactual query
- There exists a unique maximal set of features that yields an interpretable model with maximal predictive expressiveness
- Experiments show that causally interpretable models can consistently answer counterfactual questions, while non-interpretable models yield inconsistent or incorrect counterfactual predictions

## Why This Works (Mechanism)
The paper works by recognizing that causal interpretability requires consistency across all models in a class when answering counterfactual queries. For a model to be causally interpretable for a query Q(W), the features it uses must not include any descendants of W (except W itself), because such descendants would create spurious correlations that vary across different models in the class. The graphical criterion T ⊆ W ∪ ND(W) precisely captures this requirement. By restricting to T-admissible feature sets (those satisfying this criterion), the paper ensures that all models in the class will yield consistent counterfactual predictions, as they avoid using features that could introduce model-dependent bias.

## Foundational Learning
- **Causal Interpretability**: A model class is causally interpretable for query Q(W) if all models in the class yield consistent counterfactual predictions for Q(W). Needed because standard interpretability notions don't guarantee counterfactual consistency. Quick check: For any two models in the class, do they give the same counterfactual predictions?
- **T-admissibility**: A feature set T is T-admissible for query Q(W) if T ⊆ W ∪ ND(W). Needed to ensure counterfactual predictions are consistent across models. Quick check: Verify that no features in T are descendants of W (except W itself).
- **Counterfactual Query Estimation**: P(Ŷ_w'|x) = Σ_t P(Ŷ|w'∩T, t\W)P(t|x) computes the counterfactual distribution given observational data. Needed to evaluate whether models can answer counterfactual queries correctly. Quick check: Compare estimates to ground truth in synthetic settings with known SCM.
- **Maximal T-admissible Set**: The unique largest feature set that is T-admissible for a query, providing the best accuracy-interpretability tradeoff. Needed to maximize predictive power while maintaining causal interpretability. Quick check: Verify that adding any other feature violates T-admissibility.

## Architecture Onboarding

**Component Map**: Raw data X -> Feature Extractor (P(T|X)) -> Classifier (Ŷ) -> Loss (BCE)

**Critical Path**: The critical path is X → T → Ŷ, where T must satisfy the T-admissibility criterion for the query of interest. The feature extractor quality directly impacts both accuracy and counterfactual estimation.

**Design Tradeoffs**: 
- **Accuracy vs Interpretability**: Using fewer features (more interpretable) typically reduces predictive accuracy
- **Feature Selection**: Choosing T-admissible features limits the model's expressiveness but ensures causal interpretability
- **Model Complexity**: Simpler classifiers may suffice for interpretable features, but complex features may require deeper architectures

**Failure Signatures**: 
- Inconsistent counterfactual predictions across different models trained on same data
- Counterfactual estimates that diverge significantly from ground truth in synthetic settings
- Models using descendant features (e.g., T={B,D} for query on D) showing systematic bias

**First Experiments**:
1. Generate BarMNIST dataset and verify known causal structure (D→C, D→B)
2. Train GCP models with different feature sets (T={B,D,C}, {B,D}, {D,C}, {D}) and evaluate accuracy
3. Compute counterfactual query P(Ŷ_D=1|B=1,D=0,C=1) for each model and compare to ground truth

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on known causal graphs in synthetic settings may not reflect real-world uncertainty
- Limited scalability demonstration beyond MNIST-scale datasets
- No ablation on feature extractor architectures or training procedures

## Confidence
- Theoretical results: High
- Experimental validation: Medium
- Practical applicability: Medium

## Next Checks
1. Apply the framework to a real-world dataset with partially known causal structure (e.g., medical data with domain expertise guidance)
2. Test sensitivity to feature extractor accuracy by adding controlled noise to feature predictions
3. Evaluate scalability by training on larger image datasets (CIFAR-10/100) with different backbone architectures