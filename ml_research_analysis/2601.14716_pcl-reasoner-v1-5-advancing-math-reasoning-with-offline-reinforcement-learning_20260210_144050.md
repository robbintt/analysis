---
ver: rpa2
title: 'PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning'
arxiv_id: '2601.14716'
source_url: https://arxiv.org/abs/2601.14716
tags:
- training
- reasoning
- offline
- arxiv
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PCL-Reasoner-V1.5, a 32-billion-parameter mathematical
  reasoning model developed using a novel offline reinforcement learning approach.
  The method addresses training instability and computational inefficiency issues
  in online RL methods like GRPO by decoupling inference from training, resulting
  in a more stable and efficient training paradigm.
---

# PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.14716
- **Source URL:** https://arxiv.org/abs/2601.14716
- **Reference count:** 2
- **Primary result:** 32B-parameter model achieving 90.9% AIME 2024 and 85.6% AIME 2025 accuracy using offline RL

## Executive Summary
PCL-Reasoner-V1.5 is a 32-billion-parameter mathematical reasoning model that advances reasoning capabilities through a novel offline reinforcement learning approach. The method addresses training instability and computational inefficiency issues in online RL by decoupling inference from training, resulting in a more stable and efficient training paradigm. PCL-Reasoner-V1.5 achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, with 90.9% accuracy on AIME 2024 and 85.6% on AIME 2025. The primary improvement stems from enhanced long-chain-of-thought reasoning capabilities, with average response lengths increasing from 22K-26K tokens to 30K-40K tokens after offline RL fine-tuning.

## Method Summary
PCL-Reasoner-V1.5 uses a two-stage approach: first, supervised fine-tuning (SFT) on filtered math data to create PCL-Reasoner-V1, then offline reinforcement learning on a curated dataset of difficult problems. The offline RL stage generates 8 reasoning trajectories per question using the SFT model, verifies answers with Qwen3-32B, and trains using a geometric mean probability policy loss on filtered high-difficulty samples (30,215 total). The method achieves stability by operating on a fixed dataset rather than dynamic online sampling, using FP16 precision to avoid numerical issues common in online RL methods like GRPO.

## Key Results
- Achieves 90.9% accuracy on AIME 2024 and 85.6% on AIME 2025
- Response lengths increase from 22K-26K tokens to 30K-40K tokens after offline RL
- Demonstrates state-of-the-art performance among models post-trained on Qwen2.5-32B
- Shows offline RL can effectively elicit longer, more accurate reasoning chains

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Inference from Training
Offline RL improves stability by eliminating dynamic feedback loops that cause reward collapse. The approach separates reasoning trajectory generation from parameter updates, operating on a fixed dataset rather than on-the-fly sampling. This avoids distribution shift and numerical precision issues inherent in online methods. Break condition: If the static dataset distribution differs too severely from the model's policy, the loss gradient may become ineffective.

### Mechanism 2: Geometric Mean Probability Weighting
Fine-grained policy optimization uses geometric mean probability weighting to encourage longer, deliberative reasoning chains. The policy value is defined as the geometric mean of token-level probabilities, with positive gradients for correct answers and negative for incorrect ones. This explicitly up-weights probability of reasoning paths yielding correct answers, which are typically longer. Break condition: If reward signals are too sparse or the verifier is unreliable, the model may optimize for length rather than logical validity.

### Mechanism 3: Data Filtering for High Difficulty
Data filtering for high difficulty prevents capacity waste on trivial samples and forces capability elicitation. The training set excludes questions where the initial model answered correctly 8/8 times or where answers were too short (<32K tokens). This curates a dataset of "informative failures" and complex successes, maximizing utility of offline gradient updates. Break condition: If filtering is too aggressive, the dataset becomes too small, leading to overfitting on specific problem archetypes.

## Foundational Learning

### Concept: Offline vs. Online Reinforcement Learning
- **Why needed here:** The paper's central thesis replaces standard online RL loops with offline paradigms. Understanding dynamic vs. static data usage is crucial for debugging training stability.
- **Quick check question:** Can you explain why offline RL prevents the "reward collapse" phenomenon often seen in online RL loops?

### Concept: Geometric Mean of Probabilities
- **Why needed here:** The paper uses geometric mean of token probabilities to represent policy value, rather than standard joint probability. This affects how the loss landscape shapes long sequence generation.
- **Quick check question:** How does using the geometric mean differ from using the arithmetic mean or standard log-prob when optimizing for long sequences?

### Concept: Data Contamination Testing
- **Why needed here:** High benchmark scores are often suspect due to potential test-set leakage. The paper claims to run contamination tests, a critical step for validating performance.
- **Quick check question:** What specific checks would you run to ensure your SFT or RL data does not overlap with the AIME 2024/2025 evaluation sets?

## Architecture Onboarding

### Component map:
Qwen2.5-32B -> SFT (PCL-Reasoner-V1) -> vLLM Inference (8 candidates) -> Qwen3-32B Verifier -> MindSpeed-LLM Trainer

### Critical path:
The transition from PCL-Reasoner-V1 to V1.5 relies entirely on the Filtering Heuristic (Section 3.3). If you do not filter for high-difficulty (short attempts failed or mixed results) and long reasoning chains, the offline RL stage will likely yield negligible gains or degrade performance.

### Design tradeoffs:
- **Stability vs. Exploration:** You gain training stability and engineering simplicity but lose the "self-correction" or emergent discovery loop of online RL
- **FP16 vs. BF16:** The paper mandates FP16 for numerical precision, which may affect hardware compatibility or throughput compared to BF16

### Failure signatures:
1. **Length Collapse:** Response lengths do not increase during training → Check learning rate or verify positive samples contain long CoT sequences
2. **OOD Divergence:** Loss spikes or generates gibberish → Distribution mismatch between static dataset and current policy; consider lowering learning rate

### First 3 experiments:
1. **Baseline Validation:** Reproduce SFT model (PCL-Reasoner-V1) and verify baseline AIME scores (85.7/84.2) to ensure data quality
2. **Ablation on Data Filtering:** Train V1.5 with and without length/difficulty filters on a small subset to observe delta in loss convergence and response length
3. **Verifier Sanity Check:** Manually inspect 50 random samples where verifier assigned R=-1 to confirm base model's answers were incorrect

## Open Questions the Paper Calls Out
1. **Hybrid RL Approaches:** Can hybrid iterative approaches combining offline and online RL paradigms achieve performance beyond the "bounded" ceiling of pure offline RL while maintaining stability advantages?
2. **Generalization to Weaker Models:** Does the offline RL approach generalize to weaker base models that lack the strong performance baseline achieved by PCL-Reasoner-V1?
3. **Computational Budget Comparison:** How does offline RL performance compare to online methods like GRPO under equivalent computational budgets and training durations?

## Limitations
- The approach relies heavily on quality and difficulty filtering of the static dataset, making performance gains potentially attributable to data selection rather than the RL mechanism itself
- The use of binary reward signals from a verifier model introduces potential brittleness and propagation of verification errors
- The 32B parameter scale and specialized hardware requirements (Ascend 910C, MindSpeed-LLM) limit reproducibility across research groups
- While contamination tests are claimed, the exact methodology is not specified, leaving open the possibility of benchmark leakage

## Confidence

- **High confidence:** The offline RL framework is technically sound and training stability improvements over online RL are well-supported by literature. Architectural components and training procedures are clearly specified.
- **Medium confidence:** The reported AIME 2024/2025 scores are impressive, but lack of detailed contamination testing methodology introduces uncertainty about their validity.
- **Low confidence:** The claim that offline RL "elicits latent capabilities" is difficult to verify without ablations showing what the base model could achieve on filtered difficult samples before RL.

## Next Checks
1. **Contamination Test Replication:** Re-run the exact overlap detection procedure on the PCL-Reasoner-V1.5 training set against released AIME 2024/2025 problem sets to verify no leakage exists
2. **Length Ablation Study:** Train an offline RL variant without the length/difficulty filtering to quantify how much of the performance gain is due to data curation versus the RL algorithm itself
3. **Verifier Reliability Audit:** Manually validate the verifier's binary judgments on a random sample of 100 training instances to measure its accuracy and identify potential reward noise