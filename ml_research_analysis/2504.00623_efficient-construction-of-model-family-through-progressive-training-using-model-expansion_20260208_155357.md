---
ver: rpa2
title: Efficient Construction of Model Family through Progressive Training Using Model
  Expansion
arxiv_id: '2504.00623'
source_url: https://arxiv.org/abs/2504.00623
tags:
- training
- progressive
- family
- learning
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for efficiently constructing a family
  of language models with different parameter sizes through progressive training.
  Instead of training each model independently from scratch, the approach incrementally
  expands smaller models to initialize larger ones, reducing the total computational
  cost.
---

# Efficient Construction of Model Family through Progressive Training Using Model Expansion

## Quick Facts
- arXiv ID: 2504.00623
- Source URL: https://arxiv.org/abs/2504.00623
- Reference count: 30
- Primary result: Progressive training reduces total computational cost by approximately 25% while maintaining comparable or better performance than independently trained models

## Executive Summary
This paper introduces a method for efficiently constructing a family of language models with different parameter sizes through progressive training. Instead of training each model independently from scratch, the approach incrementally expands smaller models to initialize larger ones, reducing the total computational cost. Experiments with a model family ranging from 1B to 8B parameters show that this method reduces computational costs by approximately 25% while maintaining comparable or better performance than independently trained models. By strategically adjusting maximum learning rates based on model size, the method further improves performance across various benchmark tasks. Additionally, models built through progressive training exhibit greater consistency in their behavior across the family, as evidenced by lower Kullback-Leibler divergence between their probability distributions.

## Method Summary
The method constructs a model family by training smaller models first and progressively expanding them to initialize larger models. Using the bert2BERT (AKI variant) expansion function, a 1B parameter model is trained for 40B tokens with a high learning rate, then expanded to 2B parameters and trained for 60B more tokens, continuing this pattern through 4B and 8B models. The total FLOPs budget is constrained to equal what would be required to train only the largest model from scratch. A key innovation is adjusting the maximum learning rate for each stage, starting high (1.5×10⁻³) for the smallest model and decreasing linearly to 3.0×10⁻⁴ for the largest model, which improves stability and performance. The approach is evaluated on FineWeb-Edu dataset with GPT-2 tokenizer and max sequence length 1024, measuring perplexity, zero-shot accuracy on multiple benchmarks, KL divergence between model pairs, and total FLOPs reduction.

## Key Results
- Progressive training reduces total computational costs by approximately 25% compared to independently trained models
- Models built through progressive training achieve comparable or better performance across benchmark tasks
- The method produces model families with significantly lower Kullback-Leibler divergence between probability distributions of adjacent model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferring learned parameters from a smaller model to initialize a larger one reduces total computational cost by providing a superior starting point for optimization.
- Mechanism: Instead of random initialization for each model, the method uses an expansion function (e.g., bert2BERT) to map a smaller model's weights to a larger parameter space. This "warm starts" the larger model, allowing it to reach comparable performance with fewer training tokens than a random start.
- Core assumption: The loss landscape of a smaller model is sufficiently correlated with that of a larger model such that its weights provide a more optimal initialization than random chance.
- Evidence anchors: The corpus signals include related work on progressive and efficient training (e.g., "Deep Progressive Training," "SPARKLING").

### Mechanism 2
- Claim: A dynamic learning rate schedule, decreasing as model size increases, improves convergence and final performance.
- Mechanism: Smaller models can tolerate and benefit from higher learning rates for faster convergence, while larger models require lower rates to avoid instability. Progressive training allows for this staged approach: train the small model with a high rate, then systematically lower the rate for each subsequent, larger model initialized from it.
- Core assumption: Larger models are inherently more sensitive to high learning rates, making a single fixed rate suboptimal for a family of increasing sizes.
- Evidence anchors: Weak direct evidence from other papers for this specific dynamic learning rate mechanism in this context.

### Mechanism 3
- Claim: Progressive training produces a model family with more consistent internal behavior compared to independently trained models.
- Mechanism: Because each model is initialized from its smaller predecessor, their learned representations and output probability distributions are inherently more aligned. This ancestral constraint reduces the variability that would normally arise from independent, random initializations.
- Core assumption: The initialization from a smaller model biases the larger model towards a similar solution, reducing behavioral drift.
- Evidence anchors: No direct evidence in the provided corpus for this specific consistency mechanism.

## Foundational Learning

- Concept: **FLOPs and Compute-Optimal Training (Chinchilla Law)**
  - Why needed here: The paper's central claim of a "25% cost reduction" is calculated in FLOPs. The Chinchilla law provides the baseline for the "optimal" number of training tokens for a given model size.
  - Quick check question: If a 4B parameter model is trained on 120B tokens, what is the FLOPs cost?

- Concept: **Model Expansion (bert2BERT)**
  - Why needed here: This is the core operation. You must understand how a model's weights are transformed to a larger size. Key sub-concepts are *width expansion* (duplicating weights in a linear layer) and *depth expansion* (duplicating and stacking layers).
  - Quick check question: To expand a model's hidden dimension from $d$ to $2d$ using a simple duplication strategy, how are the original weights transformed?

- Concept: **KL Divergence**
  - Why needed here: This metric is used to quantify the "behavioral consistency" of the model family. It measures the difference between two probability distributions. A lower score means the models are more likely to make similar predictions.
  - Quick check question: If Model A assigns 90% probability to a token and Model B assigns 10%, would the KL divergence between their distributions be high or low?

## Architecture Onboarding

- Component map: 1B model -> bert2BERT expansion -> 2B model -> bert2BERT expansion -> 4B model -> bert2BERT expansion -> 8B model
- Critical path: The sequential dependency is absolute. The performance of the final 8B model is contingent on the quality of the 4B model, which depends on the 2B model, and so on.
- Design tradeoffs: The primary tradeoff is achieving the ~25% compute reduction while maintaining "comparable or better" performance, which is not guaranteed.
- Failure signatures: Training instability/loss spikes in larger models if the learning rate is not sufficiently reduced; Cascading errors from a poorly trained smaller model; High KL divergence indicating failed behavioral consistency.
- First 3 experiments:
  1. Baseline FLOPs Comparison: Train a small family (1B, 2B) both independently and progressively. Calculate total FLOPs and compare final validation losses.
  2. Learning Rate Ablation: Run progressive training with (a) a fixed high learning rate and (b) the proposed decreasing schedule. Compare training loss curves for stability.
  3. Consistency Verification: Measure the KL divergence between the output distributions of adjacent model sizes for both progressively trained and independently trained families.

## Open Questions the Paper Calls Out
- Question: Does the progressive training methodology scale effectively to model families larger than 8B parameters (e.g., 70B or 405B)?
- Question: How does the specific model expansion function $f$ (e.g., bert2BERT vs. depth stacking) impact the trade-off between computational savings and model performance?
- Question: How does the expansion ratio (e.g., doubling parameters vs. quadrupling) affect the efficiency and consistency of the resulting model family?

## Limitations
- The method's efficiency gains have only been demonstrated for models up to 8B parameters, with uncertain scalability to larger models
- The learning rate scheduling mechanism relies on empirical tuning without theoretical guarantees
- The behavioral consistency claims rest entirely on KL divergence measurements without establishing practical significance

## Confidence

- High Confidence: The 25% FLOPs reduction claim is well-supported by the controlled experimental design comparing progressive training against independent training with identical total compute budgets.
- Medium Confidence: The performance claims of "comparable or better" results across benchmarks are reasonably supported for the 8B model size tested, but the paper does not demonstrate that these relative advantages persist when scaling to much larger models.
- Low Confidence: The paper provides limited theoretical analysis for why progressive training should work. The core assumption that smaller model weights provide superior initialization for larger models is supported by empirical results but lacks rigorous mathematical justification.

## Next Checks

1. Scaling Boundary Analysis: Systematically test progressive training on intermediate model sizes (16B, 32B) to identify the inflection point where efficiency gains diminish or reverse.

2. Architecture-Agnostic Expansion: Implement and compare multiple expansion strategies beyond bert2BERT (e.g., random initialization with knowledge distillation, progressive layer addition) across different model families.

3. Long-Tail Distribution Consistency: Beyond KL divergence on validation sets, measure behavioral consistency on out-of-distribution examples and rare token contexts to validate whether the claimed consistency translates to more robust generalization.