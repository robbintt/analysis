---
ver: rpa2
title: 'AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration
  in Real-World Agriculture'
arxiv_id: '2601.08308'
source_url: https://arxiv.org/abs/2601.08308
tags:
- tool
- execution
- arxiv
- planning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgriAgent introduces a two-level agent framework for real-world
  agriculture that addresses the challenge of diverse task complexity and incomplete
  tool availability. The system routes tasks to System-1 for simple multimodal question
  answering or System-2 for complex tasks requiring contract-driven planning and capability-aware
  tool orchestration.
---

# AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture

## Quick Facts
- arXiv ID: 2601.08308
- Source URL: https://arxiv.org/abs/2601.08308
- Reference count: 13
- Primary result: Two-level agent framework achieving 94.4-100% contract fulfillment rates and 75-90%+ tool selection accuracy on complex agricultural tasks

## Executive Summary
AgriAgent introduces a hierarchical agent framework that addresses the challenge of diverse task complexity in real-world agriculture. The system routes tasks to System-1 for simple multimodal question answering or System-2 for complex tasks requiring contract-driven planning and capability-aware tool orchestration. System-2 uses explicit contracts to formalize task requirements, enabling verifiable multi-step execution with failure recovery. The framework demonstrates significantly improved execution success rates and robustness compared to existing tool-centric agent baselines, particularly as tool inventory scales.

## Method Summary
AgriAgent implements a two-level agent architecture where tasks are routed based on complexity analysis. Simple tasks go through System-1, which uses modality-specialized models (AgriGPT for text, AgriGPT-VL for vision, AgriGPT-Omni for fusion) with unified RAG retrieval and synthesis. Complex tasks trigger System-2, which employs contract-driven planning using Directed Acyclic Graphs (DAGs) with explicit need contracts that separate capability requirements from tool implementations. The framework uses ToolHub with dual-protocol matching (capability matching via TDI and schema compatibility via TOCI) and ToolMaker for dynamic tool generation when gaps are identified.

## Key Results
- Contract fulfillment rates of 94.4-100% across different model scales
- Tool selection accuracy (Hit@1/3/5) consistently above 75-90% even at 506+ tools
- Significantly improved execution success rates compared to tool-centric agent baselines
- ToolMaker success rate of 96.94% for generating missing tools

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Based Dual-Path Routing
Routing tasks by complexity to matched execution pathways improves both efficiency and robustness compared to unified paradigms. A router analyzes user queries for semantic complexity and execution requirements, sending simple tasks to System-1 for direct multimodal reasoning and complex tasks to System-2 for structured planning. This avoids the mismatch introduced by uniform execution paradigms.

### Mechanism 2: Contract-Driven Planning with Debate-Based Refinement
Explicit contracts encoding task requirements, constraints, and dependencies enable verifiable multi-step execution with failure recovery. Tasks are represented as DAGs where each node has explicit goals, inputs/outputs, constraints, and evidence requirements. Multi-agent debate (critique-defend-revise) surfaces structural errors before execution.

### Mechanism 3: Capability-Aware Tool Orchestration via Dual-Protocol Matching
Decoupling capability matching from schema-level composition enables scalable tool selection across large, evolving tool inventories. Tool Description Index (TDI) retrieves candidates by matching need contract capabilities against tool metadata embeddings, while Tool Output Composition Index (TOCI) verifies schema compatibility for chaining. ToolMaker dynamically generates new tools when contracts cannot be satisfied.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) for task representation**: System-2 represents plans as DAGs where nodes are execution units and edges are dependencies. Understanding DAG properties is essential for debugging plan execution.
  - Quick check: Given nodes A→B, A→C, B→D, C→D, which nodes can execute in parallel?

- **Design by Contract (DbC)**: Need contracts inherit from Meyer's DbC paradigm—preconditions, postconditions, and invariants. The paper explicitly references this lineage for verifiable execution.
  - Quick check: If a tool's precondition is "image must be RGB" but receives a grayscale image, who is responsible for the contract violation?

- **Retrieval-Augmented Generation (RAG) with multi-path retrieval**: System-1 uses dense (semantic), sparse (keyword), and graph-based (relational) retrieval paths. Understanding when each path helps is critical for multimodal QA performance.
  - Quick check: For a query about "pest symptoms on rice leaves in Zhejiang province," which retrieval path best captures the geographic constraint?

## Architecture Onboarding

- **Component map**: Router → System-1 (AgriGPT + AgriGPT-VL + AgriGPT-Omni + unified RAG → Synthesis/Aggregation) OR System-2 (Plan Generator → Supervisor Debate → Need Contracts → ToolHub (TDI+TOCI) → ToolMaker → Execution)
- **Critical path**: For complex tasks, the debate-based refinement loop is the highest-latency component, with each plan node triggering ToolHub negotiation.
- **Design tradeoffs**: Explicit contracts vs. implicit planning (higher overhead but verifiable execution); modality-specialized models vs. single omnimodal (better robustness on smaller models); ToolMaker vs. failure (96.94% success rate).
- **Failure signatures**: Router misclassification sending complex tasks to System-1; schema drift causing parsing errors; ToolHub retrieval degradation at scale; debate deadlock preventing plan convergence.
- **First 3 experiments**: 1) Router calibration measuring classification accuracy on held-out tasks; 2) Contract schema robustness testing with smaller models; 3) ToolHub scaling stress test evaluating performance at 100, 500, 1000+ tools.

## Open Questions the Paper Calls Out

- What architectural optimizations are required to adapt the multi-round contract verification process for strict real-time agricultural scenarios?
- How can the contract parsing mechanism be made robust to structural output deviations common in smaller or less capable language models?
- What advanced generation and verification pipelines are necessary to extend ToolMaker to highly complex or specialized domain-specific tools?
- What are the detailed token consumption and latency profiles of the debate-based planning and contract negotiation phases compared to single-pass baselines?

## Limitations

- Computational cost and interaction latency make System-2 less suitable for strict real-time scenarios
- Contract parsing relies on consistent structured output schemas, which may fail with smaller models
- ToolMaker is not explicitly optimized for synthesizing highly complex or domain-specific tools
- No detailed latency or token cost analysis reported for comparison with baselines

## Confidence

- **High Confidence**: Dual-path routing efficiency gains and contract fulfillment rates across model scales
- **Medium Confidence**: Debate-based refinement effectiveness in catching structural errors
- **Low Confidence**: Scalability claims for TDI/TOCI performance at 506+ tools due to limited comparative data

## Next Checks

1. **Router Classification Robustness**: Conduct ablation studies testing router accuracy across different task types, measuring false-positive and false-negative rates.
2. **Contract Schema Adherence Testing**: Systematically evaluate how smaller models adhere to structured output schemas and measure parsing failure rates.
3. **ToolHub Scaling Stress Test**: Replicate tool inventory scaling experiment with 100, 500, 1000+ tools comparing Hit@1/3/5 performance against prompt-based baselines.