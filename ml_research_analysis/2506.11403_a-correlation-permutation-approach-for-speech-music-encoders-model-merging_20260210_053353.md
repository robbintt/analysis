---
ver: rpa2
title: A correlation-permutation approach for speech-music encoders model merging
arxiv_id: '2506.11403'
source_url: https://arxiv.org/abs/2506.11403
tags:
- speech
- merging
- hubert
- mert
- permutation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a correlation-permutation approach to merge
  independently trained speech and music encoders without requiring shared initialization.
  The method computes layer-wise permutation matrices that maximize feature-wise cross-correlations
  between models, enabling effective alignment of otherwise disjoint weight spaces.
---

# A correlation-permutation approach for speech-music encoders model merging

## Quick Facts
- arXiv ID: 2506.11403
- Source URL: https://arxiv.org/abs/2506.11403
- Reference count: 40
- This paper presents a correlation-permutation approach to merge independently trained speech and music encoders without requiring shared initialization.

## Executive Summary
This paper introduces a correlation-permutation method for merging independently trained speech and music encoders, enabling the combination of domain-specific knowledge without shared pre-training. The approach computes layer-wise permutation matrices that maximize feature-wise cross-correlations between models, allowing effective alignment of otherwise disjoint weight spaces. Applied to HuBERT and MERT encoders, the method significantly enhances music performance while retaining speech capabilities, achieving a 14.83-point improvement in average score compared to linear interpolation.

## Method Summary
The method computes optimal permutation matrices layer-by-layer to maximize feature cross-correlations between two models, then applies these permutations to one model before linear interpolation with the other. For each target layer, activations are reshaped to R^(Cl×B·Tl), and the Jonker-Volgenant algorithm finds the permutation that maximizes cross-correlation. The approach requires only a small calibration dataset and preserves the functional specialization of attention heads by first aligning heads at the functional level before channel-level permutation within heads. The final merge uses weighted interpolation (0.9 HuBERT + 0.1 MERT) to maintain speech capability while incorporating music knowledge.

## Key Results
- Achieves 957.65 average score across SUPERB, MARBLE, and ESC50 tasks, representing a 14.83-point improvement over linear interpolation
- Shows CNN permutation alone provides moderate improvement (923.95), while selective transformer permutation (attention heads + channels + W2) provides additional gains
- First CNN layer requires minimal reordering (30.86% channels), while deeper layers require extensive reordering (>98%)
- Maintains speech performance while significantly improving music-specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Independently trained models with identical architectures converge to permutation-equivalent minima, enabling alignment through channel reordering.
- **Mechanism**: The approach computes layer-wise permutation matrices π* that maximize feature-wise cross-correlation between activations. For each layer l, it reshapes activations to R^(Cl×B·Tl), treating temporal and batch dimensions as samples, then uses the Jonker-Volgenant algorithm to find optimal channel assignments.
- **Core assumption**: Models trained on related domains (speech/music audio) develop semantically similar features despite different channel orderings—a hypothesis supported by the finding that only 30.86% of first CNN layer channels needed reordering.
- **Evidence anchors**:
  - [abstract]: "The method computes a permutation matrix that maximizes the model's features-wise cross-correlations layer by layer, enabling effective fusion of these otherwise disjoint models."
  - [section III.A]: "We seek π⋆ = argmax_π∈P Σ E[(θA_l - μ_θA_l)^T (π_l(θB_l) - μ_θB_l)] / (σ_θA_l · σ_θB_l)"
  - [corpus]: Limited direct corpus support for audio-specific permutation; related work (Git Re-Basin, ZipIt) addresses vision domain only.
- **Break condition**: If correlation matrices show near-random structure (no diagonal patterns after optimization), the models may occupy truly disjoint weight basins unsuitable for permutation-based merging.

### Mechanism 2
- **Claim**: Transformer merging requires hierarchical alignment: attention heads first, then channels within heads.
- **Mechanism**: Head-level permutation aligns functional specializations (e.g., MERT's head 1 may correspond to HuBERT's head 3). Without this, averaging parameters from functionally disparate heads causes destructive interference. After head alignment, channel-level permutation within each matched head fine-tunes the correspondence.
- **Core assumption**: Attention heads develop specialized functions that are comparable across speech and music encoders, despite different training objectives.
- **Evidence anchors**:
  - [section III.C]: "If MERT's head 1 functionally corresponds to HuBERT's head 3, but we try to align MERT's head 1 channels with HuBERT's head 1 channels without reordering, we are forcing a mismatch between functional units."
  - [section V.B]: CNN + "fnn+attn" configuration achieves 957.65 average score vs. 923.95 for CNN-only permutation.
  - [corpus]: USAD paper addresses unified audio representations via distillation, not permutation—suggesting alternative but not validating head-alignment hypothesis.
- **Break condition**: If head-to-head correlation matrices show uniformly low values (<0.3), functional specialization may not transfer between speech and music domains.

### Mechanism 3
- **Claim**: Selective permutation outperforms exhaustive permutation; not all transformer components benefit from alignment.
- **Mechanism**: Permuting all Q, K, V matrices disrupts learned query-key-value relationships. The optimal configuration (ID 5) permutes only: (1) CNN channels, (2) attention head ordering, (3) channels within each head, (4) FFN W2 layer. Permuting W1 showed no improvement in preliminary experiments.
- **Core assumption**: The semantic relationships within attention mechanisms (Q-K scoring, attention-weighted V) should be preserved as atomic units.
- **Evidence anchors**:
  - [section V.B]: "CNN + 'all' configuration (ID 8), where all query, key, values within the transformer blocks are permuted... results in an average score of 928.49. This is lower than the more selective CNN + 'fnn+attn' approach (957.65)."
  - [section III.C]: "Attempting to independently permute the outputs of the Query (QW_Q), Key (QW_K), and Value (VW_V) projection matrices for each head using separate permutation matrices would likely be detrimental."
  - [corpus]: No corpus papers validate selective transformer permutation strategies for audio.
- **Break condition**: If W2-only permutation degrades FFN task performance, the assumption about FFN layer roles may be incorrect for audio encoders.

## Foundational Learning

- **Concept: Linear Mode Connectivity**
  - Why needed here: Explains why naive weight averaging fails—models in different basins are connected only via permutation-aligned paths.
  - Quick check question: Can you explain why two models with identical architectures but different initializations might require channel permutation before averaging?

- **Concept: Permutation Symmetry in Neural Networks**
  - Why needed here: Justifies that reordering channels/neurons doesn't change model function, enabling the search for optimal alignment.
  - Quick check question: If you swap two hidden units in an MLP (including their input weights and output weights), does the network's computation change?

- **Concept: Multi-Head Attention Specialization**
  - Why needed here: Motivates hierarchical permutation—heads serve different functions and must be matched before channel alignment.
  - Quick check question: Why can't we simply concatenate all attention head outputs and compute a single permutation matrix?

## Architecture Onboarding

- **Component map**: Input audio → CNN encoder (7 layers: Conv1D + GroupNorm + GeLU for layer 0, Conv1D + GeLU for layers 1-6) → Transformer encoder (12 layers, each with multi-head attention + FFN)
- **Critical path**:
  1. Prepare calibration data (5K speech + 5K music samples)
  2. Forward pass through both models, collecting activations at each target layer
  3. For each layer: compute cross-correlation → solve linear sum assignment → store permutation matrix
  4. Apply inverse permutation between layers to maintain correct data flow
  5. Apply stored permutations to MERT weights → interpolate with HuBERT (0.9/0.1 weighting)

- **Design tradeoffs**:
  - Calibration data size: 10K samples used; smaller subsets may yield unstable correlations
  - Interpolation weights: 0.9/0.1 (HuBERT/MERT) preserves speech capability while adding music—symmetric weights cause collapse
  - Component selection: Permuting too few components (CNN-only: 923.95) or too many (all: 928.49) underperforms targeted selection (fnn+attn: 957.65)

- **Failure signatures**:
  - Average score < 900: Check if permutations are correctly inverted between layers
  - Speech score drops > 50 points: CNN permutation may be disrupting speaker information (SID task sensitive to early layers)
  - Music score < 850: Transformer permutation likely incomplete or misconfigured

- **First 3 experiments**:
  1. **Baseline sanity check**: Replicate (HB+MR)/2 averaging (ID 3) and 0.9 HB + 0.1 MR (ID 4) to verify your interpolation pipeline matches reported scores (476.55 and 938.11).
  2. **CNN-only permutation ablation**: Implement ID 6 to isolate CNN contribution. Verify SID task shows expected sensitivity (Table I: 77.44% vs. 80.22% for full method).
  3. **Permutation depth analysis**: Replicate Table II—count percentage of channels reordered per layer. First CNN layer should show ~30% permutation; deeper layers should exceed 98%. Deviations indicate calibration data issues.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can permutation-based merging be adapted for models that do not share the same architectural structure?
  - Basis in paper: [explicit] The authors state that extending the approach to "allow the merging of models that does not share the same architecture" is a goal for future work.
  - Why unresolved: The current method relies on identical layer shapes to compute permutation matrices (via the Linear Sum Assignment), which breaks when architectures (e.g., depth or width) diverge.
  - What evidence would resolve it: A method that handles dimension mismatches or partial layer mappings, demonstrated by successfully merging a large model with a smaller, distilled variant.

- **Open Question 2**: Can this framework effectively scale to combine knowledge from