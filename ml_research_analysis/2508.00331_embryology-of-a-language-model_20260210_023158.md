---
ver: rpa2
title: Embryology of a Language Model
arxiv_id: '2508.00331'
source_url: https://arxiv.org/abs/2508.00331
tags:
- tokens
- umap
- token
- spacing
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an "embryological" approach to studying language
  model development by visualizing susceptibility vectors using UMAP dimensionality
  reduction. The authors track how a low-dimensional representation of token-level
  susceptibilities evolves during training, revealing the emergence of internal structures
  analogous to anatomical features.
---

# Embryology of a Language Model

## Quick Facts
- arXiv ID: 2508.00331
- Source URL: https://arxiv.org/abs/2508.00331
- Authors: George Wang; Garrett Baker; Andrew Gordon; Daniel Murfet
- Reference count: 40
- Primary result: Visualizes language model development using UMAP dimensionality reduction of susceptibility vectors, revealing a "body plan" with anterior-posterior axis, dorsal-ventral stratification, and spacing fin structure

## Executive Summary
This paper introduces an "embryological" approach to studying language model development by visualizing susceptibility vectors using UMAP dimensionality reduction. The authors track how a low-dimensional representation of token-level susceptibilities evolves during training, revealing the emergence of internal structures analogous to anatomical features. The visualizations reveal a clear "body plan" consisting of an anterior-posterior axis reflecting global expression versus suppression, a dorsal-ventral stratification corresponding to the induction circuit, and a previously unknown "spacing fin" structure for counting space tokens. The rainbow-colored stratification by token pattern (word start/end, induction, spacing, etc.) becomes increasingly organized as training progresses, with the induction circuit's emergence visible as thickening along the dorsal-ventral axis.

## Method Summary
The method involves training a 3M parameter 2-layer attention-only transformer on a subset of the Pile, then computing per-token susceptibility vectors using SGLD posterior sampling at multiple training checkpoints. These 16-dimensional vectors (one per attention head) are standardized and reduced to 2D using UMAP, with tokens colored by their functional patterns (induction, spacing, word boundaries, etc.). The visualization tracks developmental progression from a thin line at early training to a complex "rainbow serpent" structure with distinct anatomical features.

## Key Results
- Emergence of rainbow-colored serpent structure stratified by token patterns, with word starts/ends forming the anterior-posterior axis
- Induction circuit formation visible as dorsal-ventral thickening, separating induction tokens from other patterns
- Discovery of "spacing fin" structure for space token processing, potentially universal across seeds
- Similar structural patterns observed across four different training seeds, suggesting universal developmental principles

## Why This Works (Mechanism)

### Mechanism 1: Susceptibility as a Functional Fingerprint
If per-token susceptibility vectors capture the expression/suppression dynamics of attention heads, then dimensionality reduction of these vectors will reveal clusters stratified by token function. The method treats the susceptibility vector $\eta_w(xy)$ as a high-dimensional coordinate for a token sequence, assuming that heads involved in predicting a specific pattern will show correlated covariance responses. UMAP projects these correlations into visible distances, separating distinct computational roles.

### Mechanism 2: Developmental Thickening via Circuit Formation
The emergence of specific circuits, such as the induction circuit, corresponds to a measurable geometric expansion in the susceptibility manifold along specific axes. As the model transitions from learning n-grams to in-context learning, specific heads specialize, increasing the variance of susceptibility vectors for relevant tokens along PC2 and creating the "dorsal-ventral" axis.

### Mechanism 3: Universal Topology from Data Manifold Constraints
While specific heads are contingent on initialization, the high-level "body plan" (topology of the susceptibility manifold) is universal across seeds because it is dictated by the data distribution and architecture. The loss landscape degeneracy implies that many local solutions implement similar functional decompositions of the data.

## Foundational Learning

- **Concept: Per-token Susceptibility ($\chi^C_{xy}$)**
  - Why needed here: This is the fundamental "gene expression" unit used to build the UMAP. It measures how a weight perturbation in component $C$ correlates with the loss change for a specific token sequence $xy$.
  - Quick check question: If a head $C$ has a large *negative* susceptibility for a token $y$, is the head expressing (raising $p(y|x)$) or suppressing $y$?

- **Concept: Induction Circuits**
  - Why needed here: The paper validates its method by showing the visual emergence of the induction circuit. You need to know this involves a previous-token head and an induction head (K-composition) to interpret the "thickening."
  - Quick check question: Why would induction tokens separate from "word end" tokens in the visualization?

- **Concept: UMAP (Uniform Manifold Approximation and Projection)**
  - Why needed here: This is the lens through which the high-dimensional susceptibility vectors are viewed. Understanding that it preserves local topology but can distort global distances is critical for interpreting the "serpent."
  - Quick check question: If two points are distant in the UMAP plot, are they guaranteed to be distant in the original 16-dimensional susceptibility space?

## Architecture Onboarding

- **Component map:** 2-layer attention-only Transformer (3M params) with 16 Attention Heads (0:0-0:7, 1:0-1:7) -> Token sequences (x, y) sampled from Pile -> Compute susceptibility vector $\eta_w(xy) \in \mathbb{R}^{16}$ for ~260k sequences -> UMAP reduction of $260k \times 16$ matrix

- **Critical path:**
  1. SGLD Sampling: Run Stochastic Gradient Langevin Dynamics to sample weights around the minimum (hyperparameters: $\gamma=300, n\beta=30$)
  2. Covariance Estimation: Compute susceptibility matrix $X$ (rows=token sequences, cols=heads)
  3. Standardization: Center and scale columns of $X$
  4. Reduction: Apply UMAP ($n\_neighbors=45$) to generate the 2D "serpent"

- **Design tradeoffs:**
  - UMAP vs. PCA: Paper chose UMAP for local topology preservation and speed. Tradeoff: UMAP does not preserve global distances; PCA is linear and might miss non-linear manifolds (like the "spacing fin")
  - Model Scale: 3M param model is easily interpretable but may lack the complexity of LLMs
  - Tokenizer: The "spacing fin" prominence is likely an artifact of the GPT-2 tokenizer splitting spaces

- **Failure signatures:**
  - Spurious Clusters: Using too low `n_neighbors` in UMAP creates artificial clusters
  - Noise Floor: If SGLD step size $\epsilon$ or inverse temperature $\beta$ are wrong, susceptibility estimates are noisy, leading to a "fuzzy" serpent with no clear stratification

- **First 3 experiments:**
  1. Validation Run: Reproduce Figure 1 using the provided checkpoint and verify the "rainbow serpent" appears. If it doesn't, check data standardization
  2. Ablation: Manually zero out the induction heads (1:6, 1:7) and re-compute the UMAP. Verify if the "dorsal-ventral" thickening collapses
  3. Tokenizer Test: Retrain (or fine-tune) with a tokenizer that pre-pends spaces to words. Check if the "spacing fin" shrinks or disappears to test the dependency on tokenization artifacts

## Open Questions the Paper Calls Out
None

## Limitations
- The 3M parameter model may not fully capture developmental dynamics of larger language models
- The spacing fin structure may be an artifact of the specific tokenizer rather than a fundamental architectural feature
- Susceptibility computation requires careful tuning of SGLD hyperparameters to avoid noise floors
- Study focuses on a single architecture (2-layer attention-only) and dataset (Pile), limiting generalizability

## Confidence
- **High confidence:** The emergence of stratified rainbow structure during training
- **Medium confidence:** The identification of the dorsal-ventral axis with the induction circuit
- **Medium confidence:** The spacing fin as a computational mechanism for space counting
- **Low confidence:** The universality of the "body plan" topology across seeds

## Next Checks
1. **Cross-model validation:** Reproduce the susceptibility analysis on a 12-layer decoder-only model trained on a different corpus (e.g., Common Crawl) to test whether the rainbow serpent topology and spacing fin emerge consistently
2. **Tokenizer ablation study:** Train the same 3M model with three different tokenization schemes (GPT-2, BPE with space-bound tokens, and SentencePiece) and compare whether the spacing fin structure persists across tokenizations
3. **Circuit ablation experiment:** Use the pathway attribution method from "You Are What You Eat" to systematically ablate the induction circuit and verify that dorsal-ventral thickening disappears while other structural features remain intact