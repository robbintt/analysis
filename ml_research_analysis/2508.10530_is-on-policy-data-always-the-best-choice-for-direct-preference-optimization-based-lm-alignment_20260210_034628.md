---
ver: rpa2
title: Is On-Policy Data always the Best Choice for Direct Preference Optimization-based
  LM Alignment?
arxiv_id: '2508.10530'
source_url: https://arxiv.org/abs/2508.10530
tags:
- preference
- alignment
- stage
- candidates
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether on-policy data is always the best
  choice for direct preference optimization (DPO)-based language model (LM) alignment.
  The authors reveal that on-policy data is not always optimal, with systematic effectiveness
  differences emerging between static and on-policy preference candidates across different
  models.
---

# Is On-Policy Data always the Best Choice for Direct Preference Optimization-based LM Alignment?

## Quick Facts
- arXiv ID: 2508.10530
- Source URL: https://arxiv.org/abs/2508.10530
- Authors: Zetian Sun; Dongfang Li; Xuhui Chen; Baotian Hu; Min Zhang
- Reference count: 40
- Primary result: On-policy data is not always optimal for DPO alignment; effectiveness depends on model and alignment stage

## Executive Summary
This paper challenges the conventional wisdom that on-policy data is always superior for Direct Preference Optimization (DPO)-based language model alignment. Through extensive experiments across 5 models (Llama-3, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO, SLiC-HF), the authors reveal that on-policy data can be 3× more effective for Llama-3 but only 0.4× as effective for Zephyr compared to static off-policy data. The key insight is that alignment exhibits two distinct stages: a preference injection stage that benefits from diverse data and a preference fine-tuning stage that favors high-quality data. To address this, the authors propose a boundary measurement algorithm that determines the current alignment stage and guides optimal data selection.

## Method Summary
The authors propose a two-stage alignment framework where models transition from a preference injection stage (benefiting from diverse off-policy data) to a preference fine-tuning stage (benefiting from high-quality on-policy data). They implement a boundary measurement algorithm that compares preference consistency between on-policy and off-policy samples to determine the current stage. The method uses a Bradley-Terry preference model to establish a bijection between text and preference distributions, enabling theoretical characterization of both stages. Training involves two iterations where data source choice is guided by the boundary score: if V_off > V_on (boundary score < 0.5), use off-policy diverse data; otherwise use on-policy high-quality samples.

## Key Results
- On-policy data effectiveness varies dramatically: 3× better for Llama-3, 0.4× better for Zephyr compared to static data
- Preference injection stage benefits from data diversity, while preference fine-tuning stage benefits from data quality
- Boundary measurement algorithm successfully identifies stage transitions with ~3.2% computational overhead
- Stage assumption generalizes across 5 models and 2 alignment methods (DPO, SLiC-HF)
- Model collapse observed when switching from PC_on to PC_off with SLiC-HF (-11.93 point drop on Llama-3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alignment process exhibits two distinct stages with different optimal data characteristics.
- Mechanism: During **preference injection stage**, models lack knowledge of ground-truth preference distribution and benefit from diverse candidates to better estimate the general text distribution πG. During **preference fine-tuning stage**, models have learned the preference landscape and benefit from high-quality on-policy samples to refine within high-reward regions.
- Core assumption: The alignment objective can be decomposed into (1) learning the general preference distribution and (2) fine-tuning within high-reward regions.
- Evidence anchors:
  - [abstract]: "divides the alignment process into two distinct stages: the preference injection stage, which benefits from diverse data, and the preference fine-tuning stage, which favors high-quality data"
  - [section 4.2]: "models trained with PC_on consistently outperform those trained with PC_off given the same initial model in every setting (∆<1)" for Llama-3, but opposite pattern for Phi-2
  - [corpus]: Limited direct validation; neighbor papers focus on on-policy benefits without stage differentiation
- Break condition: If a model has already converged to the optimal policy, both stages collapse and data source choice becomes irrelevant.

### Mechanism 2
- Claim: Preference consistency between candidate distributions and ground truth determines optimal data source.
- Mechanism: The boundary measurement algorithm (Algorithm 1) computes preference consistency by comparing P*(ground truth) with both P_θ(on-policy) and P_off(off-policy). When P_off shows higher consistency with P* than P_θ, the model is in preference injection stage (use off-policy diverse data). When P_θ shows higher consistency, use on-policy high-quality data.
- Core assumption: Preference distributions P and text distributions π form a bijection under Bradley-Terry definition (Theorem 5.4), allowing preference consistency to proxy text distribution distance.
- Evidence anchors:
  - [section 5.2]: Definition 5.5 formalizes preference consistency as Ex,y1,y2[I[P1⊙I[P2]]
  - [section 5.3]: Equations 10-11 show practical estimation using sampled responses
  - [corpus]: No direct corpus validation of this specific measurement approach
- Break condition: If the preference model P* is misspecified or weak, boundary decisions optimize for wrong target.

### Mechanism 3
- Claim: Data diversity matters more than data source (on/off-policy) for early-stage alignment.
- Mechanism: Off-policy datasets like PC_off naturally contain higher intra-diversity (measured by log-probability differences between preference pairs) because candidates are sampled from multiple diverse models. This diversity helps the model construct a better approximation of the general text distribution πG during early alignment.
- Core assumption: Diverse preference candidates provide better coverage of the infinite-sample preference dataset required for optimal DPO (Theorem 5.2).
- Evidence anchors:
  - [section 4.3]: "PC_off attributes to slight improvement for Llama-3" (already fine-tuning stage) vs large gains for Zephyr initially
  - [appendix C.5, Figure 3]: Shows larger fluctuation in ∆logps for PC_off vs PC_llama
  - [corpus]: "Optimizing LVLMs with On-Policy Data" (arxiv 2512.00706) confirms on-policy benefits but doesn't address stage dependency
- Break condition: If off-policy data is too low-quality (vs. diverse), diversity benefits may be negated by noise.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) objective**
  - Why needed here: Understanding that DPO optimizes π_θ to maximize expected reward while minimizing KL divergence from reference policy is essential for grasping why data source matters.
  - Quick check question: Can you explain why Eq. (6) is derived from Eq. (3) by substituting the reward function?

- **Concept: Bradley-Terry preference model**
  - Why needed here: The entire theoretical framework relies on P*(y1≻y2|x) = σ(log π_G(y1|x) - log π_G(y2|x)). Without this, the preference-text distribution bijection fails.
  - Quick check question: How does the sigmoid function σ convert log-probability differences into preference probabilities?

- **Concept: On-policy vs. off-policy sampling in RL**
  - Why needed here: The paper's core contribution is showing that standard RL wisdom (on-policy is always better) doesn't hold for DPO alignment. You need to understand what makes data "on-policy" (sampled from current π_θ) vs "off-policy" (sampled from different distribution).
  - Quick check question: Why does iterative DPO incorporate on-policy sampling during training?

## Architecture Onboarding

- **Component map:**
  ```
  Preference Model (P*, e.g., PairRM)
         ↓
  Boundary Measurement Algorithm (Alg 1)
         ↓ compares P* vs P_θ vs P_off
  Stage Decision: Injection or Fine-tuning
         ↓
  Data Selection: Off-policy (diverse) or On-policy (high-quality)
         ↓
  DPO Training Loop (Eq. 6)
         ↓
  Updated Policy π_θ
  ```

- **Critical path:**
  1. Implement preference annotation using a consistent preference model (authors used PairRM)
  2. For each training iteration, run Algorithm 1 on a subset (~2,000 prompts) to determine stage
  3. If V_off > V_on (boundary score < 0.5), use off-policy diverse data; otherwise use on-policy samples
  4. Computational overhead: ~3.2% of one training epoch for boundary measurement

- **Design tradeoffs:**
  - **Preference model choice**: Authors note boundary is "relative to the preference model P" - weak P gives suboptimal boundaries for human alignment
  - **Binary stage switch vs. smooth blending**: Paper acknowledges "smoother and more adaptive data blending strategies" could improve on rigid binary switch
  - **Sampling cost**: On-policy sampling requires inference overhead; off-policy is cheaper upfront but may require more iterations

- **Failure signatures:**
  - Performance drop when using PC_on in injection stage (observed in Phi-2: -1.60 LC win rate)
  - Model collapse phenomenon (observed in Llama-3 with SLiC-HF: -11.93 point drop on PC_off→on sequence)
  - Boundary score conflicts: Zephyr post-PC_on shows BS=0.58 (suggests fine-tuning) but PC_off still outperforms (8.5 vs 5.5 point gain) due to quality differences

- **First 3 experiments:**
  1. **Replicate two-iteration framework** on your target model: Train with PC_off→off, PC_off→on, PC_on→off, PC_on→on and compute ∆ ratios to verify stage assumption holds
  2. **Validate boundary measurement**: Run Algorithm 1 at multiple checkpoints and verify that BS<0.5 predicts PC_off superiority and BS>0.5 predicts PC_on superiority
  3. **Ablate data characteristics**: Construct controlled datasets varying diversity (log-prob difference spread) and quality (PairRM win rate) independently to confirm stage-specific benefits aren't confounded by on/off-policy label alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can smoother and more adaptive data blending strategies be developed to transition between the preference injection and fine-tuning stages, rather than relying on a rigid switch?
- Basis in paper: [explicit] The Conclusion states the assumption "inspires exploration of smoother and more adaptive data blending strategies rather than a rigid switch, which is not included and we leave for future research."
- Why unresolved: The current work utilizes a binary boundary measurement (Algorithm 1), which may not capture the nuances of the transition or optimize sample efficiency during the shift between stages.
- What evidence would resolve it: A study demonstrating that a curriculum or dynamic weighting of on-policy and off-policy data based on boundary scores outperforms the discrete switching method.

### Open Question 2
- Question: To what extent does the quality or bias of the external preference model used in the boundary measurement algorithm distort the accuracy of stage identification?
- Basis in paper: [inferred] Appendix B.2 acknowledges that the boundary decision is relative to the preference model and notes, "If $P$ is weak or biased, the boundary decision might be suboptimal."
- Why unresolved: The method relies on a proxy (PairRM) to estimate ground truth, but the sensitivity of the boundary detection to errors in this proxy remains unquantified.
- What evidence would resolve it: Experiments measuring the variance in boundary scores and final alignment performance when using multiple preference models of varying quality (e.g., GPT-4 vs. smaller RMs).

### Open Question 3
- Question: Does the alignment stage assumption and the preference consistency measurement hold for preference distributions that do not strictly adhere to the Bradley-Terry (BT) model definition?
- Basis in paper: [inferred] The theoretical justification (Section 5) and Definition 5.3 rely on the BT definition ($P(y_1 \succ y_2) = \sigma(\dots)$) to link text and preference distributions.
- Why unresolved: Real-world human preferences often violate BT assumptions (e.g., intransitivity), and the algorithm's robustness to such violations is not discussed.
- What evidence would resolve it: Theoretical analysis or empirical validation of the boundary measurement algorithm using datasets with known non-BT preference structures.

## Limitations
- Binary stage-switching mechanism may be overly simplistic compared to potential smooth blending strategies
- Boundary measurement algorithm's effectiveness relies heavily on the quality of the preference model P*
- Findings depend critically on the assumption that the Bradley-Terry preference model accurately captures ground-truth preference distribution

## Confidence
- **High confidence**: The empirical observation that on-policy data effectiveness varies systematically across models (Llama-3 vs Zephyr) and the general alignment stage framework
- **Medium confidence**: The theoretical justification for the preference-text distribution bijection and the boundary measurement algorithm's practical utility
- **Low confidence**: The universal applicability of the two-stage framework across all model families and the optimality of the binary stage-switching approach

## Next Checks
1. **Validate boundary measurement sensitivity**: Test Algorithm 1 with varying preference model qualities (from weak to strong) to quantify how measurement errors propagate to stage decisions and final alignment performance.

2. **Test smooth blending alternatives**: Implement and compare a continuous weighting scheme for on/off-policy data based on the boundary score rather than the current binary switch to assess potential performance improvements.

3. **Cross-domain generalizability**: Apply the framework to alignment for non-chat tasks (code generation, reasoning) to verify the two-stage assumption holds beyond conversational AI, particularly examining whether diversity requirements differ by task type.