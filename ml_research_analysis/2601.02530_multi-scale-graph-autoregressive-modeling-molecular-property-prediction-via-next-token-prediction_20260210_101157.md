---
ver: rpa2
title: 'Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via
  Next Token Prediction'
arxiv_id: '2601.02530'
source_url: https://arxiv.org/abs/2601.02530
tags:
- prediction
- graph
- molecular
- cams
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Connection-Aware Motif Sequencing (CamS), a graph-to-sequence
  interface that enables standard decoder-only Transformers to learn molecular graphs
  via Next Token Prediction (NTP). CamS bridges the gap between SMILES-based NTP (which
  lacks explicit topology) and graph-native masked modeling (which risks disrupting
  critical chemical details like activity cliffs) by serializing molecular graphs
  into multi-scale, causal sequences.
---

# Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via Next Token Prediction

## Quick Facts
- arXiv ID: 2601.02530
- Source URL: https://arxiv.org/abs/2601.02530
- Authors: Zhuoyang Jiang; Yaosen Min; Peiran Jin; Lei Chen
- Reference count: 40
- One-line primary result: Introduces CamS, a graph-to-sequence interface for molecular property prediction, achieving SOTA on MoleculeNet and MoleculeACE.

## Executive Summary
Connection-Aware Motif Sequencing (CamS) enables standard decoder-only Transformers to model molecular graphs via Next Token Prediction (NTP) by serializing graphs into multi-scale, causal sequences. The method mines data-driven connection-aware motifs, serializes them via scaffold-rooted BFS, and concatenates sequences from fine to coarse scales to enable hierarchical modeling. CamS-LLaMA, pre-trained on CamS sequences using a vanilla LLaMA backbone, achieves state-of-the-art performance on MoleculeNet (AUROC 0.845, RMSE 1.172) and MoleculeACE (RMSE 0.624), outperforming both SMILES-based language models and strong graph baselines.

## Method Summary
CamS bridges the gap between SMILES-based NTP (which lacks explicit topology) and graph-native masked modeling (which risks disrupting critical chemical details like activity cliffs) by serializing molecular graphs into multi-scale, causal sequences. The method first mines data-driven connection-aware motifs, then serializes them via scaffold-rooted BFS to establish a stable core-to-periphery order, and crucially concatenates sequences from fine to coarse scales to enable hierarchical modeling. The authors instantiate CamS-LLaMA by pre-training a vanilla LLaMA backbone on CamS sequences, achieving state-of-the-art performance on MoleculeNet and MoleculeACE.

## Key Results
- CamS-LLaMA achieves AUROC 0.845 and RMSE 1.172 on MoleculeNet, outperforming SMILES-based language models and strong graph baselines
- CamS-LLaMA achieves RMSE 0.624 on MoleculeACE, demonstrating strong performance on diverse molecular property prediction tasks
- Interpretability analysis confirms that multi-scale causal serialization drives attention toward cliff-determining structural differences

## Why This Works (Mechanism)
CamS works by serializing molecular graphs into multi-scale, causal sequences that preserve both local motif connectivity and global scaffold structure. The method mines data-driven connection-aware motifs to capture chemically meaningful substructures, then orders them via scaffold-rooted BFS to establish a stable core-to-periphery sequence. By concatenating sequences from fine to coarse scales, CamS enables hierarchical modeling where the model first learns local motif relationships before integrating them into the global molecular structure. This multi-scale approach addresses the limitations of both SMILES-based NTP (which lacks explicit topology) and graph-native masked modeling (which risks disrupting critical chemical details).

## Foundational Learning

### Connection-aware motifs
**Why needed**: Capture chemically meaningful substructures that preserve connectivity patterns essential for molecular properties
**Quick check**: Verify motif mining preserves known functional groups and pharmacophores

### Scaffold-rooted BFS ordering
**Why needed**: Establish stable core-to-periphery sequence that reflects molecular architecture
**Quick check**: Confirm BFS order maintains chemical validity across diverse molecular scaffolds

### Multi-scale serialization
**Why needed**: Enable hierarchical modeling from local motifs to global structure
**Quick check**: Test model performance with single-scale vs. multi-scale sequences

## Architecture Onboarding

### Component map
LLaMA backbone <- CamS sequence generator <- (motif mining <- molecular graph) + (BFS ordering <- scaffold extraction)

### Critical path
1. Mine connection-aware motifs from molecular graph
2. Extract scaffold and perform BFS ordering
3. Generate multi-scale sequences (fine to coarse)
4. Feed sequences to LLaMA backbone for pre-training
5. Fine-tune on molecular property prediction tasks

### Design tradeoffs
- **Pro**: Leverages standard decoder-only Transformers without architectural modifications
- **Con**: Assumes motif mining quality is stable across datasets
- **Pro**: Preserves chemical details better than masked modeling approaches
- **Con**: Fixed scaffold rooting may fail for symmetric cores or non-tree-like structures

### Failure signatures
- Poor performance on molecules with symmetric cores or non-tree-like structures
- Sensitivity to motif mining quality and selection thresholds
- Overfitting to specific scaffold extraction methods

### First experiments to run
1. Compare single-scale vs. multi-scale CamS serialization performance
2. Test CamS-LLaMA on additional molecular property benchmarks (e.g., ChEMBL)
3. Perform ablation studies with randomized motif orderings

## Open Questions the Paper Calls Out
None

## Limitations
- No ablation studies isolate the impact of multi-scale serialization versus simpler orderings
- Interpretability gains are qualitative without statistical testing or cross-model comparison
- Method assumes motif mining quality is stable across datasets without sensitivity analysis
- Reliance on fixed scaffold rooting may fail for symmetric cores or non-tree-like structures

## Confidence

### Predictive claims
- Medium: SOTA performance reported but limited benchmark comparisons

### Interpretability claims
- Low: Qualitative attention visualization without statistical testing

### Methodological robustness
- Low: No sensitivity analysis for motif selection or alternative architectures

## Next Checks
1. Perform ablation studies comparing single-scale vs. multi-scale CamS serialization on the same backbone to quantify the contribution of hierarchical ordering
2. Test CamS-LLaMA on additional molecular property benchmarks (e.g., ChEMBL, OGB-MolHiv) to assess robustness beyond MoleculeNet and MoleculeACE
3. Conduct statistical significance testing for attention-based interpretability results, comparing CamS to baseline models with shuffled or randomized motif orderings