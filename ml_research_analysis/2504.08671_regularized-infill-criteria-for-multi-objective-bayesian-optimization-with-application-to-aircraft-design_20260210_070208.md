---
ver: rpa2
title: Regularized infill criteria for multi-objective Bayesian optimization with
  application to aircraft design
arxiv_id: '2504.08671'
source_url: https://arxiv.org/abs/2504.08671
tags:
- optimization
- problems
- design
- segomoe
- pareto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the SEGOMOE (Super Efficient Global Optimization
  with Mixture of Experts) framework to handle constrained multi-objective optimization
  problems. The key innovation is the introduction of regularized infill criteria
  that combine multi-objective acquisition functions with regularization terms based
  on Gaussian Process predictions, addressing the ill-posedness of the optimization
  problem.
---

# Regularized infill criteria for multi-objective Bayesian optimization with application to aircraft design

## Quick Facts
- arXiv ID: 2504.08671
- Source URL: https://arxiv.org/abs/2504.08671
- Reference count: 40
- Primary result: SEGOMOE with regularized criteria achieves 115 function evaluations for aircraft design Pareto front vs. 2500 for NSGA-II (20x reduction)

## Executive Summary
This paper addresses the challenge of constrained multi-objective optimization in high-dimensional spaces by extending the SEGOMOE framework with regularized infill criteria. The key innovation introduces regularization terms based on Gaussian Process predictions that improve convergence by addressing ill-posedness in the acquisition optimization landscape. The method combines multi-objective acquisition functions with scalarization operators that penalize candidate points with high predicted objective values. When applied to both benchmark problems and a real-world aircraft design application, the regularized approach demonstrates significantly improved efficiency, achieving comparable Pareto front quality with substantially fewer expensive function evaluations.

## Method Summary
The method extends SEGOMOE to multi-objective problems by building separate Gaussian Process surrogates for each objective and constraint, then using regularized acquisition functions that combine multi-objective criteria (Expected Hypervolume Improvement, Probability of Improvement, or Multi-objective Probability of Improvement) with regularization terms based on GP mean predictions. Two regularization types are explored: using either the maximum or sum of predicted objectives as a penalty term. The approach iteratively enriches the design of experiments by optimizing the regularized acquisition function over an approximate feasible domain, then extracts the final Pareto front using NSGA-II on the GP surrogates. This decoupling of expensive evaluations from Pareto sampling density yields substantial computational savings while maintaining accuracy.

## Key Results
- Regularized criteria improve convergence, particularly in high-dimensional spaces (d > 10)
- Maximum-based regularization (reg=max) performs best overall, while sum-based (reg=sum) works best with EHVI acquisition
- For aircraft design application, SEGOMOE achieves 115 evaluations vs. 2500 for NSGA-II (20x reduction, 3-4 hours vs. 85 hours)
- Benchmark results on 12 problems show consistent IGD+ improvement with regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularizing infill criteria with GP mean predictions improves convergence in high-dimensional multi-objective problems.
- Mechanism: The regularization term ψ(μf(x)) penalizes candidate points with high predicted objective values, counteracting ill-posedness from multiple local maxima in the acquisition landscape. Two scalarization operators are tested: max (ψ(y) = max yi) and sum (ψ(y) = Σyi).
- Core assumption: The GP mean provides a reasonable proxy for objective quality during acquisition optimization.
- Evidence anchors:
  - [abstract] "The authors propose using regularized infill criteria to address ill-posedness in the multi-objective optimization process."
  - [Section III.B, Eq. 3] "αreg_f(x) = γ·αf(x) − ψ(μf(x))" with γ scaling based on scaled WB2 criterion.
  - [corpus] Related work on surrogate-assisted evolutionary algorithms (arxiv 2507.02892) notes infill criteria selection is critical but doesn't directly validate this specific regularization.
- Break condition: If GP mean predictions are poor (insufficient data or misspecified kernels), regularization may misguide sampling.

### Mechanism 2
- Claim: Extending SEGOMOE to multi-objective problems via per-component GP surrogates and scalar acquisition functions enables efficient Pareto front discovery.
- Mechanism: Separate GPs are built for each objective and constraint (parallelizable). At each iteration, a scalar multi-objective acquisition (EHVI, PI, MPI) is maximized over the approximate feasible domain. The Pareto front is extracted post-hoc via NSGA-II on the cheap GP surrogates.
- Core assumption: Independent GP models per component adequately capture correlations; the acquisition function scalarization doesn't lose critical Pareto information.
- Evidence anchors:
  - [abstract] "The proposed method extends SEGOMOE to solve constrained multi-objective problems."
  - [Algorithm 1, lines 2-7] Explicit steps for GP construction, feasibility domains, acquisition selection, and Pareto extraction.
  - [corpus] Multi-objective Bayesian optimization with mixed variables (arxiv 2504.09930) follows similar surrogate-based strategies, supporting general approach validity.
- Break condition: When objectives are strongly coupled and independent GPs fail to capture joint structure; when constraint approximations are too loose, causing infeasible enrichment.

### Mechanism 3
- Claim: Using GP surrogates for final Pareto extraction (via NSGA-II) rather than direct function evaluations yields large computational savings.
- Mechanism: After budget iterations, GP surrogates—which are cheap to evaluate—are optimized with NSGA-II (50 generations × 100 individuals) to produce the final Pareto approximation. This decouples expensive evaluations from Pareto sampling density.
- Core assumption: GP surrogates at convergence are sufficiently accurate across the Pareto-optimal region.
- Evidence anchors:
  - [Section V, Table 3] SEGOMOE achieves 115 evaluations vs. NSGA-II's 2500; 3-4 hours vs. 85 hours.
  - [Fig. 8b] GP-predicted Pareto points closely match re-evaluated FAST-OAD values.
  - [corpus] Limited direct validation in corpus for this specific decoupling strategy; assumption remains empirically grounded only in this paper's tests.
- Break condition: If GP uncertainty is high in Pareto-optimal regions, surrogate-based front may be inaccurate or infeasible.

## Foundational Learning

- Concept: **Gaussian Process (Kriging) Surrogates**
  - Why needed here: The entire method relies on GP prediction mean μ and uncertainty σ for objectives and constraints. Without understanding GP regression, the regularization and acquisition logic are opaque.
  - Quick check question: Given 10 data points, can you explain why a GP provides both a prediction and uncertainty estimate at a new location?

- Concept: **Multi-objective Optimization and Pareto Dominance**
  - Why needed here: The method targets Pareto front approximation. Understanding dominance, non-dominated sets, and hypervolume is essential for interpreting IGD+ results and acquisition functions like EHVI.
  - Quick check question: For two objectives to minimize, does point A=(1, 10) dominate point B=(2, 5)? Why or why not?

- Concept: **Acquisition Functions (EI, PI, EHVI)**
  - Why needed here: The regularization modifies standard acquisition functions. Grasping the exploration-exploitation tradeoff in EI/PI is prerequisite to understanding why regularization helps.
  - Quick check question: How does Expected Improvement balance exploring uncertain regions vs. exploiting known low values?

## Architecture Onboarding

- Component map:
  - SMT (Surrogate Modeling Toolbox) -> Builds GP/KPLS models for objectives and constraints
  - SEGOMOE Core Loop -> Iteratively selects enrichment points via acquisition maximization
  - Regularization Module -> Computes αreg_f = γ·αf − ψ(μf) with configurable ψ (max or sum)
  - Pymoo/NSGA-II -> Post-processing on GP surrogates to extract final Pareto front
  - Optimizers (COBYLA, SLSQP, SNOPT) -> Solve the constrained acquisition sub-problem

- Critical path:
  1. Initialize DoE (2d + 2c + 1 points via LHS)
  2. Build GP surrogates for all objectives and constraints
  3. Define approximate feasible domain Ωg ∩ Ωh via feasibility criteria
  4. Compute regularized acquisition αreg_f; find x(l+1) via constrained optimization
  5. Evaluate true functions at x(l+1); update DoE
  6. Repeat until budget (20d evaluations) exhausted
  7. Run NSGA-II on GP surrogates for final Pareto approximation

- Design tradeoffs:
  - PI vs. EHVI: PI is fastest; EHVI gives best final IGD+ but is computationally expensive. Recommendation: PI(reg=max) for d > 10
  - reg=max vs. reg=sum: max favors worst-case objective (conservative); sum balances all objectives. Sum often yields lower IGD+ for EHVI
  - Budget allocation: More iterations improve front quality but linear cost increase; surrogate quality plateaus if DoE is too sparse for high d

- Failure signatures:
  - High dispersion in IGD+ across runs → acquisition optimization may be stuck in local maxima; consider multi-start or different optimizer
  - Final Pareto points infeasible on true functions → constraint approximations too optimistic; tighten feasibility criteria
  - IGD+ plateaus early → exploration insufficient; increase γ or switch to exploration-biased acquisition (e.g., UCB-based)
  - GP predictions diverge from validation points → kernel misspecification or insufficient data; consider KPLS for high d

- First 3 experiments:
  1. **Baseline sanity check**: Run standard (non-regularized) SEGOMOE on ZDT1 (d=2) with PI and EHVI; confirm convergence plots match Fig. 2a to validate implementation
  2. **Regularization ablation**: On ZDT1 (d=10), compare PI, PI(reg=max), and PI(reg=sum); verify that regularized versions achieve lower IGD+ per Table 1
  3. **Constraint handling test**: Run on constrained TNK problem; check that feasible domain approximation correctly excludes infeasible regions by visualizing Ωg ∩ Ωh

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed regularized infill criteria be extended to handle mixed-categorical design variables?
- Basis in paper: [explicit] The conclusion states that "an extension of SEGOMOE to solve mixed-categorical multi-objective optimization problems shall be investigated in a near future."
- Why unresolved: The current methodology and benchmarks are restricted to continuous design variables, whereas realistic aircraft design problems often involve discrete choices (e.g., engine types).
- What evidence would resolve it: A modified SEGOMOE framework implementing specific kernels for categorical variables, validated on mixed-variable benchmarks or the referenced "more realistic" aircraft design problems.

### Open Question 2
- Question: Can the selection between the "sum" and "max" regularization variants be automated or adapted dynamically during optimization?
- Basis in paper: [inferred] The authors propose two distinct regularization variants (Eq. 4) and provide heuristic recommendations (PI with `reg=max` for high dimensions), but do not define a unified rule for selection.
- Why unresolved: The performance differs based on dimensionality and the specific acquisition function (e.g., `reg=sum` works best for EHVI), suggesting a "one-size-fits-all" approach does not exist yet.
- What evidence would resolve it: An adaptive algorithm that selects the scalarization operator $\psi$ based on runtime metrics (e.g., improvement stagnation or dimension), showing superior robustness compared to static configurations.

### Open Question 3
- Question: Does the regularized approach maintain its efficiency when scaling to many-objective problems ($n > 2$)?
- Basis in paper: [inferred] The method is defined for $n$ objectives but validated exclusively on bi-objective benchmarks (ZDT, BNH, CERAS) where $n=2$.
- Why unresolved: The scalarization operators (max/sum) and the hypervolume calculation may suffer from computational inefficiency or reduced discriminative power as the number of objectives increases.
- What evidence would resolve it: Application of the regularized SEGOMOE to standard many-objective test suites (e.g., DTLZ with $n \geq 3$) demonstrating consistent convergence rates.

## Limitations

- Regularization effectiveness depends on GP mean prediction quality, which degrades in high-dimensional spaces with sparse data
- No universal optimal choice between max and sum regularization; performance varies by problem and acquisition function
- Constraint handling relies on approximate feasibility domains that may exclude viable regions if criteria are too conservative
- Computational savings validated on single aircraft design case study; generalizability to other engineering domains uncertain

## Confidence

- **High confidence**: The core mechanism of regularized infill criteria is well-supported by mathematical formulation and benchmark results
- **Medium confidence**: The computational savings claim is supported by one case study but requires broader validation
- **Low confidence**: The generalizability of specific hyperparameter choices across diverse problem classes remains uncertain

## Next Checks

1. **Cross-validation of regularization effectiveness**: Test PI(reg=max) and PI(reg=sum) on a broader set of benchmark problems including those with varying correlation structures between objectives to determine which regularization type performs best under different conditions.

2. **Constraint approximation sensitivity**: Systematically vary the feasibility criteria thresholds for Ωg and Ωh on benchmark problems to quantify how constraint approximation quality affects convergence and final Pareto front accuracy.

3. **Budget efficiency analysis**: Conduct experiments varying the evaluation budget (e.g., 10d, 30d, 50d) across multiple problem dimensions to establish when the regularized approach provides meaningful improvements versus standard methods, and identify the point of diminishing returns.