---
ver: rpa2
title: 'Knowing the Facts but Choosing the Shortcut: Understanding How Large Language
  Models Compare Entities'
arxiv_id: '2510.16815'
source_url: https://arxiv.org/abs/2510.16815
tags:
- social
- atoms
- cities
- case
- buildings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) use
  numerical knowledge or surface-level heuristics when comparing entities along numerical
  attributes. The authors ask models to compare entities (e.g., "Which river is longer,
  the Danube or the Nile?") and analyze the consistency between their pairwise predictions
  and their own extracted numerical values.
---

# Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities

## Quick Facts
- arXiv ID: 2510.16815
- Source URL: https://arxiv.org/abs/2510.16815
- Reference count: 40
- Key outcome: Despite having numerical knowledge, LLMs frequently rely on surface heuristics (popularity, order, co-occurrence) rather than principled reasoning when comparing entities

## Executive Summary
This paper investigates whether large language models use their numerical knowledge or surface-level heuristics when comparing entities along numerical attributes. The authors find that smaller models (7-8B parameters) predominantly rely on heuristic shortcuts, while larger models (32B parameters) selectively use numerical knowledge when it is more reliable. Surprisingly, for smaller models, a simple logistic regression using only surface cues predicts choices more accurately than the model's own extracted numerical values, suggesting heuristics override principled reasoning. Chain-of-thought prompting steers all models toward numerical features, though not achieving full consistency.

## Method Summary
The study uses 10 numerical attributes from Wikidata (e.g., river lengths, building heights) with up to 1000 entities each. Models are evaluated on zero-shot pairwise comparison queries using 6 prompt templates × 2 orderings = 12 prompts per pair. Numerical values are extracted using 3 templates per entity, selecting the lowest-perplexity prediction. A Balanced-Orthogonal Subset (BOS) is constructed to isolate effects of popularity, mention order, and semantic co-occurrence by ensuring 50/50 balance and independence across these cues. Predictions are classified into four cases based on consistency with internal numerical knowledge, and risk ratios quantify bias effects.

## Key Results
- Smaller models (1-8B) show knowledge-use gaps: pairwise accuracy (64-72%) consistently below internal consistency (74-79%) and numerical accuracy (79-88%)
- For small models, a logistic regression using only surface cues outperforms numerical predictions in predicting model choices
- Larger models (32B) selectively rely on numerical knowledge when it is more reliable, showing no such discrimination in smaller models
- Position bias (mention order) is the strongest shortcut across all models, with risk ratios reaching ~2.0 for small models
- Chain-of-thought prompting increases internal consistency and pairwise accuracy by 5-9 percentage points across all model sizes

## Why This Works (Mechanism)

### Mechanism 1: Surface Cue Dominance in Smaller Models
Models in the 1-8B parameter range rely predominantly on surface heuristics (popularity, mention order, co-occurrence) rather than their own numerical knowledge when making pairwise comparisons. A logistic regression meta-predictor using only three surface cues predicts small model choices more accurately than the model's own extracted numerical values, with position bias being the dominant shortcut.

### Mechanism 2: Strategic Knowledge Selection in Larger Models
Larger models (~32B) conditionally switch between numerical reasoning and heuristic fallback based on the reliability of their numerical knowledge. When numerical values are highly uncertain (high SMAPE/CV), large models favor heuristics; when reliable, they favor numerical comparisons, showing substantial discrimination that smaller models lack.

### Mechanism 3: Chain-of-Thought as Alignment Pressure
CoT prompting increases internal consistency and pairwise accuracy by steering models toward numerical features across all model sizes. While CoT raises Case 1 and Case 2 classifications, it does not achieve full consistency, suggesting traces may rationalize pre-chosen answers rather than fully control decisions.

## Foundational Learning

- Concept: Position Bias
  - Why needed here: Order is the strongest shortcut (RR up to ~2.0) and can flip answers when entities are swapped
  - Quick check question: If you swap (A, B) to (B, A), does the model's choice change?

- Concept: Internal Consistency vs Numerical Accuracy
  - Why needed here: Distinguishes "knowing" from "using" knowledge; numerical accuracy can exceed pairwise accuracy
  - Quick check question: Does the pairwise choice agree with the ranking implied by the model's own extracted numbers?

- Concept: Balanced-Orthogonal Subset (BOS)
  - Why needed here: Required to isolate each cue's effect by ensuring 50/50 balance and independence across P, O, C, I
  - Quick check question: Across the subset, do all 16 (P, O, C, I) combinations appear equally often?

## Architecture Onboarding

- Component map:
  - Pairwise comparison module (6 templates × 2 orderings = 12 prompts per pair)
  - Numerical extraction module (3 templates per entity; lowest-perplexity selection)
  - Meta-predictor (logistic regression: popularity + position + co-occurrence → choice prediction)
  - BOS construction pipeline (balances 16 (P, O, C, I) cells per template)

- Critical path:
  1. Extract numerical values for both entities; select lowest-perplexity prediction
  2. Run pairwise comparison across all template/order combinations
  3. Construct BOS to ensure balance/orthogonality; compute risk ratios
  4. Classify predictions into Cases 1-4; run meta-predictor cross-validation

- Design tradeoffs:
  - 8-bit quantization for models >10B (efficiency) vs potential numerical precision loss
  - Regex parsing simplicity vs edge-case handling (authors report <6% failure)
  - BOS discard rate (often majority discarded) vs statistical control

- Failure signatures:
  - Pairwise accuracy < numerical accuracy (knowledge-use gap)
  - High position RR (>1.5) with low internal alignment RR (<1.2)
  - Case 3 dominance over Case 1 in small models
  - Polarity gap (Acc_positive − Acc_negative > 5%)

- First 3 experiments:
  1. Position bias baseline: Run 100 random pairs with both orderings; compute RRO per model
  2. BOS sanity check: For one attribute, verify 50/50 split for each feature and orthogonality across features
  3. Meta-predictor vs numerical: Train bias-only meta-predictor; compare its accuracy to numerical-prediction accuracy for Qwen3-1.7B and Qwen3-32B

## Open Questions the Paper Calls Out

### Open Question 1
Can mechanistic interpretability methods reveal the internal neural mechanisms by which LLMs switch between numerical reasoning and heuristic-based strategies? The paper notes they have not attempted mechanistic interpretation and it would be interesting to study whether updating numerical knowledge inside models would alter pairwise judgments.

### Open Question 2
How does the number and type of few-shot examples affect the balance between numerical reasoning and heuristic reliance in entity comparison tasks? Preliminary experiments suggested few-shot prompting may help overcome biases but not entirely.

### Open Question 3
Does fine-tuning on ranking tasks eliminate surface-level heuristic biases, or do these biases persist and resurface in modified forms? The paper suggests it would be interesting to study whether biases persist after fine-tuning on ranking tasks.

### Open Question 4
How do the identified heuristic biases generalize to non-numerical attributes, multi-entity ranking, and multi-hop reasoning scenarios? The authors acknowledge findings may not fully transfer to these broader contexts.

## Limitations

- The study uses only zero-shot inference without fine-tuning, which may not represent all LLM deployment scenarios
- The BOS construction discards the majority of cases to achieve statistical balance, potentially limiting generalizability
- The logistic regression meta-predictor's superior performance for small models assumes it captures the dominant decision policy, but residual variance could reflect unmodeled cues
- The study focuses on pairwise comparisons of numerical attributes, leaving unclear whether biases operate similarly in qualitative comparisons or compositional reasoning

## Confidence

**High Confidence:** The existence of knowledge-use gaps across all model sizes is well-supported by multiple metrics and visual evidence. The dominance of position bias as a surface cue is consistently observed.

**Medium Confidence:** The differential behavior between small and large models is supported by statistical evidence but relies on interpreting correlations as strategic decisions. CoT effectiveness is observed but the mechanism remains uncertain.

**Low Confidence:** The exact thresholds determining when larger models switch strategies are not precisely specified, and the BOS subset may not fully represent real-world distributions.

## Next Checks

1. **Randomized Order Swap Test:** For 100 random entity pairs, run both orderings (A vs B and B vs A) and compute the position risk ratio. Verify position bias reaches RR≈2.0 for small models and is reduced for large models.

2. **BOS Construction Verification:** For one attribute (e.g., rivers), verify that the BOS subset contains equal counts across all 16 (Popularity, Order, Co-occurrence, Internal alignment) cells and that each feature shows 50/50 balance.

3. **Meta-predictor vs Numerical Accuracy Head-to-Head:** Train the bias-only logistic regression meta-predictor on small models (1B-8B) and compare its accuracy directly against the model's own numerical prediction accuracy across the same entity pairs.