---
ver: rpa2
title: 'ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling'
arxiv_id: '2510.27610'
source_url: https://arxiv.org/abs/2510.27610
tags:
- modeling
- problem
- equivalence
- optimization
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating Large Language
  Models (LLMs) on optimization modeling tasks. The key difficulty is that equivalent
  optimization models can have different structures, making it hard to assess correctness.
---

# ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling

## Quick Facts
- **arXiv ID**: 2510.27610
- **Source URL**: https://arxiv.org/abs/2510.27610
- **Reference count**: 40
- **Key outcome**: Graph-theoretic framework ORGEval achieves 100% consistency in LLM optimization modeling evaluation vs 94.11% for solver-based methods, running in seconds instead of minutes

## Executive Summary
This paper addresses the challenge of evaluating Large Language Models (LLMs) on optimization modeling tasks where equivalent optimization models can have different structures. Traditional solver-based evaluation methods are inconsistent, computationally expensive, and fail when problems are infeasible. The authors propose ORGEval, a graph-theoretic framework that reduces model equivalence detection to graph isomorphism testing. By representing optimization models as weighted bipartite graphs and applying the Weisfeiler-Lehman test under a symmetric decomposable condition, ORGEval provides fast, consistent evaluation. Experiments show ORGEval runs in seconds and achieves 100% consistency across random parameter configurations, significantly outperforming solver-based methods.

## Method Summary
ORGEval transforms linear and mixed-integer linear programs into weighted bipartite graphs where constraint and variable nodes form two partitions connected by coefficient-weighted edges. Model equivalence detection becomes graph isomorphism testing using the Weisfeiler-Lehman (WL) test, but only under the sufficient condition of symmetric decomposable (SD) graphs. The framework first checks if graphs are SD using Algorithm 3, then applies the WL-test (Algorithm 2). If both graphs are SD and have identical color distributions, they are guaranteed isomorphic (Theorem 3.1). Evaluation uses model-data separation: the model M maps parameters θ to instances, and equivalence is tested across multiple random parameter configurations to ensure structural correctness rather than instance-specific outcomes.

## Key Results
- ORGEval achieves 100% consistency across 5 random parameter configurations vs 94.11% for solver-based methods
- Evaluation runs in seconds compared to minutes for solver-based approaches
- ORGEval correctly identifies model equivalence even when solver-based methods fail due to infeasibility (>60% failure rate)
- DeepSeek-V3 and Claude-Opus-4 achieve highest accuracy on Bench4Opt benchmark
- ORGEval handles infeasible models that solver-based methods cannot evaluate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transforming optimization models into bipartite graphs enables equivalence detection through structural comparison, bypassing solver limitations (inconsistency, infeasibility failures, computational cost).
- **Mechanism:** MILP/LP instances are converted to weighted bipartite graphs where constraint nodes (V) and variable nodes (W) form two partitions. Edges carry coefficient weights Aij, and nodes store features (bi, ◦i for constraints; cj, τj for variables). This transforms semantic equivalence into graph isomorphism testing.
- **Core assumption:** Assumes structural isomorphism (permutation-invariance under variable/constraint reordering) captures practical equivalence. This is stricter than solution-equivalence—models with identical optima but different structures (e.g., with slack variables) are considered non-equivalent.
- **Evidence anchors:**
  - [abstract]: "ORGEval represents optimization models as graphs, reducing equivalence detection to graph isomorphism testing"
  - [Section 3.1, Definition 7]: Formal specification of weighted bipartite graph representation with vertex features
  - [corpus]: Limited direct support; related work on graph-based OR (GNNs for branching) uses similar representations but for solving, not evaluation
- **Break condition:** When optimization models are solution-equivalent but not isomorphic (e.g., reformulations with auxiliary variables, slack variables), the method incorrectly classifies them as different.

### Mechanism 2
- **Claim:** The Weisfeiler-Lehman (WL) test reliably detects isomorphism when graphs satisfy the symmetric decomposable (SD) condition, providing polynomial-time guarantee.
- **Mechanism:** SD graphs partition into k disconnected subgraphs where each subgraph has uniquely colored nodes under WL coloring. Algorithm 1 first checks SD condition via Algorithm 3, then applies WL-test (Algorithm 2). If both graphs are SD and have identical color distributions, they are guaranteed isomorphic (Theorem 3.1).
- **Core assumption:** Assumes practical optimization problems satisfy SD condition under random parameter sampling. Theorems C.5-C.6 prove this holds almost surely when parameter distributions have independent continuous coordinates.
- **Evidence anchors:**
  - [abstract]: "We identify and prove a sufficient condition...under which the Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism"
  - [Theorem 3.1]: "Suppose P1, P2 are symmetric decomposable, then G1 and G2 shares the same coloring distribution after WL-test coloring ⟺ P1 ∼ P2"
  - [corpus]: No corpus papers address SD conditions or WL-test theoretical guarantees for optimization graphs
- **Break condition:** Non-SD graphs (e.g., problems with inherent symmetries like bin-packing with identical bins) may produce false positives—identical WL colorings for non-isomorphic graphs (counterexamples in Cai et al., 1992).

### Mechanism 3
- **Claim:** Model-data separation enables consistent evaluation across parameter configurations by testing structural equivalence rather than instance-level outcomes.
- **Mechanism:** Models M are treated as mappings from parameter space Θ to instances P(θ). Evaluation samples multiple θ values and tests isomorphism between test and reference instances. Since isomorphism is parameter-invariant for structurally equivalent models, results are consistent regardless of numerical values.
- **Core assumption:** Assumes parameter support Θ is correctly specified and random sampling covers representative configurations. Also assumes ground-truth models correctly capture problem semantics.
- **Evidence anchors:**
  - [abstract]: "By focusing on structural equivalence rather than instance-level configurations, ORGEval is robust to numerical variations"
  - [Table 2]: Shows 100% ORGEval consistency vs. 94.11% solver consistency across 5 random configurations; solver fails on >60% of models due to infeasibility
  - [corpus: Auto-Formulating DP Problems]: Mentions data-model separation challenges but doesn't address consistency issues in evaluation
- **Break condition:** When parameter support is misspecified or ground-truth model doesn't reflect true problem intent, consistent evaluation gives consistently wrong answers.

## Foundational Learning

- **Concept: Weisfeiler-Lehman Test for Graph Isomorphism**
  - **Why needed:** ORGEval's theoretical guarantee depends on understanding WL-test limitations and when SD condition overcomes them.
  - **Quick check:** Two non-isomorphic graphs produce identical WL color distributions after k iterations. Does this mean WL-test failed? (Yes—it's a false positive. SD condition prevents this for optimization graphs.)

- **Concept: Bipartite Graph Representation of Linear Programs**
  - **Why needed:** Interpreting evaluation results requires understanding how constraint-variable relationships map to graph structure.
  - **Quick check:** Two constraint nodes have identical WL colors after convergence. What does this imply? (Identical coefficient patterns and right-hand sides—potentially redundant constraints.)

- **Concept: Model-Data Separation in Operations Research**
  - **Why needed:** The evaluation framework assumes familiarity with how AMPL/Pyomo separate .mod (model) from .dat (data).
  - **Quick check:** Why does separating model from data enable more robust evaluation? (Tests structural correctness across multiple configurations, not just single-instance outcomes.)

## Architecture Onboarding

- **Component map:** LLM output (Gurobi code) → Compile check → Parameter instantiation → Graph encoder (bipartite) → WL-test engine (Algorithm 2) → SD detector (Algorithm 3) → Equivalence classifier (color comparison)

- **Critical path:**
  1. Parse LLM-generated code + parameter file → MILP instance
  2. Transform instance → weighted bipartite graph (Definition 7)
  3. Run WL-test → color distribution multiset
  4. Check SD condition (Algorithm 3)
  5. Compare test vs. reference color distributions

- **Design tradeoffs:**
  - **SD condition vs. completeness**: Non-SD problems auto-fail equivalence check (conservative but guaranteed correct)
  - **Strict isomorphism vs. solution-equivalence**: Rejects mathematically equivalent reformulations (e.g., slack variables)
  - **Random sampling vs. exhaustive**: 5 configurations provide practical coverage but not theoretical completeness

- **Failure signatures:**
  - Compile error: LLM generates syntactically invalid code (2-7% rates observed)
  - Non-SD test instance: Algorithm returns "not equivalent" without isomorphism check
  - Color distribution mismatch: Structural difference detected

- **First 3 experiments:**
  1. **Implement SD detection**: Code Algorithm 3, test on bin-packing instance (Figure 10). Verify graph decomposes into disconnected subgraphs.
  2. **Validate consistency**: Run ORGEval on 10 Bench4Opt problems × 5 random configurations. Confirm 100% consistency claim.
  3. **Boundary test**: Construct LP where adding slack variable preserves optimal solution but creates non-isomorphic graph. Verify correct "not equivalent" classification despite solution-equivalence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the graph-theoretic evaluation framework be extended to nonlinear optimization problems (e.g., quadratic, conic, or general nonlinear programs), and what graph representations would preserve structural equivalence in those settings?
- **Basis in paper:** [explicit] The paper states it focuses on "linear and mixed-integer linear programs—the dominant classes in practical OR applications" and does not address nonlinear formulations.
- **Why unresolved:** Nonlinear constraints and objectives introduce dependencies between variables that cannot be captured by the current bipartite graph representation with linear coefficient edges.
- **What evidence would resolve it:** A proposed graph construction for nonlinear programs with proofs showing equivalence detection maps correctly to model isomorphism, plus experimental validation on a nonlinear benchmark.

### Open Question 2
- **Question:** Why do advanced reasoning models (o1, o3, deepseek-r1) underperform compared to non-reasoning models (DeepSeek-V3, Claude-Opus-4) on optimization modeling, and can this degradation be mitigated?
- **Basis in paper:** [explicit] The authors observe "reasoning models such as deepseek-r1, o1, and o3 consistently exhibited lower accuracy compared to the base model" and hypothesize "hallucination propagation" as a potential cause.
- **Why unresolved:** The paper provides only a conjecture without empirical investigation into the failure modes or mechanisms causing reasoning models to accumulate errors during multi-step formulation.
- **What evidence would resolve it:** Ablation studies tracing intermediate reasoning steps, error analysis categorizing failure types, or modified prompting strategies that prevent cascading errors in reasoning models.

### Open Question 3
- **Question:** How should ORGEval handle models that do not satisfy the symmetric decomposable condition, and what is the failure rate of the WL-test on such instances?
- **Basis in paper:** [inferred] The paper guarantees correctness only for symmetric decomposable graphs; although Bench4Opt instances satisfy this condition by construction, real-world LLM outputs may not.
- **Why unresolved:** The methodological limitation section does not quantify how often non-symmetric-decomposable graphs arise in practice or propose fallback evaluation strategies.
- **What evidence would resolve it:** Empirical analysis of LLM-generated models measuring the frequency of non-SD cases, and development of alternative equivalence tests or approximation bounds for those scenarios.

## Limitations

- **Dataset availability**: Bench4Opt dataset is not publicly available, making independent validation difficult
- **Prompt engineering dependency**: Significant variation in LLM performance based on prompting strategies (structured vs. unstructured descriptions) without standardized protocols
- **Non-Symmetric Decomposable problems**: The theoretical guarantee relies on SD condition, but practical optimization problems may not always satisfy this property

## Confidence

- **High Confidence**: The graph-theoretic framework (bipartite representation, WL-test) is mathematically sound and well-defined. The consistency improvements over solver-based evaluation are clearly demonstrated.
- **Medium Confidence**: The SD condition theoretical guarantee is valid, but its practical applicability across diverse optimization problems needs broader validation. The equivalence detection mechanism is reliable when conditions are met.
- **Low Confidence**: The LLM benchmark results depend heavily on prompt engineering choices that are not fully specified. Without standardized prompting protocols, the relative performance rankings may not be robust.

## Next Checks

1. **SD Condition Prevalence**: Test ORGEval on a diverse set of real-world optimization problems (beyond the Bench4Opt subset) to quantify how often the SD condition fails and assess the impact on evaluation coverage.

2. **Prompt Sensitivity Analysis**: Systematically vary prompting strategies for a fixed LLM model and measure how ranking stability changes. This would reveal whether observed performance differences reflect model capability or prompt quality.

3. **Cross-Dataset Validation**: Apply ORGEval to an independently collected optimization modeling dataset to verify that the 100% consistency claim holds beyond the Bench4Opt corpus.