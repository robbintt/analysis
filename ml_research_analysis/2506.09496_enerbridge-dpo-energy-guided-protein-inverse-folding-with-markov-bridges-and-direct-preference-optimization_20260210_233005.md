---
ver: rpa2
title: 'EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges
  and Direct Preference Optimization'
arxiv_id: '2506.09496'
source_url: https://arxiv.org/abs/2506.09496
tags:
- protein
- energy
- sequence
- sequences
- enerbridge-dpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of designing energetically stable
  protein sequences in inverse folding, a task often neglected by existing deep learning
  methods focused on sequence recovery. The authors introduce EnerBridge-DPO, a novel
  framework that integrates Markov bridges with Direct Preference Optimization (DPO)
  to directly generate low-energy, high-stability protein sequences.
---

# EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization

## Quick Facts
- arXiv ID: 2506.09496
- Source URL: https://arxiv.org/abs/2506.09496
- Reference count: 40
- Primary result: Introduces EnerBridge-DPO, a framework that uses Markov bridges and Direct Preference Optimization to generate low-energy, high-stability protein sequences with improved performance on protein complexes.

## Executive Summary
EnerBridge-DPO addresses the challenge of designing energetically stable protein sequences in inverse folding, a task often neglected by existing deep learning methods focused on sequence recovery. The framework integrates Markov Bridges with Direct Preference Optimization (DPO), using energy-based preferences to fine-tune a model that starts from an informative structure-derived sequence prior. An explicit energy constraint loss enables the model to learn and predict quantitative energy features. Experiments demonstrate that EnerBridge-DPO outperforms existing baselines in designing sequences with lower predicted energies for protein complexes while maintaining comparable sequence recovery rates.

## Method Summary
The method involves two stages: first, pre-training a Markov Bridge model using a structure encoder (PiFold) to generate a prior sequence from backbone structure; second, fine-tuning with Direct Preference Optimization using energy-based preference pairs and an explicit energy constraint loss. The model uses an ESM backbone with structural adapters and predicts ΔΔG values through an auxiliary head. Training is performed on MPNN, BindingGym, and SKEMPI datasets using NVIDIA RTX 4090 GPUs.

## Key Results
- Outperforms existing baselines in designing sequences with lower predicted energies for protein complexes
- Maintains comparable sequence recovery rates while optimizing for stability
- Accurately predicts ΔΔG values with high correlation to computational energy oracles

## Why This Works (Mechanism)

### Mechanism 1: Structure-Conditioned Prior Initialization
Initializing with a structure-derived sequence prior reduces search space complexity compared to noise-based initialization. The model uses a structure encoder to map backbone structure to a discrete sequence, providing a strong sequence prior that requires fewer refinement steps.

### Mechanism 2: Energy-Guided Distribution Alignment (DPO)
DPO shifts generative probability toward low-energy states without requiring differentiable energy functions during inference. Preference pairs are constructed where one sequence has lower measured energy, teaching the model the gradient of stability.

### Mechanism 3: Explicit Energy Quantification via Auxiliary Loss
An auxiliary regression loss for energy prediction forces latent representations to encode quantitative biophysical constraints. This prevents the model from merely learning ranking heuristics and anchors preference space to physical units.

## Foundational Learning

- **Concept: Inverse Folding vs. Energy Minimization**
  - Why needed: Standard inverse folding maximizes recovery; EnerBridge-DPO argues recovery ≠ stability
  - Quick check: If a generated sequence has 0% similarity to native but significantly lower predicted energy, is it a success? (Yes, if it folds to target structure)

- **Concept: Discrete Markov Bridges**
  - Why needed: Protein sequences are discrete; Markov Bridges model transitions between distributions using transition matrices
  - Quick check: In EnerBridge-DPO, what is z₀? (An information-rich sequence prior X derived from the structure encoder)

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: Bypasses complexity of Reinforcement Learning by directly comparing likelihood of winning vs losing sequences
  - Quick check: What serves as "Human Feedback" in EnerBridge-DPO's DPO? (Calculated physical energy values from FoldX/Rosetta)

## Architecture Onboarding

- **Component map:** Structure Encoder (E) → Prior Sequence (X) → Markov Bridge Backbone → Reference Model (φ_ref) → Energy Head
- **Critical path:** Backbone Input → Structure Encoder (Stage 1) → Markov Bridge Pre-training (Stage 1) → DPO Fine-tuning w/ Energy Loss (Stage 2)
- **Design tradeoffs:**
  - Recovery vs. Stability: Aggressive DPO fine-tuning lowers energy but may drop recovery rates
  - Pair Sampling: Top/bottom 10% mutants provide strong signal but ignore middle ground of energy landscape
  - Reference Model: Must keep φ_ref frozen; updating invalidates DPO math
- **Failure signatures:**
  - Catastrophic Forgetting: Sequence recovery drops significantly during Stage 2
  - Reward Hacking: Generated sequences have extremely low predicted energy but high actual energy
  - Bridge Collapse: Model generates same sequence regardless of input structure
- **First 3 experiments:**
  1. Sanity Check - Stage 1: Run Markov Bridge without fine-tuning; does it recover native sequence?
  2. Ablation - DPO vs. Energy Loss: Train variants with only DPO or only Energy Regression
  3. Generalization Test: Evaluate on held-out protein complex; compare generated sequence energy against native

## Open Questions the Paper Calls Out

- Does EnerBridge-DPO generalize to diverse protein families and complex structural scenarios not represented in current datasets?
- Does optimizing the sampling strategy for preference pairs improve per-structure ΔΔG prediction accuracy?
- How can the diversity and quality of DPO preference pairs be systematically ensured to enhance optimization?

## Limitations

- Model-specific energy oracle dependence: Relies on FoldX/Rosetta predictions rather than experimental measurements
- Energy head generalization gap: Strong correlation with FoldX but unclear if learns generalizable biophysical understanding
- Complex structure limitations: Optimized for specific input structure; no evidence of generalization to conformational ensembles

## Confidence

- **High Confidence**: Markov Bridge + Structure Encoder mechanism is clearly described and supported by ablation
- **Medium Confidence**: DPO fine-tuning mechanism described but MSE substitution for likelihood ratios requires careful implementation
- **Low Confidence**: ΔΔG prediction accuracy based solely on correlation with computational oracles, not experimental data

## Next Checks

1. **Energy Oracle Cross-Validation**: Generate sequences for held-out complex; calculate predicted energy using both FoldX and Rosetta; check ranking consistency

2. **Structural Fidelity Under Energy Pressure**: Compare molecular dynamics stability of high-recovery native sequence vs low-recovery EnerBridge sequence for same structure

3. **DPO vs. Pure Energy Regression Ablation**: Train models with only energy regression loss vs only DPO; compare performance on energy reduction and ΔΔG prediction