---
ver: rpa2
title: 'UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice'
arxiv_id: '2509.21144'
source_url: https://arxiv.org/abs/2509.21144
tags:
- speech
- translation
- uniss
- s2st
- expressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UniSS introduces a unified single-stage architecture for expressive
  speech-to-speech translation that preserves voice, emotion, and duration consistency.
  The method leverages pre-trained LLMs and transfers their text translation capabilities
  to speech through cross-modal chain-of-thought prompting, eliminating the architectural
  complexity of prior approaches.
---

# UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice

## Quick Facts
- arXiv ID: 2509.21144
- Source URL: https://arxiv.org/abs/2509.21144
- Reference count: 27
- Primary result: State-of-the-art speech-to-speech translation with voice preservation and near-perfect duration consistency

## Executive Summary
UniSS introduces a unified single-stage architecture for expressive speech-to-speech translation that preserves voice, emotion, and duration consistency. The method leverages pre-trained LLMs and transfers their text translation capabilities to speech through cross-modal chain-of-thought prompting, eliminating the architectural complexity of prior approaches. Experiments show UniSS achieves state-of-the-art translation fidelity (32.20 Speech-BLEU on EN-ZH, 24.28 on ZH-EN), superior voice preservation (4.42 speaker similarity MOS), and near-perfect duration consistency (SLC-0.2: 0.98-0.99). The framework also provides flexible quality-efficiency trade-offs and is trained on a newly constructed 44.8k-hour UniST dataset.

## Method Summary
UniSS extends pre-trained text LLMs to handle speech by introducing a cross-modal chain-of-thought prompting mechanism that progressively aligns audio semantics with text. The model uses three distinct tokenizers: a speaker tokenizer for voice preservation, a linguistic tokenizer for content understanding, and a semantic tokenizer for waveform reconstruction. A three-phase progressive training approach first aligns speech and text, then introduces S2ST with CoT prompting, and finally refines with high-quality data. The unified architecture enables quality-performance trade-offs through different inference modes while maintaining a single-stage model.

## Key Results
- Achieves state-of-the-art translation fidelity with 32.20 Speech-BLEU on EN-ZH and 24.28 on ZH-EN
- Preserves speaker identity with 4.42 MOS speaker similarity and maintains near-perfect duration consistency (SLC-0.2: 0.98-0.99)
- Provides flexible quality-efficiency trade-offs through Quality Mode (+1.84 BLEU) and Performance Mode (7% faster inference)
- Trained on a newly constructed 44.8k-hour UniST dataset using a three-phase progressive training strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal chain-of-thought (CoT) prompting transfers pre-trained text translation capabilities from LLMs to the speech domain.
- Mechanism: The model decomposes S2ST into explicit "listen" (transcribe source), "translate" (generate target text), and "speak" (generate target semantic tokens) steps within a single autoregressive pass. This forces intermediate text reasoning, leveraging the LLM's existing translation knowledge.
- Core assumption: The LLM's text translation abilities are transferable to speech when text appears as intermediate reasoning steps.
- Evidence anchors:
  - [abstract] "To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text."
  - [section 5.6.3] Direct S2ST without CoT causes -14.94 (EN-ZH) and -14.40 (ZH-EN) Speech-BLEU degradation.
  - [corpus] Related work (PROST-LLM, Scheduled Interleaved) similarly uses progressive/interleaved text-speech training for modality adaptation, but corpus does not directly validate CoT-specific transfer.
- Break condition: If intermediate text generation is bypassed (Direct S2ST mode), translation fidelity collapses dramatically.

### Mechanism 2
- Claim: Separating speech into three token types—speaker tokens (global style), linguistic tokens (content understanding), and semantic tokens (generation)—optimizes both comprehension and synthesis.
- Mechanism: Speaker tokenizer (BiCodec global encoder) captures timbre/emotion in 32 fixed tokens; linguistic tokenizer (GLM-4, quantized Whisper) encodes content at 12.5 tokens/sec for robust understanding; semantic tokenizer (BiCodec) captures generation-relevant information at 50 tokens/sec. Different tokenizers for understanding vs. generation prevent information overload.
- Core assumption: Content-focused tokenizers are better for comprehension; semantic tokenizers (with acoustic detail) are better for synthesis.
- Evidence anchors:
  - [section 3.1.2] "Our preliminary experiments revealed that although BiCodec's semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding."
  - [section 5.6.2] Replacing GLM-4 linguistic tokenizer with BiCodec semantic tokens causes -15.01 (EN-ZH) and -8.73 (ZH-EN) BLEU drop.
  - [corpus] Corpus papers on expressive S2ST emphasize feature disentanglement (TransVIP) and separate semantic/acoustic modeling, supporting the general principle but not this specific tokenizer combination.
- Break condition: Using only semantic tokens for both understanding and generation degrades translation accuracy.

### Mechanism 3
- Claim: Three-phase progressive training is essential for cross-modal alignment while preventing catastrophic forgetting of text translation.
- Mechanism: Phase 1 aligns speech-text via ASR/TTS/S2TT/MT tasks; Phase 2 introduces CoT-based S2ST; Phase 3 refines with high-quality data and annealed learning rate. MT task in Phase 1 explicitly preserves text translation capability.
- Core assumption: Direct S2ST training without prior speech-text alignment causes severe capability loss.
- Evidence anchors:
  - [section 3.3] "Phase 1 endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities."
  - [section 5.6.1] Removing Phase 1 ("UniST only") causes -7.18 (EN-ZH) and -10.15 (ZH-EN) BLEU drop.
  - [corpus] PROST-LLM and Scheduled Interleaved papers similarly advocate progressive training for LLM-to-speech adaptation, providing convergent evidence.
- Break condition: Training only on S2ST data without Phase 1 alignment causes catastrophic performance degradation.

## Foundational Learning

- **Autoregressive Language Modeling (Next-Token Prediction)**
  - Why needed here: UniSS extends text LLMs to handle speech tokens via the same AR objective (Eq. 1). Understanding how LLMs learn sequential dependencies is foundational.
  - Quick check question: Can you explain why expanding the vocabulary to include speech tokens allows unified text-speech modeling without architecture changes?

- **Discrete Speech Units / Tokenization**
  - Why needed here: The triple-tokenizer strategy converts continuous audio into discrete tokens that LLMs can process. Understanding quantization (VQ-VAE, codec-based approaches) clarifies why different tokenizers serve different purposes.
  - Quick check question: Why might self-supervised semantic tokens work well for reconstruction but poorly for content understanding?

- **Chain-of-Thought Prompting**
  - Why needed here: Cross-modal CoT is the core innovation for capability transfer. Understanding standard CoT in text LLMs helps explain why intermediate reasoning steps improve cross-modal tasks.
  - Quick check question: How does forcing intermediate text generation differ from end-to-end speech-to-speech mapping?

## Architecture Onboarding

- **Component map:**
  Input: Source waveform → Speaker tokenizer (32 tokens) + Linguistic tokenizer (variable, 12.5 tok/s)
  LLM backbone: Qwen2.5-1.5B-Instruct with expanded vocabulary (180,407 tokens)
  Output: Semantic tokens (50 tok/s) → BiCodec decoder → Reconstructed waveform
  Control tokens: Task mode (Quality/Performance), target language, speed ratio

- **Critical path:**
  1. Source audio → Parallel tokenization (speaker + linguistic)
  2. Concatenated prompt enters LLM with control tokens
  3. Quality Mode: LLM generates [source transcription → target text → target semantic tokens]
  4. Performance Mode: LLM generates [target text → target semantic tokens]
  5. Decoder reconstructs audio from [source speaker tokens + target semantic tokens]

- **Design tradeoffs:**
  - Quality Mode vs Performance Mode: +1.84 BLEU vs 7% faster inference (Table 3)
  - UniST General (44.8k hrs) vs High-Quality (19.8k hrs): Diversity vs temporal consistency
  - Model size: 1.5B vs 0.5B (Small) trades BLEU for 25% speedup

- **Failure signatures:**
  - Direct S2ST without CoT: ~15 BLEU point drop (Table 4)
  - Missing Phase 1 alignment: ~10 BLEU point drop (Table 4)
  - Single tokenizer for understanding+generation: ~15 BLEU point drop (Table 4)
  - Vocabulary not expanded for speech tokens: Model cannot process/generate speech

- **First 3 experiments:**
  1. **Tokenizer ablation:** Train with only semantic tokens (no separate linguistic tokenizer) on a small subset. Verify content understanding degrades even if reconstruction remains plausible.
  2. **CoT必要性:** Run inference in Direct S2ST mode vs Quality Mode on same inputs. Compare BLEU and observe where translation errors cluster (semantic drift vs acoustic artifacts).
  3. **Phase dependency test:** Train Phase 2→3 only (skip Phase 1) on limited data. Measure text translation BLEU on held-out MT task to confirm catastrophic forgetting of original LLM capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling beyond 1.5B parameters is untested, potentially revealing attention/memory bottlenecks
- Cross-lingual robustness is limited to EN-ZH/ZH-EN pairs without validation on other language families
- Out-of-domain generalization to emotional storytelling or spontaneous conversation is uncharacterized

## Confidence
- Cross-modal CoT transfer mechanism: **High confidence** - Strong ablation evidence and convergent literature support
- Triple-tokenizer strategy: **High confidence** - Significant BLEU degradation when modified, with clear theoretical justification
- Three-phase progressive training: **Medium confidence** - Important for performance but specific phase ordering could potentially be optimized
- State-of-the-art translation fidelity claims: **Medium confidence** - Based on reported comparisons but full statistical details not provided
- Voice preservation quality: **Medium confidence** - High MOS reported but evaluation protocol details are limited

## Next Checks
1. **Cross-lingual scaling experiment**: Evaluate UniSS on a diverse multilingual S2ST benchmark (e.g., CoVoST 2 with multiple language pairs spanning different families). Measure whether the CoT mechanism and tokenizer strategy generalize or require language-specific tuning. Key metric: relative BLEU drop compared to strong cascaded baselines across language pairs.

2. **Zero-shot speaker preservation test**: Create a test set with speakers completely unseen during training (different demographics, recording conditions). Measure speaker similarity MOS and voice distinctiveness preservation. Key metric: degradation in speaker similarity compared to seen speakers.

3. **Computational overhead quantification**: Measure the energy/compute cost of running three separate tokenizers in parallel versus single-tokenizer approaches, including both training and inference phases. Key metric: relative FLOPs and latency compared to baseline approaches.