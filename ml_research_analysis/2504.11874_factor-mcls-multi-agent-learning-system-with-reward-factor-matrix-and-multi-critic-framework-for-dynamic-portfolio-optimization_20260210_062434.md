---
ver: rpa2
title: 'Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic
  framework for dynamic portfolio optimization'
arxiv_id: '2504.11874'
source_url: https://arxiv.org/abs/2504.11874
tags:
- portfolio
- training
- risk
- trading
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in existing deep reinforcement
  learning (DRL) agents for dynamic portfolio optimization, where agents struggle
  to learn the complex factors influencing portfolio returns and risks, and investors
  cannot intervene based on their risk preferences. The authors propose a novel reward
  factor matrix to represent the return, risk, and transaction scale factors of each
  asset, and a multi-critic framework learning system called Factor-MCLS to comprehensively
  learn these factors.
---

# Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization

## Quick Facts
- arXiv ID: 2504.11874
- Source URL: https://arxiv.org/abs/2504.11874
- Authors: Ruoyu Sun; Angelos Stefanidis; Zhengyong Jiang; Jionglong Su
- Reference count: 0
- One-line primary result: Factor-MCLS achieves 35.3% higher profitability and 63.9% higher return per unit of risk compared to benchmarks

## Executive Summary
This paper addresses fundamental limitations in deep reinforcement learning (DRL) agents for dynamic portfolio optimization, where agents struggle to learn complex return and risk factors, and investors cannot intervene based on risk preferences. The authors propose a novel reward factor matrix that decomposes portfolio rewards into interpretable return, variance, covariance, and transaction scale factors for each asset. Combined with a multi-critic framework and risk constraint term, Factor-MCLS enables comprehensive learning of these factors while allowing investor-specified risk intervention during training.

## Method Summary
Factor-MCLS is a multi-agent DRL system that uses a reward factor matrix to decompose portfolio rewards into interpretable components and a multi-critic framework to learn these factors comprehensively. The system includes an auxiliary agent (BDA) that provides baseline portfolio weights, which the main DRL agent uses along with price history to learn optimal trading policies. A risk constraint term in the actor's objective allows investors to specify risk aversion levels, enabling intervention during training.

## Key Results
- Factor-MCLS significantly improves profitability and risk control during training
- Achieves at least 35.3% higher profitability and 63.9% higher return per unit of risk compared to benchmark strategies in back-testing
- Successfully learns interpretable factor contributions for return and risk across 29 Dow Jones stocks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the reward function into interpretable factor vectors enables more thorough learning of return and risk contributions per asset.
- Mechanism: The reward factor matrix $RF^{(t)}$ breaks the scalar reward into four factor vectors—return ($F_R$), variance ($F_V$), covariance ($F_C$), and transaction scale ($F_T$)—each as an $n \times 1$ vector. This decomposition allows critic networks to learn asset-specific contributions rather than aggregate scalar signals.
- Core assumption: Asset-level factor contributions are learnable and generalize across trading periods.
- Evidence anchors:
  - [abstract]: "we propose a reward factor matrix for elucidating the return and risk of each asset in the portfolio"
  - [section 4.2]: Equations 6-9 define each factor vector explicitly
  - [corpus]: Related paper "Deep Reinforcement Learning in Factor Investment" addresses factor-based approaches but focuses on factor portfolio construction rather than reward decomposition

### Mechanism 2
- Claim: Multi-critic framework with asset-level Q-value outputs enables direct gradient propagation for each factor type.
- Mechanism: Four separate critic networks ($Q^{(R)}$, $Q^{(V)}$, $Q^{(C)}$, $Q^{(T)}$) each output $n \times 1$ vectors rather than scalars. The actor's loss (Eq. 13) combines all four critics with calibrated weights, propagating separate gradients for return maximization, variance/covariance risk control, and transaction cost awareness.
- Core assumption: Independent critic networks can learn their respective factors without interference.
- Evidence anchors:
  - [abstract]: "multi-critic framework that facilitates learning of the reward factor matrix"
  - [section 4.3]: Defines four action-value functions with vector outputs
  - [corpus]: "MARS" paper uses meta-controlled agents for risk-aware RL but with single-critic structure

### Mechanism 3
- Claim: Risk constraint term in the actor's objective function enables investor-specified risk aversion intervention.
- Mechanism: The constraint $\Pi(\lambda)$ (Eq. 12) uses SmoothL1 loss to penalize when risk-adjusted return per asset becomes negative. The vector $\lambda \in \mathbb{R}^{n \times 1}$ encodes investor risk aversion per asset, constraining exploration toward positive risk-adjusted return regions.
- Core assumption: Investors can meaningfully specify risk aversion values before training.
- Evidence anchors:
  - [abstract]: "allows investors to intervene in the training of the DRL agent according to their individual levels of risk aversion"
  - [section 4.4]: Equation 12 and 13 define the risk constraint and actor loss
  - [corpus]: Weak direct evidence; related multi-agent portfolio papers (e.g., "DeltaHedge") focus on hedging rather than investor intervention

## Foundational Learning

- Concept: **Actor-Critic with Deterministic Policy Gradient (DDPG)**
  - Why needed here: Factor-MCLS builds on DDPG for continuous action spaces (portfolio weights) and requires understanding how critic networks estimate Q-values for policy gradient updates.
  - Quick check question: Can you explain why DDPG uses target networks and replay buffers for off-policy learning?

- Concept: **Portfolio Risk Decomposition (Variance-Covariance)**
  - Why needed here: The reward factor matrix explicitly separates variance and covariance contributions; understanding how portfolio variance decomposes into individual asset contributions is essential.
  - Quick check question: Given portfolio weights $w$ and covariance matrix $\Sigma$, can you derive each asset's contribution to $\sigma_p^2 = w^T\Sigma w$?

- Concept: **Hierarchical Deep Reinforcement Learning (HDRL)**
  - Why needed here: Factor-MCLS uses an auxiliary agent (BDA) to provide baseline portfolio weights, then trains the main agent conditioned on these baselines.
  - Quick check question: How does decomposing a task into sub-problems (auxiliary baseline + main policy) affect exploration efficiency?

## Architecture Onboarding

- Component map:
  - Auxiliary Agent (BDA) -> State Module -> Main DRL Agent -> Environment -> Reward Factor Matrix Module -> Multi-critic Networks -> Risk Constraint Module -> Replay Buffer

- Critical path:
  1. State $s_t = \{X^{(t)}, \mu^{(t)}_{BL}\}$ assembled from price history and auxiliary agent output.
  2. Actor produces action $a_t = \omega^{(t)}$ (target portfolio weights).
  3. Environment returns scalar reward $r_t$ and reward factor matrix $RF^{(t)}$.
  4. Four critics compute Q-value vectors for each factor; loss computed via SmoothL1 against target Q-values.
  5. Actor updated using combined loss from all critics plus risk constraint term.

- Design tradeoffs:
  - **Multi-critic vs. single-critic**: More interpretable gradients but 4× memory/computation for critic networks.
  - **Constraint weight $\lambda_3$ (Eq. 13)**: Higher values enforce stricter risk control but may limit return maximization.
  - **Trading period length $\tau$**: Longer periods reduce noise (per Li et al., 2023) but decrease rebalancing frequency.

- Failure signatures:
  - **Gradient explosion in critics**: Check critic loss trajectories; should converge smoothly (Fig. 5b-e show expected behavior).
  - **Risk constraint not converging**: $\Pi(\lambda)$ should trend toward zero; persistent positive values indicate constraint violation.
  - **Accumulated variance increasing**: AV(tra) (Fig. 4c) should stabilize; doubling suggests risk control failure.
  - **Corpus warning**: Related work notes DRL policies can fail under regime shifts; validate on held-out market conditions.

- First 3 experiments:
  1. **Single-asset sanity check**: Run Factor-MCLS on a 2-asset portfolio (one risky, one cash) with known optimal Kelly weights. Verify critic outputs approximate theoretical factor contributions.
  2. **Ablation on risk constraint**: Compare full Factor-MCLS vs. LSV1 (no risk constraint) on same training data. Confirm AV(tra) increases without constraint (as shown in Table 4).
  3. **Lambda sensitivity analysis**: Vary $\lambda$ vector values across [0.1, 0.5, 1.0, 2.0] uniformly; plot resulting Sharpe ratios and accumulated returns to identify Pareto frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a machine learning model determine the optimal risk aversion vector for the risk constraint term based on real-time arbitrage opportunities rather than subjective investor input?
- Basis in paper: [explicit] The Conclusion states the authors lack a model to determine risk aversion based on the arbitrage opportunities of various assets, identifying this as a limitation.
- Why unresolved: The current system relies on subjective investor definitions for the risk aversion vector, which may not dynamically reflect market efficiencies.
- What evidence would resolve it: The successful integration and back-testing of a model that dynamically adjusts $\Phi$ based on market signals, outperforming static subjective vectors.

### Open Question 2
- Question: Does utilizing Elliptical distributions in the auxiliary agent's policy function significantly improve performance over the current Gaussian assumption in volatile markets?
- Basis in paper: [explicit] The Conclusion identifies that the BDA auxiliary agent assumes a normal distribution, which fails to model the heavy tails and high peaks characteristic of real markets.
- Why unresolved: Real-world financial return distributions often exhibit kurtosis that Gaussian models cannot capture, potentially leading to suboptimal baseline portfolio weights.
- What evidence would resolve it: Comparative back-tests showing that a Bayesian model using Elliptical distributions yields higher risk-adjusted returns during periods of high market stress.

### Open Question 3
- Question: Does the Factor-MCLS framework scale effectively to high-dimensional portfolios (e.g., S&P 500) without suffering from gradient instability or reduced risk control?
- Basis in paper: [inferred] The experiments are restricted to a small portfolio of 29 highly liquid stocks from the Dow Jones Index.
- Why unresolved: The multi-critic framework requires training multiple neural networks; it is unclear if this architecture remains stable and computationally efficient as the asset count increases significantly.
- What evidence would resolve it: Successful training convergence and maintenance of Sharpe ratios when the system is applied to a dataset containing hundreds of diverse assets.

## Limitations

- The assumption that factor contributions remain stable across market regimes is critical and may not hold during regime shifts
- The multi-critic architecture requires significant computational resources (4× critic networks), limiting scalability to larger portfolios
- The risk constraint mechanism depends heavily on correct specification of investor risk aversion vectors λ, with limited guidance on how to set these values for different investor profiles

## Confidence

- **High confidence**: The multi-critic framework with separate factor vectors (Mechanism 2) is technically sound and well-supported by the equations and experimental results
- **Medium confidence**: The risk constraint mechanism (Mechanism 3) is theoretically valid but relies on the assumption that investors can specify meaningful λ values
- **Medium confidence**: The reward factor matrix decomposition (Mechanism 1) is well-defined mathematically, but the assumption that asset-level factor contributions generalize across periods requires validation across different market conditions

## Next Checks

1. **Regime robustness test**: Apply Factor-MCLS to portfolio data spanning both bull and bear markets (e.g., 2015-2024 including COVID-19 period) and measure performance degradation. Compare against single-critic baselines to quantify the value of factor decomposition under regime shifts.

2. **λ sensitivity and calibration study**: Systematically vary the risk aversion vector λ across multiple investor profiles (conservative, moderate, aggressive) and measure the Pareto frontier of return vs. risk metrics. Develop guidelines for setting λ based on investor characteristics.

3. **Factor correlation stability analysis**: Compute pairwise correlations between the four factor vectors (return, variance, covariance, transaction scale) across different market conditions. If correlations exceed 0.8, the multi-critic approach may be overfitting to correlated signals rather than learning distinct factors.