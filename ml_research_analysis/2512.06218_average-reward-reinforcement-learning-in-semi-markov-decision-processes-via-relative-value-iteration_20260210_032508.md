---
ver: rpa2
title: Average-reward reinforcement learning in semi-Markov decision processes via
  relative value iteration
arxiv_id: '2512.06218'
source_url: https://arxiv.org/abs/2512.06218
tags:
- assum
- q-learning
- algorithm
- conditions
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses convergence of relative value iteration Q-learning
  in average-reward semi-Markov decision processes (SMDPs) under the average-reward
  optimality criterion. The key method is developing a generalized RVI Q-learning
  algorithm that incorporates new monotonicity conditions for estimating the optimal
  reward rate.
---

# Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration

## Quick Facts
- arXiv ID: 2512.06218
- Source URL: https://arxiv.org/abs/2512.06218
- Reference count: 35
- One-line primary result: Proves almost sure convergence of relative value iteration Q-learning to solutions of the average-reward optimality equation for weakly communicating SMDPs under new monotonicity conditions.

## Executive Summary
This paper develops and analyzes a model-free reinforcement learning algorithm for average-reward optimization in semi-Markov decision processes (SMDPs). The algorithm, called RVI Q-learning, extends classical relative value iteration to the SMDP setting by incorporating stochastic estimates of holding times and using a self-regulating scalar translation mechanism. The key theoretical contribution is establishing convergence to a compact connected subset of solutions to the average-reward optimality equation under weak communication assumptions, using an asynchronous stochastic approximation framework.

## Method Summary
The method implements asynchronous RVI Q-learning that updates Q-values and holding-time estimates based on observed transitions. The algorithm uses a reference function f that must satisfy the SISTr (Strictly Increasing under Scalar Translation) property to ensure self-regulating estimation of the optimal reward rate. Holding times are estimated via stochastic gradient descent with stepsizes βn that decay no faster than the Q-learning stepsizes αn. The asynchronous updates are governed by an irreducible Markov chain over state-action pairs, and convergence is established by showing the associated ODE has a globally asymptotically stable equilibrium at the origin.

## Key Results
- Establishes almost sure convergence of asynchronous RVI Q-learning to a compact connected subset of solutions to the average-reward optimality equation
- Introduces new monotonicity conditions (SISTr) that substantially expand the algorithmic framework beyond previously considered reference functions
- Proves convergence under unknown holding times by using a two-time-scale stochastic approximation scheme with appropriately scaled stepsizes
- Provides conditions under which convergence is strengthened to a unique sample path-dependent solution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm converges to a solution set of the Average-reward Optimality Equation (AOE) by satisfying the Borkar-Meyn stability criterion within an asynchronous Stochastic Approximation (SA) framework.
- **Mechanism:** The RVI Q-learning update is cast as an asynchronous SA algorithm. The proof establishes that the associated ODE's limiting field has the origin as its globally asymptotically stable equilibrium. This ensures the iterates remain bounded and converge to a compact, connected subset of AOE solutions.
- **Core assumption:** The SMDP is weakly communicating, and the update function satisfies Lipschitz continuity.
- **Evidence anchors:**
  - [abstract] Mentions establishing "convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm."
  - [Section 4.1.2] Proves global asymptotic stability for the origin of the limiting ODE (Proposition 4.2) and for the set of AOE solutions (Proposition 4.1).
  - [Corpus] The companion paper (arXiv:2409.03915) provides the foundational asynchronous SA theory this paper applies.
- **Break condition:** If the SMDP is not weakly communicating (e.g., multichain) or if the update function lacks the required continuity, the stability argument fails.

### Mechanism 2
- **Claim:** A new monotonicity condition on the reference function, called Strictly Increasing under Scalar Translation (SISTr), enables "self-regulating" estimation of the optimal reward rate.
- **Mechanism:** The average-reward estimate is calculated via a function f (e.g., max, min, weighted average) over the current Q-values. The SISTr property guarantees that for any Q-vector, there is a unique scalar shift that aligns f's output with the optimal reward rate. This prevents the Q-values from drifting and forces convergence of the reward-rate estimate.
- **Core assumption:** The function f is SISTr (Assumption 3.3).
- **Evidence anchors:**
  - [abstract] Highlights "new monotonicity conditions for estimating the optimal reward rate... [which] substantially expand the previously considered algorithmic framework."
  - [Section 3.2, Definition 3.1] Formally defines the SISTr property.
  - [Section 4.1.2] Proofs of convergence and Lyapunov stability (Lemmas 4.5 & 4.7) explicitly rely on SISTr to bound scalar translations.
- **Break condition:** If f is not SISTr (e.g., a simple average that isn't strictly monotonic), the scalar shift is not uniquely determined, potentially causing divergence or non-convergence of the reward-rate estimate.

### Mechanism 3
- **Claim:** Convergence is maintained under unknown holding times by using a two-time-scale or same-time-scale stochastic approximation scheme.
- **Mechanism:** The algorithm estimates holding times via SGD with stepsizes {βn}. By enforcing that {βn} decays no faster than the Q-learning stepsizes {αn}, the holding-time estimates converge sufficiently quickly. This ensures the bias introduced by using estimates instead of true values vanishes fast enough to satisfy SA convergence conditions.
- **Core assumption:** Stepsizes satisfy Assumption 3.4(ii), requiring βn ≥ ςαn for sufficiently large n.
- **Evidence anchors:**
  - [Section 3.2] Equation (3.5) defines the SGD update for holding times.
  - [Section 4.2] The proof of Theorem 3.2 analyzes the biased noise term from estimation error, showing it meets required decay conditions.
  - [Corpus] Weak corpus evidence for this specific mechanism; it is a key technical contribution.
- **Break condition:** If holding-time stepsizes {βn} decay too slowly relative to {αn}, estimation noise may overwhelm the Q-learning signal, causing divergence.

## Foundational Learning

- **Concept: Semi-Markov Decision Processes (SMDPs)**
  - **Why needed here:** This is the paper's primary domain. Unlike MDPs, transitions take variable time. Understanding how this changes the optimality equation (using reward rate instead of cumulative reward) is essential.
  - **Quick check question:** How does the AOE for an SMDP differ from a standard MDP, and what does this imply for the value function?

- **Concept: Asynchronous Stochastic Approximation (SA)**
  - **Why needed here:** The core mathematical tool for proving convergence. The paper frames Q-learning as an SA algorithm and analyzes its limit by studying an associated Ordinary Differential Equation (ODE).
  - **Quick check question:** In SA theory, what property of the associated ODE is primarily used to prove the stability of the stochastic iterates?

- **Concept: Weakly Communicating MDPs/SMDPs**
  - **Why needed here:** A core structural assumption. It defines a class of problems where an optimal policy can reach a recurrent class from any state, but Q-value solutions may not be unique up to an additive constant.
  - **Quick check question:** Why does the "weakly communicating" structure complicate the convergence proof compared to a "unichain" structure?

## Architecture Onboarding

- **Component map:**
  - State/Action Store: Vectors Qn (Q-values) and Tn (holding times)
  - Reference Function f: A user-defined, SISTr-compliant function (e.g., max Q-value, min Q-value). This is the critical "self-regulating" component.
  - Dual Stepsize Controllers: Manages two sequences: {αn} for Q-updates and {βn} for holding-time updates.
  - Asynchronous Update Engine: Selects a subset of state-action pairs Yn for update based on data availability.

- **Critical path:**
  1.  Select f: Choose a function satisfying SISTr (e.g., f(Q) = max s,a Q(s,a))
  2.  Configure Stepsizes: Set {αn} (e.g., 1/An) and {βn}. The key constraint is βn ≥ ςαn, where ς depends on a lower bound of the minimum holding time
  3.  Iterate: At each step n, update holding times Tn via SGD, then use the updated Tn to compute the Q-value update

- **Design tradeoffs:**
  - Choice of f: A simple reference state is easier to implement but may be sensitive to exploration. Max/min functions are more robust but require computing global statistics
  - Stepsize Scaling: The threshold for the scaling parameter A depends on tmin. A poor estimate of tmin can make it difficult to satisfy the theoretical conditions
  - Assumption: The algorithm requires a lower bound on the minimum expected holding time. This is minimal model knowledge

- **Failure signatures:**
  - Divergence of Q-values: If the system is not weakly communicating or if f lacks the SISTr property
  - Non-convergence of Reward Rate: If the stepsizes for holding times (βn) decay too quickly relative to Q-learning stepsizes (αn)
  - Path-Dependent Limit: The final Q-values may depend on the sample path, but the derived optimal policy should remain correct

- **First 3 experiments:**
  1.  MDP Baseline: Implement on a simple weakly communicating MDP (holding time = 1). Verify f(Qn) converges to the known optimal average reward. This validates the core loop
  2.  SMDP with Non-Uniform Holding Times: Test on a problem where tsa varies. Monitor both Qn and Tn. Verify Tn converges to true values and Qn solves the correct AOE
  3.  Ablation on Stepsizes: Intentionally use βn that decays much faster than αn to violate the assumption. Observe if the reward-rate estimate fails to converge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence guarantees for RVI Q-learning be extended to distributed computation settings that involve communication delays?
- Basis in paper: [explicit] Section 5 (Discussion) explicitly identifies this as "a natural direction for future work."
- Why unresolved: The current asynchronous stochastic approximation framework used to prove stability handles state-action update frequencies but does not explicitly model the message-passing latencies inherent in distributed systems.
- What evidence would resolve it: A theoretical extension of the stability analysis that guarantees convergence even when updates rely on delayed information from neighboring agents or nodes.

### Open Question 2
- Question: Can the strict monotonicity under scalar translation (SISTr) condition be relaxed to a localized version that only applies near the target set?
- Basis in paper: [inferred] Remark 4.3 states that the proof relies on SISTr properties primarily at points in Qf and suggests a localized relaxation is "possible," but the paper uses the global condition for simplicity.
- Why unresolved: Proving convergence under a local relaxation requires more delicate stability arguments to ensure iterates remain in the region where the monotonicity condition holds, rather than relying on global structural properties.
- What evidence would resolve it: A modified convergence proof requiring f to be strictly increasing only within a specific neighborhood of the optimal solution set Qf.

### Open Question 3
- Question: Can the algorithm be generalized to multichain (non-communicating) SMDPs where the optimal reward rate varies by initial state?
- Basis in paper: [inferred] The paper restricts its analysis to "weakly communicating SMDPs" (Section 3.1) where the optimal reward rate r* is constant, leaving the multichain case untreated.
- Why unresolved: In multichain settings, the average-reward optimality equation is structurally different (involving a vector of gains), making the current scalar estimation method f(Qn) insufficient for tracking multiple distinct optimal reward rates.
- What evidence would resolve it: A new algorithmic formulation for f and a corresponding convergence proof that handles state-dependent optimal reward rates and the resulting recurrence classes.

## Limitations
- The algorithm requires a lower bound on minimum expected holding times (tmin) for stepsize calibration, which may not be available in practice
- Convergence is established to a compact connected subset rather than a unique solution in the general weakly communicating case, introducing path dependence
- The analysis assumes the SMDP is weakly communicating, excluding multichain problems where optimal reward rates vary by initial state

## Confidence
- **High confidence:** The core mathematical framework of casting RVI Q-learning as asynchronous stochastic approximation and analyzing convergence via ODE stability (Mechanism 1) is well-established and rigorously proven
- **Medium confidence:** The SISTr property's role in ensuring self-regulating reward rate estimation (Mechanism 2) is theoretically sound, though its practical implications and the performance of different SISTr-compliant functions require empirical validation
- **Medium confidence:** The two-time-scale argument for handling unknown holding times (Mechanism 3) is technically correct but relies on stepsize relationships that may be difficult to calibrate in practice

## Next Checks
1. **SISTr function ablation study:** Implement the algorithm with multiple reference functions (max, min, weighted average) satisfying SISTr to empirically compare convergence speed and robustness to exploration noise
2. **Stepsize sensitivity analysis:** Systematically vary the ratio βn/αn below and above the theoretical threshold to quantify the impact on reward rate estimation accuracy and overall convergence
3. **Real-world SMDP testbed:** Design a benchmark SMDP environment with non-uniform holding times (e.g., queueing system or robotic task with variable execution durations) where the optimal average reward can be computed analytically, then validate that the algorithm converges to this known solution