---
ver: rpa2
title: 'DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management'
arxiv_id: '2510.15087'
source_url: https://arxiv.org/abs/2510.15087
tags:
- passage
- disaster
- huggingface
- arxiv
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DMRETRIEVER, a family of dense retrieval
  models ranging from 33M to 7.6B parameters specifically designed for disaster management
  information retrieval. The authors address the challenge that existing general-domain
  retrieval models fail to consistently achieve state-of-the-art performance across
  diverse disaster management search intents.
---

# DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management

## Quick Facts
- **arXiv ID**: 2510.15087
- **Source URL**: https://arxiv.org/abs/2510.15087
- **Authors**: Kai Yin; Xiangjue Dong; Chengkai Liu; Allen Lin; Lingfeng Shi; Ali Mostafavi; James Caverlee
- **Reference count**: 40
- **Primary result**: DMRETRIEVER achieves state-of-the-art performance across all six disaster management search intents, with the 596M variant outperforming all XL baselines (>4B) despite being over 13.3× smaller

## Executive Summary
This paper introduces DMRETRIEVER, a family of dense retrieval models specifically designed for disaster management information retrieval. The authors address the challenge that existing general-domain retrieval models fail to consistently achieve state-of-the-art performance across diverse disaster management search intents. DMRETRIEVER is trained through a novel three-stage framework: bidirectional attention adaptation for decoder-only backbones, unsupervised contrastive pre-training, and difficulty-aware progressive instruction fine-tuning. The training utilizes high-quality data generated through an advanced data refinement pipeline that includes LLM-based synthetic data generation, mutual-agreement false positive filtering, and difficulty-aware hard negative mining. Comprehensive experiments demonstrate that DMRETRIEVER achieves new state-of-the-art performance across all six search intents at every model scale, with the 33M variant surpassing all medium baselines with only 7.6% of their parameters.

## Method Summary
DMRETRIEVER is trained through a three-stage framework that progressively enhances model capability for disaster management retrieval. The first stage involves bidirectional attention adaptation, where the authors convert decoder-only backbone models (like Llama-2) into encoder-decoder architectures by adding bidirectional attention to the encoder layers, enabling effective representation learning for retrieval tasks. The second stage employs unsupervised contrastive pre-training using the Ipecav dataset, which contains over 40 million web documents with continuous scores. This pre-training stage is crucial for learning general semantic representations before domain-specific fine-tuning. The third and final stage involves difficulty-aware progressive instruction fine-tuning, where the model is fine-tuned on synthetic data generated through a sophisticated pipeline. This pipeline includes LLM-based synthetic data generation, mutual-agreement false positive filtering (using top-3 models to identify reliable positive pairs), and difficulty-aware hard negative mining to enhance the model's ability to distinguish between similar passages. The authors also employ LoRA adapters during fine-tuning to enable efficient parameter updates while maintaining model performance.

## Key Results
- DMRETRIEVER achieves state-of-the-art performance across all six search intents at every model scale
- The 596M variant outperforms all XL baselines (>4B) despite being over 13.3× smaller
- The 33M variant surpasses all medium baselines with only 7.6% of their parameters
- DMRETRIEVER establishes the first large-scale training dataset for disaster management retrieval

## Why This Works (Mechanism)
DMRETRIEVER succeeds by addressing the fundamental mismatch between general-purpose retrieval models and the specialized needs of disaster management. The bidirectional attention adaptation transforms decoder-only models into architectures better suited for retrieval tasks, while the unsupervised contrastive pre-training provides a strong semantic foundation before domain-specific adaptation. The difficulty-aware progressive instruction fine-tuning, combined with the sophisticated data refinement pipeline, ensures the model learns to handle the full spectrum of disaster management queries, from straightforward information needs to complex, context-dependent situations. The mutual-agreement false positive filtering strategy helps maintain high-quality training data by filtering out noise that could degrade performance. The model's ability to handle diverse search intents - from situational awareness to resource allocation - is enhanced by the progressive fine-tuning approach that systematically increases task complexity.

## Foundational Learning
**Dense Retrieval**: Neural models that map queries and documents to dense vector representations in a shared embedding space, enabling efficient similarity search. *Why needed*: Traditional keyword-based retrieval fails to capture semantic relationships crucial for disaster management queries. *Quick check*: Verify that the model produces similar embeddings for semantically related disaster management queries.
**Contrastive Learning**: Training method that learns representations by pulling similar items together and pushing dissimilar items apart in embedding space. *Why needed*: Essential for learning robust semantic representations that generalize across diverse disaster scenarios. *Quick check*: Ensure that contrastive loss decreases during unsupervised pre-training phase.
**LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method that injects low-rank matrices into model layers, reducing trainable parameters. *Why needed*: Enables efficient fine-tuning of large models while maintaining performance. *Quick check*: Confirm that LoRA parameters are properly initialized and updated during fine-tuning.
**Hard Negative Mining**: Training strategy that selects challenging negative examples that are semantically similar to positive examples. *Why needed*: Improves model's ability to distinguish between relevant and irrelevant passages in ambiguous cases. *Quick check*: Verify that mined hard negatives are indeed semantically similar to their corresponding queries.

## Architecture Onboarding

**Component Map**: Query Encoder -> Document Encoder -> Dense Similarity -> Ranking

**Critical Path**: The critical path for inference involves the query encoder producing a dense vector representation, the document encoder producing representations for candidate passages, and a similarity function computing cosine similarity scores to rank relevant documents. The bidirectional attention layers in the encoder are crucial for capturing context from both directions, which is essential for understanding complex disaster management terminology.

**Design Tradeoffs**: The authors chose to adapt existing decoder-only models rather than train from scratch, trading off some architectural optimization for faster development and leveraging pre-trained knowledge. The use of LoRA adapters instead of full fine-tuning reduces computational costs but may limit the model's ability to learn complex domain-specific patterns. The synthetic data generation approach provides abundant training data but introduces potential quality and bias concerns that are mitigated through the mutual-agreement filtering strategy.

**Failure Signatures**: The model may struggle with queries containing highly technical disaster management terminology not well-represented in the training data. Cross-lingual queries or those requiring multi-modal understanding (text + images) would likely fail, as the model is trained exclusively on English text. Queries that require real-time information synthesis or reasoning about dynamic disaster situations may also produce suboptimal results.

**3 First Experiments**:
1. Evaluate zero-shot performance on a held-out disaster management query set to assess generalization.
2. Compare retrieval quality using different similarity metrics (cosine vs. inner product) to optimize ranking.
3. Conduct ablation study removing the mutual-agreement filtering step to quantify its impact on data quality.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: To what extent does rigorous hyperparameter tuning of the LoRA adapter (specifically rank $r$ and scaling factor $\alpha$) improve the retrieval performance of the decoder-only DMRetriever variants compared to the fixed settings ($r=16, \alpha=32$) adopted from prior work?
- **Basis in paper**: [explicit] Appendix I states the authors apply LoRA following prior work, "leaving hyperparameter tuning for future exploration."
- **Why unresolved**: The paper establishes strong results with default LoRA settings, but it remains unknown if the specific architectural constraints of disaster management retrieval (e.g., complex terminology) would benefit from optimized adapter configurations.
- **What evidence would resolve it**: A comparison of NDCG@10 scores on DisastIR-Test across a sweep of LoRA rank and alpha values for the 596M, 4B, and 7.6B variants.

### Open Question 2
- **Question**: How can the DMRetriever architecture and data refinement pipeline be effectively extended to support multi-modal retrieval tasks, such as linking text queries to satellite imagery or disaster surveillance videos?
- **Basis in paper**: [explicit] The Limitations section states the work focuses on text and suggests that "extending it to multi-modal retrieval would enhance its applicability in real-world disaster scenarios."
- **Why unresolved**: The current model relies strictly on textual embeddings and LLM-based synthetic text generation; it lacks mechanisms to process or align visual data common in disaster response (e.g., damage assessment photos).
- **What evidence would resolve it**: The implementation of a multi-modal DMRetriever variant and its evaluation on a benchmark requiring the retrieval of images or video frames based on textual disaster queries.

### Open Question 3
- **Question**: Does the mutual-agreement false positive filtering strategy inadvertently introduce confirmation bias by discarding valid but "hard" positive pairs that existing retrieval models fail to rank highly?
- **Basis in paper**: [inferred] The method relies on top-3 models to agree on positive passages (A-2). The introduction notes that existing models "fail to handle" diverse intents and produce "irrelevant results."
- **Why unresolved**: If the models used for filtering are weak in the disaster domain, their consensus might systematically exclude novel or difficult domain-specific knowledge that lies outside their current distribution, effectively capping the training data quality at the level of the "teachers."
- **What evidence would resolve it**: A human evaluation of the discarded pairs from the DM-MTP dataset to determine the ratio of false negatives (valid pairs discarded) to true noise filtered out.

### Open Question 4
- **Question**: How robust is DMRetriever in cross-lingual or multilingual disaster scenarios, where critical information may be published in non-English languages?
- **Basis in paper**: [explicit] The Limitations section notes, "DMRETRIEVER is focused on English-language resources, and future work could explore multilingual and cross-lingual retrieval."
- **Why unresolved**: Disasters often occur in regions where English is not the primary language of official reports or social media. The current reliance on English-only PDFs and corpora limits its global applicability.
- **What evidence would resolve it**: Zero-shot evaluation of the current models on a translated version of DisastIR or a native multilingual disaster dataset to measure performance degradation compared to English.

## Limitations
- The models are focused exclusively on English-language resources, limiting applicability in non-English speaking regions where disasters occur
- The reliance on synthetic data generation introduces potential quality and bias concerns that are not fully addressed
- The computational costs associated with training large models (7.6B parameters) are not discussed, limiting practical applicability
- No operational testing or field validation demonstrates real-world deployment effectiveness
- The models lack mechanisms to process multi-modal data (images, videos) common in disaster response

## Confidence
- **High confidence**: The reported performance improvements over baselines within the disaster management domain are well-supported by the experimental results presented
- **Medium confidence**: The effectiveness of the three-stage training framework and data refinement pipeline is demonstrated, but the relative importance of each component remains unclear
- **Low confidence**: Claims about the practical deployment of these models in real disaster scenarios are not substantiated with operational testing or field validation

## Next Checks
1. Conduct cross-domain evaluation to assess how DMRETRIEVER models perform when applied to general information retrieval tasks outside disaster management
2. Perform detailed ablation studies isolating the contributions of synthetic data generation, mutual-agreement filtering, and difficulty-aware negative mining to quantify their individual impact on retrieval performance
3. Evaluate model robustness by testing performance across different disaster types (floods, earthquakes, wildfires, etc.) to ensure consistent effectiveness across the full spectrum of disaster management scenarios