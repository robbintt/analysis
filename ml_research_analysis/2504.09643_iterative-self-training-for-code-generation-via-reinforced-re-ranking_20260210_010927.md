---
ver: rpa2
title: Iterative Self-Training for Code Generation via Reinforced Re-Ranking
arxiv_id: '2504.09643'
source_url: https://arxiv.org/abs/2504.09643
tags:
- code
- generation
- arxiv
- rewardranker
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality code
  by proposing an iterative self-training approach for reward model optimization using
  Proximal Policy Optimization (PPO). The core idea is to develop a robust reward/reranking
  model that improves code generation quality by reranking and addressing errors overlooked
  by traditional reward models.
---

# Iterative Self-Training for Code Generation via Reinforced Re-Ranking

## Quick Facts
- arXiv ID: 2504.09643
- Source URL: https://arxiv.org/abs/2504.09643
- Reference count: 32
- This paper proposes an iterative self-training approach using PPO and reinforced re-ranking that achieves performance comparable to GPT-4 on code generation tasks.

## Executive Summary
This paper addresses code generation quality by developing a robust reward/reranking model that improves through iterative self-training. The core innovation is an approach that generates candidate solutions, identifies high-scoring incorrect solutions (hard negatives), and incorporates them into training to refine the reward model's accuracy. The method achieves performance comparable to GPT-4 while being computationally efficient, with a 13.4B parameter model outperforming a 33B model and being three times faster.

## Method Summary
The approach involves four main stages: (A) Supervised fine-tuning of a base LLM on code datasets, (B) Training a reward model using Bradley-Terry loss on pairs of correct and incorrect code, (C) PPO-based generation where the reward model provides feedback to optimize the generator, and (D) An iterative self-training cycle where the PPO model generates candidates, identifies high-scoring incorrect solutions (hard negatives) through execution testing, and retrains the reward model with these examples. The process repeats for multiple iterations, with each cycle refining the model's ability to distinguish correct from subtly flawed code.

## Key Results
- A 13.4B parameter RewardRanker model outperforms a 33B model while being three times faster
- Achieves performance comparable to GPT-4 on code generation tasks
- Surpasses GPT-4 in C++ programming language accuracy
- Shows consistent accuracy improvements across multiple programming languages on the MultiPL-E dataset

## Why This Works (Mechanism)

### Mechanism 1: Iterative Hard Negative Mining
Incorporating high-scoring incorrect solutions into training data improves the discriminative power of the reward/reranking model. The process involves generating candidates with the PPO-aligned model, evaluating them against test cases, and specifically identifying solutions that the current reward model ranks highly but which fail execution. These "hard negatives" are added to the training set for the next iteration, forcing the model to learn finer-grained distinctions between correct and near-correct but subtly flawed code.

### Mechanism 2: PPO-Based Alignment of Generator to Reranker
Using Proximal Policy Optimization (PPO) to align the code generator with the reward/reranker model creates a feedback loop that improves the generator's ability to produce solutions the reranker favors. The generator produces code, and the reward model assigns scores based on predicted correctness. PPO updates the generator's policy to maximize this reward, aligning the generator to the specific preferences of the evolving reranker.

### Mechanism 3: Reranker-Based Best-of-N Selection
Selecting the best solution from a pool of generated candidates using a trained reranker model significantly improves final code quality. A base code generation model samples multiple candidate solutions (e.g., 10 samples), and the RewardRanker model scores each candidate to select the highest-scoring one as the final output. This mitigates the stochastic nature of code generation by leveraging a separate, specialized model for quality assessment.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the core reinforcement learning algorithm used to align the code generator with the reward model.
  - Quick check question: How does PPO prevent the policy from changing too drastically in a single update, promoting stable learning?

- **Concept: Reward Modeling / Preference Learning (Bradley-Terry Model)**
  - Why needed here: The RewardRanker is a reward model trained on pairs of correct and incorrect code to predict which is better.
  - Quick check question: In the context of this paper, what is the core assumption behind training a reward model on pairs of "OK" and "incorrect" code solutions?

- **Concept: Hard Negative Mining**
  - Why needed here: A central contribution is the iterative refinement of the training set by specifically adding "hard negatives"—incorrect code that the model incorrectly believes is correct.
  - Quick check question: Why is adding "hard negatives" to the training data more effective for model refinement than adding random incorrect examples?

## Architecture Onboarding

- **Component map**: SFT (base model) -> Reward Model Training -> PPO Training -> Self-Training Cycle (generate candidates -> execute tests -> identify hard negatives -> retrain reward model)
- **Critical path**: The Self-Training Cycle is the most critical and computationally expensive path, involving running the PPO-aligned generator to produce candidates, executing them against test cases, and performing hard negative mining.
- **Design tradeoffs**: 
  - Compute Cost vs. Performance: The iterative self-training cycle is computationally expensive, though even a single iteration provides significant boost
  - Reranker Model Size vs. Generator Model Size: Larger reranker may have better discriminative power but adds inference latency
  - Best-of-N vs. Speed: Generating multiple candidates (N=10) improves chances of finding correct solutions but linearly increases inference time and cost
- **Failure signatures**:
  - Reward Hacking: Generator produces code that looks correct to reward model but is subtly wrong
  - Generator Failure: Base generator too weak to produce any correct solutions for a given task
  - Overfitting to Test Cases: Model could overfit to the style or specific cases in training/validation sets
- **First 3 experiments**:
  1. Baseline SFT Generator + Reranker: Implement SFT model and train initial RewardRanker on dataset, evaluate using Best-of-N sampling
  2. Single Iteration Self-Training: Run full self-training loop for one iteration, compare new reranker performance against baseline
  3. Ablation on Negative Sampling: Compare reranker training using random incorrect examples versus the paper's proposed "hard negatives"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pretraining RewardRanker on non-code domains (e.g., math, logic) improve self-training convergence and generalization?
- Basis in paper: The conclusion explicitly states a plan to "explore additional domains for pretraining."
- Why unresolved: Current study is strictly limited to code-specific datasets (CodeContests, Codeforces).
- What evidence would resolve it: Evaluation of a multi-domain RewardRanker on MultiPL-E benchmark and non-code reasoning tasks.

### Open Question 2
- Question: Does the iterative re-ranking loop continue to improve performance beyond two iterations, or does it degrade?
- Basis in paper: Results in Table 1 report performance only for "2 iter" models.
- Why unresolved: Long-term dynamics and convergence limits of the PPO-based self-training cycle are not analyzed.
- What evidence would resolve it: Accuracy curves over 5–10 iterations to identify potential overfitting or plateauing.

### Open Question 3
- Question: Can the computationally expensive full model retraining be replaced by parameter-efficient fine-tuning?
- Basis in paper: Section 4 notes that the Retraining and Iterative Refinement cycle is "computationally costly."
- Why unresolved: Paper does not investigate if incremental updates (like LoRA) can replace full PPO retraining.
- What evidence would resolve it: Comparative benchmarks of resource consumption and accuracy between full retraining and PEFT updates.

## Limitations
- The specific contribution of iterative hard negative mining versus other factors (base generator quality, reranker architecture) is not adequately isolated
- Computational cost of iterative self-training cycle is acknowledged but not quantified in terms of wall-clock time or GPU hours
- Claims of superiority over larger models (33B) cannot be independently verified without access to exact training code and hyperparameter configurations

## Confidence

- **High Confidence**: Basic reranker-based best-of-N selection mechanism and experimental results showing improvement over generator-only approaches
- **Medium Confidence**: PPO alignment mechanism and its integration with reward model, though implementation details significantly impact results
- **Low Confidence**: Specific claims of superiority over larger models and the contribution of iterative hard negative mining versus other factors

## Next Validation Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary PPO hyperparameters (learning rate, clip ratio, KL penalty) and hard negative selection threshold to determine which factors most strongly influence final performance.

2. **Ablation Study on Negative Mining Strategy**: Implement and compare three reranker training approaches: random incorrect examples, the paper's proposed "high-scoring failures," and a hybrid approach using both.

3. **Cross-Architecture Generalization Test**: Evaluate whether the iterative self-training approach transfers to different base architectures beyond DeepSeek-Coder by testing with a different code generation model family (e.g., CodeLlama or StarCoder).