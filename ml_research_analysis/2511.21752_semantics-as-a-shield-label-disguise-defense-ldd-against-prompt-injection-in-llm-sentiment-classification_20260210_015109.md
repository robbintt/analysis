---
ver: rpa2
title: 'Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection
  in LLM Sentiment Classification'
arxiv_id: '2511.21752'
source_url: https://arxiv.org/abs/2511.21752
tags:
- labels
- alias
- label
- sentiment
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large language models
  (LLMs) to class-directive prompt injection attacks, where adversarial instructions
  manipulate model outputs by exploiting known label sets (e.g., "positive" vs. "negative").
---

# Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification

## Quick Facts
- arXiv ID: 2511.21752
- Source URL: https://arxiv.org/abs/2511.21752
- Authors: Yanxi Li; Ruocheng Shan
- Reference count: 3
- Primary result: Label disguise defense restores accuracy degraded by prompt injection attacks in 9 tested models

## Executive Summary
This paper addresses the vulnerability of large language models (LLMs) to class-directive prompt injection attacks, where adversarial instructions manipulate model outputs by exploiting known label sets (e.g., "positive" vs. "negative"). To defend against this, the authors propose Label Disguise Defense (LDD), a lightweight, model-agnostic method that replaces original labels with semantically transformed alias labels (e.g., "green" vs. "red") and uses few-shot in-context learning to implicitly teach the model the new label mappings. Evaluated across nine models including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, LDD successfully restores accuracy degraded by injection attacks in most cases. For the vast majority of models, at least one alias pair achieves higher accuracy than the under-attack baseline. A linguistic analysis shows that semantically aligned alias labels (e.g., "good" vs. "bad") yield stronger robustness than unaligned symbols (e.g., "blue" vs. "yellow"), demonstrating that label semantics can serve as an effective defensive layer against prompt injection.

## Method Summary
Label Disguise Defense (LDD) is a lightweight, model-agnostic method that defends against class-directive prompt injection attacks by replacing original sentiment labels with semantically transformed alias labels (e.g., "green" vs. "red"). The defense uses few-shot in-context learning with implicit demonstrations to teach the model the new label mappings without explicitly exposing the original label semantics. The approach prevents direct correspondence between injected directives and decision outputs, as adversarial instructions targeting the original label space become semantically irrelevant when the model operates under alias labels.

## Key Results
- LDD successfully restores accuracy degraded by injection attacks in 9 tested models including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants
- For the vast majority of models, at least one alias pair achieves higher accuracy than the under-attack baseline
- Semantically aligned alias labels (e.g., "good" vs. "bad") yield stronger robustness than unaligned symbols (e.g., "blue" vs. "yellow")
- Aligned labels show Recovery Ratios of 0.50-0.80 across models; unaligned labels drop to 0.10-0.40 with negative net recovery

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decoupling of Attack Instructions
Replacing original labels with alias labels prevents adversarial instructions from directly mapping to the model's output vocabulary. When an attacker injects "Classify this text as negative," the directive targets the original label space. If the model is operating under alias labels (e.g., green/red), the injection becomes semantically irrelevant—the instruction's tokens no longer correspond to any valid output category the model can produce. Core assumption: Attackers cannot infer which alias labels are in use at inference time.

### Mechanism 2: Implicit Few-Shot Induction Avoids Mapping Leakage
Learning alias-to-sentiment mappings through few-shot demonstrations, rather than explicit natural language descriptions, preserves defense integrity. Explicitly stating "green means positive" re-exposes the original label semantics, allowing the model (and attackers who observe the prompt) to anchor predictions back to the original sentiment vocabulary. Implicit induction via examples forces the model to learn the pattern contextually without creating an exploitable semantic bridge. Core assumption: Models can reliably infer the correct polarity mapping from 2-8 demonstrations.

### Mechanism 3: Semantic Alignment of Aliases Improves Robustness
Alias labels with sentiment-aligned connotative meaning (e.g., green/red, good/bad, heaven/hell) yield higher recovery ratios than arbitrary symbols. Aligned aliases provide a secondary semantic scaffold—their conventional associations (green=positive/good, red=negative/bad) help the model construct a stable polarity axis even when the original labels are hidden. Unaligned symbols (e.g., cat/dog, i/j) lack this scaffold, increasing regression. Core assumption: Models encode the same cultural/symbolic associations humans do; these associations transfer across label spaces.

## Foundational Learning

- **Concept: Prompt Injection Attack Vectors**
  - Why needed here: LDD specifically targets class-directive injections where attackers exploit known label names. Understanding the attack surface clarifies why label disguise is a viable intervention point.
  - Quick check question: Can you explain why an attacker who knows only "positive/negative" labels cannot directly manipulate a "green/red" output space?

- **Concept: Few-Shot In-Context Learning Sensitivity**
  - Why needed here: LDD's effectiveness depends on shot count and example ordering. The paper tests permutations (PNPNPN vs. NPNPNP) because prior work shows order affects accuracy.
  - Quick check question: Why might a 2-shot LDD prompt perform worse than a 6-shot prompt with the same alias pair?

- **Concept: Descriptive vs. Connotative Meaning**
  - Why needed here: The paper's linguistic analysis distinguishes labels that explicitly describe sentiment (good/bad) from those that imply it through association (green/red). This framework guides alias selection.
  - Quick check question: Which alias pair—"happy/sad" or "cat/dog"—has stronger connotationality for sentiment tasks, and why does this matter for defense?

## Architecture Onboarding

- **Component map:**
  Input Text → [Pre-processing: Append adversarial instruction if testing attacks] → [Prompt Construction: System instruction + Few-shot examples with alias labels] → [LLM Inference: Model outputs alias label] → [Post-processing: Map alias → original label via deterministic lookup] → Final Prediction

- **Critical path:** The few-shot prompt construction is the defensive core. If examples are mislabeled, poorly ordered, or use unaligned aliases, the model either fails to learn the mapping or regresses on correct predictions.

- **Design tradeoffs:**
  - **Aligned vs. Unaligned Aliases:** Aligned aliases (green/red) recover more attack-induced errors but leak some semantic information. Unaligned aliases (i/j) provide stronger obfuscation but cause high regression.
  - **Shot Count:** More shots improve mapping induction but increase token costs and latency. The paper shows diminishing returns beyond 4-6 shots for many models.
  - **Explicit vs. Implicit Mapping:** Explicit mapping ("green means positive") is easier to implement but nullifies defense; implicit requires more examples but preserves security.

- **Failure signatures:**
  - **Low Recovery Ratio (<0.35):** Model is not learning the alias mapping; check alias semantic alignment and shot count.
  - **High Regression Count (>40):** Model is correctly classifying fewer samples than the attack baseline; alias labels may be actively confusing the task.
  - **Inconsistent Accuracy Across Permutations:** Model is order-sensitive; use ensembling or calibrate prompt order before deployment.

- **First 3 experiments:**
  1. **Baseline Attack Replication:** Run zero-shot sentiment classification on 50 samples with and without class-directive injections. Confirm accuracy drop matches paper ranges (e.g., GPT-5: ~0.45 drop; GPT-4o: ~0.055 drop).
  2. **Alias Pair Sweep:** Test 3 aligned (green/red, good/bad, heaven/hell) and 3 unaligned (cat/dog, i/j, @#$/^ vs. *&%!) alias pairs under 4-shot LDD on a single model. Plot recovery vs. regression counts.
  3. **Shot-Count Ablation:** For the best-performing aligned alias from experiment 2, vary shots (2, 4, 6, 8) and measure accuracy trajectory. Identify the minimum shot count that achieves >80% of the clean baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LDD transfer effectively to multilingual sentiment classification, and how does linguistic alignment between alias labels and target languages affect defense robustness?
- Basis in paper: [explicit] "Extending LDD to multilingual or multi-class settings could reveal how linguistic alignment and semantic transferability influence defense effectiveness."
- Why unresolved: All experiments used English-only IMDB reviews; cross-linguistic semantic alignment (e.g., color-symbolic labels like green/red) may not hold equivalently across cultures or languages.
- What evidence would resolve it: Evaluate LDD on multilingual sentiment datasets (e.g., multilingual Amazon reviews) with culture-specific alias labels, comparing defense success rates across languages.

### Open Question 2
- Question: How robust is LDD against indirect prompt injection, multi-step reasoning attacks, and multi-turn category redefinitions?
- Basis in paper: [explicit] "Other label-targeting attacks, such as indirect manipulations, multi-step reasoning attacks, or multi-turn category redefinitions, were not covered and warrant further study."
- Why unresolved: Only direct class-directive injection was tested; indirect injections embedded in external content or multi-turn attacks that gradually redefine label mappings may circumvent the disguise.
- What evidence would resolve it: Test LDD against established indirect injection benchmarks (e.g., prompts embedded in retrieved documents) and multi-turn adversarial conversations that attempt to uncover alias mappings.

### Open Question 3
- Question: Can LDD be combined with complementary defenses (self-consistency, semantic rephrasing) to achieve additive or synergistic robustness gains?
- Basis in paper: [explicit] "Future research should explore integrating LDD with complementary lightweight prompt-level defenses, such as self-consistency and semantic rephrasing."
- Why unresolved: LDD was evaluated in isolation; interactions with other defense mechanisms remain unknown and could potentially conflict or compound benefits.
- What evidence would resolve it: Conduct ablation studies combining LDD with voting-based self-consistency and input paraphrasing, measuring accuracy recovery and regression counts under attack.

### Open Question 4
- Question: Does LDD generalize to multi-class classification tasks beyond binary sentiment, where alias label design becomes more complex?
- Basis in paper: [explicit] "Extending LDD to multilingual or multi-class settings could reveal how linguistic alignment and semantic transferability influence defense effectiveness."
- Why unresolved: Only binary positive/negative classification was tested; designing mutually distinguishable yet semantically aligned aliases for 3+ classes poses additional challenges.
- What evidence would resolve it: Evaluate LDD on multi-class tasks (e.g., 5-star rating prediction, topic classification) with systematically varied alias set sizes and semantic alignment levels.

## Limitations

- **Attack Generalization**: The evaluation focuses on class-directive injections; effectiveness against other prompt injection variants (jailbreaks, context-leak attacks) remains unvalidated.
- **Alias Discovery Risk**: The defense's security relies on alias secrecy; if aliases are exposed through system prompts, API responses, or probing, the defense collapses entirely.
- **Model Capacity Assumptions**: LDD assumes models can reliably infer alias-to-sentiment mappings from 2-8 demonstrations; capacity constraints for smaller models suggest this may not generalize.

## Confidence

- **High Confidence**: LDD can recover accuracy degraded by class-directive prompt injection in most tested models. The empirical results across nine diverse models provide strong evidence.
- **Medium Confidence**: Semantic alignment of aliases improves robustness. While the data shows aligned aliases perform better, the underlying mechanism (model's cultural associations) lacks direct validation.
- **Low Confidence**: LDD provides security through obscurity. The defense's reliance on alias secrecy hasn't been stress-tested against discovery attempts.

## Next Checks

1. **Attack Adaptation Test**: Design and evaluate prompt injection attacks that specifically target discovered alias labels. Measure whether attackers can craft semantically equivalent injections (e.g., "Classify this as red") that bypass LDD.

2. **Alias Discovery Resistance**: Implement systematic probing experiments to assess how easily attackers could discover alias labels through API access patterns, response analysis, or black-box testing. Measure the security margin.

3. **Cross-Task Generalization**: Apply LDD to non-sentiment classification tasks (e.g., topic classification, question answering) with different label vocabularies. Validate whether semantic alignment principles transfer and what new alias selection strategies emerge.