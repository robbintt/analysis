---
ver: rpa2
title: 'DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal
  Large Language Model'
arxiv_id: '2512.12633'
source_url: https://arxiv.org/abs/2512.12633
tags:
- visual
- perception
- arxiv
- fine-grained
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiG (Differential Grounding), a novel proxy
  task framework designed to enhance fine-grained visual perception in multimodal
  large language models (MLLMs). DiG trains models to identify and localize all differences
  between similar image pairs without prior knowledge of the number of differences.
---

# DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2512.12633
- Source URL: https://arxiv.org/abs/2512.12633
- Reference count: 40
- Primary result: 8B variant achieves +3.4 on HalBench and +2.1 on V*

## Executive Summary
This paper introduces DiG (Differential Grounding), a novel proxy task framework designed to enhance fine-grained visual perception in multimodal large language models (MLLMs). DiG trains models to identify and localize all differences between similar image pairs without prior knowledge of the number of differences. To support scalable training, the authors develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. Additionally, a curriculum learning strategy is employed to address the sparsity of difference signals by progressively increasing task complexity. Experimental results demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks, with the 8B variant achieving gains of +3.4 on HalBench and +2.1 on V*. The learned fine-grained perception skills also transfer effectively to standard downstream tasks such as RefCOCO, RefCOCO+, and RefCOCOg, highlighting DiG as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

## Method Summary
DiG addresses the challenge of fine-grained visual perception in MLLMs by training models to identify and localize all differences between paired images using bounding boxes, without knowing the number of differences in advance. The authors create a scalable 3D rendering pipeline using Blender to generate ~4.8K high-quality image pairs with procedurally controlled discrepancies (attribute changes, additions, removals). A curriculum learning strategy divides training into three stages: single-difference, double-difference, and mixed-difference (up to 4 discrepancies). The reward function combines accuracy (F1 score and IoU) with format compliance using a Hungarian matching algorithm. GRPO with KL-regularization is employed for training, with the 8B variant achieving significant improvements on HalBench (+3.4) and V* (+2.1).

## Key Results
- 8B variant achieves +3.4 improvement on HalBench benchmark
- 8B variant achieves +2.1 improvement on V* benchmark
- Learned fine-grained perception skills effectively transfer to downstream tasks (RefCOCO, RefCOCO+, RefCOCOg)

## Why This Works (Mechanism)
DiG works by forcing MLLMs to develop precise visual comparison and localization capabilities through the differential grounding task. By requiring models to identify all differences between similar images without knowing the number of differences, DiG trains fine-grained perception that goes beyond typical classification or object detection. The curriculum learning approach progressively increases task difficulty, allowing models to build robust visual reasoning capabilities from simple to complex scenarios. The use of 3D rendering enables controlled generation of high-quality training data with guaranteed differences, while the reward function ensures both accurate localization and proper output formatting.

## Foundational Learning
- **3D Rendering for Data Generation**: Why needed: Creates controlled, high-quality image pairs with guaranteed differences for training; Quick check: Verify Blender pipeline can generate the claimed ~4.8K pairs with specified curriculum splits
- **Curriculum Learning**: Why needed: Addresses sparsity of difference signals by gradually increasing task complexity; Quick check: Monitor training stability across single→double→mixed stages
- **Hungarian Matching for Evaluation**: Why needed: Ensures accurate pairing of predicted vs. ground-truth differences for F1/IoU computation; Quick check: Validate reward computation on sample predictions
- **GRPO with KL-regularization**: Why needed: Enables reinforcement learning with output constraints while preventing reward hacking; Quick check: Monitor KL penalty during training
- **Bounding Box Format Parsing**: Why needed: Standardizes output format for consistent evaluation and reward computation; Quick check: Test parser robustness on various valid/invalid formats
- **Reward Function Design**: Why needed: Balances accuracy and format compliance to encourage both correct localization and proper output structure; Quick check: Verify reward weights (α, λ₁, λ₂) produce stable gradients

## Architecture Onboarding

**Component Map**: Blender pipeline -> JSON config -> 3D scene generation -> Image pair rendering -> Bounding box extraction -> JSON dataset -> GRPO training loop -> Reward computation -> Model output parsing -> Evaluation benchmarks

**Critical Path**: Blender data generation → Curriculum training (single→double→mixed) → Reward computation (F1+IoU+Hungarian) → GRPO optimization with KL-regularization → Evaluation on downstream benchmarks

**Design Tradeoffs**: Synthetic 3D-rendered data vs. real photographs (controlled vs. natural complexity), curriculum staged progression vs. mixed difficulty (stability vs. efficiency), GRPO vs. supervised learning (flexibility vs. sample efficiency)

**Failure Signatures**: Early training reward collapse to zero, model outputting free-text instead of bbox format, poor transfer to real-world images, overfitting to synthetic rendering artifacts

**First Experiments**:
1. Implement minimal Blender generator with basic scene config to verify ~5K pair generation capability
2. Run reward computation pipeline on sample outputs to validate F1+IoU combination
3. Train GRPO on single-difference subset with default hyperparameters to establish baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important issues remain unresolved based on the implementation details and experimental setup:

### Open Question 1
- Question: How effectively does differential grounding trained on synthetic 3D-rendered scenes transfer to real-world natural images with complex textures, lighting, and occlusions?
- Basis in paper: [inferred] The data generation pipeline uses 3D rendering with procedurally generated scenes containing controlled discrepancies, but all evaluation benchmarks (RefCOCO, HalBench, V*, etc.) use real photographs. The paper does not address whether the synthetic-to-real domain gap limits performance gains.
- Why unresolved: The paper demonstrates benchmark improvements but does not analyze whether models learn features specific to the simplified rendering environment that may not fully transfer to natural image statistics.
- What evidence would resolve it: A controlled comparison evaluating models trained on synthetic vs. real differential image pairs, or an analysis of feature representations learned from synthetic data versus those needed for real-world perception.

### Open Question 2
- Question: Would DiG training be effective across diverse MLLM architectures beyond Qwen3-VL, particularly for models with different vision encoders or LLM backbones?
- Basis in paper: [inferred] All experiments use Qwen3-VL-4B-Thinking and Qwen3-VL-8B-Thinking exclusively. The paper states this choice but does not investigate whether the differential grounding task and reward formulation are architecture-agnostic.
- Why unresolved: Different vision encoders (e.g., CLIP, SigLIP, InternViT) have varying spatial resolution and feature granularity, which may affect how effectively they learn from the bounding-box localization signal.
- What evidence would resolve it: Experiments applying DiG training to models with different vision-language architectures such as LLaVA, InternVL, or proprietary models with distinct encoder designs.

### Open Question 3
- Question: What is the optimal curriculum structure—number of stages, difficulty progression rate, and sample allocation—for maximizing fine-grained perception gains?
- Basis in paper: [explicit] The paper states: "we employ a curriculum-based difficulty scheduling strategy" with three stages (single, double, mixed), but provides no ablation on alternative curriculum designs or theoretical justification for this specific progression.
- Why unresolved: The ablation study compares curriculum stages incrementally but does not explore whether different staging strategies (e.g., more gradual transitions, different difficulty metrics, or adaptive scheduling) could yield better convergence or final performance.
- What evidence would resolve it: Systematic ablations varying the number of curriculum stages, the rate of difficulty increase, and the proportion of training data allocated to each stage.

## Limitations
- Critical hyperparameters (reward weights α, λ₁, λ₂; GRPO group size G; learning rate; KL coefficient β) are referenced to an unavailable appendix
- Blender pipeline specifics (scene templates, asset libraries, JSON schema) are not provided for reproduction
- No statistical significance measures or variance reporting for benchmark improvements
- Transferability claims lack ablation studies showing which components drive improvements

## Confidence

- **High confidence**: Core conceptual framework of DiG and 3D rendering approach are clearly specified and technically sound
- **Medium confidence**: Reported benchmark improvements are plausible but magnitude cannot be independently verified without hyperparameters
- **Low confidence**: Transferability claims to downstream tasks lack supporting evidence beyond reported score increases

## Next Checks

1. Implement a minimal Blender data generator using the described JSON-based configuration system to verify the claimed ~4.8K pair generation capability and curriculum split distributions
2. Reproduce the reward computation pipeline with configurable α, λ₁, λ₂ parameters on sample model outputs to validate the F1+IoU combination with Hungarian matching achieves the reported training stability
3. Run GRPO training with conservative default hyperparameters (α=0.5, λ₁=λ₂=0.5, G=8, learning rate=1e-5, β KL=0.01) on the single-difference curriculum subset and measure early training dynamics to establish baseline performance before scaling to full curriculum