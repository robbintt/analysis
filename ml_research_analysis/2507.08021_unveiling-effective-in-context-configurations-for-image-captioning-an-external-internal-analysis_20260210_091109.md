---
ver: rpa2
title: 'Unveiling Effective In-Context Configurations for Image Captioning: An External
  & Internal Analysis'
arxiv_id: '2507.08021'
source_url: https://arxiv.org/abs/2507.08021
tags:
- image
- arxiv
- in-context
- attention
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how In-Context Example (ICE) configurations
  affect multimodal Large Language Models (LMMs) on image captioning. The study combines
  external experiments, varying ICE retrieval, caption assignment, and shot numbers,
  with internal attention analysis to quantify model behavior.
---

# Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis

## Quick Facts
- **arXiv ID**: 2507.08021
- **Source URL**: https://arxiv.org/abs/2507.08021
- **Reference count**: 40
- **Primary result**: In-context example (ICE) configurations significantly affect LMM image captioning; increasing shots improves linguistic coherence but reduces visual-text alignment; similarity-based retrieval inflates CIDEr scores via text-copying shortcuts; ICE quality and caption style sensitivity induce multimetric conflicts; VCAR metric quantifies visual-text decoupling.

## Executive Summary
This paper investigates how in-context example (ICE) configurations affect multimodal Large Language Models (LMMs) on image captioning. The study combines external experiments, varying ICE retrieval, caption assignment, and shot numbers, with internal attention analysis to quantify model behavior. Six key findings emerged: increasing shots improves linguistic coherence but reduces visual-text alignment; pre-training corpus biases induce multimodal disparities; ICE quality sensitivity triggers multimetric conflicts; linguistic style imitation undermines visual fidelity; similarity-based retrieval inflates CIDEr scores via shortcuts; and similarity retrieval amplifies ICE caption influence. Internally, anchor token and emergent attention window patterns were identified, and attention-based metrics (ACAR, IEAR, VCAR) were developed. The study also explored lightweight model acceleration via attention pruning. The dual external/internal approach reveals nuanced performance dynamics and offers guidance for future LMM design and training.

## Method Summary
The study evaluates OpenFlamingo v2-9B and IDEFICS v1-9B on MSCOCO image captioning using 4-32 in-context examples. ICEs are retrieved via random sampling or CLIP-based similarity, and assigned human-written or machine-generated captions. External metrics (CIDEr, CLIPScore, CHAIR, ShortCut-CIDEr) measure linguistic accuracy, visual alignment, and hallucination. Internal metrics (ACAR, IEAR, VCAR) analyze attention patterns. Experiments run on RTX-3090 with FP16 precision, temperature 0.2 for OpenFlamingo and 0.1 for IDEFICS. The approach integrates ablation studies with attention flow analysis to reveal how ICE configurations affect generation quality.

## Key Results
- Increasing in-context examples improves linguistic coherence but degrades visual-text alignment, causing modalities to decouple.
- Pre-training corpus characteristics (sequence length, alignment quality) dictate model robustness and hallucination susceptibility.
- Similarity-based image retrieval inflates CIDEr scores via text-copying shortcuts, not improved visual reasoning.
- ICE caption quality and style sensitivity trigger multimetric conflicts, undermining consistent evaluation.
- Visual-to-Caption Attention Ratio (VCAR) quantifies multimodal decoupling and correlates with hallucination.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Similarity-based image retrieval inflates CIDEr scores by exploiting text-copying shortcuts, not by improving visual reasoning.
- Mechanism: When in-context example (ICE) images are visually similar to the query image, the model's cross-attention mechanism treats them as nearly identical features, enabling a shortcut. The model then relies on copying text patterns from ICE captions rather than attending to the query image. This yields high n-gram overlap (CIDEr) but poor visual-semantic alignment (CLIPScore) and increased hallucination.
- Core assumption: The paper assumes that high n-gram similarity to ground truth (CIDEr) is not synonymous with accurate visual grounding and that model attention can be "tricked" by visual similarity into prioritizing text copying.
- Evidence anchors:
  - [abstract]: "...similarity-based retrieval inflates CIDEr scores via shortcuts..."
  - [section]: Finding 5: "While using visually similar ICE images increases CIDEr scores, this is essentially due to short-cut phenomenon where captions from ICE are copied. This undermines the utilization and accuracy of visual information and exacerbates hallucination."
  - [corpus]: Corpus analysis confirms this is an active area of concern but provides limited direct mechanistic evidence for this specific shortcut behavior in LMMs. Related work (e.g., "What do vision-language models see in the context?") focuses on broader ICL effectiveness.
- Break condition: This mechanism weakens if the VCAR (Visual-to-Caption Attention Ratio) metric shows high visual attention despite high visual similarity between ICE and query images.

### Mechanism 2
- Claim: Increasing in-context examples primarily improves linguistic fluency but degrades visual-text alignment, causing modalities to decouple.
- Mechanism: More shots provide richer linguistic patterns for imitation, boosting grammatical correctness and style matching (CIDEr). However, this also introduces more textual noise, distracts from the specific query image, and amplifies the language module's inherent dominance over the visual module. The result is a more fluent but less visually faithful caption.
- Core assumption: The model's language module has a pre-existing bias and greater capacity than the vision module, causing it to favor linguistic priors over visual evidence when context is abundant.
- Evidence anchors:
  - [abstract]: "increasing shots improves linguistic coherence but reduces visual-text alignment"
  - [section]: Finding 1: "Decoupled Modality Gains in a More-shot Setting... increasing the number of shots improves linguistic coherence but degrades visual-text alignment and increases hallucination..."
  - [corpus]: This aligns with "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", which notes ICL performance "does not always improve monotonically with increasing examples" and models can be "overwhelmed."
- Break condition: Fails if models with stronger vision modules or different training show monotonically improving CLIPScore alongside CIDEr with more shots.

### Mechanism 3
- Claim: Pre-training corpus characteristics (e.g., sequence length, image-text alignment quality) dictate a model's robustness and its susceptibility to hallucinations.
- Mechanism: A corpus with short sequences (e.g., OpenFlamingo's MMC4) limits long-context reasoning, making the model fragile to many-shot settings. Poor image-text alignment (e.g., MMC4's duplicates) weakens visual grounding, forcing reliance on text shortcuts. Conversely, a corpus with long, well-aligned sequences (e.g., IDEFICS's OBELICS) supports better many-shot performance and stronger visual-text coupling.
- Core assumption: Observed performance differences between architecturally similar models (OpenFlamingo, IDEFICS) are primarily caused by their pre-training data differences.
- Evidence anchors:
  - [abstract]: "...pre-training corpus biases induce multimodal disparities..."
  - [section]: Finding 2 and Section IV-B1 detail how MMC4's shorter sequences and poor alignment cause OpenFlamingo to struggle with long shots, while IDEFICS (OBELICS) handles these better.
  - [corpus]: This is a key distinction. Related corpus papers like "Advancing Multimodal In-Context Learning..." focus on demonstration selection, not pre-training data analysis as a causal factor for these specific behaviors.
- Break condition: If fine-tuning on a different corpus reverses behavioral traits (e.g., OpenFlamingo becomes robust to long contexts), then architectural differences may play a larger role than posited.

## Foundational Learning

- **Concept: Large Multimodal Models (LMMs) & In-Context Learning (ICL)**
  - Why needed here: This paper evaluates and explains ICL in LMMs. One must understand that ICL is the ability to perform a task using a few prompt examples without weight updates, and LMMs extend this to image-text pairs.
  - Quick check question: How does ICL differ from fine-tuning?

- **Concept: Cross-Attention & Gated Cross-Attention**
  - Why needed here: The paper explains the Flamingo-style LMM architecture which uses gated cross-attention to integrate visual features into the language model. This mechanism is central to how ICEs influence generation and how the VCAR metric is derived.
  - Quick check question: In a Transformer, how does cross-attention allow one modality (e.g., images) to influence the generation of another (e.g., text)?

- **Concept: Evaluation Metrics (CIDEr, CLIPScore, Hallucination)**
  - Why needed here: The paper's conclusions rely on interpreting different metrics. Understanding that CIDEr measures n-gram overlap with ground truth (linguistic) while CLIPScore measures image-caption alignment (visual) is critical to following the "decoupled gains" and "shortcut" arguments.
  - Quick check question: Why might a caption have a high CIDEr score but still be considered a "hallucination" or of poor visual quality?

## Architecture Onboarding

- **Component map:** Pre-trained Vision Encoder (e.g., CLIP ViT) -> Gated Cross-Attention layers -> Pre-trained Language Model (e.g., LLaMA). During inference, query image and interleaved ICEs (image-text pairs) are fed in. Vision Encoder processes all images. GCA layers allow text tokens in LM to attend to relevant visual features from their corresponding images.

- **Critical path:** Tokenize input (ICE text and image placeholders), encode images, pass combined sequence through LM blocks. In GCA layers, query is text hidden state, keys/values from paired image features. Output depends on attention flow: query text tokens to query image tokens (desired) vs. to ICE text tokens (shortcut).

- **Design tradeoffs:**
  - **Visual vs. Textual Dominance:** LM is typically much larger than vision encoder, creating bias toward linguistic priors. GCA must be strong enough to override this for visual tasks.
  - **Pre-training Data:** Using corpus with short sequences (MMC4) enables cheaper pre-training but limits context window robustness. Using high-alignment corpus (OBELICS) improves visual grounding but requires more curation effort.
  - **ICE Retrieval:** Similarity-based retrieval (SIIR) seems intuitive but encourages text copying. Random retrieval (RS) is more robust but may offer less linguistic guidance.

- **Failure signatures:**
  - **High CIDEr, Low CLIPScore:** Strong indicator of "shortcut" behavior (copying ICE captions).
  - **Increasing Hallucinations with More Shots:** Suggests model is overwhelmed by context or vision module is weak and being overridden by linguistic noise.
  - **Sensitivity to ICE Caption Style:** If model's output style changes drastically with ICE caption style (e.g., short vs. long) regardless of image, it indicates failure of visual grounding.

- **First 3 experiments:**
  1. **Metric Decoupling Test:** Run ICL with 4, 8, 16, and 32 shots using random retrieval and human captions. Plot CIDEr and CLIPScore. Look for "scissors" effect (CIDEr up, CLIPScore down) to validate decoupled modality gain mechanism.
  2. **Shortcut Probe:** Run ICL using Similarity-based Image Retrieval (SIIR). Calculate "Short-cut CIDEr" score by comparing generated captions against ICE captions. High scores confirm shortcut mechanism.
  3. **Corpus Robustness Check:** Compare two LMMs with known pre-training differences (e.g., sequence length). Evaluate hallucination rates and VCAR metrics under high-shot (e.g., 32-shot) setting. Check if model trained on shorter sequences degrades more sharply, validating pre-training corpus bias mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific bottlenecks prevent attention-based pruning strategies from yielding significant inference acceleration in current Large Multimodal Models?
- **Basis in paper:** [Explicit] The authors note that despite reducing KV cache occupancy by over 50%, the acceleration effect was "not as pronounced as expected" and less effective than in LLM studies.
- **Why unresolved:** The paper hypothesizes reasons (KV-cache efficiency, short context windows of current LMMs) but does not empirically isolate whether the bottleneck is compute-bound, memory-bound, or due to the visual encoder.
- **What evidence would resolve it:** A detailed latency profiling analysis comparing time spent on visual encoding versus decoder attention layers during the pruned inference process.

### Open Question 2
- **Question:** What specific multi-factorial mechanisms drive hallucinations in multimodal in-context learning beyond the Visual-to-Caption Attention Ratio (VCAR)?
- **Basis in paper:** [Explicit] Section III-C3 states that VCAR serves as a "partial proxy" and that "the underlying mechanisms driving hallucinations demonstrate multi-factorial etiology that cannot be fully captured by this singular metric."
- **Why unresolved:** The paper establishes VCAR as a useful but incomplete correlational metric; it does not identify the other internal attention flows or neuron activations that combine with low VCAR to cause hallucination.
- **What evidence would resolve it:** Ablation studies that intervene on specific attention heads (e.g., freezing attention patterns) to see which configurations decouple VCAR scores from hallucination rates.

### Open Question 3
- **Question:** How does the "Emergent Attention Window" pattern change when LMMs are scaled to handle "many-shot" contexts (e.g., 1000+ shots)?
- **Basis in paper:** [Inferred] The authors identify the "Emergent Attention Window" as a key pattern where tokens attend within their own ICE. They note current LMMs struggle with long contexts (32 shots is near the limit), leaving the behavior in true "many-shot" regimes unexplored.
- **Why unresolved:** The study is limited to 4-32 shots due to model constraints; it is unknown if the window pattern collapses or intensifies when hundreds of examples compete for attention.
- **What evidence would resolve it:** Experiments on LMMs with extended context windows (e.g., 128k tokens) analyzing the IEAR metric across varying shot densities.

## Limitations
- Attributing behavioral differences primarily to pre-training corpus may underestimate architectural or fine-tuning variations.
- Machine-generated caption experiments depend on unspecified Transformer model architecture and training details.
- Findings focus on image captioning as a single task, limiting generalizability to other multimodal tasks.
- Mechanistic claims about shortcut behavior rely on indirect inference from metric decoupling rather than direct attention flow measurement.

## Confidence
- **High confidence**: Decoupled modality gains with increased shots (Finding 1), pre-training corpus effects on multimodal robustness (Finding 2), ICE caption style sensitivity (Finding 4)
- **Medium confidence**: Shortcut behavior with similarity-based retrieval (Finding 5), ICE quality sensitivity (Finding 3), attention-based metric validity
- **Low confidence**: Generalizability across tasks, precise quantification of shortcut mechanisms, long-context behavior beyond 32 shots

## Next Checks
1. **Direct attention verification**: Run the SIIR experiment and explicitly visualize cross-attention maps for query tokens attending to ICE captions vs. query images. Confirm that visual similarity triggers text-copying shortcuts rather than visual reasoning.

2. **Corpus intervention test**: Fine-tune OpenFlamingo on a subset of OBELICS (long sequences, high alignment) and re-evaluate its performance under 32-shot settings. Check if pre-training data differences alone explain the behavioral disparities observed between OpenFlamingo and IDEFICS.

3. **Task transfer check**: Repeat the key experiments (shot scaling, SIIR vs RS, corpus comparison) on a visual question answering task using the same models. Assess whether the identified mechanisms generalize beyond image captioning.