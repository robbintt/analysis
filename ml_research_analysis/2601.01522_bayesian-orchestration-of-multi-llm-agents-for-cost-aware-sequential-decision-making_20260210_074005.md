---
ver: rpa2
title: Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making
arxiv_id: '2601.01522'
source_url: https://arxiv.org/abs/2601.01522
tags:
- cost
- decision
- prior
- framework
- screen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Bayesian framework for orchestrating multiple
  LLMs in cost-aware sequential decision-making, addressing the limitations of current
  discriminative approaches. The core innovation is eliciting likelihood functions
  via contrastive prompting, aggregating across diverse models using robust statistics,
  and applying Bayesian updating to enable cost-sensitive action selection and principled
  information gathering.
---

# Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making

## Quick Facts
- arXiv ID: 2601.01522
- Source URL: https://arxiv.org/abs/2601.01522
- Reference count: 40
- Primary result: 34% cost reduction ($294,000 savings) and 45% improved demographic parity vs. single-LLM baseline on synthetic resume screening

## Executive Summary
This paper introduces a Bayesian framework for orchestrating multiple large language models (LLMs) in cost-aware sequential decision-making tasks. The core innovation lies in eliciting likelihood functions from diverse LLMs using contrastive prompting, aggregating these through robust statistics, and applying Bayesian updating to enable cost-sensitive action selection. Experiments on 1,000 synthetic resumes demonstrate significant cost savings, improved demographic fairness, and better-calibrated beliefs compared to single-LLM baselines. The approach addresses key limitations of current discriminative methods by enabling principled uncertainty quantification and information gathering.

## Method Summary
The framework elicits likelihood functions from multiple LLMs via contrastive prompting, asking each model to rate the typicality of an observation given a candidate's true class. These raw scores are normalized to approximate likelihood ratios, which are then aggregated using robust statistics like median or trimmed mean. Bayesian updating combines the aggregated likelihoods with prior beliefs to form posterior distributions over actions. Cost-aware decision rules select the optimal action based on expected costs, while a value-of-information approximation determines when to gather additional information. The framework explicitly models prior-mismatch and enables information gathering to reduce uncertainty before committing to decisions.

## Key Results
- 34% cost reduction ($294,000 savings) compared to best single-LLM baseline
- 45% improvement in demographic parity (0.33 to 0.48)
- Expected Calibration Error reduced from 0.18 to 0.09
- Multi-LLM aggregation contributed 51% of savings, sequential updating 43%, disagreement-triggered screening 20%

## Why This Works (Mechanism)
The framework succeeds by leveraging diverse LLM perspectives to create more robust likelihood estimates than any single model. By eliciting full likelihood functions rather than point classifications, the approach captures epistemic uncertainty and enables principled Bayesian updating. The contrastive prompting method helps calibrate LLM outputs by providing context about typical vs. atypical observations. Aggregating across models using robust statistics reduces the impact of individual model biases or errors. The cost-aware decision rules ensure resources are allocated efficiently, while the value-of-information approximation enables targeted information gathering only when it's likely to change the decision.

## Foundational Learning
- **Likelihood elicitation via contrastive prompting**: Needed to convert LLM outputs into usable probability estimates. Quick check: Are elicited scores correlated with ground-truth probabilities?
- **Robust multi-model aggregation**: Required to combine diverse LLM perspectives while handling model-specific biases. Quick check: Does median aggregation outperform simple averaging?
- **Bayesian updating with approximate likelihoods**: Enables principled belief updating despite imperfect likelihood estimates. Quick check: How sensitive are posteriors to the normalization heuristic?
- **Cost-aware sequential decision making**: Ensures resource allocation matches decision importance. Quick check: Does VOI approximation correctly predict when information gathering changes decisions?
- **Demographic parity metrics in sequential settings**: Needed to evaluate fairness beyond single decisions. Quick check: Does fairness improvement persist across different demographic groups?

## Architecture Onboarding

**Component Map**: LLM Elicitation -> Likelihood Normalization -> Robust Aggregation -> Bayesian Updating -> Cost-Aware Decision -> VOI Check -> (Information Gathering -> Resume Screening ->) Output

**Critical Path**: The core decision pipeline flows from eliciting likelihoods through Bayesian updating to cost-sensitive action selection. Information gathering occurs when VOI exceeds a threshold, followed by resume screening and final decision.

**Design Tradeoffs**: The framework trades computational overhead (querying multiple LLMs) for improved accuracy and fairness. The normalization heuristic is simple but may sacrifice calibration precision. The binary VOI approximation is computationally efficient but may oversimplify information value.

**Failure Signatures**: Poor performance when LLM likelihood estimates are systematically biased or when the normalization heuristic fails. The framework may also underperform if the value-of-information approximation misestimates the benefit of additional information.

**3 First Experiments**:
1. Compare performance when using different aggregation methods (median vs. mean vs. weighted average)
2. Test sensitivity to the normalization heuristic by varying the scaling factor
3. Evaluate impact of VOI threshold on information gathering efficiency and total cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed contrastive likelihood elicitation framework generalize effectively to domains beyond hiring, such as medical triage or credit underwriting?
- Basis in paper: The authors list "Cross-domain transfer" as an open question, asking, "How well do likelihood elicitation prompts transfer to medical triage, credit underwriting... Can we develop domain-agnostic prompt templates?"
- Why unresolved: The experiments were restricted to synthetic resume screening; the prompt templates were domain-specific (software engineering recruiting), and it remains unproven if the "typicality" framing works equally well for clinical symptoms or financial transaction features.
- What evidence would resolve it: Experimental results applying the same framework to datasets from medical triage (e.g., patient symptom logs) or fraud detection (e.g., transaction logs) showing comparable cost reductions and calibration.

### Open Question 2
- Question: Can the calibration and accuracy of the elicited likelihood estimates be improved beyond the simple 0â€“10 normalization heuristic?
- Basis in paper: In Section 4.3.1 ("Approximate Likelihoods"), the authors note that LLM scores are uncalibrated and coarse. They propose future work on "Calibration techniques" or "Fine-tuning for likelihood estimation" to map raw scores to proper probabilities.
- Why unresolved: The current method relies on a heuristic normalization (dividing the score by 10) which lacks rigorous probabilistic grounding, yet the authors note the framework is robust to this imperfection.
- What evidence would resolve it: A comparison of the current heuristic against methods like Platt scaling or isotonic regression applied to the LLM outputs, measuring the resulting change in Expected Calibration Error (ECE) and decision cost.

### Open Question 3
- Question: Do the reported cost savings and fairness improvements persist when validated on real-world data with long-term outcome tracking?
- Basis in paper: Section 4.3.4 ("Synthetic Data and Generalization") lists "Real-world deployment" as a limitation. The authors state they used synthetic data validated by experts on a subset, but note "Real-world deployment would provide additional insights."
- Why unresolved: The 1,000 resumes were synthetic, and while validated by human recruiters (89% agreement), they may not capture the full complexity, noise, or distribution shifts of actual applicant pools over time.
- What evidence would resolve it: A field study or retrospective analysis using actual historical hiring data (resumes and eventual employee performance metrics) to calculate the realized cost savings versus the theoretical savings.

### Open Question 4
- Question: Can more sophisticated Value-of-Information (VOI) approximations outperform the proposed binary informativeness model?
- Basis in paper: The authors acknowledge in Section 4.3.3 that their VOI approximation is moderate ($r=0.63$) and suggests "Learned VOI models" or "Monte Carlo VOI estimation" as future directions to improve over the simplified binary model.
- Why unresolved: The current model assumes information gathering is either fully revealing (probability $\rho$) or uninformative. This simplification may misestimate the value of screening in cases with partial or ambiguous information.
- What evidence would resolve it: An ablation study comparing the binary model against a Monte Carlo estimation of VOI (sampling possible observations $z$), measuring the resulting difference in information gathering efficiency and total decision cost.

## Limitations
- Experiments rely solely on synthetic resumes, not real-world data
- No validation of performance on long-term outcome tracking
- Framework's robustness to adversarial inputs and noisy likelihood functions not explored
- Limited testing on domains beyond hiring/recruitment

## Confidence
- High confidence in the mathematical formulation and reported experimental results within the synthetic domain
- Medium confidence in generalizability of cost savings and demographic improvements to real-world applications, pending further validation
- Low confidence in framework's robustness to adversarial scenarios and noisy inputs, as these are not addressed in the current study

## Next Checks
1. Test the framework on a real-world dataset to assess performance and cost savings outside the synthetic domain
2. Evaluate the robustness of the Bayesian orchestration framework to adversarial inputs and noisy likelihood functions
3. Investigate the scalability and adaptability of the framework to other sequential decision-making tasks beyond recruitment