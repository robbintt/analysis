---
ver: rpa2
title: 'REANIMATOR: Reanimate Retrieval Test Collections with Extracted and Synthetic
  Resources'
arxiv_id: '2504.07584'
source_url: https://arxiv.org/abs/2504.07584
tags:
- retrieval
- test
- tables
- relevance
- collections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REANIMATOR is a framework for revitalizing existing IR test collections
  by extracting additional resources (tables, full texts, captions, in-text references)
  from PDF documents and generating synthetic relevance labels using large language
  models. The framework is demonstrated by transforming TREC-COVID into TREC-COVID+,
  enabling table retrieval and retrieval-augmented generation (RAG) applications.
---

# REANIMATOR: Reanimate Retrieval Test Collections with Extracted and Synthetic Resources

## Quick Facts
- arXiv ID: 2504.07584
- Source URL: https://arxiv.org/abs/2504.07584
- Reference count: 40
- Key outcome: REANIMATOR transforms TREC-COVID into TREC-COVID+ by extracting tables and generating synthetic relevance labels, achieving 95% parsing accuracy and Elo scores of 1605 (table-enhanced) vs. 1190 (text-only).

## Executive Summary
REANIMATOR is a framework that revitalizes existing IR test collections by extracting structured resources (tables, full texts, captions, in-text references) from PDF documents and generating synthetic relevance labels using large language models. The framework is demonstrated by transforming the TREC-COVID test collection into TREC-COVID+, enabling table retrieval and retrieval-augmented generation (RAG) applications. With 95% accuracy for valid table parsing and inter-rater agreement comparable to human annotators, the framework shows promise for cost-effective test collection expansion and multi-modal retrieval evaluation.

## Method Summary
REANIMATOR extracts structured data from PDFs using Docling, chunks text into 512-character passages, and retrieves candidates using BM25 and cosine similarity with query variations. An LLM ensemble generates 4-level relevance labels via majority voting. For RAG evaluation, top candidates are formatted with tables and passed to Claude-3.5-Sonnet for answer generation, with output usefulness ranked via Elo scoring. The pipeline was applied to 64,358 biomedical documents, creating the TREC-COVID+ collection with 19,916 queries for table retrieval and 4,798 for RAG.

## Key Results
- Tables were parsed from 64,358 documents with 95% accuracy for valid tables.
- Synthetic relevance labels achieved inter-rater agreement comparable to human annotators (Cohen's Kappa 0.35-0.58).
- RAG configurations incorporating tables produced more useful outputs (Elo score 1605 vs. 1190 for text-only), with responses averaging 323 tokens when using tables versus 226-252 tokens for text-only.

## Why This Works (Mechanism)

### Mechanism 1: PDF-to-Structured-Data Extraction Pipeline
- Claim: Parsing PDF documents into machine-readable tables, captions, and in-text references expands test collection utility for multi-modal retrieval.
- Mechanism: Docling analyzes page layout, reading order, and table structure, then stores extracted resources in a relational database with binary PDF compression. This creates retrievable units (tables + context) not available in original JSON full-text parses.
- Core assumption: Modern PDF parsers can reliably identify table boundaries and associate contextual metadata (captions, in-text references) despite varied scientific article layouts.
- Evidence anchors:
  - [abstract] "Tables were parsed from 64,358 documents with 95% accuracy for valid tables."
  - [section 4.2] "62.94% were parsed perfectly... roughly 95% of actual tables are at least substantially correct."
  - [corpus] Weak/missing; neighbors (TalentMine, CORE-T) address table extraction/QA but do not validate Docling-specific accuracy.
- Break condition: Non-standard layouts (complex merged cells, nested tables) or scanned PDFs without OCR will reduce parsing accuracy below 95%, degrading retrieval quality.

### Mechanism 2: LLM-Ensemble Relevance Labeling
- Claim: An ensemble of LLMs can assign relevance judgments with inter-rater agreement comparable to human annotators, enabling cost-effective test collection expansion.
- Mechanism: UMBRELA-style prompts guide multiple LLMs (GPT-4o, GPT-4o-mini, o3-mini, open-source models) to label candidates on a 4-level scale. Majority voting aggregates predictions into final relevance labels.
- Core assumption: Prompted LLMs can operationalize relevance criteria (irrelevant, related, highly relevant, perfectly relevant) consistently with human judgment patterns.
- Evidence anchors:
  - [abstract] "Synthetic relevance labels were generated with inter-rater agreement comparable to human annotators (Cohen's Kappa 0.35-0.58)."
  - [section 4.3] "The best models perform on par with human raters, on similar Cohen's Kappa score levels."
  - [corpus] GenTREC (neighbor) demonstrates LLM-generated test collections but notes synthetic data bias risks; supports feasibility with caveats.
- Break condition: Topics requiring deep domain expertise or subjective interpretation (e.g., clinical nuance) may yield low Kappa scores, necessitating human-in-the-loop validation.

### Mechanism 3: Table-Enhanced RAG Usefulness via Elo Ranking
- Claim: Retrieval configurations including tables produce RAG outputs rated more useful than text-only baselines.
- Mechanism: Top-10 candidates from BM25 or cosine similarity are formatted with table captions/references and passed to Claude-3.5-Sonnet for answer generation. LLM-based pairwise comparisons across six criteria produce Elo scores reflecting relative usefulness.
- Core assumption: Elo ratings from LLM pairwise judgments correlate with human perceptions of answer utility.
- Evidence anchors:
  - [abstract] "RAG experiments showed that retrieval configurations incorporating tables produced more useful outputs (Elo score 1605 vs. 1190 for text-only)."
  - [section 5.3] "Cosinetable achieves the highest Elo score (1604.8)... Text-only retrieval ranks lower."
  - [corpus] mmRAG neighbor evaluates multi-modal RAG but uses different metrics; not direct validation.
- Break condition: Poorly parsed tables or missing captions cause LLM misinterpretation, potentially increasing token count without improving (or even reducing) usefulness.

## Foundational Learning

**Concept: Cranfield Paradigm (Test Collections)**
- Why needed here: REANIMATOR extends the classic IR evaluation triad (documents, topics, judgments) with new modalities.
- Quick check question: Why can't we reuse original document-level relevance labels for extracted tables?

**Concept: Inter-Annotator Agreement (Cohen's Kappa)**
- Why needed here: Quantifies synthetic label quality against human benchmarks.
- Quick check question: What does Kappa 0.35 (4-level) vs. 0.58 (binary) suggest about annotation difficulty?

**Concept: Reciprocal Rank Fusion (RRF)**
- Why needed here: Combines rankings from multiple retrievers and query variants into unified candidate pools.
- Quick check question: How does RRF mitigate biases of individual retrieval models?

## Architecture Onboarding

**Component map:**
Existing test collection -> DOI list -> PDF acquisition (OpenAlex/Unpaywall) -> Docling extraction -> Storage (relational DB) -> Chunking (512 chars) -> Pooling (BM25 + cosine + RRF) -> LLM labeling -> Downstream tasks (table retrieval, RAG)

**Critical path:** PDF availability -> Docling parsing -> table/context extraction -> chunking -> pooling -> LLM labeling -> downstream tasks. Parsing failures cascade to retrieval and evaluation.

**Design tradeoffs:**
- Proprietary vs. open LLMs: GPT-4o (0.28¢/table assessment) vs. Qwen2.5-14B (5.9s local inference) — cost vs. latency vs. agreement quality
- Human-in-the-loop: Optional validation step increases reliability but adds ~1 minute per judgment
- Chunk size: 512 chars balances granularity with retrieval efficiency; larger chunks may lose precision

**Failure signatures:**
- Parsing accuracy <90% for valid tables (layout issues)
- Human-LLM Kappa <0.3 (poor label alignment)
- Elo scores for table configurations not exceeding text-only baseline
- High percentage of tables missing captions (>30%)

**First 3 experiments:**
1. Parsing validation: Sample 500 tables from a new domain (e.g., non-biomedical); classify parsing quality (perfect/good/ok/bad) to assess generalization.
2. Labeler comparison: Run relevance assessment with GPT-4o, GPT-4o-mini, and one open model; compare Kappa against human labels on 250 pooled items.
3. Modality ablation: Execute RAG with text-only, table-only, and interleaved retrieval; compare Elo scores, token counts, and RAGAS metrics to isolate table contribution.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** How effectively does REANIMATOR generalize to non-academic domains and handle complex content like figures and equations?
- **Basis in paper:** [explicit] The authors state that their "approach does not fully capture the challenges associated with handling complex content, such as figures and equations" and that the evaluation within a specific use case may "limit the generalizability of our findings."
- **Why unresolved:** The framework was demonstrated exclusively on the scientific TREC-COVID collection, focusing on tables and text, leaving other modalities and domains untested.
- **What evidence would resolve it:** Application of the framework to diverse corpora (e.g., legal, news) and successful extraction pipelines for mathematical equations and figures.

**Open Question 2**
- **Question:** Can automatic topic generation be integrated to improve the scalability of reanimating test collections?
- **Basis in paper:** [explicit] The conclusion identifies "automatic topic generation" as a specific avenue for "future work" to "improve scalability and refine the evaluation process."
- **Why unresolved:** The current implementation relies on reusing existing topics from the legacy collection rather than synthesizing new information needs.
- **What evidence would resolve it:** An extension of the framework that creates novel, valid topics, validated through user studies or system retrieval effectiveness.

**Open Question 3**
- **Question:** To what extent do advanced semantic chunking and diverse pooling strategies improve the quality of the synthetic resources?
- **Basis in paper:** [inferred] The authors note they "prioritized strong default settings" and suggest that "more advanced techniques, such as improved semantic chunking and a more diverse pooling strategy, could further enhance the analysis."
- **Why unresolved:** The study established a baseline using fixed-size chunks (512 characters) and limited pooling models (BM25 and Cosine).
- **What evidence would resolve it:** Ablation studies comparing the default configuration against methods using variable-size semantic chunking and heterogeneous pooling.

## Limitations

- The 95% parsing accuracy figure applies specifically to biomedical PDFs and may not generalize to other scientific domains with different table structures.
- The framework's reliance on open-access PDFs limits applicability to paywalled content, and the 64,358 document corpus represents a single domain (biomedical).
- The RAG evaluation uses LLM-based Elo scoring, which may not perfectly align with human perceptions of usefulness, and the 323-token average response length could indicate verbosity rather than quality.

## Confidence

**High confidence**: PDF-to-structured-data extraction pipeline (95% accuracy empirically validated)
**Medium confidence**: LLM-ensemble relevance labeling (Kappa comparable to humans but still moderate)
**Medium confidence**: Table-enhanced RAG usefulness (Elo scores show improvement but LLM-based evaluation introduces uncertainty)

## Next Checks

1. Test Docling parsing accuracy on 500 tables from non-biomedical domains (e.g., social sciences, engineering) to assess cross-domain generalization of the 95% accuracy claim.
2. Conduct human evaluation of 100 RAG outputs from both table-enhanced and text-only configurations to validate whether Elo scores correlate with human usefulness judgments.
3. Run the full pipeline on a small test collection (50 documents) from a domain with different table layouts to identify potential parsing failures before scaling.