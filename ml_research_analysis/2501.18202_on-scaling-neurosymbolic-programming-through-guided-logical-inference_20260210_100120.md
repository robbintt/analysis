---
ver: rpa2
title: On Scaling Neurosymbolic Programming through Guided Logical Inference
arxiv_id: '2501.18202'
source_url: https://arxiv.org/abs/2501.18202
tags:
- dpnl
- oracle
- unknown
- digit
- pwmc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPNL, a novel approach for exact probabilistic
  reasoning in neurosymbolic learning that bypasses the computation of logical provenance
  formulas. Instead, DPNL uses an oracle-guided, recursive DPLL-like decomposition
  to accelerate logical inference.
---

# On Scaling Neurosymbolic Programming through Guided Logical Inference

## Quick Facts
- arXiv ID: 2501.18202
- Source URL: https://arxiv.org/abs/2501.18202
- Reference count: 40
- DPNL enables exact probabilistic reasoning in neurosymbolic learning by bypassing logical provenance computation through oracle-guided DPLL-like decomposition

## Executive Summary
This paper introduces DPNL, a novel approach for exact probabilistic reasoning in neurosymbolic learning that bypasses the computation of logical provenance formulas. Instead, DPNL uses an oracle-guided, recursive DPLL-like decomposition to accelerate logical inference. The method is formally proven to be correct and terminating. Experiments on the MNIST-N-SUM task show that DPNL scales to larger problems than existing exact inference methods (DeepProbLog, Scallop) and achieves higher accuracy than approximate methods (A-Nesi, DPLA*, Scallop with top-k). An extension, ApproxDPNL, enables approximate reasoning with guaranteed error bounds while maintaining competitive accuracy. DPNL enables more accurate models and pushes the boundaries of exact inference, while ApproxDPNL further improves scalability with reliability guarantees.

## Method Summary
DPNL introduces a novel approach to exact probabilistic reasoning in neurosymbolic learning by replacing traditional logical provenance computation with an oracle-guided, recursive DPLL-like decomposition strategy. The method leverages a separation of concerns where the neural component handles perception while the logical component manages reasoning through guided decomposition. The core innovation lies in using oracle calls to guide the decomposition process, enabling efficient handling of complex logical theories without explicitly computing provenance formulas. The approach is formally proven to be both correct and terminating, with an extension called ApproxDPNL that provides approximate reasoning capabilities with guaranteed error bounds. The method is evaluated on the MNIST-N-SUM task, demonstrating superior scalability compared to existing exact inference methods and improved accuracy over approximate approaches.

## Key Results
- DPNL scales to larger problems than existing exact inference methods (DeepProbLog, Scallop)
- DPNL achieves higher accuracy than approximate methods (A-Nesi, DPLA*, Scallop with top-k)
- ApproxDPNL extension enables approximate reasoning with guaranteed error bounds while maintaining competitive accuracy

## Why This Works (Mechanism)
The oracle-guided decomposition allows DPNL to avoid the computational bottleneck of explicitly computing logical provenance formulas. By recursively decomposing the problem space using DPLL-like strategies guided by oracle calls, the method can efficiently navigate the solution space without exhaustive enumeration. This approach separates the neural perception component from the logical reasoning component, allowing each to specialize in its domain. The formal guarantees of correctness and termination provide confidence that the method will produce reliable results, while the ApproxDPNL extension offers a practical tradeoff between accuracy and computational efficiency when exact inference becomes intractable.

## Foundational Learning
- DPLL algorithm: A complete, backtracking-based search algorithm for deciding the satisfiability of propositional logic formulae, needed to understand the decomposition strategy
- Logical provenance: The relationship between program outputs and inputs that explains how outputs depend on inputs, avoided in DPNL to improve efficiency
- Neurosymbolic programming: Integration of neural networks with symbolic reasoning systems, the foundation of this work
- Probabilistic logic programming: Extension of logic programming to handle uncertainty, relevant for understanding the exact inference context
- Oracle-guided search: Using external guidance to direct search processes, the key innovation in DPNL
- Approximate reasoning with error bounds: Techniques for trading precision for efficiency with theoretical guarantees

## Architecture Onboarding
Component map: Neural perception -> Oracle-guided DPLL decomposition -> Logical reasoning
Critical path: Input data flows through neural network, oracle provides guidance for decomposition, DPLL algorithm navigates solution space, logical theory provides constraints
Design tradeoffs: Exact vs approximate reasoning (DPNL vs ApproxDPNL), computational efficiency vs theoretical guarantees, oracle dependency vs autonomous reasoning
Failure signatures: Incomplete decomposition leading to incorrect results, oracle providing poor guidance, computational resources exhausted during decomposition
First experiments: 1) Run DPNL on small MNIST-N-SUM problems to verify correctness, 2) Compare DPNL runtime with DeepProbLog on identical problems, 3) Test ApproxDPNL error bounds by gradually increasing approximation parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Performance benefits demonstrated only on synthetic MNIST-N-SUM problems, raising questions about generalizability
- Oracle-guided decomposition may face practical challenges when oracle calls are expensive or decomposition quality varies
- Formal proofs assume ideal oracle behavior and complete logical theory knowledge

## Confidence
- Theoretical correctness and termination guarantees: High (formally proven)
- Empirical scalability claims: Medium (limited experimental scope to synthetic tasks)
- Accuracy improvements over approximate methods: High (concrete results provided)

## Next Checks
1. Evaluate DPNL on real-world datasets beyond synthetic MNIST-N-SUM problems to assess practical applicability and generalization
2. Conduct comprehensive runtime and memory usage benchmarks to quantify the practical efficiency gains of the oracle-guided approach
3. Test DPNL's robustness to oracle noise and imperfect decomposition by introducing controlled variations in oracle responses and measuring impact on accuracy and termination