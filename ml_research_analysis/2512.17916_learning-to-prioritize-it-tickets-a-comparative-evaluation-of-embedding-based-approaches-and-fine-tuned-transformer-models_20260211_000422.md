---
ver: rpa2
title: 'Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based
  Approaches and Fine-Tuned Transformer Models'
arxiv_id: '2512.17916'
source_url: https://arxiv.org/abs/2512.17916
tags:
- clustering
- umap
- urgency
- impact
- xgboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares embedding-based approaches and a fine-tuned
  transformer model for IT ticket prioritization. Embedding-based methods, including
  clustering and supervised classifiers, performed poorly due to low-quality embeddings
  and dataset noise.
---

# Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models

## Quick Facts
- **arXiv ID**: 2512.17916
- **Source URL**: https://arxiv.org/abs/2512.17916
- **Reference count**: 5
- **Primary result**: Fine-tuned transformer achieves 78.5% average F1 and 0.80 weighted Cohen's kappa for IT ticket prioritization

## Executive Summary
This paper evaluates embedding-based approaches against a fine-tuned transformer model for IT ticket prioritization, finding that generic embeddings fail to capture ITSM domain semantics while fine-tuning XLM-RoBERTa on ticket data yields strong multi-output classification performance. The transformer model processes both textual content and numerical features (time intervals) through a dual-branch architecture, achieving 86.80% F1 for impact and 70.22% F1 for urgency classification. The study demonstrates that domain adaptation is critical for real-world ITSM applications, with embedding-based methods showing severe overfitting and poor generalization.

## Method Summary
The study compares embedding-based approaches (sentence-transformers + UMAP + clustering or supervised classifiers) against a fine-tuned transformer model. The transformer uses XLM-RoBERTa-base for text encoding with a parallel numerical feature processor, concatenated representations, and separate linear heads for impact (4 classes) and urgency (5 classes) prediction. Training used stratified 60/20/20 splits on ~26,000 ITSM tickets, with the best model selected by validation F1. Embedding-based approaches failed due to poor domain representation, while the transformer achieved 78.5% average F1 and 0.80 weighted Cohen's kappa.

## Key Results
- Fine-tuned transformer achieves 78.5% average F1-score (86.80% impact, 70.22% urgency)
- Embedding-based approaches fail with ~20-25% F1 due to poor ITSM domain representation
- Weighted Cohen's kappa reaches nearly 0.80, indicating strong agreement beyond chance
- Numerical features (time intervals) provide complementary signal to text content
- Severe class imbalance (impact class 3: 94.70% of samples) handled effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint processing of textual and numerical features improves prioritization accuracy over text-only approaches.
- Mechanism: The transformer architecture encodes text through XLM-RoBERTa while parallel numerical features (e.g., time intervals) pass through a dedicated feed-forward network. These representations are concatenated before prediction, allowing the model to learn interactions between semantic content and operational metadata.
- Core assumption: Numerical features carry complementary signal not fully recoverable from text alone.
- Evidence anchors: [abstract] "fine-tuned multilingual transformer that processes both textual and numerical features"; [Section 3.3] "The model integrates a pre-trained transformer encoder backbone... in parallel, it features a numerical feature processor and a shared feed-forward network"; [corpus] Weak direct evidence; related ITSM papers focus on text-only classification
- Break condition: If numerical features are highly correlated with text-derived features, the parallel branch adds noise without signal.

### Mechanism 2
- Claim: Domain-adapted fine-tuning outperforms generic embeddings for ITSM data with technical jargon and noisy inputs.
- Mechanism: Fine-tuning adjusts XLM-RoBERTa's weights on IT ticket data, adapting representations to domain-specific vocabulary, abbreviations, and inconsistent formatting. Generic sentence embeddings (e.g., paraphrase-multilingual-mpnet-base-v2) lack this adaptation.
- Core assumption: The pre-trained model's multilingual capabilities transfer sufficiently to begin domain adaptation.
- Evidence anchors: [abstract] "limitations of generic embeddings for ITSM data and demonstrate the effectiveness of domain-adapted transformer architectures"; [Section 5.1.4] "We suspect that the embedding outputs... are not fitted to ITSM ticket data, and are more general LLMs"; [corpus] Related paper "A centroid based framework for text classification in itsm environments" similarly addresses hierarchical taxonomy challenges in ITSM
- Break condition: If dataset is too small or labels too noisy, fine-tuning may overfit to spurious patterns.

### Mechanism 3
- Claim: Multi-output prediction with shared representation captures dependencies between impact and urgency.
- Mechanism: A shared four-layer FFN processes concatenated features before branching to separate linear heads for impact and urgency. This forces the model to learn representations useful for both tasks simultaneously.
- Core assumption: Impact and urgency share underlying semantic features but differ in final mapping.
- Evidence anchors: [Section 3.3] "shared representation is mapped to the two targets using separate linear heads in a multi-output setting"; [Table 8] Impact F1: 86.80%, Urgency F1: 70.22%—different performance levels suggest distinct prediction challenges; [corpus] No direct corpus evidence on multi-output architectures for ITSM
- Break condition: If tasks conflict (opposing gradients), shared representation may degrade both.

## Foundational Learning

- Concept: **Transformer tokenization and pooling**
  - Why needed here: Understanding how XLM-RoBERTa converts raw text to embeddings and extracts pooled representations for classification.
  - Quick check question: Can you explain why pooled output ([CLS] token or mean pooling) is used for sentence-level classification rather than individual token embeddings?

- Concept: **Class imbalance and weighted metrics**
  - Why needed here: The dataset shows severe imbalance (e.g., Impact class 3: 94.70% of samples). Standard accuracy is misleading; weighted Cohen's kappa and F1-score are preferred.
  - Quick check question: Why would a model predicting only the majority class achieve high accuracy but fail operationally for ITSM?

- Concept: **Embedding quality vs. downstream task performance**
  - Why needed here: The paper demonstrates that high-quality general embeddings do not guarantee downstream performance on domain-specific tasks.
  - Quick check question: If clustering performance depends more on embedding quality than algorithm choice, what diagnostic would you run before selecting a clustering method?

## Architecture Onboarding

- Component map:
  Input layer (concatenated text fields + scaled time-interval numerical feature) -> Text encoder (XLM-RoBERTa-base → pooled output 768-dim) and Numerical processor (dedicated FFN) -> Fusion layer (concatenation of text + numerical representations) -> Shared FFN (4 layers with PReLU activations) -> Output heads (two linear layers: Impact 4 classes, Urgency 5 classes)

- Critical path:
  1. Preprocess text (remove duplicates, HTML → plain text, normalize)
  2. Scale numerical features with min-max scaler (fit on train only)
  3. Forward pass through dual-branch architecture
  4. Compute loss as sum of cross-entropy for both outputs
  5. Select best checkpoint by average F1 on validation set

- Design tradeoffs:
  - **Embedding-based vs. Fine-tuned**: Embedding pipelines are computationally cheaper at inference but fail on noisy ITSM data; fine-tuning requires GPU training but achieves usable performance.
  - **Shared vs. separate encoders**: Current design shares representation; future work could explore task-specific heads.
  - **Confidence scoring**: Margin-based metric (Eq. 2) requires no architecture changes but assumes logits reflect true uncertainty.

- Failure signatures:
  - **Clustering on UMAP embeddings produces misleading structure** (Section 5.1.4 cites UMAP documentation warning)
  - **Embedding-based classifiers overfit on train (99%+ F1) but collapse on validation (~24% F1)**—indicates poor generalization from generic embeddings
  - **Urgency predictions confuse adjacent classes** (Table 7: heavy confusion between classes 1-2) due to subjectivity

- First 3 experiments:
  1. **Baseline reproduction**: Train XGBoost on sentence-transformers embeddings (paraphrase-multilingual-mpnet-base-v2) on your ITSM data. Expect ~20-25% F1; confirms embedding quality issue.
  2. **Fine-tuning sanity check**: Fine-tune XLM-RoBERTa on text-only input (drop numerical branch). Compare to paper's 78.5% to quantify numerical feature contribution.
  3. **Class imbalance ablation**: Apply class-weighted loss or oversampling to urgency prediction. Measure improvement in minority class F1 (especially class 3: 4.76% of data).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would specialized prediction heads tailored to each target variable reduce the 16.6 percentage point performance gap between impact (86.80% F1) and urgency (70.22% F1)?
- Basis in paper: [explicit] "Future work could first explore more specialized prediction heads for each target, aiming to reduce the performance gap between impact and urgency."
- Why unresolved: The current architecture uses a shared feed-forward network before separate linear heads; whether target-specific architectures would benefit urgency prediction remains untested.
- What evidence would resolve it: Ablation experiments comparing shared vs. specialized prediction heads, with F1-score measurements per target.

### Open Question 2
- Question: Can combining the fine-tuned transformer with sentence-embedding-based representations improve prioritization accuracy?
- Basis in paper: [explicit] "Another direction is to combine the fine-tuned transformer with sentence-embedding-based representations to enrich the feature space."
- Why unresolved: Embedding-based approaches alone failed (max 24% F1), but whether they provide complementary information when fused with transformer representations is unknown.
- What evidence would resolve it: Experiments integrating sentence embeddings (concatenation or attention-based fusion) into the transformer, comparing against the 78.5% baseline F1.

### Open Question 3
- Question: Would domain-adapted embeddings trained on ITSM ticket data enable embedding-based approaches to achieve competitive performance?
- Basis in paper: [inferred] Authors attribute embedding-based failures to "poor IT-domain representation of the embedding" and note that the selected model "is not fitted to ITSM ticket data."
- Why unresolved: Only generic pre-trained embeddings were tested; whether continued pre-training on ITSM corpora would yield meaningful clusters and better classifier inputs is unexplored.
- What evidence would resolve it: Training domain-adapted embeddings via masked language modeling on ticket corpora, then re-evaluating the clustering and classification pipelines.

### Open Question 4
- Question: What techniques could improve prediction reliability for underrepresented urgency classes (e.g., class 3 with only 4.76% of samples)?
- Basis in paper: [inferred] The confusion matrix shows "unreliable predictions for class 3 due to limited samples," yet the paper does not apply imbalance-specific interventions.
- Why unresolved: Stratified splitting was used, but no oversampling, class weighting, or specialized loss functions were explored.
- What evidence would resolve it: Experiments with SMOTE, focal loss, or class-weighted cross-entropy, reporting per-class F1 improvements.

## Limitations
- The specific contribution of numerical features versus text-only fine-tuning was not isolated through ablation studies
- The ~26,000 ticket dataset, while substantial for ITSM, may limit generalization to organizations with different ticket characteristics or volume
- The paper relies on a single numerical feature (time interval), raising questions about scalability to richer operational metadata

## Confidence
- **High confidence**: The superiority of fine-tuned transformers over embedding-based approaches is well-supported by quantitative results (78.5% F1 vs ~20-25% for embeddings) and ablation studies showing embedding quality issues.
- **Medium confidence**: The specific contribution of numerical features versus text-only fine-tuning—while the dual-branch architecture showed improvement, the exact performance gain from adding time-interval features was not isolated.
- **Medium confidence**: The claim that generic embeddings fail specifically due to domain adaptation issues—while supported by poor clustering and classification results, the analysis could benefit from direct comparison of embedding distributions between general and ITSM-specific data.

## Next Checks
1. **Ablation on numerical features**: Train a text-only fine-tuned transformer (remove numerical branch) and compare performance to the dual-branch model to quantify the contribution of operational metadata.

2. **Embedding quality diagnostics**: Visualize UMAP projections of sentence-transformers embeddings versus XLM-Roberta fine-tuning outputs to identify whether poor performance stems from embedding distortion or fundamental mismatch to ITSM semantics.

3. **Minority class robustness**: Apply class-weighted loss or oversampling specifically to urgency class 3 (4.76% prevalence) and measure F1-score improvement to validate the model's handling of severe imbalance.