---
ver: rpa2
title: 'RLKD: Distilling LLMs'' Reasoning via Reinforcement Learning'
arxiv_id: '2505.16142'
source_url: https://arxiv.org/abs/2505.16142
tags:
- reasoning
- meta-reasoning
- answer
- structure
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# RLKD: Distilling LLMs' Reasoning via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.16142
- **Source URL:** https://arxiv.org/abs/2505.16142
- **Reference count:** 28
- **Primary result:** RLKD outperforms SFT in pass@k on reasoning benchmarks (AIME24: 83.3→86.7 pass@64)

## Executive Summary
RLKD introduces a reinforcement learning approach for distilling reasoning capabilities from teacher LLMs to students, addressing the limitation that supervised fine-tuning (SFT) cannot transfer implicit multi-branch reasoning structures. The method uses a Generative Structured Reward Model (GSRM) that decomposes reasoning paths into meta-reasoning and solving phases, then applies structured rewards based on step-level alignment. Experiments show RLKD achieves 1.4-2.1% improvements in pass@k on challenging benchmarks while maintaining or improving pass@1, and demonstrates better domain generalization compared to SFT.

## Method Summary
RLKD employs reinforcement learning with a novel Generative Structured Reward Model (GSRM) to distill reasoning capabilities. The GSRM generator converts reasoning paths into explicit meta-reasoning-solving sequences, while a structured reward mechanism sequentially compares steps with early-exit on meta-reasoning mismatch and progressive penalties for sub-problem/answer divergence. The training combines GRPO optimization with rewards for accuracy, structure alignment, format compliance, and tag matching. The approach is evaluated against SFT baselines on benchmarks including AIME, AMC, and Codeforces, showing consistent improvements in both in-distribution and out-of-distribution settings.

## Key Results
- RLKD achieves 1.4-2.1% improvements in pass@k on AIME24 (83.3→86.7 at pass@64)
- Outperforms SFT baselines while maintaining or improving pass@1 performance
- Demonstrates better domain generalization: SFT degrades on out-of-distribution data while RLKD improves
- Requires only 0.1% of training data when combined with pre-trained base models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Authentic reasoning in LLMs possesses an implicit multi-branch structure that SFT cannot transfer.
- **Mechanism:** Drawing from cognitive neuroscience, reasoning decomposes into interleaved meta-reasoning (sub-problem selection from candidates) and solving phases. SFT's token-level cross-entropy loss collapses this tree-like decision process into flat sequence prediction, forcing memorization of one path rather than learning the branching structure.
- **Core assumption:** Teacher LLMs genuinely exhibit this multi-branch structure in their internal reasoning process, even though their outputs are linear sequences.
- **Evidence anchors:** [abstract] "Supervised fine-tuning collapses this rich structure into a flat sequence of token prediction"; [section 1] Figure 1(a) illustrates implicit branches; Figure 1(b) shows SFT collapse; [corpus] "Reasoning Scaffolding" paper independently validates that behavioral cloning teaches "surface-level patterns rather than underlying algorithmic structure"
- **Break condition:** If teacher outputs already lack genuine branching (e.g., purely retrieval-based responses), the structural signal may be noise.

### Mechanism 2
- **Claim:** A generative reward model can extract and compare reasoning structure between teacher and student.
- **Mechanism:** GSRM first converts raw reasoning paths into explicit meta-reasoning-solving step sequences via a trained generator, then applies a structured reward mechanism that sequentially compares steps with early-exit on meta-reasoning mismatch and progressive penalties for sub-problem/answer divergence.
- **Core assumption:** The meta-reasoning-solving decomposition is learnable from data and the matching logic captures meaningful structural alignment.
- **Evidence anchors:** [section 3.1] Algorithm 1 specifies the structured reward with base score + 50% penalties; [section 3.1] "Structured Fine-grained Training" uses 3-epoch progressive training for M and Q&A tokens; [corpus] Weak corpus evidence for this specific mechanism; related work discusses reward distillation but not structural decomposition
- **Break condition:** If GSRM generates inconsistent or meaningless decompositions (e.g., conflating steps), rewards become uninformative or adversarial.

### Mechanism 3
- **Claim:** RL with structural rewards enables students to learn implicit branching behavior, measured by improved pass@k.
- **Mechanism:** GRPO optimization with combined rewards (accuracy + GSRM structure) guides students toward policies that can sample diverse valid reasoning paths. The structural reward provides step-level feedback on meta-reasoning alignment, while outcome rewards ensure correctness.
- **Core assumption:** Higher pass@k genuinely reflects learned branching capability rather than increased sampling luck or reward hacking.
- **Evidence anchors:** [section 4.2] Table 1 shows pass@k improvements (e.g., AIME24 pass@64: 83.3→86.7); Figure 3(c) shows structure matching increasing during training; [section 4.3] Figure 4 shows SFT degrades while RLKD improves on domain-shifted data; [section 4.3] Figure 5 diversity analysis shows student patterns approaching teacher's over training
- **Break condition:** If student learns to game GSRM matching without genuine reasoning transfer (reward hacking), pass@1 would degrade while pass@k appears to improve.

## Foundational Learning

- **Concept: Standard SFT-based Knowledge Distillation**
  - **Why needed here:** RLKD is positioned against this baseline; you must understand what SFT does (token-level imitation via cross-entropy) to grasp why it fails for structural transfer.
  - **Quick check question:** Given a teacher reasoning path "Let me think... First, I'll check X. Now checking Y...", what does SFT actually optimize?

- **Concept: Proximal Policy Optimization (PPO) and GRPO**
  - **Why needed here:** RLKD builds on GRPO; understanding policy gradients, KL constraints, and group-based relative advantages is essential for implementing the RL training loop.
  - **Quick check question:** How does GRPO differ from PPO in computing advantages, and why might it be more stable for LLM reasoning?

- **Concept: Reward Modeling (Generative vs. Rule-based)**
  - **Why needed here:** GSRM combines generative semantic understanding with rule-based interpretability; understanding this tradeoff helps you diagnose reward quality issues.
  - **Quick check question:** What are the failure modes of pure neural reward models vs. pure rule-based rewards, and how does GSRM attempt to balance them?

## Architecture Onboarding

- **Component map:** GPT-4o ICL + DeepSeek-V3 → GSRM Generator (Qwen2.5-7B-Instruct) → GSRM Reward Computer → GRPO Training Loop → Student Model

- **Critical path:**
  1. Construct GSRM training data via GPT-4o ICL + DeepSeek-V3 verification (Section 3.1, Figure 2 prompts)
  2. Train GSRM generator with Structured Fine-grained Training (3 epochs: M→Q&A→mixed)
  3. Initialize student from base or SFT-distilled checkpoint
  4. Run GRPO with combined rewards (accuracy + structure + format + tag), tune R_gsrm:R_acc ratio

- **Design tradeoffs:**
  - **Generative vs. rule-based reward:** GSRM uses generative matching (Qwen-2.5-7B-Instruct for text similarity) but applies rule-based scoring logic; this adds interpretability but depends on generator quality
  - **Reward weight balancing:** Appendix B shows R_gsrm:R_acc = 1:1 is optimal; higher structure weight may improve pass@k but hurt pass@1
  - **Data efficiency vs. quality:** 0.1% data works for RL-only, but GSRM requires 93k+ samples for training the generator itself

- **Failure signatures:**
  1. **GSRM collapse:** Generator produces degenerate sequences (e.g., all steps identical); detect via diversity metrics on generated outputs
  2. **Reward hacking:** Student learns to match structure keywords without correct answers; detect via pass@1 dropping while structure matching rises
  3. **Domain overfitting:** Student improves on training distribution but degrades on out-of-domain (Figure 4 shows SFT does this; monitor both)
  4. **Early-exit over-triggering:** Algorithm 1 exits too early due to false meta-reasoning mismatches; tune the Match() threshold

- **First 3 experiments:**
  1. **GSRM quality validation:** Before RL, verify GSRM generator produces sensible decompositions on held-out examples; manually inspect 50-100 samples for step granularity and M/Q&A separation
  2. **Reward ablation:** Run RLKD with R_gsrm=0 (pure GRPO) vs. R_gsrm:R_acc=1:1 vs. R_gsrm:R_acc=2:1 on a small benchmark subset; plot pass@1 and pass@16 curves to find the stability-efficiency frontier
  3. **SFT vs. RLKD comparison on domain shift:** Replicate Figure 4 setting—train both methods on data with known distribution shift from test set; confirm SFT degrades while RLKD improves or stays stable

## Open Questions the Paper Calls Out
None

## Limitations
- Teacher Model Bias: The approach assumes DeepSeek-R1 exhibits authentic multi-branch reasoning, but this cannot be verified from output sequences alone.
- GSRM Generator Quality: No validation that decompositions capture meaningful reasoning structure versus arbitrary token groupings.
- Reward Hacking Risk: The structured reward mechanism could be gamed by students learning to produce superficial structural matches.

## Confidence

**High Confidence:** The RLKD training pipeline (GRPO with combined rewards) is technically sound and reproducible. The experimental methodology (pass@k evaluation, diversity analysis) follows established practices.

**Medium Confidence:** The core claim that SFT fails to transfer implicit branching structure is supported by relative performance improvements, but the mechanism explanation relies heavily on assumptions about teacher model internal reasoning that cannot be directly verified.

**Low Confidence:** The assertion that pass@k improvements directly reflect learned branching capability is the weakest link. Alternative explanations (sampling diversity, reward optimization artifacts, or memorization of specific patterns) cannot be ruled out without additional controls.

## Next Checks

1. **Structural Transfer Validation:** Implement a controlled experiment where the teacher model is replaced with a rule-based reasoning system that explicitly generates multi-branch outputs. Compare whether RLKD transfers this explicit structure better than SFT, providing direct evidence for the structural transfer hypothesis.

2. **Reward Quality Analysis:** Create a human-annotated subset (50-100 examples) where experts label whether student outputs demonstrate genuine reasoning branching versus superficial pattern matching. Correlate these labels with GSRM scores to validate whether the reward mechanism captures meaningful structure.

3. **Minimal Data Efficiency Test:** Replicate the 0.1% data training result but with progressively smaller fractions (0.05%, 0.01%, 0.001%) to determine the minimum effective data requirement. This would validate whether the claimed data efficiency is robust or dependent on having sufficient examples for the structural signal to emerge.