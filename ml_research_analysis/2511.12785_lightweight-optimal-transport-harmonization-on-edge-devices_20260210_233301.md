---
ver: rpa2
title: Lightweight Optimal-Transport Harmonization on Edge Devices
arxiv_id: '2511.12785'
source_url: https://arxiv.org/abs/2511.12785
tags:
- color
- harmonization
- image
- images
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a lightweight approach to color harmonization
  for augmented reality (AR) applications, addressing the lack of real-time solutions
  in AR pipelines. The method leverages optimal transport theory by training a compact
  encoder to predict the Monge-Kantorovich transport map, enabling efficient on-device
  inference.
---

# Lightweight Optimal-Transport Harmonization on Edge Devices

## Quick Facts
- arXiv ID: 2511.12785
- Source URL: https://arxiv.org/abs/2511.12785
- Reference count: 30
- Primary result: Achieves real-time AR color harmonization at 175 fps with MOS comparable to state-of-the-art baselines

## Executive Summary
This paper proposes a lightweight approach to color harmonization for augmented reality (AR) applications, addressing the lack of real-time solutions in AR pipelines. The method leverages optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map, enabling efficient on-device inference. The approach is evaluated on both standard datasets and a newly collected AR-specific dataset, demonstrating superior performance in terms of aggregated speed-quality metrics. The proposed method achieves a mean opinion score comparable to state-of-the-art baselines while offering significantly faster inference, making it suitable for real-time AR applications.

## Method Summary
The method frames color harmonization as an optimal transport problem between foreground and background color distributions. Rather than computing dense pixel mappings, a compact EfficientNet-B0 encoder predicts the 12 parameters of a Monge-Kantorovich linear filter (3×3 matrix A and 3D shift vector S). The model takes 4-channel input (RGB + binary mask) and outputs these parameters, which are then applied globally to masked pixels using T(x) = Ax + S. Training uses a hybrid L1 loss combining parameter supervision (L_labels) and content reconstruction (L_content) to prevent identity collapse. The approach achieves O(n) complexity independent of model size, enabling real-time performance on edge devices.

## Key Results
- Achieves 175 fps at 256×256 resolution on target device
- Mean Opinion Score comparable to state-of-the-art baselines on AR-specific dataset
- Outperforms competitors in speed-quality tradeoff on iHarmony4 benchmark

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A linear Monge-Kantorovich (MKL) transport map can approximate complex color harmonization transforms when certain regularity conditions hold.
- **Mechanism:** The MKL filter computes an optimal transport map between Gaussian approximations of source and target color distributions via T*(x) = μ₁ + A(x - μ₀), where A is derived from covariance matrices. The approximation error is bounded by the Lipschitz constant L of the true map and the clipping error at gamut boundaries.
- **Core assumption:** The true harmonization map is L-Lipschitz continuous (smooth), and color distributions are not concentrated at the extreme boundaries of the color gamut.
- **Evidence anchors:**
  - Theorem 1 bounds total error as E ≤ 2E_clip + 2E_lin, with E_lin ≤ 2B² + 2(||A||_op + L)² · tr(Σ₀)
  - "Ideal Linear OT filter achieves MSE ~7.0 on iHarmony4, quite low compared to state-of-the-art results"
- **Break condition:** Dark objects with distributions concentrated near gamut corners produce implausible results due to high clipping error.

### Mechanism 2
- **Claim:** Predicting the 12 MKL parameters directly via a compact encoder enables real-time inference while maintaining perceptual quality.
- **Mechanism:** Rather than computing dense pixel-to-pixel mappings (U-Net style), the EfficientNet-B0 encoder predicts the matrix A and shift S, reducing output to 12 values. The MKL transformation then applies globally to all masked pixels in O(n) time independent of model complexity.
- **Core assumption:** The statistics (μ₁, Σ₁) of the target harmonized distribution can be inferred from the composite image and mask alone.
- **Evidence anchors:**
  - "training a compact encoder to predict the Monge-Kantorovich transport map"
  - Table 2 shows 175 fps at 256×256, 137 fps at 1024×2048, compared to 104 fps and 63 fps for PCT-Net
- **Break condition:** Spatially varying illumination requiring non-global transforms would exceed single-filter capacity.

### Mechanism 3
- **Claim:** Hybrid L1 loss (labels + content) prevents mode collapse to identity transformation while avoiding over-smoothed predictions.
- **Mechanism:** L_labels provides MKL-specific supervision; L_content grounds predictions in pixel-space accuracy. L1 loss allows convergence to any valid solution rather than forcing arithmetic mean of possible solutions (which L2 would do).
- **Core assumption:** Multiple valid MKL solutions exist for the ill-posed harmonization problem.
- **Evidence anchors:**
  - "When training with just L_content, model learns filters close to identity transformation"
  - L1 achieves 65.0 MSE vs L2's 66.3 MSE on iHarmony4
- **Break condition:** Without α balancing term, either identity collapse (α too low) or loss of MKL structure (α too high) occurs.

## Foundational Learning

- **Optimal Transport (Monge formulation):**
  - **Why needed here:** The paper frames harmonization as finding a transport map T that transforms source color distribution π₀ to target π₁ with minimal cost.
  - **Quick check question:** Can you explain why the quadratic cost function c(x,y) = ||x-y||² guarantees a unique optimal map exists for continuous distributions?

- **Monge-Kantorovich Linear map:**
  - **Why needed here:** This is the core theoretical contribution—under Gaussian approximation, the optimal transport has closed-form linear solution requiring only 12 parameters.
  - **Quick check question:** Given source N(μ₀, Σ₀) and target N(μ₁, Σ₁), what does the matrix A represent geometrically?

- **Exposure bias in training data:**
  - **Why needed here:** Standard datasets (iHarmony4) contain "leaked" background pixels near mask boundaries, creating train-test mismatch for AR applications with pixel-perfect masks.
  - **Quick check question:** Why might MSE metrics misrepresent perceptual quality when models exploit boundary leakage?

## Architecture Onboarding

- **Component map:** 4-channel input → EfficientNet-B0 encoder → 12-dimensional regression head → MKL filter application

- **Critical path:**
  1. Compute source statistics (μ₀, Σ₀) from masked foreground pixels
  2. Encoder predicts [A, S] from composite+mask
  3. Apply per-pixel transformation: x_harmonized = clip(Ax + S)
  4. For video: apply exponential moving average across frames

- **Design tradeoffs:**
  - Speed vs. spatial adaptivity: Single global filter vs. pixel-wise transforms
  - MSE vs. perceptual quality: Lower MSE doesn't guarantee better MOS
  - Training data: Synthetic composites have ground truth; AR composites don't

- **Failure signatures:**
  - Dark objects → implausible bright artifacts (clipping error dominant)
  - High-contrast edges in background → color bleeding if mask imperfect
  - Video temporal instability → frame-to-frame parameter variance
  - Identity collapse → output matches input (loss weight issue)

- **First 3 experiments:**
  1. **Sanity check:** Run "Ideal Linear OT" on iHarmony4 validation—should achieve MSE ~7.0; if significantly higher, check statistics computation
  2. **Ablation study:** Train with L_labels only vs. L_total (α=10)—expect identity collapse without content loss
  3. **Inference profiling:** Measure encoder inference time on target device; if >15ms for 256×256, consider further quantization or smaller backbone

## Open Questions the Paper Calls Out

- **Can the MKL-Harmonizer be extended to achieve temporally consistent video harmonization without requiring video-specific training data?**
  - Basis in paper: Authors state: "our method is not designed as a final solution for video harmonization. Since our model is not trained on video data, sequential predictions may vary significantly across frames."
  - Why unresolved: The current method applies an exponential moving average as a mitigation, but the fundamental challenge of frame-to-frame coherence without video supervision remains open.
  - What evidence would resolve it: A method achieving stable video harmonization metrics (e.g., temporal consistency scores) comparable to specialized video harmonization approaches, evaluated on standard video benchmarks.

- **How can the exposure bias caused by pixel-imperfect masks in existing datasets be systematically corrected to improve generalization to AR applications with pixel-perfect masks?**
  - Basis in paper: Authors identify that boundary pixels in imperfect masks "inadvertently teach the harmonization network to over-rely on this specific information," creating a training-inference mismatch when applied to pixel-perfect AR masks.
  - Why unresolved: The paper exposes the problem and evaluates on AR-specific data, but does not propose a systematic solution for debiasing models trained on existing datasets like iHarmony4.
  - What evidence would resolve it: A training methodology that narrows the performance gap between iHarmony4 test sets and AR-specific evaluations with pixel-perfect masks.

## Limitations
- The method fails on dark objects with color distributions concentrated near gamut boundaries, producing implausible results
- The hybrid loss design depends critically on hyperparameter α, with limited exploration of sensitivity
- Theoretical error bounds assume color distributions avoid boundary concentration, but quantitative analysis of this failure mode is limited

## Confidence
- **High Confidence:** The MKL filter computation and its O(n) complexity are mathematically sound and well-established in optimal transport theory
- **Medium Confidence:** The claim of achieving "superior performance in terms of aggregated speed-quality metrics" is supported by quantitative comparisons but depends on the specific evaluation protocol and dataset characteristics
- **Medium Confidence:** The inference speed claims (175 fps at 256×256, 137 fps at 1024×2048) appear technically plausible given the lightweight architecture, but depend on specific hardware and implementation details not fully specified

## Next Checks
1. **Boundary concentration analysis:** Quantify how often input images contain color distributions near gamut boundaries and measure the corresponding performance degradation in both MSE and MOS metrics
2. **Hyperparameter sensitivity:** Conduct ablation studies varying α in the hybrid loss across a range (e.g., 1, 10, 100) to establish robustness and identify optimal settings
3. **Cross-dataset generalization:** Test the trained model on diverse harmonization datasets beyond iHarmony4 and the custom AR dataset to evaluate real-world robustness and identify failure patterns