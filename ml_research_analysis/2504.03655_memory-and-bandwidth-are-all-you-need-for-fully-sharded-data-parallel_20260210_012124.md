---
ver: rpa2
title: Memory and Bandwidth are All You Need for Fully Sharded Data Parallel
arxiv_id: '2504.03655'
source_url: https://arxiv.org/abs/2504.03655
tags:
- training
- gpus
- size
- gbps
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study thoroughly investigates Fully Sharded Data Parallel
  (FSDP) training of large transformer models across diverse hardware configurations.
  Through extensive simulations and empirical tests on clusters with 200Gbps and 100Gbps
  interconnects using up to 512 GPUs, the research reveals that network bandwidth
  and GPU memory are critical bottlenecks for training efficiency.
---

# Memory and Bandwidth are All You Need for Fully Sharded Data Parallel

## Quick Facts
- arXiv ID: 2504.03655
- Source URL: https://arxiv.org/abs/2504.03655
- Authors: Jiangtao Wang; Jan Ebert; Oleg Filatov; Stefan Kesselheim
- Reference count: 40
- Key outcome: Network bandwidth and GPU memory are critical bottlenecks for FSDP training efficiency

## Executive Summary
This study investigates Fully Sharded Data Parallel (FSDP) training of large transformer models across diverse hardware configurations. Through extensive simulations and empirical tests on clusters with 200Gbps and 100Gbps interconnects using up to 512 GPUs, the research identifies network bandwidth and GPU memory as the primary constraints affecting training performance. The findings demonstrate that models with larger parameters and longer sequences experience varying performance impacts, with bandwidth limitations significantly affecting throughput and model FLOPs utilization.

The research provides actionable guidelines for optimizing FSDP configurations, emphasizing the critical role of network infrastructure in scaling large transformer architectures efficiently. By analyzing performance bottlenecks and their impact on training efficiency, the study offers valuable insights for practitioners working with large-scale distributed training systems.

## Method Summary
The study employs a comprehensive approach combining simulations and empirical testing to evaluate FSDP performance across different hardware configurations. Experiments were conducted on clusters with 200Gbps and 100Gbps interconnects using up to 512 GPUs, testing various transformer models including GPT-2 and LLaMA architectures. The research methodology includes performance benchmarking, bottleneck analysis, and efficiency measurements across different parameter scales and sequence lengths.

## Key Results
- Network bandwidth and GPU memory are identified as critical bottlenecks for FSDP training efficiency
- Models with larger parameters and longer sequences show varying performance impacts
- Bandwidth limitations significantly reduce model FLOPs utilization (MFU) and throughput
- Doubling bandwidth can improve training efficiency by up to 9% for 7B and 13B models

## Why This Works (Mechanism)
The mechanism behind FSDP performance bottlenecks stems from the fundamental trade-offs in distributed training systems. When model parameters are sharded across multiple GPUs, communication overhead increases, particularly during parameter synchronization phases. Network bandwidth limitations create bottlenecks during these synchronization operations, while GPU memory constraints affect the ability to efficiently manage sharded parameters and intermediate activations. The interplay between these factors determines overall training efficiency and throughput.

## Foundational Learning
1. **FSDP (Fully Sharded Data Parallel)** - A distributed training technique that shards model parameters, gradients, and optimizer states across multiple GPUs
   - Why needed: Enables training of models that exceed single-GPU memory capacity
   - Quick check: Can train models larger than available GPU memory by distributing parameters

2. **MFU (Model FLOPs Utilization)** - A metric measuring how efficiently computational resources are utilized during training
   - Why needed: Quantifies the gap between theoretical and actual computational throughput
   - Quick check: Higher MFU indicates better hardware utilization and training efficiency

3. **Network Bandwidth Impact** - The effect of interconnect speed on distributed training performance
   - Why needed: Determines how quickly parameters can be synchronized across GPUs
   - Quick check: Limited bandwidth creates bottlenecks during parameter exchange operations

## Architecture Onboarding

**Component Map:**
GPUs (with memory) -> Network Interconnect -> Parameter Sharding -> Synchronization Operations

**Critical Path:**
Model parameter access → Sharding computation → Network communication → Synchronization → Computation resumption

**Design Tradeoffs:**
- Memory vs. Communication: More aggressive sharding reduces memory usage but increases communication overhead
- Bandwidth vs. Efficiency: Higher bandwidth improves synchronization speed but requires more expensive infrastructure
- Model Size vs. Scalability: Larger models benefit more from distributed training but face greater communication challenges

**Failure Signatures:**
- Low MFU indicating underutilization of computational resources
- High communication overhead relative to computation time
- Memory bottlenecks causing training stalls or reduced batch sizes

**First 3 Experiments:**
1. Measure baseline MFU with different sharding configurations
2. Test bandwidth impact by varying interconnect speeds
3. Evaluate memory usage patterns under different sharding strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to specific transformer architectures (GPT-2 and LLaMA)
- Hardware configurations restricted to NVIDIA A100 GPUs with specific interconnect speeds
- Focus on upstream performance metrics without downstream task evaluation
- Reliance on synthetic data generation may not reflect real-world training dynamics

## Confidence
- Network bandwidth as primary bottleneck for large models: High
- GPU memory limitations as secondary constraint: High
- Doubling bandwidth improves efficiency by 9% for 7B/13B models: Medium
- Findings generalizable across all transformer architectures: Low

## Next Checks
1. Replicate experiments with alternative transformer variants (BERT, ViT) and different GPU architectures (H100, AMD Instinct)
2. Test with real-world training datasets to validate synthetic data assumptions
3. Extend analysis to include downstream task performance metrics and training convergence rates