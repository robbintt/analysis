---
ver: rpa2
title: Grounded Multilingual Medical Reasoning for Question Answering with Large Language
  Models
arxiv_id: '2512.05658'
source_url: https://arxiv.org/abs/2512.05658
tags:
- medical
- zhang
- reasoning
- traces
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multilingual medical reasoning
  in question answering, where existing approaches rely on English-only datasets and
  lack factual grounding in medical knowledge. The authors propose a retrieval-augmented
  generation pipeline that creates reasoning traces in English, Italian, and Spanish,
  grounded in Wikipedia medical content.
---

# Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models

## Quick Facts
- **arXiv ID:** 2512.05658
- **Source URL:** https://arxiv.org/abs/2512.05658
- **Reference count:** 39
- **Primary result:** Retrieval-augmented generation pipeline creates reasoning traces in English, Italian, and Spanish, grounded in Wikipedia medical content, improving accuracy across all tested languages.

## Executive Summary
This paper addresses multilingual medical reasoning in question answering by proposing a retrieval-augmented generation pipeline that generates reasoning traces grounded in Wikipedia medical content. The approach creates traces for questions from MedQA and MedMCQA in English, Italian, and Spanish, verifies their correctness, and uses them for fine-tuning 8B-parameter models. Experiments show state-of-the-art performance across languages, with cross-lingual training providing additional benefits. The method demonstrates that grounding reasoning in factual medical knowledge significantly improves multilingual medical QA performance.

## Method Summary
The methodology involves constructing a medical knowledge base from Wikipedia, retrieving relevant chunks for each question using embedding similarity, rewriting the retrieved context for conciseness, and generating reasoning traces conditioned on the correct answer. These traces are verified through regex matching and used to fine-tune 8B-parameter models through supervised fine-tuning. The pipeline generates traces in three languages and demonstrates that both in-context learning and fine-tuning on these grounded traces improve accuracy, with multilingual training consistently outperforming single-language approaches.

## Key Results
- Retrieval-augmented generation pipeline improves accuracy across all tested languages (English, Italian, Spanish)
- Cross-lingual fine-tuning consistently outperforms single-language training approaches
- State-of-the-art performance among 8B-parameter models on MedQA, MedMCQA, and MedExpQA benchmarks
- Error analysis identifies limited medical knowledge as the primary bottleneck rather than reasoning ability

## Why This Works (Mechanism)

### Mechanism 1: Grounding Reduces Parametric Hallucination
- **Claim:** RAG shifts the model's role from a knowledge store to a reasoning engine, reducing factual errors in low-resource languages.
- **Mechanism:** The pipeline retrieves specific medical chunks from Wikipedia and rewrites them for conciseness. By conditioning the LLM on external context rather than its internal weights, generation is constrained to factual evidence.
- **Core assumption:** Retrieved Wikipedia segments contain sufficient and correct evidence to solve medical questions.
- **Evidence anchors:** Abstract states traces are "grounded in factual medical knowledge from Wikipedia"; Section 3.3 describes selecting top-5 chunks and rewriting for consistency; related work supports efficacy of retrieval for multilingual medical QA.

### Mechanism 2: Verifiable Reasoning Distillation
- **Claim:** Providing the correct answer during trace generation forces the teacher model to construct coherent reasoning that justifies the ground truth.
- **Mechanism:** A 32B model generates traces given question, context, and correct answer, then verifies via regex matching the trace's conclusion against the provided answer.
- **Core assumption:** A reasoning trace leading to a correct answer is inherently logically valid for training.
- **Evidence anchors:** Section 3.4 enforces use of correct answer to prevent collapsing into stating answers; Abstract mentions traces are "verified for correctness"; related work emphasizes evidence verification.

### Mechanism 3: Cross-Lingual Knowledge Transfer
- **Claim:** Joint training on multilingual traces creates shared semantic representations that improve individual language performance.
- **Mechanism:** Fine-tuning on a mixture of English, Italian, and Spanish traces helps the model learn generalized medical reasoning patterns that transfer across languages.
- **Core assumption:** Medical reasoning logic is largely language-independent and transferable.
- **Evidence anchors:** Section 4.4 shows multilingual training consistently matches or surpasses single-language performance; Abstract notes cross-lingual experiments show multilingual training outperforms single-language approaches.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** This is the core engine for "Grounding." Without understanding RAG, one cannot understand how the paper moves beyond the model's internal parametric knowledge to use Wikipedia.
  - **Quick check question:** Can you explain why passing a retrieved document as context reduces hallucination compared to standard prompting?

- **Concept: Chain-of-Thought (CoT) Distillation**
  - **Why needed here:** The paper generates 500k "reasoning traces." This concept explains how a larger model (Teacher) guides a smaller model (Student) by revealing intermediate steps, not just the final answer.
  - **Quick check question:** How does a "reasoning trace" differ from a standard prompt-response pair in a training dataset?

- **Concept: Embedding Spaces & Cosine Similarity**
  - **Why needed here:** Used in Section 3.3 to retrieve context. The paper relies on embedding questions and chunks into a vector space to find relevant medical knowledge.
  - **Quick check question:** If two medical terms are synonyms (e.g., "high blood pressure" vs. "hypertension"), how should their embeddings relate geometrically?

## Architecture Onboarding

- **Component map:** Data Source (MedQA/MedMCQA) -> Knowledge Base (Medical-Wikipedia) -> Retrieval (Qwen3-Embedding-8B + Cosine Search) -> Context Rewriter (Qwen3-32B) -> Trace Generator (Qwen3-32B) -> Verifier (Regex filter) -> Target Model (Qwen3-8B)

- **Critical path:** The Context Rewriter and Trace Generator are the bottlenecks. Raw retrieved chunks are noisy; failure to rewrite them effectively would likely result in confused reasoning traces, as the 8B student model is sensitive to context noise.

- **Design tradeoffs:**
  - **Context vs. Noise:** Retrieving top-5 chunks provides high recall but includes irrelevant data. The paper uses a rewriting step to fix this, adding latency/compute cost.
  - **SFT vs. Few-Shot:** The paper finds Few-Shot yields higher gains (+5.8 avg delta) than SFT (+3.9 avg delta), but SFT is necessary for state-of-the-art results when combined with Few-Shot.
  - **KB Quality:** Relying on Wikipedia allows for easy multilingual alignment but lacks the clinical authority of curated guidelines.

- **Failure signatures:**
  - **Knowledge Gaps:** Error analysis shows the model fails not on logic, but on applying standard diagnostic protocols (clinical knowledge limits).
  - **Translation Artifacts:** While back-translation scores are high, low-resource languages might still suffer from semantic drift in translated questions.
  - **Over-reliance on Context:** If retrieval fails to find the answer, the generator model is left with parametric knowledge, which the paper implies is insufficient for complex reasoning.

- **First 3 experiments:**
  1. **Sanity Check (Retrieval):** Run the pipeline with retrieval disabled (zero context) vs. enabled to quantify the specific contribution of Wikipedia grounding on MedExpQA accuracy.
  2. **Ablation (Rewriting):** Compare model performance when trained on traces from *raw* chunks vs. *rewritten* chunks to validate the specific utility of the rewriting step.
  3. **Generalization Test:** Train the model on English traces only, then test on Italian/Spanish to replicate the cross-lingual transfer drop shown in Figure 4 and confirm the necessity of multilingual data.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the methodology of generating grounded reasoning traces transfer effectively to open-ended clinical reasoning tasks? The authors state they "limit our study to multiple-choice medical question answering, leaving open-ended clinical reasoning tasks for future work."

- **Open Question 2:** To what extent do more authoritative medical knowledge sources improve the reliability of reasoning traces compared to Wikipedia? The paper notes they "rely on Wikipedia... though more authoritative medical sources could further strengthen grounding."

- **Open Question 3:** Does training on grounded reasoning traces yield significant performance improvements for models with parameters significantly larger than 8B? The authors note their "fine-tuning experiments are restricted to 8B-parameter models because of computational constraints; scaling to larger models remains unexplored."

## Limitations

- **KB Completeness and Retrieval Quality:** System effectiveness critically depends on Wikipedia corpus containing relevant medical information, with no systematic evaluation of retrieval failure modes or coverage gaps.
- **Verification Rigidity:** Regex-based trace verification may be overly restrictive or too permissive, potentially allowing logically flawed traces that coincidentally match answer format.
- **Trace Quality Validation:** No human evaluation of generated reasoning traces to assess medical soundness or logical coherence beyond answer correctness.

## Confidence

**High Confidence:** The core empirical findings - that the proposed pipeline improves accuracy across all tested languages and that multilingual training outperforms single-language training - are well-supported by experimental results in Tables 2 and 3.

**Medium Confidence:** The mechanisms explaining why the approach works are plausible and supported by the paper's design, but lack direct empirical validation through ablation studies isolating each component's contribution.

**Low Confidence:** The claim of "state-of-the-art performance among 8B-parameter models" is based on comparison with only three other models without comprehensive benchmarking against all relevant baselines.

## Next Checks

1. **Knowledge Base Coverage Analysis:** Systematically evaluate the proportion of MedQA/MedMCQA questions for which the retrieval component fails to find relevant context to quantify limitations of Wikipedia grounding approach.

2. **Trace Quality Human Evaluation:** Conduct expert medical review of a random sample of generated reasoning traces to assess logical validity and medical accuracy, independent of answer correctness.

3. **Component Ablation Study:** Design experiments that isolate the contribution of each pipeline component (context rewriting, answer-conditional generation, verification) through systematic removal and comparison.