---
ver: rpa2
title: 'PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with
  Iterative Self-Correction and Multilingual Agents'
arxiv_id: '2512.23713'
source_url: https://arxiv.org/abs/2512.23713
tags:
- code
- generation
- bangla
- language
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating Python code from
  Bangla natural language instructions, a task hindered by the scarcity of resources
  for low-resource languages. The authors introduce BanglaCodeAct, an agent-based
  framework leveraging multi-agent prompting and iterative self-correction within
  a Thought-Code-Observation loop.
---

# PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents

## Quick Facts
- arXiv ID: 2512.23713
- Source URL: https://arxiv.org/abs/2512.23713
- Reference count: 5
- Primary result: 71.6% pass@1 accuracy on blind test set of mHumanEval Bangla-to-Python code generation

## Executive Summary
This paper introduces BanglaCodeAct, an agent-based framework for translating Bangla natural language instructions into executable Python code. The system leverages multi-agent prompting and iterative self-correction within a Thought-Code-Observation loop, achieving state-of-the-art results on the mHumanEval dataset without task-specific fine-tuning. By using an open-source multilingual LLM (Qwen3-8B) and dynamic refinement, BanglaCodeAct demonstrates significant improvements over baseline methods for low-resource language code generation.

## Method Summary
The authors address Bangla-to-Python code generation through BanglaCodeAct, an agent-based framework that avoids fine-tuning by using an open-source multilingual LLM (Qwen3-8B) with iterative self-correction. The system employs a Thought-Code-Observation loop where the agent generates code from Bangla instructions, tests it, observes results, and refines the solution. The approach uses self-consistency decoding with n=5 samples and operates through vLLM inference with specified hyperparameters (temperature=0.7, top_p=0.9). No model training is performed - the system relies entirely on prompt engineering and agent reasoning to achieve high accuracy on the mHumanEval benchmark.

## Key Results
- Achieves 94.0% pass@1 accuracy on development set of mHumanEval
- Scores 71.6% pass@1 accuracy on blind test set, outperforming zero-shot and self-consistency baselines
- Establishes new benchmark for Bangla-to-Python translation without requiring task-specific fine-tuning
- Demonstrates effectiveness of agent-based reasoning for reliable code generation in low-resource languages

## Why This Works (Mechanism)
The iterative self-correction mechanism allows the system to refine code through multiple attempts, catching errors that single-shot generation would miss. The multi-agent prompting structure enables different reasoning perspectives during the Thought-Code-Observation loop, while the open-source multilingual LLM provides strong foundational capabilities without the need for expensive fine-tuning. The self-consistency decoding with multiple samples helps select the most reliable output from varied attempts.

## Foundational Learning
- **Multi-agent prompting**: Using multiple agents with different roles to collaboratively solve complex tasks - needed for sophisticated reasoning in code generation; quick check: verify distinct agent roles in prompt templates
- **Iterative self-correction**: Repeatedly generating, testing, and refining outputs to improve quality - essential for handling ambiguous or complex instructions; quick check: confirm iteration limit and convergence behavior
- **Zero-shot code generation**: Generating code without task-specific training by leveraging pre-trained model capabilities - crucial for low-resource language applications; quick check: verify no fine-tuning steps in implementation
- **Thought-Code-Observation loop**: Structured reasoning cycle where thoughts generate code, code is executed, and observations inform next thoughts - provides systematic refinement approach; quick check: validate loop termination conditions
- **Self-consistency decoding**: Generating multiple samples and selecting the most consistent answer - improves reliability over single best-of-N sampling; quick check: confirm n=5 parameter usage

## Architecture Onboarding
**Component map**: Bangla instruction -> Thought/Code/Observation loop -> PythonREPL sandbox -> Test results -> Refined code output
**Critical path**: Input processing → Agent reasoning (Thought) → Code generation → Execution sandbox (Code) → Test observation → Iteration/refinement (Observation) → Final output
**Design tradeoffs**: Avoids fine-tuning costs but relies heavily on prompt quality and iteration limits; uses open-source LLM for accessibility but may sacrifice some performance vs. proprietary models; implements self-consistency for reliability at increased computational cost
**Failure signatures**: Non-convergence within 10 iterations on complex instructions; invalid LLM responses triggering retry exhaustion; timeout failures in PythonREPL sandbox
**First experiments**: 1) Test iteration mechanism with placeholder prompts to verify timeout and retry logic; 2) Validate PythonREPL sandbox captures and formats observations correctly; 3) Reproduce baseline zero-shot results on small development subset to verify evaluation pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Exact system and user prompt templates are unspecified, potentially affecting reproducibility
- PythonREPL sandbox implementation details (security, timeout handling, observation format) are not provided
- Performance characterization beyond 10-iteration limit is missing for complex Bangla instructions
- Generalizability to Bangla instructions outside mHumanEval dataset scope is not demonstrated

## Confidence
- **High confidence**: Methodology of agent-based reasoning with iterative self-correction is sound and well-documented; evaluation metric (pass@1 accuracy) is clearly defined and appropriate
- **Medium confidence**: Performance claims (94.0% dev, 71.6% test) are reasonable given approach but depend on unspecified implementation details; baseline comparisons are valid but limited in scope
- **Low confidence**: Generalizability to Bangla instructions beyond mHumanEval dataset; system robustness to instructions requiring extensive reasoning or multiple code modules

## Next Checks
1. Implement Thought-Code-Observation loop with placeholder prompts and validate iteration mechanism handles timeout and retry logic correctly
2. Create minimal PythonREPL sandbox that captures stdout, stderr, and test results in expected observation format, then test with simple Bangla instructions
3. Reproduce baseline zero-shot results on small development subset to verify evaluation pipeline before running full BanglaCodeAct Agent implementation