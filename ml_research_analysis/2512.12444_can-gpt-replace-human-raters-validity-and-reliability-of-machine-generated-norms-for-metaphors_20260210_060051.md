---
ver: rpa2
title: Can GPT replace human raters? Validity and reliability of machine-generated
  norms for metaphors
arxiv_id: '2512.12444'
source_url: https://arxiv.org/abs/2512.12444
tags:
- ratings
- metaphors
- human
- familiarity
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models (LLMs) can
  replace human raters in generating psycholinguistic norms for metaphors. Three GPT
  models (GPT3.5-turbo, GPT4o-mini, GPT4o) were prompted to rate 687 metaphors in
  English and Italian on familiarity, comprehensibility, and imageability.
---

# Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors

## Quick Facts
- arXiv ID: 2512.12444
- Source URL: https://arxiv.org/abs/2512.12444
- Reference count: 26
- Primary result: GPT models can validly and reliably generate metaphor ratings comparable to human raters, though they struggle with familiar and highly imageable metaphors.

## Executive Summary
This study systematically tested whether large language models can replace human raters in generating psycholinguistic norms for metaphors. Three GPT models (GPT3.5-turbo, GPT4o-mini, GPT4o) were prompted to rate 687 metaphors in English and Italian on familiarity, comprehensibility, and imageability. Results showed moderate-to-strong positive correlations between machine-generated and human-generated ratings, with larger models outperforming smaller ones. Machine-generated ratings significantly predicted human behavioral and electrophysiological responses, comparable to human ratings. Ratings were highly stable across sessions. However, LLMs showed greater misalignment with humans for familiar and highly imageable metaphors, and struggled with sensorimotor aspects of metaphors. Overall, GPT models can validly and reliably augment or replace human raters, but limitations remain in handling conventionality and embodied meaning.

## Method Summary
The study used 687 metaphors (469 Italian, 218 English) from established datasets. Three GPT models were prompted via OpenAI API with temperature=0, max_tokens=1, and top_logprobs=3 to rate each metaphor on familiarity, comprehensibility, and imageability using adapted human instructions. Overall ratings were computed as weighted averages of the top-3 token log probabilities. Human ratings were validated against behavioral response times and N400 ERP amplitude from previous EEG studies. Validity was assessed via Spearman correlations, reliability via test-retest correlations across two sessions, and substitution analysis through LME models predicting RTs and N400.

## Key Results
- Machine-generated ratings showed moderate-to-strong positive correlations with human ratings (r = 0.46-0.72 for familiarity, r = 0.41-0.60 for comprehensibility, r = 0.22-0.55 for imageability)
- GPT4o outperformed smaller models, with reliability coefficients above 0.90 across sessions
- Machine-generated ratings predicted N400 amplitude and response times as well as human ratings, with comparable AIC and R² values
- LLMs showed weakest alignment for familiar and highly imageable metaphors, and for body-related and motion metaphors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs approximate human metaphor ratings through contextualized representations that capture figurative meaning patterns from training data.
- Mechan: Transformer attention mechanisms encode relationships between metaphor vehicle and topic, producing rating outputs that correlate with human judgments because both derive from similar linguistic distributional patterns.
- Core assumption: The statistical regularities in human rating behavior are recoverable from textual patterns in training corpora.
- Evidence anchors:
  - [abstract] "Machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors"
  - [section p.3] "LLMs have shown good capacity for capturing the context-dependent meaning of figurative expressions, as indicated by their high accuracy (e.g., 78%) when prompted to interpret metaphors"
  - [corpus] Limited direct corpus evidence for metaphor-specific mechanisms; neighboring papers focus on emotion appraisal and general annotation tasks, not figurative language.
- Break condition: When metaphors require sensorimotor grounding unavailable in text-only training data, the mechanism degrades (see Mechanism 3).

### Mechanism 2
- Claim: Probability-weighted aggregation of top-k token outputs produces continuous ratings that mirror human rating distributions.
- Mechan: By extracting log probabilities for the top 3 most likely rating tokens and computing weighted averages, the method transforms discrete token predictions into continuous scores, approximating the aggregated judgments across multiple human raters.
- Core assumption: Token probability distributions encode meaningful uncertainty about rating values rather than random noise.
- Evidence anchors:
  - [section p.7-8] "We derived the overall rating by weighing each of the three most likely ratings for their log probabilities... resulting in an overall rating of 2.158"
  - [section p.15] "correlations between machine-generated metaphor ratings from two independent sessions showed very high reliability for the three models prompted through the API, with all correlation coefficients above 0.90"
  - [corpus] No corpus papers examine probability-weighted rating aggregation specifically.
- Break condition: Assumption fails if token probabilities reflect prompt artifacts rather than genuine rating uncertainty; the paper does not test this directly.

### Mechanism 3
- Claim: Model scale determines rating quality through emergent representational capacity, but sensorimotor grounding remains a fundamental limitation across scales.
- Mechan: Larger models (GPT4o > GPT4o-mini > GPT3.5-turbo) capture more nuanced linguistic patterns, improving familiarity/comprehensibility ratings, but all models lack embodied experience necessary for sensorimotor features like imageability.
- Core assumption: Performance gains from scale are partially independent of the modality gap between text training and perceptual experience.
- Evidence anchors:
  - [abstract] "larger models (GPT4o) outperforming smaller ones... alignment was weaker for high-familiarity and imageable metaphors, and for sensorimotor aspects of meaning"
  - [section p.11-12] "ratings for motion metaphors reported strong correlations... while ratings of auditory metaphors reported moderate to strong correlations... GPT3.5-turbo: r = -0.16 for body-related metaphors"
  - [corpus] "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli" (arxiv 2508.14214) reports similar human-LLM alignment for emotional content, suggesting modality-specific limitations may generalize.
- Break condition: Scale improvements cannot close the grounding gap; multimodal architectures may be required (not tested in this paper).

## Foundational Learning

- Concept: N400 ERP component and semantic processing
  - Why needed here: The paper validates GPT ratings by testing whether they predict N400 amplitude; understanding this neural signature is essential to interpret the substitution analysis results.
  - Quick check question: Would you expect larger or smaller N400 amplitude for familiar vs. novel metaphors, and why?

- Concept: Temperature parameter and LLM determinism
  - Why needed here: The study sets temperature=0 to ensure reliability; understanding this parameter is critical for reproducing stable ratings vs. exploring variability.
  - Quick check question: If temperature=1 instead of 0, would you expect higher or lower test-retest reliability?

- Concept: Psycholinguistic norming dimensions (familiarity, imageability, comprehensibility)
  - Why needed here: These are the target variables being approximated; distinguishing them is necessary to understand why GPT performs differently across dimensions.
  - Quick check question: Which dimension would you expect to be most challenging for a text-only model to rate accurately—familiarity (frequency of experience), comprehensibility (linguistic naturalness), or imageability (ease of mental imagery)?

## Architecture Onboarding

- Component map: Input metaphor → Prompt (human instructions adapted) → LLM API call → Token generation (top-k=3) → Log probability extraction → Weighted rating aggregation → Validation path: Human ratings correlation / RT prediction / N400 prediction
- Critical path: Setting temperature=0 via API is non-negotiable for reliability; ChatGPT interface showed degraded reliability (r=0.66-0.91 vs. r>0.98 for API).
- Design tradeoffs:
  - GPT4o: Highest validity but higher cost; recommended for final datasets
  - GPT4o-mini: Good validity for English, lower for Italian; acceptable for pilot work
  - GPT3.5-turbo: Not recommended for Italian stimuli; weak correlations (r=0.20)
  - API vs. interface: API required for parameter control; interface unsuitable for reproducible research
- Failure signatures:
  - High-familiarity metaphors: GPT assigns lower ratings than humans, maintaining rigid literal/figurative boundaries
  - Sensorimotor content: Body-related metaphors show weakest alignment (r=-0.16 for GPT3.5)
  - Imageability dimension: Largest human-model discrepancy; models lack perceptual grounding
  - Non-English languages: Smaller models show degraded performance; verify language coverage
- First 3 experiments:
  1. Baseline correlation test: Prompt your target model with 20 metaphors having existing human ratings; require r>0.50 before scaling up.
  2. Reliability check: Run the same 20 metaphors in two independent sessions; require r>0.95 via API with temperature=0.
  3. Edge case probe: Include 5 high-imageability and 5 body-related metaphors; if ratings diverge substantially from human baselines, restrict stimulus set to lower sensorimotor load items.

## Open Questions the Paper Calls Out

- Question: Can LLMs generate valid and reliable ratings for highly conventional figurative expressions such as idioms, where the boundary between literal and figurative language is blurred?
  - Basis in paper: [explicit] Authors state: "Future research could further test this speculation by assessing, for instance, the validity of LLM-generated ratings for highly conventional yet figurative expressions such as idioms (for initial evidence, see O'Reilly et al., 2025)."
  - Why unresolved: This study focused on metaphors; idioms represent a different class of figurative language with higher conventionality, where LLMs showed greater misalignment with humans.
  - What evidence would resolve it: A systematic comparison of LLM-generated vs. human ratings for idiom datasets across familiarity and imageability dimensions, validated against behavioral and neural measures.

- Question: How does data contamination (presence of benchmark datasets in training corpora) affect the validity of LLM-generated metaphor ratings?
  - Basis in paper: [explicit] Authors acknowledge: "Given that GPT models' training data has not been released, we could not conclude that some of the datasets in our study could be part of it."
  - Why unresolved: Without access to model training data, it remains unclear whether strong human-model correlations reflect genuine linguistic understanding or memorization of norming studies.
  - What evidence would resolve it: Testing LLMs on newly created metaphor datasets unavailable before model cutoffs, or comparing performance across models with known vs. unknown training data composition.

- Question: Can LLMs capture individual variability in metaphor ratings, or are they limited to simulating the "average" human rater?
  - Basis in paper: [explicit] Authors note: "LLMs at this point can only approximate an average human participant, or the wisdom of the crowd... This does not allow for focus on individual variability, for which the recruitment of human participants is still essential."
  - Why unresolved: The study only examined aggregate human ratings; whether LLMs could simulate different demographic or individual response patterns was not tested.
  - What evidence would resolve it: Prompting LLMs to adopt different personas or demographic characteristics and comparing the resulting rating distributions to human data stratified by individual differences.

- Question: Can multimodal or grounded AI architectures overcome LLMs' systematic weakness in representing sensorimotor aspects of metaphorical meaning?
  - Basis in paper: [explicit] Authors state: "Perhaps the most relevant limitation of LLMs as metaphor raters regards their ability to capture the embodied aspects of meaning" and identify "lack of grounding as one of the major points of distance between humans and LLMs."
  - Why unresolved: Current LLMs showed weaker alignment for physical metaphors, body-related metaphors, and high-imageability items, suggesting a fundamental limitation of text-only training.
  - What evidence would resolve it: Comparing ratings from multimodal models (trained on vision, sensorimotor data) against text-only models for metaphor subsets varying in sensorimotor load.

## Limitations
- The study exclusively focuses on metaphors, limiting generalizability to other figurative language types like idioms and similes.
- Text-only GPT models show systematic weaknesses in rating sensorimotor aspects like imageability and body-related metaphors.
- Prompts are English-centric adaptations of human instructions, and validation is limited to English and Italian languages.

## Confidence
- High confidence: Test-retest reliability (r > 0.90), validity correlations for familiarity and comprehensibility ratings, and the ability to predict behavioral (RT) and neural (N400) responses.
- Medium confidence: Generalization to other figurative language types, performance with languages beyond English and Italian, and the stability of prompt-engineering requirements across different metaphor sets.
- Low confidence: Model performance on idioms and metonyms, long-term stability of ratings across model version updates, and the robustness of ratings when using different prompt formulations.

## Next Checks
1. Test GPT ratings on a mixed-stimulus set containing idioms, similes, and metonyms alongside metaphors to assess generalizability beyond the current domain.
2. Run the same 20 metaphor stimuli through GPT-4o with two different prompt formulations (one literal adaptation of human instructions, one simplified rating-only prompt) to verify prompt-engineering stability.
3. Include 10 metaphors specifically designed to be low-imageability and low-body-related content, then compare GPT-4o correlations on this subset versus the full dataset to quantify the sensorimotor limitation impact.