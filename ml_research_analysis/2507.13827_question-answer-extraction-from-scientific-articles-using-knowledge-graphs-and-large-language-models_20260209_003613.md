---
ver: rpa2
title: Question-Answer Extraction from Scientific Articles Using Knowledge Graphs
  and Large Language Models
arxiv_id: '2507.13827'
source_url: https://arxiv.org/abs/2507.13827
tags:
- article
- questions
- articles
- extraction
- triplet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two approaches for generating question-answer
  (QA) pairs from scientific articles. The first method, CCQG, uses large language
  models (LLMs) to generate questions and answers from salient paragraphs selected
  based on similarity to article metadata.
---

# Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models

## Quick Facts
- arXiv ID: 2507.13827
- Source URL: https://arxiv.org/abs/2507.13827
- Reference count: 35
- Two approaches for generating QA pairs from scientific articles, with KG-based method outperforming CCQG

## Executive Summary
This paper proposes two approaches for generating question-answer pairs from scientific articles. The KG-based method constructs a knowledge graph from extracted entity relationships, ranks salient triplets using a TF-IDF-like measure, and generates QAs based on these novel aspects. The CCQG method uses large language models to generate questions and answers from salient paragraphs selected based on similarity to article metadata. Both approaches were evaluated by subject matter experts, with the KG-based method showing higher scores in relevance, usefulness, and factuality. Fine-tuning the entity relationship extraction model on scientific data proved crucial for high-quality triplet extraction.

## Method Summary
The KG-based approach extracts entity relationships from scientific articles using a fine-tuned BART model (REBEL), constructs a knowledge graph in Neo4j, and ranks salient triplets using a TF-IDF-like measure combining graph centrality and semantic similarity. The CCQG approach selects salient paragraphs based on metadata similarity and uses LLMs to generate questions and answers. Both methods employ hallucination filtering using SelfCheckGPT and undergo SME evaluation. The KG-based method requires more computational resources but yields higher quality QAs by capturing novel aspects of articles through the knowledge graph structure.

## Key Results
- KG-based approach outperformed CCQG method on SME evaluation metrics (relevance, usefulness, factuality)
- Fine-tuning REBEL on scientific data significantly improved triplet extraction accuracy from 58% to higher levels
- Saliency scoring using TF-IDF-like measure effectively captured article novelty compared to simple frequency analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Filtering Entity Relationship (ER) triplets using a corpus-wide TF-IDF-like measure may better capture an article's novelty compared to simple frequency analysis.
- **Mechanism:** The system constructs a Knowledge Graph (KG) from a corpus of articles. It calculates a saliency score ($S_{graph}$) for triplets by combining the frequency of entities/relations within the target article against their prevalence across the entire literature corpus (Inverse Document Frequency). This prioritizes triplets that are unique to the specific document rather than common knowledge.
- **Core assumption:** Scientific novelty is correlated with the statistical distinctiveness of entity relationships relative to the broader corpus.
- **Evidence anchors:**
  - [abstract]: "This measure assesses the saliency of a triplet based on its importance within the article compared to its prevalence in the literature."
  - [section 3.2.3]: "The motivation behind $S_{graph}$ is that we can retrieve triplets... but with additional structural information... we rank triplets higher that are more novel conditioned on the entire corpus."
  - [corpus]: Weak direct evidence in provided corpus; related works like 'KARMA' discuss KG enrichment but not the specific TF-IDF saliency mechanism.
- **Break condition:** Performance degrades if the background corpus is too small or topic-shifted, causing common triplets to appear "novel" (low IDF erroneously).

### Mechanism 2
- **Claim:** Fine-tuning relation extraction models on domain-specific scientific text is likely required to achieve sufficient triplet quality for downstream tasks.
- **Mechanism:** The authors use an LLM to generate a training dataset of triplets from a representative subset of articles. This dataset is then used to fine-tune the REBEL (BART-based) model. Without this step, generic models fail to capture scientific entities and relations accurately.
- **Core assumption:** The LLM-generated training triplets are of sufficient quality to act as ground truth for the fine-tuning process.
- **Evidence anchors:**
  - [abstract]: "Fine-tuning the ER extraction model on scientific data is crucial for extracting high-quality triplets."
  - [section 5.2]: "The off-the-shelf model correctly detects only 58% of the triplets... highlights the importance of fine-tuning the triplet extraction model on the target corpus."
  - [corpus]: Indirect support from 'ComProScanner' regarding structured data extraction from scientific literature using LLMs, validating the domain difficulty.
- **Break condition:** Mechanism fails if the LLM used for generating training data produces systematic hallucinations, propagating errors into the fine-tuned REBEL model.

### Mechanism 3
- **Claim:** A two-step question ranking process (greedy + supplementary) appears to improve question diversity and coverage compared to single-pass generation.
- **Mechanism:** The system first greedily matches high-scoring questions to salient paragraphs. If paragraphs remain unassigned, a supplementary ranking step generates further questions to ensure broader article coverage, explicitly limiting questions per section to avoid overfitting on specific parts.
- **Core assumption:** Semantic similarity between a question and a paragraph is a reliable proxy for the question's answerability and relevance.
- **Evidence anchors:**
  - [section 3.1.2]: "Greedy ranking starts by aggregating the similarity scores... The process stops when all questions are paired... or there are no paragraphs left."
  - [section 3.1.2]: "We limit the number of questions originated from each section to two questions."
  - [corpus]: No direct evidence for this specific ranking algorithm in the provided neighbor corpus.
- **Break condition:** The mechanism yields redundant or repetitive questions if the semantic embeddings fail to distinguish between different aspects of the same topic.

## Foundational Learning

- **Concept:** **Entity Relationship (ER) Triplets**
  - **Why needed here:** These form the atomic units of the Knowledge Graph (Subject, Relation, Object). Understanding this structure is required to implement the saliency scoring logic (Mechanism 1).
  - **Quick check question:** Given the text "The drug inhibits the protein," what is the likely (Head, Relation, Tail) triplet?

- **Concept:** **Inverse Document Frequency (IDF)**
  - **Why needed here:** The core of the saliency mechanism relies on a TF-IDF-like score. You must understand that IDF penalizes items that appear frequently everywhere (background noise) and boosts rare items (novelty).
  - **Quick check question:** If a triplet appears in 90% of the documents in your corpus, will its IDF score be high or low?

- **Concept:** **Hallucination Filtering (NLI)**
  - **Why needed here:** The architecture uses SelfCheckGPT with Natural Language Inference (NLI) to verify answers. You need to know that LLMs can generate plausible-sounding but factually incorrect statements that must be validated against source text.
  - **Quick check question:** In an NLI context, if a generated answer contradicts the source paragraph, should it be labeled "Entailment," "Neutral," or "Contradiction"?

## Architecture Onboarding

- **Component map:** Article XML -> TopicBert Clustering -> LLM Triplet Generation -> REBEL Fine-tuning -> Neo4j KG -> Saliency Calculator -> LLM QA Generation -> SelfCheckGPT Filtering -> SME Review
- **Critical path:** The Fine-tuning of the REBEL model (Section 3.2.1) is the most sensitive dependency. If the triplet extraction is poor (accuracy < 60%), the subsequent saliency scoring and KG construction will amplify noise rather than signal.
- **Design tradeoffs:**
  - **CCQG vs. KG-based:** CCQG is computationally cheaper (no graph building) but less effective at identifying "novelty" or "factuality" (Abstract). KG-based requires maintaining a graph database and a fine-tuned model but yields higher specificity (Table 2).
  - **LLM vs. Embeddings:** The system uses embeddings for fast similarity search (ranking) but relies on the LLM for the complex cognitive task of question formulation.
- **Failure signatures:**
  - **Generic Questions:** "What is the introduction about?" (Indicates Salient Paragraph selection failed).
  - **Low Triplet Count:** Graph remains empty (Indicates REBEL fine-tuning failed or input parsing error).
  - **High Hallucination Rate:** Answers contain facts not in the text (Indicates SelfCheckGPT threshold is too loose or context window is misconfigured).
- **First 3 experiments:**
  1. **Triplet Extraction Validation:** Run the vanilla REBEL model vs. the fine-tuned model on 10 unseen scientific abstracts. Manually verify if the fine-tuned model captures scientific relations (e.g., "method A outperforms method B") that the vanilla model misses.
  2. **Saliency Threshold Testing:** Take one known article and adjust the $S_{graph}$ threshold. Observe if lowering the threshold results in generic triplets (e.g., "study uses data") entering the top-10 ranked list.
  3. **Hallucination Injection:** Deliberately feed the Answer Generation module a paragraph unrelated to the question to verify that the SelfCheckGPT filter successfully flags the output as a contradiction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of the KG-based approach generalize to scientific domains with distinct terminology beyond Computer Science and Life Sciences?
- Basis in paper: [explicit] The authors state, "A possible future work would be to expand our approach to other scientific domains and increase the size of the corpus."
- Why unresolved: The current study validates the method only on CS and Life Sciences articles; it is untested whether the fine-tuned triplet extraction model maintains high accuracy in fields with vastly different semantic structures.
- What evidence would resolve it: Evaluation results showing the triplet extraction model's accuracy and QA relevance scores when applied to new domains like Chemistry or Physics.

### Open Question 2
- Question: Do the performance gains of the KG-based method over the CCQG method persist when evaluated on a significantly larger sample of QA pairs?
- Basis in paper: [explicit] The authors note, "The SME evaluation of the generated QAs has been conducted on a limited sample of 200 QAs. Future work would involve extending this evaluation to a larger and more diverse set of QAs."
- Why unresolved: Statistical significance and consistency are harder to guarantee with a small sample size (20 articles per domain), raising the possibility that the results are specific to the evaluated subset.
- What evidence would resolve it: A replication of the SME evaluation study involving thousands of QA pairs across a wider variety of articles.

### Open Question 3
- Question: Does the proposed triplet saliency metric quantitatively align with human expert judgment regarding the "novelty" of scientific contributions?
- Basis in paper: [inferred] The paper claims the saliency measure assesses novelty (Section 3.2.3), but the SME evaluation metrics (Section 4.2.1) focus on relevance and factuality rather than explicitly measuring perceived novelty.
- Why unresolved: It remains unclear if the TF-IDF-like saliency score successfully identifies concepts experts deem "novel," or if it merely identifies high-frequency key terms.
- What evidence would resolve it: A specific ablation study where experts rate the "novelty" of generated questions compared to a baseline that does not use the saliency ranking.

## Limitations
- Domain-specific dependency on proprietary Elsevier corpus limits reproducibility and generalization testing
- LLM-as-ground-truth assumption for REBEL fine-tuning creates potential "garbage in, garbage out" scenario
- Evaluation subjectivity from SME assessments introduces human bias not quantified in metrics

## Confidence

**High confidence**: The overall architecture design (KG construction + saliency ranking + LLM QA generation) is internally consistent and the evaluation methodology (SME review + ROUGE scoring) is methodologically sound. The claim that fine-tuning REBEL on scientific data improves performance is directly supported by the 58%â†’higher accuracy comparison.

**Medium confidence**: The superiority of the KG-based approach over CCQG is well-supported by SME scores (Table 2), but the evaluation sample size (20 articles) and lack of statistical significance testing limit generalizability. The TF-IDF-like novelty detection mechanism is theoretically sound but its practical effectiveness depends heavily on corpus size and composition.

**Low confidence**: The paper doesn't address computational costs or latency differences between approaches, nor does it test scalability beyond the 20,000-article corpus. The hallucination filtering mechanism (SelfCheckGPT) is described but not independently validated for scientific text.

## Next Checks
1. **Ground-truth quality audit**: Manually validate 50 LLM-generated triplets from the fine-tuning dataset against their source paragraphs to assess labeling accuracy and hallucination rates.

2. **Cross-domain transferability test**: Apply the fine-tuned REBEL model to a completely different scientific domain (e.g., physics papers if trained on biology) to measure domain adaptation limits.

3. **Statistical significance verification**: Recompute SME evaluation scores with confidence intervals and conduct paired t-tests to verify that KG-based improvements over CCQG are statistically significant, not just numerically higher.