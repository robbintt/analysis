---
ver: rpa2
title: 'SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models'
arxiv_id: '2506.11120'
source_url: https://arxiv.org/abs/2506.11120
tags:
- pruning
- performance
- loss
- methods
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SDMPrune, a novel self-distillation MLP pruning
  method for compressing large language models (LLMs) with minimal performance degradation.
  The method addresses the limitations of existing gradient-based pruning approaches
  that rely on one-hot labels and miss potential predictions from other words, which
  is critical for preserving the generative capability of the original model.
---

# SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models

## Quick Facts
- arXiv ID: 2506.11120
- Source URL: https://arxiv.org/abs/2506.11120
- Reference count: 40
- Reduces MLP parameters by up to 40% while maintaining competitive zero-shot performance on 1B-scale LLMs

## Executive Summary
This paper introduces SDMPrune, a novel two-stage self-distillation method for structured pruning of MLP modules in large language models. The approach addresses the limitations of gradient-based pruning that uses one-hot labels, which miss the full generative capability of the model. By employing self-distillation loss during the pruning phase, SDMPrune preserves the original model's output distribution, obtaining more accurate gradient information for pruning. Experiments on zero-shot benchmarks demonstrate that SDMPrune significantly outperforms existing pruning methods while achieving competitive performance among 1B-scale open-source LLMs.

## Method Summary
SDMPrune targets MLP modules in LLMs using a two-stage pruning approach. Stage 1 performs initial pruning using standard cross-entropy loss to create parameter-space divergence between the original and pruned models. Stage 2 then uses the original model as teacher and the stage-1 model as student, computing importance scores under distillation loss (combining hard cross-entropy with soft KL divergence) to preserve the full output distribution. The method employs Taylor-based importance scores to selectively prune MLP hidden dimensions while preserving attention structure, achieving higher compression with lower performance degradation. The pruned model is then fine-tuned using LoRA or full-parameter training.

## Key Results
- MLP-only pruning at 40% ratio outperforms joint attention+MLP pruning across all tested configurations
- Maintains competitive zero-shot accuracy on ARC-e, ARC-c, BOOLQ, PIQA, and Winogrande benchmarks
- Achieves better perplexity scores on WikiText2 compared to existing pruning methods
- Reduces parameter count by over 40% while preserving generative capabilities

## Why This Works (Mechanism)

### Mechanism 1: Self-Distillation Loss Preserves Full Prediction Distribution During Pruning
The original model's full probability distribution is used as the pruning objective, yielding more accurate importance scores than one-hot labels. Instead of computing gradients from cross-entropy with hard labels, self-distillation loss combines hard loss with KL divergence between teacher and student output distributions. This propagates gradient information from the entire vocabulary, preserving the model's generative capability rather than just classification accuracy.

### Mechanism 2: MLP Modules Are Disproportionately Low-Sensitivity, High-Parameter Targets
Pruning MLP hidden-layer neurons achieves higher compression with lower performance degradation than pruning attention modules. Taylor-based importance scores show MLP parameters have substantially lower average importance than attention parameters, yet MLP modules contain >5× more parameters. By selectively pruning MLP hidden dimensions while preserving attention structure, the model retains token-interaction modeling while reducing the dominant parameter count.

### Mechanism 3: Two-Stage Pruning Resolves the Teacher-Student Identity Problem
A cold-start stage with one-hot loss followed by a calibration stage with distillation loss is necessary because self-distillation is mathematically infeasible when student equals teacher. Stage 1 performs initial pruning using standard classification loss to create parameter-space divergence between original and pruned models. Stage 2 then uses the original model as teacher and the stage-1 model as student.

## Foundational Learning

- Concept: First-Order Taylor Expansion for Importance Scoring
  - Why needed here: The method uses |∂L/∂W × W| to approximate how much loss would increase if a weight were set to zero, without actually removing it.
  - Quick check question: Can you explain why multiplying gradient by weight magnitude estimates sensitivity to removal?

- Concept: Knowledge Distillation with Temperature Scaling
  - Why needed here: The soft loss uses temperature T to soften probability distributions before KL divergence, controlling how much attention is paid to non-target tokens.
  - Quick check question: What happens to the gradient signal when temperature is very high vs. very low?

- Concept: Structural vs. Unstructured Pruning
  - Why needed here: SDMPrune removes entire neurons (structural) rather than individual weights, requiring coordinated removal across up-projection, gate, and down-projection layers.
  - Quick check question: Why does structural pruning provide actual inference speedup while unstructured pruning often doesn't?

## Architecture Onboarding

- Component map:
  Input → [Embedding] → ×L [Attention Block → MLP Block] → [Output Head]

- Critical path:
  1. Load pretrained LLM and calibration data (C4, 1024 sequences × 1024 tokens)
  2. Stage 1: Forward pass with hard loss, compute gradients, calculate Taylor importance scores, retain top-k' neurons
  3. Stage 2: Load original model as teacher, pruned model as student, compute distillation loss, recalculate importance, retain final k neurons
  4. Fine-tune with LoRA or full-parameter training

- Design tradeoffs:
  - Higher α (distillation weight) preserves generative capability but may reduce task-specific accuracy
  - MLP-only pruning sacrifices maximum compression for better recoverability
  - Two-stage approach doubles forward passes but avoids post-hoc distillation training

- Failure signatures:
  - PPL spikes dramatically (>3× original) → likely pruned attention layers or too aggressive ratio
  - Zero-shot accuracy drops uniformly across tasks → distillation not working, check teacher-student alignment
  - Inference doesn't accelerate → structural pruning not applied correctly, still sparse tensors

- First 3 experiments:
  1. Replicate Figure 1b on your target model: compute Taylor importance scores for attention vs. MLP layers to verify the sensitivity gap holds.
  2. Ablation study from Table 4: compare attn+MLP pruning vs. MLP-only at 20% ratio on WikiText2 PPL.
  3. Hyperparameter sweep on α and T (Figure 4): find optimal distillation weight and temperature for your target model size.

## Open Questions the Paper Calls Out

### Open Question 1
How can the computational overhead and memory usage of the teacher model be reduced during the pruning process without compromising accuracy? The current implementation requires maintaining the original model as a teacher during the pruning phase, which doubles the memory footprint compared to single-model pruning methods.

### Open Question 2
Can the method be modified to maintain model performance at pruning ratios significantly higher than the tested 40%? The paper demonstrates strong results up to a 40% pruning ratio, but performance drops noticeably at higher rates, suggesting a performance cliff.

### Open Question 3
How does the method's reliance on the "cold-start" threshold (k') affect the stability and final performance of the self-distillation stage? It is unclear if the "low pruning ratio" used for the cold start is a robust heuristic or a sensitive hyperparameter that requires tuning for different model sizes.

## Limitations

- The two-stage process doubles forward passes during pruning, increasing computational overhead
- Method hasn't been validated on architectures beyond LLaMA3.2-1.2B and LLaMA3-8B
- Doesn't analyze how dataset size or domain affects pruning quality
- No investigation of layer-specific pruning ratios for deeper models

## Confidence

**High confidence** in: The fundamental claim that self-distillation preserves more generative capability than one-hot pruning, supported by clear ablation showing performance degradation when α=0.

**Medium confidence** in: The specific two-stage approach being optimal. While logically sound and effective, the paper doesn't compare against alternative single-stage distillation methods.

**Low confidence** in: Generalization to other model architectures beyond LLaMA3.2-1.2B and LLaMA3-8B. The paper doesn't test whether the MLP/attention sensitivity ratio holds for architectures like Mistral, Gemma, or domain-specific models.

## Next Checks

1. **Layer-wise sensitivity analysis**: Run the Taylor importance computation layer by layer on your target model to verify that the MLP/attention sensitivity gap exists in your architecture.

2. **Distillation hyperparameter sweep**: Systematically vary α (0.0 to 1.0) and temperature (0.5 to 10.0) on a small calibration set to find optimal values before full pruning.

3. **Compression-recoverability test**: After Stage 1 pruning, evaluate zero-shot performance. If it drops >20% from baseline, the Stage 1 ratio is too aggressive and will likely prevent successful Stage 2 recovery.