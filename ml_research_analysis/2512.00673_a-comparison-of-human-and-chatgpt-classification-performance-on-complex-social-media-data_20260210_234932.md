---
ver: rpa2
title: A Comparison of Human and ChatGPT Classification Performance on Complex Social
  Media Data
arxiv_id: '2512.00673'
source_url: https://arxiv.org/abs/2512.00673
tags:
- gpt-4
- content
- prompt
- language
- tactics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the performance of GPT-3.5, GPT-4, and GPT-4o
  in classifying nuanced objection tactics in social media comments, using four different
  prompt styles. The best results came from prompt type 2, which provided strategy
  definitions and exemplars, achieving F1 scores above 0.8 for some tactics like physical
  threat.
---

# A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data

## Quick Facts
- arXiv ID: 2512.00673
- Source URL: https://arxiv.org/abs/2512.00673
- Reference count: 36
- Primary result: GPT-4 struggles with cultural euphemisms, internet slang, and nuanced social media content despite achieving F1 scores above 0.8 for some tactics with optimal prompt engineering

## Executive Summary
This study evaluated the performance of GPT-3.5, GPT-4, and GPT-4o in classifying nuanced objection tactics in social media comments using four different prompt styles. The research found that while GPT-4 achieved strong performance with certain prompt types (particularly those providing strategy definitions and exemplars), it consistently struggled with cultural euphemisms, internet slang, and distinguishing between similar threat categories. GPT-4o unexpectedly underperformed relative to GPT-3.5, suggesting newer model versions may not always be superior for nuanced classification tasks. The findings highlight the need for human oversight when deploying LLMs for complex social media content analysis involving culturally embedded language.

## Method Summary
The researchers fine-tuned GPT-3.5, GPT-4, and GPT-4o on 400 social media comments to classify them into eight categories of objection tactics. Four different prompt engineering approaches were tested, varying in their use of examples and explanations. Performance was evaluated using F1 scores across all eight categories, with particular attention to tasks involving nuanced language, cultural references, and internet slang. Qualitative analysis was conducted to understand model failures and inconsistencies in reasoning patterns.

## Key Results
- GPT-4 achieved F1 scores above 0.8 for physical threat classification using prompt type 2 (definitions + exemplars)
- GPT-4o underperformed GPT-3.5, contrary to expectations for newer model versions
- Models consistently struggled with cultural euphemisms, internet slang, and distinguishing between ad hominem attacks and content threats
- Performance varied significantly across objection tactics, with some categories showing poor consistency

## Why This Works (Mechanism)
The study demonstrates that prompt engineering significantly impacts LLM performance on nuanced classification tasks, with structured prompts providing definitions and exemplars yielding the best results. The mechanism relies on the model's ability to recognize patterns when provided with clear task framing and concrete examples, though this advantage diminishes when dealing with culturally embedded or ambiguous content that falls outside typical training distributions.

## Foundational Learning
- Cultural context awareness: Why needed - Social media content often contains culturally specific references and euphemisms; Quick check - Can the model correctly interpret region-specific slang or idioms
- Prompt engineering principles: Why needed - Task framing significantly impacts model performance; Quick check - Does the model maintain performance across different prompt structures for the same task
- Fine-tuning data requirements: Why needed - Small datasets may not capture the full complexity of social media language; Quick check - Does performance improve with larger, more diverse training samples
- Cultural-linguistic model limitations: Why needed - LLMs may not be trained on sufficient culturally diverse data; Quick check - Does the model handle non-Western social media contexts effectively
- Human-AI collaborative workflows: Why needed - Human oversight can compensate for model limitations; Quick check - Does human review of ambiguous cases improve overall classification accuracy

## Architecture Onboarding
- Component map: Human-curated training data -> Fine-tuning process -> Multiple GPT models (3.5, 4, 4o) -> Four prompt variants -> Classification output -> Performance evaluation
- Critical path: Fine-tuning data preparation -> Model training -> Prompt engineering testing -> Classification task execution -> F1 score calculation -> Qualitative error analysis
- Design tradeoffs: Using smaller training datasets allows for qualitative analysis but may limit generalizability; Testing multiple prompt types reveals optimal strategies but increases experimental complexity
- Failure signatures: Inconsistent reasoning across similar examples, inability to distinguish between closely related categories, poor performance on culturally specific content, unexpected underperformance of newer model versions
- First 3 experiments: 1) Test GPT-4o with expanded prompt variations beyond the four types evaluated; 2) Fine-tune models on culturally diverse social media datasets; 3) Implement human review workflow for cases involving ambiguous or culturally specific content

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Small sample size of 400 comments may limit generalizability to broader social media contexts
- Performance variability across objection tactics suggests task-specific rather than universal model capabilities
- Unexpected GPT-4o underperformance raises questions about newer model versions' suitability for nuanced tasks
- Focus on U.S.-centric social media content limits applicability to other cultural contexts

## Confidence
- High confidence: GPT models struggle with cultural euphemisms and internet slang
- Medium confidence: Overall classification performance metrics given limited sample size
- Low confidence: Generalizability of prompt type 2 superiority across different classification tasks

## Next Checks
1. Replicate the study with a larger, more diverse corpus of social media comments spanning multiple cultural contexts and topics to test the robustness of findings across different types of nuanced content
2. Conduct a systematic comparison of multiple prompt engineering strategies across different GPT model versions to determine whether GPT-4o's underperformance was specific to the prompt types tested
3. Implement a human-AI collaborative workflow where model classifications are subjected to targeted human review for cases involving cultural references, slang, or ambiguous content to measure potential performance improvements