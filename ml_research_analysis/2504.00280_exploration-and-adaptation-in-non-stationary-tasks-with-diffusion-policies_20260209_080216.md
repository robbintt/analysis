---
ver: rpa2
title: Exploration and Adaptation in Non-Stationary Tasks with Diffusion Policies
arxiv_id: '2504.00280'
source_url: https://arxiv.org/abs/2504.00280
tags:
- policy
- diffusion
- action
- visual
- maze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of Diffusion Policy for non-stationary,
  vision-based reinforcement learning tasks. The method employs a denoising diffusion
  probabilistic model to iteratively refine noisy action sequences, conditioned on
  visual embeddings from a convolutional neural network.
---

# Exploration and Adaptation in Non-Stationary Tasks with Diffusion Policies

## Quick Facts
- arXiv ID: 2504.00280
- Source URL: https://arxiv.org/abs/2504.00280
- Authors: Gunbir Singh Baveja
- Reference count: 15
- One-line primary result: Diffusion Policy outperforms PPO and DQN on non-stationary vision-based RL tasks but faces challenges with extreme non-stationarity and high computational demands.

## Executive Summary
This study introduces Diffusion Policy for non-stationary, vision-based reinforcement learning tasks, using a denoising diffusion probabilistic model to iteratively refine noisy action sequences conditioned on visual embeddings from a convolutional neural network. Experiments on Procgen and PointMaze benchmarks demonstrate that Diffusion Policy achieves higher mean and maximum rewards with reduced variability compared to standard RL methods like PPO and DQN. The method shows strong potential for adaptive control in dynamic settings, particularly in tasks requiring coherent, contextually relevant action sequences, though it struggles with extreme non-stationarity and requires significant computational resources.

## Method Summary
The approach employs a denoising diffusion probabilistic model (DDPM) to iteratively refine noisy action sequences, conditioned on visual embeddings from a convolutional neural network (CNN) encoder. The policy operates in a closed-loop receding-horizon control fashion, executing only the first action from the predicted sequence and re-planning at each timestep based on new observations. The visual encoder (modified ResNet with GroupNorm) processes 64×64 RGB frames, which are combined with state vectors and temporally stacked over an observation horizon. A conditional U-Net denoises the action sequence through multiple diffusion steps, with the model trained using MSE loss on predicted noise and optimized with AdamW. The method is evaluated on CoinRun, Maze (Procgen), and PointMaze (D4RL) environments.

## Key Results
- Diffusion Policy consistently outperforms PPO and DQN baselines on Procgen and PointMaze benchmarks
- Achieves higher mean and maximum rewards with reduced variability across training episodes
- Excels in environments requiring coherent, contextually relevant action sequences
- Demonstrates strong potential for adaptive control in dynamic settings
- Faces challenges with extreme non-stationarity where environmental dynamics shift faster than diffusion inference time

## Why This Works (Mechanism)

### Mechanism 1: Iterative Denoising for Coherent Action Sequence Generation
The multi-step denoising process produces more coherent and contextually relevant action sequences compared to single-shot policy outputs. A noisy action sequence is progressively refined through T diffusion steps, where each step conditionally denoises based on visual embeddings φ(s). This iterative refinement allows the model to "think longer" about action consistency, potentially smoothing out discontinuities that single-pass policies might produce.

### Mechanism 2: Closed-Loop Receding-Horizon Control
Executing only the first action from the predicted sequence and re-planning at each timestep provides resilience to environmental changes. Rather than committing to an open-loop action sequence, the policy samples a new observation after each action, re-encodes it through the visual encoder, and re-initiates the diffusion process. This creates a feedback loop that incorporates new state information before each decision.

### Mechanism 3: Unified Visual-State Embedding with Temporal Stacking
Combining CNN-derived visual features with low-dimensional state vectors, plus stacking frames over an observation horizon, improves the model's ability to infer dynamics. Raw 64×64 RGB frames pass through a ResNet-based encoder (with GroupNorm for stability). State vectors (agent position, goal locations) are concatenated with visual embeddings. Multiple frames are stacked over horizon To to provide temporal context.

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: The entire policy architecture rests on understanding how forward diffusion (adding noise) and reverse denoising (predicting and removing noise) work. Without this, the training objective and inference procedure will be opaque.
  - Quick check question: Can you explain why DDPMs train a network to predict noise rather than directly predicting clean data?

- Concept: Visual Encoders for Control (ResNet, CNN feature extraction)
  - Why needed here: The policy receives 64×64 RGB images that must be compressed into embeddings. Understanding how spatial features are extracted and what inductive biases CNNs provide is essential for debugging perception failures.
  - Quick check question: Why might GroupNorm be preferred over BatchNorm when training on diverse, small datasets?

- Concept: Off-Policy vs On-Policy RL and Experience Replay
  - Why needed here: The method uses collected demonstration datasets stored in Zarr format. Understanding how data is collected, stored, and reused helps diagnose sample efficiency issues mentioned in the paper.
  - Quick check question: What is the difference between on-policy and off-policy learning, and which category does this diffusion policy approach fall into?

## Architecture Onboarding

- Component map: [RGB Frame 64×64×3] → [ResNet Encoder + GroupNorm] → [Visual Embedding] → [Concatenation] → [Unified Representation] → [DDPM Noise Injection] → [Noisy Actions] → [U-Net Denoiser] → [Refined Action Sequence] → [Execute First Action] → [Environment] → [New Observation]

- Critical path: Visual Encoder → Unified Representation → U-Net Denoiser. Errors in the visual encoder cascade through the entire pipeline; if spatial features are corrupted, action sequences will be misaligned with the environment.

- Design tradeoffs:
  - Inference steps vs latency: More diffusion steps improve action quality but increase inference time
  - Encoder depth vs training stability: Deeper encoder improves performance but may require more data
  - Observation horizon To vs memory: Longer horizons capture more dynamics but increase memory and computation

- Failure signatures:
  - High variance in rewards across episodes suggests visual encoder is not learning invariant features
  - Consistently suboptimal actions in familiar states suggests diffusion process not converging
  - Sudden performance drops when environment changes indicates adaptation failure
  - Training instability may indicate BatchNorm issues—verify GroupNorm is used

- First 3 experiments:
  1. Reproduce baseline on single environment: Train on CoinRun with specified architecture and verify mean reward approaches ~8.15 within 500K steps
  2. Ablate diffusion steps: Reduce diffusion steps during inference and measure reward degradation and inference time
  3. Test closed-loop vs open-loop execution: Compare executing full predicted sequence without re-planning vs re-planning each step on non-stationary variant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can autoregressive action generation provide better adaptability than whole-sequence generation in highly dynamic non-stationary environments?
- Basis in paper: Section 6.4 explicitly lists "Autoregressive vs. Whole Policy Generation" as a missed opportunity, noting that this comparison was intended but rendered infeasible by sample inefficiency and resource constraints
- Why unresolved: High sample inefficiency of the Diffusion Policy and limited computational resources prevented execution of this specific comparative experiment
- What evidence would resolve it: A study directly comparing training stability and reward performance of autoregressive models against whole-sequence Diffusion Policy on the same benchmarks

### Open Question 2
- Question: Does implementing a transformer-based visual encoder significantly enhance the model's ability to capture long-range dependencies compared to the current ResNet-based approach?
- Basis in paper: Section 3.1 and Section 6.4 state that exploring transformer-based encoders could improve capture of long-range dependencies and temporal dynamics
- Why unresolved: The current architecture utilizes modified ResNet with skip connections and GroupNorm, and transformer modification was identified as an alternative approach not pursued
- What evidence would resolve it: Empirical results from training Diffusion Policy with transformer-based encoder on non-stationary tasks, specifically analyzing performance on tasks requiring long-term memory

### Open Question 3
- Question: Can the inherent latency of the iterative denoising process be mitigated to allow effective control in environments with extreme non-stationarity?
- Basis in paper: Section 6.2 notes that the policy struggles in environments where dynamics shift "too rapidly for the Diffusion Policy to adjust effectively within the iterative denoising process"
- Why unresolved: The paper identifies extreme non-stationarity as a failure mode but does not propose or test architectural changes specifically designed to accelerate the refinement process for rapid real-time adaptation
- What evidence would resolve it: Demonstration of modified Diffusion Policy maintaining high reward convergence in environments with dynamics exceeding current model's adaptation threshold

## Limitations
- Faces challenges with extreme non-stationarity where environmental dynamics shift faster than diffusion inference time
- High computational demands due to iterative denoising process
- Lacks ablation studies isolating each mechanism's contribution to overall performance
- Specific hyperparameters (diffusion steps, observation horizon, learning rate) are not specified

## Confidence
- Medium: The iterative denoising mechanism for coherent action sequences is well-supported by experimental results showing superior performance to PPO and DQN, but exact contribution of each architectural component remains unclear
- Medium: The closed-loop adaptation mechanism is plausible but untested against open-loop variants in the paper
- Medium: The unified embedding approach is standard but its specific effectiveness for non-stationary adaptation is not definitively proven

## Next Checks
1. Ablate diffusion steps during inference to quantify the speed-quality tradeoff and identify minimum steps needed for task completion
2. Test closed-loop vs open-loop execution on non-stationary environments to isolate contribution of feedback mechanism
3. Reduce observation horizon To to determine minimum temporal context needed for effective action prediction and adaptation