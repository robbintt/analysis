---
ver: rpa2
title: A Grey-box Text Attack Framework using Explainable AI
arxiv_id: '2503.08226'
source_url: https://arxiv.org/abs/2503.08226
tags:
- adversarial
- attack
- sentences
- surrogate
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Grey-box Text Attack Framework using Explainable
  AI to generate adversarial text that can fool transformer models without requiring
  access to the model's internal parameters. The method leverages LIME to identify
  the most influential words in a sentence, then replaces them with semantically similar
  synonyms to create adversarial examples.
---

# A Grey-box Text Attack Framework using Explainable AI

## Quick Facts
- arXiv ID: 2503.08226
- Source URL: https://arxiv.org/abs/2503.08226
- Authors: Esther Chiramal; Kelvin Soh Boon Kai
- Reference count: 17
- Primary result: Grey-box text attack framework using LIME and surrogate models achieves 91% success rate on BERT sentiment classification

## Executive Summary
This paper introduces a grey-box text attack framework that generates adversarial examples without requiring access to a target model's internal parameters. The approach combines LIME for word importance identification with synonym replacement to create minimal perturbations that fool transformer models. By validating attacks through surrogate models, the framework ensures transferability across different transformer architectures like BERT, RoBERTa, and DistilBERT. The method demonstrates high success rates in sentiment classification tasks while maintaining semantic similarity to original inputs.

## Method Summary
The framework operates in three stages: first, LIME generates explanations to identify the most influential words in a given sentence; second, these words are replaced with semantically similar synonyms to create adversarial examples; third, the examples are validated using surrogate transformer models to ensure transferability. The approach targets the output probabilities of the model without requiring gradient information or model architecture details. The attack minimizes word changes while maximizing the likelihood of misclassification, making it particularly effective against transformer-based sentiment classifiers.

## Key Results
- Achieved 91% attack success rate on BERT model for sentiment classification
- Successfully transferred attacks across BERT, RoBERTa, and DistilBERT architectures
- Maintained semantic similarity with minimal word replacements (typically 1-2 words per sentence)

## Why This Works (Mechanism)
The framework exploits the inherent vulnerability of transformer models to small, semantically-preserving perturbations in input text. By using LIME to identify words with the highest influence on model predictions, the attack targets the most sensitive components of the input. The replacement with synonyms creates adversarial examples that appear natural to human readers while causing misclassification. The validation through surrogate models ensures that the attacks are not model-specific and can transfer across different transformer architectures, making the approach robust and widely applicable.

## Foundational Learning

- **LIME (Local Interpretable Model-agnostic Explanations)**: Used to identify influential words in text; needed for understanding which words to target for minimal perturbation; quick check: verify LIME explanations align with human intuition of word importance
- **Transformer models (BERT, RoBERTa, DistilBERT)**: Target models for attack; needed as state-of-the-art NLP models vulnerable to adversarial attacks; quick check: confirm model architecture and pre-trained weights are correctly loaded
- **Synonym replacement**: Method for creating adversarial examples; needed to maintain semantic similarity while changing model predictions; quick check: validate that synonyms preserve original meaning through human evaluation
- **Surrogate model validation**: Ensures attack transferability; needed because grey-box setting doesn't allow direct access to target model internals; quick check: test attack success rate on multiple surrogate models
- **Adversarial transferability**: Property that allows attacks to work across different models; needed for practical application without target model access; quick check: measure attack success rate across different transformer variants

## Architecture Onboarding

**Component Map**: Input text -> LIME explanation -> Word selection -> Synonym replacement -> Surrogate model validation -> Adversarial example

**Critical Path**: The most critical sequence is Input text → LIME explanation → Word selection → Surrogate model validation. LIME must accurately identify influential words, and surrogate models must reliably predict attack success before deploying against the target model.

**Design Tradeoffs**: The framework trades computational efficiency for attack effectiveness by using LIME explanations instead of gradient-based methods. This makes it more broadly applicable but potentially less efficient than white-box attacks. The reliance on surrogate models introduces uncertainty but enables grey-box operation.

**Failure Signatures**: Common failure modes include: LIME failing to identify truly influential words, synonyms that don't preserve semantic meaning, or poor transferability between surrogate and target models. These manifest as low attack success rates or semantic drift in generated examples.

**First Experiments**:
1. Test LIME explanation quality on a small set of sentences by comparing top-k words with human-identified important words
2. Validate synonym replacement maintains semantic similarity through human evaluation or automated semantic similarity metrics
3. Verify attack transferability by testing on a known model pair (e.g., BERT → RoBERTa) before broader deployment

## Open Questions the Paper Calls Out

The paper identifies several open questions: How can this framework be extended to more complex NLP tasks beyond sentiment classification? What is the impact of different synonym databases or selection strategies on attack success rates? How can the framework be adapted to work with defense mechanisms like adversarial training or input preprocessing? Additionally, the paper questions the generalizability of transferability patterns across different model architectures and datasets.

## Limitations

- Limited evaluation to sentiment classification tasks without testing on more diverse NLP applications
- Does not examine robustness against common defense mechanisms like adversarial training or input sanitization
- Quality and contextual appropriateness of synonyms is not thoroughly analyzed, potentially affecting semantic preservation

## Confidence

- Core attack methodology: High
- Transferability across transformer variants: High
- Generalizability to other NLP tasks: Medium
- Defense resistance: Low

## Next Checks

1. Test transferability against adversarially trained models and common defense mechanisms to assess practical robustness
2. Evaluate performance on diverse NLP tasks beyond sentiment classification (e.g., NLI, NER) to establish generalizability
3. Conduct ablation studies to quantify the impact of LIME explanation quality on attack success rates and identify potential improvements