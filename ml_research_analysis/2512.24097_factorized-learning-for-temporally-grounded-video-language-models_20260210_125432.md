---
ver: rpa2
title: Factorized Learning for Temporally Grounded Video-Language Models
arxiv_id: '2512.24097'
source_url: https://arxiv.org/abs/2512.24097
tags:
- grounding
- video
- temporal
- textual
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes D2VLM, a factorized learning framework for
  temporally grounded video-language models that decouples temporal evidence grounding
  and textual response generation. The approach introduces evidence tokens to explicitly
  capture event-level visual semantics and adopts a "grounding then answering with
  evidence referencing" paradigm.
---

# Factorized Learning for Temporally Grounded Video-Language Models

## Quick Facts
- **arXiv ID:** 2512.24097
- **Source URL:** https://arxiv.org/abs/2512.24097
- **Reference count:** 40
- **Primary result:** Introduces D2VLM, a factorized learning framework achieving state-of-the-art performance on temporally grounded video-language tasks.

## Executive Summary
This paper proposes D2VLM, a factorized learning framework that decouples temporal evidence grounding and textual response generation for video-language models. The approach introduces evidence tokens to explicitly capture event-level visual semantics and adopts a "grounding then answering with evidence referencing" paradigm. A novel factorized preference optimization (FPO) algorithm is also introduced, incorporating probabilistic temporal grounding modeling to enable preference learning for both tasks. Experimental results demonstrate clear superiority across various tasks, with the 3.8B model outperforming existing state-of-the-art methods by significant margins.

## Method Summary
D2VLM employs a two-stage generation process where the model first performs pure temporal grounding using dedicated evidence tokens, then generates an interleaved text-evidence response that references the grounded evidence. The architecture uses EVA-CLIP ViT-G/14 for visual encoding, a Q-Former-like compressor, and Phi-3-Mini-3.8B as the base LLM. Evidence tokens capture event-level visual semantics through similarity-based grounding and visual semantic aggregation. The framework includes a consistency loss to align evidence tokens between stages and employs factorized preference optimization (FPO) that incorporates probabilistic temporal grounding modeling. A synthetic dataset with factorized perturbations is constructed to address the lack of suitable data for factorized preference learning.

## Key Results
- On E.T. Bench Grounding, D2VLM achieves 60.2 F1 score, outperforming Qwen2.5-VL-7B's 46.6.
- On Charades-STA, the method reaches 50.3% R@1(IoU=0.5), surpassing VideoChat-T-7B's 48.7%.
- Ablation studies confirm the effectiveness of both event-level modeling and visual semantic capture components.

## Why This Works (Mechanism)

### Mechanism 1
The model first performs pure evidence grounding using dedicated evidence tokens (`<evi>`) that capture event-level visual semantics, then generates an interleaved text-evidence response that references the grounded evidence. A consistency loss (`L_cons`) enforces alignment between evidence tokens from both stages. This sequential "grounding then answering" paradigm improves both tasks by establishing a logical hierarchy where accurate grounding is a prerequisite for reliable answer generation.

### Mechanism 2
Factorized Preference Optimization (FPO) extends standard preference optimization by adding a grounding probability term. For each generated evidence token, the interval-level grounding probability is computed as the product of frame-level similarities between the `<evi>` token and video frame features. This grounding log-probability is summed with the standard textual token log-probability in the overall optimization objective, explicitly rewarding correct temporal localization.

### Mechanism 3
A factorized, controllable synthetic dataset enables effective training of the proposed FPO algorithm where real data is lacking. The data synthesis pipeline applies factorized perturbations to ground-truth responses at the sub-video event level, categorized into temporal grounding and textual response factors with sub-factors like time shift and content distortion. This creates dispreferred samples with precisely known, localized errors for targeted training.

## Foundational Learning

**Concept: Temporal Grounding in Video-Language Models**
- Why needed here: The entire paper revolves around improving this specific capability—the ability to localize events in a video based on textual queries.
- Quick check question: Can you explain the difference between temporal grounding (moment retrieval) and dense video captioning?

**Concept: Preference Optimization (e.g., DPO)**
- Why needed here: The paper's core algorithm, FPO, is a direct modification of this paradigm. Understanding DPO's goal of aligning a model with preferred outputs over dispreferred ones is essential.
- Quick check question: What is the basic intuition behind using preferred/dispreferred response pairs for model training, as in DPO?

**Concept: Autoregressive Modeling in LLMs**
- Why needed here: The D2VLM framework operates within this paradigm. Its "grounding then answering" sequence is implemented as a specific token generation flow.
- Quick check question: In an autoregressive model, how is the probability of a token sequence (P(w1, w2, ..., wn)) typically decomposed?

## Architecture Onboarding

**Component map:**
Visual Encoder (ViT-G/14) → Feature Compressor (Q-Former-like) → Base LLM (Phi-3-Mini-3.8B) → Evidence Token Module → Loss Functions

**Critical path:** Input video/question → Encoder → LLM generates `<evi>` tokens (Grounding Stage) → `</evi>` token marks transition → LLM generates interleaved text & `<evi>` tokens (Response Stage). Each `<evi>` token's similarity to frame tokens determines the grounded interval.

**Design tradeoffs:**
- **Threshold Sensitivity:** Choosing the similarity threshold (e.g., 0.6 * max_sim) for identifying "salient frames" during inference affects grounding precision/recall. Ablation shows a sweet spot exists.
- **Decoupled Sequence Design:** Enforcing a two-stage response (`... </evi> Answer...`) provides structure but reduces flexibility compared to fully interleaved generation.
- **Synthetic vs. Real Data for FPO:** Using synthetic data ensures factorized control but may not cover the full diversity of real model errors.

**Failure signatures:**
1. **Inconsistent Grounding:** The model grounds evidence correctly in the first stage but references different, incorrect intervals in its final answer. (Mitigated by `L_cons`).
2. **Degenerate Grounding:** `<evi>` tokens have uniformly low similarity with all frames, failing to localize any event. (Likely due to poor `L_gnd` training).
3. **Repetitive Response:** The textual description repeats across different grounded events, a known LLM issue that the synthetic data attempts to address.

**First 3 experiments:**
1. **Validate the Decoupled Objective:** Train a baseline (coupled) model and the D2VLM model (without FPO) on the same data. Compare performance on grounding (F1) and captioning (Sim) to isolate the gain from the factorized architecture.
2. **Ablate Evidence Token Components:** Train three variants: (a) without event-level modeling (single-frame), (b) without visual semantic capture, (c) full model. Compare on dense captioning tasks where textual quality depends heavily on captured semantics.
3. **Evaluate FPO Contribution:** Train with and without the FPO stage (using the synthetic dataset). Measure the improvement on the E.T. Bench Grounding task, which explicitly requires precise temporal localization.

## Open Questions the Paper Calls Out
None

## Limitations
- The core assumption that decoupling temporal grounding from text generation is universally beneficial may not hold for all video-language tasks.
- The synthetic dataset generation approach may not fully capture the complexity and diversity of real-world model failures, potentially limiting the generalization of the FPO algorithm.
- The effectiveness of the evidence token mechanism relies heavily on the quality of the similarity computation between evidence tokens and video frame features.

## Confidence
- **High Confidence:** The architectural design of D2VLM is clearly specified and the reported performance improvements on benchmark datasets are substantial and consistent.
- **Medium Confidence:** The FPO algorithm's theoretical contribution is well-defined, but implementation details and hyperparameters are sparse.
- **Medium Confidence:** The synthetic dataset methodology is novel and well-motivated, but lacks direct comparison with alternative data synthesis approaches.

## Next Checks
1. **Cross-Task Robustness:** Evaluate D2VLM on a broader set of video-language tasks beyond temporal grounding to assess whether the decoupled approach maintains its advantages or introduces limitations in less temporally-constrained scenarios.
2. **Real vs. Synthetic Preference Data:** Conduct an ablation study comparing FPO trained solely on synthetic dataset versus FPO trained on a smaller set of high-quality, manually-annotated real preference data.
3. **Similarity Measure Analysis:** Perform a detailed analysis of the evidence token similarity computation, including visualizing similarity score distributions and testing alternative similarity metrics.