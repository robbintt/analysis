---
ver: rpa2
title: Information-Theoretic Perspectives on Optimizers
arxiv_id: '2502.20763'
source_url: https://arxiv.org/abs/2502.20763
tags:
- entropy
- steps
- information
- where
- optimizers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework for analyzing
  neural network optimizers, addressing the limitations of traditional sharpness metrics.
  The authors propose the entropy gap as a complementary metric to better capture
  optimization dynamics and generalization performance.
---

# Information-Theoretic Perspectives on Optimizers

## Quick Facts
- arXiv ID: 2502.20763
- Source URL: https://arxiv.org/abs/2502.20763
- Authors: Zhiquan Tan; Weiran Huang
- Reference count: 8
- Primary result: Entropy gap metric complements sharpness in optimizer analysis and improves Lion optimizer

## Executive Summary
This paper introduces an information-theoretic framework for analyzing neural network optimizers, addressing limitations of traditional sharpness metrics. The authors propose the entropy gap as a complementary metric to better capture optimization dynamics and generalization performance. Through theoretical analysis and experiments on ResNet and ViT architectures, they demonstrate that both sharpness and entropy gap significantly impact optimizer effectiveness. The study also applies these tools to analyze and improve the recently proposed Lion optimizer, showing it can be viewed as an effective update direction information compressor.

## Method Summary
The authors develop an information-theoretic framework using α-order matrix entropy (with α=1 recovering von Neumann entropy) to compute entropy gap as log n - H(H) for Hessian H. They analyze optimizers (SGD, Adam, Lion) on ResNet18 and ViT using CIFAR-10, computing sharpness (Hessian trace), positive eigenvalue ratio, and entropy gap at multiple training steps. For Lion, they model update directions as binary classification problems under Gaussian observations and derive tanh as an optimal information bottleneck representation.

## Key Results
- Entropy gap significantly correlates with optimizer effectiveness beyond what sharpness alone can explain
- ResNet18 performs better with SGD (entropy gap ~0.5-0.6) while ViT performs better with Adam (entropy gap ~0.3-0.4)
- Lion optimizer's sign() operation implements Bayes optimal classifier for binary update direction problems
- The tanh modification theoretically improves Lion by optimal information compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropy gap (log n − H(H)) complements sharpness by capturing the uniformity of escape directions from saddle points.
- Mechanism: Displacement from a saddle point along eigen-direction i follows Δθ²ᵢ ∼ O(|λᵢ|). Sharpness (trace of Hessian) quantifies total escape speed, while entropy gap measures how uniformly escape occurs across directions. A smaller entropy gap indicates more uniform escape across eigen-directions.
- Core assumption: The relationship between eigenvalue distribution and escape dynamics generalizes beyond the specific architectures tested (ResNet18, ViT on CIFAR10).
- Evidence anchors:
  - [abstract] "traditionally used sharpness metric does not fully explain the intricate interplay and introduces information-theoretic metrics called entropy gap"
  - [section 2.1] "The entropy gap log n − H₁(H) measures the 'uniformness' of escaping. A small gap will ensure the network escapes from the saddle point well in all directions."
  - [corpus] Neighbor paper "Adam Reduces a Unique Form of Sharpness" supports that standard sharpness metrics are incomplete for optimizer characterization.
- Break condition: If entropy gap computation becomes numerically unstable for very large Hessians (n → millions of parameters), the metric may require approximation methods not discussed in this paper.

### Mechanism 2
- Claim: A larger entropy gap implies a larger Hessian condition number, which correlates with slower convergence.
- Mechanism: Theorem 2.2 establishes that log n − H₁(H) ≤ log k, where k is the condition number. This bound tightens as the eigenvalue distribution becomes more extreme. Since condition number directly affects gradient descent convergence rates, entropy gap serves as a proxy for expected convergence speed.
- Core assumption: The Hessian remains positive definite throughout training (or at least at the points being analyzed).
- Evidence anchors:
  - [section 2.1] Theorem 2.2 with full proof showing the bound between entropy gap and condition number
  - [section 2.1] "a smaller gap is favorable for faster convergence of training"
  - [corpus] Weak corpus evidence; neighbor papers do not directly address entropy-condition number relationships.
- Break condition: If the Hessian has significant negative eigenvalues (saddle points vs. minima), the positive definiteness assumption fails and the bound may not apply.

### Mechanism 3
- Claim: Entropy gap appears explicitly in PAC-Bayes generalization bounds, suggesting a theoretical link to generalization performance.
- Mechanism: Under Gaussian prior N(0, σ²I) and posterior N(θ*, (H + σ⁻²I)⁻¹), the KL divergence computation yields a generalization bound containing the term d(ln tr(H) + (ln d − H(H))) and a correction term involving (ln d − H(H))².
- Core assumption: The Laplace approximation (Gaussian posterior) is valid near the local minimum θ*.
- Evidence anchors:
  - [section 2.2] Theorem 2.4 provides the full PAC-Bayes bound with entropy gap term
  - [table 4] Shows entropy gap values for ResNet18 (SGD: 0.57, Adam: 1.48) and ViT (SGD: 0.34, Adam: 0.32) at 100% training steps, correlating with the observation that SGD works better for ResNet while Adam works better for ViT
  - [corpus] Neighbor paper "Beyond Sharpness: A Flatness Decomposition Framework" supports multi-metric generalization analysis.
- Break condition: If the loss landscape is highly non-Gaussian near minima (e.g., sharp valleys, asymmetric curvature), the Laplace approximation underlying the bound degrades.

### Mechanism 4
- Claim: The Lion optimizer's sign() operation implements the Bayes optimal classifier for a binary update direction problem, and can be improved using tanh() as an information bottleneck optimal representation.
- Mechanism: Lion's update direction U_t is treated as encoding a "true" direction Y ∈ {+1, −1}. Under Gaussian observation model X|Y ~ N(±μ, σ²), the Bayes optimal classifier is sign(X), matching Lion's sign(U_t). The information bottleneck analysis shows tanh(μx/σ²) is the optimal compression of X preserving information about Y.
- Core assumption: The gradient observations follow approximately symmetric Gaussian distributions around a true direction.
- Evidence anchors:
  - [section 3] Full derivation of Bayes optimal classifier (Theorem 3.2, 3.3)
  - [section 3.1] Information bottleneck derivation showing tanh as optimal representation
  - [corpus] Weak corpus evidence; neighbor papers do not analyze Lion through information-theoretic lenses.
- Break condition: If gradient distributions are multimodal or heavily skewed, the Gaussian model fails and the sign() operation may no longer be near-optimal.

## Foundational Learning

- Concept: **Rényi entropy and von Neumann entropy for matrices**
  - Why needed here: The paper defines α-order matrix entropy H_α(R) = 1/(1−α) log[tr((R/n)^α)], with α=1 recovering von Neumann entropy. Understanding this generalization is essential for computing entropy gap.
  - Quick check question: Given a 3×3 positive semi-definite matrix with eigenvalues [1.0, 0.5, 0.1], can you compute its von Neumann entropy H₁(H)?

- Concept: **Hessian eigenvalue spectrum analysis**
  - Why needed here: The paper uses Hessian trace (sharpness), positive eigenvalue ratio, and eigenvalue entropy. Understanding how these metrics relate to curvature and optimization dynamics is foundational.
  - Quick check question: What does a high positive eigenvalue ratio (e.g., 0.98) versus a low ratio (e.g., 0.54) suggest about the optimizer's position in the loss landscape?

- Concept: **PAC-Bayes generalization bounds**
  - Why needed here: The paper derives a PAC-Bayes bound connecting entropy gap to generalization. Understanding KL divergence between Gaussians is required to follow the proof.
  - Quick check question: In PAC-Bayes, why does a smaller KL(Q||P) between posterior Q and prior P generally indicate better generalization?

## Architecture Onboarding

- Component map:
  - Entropy gap computation -> Hessian eigenvalue analysis -> Block-level statistics aggregation -> Optimizer comparison
  - Lion analyzer -> Binary classification model -> Information bottleneck analysis -> tanh modification

- Critical path:
  1. Choose checkpoint (training step percentage) for analysis.
  2. Compute per-block Hessian statistics (trace, positive ratio, entropy).
  3. Compute entropy gap: log n − H₁(H) where n is Hessian dimension.
  4. Compare across optimizers (SGD vs. Adam vs. Lion) and architectures (ResNet vs. ViT).
  5. For Lion analysis: fit Gaussian model to gradient observations, extract μ/σ ratio, evaluate sign vs. tanh alternatives.

- Design tradeoffs:
  - Exact Hessian computation vs. approximation: Exact diagonalization is O(n³); Hutchinson's estimator trades bias for O(n) per sample but requires many samples for entropy.
  - Block granularity: Finer blocks (layer-wise) provide more detail but increase variance; coarser blocks (stage-wise) smooth metrics but may miss localized phenomena.
  - α parameter in Rényi entropy: α > 1 emphasizes large eigenvalues; α < 1 emphasizes small eigenvalues; α = 1 (von Neumann) is the standard choice but may not be optimal for all architectures.

- Failure signatures:
  - **Negative entropy gap**: Check that Hessian normalization (diagonal = 1) is applied before entropy computation.
  - **Inconsistent block entropy trends**: May indicate numerical instability in Hessian estimation; increase samples in Hutchinson estimator.
  - **Lion tanh improvement shows no gain**: Verify μ/σ ratio is estimated correctly; if gradients are nearly symmetric (μ ≈ 0), sign and tanh converge.

- First 3 experiments:
  1. **Reproduce Table 4 entropy gap dynamics**: Train ResNet18 and ViT on CIFAR10 with SGD and Adam; compute entropy gap at 0%, 25%, 50%, 75%, 100% steps. Verify that ResNet18+SGD maintains low gap (~0.5-0.6) while ResNet18+Adam shows increasing gap (~0.5→1.5).
  2. **Ablate entropy gap vs. sharpness correlation with generalization**: Train multiple models with varying weight decay; plot test accuracy against both sharpness and entropy gap. Check if entropy gap provides additional predictive signal beyond sharpness alone.
  3. **Validate Lion-tanh modification**: Replace sign(U_t) with tanh(μ·U_t/σ²) where μ/σ² is estimated from recent gradient statistics. Compare convergence speed and final accuracy against vanilla Lion on ViT-CIFAR10.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the tanh-based modification to the Lion optimizer improve empirical performance across diverse tasks and architectures?
- Basis in paper: [inferred] The paper theoretically derives a tanh improvement for Lion (Section 3.1) from Information Bottleneck principles and maximum entropy arguments, but provides no experimental validation of this proposed modification.
- Why unresolved: The derivation shows tanh is optimal under specific Gaussian assumptions about the update direction distribution, but these assumptions may not hold in practice during actual training.
- What evidence would resolve it: Empirical comparison of standard Lion vs. tanh-modified Lion on standard benchmarks (ImageNet, CIFAR, language modeling) measuring both final accuracy and training dynamics.

### Open Question 2
- Question: Why does the entropy gap exhibit opposite trends between ResNet and ViT architectures under Adam optimization?
- Basis in paper: [explicit] Table 4 shows ResNet18 (Adam) entropy gap increases from 0.55 to 1.48 during training, while ViT (Adam) decreases from 0.43 to 0.32. The paper states "sharpness (trace of hessian) cannot fully explain this behavior" but does not fully explain the mechanism.
- Why unresolved: The theoretical connection between entropy gap and condition number (Theorem 2.2) suggests architectural properties affect how Adam shapes the loss landscape differently, but the specific architectural features causing this divergence remain unidentified.
- What evidence would resolve it: Ablation studies varying architectural components (skip connections, attention mechanisms, normalization layers) while tracking entropy gap dynamics to isolate which structural properties drive the observed differences.

### Open Question 3
- Question: Can information-theoretic metrics guide the design of optimizers that automatically adapt their compression-accuracy tradeoff during training?
- Basis in paper: [explicit] The conclusion states "This work opens the door for further research on integrating information theory into the development and analysis of optimizers" and suggests "information-theoretic perspectives can significantly contribute to a better understanding of optimization dynamics."
- Why unresolved: The paper demonstrates analysis tools but does not propose a systematic design methodology for creating new optimizers based on information-theoretic principles.
- What evidence would resolve it: A framework that uses entropy gap and mutual information as objectives or constraints in optimizer design, validated by showing such optimizers outperform existing ones on standard benchmarks.

## Limitations
- Gaussian gradient assumption may not hold for complex loss landscapes
- Hessian computation becomes intractable for large-scale models requiring approximations
- Block-level analysis lacks clear architectural boundary definitions
- Theoretical bounds rely on positive definiteness assumptions that may fail at saddle points

## Confidence
- High: Empirical correlation between entropy gap and optimizer performance across tested architectures
- Medium: Theoretical bounds connecting entropy gap to condition number and generalization (dependent on Gaussian approximations)
- Low: Lion optimizer analysis (tanh improvement requires careful hyperparameter tuning not validated experimentally)

## Next Checks
1. **Cross-architecture validation**: Test entropy gap predictions on architectures beyond ResNet and ViT (e.g., Transformer-based vision models, convolutional networks with different depths) to verify the framework's generality.
2. **Lion-tanh ablation**: Systematically vary the μ/σ ratio estimation window and learning rate schedule to determine whether the tanh improvement is robust or highly sensitive to hyperparameters.
3. **Large-scale Hessian validation**: Compare exact Hessian eigenvalues against Hutchinson estimator results for entropy gap computation, establishing error bounds and sample requirements for reliable metric estimation.