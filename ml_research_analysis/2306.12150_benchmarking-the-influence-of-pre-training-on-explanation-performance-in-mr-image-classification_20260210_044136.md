---
ver: rpa2
title: Benchmarking the Influence of Pre-training on Explanation Performance in MR
  Image Classification
arxiv_id: '2306.12150'
source_url: https://arxiv.org/abs/2306.12150
tags:
- methods
- data
- explanation
- performance
- lesions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark for evaluating the performance
  of explainable AI (XAI) methods in the context of MR image classification. The key
  innovation is the creation of a synthetic dataset where artificial lesions are overlaid
  on real MR images, with the ground truth for explanations being the exact location
  of these lesions.
---

# Benchmarking the Influence of Pre-training on Explanation Performance in MR Image Classification

## Quick Facts
- arXiv ID: 2306.12150
- Source URL: https://arxiv.org/abs/2306.12150
- Authors: Marta Oliveira; Rick Wilming; Benedict Clark; CÃ©line Budding; Fabian Eitel; Kerstin Ritter; Stefan Haufe
- Reference count: 18
- Primary result: MRI-pretrained models produce better and more stable explanations than ImageNet-pretrained models for MR image classification tasks

## Executive Summary
This paper introduces a novel benchmark for evaluating explainable AI (XAI) methods in medical imaging by creating a synthetic dataset where artificial lesions are overlaid on real MR images, providing exact ground truth for explanation quality assessment. The study systematically investigates how transfer learning from different pre-training domains (natural images vs. MRI data) affects the performance of explanations for classification models. Results demonstrate that models pre-trained on MRI data generate more reliable and stable explanations compared to those pre-trained on natural images, particularly across varying levels of classification accuracy. The research also reveals significant variability in explanation quality across different XAI methods, even for models with high classification accuracy, highlighting potential risks in clinical applications.

## Method Summary
The authors developed a controlled synthetic dataset by overlaying artificial lesions on real MR images, creating a ground truth where explanation quality can be quantitatively measured against the known lesion locations. They evaluated multiple XAI methods across models pre-trained either on ImageNet (natural images) or domain-specific MRI data, then fine-tuned on the synthetic lesion classification task. The study systematically varied the extent of fine-tuning to examine how this affects both classification accuracy and explanation performance, establishing a correlation between these two metrics while revealing that MRI-pretrained models consistently produce superior explanations.

## Key Results
- MRI-pretrained models produce better and more stable explanations compared to ImageNet-pretrained models across various classification accuracy levels
- Strong correlation exists between classification accuracy and explanation performance, though models with greater fine-tuning generally achieve better explanations
- High variability in explanation performance across different XAI methods persists even for models with high classification accuracy, indicating significant risk of misinterpretation

## Why This Works (Mechanism)
The synthetic lesion dataset provides a controlled environment with known ground truth, enabling quantitative evaluation of explanation methods that is not possible with real medical data where ground truth explanations are unknown. MRI pre-training likely provides domain-specific features and representations that better capture the characteristics of medical images, leading to more accurate identification of relevant features during classification and consequently better explanations. The correlation between accuracy and explanation performance suggests that models learning to identify relevant features for classification also learn to produce more faithful explanations.

## Foundational Learning
- **Synthetic lesion generation**: Creating artificial pathologies overlaid on real images with known locations provides ground truth for explanation evaluation; quick check: verify lesion properties match intended distribution
- **Transfer learning from MRI vs natural images**: Domain-specific pre-training captures medical image characteristics that general image pre-training may miss; quick check: compare feature representations between pre-training domains
- **Explanation method variability**: Different XAI techniques have varying sensitivity to model architecture and data characteristics; quick check: assess consistency across multiple runs and models
- **Fine-tuning dynamics**: The extent of adaptation from pre-trained weights affects both classification and explanation performance; quick check: monitor performance across fine-tuning epochs
- **Correlation analysis**: Statistical relationships between multiple performance metrics reveal underlying patterns; quick check: validate correlation stability across different model families

## Architecture Onboarding

### Component Map
Synthetic Dataset Generator -> Pre-training Phase (ImageNet/MRI) -> Fine-tuning Phase -> Classification Model -> XAI Methods -> Explanation Quality Metrics

### Critical Path
The critical path flows from pre-training choice through fine-tuning to final explanation generation, as the pre-training domain fundamentally shapes the feature representations that XAI methods analyze.

### Design Tradeoffs
- Synthetic vs real data: Controlled ground truth vs ecological validity
- Domain-specific vs general pre-training: Better medical features vs broader generalizability
- Fine-tuning extent: Optimal explanation vs classification performance balance
- Explanation method selection: Quantitative rigor vs practical interpretability

### Failure Signatures
- Poor explanations despite high accuracy: Model may be relying on spurious correlations
- High variability across XAI methods: Inconsistent feature attribution suggests instability
- MRI-pretrained models underperforming: Pre-training may not capture relevant medical features
- Weak correlation between accuracy and explanation quality: Potential misalignment in what models learn

### First Experiments
1. Compare explanation stability across multiple runs of the same model configuration
2. Test explanation performance on a subset of synthetic data with varying lesion complexity
3. Evaluate whether explanation quality degrades predictably as classification accuracy decreases

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic nature of lesions may not capture the complexity of real pathological features in authentic medical images
- Domain-specific advantages could be influenced by factors beyond pre-training task, such as preprocessing differences or architectural choices
- Correlation between classification accuracy and explanation performance does not establish causation and may involve confounding factors
- High variability across XAI methods may be partially attributable to the artificial nature of the dataset

## Confidence

**High Confidence**: The benchmark framework (synthetic lesion dataset with known ground truth) is methodologically sound and provides valid quantitative approach to evaluating explanation methods.

**Medium Confidence**: Results showing MRI-pretrained models outperforming ImageNet-pretrained models require careful interpretation and may not generalize perfectly to real-world clinical scenarios.

**Medium Confidence**: The correlation between classification accuracy and explanation performance is statistically supported but requires deeper investigation of underlying mechanisms and potential confounding factors.

## Next Checks

1. **Cross-dataset validation**: Apply the benchmark to authentic MR image datasets with clinically validated ground truth annotations to assess whether pre-training findings hold for real pathological features.

2. **Controlled ablation studies**: Systematically vary pre-training conditions (image resolution, contrast settings, anatomical regions) while keeping classification architecture constant to isolate which aspects of MRI pre-training contribute to improved explanations.

3. **Multi-feature ground truth**: Extend the synthetic dataset to include multiple relevant features beyond lesion location to better approximate real medical image interpretation complexity and test whether current explanation methods can capture multi-modal relevance.