---
ver: rpa2
title: 'TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER'
arxiv_id: '2503.22714'
source_url: https://arxiv.org/abs/2503.22714
tags:
- tridis
- test
- script
- lines
- medieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRIDIS, a comprehensive corpus of medieval
  and early modern manuscripts designed to advance Handwritten Text Recognition (HTR)
  and Named Entity Recognition (NER). TRIDIS aggregates multiple open-source sub-corpora,
  ensuring rich metadata, diverse languages, and challenging document types.
---

# TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER

## Quick Facts
- arXiv ID: 2503.22714
- Source URL: https://arxiv.org/abs/2503.22714
- Reference count: 22
- This paper introduces TRIDIS, a comprehensive corpus of medieval and early modern manuscripts designed to advance Handwritten Text Recognition (HTR) and Named Entity Recognition (NER).

## Executive Summary
This paper introduces TRIDIS, a comprehensive corpus of medieval and early modern manuscripts designed to advance Handwritten Text Recognition (HTR) and Named Entity Recognition (NER). TRIDIS aggregates multiple open-source sub-corpora, ensuring rich metadata, diverse languages, and challenging document types. A key contribution is the introduction of an outlier-driven test split strategy, which identifies and isolates challenging examples based on their deviation from the corpus's central tendency in a joint embedding space. This approach provides a more realistic evaluation of HTR model robustness compared to traditional random splits. Preliminary baseline experiments using TrOCR and MiniCPM-Llama3-V 2.5 demonstrate that the outlier-based test set significantly increases difficulty, with higher Character Error Rates (CER) and Word Error Rates (WER) observed compared to random splits. TRIDIS is publicly available and aims to stimulate joint robust HTR and NER research across medieval and early modern textual heritage.

## Method Summary
The TRIDIS corpus is constructed by aggregating multiple open-source sub-corpora of medieval and early modern manuscripts, with unified metadata and semi-diplomatic transcription rules. The outlier-driven test split strategy uses joint embeddings (concatenated mean-pooled vision + text encoder outputs) to identify challenging lines based on their Euclidean distance from the corpus centroid. Baseline experiments use TrOCR Large (558M parameters) and MiniCPM-Llama3-V 2.5 (8B parameters) with standard HTR training procedures. The corpus is publicly available in Apache Parquet format with line-level records containing images, text, and metadata fields.

## Key Results
- Outlier-based test splits significantly increase difficulty: CER increases from ~9% to ~11-12% and WER from ~25% to ~32-34% compared to random splits
- Joint embeddings effectively stratify challenging examples, with outliers typically >9 units from centroid in UMAP visualization
- Semi-diplomatic transcription rules create systematic gaps between visual input and textual output that models must learn to bridge
- TRIDIS aggregates 177,660 training lines from 8+ sub-corpora with unified schema and rich metadata

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Outlier-based test splits expose domain generalization failures that random splits conceal.
- **Mechanism:** Joint embeddings (concatenated mean-pooled vision + text encoder outputs) are computed for each line. A centroid (element-wise median) represents the corpus's central tendency. Lines with the largest Euclidean distances (top 5%) form the test set. This stratifies by "atypicality" rather than random sampling.
- **Core assumption:** Distance from centroid correlates with challenge density (rare scripts, unusual layouts, degraded images). The embedding space meaningfully captures both visual and linguistic difficulty factors.
- **Evidence anchors:**
  - [abstract] "outlier-driven test split strategy, which identifies and isolates challenging examples based on their deviation from the corpus's central tendency in a joint embedding space"
  - [section] Algorithm 1: explicit procedure for embedding computation, centroid calculation, distance-based stratification
  - [corpus] Figure 2 shows UMAP visualization with outliers typically >9 units from centroid; Table II shows CER increases from 9.1% → 11.3% (TrOCR) and 10.2% → 12.6% (MiniCPM) when switching from random to outlier test sets

### Mechanism 2
- **Claim:** Semi-diplomatic transcription rules create a systematic gap between visual input and textual output that models must learn to bridge.
- **Mechanism:** Transcribers expand abbreviations silently (e.g., "dms" → "dominus"), modernize punctuation, normalize allographs, and resolve notarial marks. This means the HTR model must predict characters that have no direct visual correspondent, relying on learned priors about Latin/French/German morphology and scribal conventions.
- **Core assumption:** The benefits of modern-NLP compatibility outweigh the interpretative noise introduced by editorial decisions.
- **Evidence anchors:**
  - [abstract] "semi-diplomatic transcription rules (expansion, normalization, punctuation)"
  - [section] Section II.A lists 10 specific rules including abbreviation expansion, allograph normalization, named entity capitalization
  - [corpus] Explicit: ~200k lines transcribed under these rules across 10+ sub-corpora with varying editorial teams

### Mechanism 3
- **Claim:** Aggregating heterogeneous sub-corpora with rich metadata enables cross-domain transfer that single-source corpora cannot support.
- **Mechanism:** Each line carries metadata (language, century, script_family, manuscript_id, NER_annotation). Models can leverage this for conditional learning or analysis. The diversity in scripts (Textualis, Cursiva, Hybrida, Praegotica) and languages (Latin 57.5%, French 29%, German/Dutch 9%, Spanish 4.5%) provides coverage of Western European documentary handwriting evolution.
- **Core assumption:** Diversity in training data improves generalization more than it introduces noise from inconsistent annotation standards.
- **Evidence anchors:**
  - [abstract] "aggregates multiple open-source sub-corpora, ensuring rich metadata, diverse languages, and challenging document types"
  - [section] Section III.C lists all metadata fields per record; Table I shows language/script/hand distribution
  - [corpus] Explicit: 177,660 training lines from 8+ sub-corpora with unified Parquet schema; Figure 1 shows distribution percentages

## Foundational Learning

- **Concept: Joint Vision-Language Embeddings**
  - **Why needed here:** The outlier detection strategy requires both visual features (layout, handwriting style) and linguistic features (vocabulary, syntax) to identify challenging lines. Understanding how these modalities are combined and what each contributes is essential.
  - **Quick check question:** Can you explain why mean-pooling encoder outputs might lose important fine-grained information about line-level challenges?

- **Concept: Out-of-Domain (OOD) Evaluation vs. In-Domain**
  - **Why needed here:** The paper's central contribution is demonstrating that random splits yield inflated metrics (CER ~9-10%) while outlier splits reveal true generalization gaps (CER ~11-13%). Understanding OOD evaluation paradigms is critical for interpreting these results.
  - **Quick check question:** Why might a model achieve 9% CER on random split but 12.6% on outlier split? What specific document features might drive this gap?

- **Concept: Semi-Diplomatic Transcription Paradigm**
  - **Why needed here:** All TRIDIS annotations follow this editorial approach. Without understanding that abbreviations are expanded, punctuation modernized, and allographs normalized, you cannot interpret error patterns or design appropriate evaluation metrics.
  - **Quick check question:** If a manuscript line visually reads "dñs nstr" but the ground truth is "dominus noster," where do errors typically originate—the visual recognition or the editorial expansion?

## Architecture Onboarding

- **Component map:**
  - Data Layer (Parquet file with line-level records) -> Embedding Layer (Vision encoder + Text encoder → mean-pooling → concatenation) -> Split Logic (Centroid computation → Euclidean distance ranking → stratified sampling) -> Model Layer (TrOCR or MiniCPM training)

- **Critical path:**
  1. Load Parquet → 2. Encode lines (joint embeddings) → 3. Compute centroid → 4. Rank by distance → 5. Stratified outlier selection → 6. Train models on non-outlier data → 7. Evaluate on outlier test set

- **Design tradeoffs:**
  - Random vs. outlier split: Random = optimistic metrics, easy comparison to prior work; Outlier = realistic OOD assessment, harder to achieve competitive numbers
  - TrOCR vs. MiniCPM: TrOCR (558M) is faster, purpose-built for OCR/HTR; MiniCPM (8B) offers multilingual pretraining but higher compute cost
  - Unified transcription vs. sub-corpus specific: Unified schema enables joint training but may obscure sub-corpus-specific challenges

- **Failure signatures:**
  - Large CER gap (>2 points) between random and outlier splits indicates domain overfitting
  - High error rates on specific outlier categories (names, long lines, underrepresented scripts) suggest targeted data augmentation needed
  - BERTScore drop (0.94 → 0.91) indicates semantic degradation despite character-level differences

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train TrOCR on 90% TRIDIS with random split vs. outlier split. Verify CER gap matches reported ~2+ points.
  2. **Embedding ablation:** Test outlier detection using vision-only vs. text-only embeddings vs. joint. Quantify which modality contributes more to identifying challenging lines.
  3. **Sub-corpus cross-validation:** Train on 7 sub-corpora, test on held-out 8th. Compare to outlier-split performance to assess whether outliers capture similar domain shift as entirely unseen collections.

## Open Questions the Paper Calls Out
- How does the inclusion of under-represented scripts (e.g., Carolingian minuscule) and cross-language collections alter the performance gap between random splits and the proposed outlier-based test splits?
- Does the outlier-detection strategy based on Euclidean distance in a joint embedding space correlate more strongly with transcription difficulty than specific paleographic features (e.g., ligatures, abbreviations)?
- Can the corpus effectively support joint HTR and NER training across all sub-corpora given the scarcity of aligned NER annotations?

## Limitations
- The choice of 5% as the outlier threshold appears arbitrary, with no sensitivity analysis shown for different percentages
- The vision and text encoders used for joint embedding computation are not specified, making it impossible to assess their appropriateness for medieval manuscripts
- The semi-diplomatic transcription approach introduces interpretative gaps that may confound error analysis, particularly for abbreviation expansion

## Confidence
- **High confidence:** The outlier detection methodology (joint embeddings, centroid calculation, distance-based stratification) is clearly specified and reproducible. The reported CER/WER increases on outlier vs. random splits are consistent with the mechanism described.
- **Medium confidence:** The corpus metadata quality and diversity claims are well-supported by the detailed schema and distribution tables, but the actual annotation reliability across 10+ sub-corpora with different editorial teams is not independently verified.
- **Low confidence:** The generalization claims about TRIDIS's utility for joint HTR/NER research are plausible but not demonstrated—the paper only reports HTR baselines without any NER experiments or joint model training.

## Next Checks
1. **Encoder Ablation Study:** Reproduce the outlier detection using different vision encoder architectures (ResNet, ViT, Swin) and text encoders (BERT, RoBERTa, Llama) to quantify which modalities contribute most to identifying challenging lines.
2. **Threshold Sensitivity Analysis:** Systematically vary the outlier percentage (1%, 3%, 5%, 10%, 15%) and measure the corresponding CER/WER gaps. Plot these relationships to identify whether 5% represents a meaningful inflection point.
3. **Cross-Sub-Corpus Validation:** Hold out one complete sub-corpus (e.g., treHTR or pampHTR) as a test set and compare performance to the outlier-split results to determine whether the outlier strategy captures similar domain shift as entirely unseen collections.