---
ver: rpa2
title: Divergent Emotional Patterns in Disinformation on Social Media? An Analysis
  of Tweets and TikToks about the DANA in Valencia
arxiv_id: '2501.18640'
source_url: https://arxiv.org/abs/2501.18640
tags:
- disinformation
- dana
- content
- audio
- emotional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study analyzed disinformation spread on X and TikTok during\
  \ the Valencia DANA floods (Oct 2024), creating a 650-post dataset manually labeled\
  \ and validated with GPT-4o (kappa = 0.684\u20130.695). Emotion analysis showed\
  \ X disinformation linked to sadness/fear, while TikTok disinformation aligned with\
  \ anger/disgust."
---

# Divergent Emotional Patterns in Disinformation on Social Media? An Analysis of Tweets and TikToks about the DANA in Valencia

## Quick Facts
- arXiv ID: 2501.18640
- Source URL: https://arxiv.org/abs/2501.18640
- Reference count: 11
- This study analyzed disinformation spread on X and TikTok during the Valencia DANA floods (Oct 2024), creating a 650-post dataset manually labeled and validated with GPT-4o (kappa = 0.684–0.695). Emotion analysis showed X disinformation linked to sadness/fear, while TikTok disinformation aligned with anger/disgust. Linguistic analysis via LIWC revealed disinformation uses more negations, perceptual language, and personal anecdotes, whereas trustworthy content is more articulate. Audio feature analysis of TikToks found trustworthy posts use brighter tones, monotone narration, and clearer speech, while disinformation employs tonal variation and manipulative music. Detection models showed SVM+TF-IDF achieved the best F1-Score (0.7949), while incorporating audio features into roberta-large-bne improved accuracy (0.7646) and F1-Score (0.7664). GPT-4o Few-Shot also performed strongly (accuracy = 0.7834), highlighting the potential of LLMs for automated annotation. Multimodal approaches combining text and audio features proved effective for disinformation detection.

## Executive Summary
This study examines how disinformation spreads differently across X and TikTok during the Valencia DANA floods, analyzing 650 posts using both textual and audio features. The research reveals distinct emotional strategies: X disinformation exploits sadness and fear through casualty exaggeration, while TikTok disinformation leverages anger and disgust through government criticism and sensational claims. By combining LIWC linguistic analysis with audio feature extraction and applying both traditional ML (SVM+TF-IDF) and transformer-based models (roberta-large-bne), the study demonstrates that multimodal approaches significantly improve disinformation detection accuracy. The work also validates few-shot LLM annotation as a viable method for large-scale content labeling.

## Method Summary
The study collected 650 posts (382 Tweets, 268 TikToks) about the Valencia DANA floods using Apify crawlers, with content transcribed via Whisper-medium and visual text extracted using PaddleOCR. Posts were manually annotated and validated using GPT-4o with four intermediate categories collapsed to binary labels. LIWC linguistic features were extracted alongside audio features including MFCCs, spectral properties, and tonal measures. Five-fold stratified cross-validation was used to evaluate SVM+TF-IDF, roberta-large-bne, and multimodal fusion models combining text embeddings with normalized audio features. GPT-4o few-shot annotation was also evaluated for automated labeling capability.

## Key Results
- SVM+TF-IDF achieved the highest F1-Score of 0.7949, demonstrating strong performance with limited data
- roberta-large-bne with audio feature fusion improved accuracy to 0.7646 and F1-Score to 0.7664, surpassing text-only counterparts
- GPT-4o few-shot annotation achieved accuracy of 0.7834 with substantial inter-rater agreement (kappa = 0.684–0.695)
- X disinformation predominantly shows sadness and fear, while TikTok disinformation aligns with anger and disgust (p < 0.001)
- Disinformation content uses more negations, perceptual language, and personal anecdotes compared to trustworthy content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal feature integration (text + audio) improves disinformation detection on video platforms.
- Mechanism: Audio features capture manipulative production patterns (tonal variation, emotional music, lower spectral flatness) that text-only models miss. Fusing normalized audio features with text embeddings from the [CLS] token provides complementary signals.
- Core assumption: Audio production choices systematically differ between trustworthy and manipulative content.
- Evidence anchors:
  - [abstract] "Incorporating audio features into roberta-large-bne improved both Accuracy and F1-Score (0.7646 and 0.7664), surpassing its text-only counterpart"
  - [Section 6] Trustworthy audio shows higher spectral flatness, onset strength, brighter tones; disinformation uses lower MFCC2 (deeper tones) and higher Tonnetz4 (suspenseful music)
  - [corpus] "A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation on TikTok" validates multimodal detection viability
- Break condition: If audio features become homogeneous across content types, or if creators adapt production to mimic trustworthy audio signatures.

### Mechanism 2
- Claim: Platform-specific emotional profiles characterize disinformation—sadness/fear on X, anger/disgust on TikTok.
- Mechanism: Platform affordances and audience expectations shape emotional framing. X disinformation exploits fear/sadness through casualty exaggeration; TikTok disinformation leverages anger/disgust through government criticism and sensational claims.
- Core assumption: Emotion classification models transfer accurately to crisis-event social media content in Spanish.
- Evidence anchors:
  - [abstract] "Emotion analysis revealed that disinformation on X is mainly associated with increased sadness and fear, while on TikTok, it correlates with higher levels of anger and disgust"
  - [Section 4.1/4.2] Mann-Whitney U tests show p = 0.000061 (sadness on X), p = 0.000003 (anger/disgust on TikTok)
  - [corpus] "Misinformation exploits outrage to spread online" (McLoughlin et al. via Science) supports anger mechanism on X; platform divergence is novel to this paper
- Break condition: If emotional patterns are event-specific rather than platform-specific, or if platform algorithm changes shift engagement patterns.

### Mechanism 3
- Claim: Few-shot LLM annotation can achieve substantial agreement with manual labels when prompts use fine-grained intermediate categories collapsed to binary.
- Mechanism: Decomposing annotation into 4 categories (not-related, trustworthy, criticized-disinfo, disinformation) then collapsing reduces ambiguity and improves GPT-4o's classification boundary precision.
- Core assumption: LLM embedding space captures linguistic distinctions relevant to disinformation detection.
- Evidence anchors:
  - [abstract] "Few-Shot annotation approach with GPT-4o achieved substantial agreement (Cohen's kappa of 0.684–0.695)"
  - [Section 3.4] "refining the prompt to include four detailed categories and then collapsing them into two yielded higher agreement values"
  - [corpus] PCoT paper explores persuasion-augmented chain-of-thought for detection but doesn't address annotation specifically
- Break condition: If domain shifts (different languages, event types) reduce LLM alignment, or if adversarial content exploits LLM blind spots.

## Foundational Learning

- **LIWC (Linguistic Inquiry and Word Count)**
  - Why needed here: Quantifies linguistic dimensions (negations, perceptual words, prepositions) that distinguish disinformation from trustworthy content statistically.
  - Quick check question: Can you explain why higher preposition usage correlates with more articulate/factual discourse?

- **Spectral Audio Features (MFCCs, Spectral Flatness, Tonnetz)**
  - Why needed here: Encode acoustic properties that differentiate monotone/news-style narration from emotionally manipulative audio production.
  - Quick check question: What does higher spectral flatness indicate about audio content (hint: noisier vs. tonal)?

- **Cohen's Kappa for Annotation Agreement**
  - Why needed here: Measures inter-rater reliability beyond raw accuracy; values 0.61-0.80 indicate "substantial agreement" per Landis-Koch scale.
  - Quick check question: Why is kappa preferred over simple percent agreement for class-imbalanced annotation tasks?

## Architecture Onboarding

- **Component map**: Apify crawlers → Whisper-medium (transcription) → PaddleOCR (overlay text) → LIWC + emotion classifier + audio feature extraction → SVM+TF-IDF/RoBERTa models → evaluation

- **Critical path**: Audio feature extraction → normalization → concatenation with [CLS] token embedding → classifier head. Text preprocessing removes hashtags/mentions to prevent shortcut learning.

- **Design tradeoffs**: SVM+TF-IDF excels with limited data (n=650) but lacks scalability; RoBERTa underperforms text-only but gains +1.34pp accuracy with audio; GPT-4o Few-Shot achieves best accuracy but no cross-validation (single evaluation).

- **Failure signatures**:
  - Low recall with high precision: Model relies on narrow lexical features (hashtag/mention leakage)
  - Audio features hurt performance: Normalization mismatch or feature dimension explosion
  - GPT-4o annotation disagreement: Prompt lacks domain-specific examples or category boundaries are ambiguous

- **First 3 experiments**:
  1. Replicate SVM+TF-IDF vs. RoBERTa-large-bne comparison to validate baseline on held-out fold.
  2. Ablate audio feature groups (spectral vs. MFCC vs. Tonnetz) to identify most predictive subset.
  3. Test GPT-4o Few-Shot with cross-fold evaluation to establish variance bounds on accuracy claims.

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does incorporating visual features (video frames and images) alongside text and audio improve disinformation detection accuracy compared to the text-audio fusion model?
  - **Basis in paper**: [explicit] The "Future Work" section explicitly states the intention to "Include images and videos to capture a broader range of disinformation strategies" beyond the current audio and text analysis.
  - **Why unresolved**: The current study limits its multimodal approach to text embeddings and audio features (tonal variation, MFCCs), acknowledging that visual data extraction was reserved for future research.
  - **What evidence would resolve it**: An ablation study comparing the performance (F1-Score/Accuracy) of the current roberta-large-bne + audio model against a model that also integrates visual embeddings (e.g., ResNet or ViT features).

- **Open Question 2**: Do platform affordances (e.g., short-form video vs. micro-text) or audience demographics primarily drive the observed divergence in emotional disinformation strategies (anger/disgust on TikTok vs. sadness/fear on X)?
  - **Basis in paper**: [explicit] The authors identify "Emotional Divergences" as a key area for future research to "Examine how distinct emotional triggers influence disinformation dissemination on different social media platforms."
  - **Why unresolved**: While the paper establishes that different emotions dominate on different platforms, it does not determine the causal mechanism—whether the format dictates the emotional strategy or if distinct user bases are targeted differently.
  - **What evidence would resolve it**: A comparative analysis controlling for content topics across platforms, or user studies assessing emotional reactions to the same disinformation narratives presented in different formats (text vs. video).

- **Open Question 3**: Do the linguistic markers (negations, perceptual language) and audio signatures (tonal variation, monotone narration) identified in Spanish DANA content generalize to disinformation in other languages and crisis contexts?
  - **Basis in paper**: [explicit] The "Future Work" section calls for "Cross-Linguistic Studies" to "Analyze disinformation patterns across different languages and regions."
  - **Why unresolved**: The models and analysis were trained and validated exclusively on Spanish-language content related to a specific weather event, leaving the universality of the identified "manipulative" audio and linguistic features untested.
  - **What evidence would resolve it**: Testing the proposed detection models and LIWC analysis on comparable multilingual datasets (e.g., floods in Germany or wildfires in Australia) to see if similar feature importance and emotional divergences hold.

## Limitations
- Data generalization: Results based on single crisis event in Spanish may not transfer to other events, languages, or platform cultures
- Audio feature causality: Study doesn't establish whether audio differences are causal production choices or artifacts of content type
- GPT-4o reliability: Few-shot annotation lacks cross-validation variance estimates, making performance claims uncertain

## Confidence
- **High confidence**: Multimodal fusion improves detection accuracy over text-only baselines (0.7646 vs 0.7534 accuracy). SVM+TF-IDF baseline performance (F1=0.7949) is well-established for limited datasets.
- **Medium confidence**: Platform-specific emotional profiles (sadness/fear on X, anger/disgust on TikTok) are statistically significant but may be event-specific.
- **Low confidence**: GPT-4o Few-Shot annotation reliability across different events and languages. The claim that audio production choices systematically differ between content types requires external validation.

## Next Checks
1. Cross-event validation: Test whether emotional patterns (sadness/fear on X, anger/disgust on TikTok) persist across different crisis events and languages
2. Audio feature ablation study: Systematically remove individual audio feature groups to identify which contribute most to detection performance
3. GPT-4o cross-validation: Implement k-fold evaluation of Few-Shot annotation approach to establish performance variance and confidence bounds