---
ver: rpa2
title: Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement
  Learning
arxiv_id: '2512.12046'
source_url: https://arxiv.org/abs/2512.12046
tags:
- steps
- learning
- value
- eik-qrl
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Eik-QRL, a continuous-time reformulation
  of quasimetric reinforcement learning based on the Eikonal partial differential
  equation. The key innovation is replacing trajectory-based local constraints with
  a PDE-based regularizer, making the method trajectory-free and improving generalization.
---

# Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.12046
- Source URL: https://arxiv.org/abs/2512.12046
- Reference count: 40
- This paper introduces Eik-QRL, a continuous-time reformulation of quasimetric reinforcement learning based on the Eikonal partial differential equation, replacing trajectory-based constraints with a PDE-based regularizer.

## Executive Summary
This paper introduces Eik-QRL, a continuous-time reformulation of quasimetric reinforcement learning based on the Eikonal partial differential equation. The key innovation is replacing trajectory-based local constraints with a PDE-based regularizer, making the method trajectory-free and improving generalization. Eik-QRL enforces the Eikonal condition (∥∇s d(s,g)∥ = 1) directly through automatic differentiation, requiring only sampled states and goals rather than rollouts. Theoretical analysis provides value recovery guarantees under regularity assumptions, though these may not hold in complex dynamics. To address limitations, the authors propose Eik-HiQRL, a hierarchical extension that learns a quasimetric in a low-dimensional abstract space while using a standard value function at the low level. Empirical results show Eik-HiQRL achieves state-of-the-art performance on navigation benchmarks and consistent gains over QRL in manipulation tasks, while matching temporal-difference methods. The trajectory-free property is particularly advantageous in large environments and stitched-data regimes.

## Method Summary
The paper presents Eik-QRL, which reformulates quasimetric reinforcement learning using the Eikonal partial differential equation. Instead of enforcing local constraints through trajectories, it directly imposes the Eikonal condition (∥∇s d(s,g)∥ = 1) using automatic differentiation. This trajectory-free approach requires only sampled states and goals, not rollouts. The method is theoretically grounded with value recovery guarantees under certain regularity assumptions. To overcome limitations in complex dynamics, the authors introduce Eik-HiQRL, a hierarchical extension that learns a quasimetric in a low-dimensional abstract space while maintaining a standard value function at the low level. This hierarchical approach combines the benefits of quasimetric learning with hierarchical reinforcement learning.

## Key Results
- Eik-HiQRL achieves state-of-the-art performance on navigation benchmarks and consistent gains over QRL in manipulation tasks
- The trajectory-free property of Eik-QRL is particularly advantageous in large environments and stitched-data regimes
- Eik-HiQRL matches the performance of temporal-difference methods while offering improved generalization

## Why This Works (Mechanism)
The core mechanism works by reformulating the quasimetric learning problem as a continuous-time optimization using the Eikonal PDE. Instead of enforcing constraints through sampled trajectories (which can be computationally expensive and sample-inefficient), Eik-QRL directly imposes the Eikonal condition on the quasimetric function through automatic differentiation. This makes the method trajectory-free, requiring only state-goal pairs rather than full rollouts. The hierarchical extension Eik-HiQRL addresses limitations in complex dynamics by learning a quasimetric in an abstract space while using standard value functions at the low level, combining the benefits of both approaches.

## Foundational Learning
- Quasimetric reinforcement learning: Why needed - provides goal-conditioned policies without requiring goal-specific training; Quick check - verify the quasimetric satisfies triangle inequality properties
- Eikonal partial differential equation: Why needed - provides a principled way to enforce geometric constraints on distance functions; Quick check - confirm ∇d(s,g) has unit norm where the metric is well-defined
- Hierarchical reinforcement learning: Why needed - enables decomposition of complex tasks into manageable sub-problems; Quick check - ensure abstract state transitions preserve task-relevant information

## Architecture Onboarding

### Component Map
Eik-QRL: State-Goal Pairs -> Quasimetric Network -> Eikonal Constraint Regularizer -> Loss Function
Eik-HiQRL: Abstract States -> Abstract Quasimetric -> Low-Level Value Function -> Hierarchical Policy

### Critical Path
1. Sample state-goal pairs from the environment
2. Pass through quasimetric network to compute distances
3. Compute Eikonal constraint violation using automatic differentiation
4. Backpropagate through the constraint to update network parameters
5. For hierarchical version: map states to abstract space, apply abstract quasimetric, use low-level value function for execution

### Design Tradeoffs
- Trajectory-free vs. trajectory-based: Eik-QRL sacrifices explicit trajectory modeling for computational efficiency and better generalization
- Abstract space dimension: Lower dimensions improve computational efficiency but may lose task-relevant information
- Regularity assumptions: Theoretical guarantees require smooth dynamics that may not hold in practice

### Failure Signatures
- Violation of Eikonal condition indicates poor quasimetric learning
- Inconsistent abstract state transitions suggest inadequate abstraction
- Poor transfer to unseen goals indicates over-fitting to training distribution

### Exactly 3 First Experiments
1. Verify Eik-QRL learns a valid quasimetric by testing triangle inequality on random state triples
2. Compare trajectory-free vs. trajectory-based constraint enforcement on a simple navigation task
3. Test hierarchical ablation by comparing Eik-HiQRL with and without the abstract quasimetric component

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the trajectory-free property of Eik-QRL in high-dimensional state spaces with complex dynamics, the quality of abstract space representation in Eik-HiQRL and its impact on long-horizon planning, the validity of theoretical recovery guarantees under practical regularity conditions, and the performance on diverse problem classes beyond navigation and manipulation tasks.

## Limitations
- Theoretical recovery guarantees assume regularity conditions that may not hold in practical robotics applications with discontinuities
- The hierarchical extension assumes the abstract space adequately captures task-relevant information, but quality analysis is limited
- Empirical evaluation focuses primarily on navigation and manipulation tasks; performance on more diverse problem classes remains unknown

## Confidence
- Mathematical formulation: High
- Theoretical analysis: Medium
- Practical applicability across diverse domains: Low-Medium
- Hierarchical extension effectiveness: Medium

## Next Checks
1. Test Eik-HiQRL on high-dimensional manipulation tasks with complex contact dynamics to assess the robustness of the abstract space representation
2. Conduct ablation studies isolating the impact of the Eikonal constraint versus other components of the training procedure on final performance
3. Evaluate the method's performance when trained on heterogeneous, temporally-stitched datasets typical of real-world robotics scenarios