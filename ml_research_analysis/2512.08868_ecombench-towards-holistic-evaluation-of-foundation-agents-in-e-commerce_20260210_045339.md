---
ver: rpa2
title: 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce'
arxiv_id: '2512.08868'
source_url: https://arxiv.org/abs/2512.08868
tags:
- e-commerce
- agent
- arxiv
- ecombench
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EcomBench is a new benchmark for evaluating foundation agents in
  realistic e-commerce scenarios. It is constructed from real-world user demands sourced
  from global e-commerce ecosystems and curated by human experts to ensure authenticity,
  professionalism, and domain relevance.
---

# EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce

## Quick Facts
- arXiv ID: 2512.08868
- Source URL: https://arxiv.org/abs/2512.08868
- Reference count: 9
- Primary result: New benchmark for foundation agents in e-commerce with 7 task categories and 3 difficulty levels, showing performance drops from 80-95% to below 35% on hardest tasks.

## Executive Summary
EcomBench is a new benchmark for evaluating foundation agents in realistic e-commerce scenarios. It is constructed from real-world user demands sourced from global e-commerce ecosystems and curated by human experts to ensure authenticity, professionalism, and domain relevance. The benchmark covers seven task categories and three difficulty levels, assessing capabilities such as information retrieval, reasoning, and cross-source knowledge integration. EcomBench is designed to be dynamic, with quarterly updates to reflect market trends and evolving agent capabilities. Evaluation results show that while models perform well on basic tasks, they struggle significantly with high-difficulty questions requiring complex reasoning and multi-step planning. This benchmark provides a rigorous, real-world testbed for measuring and advancing the practical capabilities of foundation agents in e-commerce.

## Method Summary
EcomBench is constructed from real user demands collected from global e-commerce platforms, filtered by LLMs to remove subjective or unanswerable items, and refined by domain experts through peer validation. The benchmark defines three difficulty levels (20% Level 1, 30% Level 2, 50% Level 3) using a tool-hierarchy rejection sampling approach where tasks requiring more complex reasoning chains are retained as harder. Evaluation uses binary correctness scoring with LLM-as-judge comparison against ground-truth answers, supplemented by manual spot-checks. The dataset is available on HuggingFace and includes seven task categories covering policy consulting, cost and pricing, fulfillment execution, marketing strategy, product selection, opportunity discovery, and inventory control.

## Key Results
- Performance drops from 80-95% accuracy on easy tasks to below 35% on the most difficult ones
- Models maintain high accuracy on Level 1 (foundational expertise, basic tools) but collapse on Level 3 (cross-source integration, long-horizon planning)
- Different models exhibit domain-specific strengths, posing challenges for building a more general e-commerce agent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding benchmark tasks in genuine user demands improves ecological validity of agent evaluation compared to synthetic or academic scenarios.
- Mechanism: Real user queries from global e-commerce platforms (e.g., Amazon) are collected, filtered by LLMs to remove subjective/unanswerable items, then refined by domain experts through peer validation (≥3 independent labelers). This preserves authentic intent patterns while ensuring verifiability.
- Core assumption: Real user demands better predict agent performance in production than artificially designed tasks.
- Evidence anchors:
  - [abstract] "EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts"
  - [Section 2.1] Describes collection from real user demands, LLM filtering, and expert refinement with peer validation
  - [corpus] DeepShop and other e-commerce benchmarks similarly emphasize realism over synthetic queries, though corpus evidence for superiority of real-user grounding specifically is limited
- Break condition: If user behavior patterns shift faster than quarterly update cycle, or if expert labeling introduces systematic bias not present in actual user contexts.

### Mechanism 2
- Claim: Tool hierarchy-based filtering identifies tasks requiring longer reasoning chains and cross-source integration, creating a difficulty gradient.
- Mechanism: An LLM equipped with advanced e-commerce tools (price retrieval, trend analysis) attempts tasks; those solvable in few steps with high-level tools are retained as Level-3 difficulty. This creates rejection sampling where complex tasks survive because they resist shortcut solutions.
- Core assumption: Tasks requiring more tool actions correlate with deeper reasoning requirements; tool-equipped LLM serves as adequate proxy for agent capability ceiling.
- Evidence anchors:
  - [abstract] "defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration"
  - [Section 2.2] "we equip the LLM with specialized e-commerce tools... and then apply rejection sampling to retain questions that require complex reasoning chains"
  - [corpus] Weak direct evidence for tool-hierarchy approach specifically; related work on agent evaluation emphasizes complexity scaling generally
- Break condition: If tool-equipped LLM has systematic blind spots that cause misclassification, or if complexity correlates with tool count rather than reasoning depth.

### Mechanism 3
- Claim: Performance degradation across difficulty levels (80-95% → <35%) reveals gap between surface-level domain knowledge and integrated multi-step reasoning.
- Mechanism: Three difficulty tiers (Level 1: 20%, Level 2: 30%, Level 3: 50%) test progressively deeper capabilities. Models maintain high accuracy on Level 1 (foundational expertise, basic tools) but collapse on Level 3 (cross-source integration, long-horizon planning).
- Core assumption: Difficulty stratification validly isolates specific capability gaps rather than confounding factors like question ambiguity.
- Evidence anchors:
  - [abstract] "performance drop from 80-95% accuracy on easy tasks to below 35% on the most difficult ones"
  - [Section 4.2] Figure 3 shows progressive decline: ChatGPT-5.1 and Gemini DeepResearch drop from 95% to 46% across levels
  - [corpus] Consistent with patterns in GAIA, GPQA showing steep drops on multi-step reasoning tasks
- Break condition: If Level-3 failures stem from unclear questions rather than reasoning gaps; evaluation methodology notes LLM-based scoring with manual spot-checks but not full human verification.

## Foundational Learning

- Concept: **ReAct-style interleaved reasoning and tool use**
  - Why needed here: Tasks require both reasoning traces (planning, decomposition) and tool invocations (search, data retrieval). Understanding thought-action-observation loops is prerequisite for analyzing agent behavior.
  - Quick check question: Can you explain why separating "thought" from "action" in agent traces improves debugging of failure modes?

- Concept: **Cross-source knowledge integration**
  - Why needed here: Level-3 tasks require combining information from policy documents, pricing databases, market trends—different sources with different formats and reliability levels.
  - Quick check question: Given two conflicting data sources about import duty rates, what heuristics would an agent use to resolve the discrepancy?

- Concept: **E-commerce domain primitives (VAT, AQL sampling, EIRP calculations)**
  - Why needed here: Benchmark questions reference specific regulatory frameworks (EU RED, DOE Level VI efficiency, Canadian toy safety) that require domain literacy to parse and solve.
  - Quick check question: If asked about customs duties on a mixed goods shipment to Germany, which components would you need to identify before calculating?

## Architecture Onboarding

- Component map:
  - Data Collection Layer -> LLM Filtering Layer -> Expert Annotation Layer -> Difficulty Assignment Layer -> Evaluation Layer -> Maintenance Layer

- Critical path: User demand → LLM filter → Expert refinement → Tool-hierarchy difficulty classification → Ground-truth labeling → Benchmark release → Quarterly refresh

- Design tradeoffs:
  - Human curation ensures quality but limits scalability (authors acknowledge "substantial human effort" requirement)
  - Question-answering format enables verifiable evaluation but excludes interactive/transactional e-commerce tasks (explicitly noted as limitation)
  - Quarterly updates reduce contamination risk but increase maintenance cost

- Failure signatures:
  - False difficulty: Level-3 questions that are actually ambiguous rather than complex (check: can multiple experts independently reach same answer?)
  - Tool mismatch: Agent fails not from reasoning gap but from lacking specific e-commerce tools benchmark assumes available
  - Staleness: Regulatory/market changes make ground-truth answers outdated between quarterly releases

- First 3 experiments:
  1. Calibration run: Test your agent on Level-1 tasks only; if accuracy <80%, investigate basic e-commerce knowledge gaps before attempting harder levels
  2. Error taxonomy: Run 20 Level-3 failures through manual review; categorize as (a) knowledge gap, (b) reasoning breakdown, (c) tool access failure, or (d) ambiguous question
  3. Tool ablation: Compare performance with full e-commerce tool suite vs. generic web-search-only tools to quantify tool-hierarchy impact on your specific agent

## Open Questions the Paper Calls Out

- Question: How can agent architectures be improved to handle Level 3 tasks that require cross-source knowledge integration and long-horizon reasoning, where current models drop below 35% accuracy?
  - Basis in paper: [explicit] The paper states these tasks "remain a significant challenge" and that even leading models "see their scores fall to 46% in Level 3."
  - Why unresolved: Current models lack the multi-step reasoning chains and adaptive tool usage required; the paper identifies this gap but offers no solution.
  - What evidence would resolve it: A new agent architecture or training methodology achieving >60% on Level 3 tasks.

- Question: How can evaluation be extended from question-answering to interactive e-commerce environments with real-time decision-making?
  - Basis in paper: [explicit] The limitations section states: "EcomBench currently focuses on question-answering tasks and does not explicitly evaluate agents in environments with interactions."
  - Why unresolved: The benchmark design centers on verifiable answers, not dynamic agent-environment interaction loops.
  - What evidence would resolve it: An expanded benchmark incorporating simulated e-commerce environments with interaction-based evaluation metrics.

- Question: How can a single generalist e-commerce agent be built to excel across all task domains rather than exhibiting domain-specific strengths?
  - Basis in paper: [explicit] The paper observes that "different models exhibit domain-specific strengths" and concludes: "This poses a challenge for building a more general e-commerce agent."
  - Why unresolved: Trade-offs between specialized and general capabilities remain unexplored.
  - What evidence would resolve it: An agent achieving top-quartile performance across all seven task categories simultaneously.

## Limitations
- Benchmark focuses on question-answering format, excluding interactive/transactional e-commerce tasks
- Tool specifications are incomplete, making exact reproduction difficult
- Quarterly update cycles create maintenance burden and potential version control issues

## Confidence
- High confidence in benchmark construction methodology and difficulty stratification
- Medium confidence in tool-hierarchy difficulty classification mechanism
- Low confidence in generalizability claims beyond e-commerce QA tasks

## Next Checks
1. Conduct a small-scale human evaluation of 20 Level-3 questions to verify whether failures stem from reasoning gaps versus question ambiguity
2. Perform tool ablation studies comparing performance with full e-commerce tool suite versus generic search-only tools
3. Test calibration by running agents exclusively on Level-1 tasks first to establish baseline e-commerce knowledge before attempting higher difficulty levels