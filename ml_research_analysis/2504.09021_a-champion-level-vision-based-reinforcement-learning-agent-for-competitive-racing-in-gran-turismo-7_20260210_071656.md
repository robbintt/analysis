---
ver: rpa2
title: A Champion-level Vision-based Reinforcement Learning Agent for Competitive
  Racing in Gran Turismo 7
arxiv_id: '2504.09021'
source_url: https://arxiv.org/abs/2504.09021
tags:
- agent
- racing
- learning
- track
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a vision-based autonomous racing agent for
  Gran Turismo 7 that achieves champion-level performance in competitive scenarios.
  The agent uses only ego-centric camera views and onboard sensor data, eliminating
  reliance on global features during inference.
---

# A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7
## Quick Facts
- arXiv ID: 2504.09021
- Source URL: https://arxiv.org/abs/2504.09021
- Reference count: 40
- Primary result: Vision-based autonomous racing agent achieves champion-level performance in Gran Turismo 7 competitive scenarios

## Executive Summary
This paper introduces a vision-based autonomous racing agent that achieves champion-level performance in Gran Turismo 7's competitive racing scenarios. The agent operates using only ego-centric camera views and onboard sensor data, eliminating reliance on global features during inference. Through an asymmetric actor-critic architecture, the agent processes visual and proprioceptive inputs through a recurrent neural network while leveraging global features during training. The approach demonstrates superior performance against both built-in AI and human expert baselines across multiple tracks, establishing a new benchmark for vision-based racing agents in competitive environments.

## Method Summary
The proposed agent employs an asymmetric actor-critic architecture where the actor network processes only egocentric camera views and onboard sensor data through a recurrent neural network, while the critic network utilizes global features during training. This design enables the agent to handle partial observability inherent in competitive racing scenarios while still benefiting from global information during the learning process. The actor consists of a convolutional neural network that processes visual inputs, combined with a long short-term memory (LSTM) network that handles proprioceptive data and temporal dependencies. The agent is trained using reinforcement learning with reward shaping that encourages competitive behavior while maintaining safe racing practices. Regularization techniques are employed to improve generalization across different tracks and racing conditions.

## Key Results
- The agent consistently outperforms GT7's built-in AI and human expert baselines, achieving the highest winning margins across three different tracks (Tokyo, Spa, and Sarthe)
- Ablation studies confirm the importance of the recurrent module for opponent tracking and the asymmetric architecture for leveraging global information during training
- The agent demonstrates champion-level performance while using only ego-centric vision and onboard sensor data, eliminating dependence on privileged global features during inference

## Why This Works (Mechanism)
The agent's success stems from its ability to handle partial observability through the recurrent neural network while maintaining competitive performance. The asymmetric architecture allows the critic to leverage global features during training, providing richer supervision signals, while the actor learns to make decisions based only on available in-race information. The recurrent module enables the agent to track opponents' positions and intentions over time, which is crucial for strategic decision-making in competitive racing. Regularization techniques further enhance generalization, allowing the agent to perform well across different track layouts and opponent behaviors.

## Foundational Learning
- **Partial Observability**: Understanding that agents don't have complete information about the environment state, particularly in competitive scenarios where opponent intentions are hidden. Needed to design the recurrent architecture that tracks opponents over time.
- **Asymmetric Actor-Critic**: A training paradigm where the actor and critic have different input capabilities, with the critic using more information during training than the actor has during inference. Critical for leveraging global features during learning while maintaining practical deployment constraints.
- **Reinforcement Learning with Reward Shaping**: The process of designing reward functions that encourage desired behaviors while discouraging undesirable ones. Essential for balancing competitive performance with safe racing practices.
- **Vision-based Control**: Using camera inputs as the primary sensory modality for decision-making, as opposed to relying on internal game state information. Fundamental to creating agents that can operate in real-world scenarios.
- **Recurrent Neural Networks for Temporal Processing**: Using LSTM or similar architectures to maintain and update internal state representations over time. Necessary for tracking opponents' positions and intentions across multiple timesteps.

## Architecture Onboarding
- **Component Map**: Ego-centric camera -> CNN -> Feature extraction -> LSTM -> Action selection; Onboard sensors -> LSTM input; Global features (training only) -> Critic network
- **Critical Path**: Visual input through CNN → LSTM → Policy output; Sensor data through LSTM → Policy output
- **Design Tradeoffs**: The asymmetric architecture sacrifices some training efficiency by limiting the actor's access to global features, but enables deployment in realistic scenarios where only local information is available. The recurrent module adds computational complexity but is essential for tracking opponents.
- **Failure Signatures**: Without the recurrent module, the agent struggles to maintain awareness of opponent positions over time. Without the asymmetric design, the agent may overfit to global features that aren't available during deployment.
- **3 First Experiments**: 1) Test the agent's performance with and without the recurrent module to quantify the impact on opponent tracking. 2) Evaluate the agent's behavior when global features are available during inference to measure the asymmetric design's effectiveness. 3) Assess the agent's performance across different opponent skill levels to understand its competitive range.

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability and robustness of the approach. These include uncertainty about performance in novel racing scenarios beyond the specific tracks and opponent types evaluated, questions about behavior when encountering unexpected opponent strategies or aggressive driving styles, and concerns about computational requirements for real-time inference in practical deployment scenarios.

## Limitations
- The asymmetric architecture relies on global features during training, which may limit applicability to racing games without similar access to privileged information
- Evaluation focuses primarily on specific track configurations and opponent types, raising questions about performance in diverse racing scenarios
- Ablation studies don't explore alternative architectural choices or different levels of partial observability that might be encountered in other racing games

## Confidence
- **High**: Technical implementation and benchmark results are well-documented and reproducible
- **Medium**: Generalization claims are supported but limited by the scope of evaluation scenarios
- **High**: Claim of being the first vision-based champion-level agent is well-supported by thorough literature review

## Next Checks
1. Test the agent's performance across a broader range of track layouts and weather conditions to assess true generalizability
2. Evaluate the agent's behavior in scenarios with unexpected opponent strategies or aggressive driving styles
3. Assess the computational requirements and real-time inference capabilities to determine practical deployment feasibility