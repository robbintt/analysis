---
ver: rpa2
title: What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings
  in Their Interactions
arxiv_id: '2510.05378'
source_url: https://arxiv.org/abs/2510.05378
tags:
- meaning
- participants
- meanings
- symbols
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how humans and conversational AI agents
  collaboratively construct and reshape meanings of symbols through interaction. Two
  studies examined participants' meaning-making processes with AI agents across different
  scenarios and control conditions.
---

# What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions

## Quick Facts
- **arXiv ID**: 2510.05378
- **Source URL**: https://arxiv.org/abs/2510.05378
- **Reference count**: 40
- **Primary result**: Humans and AI collaboratively construct and reshape symbol meanings through iterative interaction, with AI perceived as an active co-constructor rather than passive tool

## Executive Summary
This paper investigates how humans and conversational AI agents collaboratively construct and reshape meanings of symbols through interaction. Two studies examined participants' meaning-making processes with AI agents across different scenarios and control conditions. Participants engaged in iterative cycles of exploration, explanation, clarification, specification, integration, and consolidation of meanings. Results showed that participants' initial meanings shifted in response to AI suggestions and social contexts, particularly when conflicts were introduced. The AI was perceived as an active co-constructor of meaning rather than a passive tool, with participants viewing it as a thought partner or collaborator.

## Method Summary
The study employed two experimental designs: Study 1 used pre-scripted dialogues with 20 participants across two scenarios, while Study 2 used live LLM conversations with 16 participants. Both studies followed a three-phase protocol (pre-survey, two scenarios, post-survey) and introduced conflicts via social context to trigger meaning shifts. Study 2 specifically used a four-Act structure per scenario with distinct system prompts designed to elicit initial meaning, introduce social conflict, co-construct meaning, and synthesize new symbols. Participants rated their involvement, clarification effectiveness, and conversation depth on 7-point Likert scales, with turn-by-turn conversation analysis and qualitative coding for meaning-making processes.

## Key Results
- Participants' initial meanings shifted in response to AI suggestions and social contexts, particularly when conflicts were introduced
- AI was perceived as an active co-constructor of meaning rather than a passive tool, with participants viewing it as a thought partner or collaborator
- Statistical analysis revealed moderate but not strong effects, with participants feeling actively involved in meaning construction but reporting some conversations as shallow

## Why This Works (Mechanism)

### Mechanism 1: Structured conflict as catalyst
- **Claim**: Introduction of structured conflict functions as a catalyst for deepening shared meaning between humans and AI
- **Mechanism**: When AI introduces social constraints that conflict with user's initial symbol, users must articulate latent criteria, shifting interaction from preference matching to active negotiation
- **Core assumption**: Users will engage in cognitive effort to resolve discrepancies when social stakes are significant
- **Evidence anchors**: Abstract states shared understanding emerges from "bi-directional exchange and reinterpretation of symbols"; Section 5.2 identifies conflict as "engine for co-construction of meaning"
- **Break condition**: If user perceives conflict as manufactured or irrelevant, they may disengage or rigidly adhere to initial definition

### Mechanism 2: Social contextualization amplifies influence
- **Claim**: Social contextualization amplifies AI's influence on user meaning-making more effectively than logical argumentation alone
- **Mechanism**: Presenting new symbols within social narratives leverages human social processing, allowing AI to reshape internal definitions of abstract concepts
- **Core assumption**: Humans anthropomorphize interaction sufficiently to treat AI-introduced "social facts" with same weight as human social constraints
- **Evidence anchors**: Abstract notes participants shifted definitions "especially when social context is introduced"; Section 4.1.1 shows participants incorporated external perspectives as "socially tailored"
- **Break condition**: If social context is too abstract or contradicts deeply held personal values, user may reject AI's framing entirely

### Mechanism 3: Iterative state-synchronization
- **Claim**: Meaning is stabilized through iterative cycles of state-synchronization rather than single-turn definition
- **Mechanism**: Interaction proceeds through phases (Exploration → Clarification → Specification → Consolidation), allowing meaning states to fluctuate between diverged, overlapped, and synchronized
- **Core assumption**: AI can accurately track "meaning state" and determine when to move from exploration to consolidation
- **Evidence anchors**: Section 4.2.2 details specific processes including "exploration, explanation, clarification, specification, acknowledgment, integration, and consolidation"
- **Break condition**: If AI attempts consolidation too early, users perceive it as "assertive" or "controlling," breaking collaboration illusion

## Foundational Learning

- **Concept: Symbolic Interactionism (SI)**
  - **Why needed here**: Theoretical backbone showing meaning is dynamic property emerging in interaction, not static lookup value
  - **Quick check question**: If user changes definition of "Good" halfway through chat, is that bug (state inconsistency) or feature (meaning construction)?

- **Concept: Meaning Synchronization States**
  - **Why needed here**: System needs taxonomy to track alignment (Synchronized, Overlapped, Diverged) to decide next move
  - **Quick check question**: AI suggests "Thrilling," user wants "Relaxing." Is this state Diverged or Overlapped? (Depends on higher-level abstraction like "Enjoyable")

- **Concept: The "Social Other" in AI**
  - **Why needed here**: To implement "Social Context" mechanism, system must simulate social perspective rather than act as objective oracle
  - **Quick check question**: How do you inject constraint without sounding like logic puzzle? (Frame as "X person prefers...")

## Architecture Onboarding

- **Component map**: User Utterance + Current Symbol Definition → State Manager (tracks Meaning State and Alignment Level) → Conflict Generator (injects social scenarios) → Consolidator (summarizes negotiated meaning)

- **Critical path**:
  1. **Elicitation**: AI prompts for initial symbol definition
  2. **Conflict/Context**: AI introduces "social other's" perspective challenging initial definition
  3. **Negotiation**: Bi-directional refinement (Clarification/Specification)
  4. **Synthesis**: AI proposes new activity/concept reflecting merged meaning

- **Design tradeoffs**:
  - **Scripted vs. LLM**: Scripted ensures "Conflict" step happens but feels rigid; LLM allows fluid negotiation but risks "hallucinating" user preferences
  - **Assertiveness**: High assertiveness speeds up task but lowers user agency

- **Failure signatures**:
  - **"Shallow" Conversation**: AI fails to probe deeper than surface-level keywords
  - **"Forcing"**: AI insists on definition user has already rejected
  - **Lost Context**: AI suggests final activity contradicting user's refined criteria

- **First 3 experiments**:
  1. **Conflict A/B Test**: Run "Good Activity" scenario. Group A gets generic suggestions; Group B gets "Social Conflict" (friend's preference). Measure symbol definition shift rate
  2. **Timing of Consolidation**: Test when to summarize meaning (Early turn 5 vs. Late turn 20). Measure user frustration/feeling of being "forced"
  3. **Alignment Accuracy**: Manually label conversation turns as "Synchronized" or "Diverged." Evaluate if AI can detect these states and trigger "Clarification" when Diverged

## Open Questions the Paper Calls Out
- How does human-AI meaning co-construction unfold over extended periods in naturalistic, unstructured settings?
- How do meaning-making processes differ in high-stakes contexts compared to the low-stakes scenarios tested?
- What dynamics of symbolic convergence and divergence emerge in multi-party human-AI interactions?

## Limitations
- Moderate effect sizes suggest meaning construction process may not be as robust as implied
- Cannot definitively establish causation between conflict introduction and meaning shifts
- Limited generalizability beyond specific "activity planning" domain

## Confidence
- **Core finding (AI as co-constructor)**: Medium-High confidence - supported by qualitative feedback but limited by sample size and artificial scenarios
- **Conflict as catalyst mechanism**: Medium confidence - demonstrates correlation but cannot definitively establish causation
- **Social contextualization amplification**: Low-Medium confidence - shows effectiveness but lacks direct comparison against other mechanisms

## Next Checks
1. **Controlled conflict vs. control**: Run within-subjects experiment with conflict-introduction and non-conflict conditions, measuring meaning stability and satisfaction to establish causal effects
2. **Longitudinal meaning tracking**: Conduct follow-up sessions 1-2 weeks later to assess whether negotiated meanings persist or revert
3. **Cross-domain generalization**: Replicate study in non-social domains (technical problem-solving, information retrieval) to test framework applicability across different interaction types