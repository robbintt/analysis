---
ver: rpa2
title: 'Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in
  Latent Space'
arxiv_id: '2505.13308'
source_url: https://arxiv.org/abs/2505.13308
tags:
- latent
- reasoning
- latentseek
- space
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LATENTSEEK introduces a test-time instance-level adaptation method
  that optimizes language model reasoning by iteratively updating latent representations
  using policy gradient, without modifying model parameters. This approach addresses
  the challenge of improving reasoning performance without costly fine-tuning or catastrophic
  forgetting.
---

# Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space

## Quick Facts
- **arXiv ID:** 2505.13308
- **Source URL:** https://arxiv.org/abs/2505.13308
- **Reference count:** 40
- **Primary result:** Test-time latent space optimization improves LLM reasoning accuracy by 15.23 points on GSM8K and 6.67 points on AIME2024 without parameter updates

## Executive Summary
LATENTSEEK introduces a test-time instance-level adaptation method that optimizes language model reasoning by iteratively updating latent representations using policy gradient, without modifying model parameters. This approach addresses the challenge of improving reasoning performance without costly fine-tuning or catastrophic forgetting. By leveraging a self-reward mechanism, LATENTSEEK refines latent vectors to guide the model toward better reasoning paths. Evaluated across GSM8K, MATH-500, and AIME2024 benchmarks, it consistently outperforms strong baselines like Chain-of-Thought and reinforcement learning methods, achieving average gains of 15.23 points on GSM8K and 6.67 points on AIME2024. Experiments with a perfect sparse reward model further demonstrate the effectiveness of latent space exploration. The method converges within a few iterations for average problems and offers a lightweight, scalable solution for enhancing LLM reasoning at test time.

## Method Summary
LATENTSEEK operates by extracting latent representations from a frozen LLM during test-time reasoning, then applying policy gradient updates to these latents to improve answer quality. The method initializes latent vectors from Chain-of-Thought generation, keeps a fraction (ρ∈[0.05,0.3]) of the latent sequence, and iteratively updates them via gradient ascent guided by self-generated rewards. The reward comes from the model's own evaluation of its reasoning across four dimensions: correctness, comprehension, calculation, and completeness. Updates are computed using REINFORCE-style policy gradient, treating latents as independent variables optimized through the LM head. The process terminates when a reward threshold is met or maximum iterations are reached, typically converging in fewer than 2 iterations for average problems.

## Key Results
- Achieves 15.23 average accuracy gain on GSM8K over Best-of-N baseline
- Outperforms standard Chain-of-Thought by 6.67 points on AIME2024
- Maintains consistent improvements across model scales from 1.5B to 70B parameters
- Demonstrates rapid convergence with average <2 iterations per problem
- Shows effectiveness with perfect sparse reward model achieving 9.27 additional points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing latent representations via policy gradient at test time can improve reasoning accuracy without parameter updates.
- **Mechanism:** LATENTSEEK treats latent vectors z = (z₁, z₂, ..., zₙ) as independent variables and applies REINFORCE-style updates: z ← z + η∇_z J(z), where ∇_z J(z) = E[R(x,c) ∇_z log π(x|z,c)]. The latent vectors act as a "control mechanism" that biases subsequent decoding toward higher-reward paths.
- **Core assumption:** Latent representations carry sufficient semantic information to encode reasoning trajectories that improve final answer quality when decoded.
- **Evidence anchors:**
  - [abstract] "LATENTSEEK leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals."
  - [Section 2.2, Eq. 5-7] Derivation of policy gradient in latent space with independence assumption justified by "Enlarged Exploration Space" argument.
  - [corpus] Related work on latent space reasoning (LatentEvolve, SoftCoT++) shows growing interest, but FMR=0.55 and zero citations suggest early-stage evidence with limited external validation.

### Mechanism 2
- **Claim:** Self-rewarding provides sufficient signal for latent optimization when base model has adequate self-evaluation capability.
- **Mechanism:** The model evaluates its own outputs using prompts asking for correctness, calculation accuracy, problem comprehension, and answer completeness (weighted 1:1:2:2), producing dense rewards in [-1, 0]. This creates a closed-loop optimization where the model's generation and evaluation abilities cooperate.
- **Core assumption:** Base model's self-evaluation correlates with ground-truth correctness at levels adequate for gradient guidance.
- **Evidence anchors:**
  - [Section 3.6, Table 6] Self-reward achieves ~80% accuracy on GSM8K across backbones.
  - [Section 3.6, Table 7] Ablation with constant -1 reward drops performance 14.21 points, demonstrating reward guidance is essential.
  - [corpus] Corpus lacks direct evidence on self-reward reliability; related papers (Amortized Latent Steering) mention LatentSeek's approach but don't validate the mechanism independently.

### Mechanism 3
- **Claim:** Fractional optimization (updating only ρ·T tokens) balances exploration capacity with reward reliability.
- **Mechanism:** Instead of updating all T latent vectors, only the first ρ·T are modified (ρ typically 0.1-0.2). This reduces computational cost while maintaining a "launch pad" for subsequent autoregressive generation that completes the reasoning path.
- **Core assumption:** Early-token latent representations dominate the trajectory of subsequent reasoning.
- **Evidence anchors:**
  - [Section 2.3] "ρ must strike a balance between maintaining adequate representational capacity...and limiting the number of latent representations being updated."
  - [Section E.7, Table 13] Optimizing middle-stage tokens (starting from 40%) underperforms initial-stage optimization by 7.58 points.
  - [corpus] No direct corpus evidence on fractional optimization tradeoffs.

## Foundational Learning

- **Policy Gradient (REINFORCE)**
  - **Why needed here:** LATENTSEEK applies policy gradient to continuous latent vectors rather than discrete tokens. Understanding the score function estimator ∇_θ log π(a|s) is essential to grasp how gradients flow through the latent space.
  - **Quick check question:** Given reward R and policy π(z), what is the gradient of the objective E[R] with respect to latent z?

- **Transformer Hidden States as Latent Representations**
  - **Why needed here:** The paper defines latent space as transformer outputs before the LM head (z_t = π_Transformer(x_<t, c)). Understanding how these representations encode semantic information is critical.
  - **Quick check question:** In a standard transformer decoder, what is the shape and semantic content of the output at position t before the language model head?

- **Test-Time Computation vs Training-Time Optimization**
  - **Why needed here:** LATENTSEEK exemplifies the paradigm of improving performance through increased test-time compute without parameter updates. This contrasts with fine-tuning and RLHF approaches.
  - **Quick check question:** What are the tradeoffs between test-time optimization (instance-level adaptation) versus training-time optimization (parameter updates)?

## Architecture Onboarding

- **Component map:**
  Base LLM (frozen) -> Latent Vector Store -> LM Head (partial gradients) -> Self-Reward Module -> Gradient Updater -> Stopping Criterion

- **Critical path:**
  1. Initialize latents from CoT generation (Section 2.3)
  2. Keep ρ fraction (typically 0.2) of latent sequence
  3. Decode latents → tokens → autoregressive completion
  4. Compute self-reward R(x,c) ∈ [-1, 0]
  5. Compute gradients ∇_z log π(x_t|z_t) through LM head only
  6. Update latents: z ← z + η∇_z J(z)
  7. Repeat until R > τ or k > K

- **Design tradeoffs:**
  - **ρ (fraction ratio)**: Higher = more exploration but risk of incoherent tokens; lower = faster but limited search space (Section E.8 shows 0.2-0.3 optimal for most models)
  - **Learning rate η**: 0.03-0.05 for most models; higher rates cause instability
  - **Max iterations K**: Average convergence <2 iterations for GSM8K/MATH-500, but scaling to K=256 with PSRM shows continued gains (Table 4)
  - **Decoding method**: Greedy decoding preferred over sampling for stability (Section E.4)

- **Failure signatures:**
  - **Incoherent token generation**: Tokens like "thecy," "theella" indicate latent optimization has drifted from semantic coherence (Section 3.8)
  - **Reward plateau with self-reward**: Iteration curves plateau after initial gains (Figure 2), suggesting self-reward limitations
  - **Middle-stage optimization underperformance**: 7.58 point gap vs. initial-stage (Table 13) indicates prefix independence limits

- **First 3 experiments:**
  1. **Reproduce GSM8K results with Qwen2.5-7B-Instruct**: Use Prompt 2 (JSON format), ρ=0.2, η=0.05, K=10. Expected: ~85% accuracy, convergence in 1-2 iterations. This validates basic pipeline and reward mechanism.
  2. **Ablate fraction ratio ρ**: Test ρ ∈ {0.1, 0.2, 0.3, 0.5} on GSM8K subset (100 problems). Expect peak at 0.2-0.3, degradation at 0.5+. This identifies optimal exploration capacity.
  3. **Compare self-reward vs. PSRM**: Implement oracle reward (exact match to ground truth) and compare iteration curves. Expect PSRM to show monotonic improvement while self-reward plateaus (Figure 2). This quantifies the gap between ideal and practical reward models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can advanced reinforcement learning algorithms (e.g., PPO) or latent-space-specific optimizers improve the convergence rate and stability of LatentSeek compared to the standard REINFORCE method?
- **Basis in paper:** [explicit] The authors state they "adopt standard policy-gradient methods... leaving the exploration of more advanced reinforcement learning algorithms—such as Proximal Policy Optimization (PPO)—to future work."
- **Why unresolved:** The current implementation relies on a simple gradient ascent approach; the potential benefits of second-order optimizers or algorithms specifically designed for continuous latent spaces were not evaluated.
- **What evidence would resolve it:** Empirical results showing that PPO or latent-specific optimizers reduce the average number of iterations required to reach a threshold reward or achieve higher accuracy on GSM8K/MATH-500.

### Open Question 2
- **Question:** Does the performance and efficiency of test-time latent optimization scale effectively to large models (>14B parameters) without prohibitive computational costs?
- **Basis in paper:** [explicit] The paper notes that experiments were constrained to models up to 14B parameters and states, "Scaling the approach to larger base models remains an important avenue for future investigation."
- **Why unresolved:** It is unclear if the rapid convergence observed in 1.5B–14B models holds for 70B+ models, where the latent space dimensionality and forward pass costs are significantly higher.
- **What evidence would resolve it:** Benchmarking LatentSeek on 70B or 405B parameter models to demonstrate that the iteration count remains low and the accuracy gains scale linearly or super-linearly.

### Open Question 3
- **Question:** Can a robust, external Outcome Reward Model (ORM) be designed to outperform the self-rewarding mechanism for guiding latent space exploration?
- **Basis in paper:** [explicit] The authors identify the reliance on self-rewarding as a limitation and argue for "the development of more powerful verifiers capable of reliably scoring complex reasoning tasks."
- **Why unresolved:** The paper found that current public ORMs are noisy and perform worse than the self-reward mechanism, leaving the potential of external supervision untapped.
- **What evidence would resolve it:** A study showing an external ORM providing a cleaner gradient signal than self-reward, leading to higher accuracy on complex tasks like AIME2024 without increasing hallucination.

## Limitations

- **Reward reliability gap:** Self-reward achieves ~80% accuracy on GSM8K but degrades on harder problems like AIME2024, potentially limiting optimization effectiveness on complex reasoning tasks.
- **Semantic coherence tradeoff:** LATENTSEEK can produce incoherent tokens while maintaining correct answers, raising questions about interpretability and generalization.
- **Scaling constraints:** Improvement magnitude varies significantly across model scales, with larger models showing diminishing returns that suggest limited value for parameter-efficient fine-tuning.

## Confidence

**High Confidence:** The basic mechanism of test-time latent space optimization via policy gradient is sound and reproducible. The derivation in Section 2.2 is rigorous, and the implementation approach (fractional optimization with ρ=0.2) is clearly specified.

**Medium Confidence:** The self-reward mechanism's reliability across diverse problem types. While validation on GSM8K shows 80% accuracy, the method's performance on AIME2024 (6.67 point gain) suggests potential limitations on more complex reasoning tasks.

**Low Confidence:** The claim that LATENTSEEK "consistently outperforms" all baselines. The paper shows strong results on specific benchmarks but doesn't provide comprehensive comparisons across diverse reasoning domains or establish statistical significance beyond reporting average gains.

## Next Checks

1. **Reward Model Ablation Study:** Implement both self-reward and oracle reward (ground truth comparison) on AIME2024. Measure the correlation between self-reward accuracy and final answer correctness across problem difficulty levels. This quantifies how reward reliability impacts optimization effectiveness.

2. **Cross-Domain Generalization Test:** Apply LATENTSEEK to non-mathematical reasoning tasks (e.g., commonsense reasoning, multi-hop QA) with modified reward prompts. Compare performance degradation against mathematical domains to identify the method's reasoning type limitations.

3. **Latent Space Trajectory Analysis:** For a subset of problems where LATENTSEEK succeeds, visualize the latent vector evolution (e.g., t-SNE of z across iterations). Analyze whether successful optimizations follow consistent patterns or if each problem requires unique exploration strategies.