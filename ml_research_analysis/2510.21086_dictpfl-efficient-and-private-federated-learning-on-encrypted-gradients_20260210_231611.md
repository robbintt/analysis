---
ver: rpa2
title: 'DictPFL: Efficient and Private Federated Learning on Encrypted Gradients'
arxiv_id: '2510.21086'
source_url: https://arxiv.org/abs/2510.21086
tags:
- gradients
- dictpfl
- learning
- should
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DictPFL, a framework that enables private
  federated learning using homomorphic encryption with significantly reduced overhead.
  The key innovation is decomposing model weights into a static dictionary and a trainable
  lookup table, where only the lookup table is encrypted and transmitted for aggregation.
---

# DictPFL: Efficient and Private Federated Learning on Encrypted Gradients

## Quick Facts
- **arXiv ID**: 2510.21086
- **Source URL**: https://arxiv.org/abs/2510.21086
- **Reference count**: 40
- **Primary result**: Achieves 402-748× reduction in communication costs and 28-65× faster training compared to fully encrypted methods

## Executive Summary
DictPFL introduces an innovative framework for private federated learning that dramatically reduces the overhead of homomorphic encryption. The key insight is decomposing model weights into a static dictionary and a trainable lookup table, where only the lookup table requires encryption and transmission. By applying a pruning strategy that removes parameters with persistently small gradients (using shared historical statistics for consistency), DictPFL achieves efficiency levels within 2× of plaintext federated learning while maintaining privacy guarantees. The framework outperforms state-of-the-art selective encryption approaches by 51-155× in overhead and 4-19× in speed.

## Method Summary
DictPFL works by decomposing model weights into two components: a static dictionary and a trainable lookup table. Only the lookup table needs encryption and transmission during federated aggregation. The framework employs a pruning strategy that removes parameters with persistently small gradients, using shared historical statistics across clients to ensure consistent pruning decisions. This dual approach of weight decomposition and intelligent pruning significantly reduces the computational and communication overhead of homomorphic encryption while preserving model performance.

## Key Results
- Achieves 402-748× reduction in communication costs compared to fully encrypted methods
- Improves training speed by 28-65× while maintaining comparable accuracy
- Outperforms state-of-the-art selective encryption approaches by 51-155× in overhead and 4-19× in speed
- Runtime remains within 2× of plaintext federated learning

## Why This Works (Mechanism)
The efficiency gains stem from two complementary mechanisms. First, the weight decomposition strategy isolates only the essential trainable components that need encryption, dramatically reducing the encrypted payload size. Second, the pruning strategy eliminates redundant or low-impact parameters based on gradient history, further minimizing the encrypted data volume. The shared historical statistics ensure consistent pruning across heterogeneous clients, preventing model divergence. Together, these approaches transform the computational bottleneck of homomorphic encryption from a prohibitive constraint to a manageable overhead.

## Foundational Learning

**Homomorphic Encryption**: Allows computation on encrypted data without decryption, essential for privacy-preserving federated learning but computationally expensive.
*Why needed*: Provides the privacy guarantees for federated learning without exposing raw gradients.
*Quick check*: Verify that the encryption scheme supports the required operations (addition, multiplication) on the specific data types.

**Federated Learning Aggregation**: The process where clients collaboratively train a model without sharing raw data.
*Why needed*: Enables distributed model training while preserving data locality.
*Quick check*: Ensure aggregation algorithm can handle the modified gradient format after pruning and decomposition.

**Gradient Pruning**: Removing gradients below a certain threshold to reduce communication overhead.
*Why needed*: Eliminates redundant information that contributes minimally to model updates.
*Quick check*: Validate that pruned gradients don't significantly impact convergence or final accuracy.

## Architecture Onboarding

**Component Map**: Client Model -> Gradient Decomposition -> Pruning Filter -> Encryption -> Server Aggregation -> Model Update -> Client Model

**Critical Path**: The gradient computation and encryption path represents the performance bottleneck. Optimizing the decomposition and pruning stages has the most significant impact on overall efficiency.

**Design Tradeoffs**: 
- Privacy vs. Efficiency: More aggressive pruning improves efficiency but may reduce privacy guarantees.
- Accuracy vs. Speed: Aggressive pruning can speed up training but risks model performance degradation.
- Complexity vs. Maintainability: The dual decomposition-pruning approach adds implementation complexity.

**Failure Signatures**: 
- Model divergence occurs if pruning statistics are inconsistent across clients.
- Communication bottlenecks arise if the lookup table size remains large.
- Convergence issues manifest when essential parameters are incorrectly pruned.

**First Experiments**:
1. Baseline accuracy comparison between DictPFL and plaintext FL on a standard dataset.
2. Communication cost measurement under varying client numbers and data heterogeneity.
3. Runtime profiling to identify bottlenecks in the encryption-decryption pipeline.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance benefits may not generalize well beyond computer vision tasks to NLP or tabular data.
- Effectiveness depends heavily on model architecture choices and may not scale linearly across all federated learning scenarios.
- Pruning strategy relies on historical gradient statistics being representative, which may fail in highly heterogeneous federated environments.

## Confidence
- **High**: Efficiency improvements (402-748× reduction in communication, 28-65× faster training) on tested datasets
- **Medium**: Generalizability across different model types and domains beyond tested computer vision tasks
- **Low**: Robustness in highly heterogeneous federated environments with significant data distribution variations

## Next Checks
1. Test DictPFL's performance and accuracy on non-vision tasks (e.g., language models, tabular data) to verify cross-domain applicability.
2. Evaluate the framework's behavior under extreme data heterogeneity conditions where client data distributions vary significantly.
3. Conduct ablation studies on the pruning strategy to quantify the impact of removing parameters with small gradients on model convergence and final accuracy.