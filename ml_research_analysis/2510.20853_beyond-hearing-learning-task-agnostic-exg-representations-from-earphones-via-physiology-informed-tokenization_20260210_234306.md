---
ver: rpa2
title: 'Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones
  via Physiology-informed Tokenization'
arxiv_id: '2510.20853'
source_url: https://arxiv.org/abs/2510.20853
tags:
- data
- tasks
- signals
- pimt
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building generalizable foundation
  models for electrophysiological (ExG) signals, which are typically limited by data
  diversity and task-specific designs. The authors propose an earphone-based hardware
  prototype (NeuroBuds) to collect free-living ExG data and introduce Physiology-informed
  Multi-band Tokenization (PiMT) for learning task-agnostic ExG representations.
---

# Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization

## Quick Facts
- **arXiv ID:** 2510.20853
- **Source URL:** https://arxiv.org/abs/2510.20853
- **Reference count:** 28
- **Primary result:** Introduces NeuroBuds hardware and PiMT tokenization to achieve 87% average F1-score across diverse ExG tasks

## Executive Summary
This paper addresses the challenge of building generalizable foundation models for electrophysiological (ExG) signals, which are typically limited by data diversity and task-specific designs. The authors propose an earphone-based hardware prototype (NeuroBuds) to collect free-living ExG data and introduce Physiology-informed Multi-band Tokenization (PiMT) for learning task-agnostic ExG representations. PiMT decomposes signals into 12 sub-bands aligned with physiological modalities, enabling adaptive feature extraction. Evaluated on a new DailySense dataset (50 hours free-living, 20 hours task-specific) and four public benchmarks, PiMT achieves an average F1-score of 87% and consistently outperforms state-of-the-art methods across diverse tasks. The approach demonstrates robust generalization, real-time feasibility, and strong cross-subject performance.

## Method Summary
The authors propose a physiology-informed multi-band tokenization (PiMT) framework for task-agnostic ExG representation learning. The method decomposes raw multi-channel ExG signals into 12 physiologically defined frequency sub-bands (e.g., EEG-delta 0.5-4 Hz, EMG-LF 15-45 Hz) using fixed filter banks. These sub-band signals are then segmented into patches and projected into embeddings. A Bidirectional-Mamba encoder processes these embeddings, chosen for its linear-time complexity compared to Transformers. The model is pre-trained on 50 hours of free-living ExG data using six reconstruction tasks (autoencoding, masked reconstruction, FFT amplitude/phase recovery), then fine-tuned on 20 hours of task-specific data across four public benchmarks (DREAMER, SEED, Sleep-EDF, open-ear challenge).

## Key Results
- Achieves 87% average F1-score across diverse ExG tasks, outperforming state-of-the-art methods
- Demonstrates robust generalization with consistent performance across different tasks and subjects
- PiMT tokenization provides 6.4% improvement over Transformer-based models due to efficient handling of long sequences
- Classification performance saturates at ~30% of pre-training data while regression continues improving up to 50%

## Why This Works (Mechanism)

### Mechanism 1: Physiological Sub-band Decoupling
The PiMT module applies a bank of 12 fixed filters to raw signals, creating a 3D tensor (frequency band × channel × time). By segregating these frequencies explicitly, the model receives disentangled spectral features, allowing it to attend to relevant bands dynamically via learnable weights rather than hard-coded preprocessing. This assumes distinct physiological phenomena manifest primarily in non-overlapping frequency ranges.

### Mechanism 2: Unsupervised Spectral-Temporal Reconstruction
The model undergoes pre-training using six distinct reconstruction objectives, including masked autoencoding and frequency-domain (FFT) recovery. This "denoising" objective compels the Bidirectional-Mamba encoder to capture the underlying generative factors of the ExG signals rather than dataset-specific artifacts or labels.

### Mechanism 3: Linear-Complexity Sequence Modeling (Mamba)
Multi-band tokenization inflates the input sequence length. Mamba uses State Space Models (SSMs) to model dependencies with linear complexity O(L) relative to sequence length L, rather than the O(L²) of Transformers. This preserves the ability to model long-range temporal dependencies essential for biosignals.

## Foundational Learning

- **Concept: Spectral Decomposition / Filter Banks**
  - **Why needed here:** The core innovation (PiMT) relies on the assumption that splitting signals by frequency before the neural network is superior to letting the network learn filters from raw data. You must understand the Fourier-domain implications of the 12-band split.
  - **Quick check question:** Why would a model struggle to learn the 15-45 Hz EMG features if trained only on a raw 0-100 Hz signal without this explicit decomposition?

- **Concept: Self-Supervised Reconstruction (Masked Autoencoding)**
  - **Why needed here:** The system uses unlabeled "free-living" data. Understanding how masking ~50% of patches and forcing the model to reconstruct them teaches the model "grammar" (temporal structure) without labels is critical to grasping the "Foundation Model" aspect.
  - **Quick check question:** If the model successfully reconstructs masked patches, does that guarantee it has learned high-level semantic features (like "focus") or just low-level signal continuity?

- **Concept: State Space Models (Mamba/SSMs)**
  - **Why needed here:** The authors argue Transformers are too slow for the long sequences generated by their tokenization method. Understanding the difference between O(N²) attention and O(N) recurrence is necessary to evaluate the architectural choice.
  - **Quick check question:** How does the "scanning" direction (frequency-first vs. time-first) in a recurrent model like Mamba potentially impact the learning of simultaneous multi-band features compared to a parallel Transformer attention layer?

## Architecture Onboarding

- **Component map:** Raw ExG → Preprocessing (Notch/Resample) → Tokenization (PiMT: Filter Bank → Patching → Linear Projection) → 3D Embeddings → Bidirectional-Mamba Encoder → Pre-training Heads (6 MLP decoders) → Fine-tuning Heads (Task-specific Classifier/Regressor)

- **Critical path:** The Filter Bank design. If the fixed frequency bands do not align with the actual physiological ranges of the target downstream task, the tokenization will fragment the feature, rendering the model blind to it.

- **Design tradeoffs:**
  - Fixed vs. Learnable Filters: The paper uses fixed physiological bands, enforcing strong priors but lacking flexibility if the user's physiology deviates.
  - Mamba vs. Transformer: Mamba is faster (linear time) but newer/harder to debug than Transformers.
  - Pre-training Scale: Classification saturates at ~30% of pre-training data; regression keeps improving.

- **Failure signatures:**
  - Spectral Leakage/Artifacts: If reconstruction loss drops but downstream accuracy is low, the model may have learned to reconstruct power-line noise or motion artifacts.
  - Cross-Subject Collapse: The paper notes a performance drop in LOSO. If a new user has different skull thickness or ear shape, the impedance changes, potentially shifting the signal amplitude/phase outside the learned distribution.
  - Latency Spikes: If implementing on the suggested smartphone companion device, ensure the Mamba kernel is optimized for mobile CPU/GPU.

- **First 3 experiments:**
  1. **Tokenization Validity Check:** Run the model on a known dataset (e.g., Sleep-EDF) using (a) Raw single-band input vs. (b) PiMT 12-band input. Verify that the F1-score gap supports the overhead of the filter bank.
  2. **Pre-training Ablation:** Train the model from scratch (random init) vs. loading the provided pre-trained weights on DailySense. Quantify the "free-living data dividend" (e.g., 5% gain vs. 0% gain).
  3. **Saliency Verification:** For a specific task (e.g., Gaze), visualize the attention/activation map over the 12 frequency bands. Confirm that the model attends to EOG/EEG bands (low freq) and not EMG bands (high freq) for this specific task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ExG foundation models bridge the performance gap between subject-agnostic pre-training and robust cross-subject generalization?
- Basis in paper: The authors state in the Discussion that "performance drops when training and testing on different users highlight the persistent challenge of personalized modeling." Additionally, Appendix L reports a significant performance drop (to 58.6% F1) in cross-subject settings compared to within-session.
- Why unresolved: While the model generalizes well in Leave-One-Subject-Out (LOSO) pre-training splits, it struggles when the test subject is entirely excluded from the training pipeline, a common hurdle for real-world deployment.
- What evidence would resolve it: Demonstrating a training strategy (e.g., personalization algorithms or meta-learning) that narrows the performance gap between within-session and cross-subject evaluations on the DailySense dataset.

### Open Question 2
- Question: What are the optimal data scaling limits for task-agnostic ExG pre-training across classification versus regression tasks?
- Basis in paper: Section 5.5 ("Impact of Pre-training Data Scale") notes that while classification performance saturates at roughly 30% of the pre-training data, regression tasks (gaze tracking) continue to improve up to 50%. The authors leave the long-term scaling behavior for larger datasets unexplored.
- Why unresolved: The study is limited to 50 hours of data, which is insufficient to determine if regression tasks would eventually saturate or if classification tasks require different data diversity rather than just volume.
- What evidence would resolve it: A follow-up study scaling pre-training data beyond 50 hours to plot performance curves until both task types reach a clear plateau.

### Open Question 3
- Question: Can the Bidirectional-Mamba architecture achieve its theoretical efficiency advantages on resource-constrained edge devices for real-time ExG inference?
- Basis in paper: Appendix K notes that "Because the Mamba architecture is not yet supported on Android and lacks corresponding hardware acceleration, we substituted Mamba with a Transformer" for efficiency analysis.
- Why unresolved: The paper claims Mamba is suitable for linear-time complexity but could not verify its actual on-device runtime on the target smartphone platform due to software library limitations.
- What evidence would resolve it: Implementation of Mamba kernels for mobile chipsets (e.g., via TensorFlow Lite or dedicated accelerators) showing latency and memory usage comparable to or better than the Transformer baseline on the Samsung Galaxy S24.

## Limitations
- The DailySense dataset used for pre-training is not publicly available, preventing exact reproduction of results
- Fixed 12-band decomposition may not capture task-specific features that span multiple bands or fall outside defined ranges
- Significant performance degradation (87.6% to 58.6% F1) in cross-subject settings indicates domain shift challenges

## Confidence

- **High Confidence:** The PiMT tokenization mechanism and its physiological basis are well-supported by the literature on ExG signal characteristics. The architectural choice of Mamba over Transformer for long sequences is technically sound.
- **Medium Confidence:** The pre-training methodology using masked autoencoding and frequency reconstruction is standard in self-supervised learning, but its specific effectiveness for ExG signals depends heavily on the quality of the unlabeled DailySense data.
- **Low Confidence:** The absolute performance numbers (87% F1) cannot be independently verified without access to DailySense, and the generalization claims rely on this dataset being representative of real-world "free-living" conditions.

## Next Checks

1. **Public Dataset Replication:** Implement PiMT on a publicly available ExG dataset (e.g., Sleep-EDF) and compare single-band vs. multi-band performance to validate the tokenization overhead is justified.

2. **Pre-training Ablation Study:** Train the model from scratch on limited labeled data versus using pre-trained weights (if available from other sources) to quantify the "free-living data dividend."

3. **Cross-Subject Robustness Test:** Evaluate the model on a dataset with known cross-subject variability (e.g., SEED with multiple subjects) to measure the magnitude of performance degradation and test domain adaptation strategies.