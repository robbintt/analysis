---
ver: rpa2
title: Incorporating Structure and Chord Constraints in Symbolic Transformer-based
  Melodic Harmonization
arxiv_id: '2512.07627'
source_url: https://arxiv.org/abs/2512.07627
tags:
- chord
- tokens
- melody
- algorithm
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces B, an algorithm that combines beam search,
  A, and backtracking to enforce user-specified chord constraints during melody harmonization.
  Standard autoregressive models struggle to incorporate constraints at precise positions,
  as they cannot look ahead or deviate from learned statistical priors.
---

# Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization

## Quick Facts
- arXiv ID: 2512.07627
- Source URL: https://arxiv.org/abs/2512.07627
- Reference count: 10
- Primary result: B* algorithm achieves >90% chord constraint satisfaction via beam search, A*, and backtracking

## Executive Summary
This paper introduces B*, an algorithm that combines beam search, A*, and backtracking to enforce user-specified chord constraints during melody harmonization. Standard autoregressive models struggle to incorporate constraints at precise positions, as they cannot look ahead or deviate from learned statistical priors. B* explores multiple candidate sequences and backtracks when necessary to ensure constraints are satisfied, overcoming this limitation. Experiments show that simply appending constraint tokens (soft constraints) improves satisfaction rates but still falls short; B* achieves over 90% success for chord symbol tokenization and over 98% for pitch-class tokenization within 10,000 model calls. However, B* often resorts to sub-optimal sequences, degrading musical coherence relative to unconstrained generation.

## Method Summary
The method trains transformer models (BART and GPT-2) on lead sheets from Hook Theory, using two tokenization schemes: ChordSymbolTokenizer (single token per chord) and PitchClassTokenizer (multiple tokens per chord). During training, constraint tokens are appended to melody inputs as soft constraints. At inference, B* decoding explores multiple candidate sequences using beam search with backtracking: it maintains a priority queue of partial sequences, expands top candidates with model predictions, checks constraint satisfaction at target positions, and backtracks when paths fail. The scoring function combines log-probability with an overcrowding penalty to prevent excessive tokens before constraint positions.

## Key Results
- B* achieves >90% constraint satisfaction for chord symbol tokenization and >98% for pitch-class tokenization within 10,000 model calls
- Soft constraint encoding improves satisfaction rates 2-3x compared to blind generation (from ~22-27% to ~55-60%)
- GPT-2 slightly outperforms BART for B* decoding across tokenizers, with 290-400 average model calls for chord symbol constraints
- Constrained generation degrades musical coherence: CHE drops from 1.36 to 0.75, CTD from 0.90 to 0.72, HRHE from 0.69 to 0.15 compared to ground truth

## Why This Works (Mechanism)

### Mechanism 1: B* Search with Backtracking for Hard Constraint Enforcement
The B* algorithm achieves >90% constraint satisfaction by exploring multiple token sequences and backtracking when paths fail, overcoming the limited lookahead of standard autoregressive decoding. B* maintains a priority queue (open set Q) of partial sequences, expands the top-b scored sequences with model top-k predictions at each step, checks for constraint satisfaction when sequences reach the target bar/position, and backtracks to parent nodes to try alternative continuations when children fail. This allows exploration of sub-optimal but constraint-satisfying paths that beam search alone would discard. Core assumption: Constraint tokens may appear outside the model's learned statistical priors (e.g., secondary dominants), requiring exploration of less probable paths that the model would not "naturally" select.

### Mechanism 2: Soft Constraint Encoding via Structured Input Tokens
Appending chord constraint tokens to the melody input improves satisfaction rates 2-3x compared to blind generation, but remains insufficient (~55-60%) because transformers default to learned statistical priors over explicit instructions. During training, the model learns to associate structured prompts (bar markers, position tokens, chord tokens placed between </m> and <h> delimiters) with harmonic sequences containing those chords. At inference, the same prompt structure is provided with <fill> tokens indicating "generate here" and actual chord tokens indicating "use this specific chord." Core assumption: The model has seen sufficient training examples where constraint tokens were honored and will generalize this conditional behavior.

### Mechanism 3: Scoring Function with Overcrowding Penalty
The scoring function score = Σ log P(t|context) - |S|/(|S==<bar>|+1) prevents the algorithm from accumulating excessive tokens before reaching the constraint position. Without the penalty, longer sequences accumulate more log-probability contributions, biasing the algorithm toward sequences that are "overcrowded" with chords in bars preceding the constraint. The penalty normalizes by bar count, favoring sequences that progress efficiently toward the constraint. Core assumption: The model's log-probabilities correlate with musical quality, and the ratio of total tokens to bar tokens captures overcrowding behavior.

## Foundational Learning

- **Concept: Autoregressive generation with beam search**
  - Why needed here: The B* algorithm extends standard beam search; understanding how beam search maintains and prunes candidate sequences (keeping top-b by cumulative probability) is prerequisite to grasping the A* and backtracking additions.
  - Quick check question: Given beam width b=4 and top-k=2 token expansions per sequence, what is the maximum number of candidate sequences evaluated after one expansion step?

- **Concept: A* search with priority queues and open sets**
  - Why needed here: B* uses an open set (priority queue Q) to store all explored but not fully expanded nodes, unlike beam search which discards low-probability paths. The scoring function combines path cost (log-probability) with heuristics (overcrowding penalty).
  - Quick check question: In A* search, what is the role of the open set versus the closed set, and which does B* primarily rely on?

- **Concept: Musical tokenization schemes (symbolic vs. constituent-based)**
  - Why needed here: The paper compares ChordSymbolTokenizer (single token per chord) versus PitchClassTokenizer (spells out chord notes), which directly impacts sequence length, constraint position index C, and B* complexity O(|V|^C).
  - Quick check question: If a Cmaj7 chord is encoded as [chord_pc_0, chord_pc_4, chord_pc_7, chord_pc_11] versus a single "C:maj7" token, how does this affect the worst-case complexity of reaching a constraint at position C?

## Architecture Onboarding

- **Component map:**
  - Input processor -> Transformer model -> B* decoder -> Constraint checker -> Scoring module
  - Melody tokens + structure/constraint tokens -> BART/GPT-2 small -> Priority queue Q + beam set B -> Well-formedness validation + constraint satisfaction -> Log-probability + overcrowding penalty

- **Critical path:**
  1. **Initialization:** Q contains initial empty sequence (GPT-2: melody tokens + `<h>`; BART: just `<h>`)
  2. **Expansion loop:** Pop top-b from Q → Run model to get top-k next tokens → For each token: extend sequence, check well-formedness, check constraint if at target bar → Add valid children to B
  3. **Queue update:** Transfer all B nodes to Q; if any sequence satisfies constraint AND ends with EOS token, return it
  4. **Backtrack:** If no valid children found, expand parent node with next unvisited top-k tokens

- **Design tradeoffs:**
  - **Beam width (b) vs. speed:** Larger b explores more paths in parallel but increases memory and computation; paper uses b=4
  - **Lookahead (k) vs. diversity:** Larger k considers more tokens per expansion, potentially finding constraint-satisfying paths faster; paper uses k=2
  - **CS vs. PC tokenization:** CS has shorter sequences (average 37.6 tokens) → faster B*; PC has longer sequences (67.4 tokens) but more flexibility. Empirical: GPT-2 CS achieves 96% success with only 290 avg model calls; BART PC requires 1925 avg calls
  - **Model architecture:** GPT-2 slightly more efficient for B* than BART across tokenizers (Table 2 model calls)

- **Failure signatures:**
  - **Exponential blow-up:** When constraint is at late position C with large vocabulary |V|, worst-case complexity O(|V|^C); monitor model call count
  - **Overcrowded generation:** If scoring penalty is too weak, average tokens per bar increases; check |S|/|S==<bar>| ratio
  - **Constraint deadlock:** No well-formed path reaches constraint within 10,000 calls → return failure; happens 2-10% of cases
  - **Musical incoherence:** B* accepts sub-optimal sequences to satisfy constraints, degrading CHE (~0.75 vs 1.36 GT), CTD (~0.72 vs 0.90 GT), HRHE (~0.15 vs 0.69 GT) per Table 5

- **First 3 experiments:**
  1. **Reproduce soft constraint baseline:** Train BART and GPT-2 models with soft constraint tokens in input (Section 2.1.3 protocol), measure satisfaction rate on 1520-piece test set. Expected: ~55-62% satisfaction vs ~20-27% for unconstrained generation.
  2. **Implement B* with b=4, k=2:** Apply B* algorithm to soft-constraint-trained models, measure satisfaction rate and average model calls. Expected: >90% success (CS) / >98% success (PC) within 10,000 calls; ~290-400 model calls for GPT-2 CS.
  3. **Ablate overcrowding penalty:** Remove the `-|S|/(|S==<bar>|+1)` term from scoring, compare average tokens per bar and convergence speed. Expected: Increased tokens per bar in pre-constraint bars, potentially slower convergence due to longer sequences dominating the queue.

## Open Questions the Paper Calls Out
None

## Limitations
- The B* algorithm's exponential worst-case complexity O(|V|^C) remains a practical concern despite empirical success within 10,000 model calls
- Constrained generation degrades musical coherence (CHE drops from 1.36 to 0.75, CTD from 0.90 to 0.72, HRHE from 0.69 to 0.15) as B* resorts to sub-optimal sequences
- Reproducibility is limited by unspecified "well-formedness heuristics" and missing training hyperparameters

## Confidence

**High Confidence (8-10/10):** The experimental results showing B* achieving >90% constraint satisfaction within 10,000 model calls are well-supported by the data presented. The methodology for measuring constraint satisfaction and musical coherence metrics is clearly specified.

**Medium Confidence (5-7/10):** The theoretical analysis of B*'s complexity and the effectiveness of the overcrowding penalty scoring function are supported by empirical results but lack rigorous theoretical justification.

**Low Confidence (2-4/10):** The reproducibility of the well-formedness heuristics and exact training procedures is uncertain due to missing specification.

## Next Checks

1. **Well-formedness Heuristic Validation:** Implement and test the unspecified "basic heuristics of well-formedness" by systematically generating invalid sequences that should be pruned (e.g., chords in the middle of position tokens, malformed chord progressions). Measure the impact on B* efficiency and constraint satisfaction rates.

2. **Overcrowding Penalty Sensitivity Analysis:** Conduct an ablation study varying the overcrowding penalty coefficient in the scoring function. Measure changes in average tokens per bar, model calls to convergence, and constraint satisfaction rates. Determine whether the current penalty value is optimal or if alternative formulations could improve performance.

3. **Constraint Type Coherence Degradation Analysis:** Systematically categorize constraints by type (e.g., secondary dominants, modal mixture, chromatic mediants) and measure the corresponding degradation in musical coherence metrics. Identify whether certain harmonic functions are inherently more difficult to satisfy while maintaining musical quality, and whether this suggests fundamental limitations of the B* approach.