---
ver: rpa2
title: Dynamic Vision Mamba
arxiv_id: '2504.04787'
source_url: https://arxiv.org/abs/2504.04787
tags:
- token
- block
- pruning
- tokens
- dyvm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the inefficiency in Vision Mamba models caused
  by spatial redundancy at both token and block levels. The authors propose Dynamic
  Vision Mamba (DyVM), which customizes token pruning for Mamba's recurrent structure
  by rearranging pruned tokens before feeding them to the next block, avoiding training-inference
  inconsistency and extra computation.
---

# Dynamic Vision Mamba

## Quick Facts
- **arXiv ID**: 2504.04787
- **Source URL**: https://arxiv.org/abs/2504.04787
- **Reference count**: 40
- **Key outcome**: Achieves 35.2% FLOPs reduction with only 1.7% accuracy loss on Vim-S by dynamically pruning tokens and blocks

## Executive Summary
This work addresses the inefficiency in Vision Mamba models caused by spatial redundancy at both token and block levels. The authors propose Dynamic Vision Mamba (DyVM), which customizes token pruning for Mamba's recurrent structure by rearranging pruned tokens before feeding them to the next block, avoiding training-inference inconsistency and extra computation. Additionally, DyVM dynamically selects SSM blocks for each image, reducing the number of active blocks based on empirical observation that inference speed is largely affected by the number of SSM blocks. DyVM achieves a 35.2% reduction in FLOPs with only 1.7% accuracy loss on Vim-S, generalizes well across different Mamba vision model architectures and tasks, and demonstrates consistent performance improvements on VideoMamba, MambaReg, and semantic segmentation tasks.

## Method Summary
DyVM implements two complementary efficiency mechanisms: token pruning with rearrangement and dynamic block selection. Token pruning uses lightweight predictors to score tokens at three stages during the network, then rearranges retained tokens to the front to maintain training-inference consistency in the recurrent SSM structure. Dynamic block selection uses predictors to decide whether forward and backward SSM blocks should be active for each sample. The approach is trained with knowledge distillation from the original model and achieves significant efficiency gains while maintaining accuracy across multiple Mamba-based architectures and tasks.

## Key Results
- 35.2% reduction in FLOPs on Vim-S with only 1.7% accuracy loss
- Consistent performance improvements across VideoMamba, MambaReg, and semantic segmentation tasks
- Generalizes well to different Mamba vision model architectures
- Dynamic block selection provides disproportionate throughput improvements (2.83Ã— speedup) compared to FLOP reduction

## Why This Works (Mechanism)

### Mechanism 1: Sequence Consistency via Rearrangement
The core innovation addresses a fundamental mismatch in recurrent SSMs: if pruned tokens are masked in-place during training, the recurrent state evolution of retained tokens becomes inconsistent between training and inference. By rearranging retained tokens to the front of the sequence, DyVM ensures the i-th retained token always undergoes the exact same number of transformations during both training and inference. This solves the index/history mismatch that would otherwise cause significant accuracy drops.

### Mechanism 2: Throughput Optimization via Block Skipping
Vision Mambas use bidirectional SSMs, and the authors observe that the sheer number of scanning blocks acts as a throughput bottleneck. By using a block selector to dynamically deactivate forward or backward blocks for specific samples, the system reduces sequential load on hardware, speeding up processing more effectively than FLOP counts alone would suggest. This hardware-aware optimization recognizes that latency is dominated by sequential depth rather than arithmetic operations.

### Mechanism 3: Gradual Token Pruning
Abruptly removing tokens shocks the representation. DyVM uses a multi-stage approach where predictors gradually identify and rearrange less informative tokens to the end of the sequence. This hierarchical approach preserves accuracy better than single-stage pruning by distributing redundancy reduction across the network depth.

## Foundational Learning

- **Discrete State Space Models (SSMs) & Mamba**: Understanding how Mamba maps a 1D input sequence to an output via a recurrent hidden state $h(t)$ is crucial. Without this, the "index mismatch" problem Mechanism 1 solves is unintelligible. *Quick check*: Explain why changing the index position of a token in a sequence affects its final representation in a recurrent SSM but might not in a standard Transformer.

- **Gumbel-Softmax Trick**: The paper uses this for "differentiable sampling" to generate binary masks for token pruning. You need to know how this allows backpropagation through discrete decisions. *Quick check*: How does Gumbel-Softmax allow a model to learn which tokens to drop if "dropping" is a non-differentiable discrete act?

- **Knowledge Distillation**: DyVM uses the original, unpruned model as a "teacher" to correct the pruned student. *Quick check*: Why is distillation necessary here if we are simply removing "redundant" tokens?

## Architecture Onboarding

- **Component map**: Input Image -> Patch Embedding -> Initial blocks -> Stage s: Predictor scores tokens -> Gumbel-Softmax generates Mask -> Rearrange: [Retained | Class | Pruned] -> Block Selection: Skip Forward/Backward SSMs -> Repeat for next stages -> Output -> Classification Head + Distillation Loss

- **Critical path**: 1) Input passes through initial blocks 2) At each stage (6, 12, 18), predictor scores tokens 3) Gumbel-Softmax generates binary masks 4) Rearrange logic moves high-score tokens to front 5) Block selector decides if Forward/Backward SSMs are skipped 6) Process repeats through network 7) Final output goes to classification head with distillation loss

- **Design tradeoffs**: FLOPs vs. Throughput (token pruning reduces FLOPs, block selection targets throughput), Ratio ($\rho$) aggressiveness (aggressive token ratios drop accuracy significantly; block skipping is safer for accuracy but reduces FLOPs less)

- **Failure signatures**: Accuracy Collapse (check if rearrangement step is implemented correctly; in-place masking causes training to diverge), Stagnant Throughput (ensure block selection actually skips execution rather than just multiplying by zero)

- **First 3 experiments**: 1) Train small Vim-T model with "plain masking" vs. "DyVM rearrangement" to verify training-inference mismatch causes accuracy failure, 2) Run DyVM with only token pruning vs. only block pruning to compare FLOP reduction vs. accuracy drop curves, 3) Measure actual images/sec on A100/V100 to verify block reduction yields disproportionate speedups

## Open Questions the Paper Calls Out

1. **Adapting token pruning for dense prediction**: For semantic segmentation, pruned tokens must be "restored" to original positions to form complete feature maps, negating memory savings and introducing extra processing steps for dense predictions.

2. **Applying rearrangement to multi-directional scanning**: The rearrangement strategy may break if applied to architectures like Cross-Scan in VMamba that use different spatial scanning patterns, as the contiguous block logic relies on specific sequential propagation.

3. **Balancing spatial and computational redundancy**: The paper makes an early effort to reduce scanning blocks and calls for future studies to maintain good balance when designing new vision mamba architectures from scratch.

## Limitations

- The rearrangement mechanism's necessity depends on precise assumptions about recurrent SSMs' sensitivity to token position history, though empirical evidence supports it
- The 35.2% FLOPs reduction claim is demonstrated primarily on Vim-S, with generalization to architecturally similar Mamba variants
- Throughput claims appear hardware-dependent, and the paper doesn't provide detailed profiling across different GPU configurations

## Confidence

**High Confidence**: The fundamental claim that spatial redundancy exists in Mamba models and can be reduced through token/block pruning, and that the training-inference consistency problem is well-documented

**Medium Confidence**: The specific architecture details (predictor dimensions, Gumbel temperature schedules) and their optimal settings, as the paper provides sufficient detail but leaves some hyperparameters unspecified

**Low Confidence**: The universality of the approach across all SSM architectures and tasks, as the methodology may not transfer equally well to regression tasks or non-image modalities

## Next Checks

1. **Cross-Architecture Validation**: Test DyVM on a non-Mamba SSM architecture (e.g., liquid time-constant models or traditional RNNs) to verify whether the rearrangement mechanism is universally necessary or Mamba-specific

2. **Hardware Profiling**: Implement both "skip execution" and "multiply by zero" versions of block selection, then measure actual throughput on multiple GPU types (A100, V100, RTX 4090) to validate the claimed hardware-dependent speedups

3. **Extreme Pruning Boundary**: Systematically test token ratios below 0.5 (e.g., 0.3, 0.2) to identify the breaking point where accuracy collapses, and determine whether this is due to the pruning mechanism itself or the model's fundamental capacity limits