---
ver: rpa2
title: Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)
arxiv_id: '2509.05505'
source_url: https://arxiv.org/abs/2509.05505
tags:
- medical
- biomedical
- system
- retrieval
- mistral-7b-v0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a biomedical question answering system based
  on Retrieval-Augmented Generation (RAG) to improve access to accurate, evidence-based
  medical information. The system integrates PubMed articles, curated Q&A datasets,
  and medical encyclopedias using MiniLM-based embeddings and FAISS vector search,
  with answer generation performed by a fine-tuned Mistral-7B-v0.3 model optimized
  using QLoRA.
---

# Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)

## Quick Facts
- arXiv ID: 2509.05505
- Source URL: https://arxiv.org/abs/2509.05505
- Reference count: 7
- Introduces RAG-based biomedical Q&A system that achieves BERTScore F1 up to 0.90 on domain-specific queries

## Executive Summary
This work introduces a biomedical question answering system based on Retrieval-Augmented Generation (RAG) to improve access to accurate, evidence-based medical information. The system integrates PubMed articles, curated Q&A datasets, and medical encyclopedias using MiniLM-based embeddings and FAISS vector search, with answer generation performed by a fine-tuned Mistral-7B-v0.3 model optimized using QLoRA. Evaluation on breast cancer literature and general medical queries shows that RAG significantly improves semantic relevance and factual consistency compared to baseline models, with BERTScore (F1) reaching up to 0.90 in domain-specific contexts.

## Method Summary
The system employs a RAG pipeline where biomedical documents are chunked via Recursive Character Text Splitting, embedded with multi-qa-MiniLM-L6-cos-v1, and indexed in FAISS for cosine similarity retrieval. User queries are processed through the same embedding model, retrieving top-5 document chunks that serve as context for Mistral-7B-v0.3 generation. The model is fine-tuned using QLoRA on MedQuAD with instruction-tuned prompts to enhance biomedical terminology usage and response completeness. The approach is evaluated using BERTScore F1 against reference answers, with domain-specific corpora showing improved performance over general medical corpora.

## Key Results
- BERTScore F1 reaches 0.90 on breast cancer-specific queries versus 0.83-0.84 on general medical corpora
- RAG significantly improves semantic relevance and factual consistency compared to baseline models
- Fine-tuned Mistral-7B-v0.3 with QLoRA demonstrates enhanced biomedical terminology usage and answer completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation reduces factual hallucinations by grounding generation in verified biomedical sources.
- Mechanism: User queries are embedded using `multi-qa-MiniLM-L6-cos-v1`, matched via FAISS cosine similarity to retrieve top-5 document chunks, which are concatenated to the prompt as context before answer generation.
- Core assumption: The retrieval corpus contains accurate, relevant information; embedding quality sufficiently captures semantic meaning for medical queries.
- Evidence anchors:
  - [abstract] "The retrieval pipeline uses MiniLM-based semantic embeddings and FAISS vector search...RAG significantly improves semantic relevance and factual consistency compared to baseline models."
  - [section 3.1.1] "All vanilla Mistral-7B variants exhibited a notable limitation: the lack of access to domain-based biomedical sources. This constraint frequently resulted in hallucinated or shallow responses."
  - [corpus] Related work (Bora and Cuayáhuitl, 2024) confirms RAG with fine-tuned Mistral-7B achieved 57% exact match accuracy; corpus literature consistently identifies RAG as a key factor in reducing hallucinations.

### Mechanism 2
- Claim: Domain-specific corpus alignment improves semantic relevance and factual accuracy more than general medical corpora.
- Mechanism: Specialized breast cancer corpus was scraped from PubMed, chunked with NLTK sentence-aware splitting, and indexed separately; retrieval from this domain-aligned index yielded higher BERTScore F1 (0.88–0.90) compared to general medical corpus (0.83–0.84).
- Core assumption: Domain specificity correlates with answer quality when evaluation queries match the retrieval corpus scope.
- Evidence anchors:
  - [abstract] "Evaluation on breast cancer literature and general medical queries shows that RAG significantly improves semantic relevance...with BERTScore (F1) reaching up to 0.90 in domain-specific contexts."
  - [section 4.2] "The integration of breast cancer–specific documents into the retrieval pipeline led to a substantial improvement in performance relative to general medical corpora, with BERTScore F1 increasing from approximately 0.83–0.84 to 0.88–0.90."
  - [corpus] RAG-BioQA and MedBioRAG papers similarly report that domain-aligned retrieval enhances biomedical QA performance, though specific metrics vary by dataset and model.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (QLoRA) on biomedical QA pairs improves terminology usage and response completeness.
- Mechanism: Mistral-7B-v0.3 was fine-tuned using QLoRA (4-bit precision) on the MedQuAD dataset with instruction-tuned prompts, enabling domain adaptation on a single A100 GPU without full parameter updates.
- Core assumption: The fine-tuning dataset (MedQuAD) accurately represents target biomedical QA distribution; QLoRA preserves base model capabilities while adapting domain-specific behavior.
- Evidence anchors:
  - [abstract] "Answer generation is performed by a fine-tuned Mistral-7B-v0.3 model optimized using QLoRA."
  - [section 3.3] "The fine-tuned model demonstrated: Enhanced relevance and completeness of generated answers; Improved utilization of biomedical terminology; Higher BERTScore metrics compared to both zero-shot and retrieval-only variants."
  - [corpus] Limited direct corpus evidence on QLoRA-specific biomedical fine-tuning; related work focuses on LoRA/general fine-tuning approaches rather than QLoRA specifically.

## Foundational Learning

- **Dense Vector Retrieval (Embeddings + Similarity Search)**
  - Why needed here: Understanding how semantic search converts text to vectors and retrieves relevant chunks is foundational to debugging retrieval quality, which directly impacts generation grounding.
  - Quick check question: Given two medical queries with different wording but identical intent (e.g., "treatment for hypertension" vs. "how to lower blood pressure"), will MiniLM embeddings return similar top-k chunks? What could cause divergence?

- **Parameter-Efficient Fine-Tuning (LoRA/QLoRA)**
  - Why needed here: The system relies on QLoRA for domain adaptation; understanding low-rank adaptation and quantization is essential for reproducing training or extending to new biomedical subdomains.
  - Quick check question: If fine-tuning on a new dataset causes the model to forget general medical knowledge (catastrophic forgetting), which QLoRA hyperparameters would you adjust first?

- **BERTScore Evaluation for Semantic Similarity**
  - Why needed here: The paper uses BERTScore F1 as the primary metric; understanding its limitations (e.g., sensitivity to paraphrasing, lack of factual correctness verification) is critical for interpreting results and designing better evaluations.
  - Quick check question: A generated answer achieves high BERTScore F1 but contains a medically incorrect statement. Does BERTScore capture this error? What additional evaluation would be needed?

## Architecture Onboarding

- **Component map:**
  Documents → JSON preprocessing → Recursive Character Text Splitting → Embeddings (multi-qa-MiniLM-L6-cos-v1) → FAISS index → Retrieval (top-5) → Prompt construction → Generation (Mistral-7B-v0.3) → BERTScore evaluation

- **Critical path:**
  Retrieval quality (chunk coherence, embedding relevance) → Context window fit (top-5 chunks must fit within token limit) → Generation grounding (model must attend to retrieved context) → Answer conciseness (prompt enforces 3–4 sentences). Failures cascade: poor retrieval → irrelevant context → hallucinated or generic answers.

- **Design tradeoffs:**
  - Chunk size: Larger chunks preserve context but may exceed token limits; smaller chunks improve retrieval precision but may fragment meaning. Paper selected Recursive Character Text Splitting empirically.
  - Top-k retrieval: k=5 balances context richness with token constraints; higher k risks noise, lower k risks missing relevant evidence.
  - QLoRA vs. LoRA: QLoRA enables 4-bit training on single A100; LoRA with 8-bit quantization encountered memory overflows. Tradeoff is potential precision loss in adaptation.
  - General vs. domain-specific corpus: Domain-specific (breast cancer) yields higher BERTScore (0.88–0.90) but reduces generalizability; general corpus maintains breadth at lower peak performance.

- **Failure signatures:**
  - Empty/irrelevant retrieved chunks → Model generates generic or hallucinated answers (check FAISS index quality, embedding model alignment).
  - Token overflow errors → Chunk size or top-k exceeds context window (reduce chunk overlap or k value).
  - Fine-tuned model produces verbose or off-topic responses → Prompt format mismatch between training and inference (verify instruction-tuned prompt consistency).
  - High BERTScore but medically incorrect answers → Evaluation metric limitation; BERTScore measures semantic similarity, not factual accuracy (add expert review or fact-checking layer).

- **First 3 experiments:**
  1. **Baseline validation:** Run vanilla Mistral-7B-v0.3 (zero-shot) on 20 medical queries from the evaluation set; confirm hallucination patterns and establish BERTScore baseline (~0.838 expected per paper).
  2. **Retrieval ablation:** Disable FAISS retrieval (empty context) for same queries; compare answer quality to confirm retrieval contribution. Then test k=3, k=5, k=10 to identify optimal retrieval depth.
  3. **Domain corpus swap:** Replace general medical corpus with breast cancer corpus for breast cancer queries; measure BERTScore improvement (target: 0.88–0.90). Then test on non-breast-cancer queries to assess generalization loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does high BERTScore performance correlate with clinical safety and utility when validated by medical professionals?
- Basis in paper: [explicit] Section 7 notes the need to improve evaluation "beyond BERTScore by introducing clinically meaningful benchmarks and human-in-the-loop assessments."
- Why unresolved: Semantic similarity does not guarantee the absence of medically dangerous hallucinations or logical errors.
- What evidence would resolve it: Correlation analysis between automated scores and "pass/fail" safety ratings from clinicians.

### Open Question 2
- Question: How does the RAG pipeline perform when extended to multilingual biomedical corpora?
- Basis in paper: [explicit] The authors identify "multilingual adaptation" as necessary to bridge the gap for users across "linguistic and cultural boundaries" (Section 7).
- Why unresolved: The current system uses English-specific embeddings and corpora, limiting applicability to non-English speakers.
- What evidence would resolve it: Performance benchmarks on a multilingual medical QA dataset using cross-lingual embeddings.

### Open Question 3
- Question: Can differential privacy be integrated into the fine-tuning or inference stage without sacrificing answer relevance?
- Basis in paper: [explicit] Future work includes "privacy-preserving inference" to safeguard sensitive health information (Section 7).
- Why unresolved: Privacy techniques typically introduce noise that may reduce the model's ability to retrieve precise biomedical context.
- What evidence would resolve it: Comparing BERTScore drops between standard QLoRA and differential-privacy-enabled training runs.

## Limitations

- Evaluation relies exclusively on BERTScore F1, which measures semantic similarity but cannot detect factual errors or hallucinations in generated answers.
- Domain-specific performance gains were only tested on breast cancer queries, creating uncertainty about generalizability to other medical specialties.
- QLoRA fine-tuning methodology lacks detailed hyperparameter specifications, limiting reproducibility of claimed enhancements.

## Confidence

- **High confidence** in retrieval mechanism effectiveness: Multiple related works confirm that RAG architectures reduce hallucinations when properly grounded in domain corpora.
- **Medium confidence** in domain-specific performance gains: While BERTScore improvements are documented, the evaluation corpus and question distribution are not fully specified.
- **Low confidence** in fine-tuning methodology: Insufficient detail on QLoRA hyperparameters and dataset composition limits reproducibility of the claimed enhancements.

## Next Checks

1. **Factual accuracy validation**: Have medical domain experts review 50 randomly sampled answers to assess whether high BERTScore correlates with medical correctness, not just semantic similarity.
2. **Cross-domain generalization**: Test the system on 100 medical queries spanning multiple specialties (cardiology, neurology, pediatrics) to verify that domain-specific corpus benefits transfer beyond breast cancer.
3. **Ablation study on fine-tuning**: Compare performance across different fine-tuning approaches (full fine-tuning vs. QLoRA with varying ranks) and datasets to determine the actual contribution of parameter-efficient adaptation.