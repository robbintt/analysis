---
ver: rpa2
title: 'SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis'
arxiv_id: '2602.00249'
source_url: https://arxiv.org/abs/2602.00249
tags:
- object
- evaluation
- attribute
- prompt
- saneval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SANEval introduces a comprehensive benchmark for evaluating text-to-image
  (T2I) models'' compositional capabilities, addressing limitations in existing methods
  such as fixed vocabularies and lack of diagnostic feedback. It employs a hybrid
  pipeline combining large language models (LLMs) for prompt understanding and LLM-enhanced
  open-vocabulary object detection to enable scalable, interpretable assessment across
  three compositional axes: attribute binding, spatial relationships, and numeracy.'
---

# SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis

## Quick Facts
- **arXiv ID:** 2602.00249
- **Source URL:** https://arxiv.org/abs/2602.00249
- **Reference count:** 33
- **Primary result:** Automated compositional evaluation scores align more closely with human judgments than existing benchmarks (statistically significant Spearman correlations).

## Executive Summary
SANEval introduces a comprehensive benchmark for evaluating text-to-image models' compositional capabilities, addressing limitations in existing methods such as fixed vocabularies and lack of diagnostic feedback. It employs a hybrid pipeline combining large language models (LLMs) for prompt understanding and LLM-enhanced open-vocabulary object detection to enable scalable, interpretable assessment across three compositional axes: attribute binding, spatial relationships, and numeracy. Experiments on six state-of-the-art T2I models demonstrate that SANEval's automated scores align more closely with human judgments than existing benchmarks, achieving statistically significant Spearman correlations across tasks. Ablation studies confirm robustness to LLM backbone choice, and failure-mode feedback enables actionable insights into specific compositional weaknesses.

## Method Summary
SANEval uses a hybrid pipeline combining LLM-based prompt parsing with open-vocabulary object detection to evaluate T2I model outputs on three compositional dimensions. The system first extracts objects, attributes, and spatial relationships from prompts using structured LLM parsing. It then employs synonym expansion via LLM to bridge vocabulary gaps between user prompts and open-vocabulary detectors (YOLO-E). For scoring, it crops detected objects and queries a VLM about attributes, while spatial and numeracy scores are computed using geometric relationships between object centroids and detection counts. The pipeline generates continuous scores (0.0-1.0) per category with diagnostic feedback on failure modes.

## Key Results
- SANEval's automated scores achieve statistically significant Spearman correlations with human judgments across attribute binding, spatial, and numeracy tasks
- The benchmark demonstrates robustness to LLM backbone choice through ablation studies
- Failure-mode feedback provides actionable insights into specific compositional weaknesses
- Open-vocabulary approach detects 50% more unique objects compared to fixed-vocabulary baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-enhanced synonym expansion bridges the "vocabulary mismatch" between open-world prompts and fixed-vocabulary detectors.
- **Mechanism:** The pipeline identifies objects in the prompt, then prompts an LLM to generate synonyms (e.g., "albatross" → "seabird"). It uses an open-vocabulary detector (YOLO-E) to detect these expanded terms. By mapping detected labels back to the prompt concepts, it identifies objects that rigid detectors would miss.
- **Core assumption:** The semantic gap between the user's term and the detector's visual concepts can be bridged by lexical synonyms known to the LLM.
- **Evidence anchors:**
  - [abstract] "LLM-enhanced, open-vocabulary object detector to robustly evaluate... unconstrained by a fixed vocabulary."
  - [section 3.3] "For each, we prompt an LLM to expand the query into multiple synonyms... Mapping this detection back to the synonym set allows the system to correctly verify the presence."
  - [corpus] Corpus context (e.g., *CompAlign*, *Iterative Refinement*) confirms that compositional evaluation relies on robust concept grounding; however, citation signals are weak for this specific paper.
- **Break condition:** If the visual representation of an object is highly distinct (idiosyncratic) and lacks semantic overlap with terms the detector has seen, synonym expansion will fail.

### Mechanism 2
- **Claim:** Decoupling prompt parsing from geometric evaluation creates robustness to phrasing variations.
- **Mechanism:** Instead of using brittle Named Entity Recognition (NER) or fixed dictionaries to parse spatial relations, the system uses an LLM to convert natural language into structured triplets (e.g., `<obj1, relation, obj2>`). This structured representation is then used to compute geometric scores (e.g., centroid positions) from bounding boxes.
- **Core assumption:** The LLM possesses sufficient reasoning capability to normalize diverse linguistic structures (e.g., "to the left of" vs. "left of") into a standard geometric relation.
- **Evidence anchors:**
  - [abstract] "LLM for prompt understanding... enables scalable, interpretable assessment."
  - [section 3.2] "This structured representation forms the foundation of our scoring framework... robust to grammatical errors, stylistic variations."
  - [corpus] *TangramPuzzle* and *DetailMaster* corpus entries highlight the difficulty of spatial/long-prompt reasoning, validating the need for this structural decoupling.
- **Break condition:** If the LLM misinterprets a complex prepositional phrase attachment (e.g., "the dog on the table with a red collar"), the structured triplet extraction will be incorrect, invalidating the geometric check.

### Mechanism 3
- **Claim:** Crop-based VLM interrogation improves attribute binding accuracy by isolating visual attention.
- **Mechanism:** Rather than scoring the whole image (which dilutes signal), the system crops detected objects and asks a VLM targeted questions (e.g., "What is the color?"). It then uses an LLM to score the semantic similarity of the VLM's answer against the expected attribute on a continuous scale (0.0–1.0).
- **Core assumption:** The object detection bounding box is tight enough to exclude distracting visual features from other objects or the background.
- **Evidence anchors:**
  - [abstract] "Failure-mode feedback enables actionable insights into specific compositional weaknesses."
  - [section 3.4] "We first crop the detected objects... This avoids bias by leaking the expected attribute while still granting partial scores."
  - [corpus] *Multi-Modal Language Models as Text-to-Image Model Evaluators* supports the general trend of using VLMs as judges.
- **Break condition:** If the object detector produces loose bounding boxes that include multiple objects, the VLM may attribute features of one object to another (attribute leakage).

## Foundational Learning

- **Concept:** Open-Vocabulary Object Detection (e.g., YOLO-E vs. Standard YOLO)
  - **Why needed here:** Standard detectors fail SANEval's goal because they can only "see" a predefined list (e.g., 80 COCO classes). You must understand that open-vocab detectors encode text prompts to align with image regions, allowing them to detect arbitrary classes.
  - **Quick check question:** If I ask for a "flibberflabber" (a made-up object), can a standard detector find it? (No). Can an open-vocab detector find it if given a text description? (Potentially yes, if grounded).

- **Concept:** Spearman's Rank Correlation
  - **Why needed here:** The paper validates itself by showing its scores rank models similarly to how humans do. Spearman correlation measures monotonic relationships (rankings) rather than absolute values.
  - **Quick check question:** If SANEval gives a model a score of 0.8 and humans give it 0.5, but the ranking relative to other models is the same, is the Spearman correlation high? (Yes).

- **Concept:** Structured Prompt Parsing (Triplets/JSON)
  - **Why needed here:** You cannot evaluate "spatial" or "numeracy" without first decomposing the prompt into entities and relations. Understanding how LLMs convert text to JSON schemas is critical for debugging the "Prompt Understanding Module."
  - **Quick check question:** Given "A red cat under the blue table," what is the spatial triplet? (`<cat, under, table>`).

## Architecture Onboarding

- **Component map:** Input Prompt → **Prompt Parser** (LLM → JSON structure) → **Synonym Expander** (LLM) → **Object Detector** (YOLO-E) → **Crop & VLM Judge** → **LLM Scorer**
- **Critical path:** The **Prompt Understanding Module (Section 3.2)**. If this step fails to extract an object or relation, the rest of the pipeline (detection + scoring) will inevitably report a "False Negative" (missing object), causing cascading failure.
- **Design tradeoffs:** The authors chose a hybrid (OD + VLM) approach over a pure VQA approach (Section 2.3). This trades off the "flexibility" of pure VQA for the "interpretability" and "localization precision" of Object Detection (bounding boxes).
- **Failure signatures:** Check Table 5 (Ablation). The signature of a failure in the "Enhanced OD Module" is a significant drop in "Unique Objects" detected (e.g., 118 → 77). If you see low "Unique Objects" counts in logs, check the synonym mapping logic.
- **First 3 experiments:**
  1. **Reproduce Table 5 (Ablation):** Run the evaluation pipeline on a subset of prompts (e.g., 50) with the synonym expansion disabled (`w/o Syn`). Verify the drop in detected unique objects.
  2. **Sanity Check on SANEval-Hard:** Input a complex metaphor from the "Hard" dataset (Section 3.1.2) into the *Prompt Parser* to see if it hallucinates objects or correctly parses literal components.
  3. **Attribute Binding Verification:** Manually inspect 20 images where the "Attribute Score" was low (Section 3.4). Visually confirm if the bounding boxes were too loose (causing the VLM to see the wrong thing) or if the model actually failed to generate the attribute.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the diagnostic conformity labels generated by SANEval be effectively utilized as a reward signal in Reinforcement Learning from AI Feedback (RLAIF) to remediate specific compositional failure modes in T2I models?
- Basis in paper: [explicit] The Conclusion states that SANEval "provides actionable signals for RL with AI feedback, transforming evaluation into supervision," suggesting this as the primary application for future work.
- Why unresolved: The current work validates the metric's correlation with human judgment but does not implement or validate a training loop using the feedback for model optimization.
- What evidence would resolve it: A study demonstrating that fine-tuning a T2I model using SANEval scores as a reward function results in statistically significant improvements in attribute binding or spatial reasoning metrics.

### Open Question 2
- Question: To what extent does the reliance on 2D bounding box centroids for spatial verification fail to capture complex 3D spatial relationships (e.g., "behind," "inside") involving occlusion?
- Basis in paper: [inferred] Section 3.5 describes the spatial evaluation as a "geometric evaluation function (e.g., verifying whether the centroid of object A lies to the left of object B)," which relies on 2D coordinates.
- Why unresolved: 2D centroid heuristics cannot distinguish depth; an object generated "behind" another may have an overlapping centroid in 2D space, leading the benchmark to potentially misclassify a correct image as a failure.
- What evidence would resolve it: A failure analysis of the spatial module specifically on "depth" or "occlusion" based relationships compared against human evaluation of 3D spatial adherence.

### Open Question 3
- Question: Does the choice of proprietary VLM/LLM backbones (e.g., Gemini, Llama) as judges introduce a "ceiling effect" or systematic bias that limits the ability to evaluate models that may exceed the judge's own reasoning capabilities?
- Basis in paper: [inferred] While Section 4.1 shows robustness to backbone swapping, the Impact Statement explicitly warns that "outputs may reflect biases present in those models and should be interpreted with care."
- Why unresolved: If the VLM judge cannot perceive subtle distinctions (e.g., between "turquoise" and "cyan") or hallucinates objects, the benchmark will propagate these errors, potentially penalizing models that are actually correct.
- What evidence would resolve it: A comparison of SANEval scores against a ground-truth dataset where the compositional complexity exceeds the known limitations of the VLM judge used in the pipeline.

## Limitations

- The SANEval-Simple and SANEval-Hard datasets are pending release, preventing independent validation
- The pipeline requires multiple LLM/VLM API calls per image, creating computational costs (~$0.02/image estimated)
- Geometric scoring thresholds for spatial relations are not explicitly defined, introducing potential variability
- The method may struggle with highly abstract or novel concepts that lack semantic overlap with detector training data

## Confidence

- **High Confidence:** The automated scores aligning with human judgments (Spearman correlation results) - supported by direct experimental evidence in Table 2 and statistical significance claims.
- **Medium Confidence:** The mechanism of synonym expansion bridging vocabulary gaps - theoretically sound but dependent on the quality of LLM reasoning and the semantic overlap between user terms and detector training data.
- **Medium Confidence:** The decoupling of prompt parsing from geometric evaluation creating robustness - the structural approach is logical, but performance depends on LLM's ability to correctly parse complex linguistic structures.

## Next Checks

1. **Dataset Validation:** Once released, independently verify the SANEval datasets by manually annotating a random sample of 100 prompts and their corresponding images to assess the ground truth quality and difficulty distribution.
2. **Geometric Threshold Calibration:** Implement the spatial scoring module and conduct a parameter sweep to determine optimal overlap thresholds for different spatial relations (on, above, beside, etc.), documenting how sensitive the scores are to these choices.
3. **Failure Mode Analysis:** Reproduce the ablation study (Table 5) with a focus on the "Enhanced OD Module" - specifically, analyze cases where synonym expansion fails to map objects correctly, categorizing the types of vocabulary mismatches that cause these failures.