---
ver: rpa2
title: Detecting Data Contamination in LLMs via In-Context Learning
arxiv_id: '2510.27055'
source_url: https://arxiv.org/abs/2510.27055
tags:
- contamination
- nvidia
- qwen
- codec
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDeC (Contamination Detection via Context),
  a method to detect and quantify training data contamination in large language models
  by leveraging in-context learning. The core idea is that adding in-context examples
  from a dataset improves model confidence for unseen data but reduces it for memorized
  training data, due to disrupted memorization patterns.
---

# Detecting Data Contamination in LLMs via In-Context Learning

## Quick Facts
- **arXiv ID:** 2510.27055
- **Source URL:** https://arxiv.org/abs/2510.27055
- **Reference count:** 40
- **Primary result:** Achieves 99.9% dataset-level AUC for separating seen vs unseen datasets using in-context learning to detect training data contamination.

## Executive Summary
This paper introduces CoDeC (Contamination Detection via Context), a method to detect and quantify training data contamination in large language models by leveraging in-context learning. The core idea is that adding in-context examples from a dataset improves model confidence for unseen data but reduces it for memorized training data, due to disrupted memorization patterns. CoDeC measures this effect by comparing model confidence with and without context, producing interpretable percentage-based contamination scores. Experiments show that CoDeC cleanly separates seen from unseen datasets, achieving dataset-level AUC of 99.9% across diverse models and architectures, outperforming baseline methods. It is model-agnostic, parameter-free, and computationally efficient, making it practical for large-scale benchmark evaluations and fair model comparisons.

## Method Summary
CoDeC detects training data contamination by measuring how in-context examples affect model confidence. For each sample in a target dataset, it computes the log-probability with and without prepended context from the same dataset. The key insight is that if the model has memorized training data, adding context disrupts these memorized sequences, reducing confidence. If the data is unseen, context provides useful distributional priors that increase confidence. The final score is the percentage of samples where confidence decreases with context, interpreted as contamination percentage. The method requires only gray-box access to token probabilities, is parameter-free, and computationally efficient with minimal context (typically one example).

## Key Results
- Achieves 99.9% dataset-level AUC for separating seen vs unseen datasets across diverse models and architectures
- Successfully detects contamination in variants like paraphrased or augmented datasets where exact overlap detection would fail
- Provides interpretable percentage-based contamination scores that scale with contamination levels
- Demonstrates consistent performance across multiple model families (Pythia, GPT-Neo, RWKV, OLMo, Nemotron)

## Why This Works (Mechanism)

### Mechanism 1: Context-Induced Disruption of Memorization
If a model has memorized specific training sequences, adding in-context examples from the same distribution interferes with this rigid memorization, reducing prediction confidence. Memorized sequences likely exist in sharp, narrow regions of the loss landscape. When semantically related context (which is also memorized) is prepended, it creates a conflicting signal or "perturbation" that disrupts the precise token sequence recall required for high confidence. The core assumption is that memorization relies on specific token-to-token transitions that are fragile to changes in the immediate context window. This is supported by the abstract stating that in-context examples reduce confidence for memorized training data "due to disrupted memorization patterns."

### Mechanism 2: ICL Simulating Finetuning Dynamics
In-Context Learning (ICL) functions similarly to gradient descent (finetuning); consequently, CoDeC measures "remaining learning capacity." Unseen datasets represent a "flat" loss surface where the model can easily adapt (confidence increases with context). Contaminated datasets represent a "saturated" or "sharp" surface (overfitting) where further adaptation signals (context) are unhelpful or destabilizing. The core assumption is that ICL behaves like a single step of finetuning, meaning the model's reaction to context predicts its reaction to further training. This is supported by Section 3.3 showing that "Confidence curves obtained by finetuning... and by adding extra context follow an identical pattern."

### Mechanism 3: Exhaustion of Distributional Priors
Contaminated models have already internalized the unique style, vocabulary, and structure (priors) of the dataset. For unseen data, context provides "free information" about these priors, boosting performance. For seen data, the context provides redundant information, resulting in no gain or negative interference. The core assumption is that the model relies on these shallow distributional cues to generate predictions for the target dataset. This is supported by Section 2.2 stating that "context sampled from a dataset seen during training provides no new information... while unseen data can improve predictions."

## Foundational Learning

- **Concept: In-Context Learning (ICL)** - Understanding ICL is essential as CoDeC uses it as the intervention mechanism. Quick check: Can you explain why prepending examples changes a model's output without updating weights?

- **Concept: Log-Likelihood/Log-Probabilities** - The core metric relies on comparing the model's confidence (log-prob) of the next token with and without context. Quick check: Does a higher log-probability indicate the model is more or less confident in its prediction?

- **Concept: Overfitting vs. Generalization** - CoDeC posits that contamination is a form of overfitting (sharp minima). Understanding the difference helps interpret why "learning" (ICL) hurts performance on contaminated data. Quick check: Why might a model perform well on training data but fail on new, unseen data?

## Architecture Onboarding

- **Component map:** Target dataset -> Context Sampler -> Gray-Box Evaluator (model with token probabilities) -> Aggregator (percentage calculator)

- **Critical path:**
  1. **Baseline:** Feed sample x → Record log p(x)
  2. **Intervention:** Prepend context c to x → Record log p(x|c) (ignoring logits for c)
  3. **Score:** Indicator function 1[log p(x|c) - log p(x) < 0]
  4. **Result:** Average score over dataset. High score (>80%) implies contamination

- **Design tradeoffs:**
  - **Context Size (N):** Paper uses 1. Increasing N improves separation but drastically increases inference cost (quadratic attention)
  - **Dataset Size:** Paper claims stability at ~100 samples. Smaller sets increase variance
  - **Formatting:** CoDeC is sensitive to formatting changes (e.g., adding "Question:" labels). Evaluation data must strictly match training formatting to avoid false negatives

- **Failure signatures:**
  - **High-Score False Positive:** Datasets with high internal diversity (e.g., Global News) may score 60%+ even if unseen because context acts as noise
  - **Chat Models:** Models heavily fine-tuned for dialogue (e.g., GPT-OSS 20B) may score 99% on all data due to broken sequence modeling priors
  - **Identical Data:** A dataset of 1,000 identical samples breaks the mechanism (score will be low regardless of training)

- **First 3 experiments:**
  1. **Validation Run:** Select a model with known training data (e.g., Pythia). Run CoDeC on a known training subset (e.g., ArXiv from Pile) vs. a newly released dataset. Verify the score separation (high vs. low).
  2. **Ablation on Context:** Run the detection on a contaminated dataset using N=1, 3, 5 context samples to observe the margin of separation vs. compute cost.
  3. **Transfer Contamination Test:** Fine-tune a clean model on a synthetic variant of a benchmark (e.g., paraphrased MMLU), then run CoDeC on the original MMLU to test if the method detects "distributional" contamination rather than just verbatim overlap.

## Open Questions the Paper Calls Out

- **Question 1:** Can training strategies be engineered to evade CoDeC detection without degrading the model's ability to learn or generalize? The authors state in Section 4 that it may be possible to "engineer training strategies that bypass increases in CoDeC estimates," noting that such methods would represent a significant advance. This remains unresolved as the paper does not test CoDeC against adversarial training regimes designed specifically to mask memorization while retaining dataset-specific priors.

- **Question 2:** Can the CoDeC method be effectively adapted for strict membership inference at the individual sample level? Section 4 explicitly lists "adapting CoDeC for strict membership inference at the sample level" as a direction for future work to enable finer-grained analysis. The current method aggregates scores at the dataset level (S_CoDeC) to identify distributional memorization, whereas sample-level inference requires distinguishing specific instances within a mixed distribution.

- **Question 3:** How can the method be modified to handle models optimized for non-standard behaviors, such as persistent reasoning or chat transitions, which currently trigger false positives? Appendix A.3.2 notes that GPT-OSS 20B yields scores >99% on all datasets due to optimization for chat/reasoning tasks, stating, "Addressing that issue remains for future work." CoDeC relies on confidence drops triggered by context shifts; models trained to persistently switch to "thinking" modes exhibit this behavior regardless of contamination, breaking the core assumption.

- **Question 4:** Can the scoring criterion be refined to ensure that untrained models or unrelated data mixtures consistently yield scores well below the 50% baseline? Section 4 suggests that "Improving the criterion so that untrained models or unrelated data mixtures consistently yield scores well below 50% could further enhance interpretability." Currently, random confidence fluctuations in untrained models center around 50%, which can obscure the interpretation of moderate contamination scores (e.g., 40-60%).

## Limitations

- **Formatting sensitivity:** The method is highly sensitive to formatting and preprocessing choices, with even small changes (like adding "Question:" prefixes) potentially causing false negatives.
- **High diversity datasets:** Datasets with high internal diversity or chat-specific datasets may yield scores up to 60% even when the model hasn't seen the data during training.
- **Style contamination:** The approach doesn't detect "style contamination" where a model learns distributional priors without memorizing exact sequences.

## Confidence

**High Confidence:** The core mechanism (disrupted memorization patterns reducing confidence with added context) and the experimental results showing 99.9% AUC separation are well-supported by the paper's results and ablation studies. The model-agnostic nature and computational efficiency are clearly demonstrated.

**Medium Confidence:** The finetuning analogy (Mechanism 2) is compelling but based on a single comparison figure. The distributional priors explanation (Mechanism 3) is logically sound but not directly tested. The claims about parameter-free nature and robustness to model size are supported but not extensively validated across diverse architectures.

**Low Confidence:** The exact preprocessing requirements needed for reliable operation are not fully specified. The method's behavior on extremely diverse datasets or in cases of partial contamination (mixing seen and unseen data) is not thoroughly characterized.

## Next Checks

1. **Formatting Sensitivity Test:** Systematically vary context formatting (newline counts, prefixes, special tokens) across 3-5 formatting variants and measure score stability on known training vs. unseen datasets to quantify the impact of formatting choices.

2. **Partial Contamination Analysis:** Create datasets with controlled mixtures of seen and unseen data (e.g., 0%, 25%, 50%, 75%, 100% contamination) and verify that CoDeC scores scale linearly with contamination percentage, providing interpretable contamination quantification.

3. **Cross-Architecture Transfer:** Apply CoDeC to at least two different architecture families (e.g., transformer and RNN) trained on the same dataset to verify that the method works consistently across architectures, or identify architectural factors that affect performance.