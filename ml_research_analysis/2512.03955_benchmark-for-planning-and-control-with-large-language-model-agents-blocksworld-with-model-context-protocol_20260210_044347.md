---
ver: rpa2
title: 'Benchmark for Planning and Control with Large Language Model Agents: Blocksworld
  with Model Context Protocol'
arxiv_id: '2512.03955'
source_url: https://arxiv.org/abs/2512.03955
tags:
- planning
- agent
- agents
- benchmark
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a benchmark for evaluating LLM-based agents
  in planning and control tasks using the Blocksworld domain. The benchmark provides
  a simulation environment with five complexity categories and uses the Model Context
  Protocol (MCP) for standardized agent-environment interaction.
---

# Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol

## Quick Facts
- arXiv ID: 2512.03955
- Source URL: https://arxiv.org/abs/2512.03955
- Authors: Niklas Jobs; Luis Miguel Vieira da Silva; Jayanth Somashekaraiah; Maximilian Weigand; David Kube; Felix Gehlhoff
- Reference count: 3
- Primary result: Single-agent LLM evaluation achieves 60-80% success on solvable Blocksworld tasks, 100% accuracy on impossible scenarios

## Executive Summary
This paper introduces a benchmark for evaluating LLM-based agents in planning and control tasks using the Blocksworld domain. The benchmark provides a simulation environment with five complexity categories and uses the Model Context Protocol (MCP) for standardized agent-environment interaction. A single-agent implementation was evaluated across 50 scenarios, achieving 60-80% success rates on solvable tasks and 100% accuracy in identifying impossible scenarios. Performance degraded with increasing complexity, especially under partial observability and additional constraints.

## Method Summary
The benchmark combines a Blocksworld simulation with a REST API interface wrapped by the Model Context Protocol (MCP) to enable standardized LLM agent evaluation. The environment supports five complexity categories ranging from basic Blocksworld to partial observability with additional constraints. A single ReAct-style LLM agent using o3-mini-high was evaluated on 50 scenarios, with success measured by goal achievement, correct impossibility identification, and execution metrics including token usage and time.

## Key Results
- Single-agent implementation achieved 60-80% success rates on solvable tasks across complexity categories
- 100% accuracy in identifying impossible scenarios (10 out of 50 total)
- Performance degraded significantly in Category 5 (partial observability): 60% success rate, 676 seconds average execution time, 3.1 attempts per scenario
- Category 4 (additional constraints) required 732 seconds average execution time and 143,700 tokens

## Why This Works (Mechanism)

### Mechanism 1: Standardized Tool Interface via MCP
- Claim: The Model Context Protocol (MCP) enables systematic comparison of diverse LLM agent architectures by decoupling agent implementation from environment interaction.
- Mechanism: MCP wraps simulation REST API endpoints as standardized tools with natural language descriptions. Agents discover and invoke tools through a uniform client interface without requiring domain-specific integration code. This abstraction layer allows researchers to swap agent implementations while keeping the evaluation environment constant.
- Core assumption: Agents can interpret tool descriptions sufficiently to select appropriate actions without hardcoded domain knowledge.
- Evidence anchors:
  - [abstract] "By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications."
  - [Section 3] "This modular design emphasizes interchangeability at the agent level: researchers can evaluate different LLM agent architectures by simply connecting their own MCP host and agent implementation to the unchanged simulation and MCP server."
  - [corpus] Weak direct support; neighbor papers focus on embodied agents and benchmarks but not MCP standardization specifically.
- Break condition: If tool descriptions become ambiguous or insufficient for novel constraint types, agents may fail to discover correct action sequences regardless of reasoning capability.

### Mechanism 2: Verification-Before-Execution Workflow
- Claim: Separating plan verification from execution enables error detection without state corruption, supporting iterative replanning.
- Mechanism: The `verify_plan` tool accepts a complete action sequence and checks each action against current constraints without modifying simulation state. Constraint violations return detailed natural language explanations, allowing agents to understand failures and revise plans before committing to execution.
- Core assumption: Agents can parse error feedback and generate corrected plans; verification covers all relevant failure modes.
- Evidence anchors:
  - [Section 3.4] "If any action violates constraints, the tool identifies the first incorrect action and provides a detailed explanation of the constraint violation in natural language, enabling agents to understand and perform replanning."
  - [Section 4.1] The agent workflow includes: "(4) verify the plan through verify plan, (5) correct incorrect steps based on verification feedback and re-verify if necessary."
  - [corpus] Not directly addressed in neighbor papers.
- Break condition: If error explanations are misinterpreted or verification misses edge cases (e.g., cascading constraint interactions), agents may repeatedly generate invalid plans.

### Mechanism 3: Constraint Layering for Complexity Scaling
- Claim: Layering additional constraints (block size, partial observability) on base Blocksworld rules creates controlled difficulty gradients that stress different agent capabilities.
- Mechanism: Three constraint sets (base, block size, partial observability) validate actions before execution. Block size restricts stacking order; partial observability limits visible state to top two blocks per stack. These constraints reduce valid action spaces and require explicit information-gathering actions.
- Core assumption: Difficulty increases monotonically with constraint additions; each constraint type isolates specific reasoning deficits.
- Evidence anchors:
  - [Section 3.2] "Category 5 (Partial Observability) exhibited the poorest results across the majority of metrics: 60% success rate, average execution time of 676 seconds, 3.1 attempts per scenario."
  - [Section 4.2] "Category 4 (additional constraints) required significantly longer execution times (732 seconds on average), higher token consumption (143,700), and more solution attempts (2.2)."
  - [corpus] ViPlan benchmark similarly uses symbolic predicates for visual planning complexity scaling, supporting constraint-based difficulty modulation.
- Break condition: If constraint combinations produce emergent interactions not reflected in individual constraint descriptions, agents may face unpredictable difficulty spikes.

## Foundational Learning

- Concept: **ReAct Agent Pattern**
  - Why needed here: The single-agent implementation uses ReAct (Reasoning + Acting) to iteratively select tools based on intermediate results. Understanding this pattern is essential for interpreting agent behavior and designing system prompts.
  - Quick check question: Can you explain why ReAct requires a memory of previous tool outputs to function effectively?

- Concept: **PDDL-Style Planning Domains**
  - Why needed here: Blocksworld is a classical symbolic planning domain with well-defined preconditions and effects. Understanding action schemas helps interpret why non-constructive actions increase planning difficulty.
  - Quick check question: What makes an action "non-constructive" in Blocksworld, and why does this complicate planning?

- Concept: **REST API Statelessness**
  - Why needed here: The simulation exposes functionality through a Flask REST API. Understanding stateless request-response patterns is necessary for debugging agent-environment interactions.
  - Quick check question: How does the simulation maintain state across multiple REST API calls if HTTP is stateless?

## Architecture Onboarding

- Component map:
  - Blocksworld Simulation (Python/pygame) -> REST API (Flask) -> MCP Server -> MCP Client/Host -> LLM Agent (LangGraph ReAct)

- Critical path:
  1. Agent calls `get_rules` and `get_status` to understand environment
  2. Agent generates candidate action sequence
  3. Agent calls `verify_plan`; if errors, revise and re-verify
  4. Agent executes verified plan via primitive action tools
  5. Agent confirms goal achievement via `get_status`

- Design tradeoffs:
  - Single-agent vs. multi-agent: Single-agent simpler but lacks specialized modules for planning vs. execution
  - Prompt-based vs. architectural control: Prompts offer flexibility but weaker guarantees than hardcoded flows
  - Full vs. partial observability: Realistic sensing limitations vs. debugging tractability

- Failure signatures:
  - **Invalid intermediate steps**: Plan passes verification but execution fails (verification gap)
  - **Premature termination**: Agent declares success before goal achieved (state misinterpretation)
  - **Incorrect tool arguments**: Calling tools with wrong block names (first observed in Category 5)
  - **Misclassification**: Solvable problems declared unsolvable (observed in Category 4)

- First 3 experiments:
  1. Run the 10 Category 1 scenarios with the provided single-agent to establish baseline token consumption and timing metrics.
  2. Replace the o3 model with a smaller model (e.g., GPT-4o-mini) on Category 1-2 scenarios to measure performance degradation relative to model capability.
  3. Implement a hybrid agent that uses a classical PDDL planner for plan generation and LLM only for translation; compare success rates and token usage against pure LLM agent on Category 2-4 scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's generalization to non-symbolic domains remains untested. While Blocksworld provides clean semantics for planning and control evaluation, results may not transfer to continuous control spaces or high-dimensional sensory inputs where classical planners also struggle.
- MCP standardization is demonstrated but not validated across diverse agent architectures. The benchmark shows one agent implementation succeeding, but the claim that "diverse agent architectures can be connected without implementation-specific modifications" requires broader testing across different LLM frameworks and reasoning patterns.
- The difficulty scaling mechanism assumes additive constraint effects, but the benchmark does not test constraint interaction effects systematically. Category 5 results suggest partial observability creates disproportionate difficulty, but the underlying cause (state representation vs. action selection vs. verification interpretation) is not isolated.

## Confidence

- **High confidence**: The benchmark implementation and single-agent evaluation results are well-documented with specific metrics (60-80% success rates, 100% impossible detection accuracy). The MCP integration approach is technically sound and reproducible.
- **Medium confidence**: Claims about MCP enabling standardized comparison across agent architectures are plausible but minimally validated. The single-agent results support difficulty scaling claims, but the specific mechanisms causing performance degradation in Categories 4-5 need deeper investigation.
- **Low confidence**: Claims about the benchmark's utility for comparing LLM agents against classical planners are forward-looking but not demonstrated in the current work. The neighbor corpus provides no direct support for this comparison capability.

## Next Checks

1. **Cross-architecture MCP validation**: Implement and evaluate three different agent architectures (ReAct, tree-of-thought, and hybrid classical-LLM) on the same 50 scenarios. Measure whether MCP standardization truly eliminates environment-specific integration effort and whether architectural differences manifest clearly in performance metrics.

2. **Constraint interaction isolation**: Design Category 5 scenarios that isolate partial observability from other constraints (e.g., solvable problems where only visibility is limited, not stackability). Compare performance against full observability versions to determine if difficulty stems from state representation or plan generation challenges.

3. **Classical planner baseline comparison**: Implement a PDDL-based planner using the same Blocksworld semantics and constraint sets. Run the same scenario suite and compare success rates, planning time, and token usage (for LLM) to quantify where LLM agents provide advantages or disadvantages relative to classical approaches.