---
ver: rpa2
title: 'Align to Structure: Aligning Large Language Models with Structural Information'
arxiv_id: '2504.03622'
source_url: https://arxiv.org/abs/2504.03622
tags:
- discourse
- text
- alignment
- reward
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Structural Alignment, a novel alignment
  framework that integrates linguistically grounded discourse structures into large
  language models to enhance long-form text generation. The approach employs two complementary
  reward models within a Proximal Policy Optimization framework: one scoring surface-level
  text structures for readability and another analyzing hierarchical discourse motifs
  to reinforce coherence.'
---

# Align to Structure: Aligning Large Language Models with Structural Information

## Quick Facts
- arXiv ID: 2504.03622
- Source URL: https://arxiv.org/abs/2504.03622
- Reference count: 10
- Key outcome: Structurally aligned models outperform standard RLHF baselines, generating more coherent essays and summaries with graph-level alignment yielding strongest improvements (ROUGE-1: 55.86, ROUGE-2: 21.72, ROUGE-L: 52.81 on GOVREPORT)

## Executive Summary
This paper introduces Structural Alignment, a novel framework that integrates linguistically grounded discourse structures into large language models to enhance long-form text generation. The approach employs two complementary reward models within a Proximal Policy Optimization framework: one scoring surface-level text structures for readability and another analyzing hierarchical discourse motifs to reinforce coherence. Experiments show that structurally aligned models outperform standard RLHF baselines, generating more coherent and human-like essays and summaries, with graph-level alignment yielding the strongest improvements in tasks like long-document summarization.

## Method Summary
The method employs Proximal Policy Optimization with two complementary reward models: a surface-level LLM-as-judge evaluator (QWEN2-72B-INSTRUCT-AWQ) scoring flow/organization/balance, and a graph-level authorship classifier analyzing discourse motif distributions from RST-parsed text. A dense reward scheme provides token-level bonuses for contributions to human-distinctive discourse motifs, improving training stability for long-form generation. The policy model (QWEN2-1.5B-INSTRUCT) is trained on custom essay prompts and evaluated on summarization tasks using ROUGE metrics and GPT-4o pairwise comparisons.

## Key Results
- Structurally aligned models outperform standard RLHF baselines on essay quality (GPT-4o pairwise evaluation)
- Graph-level alignment yields strongest improvements in long-document summarization (ROUGE-1: 55.86, ROUGE-2: 21.72, ROUGE-L: 52.81 on GOVREPORT)
- Dense rewards improve training stability and maintain high-quality long-form generations
- Two-stage alignment (surface→graph or graph→surface) provides marginal gains over single-stage approaches

## Why This Works (Mechanism)

### Mechanism 1: Surface-Level Text Structure Scoring via LLM-as-Judge
LLM-based evaluators provide explicit structural feedback across three pragmatics-grounded dimensions (flow, organization, balance). This mechanism improves logical flow and hierarchical organization by giving the model direct structural feedback during training. The core assumption is that LLM evaluators can reliably approximate human judgments of discourse quality.

### Mechanism 2: Graph-Level Discourse Scoring via Authorship Classification
Text is parsed into RST discourse trees and analyzed for discourse motifs—recurring structural patterns that distinguish human from AI writing. A Longformer-based classifier trained on these motif distributions produces a "human vs. AI" probability reward. This exploits the fact that human texts display richer, more varied structural patterns while AI texts rely on uniform sequential prediction.

### Mechanism 3: Dense Reward Shaping with Discourse Motifs
Token-level rewards are assigned to tokens contributing to human-distinctive discourse motifs, stabilizing long-form RL training. This addresses credit assignment problems in sparse reward settings by providing immediate feedback for structural contributions rather than waiting until generation completion.

## Foundational Learning

- **Concept: Rhetorical Structure Theory (RST)**
  - Why needed here: Provides the formal framework for parsing text into Elementary Discourse Units and hierarchical trees with labeled relations (elaboration, contrast, cause, etc.). Without understanding RST, you cannot interpret the discourse motif extraction or the graph-level reward model.
  - Quick check question: Can you explain what "nucleus" and "satellite" mean in RST, and why they matter for discourse structure?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: The paper's entire RL pipeline runs on PPO. Understanding the clipped surrogate objective and how reward signals propagate through gradient updates is essential for debugging training dynamics and modifying the reward scheme.
  - Quick check question: What happens to policy updates if the reward signal has high variance across episodes?

- **Concept: Reward Shaping in RL**
  - Why needed here: The paper's key contribution is dense reward shaping to address sparse rewards in long-form generation. You need to understand why sparse rewards cause credit assignment problems and how token-level bonuses can stabilize training without altering optimal policy.
  - Quick check question: If you add dense rewards incorrectly, what property of optimal policy might you violate (hint: potential-based shaping)?

## Architecture Onboarding

- **Component map:**
  Writing Prompt -> Policy Model (Qwen2-1.5B) -> Generated Essay
                      ↓
          ┌──────────┴──────────┐
          ↓                     ↓
  [Path A: Surface Scoring]     [Path B: Graph Scoring]
  LLM Evaluator (Qwen2-72B)     RST Parser (DMRST)
  - Flow/Org/Balance scores     ↓
          ↓                     Motif Extractor (MF-IDF)
          ↓                     ↓
          ↓             Authorship Classifier (Longformer)
          ↓                     ↓
  Combined Reward ←─────────────┘
          ↓
  PPO Update -> New Policy

- **Critical path:**
  1. Set up SGLang server for 72B evaluator (requires A100 80GB, AWQ quantization)
  2. Implement RST parsing pipeline with paragraph-level segmentation (400-512 tokens/chunk)
  3. Train/fine-tune authorship classifier on motif distributions (add domain-specific negatives)
  4. Modify TRL library to support online reward queries and token-level dense reward injection
  5. Run PPO training with KL coefficient 0.03, batch size 2, gradient accumulation 4

- **Design tradeoffs:**
  - Surface scoring is simpler but captures only local signals; graph scoring captures global structure but requires accurate RST parsing
  - Dense rewards improve stability but add computational overhead (motif extraction per generation)
  - Two-stage alignment (S→G or G→S) yields marginal gains but doubles training cost

- **Failure signatures:**
  - Reward plateau before convergence -> check motif extraction pipeline for errors
  - Generated text overuses connectives without actual coherence -> surface scoring dominating, reduce weight
  - Length penalty too aggressive -> generations become verbose to avoid penalty

- **First 3 experiments:**
  1. Validate LLM evaluator correlation with human judgments on your domain-specific essay samples before full training.
  2. Ablate dense vs. episodic rewards: train two models with identical settings, one with token-level bonuses disabled, compare ROUGE on GOVREPORT.
  3. Test generalization: apply trained model to out-of-domain prompts (e.g., scientific writing vs. persuasive essays) and measure motif distribution shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a unified alignment framework effectively integrate both surface-level and graph-level reward signals to optimize for comprehensive structural coherence?
- Basis in paper: Page 6 states that since the two alignment methods are complementary, "Developing more effective multi-reward alignment strategies could further improve performance, making this a promising avenue for future research."
- Why unresolved: The current two-step process (S→G and G→S) yielded only marginal improvements over single-step alignment while incurring additional computational costs.
- What evidence would resolve it: A training paradigm that simultaneously optimizes a combined loss function of both reward types, demonstrating statistically significant quality improvements over sequential baselines without doubling training time.

### Open Question 2
- Question: Does the stability and efficacy of structural alignment persist when applied to large-scale models (e.g., >7B parameters) and more diverse datasets?
- Basis in paper: The Conclusion (Page 8) lists "scaling to larger models and diverse datasets for greater robustness" as a primary direction for future work.
- Why unresolved: Experiments were restricted to the QWEN2-1.5B-INSTRUCT model due to resource constraints, leaving the scalability of the dense reward scheme unproven for state-of-the-art architectures.
- What evidence would resolve it: Successful replication of the reward improvement curves and human-evaluated coherence gains on 7B or 70B parameter models across non-essay domains like creative writing or technical documentation.

### Open Question 3
- Question: Can discourse analysis be extended to process full documents without segmentation to ensure cross-segment rhetorical coherence?
- Basis in paper: Page 4 notes the RST parser segments text into 400-512 token chunks, explicitly identified as a "technical limitation that future work can address."
- Why unresolved: Segmenting text breaks the continuity of discourse trees at paragraph boundaries, potentially losing global rhetorical dependencies that span multiple segments.
- What evidence would resolve it: A reward model utilizing a parser capable of processing >2K tokens in a single pass, resulting in higher retention of human-distinctive motifs compared to the aggregated segmented approach.

## Limitations
- Domain specificity: Discourse structures may vary significantly between academic, persuasive, and narrative writing domains
- Computational overhead: Running 72B evaluator and RST parsing for every generation creates substantial latency
- Evaluation gap: Lack of human preference studies means quality improvements may not translate to perceived user experience

## Confidence
- **High Confidence**: Technical implementation of PPO-based structural alignment, RST parsing pipeline, and reward shaping mechanisms are well-specified and reproducible
- **Medium Confidence**: Performance improvements (ROUGE scores, GPT-4o evaluations) are methodologically sound but depend on automated metric reliability
- **Low Confidence**: Generalizability of discourse motif distributions across domains and long-term stability of structurally aligned models beyond initial training

## Next Checks
1. Conduct human preference studies comparing structurally aligned outputs against standard RLHF baselines across multiple domains (academic, creative, technical writing)
2. Test motif distribution stability by training on GOVREPORT then evaluating on out-of-domain scientific papers to measure structural alignment transfer
3. Measure inference-time overhead by profiling the complete pipeline (evaluator + parser + classifier) on standard GPU hardware to assess practical deployment viability