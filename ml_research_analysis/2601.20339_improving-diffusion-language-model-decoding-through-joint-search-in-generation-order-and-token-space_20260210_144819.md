---
ver: rpa2
title: Improving Diffusion Language Model Decoding through Joint Search in Generation
  Order and Token Space
arxiv_id: '2601.20339'
source_url: https://arxiv.org/abs/2601.20339
tags:
- search
- decoding
- order-token
- token
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion Language Models enable order-agnostic generation, allowing
  multiple decoding trajectories. However, existing decoding methods commit to a single
  trajectory, limiting exploration of the joint space of generation order and token
  values.
---

# Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space

## Quick Facts
- arXiv ID: 2601.20339
- Source URL: https://arxiv.org/abs/2601.20339
- Reference count: 40
- Key outcome: Order-Token Search improves pass@1 by 6.8% on HumanEval and 7.9% on Countdown through joint exploration of generation order and token values

## Executive Summary
Diffusion Language Models enable order-agnostic generation but existing decoding methods commit to single trajectories, limiting exploration of the joint space of generation order and token values. Order-Token Search addresses this by maintaining beams of partial trajectories, expanding them through denoising actions that change both position and token, and pruning using a stable incremental likelihood estimator. The method consistently improves pass@1 accuracy across mathematical reasoning and coding benchmarks, matching or surpassing gains from post-training approaches like diffu-GRPO. By explicitly exploring both what tokens to write and when/where to write them, OTS converts the multi-trajectory potential visible in pass@k into improved single-sample accuracy.

## Method Summary
Order-Token Search introduces a beam search framework that operates jointly over generation order and token space for Diffusion Language Models. The algorithm maintains K partially denoised candidates, periodically expands each by proposing multiple denoising actions that differ in both position selection and token assignment, then prunes to top-K using a stable incremental likelihood score. Unlike traditional beam search that only explores token space, OTS branches over both position (which masked tokens to denoise) and token value (what to write). The search operates at block boundaries rather than every denoising step for computational efficiency, reducing decision points from O(T) to O(B). The key innovation is the incremental denoising score that evaluates partial trajectories by their incremental denoising actions rather than full-sequence likelihood, providing more stable comparisons across candidates with different generation histories.

## Key Results
- HumanEval pass@1 improved by 6.8% absolute over the backbone
- Countdown pass@1 improved by 7.9% absolute over the backbone
- OTS matches or surpasses gains from diffu-GRPO post-training without additional training
- Low-confidence remasking alone provides 2.1% improvement on HumanEval, but OTS with search provides additional gains

## Why This Works (Mechanism)

### Mechanism 1: Joint Search Over Generation Order and Token Space
The algorithm maintains K partially denoised candidates, periodically expands each by proposing multiple denoising actions that differ in both position selection and token assignment, then prunes to top-K using likelihood scores. This converts latent multi-trajectory potential (visible in pass@k) into improved single-sample accuracy. The core assumption is that the model's likelihood estimates are sufficiently correlated with solution correctness to enable meaningful trajectory comparison.

### Mechanism 2: Incremental Denoising Score for Stable Pruning
Scoring partial trajectories by their incremental denoising actions—rather than full-sequence likelihood—provides a lower-variance, more comparable signal across candidates with different generation histories. Instead of evaluating how well the model reconstructs the entire clean sequence from an intermediate state, the score function evaluates only newly revealed blocks conditioned on the model's full-sequence prediction. The cumulative score sums these incremental terms over all branching points.

### Mechanism 3: Block-Structured Search for Computational Efficiency
Organizing search at block boundaries rather than every denoising step reduces complexity while preserving sufficient order-token exploration. The algorithm only branches at contiguous token block boundaries (B blocks vs. T steps), reducing decision points from O(T) to O(B). With typical settings (S=L/2 steps, B=L/32 blocks, K≈4 beams), NFE ≈ 2.5·L², comparable to 5-sample majority voting.

## Foundational Learning

- **Masked Diffusion Models (MDMs):** Why needed: OTS operates on MDMs' unique property of order-agnostic parallel denoising; understanding the forward/reverse process is prerequisite. Quick check: Can you explain why MDMs enable parallel decoding while autoregressive models cannot?

- **pass@k Metric and Exploration-Exploitation Tradeoff:** Why needed: The paper diagnoses DLM decoding through pass@k curves; understanding how pass@1 vs. pass@k growth rates reveal trajectory diversity is essential. Quick check: If pass@1 is high but pass@k grows slowly, what does this indicate about sampling diversity?

- **Beam Search Basics:** Why needed: OTS extends beam search to the joint order-token space; knowing standard beam search (top-K selection, pruning) provides the conceptual scaffold. Quick check: How does beam search differ from greedy decoding, and what is its primary limitation?

## Architecture Onboarding

- **Component map:** Prompt + MASK_L → [Beam Set (K candidates)] → [Expansion Phase] → [Pruning Phase] → Repeat for B blocks → Select highest-scored final sequence

- **Critical path:** The likelihood estimator (Eq. 2) is the single most sensitive component. Ablations show 40-50% accuracy drops on Countdown when using alternative scoring. Verify scoring implementation against the formula before debugging other components.

- **Design tradeoffs:**
  - Beam size K: Higher K improves accuracy but scales NFE linearly; the paper uses K∈{3,5,8}
  - Block size: Controls granularity vs. efficiency; intermediate values (2-64) perform similarly, extremes fail
  - Temperature τ: Balances diversity among beams vs. stability; too high destabilizes order-token selection

- **Failure signatures:**
  - Accuracy comparable to random baseline: Likelihood estimator may be misaligned with task success
  - No improvement over low-confidence baseline: Beam size may be too small, or temperature too low for meaningful exploration
  - Excessive compute: Verify block size isn't set to 1 (degenerates to per-token search)

- **First 3 experiments:**
  1. Reproduce the pass@k tradeoff: Run low-confidence vs. random remasking on GSM8K or MATH500 with k∈{1,2,4,...,256} to confirm the exploration-exploitation diagnosis
  2. Ablate the scoring function: Compare full OTS scoring (Eq. 2) against "OTS All Blocks" and "OTS Future Blocks" on Countdown; expect large degradation if implementation is correct
  3. Sweep block size: On MATH500 with fixed beam size, sweep block size ∈{1,2,4,8,16,32,64,128} to verify the plateau region and identify optimal granularity

## Open Questions the Paper Calls Out
None

## Limitations
- The incremental denoising score's superiority over full-sequence likelihood scoring is empirically demonstrated but theoretically under-explained
- Results on Sudoku suggest the method may amplify noise when likelihood scores poorly correlate with correctness
- The claim that block size 2-64 provides a performance plateau may be task-dependent rather than a general principle

## Confidence

**High Confidence (5/5):** The mechanism that joint search over order and token space improves exploration is well-supported by consistent pass@1 improvements across multiple benchmarks and the ablation showing low-confidence remasking alone is insufficient.

**Medium Confidence (3/5):** The incremental denoising score's superiority over full-sequence likelihood scoring is empirically demonstrated but theoretically under-explained. While the paper shows it works, the specific reasons why incremental scoring provides more stable comparisons need deeper analysis.

**Low Confidence (2/5):** The claim that block size 2-64 provides a performance plateau may be task-dependent. The narrow band of 23.0%-28.0% accuracy across block sizes could reflect the specific characteristics of the mathematical reasoning tasks tested rather than a general principle.

## Next Checks

1. **Cross-task scoring validation:** Apply OTS to tasks with known likelihood-correctness misalignment (like Sudoku) and measure whether incremental scoring consistently amplifies noise. Compare against tasks where likelihood and correctness are better aligned to quantify the correlation between scoring stability and task structure.

2. **Score function ablation across domains:** Systematically compare the full incremental scoring (Eq. 2) against OTS All Blocks and OTS Future Blocks on a diverse set of tasks including creative writing, commonsense reasoning, and structured output generation. Measure not just accuracy but also the correlation between score differences and final outcome differences to quantify signal quality.

3. **Block size scaling with task complexity:** On a task hierarchy from simple pattern matching to complex constraint satisfaction, measure how optimal block size scales with task difficulty. Include very fine-grained tasks where token-level order might matter (e.g., syntactic parsing or constraint programming) to test the limits of block-structured search assumptions.