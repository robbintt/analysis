---
ver: rpa2
title: 'NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and
  Results'
arxiv_id: '2504.10685'
source_url: https://arxiv.org/abs/2504.10685
tags:
- detection
- object
- few-shot
- team
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the results of the 1st NTIRE 2025 Challenge
  on Cross-Domain Few-Shot Object Detection (CD-FSOD), which aims to advance object
  detection under domain shifts with limited labeled data. The challenge attracted
  152 registered participants, with 42 teams submitting results and 13 making final
  submissions.
---

# NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results

## Quick Facts
- arXiv ID: 2504.10685
- Source URL: https://arxiv.org/abs/2504.10685
- Reference count: 40
- Top open-source team achieved score of 231.01, significantly surpassing baseline

## Executive Summary
The 1st NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection (CD-FSOD) attracted significant attention from the research community, with 152 registered participants and 42 teams submitting results. The challenge introduced a novel open-source CD-FSOD setting that allows leveraging foundation models and diverse knowledge sources. The competition was structured into open-source and closed-source tracks, with the open-source track permitting use of any foundation models while the closed-source track restricted participants to non-commercial models. The challenge successfully pushed the boundaries of object detection under domain shifts with limited labeled data, achieving new state-of-the-art results across both tracks.

## Method Summary
Participants approached the CD-FSOD task from diverse perspectives, proposing novel models that significantly outperformed baseline approaches. The open-source track, which allowed leveraging foundation models, saw top teams achieve scores exceeding 215 points, demonstrating the effectiveness of incorporating large-scale pre-trained models. In contrast, the closed-source track, with its restriction to non-commercial models, still achieved competitive results with the top team scoring 125.90. The diversity of approaches and the significant performance improvements over baselines indicate that the proposed methods effectively address the challenges of cross-domain adaptation with few-shot learning constraints.

## Key Results
- Top three open-source teams achieved scores of 231.01, 215.92, and 215.48 respectively
- Closed-source track top team achieved score of 125.90
- 13 teams made final submissions out of 42 teams that submitted results
- All top-performing teams significantly surpassed baseline performance

## Why This Works (Mechanism)
The success of the top-performing methods can be attributed to their ability to effectively leverage cross-domain adaptation techniques combined with few-shot learning strategies. By utilizing foundation models in the open-source track, teams were able to transfer rich semantic knowledge across domains, while still addressing the few-shot constraint through specialized adaptation mechanisms. The closed-source track results demonstrate that even without commercial models, carefully designed architectures can achieve substantial improvements through domain-aware feature extraction and few-shot adaptation strategies.

## Foundational Learning
- **Cross-domain adaptation**: Essential for handling domain shifts between source and target domains; quick check: evaluate performance degradation when domain adaptation is removed
- **Few-shot learning**: Critical for operating with limited labeled data; quick check: measure performance scaling with number of shots
- **Object detection fundamentals**: Required for understanding bounding box regression and classification; quick check: baseline mAP on in-domain data
- **Domain generalization**: Important for handling unseen target domains; quick check: performance on held-out domain test sets
- **Foundation model integration**: Enables leveraging pre-trained knowledge; quick check: ablation study comparing with and without foundation models

## Architecture Onboarding
**Component Map**: Input Images -> Backbone -> Feature Extractor -> Few-Shot Adapter -> Detection Head -> Output Predictions
**Critical Path**: Backbone Feature Extraction -> Cross-Domain Adaptation -> Few-Shot Classification -> Bounding Box Regression
**Design Tradeoffs**: Open-source allows foundation models (higher performance, more complexity) vs. closed-source (restricted but more practical)
**Failure Signatures**: Domain shift sensitivity, few-shot sample overfitting, computational resource constraints
**First Experiments**: 1) In-domain baseline evaluation, 2) Cross-domain performance without adaptation, 3) Few-shot learning curve analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies solely on COCO dataset splits, potentially limiting generalizability
- High dropout rate (152 registered, only 13 final submissions) suggests implementation challenges
- Open-source track's foundation model allowance may create uneven competition conditions
- Closed-source restrictions may exclude potentially effective commercial solutions

## Confidence
- Challenge Success and Participant Engagement: Medium Confidence - Registration to submission gap raises feasibility questions
- Novelty of Open-Source CD-FSOD Setting: Medium Confidence - Insufficient detail on differentiation from existing paradigms
- State-of-the-Art Results: High Confidence - Clear numerical improvements with sound methodology

## Next Checks
1. Conduct ablation studies on COCO validation set to isolate foundation model contributions
2. Evaluate top methods on additional cross-domain datasets (PASCAL VOC, Cityscapes)
3. Analyze computational requirements and inference times for practical deployment assessment