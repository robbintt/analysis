---
ver: rpa2
title: 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality'
arxiv_id: '2508.00109'
source_url: https://arxiv.org/abs/2508.00109
tags:
- prompts
- factory
- question
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACTORY is a human-verified, challenging prompt set designed for
  long-form factuality evaluation. Unlike existing benchmarks that lack human quality
  control, FACTORY employs a model-in-the-loop approach to generate prompts, then
  has human annotators revise them to ensure they are fact-seeking, answerable, unambiguous,
  and not time-sensitive.
---

# FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality

## Quick Facts
- arXiv ID: 2508.00109
- Source URL: https://arxiv.org/abs/2508.00109
- Authors: Mingda Chen; Yang Li; Xilun Chen; Adina Williams; Gargi Ghosh; Scott Yih
- Reference count: 35
- FACTORY achieves ~60% factual precision on hard split vs ~90% on existing benchmarks

## Executive Summary
FACTORY is a human-verified, challenging prompt set designed for long-form factuality evaluation. Unlike existing benchmarks that lack human quality control, FACTORY employs a model-in-the-loop approach to generate prompts, then has human annotators revise them to ensure they are fact-seeking, answerable, unambiguous, and not time-sensitive. The resulting dataset contains 10,156 prompts across diverse topics, with a particularly challenging subset of 421 prompts.

Human evaluations on six state-of-the-art language models reveal that FACTORY is significantly more difficult than existing benchmarks: models achieve only ~60% factual precision on FACTORY's hard split compared to ~90% on other datasets. Analysis shows that FACTORY's challenges stem from requiring models to reason across long-tailed facts and provide specific details. Even when prompts are decomposed into simpler "atomic" questions, models struggle to achieve high factual precision, indicating knowledge gaps beyond just prompt complexity.

## Method Summary
FACTORY uses a model-in-the-loop approach: Wikipedia titles seed prompt generation, which is then automatically filtered using VeriScore to retain only prompts where models achieve <60% factual precision. Human annotators then revise/reject prompts across five quality criteria, with ~20% rejection rate. The hard split (421 prompts) is created by further filtering to prompts where SOTA models achieve ~50% precision. Evaluation uses VeriScore claim extraction + human verification of sampled sentences.

## Key Results
- FACTORY achieves ~60% factual precision on hard split vs ~90% on existing benchmarks
- Even decomposed "atomic" prompts yield only ~67% precision (vs ~31% for original prompts), indicating knowledge gaps beyond complexity
- Human verification eliminates ~20% of automatically generated prompts that fail quality criteria
- FACTORY covers diverse topics with long-tailed facts requiring multi-entity reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-in-the-loop filtering identifies prompts that challenge current SOTA models before human review.
- Mechanism: LLMs first generate candidate prompts from Wikipedia seed topics, then respond to these prompts using retrieved passages. Prompts where models achieve <60% factual precision (via VeriScore) are retained as challenging; easier prompts are filtered out. This concentrates human annotation effort on genuinely difficult cases.
- Core assumption: Low factual precision on initial model responses predicts genuine knowledge gaps rather than prompt ambiguity or unanswerability.
- Evidence anchors:
  - [section] "We then employ VeriScore to fact-check each model's response, filtering out questions that LLMs can answer with high factual precision. We retain prompts for which the Llama models achieve less than 60% factual precision."
  - [abstract] "Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous."
- Break condition: If models fail due to prompt incoherence rather than knowledge gaps, filtering will retain noisy prompts.

### Mechanism 2
- Claim: Human verification eliminates systematic benchmark quality issues that inflate model performance.
- Mechanism: Human annotators revise/reject prompts across five criteria: fact-seeking, self-contained, answerable from public sources, time-invariant, and safe. This removes unanswerable prompts (e.g., "Who is Emilia Chico?"), time-sensitive queries, and hypothetical questions that artificially lower measured factuality without testing genuine knowledge.
- Core assumption: Human judgment reliably identifies these quality issues that automatic generation misses.
- Evidence anchors:
  - [section] "Annotators are instructed to reject prompts if a complete rewrite is necessary... around 20% of the prompts were rejected or edited."
  - [section] "We discover that prior benchmarks suffer from issues such as answerability, hallucinations, and time-sensitivity, likely due to their lack of human verification."
- Break condition: If annotators introduce their own biases or miss domain-specific issues, verification quality degrades.

### Mechanism 3
- Claim: FACTORY's difficulty stems from both long-tailed knowledge requirements and cross-fact reasoning demands.
- Mechanism: Prompts require specific details about multiple entities (e.g., both Shohé Tanaka AND the Enharmonium instrument). Decomposing prompts into atomic questions (one per proper noun) improves factual precision but models still underperform (~67% vs ~31% for Claude on atomic vs original prompts), indicating knowledge gaps beyond prompt complexity alone.
- Core assumption: Atomic prompt performance isolates knowledge availability from reasoning complexity.
- Evidence anchors:
  - [section] "Interestingly, while making the prompts more atomic helps in making them more solvable, they remain relatively challenging compared to current long-form factuality benchmarks."
  - [section] "These findings suggest that FACTORY is challenging to solve due to both its long-tailed knowledge and the reasoning capabilities required."
- Break condition: If decomposition loses essential relational context between facts, atomic prompts may misrepresent the original task.

## Foundational Learning

- **Factual precision vs. recall in long-form evaluation**
  - Why needed here: FACTORY reports ~60% precision on hard split. Understanding this measures "what fraction of model claims are supported" (not "what fraction of required facts are included") is essential for interpreting results correctly.
  - Quick check question: If a model makes 10 claims and 6 are supported, what is the factual precision? (Answer: 60%)

- **Claim extraction and verification pipelines (VeriScore)**
  - Why needed here: FACTORY relies on VeriScore for automated filtering and uses its claim extractor in evaluation. The pipeline decomposes responses into atomic claims, searches for evidence, and labels claims as supported/unsupported/inconclusive.
  - Quick check question: Why might a claim be labeled "inconclusive" rather than "unsupported"? (Answer: Insufficient evidence found to confirm or deny)

- **Retrieval-augmented generation (RAG) in evaluation**
  - Why needed here: All benchmarked models use RAG (top 20 passages from MassiveDS). RAG should improve factuality but models still achieve only ~60% precision on FACTORY Hard, suggesting either retrieval failures or reasoning limitations.
  - Quick check question: If retrieval returns irrelevant passages, would factual precision likely increase or decrease? (Answer: Decrease—model has less grounding for accurate claims)

## Architecture Onboarding

- Component map:
  - Wikipedia titles -> MassiveDS retrieval -> LLM prompt generation
  - Model response generation -> VeriScore fact-checking -> Threshold-based filtering (<60% precision retained)
  - Human annotators revise/reject prompts across 5 quality criteria
  - Hard split creation: Secondary filtering on subset where SOTA models achieve ~50% precision
  - Evaluation: VeriScore claim extraction + human verification (100 sentences/model)

- Critical path:
  1. Prompt quality depends on human revision catching issues automatic filtering misses
  2. Hard split identification depends on having multiple SOTA models to calibrate difficulty
  3. Reliable factuality scores depend on human verification of sampled claims (not just automated scoring)

- Design tradeoffs:
  - Scale vs. verification depth: 10k prompts with 20% rejection vs. fully manual creation
  - Difficulty vs. answerability: Aggressive filtering could retain genuinely unanswerable prompts
  - Cost vs. reliability: Human evaluation takes 12 min/sentence; automated VeriScore faster but noisier

- Failure signatures:
  - Models score high on existing benchmarks (>90%) but ~60% on FACTORY Hard—this gap indicates genuine difficulty difference, not evaluation error
  - High "inconclusive" rate suggests prompt ambiguity or retrieval gaps
  - Atomic prompts still yielding low precision indicates knowledge gaps rather than just complexity

- First 3 experiments:
  1. **Baseline replication**: Run the 6 evaluated models on FACTORY Hard with identical RAG setup (top 20 MassiveDS passages) to reproduce ~60% precision finding.
  2. **Ablation on retrieval quality**: Compare factual precision when using top-5 vs. top-20 passages to quantify retrieval's contribution to model performance.
  3. **Error categorization**: Sample 50 unsupported claims from a single model's outputs and classify error types (hallucination, misattribution, temporal confusion, etc.) to identify systematic failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the low factual precision scores on decomposed "atomic" prompts indicate that models lack parametric knowledge for long-tailed entities, or that standard retrieval-augmented generation pipelines fail to locate relevant documents?
- Basis in paper: [explicit] The authors find that even when prompts are decomposed into simpler atomic questions, models struggle to achieve high precision, "indicating knowledge gaps beyond just prompt complexity."
- Why unresolved: The analysis identifies that knowledge gaps exist but does not isolate whether the failure stems primarily from the model's internal weights or the retrieval mechanism's inability to surface obscure information.
- What evidence would resolve it: A comparative study measuring retrieval success rates for the atomic prompts versus the model's parametric recall without retrieval augmentation.

### Open Question 2
- Question: How can model architectures or training methods be specifically optimized to improve reasoning across multiple long-tailed facts required by detailed prompts?
- Basis in paper: [explicit] The conclusion states that FACTORY sets a new standard and "paves the way for future advancements in model development" specifically to handle the necessity of reasoning across long-tailed facts.
- Why unresolved: The paper successfully highlights the failure of current SOTA models (approx. 60% precision on the hard split) but does not propose specific architectural or fine-tuning solutions to bridge this gap.
- What evidence would resolve it: The development and benchmarking of a new model trained on multi-hop synthesis of specific details, showing significantly higher precision on the FACTORY hard split.

### Open Question 3
- Question: Can automated factuality evaluators be refined to reliably replace the expensive human verification required for long-tailed topics?
- Basis in paper: [inferred] The authors note that human evaluation is laborious (taking approximately 12 minutes per sentence) and that current automated methods like VeriScore required adaptation to avoid costly online search engines.
- Why unresolved: While automated tools were used for filtering, the reliance on human annotators for the final benchmark suggests current automated metrics may be insufficient for validating challenging, long-tailed claims.
- What evidence would resolve it: Correlating the scores of a new automated evaluator against the human judgments collected in the study to see if they can replicate the 40% non-factual claim detection rate.

## Limitations

- The hard split's ~50% precision threshold selection criteria lack full transparency regarding exact aggregation method and potential model bias in difficulty calibration.
- Human annotation process is not fully specified in terms of inter-annotator agreement thresholds or adjudication procedures for ambiguous cases.
- The attribution of difficulty to both long-tailed knowledge and reasoning complexity could benefit from more granular error analysis to definitively separate these factors.

## Confidence

- **High confidence**: FACTORY's overall methodology (model-in-the-loop filtering + human verification) and its demonstrated superiority over existing benchmarks (60% vs 90% precision) are well-supported by the presented data.
- **Medium confidence**: The attribution of difficulty to both long-tailed knowledge and reasoning complexity is plausible but could benefit from more granular error analysis to definitively separate these factors.
- **Medium confidence**: The VeriScore evaluation pipeline's reliability depends on the claim extractor's performance, which isn't extensively validated on the FACTORY-specific prompt types.

## Next Checks

1. Replicate the 6-model benchmarking on FACTORY Hard using identical RAG configuration (top-20 MassiveDS passages) to verify the ~60% precision finding.
2. Conduct retrieval quality ablation by comparing factual precision across different passage retrieval depths (top-5 vs top-20) to quantify retrieval's contribution to performance.
3. Perform systematic error categorization on 50 unsupported claims from a single model to identify whether failures stem primarily from knowledge gaps, reasoning errors, or evaluation pipeline issues.