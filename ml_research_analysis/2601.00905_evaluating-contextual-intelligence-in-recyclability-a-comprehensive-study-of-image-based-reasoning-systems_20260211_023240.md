---
ver: rpa2
title: 'Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study
  of Image-Based Reasoning Systems'
arxiv_id: '2601.00905'
source_url: https://arxiv.org/abs/2601.00905
tags:
- items
- accuracy
- gpt-4o
- recycling
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the ability of advanced vision-language models
  (GPT-4o, GPT-4o-mini, and Claude 3.5) to classify waste items for recycling across
  four challenging contexts: matching items to appropriate bins (considering size
  constraints), adapting to location-specific recycling guidelines, handling contamination
  or structural damage, and classifying multi-material objects. Using a curated dataset
  of 100 images representing common waste materials, the models demonstrated strong
  contextual understanding, with GPT-4o achieving the highest accuracy (0.94) in location-specific
  guidelines and 0.98 in multi-material classification.'
---

# Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems

## Quick Facts
- arXiv ID: 2601.00905
- Source URL: https://arxiv.org/abs/2601.00905
- Reference count: 9
- Key outcome: GPT-4o achieved highest accuracy (0.94) in location-specific guidelines and 0.98 in multi-material classification, with challenges remaining in spatial reasoning and contamination detection.

## Executive Summary
This study evaluates advanced vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) on recyclability classification across four challenging contexts: bin matching with size constraints, location-specific guidelines, contamination assessment, and multi-material object classification. Using a curated dataset of 100 images representing common waste materials, the models demonstrated strong contextual understanding, with GPT-4o achieving the highest overall performance. The study reveals that while models excel at integrating visual recognition with textual guidelines, they struggle with spatial reasoning for physical size comparison and instruction-following in smaller models.

## Method Summary
The study employed inference-only API calls to evaluate three vision-language models on 100 curated waste images across 10 material categories. Four experiments tested bin matching with size constraints, location-specific recycling guidelines (Boston, London, San Francisco), contamination/structural damage assessment, and multi-material object classification. Models received image inputs combined with specific prompts and location-based guidelines, with performance measured using classification accuracy against manually annotated ground truth labels.

## Key Results
- GPT-4o achieved highest accuracy at 0.94 for location-specific guidelines and 0.98 for multi-material classification
- GPT-4o-mini showed strong residential bin testing performance at 0.73 accuracy but exhibited instruction-following failures
- All models struggled with spatial reasoning for bin openings, particularly with large items like cardboard
- Claude 3.5 lagged behind in multi-material scenarios with 0.78 accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models can integrate visual object recognition with text-based location guidelines to improve recyclability classification accuracy.
- Mechanism: Multi-modal architectures encode visual features (material type, object condition) alongside textual context (city-specific recycling rules), enabling conditional reasoning where the same object receives different classifications based on provided guidelines.
- Core assumption: Models have sufficient pre-training on common waste items and can map visual features to material categories reliably.
- Evidence anchors:
  - [abstract] "GPT-4o achieving the highest accuracy (0.94) in location-specific guidelines"
  - [section 4.2] "When comparing the 'No Location' scenario... to the Boston-specific prompt, there was a noticeable improvement in accuracy for all models. GPT-4o's accuracy increased from 0.89... to 0.94"
  - [corpus] Weak direct evidence; neighbor papers focus on user behavior and service design rather than VLM reasoning mechanisms
- Break condition: Performance degrades when object conditions (contamination, damage) require nuanced judgment not explicitly covered in provided guidelines; models may over-apply rules without assessing actual item state.

### Mechanism 2
- Claim: Spatial reasoning for physical size comparison between objects and bin openings remains unreliable across all tested models.
- Mechanism: Models attempt to infer relative sizes from 2D images without depth cues or reference scales, leading to systematic errors where large items (cardboard boxes) are incorrectly classified as fitting into small bin openings.
- Core assumption: Image resolution and framing provide sufficient information for size inference; models can learn size constancy from pre-training data.
- Evidence anchors:
  - [abstract] "Challenges remained in spatial reasoning for bin openings"
  - [section 4.1] "GPT-4o encountered significant challenges with cardboard (0.2), largely due to the inability to correctly assess the size of the BigBelly bin openings relative to the size of the items"
  - [corpus] No direct corpus evidence on spatial reasoning in waste classification
- Break condition: When bin images lack clear scale references or when items are photographed at varying distances, size inference fails systematically. Residential bin performance improved (0.63-0.73) partly because larger capacity reduced the need for precise size assessment.

### Mechanism 3
- Claim: Smaller models (GPT-4o-mini) exhibit instruction-following degradation, producing outputs outside specified label sets despite explicit constraints.
- Mechanism: Instruction tuning in smaller models may prioritize fluent responses over strict schema adherence; when faced with complex multi-class decisions, models revert to generating semantically appropriate but instructionally invalid outputs (e.g., "compost" instead of required "left/right/middle/none").
- Core assumption: Instruction-following reliability scales with model capacity; smaller models have weaker binding between constraint specifications and output generation.
- Evidence anchors:
  - [section 4.1] "GPT-4o-mini... hallucinated the existence of a third bin on four occasions... the model repeatedly outputting 'compost' seven times, despite explicit instructions in the prompt to use only 'left', 'right', 'middle', or 'none'"
  - [section 4.1] "Claude also struggled... failing to recognize that these items were too large for the bin openings"
  - [corpus] Weak evidence; neighbor papers don't address instruction-following failures
- Break condition: Complex scenarios with multiple valid classification schemes (compost vs. trash vs. recycling) increase probability of schema violations, particularly when semantic labels are more natural than positional labels.

## Foundational Learning

- Concept: Multi-modal fusion in transformer architectures
  - Why needed here: Understanding how VLMs combine visual tokens with text tokens explains why location guidelines can modulate visual classification but cannot fully compensate for spatial reasoning gaps.
  - Quick check question: Can you explain why adding 377-557 words of guidelines to a prompt improves accuracy, but does not help models determine if a cardboard box fits through a small opening?

- Concept: Zero-shot vs. in-context learning
  - Why needed here: The study evaluates models without task-specific training, relying entirely on pre-trained knowledge plus prompt-provided context. Performance ceilings reflect both pre-training coverage and in-context reasoning capacity.
  - Quick check question: Why might GPT-4o's 0.98 accuracy on multi-material objects reflect pre-training exposure rather than pure reasoning, and how would you design a test to disentangle these?

- Concept: Hallucination in constrained generation
  - Why needed here: GPT-4o-mini outputting "compost" when instructed to use only positional labels represents a specific failure mode where semantically plausible outputs override instruction constraints.
  - Quick check question: What decoding parameters or architectural choices might reduce this type of instruction-following failure without requiring model retraining?

## Architecture Onboarding

- Component map: Image encoder (ViT-based) -> Text tokenizer -> Cross-attention fusion -> Classification head -> Constrained decoding
- Critical path: Image preprocessing → visual encoding → multi-modal fusion with guideline embeddings → constrained decoding. The spatial reasoning failures occur at the fusion/inference stage, while instruction-following failures occur at the decoding stage.
- Design tradeoffs:
  - Larger models (GPT-4o) improve spatial and multi-material reasoning but increase latency/cost
  - Explicit positional labels ("left/right") reduce ambiguity but conflict with learned semantic priors ("compost")
  - Detailed guidelines improve accuracy but may not generalize to truly novel recycling contexts
- Failure signatures:
  - Size-related misclassification: Large items predicted as fitting small openings (cardboard: 0.0-0.2 accuracy on BigBelly bins)
  - Schema violation: Outputting category names instead of positional labels (GPT-4o-mini: "compost" x7)
  - Contamination over-generalization: Clean items correctly classified, but contaminated items incorrectly marked non-recyclable (Figure 3b shows Y→N errors)
- First 3 experiments:
  1. Replicate bin-matching with controlled camera distance and include reference objects (coins, hands) in frame to test whether explicit scale cues improve spatial reasoning.
  2. Ablate guideline length (short vs. detailed) while holding content constant to separate clarity effects from genuine context-adaptation.
  3. Test instruction-following with counter-intuitive label mappings (e.g., "say 'left' for compostable items") to measure constraint-binding strength across model sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of an asymmetric cost function—penalizing false positives more heavily than false negatives—affect the operational accuracy and utility of these models?
- Basis in paper: [explicit] The Discussion notes that false positives (incorrectly predicting an item is recyclable) are more costly because they contaminate the waste batch, suggesting a weight function could be incorporated.
- Why unresolved: The current study evaluated performance using standard accuracy, treating all misclassifications equally without accounting for the differential environmental or economic costs of errors.
- What evidence would resolve it: A comparative analysis of model performance using a weighted loss function or cost-sensitive metric that prioritizes the reduction of contamination events over landfill diversion.

### Open Question 2
- Question: Can specific architectural improvements or prompt engineering strategies overcome the "spatial reasoning" deficits that cause models to misjudge bin opening sizes?
- Basis in paper: [inferred] The Results section highlights that models frequently failed to assess the size of bin openings relative to items (e.g., trying to fit large cardboard boxes into small BigBelly bin apertures), indicating a lack of physical reasoning.
- Why unresolved: While the models excelled at material recognition, they struggled with the geometric and spatial constraints necessary for real-world disposal, a limitation not addressed by standard visual training.
- What evidence would resolve it: Successful classification results on a targeted dataset of items and bins where the primary challenge is physical fit rather than material type.

### Open Question 3
- Question: How can evaluation frameworks be standardized to handle the inherent ambiguity in recycling guidelines where definitive "ground truth" labels do not exist?
- Basis in paper: [explicit] The authors state that ascertaining true labels is a "continuing challenge" because city managers sometimes provide no definitive answers due to varying requirements from companies that purchase recycled materials.
- Why unresolved: The variability in downstream processing means binary "recyclable/not recyclable" labels may be scientifically inaccurate for specific locales, complicating the benchmarking of model "correctness."
- What evidence would resolve it: The development of a probabilistic evaluation dataset or a multi-label standard that accounts for regional variations and vendor-specific rules rather than a single binary truth.

## Limitations
- The 100 curated images, specific guideline texts, and ground truth labels remain proprietary, making exact replication challenging
- Performance variations across models may reflect both pre-training exposure and in-context reasoning capacity, but the relative contributions are unclear
- Models struggle with spatial reasoning for physical size comparison, particularly for large items like cardboard boxes

## Confidence

**High Confidence**: Models improve recyclability classification accuracy when provided with location-specific guidelines (GPT-4o accuracy increased from 0.89 to 0.94 with Boston guidelines).

**Medium Confidence**: Spatial reasoning for bin size matching remains unreliable across all models, with systematic errors for large items like cardboard.

**Low Confidence**: Instruction-following reliability scales predictably with model size, as the study's evidence is limited to one model exhibiting schema violations.

## Next Checks
1. Replicate bin-matching experiment with controlled camera distance and explicit reference objects to test whether scale cues improve spatial reasoning accuracy.
2. Ablate guideline length while holding content constant to distinguish between clarity effects and genuine context-adaptation.
3. Test instruction-following with counter-intuitive label mappings to measure constraint-binding strength across model sizes.