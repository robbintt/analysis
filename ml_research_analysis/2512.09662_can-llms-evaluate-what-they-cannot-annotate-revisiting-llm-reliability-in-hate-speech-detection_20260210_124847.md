---
ver: rpa2
title: Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in
  Hate Speech Detection
arxiv_id: '2512.09662'
source_url: https://arxiv.org/abs/2512.09662
tags:
- human
- hate
- speech
- llms
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether Large Language Models (LLMs) can\
  \ reliably serve as annotators for hate speech detection, a highly subjective NLP\
  \ task. Traditional agreement metrics like Cohen's \u03BA oversimplify disagreement\
  \ in subjective tasks, so the study employs subjectivity-aware metrics (xRR framework)\
  \ alongside standard IAA measures."
---

# Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection

## Quick Facts
- arXiv ID: 2512.09662
- Source URL: https://arxiv.org/abs/2512.09662
- Reference count: 16
- Primary result: LLMs show poor human agreement (κ≈0.1) in hate speech annotation but can reliably rank model performance (τ≈0.9)

## Executive Summary
This study investigates whether Large Language Models can serve as reliable annotators for hate speech detection, a highly subjective NLP task. Using English and Spanish datasets, the authors compare LLM annotations with human judgements using both traditional IAA metrics and subjectivity-aware xRR framework. Results show that while LLMs achieve only slight-to-fair agreement with humans under traditional metrics, they can reliably reproduce relative rankings of hate speech detection models, achieving high Kendall's τ correlations. The study concludes that LLMs are not yet reliable as annotators but can serve as scalable proxy evaluators for relative model performance in subjective tasks.

## Method Summary
The study compares LLM annotations with human judgements on three hate speech datasets (HateXplain, DETESTS, MHS) using both traditional agreement metrics (Cohen's κ, Fleiss's κ) and subjectivity-aware xRR framework. Three LLMs (Llama 3.1, Nemo, DeepSeek) generate binary hate/no-hate predictions via constrained decoding. The authors also evaluate whether LLMs can reliably rank hate speech detection models by computing Kendall's τ between rankings derived from human vs. LLM labels, using synthetic degraded classifiers.

## Key Results
- Traditional agreement metrics show only slight-to-fair agreement between LLMs and humans (mean pairwise Cohen's κ: 0.12-0.13)
- Subjectivity-aware metrics (xRR) reveal partial alignment patterns obscured by traditional metrics (normalized κx: 0.357-0.412)
- LLMs can reliably reproduce relative model rankings (Kendall's τ: 0.84-0.96) despite poor absolute agreement
- Nemo consistently outperforms other models in ranking correlation across all datasets
- LLM detection bias is target-specific, with stronger performance on frequent targets and weaker on rare/nuanced ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subjectivity-aware metrics (xRR) reveal partial alignment between LLM and human annotation patterns that traditional metrics obscure.
- Mechanism: Traditional κ metrics penalize all disagreement equally, assuming a single objective ground truth. The xRR framework instead measures whether one annotator group can reproduce the distribution of labels from another, capturing systematic disagreement patterns rather than treating them as noise.
- Core assumption: Disagreement in subjective tasks reflects legitimate perspective diversity rather than pure error.
- Evidence anchors:
  - [abstract] "Traditional agreement metrics, such as Cohen's κ, oversimplify this disagreement, treating it as an error rather than meaningful diversity."
  - [Section 4, Table 6] normalized κx values of 0.357-0.412 show "fair to moderate" similarity between human and LLM groups despite low Fleiss's κ (-0.032 to 0.045 for LLMs).
  - [corpus] Weak direct corpus support for xRR specifically; neighboring papers focus on LLM annotation behavior but not this metric.
- Break condition: If LLMs show uniformly low normalized κx across datasets, the claim that subjectivity-aware metrics reveal partial alignment fails.

### Mechanism 2
- Claim: Preserving relative model rankings is a weaker requirement than instance-level annotation agreement, enabling LLMs to serve as proxy evaluators despite low κ scores.
- Mechanism: Ranking correlation (Kendall's τ) measures ordinal consistency rather than per-instance correctness. A model can systematically over- or under-predict hate labels while still preserving which classifiers perform better than others, as long as the bias is consistent across evaluated systems.
- Core assumption: The systematic bias in LLM annotations affects all evaluated classifiers similarly, preserving relative ordering even when absolute scores are distorted.
- Evidence anchors:
  - [Section 5, Table 7] Nemo achieves τ ≥ 0.95 across all datasets with mean F1 differences as low as 0.041 (DETESTS), while DeepSeek fails on MHS (τ < 0).
  - [Section 5] "preserving relative model rankings is a less stringent requirement unlike instance agreement and can be achieved by some LLMs even when κ is low."
  - [corpus] Related work on LLM-generated hate speech detection (HateBench) focuses on detector effectiveness against LLM outputs, not the reverse use case of LLMs as evaluators.
- Break condition: If LLM-generated labels produce inverted rankings (negative τ) consistently across models and datasets, the proxy evaluator mechanism fails.

### Mechanism 3
- Claim: LLM detection bias is target-specific, with stronger performance on frequent or explicitly signaled categories and weaker performance on rare or nuanced targets.
- Mechanism: LLMs trained on web-scale data may have stronger representations for high-frequency hate speech patterns (e.g., gender-based) while lacking exposure to rare targets, leading to systematic false negative clusters.
- Core assumption: Target frequency in pre-training data correlates with detection accuracy.
- Evidence anchors:
  - [Section 4, Figure 2] LLM majority misses 100% of "Asexual" targets (5 posts), 55.2% of "Non Religious," 35.6% of "Jewish," but only 9.0-11.2% of "Women," "Men," "Refugee."
  - [Section 4] Llama 3.1 and DeepSeek show conservative detection bias (high false negatives, low false positives); Nemo over-predicts hate (7,175 false positives vs. 620 false negatives on HateXplain).
  - [corpus] "A Modular Taxonomy for Hate Speech Definitions" explores how definition framing affects zero-shot LLM classification, supporting target-sensitivity claims.
- Break condition: If error rates are uniform across all target categories regardless of frequency or type, the target-specific bias mechanism does not hold.

## Foundational Learning

- Concept: Inter-Annotator Agreement (IAA) metrics (Cohen's κ, Fleiss's κ, Krippendorff's α)
  - Why needed here: Understanding why traditional metrics underestimate LLM reliability in subjective tasks requires knowing how κ penalizes disagreement and assumes objective ground truth.
  - Quick check question: If three annotators label 100 items with 70% raw agreement but one annotator is systematically stricter, will κ be higher or lower than raw agreement suggests?

- Concept: Rank correlation (Kendall's τ)
  - Why needed here: The paper's core claim—that LLMs can serve as evaluators despite poor annotation agreement—rests on rank correlation preserving relative model performance.
  - Quick check question: If Classifier A scores F1=0.80 and Classifier B scores F1=0.70 under human labels, but both score 0.10 higher under LLM labels, what is Kendall's τ?

- Concept: Subjectivity in annotation / data perspectivism
  - Why needed here: Hate speech detection is inherently subjective; disagreement may reflect legitimate perspective diversity rather than error.
  - Quick check question: In a dataset where annotators from different demographic groups systematically disagree, would enforcing majority-vote gold labels increase or decrease annotation validity?

## Architecture Onboarding

- Component map:
  - Annotation layer: Human annotators (3 per post) → majority vote gold labels; LLM annotators (Llama 3.1, Nemo, DeepSeek) → binary hate/no-hate predictions via constrained decoding (max_tokens=1, logits filtered to True/False only).
  - Agreement metrics layer: Traditional (Cohen's κ pairwise, Fleiss's κ, Krippendorff's α) + subjectivity-aware (xRR normalized κx).
  - Evaluation layer: Degraded classifier simulation (flip p% of gold labels) → compute F1 vs. human labels and vs. LLM labels → rank classifiers → compute Kendall's τ between rankings.

- Critical path:
  1. Filter datasets to instances with exactly 3 human annotators (for parity with 3 LLM raters).
  2. Query each LLM with identical system prompt + base instruction; enforce single-token output.
  3. Compute pairwise and cross-group κ, plus normalized κx via xRR framework.
  4. Generate synthetic classifiers by degrading oracle predictions at multiple p levels; evaluate against both human and LLM labels; compute rank correlation.

- Design tradeoffs:
  - Binary detection vs. fine-grained labels: Simplifies evaluation but collapses nuanced categories (HateXplain's "offensive" merged into "non-hate").
  - Three LLMs to match three human annotators: Enables direct comparison but limits generalization to other model sizes/families.
  - 4-bit quantization: Reduces computational cost; Assumption: does not significantly affect annotation quality for this task.
  - No explicit hate speech definition in prompt: Tests "internal" model understanding but increases variability across models.

- Failure signatures:
  - Negative Kendall's τ (e.g., DeepSeek on MHS: -0.747): Indicates inverted rankings—LLM labels are actively misleading for evaluation.
  - Near-zero or negative Fleiss's κ for LLMs (e.g., -0.032 on HateXplain): Indicates LLMs perform worse than random chance agreement.
  - Extreme asymmetry in false positives vs. false negatives (e.g., Nemo: 7,175 FP vs. 620 FN): Indicates systematic over- or under-detection bias.

- First 3 experiments:
  1. Replicate agreement analysis on a held-out dataset with different annotation guidelines (e.g., sTress-test or a non-English dataset not in MetaHate) to test generalization of κ and normalized κx patterns.
  2. Ablate prompt design: Add explicit hate speech definition + few-shot examples; measure whether κ improves without degrading ranking correlation (τ).
  3. Test ranking preservation on real (non-synthetic) classifiers: Train 5+ hate speech detection models on separate data splits, evaluate on human vs. LLM labels, and compute Kendall's τ to validate whether synthetic degradation generalizes to actual model comparison.

## Open Questions the Paper Calls Out
- Whether prompt engineering (explicit definitions, few-shot examples) could improve LLM annotation reliability in subjective tasks
- Whether findings generalize to low-resource languages and fine-grained hate speech categories beyond binary classification
- What architectural or training characteristics enable certain LLMs (like Nemo) to achieve higher human alignment and ranking correlation than others

## Limitations
- Analysis limited to binary hate speech detection and English/Spanish datasets
- Simple prompts used deliberately to isolate model behavior, leaving potential improvements from prompt engineering unexplored
- Does not investigate whether calibration techniques could enable accurate absolute performance estimation from LLM labels

## Confidence
- Mechanism 1: High - Well-supported by direct experimental evidence and established theory of subjectivity in annotation
- Mechanism 2: Medium - Supported by synthetic experiments but requires validation with real classifiers
- Mechanism 3: Medium - Target-specific bias observed but causal link to pre-training data frequency not directly tested

## Next Checks
1. Verify LogitsProcessor implementation correctly constrains outputs to only True/False tokens
2. Cross-check post-harmonization label distributions against reported % hate values (HateXplain: 29.49%)
3. Replicate agreement analysis on a held-out dataset with different annotation guidelines to test generalization patterns