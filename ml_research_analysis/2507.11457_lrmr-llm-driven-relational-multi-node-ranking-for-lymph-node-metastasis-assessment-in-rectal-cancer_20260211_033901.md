---
ver: rpa2
title: 'LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment
  in Rectal Cancer'
arxiv_id: '2507.11457'
source_url: https://arxiv.org/abs/2507.11457
tags:
- stage
- patient
- cancer
- lymph
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LRMR, a two-stage LLM-driven framework for
  assessing lymph node metastasis in rectal cancer using MRI. The method reframes
  diagnosis as a structured reasoning and ranking process, first extracting patient-level
  radiological features from lymph node images via a multimodal LLM, then ranking
  patients by relative risk using a text-based LLM for pairwise comparisons.
---

# LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer

## Quick Facts
- arXiv ID: 2507.11457
- Source URL: https://arxiv.org/abs/2507.11457
- Reference count: 36
- Two-stage LLM framework for LN metastasis assessment in rectal cancer achieves AUC of 0.7917 and F1-score of 0.7200

## Executive Summary
This paper introduces LRMR, a two-stage LLM-driven framework for assessing lymph node metastasis in rectal cancer using MRI. The method reframes diagnosis as a structured reasoning and ranking process, first extracting patient-level radiological features from lymph node images via a multimodal LLM, then ranking patients by relative risk using a text-based LLM for pairwise comparisons. Evaluated on 117 patients, LRMR achieved an AUC of 0.7917 and an F1-score of 0.7200, outperforming deep learning baselines including ResNet50. Ablation studies showed significant performance drops without either the relational ranking or structured prompting stage. The framework offers interpretability and holistic assessment, advancing AI for clinical radiology diagnostics.

## Method Summary
LRMR reframes lymph node metastasis assessment as a two-stage process. Stage 1 compiles lymph node patches from each patient into labeled montage images and uses a multimodal LLM (Gemini 2.5 Flash) with a structured 10-question prompt to generate JSON reports containing radiological features for each node. Stage 2 uses a text-only LLM to perform pairwise comparisons between patients' JSON reports, aggregating "Net Wins" scores to create a relative risk ranking. The framework was evaluated on 117 patients with a 70/30 train/test split, using histopathological results as ground truth.

## Key Results
- Achieved AUC of 0.7917 and F1-score of 0.7200 on 36-patient test set
- Outperformed deep learning baselines including ResNet50
- Ablation studies showed performance drops to AUC 0.6875 without relational ranking and 0.6458 without structured prompting
- Framework demonstrated improved interpretability compared to black-box deep learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured prompting enforces consistent, clinically-relevant feature extraction from multimodal LLMs
- Mechanism: The 10-question prompt constrains the multimodal LLM to output a standardized JSON report targeting specific radiological features (shape, border contour, internal signal texture, fatty hilum, necrosis), rather than allowing open-ended descriptions that may omit critical indicators or vary in format across patients.
- Core assumption: Multimodal LLMs can reliably identify radiological features in compiled montage images when given explicit, checklist-style guidance.
- Evidence anchors:
  - [abstract] "Ablation studies confirmed the value of our two main contributions: removing the relational ranking stage or the structured prompting stage led to a significant performance drop, with AUCs falling to 0.6875 and 0.6458, respectively."
  - [section IV] "This result underscores the value of our structured prompting mechanism. Guiding the LLM's analysis with a checklist of specific, clinically relevant features is necessary for extracting reliable and consistent information."
  - [corpus] Weak direct corpus support for this specific mechanism; related work on interpretable LN prediction focuses on VAE-based feature encoding rather than structured prompting strategies.

### Mechanism 2
- Claim: Pairwise relational ranking captures nuanced relative risk assessments better than absolute classification
- Mechanism: Instead of predicting metastatic status directly, Stage 2 LLM compares two patients' structured reports and determines which presents higher risk based on type, severity, and number of abnormalities. The final risk score ("Net Wins") aggregates multiple comparisons, grounding each patient's risk relative to the cohort distribution rather than as an isolated absolute score.
- Core assumption: Relative comparisons between patients are cognitively easier and more robust for LLMs than assigning absolute risk scores, and the aggregation of pairwise judgments produces stable rankings.
- Evidence anchors:
  - [abstract] "ranking patients by relative risk using a text-based LLM for pairwise comparisons"
  - [section II.C] "We chose a relational ranking approach because determining a patient's risk profile by direct comparison against peers can be more robust than assigning an absolute, independent score."
  - [section III.E] Ablation without relational ranking: AUC dropped from 0.7917 to 0.6875, F1 from 0.7200 to 0.6000.
  - [corpus] Limited corpus validation for LLM-based pairwise ranking in medical imaging; most prior work uses direct classification or VAE-based encoding.

### Mechanism 3
- Claim: Holistic montage-based evaluation preserves patient-level context lost in node-by-node analysis
- Mechanism: All lymph node patches from a patient are compiled into a single labeled montage image (128×128 per patch, with node IDs overlaid). The multimodal LLM analyzes the entire montage in one pass, enabling it to reason about the overall pattern and constellation of nodal findings, rather than evaluating each node in isolation and then aggregating.
- Core assumption: The multimodal LLM can maintain attention across multiple patches in a montage and correctly attribute features to specific labeled nodes.
- Evidence anchors:
  - [abstract] "evaluates each lymph node as an isolated entity... neglecting the holistic, patient-level context"
  - [section II.B] "To enable a holistic assessment in one analytical step, these individual patches are algorithmically arranged into a single composite 'montage' image."
  - [corpus] Neighbor papers focus on node-level segmentation and classification; no direct corpus evidence for montage-based holistic evaluation efficacy.

## Foundational Learning

- Concept: Multimodal LLM structured output (JSON mode)
  - Why needed here: Stage 1 requires the multimodal LLM to output machine-readable JSON reports for programmatic use in Stage 2. Understanding how to prompt for and parse structured outputs is essential for reproducibility.
  - Quick check question: Can you configure a multimodal LLM (e.g., Gemini, GPT-4o) to return a valid JSON object with required keys and handle parse failures gracefully?

- Concept: Pairwise ranking and aggregation (Elo-like systems)
  - Why needed here: Stage 2 uses pairwise comparisons aggregated into "Net Wins" scores. Understanding how comparison-based ranking systems work helps diagnose instability and choose the number of opponents per patient.
  - Quick check question: Given a set of pairwise comparison results with some inconsistencies, how would you detect and handle cyclic preferences?

- Concept: Radiological LN features in rectal cancer MRI
  - Why needed here: The 10-question prompt targets clinically validated features (border irregularity, necrosis, fatty hilum, etc.). Engineers must understand what these features signify to evaluate prompt design and interpret LLM outputs.
  - Quick check question: Which radiological features are most predictive of metastatic involvement, and why might size criteria alone be insufficient?

## Architecture Onboarding

- Component map:
  - Input: T2-weighted LN image patches per patient → Montage compiler (128×128 patches, labeled N1, N2, ...)
  - Stage 1: Multimodal LLM (Gemini 2.5 Flash) + 10-question structured prompt → JSON report (10 features, abnormal node IDs)
  - Stage 2: Text-only LLM + pairwise prompt (two JSON reports) → comparison result (A/B/Comparable)
  - Aggregator: Net Wins calculation → risk score → threshold-based classification
  - Output: Patient-level risk ranking and binary metastasis prediction

- Critical path:
  1. Montage quality (patch resolution, labeling clarity)
  2. Prompt design for Stage 1 (completeness and clinical relevance of 10 features)
  3. LLM temperature setting (0.1 for Stage 1 determinism)
  4. Number of opponents per patient (N=6) for stable ranking
  5. Threshold selection using training set

- Design tradeoffs:
  - More opponents per patient increases ranking stability but raises API cost and latency
  - Lower temperature improves consistency but may reduce nuance in borderline cases
  - Montage size limits: too many patches may exceed LLM context window or reduce per-patch resolution

- Failure signatures:
  - JSON parse failures in Stage 1 output → prompt or model configuration issue
  - High rate of "Comparable" judgments in Stage 2 → prompt not distinguishing risk sufficiently
  - Ranking instability (high variance in Net Wins across runs) → insufficient opponents or temperature too high
  - Performance drop vs. baselines → check if structured prompt is being followed correctly

- First 3 experiments:
  1. **Prompt validation**: Run Stage 1 on 5-10 patients manually; verify JSON structure and feature coverage. Check if abnormal node IDs are correctly cited.
  2. **Pairwise consistency test**: For a subset of 10 patients, generate all pairwise comparisons and check for cycles (A>B, B>C, C>A). Quantify inconsistency rate.
  3. **Opponent count sweep**: Vary N (3, 6, 10) and measure ranking stability (variance in Net Wins across multiple runs) and final AUC on held-out set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the LRMR framework's performance robust across different Large Language Model backbones, or is it dependent on the specific capabilities of the proprietary model used?
- Basis in paper: [inferred] The experimental setup specifies the use of a single model version ("gemini-2.5-flash-preview-05-20") for both the visual and textual stages, without testing other providers or open-source alternatives.
- Why unresolved: LLMs vary significantly in their instruction-following abilities and visual reasoning skills; reliance on a single proprietary model raises concerns about reproducibility and vendor lock-in.
- What evidence would resolve it: Comparative experiments running the LRMR pipeline using alternative multimodal and text-based LLMs (e.g., GPT-4o, LLaMA-based models) on the same dataset.

### Open Question 2
- Question: Does the spatial arrangement of lymph nodes within the composite montage image introduce positional bias into the feature extraction process?
- Basis in paper: [inferred] The methods section describes algorithmically arranging variable-sized patches into a fixed montage, but does not analyze if the location (e.g., top-left vs. bottom-right) affects the LLM's attention.
- Why unresolved: Vision-language models can exhibit sensitivity to spatial layout; if the model implicitly weights nodes differently based on their grid position, the "holistic" assessment is compromised.
- What evidence would resolve it: Ablation studies where the node order is randomly shuffled for the same patients to verify that the generated structured reports and final rankings remain consistent.

### Open Question 3
- Question: How sensitive is the relational ranking stability to the number of randomly sampled opponents (N) used in the pairwise comparison stage?
- Basis in paper: [inferred] The authors fixed the opponent sample size at N=6 for the experiments but did not perform a sensitivity analysis on this hyperparameter.
- Why unresolved: With a small N, the resulting "Net Wins" score may be noisy and fail to converge on a true global ranking, whereas a large N increases computational cost and latency.
- What evidence would resolve it: Analysis of ranking consistency and AUC performance across a range of N values (e.g., 2 to 20) to identify the optimal trade-off.

### Open Question 4
- Question: Can the framework generalize to external datasets involving different MRI scanners and protocols without retraining or prompt engineering?
- Basis in paper: [inferred] The study utilizes a retrospective cohort from a single institution (Fudan University Shanghai Cancer Center), limiting the diversity of imaging data.
- Why unresolved: Single-center studies often fail to capture the variance in image contrast and noise found across different clinical sites, which could severely impact the Stage 1 visual analysis.
- What evidence would resolve it: Evaluation of the LRMR framework on a multi-center external test set to assess domain generalization capabilities.

## Limitations

- Small sample size (117 patients, 36 test) raises concerns about overfitting and statistical stability of reported metrics
- Lack of detailed specification for key implementation details (prompt templates, montage layout algorithm, patch extraction method) hinders reproducibility
- Limited direct corpus support for the specific mechanisms proposed (structured prompting for multimodal LLMs, pairwise ranking for radiological risk assessment)

## Confidence

- **High confidence** in the general framework design (two-stage LLM approach, use of structured prompting and pairwise ranking) and its novelty relative to prior work
- **Medium confidence** in the claimed performance gains over deep learning baselines, given the small test set and lack of confidence intervals
- **Medium confidence** in the specific mechanisms (structured prompting, pairwise ranking, montage-based holistic evaluation) due to limited direct empirical validation and corpus support

## Next Checks

1. **Prompt validation and feature coverage**: Run Stage 1 on a subset of 5-10 patients and manually verify that the structured JSON output correctly captures all 10 radiological features and accurately references abnormal lymph nodes. Assess if the prompt design is complete and clinically relevant.

2. **Pairwise comparison consistency audit**: For a sample of 10 patients, generate all possible pairwise comparisons and analyze for cycles or inconsistencies (e.g., A>B, B>C, but C>A). Quantify the rate of "Comparable" judgments and investigate if the prompt is failing to distinguish risk adequately.

3. **Opponent count sensitivity analysis**: Systematically vary the number of opponents per patient (N=3, 6, 10) and measure the impact on ranking stability (variance in Net Wins across multiple runs) and final AUC on the held-out test set. Determine if N=6 is optimal or if results are sensitive to this hyperparameter.