---
ver: rpa2
title: 'ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over
  Knowledge Graphs'
arxiv_id: '2602.02382'
source_url: https://arxiv.org/abs/2602.02382
tags:
- reasoning
- query
- logical
- complex
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROG, a retrieval-augmented LLM reasoning
  framework for answering first-order logic queries over incomplete knowledge graphs.
  ROG addresses the challenge of complex logical reasoning by decomposing queries
  into single-operator sub-queries and grounding each step in query-relevant neighborhood
  evidence retrieved from the KG.
---

# ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2602.02382
- **Source URL**: https://arxiv.org/abs/2602.02382
- **Reference count**: 12
- **Primary result**: Retrieval-augmented LLM reasoning achieves up to 35% MRR improvement over embedding-based baselines on complex FOL queries over incomplete KGs.

## Executive Summary
This paper introduces ROG, a retrieval-augmented LLM reasoning framework for answering first-order logic queries over incomplete knowledge graphs. ROG addresses the challenge of complex logical reasoning by decomposing queries into single-operator sub-queries and grounding each step in query-relevant neighborhood evidence retrieved from the KG. The method uses abstraction to replace entity/relation names with identifiers, reducing hallucination and improving portability. Intermediate answer sets are cached and reused, stabilizing multi-step inference. Experiments on standard benchmarks (FB15k and NELL995) show consistent gains in Mean Reciprocal Rank over strong embedding-based baselines, especially on high-complexity and negation-heavy query types, with improvements of up to 35% MRR. ROG provides a practical, adaptable alternative to learned embedding operators by leveraging retrieval and step-wise LLM inference.

## Method Summary
ROG processes first-order logic queries over incomplete knowledge graphs by decomposing them into single-operator sub-queries, retrieving compact, query-relevant k-hop neighborhoods for each step, and executing them sequentially using an LLM. Entities and relations are abstracted to unique identifiers to force reasoning from provided evidence rather than parametric memory, reducing hallucination. Intermediate answer sets are cached and referenced in later prompts to ensure consistency. The system uses ChatGLM as the base LLM, with fixed prompt templates that constrain output to newline-separated entity IDs or "NONE". Queries are parsed into a deterministic execution plan, executed step-by-step, and final answers are mapped back to surface forms. The approach is evaluated on FB15k and NELL995 benchmarks across 9+ query types.

## Key Results
- ROG achieves consistent MRR gains over embedding-based baselines (GQE, Query2Box, BetaE) on FB15k and NELL995.
- Largest improvements observed on negation-heavy query types (2in, 3in, inp, pin, pni), up to 35% MRR gain.
- Gains persist even on incomplete graphs where embedding methods struggle due to missing edges.
- Caching intermediate results improves consistency on deep reasoning chains.

## Why This Works (Mechanism)

### Mechanism 1: Query Decomposition with Local Retrieval Grounding
Decomposing multi-operator queries into single-operator sub-queries and grounding each step in compact neighborhood evidence reduces error compounding in complex reasoning chains. Complex FOL query → syntactic decomposition → each sub-query receives only query-relevant k-hop neighborhood → LLM executes one logical operation per step → intermediate results propagate forward. Assumes LLMs perform more reliably on simple, well-scoped operations with bounded evidence than on complex multi-operator reasoning in a single prompt.

### Mechanism 2: Entity/Relation Abstraction to Identifiers
Replacing surface-form names with abstract identifiers forces reasoning from provided evidence rather than parametric memory, reducing hallucination risk. Entity "Barack Obama" → ID "E_0421"; Relation "bornIn" → ID "R_0017" → LLM receives only abstract triples → must infer from serialized adjacency lists, not world knowledge. Assumes LLMs cannot reliably "fill in" missing graph structure from pretraining when identifiers are opaque; they must use presented evidence.

### Mechanism 3: Intermediate Answer Set Caching
Explicitly materializing and caching intermediate answer sets stabilizes multi-step inference by eliminating redundant computation and ensuring consistency across dependent sub-queries. Sub-query 1 output → cached as "Set_A" → subsequent prompts reference "Set_A" via placeholder → LLM does not re-derive → error localization possible via cache inspection. Assumes earlier steps produce sufficiently correct intermediate sets; caching errors does not help, but it prevents drift from re-prompting instability.

## Foundational Learning

- **First-Order Logic (FOL) Queries over KGs**
  - Why needed: ROG's entire input space is FOL queries composed of projection, intersection, union, and negation operators. Understanding operator semantics (e.g., "2p" = two sequential projections, "3i" = three-way intersection) is prerequisite to parsing the decomposition logic.
  - Quick check: Given entities A, B and relations R1, R2, what entities satisfy "∃x: R1(A, x) ∧ R2(B, x)"?

- **Knowledge Graph Incompleteness and Sparsity**
  - Why needed: The paper explicitly targets incomplete KGs where required facts may be missing. Understanding why embedding methods struggle here (latent space cannot represent missing edges) clarifies why retrieval-grounding is proposed as alternative.
  - Quick check: Why might a 3-hop projection query fail on a sparse KG even if the logical path exists in the real world?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed: ROG frames logical query answering as sequential CoT-style inference. Understanding how CoT decomposes reasoning into verifiable steps explains the design choice of single-operator sub-queries.
  - Quick check: How does CoT differ from single-prompt inference for multi-step arithmetic? What failure modes does it introduce?

## Architecture Onboarding

- **Component map**: Query Parser -> Abstraction Layer -> Decomposition Engine -> Retrieval Module -> LLM Inference Core -> Cache Manager -> Output Parser
- **Critical path**: Query input → Parse + Abstract → Decompose into sub-queries → For each sub-query: Retrieve neighborhood → Serialize evidence → Prompt LLM with operator + evidence + cached sets → Parse output → Cache result → Aggregate final answer set
- **Design tradeoffs**:
  - Retrieval granularity vs. context length: Larger neighborhoods provide more evidence but risk distraction and context overflow. Paper prioritizes query-relevant relations and truncates aggressively.
  - Decomposition depth vs. prompt count: Deeper decomposition simplifies each step but increases LLM calls and cache dependencies.
  - Strict output constraints vs. flexibility: Constraining output to newline-separated IDs improves determinism but limits handling of ambiguous cases.
  - Assumption: The paper assumes ChatGLM generalizes well to abstracted inputs—other LLMs may require prompt tuning.
- **Failure signatures**:
  - Empty/NONE outputs on queries that should return results: Indicates retrieval neighborhood insufficient—check k-hop depth, relation filtering, or KG incompleteness.
  - Hallucinated entity IDs not in evidence: Abstraction may have failed, or LLM ignored constraints—audit prompt templates.
  - Inconsistent intermediate sets across runs: Non-deterministic LLM outputs—consider multi-agent consensus or temperature=0.
  - Correct final answer but wrong intermediate cache: LLM may have "lucky" error cancellation—cache inspection required.
- **First 3 experiments**:
  1. Validate retrieval and abstraction on 1p queries: Run single-projection queries, verify LLM outputs match ground-truth neighbors in KG. Check that abstracted IDs serialize correctly.
  2. Test caching on 2p/3p multi-hop queries: Execute multi-hop projections, inspect cached intermediate sets at each step. Verify no information loss between stages.
  3. Compare negation-heavy queries against BetaE baseline: Run queries with ¬ operator (2in, 3in, inp, pin, pni), measure MRR gap. Analyze failure cases where ROG under-generates vs. embedding baselines.

## Open Questions the Paper Calls Out

- How can retrieval policies be optimized to dynamically balance context window constraints with the need for comprehensive evidence in dense graph regions?
- What specific mechanisms can effectively integrate symbolic execution plans with LLM prompting to reduce error rates in logical inference?
- Does the query-time computational overhead of sequential LLM calls and database retrieval limit ROG's scalability compared to embedding-based methods?
- Does the abstraction of entity names into identifiers degrade performance on sparse graphs where semantic priors could help bridge missing structural links?

## Limitations
- Prompt templates and evidence serialization formats are not disclosed, blocking faithful reproduction.
- Retrieval hyperparameters (k-hop depth, triple count caps, relation prioritization) are underspecified.
- Evaluation limited to FB15k and NELL995; generalization to truly sparse or domain-specific KGs untested.
- Abstraction prevents use of LLM's parametric knowledge, potentially hurting performance on extremely incomplete graphs.

## Confidence

- **High Confidence**: Decomposition mechanism and caching strategy well-justified by ablation and comparison results; robust gains on negation-heavy queries.
- **Medium Confidence**: Abstraction layer's effectiveness plausible but not empirically validated against non-abstracted baseline.
- **Low Confidence**: Exact prompt structure and retrieval hyperparameters critical but undisclosed; reproducibility uncertain.

## Next Checks

1. **Prompt Template Sensitivity Test**: Systematically vary prompt phrasing (e.g., evidence ordering, placeholder syntax, output constraints) and measure impact on MRR for a fixed query set. Identify which elements are truly critical.

2. **Retrieval Granularity Sweep**: Vary k-hop depth and neighborhood size limits across a range of query complexities. Plot MRR vs. retrieval cost to find optimal tradeoff points and assess brittleness to truncation.

3. **Cross-KG Generalization**: Apply ROG to a held-out KG (e.g., Wikidata subset) not seen during development. Compare performance drop against embedding-based baselines to assess real-world portability.