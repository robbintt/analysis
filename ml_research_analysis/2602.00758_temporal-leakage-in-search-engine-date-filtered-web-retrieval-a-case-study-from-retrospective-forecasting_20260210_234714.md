---
ver: rpa2
title: 'Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study
  from Retrospective Forecasting'
arxiv_id: '2602.00758'
source_url: https://arxiv.org/abs/2602.00758
tags:
- leakage
- information
- post-cutoff
- question
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that search-engine date filters are unreliable
  for retrospective forecasting evaluations, as they frequently return documents containing
  post-cutoff information. An audit of Google Search across 393 forecasting questions
  found that 71% returned at least one page with major post-cutoff leakage, and 41%
  contained documents directly revealing the answer.
---

# Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting

## Quick Facts
- **arXiv ID:** 2602.00758
- **Source URL:** https://arxiv.org/abs/2602.00758
- **Reference count:** 23
- **Primary result:** Date-filtered search returns post-cutoff information in 71% of cases, inflating LLM forecasting accuracy by Brier score from 0.24 to 0.108.

## Executive Summary
This paper demonstrates that search-engine date filters are unreliable for retrospective forecasting evaluations, as they frequently return documents containing post-cutoff information. An audit of Google Search across 393 forecasting questions found that 71% returned at least one page with major post-cutoff leakage, and 41% contained documents directly revealing the answer. This leakage artificially inflated LLM forecasting performance, improving Brier scores from 0.24 (baseline) to 0.108 when leaked documents were included. The authors identified four leakage mechanisms: updated page content, related-content modules, unreliable metadata, and absence-based signals. They conclude that date-restricted search is insufficient for temporal evaluation and recommend using frozen, time-stamped web snapshots instead.

## Method Summary
The study collected 393 resolved Metaculus forecasting questions (2021-2025) and used an LLM to generate 10-20 search queries per question. Google Search API retrieved ~100 URLs per question with the `before:` operator set to the question's open date. Retrieved pages were processed using MMR-based chunking for long documents and scored for leakage by gpt-oss-120b (temperature 0.5) using a 0-4 rubric. Forecasting experiments compared Brier scores using documents grouped by leakage level, with leak-free documents as baseline.

## Key Results
- 71% of questions returned at least one page containing strong post-cutoff leakage
- 41% of questions had documents directly revealing the answer (score-4 leakage)
- Brier score improved from 0.24 (leak-free) to 0.108 when leaked documents were included
- Four leakage mechanisms identified: updated content, related modules, unreliable timestamps, absence signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Search-engine date filters fail to exclude post-cutoff information because pages are dynamically updated after their original publication date.
- **Mechanism:** The `before:` operator filters by metadata timestamp (often publication date), not by content modification date. When a page published in 2017 is updated in 2023, the filter returns it as a "2017 document" despite containing 2023 information.
- **Core assumption:** Search engines index pages at their original URL and do not track content-level changes separately from metadata-level timestamps.
- **Evidence anchors:**
  - [abstract] "71% of questions return at least one page containing strong post-cutoff leakage"
  - [section 4.3] "the date filter returned a missile tracking database first published in 2017, but the page had been continuously updated to include launch activities through 2023"
- **Break condition:** If search engines tracked content-version timestamps separately from page-creation dates, this mechanism would be mitigated.

### Mechanism 2
- **Claim:** Static article pages leak post-cutoff information through dynamic page modules (sidebars, "related articles," "trending" sections).
- **Mechanism:** A 2016 article's main content is historically accurate, but the page template injects current content via client-side or server-side modules. Date filters evaluate the page URL/metadata, not the rendered DOM.
- **Core assumption:** Date-filtering operates at the page level, not at the component or DOM-element level.
- **Evidence anchors:**
  - [abstract] "related-content modules" identified as a leakage mechanism
  - [section 4.3] "a related articles section on the page included a snippet about a December 2023 ICBM launch, fully revealing the answer despite the main article predating the cutoff"
- **Break condition:** If retrieval systems fetched and parsed only the main article content (excluding template modules), this leakage path would close.

### Mechanism 3
- **Claim:** Self-reported timestamps on web pages are frequently stale or incorrect, bypassing secondary metadata filtering.
- **Mechanism:** Some pages display a "last updated" date that predates the cutoff while the actual content references post-cutoff events. Pipelines that scrape and filter by these dates are misled.
- **Core assumption:** Publishers do not reliably maintain accurate timestamps, and there is no universal verification mechanism.
- **Evidence anchors:**
  - [section 4.3] "a retrieved page reports as last updated in 2020 yet contains text stating Finland joined NATO in 2023"
  - [section 4.3] "This mechanism can bypass pipelines that double check retrieval by filtering on extracted publication or update dates"
- **Break condition:** If a trusted third-party timestamp verification layer existed (e.g., Internet Archive's crawl dates as canonical), this could be mitigated.

## Foundational Learning

- **Concept: Retrospective Forecasting (RF)**
  - **Why needed here:** The entire paper evaluates whether LLM forecasters can be fairly tested on past questions. Understanding RF explains why temporal leakage is a validity threat, not just a data-quality nuisance.
  - **Quick check question:** If you evaluate a forecaster on "Will X happen by 2024?" in 2025, what constraint must you enforce on the evidence they access?

- **Concept: Brier Score**
  - **Why needed here:** The paper quantifies leakage impact via Brier score changes (0.24 → 0.108). You need to understand that lower Brier = better, and a ~55% reduction signals substantial artificial inflation.
  - **Quick check question:** A Brier score of 0.25 on binary questions is equivalent to what naive baseline strategy?

- **Concept: Information Cutoff vs. Retrieval Date**
  - **Why needed here:** The distinction between "when the question was asked" (cutoff) and "when you retrieve documents" (retrieval date) is the root cause of leakage. Confusing these leads to invalid evaluations.
  - **Quick check question:** If a question opened on 2021-11-11 and you retrieve documents today using a date filter, what date should the filter use?

## Architecture Onboarding

- **Component map:** Query Generator -> Retrieval Layer -> Document Processor -> Leakage Detector -> Forecaster
- **Critical path:** Query generation → Date-filtered retrieval → Document fetching → Leakage scoring → Forecasting. The leakage score determines whether documents proceed to the forecaster.
- **Design tradeoffs:**
  - **Live search + date filter** (current approach): Fast, broad coverage, but 71% leakage rate.
  - **Frozen web snapshots** (recommended): Temporally valid, but limited coverage and requires pre-collection infrastructure.
  - **Post-hoc metadata filtering**: Adds a layer but fails when timestamps are unreliable (Mechanism 3).
- **Failure signatures:**
  - **Unexpectedly high accuracy:** Brier scores significantly better than human baselines may indicate leakage.
  - **Direct-answer phrases in retrieved text:** If documents contain explicit resolution statements, score-4 leakage is present.
  - **Recent dates in "historical" content:** Mentions of 2023 events in documents ostensibly from 2021.
- **First 3 experiments:**
  1. **Leakage audit replication:** Run the `before:` filter on 50 resolved questions, manually score the top 10 results per question for post-cutoff content.
  2. **Snapshot comparison:** For the same 50 questions, retrieve from a frozen snapshot (e.g., Internet Archive) and compare leakage rates to live search.
  3. **Ablation by leakage score:** Forecast using only score-0 documents vs. score-4 documents on the same question set; quantify Brier score delta.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does leakage prevalence and mechanism distribution differ substantially across alternative search engines (e.g., Bing, DuckDuckGo) and non-Metaculus forecasting domains?
  - **Basis in paper:** [explicit] "Our audit focuses exclusively on Google Search and the Metaculus forecasting platform... leakage prevalence and mechanisms may differ across other search engines or prediction domains."
  - **Why unresolved:** The study provides no data on whether the 71% major-leakage rate generalizes to other search infrastructure or question types.
  - **What evidence would resolve it:** Replicate the audit methodology across multiple search engines and diverse forecasting question sources.

- **Open Question 2:** Can frozen, time-stamped web snapshots fully eliminate temporal leakage without introducing new evaluation biases?
  - **Basis in paper:** [explicit] The authors recommend "evaluation on frozen, time-stamped web snapshots" but also note that live search ranking in snapshot-based systems "may still introduce bias, as the ordering of results reflects present-day relevance signals."
  - **Why unresolved:** The paper critiques date-filtered search but does not empirically validate that proposed alternatives solve the problem.
  - **What evidence would resolve it:** Comparative evaluation of forecasting performance using frozen snapshots versus date-filtered search on the same question set.

- **Open Question 3:** How does the MMR-based document truncation pipeline affect leakage detection rates for dispersed or subtle post-cutoff signals?
  - **Basis in paper:** [explicit] "Our document processing pipeline utilized Maximal Marginal Relevance (MMR) to handle long texts, which poses a risk that dispersed leakage signals in excluded chunks were omitted from our analysis."
  - **Why unresolved:** Long documents are truncated, potentially missing leakage in excluded passages; the false-negative rate is unknown.
  - **What evidence would resolve it:** Re-score full documents without truncation on a sample subset and compare detection rates.

- **Open Question 4:** Does susceptibility to temporal leakage vary across different LLM architectures or prompting strategies?
  - **Basis in paper:** [inferred] The study uses a single model (gpt-oss-120b) for both leakage scoring and forecasting experiments, with no comparison across architectures.
  - **Why unresolved:** Different models may weight leaked evidence differently or have varying sensitivity to absence-based signals.
  - **What evidence would resolve it:** Run the forecasting experiment across multiple LLM families with controlled leaked versus clean retrieval conditions.

## Limitations
- Model availability uncertainty: Study relies on gpt-oss-120b, not publicly accessible, introducing replication concerns
- Temporal sampling bias: 393 questions span 2021-2025, but leakage mechanisms may vary across domains
- Google-specific findings: All experiments used Google Search API; leakage mechanisms may differ for other search engines

## Confidence
- **High confidence:** The existence of temporal leakage in date-filtered search (71% prevalence) and its measurable impact on forecasting accuracy (Brier score improvement from 0.24 to 0.108).
- **Medium confidence:** The four specific leakage mechanisms identified, as they are based on qualitative examples rather than comprehensive quantitative validation.
- **Low confidence:** The recommendation that frozen snapshots are the only viable alternative, as this requires infrastructure assumptions not validated in the current study.

## Next Checks
1. **Human validation of leakage detection:** Have three independent annotators score 100 randomly selected documents from the original dataset to establish ground truth leakage rates and compare against LLM judge scores.
2. **Cross-engine replication:** Repeat the leakage audit using Bing Search API and a commercial web snapshot service (e.g., Internet Archive's Wayback Machine) to compare leakage rates across retrieval methods.
3. **Domain-specific analysis:** Stratify the 393 questions by topic domain and quantify leakage prevalence per domain to identify whether certain forecasting domains are disproportionately affected.