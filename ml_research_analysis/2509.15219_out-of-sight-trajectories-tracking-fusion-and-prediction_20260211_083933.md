---
ver: rpa2
title: 'Out-of-Sight Trajectories: Tracking, Fusion, and Prediction'
arxiv_id: '2509.15219'
source_url: https://arxiv.org/abs/2509.15219
tags:
- visual
- sensor
- trajectory
- prediction
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Out-of-Sight Trajectory (OST), a novel task
  predicting noise-free visual trajectories of out-of-sight objects using noisy sensor
  data. It addresses safety risks in autonomous systems where objects become occluded
  or exit camera view.
---

# Out-of-Sight Trajectories: Tracking, Fusion, and Prediction

## Quick Facts
- **arXiv ID:** 2509.15219
- **Source URL:** https://arxiv.org/abs/2509.15219
- **Authors:** Haichao Zhang; Yi Xu; Yun Fu
- **Reference count:** 37
- **Primary result:** State-of-the-art trajectory denoising and prediction for out-of-sight objects using camera-calibration-based unsupervised learning

## Executive Summary
This paper introduces the Out-of-Sight Trajectory (OST) task, predicting noise-free visual trajectories of objects outside camera view using noisy sensor data. The method employs a Vision-Positioning Denoising Module that leverages camera calibration to establish a vision-positioning mapping and denoises sensor data in an unsupervised manner. Evaluations on Vi-Fi and JRDB datasets show substantial performance improvements over baselines including Kalman filtering and recent prediction models. The plug-and-play design achieves significant gains while maintaining computational efficiency, representing the first integration of vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents.

## Method Summary
The framework uses a Vision-Positioning Denoising (VPD) module with four key components: a Sensor Denoising Encoder (SDE) that refines noisy sensor inputs, a Mapping Parameters Estimator (MPE) that learns camera projection matrices from in-view agents, a Visual Positioning Projection (VPP) that transforms denoised sensor data into visual coordinates, and an Out-of-Sight Prediction Decoder (OPD) that predicts future trajectories. The MPE analyzes dual-modality data from in-view agents to predict camera matrices, which are then applied to out-of-sight sensor data. The system is trained using ground truth visual trajectories of out-of-sight agents to supervise the denoising and prediction tasks, operating in an unsupervised manner without requiring clean sensor data.

## Key Results
- State-of-the-art performance on Vi-Fi and JRDB datasets for both trajectory denoising (MSE-D) and prediction (MSE-P)
- Plug-and-play design achieves substantial performance gains while maintaining computational efficiency
- Outperforms Kalman filtering and recent prediction models including LSTM and Transformer baselines
- First framework to integrate vision-positioning projection for unsupervised denoising of noisy sensor trajectories

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Calibration Transfer
The framework suggests that camera calibration parameters (intrinsic/extrinsic matrices) are shared across all agents in a scene, allowing geometric relationships learned from visible agents to process out-of-sight agents. A Mapping Parameters Estimator (MPE) analyzes dual-modality data from in-view agents to predict camera projection matrices, which are then applied to out-of-sight sensor data. The core assumption is that camera intrinsic matrix remains stable while extrinsic matrix changes predictably over time.

### Mechanism 2: Visual-Space Unsupervised Denoising
The model optimizes sensor trajectories in visual coordinate space as a proxy for ground-truth sensor denoising without requiring clean sensor data. The Sensor Denoising Encoder (SDE) refines noisy sensor inputs, which are projected into visual domain using MPE-derived matrix and minimized against ground-truth visual trajectory distance. Visual trajectories serve as effective supervisory signal since they are inherently noise-free compared to sensor trajectories.

### Mechanism 3: Modular Plug-and-Play Enhancement
The denoising task is decoupled from prediction task, allowing the framework to act as pre-processing module that improves existing trajectory prediction models. The VPD module provides clean visual-space representation of history, which standard predictors (LSTM, Transformers) consume to generate more accurate future paths. Quality of historical trajectory is the primary bottleneck for predicting out-of-sight agents.

## Foundational Learning

- **Camera Projection Geometry (World-to-Image):** Understand how to transform 3D sensor coordinates into 2D camera pixels, including intrinsic (focal length, center) vs extrinsic (rotation, translation) parameters. Quick check: If camera rotates 90 degrees, does intrinsic or extrinsic matrix change?

- **Unsupervised Learning via Proxy Loss:** Learn to denoise without clean sensor data by defining loss function on related but different domain (visual space). Quick check: Why is L2 loss safer in visual domain than sensor domain for this architecture?

- **Transformers for Sequence Modeling:** Understand attention mechanisms for time-series smoothing used in both denoising (SDE) and parameter estimation (MPE). Quick check: How does Transformer handle variable sequence lengths compared to RNN?

## Architecture Onboarding

- **Component map:** Input (Noisy Sensor Trajectory + In-View Visual Trajectories) -> MPE (learns Camera Matrix) -> SDE (cleans Sensor) -> VPP (projects to Visual) -> OPD (predicts future)

- **Critical path:** The MPE is the keystone. If it fails to accurately predict camera matrix embedding from in-view agents, the VPP will misalign, causing denoising loss to penalize wrong behaviors and final prediction to diverge.

- **Design tradeoffs:** Visual proxy vs sensor truth - model optimizes for visual alignment, not sensor fidelity. Dependency on in-view agents - system cannot calibrate if zero agents are visible to camera.

- **Failure signatures:** Sudden drift if MPE produces unstable matrix sequence causing projected trajectory to jitter. Empty input if set of in-view agents is empty during inference.

- **First 3 experiments:** 1) Static calibration check - freeze MPE and feed known camera matrices to isolate SDE performance. 2) Ablation on in-view agents - systematically reduce number to see how many are required for stable matrix estimation. 3) Noise injection - add Gaussian noise to ground truth visual trajectories to test robustness against visual tracker jitter.

## Open Questions the Paper Calls Out

- **Distance generalization:** How does prediction accuracy degrade for extreme distances (several miles) beyond current dataset ranges? The authors explicitly state this remains unevaluated.

- **Real-time implementation:** Can VPD framework be optimized for strict latency requirements in high-speed autonomous systems? Real-world camera operations like zooming and autofocus can modify intrinsic matrix.

- **Dynamic calibration:** How to adapt MPE to maintain accuracy during rapid camera intrinsic changes like zooming or autofocus? Current method estimates unified embedding that may fail with dynamic optical properties.

## Limitations

- Heavy dependence on visible in-sight agents for camera parameter estimation, with no solution for zero visibility scenarios
- Assumes visual trajectories are significantly cleaner than sensor data, which may not hold with systematic visual tracking errors
- Framework assumes stable camera calibration parameters, limiting generalization to dynamic camera systems

## Confidence

- **High confidence:** Modular plug-and-play design demonstrably improves baseline predictors (validated by Table I results)
- **Medium confidence:** Unsupervised denoising mechanism via visual-space supervision is theoretically sound but lacks empirical validation against true sensor ground truth
- **Medium confidence:** Cross-agent calibration transfer is plausible given shared camera parameters but untested for scenarios with very few in-view agents

## Next Checks

1. **MPE Stability Test:** Systematically reduce number of in-view agents from N to 1 and measure degradation in camera matrix estimation accuracy and downstream OST performance

2. **Visual Noise Robustness:** Add Gaussian noise to ground-truth visual trajectories during training to quantify sensitivity to visual tracker jitter

3. **Cross-Domain Generalization:** Evaluate model on dataset with different sensor type (IMU or radar) to test whether unsupervised denoising framework generalizes beyond GPS and LiDAR