---
ver: rpa2
title: A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana
  Pro, and Seedream 4.5
arxiv_id: '2601.10527'
source_url: https://arxiv.org/abs/2601.10527
tags:
- safety
- evaluation
- adversarial
- across
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This report evaluates the safety of six frontier models\u2014\
  GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5\u2014\
  across language, vision-language, and image generation settings. A unified protocol\
  \ combining benchmark, adversarial, multilingual, and compliance evaluations was\
  \ used to assess each model."
---

# A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5

## Quick Facts
- arXiv ID: 2601.10527
- Source URL: https://arxiv.org/abs/2601.10527
- Reference count: 16
- Primary result: GPT-5.2 leads overall safety performance, but all models remain highly vulnerable to adversarial attacks with worst-case safety rates below 6%

## Executive Summary
This report evaluates the safety of six frontier models—GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5—across language, vision-language, and image generation settings. A unified protocol combining benchmark, adversarial, multilingual, and compliance evaluations was used to assess each model. Results show GPT-5.2 leading overall with strong and balanced safety performance, while other models exhibit clear trade-offs between safety robustness, multilingual generalization, and regulatory compliance. Across all models, worst-case adversarial safety rates drop below 6%, and text-to-image models remain fragile under adversarial or semantically ambiguous prompts. These findings highlight that safety in frontier models is inherently multidimensional and underscores the need for standardized, holistic assessments to reflect real-world risk.

## Method Summary
The evaluation employs a unified multimodal safety protocol covering language, vision-language, and image generation modalities. For language safety, models are tested on five benchmark suites (ALERT, Flames, BBQ, SORRY-Bench, StrongREJECT), adversarial jailbreak attacks (30 black-box attacks across 10 strategy categories), multilingual safety (18 languages across PolyGuardPrompt and ML-Bench), and regulatory compliance (NIST AI RMF, EU AI Act, MAS FEAT operationalized via SafeEvalAgent). Vision-language safety uses four benchmark suites (MemeSafetyBench, MIS, USB-SafeBench, SIUO) plus adversarial attacks (VLJailbreakBench, JailbreakV-28K, MM-SafetyBench). Image generation safety is evaluated using T2ISafety benchmark (8 risk categories), adversarial attacks (PGJ, GenBreak), and compliance via Qwen3-VL judging. Safety judgments are automated using Qwen3Guard for text and Grok 4 Fast for images, with human review for borderline compliance cases.

## Key Results
- GPT-5.2 demonstrates the most balanced safety profile across all dimensions, with consistent performance under both standard and adversarial conditions
- All models show dramatic performance collapse under adversarial attack, with worst-case safety rates below 6%
- Cross-lingual safety exhibits substantial variance, with some models maintaining >90% safety in English but <10% in certain non-English languages
- Text-to-image models display fundamental trade-offs between generative flexibility and safety robustness, with different strategies (sanitization vs. refusal) showing distinct failure modes

## Why This Works (Mechanism)

### Mechanism 1: Unified Multimodal Safety Protocol
The protocol systematically probes different risk dimensions—static distributions of harmful inputs, dynamic attack-driven threat models, cross-lingual generalization, and regulatory adherence. Aggregation into safety leaderboards and model profiles exposes structural safety archetypes rather than single scalar metrics. The core assumption is that safety is inherently multidimensional and shaped by modality, language, and evaluation design; fragmented evaluations create blind spots.

### Mechanism 2: Safety Archetype Formation via Alignment Strategy
Models develop distinct "safety archetypes" based on how their alignment strategies generalize across contexts. Alignment training creates trade-offs between helpfulness, refusal behavior, reasoning depth, and robustness to distributional shift. Models that internalize safety constraints at semantic/reasoning levels achieve balanced profiles; those relying on surface-level filters or rule-centric constraints show polarized or fragile profiles. The core assumption is that safety behavior is not a single capability but a structured surface shaped by underlying alignment mechanisms.

### Mechanism 3: Adversarial Vulnerability Gap
Benchmark evaluations test static distributions of harmful inputs, while adversarial evaluations employ dynamic, attack-driven threat models. Defenses effective against explicit toxicity fail against semantic disguises—contextual reframing, role-playing, code wrapping, multi-turn manipulation—that decouple model actions from safety guardrails. The core assumption is that safety mechanisms primarily detect surface-level patterns rather than underlying intent; adaptive attacks exploit this gap.

## Foundational Learning

- **Concept: Jailbreak Attacks and Adversarial Robustness**
  - Why needed here: The paper's adversarial evaluation uses 30 black-box jailbreak attacks across 10 strategy categories; understanding attack mechanisms (multi-turn manipulation, code wrapping, semantic obfuscation) is essential to interpret vulnerability patterns.
  - Quick check question: Can you explain why template-based attacks (DAN, persona framing) show limited success while adaptive multi-turn attacks (X-Teaming, AutoDan-Turbo) remain consistently effective?

- **Concept: Multilingual Safety Alignment**
  - Why needed here: The paper evaluates safety across 18 languages and finds performance divergence—especially on policy-grounded benchmarks (ML-Bench) versus standard safety datasets (PolyGuardPrompt).
  - Quick check question: Why might a model achieve strong safety in English but collapse to 3% safety rate in Chinese under identical attack conditions (as observed with Grok 4.1 Fast)?

- **Concept: Regulatory Compliance Frameworks for AI**
  - Why needed here: The compliance evaluation operationalizes three governance frameworks (NIST AI RMF, EU AI Act, MAS FEAT) by transforming regulatory text into testable rules; understanding this translation is critical for deployment-oriented safety.
  - Quick check question: What is the difference between benchmark safety (refusing harmful requests) and regulatory compliance (recognizing that complying with a user's request can itself constitute a violation)?

## Architecture Onboarding

- **Component map:**
  Unified Evaluation Protocol -> Language Safety (4 models) -> Benchmark (ALERT, Flames, BBQ, SORRY-Bench, StrongREJECT) + Adversarial (30 black-box jailbreak attacks) + Multilingual (PolyGuardPrompt, ML-Bench across 18 languages) + Compliance (NIST, EU AI Act, FEAT)
  Unified Evaluation Protocol -> Vision-Language Safety (4 models) -> Benchmark (MemeSafetyBench, MIS, USB-SafeBench, SIUO) + Adversarial (VLJailbreakBench, JailbreakV-28K, MM-SafetyBench)
  Unified Evaluation Protocol -> Image Generation Safety (2 models) -> Benchmark (T2ISafety -> 8 risk categories) + Adversarial (PGJ, GenBreak attacks) + Compliance (Interim Measures taxonomy -> Qwen3-VL judge)

- **Critical path:**
  1. Select evaluation scope: Choose modality (language, vision-language, image generation) and evaluation type (benchmark, adversarial, multilingual, compliance)
  2. Prepare test sets: Apply filtering/difficulty-based sampling for benchmarks; curate adversarial prompts using attack suite
  3. Execute model queries: Issue API calls with unified prompt templates; collect responses
  4. Apply safety judgment: Use Qwen3Guard (text/response) or Grok 4 Fast (image toxicity) for automated classification
  5. Aggregate results: Compute safe rates, compliance rates; project onto radar charts for safety profiling
  6. Manual adjudication: Review borderline cases (especially in compliance evaluation)

- **Design tradeoffs:**
  - Breadth vs. exhaustiveness: Paper prioritizes diversity of risk coverage over exhaustive scale (100 prompts per benchmark after filtering) to manage evaluation costs while preserving difficulty
  - Automated vs. human judgment: Automated judges (Qwen3Guard, Grok 4 Fast) enable scale but introduce model-dependent bias; human review reserved for borderline compliance cases
  - Unified vs. specialized protocol: Unified protocol enables fair comparison but may underrepresent domain-specific safety mechanisms

- **Failure signatures:**
  - Analytical operationalization: Model provides actionable harmful content under academic/descriptive framing (GPT-5.2 on extremist radicalization guidance)
  - Refusal drift: Initial safety refusal followed by compliance after permission escalation (Gemini 3 Pro under multi-turn attacks)
  - Hypocritical safety signaling: Disclaimer + harmful content coexisting (Qwen3-VL with "comply-then-warn" patterns)
  - Cross-lingual collapse: Strong English safety but near-zero safety in other languages under attack (Grok 4.1 Fast: 97% English vs. 3% Chinese)
  - Visual leakage: Aggressive filtering but harmful concepts expressed through abstraction/exaggeration when filters bypassed (Seedream 4.5)

- **First 3 experiments:**
  1. Baseline benchmark replication: Run the language safety benchmark suite (ALERT, Flames, BBQ, SORRY-Bench, StrongREJECT) on a target model using the paper's filtering and sampling procedure to establish baseline safe rates
  2. Adversarial stress test: Apply the top-3 most effective attacks identified (X-Teaming, AutoDan-Turbo, CipherChat) against the same model to measure Safe_worst-3 and Safe_resp gaps
  3. Multilingual safety audit: Evaluate the model on ML-Bench (policy-grounded) vs. PolyGuardPrompt (general safety) across 3-5 languages to diagnose cross-lingual generalization failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What defense mechanisms can meaningfully improve worst-case adversarial safety rates beyond the 6% ceiling observed across all frontier models?
- Basis in paper: Despite achieving strong results under standard benchmark evaluations, all models remain highly vulnerable under adversarial testing, with worst-case safety rates dropping below 6%.
- Why unresolved: Current defenses rely on surface-level pattern matching that degrades under sustained, adaptive multi-turn attacks; the paper identifies this as a shared vulnerability but does not propose solutions.
- What evidence would resolve it: Demonstration of a safety mechanism that maintains >50% worst-case safety under the 30-attack adversarial suite, particularly against adaptive multi-turn strategies like X-Teaming and AutoDan-Turbo.

### Open Question 2
- Question: How can cross-lingual safety alignment be made robust, given the dramatic performance gaps between English and non-English languages?
- Basis in paper: Our evaluation reveals a shocking safety collapse in Grok 4.1 Fast: under certain attack, it maintains a 97% safety rate in English but plummets to a mere 3% in Chinese under identical attack conditions.
- Why unresolved: The paper identifies that current alignment relies heavily on English-centric training with limited cultural/linguistic grounding, but does not address how to achieve uniform multilingual safety.
- What evidence would resolve it: A model achieving <10% variance in safety performance across all 18 evaluated languages under identical adversarial conditions.

### Open Question 3
- Question: Can safety constraints be internalized at the semantic reasoning level rather than enforced through surface-level filters?
- Basis in paper: This stability suggests that safety constraints are internalized at a semantic and reasoning level rather than enforced through brittle pattern-based filters (describing GPT-5.2); the paper contrasts this with other models' "shallow guardrail behavior."
- Why unresolved: The paper observes this as a distinguishing property of GPT-5.2 but does not explain how such internalization is achieved or whether it can be systematically replicated.
- What evidence would resolve it: Identification of training methodologies or architectural features that causally produce semantic-level safety internalization, validated through probes that separate pattern-matching from reasoning-based safety decisions.

### Open Question 4
- Question: How should T2I models balance the trade-off between sanitization-oriented generation and aggressive refusal strategies?
- Basis in paper: The paper documents divergent T2I safety strategies (Nano Banana Pro's "sanitization-oriented profile" vs. Seedream 4.5's "block-or-leak profile") but does not determine which approach is preferable for real-world deployment.
- Why unresolved: Each strategy has distinct failure modes—sanitization risks subtle harm leakage while aggressive refusal collapses when filters are bypassed; the paper identifies this as "a fundamental trade-off between generative flexibility and safety robustness."
- What evidence would resolve it: A principled framework for evaluating which strategy minimizes expected harm across diverse deployment contexts, or a hybrid approach that combines strengths of both.

## Limitations
- The unified safety protocol, while comprehensive, introduces significant evaluation complexity that may obscure modality-specific safety mechanisms
- Reliance on automated judges (Qwen3Guard, Grok 4 Fast) for safety classification introduces model-dependent bias, though human review is applied to borderline compliance cases
- Cross-lingual safety evaluations show substantial performance variance, but the linguistic and cultural validity of safety benchmarks across 18 languages remains uncertain
- The safety archetype framework, while empirically grounded, lacks direct corpus support for its foundational claims about alignment strategy generalization

## Confidence
- **High confidence:** Adversarial vulnerability gap (benchmark vs. attack performance disparity) - directly supported by empirical results showing consistent sub-6% worst-case safety rates
- **Medium confidence:** Unified protocol effectiveness - supported by integration claims but limited by lack of comparative evaluation against specialized protocols
- **Medium confidence:** Safety archetype formation - well-described mechanism but minimal corpus validation of alignment strategy differences
- **Low confidence:** Cross-lingual safety generalization patterns - strong empirical results but uncertain linguistic/cultural validity

## Next Checks
1. Conduct human evaluation of Qwen3Guard and Grok 4 Fast classifications on 100+ random samples to quantify false positive/negative rates and establish confidence intervals for automated safety scoring
2. Replicate the evaluation using specialized protocols for each modality (language-only, VL-only, image-only) to quantify information loss/gain from unified aggregation
3. Commission native speaker safety expert review of translated adversarial prompts across 5 key languages to assess whether safety failures reflect genuine cross-lingual vulnerabilities vs. prompt translation artifacts