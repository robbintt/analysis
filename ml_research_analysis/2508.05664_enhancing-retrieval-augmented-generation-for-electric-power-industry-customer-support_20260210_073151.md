---
ver: rpa2
title: Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer
  Support
arxiv_id: '2508.05664'
source_url: https://arxiv.org/abs/2508.05664
tags:
- dataset
- queries
- retrieval
- accuracy
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates retrieval-augmented generation techniques
  for electric power industry customer support. The study compares vector-store and
  graph-based RAG frameworks, ultimately selecting a graph-based approach for superior
  performance on complex queries.
---

# Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support

## Quick Facts
- arXiv ID: 2508.05664
- Source URL: https://arxiv.org/abs/2508.05664
- Reference count: 14
- Primary result: Optimized graph-based RAG achieves 97.9% accuracy on GPT-4-generated dataset and 89.6% on real-world FAQ dataset

## Executive Summary
This paper evaluates retrieval-augmented generation techniques for electric power industry customer support, comparing vector-store and graph-based RAG frameworks. The study identifies a graph-based approach as superior for handling complex queries with disambiguation and multi-source requirements. Through systematic optimization including query rewriting, RAG Fusion, intent recognition, and context reranking, the authors develop a pipeline that significantly outperforms baseline RAG models on both synthetic and real-world datasets.

## Method Summary
The research builds on LightRAG (graph-based) as a baseline, then applies a series of optimizations: query rewriting using LLMs to clarify and technicalize queries, RAG Fusion generating sub-queries in English to retrieve multiple contexts, KNN-based intent recognition to identify top-2 intents, and context reranking using semantic similarity between query, entities, and relations. The final pipeline combines intent recognition, RAG Fusion, and reranking. Keyword augmentation was tested but excluded due to biased entity selection degrading recall. The system is evaluated on both a GPT-4-generated dataset (191 questions) and a real-world FAQ dataset (48 questions from multiple utilities).

## Key Results
- Graph-based LightRAG with optimizations achieves 97.9% accuracy on synthetic dataset vs 73.3% baseline
- Same pipeline achieves 89.6% accuracy on real-world FAQ dataset vs 58.5% baseline
- Query rewriting and RAG Fusion significantly improve retrieval of relevant contexts
- Context reranking (top-10 docs + top-15 entities/relations, reversed order) mitigates "Lost in the Middle" issue
- Keyword augmentation was found detrimental due to biased entity selection

## Why This Works (Mechanism)
The graph-based approach excels at capturing complex relationships between entities and concepts specific to the electric power domain, enabling better handling of multi-intent and detail-specific queries. Query rewriting improves the clarity and technical specificity of user queries before retrieval, while RAG Fusion generates multiple sub-queries to capture different aspects of complex questions. Intent recognition constrains the search space by identifying primary user goals, and context reranking ensures the most relevant information appears prominently in the generation context.

## Foundational Learning
- **LightRAG (graph-based RAG)**: Uses knowledge graphs to represent entity relationships rather than pure vector similarity; needed for complex domain-specific queries where relationships matter; quick check: verify graph construction captures power industry entities and their connections
- **RAG Fusion**: Generates multiple sub-queries from a single user query and merges results; needed for multi-intent questions; quick check: ensure sub-queries are semantically diverse
- **Context reranking with reversed order**: Sorts retrieved documents by semantic similarity but reverses order before generation; needed to combat "Lost in the Middle" where LLMs ignore middle context; quick check: verify middle documents contain critical information
- **KNN intent recognition**: Uses nearest-neighbor classification to identify user intent categories; needed to constrain search scope for ambiguous queries; quick check: confirm intent taxonomy covers power industry use cases
- **Semantic similarity evaluation**: Uses Spacy-based similarity scoring rather than exact match; needed for nuanced answer quality assessment; quick check: test similarity scores on paraphrased answers
- **Recall@10/20 metrics**: Measures retrieval completeness by checking if gold documents appear in top-N results; needed to evaluate retrieval effectiveness independently of generation; quick check: manually verify gold document annotations

## Architecture Onboarding

Component map: User Query -> Query Rewriting -> Intent Recognition -> RAG Fusion -> Context Reranking -> Answer Generation

Critical path: Intent Recognition -> RAG Fusion -> Context Reranking -> Generation (reranking ensures relevant contexts are prioritized)

Design tradeoffs: Graph-based vs vector-store RAG (graphs better for complex relationships but potentially slower); English sub-queries even for Chinese queries (leverages stronger English LLMs but requires translation layer)

Failure signatures: Low recall despite high answer similarity suggests keyword augmentation bias; inconsistent sub-query contexts indicate need for stronger intent constraints; middle-document information loss indicates reranking order needs adjustment

First experiments: 1) Test graph construction on sample power industry text to verify entity relationship capture; 2) Run RAG Fusion with simple queries to verify sub-query diversity; 3) Validate reranking by checking if relevant documents move to top positions

## Open Questions the Paper Calls Out
- **Multi-turn dialog optimization**: The authors explicitly state future work will focus on optimizing the pipeline for multi-turn dialog interactions, as the current study only handles single-turn query resolution without memory or context retention mechanisms.
- **Robust reasoning processes**: The paper identifies using more robust reasoning processes for post-retrieval augmentation as a specific future direction, noting that while retrieval and reranking are optimized, advanced reasoning steps to synthesize conflicting or complex contexts are not implemented.
- **Generalizability to human-authored data**: The primary dataset was GPT-4-generated, raising questions about whether the high performance (97.9%) generalizes to larger, purely human-authored datasets with messier, more diverse syntax than synthetic data.

## Limitations
- Limited detail on source text corpus for Dataset 1 generation and gold document annotations for recall evaluation
- Missing specification of the exact LLM used for query rewriting, sub-query generation, and answer generation
- No explanation for why keyword augmentation degraded performance despite being a common RAG enhancement technique
- Small real-world FAQ dataset (48 questions) may not capture full complexity of production customer support traffic

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Graph-based RAG with optimizations achieves reported accuracy scores | High |
| Individual optimization components (query rewriting, RAG Fusion, etc.) improve performance | Medium |
| Generalizability to other domains beyond electric power | Low |

## Next Checks
1. Test the optimized pipeline on a different domain (e.g., telecommunications or healthcare) to assess cross-domain transferability
2. Conduct ablation studies with alternative embedding models and similarity metrics for the reranking component
3. Implement the full system with specified components (LLM choice, embedding model, intent taxonomy) to verify reproducibility of the reported 97.9% and 89.6% accuracy scores