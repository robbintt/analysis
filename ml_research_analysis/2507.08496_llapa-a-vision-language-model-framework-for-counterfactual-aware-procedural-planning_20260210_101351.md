---
ver: rpa2
title: 'LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural
  Planning'
arxiv_id: '2507.08496'
source_url: https://arxiv.org/abs/2507.08496
tags:
- counterfactual
- planning
- arxiv
- visual
- llapa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLaPa is a vision-language model framework for procedural planning
  that addresses the dual challenges of multimodal perception and counterfactual reasoning.
  It introduces two specialized modules: a Task-Environment Reranker that uses task-oriented
  segmentation to focus on task-relevant objects, and a Counterfactual Activities
  Retriever that detects and emphasizes counterfactual conditions through visual token
  selection.'
---

# LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning

## Quick Facts
- **arXiv ID**: 2507.08496
- **Source URL**: https://arxiv.org/abs/2507.08496
- **Reference count**: 40
- **Primary result**: State-of-the-art performance on counterfactual procedural planning with 53.2% executability and 36.1% correctness

## Executive Summary
LLaPa is a vision-language model framework that addresses the dual challenges of multimodal perception and counterfactual reasoning in procedural planning. It introduces two specialized modules: a Task-Environment Reranker that uses task-oriented segmentation to focus on task-relevant objects, and a Counterfactual Activities Retriever that detects and emphasizes counterfactual conditions through visual token selection. The framework generates executable action sequences from textual task descriptions and environmental images, achieving state-of-the-art performance on both ActPlan-1K and ALFRED benchmarks.

## Method Summary
LLaPa employs a two-stage training approach with a VLM backbone (InternVL2-8B with InternViT encoder) augmented by TER and CAR modules. Stage 1 trains only the counterfactual classifier using binary cross-entropy loss on COPLAN and ActPlan-1K datasets with all other parameters frozen. Stage 2 fine-tunes TER's self-attention layers, MLP projector, and LLM using language modeling loss on EgoCOT and ActPlan-1K. TER uses Grounded-SAM for task-oriented segmentation and integrates binary masks into self-attention via log(W) bias, while CAR detects counterfactual clauses, extracts corresponding visual tokens via conditional pooling, and concatenates them with a special prompt for LLM decoding.

## Key Results
- Achieves 53.2% executability and 36.1% correctness on counterfactual tasks in ActPlan-1K benchmark
- Outperforms advanced models by 8.4% executability and 5.4% correctness on counterfactual tasks
- Demonstrates strong transferability when integrated with other VLMs, improving Qwen2-VL-7B correctness from 32.9% to 41.1%

## Why This Works (Mechanism)

### Mechanism 1
TER improves visual grounding by selectively emphasizing task-relevant visual regions through attention-based feature reranking. It employs Grounded-SAM to generate binary masks for objects mentioned in task descriptions, uses adaptive max pooling to produce patch-level weight matrices, and integrates these weights into self-attention via log(W) bias to suppress irrelevant regions while amplifying task-critical features.

### Mechanism 2
CAR enables single-pass detection and visual grounding of conditional clauses without requiring iterative replanning loops. It segments text embeddings into clauses, uses an MLP classifier with sigmoid activation to compute counterfactual likelihood per clause, and for detected counterfactuals, extracts compact features from K×K sub-grids using conditional pooling, preserving only regions where counterfactual masks equal 1.

### Mechanism 3
Staged training separates classifier optimization from generation fine-tuning, preventing gradient interference between counterfactual detection and action-sequence learning. Stage 1 trains only the counterfactual classifier with frozen backbone, while Stage 2 fine-tunes TER's self-attention layers, MLP projector, and LLM, optimizing the total loss as L = L_cf + L_LLM but executing stages sequentially.

## Foundational Learning

- **Concept**: Self-attention with additive bias
  - **Why needed here**: TER integrates binary masks into self-attention via log(W) bias term; understanding how additive masks modify attention distributions is essential for debugging reranking effectiveness.
  - **Quick check question**: Given attention logits [2.1, 1.5, 0.8] and mask bias log([1, 1, 0]), what is the resulting attention distribution after softmax?

- **Concept**: Task-oriented segmentation vs. generic semantic segmentation
  - **Why needed here**: TER relies on promptable segmentation that conditions on task text, not just visual salience; the difference determines whether segmentation produces task-relevant or merely prominent regions.
  - **Quick check question**: In a kitchen scene with task "microwave cleaning," should a prominently visible but task-irrelevant object (e.g., refrigerator) receive a non-zero mask? How does Grounded-SAM handle this?

- **Concept**: Conditional pooling for feature compression
  - **Why needed here**: CAR extracts K² compact tokens from masked visual features; understanding how selective aggregation preserves signal while suppressing noise is critical for setting K appropriately.
  - **Quick check question**: If a sub-grid g contains 16 patches but only 3 have W^cf = 1, how does conditional pooling differ from average pooling over all 16 patches?

## Architecture Onboarding

- **Component map**: Input Images I, Text S → Vision Encoder → v → TER Module → v_rerank → CAR Module → v_cf → MLP Projector → s_input → LLM → Action sequence A

- **Critical path**: Grounded-SAM mask quality → TER attention weighting → v_rerank quality → Classifier accuracy → Correct K_cf selection → v_cf relevance → Prompt construction → LLM context → Action correctness

- **Design tradeoffs**:
  - K=4 balances granularity and noise (optimal at 36.4% correctness); K=2 loses detail (32.5%), K=16 introduces noise (21.5%)
  - Grounded-SAM provides open-vocabulary grounding but adds inference overhead (~50-100ms per image)
  - Freezing encoders preserves pre-trained features but limits domain adaptation
  - Bitwise OR (42.0% correctness) outperforms Sum (37.1%) aggregation on ActPlan-1K

- **Failure signatures**:
  - Hallucination of extra tools: TER fails to suppress irrelevant objects; check if W has non-zero values outside task-relevant regions
  - Ignored counterfactuals: CAR classifier misses condition; inspect p_k scores for counterfactual clauses
  - Incorrect image understanding: Segmentation mislocalizes objects; visualize M_i^j overlays on input images
  - Cascade failure: If TER produces poor v_rerank, CAR's conditional pooling amplifies noise

- **First 3 experiments**:
  1. TER isolation test: Run TER-only variant on ActPlan-1K normal tasks; expect ~62.9% executability. Visualize attention maps W to verify task-relevant object highlighting.
  2. CAR classifier calibration: Evaluate classifier α_cls on held-out counterfactual clauses; target accuracy >85%. Plot precision-recall curve.
  3. Module transfer validation: Integrate TER+CAR into Qwen2-VL-7B; expect correctness improvement from 32.9% to ~41.1%.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can LLaPa's dependency on comprehensive textual task descriptions be reduced or eliminated to enable deployment in scenarios with minimal or ambiguous instructions? The current architecture requires detailed textual input to generate counterfactual conditions via the CAR module.
- **Open Question 2**: How can dynamic video context integration be incorporated into LLaPa to improve real-time adaptability in embodied scenarios? TER operates on static images using patch-level ViT features, and CAR extracts counterfactual tokens from single frames.
- **Open Question 3**: What is the computational overhead and latency introduced by the TER and CAR modules during inference, and how does this scale with image resolution and number of counterfactual conditions? The paper provides no analysis of inference time, parameter counts for auxiliary modules, or comparisons of computational cost relative to the base VLM.

## Limitations

- Heavy reliance on comprehensive textual task descriptions, limiting effectiveness in scenarios with minimal or ambiguous instructions
- Current approach does not support dynamic video context integration, which could further enhance adaptability in real-world scenarios
- No analysis of computational overhead and latency introduced by TER and CAR modules during inference

## Confidence

- **High confidence**: LLaPa's core architecture (TER + CAR modules) and state-of-the-art performance on ActPlan-1K and ALFRED benchmarks (53.2% executability, 36.1% correctness on counterfactual tasks)
- **Medium confidence**: Staged training procedure's effectiveness in preventing gradient interference between classifier optimization and generation fine-tuning
- **Low confidence**: Robustness of counterfactual detection across diverse task domains and generalizability of K=4 parameter choice for conditional pooling

## Next Checks

1. Measure CAR classifier accuracy, precision, and recall on held-out counterfactual clauses from ActPlan-1K. Target accuracy >85%. If precision <70% at recall=80%, investigate training data quality or increase MLP capacity.

2. Evaluate LLaPa's counterfactual detection and planning performance on datasets outside ActPlan-1K (e.g., COVR, CLEVR). Compare K=4 performance to K=2 and K=16 to verify parameter robustness across visual complexity levels.

3. Conduct failure mode analysis by comparing TER-only vs. full LLaPa performance on normal tasks (target: TER-only ~62.9% executability). Visualize attention maps and counterfactual token distributions to identify whether failures originate from segmentation quality, classifier accuracy, or LLM decoding.