---
ver: rpa2
title: Personalized Vision via Visual In-Context Learning
arxiv_id: '2509.25172'
source_url: https://arxiv.org/abs/2509.25172
tags:
- tasks
- visual
- image
- pico
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Personalized Vision via Visual In-Context Learning

## Quick Facts
- arXiv ID: 2509.25172
- Source URL: https://arxiv.org/abs/2509.25172
- Authors: Yuxin Jiang; Yuchao Gu; Yiren Song; Ivor Tsang; Mike Zheng Shou
- Reference count: 28
- Primary result: Task diversity drives generalization more than dataset scale in personalized vision ICL

## Executive Summary
PICO introduces a visual in-context learning framework for personalized vision tasks by fine-tuning FLUX.1-dev on a compact but diverse dataset (VisRel). Unlike traditional personalization methods requiring extensive user data, PICO learns a visual-relation space from 27 tasks and generalizes to novel user-defined tasks through zero-shot transfer. The method achieves strong performance across segmentation, style transfer, and edge detection tasks while requiring only a single exemplar pair at inference time.

## Method Summary
PICO fine-tunes FLUX.1-dev with LoRA (rank 256) on VisRel, a dataset spanning 27 diverse tasks organized by semantic complexity and spatial locality. During training, noise is injected only into the target quadrant while exemplar context remains clean, enabling stable relation transfer. At inference, given a single annotated exemplar pair (A→A') and query B, the model infers the underlying transformation and applies it to generate B'. An optional attention-guided seed scorer ranks initial denoising seeds based on early attention patterns to improve output reliability.

## Key Results
- Task diversity in training data drives generalization more effectively than scale
- Clean-context noising strategy prevents exemplar corruption and enables stable relation transfer
- Attention-pivot seed selection improves output quality for deterministic/localized tasks
- Outperforms MC-LLaVA and PerPilot on PODS segmentation while requiring no retraining

## Why This Works (Mechanism)

### Mechanism 1: Visual Relation Space Transfer
Task diversity in training enables generalization to novel personalized tasks through interpolation within a learned visual-relation space. The VisRel dataset spans 27 tasks organized along two axes—semantic complexity (pixel-level → object-level reasoning) and spatial locality (local → global operations). This structured diversity creates a unified transformation space. At test time, given (A→A'), the model identifies the nearest transformation in this learned space and applies it to B, enabling zero-shot adaptation to user-defined tasks. Core assumption: Visual tasks share underlying transformation primitives that can be composed and interpolated. Break condition: Tasks entirely outside the trained relation space (e.g., 3D geometric operations, video) show degraded reliability.

### Mechanism 2: Clean-Context Conditional Generation
Preserving exemplar information through selective noising enables stable relation transfer during diffusion training. Unlike standard diffusion that perturbs all latents, PICO injects noise only into the target quadrant x₀ (B'), leaving visual context c_vp = E([A, A', B]) and text c_txt clean. The CFM objective L_CFM (Eq. 3) is applied only to the target. This forces the model to extract transformation rules from pristine context rather than learning to denoise corrupted exemplars. Core assumption: Exemplar corruption would prevent the model from reliably "reading" the task definition from context. Break condition: If context quadrants are noised during training, the model may output generic transformations rather than exemplar-aligned results.

### Mechanism 3: Attention-Pivot Seed Selection
Early attention dynamics predict output quality, enabling efficient test-time scaling via seed ranking. During early denoising steps (0,1,2), track attention from bottom-right (target) queries to (1) target keys (D_br) and (2) visual context keys (D_vp). High-quality seeds show a pivot pattern: queries initially attend to context for task understanding, then pivot to target for generation. The score S_pivot = z(D_br) - z(D_vp) ranks seeds; the best seed continues to full denoising. Core assumption: The attention pivot pattern during warmup reflects successful task interpretation. Break condition: If critical blocks B† (identified as {9,11,12}) are mis-specified, or probe steps are insufficient, correlation with quality degrades.

## Foundational Learning

- **Concept: Conditional Flow Matching (CFM)**
  - Why needed: PICO trains using CFM (not standard DDPM), predicting velocity fields v_θ(x_t, t | c_txt, c_vp). Understanding this objective is essential for interpreting Eq. 3 and the rectified flow schedule.
  - Quick check: Can you explain why CFM uses velocity prediction rather than noise prediction, and how rectified flow differs from standard diffusion schedules?

- **Concept: Multi-Head Cross-Attention with Spatial Partitioning**
  - Why needed: The seed scorer requires understanding how queries, keys, and values are partitioned across quadrants (BR, visual context, text) in DiT blocks.
  - Quick check: Given a quad-grid input, trace how attention from bottom-right queries interacts with keys from A, A', and B. Which attention patterns indicate task understanding vs. copying?

- **Concept: Visual In-Context Learning vs. Few-Shot Fine-Tuning**
  - Why needed: PICO's core innovation is treating personalized vision as ICL—learning from demonstration at inference time without weight updates.
  - Quick check: How does visual ICL differ from few-shot fine-tuning in terms of flexibility, compute cost, and task-specific optimization? What trade-offs does PICO make?

## Architecture Onboarding

- **Component map:**
  Input: Quad-grid [A, A', B, X] at 512×512 each → 1024×1024 total → VAE Encoder → Latents (per-quadrant encoding) → FLUX.1-dev DiT + LoRA (rank 256) → CFM objective: predict velocity v_θ, loss on target quadrant only → VAE Decoder → Output B' (512×512)

- **Critical path:**
  1. Dataset construction: Ensure VisRel covers semantic complexity × spatial locality (27 tasks, ~10 samples each, minimal text labels for disambiguation)
  2. Training: Clean noising on target only; CFM loss applied only to B' quadrant; LoRA fine-tuning for 30K steps
  3. Inference: Initialize X as noise; integrate flow t=1→0; optionally use seed scorer with B†={9,11,12}, probe steps {0,1,2}, |S|=10 candidates

- **Design tradeoffs:**
  - Data scale vs. task diversity: Figure 6B-c shows many-tasks–few-shots increasingly outperforms few-tasks–many-shots as task count grows. Prioritize diversity for novel-task generalization
  - Text prompts: Minimal text (+1.84 mIoU on PODS, Table 4) helps disambiguate conflicting tasks but introduces language dependency
  - Seed scorer overhead: 3 warmup steps × 10 seeds = 30 extra steps; provides reliability gains for deterministic/localized tasks

- **Failure signatures:**
  - Task bleeding: If context quadrants are noised during training, outputs become generic rather than exemplar-specific
  - Poor seed selection: Misidentifying critical blocks B† breaks S_pivot correlation with quality (Figure 9 shows sensitivity in blocks 9-12)
  - Out-of-space tasks: Tasks outside VisRel's relation space (e.g., 3D geometry) produce unreliable outputs

- **First 3 experiments:**
  1. Reproduce noising ablation: Train clean-context vs. all-noised; compare on PODS segmentation. Expect >10 mIoU gap
  2. Validate diversity hypothesis: Fix 100 samples; compare N=20 tasks × 5 shots vs. N=5 tasks × 20 shots on unseen Taskonomy tasks. Expect diversity to win
  3. Test seed scorer transferability: Apply attention-guided scorer (B†={9,11,12}) to a different DiT model on similar ICL tasks. Determine if blocks must be re-identified via variance analysis (Figure 9 method)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can visual in-context learning frameworks be designed to extrapolate effectively to task types that lie entirely outside the visual-relation space defined by the tuning dataset?
- Basis in paper: The paper states in the Limitations section: "PICO generalizes well within the trained visual-relation space but is less reliable on entirely novel task types outside it... broadening the method to truly novel tasks remains an open challenge."
- Why unresolved: The current performance relies on interpolation within the structured VisRel dataset; the model struggles when user-defined tasks deviate significantly from the semantic or geometric primitives seen during training.
- What evidence would resolve it: Demonstrating zero-shot adaptation to task categories (e.g., novel sensor modalities or abstract logical operations) explicitly excluded from the VisRel taxonomy without requiring dataset expansion or retraining.

### Open Question 2
- Question: Can the PICO framework be extended to handle richer or sequential contexts, such as video demonstrations, to define more complex or temporal tasks?
- Basis in paper: The paper notes in Future Work: "The four-panel input format... inherently limits the number and richness of demonstrations. Future work includes extending PICO to richer or sequential context (e.g., videos or long-context models)."
- Why unresolved: The current architecture relies on a fixed 2x2 grid (quad-grid) format, which restricts the model to a single static exemplar pair and prevents it from reasoning over sequences of actions or temporal dynamics.
- What evidence would resolve it: A modified architecture that ingests video frames or multi-image sequences as context, successfully performing tasks like "animate this object" or "apply this multi-step edit" defined by a video demonstration.

### Open Question 3
- Question: Why does the inclusion of semantic perception tasks (e.g., class-level segmentation) appear to suppress performance on fine-grained, instance-level personalized segmentation?
- Basis in paper: In the ablation study (Table 6), removing semantic tasks improved personalized segmentation metrics (e.g., PerSeg mIoU rose from 90.97 to 92.90). The authors hypothesize: "class-level labels may suppress fine instance-level distinctions," but this negative transfer is not mechanistically explained.
- Why unresolved: It is counter-intuitive that adding "semantic understanding" tasks would harm "instance segmentation." The underlying attention mechanism or feature interference causing this trade-off is not analyzed.
- What evidence would resolve it: An analysis of feature embedding spaces or attention maps showing distinct clusters or interference patterns when semantic tasks are mixed with instance tasks, or a training strategy that disentangles these representations to remove the penalty.

## Limitations
- Generalization reliability degrades for tasks outside the learned visual-relation space
- The attention-pivot seed scorer requires re-identification of critical blocks for different model architectures
- Task bleeding risk if exemplar context is corrupted during training

## Confidence
- High Confidence: Task diversity driving generalization (supported by Figure 6B, VisRel ablation results)
- Medium Confidence: Clean-context conditional generation mechanism (novel implementation detail, limited ablation evidence)
- Medium Confidence: Attention-pivot seed selection (Spearman correlations shown, but limited cross-model validation)

## Next Checks
1. Test clean-context vs. corrupted-context training on PODS segmentation to quantify task confusion risk
2. Replicate diversity hypothesis validation with controlled N=20×5 vs. N=5×20 task-shot combinations on Taskonomy
3. Validate attention-pivot scorer transferability by re-identifying critical blocks on a different DiT model using the variance analysis method from Figure 9