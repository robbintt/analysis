---
ver: rpa2
title: Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology
  Texts using BERT Embeddings
arxiv_id: '2510.17437'
source_url: https://arxiv.org/abs/2510.17437
tags:
- recognition
- clinical
- spanish
- medications
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study developed deep contextual embedding models for clinical
  named entity recognition (NER) in the cardiology domain across three languages:
  Spanish, English, and Italian. The research focused on extracting disease and medication
  mentions from clinical case reports using BERT-based models fine-tuned on general
  domain text.'
---

# Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings

## Quick Facts
- arXiv ID: 2510.17437
- Source URL: https://arxiv.org/abs/2510.17437
- Authors: Manuela Daniela Danu; George Marica; Constantin Suciu; Lucian Mihai Itu; Oladimeji Farri
- Reference count: 40
- Key outcome: BERT-based models achieved F1-scores of 77.88% for Spanish disease recognition, 92.09% for Spanish medications, 91.74% for English medications, and 88.9% for Italian medications, outperforming leaderboard baselines across all subtasks.

## Executive Summary
This study developed deep contextual embedding models for clinical named entity recognition (NER) in cardiology texts across three languages: Spanish, English, and Italian. The research focused on extracting disease and medication mentions from clinical case reports using BERT-based models fine-tuned on general domain text. Four monolingual models were created for Spanish Diseases Recognition (SDR), Spanish Medications Recognition (SMR), English Medications Recognition (EMR), and Italian Medications Recognition (IMR), along with two multilingual models for SDR and medications recognition across all three languages. The models were fine-tuned using the MultiCardioNER dataset, which includes clinical case reports annotated with diseases and medications. The best-performing models achieved F1-scores of 77.88% for SDR, 92.09% for SMR, 91.74% for EMR, and 88.9% for IMR, outperforming the mean and median F1 scores in the test leaderboard across all subtasks.

## Method Summary
The study fine-tuned HuggingFace BERT-based NER models on the MultiCardioNER corpus, which includes DisTEMIST (for diseases) and DrugTEMIST (for medications) datasets. The approach used BIO tagging with disease entities labeled as B-ENFERMEDAD/I-ENFERMEDAD and medications as B-FARMACO/I-FARMACO. Models were fine-tuned for 10 epochs with batch size 8 and learning rate 9e-6, using max_seq_length=256. Both monolingual BERT models (Spanish, English, Italian) and a multilingual BERT model were evaluated. The study employed a two-stage fine-tuning approach: first on the TEMIST datasets, then optionally on the CardioCCC development set.

## Key Results
- BERT-based models achieved F1-scores of 77.88% for Spanish disease recognition, 92.09% for Spanish medications, 91.74% for English medications, and 88.9% for Italian medications
- Multilingual models matched or exceeded monolingual performance for medication recognition across all three languages
- Medication recognition consistently outperformed disease recognition, with F1 gaps of 14-15 percentage points
- Models outperformed mean and median F1 scores in the test leaderboard across all subtasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning general-domain BERT NER models on clinical corpora produces cardiology-capable extractors without biomedical pretraining.
- **Mechanism:** The authors take off-the-shelf BERT models trained on CoNLL/WikiANN (news/Wikipedia entities: PER, ORG, LOC, MISC) and fine-tune on MultiCardioNER using BIO tags mapped to clinical entity types (B-ENFERMEDAD/I-ENFERMEDAD for diseases; B-FARMACO/I-FARMACO for medications). Transfer learning leverages BERT's contextual representations and token-level classification head while re-learning entity semantics for the clinical domain.
- **Core assumption:** General-domain contextual embeddings are sufficiently reusable for specialized vocabulary; entity boundary detection skills transfer even when entity semantics change.
- **Evidence anchors:**
  - [section 3.2]: "bert-base-NER [43]: a BERT cased model fine-tuned on the English version of the standard CoNLL-2003 dataset [44]. It was trained to recognize four types of entities, namely locations (LOC), organizations (ORG), persons (PER), and miscellaneous (MISC)."
  - [section 3.2.1-3.2.2]: Models fine-tuned on MultiCardioNER with BIO labels; test F1-scores outperform mean/median leaderboard.
  - [corpus]: Neighbor papers (e.g., OpenMed NER) similarly use domain-adapted transformers for biomedical NER, but not necessarily for this cardiology-multilingual setting; corpus support for exact mechanism is weak.
- **Break condition:** If clinical vocabulary and syntax diverge too far from general-domain data, expect brittle boundary detection and high false positives on rare disease mentions (reflected in lower SDR F1 and observed overfitting).

### Mechanism 2
- **Claim:** Medication NER generalizes better across languages and domains than disease NER under the same modeling pipeline.
- **Mechanism:** Medications (drug names) often form a more closed set with frequent surface-form matches and standardized nomenclature, while diseases have broader synonymy and compositional expressions. The paper's models achieve >88% F1 on medication subtasks vs. 77.88% on Spanish disease recognition, with qualitative examples showing incomplete/incorrect disease spans.
- **Core assumption:** Training data annotation consistency and vocabulary regularization are higher for medications than diseases; cross-lingual projection (via translation) preserves medication mentions better than disease descriptions.
- **Evidence anchors:**
  - [abstract]: "best-performing models achieved F1-scores of 77.88% for SDR, 92.09% for SMR, 91.74% for EMR, and 88.9% for IMR."
  - [section 3.1]: DrugTEMIST multilingual corpus was machine-translated and then revised by clinical experts; disease annotations originated in Spanish SPACCC and projected similarly.
  - [corpus]: Corpus evidence for medication vs. disease difficulty is indirect; no cited study directly quantifies this gap.
- **Break condition:** If new clinical subdomains introduce heavy abbreviation/jargon for medications (e.g., chemotherapy protocols), medication NER may drop sharply.

### Mechanism 3
- **Claim:** Multilingual BERT can match or exceed monolingual models for medication recognition when training aggregates data across languages.
- **Mechanism:** The authors fine-tune `bert-base-multilingual-cased-ner-hrl` on pooled Spanish, English, and Italian medication data, then evaluate per language. Shared multilingual representations appear to provide beneficial cross-lingual signal, at least for medication names with similar surface forms or shared Latin/Greek roots.
- **Core assumption:** Cross-lingual parameter sharing transfers positive inductive bias; entity semantics for medications are partially language-independent.
- **Evidence anchors:**
  - [section 3.2.2]: "multilingual model was trained on an aggregated dataset encompassing all three languages, but separately evaluated for each language."
  - [table 1]: MultiCardio-MMR achieves 92.09% (ES), 91.74% (EN), 88.67% (IT) vs. best monolingual Cardio-SMR 91.65%, Cardio-EMR 91.46%, Cardio-IMR 88.90%.
  - [corpus]: FewTopNER (neighbor) supports cross-lingual NER with shared encoders, but not specific to clinical medications.
- **Break condition:** If languages have divergent tokenization or script (not this study's case), multilingual pooling may degrade monolingual performance.

## Foundational Learning

- **Concept: Transfer Learning (Domain Adaptation)**
  - Why needed here: The core pipeline takes general-domain NER models and adapts them to cardiology texts; understanding this concept prevents naive from-scratch training and clarifies why overfitting appears on small dev sets.
  - Quick check question: If you fine-tune on only 258 cardiology dev documents, what performance gap between dev and test signals overfitting?

- **Concept: BIO Tagging Schema**
  - Why needed here: All models are trained to predict B-/I-/O tags for entity boundaries; this defines the output space and evaluation protocol.
  - Quick check question: How would the model tag "acute myocardial infarction" under BIO for disease entities?

- **Concept: Multilingual vs Monolingual Representations**
  - Why needed here: The study compares multilingual BERT against language-specific BERT models; knowing this informs model selection for new language pairs.
  - Quick check question: What tradeoff would you expect if adding a fourth language with a different script (e.g., Russian) to the multilingual medication model?

## Architecture Onboarding

- **Component map:**
  Preprocessing: Sentence splitting (max seq length 256) → word tokenization with offset tracking → BIO label encoding.
  Backbone: BERT-base (cased) encoder with token classification head.
  Training: Fine-tune on MultiCardioNER (DisTEMIST/DrugTEMIST + CardioCCC dev split).
  Inference: BIO output → post-processing → BRAT format spans.
  Evaluation: Flat micro-averaged precision/recall/F1 on exact span matches.

- **Critical path:**
  1. Select pretrained BERT NER model (mono vs multilingual).
  2. Map entity types to BIO label set (e.g., B-ENFERMEDAD, I-ENFERMEDAD, O).
  3. Fine-tune on MultiCardioNER (10 epochs, batch size 8, lr 9e-6).
  4. Post-process predictions to BRAT; evaluate via official library.

- **Design tradeoffs:**
  - Monolingual models: Potentially better for low-resource language nuance; requires separate training per language.
  - Multilingual model: Single model for all three languages; may leverage cross-lingual signal but risks negative transfer for minority language patterns.
  - Dev-set fine-tuning (Cardio-*) boosts performance but introduces overfitting risk, especially for diseases with limited diversity.

- **Failure signatures:**
  - Large dev-test F1 gap (>15 points): Overfitting to small dev set (seen in SDR models).
  - Incomplete entity spans in qualitative examples: Model may predict B- but miss I- tokens for long disease phrases.
  - Multilingual model underperforming monolingual on specific language: Likely negative transfer or insufficient language-specific data.

- **First 3 experiments:**
  1. Replicate monolingual Cardio-EMR: Load `bert-base-NER`, fine-tune on English DrugTEMIST + CardioCCC dev, evaluate on test; compare F1 to reported 91.74%.
  2. Ablate multilingual pooling: Train Multi-MMR on Spanish-only subset, then on aggregated data; measure per-language F1 delta.
  3. Overfitting mitigation for SDR: Apply early stopping or dropout regularization; track dev-test gap reduction.

## Open Questions the Paper Calls Out
- Can Large Language Models (LLMs) improve the accuracy of disease entity recognition compared to the current fine-tuned BERT models? The authors state in the conclusion that to address weaknesses in disease recognition (incomplete or incorrect predictions), they "aim to explore the capabilities of recent large language models (LLMs)." The current study was limited to BERT-based architectures; LLMs were not evaluated in the experiments.
- What methods can effectively mitigate the severe overfitting observed in the Spanish Diseases Recognition (SDR) models? The results show an 18.35% F1-score gap between development and test sets for Cardio-SDR. The authors attribute this to the model being "too complex for the limited diversity of cardiology-specific entities" but do not test solutions.
- Does incorporating the provided "background set" to create a silver-standard corpus improve model generalization? The methods section describes an auxiliary collection of multilingual clinical case reports (background set) provided to "facilitate the creation of a silver standard corpus," but the reported results rely solely on the manually annotated training data.

## Limitations
- Severe overfitting to the small CardioCCC dev set (258 documents), particularly evident in disease recognition where dev-test F1 gaps reach 16-18 percentage points
- Lower performance on disease recognition (77.88% F1) compared to medication recognition (>88% F1), suggesting fundamental challenges with clinical entity complexity
- Reliance on translated clinical data for English and Italian medications without validation of translation quality for entity extraction

## Confidence
- **High confidence**: The BERT fine-tuning methodology and BIO tagging approach are sound and well-established. The reported F1 scores for medication recognition (>88%) are internally consistent and demonstrate clear superiority over disease recognition.
- **Medium confidence**: The claim that multilingual models can match or exceed monolingual performance for medication recognition, as the margin is small and could vary with different language combinations or dataset sizes.
- **Low confidence**: The generalizability of these results to other clinical subdomains beyond cardiology, and the effectiveness of the approach for disease recognition without additional biomedical pretraining.

## Next Checks
1. **Overfitting validation**: Replicate the SDR fine-tuning process and systematically measure dev-test F1 gaps across multiple random seeds. If gaps consistently exceed 15%, implement early stopping or dropout regularization and compare performance.
2. **Cross-lingual robustness test**: Take the multilingual medication model and evaluate on a held-out language (e.g., French or German clinical texts) to assess whether cross-lingual transfer generalizes beyond the three training languages.
3. **Domain adaptation ablation**: Train an identical NER architecture from scratch on the MultiCardioNER data (without BERT initialization) to quantify the contribution of pretrained embeddings versus fine-tuning alone.