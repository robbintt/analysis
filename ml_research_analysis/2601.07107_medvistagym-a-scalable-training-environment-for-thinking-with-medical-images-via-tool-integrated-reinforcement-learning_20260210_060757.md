---
ver: rpa2
title: 'MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images
  via Tool-Integrated Reinforcement Learning'
arxiv_id: '2601.07107'
source_url: https://arxiv.org/abs/2601.07107
tags:
- tool
- reasoning
- medical
- tools
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEDVISTAGYM is a scalable training environment for medical image
  analysis that enables vision language models to learn effective tool-integrated
  reasoning through agentic reinforcement learning. The framework provides an executable
  interface with diverse visual tools and medical tasks, allowing models to dynamically
  invoke and coordinate tools during multi-turn reasoning.
---

# MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.07107
- **Source URL**: https://arxiv.org/abs/2601.07107
- **Reference count**: 40
- **Primary result**: 19.10–24.21% accuracy gains over baselines on 6 medical VQA benchmarks

## Executive Summary
MEDVISTAGYM introduces a scalable training environment that enables vision-language models to learn effective tool-integrated reasoning for medical image analysis through agentic reinforcement learning. The framework provides an executable interface with diverse visual tools and medical tasks, allowing models to dynamically invoke and coordinate tools during multi-turn reasoning. Training MEDVISTA-R1 with MEDVISTAGYM yields consistent performance gains of 19.10–24.21% over tool-augmented baselines across six medical VQA benchmarks, demonstrating that structured agentic training—not tool access alone—enables robust medical visual reasoning.

## Method Summary
The approach formulates medical VQA as a partially observable Markov decision process (POMDP) with trajectory sampling and reinforcement learning. It employs a two-stage pipeline: (1) Cold-start supervised fine-tuning (SFT) on GPT-5-generated tool-augmented trajectories with quality filtering, and (2) Online reinforcement learning via group-relative policy optimization (GRPO) with a reward function combining format validity, answer accuracy, and tool-use bonus. The environment provides 15 tools across four families (resolution/region refinement, localization/segmentation, medical parsing, knowledge retrieval) deployed as async FastAPI services. The framework trains InternVL3-2B/8B models using 10× A100 GPUs with a strict output format requiring interleaved thoughts and tool calls.

## Key Results
- MEDVISTA-R1 achieves 19.10–24.21% accuracy gains over tool-augmented baselines across six medical VQA benchmarks
- The two-stage training pipeline (SFT + RL) is essential, as naive tool access degrades performance from 54.66% to 38.88%
- Failure analysis reveals 56.7% of errors are argument content errors and 73.8% are tool-induced reasoning errors in untrained models

## Why This Works (Mechanism)
The framework succeeds by providing structured training for tool-integrated reasoning rather than merely granting tool access. The cold-start SFT stage establishes proper tool syntax and selection discipline, while the RL stage aligns tool invocation with task success through answer-conditioned rewards. The environment's POMDP formulation enables the model to learn optimal multi-turn reasoning paths through tool coordination, addressing the complexity of medical visual reasoning that requires both perceptual analysis and domain knowledge.

## Foundational Learning
- **POMDP formulation for tool-integrated reasoning**: Enables modeling of sequential decision-making where tool choices affect future observations
- **Group-relative policy optimization (GRPO)**: Stabilizes RL training by normalizing advantages across trajectory groups
- **Tool-augmented trajectory generation**: Creates high-quality training data through multi-turn interactions with real tools
- **Multi-family tool coordination**: Integrates visual, analytical, and knowledge-based tools for comprehensive medical reasoning
- **Cold-start SFT before RL**: Establishes behavioral priors necessary for effective reinforcement learning
- **Answer-conditioned tool-use reward**: Aligns tool invocation with downstream task success rather than tool usage frequency

## Architecture Onboarding

**Component Map**: User Query -> Tool Selection Logic -> Async Tool Execution -> Observation Generation -> Reward Computation -> Policy Update

**Critical Path**: User Query → Tool Selection Logic → Async Tool Execution → Observation Generation → Answer Generation → Reward Computation

**Design Tradeoffs**: The framework prioritizes training efficiency over real-time inference speed by using offline trajectory generation and batch processing. The strict output format ( →  →  → ) enables automated reward computation but may constrain natural reasoning patterns. The choice of 6 maximum tool calls balances reasoning depth against computational cost and exploration complexity.

**Failure Signatures**: 
- Naive tool access degrades performance without proper training (54.66% → 38.88%)
- Argument content errors (E4) occur when models invoke syntactically valid tools with inappropriate arguments
- Tool-induced reasoning errors (E6) arise when models reason incorrectly over tool outputs despite correct tool selection

**First Experiments**:
1. Verify cold-start data quality by reproducing the GPT-5 trajectory generation and filtering pipeline
2. Test single-tool reasoning performance to establish baseline tool coordination capabilities
3. Evaluate multi-turn reasoning with different maximum tool call limits (3, 6, 9) to identify optimal depth

## Open Questions the Paper Calls Out
1. **Knowledge tool integration**: Can complementary medical knowledge tools (e.g., structured diagnostic knowledge bases) systematically resolve diagnosis failures caused by knowledge gaps rather than perceptual limitations?
2. **Computational scaling limits**: How does performance scale with increased rollout depth and action-space size before computational constraints become prohibitive?
3. **Task generalization**: Does the two-stage training paradigm transfer effectively to non-VQA clinical tasks such as report generation or longitudinal tracking?
4. **GPT-5 dependency**: How dependent is performance on GPT-5 for trajectory generation, and can high-quality training data be synthesized with open-source models?

## Limitations
- Multi-turn agentic training with frequent tool invocation is computationally expensive, limiting rollout depth and action-space exploration
- Current environment and evaluation focus primarily on medical VQA tasks, requiring adaptation for other clinical reasoning paradigms
- Cold-start data generation depends critically on GPT-5 with unspecified prompts and quality rubrics, creating reproducibility challenges

## Confidence
**High Confidence**: The core architectural claim that structured agentic training (SFT + RL) yields superior performance compared to naive tool access is well-supported by ablation studies showing clear performance degradation (54.66% → 38.88%) without proper training.

**Medium Confidence**: The specific performance gains of 19.10-24.21% over tool-augmented baselines are credible given the controlled experimental setup, but depend on the quality of the cold-start data generation and the precise implementation of the reward functions.

## Next Checks
1. **Cold-start Data Quality Verification**: Reproduce the GPT-5 trajectory generation with the same or similar quality filtering criteria and verify that the curated dataset maintains the reported high-quality standard (4-point scale filtering) while covering diverse tool interaction patterns across the 4 tool families.

2. **Ablation on Tool Families**: Systematically disable individual tool families during RL training to quantify their relative contributions to the overall performance gains, particularly focusing on whether medical parsing and knowledge retrieval tools drive the largest improvements as hypothesized.

3. **Out-of-Distribution Stress Test**: Evaluate the trained MEDVISTA-R1 model on entirely new medical imaging tasks or modalities not present in any of the training or evaluation datasets to assess true generalization beyond the reported benchmarks.