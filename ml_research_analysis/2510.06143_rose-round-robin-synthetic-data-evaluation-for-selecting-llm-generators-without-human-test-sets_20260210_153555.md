---
ver: rpa2
title: 'RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without
  Human Test Sets'
arxiv_id: '2510.06143'
source_url: https://arxiv.org/abs/2510.06143
tags:
- data
- rose
- llms
- proxy
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoSE (Round-robin Synthetic data Evaluation),
  a novel proxy metric for selecting the best LLM as a synthetic data generator when
  human test sets are unavailable. RoSE trains a small model on synthetic data from
  one LLM and evaluates it on synthetic data generated by all other LLMs, using the
  mean performance as the RoSE score.
---

# RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets

## Quick Facts
- **arXiv ID**: 2510.06143
- **Source URL**: https://arxiv.org/abs/2510.06143
- **Reference count**: 33
- **Primary result**: RoSE consistently identifies optimal LLM generators for synthetic data creation with only 0.76 F1 gap compared to human test set selection

## Executive Summary
This paper introduces RoSE (Round-robin Synthetic data Evaluation), a novel proxy metric for selecting the best LLM as a synthetic data generator when human test sets are unavailable. RoSE trains a small model on synthetic data from one LLM and evaluates it on synthetic data generated by all other LLMs, using the mean performance as the RoSE score. Tested across 6 LLMs, 11 languages, and 3 tasks (sentiment, topic, intent), RoSE consistently identifies the optimal generator more often than any other intrinsic heuristics. It achieves an average F1 gap of only 0.76 percentage points compared to the optimal generator baseline, outperforming other metrics which have gaps up to 2.52 points. RoSE is the only metric to achieve a positive correlation with downstream human test performance.

## Method Summary
RoSE operates through a round-robin evaluation framework where each LLM generator is assessed by training a small proxy model on its synthetic data and evaluating on synthetic data from all other generators. The method iteratively evaluates all generator pairs, computing mean performance scores to identify the optimal generator. The approach is tested across 6 LLMs, 11 languages, and 3 classification tasks, demonstrating superior performance compared to intrinsic heuristics like token diversity, self-evaluation, and prompt engineering strategies. The metric remains effective with as few as three LLMs and when comparing models of similar sizes, though it scales quadratically with the number of generators.

## Key Results
- RoSE identifies optimal generators more frequently than intrinsic heuristics across 11 languages and 3 tasks
- Achieves only 0.76 F1 gap compared to optimal generator baseline, outperforming other metrics with gaps up to 2.52 points
- RoSE is the only metric showing positive correlation with downstream human test performance
- Method remains effective with as few as three LLMs and for similarly-sized models

## Why This Works (Mechanism)
RoSE leverages the principle that a small proxy model trained on high-quality synthetic data will perform better when evaluated on data from the same high-quality generator. By training and evaluating across all generator pairs in a round-robin fashion, RoSE captures relative data quality differences that intrinsic metrics miss. The method creates a transitive evaluation framework where each generator's synthetic data is stress-tested against all others, revealing consistent performance patterns that correlate with downstream task success.

## Foundational Learning

**Round-robin evaluation** - Why needed: Enables pairwise comparison of all generators to identify consistent quality patterns. Quick check: Verify all generator pairs are evaluated exactly once.

**Proxy model training** - Why needed: Small models trained on synthetic data serve as efficient quality indicators. Quick check: Confirm proxy model size and training time are practical for the scale.

**Cross-generator evaluation** - Why needed: Tests synthetic data generalization across different generation styles. Quick check: Ensure evaluation data comes from different generators than training data.

**Mean performance aggregation** - Why needed: Aggregates pairwise comparisons into a single generator quality score. Quick check: Verify mean calculation includes all evaluation results for each generator.

**Zero-shot evaluation baseline** - Why needed: Provides upper bound for synthetic data quality assessment. Quick check: Confirm zero-shot results are computed correctly as reference point.

## Architecture Onboarding

**Component map**: LLM generators -> Proxy model training -> Cross-generator evaluation -> Mean performance aggregation -> Generator ranking

**Critical path**: The evaluation loop where each generator's synthetic data is used to train proxy models that are then evaluated on all other generators' data. This path determines the computational bottleneck and accuracy of the final ranking.

**Design tradeoffs**: RoSE trades computational efficiency for selection accuracy, scaling quadratically with generator count. The method requires access to multiple LLM generators but avoids human test sets. Using smaller proxy models reduces cost but may miss subtle quality differences.

**Failure signatures**: Poor proxy model training indicates low-quality synthetic data. Inconsistent evaluation results across generators suggest domain mismatch or task complexity issues. High variance in mean scores indicates unreliable generator quality assessment.

**First experiments**: 1) Test RoSE on two generators with one language/task to verify basic functionality. 2) Scale to three generators to confirm quadratic complexity scaling. 3) Compare RoSE against zero-shot baseline to establish baseline performance gap.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three classification tasks (sentiment, topic, intent) and 11 languages, uncertain performance on generation or reasoning tasks
- Quadratic computational cost scales poorly with large generator pools, potentially impractical without approximation
- Method assumes access to at least one high-quality synthetic generator but doesn't explore scenarios where all generators are weak
- Ablation studies suggest metric may still require some human data despite premise of avoiding human test sets

## Confidence
- **High confidence**: RoSE consistently identifies optimal generator more often than intrinsic heuristics
- **Medium confidence**: RoSE achieves only 0.76 F1 gap compared to optimal generator baseline
- **Medium confidence**: RoSE is the only metric with positive correlation to downstream human test performance

## Next Checks
1. Evaluate RoSE on generation tasks (summarization, question answering) and structured prediction to assess generalizability beyond classification
2. Test RoSE's performance when training and evaluation domains differ significantly to understand domain transfer limitations
3. Implement approximation techniques (sampling strategies, model compression) to reduce computational cost while maintaining selection accuracy