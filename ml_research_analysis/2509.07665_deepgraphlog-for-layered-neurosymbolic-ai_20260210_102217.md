---
ver: rpa2
title: DeepGraphLog for Layered Neurosymbolic AI
arxiv_id: '2509.07665'
source_url: https://arxiv.org/abs/2509.07665
tags:
- neural
- graph
- reasoning
- symbolic
- ground
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DeepGraphLog, a neurosymbolic AI framework\
  \ that extends ProbLog with Graph Neural Predicates to enable multi-layer neural-symbolic\
  \ reasoning. Unlike existing approaches like DeepProbLog that enforce a fixed subsymbolic\u2192\
  neural\u2192symbolic flow, DeepGraphLog allows neural and symbolic components to\
  \ interact in arbitrary order, treating symbolic representations as graphs processed\
  \ by Graph Neural Networks (GNNs)."
---

# DeepGraphLog for Layered Neurosymbolic AI

## Quick Facts
- arXiv ID: 2509.07665
- Source URL: https://arxiv.org/abs/2509.07665
- Authors: Adem Kikaj; Giuseppe Marra; Floris Geerts; Robin Manhaeve; Luc De Raedt
- Reference count: 29
- Primary result: DeepGraphLog enables arbitrary-order neural-symbolic reasoning through Graph Neural Predicates, achieving 100% accuracy on blocks world planning versus 65.99% for baseline GNNs

## Executive Summary
DeepGraphLog introduces a neurosymbolic AI framework that extends ProbLog with Graph Neural Predicates to enable multi-layer neural-symbolic reasoning. Unlike existing approaches that enforce a fixed subsymbolic→neural→symbolic flow, DeepGraphLog allows neural and symbolic components to interact in arbitrary order. The framework treats symbolic representations as graphs processed by Graph Neural Networks (GNNs), enabling reasoning over incomplete symbolic data and complex relational dependencies. Experimental results demonstrate significant improvements over baseline GNNs in planning tasks, knowledge graph completion, and GNN expressivity enhancement.

## Method Summary
DeepGraphLog extends ProbLog with Graph Neural Predicates (GNPs) that implement Graph Neural Networks within the probabilistic logic programming framework. The key innovation is enabling arbitrary-order neural-symbolic interactions rather than enforcing a fixed pipeline. GNPs can be called from both ProbLog rules and other GNPs, allowing flexible composition of neural and symbolic reasoning layers. The framework supports three key tasks: planning (finding action sequences to achieve goals), knowledge graph completion (inferring missing relations with distant supervision), and GNN expressivity enhancement (improving graph representation learning). Learning is achieved through parameter learning that tunes both neural network weights and symbolic predicate parameters simultaneously, with grounding handled by transforming rules into a factor graph for efficient inference.

## Key Results
- Achieves 100% accuracy on blocks world planning task versus 65.99% for baseline GNNs
- Knowledge graph completion F1 score of 98.94% for fatherOf relation versus 22.95% for baseline GNNs
- Successfully combines learning and reasoning, supporting structure learning via parameter learning and reasoning from distantly supervised examples

## Why This Works (Mechanism)
DeepGraphLog works by breaking the traditional sequential constraint in neurosymbolic systems where neural processing must precede symbolic reasoning. By implementing Graph Neural Predicates that can be called recursively from both ProbLog rules and other GNPs, the framework creates a bidirectional information flow. This allows symbolic representations to be enriched with neural features, which can then inform subsequent symbolic reasoning steps, and vice versa. The graph-based representation enables the system to handle incomplete symbolic data by using GNNs to predict missing information and guide reasoning. The probabilistic nature of ProbLog ensures uncertainty is properly handled throughout the reasoning process.

## Foundational Learning
- Probabilistic Logic Programming (ProbLog): Combines logic programming with probability theory; needed for handling uncertainty in symbolic reasoning; quick check: can compute success probability of logical queries
- Graph Neural Networks (GNNs): Neural networks that operate on graph-structured data; needed for processing symbolic representations as graphs; quick check: can propagate node features through graph structure
- Distant Supervision: Weak supervision signal derived from knowledge bases; needed for learning from incomplete or noisy training data; quick check: can generate training examples from known facts without explicit labeling

## Architecture Onboarding

Component map: User queries -> ProbLog rules -> Graph Neural Predicates -> GNNs -> Symbolic representations -> Probabilistic inference

Critical path: Query input → Rule matching → GNP evaluation → Neural feature extraction → Symbolic reasoning update → Probability calculation → Answer output

Design tradeoffs: Flexibility vs complexity - arbitrary-order interaction enables powerful reasoning but increases implementation complexity and potential for suboptimal configurations. Interpretability vs performance - maintaining probabilistic logic guarantees while leveraging neural network power.

Failure signatures: Poor performance when graph structure is insufficient for GNN learning, failure to converge when symbolic rules create cyclic dependencies with GNPs, degraded accuracy when distant supervision is too weak to provide meaningful training signals.

First experiments:
1. Simple blocks world planning with complete state information to verify basic GNP integration
2. Knowledge graph completion on small family tree with known relations to test distant supervision
3. Comparison of fixed-order vs arbitrary-order neural-symbolic reasoning on simple planning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to three domains (blocks world planning, family relations, and kinship graphs) with small graphs (max 50 nodes)
- No evaluation on real-world knowledge graphs or larger-scale benchmarks
- Limited comparison with other neurosymbolic approaches (Neural LP, DRUM, differentiable ILP systems)
- Scalability concerns for larger graphs not addressed through empirical analysis

## Confidence
- High confidence in core technical contribution and framework design
- Medium confidence in performance claims due to limited experimental scope
- Medium confidence in scalability assertions without larger-scale experiments

## Next Checks
1. Evaluate DeepGraphLog on real-world knowledge graphs (e.g., Wikidata or NELL) with varying graph sizes to assess scalability and practical applicability
2. Compare against other neurosymbolic systems (Neural LP, DRUM) on standard benchmarks to establish relative performance
3. Conduct ablation studies removing the layered architecture to quantify the contribution of flexible neural-symbolic ordering to performance gains