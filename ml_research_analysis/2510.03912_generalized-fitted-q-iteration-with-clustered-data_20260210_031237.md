---
ver: rpa2
title: Generalized Fitted Q-Iteration with Clustered Data
arxiv_id: '2510.03912'
source_url: https://arxiv.org/abs/2510.03912
tags:
- learning
- data
- policy
- optimal
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a generalized fitted Q-iteration (GFQI) algorithm
  that incorporates generalized estimating equations to handle intra-cluster correlations
  in reinforcement learning. The method addresses the challenge of correlated data
  commonly found in healthcare applications.
---

# Generalized Fitted Q-Iteration with Clustered Data

## Quick Facts
- arXiv ID: 2510.03913
- Source URL: https://arxiv.org/abs/2510.03913
- Reference count: 40
- Primary result: GFQI achieves up to 80% regret reduction compared to standard FQI on clustered data

## Executive Summary
This paper introduces Generalized Fitted Q-Iteration (GFQI), a reinforcement learning algorithm designed to handle clustered data with intra-cluster correlations. The method integrates generalized estimating equations into the Q-learning framework to account for correlation structures within clusters, which is particularly relevant for healthcare applications where data naturally forms clusters (e.g., patients within hospitals). The algorithm provides theoretical guarantees for both correctly specified and mis-specified correlation structures, demonstrating robustness to correlation structure uncertainty. Empirical evaluations show significant performance improvements over standard FQI and competitive results against deep RL baselines like CQL and DDQN.

## Method Summary
GFQI extends standard Fitted Q-Iteration by incorporating generalized estimating equations to handle correlated data within clusters. The algorithm modifies the Bellman residual calculation to account for intra-cluster correlations, using a working correlation matrix to weight observations appropriately. This adjustment ensures that the Q-function estimation properly accounts for the dependencies between samples from the same cluster. The method maintains the iterative structure of FQI while introducing correlation-aware loss functions that can handle both correctly specified and mis-specified correlation structures, providing theoretical consistency guarantees in both cases.

## Key Results
- GFQI achieves approximately 50% regret reduction under weak correlations compared to standard FQI
- Performance improves to 80% regret reduction under strong correlations
- Outperforms deep neural network-based approaches (CQL, DDQN) in semi-synthetic studies using real-world mobile health data

## Why This Works (Mechanism)
GFQI works by properly accounting for the statistical dependencies inherent in clustered data. In standard reinforcement learning, each observation is treated as independent, but in clustered settings (like healthcare), observations from the same cluster (e.g., same patient or hospital) are correlated. By incorporating generalized estimating equations, GFQI adjusts the weight of each observation based on its correlation structure, leading to more accurate value function estimates. The algorithm leverages the fact that while correlation structures may be unknown or mis-specified, the method remains consistent as long as the mean structure is correct, providing robustness to model uncertainty.

## Foundational Learning

**Generalized Estimating Equations (GEE)**: Statistical method for analyzing correlated data
- Why needed: Standard regression assumes independence; GEE handles within-cluster correlations
- Quick check: Can estimate parameters consistently even with mis-specified correlation structure

**Fitted Q-Iteration (FQI)**: Batch reinforcement learning algorithm that iteratively fits Q-functions
- Why needed: Standard Q-learning requires online interaction; FQI works with offline datasets
- Quick check: Converges to optimal Q-function under certain conditions with sufficient data

**Bellman Residual**: Difference between current Q-value and target Q-value
- Why needed: Measures learning progress and guides Q-function updates
- Quick check: Should decrease over iterations in stable learning

## Architecture Onboarding

**Component Map**: 
Data Preprocessing -> Correlation Structure Specification -> GFQI Iteration (GEE Loss + Function Approximation) -> Q-Function Update -> Policy Evaluation

**Critical Path**: 
The core algorithm flow involves computing Bellman targets using current Q-estimates, calculating GEE-based residuals that account for cluster correlations, updating the Q-function through function approximation, and iterating until convergence.

**Design Tradeoffs**: 
The method requires specifying a working correlation structure, trading off between model complexity and robustness. While correctly specified correlations improve performance, the algorithm maintains consistency under mis-specification at the cost of potentially slower convergence.

**Failure Signatures**: 
Poor performance when cluster structure is ignored (standard FQI), instability when correlation structure is severely mis-specified, and convergence issues with insufficient data within clusters.

**First Experiments**:
1. Compare GFQI vs standard FQI on synthetic clustered data with known correlations
2. Test sensitivity to correlation structure mis-specification across different levels
3. Evaluate performance on small-scale healthcare datasets with clear cluster structure

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes known or correctly specified correlation structures within clusters
- Limited to discrete action spaces and tabular Q-learning
- Practical impact of correlation mis-specification on algorithm performance remains unclear

## Confidence

**Theoretical guarantees under correct specification**: High
**Performance in empirical evaluations**: Medium (based on semi-synthetic data)
**Robustness to correlation structure mis-specification**: Medium
**Comparison with deep RL methods**: Medium (limited to specific architectures)

## Next Checks

1. Evaluate GFQI on continuous action spaces using function approximation techniques to assess scalability beyond tabular settings
2. Conduct extensive sensitivity analysis on correlation structure mis-specification to quantify performance degradation
3. Test the algorithm on diverse real-world healthcare datasets with varying cluster sizes and correlation strengths to validate practical applicability