---
ver: rpa2
title: 'Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for
  Privacy-Aware Generative Agents'
arxiv_id: '2512.12856'
source_url: https://arxiv.org/abs/2512.12856
tags:
- privacy
- memory
- budget
- policies
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of memory management for generative
  agents operating under computational and privacy constraints. The authors introduce
  the Memory-Aware Retention Schema (MaRS), a cognitively-inspired architecture that
  organizes memories as typed, provenance-tracked nodes with multiple indices for
  efficient retrieval.
---

# Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents

## Quick Facts
- **arXiv ID**: 2512.12856
- **Source URL**: https://arxiv.org/abs/2512.12856
- **Reference count**: 18
- **Primary result**: Hybrid forgetting policy achieves composite score 0.911 across 300 simulation runs while maintaining computational tractability and privacy guarantees

## Executive Summary
This paper introduces the Memory-Aware Retention Schema (MaRS), a cognitively-inspired memory architecture for generative agents that balances narrative coherence, goal completion, and privacy under computational constraints. The architecture organizes memories as typed, provenance-tracked nodes with multiple indices for efficient retrieval and implements six forgetting policies ranging from simple heuristics to hybrid approaches with differential privacy guarantees. The Forgetful but Faithful Agent (FiFA) benchmark evaluates agent performance across five memory budgets using LLM-as-judge metrics. Results show that principled forgetting-by-design can simultaneously support coherence, efficiency, and privacy, with the hybrid policy achieving superior performance while maintaining computational tractability.

## Method Summary
The method implements a graph-based memory store where nodes contain content, type, timestamp, sensitivity, weight, and provenance. Six forgetting policies (FIFO, LRU, Priority Decay, Reflection-Summary, Random, Hybrid) reduce memory to fit budget constraints using a density-based knapsack optimization framework. The Hybrid policy combines temporal filtering, reflection-based consolidation, priority eviction, and privacy tie-breaking. Evaluation uses an LLM-as-judge to compute five metrics across five memory budgets (2k-32k tokens) in a multi-agent simulation environment with five scenario types.

## Key Results
- Hybrid forgetting policy achieves composite score of 0.911 across 300 simulation runs
- Random Drop policy outperforms sophisticated policies on composite score due to high Cost Efficiency weighting
- Privacy Preservation metric shows limited variance across policies in baseline scenarios
- Reflection-Summary consolidation reduces token load while preserving narrative arcs

## Why This Works (Mechanism)

### Mechanism 1: Density-Based Knapsack Optimization
Prioritizes memory retention based on "utility density" (marginal utility per token) by framing retention as a submodular knapsack problem. The system scores each memory node using a function `score(n) = (Û_n - λ_priv * s_n) / w_n` and evicts nodes with lowest density first. The utility of memory items is assumed to be monotone and submodular, with diminishing returns.

### Mechanism 2: Provenance-Governed Consolidation
Maintains narrative coherence by replacing redundant episodic memories with semantic summaries while preserving dependency closure. The system enforces that if a summary is retained, the facts it summarizes must remain structurally valid, preventing dangling references in the memory graph.

### Mechanism 3: Sensitivity-Modulated Differential Privacy
Reduces privacy leakage by penalizing retention of high-sensitivity memories and randomizing eviction decisions at the decision boundary. A privacy penalty is subtracted from retention scores, and the Exponential Mechanism is used to randomize selection among tied items, providing (ε, δ)-Differential Privacy guarantees.

## Foundational Learning

- **Submodularity & Greedy Approximation**: Why needed here - justifies the density-based policy's effectiveness by guaranteeing that greedy eviction is near-optimal for submodular utility functions. Quick check: Why does the "diminishing returns" property of submodular functions make greedy algorithms effective for memory retention?
- **Antimatroids (Provenance Closure)**: Why needed here - prevents keeping summaries while deleting dependent facts by modeling feasible memory sets as an antimatroid. Quick check: In the MaRS architecture, why must a valid memory set be "provenance-closed" before an eviction can be valid?
- **Exponential Mechanism (Differential Privacy)**: Why needed here - enables privacy protection at the retention phase rather than training phase. Quick check: How does the exponential mechanism balance the "score" (utility) of a memory against the probability of its selection to ensure privacy?

## Architecture Onboarding

- **Component map**: Agent input -> Memory Node creation -> Budget Check -> Policy Layer (FIFO/LRU/Hybrid/etc) -> Graph update with provenance closure -> Index update -> Audit trail
- **Critical path**: 1) Ingestion creates memory node with type, sensitivity, weight 2) Budget check triggers if weight exceeds capacity 3) Policy Layer executes retention logic 4) Indices and audit trail updated
- **Design tradeoffs**: Simple policies (Random/FIFO) are highly cost-efficient but may delete bridge episodes needed for coherence; LRU favors recent context while Priority Decay favors semantic centrality; Reflection consolidation is computationally expensive but reduces token load long-term
- **Failure signatures**: Eviction thrash (constant capacity breaches), provenance violation (hallucination from broken dependencies), privacy budget exhaustion (forced data exposure)
- **First 3 experiments**: 1) Reproduce Table 2 policy sweep to validate Cost vs Coherence frontier 2) Ablate Reflection in Hybrid to quantify token savings vs coherence loss 3) Stress test privacy with adversarial prompts to measure PP metric variance

## Open Questions the Paper Calls Out

1. How can agents learn to adapt retention policies dynamically as a sequential decision problem rather than relying on static heuristics? (Explicit) - Future work should explore contextual bandits or constrained MDPs for online telemetry-based learning.

2. Can the MaRS framework effectively manage multi-modal memory (images, audio) while maintaining utility bounds and privacy guarantees? (Explicit) - Extension needed for defining Lipschitz-like bounds with mixed embeddings and privacy scores for visual identifiers.

3. Do the rubricized LLM-as-judge metrics used in FiFA correlate with actual human satisfaction and trust in real-world deployments? (Inferred) - Authors call for field validations to capture subtle discourse qualities and longitudinal trust.

## Limitations

- Evaluation relies heavily on LLM-based judgments without inter-annotator agreement statistics or human validation studies
- Privacy guarantees are validated through scenario simulations rather than real-world attack scenarios
- Performance gains depend on explicitly designed composite score weighting that may not generalize to alternative utility functions
- Theoretical utility proxy accuracy is assumed but not fully validated outside controlled simulation environment

## Confidence

- **High Confidence**: Core MaRS architecture design and computational complexity analysis for each policy
- **Medium Confidence**: Submodular knapsack formulation and Hybrid policy performance (composite score 0.911)
- **Medium Confidence**: Density-based optimization mechanism effectiveness depending on utility proxy accuracy
- **Low Confidence**: Differential privacy mechanism robustness against adversarial manipulation of sensitivity scores

## Next Checks

1. Cross-Scenario Generalization: Re-run policy sweep on held-out scenarios to verify Hybrid superiority isn't overfit to original narrative structures

2. Privacy Stress Test: Modify evaluation with adversarial prompts to test DP mechanism guarantees under sustained pressure and measure PP metric variance

3. Human Evaluation Benchmark: Conduct small-scale human evaluation (n≥20) comparing LLM-judge outputs against human ratings for Narrative Coherence and Goal Completion metrics