---
ver: rpa2
title: Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors
arxiv_id: '2511.16340'
source_url: https://arxiv.org/abs/2511.16340
tags:
- warm
- linear
- posterior
- points
- starting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving Gaussian process
  (GP) scalability in sequential settings where data points are added incrementally.
  The authors propose warm starting iterative linear solvers for GP posterior inference
  by leveraging solutions from previous, smaller linear systems.
---

# Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors

## Quick Facts
- arXiv ID: 2511.16340
- Source URL: https://arxiv.org/abs/2511.16340
- Authors: Alan Yufei Dong; Jihao Andreas Lin; José Miguel Hernández-Lobato
- Reference count: 40
- Key outcome: Warm starting iterative linear solvers for GP posterior inference reduces initial distance to solution and accelerates convergence across multiple solvers (CG, SGD, AP)

## Executive Summary
This paper addresses the scalability challenge in Gaussian processes (GPs) when data points arrive sequentially. The authors propose a method to improve iterative linear solvers used in GP posterior inference by leveraging solutions from previous, smaller linear systems. By using these prior solutions as warm starts for new computations, the approach significantly reduces the number of iterations needed to reach a desired tolerance level. This technique is particularly valuable for applications requiring repeated GP updates as new data arrives, such as Bayesian optimization and online learning scenarios.

## Method Summary
The authors propose warm starting iterative linear solvers for Gaussian process posterior inference in sequential settings. When new data points arrive incrementally, the GP requires updating the posterior, which involves solving linear systems of increasing size. Instead of starting each solver from scratch (cold start), the method reuses the solution from the previous, smaller linear system as an initial guess. This warm start reduces the initial distance to the true solution, allowing solvers like conjugate gradients, stochastic gradient descent, and alternating projections to converge faster. The approach is particularly effective when data arrives incrementally and the underlying kernel structure remains relatively stable between updates.

## Key Results
- Warm starting achieves speed-ups of 1.6× for conjugate gradient solvers
- Speed-ups of 1.7× for stochastic gradient descent when solving to tolerance
- 5.9× speed-up for alternating projections in regression tasks
- In Bayesian optimization with parallel Thompson sampling under limited compute budgets, warm starting yields more accurate posteriors and better optimization performance

## Why This Works (Mechanism)
The method works by exploiting the continuity between consecutive posterior updates in sequential GP inference. When new data points arrive incrementally, the solution to the previous linear system serves as a good approximation to the solution of the new, slightly larger system. This is because the underlying kernel structure and data patterns remain relatively stable between updates. By starting the iterative solver from this approximate solution rather than from zero or a generic initialization, the algorithm requires fewer iterations to converge to the desired tolerance level.

## Foundational Learning
- **Gaussian Processes**: Non-parametric Bayesian models for regression and classification that provide uncertainty estimates; needed because the paper focuses on improving GP scalability in sequential settings; quick check: verify understanding of GP posterior updates and kernel matrices
- **Iterative Linear Solvers**: Methods like conjugate gradients that approximate solutions to linear systems through iterative refinement; needed because warm starting is applied to these solvers in GP inference; quick check: understand how solvers like CG work and their convergence criteria
- **Sequential Posterior Updates**: The process of updating GP posteriors as new data arrives incrementally; needed because the paper specifically addresses this scenario; quick check: understand how kernel matrices grow and how posteriors change with new data
- **Warm Starting**: Initializing iterative algorithms with previous solutions rather than starting from scratch; needed as the core technique being proposed; quick check: understand when and why warm starting is beneficial for iterative methods
- **Bayesian Optimization**: Sequential optimization technique that uses GP models to balance exploration and exploitation; needed because the paper demonstrates applications in this domain; quick check: understand how GP posteriors are used for Thompson sampling

## Architecture Onboarding

**Component Map**: Data stream -> GP kernel construction -> Linear system formulation -> Iterative solver (CG/SGD/AP) -> Warm start initialization -> Solution convergence

**Critical Path**: The most critical computational path is the linear system solution phase, where warm starting provides the primary benefit. The quality of the warm start directly impacts the number of iterations required for convergence.

**Design Tradeoffs**: The method trades memory overhead (storing previous solutions) for computational speed-up. This is favorable when the storage cost is manageable and updates are frequent. The approach assumes that data arrives in a way that maintains correlation between consecutive problems.

**Failure Signatures**: Warm starting may provide minimal benefit when data points arrive in a highly non-stationary manner, causing large shifts in the posterior distribution between updates. The technique could also fail if the iterative solver used is not amenable to warm starting (e.g., methods that reset internal state each iteration).

**First Experiments**:
1. Implement warm starting with conjugate gradient solver on a simple GP regression problem with sequential data arrival
2. Compare convergence speed and solution accuracy between warm-started and cold-started solvers across varying data stream patterns
3. Test the method on a Bayesian optimization task using Thompson sampling to verify performance claims in optimization scenarios

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Empirical validation is primarily focused on regression tasks and Bayesian optimization with Thompson sampling, leaving uncertainty about performance in other GP applications like classification or active learning
- Speed-up measurements depend on solving to a fixed tolerance, but the impact of warm starting on solution accuracy compared to cold starts remains unclear
- Computational overhead of maintaining and reusing previous solver states may become significant for very long sequences or when storage constraints apply

## Confidence

**Major Claim Confidence:**
- Speed-up measurements across solvers: High
- Effectiveness in Bayesian optimization: Medium
- General applicability to all GP tasks: Low
- Computational overhead considerations: Medium

## Next Checks

1. Evaluate warm starting performance on non-regression GP tasks (classification, active learning) to assess broader applicability
2. Conduct ablation studies comparing solution accuracy between warm-started and cold-started solvers across varying tolerance levels
3. Test the method with batched data arrivals to understand practical implications for different data stream patterns