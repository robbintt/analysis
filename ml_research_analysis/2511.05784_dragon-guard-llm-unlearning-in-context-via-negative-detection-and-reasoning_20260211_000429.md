---
ver: rpa2
title: 'DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning'
arxiv_id: '2511.05784'
source_url: https://arxiv.org/abs/2511.05784
tags:
- unlearning
- arxiv
- dragon
- detection
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DRAGON, a systematic framework for LLM unlearning
  that protects deployed models through in-context reasoning-based intervention. Unlike
  existing methods that rely on fine-tuning or access to retain data, DRAGON uses
  a lightweight detection module to identify forget-worthy prompts via paraphrased
  negative data and a trained scoring model, then applies a CoT-based guard model
  to generate safety instructions for in-context refusal or redirection.
---

# DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning

## Quick Facts
- arXiv ID: 2511.05784
- Source URL: https://arxiv.org/abs/2511.05784
- Reference count: 40
- Key outcome: DRAGON achieves 99.1% detection accuracy on privacy unlearning while preserving MMLU accuracy at 68.0% (identical to original), outperforming fine-tuning baselines.

## Executive Summary
This paper introduces DRAGON, a systematic framework for LLM unlearning that protects deployed models through in-context reasoning-based intervention. Unlike existing methods that rely on fine-tuning or access to retain data, DRAGON uses a lightweight detection module to identify forget-worthy prompts via paraphrased negative data and a trained scoring model, then applies a CoT-based guard model to generate safety instructions for in-context refusal or redirection. It introduces novel metrics including Refusal Quality, Dynamic Deviation Score, and Dynamic Utility Score for evaluating unlearning effectiveness, coherence, and stability under continual unlearning. Experiments across three unlearning tasks—privacy record removal, harmful knowledge unlearning, and copyrighted content removal—show DRAGON consistently outperforms baselines in unlearning performance while preserving general language capabilities, with strong scalability to various model sizes and robustness to adversarial attacks.

## Method Summary
DRAGON implements in-context LLM unlearning through a three-stage pipeline: detection, policy retrieval, and guard model intervention. The detection module uses a fine-tuned scoring model combined with similarity metrics (BERTScore, ROUGE-L, cosine similarity) to identify forget-worthy prompts from paraphrased negative data stored in an Unlearn Store. When triggered, the system retrieves relevant safety policies and generates Chain-of-Thought instructions via a guard model (fine-tuned Llama3.1-8B-Instruct), which are prepended to the original prompt to guide the base LLM toward refusal or redirection. The framework is trained on synthetic benchmarks including TOFU for privacy records, WMDP for harmful knowledge, and MUSE for copyrighted content, using a CoT dataset generated by GPT-4o.

## Key Results
- DRAGON achieves 99.1% detection accuracy on privacy unlearning (TOFU) with no degradation to MMLU accuracy (68.0%)
- CoT-based refusals maintain consistency score gap of only 0.01 vs. NPO-RT template approach (gap of 0.44)
- Dynamic Deviation Score of 21.4 (vs. 43.9 without CoT) shows superior coherence preservation
- Scales effectively across model sizes from Llama2-7B to Yi-34B with consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRAGON detects forget-worthy prompts without retain data by combining a trained scoring model with similarity-based verification.
- Mechanism: The detection module computes a unified confidence score f(x, D_u) using two signals: (1) a fine-tuned scoring model F that assigns probability p_F(x) of harmful/forget-worthy content, and (2) similarity metrics (BERTScore, ROUGE-L) against paraphrased prompts stored in the Unlearn Store. If f(x, D_u) > threshold τ, intervention is triggered.
- Core assumption: Paraphrased negative data sufficiently covers the semantic space of forget-worthy queries without needing original forget samples or retain data.
- Evidence anchors:
  - [section 4.1]: "Our detector uses only paraphrased negative unlearning data to identify incoming prompts that require unlearning."
  - [section 4.1, Eq. 4-5]: Formal confidence score definitions combining exact match, embedding similarity, and trained model probability.
  - [corpus]: Related work (GUARD, BLUR) confirms detection-based unlearning is an active research direction, but DRAGON uniquely requires no retain data.
- Break condition: If paraphrased queries fail to generalize to novel attack formulations (e.g., multilingual obfuscation), detection accuracy degrades. Table 19 shows 12% drop on 4-language mixing.

### Mechanism 2
- Claim: Chain-of-thought instructions generated by a guard model produce context-aware refusals that preserve response coherence better than template-based approaches.
- Mechanism: The guard model (fine-tuned Llama3.1-8B-Instruct) takes the detected query and retrieved safety policy, then generates stepwise CoT reasoning instructions. These are prepended to the original prompt, leveraging the base LLM's instruction-following capability to guide refusal behavior without weight modification.
- Core assumption: The base LLM has sufficient instruction-following capability to comply with dynamically generated CoT instructions.
- Evidence anchors:
  - [section 4.2]: "These reasoning outputs can then be used to guide the original model to reason more carefully and follow instructions more reliably."
  - [section 6.2, Table 7]: Ablation shows DRAGON achieves DS=21.4 vs. 43.9 without CoT; consistency score gap to NPO-RT is 0.01 vs. 0.44 for template refusal.
  - [corpus]: Corpus lacks direct comparison of dynamic vs. static CoT for unlearning; this mechanism remains underexplored in prior work.
- Break condition: Smaller models (e.g., Phi-1.5B) with weaker instruction-following may fail to comply with CoT instructions, degrading refusal quality.

### Mechanism 3
- Claim: In-context intervention preserves general language capabilities because it does not modify model weights, avoiding the forget-retain trade-off inherent in fine-tuning approaches.
- Mechanism: DRAGON intercepts prompts at inference time, routing forget-worthy queries through the guard model's CoT instructions while passing benign queries unchanged. No gradient updates occur, so the model's original knowledge representation remains intact.
- Core assumption: The detection module has sufficiently low false positive rates on non-forget content to avoid unnecessary intervention on benign queries.
- Evidence anchors:
  - [abstract]: "Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs."
  - [Table 1-2]: MMLU accuracy remains at 68.0% for Llama3.1-8B (identical to original) after DRAGON, while training-based baselines show degradation.
  - [corpus]: BLUR benchmark (arXiv:2506.15699) confirms forget-retain overlap creates persistent trade-offs in fine-tuning approaches—DRAGON's design explicitly avoids this.
- Break condition: If detection false positives are high, benign queries receive unnecessary refusals, harming utility. Table 20 shows <1% false positive on SimpleQA/Alpaca, but domain shift could increase this.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: DRAGON's guard model generates CoT instructions to guide refusal reasoning. Understanding how CoT elicits stepwise reasoning is essential for debugging guard model outputs.
  - Quick check question: Can you explain why "Let's think step by step" improves reasoning in LLMs, and what happens if the CoT instruction is generic vs. query-specific?

- **In-Context Learning (ICL)**
  - Why needed here: DRAGON relies on the base model's ability to follow prepended instructions without weight updates. This is fundamentally an ICL mechanism.
  - Quick check question: If a model has weak ICL capability, what would you expect to happen to DRAGON's refusal quality when scaling to smaller models?

- **Embedding Similarity Metrics (BERTScore, ROUGE-L, Cosine Similarity)**
  - Why needed here: The detection module uses these metrics as a secondary verification signal. Understanding their limitations (e.g., BERTScore captures semantic similarity but not intent) is critical for threshold tuning.
  - Quick check question: Why might BERTScore alone be insufficient for detecting adversarial paraphrases of harmful queries?

## Architecture Onboarding

- **Component map:**
  User Query (x) -> [Detection Module] -> Confidence Score f(x, D_u) -> Threshold τ? -> [Policy Retrieval] -> [Guard Model] -> CoT Instruction Generation -> [Prompt Assembly] -> [Base LLM Inference] -> Refusal/Redirection Output

- **Critical path:** Detection module → (if triggered) Policy retrieval → Guard model CoT generation → Base LLM inference. Latency bottleneck is guard model inference (~665ms on A100).

- **Design tradeoffs:** Detection module balances false positive rate (affects utility) vs. false negative rate (affects unlearning effectiveness). The use of paraphrased negative data avoids retain data requirements but requires more storage for paraphrase embeddings.

- **Failure signatures:** Low detection accuracy (<90%) indicates insufficient paraphrase coverage or poor scoring model performance. High Dynamic Deviation Score (>40) suggests CoT instructions are causing coherence issues in base LLM responses.

- **First experiments:** 1) Test detection accuracy on forget set (target: >95%). 2) Measure MMLU accuracy preservation after DRAGON deployment. 3) Evaluate Dynamic Deviation Score with and without CoT instructions.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on synthetic benchmarks where forget/retain boundaries are predefined, raising questions about real-world robustness.
- Detection module's reliance on paraphrased negative data without retain examples creates fundamental trade-offs between coverage and storage costs.
- CoT dataset generation using GPT-4o may introduce bias toward high-resource languages and Western reasoning patterns.

## Confidence
- **High confidence**: The claim that DRAGON preserves base model capabilities better than fine-tuning approaches is well-supported by MMLU accuracy comparisons showing no degradation (68.0% vs. 68.0%).
- **Medium confidence**: The superiority of CoT-based refusals over template approaches (DS=21.4 vs. 43.9) is convincing but depends on the guard model's instruction-following capability, which may not generalize to smaller models.
- **Low confidence**: Claims about robustness to adversarial attacks (12% performance drop on 4-language mixing) are based on a single attack vector without exploring other common evasion techniques.

## Next Checks
1. Test DRAGON on naturally occurring harmful queries from platforms like Reddit or Twitter to verify detection accuracy beyond synthetic benchmarks. Measure false positive rates on benign queries in the same domain.

2. Deploy DRAGON on Phi-1.5B or Qwen2.5-1.5B to empirically test whether CoT instructions maintain refusal quality. If refusal rates drop below 80% while detection remains >90%, the instruction-following assumption breaks.

3. Evaluate DRAGON against established attack suites like AdvGLUE or TextFlint, measuring both attack success rate and utility degradation. Compare performance against fine-tuning baselines under identical attack conditions.