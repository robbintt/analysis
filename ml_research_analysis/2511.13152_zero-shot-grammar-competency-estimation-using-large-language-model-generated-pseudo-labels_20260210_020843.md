---
ver: rpa2
title: Zero-Shot Grammar Competency Estimation Using Large Language Model Generated
  Pseudo Labels
arxiv_id: '2511.13152'
source_url: https://arxiv.org/abs/2511.13152
tags:
- grammar
- scoring
- training
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of grammar competency estimation
  in spoken language, where spontaneous, unstructured, and disfluent speech makes
  manual annotation impractical. To overcome this, the authors propose a zero-shot
  framework that uses large language models (LLMs) to generate pseudo-labels from
  unlabeled data, guided by rubric-based prompts.
---

# Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels

## Quick Facts
- arXiv ID: 2511.13152
- Source URL: https://arxiv.org/abs/2511.13152
- Reference count: 24
- Primary result: Zero-shot grammar assessment framework using LLM-generated pseudo-labels achieves QWK scores up to 0.776 on spoken response datasets

## Executive Summary
This paper addresses the challenge of grammar competency estimation in spoken language, where spontaneous, unstructured, and disfluent speech makes manual annotation impractical. The authors propose a zero-shot framework that uses large language models (LLMs) to generate pseudo-labels from unlabeled data, guided by rubric-based prompts. These pseudo-labels train a transformer-based model with adaptive sample-weighting to handle label noise. The approach works for both written and spoken responses, offering scalability without expert-annotated data. Experimental results on two in-house datasets show strong performance, outperforming strong baselines.

## Method Summary
The proposed zero-shot framework generates pseudo-labels using LLMs guided by rubric-based prompts, which are then used to train a transformer-based model. A novel adaptive sample-weighting method handles label noise inherent in pseudo-labels. The framework operates without requiring expert-annotated data, making it scalable for grammar assessment tasks. The approach is validated on spoken response datasets (SGAD and WGAD), demonstrating effectiveness through comparative analysis with baseline methods.

## Key Results
- Achieves QWK scores up to 0.776 on in-house datasets
- Demonstrates strong performance with low RMSE values
- Outperforms strong baseline methods in grammar competency estimation
- Shows effectiveness for both written and spoken response assessment

## Why This Works (Mechanism)
The framework leverages LLMs' ability to understand grammar rules through rubric-guided prompts, generating pseudo-labels that capture grammatical competency. The adaptive sample-weighting method addresses the inherent noise in these pseudo-labels by adjusting training weights based on confidence estimates. This combination allows effective learning from unlabeled data while maintaining assessment quality, bridging the gap between zero-shot requirements and accurate grammar evaluation.

## Foundational Learning

**LLM Pseudo-Label Generation**
- Why needed: Enables zero-shot learning without manual annotation
- Quick check: Verify rubric prompt quality and consistency

**Adaptive Sample Weighting**
- Why needed: Mitigates noise in pseudo-labeled data
- Quick check: Monitor weight distribution during training

**Grammar Assessment Rubrics**
- Why needed: Provides structured guidance for LLM evaluation
- Quick check: Validate rubric coverage of grammar aspects

**Transformer-Based Classification**
- Why needed: Handles complex grammar patterns in responses
- Quick check: Test model sensitivity to different grammar errors

## Architecture Onboarding

**Component Map**
LLM Pseudo-Label Generator -> Sample Weight Calculator -> Transformer Classifier -> Grammar Score Output

**Critical Path**
Pseudo-label generation → Weight calculation → Model training → Score prediction

**Design Tradeoffs**
Zero-shot approach sacrifices some accuracy for scalability and eliminates annotation costs. Adaptive weighting adds complexity but improves robustness to noisy pseudo-labels.

**Failure Signatures**
Poor rubric prompts lead to inconsistent pseudo-labels. Inadequate weight adaptation fails to suppress noisy samples. Transformer model may struggle with highly disfluent speech patterns.

**First Experiments**
1. Validate pseudo-label quality with sample responses
2. Test weight adaptation on controlled noise scenarios
3. Compare performance across different rubric prompt variations

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but implicit challenges remain around generalizability, scalability costs, and performance across diverse linguistic contexts.

## Limitations
- Performance relies heavily on LLM-generated pseudo-label quality
- Framework tested only on two in-house datasets, limiting generalizability
- Real-world deployment challenges and computational costs not fully addressed

## Confidence

**High confidence**: Technical feasibility of LLM-based pseudo-label generation for grammar assessment is well-supported by experimental results and baseline comparisons.

**Medium confidence**: Scalability claims are reasonable given zero-shot approach, though real-world deployment and cost implications require further validation.

**Medium confidence**: Adaptive sample-weighting method shows effectiveness in controlled conditions, but robustness to varying noise distributions needs additional testing.

## Next Checks
1. Conduct cross-validation on external datasets from different educational contexts and linguistic backgrounds to assess generalizability beyond the in-house datasets.

2. Perform ablation studies to quantify the specific contribution of the adaptive sample-weighting method versus other components of the framework.

3. Evaluate the framework's performance under varying levels of LLM quality and cost constraints to understand trade-offs between pseudo-label accuracy and computational resources.