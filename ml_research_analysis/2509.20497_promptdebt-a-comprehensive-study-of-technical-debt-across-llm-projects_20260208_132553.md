---
ver: rpa2
title: 'PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects'
arxiv_id: '2509.20497'
source_url: https://arxiv.org/abs/2509.20497
tags:
- satd
- prompt
- debt
- prompts
- comments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes self-admitted technical debt (SATD) in LLM
  projects, identifying 6.61% of debt from prompt configuration and 4.51% from hyperparameter
  tuning. OpenAI accounts for 54.49% of SATD instances, with prompt design and framework
  integration being primary sources.
---

# PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects

## Quick Facts
- **arXiv ID:** 2509.20497
- **Source URL:** https://arxiv.org/abs/2509.20497
- **Reference count:** 40
- **Primary result:** 6.61% of technical debt in LLM projects stems from prompt configuration issues, with OpenAI accounting for 54.49% of self-admitted technical debt instances.

## Executive Summary
This study systematically analyzes self-admitted technical debt (SATD) in LLM-based software projects, revealing that prompt configuration and hyperparameter tuning represent significant sources of maintenance burden. Through analysis of 93,142 Python files from 37,944 GitHub repositories, researchers identified that 1.77% of all code comments contain SATD, with 6.61% specifically related to prompt configuration and 4.51% to hyperparameter tuning. Instruction-based prompts (38.60%) and few-shot prompts (18.13%) are particularly vulnerable to debt accumulation due to instruction clarity and example quality issues.

## Method Summary
The researchers employed a multi-stage methodology using the PromptSet dataset: first filtering Python files containing LLM API keywords, then extracting and cleaning comments via Python's tokenize library, followed by SATD detection using the SATDDetector tool augmented with keyword matching, and finally manual classification of a stratified sample of 998 SATD comments to identify LLM-specific debt types. The approach combined automated detection with human validation to categorize debt across prompt design, framework integration, cost management, and hyperparameter tuning domains.

## Key Results
- Instruction-based prompts (38.60%) and few-shot prompts (18.13%) are most vulnerable to SATD due to instruction clarity and example quality issues
- 6.61% of technical debt relates to prompt configuration and optimization, primarily involving incomplete prompt structures
- OpenAI dominates LLM SATD (54.49% of instances), followed by LangChain (12.35%) and Anthropic (7.02%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If prompt configuration is decentralized and lacks templating, technical debt accumulates in LLM-based systems.
- Mechanism: Hardcoded prompts without centralized templates increase maintenance effort because changes require manual updates across multiple files, as seen in SATD comments like `#TODO: other conversation template` and `#TODO: dynamically set the city here in the prompt`.
- Core assumption: Developers will document prompt-related shortcuts as comments rather than immediately refactoring.
- Evidence anchors:
  - [abstract] "6.61% of debt related to prompt configuration and optimization issues"
  - [section 4.2] "the most frequent issue involves incomplete prompt configurationsâ€”that is, the need for prompt structures or templates to be dynamically adjustable"
  - [corpus] Weak corpus support; related work focuses on general SATD but not specifically prompt debt patterns.
- Break condition: If prompts are abstracted into version-controlled, parameterized templates early, debt accumulation decreases.

### Mechanism 2
- Claim: If instruction-based prompts exceed clarity thresholds or length limits, they attract more self-admitted debt.
- Mechanism: Long instructions bury critical details in the middle of context, reducing LLM comprehension and requiring repeated fixes, as noted in comments like `#If prompt is too long ...` and `#TODO: Make it follow the prompt? Could be missing instructions`.
- Core assumption: Instruction clarity correlates with output quality and debt accumulation frequency.
- Evidence anchors:
  - [abstract] "instruction-based prompts (38.60%) and few-shot prompts (18.13%) are most vulnerable due to instruction clarity and example quality issues"
  - [section 4.3] "Instruction Block prompts... were identified as particularly vulnerable to SATD, comprising 37.7% of our dataset"
  - [corpus] No direct corpus evidence for instruction-clarity debt; neighboring papers address general SATD, not prompt-specific patterns.
- Break condition: If prompts are structured with explicit output format instructions and length constraints, debt reduces.

### Mechanism 3
- Claim: If hyperparameter tuning is deferred without documented thresholds, it creates recurring debt during evaluation and optimization cycles.
- Mechanism: Undocumented or ad-hoc hyperparameter choices (e.g., temperature, top_p) lead to repeated trial-and-error, as seen in `#TODO: figure out which temperature is best for evaluation` and `#TODO handle top_p, top_k, etc.`, increasing maintenance cost.
- Core assumption: Optimal hyperparameter values are task-specific and require empirical determination.
- Evidence anchors:
  - [abstract] "4.51% from hyperparameter tuning"
  - [section 4.2] "tuning models and adjusting their hyperparameters is critical to achieving optimal results [7, 36]. However, improperly configured parameters... can lead to suboptimal results and technical debt."
  - [corpus] Limited support; corpus papers discuss ML SATD broadly but not LLM hyperparameter-specific debt.
- Break condition: If hyperparameter defaults are established per task type and documented, tuning debt decreases.

## Foundational Learning

- Concept: Self-Admitted Technical Debt (SATD)
  - Why needed here: The paper's entire analysis rests on detecting and classifying SATD comments in LLM projects to understand where debt accumulates.
  - Quick check question: Can you identify a SATD comment in your own codebase (e.g., `#TODO: refactor prompt logic`)?

- Concept: Prompt Engineering Techniques
  - Why needed here: The paper categorizes debt by prompt technique (instruction-based, few-shot, documentation, CoT, code block, zero-shot), so understanding these is essential for interpreting the findings.
  - Quick check question: What is the difference between few-shot and zero-shot prompting in terms of example usage?

- Concept: LLM Hyperparameters
  - Why needed here: Hyperparameter tuning debt (4.51%) is a significant category; understanding parameters like temperature, top_p, and max_tokens is necessary to contextualize the findings.
  - Quick check question: What does the temperature parameter control in LLM output generation?

## Architecture Onboarding

- Component map: LLM API clients (OpenAI 54.49%, Anthropic, Cohere) -> orchestration frameworks (LangChain 12.35%) -> prompt templates and configurations -> hyperparameter settings -> cost/token management logic

- Critical path: Prompt design and configuration flow directly into framework integration (e.g., LangChain's PromptTemplate), then into API calls with hyperparameters, generating outputs that must be parsed and cost-tracked. Debt accumulates at prompt configuration (6.61%) and framework integration (4.31%) stages.

- Design tradeoffs: Using frameworks like LangChain reduces boilerplate but introduces framework-specific debt (e.g., `#TODO: use langchaing output parser`); centralized prompt templates improve maintainability but require initial setup; lower-cost models reduce cost debt but may sacrifice output quality.

- Failure signatures: (1) Prompt too long warnings, (2) Missing or hardcoded context in prompts, (3) Undocumented hyperparameter values, (4) Deferred adoption of output parsers, (5) Cost-related TODOs about token limits.

- First 3 experiments:
  1. Audit your codebase for SATD comments related to prompts, hyperparameters, and framework usage using keyword patterns from the paper (e.g., `TODO`, `FIXME`, `prompt`, `temperature`).
  2. Implement a centralized prompt template registry and measure reduction in prompt-related SATD over 2 sprints.
  3. Document default hyperparameter values per task type (e.g., temperature=0.3 for classification) and track whether tuning-related SATD comments decrease.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prevalence and taxonomy of technical debt differ in highly customized transformer models compared to API-based LLM integrations?
- Basis in paper: [explicit] The authors acknowledge the study is limited to API-based projects and state, "Future studies should expand to a wider range of models... [as] SATD identified might not apply equally to highly customized transformer models."
- Why unresolved: The current dataset (PromptSet) excludes projects focusing on deeper configurations like learning rates or optimizer choices, limiting generalizability to in-house training efforts.
- What evidence would resolve it: An empirical analysis of SATD in repositories with custom training loops, comparing the frequency of Prompt Debt vs. Model/Training Debt.

### Open Question 2
- Question: What are the specific technical debt accumulation patterns associated with complex prompt architectures like Chain-of-Thought (CoT) or Retrieval-Augmented Generation (RAG)?
- Basis in paper: [explicit] The conclusion states, "we aim to investigate the specific technical debts linked to various prompt techniques," and the authors note RAG as a potential mitigation strategy not yet explored in depth.
- Why unresolved: While the study identified Instruction and Few-shot prompts as prone to debt, the sample size for complex techniques like CoT was small (12 instances), and RAG was only mentioned as a mitigation strategy.
- What evidence would resolve it: A focused qualitative analysis of codebases utilizing advanced prompt engineering frameworks to identify unique debt lifecycles for these architectures.

### Open Question 3
- Question: Can automated static analysis tools effectively identify LLM-specific technical debt (e.g., prompt smells) without relying on developer comments?
- Basis in paper: [inferred] The methodology relied on SATDDetector (designed for traditional software) which produced false positives, and the authors highlighted the need to "motivate the development of automated tools for effective prompt refactoring."
- Why unresolved: Current findings are restricted to Self-Admitted Technical Debt (SATD), which misses unreported structural issues (smells) in prompt code that developers do not annotate.
- What evidence would resolve it: The development and validation of a linter or static analyzer that flags non-admitted prompt issues (e.g., hardness, ambiguity) and correlates them with model performance.

## Limitations
- Manual classification of SATD comments introduces potential sampling bias in the analysis
- LLM-based classification of prompt techniques may misclassify edge cases with mixed instruction/reasoning patterns
- SATD detection methodology depends on undisclosed supplementary keyword lists, limiting reproducibility

## Confidence
- **High Confidence:** Overall SATD prevalence (1.77%) and OpenAI dominance (54.49%) are well-supported
- **Medium Confidence:** Prompt type breakdown (instruction-based 38.60%, few-shot 18.13%) depends on LLM classification accuracy
- **Low Confidence:** Specific debt category percentages (6.61% prompt configuration, 4.51% hyperparameter tuning) are sample-based estimates

## Next Checks
1. Reproduce SATD detection by applying SATDDetector with undisclosed keyword list [14] to a PromptSet subset and compare 1.77% prevalence
2. Manually verify 50 randomly selected "instruction-based" prompts to confirm they contain complex instructions rather than reasoning steps
3. Calculate SATD type distribution in remaining unlabeled corpus (n=93,142 - 998) to assess sampling representativeness