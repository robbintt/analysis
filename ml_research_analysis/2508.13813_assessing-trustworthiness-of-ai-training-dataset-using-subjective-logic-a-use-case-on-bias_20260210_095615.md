---
ver: rpa2
title: Assessing Trustworthiness of AI Training Dataset using Subjective Logic --
  A Use Case on Bias
arxiv_id: '2508.13813'
source_url: https://arxiv.org/abs/2508.13813
tags:
- dataset
- trust
- uncertainty
- bias
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first formal framework for assessing
  the trustworthiness of AI training datasets using Subjective Logic. The method quantifies
  trust in global properties such as bias, fairness, and representativeness, particularly
  when evidence is incomplete or distributed.
---

# Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias

## Quick Facts
- **arXiv ID:** 2508.13813
- **Source URL:** https://arxiv.org/abs/2508.13813
- **Reference count:** 29
- **Primary result:** First formal framework for assessing AI training dataset trustworthiness using Subjective Logic

## Executive Summary
This paper introduces a novel framework for assessing the trustworthiness of AI training datasets using Subjective Logic, specifically addressing global properties like bias, fairness, and representativeness that emerge from collective data patterns. The framework quantifies trust in dataset properties even when evidence is incomplete or distributed across federated settings. It extends previous work focused on individual data trustworthiness to dataset-level properties, providing uncertainty-aware assessments that can guide dataset selection and model improvement decisions.

## Method Summary
The framework extends Subjective Logic to assess dataset trustworthiness by modeling properties such as bias, fairness, and representativeness as collective attributes that emerge from the entire dataset rather than individual data points. The method quantifies uncertainty through belief, disbelief, and uncertainty parameters, enabling trust assessments even with incomplete evidence. The framework was instantiated for bias assessment using a Traffic Sign Recognition dataset, where bias was measured through class imbalance metrics. Two operational methods were proposed: Method 1 (multiplicative) and Method 2 (additive) for updating trust assessments based on evidence, with the choice depending on context and evidence type.

## Key Results
- Successfully detected class imbalance in traffic sign datasets, showing belief increasing from 0.50 to 0.64 after data augmentation
- In federated settings, Method 1 showed sharper belief decreases compared to Method 2 when handling imbalanced sub-datasets
- Accuracy on warning signs improved from 95.19% to 95.40% with data augmentation, demonstrating practical impact

## Why This Works (Mechanism)
The framework leverages Subjective Logic's ability to handle uncertainty and incomplete evidence by representing trust as belief, disbelief, and uncertainty parameters. This approach is particularly effective for dataset-level properties because it can aggregate evidence from distributed sources while maintaining uncertainty quantification. The mathematical foundations of Subjective Logic provide a principled way to update trust assessments as new evidence becomes available, making it suitable for both centralized and federated learning contexts.

## Foundational Learning

**Subjective Logic** - A probabilistic logic framework that extends traditional logic to handle uncertainty through belief, disbelief, and uncertainty parameters. Needed to quantify trust in dataset properties when evidence is incomplete or conflicting. Quick check: Verify the framework correctly reduces to classical logic when uncertainty approaches zero.

**Trust Metrics** - Quantitative measures of confidence in dataset properties like bias, fairness, and representativeness. Needed to provide interpretable assessments that can guide model development decisions. Quick check: Confirm metrics align with established bias detection methods in validation experiments.

**Federated Learning Context** - Distributed learning scenarios where data remains decentralized across multiple sources. Needed to ensure the framework works in real-world deployment scenarios where data cannot be centralized. Quick check: Test framework performance across varying degrees of data imbalance in distributed settings.

## Architecture Onboarding

**Component Map:** Data Sources -> Trust Assessment Engine -> Property Analysis -> Trust Update Mechanism -> Final Trust Score

**Critical Path:** The framework processes data through trust assessment (calculating belief/disbelief/uncertainty), analyzes dataset properties (bias/fairness/representativeness), and updates trust scores based on evidence using either multiplicative or additive methods.

**Design Tradeoffs:** Method 1 (multiplicative) provides more conservative trust updates but may be overly sensitive to extreme evidence, while Method 2 (additive) offers more gradual adjustments but may underreact to significant evidence changes.

**Failure Signatures:** The framework may produce misleading trust assessments when evidence is severely imbalanced or when dataset properties are not well-represented by the chosen metrics. Trust scores may become unstable when uncertainty parameters dominate belief/disbelief values.

**First Experiments:**
1. Test framework across diverse dataset types (text, tabular, medical imaging) to validate generalizability
2. Compare Subjective Logic trust assessments against established bias detection metrics on benchmark datasets
3. Implement real-world federated learning deployment to verify operational performance

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to traffic sign recognition, constraining generalizability to other domains
- Theoretical assumption that belief functions accurately represent complex dataset properties requires empirical validation
- Federated learning experiments simulated rather than deployed in real operational settings

## Confidence

**Theoretical Framework:** Medium - Subjective Logic foundations are well-established, but adaptation to dataset-level properties is novel
**Experimental Results:** High for traffic sign dataset, Medium for federated learning scenarios
**Generalizability:** Low - Limited validation across diverse dataset types and domains

## Next Checks

1. Validate framework across diverse dataset types (text, tabular, medical imaging) to test generalizability beyond computer vision
2. Conduct controlled experiments comparing Subjective Logic trust assessments against established bias detection metrics to verify alignment
3. Implement real-world federated learning deployments to verify the method's performance under operational conditions with actual distributed data sources