---
ver: rpa2
title: 'V-CECE: Visual Counterfactual Explanations via Conceptual Edits'
arxiv_id: '2509.16567'
source_url: https://arxiv.org/abs/2509.16567
tags:
- edits
- image
- classifier
- counterfactual
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V-CECE addresses the gap between human and neural model reasoning
  in visual counterfactual explanations by proposing optimal semantic edits and applying
  them through a diffusion model. It leverages a knowledge graph and bipartite matching
  to determine minimal, actionable conceptual edits (insertions, deletions, substitutions)
  to transform an image from one class to another.
---

# V-CECE: Visual Counterfactual Explanations via Conceptual Edits

## Quick Facts
- arXiv ID: 2509.16567
- Source URL: https://arxiv.org/abs/2509.16567
- Reference count: 40
- One-line primary result: V-CECE achieves 88.9-99.5% success rate on BDD100K and shows CNNs require more edits with lower image quality than LVLMs, revealing different semantic understanding patterns.

## Executive Summary
V-CECE addresses the gap between human and neural model reasoning in visual counterfactual explanations by proposing optimal semantic edits and applying them through a diffusion model. It leverages a knowledge graph and bipartite matching to determine minimal, actionable conceptual edits (insertions, deletions, substitutions) to transform an image from one class to another. Edits are ordered either locally via an LVLM, globally by classifier importance, or a hybrid approach. Applied to BDD100K and Visual Genome, V-CECE generates counterfactuals without training, showing strong performance: SR of 88.9-99.5% on BDD100K and Avg.|E| of 2.54-4.77 across models. Human evaluation shows LVLMs like Claude 3.5 Sonnet align better with human reasoning than CNNs, which often require more edits and produce lower-quality images, revealing differing semantic understanding.

## Method Summary
V-CECE is a black-box, zero-training pipeline for generating visual counterfactual explanations. The framework first computes an optimal set of semantic edits (insertions, deletions, substitutions) between source and target classes using WordNet semantic distances and bipartite matching (Hungarian algorithm). These edits are then ordered using one of three strategies: local (LVLM selects next edit per step), global (edits ranked by classifier importance scores), or local-global (local edits ordered by global importance). The generative component applies edits iteratively using GroundingDINO+SAM for object masking and frozen Stable Diffusion v1.5 Inpainting for execution, terminating when the classifier's label flips. The method is evaluated on BDD100K and Visual Genome using success rate, average edits required, FID, CMMD, and human evaluation.

## Key Results
- Success Rate of 88.9-99.5% on BDD100K across different classifiers and ordering strategies
- Average edits required (Avg|E|) of 2.54-4.77 across models, with Local-Global ordering generally performing best
- Human evaluation reveals LVLMs like Claude 3.5 Sonnet align better with human reasoning than CNNs, which require more edits and produce lower-quality images
- CNN models (DenseNet, ConvNext, EfficientNet, Swin) show higher Avg|E| and more artifacts compared to LVLM classifiers

## Why This Works (Mechanism)

### Mechanism 1: Optimal Semantic Edit Computation via Bipartite Matching
The framework computes the provably minimal set of conceptual edits to transform class L→L* by placing concepts from source and target classes on a bipartite graph with edge weights derived from WordNet shortest-path distances. The Hungarian algorithm solves the assignment problem in O(mn log n) time, yielding minimum-cost insertions, deletions, and substitutions. Core assumption: Semantic distances on WordNet meaningfully approximate the "cost" of conceptual transformations; the classifier reasons at a comparable semantic granularity. Break condition: If WordNet distances don't align with classifier decision boundaries (e.g., classifiers use spurious correlations), optimal semantic edits may fail to flip labels efficiently.

### Mechanism 2: Iterative Edit Ordering via LVLM or Global Importance
Ordering edits by LVLM reasoning, global classifier importance, or hybrid local-global improves label-flip efficiency over random ordering. Local: LVLM selects the most plausible next edit given scene context. Global: Importance scores from edit frequency across dataset rank edits. Local-Global: Local edits ordered by global importance scores. Core assumption: LVLMs possess sufficient commonsense and spatial reasoning to prioritize semantically influential edits; global importance captures systematic classifier biases. Break condition: If LVLM ordering introduces hallucinated priorities or global importance overfits to dataset-specific biases, edit sequences become suboptimal.

### Mechanism 3: Zero-Training Counterfactual Generation via Frozen Diffusion Inpainting
A frozen Stable Diffusion inpainting model can execute semantic edits without task-specific training, producing high-quality counterfactuals. GroundingDINO + SAM generates object masks; SD v1.5 Inpainting applies edits with denoising strength 1.0 and guidance scale 10. Edits applied iteratively until classifier label flips or edit set exhausted. Core assumption: The pre-trained diffusion model generalizes sufficiently to diverse semantic edits; masking and inpainting preserve semantic integrity without introducing confounding artifacts. Break condition: Large object removals or complex multi-step edits cause artifacts that flip labels via distributional shifts rather than semantic content.

## Foundational Learning

- Concept: **Bipartite Matching / Hungarian Algorithm**
  - Why needed here: Core to computing minimal edit sets efficiently.
  - Quick check question: Given two sets of concepts {A, B} and {X, Y} with substitution costs, can you trace how Hungarian algorithm finds the minimum-cost assignment?

- Concept: **Knowledge Graph Distance Metrics**
  - Why needed here: Determines edit costs via WordNet path distances.
  - Quick check question: How would you compute the distance between "car" and "truck" in WordNet? What does a shorter path imply for edit cost?

- Concept: **Diffusion Inpainting Mechanics**
  - Why needed here: Understanding how masked regions are regenerated without retraining.
  - Quick check question: If you set denoising strength to 0.5 instead of 1.0, what happens to the inpainted region's fidelity to the original?

## Architecture Onboarding

- Component map:
  - Explanation Component: WordNet + Bipartite Matching → Edit set E
  - Edit Ordering: LVLM (Local) / Importance Scores (Global) / Hybrid
  - Generative Component: GroundingDINO + SAM (masking) → SD v1.5 Inpainting (execution)
  - Classification Check: Query classifier C after each edit; terminate on label flip

- Critical path:
  1. Extract source/target concept sets from images (object annotations)
  2. Compute edit set E via bipartite matching (Section 3.1)
  3. Order edits via selected strategy
  4. For each edit: mask → inpaint → classify → stop if flipped
  5. Evaluate with FID, CMMD, SR, Avg|E|, human survey

- Design tradeoffs:
  - **Frozen vs. fine-tuned diffusion**: Frozen avoids dataset bias transfer but may produce lower-quality edits on domain-specific images.
  - **Local vs. global ordering**: Local captures scene context; global exploits systematic biases; hybrid balances both at computational cost.
  - **Black-box vs. white-box**: Black-box enables proprietary model explanation but loses gradient-based optimization efficiency.

- Failure signatures:
  - High Avg|E| with low SR: Classifier semantic space misaligned with WordNet (e.g., CNNs requiring 5+ edits with artifacts)
  - Low visual correctness (>40% artifacts): Large object removal or multi-step edits causing diffusion failures
  - Random global importance distribution: Classifier relies on spurious features rather than semantic concepts

- First 3 experiments:
  1. Reproduce Table 1 on BDD100K subset with DenseNet and Claude 3.5 Sonnet; verify SR and Avg|E| gaps.
  2. Ablate ordering strategies: Compare Local, Global, Local-Global on same image set; measure steps to flip and visual quality.
  3. Stress test inpainting: Manually select 10 images requiring large object removal; catalog artifact types and their correlation with classifier confidence drops.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does inter-rater variability among human evaluators influence the measured "explanatory gap" between neural networks and human reasoning?
- Basis in paper: [explicit] The authors state a need to "quantify inter-individual variability" to "assess inter-rater reliability and identify sources of disagreement" as the current cohort was modest.
- Why unresolved: The current study used a modest cohort of 31 participants, which limits the statistical power and precision necessary to model how different humans interpret the same counterfactuals.
- What evidence would resolve it: A large-scale human evaluation involving diverse disciplines that calculates Inter-Rater Reliability (IRR) scores and identifies specific semantic features causing disagreement.

### Open Question 2
- Question: How does the V-CECE framework perform in highly field-dependent domains, such as medical imaging, where concepts are specialized?
- Basis in paper: [explicit] The authors propose to "extend the evaluation of the framework across additional disciplines, including the medical domain, to surface challenges... in settings with greater field dependence."
- Why unresolved: The experiments were restricted to BDD100K (driving) and Visual Genome (general scenes), leaving the efficacy of the method in high-stakes, specialized domains untested.
- What evidence would resolve it: Application of V-CECE to a medical dataset (e.g., X-rays) showing successful counterfactual generation and alignment with medical expert reasoning.

### Open Question 3
- Question: Do white-box generative models introduce detrimental additive bias compared to the frozen diffusion models used in this study?
- Basis in paper: [explicit] The authors currently avoid training generative modules to prevent transferring statistical biases, but they plan to "evaluate white-box generative models and examine whether additive bias is detrimental."
- Why unresolved: The paper relies on a plug-and-play approach (frozen Stable Diffusion) to ensure fairness, leaving the trade-offs regarding the optimization of the generative component unexplored.
- What evidence would resolve it: A comparative study measuring bias metrics and counterfactual success rates between frozen pre-trained models and models optimized/fine-tuned for the specific target dataset.

## Limitations

- WordNet semantic distances may not align with classifier decision boundaries, especially for CNNs trained on spurious correlations
- LVLM reasoning quality (Claude 3.5 Sonnet) introduces second model biases and potential hallucinations
- Frozen diffusion model may struggle with complex multi-object scenes or large removals, causing artifacts that flip labels through distributional shifts

## Confidence

**High Confidence**: The framework's overall pipeline architecture and implementation details (bipartite matching, Hungarian algorithm, GroundingDINO+SAM masking, SD v1.5 Inpainting parameters) are clearly specified and reproducible.

**Medium Confidence**: The comparative claims between CNNs and LVLMs showing different semantic understanding patterns are supported by SR and Avg|E| metrics, but the interpretation of what constitutes "better" semantic reasoning remains subjective without ground truth semantic edit optimality.

**Low Confidence**: The claim that WordNet-based bipartite matching yields "provably minimal" edits that optimally flip classifier labels assumes semantic distances align with decision boundaries—this alignment is not empirically validated for the tested classifiers.

## Next Checks

1. **Classifier Alignment Test**: Take 20 images where DenseNet requires >3 edits to flip labels. Manually inspect each edit's semantic relevance using human annotators to determine if WordNet distances correlate with actual classifier reasoning patterns.

2. **LVLM Ordering Ablation**: Implement a ground-truth ordering baseline using human experts to rank edit sequences on 50 images. Compare against Claude 3.5 Sonnet's ordering in terms of SR, Avg|E|, and label-flip timing to quantify LVLM reasoning quality.

3. **Artifact Correlation Analysis**: For the 30 images with highest FID scores in Table 1, perform detailed visual inspection of intermediate steps. Document artifact types (blurring, object ghosting, background distortion) and correlate with classifier confidence drops to distinguish semantic from distributional label flips.