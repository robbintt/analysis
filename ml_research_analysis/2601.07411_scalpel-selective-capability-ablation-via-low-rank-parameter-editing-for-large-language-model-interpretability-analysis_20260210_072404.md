---
ver: rpa2
title: 'SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for
  Large Language Model Interpretability Analysis'
arxiv_id: '2601.07411'
source_url: https://arxiv.org/abs/2601.07411
tags:
- capability
- language
- tasks
- capabilities
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCALPEL is a framework for selective capability ablation in large
  language models through low-rank parameter editing. It addresses the problem of
  incomplete understanding of how LLMs encode capabilities, which limits their deployment
  in high-stakes applications.
---

# SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis

## Quick Facts
- arXiv ID: 2601.07411
- Source URL: https://arxiv.org/abs/2601.07411
- Reference count: 40
- Primary result: Removes target capabilities while preserving general language abilities, with accuracy drops of 20-43% on target tasks while maintaining near-baseline perplexity (11.1-11.2) and overall capability scores (0.48-0.50) across five diverse tasks.

## Executive Summary
SCALPEL addresses the fundamental challenge of understanding how large language models encode capabilities by proposing a method for selective capability ablation. Rather than attempting to interpret individual neurons or attention heads, SCALPEL treats capabilities as low-rank parameter subspaces distributed across the model. By training LoRA adapters to equalize probabilities between correct and incorrect answers for target capabilities while preserving general language modeling through regularization, the framework achieves selective capability removal with minimal collateral damage to core language abilities.

## Method Summary
SCALPEL uses LoRA adapters (rank=2, α=16) to learn low-rank modifications that disrupt target capabilities. The method trains on capability-specific datasets containing prompt/correct/incorrect triplets, optimizing a probability equalization loss that minimizes the log-probability gap between correct and incorrect answers. Three regularization terms (TextReg, NormReg, SparsityReg) constrain modifications to preserve general language modeling quality. The framework is evaluated across five diverse capabilities including translation, common sense reasoning, and moral reasoning, measuring both removal effectiveness and preservation of general abilities.

## Key Results
- Achieves 20-43% accuracy drops on target capabilities while maintaining baseline perplexity (11.1-11.2)
- Preserves general language abilities with capability scores of 0.48-0.50 across 24 held-out tasks
- Rank ablation shows rank 2 provides stable performance across all tested capabilities
- Multi-component regularization enables selective ablation without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Capability Subspaces
Capabilities can be characterized by low-rank modifications distributed across layers and modules, rather than being localized to discrete components. LoRA adapters learn a low-dimensional subspace that disrupts the target capability, with rank 2 providing stable performance across tasks.

### Mechanism 2: Probability Equalization Loss
Training the model to assign equal probability to correct and incorrect answers effectively removes the target capability while revealing its parameter subspace. This creates gradient signals that push the model toward "maximal confusion" on the target task.

### Mechanism 3: Multi-Component Regularization for Disentanglement
Three complementary regularization terms enable selective capability removal without damaging general language abilities. TextReg minimizes LoRA output magnitude on general text, NormReg applies L2 penalties to LoRA weights, and SparsityReg applies L1 penalties to concentrate modifications.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed: SCALPEL uses LoRA as its parameter modification substrate; Quick check: Can you explain why rank r=2 constrains the expressiveness of learned modifications compared to full fine-tuning?

- **Polysemanticity and Distributed Representations**: Why needed: The paper explicitly critiques component-level interpretability for ignoring that single modules encode multiple capabilities; Quick check: Why does polysemanticity make it difficult to attribute a specific capability to a specific neuron?

- **Probability Equalization vs. Standard Fine-tuning**: Why needed: Unlike standard LoRA fine-tuning that maximizes correct-answer probability, SCALPEL minimizes the probability gap; Quick check: What would happen if you applied standard cross-entropy loss instead of probability equalization?

## Architecture Onboarding

- **Component map**: Frozen backbone (Llama-3.2-1B) -> Trainable LoRA adapters (W_Q, W_K, W_V, W_O, W_gate, W_up, W_down) -> Loss computation (target + 3 regularizations) -> Evaluation pipeline (target accuracy, WikiText-103 perplexity, 24 held-out tasks, 67 BLiMP tasks)

- **Critical path**: 1. Prepare target dataset with (prompt, correct, incorrect) triplets, 2. Initialize LoRA adapters with rank=2, α=16, 3. Optimize L_total with AdamW, 4. Extract learned LoRA weights for analysis

- **Design tradeoffs**: Lower rank → stronger locality constraint but may miss complex capabilities; Higher λ_TextReg → better preservation but weaker removal; Modifying more components → stronger effects but higher collateral damage risk

- **Failure signatures**: Catastrophic perplexity spike (>50) indicates over-aggressive modification breaking core language circuits; Near-zero accuracy drop with high perplexity indicates unstable optimization; Task-specific results like IOI showing 210 perplexity indicate poor component identification

- **First 3 experiments**: 1. Replicate translation ablation with different λ_TextReg values to find the preservation-removal frontier, 2. Test rank r ∈ {1, 2, 4, 8} on a new capability (e.g., code generation), 3. Apply learned LoRA weights from one task to a semantically related task (e.g., moral reasoning → emotional understanding)

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but raises several implicit ones through its experimental design and results, including the anomalous spatial reasoning accuracy increase, the rank-2 assumption's generality across capabilities and model scales, and the relationship between incorrect answer selection and removal specificity.

## Limitations

- Rank-2 assumption may not generalize to more complex capabilities or larger model scales
- Probability equalization could represent overfitting to training distribution rather than genuine capability removal
- Limited evaluation across diverse capability types makes generalizability uncertain

## Confidence

- **High Confidence**: The selective capability removal mechanism works as described for the tested capabilities at the 1B parameter scale
- **Medium Confidence**: The low-rank subspace characterization generalizes beyond the tested capabilities and model scales
- **Low Confidence**: The method achieves genuine capability removal rather than superficial loss gaming

## Next Checks

1. **Rank Scaling Experiment**: Test SCALPEL across multiple model scales (100M → 7B parameters) with rank values {1, 2, 4, 8, 16} on a standardized capability set to establish whether rank-2 assumption is scale-invariant.

2. **Novel Capability Transfer**: Apply LoRA adapters trained on one capability (e.g., translation) to semantically related capabilities (e.g., code generation, summarization) to measure performance transfer as evidence for shared capability subspaces.

3. **Capability Removal Robustness**: Evaluate capability removal on systematically perturbed versions of training prompts - paraphrases, syntactic variants, and compositional extensions - to distinguish genuine capability removal from overfitting to specific prompt templates.