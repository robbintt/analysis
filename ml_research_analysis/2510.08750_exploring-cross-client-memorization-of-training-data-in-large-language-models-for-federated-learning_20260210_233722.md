---
ver: rpa2
title: Exploring Cross-Client Memorization of Training Data in Large Language Models
  for Federated Learning
arxiv_id: '2510.08750'
source_url: https://arxiv.org/abs/2510.08750
tags:
- memorization
- data
- table
- mrinter
- mrintra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for measuring cross-client memorization
  in federated learning (FL) using fine-grained cross-sample assessment adapted from
  centralized learning. The framework estimates intra- and inter-client memorization
  by measuring how often prefixes from one client induce memorization of suffixes
  belonging to the same or different clients.
---

# Exploring Cross-Client Memorization of Training Data in Large Language Models for Federated Learning

## Quick Facts
- **arXiv ID**: 2510.08750
- **Source URL**: https://arxiv.org/abs/2510.08750
- **Reference count**: 40
- **Key outcome**: Cross-client memorization is measurable in FL-trained LLMs, with intra-client memorization consistently higher than inter-client memorization across multiple tasks.

## Executive Summary
This paper introduces a framework for measuring cross-client memorization in federated learning using fine-grained cross-sample assessment adapted from centralized learning. The framework estimates intra- and inter-client memorization by measuring how often prefixes from one client induce memorization of suffixes belonging to the same or different clients. Experiments across summarization, dialog, QA, and classification tasks show that FL models do memorize training data, with intra-client memorization consistently higher than inter-client memorization. Memorization is influenced by prefix length, decoding strategy, and federated algorithm, but not by model size or communication rounds. No clear trade-off between centralized and federated memorization was observed.

## Method Summary
The framework samples prefixes from client C_j, uses them to prompt the global model M, generates text M(p), then retrieves top-k similar suffixes from client C_k via Elasticsearch. The PAN2014 plagiarism detector acts as discriminative function F to determine if generated text matches any retrieved suffix at verbatim, paraphrase, or idea-level granularity. If F returns True, prefix p_j induces memorization of suffix s_k. The pairwise memorization ratio (MR_{j→k}) is computed as the fraction of prefixes from C_j causing the model to memorize suffixes from C_k. Intra-client (MR_{Intra}) and inter-client (MR_{Inter}) memorization are computed as weighted averages across client pairs.

## Key Results
- FL models exhibit both intra-client and inter-client memorization across all evaluated tasks
- Intra-client memorization is consistently higher than inter-client memorization (e.g., Dialog: 1.533% vs 1.446%)
- Shorter prefixes and nucleus decoding amplify memorization extraction rates
- FedProx induces higher memorization rates than FedAvg across tasks
- Memorization is unaffected by model size or communication rounds

## Why This Works (Mechanism)

### Mechanism 1: Cross-Sample Memorization Detection via Pairwise Assessment
The framework samples prefixes from client C_j, uses them to prompt the global model M, generates text M(p), then retrieves top-k similar suffixes from client C_k via Elasticsearch. The PAN2014 plagiarism detector acts as discriminative function F to determine if generated text matches any retrieved suffix at verbatim, paraphrase, or idea-level granularity. If F returns True, prefix p_j induces memorization of suffix s_k. Memorization manifests as text similarity detectable by plagiarism detection; the 50-character minimum match threshold captures meaningful leakage while filtering noise.

### Mechanism 2: Intra-Client Memorization Exceeds Inter-Client Due to Local Training Exposure
In FedAvg, each client trains locally on D_i before server aggregation. The model sees prefixes and suffixes from the same client together during local optimization, creating stronger conditional dependencies (p_i → s_i) than cross-client dependencies (p_j → s_k for j≠k). FedProx's proximal term further anchors local updates to client data, increasing both intra- and inter-client memorization compared to FedAvg. Memorization correlates with gradient exposure frequency; patterns seen together in training batches form stronger associations.

### Mechanism 3: Shorter Prefixes and Nucleus Decoding Amplify Memorization Extraction
With shorter prefixes (10 tokens), the model has less constraining context, allowing it to access broader memorization. Top-k (k=40) and top-p (p=0.8) decoding truncate the vocabulary to plausible tokens, increasing probability of sampling exact training sequences compared to temperature-based sampling which flattens the distribution. Memorized sequences occupy high-probability regions of the output distribution; decoding strategies that exploit these regions will reveal more memorization.

## Foundational Learning

- **Federated Averaging (FedAvg)**: Why needed here: The paper assumes familiarity with local training + server aggregation cycles; memorization metrics are measured on the global model produced by this process. Quick check: Can you explain why FedAvg might preserve client-specific patterns despite model averaging?
- **Cross-Sample vs. Same-Sample Memorization**: Why needed here: The core contribution adapts cross-sample assessment (prefix from sample A triggers suffix from sample B) from centralized to federated settings. Quick check: Why might cross-sample memorization be more concerning for privacy than same-sample memorization?
- **Plagiarism Detection as Memorization Proxy**: Why needed here: PAN2014's three-level granularity (verbatim, paraphrase, idea) determines what counts as "memorization" in this framework. Quick check: What are the limitations of using text similarity as a proxy for training data extraction?

## Architecture Onboarding

- Component map: Client Data (D_j) → Prefix Sampling (n=4K) → Model Inference (M with decoding strategy) → Generated Text M(p) → Client Data (D_k) → Suffix Indexing ← Elasticsearch Retrieval (top-10) ← PAN2014 Plagiarism Detector (F) ← Memorization Label (True/False) ← MR_{j→k}, MR_{Intra}, MR_{Inter} Computation
- Critical path: 1. Train FL model (FedAvg/FedProx) across L clients for R rounds 2. For each client pair (j, k), sample 4K prefixes from P_j and 4K suffixes from S_k 3. Generate completions for all sampled prefixes using trained global model 4. Index all suffixes in Elasticsearch; retrieve top-10 similar suffixes per generation 5. Apply PAN2014 detector with 50-character minimum threshold 6. Compute pairwise MR_{j→k}, aggregate to MR_{Intra} (j=k weighted average) and MR_{Inter} (j≠k weighted average)
- Design tradeoffs: Sample size (n=4K) vs. estimation accuracy; prefix length vs. memorization sensitivity; FedAvg vs. FedProx; PAN2014 granularity vs. false positives
- Failure signatures: Zero memorization in classification (output length below 50-character threshold); false positives from incoherent outputs; MR_{Intra} ≈ MR_{Intra} (near-IID data partitioning); inconsistent results across trials (decoding stochasticity)
- First 3 experiments: 1. Baseline memorization measurement with FedAvg on single task 2. Prefix length ablation with lengths [10, 30, 50, 100] 3. Algorithm comparison between FedAvg and FedProx

## Open Questions the Paper Calls Out

### Open Question 1
Why does intra-client memorization consistently exceed inter-client memorization across all tasks and model configurations? The paper provides empirical observations but no theoretical explanation for the underlying mechanism causing same-client data to be memorized more than cross-client data.

### Open Question 2
What mechanisms cause FedProx to induce higher memorization rates than FedAvg across tasks? The proximal regularization term's effect on memorization remains unexplored; the paper only documents the empirical finding without analyzing whether constrained local updates or other factors drive this increase.

### Open Question 3
How can cross-client memorization be accurately measured for non-English texts? The framework relies entirely on PAN2014 for the discriminative function F; no multilingual plagiarism detector with equivalent granularity is currently integrated or validated.

### Open Question 4
To what extent do dataset characteristics (domain, vocabulary overlap, semantic density) influence which clients' suffixes are more susceptible to memorization? The observation that dataset characteristics play an important role was reported as a surprise finding; systematic investigation of how data properties affect memorization vulnerability across clients was not conducted.

## Limitations
- Framework relies on text similarity as a proxy for memorization, potentially underestimating semantic memorization
- 50-character threshold creates blind spot for short-output tasks like classification where memorization may occur but remains undetectable
- Pairwise assessment samples a small fraction of possible prefix-suffix pairs, potentially missing rare but significant memorization events

## Confidence
- **High Confidence**: Cross-client memorization is measurable and consistently lower than intra-client memorization; decoding strategy and prefix length significantly affect measured memorization rates; FedProx increases memorization compared to FedAvg
- **Medium Confidence**: The absence of a trade-off between centralized and federated memorization; model size and communication rounds don't affect memorization rates; no significant inter-client leakage beyond expected similarity
- **Low Confidence**: The framework's sensitivity to detect rare but severe privacy breaches; whether observed memorization patterns generalize to larger model scales or different federated algorithms beyond FedAvg/FedProx

## Next Checks
1. Test with variable local epochs: Replicate memorization measurements across different local epoch configurations (1, 5, 10) to verify the robustness of findings regarding FedAvg vs FedProx differences and the lack of model size effects.
2. Evaluate with semantic memorization probes: Supplement the text similarity approach with targeted semantic probes designed to elicit specific facts or concepts from each client's training data.
3. Stress-test the plagiarism detection pipeline: Systematically evaluate PAN2014's false positive and false negative rates on synthetic data with known memorization patterns, particularly focusing on cases with repetitive outputs or near-identical client data distributions.