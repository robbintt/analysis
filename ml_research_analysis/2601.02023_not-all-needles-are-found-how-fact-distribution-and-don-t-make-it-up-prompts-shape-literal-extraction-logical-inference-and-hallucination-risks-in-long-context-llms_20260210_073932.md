---
ver: rpa2
title: 'Not All Needles Are Found: How Fact Distribution and Don''t Make It Up Prompts
  Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context
  LLMs'
arxiv_id: '2601.02023'
source_url: https://arxiv.org/abs/2601.02023
tags:
- arxiv
- context
- performance
- preprint
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how fact distribution, context length, and
  anti-hallucination prompts affect literal extraction, logical inference, and hallucination
  risk in long-context LLMs. An extended needle-in-a-haystack benchmark with four
  production models (Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, Deepseek-v3.2-chat)
  tests performance across variable context lengths and nine probabilistic fact distributions,
  comparing standard and anti-hallucination prompts.
---

# Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs

## Quick Facts
- arXiv ID: 2601.02023
- Source URL: https://arxiv.org/abs/2601.02023
- Reference count: 40
- Key outcome: Longer contexts alone don't guarantee better performance; anti-hallucination prompts reduce fabrication but can trigger over-refusal, sharply lowering extraction and inference accuracy.

## Executive Summary
This study evaluates how fact distribution, context length, and anti-hallucination prompts affect literal extraction, logical inference, and hallucination risk in long-context LLMs. An extended needle-in-a-haystack benchmark with four production models (Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, Deepseek-v3.2-chat) tests performance across variable context lengths and nine probabilistic fact distributions, comparing standard and anti-hallucination prompts. Results show that longer contexts alone do not guarantee better performance; some models degrade sharply when relevant facts are diluted or clustered. Anti-hallucination prompts reduce hallucinations but can trigger over-refusal, sharply lowering extraction and inference accuracy—particularly for ChatGPT-5-mini. Models like Gemini-2.5-flash and Deepseek-v3.2-chat remain robust across distributions, while others exhibit significant positional bias or distributional collapse. Effective context length and distributional robustness are critical for reliable LLM deployment in enterprise settings.

## Method Summary
The study uses an extended needle-in-a-haystack (NIAH) benchmark to evaluate four frontier LLMs across three tasks: literal extraction, logical inference, and faithfulness assessment. Protocol A conducts a uniform sweep across 10 context lengths and 10 depths per model, generating 200 quizzes per model. Protocol B tests nine probabilistic fact distributions with 18 quizzes per model. The La Comédie Humaine corpus serves as the haystack, with synthetic facts injected as needles. Two prompt variants are tested: standard and anti-hallucination ("Don't Make It Up"). Responses are graded by an LLM judge using strict answer keys. The study quantifies safety tax as the performance delta between prompt types and examines positional bias through depth-based performance curves.

## Key Results
- Longer contexts alone do not guarantee better performance; some models degrade sharply when relevant facts are diluted or clustered.
- Anti-hallucination prompts reduce hallucinations but can trigger over-refusal, sharply lowering extraction and inference accuracy—particularly for ChatGPT-5-mini.
- Models like Gemini-2.5-flash and Deepseek-v3.2-chat remain robust across distributions, while others exhibit significant positional bias or distributional collapse.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Anti-hallucination prompts reduce fabrication but can trigger over-refusal, degrading extraction and inference accuracy.
- **Mechanism:** Explicit "Don't Make It Up" instructions raise the model's internal confidence threshold for output. Under long-context pressure, attention representations weaken, causing legitimate evidence to fall below this threshold, prompting refusal rather than retrieval.
- **Core assumption:** Over-refusal stems from safety calibration interacting with degraded context representations at scale.
- **Evidence anchors:**
  - [abstract] "Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference."
  - [Section 4.1] ChatGPT-5-mini's literal extraction drops from 96.4% (standard) to 72.0% (AH) at maximum capacity.
  - [corpus] Related work on over-refusal benchmarks (OR-Bench, ref [89]) confirms safety prompts demand strict retrieval confidence.
- **Break condition:** Models with robust attention mechanisms (Gemini, DeepSeek) show minimal safety tax; the mechanism breaks when context representations remain stable at scale.

### Mechanism 2
- **Claim:** Performance degrades when relevant facts are clustered rather than uniformly distributed, a failure mode termed "distributional collapse."
- **Mechanism:** Models trained on uniform retrieval tasks may misinterpret dense evidence clusters as noise or redundancy. Attention mechanisms optimized for dispersed retrieval fail to aggregate multiple proximate facts into coherent representations.
- **Core assumption:** Distributional sensitivity reflects training distribution mismatch, not fundamental architectural limits.
- **Evidence anchors:**
  - [Section 4.6] ChatGPT-5-mini scores 0% on Normal and Lorentzian distributions under AH prompts.
  - [Section C.3] Claude-4.5-haiku maintains 30-60% extraction but 0% inference on Uniform/Normal distributions—extraction-reasoning decoupling.
  - [corpus] NoLiMa (arXiv:2502.05167) similarly evaluates beyond literal matching, suggesting distributional robustness is an emerging concern.
- **Break condition:** Gemini-2.5-flash and Deepseek-v3.2-chat maintain >80-90% across all distributions; mechanism breaks with architectures that preserve semantic continuity regardless of clustering.

### Mechanism 3
- **Claim:** Positional bias ("lost-in-the-middle") persists in some models and intensifies under context saturation.
- **Mechanism:** U-shaped attention patterns favor primacy and recency. As context length increases, middle-position tokens receive diluted attention scores, causing retrieval failures for centrally-located information.
- **Core assumption:** Positional bias reflects architectural attention constraints, not training data alone.
- **Evidence anchors:**
  - [Section 4.3] Claude-4.5-haiku shows U-shaped curve; logical inference drops to ~50% at 20-60% depth.
  - [Section 4.3] ChatGPT-5-mini exhibits "performance cliff" at 50% depth mark.
  - [corpus] "Lost in the middle" phenomenon (Liu et al., 2024, ref [5]) is well-documented; this paper confirms persistence in newer models.
- **Break condition:** Models with adaptive attention (Gemini, DeepSeek) show near-uniform performance across depths.

## Foundational Learning

- **Concept:** Effective Context Length vs. Maximum Context Window
  - **Why needed here:** The paper distinguishes between what models *can* accept (max tokens) and what they *reliably utilize* for retrieval/inference. This is critical for interpreting benchmark results and deployment decisions.
  - **Quick check question:** If a model advertises 1M tokens but shows performance cliffs at 60% capacity, what is its *effective* context length for enterprise use?

- **Concept:** Literal Extraction vs. Logical Inference vs. Faithfulness
  - **Why needed here:** The paper evaluates three distinct capabilities that can diverge. A model may extract facts but fail inference, or achieve high faithfulness through over-refusal rather than genuine discrimination.
  - **Quick check question:** A model scores 100% faithfulness on AH prompts but 0% extraction—what failure mode does this indicate?

- **Concept:** Safety Tax
  - **Why needed here:** Quantifies the trade-off between hallucination reduction and accuracy loss. Critical for enterprise decisions on whether to deploy AH prompts.
  - **Quick check question:** If AH prompts improve faithfulness by 15% but reduce inference accuracy by 40%, is the trade-off acceptable for your use case?

## Architecture Onboarding

- **Component map:** Input context -> Attention mechanism -> Evidence aggregation -> Safety calibration -> Output layer
- **Critical path:** 1. Context enters at full length 2. Attention distributes across tokens (positional bias emerges here) 3. Evidence is aggregated (distributional sensitivity affects this) 4. Safety calibration gates output (AH prompts raise thresholds) 5. Response generated or refused
- **Design tradeoffs:** AH prompts reduce hallucinations vs. risk over-refusal; model selection between robust but task-different models vs. strong in other areas but weaker long-context; context pre-processing to avoid clustering vs. accept raw inputs
- **Failure signatures:** Over-refusal (high faithfulness + low extraction/inference); distributional collapse (near-zero performance on clustered distributions); U-shaped degradation (strong start/end, weak middle); performance cliff (sharp drop at specific context percentage)
- **First 3 experiments:**
  1. Baseline sweep: Run Protocol A on your target model at 10%–100% capacity to identify performance cliffs and positional bias before deployment.
  2. AH prompt calibration: Compare standard vs. AH prompts at 80% and 100% capacity; quantify safety tax as (standard accuracy – AH accuracy) for your domain-specific queries.
  3. Distribution stress test: Run Protocol B on your actual document corpus structure rather than synthetic uniform distributions; if your documents cluster evidence centrally, prioritize distributionally-robust models.

## Open Questions the Paper Calls Out

- **Question:** How do long-context LLMs compare directly against mature RAG systems on literal extraction, logical inference, and faithfulness metrics when controlling for cost and latency?
  - **Basis in paper:** [explicit] The Discussion states: "future benchmarking should directly compare these results, along with cost and response time, against mature RAG solutions."
  - **Why unresolved:** This study deliberately did not compare RAG and CAG, only suggesting that failures stem from ineffective context utilization.
  - **What evidence would resolve it:** A controlled benchmark evaluating the same models with identical queries, varying only the retrieval strategy (full-context vs. RAG pipeline).

- **Question:** Do the observed positional biases and distributional collapse phenomena generalize to specialized domains such as healthcare, legal documentation, or software engineering?
  - **Basis in paper:** [explicit] The Limitations section states: "LLM behavior may shift significantly in specialized domains such as healthcare, legal documentation, or software engineering, where structural and linguistic patterns differ."
  - **Why unresolved:** The study used only a narrative literary corpus (Balzac), which differs structurally from technical or specialized documents.
  - **What evidence would resolve it:** Replicating the extended NIAH framework on domain-specific corpora with equivalent context lengths and fact distributions.

- **Question:** What architectural or attention mechanisms cause some models (e.g., Gemini, DeepSeek) to maintain spatial invariance while others exhibit distributional collapse under clustered fact distributions?
  - **Basis in paper:** [inferred] The paper documents sharp divergence in robustness but does not investigate internal causes.
  - **Why unresolved:** The methodology evaluates outputs, not internal representations or attention patterns.
  - **What evidence would resolve it:** Attention visualization or probing studies comparing models under different fact distribution regimes.

## Limitations

- The study uses synthetic probe injection into a single literary corpus, raising questions about external validity for enterprise documents.
- The "story-congruent" fact injection method is underspecified, potentially affecting ecological validity.
- The LLM judge grading system may inherit biases from its own training distribution.

## Confidence

**High confidence:** Positional bias findings and general over-refusal patterns under AH prompts are well-supported by systematic sweeps across models and depths.

**Medium confidence:** Distributional robustness findings show clear patterns but may be sensitive to the specific injection method and corpus structure.

**Low confidence:** External validity for enterprise document types beyond the literary corpus used.

## Next Checks

1. Cross-corpus validation: Replicate the benchmark using enterprise document types (legal contracts, technical manuals, financial reports) to verify distributional sensitivity patterns hold across document structures.

2. AH prompt optimization: Systematically vary AH prompt formulations to identify configurations that minimize safety tax while maintaining hallucination reduction, particularly for over-refusing models like ChatGPT-5-mini.

3. Effective context length mapping: For your specific document types and query patterns, empirically determine each model's effective context length by identifying the point where performance plateaus or declines, then size contexts to stay within this window rather than using maximum capacity.