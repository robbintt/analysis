---
ver: rpa2
title: Shapley Uncertainty in Natural Language Generation
arxiv_id: '2507.21406'
source_url: https://arxiv.org/abs/2507.21406
tags:
- uncertainty
- arxiv
- shapley
- correlation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Shapley uncertainty, a novel uncertainty metric
  for natural language generation that leverages inter-sentence correlations to improve
  uncertainty estimation. Unlike existing approaches that cluster semantically equivalent
  sentences with fixed thresholds, this method employs a Shapley-based framework that
  incorporates a continuous correlation matrix derived from bidirectional entailment
  probabilities between sentences.
---

# Shapley Uncertainty in Natural Language Generation

## Quick Facts
- arXiv ID: 2507.21406
- Source URL: https://arxiv.org/abs/2507.21406
- Reference count: 29
- Key outcome: Introduces Shapley uncertainty, achieving 2.4% AUROC improvement over semantic entropy and 6.2% over naive entropy across multiple QA datasets and model sizes

## Executive Summary
This work addresses uncertainty estimation in natural language generation by introducing Shapley uncertainty, a novel metric that leverages inter-sentence correlations to improve uncertainty estimation. Unlike existing approaches that cluster semantically equivalent sentences with fixed thresholds, this method employs a Shapley-based framework that incorporates a continuous correlation matrix derived from bidirectional entailment probabilities between sentences. The proposed metric satisfies three essential properties—minimal uncertainty, maximal uncertainty, and consistency—that ensure its validity as an uncertainty measure. Experimental results across multiple question-answering datasets and language models demonstrate that Shapley uncertainty achieves superior AUROC scores compared to baseline methods, with an average improvement of 2.4% over semantic entropy and 6.2% over naive entropy across tasks.

## Method Summary
The method samples n answers per input using an LLM, then computes bidirectional entailment probabilities P(si ⇒ sj|x) and P(sj ⇒ si|x) for all sentence pairs using a DeBERTa NLI model. These are combined into a continuous correlation matrix C(si, sj|x) = 0.5·P(si ⇒ sj|x) + 0.5·P(sj ⇒ si|x), which is transformed via a Gaussian kernel to ensure positive semi-definiteness. Shapley uncertainty is then computed by decomposing the multivariate differential entropy into weighted marginal contributions across all subsets of sentences, summing to produce the final uncertainty score. The method is evaluated on six QA datasets using seven different LLM architectures, with correctness thresholds based on RougeL and BLEU scores.

## Key Results
- Shapley uncertainty achieves 2.4% average AUROC improvement over semantic entropy baseline across CoQA, TriviaQA, SQuAD, BioASQ, NQ-Open, and SVAMP datasets
- Method shows particularly strong performance with smaller models (1.3B-7B parameters), improving AUROC by 6.2% over naive entropy baseline
- Correlation matrix effectively captures semantic relationships, as demonstrated by higher correlations between semantically related entities (e.g., "Mozart" vs "Beethoven" vs "da Vinci")

## Why This Works (Mechanism)

### Mechanism 1
Continuous correlation matrices capture semantic relationships more precisely than binary threshold clustering. Replace hard threshold-based equivalence (P(si ⇒ sj|x) ≥ t) with bidirectional entailment probability C(si, sj|x) = ½P(si ⇒ sj|x) + ½P(sj ⇒ si|x), preserving degree of semantic relatedness. Core assumption: Bidirectional entailment probabilities from NLI models accurately reflect semantic similarity. Evidence: Tables 1-2 show "Beethoven" receives higher correlation with "Mozart" than "da Vinci", yielding uncertainty 0.40 vs 0.51. Break condition: If entailment model fails on domain-specific language or adversarial paraphrases, correlation estimates degrade.

### Mechanism 2
Kernel transformation ensures mathematical validity of correlation matrices for downstream entropy computation. Apply K(C(si, sj)) = β · κ(1 - C(si, sj)) with Gaussian kernel; Proposition 3.1 proves existence of β ensuring positive semi-definiteness via Gershgorin circle theorem. Core assumption: Mercer's condition is necessary for valid entropy calculation; violations indicate incoherent correlation structure. Evidence: Section 3.2 proves correlation matrix generated by (5) can be proved to be positive semi-definite. Break condition: If β requires very small values (β < 1/(n+1) for n sentences), kernel may over-flatten meaningful differences.

### Mechanism 3
Shapley value decomposition prevents differential entropy's pathologies in high-dimensional correlated spaces. Instead of h(s̃) depending on log-det(R) which → -∞ when dimensions correlate highly, compute ϕ(si|s̃) as weighted marginal entropy contributions across all subsets (Eq. 7), then sum. Core assumption: Uncertainty should satisfy three axioms (minimal, maximal, consistency); differential entropy violates first two. Evidence: Section 3.4 shows differential entropy over the whole multivariate distribution does not satisfy Properties 3.2 and 3.3; Proposition 3.6 proves Shapley uncertainty satisfies all three properties. Break condition: Computational cost scales combinatorially with sentence count; approximations may violate axiom satisfaction.

## Foundational Learning

- **Predictive entropy in NLG**: Baseline that treats each sampled sentence independently; Shapley improves on this by incorporating correlations. Quick check: Given 3 samples with probabilities [0.5, 0.4, 0.1], compute naive entropy. (Answer: -Σp·log(p) ≈ 0.94 nats)

- **Shapley values from cooperative game theory**: Core mathematical tool for fair attribution; equation (7) adapts this to uncertainty decomposition. Quick check: If player A contributes marginal value 0.2 when added to any coalition, what's their Shapley value? (Answer: 0.2, by symmetry/additivity axioms)

- **Mercer's condition and positive semi-definite kernels**: Ensures correlation matrix represents valid covariance structure; violated matrices produce incoherent entropy values. Quick check: For matrix [[1, 0.9], [0.9, 1]], check if eigenvalues are non-negative. (Answer: λ = 1.9, 0.1 → PSD ✓)

## Architecture Onboarding

- **Component map**: Sample generator -> Entailment scorer -> Correlation builder -> Shapley computer -> Aggregator
- **Critical path**: Entailment scoring (O(n²) NLI calls) → Shapley computation (O(2ⁿ) subset evaluations, typically approximated). Bottleneck: For n=10 samples, 45 NLI pairs + 1023 subset evaluations
- **Design tradeoffs**: Sample count n (more samples → better uncertainty estimate but O(n²) NLI cost and O(2ⁿ) Shapley cost); Kernel β (smaller β → guaranteed PSD but may over-flatten correlations); Approximation (can Monte Carlo sample subsets for Shapley)
- **Failure signatures**: Constant uncertainty across inputs (entailment model may output uniform probabilities); Negative or extreme values (matrix not PSD → check kernel β adjustment); High variance on repeated queries (too few samples or unstable LLM generations)
- **First 3 experiments**:
  1. Reproduce Tables 1-2 manually: For question "Who wrote Queen of the Night aria?", sample 10 outputs, compute correlation matrix, verify Shapley uncertainty distinguishes (Mozart, Beethoven) from (Mozart, da Vinci)
  2. Ablate kernel β: Test β ∈ {0.1, 0.3, 0.5, 0.7, 1.0} on CoQA subset; plot AUROC vs β to validate 0.5 default
  3. Scale test: Measure wall-clock time for n ∈ {5, 10, 20} samples; identify if Shapley subset enumeration or NLI calls dominate

## Open Questions the Paper Calls Out

- **Does Shapley uncertainty maintain its superior predictive performance when applied to large language models exceeding 30 billion parameters?** The Limitations section explicitly states that computational constraints restricted the analysis to models under 30 billion parameters, leaving the behavior of larger LLMs unexplored. Evidence to resolve: AUROC benchmarks on 70B+ parameter models (e.g., LLaMA-3-70B) comparing Shapley uncertainty against semantic entropy and naive entropy baselines.

- **Can the correlation measure be theoretically extended to satisfy Mercer's condition for continuous or infinite semantic spaces?** The authors note that the kernel function's ability to generate positive semi-definite correlation matrices is currently limited to finite sentence sets. Evidence to resolve: A theoretical extension of Proposition 3.1 that proves validity for infinite sets, or a reformulation of the correlation matrix that intrinsically satisfies Mercer's condition without finite constraints.

- **How does the method perform on open-ended generation tasks with less defined ground truths, such as abstractive summarization?** While the introduction mentions summarization as a key NLG task, the experiments are restricted to QA and machine translation which utilize specific correctness thresholds. Evidence to resolve: Experiments on summarization datasets (e.g., CNN/DailyMail) correlating Shapley uncertainty with human evaluations of factuality and consistency.

## Limitations
- Computational constraints limited analysis to models under 30 billion parameters, leaving scalability to larger models unexplored
- Kernel parameter β=0.5 is chosen empirically without showing sensitivity across different sentence count regimes
- Dependence on NLI model quality for bidirectional entailment probabilities creates potential domain-specific failure modes

## Confidence
- **AUROC improvement claims (2.4% over semantic entropy, 6.2% over naive entropy)**: High confidence - results are reported across multiple datasets and models with consistent improvements
- **Three essential properties satisfaction**: High confidence - mathematical proofs are provided in the appendices
- **Continuous correlation matrix superiority**: Medium confidence - qualitative examples support the claim but systematic ablation against alternative continuous measures is absent
- **Generalization across model sizes**: Medium confidence - strong results shown for smaller models but limited testing on the largest frontier models

## Next Checks
1. **Ablation study on NLI model quality**: Replace DeBERTa with a smaller NLI model and a domain-specific model (e.g., biomedical NLI for BioASQ) to quantify sensitivity to entailment estimation quality
2. **Kernel parameter sweep**: Systematically vary β across different sentence counts (n=5, 10, 15, 20) and plot AUROC to validate the chosen default and identify optimal ranges
3. **Computational scaling analysis**: Implement Monte Carlo approximation for Shapley values and measure runtime versus exact computation for n=10, 15, 20, 25 to establish practical limits and verify approximation doesn't violate the three properties