---
ver: rpa2
title: 'RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question
  Answering'
arxiv_id: '2505.21940'
source_url: https://arxiv.org/abs/2505.21940
tags:
- question
- reasoning
- answer
- rise
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RISE tackles multi-hop question answering by integrating retrieval-augmented
  generation with iterative self-exploration. It repeatedly decomposes questions into
  sub-questions, retrieves evidence, and critiques the reasoning path to correct errors
  and refine answers.
---

# RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering

## Quick Facts
- **arXiv ID:** 2505.21940
- **Source URL:** https://arxiv.org/abs/2505.21940
- **Reference count:** 34
- **Primary result:** RISE achieves consistent accuracy gains across iterations through iterative self-exploration in multi-hop question answering

## Executive Summary
RISE tackles multi-hop question answering by integrating retrieval-augmented generation with iterative self-exploration. It repeatedly decomposes questions into sub-questions, retrieves evidence, and critiques the reasoning path to correct errors and refine answers. Through multi-objective training across question decomposition, retrieval-then-read, and self-critique tasks, RISE continuously improves reasoning accuracy without manual supervision. Experiments on multiple benchmarks show consistent accuracy gains across iterations, with notable improvements in complex reasoning tasks, demonstrating both robustness and efficiency compared to existing methods.

## Method Summary
RISE employs a self-exploration mechanism that iteratively decomposes questions into sub-questions, retrieves relevant evidence, and critiques the reasoning path. The framework generates three training datasets from model-generated reasoning traces and jointly trains across decomposition, retrieval-then-read, and self-critique tasks. This process repeats across multiple iterations, with each iteration using the improved model to generate more refined training data. The approach eliminates manual annotation by using the model's own exploration traces as training signals.

## Key Results
- RISE demonstrates consistent accuracy improvements across 4 iterations on multiple benchmarks
- The method shows particular strength in complex reasoning tasks requiring multi-hop evidence integration
- Joint multi-objective training outperforms separate training across all tested datasets
- RISE achieves robustness and efficiency gains compared to existing multi-hop QA methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured self-exploration enables models to autonomously discover valid reasoning paths for complex multi-hop questions.
- Mechanism: The model iteratively performs three actions—question decomposition, retrieve-then-read, and self-critique—accumulating historical information H at each node. Valid nodes (σ=1) extend the reasoning chain; invalid nodes trigger backtracking. This process generates three training datasets (Dd, Dr, Dc) from the model's own exploration traces.
- Core assumption: The base model has sufficient initial capability to generate at least some correct reasoning paths that can be amplified through training.
- Evidence anchors:
  - [abstract] "RISE involves three key steps... question decomposition, retrieve-then-read, and self-critique"
  - [section 2.2] "The self-exploration mechanism enables the model to address complex problems through iterative reasoning, comprising three core actions"
  - [corpus] Related work "Self-Critique Guided Iterative Reasoning" confirms self-critique as an emerging strategy for multi-hop QA
- Break condition: Fails if initial model produces consistently invalid reasoning paths; maximum node limit (Nmax=20) terminates unproductive exploration.

### Mechanism 2
- Claim: Multi-objective joint training creates synergistic improvements across decomposition, reading, and critique capabilities.
- Mechanism: Three loss functions (Ld, Lr, Lc) are combined with weights α, β, γ into a unified objective L = αLd + βLr + γLc. Joint training allows gradient signals from one task to inform others—for example, better decomposition should improve retrieve-then-read inputs.
- Core assumption: The three tasks share underlying reasoning representations that benefit from shared parameter updates.
- Evidence anchors:
  - [section 2.3] "We believe that joint training facilitates complementary learning and enhances model capabilities"
  - [table 4] Ablation shows separate training underperforms joint training across all datasets (e.g., 2Wiki: 40.86% vs 41.13%)
  - [corpus] No direct corpus evidence for this specific multi-objective approach in MHQA
- Break condition: Performance plateaus or degrades if task weights are poorly balanced; uniform weighting (α=β=γ=1) used to avoid overfitting.

### Mechanism 3
- Claim: Iterative self-improvement creates a positive feedback loop where better models generate higher-quality training data.
- Mechanism: After training, model Mi+1 generates new question set Qi+1 from previous seeds via in-context learning. This expanded dataset feeds the next exploration round, progressively improving reasoning quality. Figure 3 shows accuracy gains across 4 iterations.
- Core assumption: Model improvements generalize to generate better training data for subsequent iterations.
- Evidence anchors:
  - [figure 3a] Accuracy per iteration shows consistent upward trend across 2Wiki, Hotpot, and MSQ datasets
  - [section 4.1] "The results demonstrate a consistent upward trend in accuracy with each iteration"
  - [corpus] Self-improvement paradigm validated in related work (Self-Rewarding, Self-Refine)
- Break condition: Diminishing returns if generated questions become too similar to training distribution; potential overfitting to self-generated patterns.

## Foundational Learning

- Concept: **Multi-hop reasoning chains**
  - Why needed here: RISE explicitly decomposes questions requiring multiple evidence integration steps; understanding that "Which film was released more recently, The Book of Eli or Fire Birds?" requires two separate retrieval+reasoning steps.
  - Quick check question: Can you trace how the model would handle "Who was the advisor of the PhD advisor of Geoffrey Hinton?"

- Concept: **Self-supervised data generation**
  - Why needed here: RISE eliminates manual annotation by using model-generated reasoning traces as training data; this requires understanding how pseudo-labels can bootstrap learning.
  - Quick check question: What potential biases might accumulate when a model trains on its own outputs across multiple iterations?

- Concept: **Retrieve-then-read with noisy evidence**
  - Why needed here: RAG introduces retrieval noise; RISE's self-critique mechanism is designed to filter irrelevant retrieved documents before they corrupt reasoning chains.
  - Quick check question: How would you distinguish between evidence that is retrieved but irrelevant versus evidence that is relevant but incorrectly interpreted?

## Architecture Onboarding

- **Component map:**
  Seed Questions (Q₀) → Self-Exploration Module → Historical Experience (H) → Multi-Objective Training → Updated Model (Mᵢ₊₁) → Question Expansion → Qᵢ₊₁

- **Critical path:** Self-exploration loop (lines 5-14 in Algorithm 1). This is where reasoning paths are constructed and validated. A bug here cascades to all downstream training.

- **Design tradeoffs:**
  - Weight tuning (α, β, γ): Paper uses uniform weights to avoid manual overfitting, but Table 1 suggests optimal ratios vary by dataset
  - Iteration count: More iterations improve accuracy (Figure 3a) but increase computational cost
  - Maximum nodes (Nmax=20): Prevents infinite loops but may truncate valid long-chain reasoning

- **Failure signatures:**
  - Reasoning decomposition errors: Sub-questions misaligned with original intent (Figure 1, red path asking production year instead of release year)
  - Evidence aggregation errors: Multiple evidences incorrectly synthesized (Figure 1, blue path confusing Blu-ray release with theatrical release)
  - Self-critique over-rejection: Excessive σ=0 judgments preventing valid exploration paths
  - Token budget exhaustion: Multi-step reasoning requires 2881 avg input tokens (Table 8)

- **First 3 experiments:**
  1. Validate self-exploration on small subset: Run Algorithm 1 on 50 questions with Nmax=10, manually inspect if generated sub-questions are semantically valid and retrieval returns relevant evidence
  2. Ablate self-critique: Compare full RISE vs. version with σ always set to 1 (no filtering); expect degradation on noisy datasets like HotpotQA
  3. Single vs. multi-iteration comparison: Train RISE with 1 iteration vs. 4 iterations on held-out validation set; verify Figure 3's accuracy trajectory reproduces

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends critically on initial model's reasoning capabilities with unclear failure bounds
- Uniform weighting strategy may not represent optimal task balancing despite avoiding overfitting
- Substantial computational overhead increases with each iteration due to processing historical traces

## Confidence

**High Confidence:** The core mechanism of iterative self-exploration with three distinct actions (decomposition, retrieve-then-read, self-critique) is well-supported by experimental results across multiple benchmarks. The consistent accuracy improvements across iterations (Figure 3) provide strong empirical validation.

**Medium Confidence:** The multi-objective joint training benefits are demonstrated through ablation studies, but the evidence is primarily internal to the RISE framework. The claim that joint training facilitates "complementary learning" between tasks is plausible but not extensively validated against alternative optimization strategies.

**Low Confidence:** The generalizability of self-improvement across iterations relies heavily on in-paper results. The framework assumes that model improvements will continue to generate better training data, but this feedback loop could potentially amplify biases or lead to overfitting to self-generated patterns without external validation.

## Next Checks

1. **Long-chain reasoning stress test:** Evaluate RISE on questions requiring 5+ reasoning steps (e.g., "Who was the advisor of the PhD advisor of the advisor of Geoffrey Hinton?") to determine if Nmax=20 constrains valid reasoning chains.

2. **Bias propagation analysis:** Track semantic drift across iterations by comparing generated question distributions Q₀ through Q₄ using embedding similarity metrics. This would reveal whether the model progressively diverges from the original seed distribution.

3. **External knowledge transfer:** Test RISE-trained models on entirely different reasoning domains (e.g., mathematical word problems or logical puzzles) to assess whether improvements generalize beyond the MHQA training distribution.