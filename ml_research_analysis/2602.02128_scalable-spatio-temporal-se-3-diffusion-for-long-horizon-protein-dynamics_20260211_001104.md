---
ver: rpa2
title: Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics
arxiv_id: '2602.02128'
source_url: https://arxiv.org/abs/2602.02128
tags:
- star-md
- protein
- attention
- frames
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STAR-MD is an SE(3)-equivariant diffusion model that generates
  physically plausible protein trajectories over microsecond timescales. It uses a
  causal diffusion transformer with joint spatio-temporal attention to capture complex
  space-time dependencies while avoiding memory bottlenecks of existing methods.
---

# Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics

## Quick Facts
- arXiv ID: 2602.02128
- Source URL: https://arxiv.org/abs/2602.02128
- Reference count: 40
- STAR-MD achieves state-of-the-art performance on the ATLAS benchmark, substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods.

## Executive Summary
STAR-MD is an SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. It uses a causal diffusion transformer with joint spatio-temporal attention to capture complex space-time dependencies while avoiding memory bottlenecks of existing methods. On the ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics, substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. The model successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout extended rollouts.

## Method Summary
STAR-MD uses a causal diffusion transformer with joint spatio-temporal (S×T) attention operating on single-residue tokens across multiple frames. The model employs SE(3) equivariance with separate noise schedules for translations (Gaussian) and rotations (IGSO3). A key innovation is contextual noise perturbation applied to historical frames during both training and inference, preventing error accumulation in long-horizon autoregressive generation. The architecture uses a KV cache storing only single-residue features (O(NL) memory) rather than explicit pair features (O(N²L) memory), enabling scaling to longer trajectories. Training uses block-causal attention for parallel optimization, while inference generates trajectories autoregressively with 200 diffusion steps per frame.

## Key Results
- Achieves state-of-the-art performance on ATLAS benchmark with JSD=0.42, Recall=0.57, and CA+AA validity=85.18% at 100ns
- Successfully generates stable microsecond-scale trajectories while baseline methods fail catastrophically
- Maintains high structural validity (>85%) throughout extended rollouts where others decay sharply
- Reduces KV cache memory by ~196× compared to pair-caching baselines

## Why This Works (Mechanism)

### Mechanism 1: Joint Spatio-Temporal Attention Captures Non-Separable Dependencies
Joint S×T attention on single-residue tokens captures coupled space-time dependencies that factorized "space-then-time" attention cannot. Each residue-frame pair (i, ℓ) becomes a token, allowing the model to learn how motion at residue i depends on past motion at residue j—couplings that cannot be factored into separate spatial and temporal operations. This follows from the Mori-Zwanzig formalism where projecting out explicit pair features creates spatio-temporal coupling in the residual dynamics.

### Mechanism 2: Contextual Noise Perturbation Prevents Error Accumulation
Adding small noise to historical context frames during both training and inference stabilizes long-horizon autoregressive generation. During training, history frames are corrupted via forward diffusion; the model learns to denoise frame ℓ conditioned on this perturbed history. At inference, the same noise level is applied to each generated frame before it becomes context, preventing the model from encountering out-of-distribution contexts caused by its own accumulated prediction errors.

### Mechanism 3: Implicit Pair Encoding via Memory-Enriched Temporal History
Removing explicit pair features from the KV cache enables long-horizon scaling; joint attention over singles implicitly recovers pair information through temporal evolution. By avoiding pair-feature caching (O(N²L) memory → O(NL) memory), STAR-MD can maintain full history for large proteins. The trade-off between spatial complexity and temporal complexity is favorable when N ≫ L—the typical protein regime where residues (100s-1000s) exceed context frames (10s-100s).

## Foundational Learning

- **SE(3) Equivariance and Riemannian Diffusion**: Proteins exist in 3D space requiring different treatment of translations and rotations. Standard Gaussian diffusion doesn't preserve rotational symmetry; SE(3) requires translation (Gaussian) and rotation (IGSO3) noise schedules. *Quick check*: Can you explain why adding Gaussian noise to rotation matrices doesn't yield valid rotation matrices?

- **Mori-Zwanzig Formalism for Coarse-Graining**: When projecting full-atom dynamics to per-residue coordinates, the resulting dynamics are no longer Markovian—they require a memory kernel. This explains why history matters and why non-separable spatio-temporal coupling emerges. *Quick check*: What are the three terms in the Generalized Langevin Equation after Mori-Zwanzig projection?

- **KV Caching in Autoregressive Transformers**: Long-horizon generation requires efficient access to all prior frames. KV caching stores keys and values from previous attention operations, avoiding recomputation. The memory cost depends on what you cache—singles vs. pairs is the critical design choice here. *Quick check*: For N=500 residues, L=400 frames, d=256 hidden dim, what's the KV cache size for singles-only vs. singles+pairs?

## Architecture Onboarding

- **Component map**: Input sequence + starting conformation → Frozen OpenFold → single/pair features → FrameEncoder → Diffusion Block (IPA + S×T + EdgeTransition) → Score prediction → Denoising → Clean frame → Autoregressive loop

- **Critical path**: Training uses block-causal attention for parallel optimization over all frames. Contextual noise injection (τ ~ U[0, 0.1]) applied to history during both training and inference. Inference uses autoregressive rollout with KV cache storing only single-residue features.

- **Design tradeoffs**:
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Joint S×T vs. factorized | Captures non-separable coupling | O(N²L²) compute |
  | Singles-only KV cache | O(NL) memory, enables long rollouts | Requires richer temporal modeling |
  | Contextual noise | Prevents error accumulation | Requires tuning τ range |
  | Block-causal training | Parallel optimization | 2× sequence length |

- **Failure signatures**: Rapid validity decay over time indicates contextual noise not enabled at inference. Out-of-memory on large proteins suggests KV cache storing pairs instead of singles. Low conformational coverage with valid structures suggests S×T attention not inside diffusion blocks.

- **First 3 experiments**: 1) Replicate 100ns benchmark to verify CA+AA validity > 80% and recall > 0.50. 2) Ablation: remove contextual noise to observe validity decay matching Figure 12. 3) Memory scaling test to confirm O(NL) scaling (N=500, L=400: ~200MB vs. ~40GB for pair-caching).

## Open Questions the Paper Calls Out
- Can the architecture be extended to simulate dynamics of protein complexes or their interactions with small molecules?
- Would training on larger, more diverse datasets like MDCATH yield significant improvements in generalization?
- What modifications are required to close the remaining gap in temporal consistency with oracle MD simulations?
- Can a trajectory generation model match the equilibrium distribution accuracy of specialized ensemble emulators?

## Limitations
- Theoretical claims about non-separable memory kernels rely on asymptotic results not empirically validated on actual protein trajectories
- Contextual noise perturbation mechanism uses heuristic noise schedule without theoretical justification
- Scalability claims depend on implicit pair encoding but theoretical guarantee not empirically validated

## Confidence
- **High**: STAR-MD achieves state-of-the-art performance on ATLAS benchmarks with robust results across multiple metrics
- **Medium**: Joint spatio-temporal attention architecture is necessary but theoretical justification less certain
- **Medium**: Contextual noise perturbation prevents error accumulation but specific schedule appears heuristic

## Next Checks
1. Generate synthetic protein-like trajectories with known non-separable spatio-temporal coupling and verify whether learned dynamics match ground truth memory kernel structure
2. Systematically vary contextual noise level τ and evaluate trade-off between error prevention and generation quality across different protein families
3. Empirically measure memory usage and generation speed for range of protein sizes (N=50, 200, 500) and trajectory lengths (L=10, 50, 200) to confirm O(NL) scaling advantage