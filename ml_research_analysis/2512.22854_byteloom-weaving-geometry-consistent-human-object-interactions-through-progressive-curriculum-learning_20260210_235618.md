---
ver: rpa2
title: 'ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive
  Curriculum Learning'
arxiv_id: '2512.22854'
source_url: https://arxiv.org/abs/2512.22854
tags:
- object
- human
- video
- pose
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ByteLoom addresses the challenge of generating human-object interaction
  videos with geometric consistency and multi-view object control. The method introduces
  two key innovations: a Relative Coordinate Maps (RCM)-cache mechanism for maintaining
  object geometry consistency across views, and a progressive three-stage curriculum
  learning approach that leverages different data sources (human pose, hand-object
  interaction, and full HOI) to improve model capabilities.'
---

# ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning

## Quick Facts
- arXiv ID: 2512.22854
- Source URL: https://arxiv.org/abs/2512.22854
- Reference count: 40
- Primary result: Achieves state-of-the-art object fidelity (Obj-IoU: 0.8288) and geometry consistency (T-SSIM: 0.5682) for HOI video generation

## Executive Summary
ByteLoom addresses the challenge of generating human-object interaction videos with geometric consistency and multi-view object control. The method introduces two key innovations: a Relative Coordinate Maps (RCM)-cache mechanism for maintaining object geometry consistency across views, and a progressive three-stage curriculum learning approach that leverages different data sources (human pose, hand-object interaction, and full HOI) to improve model capabilities. Extensive experiments on the newly proposed Mani4D benchmark demonstrate that ByteLoom achieves state-of-the-art performance in object fidelity, human similarity, and geometry consistency, while maintaining smooth motion and reducing dependency on fine-grained hand mesh annotations.

## Method Summary
ByteLoom builds on a Wan2.1 DiT backbone with MLP latent fusion for condition injection. The method uses Relative Coordinate Maps (RCM) as a universal representation to maintain object geometry consistency across views. An RCM-cache stores multi-view RCM-RGB pairs as appearance priors. Per-frame RCM inputs specify target 6-DoF pose, and the model learns to retrieve and texture correctly from the cache. The training curriculum progresses through three stages: Stage I trains pose-following on large human motion datasets, Stage II adds hand-object interaction reasoning using DexYCB/HO3D/ARCTIC, and Stage III refines on curated full-body HOI data (Mani4D-Train).

## Key Results
- Achieves Obj-IoU of 0.8288, outperforming baselines by significant margins
- Reaches Face-Cos-Similarity of 0.8891 for human appearance consistency
- Demonstrates geometry consistency with T-SSIM of 0.5682 across frames
- Reduces LMD (Local Motion Distortion) to 0.1427 through curriculum learning
- Successfully generates realistic HOI videos with consistent object appearance across multiple viewpoints

## Why This Works (Mechanism)

### Mechanism 1: RCM-Cache for Cross-View Geometry Consistency
- Claim: Relative Coordinate Maps (RCM) provide explicit 3D spatial correspondence that enables geometry-consistent object rendering across viewpoints.
- Mechanism: RCM encodes each surface point's normalized position within the object's bounding box into RGB color space. Multi-view RCM-RGB pairs are cached as a persistent appearance prior. Per-frame RCM inputs specify target 6-DoF pose, and the model learns to retrieve and texture correctly from the cache.
- Core assumption: The self-attention mechanism can learn the implicit mapping between per-frame RCM pose signals and cached multi-view appearance information.
- Evidence anchors: [abstract] states RCM maintains geometry consistency and controls 6-DoF transformations. [Section 3.2] explains RCM ensures exact spatial coordinates. Weak corpus evidence - related papers focus on HOI detection/synthesis but don't discuss RCM-based geometry mechanisms.
- Break condition: When cached views have insufficient coverage of the object's surface, or RCM rendering contains artifacts from mesh quality issues.

### Mechanism 2: Three-Stage Curriculum for Data Scarcity Compensation
- Claim: Progressive training from abundant pose data → hand-object data → scarce full HOI data enables capability building without requiring fine-grained hand mesh annotations.
- Mechanism: Stage I trains pose-following on large human motion datasets. Stage II adds hand-object interaction reasoning (contact, grasp, occlusion) using DexYCB/HO3D/ARCTIC. Stage III refines on curated full-body HOI data (Mani4D-Train).
- Core assumption: Foundational motion understanding transfers from pose-only data to interaction tasks, and hand-object datasets provide sufficient occlusion reasoning skills.
- Evidence anchors: [abstract] mentions curriculum enhances capabilities and relaxes hand mesh demand. [Section 4.5, Table 2] shows I+II+III achieves Obj-IoU 0.8288 vs I+III at 0.7627. LMD improves from 0.2054 to 0.1427 with Stage II. [corpus] GenHOI paper validates data scarcity challenge.
- Break condition: When hand-object dataset distribution diverges significantly from target HOI domain (noted in Section 4.5: synthetic NVS task showed "marginal" gains due to distribution mismatch).

### Mechanism 3: T-SSIM Metric for Geometry Consistency Quantification
- Claim: Aggregating multi-frame 3D reconstructions and measuring reprojection error provides a ground-truth-free metric for geometry consistency.
- Mechanism: Segment object with SAM2 → reconstruct 3D with foundation model (e.g., VGGT) → aggregate point cloud → reproject to each frame → compute temporal average SSIM between original and reprojected crops.
- Core assumption: Vision foundation models produce sufficiently accurate 3D reconstructions that reprojection error reflects generation quality rather than reconstruction noise.
- Evidence anchors: [Section 3.4] describes T-SSIM through photometric error comparison. [Section 4.2] reports T-SSIM results (Ours: 0.5682 vs baselines 0.5279–0.5870). [corpus] No direct corpus evidence for this specific metric approach.
- Break condition: When foundation model reconstruction fails on cropped object segments (e.g., heavy occlusion, motion blur).

## Foundational Learning

- Concept: Diffusion Transformer (DiT) architecture
  - Why needed here: ByteLoom builds on Wan2.1 DiT backbone; understanding attention-based denoising is essential for modifying conditioning mechanisms.
  - Quick check question: Can you explain how self-attention in a DiT differs from UNet-based diffusion for handling variable-length conditioning?

- Concept: 6-DoF transformations and coordinate systems
  - Why needed here: Object control requires understanding rotation/translation in 3D space and how RCM encodes normalized coordinates.
  - Quick check question: Given an object bounding box [b_min, b_max] and vertex V, can you compute the RCM encoding?

- Concept: Curriculum learning principles
  - Why needed here: Training strategy requires understanding why progressive complexity helps when data availability varies inversely with task difficulty.
  - Quick check question: What would happen if you trained Stage III before Stage II, given dataset size constraints?

## Architecture Onboarding

- Component map: VAE Encoder → DiT Backbone → MLP Latent Fusion → VAE Decoder

- Critical path:
  1. Prepare RCM-cache from object mesh (render N sparse views with RCM encoding)
  2. Render per-frame RCM using 6-DoF trajectory
  3. Concatenate conditions: human ref + pose + RCM-cache (frame dim) + per-frame RCM (channel dim)
  4. Latent fusion via MLP
  5. DiT denoising → VAE decode to video

- Design tradeoffs:
  - RCM-cache size vs. memory: More views improve coverage but increase latency
  - Curriculum stage duration: Table 1 shows Stage I (2000 steps) >> Stage III (400 steps)
  - Conditioning granularity: Paper chose OpenPose skeleton over hand mesh to relax annotation requirements (Section 1)

- Failure signatures:
  - Geometry collapse: Object shape distorts across frames (Figure 3) → check RCM quality, cache coverage
  - Penetration effects: Hand passes through object (Figure 10) → pose-condition mismatch, scale calibration error
  - Texture drift: Object appearance changes unpredictably → cache may lack required view angles

- First 3 experiments:
  1. Ablate RCM mechanism: Train I+II+III without RCM (use single-view object reference) → expect Obj-IoU drop from 0.8288 to ~0.66 (Table 3)
  2. Ablate curriculum Stage II: Train I→III directly → expect LMD degradation from 0.1427 to ~0.2054 (Table 2, row "I + III")
  3. Validate T-SSIM metric: Generate video, compute T-SSIM, visually inspect high vs. low scoring outputs to confirm correlation with perceived geometry consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework explicitly enforce physical plausibility to resolve penetration artifacts when there is a significant mismatch between object geometry and human pose conditions?
- Basis in paper: [explicit] The authors observe that "when the mismatch between the object geometry and human pose condition is too large, there are obvious penetration effects observed" during inference with novel objects.
- Why unresolved: The current model relies on latent fusion and self-attention to learn interactions implicitly but lacks an explicit collision detection or physics-based constraint to prevent objects from passing through hands during generation.
- What evidence would resolve it: A study integrating a differentiable physics layer or collision loss, demonstrating the qualitative elimination of penetration and introducing a quantitative metric for physical correctness.

### Open Question 2
- Question: How can error accumulation in multi-segment inference be mitigated to improve background consistency without compromising motion smoothness?
- Basis in paper: [explicit] In the quantitative results, the authors attribute the "slightly fall back of background consistency" to "error accumulation when we perform multi-segments inference."
- Why unresolved: The paper utilizes a multi-segment approach for longer videos but does not propose a specific mechanism to correct the drift of background features across segment boundaries.
- What evidence would resolve it: An ablation study comparing the current inference strategy against recurrent or sliding-window approaches that utilize global context, demonstrating improved Back-Cons scores.

### Open Question 3
- Question: Can the "Object Multiview" curriculum stage be redesigned to provide significant gains, given the current marginal utility caused by the distribution gap between synthetic floating objects and authentic interactions?
- Basis in paper: [explicit] The authors note that the yield from the optional object NVS curriculum is "marginal" and the "data distribution... is dramatically different from the authentic human object interaction setup."
- Why unresolved: While the curriculum aims to teach geometry consistency, the domain shift between isolated rotating objects and human-held objects limits the transferability of this training stage.
- What evidence would resolve it: A revised curriculum utilizing in-context object rotation (objects held by hands) showing a statistically significant improvement in T-SSIM and Obj-IoU over the current optional stage.

### Open Question 4
- Question: Does the Relative Coordinate Map (RCM) mechanism scale effectively to highly articulated objects where topology changes dynamically during interaction?
- Basis in paper: [inferred] The method claims RCM is applicable to articulable objects "as long as there is a consistent mapping," but the Mani4D benchmark and qualitative results focus primarily on rigid or simple objects.
- Why unresolved: RCM relies on per-vertex mapping relative to a bounding box; significant topology changes or self-occlusions in articulated objects (e.g., scissors) might degrade the map's utility as a universal representation.
- What evidence would resolve it: Evaluation on a dataset of complex articulated objects (e.g., ARCTIC) to verify if T-SSIM and texture fidelity are maintained comparable to rigid object performance.

## Limitations

- RCM-cache mechanism lacks sufficient technical detail for replication, particularly view selection strategy and cache size determination
- Proprietary human pose dataset (6.4M frames) remains inaccessible, requiring substitute data that may not match original distribution
- T-SSIM metric relies on vision foundation model reconstructions whose accuracy directly impacts geometry consistency measurements

## Confidence

- **High Confidence**: Obj-IoU (0.8288), Face-Cos-Similarity (0.8891), and LMD (0.1427) metrics - these use established evaluation methods with reproducible implementations
- **Medium Confidence**: RCM-cache mechanism effectiveness - theoretically sound but implementation details unclear, corpus evidence limited to this paper
- **Medium Confidence**: Three-stage curriculum benefits - ablation results are clear, but curriculum stage durations and transition criteria are underspecified
- **Low Confidence**: T-SSIM metric validity - novel metric with no external validation or corpus precedent, heavily dependent on foundation model quality

## Next Checks

1. Implement the RCM-cache mechanism with various view coverage strategies and measure impact on Obj-IoU to quantify the geometry consistency benefit
2. Replicate the Stage II ablation by training I→III directly versus I→II→III on HO3D/dexycb/ARCTIC to verify LMD degradation from 0.1427 to ~0.2054
3. Generate videos with known geometry consistency variations and compute T-SSIM scores to verify correlation between metric values and visual quality