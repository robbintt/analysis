---
ver: rpa2
title: An Efficient Diffusion-based Non-Autoregressive Solver for Traveling Salesman
  Problem
arxiv_id: '2501.13767'
source_url: https://arxiv.org/abs/2501.13767
tags:
- solution
- deitsp
- noise
- instances
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DEITSP, a diffusion-based non-autoregressive
  model for solving the Traveling Salesman Problem (TSP) efficiently. The core method
  idea involves using a one-step diffusion model with self-consistency enhancement
  to predict optimal TSP solutions through single-step denoising of multiple solutions.
---

# An Efficient Diffusion-based Non-Autoregressive Solver for Traveling Salesman Problem

## Quick Facts
- arXiv ID: 2501.13767
- Source URL: https://arxiv.org/abs/2501.13767
- Reference count: 40
- One-line primary result: DEITSP outperforms 16 neural network baselines in solution quality, inference latency, and generalization ability across various TSP instances, achieving competitive performance with only one iteration step.

## Executive Summary
This paper introduces DEITSP, a diffusion-based non-autoregressive model designed to efficiently solve the Traveling Salesman Problem (TSP). The model leverages a one-step diffusion approach with self-consistency enhancement to predict optimal TSP solutions through single-step denoising of multiple solutions. DEITSP employs a dual-modality graph transformer to enhance feature extraction and fusion, along with an iterative strategy that alternates between adding and removing noise to improve exploration. Experimental results demonstrate that DEITSP outperforms 16 neural network baselines in terms of solution quality, inference latency, and generalization ability across various TSP instances.

## Method Summary
DEITSP is a non-autoregressive diffusion model that predicts optimal TSP solutions through single-step denoising. The model uses a dual-modality graph transformer architecture with 6 layers, 256 hidden dimensions, and 8 heads. It is trained using a self-consistency loss that ensures all states along a noise trajectory map back to the same original state. During inference, the model iteratively denoises solutions, adds scheduled noise, and repeats, selecting the best solution from all iterations. The training data consists of 1.28M instances per size with node coordinates uniformly sampled from [0,1], and ground truth solutions are generated by Concorde. The model is evaluated on TSP instances of varying sizes (20-1000 nodes) and real-world benchmarks.

## Key Results
- DEITSP outperforms 16 neural network baselines in terms of solution quality, inference latency, and generalization ability across various TSP instances.
- DEITSP achieves competitive performance with only one iteration step, significantly reducing inference time compared to other methods.
- The dual-modality graph transformer and inverse function noise schedule contribute to DEITSP's effectiveness in solving TSP.

## Why This Works (Mechanism)

### Mechanism 1: Consistency-Driven One-Step Denoising
The model uses a self-consistency loss to map any noisy state directly to the optimal solution in a single step, bypassing the computational cost of multi-step Markov chains. This mechanism assumes that TSP solutions are discrete, sparse, and have well-defined ground truths, making them amenable to single-step mapping without iterative refinement artifacts.

### Mechanism 2: Dual-Modality Feature Fusion
The Dual-modal Learning (DML) Layer enhances solution quality by disentangling the representation of continuous node coordinates and discrete edge connectivities. This mechanism assumes that TSP constraints (edges) and spatial relationships (nodes) require distinct processing pathways that standard GNNs might conflate or oversmooth.

### Mechanism 3: Iterative Noise Refinement & Inverse Scheduling
The model improves generalization and exploration by iteratively re-noising and denoising solutions, rather than generating from scratch once. This mechanism assumes that high noise states are too distinct from the optimal solution to be useful for TSP, and that spending compute on low-noise refinement yields better marginal gains than standard linear diffusion schedules.

## Foundational Learning

- **Concept: Discrete Diffusion Models**
  - **Why needed here:** DEITSP operates on discrete adjacency matrices, unlike image diffusion (continuous pixels).
  - **Quick check question:** How does the transition matrix $Q_t$ differ for a binary edge variable versus a standard Gaussian pixel value?

- **Concept: Self-Consistency in Consistency Models**
  - **Why needed here:** This is the core acceleration technique used in DEITSP.
  - **Quick check question:** In the loss function (Eq. 3), why do we enforce consistency between $t$ and $t+k$ rather than just predicting $a_0$ from $a_t$?

- **Concept: Non-Autoregressive (NAR) vs. Autoregressive (AR)**
  - **Why needed here:** DEITSP explicitly trades the sequential generation of AR models for the parallel generation of NAR.
  - **Quick check question:** Since NAR models output a heatmap (independent edge probabilities), how does the paper ensure the final output is a valid Hamiltonian cycle (one entry/exit per node)?

## Architecture Onboarding

- **Component map:** Input Embedding -> DML Layers -> Classifier -> Inference Loop
- **Critical path:** The interaction inside the DML Layer (Eq. 5-8), where the model calculates implicit attention scores from nodes, mixes them with existing edge embeddings, and uses the result to update node embeddings.
- **Design tradeoffs:** Speed vs. Quality (1 Iteration is 18x faster but loses ~0.2% optimality), Noise Schedule (Inverse Function schedule is empirically better for TSP than standard Cosine/Linear).
- **Failure signatures:** Greedy Decoding Failures (disjointed sub-tours or cycles), OOM on Large Instances (quadratic scaling with node count).
- **First 3 experiments:**
  1. Overfit 1-Step Sanity Check: Train on TSP20 with only 1 iteration step and verify if the model can map pure noise to the optimal solution using the Consistency Loss (Eq. 3).
  2. Schedule Ablation: Run inference on TSP50 using Linear vs. Inverse schedules with 4 iterations and plot the Gap % to verify Figure 4.
  3. Modality Ablation: Disable the Edge-Node mixing (replace DML with standard Transformer layers) and observe the drop in Gap % compared to Table 4 ("W/o DML").

## Open Questions the Paper Calls Out
1. How can DEITSP be effectively adapted to utilize unsupervised training mechanisms to eliminate the dependency on exact solvers for ground truth generation in extremely large-scale instances?
2. Can the dual-modality graph transformer and iterative denoising strategy be generalized to solve Vehicle Routing Problems (VRPs) with complex constraints (e.g., time windows, capacity) without significant architectural modifications?
3. Does the proposed inverse function noise schedule remain optimal for combinatorial optimization problems where the solution space is dense rather than sparse?

## Limitations
- The self-consistency mechanism is not extensively validated across diverse problem instances in the neighbor corpus.
- The exact contribution of dual-modality feature fusion to solving complex TSP instances with larger node counts remains unclear.
- The superiority of the iterative noise refinement strategy over standard linear diffusion schedules lacks theoretical justification or broader validation across different problem domains.

## Confidence
- **High confidence** in the experimental results demonstrating DEITSP's performance improvements over 16 neural network baselines.
- **Medium confidence** in the claimed mechanisms (self-consistency, dual-modality fusion, inverse noise scheduling) due to limited evidence in the neighbor corpus.
- **Low confidence** in the model's scalability and performance on real-world TSP instances beyond the tested benchmarks.

## Next Checks
1. Conduct a comprehensive ablation study isolating the contributions of self-consistency, dual-modality fusion, and inverse noise scheduling to verify their individual impacts on solution quality and inference speed.
2. Validate DEITSP's performance on a broader range of TSP instances, including those with non-uniform node distributions and real-world benchmarks not covered in the original experiments.
3. Investigate the model's scalability by testing its performance on TSP instances with node counts significantly larger than 1000 to assess its practical applicability to large-scale problems.