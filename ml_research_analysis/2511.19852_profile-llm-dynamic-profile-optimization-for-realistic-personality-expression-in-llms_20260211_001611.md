---
ver: rpa2
title: 'Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression
  in LLMs'
arxiv_id: '2511.19852'
source_url: https://arxiv.org/abs/2511.19852
tags:
- personality
- prompts
- traits
- prompt
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Profile-LLM, a framework that iteratively optimizes
  persona prompts to elicit personality traits in LLMs. It uses OPRO to refine prompts
  based on TRAIT benchmark scores, improving personality expression.
---

# Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs

## Quick Facts
- **arXiv ID:** 2511.19852
- **Source URL:** https://arxiv.org/abs/2511.19852
- **Reference count:** 24
- **Primary result:** Profile-LLM iteratively optimizes persona prompts using OPRO, improving personality trait expression in LLMs and outperforming baselines like DP and P2 on the TRAIT benchmark.

## Executive Summary
Profile-LLM is a framework that dynamically optimizes persona prompts to elicit specific personality traits in large language models. The approach uses an LLM-as-optimizer (OPRO) to iteratively refine prompts based on TRAIT benchmark scores, enabling better expression of Big Five personality traits (OCEAN). The method shows that larger models better express personality traits, and optimization can control expression intensity for some traits, though smaller models struggle due to limited internal personality representations.

## Method Summary
Profile-LLM employs an LLM-as-optimizer (OPRO) framework where an instruction-tuned optimizer LLM generates candidate persona prompts. These prompts are evaluated using the TRAIT benchmark, which scores personality trait expression. The optimization loop iteratively selects top-performing prompts to guide the generation of new candidates, refining the prompt until desired trait expression is achieved. The approach is black-box compatible and works on frozen models, distinguishing it from fine-tuning methods.

## Key Results
- Profile-LLM outperforms baselines (DP, P2) in evoking Big Five traits across multiple model sizes
- Larger models (8B+) show better personality expression than smaller models (1B), which lack internal personality representations
- Optimization can control trait expression intensity for Openness, Extraversion, and Neuroticism, but not Agreeableness or Conscientiousness due to early saturation

## Why This Works (Mechanism)
Profile-LLM works by leveraging an LLM's ability to generate diverse, semantically rich prompts that can evoke specific personality traits. The OPRO framework allows the optimizer to iteratively refine prompts based on quantitative feedback from the TRAIT benchmark, effectively "learning" which prompt formulations best elicit the target personality. This approach is more flexible than static prompts or fine-tuning, as it can adapt to different models and personality requirements without modifying model weights.

## Foundational Learning

- **Big Five Personality Model (OCEAN)**
  - Why needed here: The paper's entire goal is to elicit these five specific traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) in LLMs. The TRAIT benchmark and the optimization targets are all defined by this framework.
  - Quick check question: Can you name the Big Five traits and give a one-sentence description for "Conscientiousness" and "Neuroticism"?

- **LLM-as-a-Judge / Scoring Functions**
  - Why needed here: The OPRO optimization loop is not gradient-based; it relies entirely on a scoring function (from the TRAIT benchmark) to evaluate prompt effectiveness. Understanding how we quantify "personality expression" is critical to understanding what the optimizer is maximizing.
  - Quick check question: In the Profile-LLM framework, what provides the quantitative feedback signal that allows the optimizer LLM to judge and refine its own prompts?

- **In-Context Learning vs. Parameter Modification**
  - Why needed here: The paper explicitly contrasts its approach (prompt optimization, leaving model weights frozen) with methods like fine-tuning (e.g., PsychAdapter). Grasping this distinction is key to understanding why Profile-LLM is more flexible and transferable across models.
  - Quick check question: Does Profile-LLM change the internal weights of the target LLM, or does it only change the input provided to the model? Why is this advantageous for black-box APIs?

## Architecture Onboarding

- **Component map:**
  Optimizer LLM (`M_optimizer`) -> Meta-Prompt Template -> Target LLM (`M_target`) -> TRAIT Benchmark -> Scoring Function

- **Critical path:**
  1. Initialize the buffer with a generic or baseline persona prompt
  2. Evaluate: Feed current top prompts to the Target LLM, have it answer TRAIT benchmark questions, and calculate a score
  3. Select: Rank prompts by score and select the top `n` to build the optimization trajectory
  4. Optimize: Feed the trajectory to the Optimizer LLM via the Meta-Prompt, instructing it to generate `k` new, better prompts
  5. Update: Add new prompts and their scores to the buffer
  6. Iterate: Repeat steps 2-5 for a set number of steps or until convergence
  7. Control: Select the final prompt from a specific step in the trajectory to achieve the desired trait intensity

- **Design tradeoffs:**
  - Optimizer Model Size: A larger, more capable optimizer may find better prompts faster but is more computationally expensive. The paper uses the same model for both, simplifying the setup
  - Prompt Buffer Size (`n`) and Candidates (`k`): A larger `n` gives the optimizer more context, while a larger `k` increases search space diversity. Both increase the length of the meta-prompt and computational cost per step
  - Scoring Metric: Using the full TRAIT benchmark for scoring at each step is expensive. The paper uses a subset (`q=3` questions) for evaluation during optimization, which is faster but noisier

- **Failure signatures:**
  - Score Plateau: Training curves for Agreeableness and Conscientiousness saturate quickly, limiting the ability to control intensity via early stopping
  - Small Model Failure: On models like Gemma-1B, all prompts (optimized or not) perform poorly, as they lack the necessary internal representations of personality
  - Over-Optimization/Drift: Without careful meta-prompting, the optimizer could generate increasingly bizarre or verbose prompts that "game" the scoring metric without truly reflecting the desired personality
  - Poor Transfer: Prompts optimized for one model (e.g., LLaMA-8B) may not transfer effectively to smaller models, requiring model-specific re-optimization

- **First 3 experiments:**
  1. Baseline Replication: Implement the DP (Description Prompt) and P2 (Personality Prompt) baselines on LLaMA3.1-8B-Instruct and evaluate them on the TRAIT benchmark. This confirms the evaluation pipeline and establishes a performance floor
  2. Single-Trait Optimization Run: Run the full Profile-LLM optimization loop for one trait (e.g., Openness) on LLaMA3.1-8B-Instruct. Plot the training curve (optimization step vs. score) and compare the final prompt's performance against the baselines
  3. Cross-Model Transfer Test: Take the best prompt for Openness from Experiment 2 and directly apply it to a different model (e.g., Mistral-7B-Instruct). Evaluate its performance and compare it to a baseline prompt. This tests the transferability of the learned prompts

## Open Questions the Paper Calls Out

- Can Profile-LLM effectively generalize to non-instruction-tuned models and diverse architectures beyond the LLaMA and Gemma families?
  - Basis in paper: [explicit] The "Limitations" section states that evaluations were conducted on a limited set of Instruct-tuned models and that further investigation is needed for non-instruction-following models and other architectures
  - Why unresolved: The current experiments exclusively utilize instruct variants (e.g., LLaMA-3.1-8B-Instruct), leaving the efficacy of dynamic profile optimization on base models or structurally different architectures unknown
  - What evidence would resolve it: Applying Profile-LLM to base pre-trained models (without instruction tuning) and alternative architectures (e.g., Mixture-of-Experts) and comparing trait elicitation success rates

- How can personality expression be reliably induced in smaller models (<3B parameters) that appear to lack internal personality representations?
  - Basis in paper: [explicit] The "Conclusion" explicitly identifies "enhancing personality expression control in smaller models" as a focus for future work, noting that smaller models struggle due to limited internal representations
  - Why unresolved: The study found that methods like naive prompting and optimization failed to improve scores in 1B models, suggesting a fundamental capability gap rather than a prompting issue
  - What evidence would resolve it: Developing and testing alternative methods (e.g., specialized fine-tuning or chain-of-thought conditioning) that successfully elevate trait scores in small models like Gemma-1B or LLaMA-3.2-1B

- Does optimizing for high personality scores via questionnaires inadvertently degrade the semantic reasoning or factual accuracy of the model in downstream tasks?
  - Basis in paper: [inferred] The paper relies entirely on questionnaire-based benchmarks (TRAIT, MPI) for optimization and admits in the "Limitations" section that these "may not fully capture the nuances" and that applying human frameworks may lead to "oversimplifications"
  - Why unresolved: Maximizing a specific metric like the Paraphrase-sensitive score might encourage the model to adopt superficial linguistic markers of a trait (e.g., neurotic vocabulary) at the expense of logical consistency or truthfulness
  - What evidence would resolve it: A dual evaluation measuring both personality benchmark scores and performance on general reasoning/factuality benchmarks (e.g., MMLU or TruthfulQA) using the optimized profiles

## Limitations

- Profile-LLM fails on small models (<3B parameters) due to their lack of internal personality representations
- Agreeableness and Conscientiousness show rapid score saturation, limiting control over personality intensity
- The exact meta-prompt template content is not provided, affecting reproducibility of results

## Confidence

- **High Confidence**: The OPRO-based iterative optimization framework is technically sound and the methodology for prompt optimization is clearly described
- **Medium Confidence**: Results showing larger models express personality better are supported, but the exact minimum model size threshold is unclear
- **Low Confidence**: Claims about practical applicability across diverse real-world scenarios are limited by the specific TRAIT benchmark and controlled experimental conditions

## Next Checks

1. **Minimum Model Size Validation**: Test Profile-LLM on a spectrum of model sizes (500M, 1B, 3B, 7B) to empirically determine the minimum parameter threshold required for successful personality expression

2. **Cross-Architecture Transfer**: Evaluate whether prompts optimized on transformer-based models transfer effectively to alternative architectures (e.g., Mamba, RWKV) to test the generality of learned prompt representations

3. **Control Granularity Assessment**: Systematically vary the optimization trajectory length and early stopping points for all five traits to quantify the actual range of personality intensity control achievable, particularly for traits with ceiling effects