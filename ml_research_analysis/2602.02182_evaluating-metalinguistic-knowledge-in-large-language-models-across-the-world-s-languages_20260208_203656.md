---
ver: rpa2
title: Evaluating Metalinguistic Knowledge in Large Language Models across the World's
  Languages
arxiv_id: '2602.02182'
source_url: https://arxiv.org/abs/2602.02182
tags:
- languages
- language
- linguistic
- across
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark based on the World Atlas of Language
  Structures (WALS) to evaluate the metalinguistic knowledge of large language models
  across the world's languages. The benchmark covers 2,660 languages and 192 grammatical
  features, converted into a QA-style format.
---

# Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages

## Quick Facts
- arXiv ID: 2602.02182
- Source URL: https://arxiv.org/abs/2602.02182
- Reference count: 12
- Primary result: LLM metalinguistic performance correlates with digital resource availability, with GPT-4o achieving 0.367 accuracy across 2,660 languages

## Executive Summary
This paper introduces a benchmark based on the World Atlas of Language Structures (WALS) to evaluate the metalinguistic knowledge of large language models across 2,660 languages and 192 grammatical features. The benchmark converts WALS feature descriptions into a QA-style format and evaluates three models (GPT-4o, Llama-3.3-70B, and Gemma-3-27B) in zero-shot mode. Results show that metalinguistic performance is limited, with GPT-4o achieving only moderate accuracy (0.367) and no model outperforming the majority-class baseline. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features the lowest. Crucially, accuracy is strongly associated with digital language status, with low-resource languages exhibiting substantially lower performance.

## Method Summary
The authors constructed a benchmark by converting WALS feature descriptions into multiple-choice questions with natural language answer options, then prompted three LLMs (GPT-4o, Llama-3.3-70B, and Gemma-3-27B) in zero-shot mode with constrained output format. They evaluated accuracy and macro-F1 across features, domains, and languages, computing baselines from random chance and majority-class predictions. The analysis examined correlations with digital resource availability, linguistic predictors, and feature online visibility, using random forest feature importance to identify key factors.

## Key Results
- GPT-4o achieved 0.367 accuracy, falling between chance (0.234) and majority-class (0.539) baselines
- Performance strongly correlates with digital language status (ρ=0.734 for GPT-4o on 100-language sample)
- Wikipedia size is the most informative predictor of accuracy (feature importance: 0.136-0.151)
- Lexical features show highest accuracy while phonological features show lowest
- No evidence that language family or geographic proximity predicts performance

## Why This Works (Mechanism)

### Mechanism 1: Resource Availability Shapes Metalinguistic Knowledge
- Claim: LLMs' explicit knowledge about language structure correlates with digital resource availability rather than generalizable grammatical competence.
- Mechanism: Models encode metalinguistic patterns proportionally to their frequency in training corpora; languages and features with more online descriptive content receive stronger representations.
- Core assumption: Training data contains metalinguistic descriptions (grammar discussions, linguistic analyses) whose distribution mirrors digital language status.
- Evidence anchors:
  - [abstract] "accuracy is strongly associated with digital language status, with low-resource languages exhibiting substantially lower performance"
  - [Section 5.3.1] "Correlations are substantially stronger for the 100-language sample (GPT-4o: ρ=0.734)"
  - [corpus] UrBLiMP paper confirms LLMs "include significantly less data for low-resource languages"

### Mechanism 2: Domain-Level Performance Reflects Online Feature Visibility
- Claim: Linguistic domains with greater online discussion frequency achieve higher model accuracy.
- Mechanism: Features commonly discussed in linguistic literature and online resources accumulate more training signal, enabling better explicit recall.
- Core assumption: Google search hit counts serve as a reasonable proxy for training data frequency of metalinguistic content.
- Evidence anchors:
  - [Section 5.2] "For GPT-4o, accuracy correlates strongly with online visibility (r = 0.715)"
  - [Section 5.2] "lexical features showing the highest accuracy and phonological features among the lowest"
  - [corpus] PhonologyBench confirms LLM phonological competence remains "below human performance, especially for tasks that require abstract phonological reasoning"

### Mechanism 3: Statistical Pattern Capture Without Fine-Grained Competence
- Claim: Models capture dominant cross-linguistic patterns but lack language-specific grammatical distinctions.
- Mechanism: Training on multilingual data produces statistical regularities that beat chance but default to majority patterns when specific knowledge is absent.
- Core assumption: Models cannot reliably distinguish between similar languages or fine-grained feature values without explicit exposure.
- Evidence anchors:
  - [abstract] "All models perform above chance but fail to outperform the majority-class baseline"
  - [Section 5.1] Chance baseline: 0.234; Majority-class baseline: 0.539; Best model (GPT-4o): 0.367
  - [corpus] MultiBLiMP 1.0 finds "grammatical competence is mainly data-driven and may deteriorate during post-training"

## Foundational Learning

- Concept: **Metalinguistic vs. Performance Knowledge**
  - Why needed here: The benchmark explicitly distinguishes using language from reasoning about its structure—a gap this paper demonstrates.
  - Quick check question: Can a model generate grammatically correct sentences in Urdu while being unable to identify whether Urdu uses SOV or SVO word order?

- Concept: **Typological Feature Representation**
  - Why needed here: WALS encodes grammatical properties as discrete feature values (e.g., "plural suffix" for nominal plurality) rather than gradient frequencies.
  - Quick check question: Why might representing word order as a discrete category (SOV/SVO/VSO) miss variation in actual corpus usage?

- Concept: **Digital Language Status Taxonomy**
  - Why needed here: The six-class Joshi taxonomy (0–5) provides the primary framework for analyzing resource-driven performance gaps.
  - Quick check question: A language with substantial Wikipedia content but minimal labeled NLP data would fall into which status class?

## Architecture Onboarding

- Component map:
  WALS database (192 features, 2,660 languages) -> Feature-to-question conversion -> QA template instantiation per language -> Zero-shot model prompting -> Accuracy/macro-F1 computation -> Domain and language-level aggregation

- Critical path:
  1. Transform WALS feature descriptions into natural language questions with discrete answer options
  2. Rephrase terminology-heavy labels (e.g., "NegSOV" -> "Negative word before subject, object, verb") to improve interpretability
  3. Prompt models with constrained format ("Answer with one of the options only. Do not explain.")
  4. Weight domain-level accuracy by feature coverage (number of languages attested)

- Design tradeoffs:
  - **Multiple-choice format**: Enables automated evaluation but Raman et al. (2025) suggest models may exploit option-level artifacts
  - **WALS 100-language sample**: Denser annotation (95–159 features/language) but reduced language diversity
  - **Zero-shot prompting**: Isolates intrinsic knowledge but underestimates potential with examples
  - **Discrete feature values**: Clear evaluation targets but miss gradient usage patterns in corpora

- Failure signatures:
  - Accuracy between chance (0.234) and majority-class (0.539) -> model captured broad patterns without specific knowledge
  - Strong Wikipedia-size correlation (feature importance: 0.136–0.151) -> resource-dependency confirmed
  - Consistently weak phonology performance -> orthographic training insufficient for sound-level phenomena
  - No genealogical signal (feature importance: 0.015–0.028) -> language family provides no transfer benefit

- First 3 experiments:
  1. **Baseline establishment**: Evaluate zero-shot accuracy on WALS-100 sample across 192 features; confirm performance falls between chance and majority-class baselines.
  2. **Domain visibility correlation**: Compute per-domain accuracy and correlate with log-transformed Google search hit counts; expect r > 0.5 for well-resourced models.
  3. **Predictor importance analysis**: Train random forest classifier on language accuracy groups (high/middle/low) using eight predictors; verify Wikipedia size and resource availability rank highest, language family lowest.

## Open Questions the Paper Calls Out

- Does paraphrasing WALS-based questions and answer options meaningfully reduce potential memorization effects in benchmark evaluations? The authors note that "future work could introduce additional controls such as paraphrased questions and answers across the full set of features" but no controlled experiments comparing paraphrased vs. original prompts have been conducted.

- Why does domain-level accuracy correlate with online visibility for GPT-4o (r=0.715) and Gemma-3-27B (r=0.571) but not for Llama-3.3-70B (r=0.045)? The authors hypothesize differences in "training data composition and curation strategies" but provide no empirical test of this explanation.

- Would using Grambank instead of WALS yield different conclusions about LLM metalinguistic knowledge given its more systematic per-language coverage? The authors note that "future benchmarks could...draw on similar resources like Grambank, which offers more systematic per-language coverage, though without the phonological and lexical domains examined here" but no comparative evaluation using Grambank has been conducted.

## Limitations

- Template Fidelity: The paper specifies prompt rephrasing for 31 features but does not fully detail templates for all 192 features, potentially affecting reproducibility.
- Feature Coverage Balance: WALS contains highly variable feature coverage across languages (1–156 features per language), which may mask performance patterns in sparsely annotated languages.
- Digital Status Proxy Validity: The categorical Joshi digital language status taxonomy may oversimplify the continuous nature of digital language presence and usage patterns.

## Confidence

- **High Confidence**: The core finding that LLM metalinguistic performance is strongly correlated with digital language status (ρ=0.734 for GPT-4o on 100-language sample) is well-supported by multiple analyses and robust predictor importance rankings showing Wikipedia size as the top predictor.
- **Medium Confidence**: The conclusion that models fail to outperform majority-class baselines is methodologically sound, though the multiple-choice format may introduce artifact-driven performance that could affect the magnitude of this gap.
- **Low Confidence**: The interpretation that phonological features show the lowest accuracy due to orthographic training limitations is plausible but not definitively proven, as the analysis does not control for other factors that might affect phonological feature visibility in training data.

## Next Checks

1. **Feature Template Validation**: Reconstruct and validate all 192 feature templates using the described rephrasing rules, then measure inter-rater agreement between original WALS descriptions and the natural language formulations to ensure semantic preservation.

2. **Open-Ended Response Analysis**: Repeat the evaluation using open-ended responses (allowing model-generated answers) to determine whether multiple-choice artifacts are inflating accuracy, particularly for features with limited online visibility.

3. **Cross-Lingual Transfer Experiment**: Systematically test whether models can accurately predict metalinguistic features for low-resource languages when prompted with similar high-resource languages from the same family, isolating the role of genuine cross-linguistic knowledge from data-driven pattern matching.