---
ver: rpa2
title: Reevaluating Self-Consistency Scaling in Multi-Agent Systems
arxiv_id: '2511.00751'
source_url: https://arxiv.org/abs/2511.00751
tags:
- reasoning
- self-consistency
- paths
- accuracy
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reevaluates self-consistency scaling for modern LLMs
  using Gemini 2.5 models on HotpotQA and Math-500 benchmarks. Experiments tested
  3, 5, 10, 15, and 20 reasoning agents versus single CoT baselines, measuring accuracy
  gains and token costs.
---

# Reevaluating Self-Consistency Scaling in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2511.00751
- Source URL: https://arxiv.org/abs/2511.00751
- Reference count: 1
- Key finding: Self-consistency accuracy gains plateau after moderate agent sampling; 3-5 agents capture most benefits.

## Executive Summary
This study reevaluates self-consistency scaling for modern LLMs using Gemini 2.5 models on HotpotQA and Math-500 benchmarks. Experiments tested 3, 5, 10, 15, and 20 reasoning agents versus single CoT baselines, measuring accuracy gains and token costs. Results show accuracy improves with more agents but plateaus after moderate sampling, matching earlier findings (Wang et al., 2022). Gemini 2.5 Pro improved from 98% to 99.6% accuracy on Math-500 with 15 agents; Flash Lite showed smaller gains (1.6% and 0.4% on Math-500 and HotpotQA at 20 agents). Diminishing returns stem from reasoning path redundancy, not model capability. Larger models showed smoother improvement curves. High-sample configurations offer minimal accuracy benefit relative to their computational cost, suggesting moderate sampling is optimal for balancing accuracy and efficiency.

## Method Summary
The study evaluates self-consistency scaling by testing multi-agent configurations against single chain-of-thought (CoT) baselines on HotpotQA and Math-500 benchmarks. Agent counts of 3, 5, 10, 15, and 20 generate independent CoT responses under fixed sampling parameters, with an aggregator model selecting the most semantically coherent response. Accuracy is measured via semantic equivalence scoring using an evaluator LLM, while token costs are tracked. Gemini-2.5-Flash-Lite is tested with up to 20 agents on 250 samples from each dataset, while Gemini-2.5-Pro is tested on Math-500 with up to 15 agents.

## Key Results
- Accuracy plateaus after moderate sampling (5-10 agents) across both benchmarks and model sizes
- Gemini 2.5 Pro: 98% → 99.6% accuracy on Math-500 with 15 agents
- Gemini 2.5 Flash-Lite: 1.6% gain on Math-500 and 0.4% gain on HotpotQA at 20 agents
- Larger models (Pro) show smoother, more consistent improvement curves than smaller models (Flash-Lite)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating multiple independently-sampled reasoning paths improves accuracy by selecting the most consistent answer.
- Mechanism: Multiple agents generate separate chain-of-thought traces under identical prompts; a secondary aggregator model identifies the response with highest semantic agreement among agents. This cancels stochastic errors when incorrect paths diverge while correct paths converge.
- Core assumption: Reasoning errors are uncorrelated across samples; correct answers exhibit higher agreement than incorrect ones.
- Evidence anchors:
  - [abstract] "pooling outputs from varying sampled reasoning paths... performance gains taper off after moderate sampling"
  - [section 3.1] "multiple independent reasoning agents... generating separate chain-of-thought responses... aggregator model... determine the most internally consistent or semantically coherent response"
  - [corpus] Wang et al. (2022) in references established original self-consistency mechanism
- Break condition: When sampled paths become highly redundant (see Mechanism 2), aggregation provides no new signal.

### Mechanism 2
- Claim: Accuracy gains plateau because reasoning paths become redundant beyond moderate sample counts.
- Mechanism: As more paths are sampled from the same model under fixed parameters, the probability of generating meaningfully distinct reasoning trajectories decreases. Overlap increases marginal information per sample diminishes.
- Core assumption: The model's reasoning distribution has finite diversity under fixed sampling parameters.
- Evidence anchors:
  - [abstract] "plateau suggests diminishing returns driven by overlap among reasoning paths"
  - [section 5] "diminishing returns arise primarily from redundancy among reasoning paths rather than from differences in model capability"
  - [corpus] "Scaling over Scaling: Exploring Test-Time Scaling Plateau" (arXiv:2505.20522) corroborates plateau phenomenon in test-time compute
- Break condition: If sampling parameters (temperature, prompts) were varied per agent, redundancy might decrease—but this was held constant (Section 3.1).

### Mechanism 3
- Claim: Larger models exhibit smoother, more consistent improvement curves from self-consistency.
- Mechanism: Larger models (Gemini 2.5 Pro) produce more coherent reasoning traces internally, so aggregated paths agree more consistently. Smaller models (Flash-Lite) show noisier improvement patterns.
- Core assumption: Model scale correlates with reasoning stability and internal coherence.
- Evidence anchors:
  - [abstract] "Larger models exhibited a more stable and consistent improvement curve"
  - [section 4] "Gemini-2.5-Pro... accuracy curve was smoother and more consistent than Flash-Lite's, reflecting stronger internal coherence"
  - [corpus] Corpus evidence on scale-stability relationship is weak; no direct neighbor papers address this specific claim
- Break condition: Assumption: Smaller models may benefit more from diversity-inducing strategies (varied prompts, temperatures) to compensate for instability.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: Self-consistency builds on CoT as the base reasoning format; each agent generates a CoT trace before aggregation.
  - Quick check question: Can you explain why CoT enables error detection across multiple samples, whereas single-token answers would not?

- **Concept: Ensemble Agreement / Majority Voting**
  - Why needed here: The aggregator selects the "most consistent" response—fundamentally an ensemble voting mechanism applied to reasoning traces.
  - Quick check question: If three agents output answers A, A, and B, what does majority voting select? What if outputs are semantically similar but textually different?

- **Concept: Marginal Utility and Cost-Accuracy Tradeoffs**
  - Why needed here: The paper's core conclusion is that additional agents provide diminishing returns; understanding marginal benefit per unit cost is essential for system design.
  - Quick check question: If 3 agents yield 95% accuracy at $0.10 and 20 agents yield 95.4% at $0.60, which configuration is preferable for a high-throughput production system?

## Architecture Onboarding

- **Component map:**
  - Query -> N agents (parallel) -> Aggregator model -> Final answer

- **Critical path:**
  1. Query received → broadcast to N agents in parallel
  2. Each agent generates CoT trace (sampling parameters fixed)
  3. Aggregator reviews all N traces → selects most consistent answer
  4. Return final answer + record token cost
  5. Latency bottleneck: longest agent generation time + aggregator inference

- **Design tradeoffs:**
  - **Agent count vs. accuracy**: Paper shows plateau around 10-15 agents; 3-5 agents capture most gains (Section 4, Figures 1-3).
  - **Model size vs. stability**: Larger models (Pro) give smoother curves; smaller models (Flash-Lite) fluctuate but are cheaper.
  - **Cost scaling**: Token cost increases ~linearly with agent count (Section 4); accuracy gains sublinear.
  - Assumption: Parallel execution infrastructure can reduce latency impact of multiple agents.

- **Failure signatures:**
  - **Redundancy collapse**: All N agents converge to same incorrect answer → aggregation provides no correction (inherent limitation).
  - **Fluctuation instability**: Smaller models show irregular improvement curves (Figure 1); may not be reliable for production without higher counts.
  - **Cost blowout**: High agent counts (15-20) with <0.5% accuracy gain over moderate counts (Section 5).

- **First 3 experiments:**
  1. **Baseline calibration**: Run single-agent CoT on 50 samples from your target domain; record accuracy and token cost. Compare to paper's reported baseline (e.g., 98% for Pro on Math-500).
  2. **Scaling curve probe**: Test 3, 5, and 10 agents on same 50 samples; plot accuracy vs. agent count to identify your plateau point. Expect diminishing returns past 5-10 agents per paper findings.
  3. **Cost-efficiency threshold**: For each agent count, compute accuracy gain per 1000 tokens. Identify the "knee" where marginal accuracy per token drops sharply—the paper suggests this occurs at moderate counts (5-10 agents).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would more challenging benchmarks reveal larger self-consistency benefits that plateau at higher agent counts?
- Basis in paper: [explicit] "Additionally, the datasets used may be too easy for the evaluated models, potentially masking meaningful differences in reasoning capability. Developing and testing on more challenging datasets would provide a more accurate assessment of model performance and robustness."
- Why unresolved: HotpotQA and Math-500 may have ceiling effects; Pro achieved 98% baseline accuracy on Math-500, leaving little room for improvement.
- What evidence would resolve it: Testing self-consistency scaling on datasets where baseline accuracy is substantially lower (e.g., <70%), then observing whether plateaus occur at higher agent counts.

### Open Question 2
- Question: How does the optimal agent count vary with reasoning type, task complexity, and dataset structure?
- Basis in paper: [explicit] "Larger and more varied benchmarks could reveal performance differences specific to reasoning type, task complexity, or dataset structure."
- Why unresolved: Only two datasets were tested, representing just multi-hop QA and mathematical reasoning; generalization to other domains (e.g., code, logic puzzles, scientific reasoning) remains unknown.
- What evidence would resolve it: Systematic evaluation across diverse reasoning benchmarks with varying complexity levels, tracking where plateaus occur for each category.

### Open Question 3
- Question: Do self-consistency scaling patterns generalize to other modern LLM families beyond Gemini 2.5?
- Basis in paper: [inferred] Only Gemini models were tested; no justification provided for why these findings should apply to other architectures.
- Why unresolved: Model-specific training, tokenization, or sampling behaviors could produce different redundancy patterns.
- What evidence would resolve it: Replicating the same experimental protocol on GPT, Claude, Llama, and other model families to compare plateau curves.

## Limitations

- Exact sampling parameters (temperature, top-p) and aggregator implementation details are not fully specified, affecting reproducibility
- Study focuses exclusively on Gemini 2.5 models, limiting generalizability across model families
- Semantic equivalence evaluation relies on an evaluator LLM whose configuration and potential biases are not detailed

## Confidence

**High Confidence**: The observation that accuracy gains plateau after moderate sampling (5-10 agents) is well-supported by the data across both benchmarks and model sizes. The cost-accuracy tradeoff analysis showing diminishing returns is directly observable from the reported token counts and accuracy figures.

**Medium Confidence**: The claim that redundancy among reasoning paths is the primary driver of diminishing returns is plausible but not definitively proven. While the data shows plateauing performance, the mechanism connecting path overlap to accuracy saturation is inferred rather than directly measured.

**Low Confidence**: The assertion that larger models exhibit more stable improvement curves lacks strong external validation. The paper's internal comparison between Pro and Flash-Lite models shows this pattern, but the corpus evidence is weak and no direct neighbor papers address this specific claim.

## Next Checks

1. **Redundancy Quantification**: Implement path similarity metrics (e.g., ROUGE, semantic embedding distance) across agents for each sample. If similarity scores exceed 70-80% for high agent counts, this would confirm the redundancy hypothesis. Track whether similarity plateaus correlate with accuracy plateaus.

2. **Cross-Model Generalization**: Replicate the scaling experiments using a different model family (e.g., GPT-4o, Claude 3) on the same benchmarks. Compare plateau points and improvement curves to test whether the observed patterns are model-specific or represent a general scaling phenomenon.

3. **Aggregator Sensitivity Analysis**: Test multiple aggregation strategies (majority voting, LLM-based coherence selection, answer confidence weighting) across the same agent configurations. Measure how aggregation method choice affects plateau onset and magnitude of accuracy gains, particularly for smaller models showing unstable curves.