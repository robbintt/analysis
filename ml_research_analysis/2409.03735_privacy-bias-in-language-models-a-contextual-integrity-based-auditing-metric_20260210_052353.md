---
ver: rpa2
title: 'Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric'
arxiv_id: '2409.03735'
source_url: https://arxiv.org/abs/2409.03735
tags:
- privacy
- llms
- information
- bias
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces privacy bias as a novel metric to evaluate
  how large language models (LLMs) deviate from expected information-sharing norms.
  The core method uses multi-prompt assessment to handle prompt sensitivity, filtering
  for consistent responses across prompt variations.
---

# Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric

## Quick Facts
- arXiv ID: 2409.03735
- Source URL: https://arxiv.org/abs/2409.03735
- Reference count: 40
- Introduces privacy bias metric to evaluate LLM information-sharing norms

## Executive Summary
This paper introduces privacy bias as a novel metric to evaluate how large language models deviate from expected information-sharing norms. The core method uses multi-prompt assessment to handle prompt sensitivity, filtering for consistent responses across prompt variations. Privacy biases were analyzed for eight different LLMs, revealing systematic differences based on model capacity, alignment, and quantization.

The metric quantifies deviation from crowd-sourced privacy expectations using a privacy bias delta (Δbias), showing that most models differ significantly from these expectations. The approach enables auditors to assess ethical impacts, select appropriate models, and evaluate policy compliance. Results demonstrate that 7B models tend to be more conservative while 13B models are more liberal in their information sharing.

## Method Summary
The privacy bias metric evaluates LLM responses to privacy-related scenarios using contextual integrity principles. The method employs multi-prompt assessment where each scenario is presented through multiple prompts to capture prompt sensitivity. Responses are filtered to ensure consistency across variations, addressing the challenge that different prompts can yield different acceptability judgments. Privacy bias delta (Δbias) is calculated by comparing model responses against crowd-sourced expectations, with positive values indicating more permissive behavior than expected and negative values indicating more restrictive behavior. Regression analysis examines how factors like consent, indefinite storage, and advertising use affect model acceptability judgments.

## Key Results
- 7B models showed more conservative information sharing than 13B models (tulu-2-7B vs tulu-2-13B)
- Aligned models demonstrated higher acceptability than base models
- Regression analysis revealed LLMs are more accepting of information flows with consent but less accepting of indefinite storage or advertising use
- Most models showed significant deviation from crowd-sourced privacy expectations

## Why This Works (Mechanism)
The metric works by operationalizing contextual integrity theory, which posits that privacy violations occur when information flows violate contextual norms. By comparing model responses against crowd-sourced expectations across multiple prompts, the approach captures both the model's internal reasoning about privacy and its sensitivity to prompt framing. The multi-prompt filtering ensures that measured biases reflect consistent model behavior rather than prompt-dependent artifacts.

## Foundational Learning

1. **Contextual Integrity Theory** - Framework for privacy based on appropriate information flows within social contexts
   - Why needed: Provides theoretical foundation for defining privacy expectations
   - Quick check: Verify scenarios align with established contextual integrity principles

2. **Privacy Bias Delta (Δbias)** - Quantitative measure of deviation from expected privacy norms
   - Why needed: Enables objective comparison between models and expectations
   - Quick check: Ensure delta calculations correctly handle edge cases

3. **Multi-prompt Assessment** - Technique using multiple prompts per scenario to capture prompt sensitivity
   - Why needed: Addresses variability in model responses to different prompt formulations
   - Quick check: Confirm filtering criteria effectively identifies consistent responses

4. **Regression Analysis for Acceptability** - Statistical method to identify factors influencing privacy judgments
   - Why needed: Reveals systematic patterns in model decision-making
   - Quick check: Validate model assumptions and significance levels

## Architecture Onboarding

Component Map: Crowd-sourced Expectations -> Multi-prompt Scenarios -> LLM Responses -> Consistency Filtering -> Privacy Bias Calculation -> Regression Analysis

Critical Path: Scenario Design → Multi-prompt Generation → Response Collection → Consistency Filtering → Δbias Calculation → Result Analysis

Design Tradeoffs: Multi-prompt filtering improves consistency but may exclude nuanced responses; crowd-sourced expectations provide normative standards but may reflect cultural bias; regression analysis identifies patterns but assumes linear relationships.

Failure Signatures: Inconsistent filtering leads to unreliable bias scores; cultural bias in crowd-sourcing skews expectations; insufficient prompt variation misses important model behaviors.

3 First Experiments:
1. Test metric on simple binary privacy scenarios to verify basic functionality
2. Compare 7B vs 13B models on identical scenarios to validate capacity effects
3. Apply metric to aligned vs base models to confirm alignment impacts

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability of crowd-sourced expectations as normative privacy standards may not hold across different cultural contexts
- Potential confounding effects from model-specific training data variations that weren't controlled for
- Assumption that multi-prompt filtering adequately addresses response consistency issues

## Confidence

High:
- Empirical observations about model differences (7B vs 13B behavior patterns, alignment effects on acceptability)

Medium:
- Statistical relationships (regression coefficients showing consent effects), practical implications for model selection

Low:
- Normative claims about what constitutes "correct" privacy expectations, universal applicability across different cultural contexts

## Next Checks
1. Test metric stability across different cultural contexts by replicating with non-Western crowd-sourced expectations to assess cultural bias in the framework
2. Conduct ablation studies removing the multi-prompt filtering to quantify its impact on privacy bias scores and identify potential loss of nuanced responses
3. Implement controlled experiments varying only model capacity while holding training data constant to isolate the effects of model size from training corpus differences