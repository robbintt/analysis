---
ver: rpa2
title: 'BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable
  Queries'
arxiv_id: '2503.12446'
source_url: https://arxiv.org/abs/2503.12446
tags:
- learnable
- image
- queries
- wang
- breen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BREEN introduces a data-efficient encoder-free multimodal model
  that addresses the performance gap between encoder-based and encoder-free MLLMs.
  The core innovation is a learnable query mechanism that bridges visual and textual
  modalities by distilling knowledge from a pretrained CLIP model.
---

# BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries

## Quick Facts
- arXiv ID: 2503.12446
- Source URL: https://arxiv.org/abs/2503.12446
- Authors: Tianle Li; Yongming Rao; Winston Hu; Yu Cheng
- Reference count: 40
- Primary result: Achieves competitive multimodal performance with only 13M text-image pairs (1% of typical data requirements)

## Executive Summary
BREEN introduces a novel encoder-free multimodal learning approach that significantly reduces data requirements while maintaining competitive performance. The model bridges visual and textual modalities through a learnable query mechanism that distills knowledge from pretrained CLIP models. By positioning learnable queries between image and text tokens and processing them through an independent image expert module, BREEN minimizes interference with the LLM's textual reasoning capabilities. The approach achieves state-of-the-art results among encoder-free models while requiring only 1% of the training data typically needed.

## Method Summary
BREEN addresses the data efficiency challenge in multimodal learning by introducing a learnable query mechanism that acts as an intermediary between visual and textual modalities. The model leverages pretrained CLIP's visual representations to supervise the learning of these queries, ensuring effective knowledge transfer from vision to language. An image expert module processes image tokens and learnable queries independently, preventing interference with the LLM's core textual reasoning capabilities. The model employs multi-granular learnable queries to handle diverse tasks ranging from fine-grained OCR to high-level perceptual reasoning, achieving competitive performance across multiple benchmarks including MMMU, MME, and MMVet.

## Key Results
- Outperforms prior encoder-free models like Mono-InternVL by 2% on average across MMMU, MME, and MMVet benchmarks
- Achieves competitive performance with only 13 million text-image pairs (approximately 1% of typical training data requirements)
- Demonstrates effectiveness across diverse visual tasks through multi-granular learnable queries

## Why This Works (Mechanism)
The learnable query mechanism works by positioning a bridge token between image and text representations that can be optimized to capture relevant visual information. This query is supervised by CLIP-derived visual representations, ensuring it learns meaningful visual concepts while maintaining compatibility with the LLM's language processing. The image expert module processes visual information separately from the LLM's text processing stream, preventing the visual modality from interfering with the model's textual reasoning capabilities. Multi-granular queries allow the model to adapt to different levels of visual abstraction, from pixel-level details to high-level semantic concepts.

## Foundational Learning
- **CLIP Knowledge Distillation**: Required to transfer visual understanding from pretrained models; quick check: verify CLIP supervision signal quality and stability during training
- **Learnable Query Optimization**: Critical for bridging modalities; quick check: monitor query embedding evolution and stability across training epochs
- **Image Expert Module Design**: Necessary to prevent modality interference; quick check: measure performance degradation when expert module is removed or bypassed
- **Multi-Granular Representation**: Enables handling diverse visual tasks; quick check: test performance across fine-grained vs. high-level visual reasoning tasks
- **Encoder-Free Architecture**: Reduces computational overhead and training complexity; quick check: compare parameter count and inference latency vs. encoder-based alternatives
- **Token-Level Supervision**: Ensures precise alignment between visual and textual representations; quick check: analyze attention patterns between learnable queries and other token types

## Architecture Onboarding

Component Map:
Image Tokens -> Image Expert -> Learnable Queries -> LLM Text Processing Stream -> Output

Critical Path:
Image -> Image Expert -> Learnable Queries -> LLM Reasoning -> Text Output

Design Tradeoffs:
The encoder-free design reduces computational complexity but relies heavily on effective knowledge transfer from CLIP. Multi-granular queries provide task flexibility but increase parameter count and training complexity. The image expert module prevents interference but adds architectural complexity that may limit end-to-end optimization.

Failure Signatures:
- Poor performance on fine-grained visual tasks suggests inadequate learnable query granularity
- Degradation in textual reasoning indicates excessive interference from visual processing
- Unstable training may indicate insufficient CLIP supervision signal quality
- Performance bottlenecks suggest suboptimal image expert module design

3 First Experiments:
1. Test learnable query effectiveness by comparing performance with and without CLIP supervision
2. Evaluate interference effects by measuring textual reasoning performance with disabled image expert
3. Assess granularity impact by training variants with single vs. multi-granular query configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Data-efficiency claims lack clear baseline comparison methodology and specification of "existing methods"
- CLIP dependency may limit discovery of novel visual representations beyond CLIP's learned space
- Learnable query effectiveness may degrade on visual domains significantly different from CLIP's training distribution
- Performance improvements may not account for potential architectural differences beyond encoder-free design

## Confidence
High confidence in technical implementation of learnable query mechanism and LLM integration
Medium confidence in claimed data-efficiency improvements due to unclear experimental methodology
Medium confidence in generalizability of multi-granular queries across diverse visual tasks

## Next Checks
1. Conduct controlled experiments comparing performance when trained on varying percentages (1%, 10%, 50%, 100%) of the 13M dataset to validate data-efficiency claims
2. Implement cross-domain testing using datasets outside CLIP's training distribution (medical imaging, satellite imagery, synthetic data) to evaluate robustness
3. Perform ablation studies isolating image expert module vs. learnable queries contributions to quantify individual performance impacts