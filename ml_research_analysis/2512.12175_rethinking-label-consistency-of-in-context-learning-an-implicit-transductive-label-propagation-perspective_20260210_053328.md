---
ver: rpa2
title: 'Rethinking Label Consistency of In-Context Learning: An Implicit Transductive
  Label Propagation Perspective'
arxiv_id: '2512.12175'
source_url: https://arxiv.org/abs/2512.12175
tags:
- label
- demonstrations
- arxiv
- learning
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rethinks in-context learning (ICL) from a transductive
  learning perspective, proposing a label propagation framework based on Bayesian
  inference. The authors argue that existing demonstration selection methods fail
  to ensure label consistency, which is critical for guiding latent concepts and estimating
  propagation errors.
---

# Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective

## Quick Facts
- **arXiv ID:** 2512.12175
- **Source URL:** https://arxiv.org/abs/2512.12175
- **Reference count:** 7
- **Key outcome:** Proposes a transductive label propagation framework for ICL, improving accuracy by 1.4% on average with TopK-SD method.

## Executive Summary
This paper rethinks in-context learning (ICL) from a transductive learning perspective, proposing a label propagation framework based on Bayesian inference. The authors argue that existing demonstration selection methods fail to ensure label consistency, which is critical for guiding latent concepts and estimating propagation errors. To address this, they introduce a data synthesis method that interpolates semantic and label embeddings, followed by TopK sampling with synthetic data (TopK-SD) to select demonstrations with consistent labels. Experiments across nine datasets and four language models show that TopK-SD significantly improves ICL accuracy by 1.4% on average compared to the standard TopK method.

## Method Summary
The authors propose a transductive learning framework for ICL that treats demonstrations as unlabeled data and uses label propagation to infer labels. They introduce a synthetic data generation method that interpolates semantic and label embeddings to create representative examples. The TopK-SD algorithm then selects demonstrations based on label consistency using these synthetic examples. The approach is grounded in Bayesian inference, treating demonstrations as samples from a latent concept distribution. This framework addresses the limitations of existing ICL methods that rely on static demonstration sets without considering label consistency.

## Key Results
- TopK-SD improves ICL accuracy by 1.4% on average across nine datasets
- The method outperforms standard TopK selection across four different language models
- Label consistency emerges as a critical factor for effective ICL demonstration selection

## Why This Works (Mechanism)
The paper's mechanism centers on treating ICL as a transductive learning problem where label propagation can be guided by consistent demonstrations. By ensuring label consistency through synthetic data interpolation, the framework provides better guidance for latent concept learning and more reliable error estimation in the propagation process. The Bayesian inference approach allows for probabilistic reasoning about label assignments based on the available demonstrations.

## Foundational Learning

**Transductive Learning** - Why needed: Provides theoretical framework for using demonstrations as unlabeled data to propagate labels. Quick check: Verify that demonstrations are treated as unlabeled examples in the propagation process.

**Bayesian Inference** - Why needed: Enables probabilistic reasoning about label assignments and uncertainty estimation. Quick check: Confirm that prior distributions are properly defined for latent concepts.

**Label Propagation** - Why needed: Allows information from labeled demonstrations to spread to unlabeled data points. Quick check: Validate that the propagation algorithm converges and maintains label consistency.

**Semantic Embeddings** - Why needed: Captures semantic relationships between data points for effective interpolation. Quick check: Ensure embeddings preserve semantic similarity in