---
ver: rpa2
title: 'Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series
  Models'
arxiv_id: '2511.11622'
source_url: https://arxiv.org/abs/2511.11622
tags:
- mean
- time
- series
- tokenization
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically examines the interplay between tokenizer
  design and pretraining in time series forecasting models. It evaluates different
  scaling (mean, min-max, normal) and quantization (uniform, normal, exponential-decay)
  strategies across vocabulary sizes of 512, 1024, and 4096 tokens, comparing random
  initialization against pretrained Qwen 3 weights.
---

# Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models

## Quick Facts
- arXiv ID: 2511.11622
- Source URL: https://arxiv.org/abs/2511.11622
- Authors: Alexis Roger; Gwen Legate; Kashif Rasul; Yuriy Nevmyvaka; Irina Rish
- Reference count: 7
- Primary result: Systematic evaluation of tokenizer design and pretraining in time series forecasting models

## Executive Summary
This paper investigates the critical interplay between tokenizer design and pretraining in time series foundation models. Through systematic experimentation across different scaling strategies (mean, min-max, normal), quantization methods (uniform, normal, exponential-decay), and vocabulary sizes (512, 1024, 4096 tokens), the authors demonstrate that tokenizer configuration primarily determines representational capacity and stability, while pretraining affects optimization efficiency. The study finds that normal scaling with uniform binning consistently delivers optimal performance, with pretraining providing substantial gains especially for challenging tokenizer configurations and smaller vocabularies.

## Method Summary
The authors evaluate various tokenizer configurations by systematically varying scaling strategies (mean, min-max, normal) and quantization methods (uniform, normal, exponential-decay) across three vocabulary sizes. They compare random initialization against pretrained Qwen 3 weights, using MASE as the primary performance metric. Token space utilization is measured via Cramér's V to analyze the relationship between tokenizer design and model performance. Theoretical power law bounds are derived for vocabulary scaling, and ablation studies examine pretraining's impact on optimization and convergence.

## Key Results
- Normal scaling with uniform binning consistently delivers the best performance across all vocabulary sizes
- Pretrained models achieve lower MASE scores and faster convergence, particularly for challenging tokenizer configurations
- Power law bounds show diminishing returns at larger vocabularies
- Token space utilization correlates positively with performance for smaller vocabularies but negatively at larger sizes
- At smaller vocabularies, well-designed tokenizers combined with pretraining yield substantial improvements

## Why This Works (Mechanism)
The effectiveness of tokenizer design stems from its fundamental role in converting continuous time series data into discrete representations that neural networks can process. Normal scaling stabilizes the distribution of input values, making the quantization process more robust, while uniform binning ensures consistent token spacing across the value range. Pretraining provides a strong initialization that accelerates optimization, particularly when the tokenizer creates challenging or sparse representations. The correlation between token space utilization and performance reflects how efficiently the tokenizer maps input values to meaningful tokens - higher utilization at smaller vocabularies indicates better coverage, while fragmentation at larger vocabularies suggests diminishing returns from excessive token diversity.

## Foundational Learning

**Time Series Tokenization**: Converting continuous time series values into discrete tokens enables the application of transformer architectures. This is needed because transformers operate on discrete inputs, requiring a mapping from continuous values to a finite vocabulary. Quick check: Does the tokenizer preserve important patterns and relationships in the original time series?

**Scaling Strategies**: Different methods for normalizing input values (mean, min-max, normal) affect how values are distributed before quantization. This is needed to stabilize training and ensure consistent tokenization across diverse time series. Quick check: Does the chosen scaling method maintain the relative distances between important values?

**Token Space Utilization**: Measured via Cramér's V, this quantifies how efficiently tokens are used across the input space. This is needed to understand whether the tokenizer is effectively covering the input distribution or creating sparse, fragmented representations. Quick check: Is the utilization high for smaller vocabularies and low for larger ones?

**Pretraining Impact**: Initializing models with weights from pretrained networks accelerates optimization and improves convergence. This is needed because time series models benefit from transfer learning, especially when dealing with limited training data or complex tokenizations. Quick check: Does pretraining consistently improve MASE scores across different tokenizer configurations?

**Power Law Scaling**: Theoretical bounds suggest that performance gains from increasing vocabulary size follow a power law with diminishing returns. This is needed to understand the trade-offs between model capacity and computational efficiency. Quick check: Do empirical results validate the theoretical scaling relationships?

## Architecture Onboarding

**Component Map**: Raw time series -> Scaling function -> Quantization -> Token embedding -> Transformer encoder -> Forecasting head

**Critical Path**: The most critical components are the scaling function and quantization step, as these directly determine the quality of the discrete representation that the transformer processes. Poor scaling or quantization can create fragmented token spaces that the transformer cannot effectively learn from.

**Design Tradeoffs**: Larger vocabularies provide more expressive power but increase computational cost and can lead to sparse token distributions. The choice of scaling method affects numerical stability, while quantization strategy determines token spacing. Pretraining offers faster convergence but requires careful learning rate scheduling.

**Failure Signatures**: Token fragmentation (low utilization in large vocabularies), poor scaling leading to skewed distributions, and mismatched pretraining weights can all result in degraded performance. Models may show high MASE scores and unstable training curves.

**First Experiments**:
1. Compare MASE scores across the three scaling methods (mean, min-max, normal) with uniform quantization
2. Test pretraining impact by initializing models with Qwen 3 weights versus random initialization
3. Measure token space utilization using Cramér's V for each vocabulary size

## Open Questions the Paper Calls Out
The paper identifies several areas for future research, including exploring alternative tokenizer architectures beyond the evaluated scaling and quantization strategies, validating the theoretical power law bounds across diverse datasets, and investigating more sophisticated token space utilization metrics that capture domain-specific characteristics of time series data.

## Limitations
- The evaluation focuses on specific scaling and quantization strategies without exploring the full design space of tokenizer architectures
- Power law bounds for vocabulary scaling are theoretical rather than empirically validated across diverse datasets
- Token space utilization analysis using Cramér's V may not capture all relevant aspects of tokenizer effectiveness for specialized domains

## Confidence

**Major Claim Confidence Labels:**
- Tokenizer configuration primarily governs representational capacity and stability: **High**
- Pretraining influences optimization efficiency: **High**
- Normal scaling with uniform binning consistently delivers best performance: **Medium** (dataset-dependent)
- Power law scaling bounds for vocabulary size: **Low** (theoretical derivation)
- Token space utilization correlates with performance: **Medium** (complex relationship observed)

## Next Checks
1. Test the scaling and quantization strategies on additional diverse time series datasets (e.g., medical, financial, industrial sensors) to validate generalizability beyond the current benchmark sets
2. Explore alternative tokenizer architectures and binning strategies (e.g., adaptive, data-driven approaches) to establish whether the identified optimal configurations are truly fundamental or dataset-specific
3. Conduct ablation studies on pretraining initialization versus architecture design to quantify the relative contributions of each component to the observed performance improvements