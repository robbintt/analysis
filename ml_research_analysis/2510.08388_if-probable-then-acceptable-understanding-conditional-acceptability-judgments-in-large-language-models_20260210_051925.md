---
ver: rpa2
title: If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments
  in Large Language Models
arxiv_id: '2510.08388'
source_url: https://arxiv.org/abs/2510.08388
tags:
- conditional
- probability
- relation
- vanilla
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models (LLMs) judge\
  \ the acceptability of conditional statements like \u201CIf A, then B,\u201D a fundamental\
  \ aspect of human reasoning. Prior research has focused on LLMs\u2019 ability to\
  \ draw inferences from conditionals, but little is known about how they assess their\
  \ plausibility."
---

# If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models

## Quick Facts
- arXiv ID: 2510.08388
- Source URL: https://arxiv.org/abs/2510.08388
- Reference count: 37
- LLMs judge conditional acceptability based on conditional probability and semantic relevance, but smaller models show stronger relevance sensitivity than larger ones.

## Executive Summary
This study investigates how large language models assess the acceptability of conditional statements like "If A, then B," examining whether their judgments align with human reasoning patterns. Using four models (Llama 3.1 and Qwen 2.5 at 7B/70B scales), the researchers tested three judgment types (conditional probability, if-probability, and acceptability) across different prompting strategies with a structured dataset from human judgment studies. They found that LLMs are sensitive to both conditional probability and semantic relevance, though this sensitivity varies by model family, size, and prompting technique. Larger models don't necessarily align better with human trends, and few-shot prompting can introduce biases toward negative semantic relations. While LLMs broadly mimic human patterns, their integration of probabilistic and semantic cues is less systematic and robust than in humans, raising questions about the nature of LLM reasoning in conditional judgment tasks.

## Method Summary
The study evaluated four LLM families (Llama-3.1-8B/70B-Instruct, Qwen2.5-7B/72B-Instruct) using a dataset of 144 conditional statements from human judgment studies. Three tasks were tested: conditional probability estimation (P(B|A)), if-probability, and acceptability ratings. Three prompting strategies were employed: vanilla zero-shot, few-shot with 3 examples, and chain-of-thought (for 70B/72B models). Models generated single integer outputs (0-100) across 5 sampling iterations per item. Linear mixed-effects models analyzed the relationship between judgments and predictors (conditional probability, semantic relevance types) with random intercepts for scenario and instance. ANOVA tests and pairwise comparisons assessed statistical significance.

## Key Results
- LLMs show sensitivity to both conditional probability and semantic relevance, with judgments increasing as probability rises and varying by relation type
- Smaller models (7-8B) exhibit stronger and more consistent relevance sensitivity than larger models (70B+)
- Few-shot and chain-of-thought prompting introduced bias toward negative relations, reversing the probability-relevance patterns seen in smaller vanilla models
- Model size doesn't predict better alignment with human judgment patterns; larger models sometimes show less systematic integration of relevance cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are sensitive to conditional probability P(B|A) when judging acceptability, with judgments increasing monotonically as probability rises.
- Mechanism: The models encode probabilistic relationships between antecedents and consequents through their training on natural language, enabling them to estimate how likely B becomes given A, even when the semantic connection is weak or absent.
- Core assumption: Models have internalized statistical regularities from pretraining that approximate conditional probability estimation.
- Evidence anchors:
  - [abstract] "we find that models are sensitive to both conditional probability and semantic relevance"
  - [section 4] "judgments increase with rising conditional probability... ANOVA results confirm a highly significant main effect of conditional probability across all models (p <0.0001)"
  - [corpus] No direct corpus support; related work on linguistic acceptability judgments (QFrCoLA, grammar prompting) focuses on syntactic rather than probabilistic evaluation.
- Break condition: If models relied purely on P(B|A), they would rate "If Mark wears socks, then his TV works" as highly acceptable (since both are probable independently), but they do not—suggesting the mechanism is incomplete alone.

### Mechanism 2
- Claim: LLMs detect semantic relevance—the evidential connection between antecedent and consequent—and modulate acceptability judgments accordingly.
- Mechanism: Models distinguish between positive (supporting), negative (contradicting), and irrelevant relations, assigning higher ratings to conditionals where A causally or logically supports B. Smaller models (8B, 7B) show stronger and more consistent relevance sensitivity than larger models (70B, 72B).
- Core assumption: Relevance detection emerges from learned representations of causal and logical relationships in training data.
- Evidence anchors:
  - [abstract] "the semantic relevance of the antecedent A given the consequent B (i.e., whether A meaningfully supports B)"
  - [section 4] "Llama 8B (vanilla) exhibits significant slope differences between relation types... unlike Llama 70B (vanilla), which shows virtually no influence of relevance"
  - [corpus] Related work on grammatical acceptability (Explain-then-Process) shows LLMs can explain rules but fail to apply them, suggesting a gap between knowledge and systematic application.
- Break condition: If models fully integrated relevance, their ratings for irrelevant conditionals should approach those of negative ones consistently; instead, models show high variability and sometimes conflate these categories.

### Mechanism 3
- Claim: Few-shot and chain-of-thought prompting bias LLMs toward privileging negative relations, altering their probability-relevance integration.
- Mechanism: Prompting examples prime models to weight certain relation types more heavily. The unbalanced selection of examples (randomly chosen without stratification by relation type) appears to shift slope patterns: negative relations show steeper probability slopes than positive ones in prompted conditions, reversing the pattern seen in smaller vanilla models.
- Core assumption: The few-shot examples inadvertently emphasized negative or contrasting relationships through random selection.
- Evidence anchors:
  - [abstract] "few-shot and chain-of-thought prompting introduced bias toward negative relations"
  - [section 4, Table 2] "all few-shot variants produce steeper interaction slopes for negatively related conditionals than for positively related ones (e.g., Llama 70B (few-shot): 0.20, p <0.001)"
  - [corpus] Prompt sensitivity is documented (Sclar et al., 2023 cited in limitations), but no corpus papers directly address conditional acceptability prompting.
- Break condition: If prompts were balanced across relation types (e.g., 3 examples spanning POS/NEG/IRR), the bias should diminish or disappear—this is testable.

## Foundational Learning

- **Conditional probability P(B|A)**:
  - Why needed here: This is the primary predictor in the study. Understanding how P(B|A) differs from joint probability P(A ∧ B) and from the probability of the conditional statement itself is essential for interpreting the findings.
  - Quick check question: Given P(A) = 0.8 and P(B|A) = 0.6, what is P(A ∧ B)?

- **Evidentiality hypothesis vs. conditional probability hypothesis**:
  - Why needed here: The paper tests two competing accounts of human acceptability judgments. The conditional probability hypothesis claims acceptability ≈ P(B|A); the evidentiality hypothesis claims semantic relevance adds an independent constraint.
  - Quick check question: Why might "If it's 2025, then Paris is in France" be judged unacceptable despite P(B|A) ≈ 1?

- **Linear mixed-effects models**:
  - Why needed here: The analysis uses LMEMs to partition variance from fixed effects (probability, relation type) vs. random effects (scenarios, sampling iterations).
  - Quick check question: In an LMEM analyzing acceptability judgments, why include "scenario" as a random effect rather than a fixed effect?

## Architecture Onboarding

- **Component map**:
  - Input layer: Contextual scenario + conditional statement (full or split format)
  - Task variants: (i) conditional probability P(B|A), (ii.a) if-probability, (ii.b) acceptability
  - Prompting: Vanilla (zero-shot) → Few-shot (3 examples) → CoT (reasoning chain)
  - Output: Single integer 0–100 (regex-extracted)
  - Analysis: LMEM → ANOVA → EMM comparisons

- **Critical path**:
  1. Prompt construction with scenario context + conditional statement
  2. Model inference (5 sampling iterations per item for consistency)
  3. Numerical extraction via regex
  4. Mean-centering (−0.5 to 0.5 scale)
  5. LMEM fitting with random intercepts for scenario and instance
  6. ANOVA Type III tests + pairwise slope comparisons

- **Design tradeoffs**:
  - Direct elicitation vs. sentence probability: Authors chose direct elicitation (ask model to output a number) over token likelihood, trading theoretical purity for better calibration in instruct models (Tian et al., 2023).
  - Model selection: Two families (Llama, Qwen) at two scales (~7B, ~70B) enable size/architecture comparisons but limit generalization to other families.
  - Prompt balance: Few-shot examples were randomly selected without stratification—simpler but introduced bias.

- **Failure signatures**:
  - Spiked distributions: Llama 70B (vanilla) outputs cluster at extremes (near 0 or 100) rather than spreading continuously.
  - Collapsed distinctions: At low conditional probability, models fail to distinguish irrelevant from negative relations.
  - Negativity bias: Few-shot/CoT conditions show steeper slopes for NEG than POS, reversing human-like patterns.

- **First 3 experiments**:
  1. **Balanced few-shot prompting**: Construct 9 examples stratified by relation type × probability level to test whether prompt bias is causal.
  2. **Base vs. instruct comparison**: Run vanilla prompts on non-instruct checkpoints to isolate the effect of instruction tuning on acceptability judgments.
  3. **Qualitative reasoning analysis**: Instead of single-number outputs, require models to generate free-text justifications; analyze whether high/low ratings reflect probabilistic vs. relevance-based reasoning.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do similar acceptability ratings in LLMs and humans reflect similar underlying reasoning strategies, or merely coincidental alignment?
  - Basis in paper: [explicit] Section 7 states that "numerical ratings may obscure important distinctions in underlying reasoning" and suggests "qualitative analysis" to determine if ratings reflect "genuinely similar reasoning."
  - Why unresolved: This study relied solely on scalar numerical outputs, which lack the explanatory power to distinguish between matching reasoning processes and superficial output alignment.
  - What evidence would resolve it: Comparative analysis of human think-aloud protocols against LLM chain-of-thought reasoning traces for the same conditional statements.

- **Open Question 2**: Does employing balanced few-shot examples stratified by relation type eliminate the bias toward negative relations observed in LLM judgments?
  - Basis in paper: [explicit] Section 7 notes that random selection of examples may have biased models toward negative relations and suggests "future work should take greater care... [using] a balanced set of few-shot examples."
  - Why unresolved: The observed negativity bias in few-shot and chain-of-thought conditions was a consistent artifact across models, but the specific remedy (balanced examples) was not tested.
  - What evidence would resolve it: Ablation experiments comparing current random prompting strategies against prompts containing equal representation of positive, negative, and irrelevant relations.

- **Open Question 3**: Why do smaller parameter models (7-8B) exhibit stronger sensitivity to semantic relevance than larger models (70B+) in conditional acceptability tasks?
  - Basis in paper: [inferred] Section 4 (Impact of Model Size) reports the counter-intuitive finding that smaller models "more consistently integrate relevance cues," while larger models like Llama 70B showed "virtually no influence of relevance."
  - Why unresolved: The paper documents this divergence but does not investigate whether it stems from differences in training data, instruction tuning, or model architecture.
  - What evidence would resolve it: Probing studies or layer-wise analyses targeting relevance detection mechanisms in small vs. large model variants.

## Limitations
- The study used instruct-tuned models exclusively, which may have been trained to prioritize probability over relevance in acceptability judgments
- Few-shot examples were randomly selected without stratification by relation type, potentially introducing systematic bias toward negative relations
- The analysis relied on single-number outputs rather than qualitative reasoning explanations, limiting understanding of underlying cognitive processes

## Confidence
- Methodological rigor: High
- Model selection and scale comparison: High
- Prompt design and implementation: Medium (due to random few-shot selection)
- Interpretation of results and human comparison: Medium

## Next Checks
1. Reproduce the LMEM analysis with the exact dataset and confirm the significance patterns for conditional probability and relation type effects
2. Implement balanced few-shot prompting (3 examples spanning POS/NEG/IRR) to test whether the negativity bias is prompt-induced
3. Compare base model outputs (non-instruct) with instruct models to isolate the effect of instruction tuning on acceptability judgments