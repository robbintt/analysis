---
ver: rpa2
title: Robust Relevance Feedback for Interactive Known-Item Video Search
arxiv_id: '2505.15128'
source_url: https://arxiv.org/abs/2505.15128
tags:
- search
- user
- feedback
- video
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inconsistent user feedback
  in interactive known-item video search (KIS) systems, where user judgments may not
  align with machine similarity measures, particularly when using complex embedding
  features. The authors propose a robust relevance feedback approach that decomposes
  user perception into multiple sub-perceptions, each represented by an independent
  embedding space.
---

# Robust Relevance Feedback for Interactive Known-Item Video Search

## Quick Facts
- **arXiv ID:** 2505.15128
- **Source URL:** https://arxiv.org/abs/2505.15128
- **Reference count:** 40
- **Primary result:** >60% success rate moving targets from ranks 10-50 to rank-1 in 7 iterations; >40% from ranks 1,000-5,000

## Executive Summary
This paper tackles the challenge of inconsistent user feedback in interactive known-item video search (KIS), where user similarity judgments often diverge from machine similarity measures based on complex embeddings. The authors propose a robust relevance feedback approach that decomposes user perception into multiple independent embedding spaces, each representing a sub-perception. A predictive user model filters out misaligned sub-perceptions by analyzing pairwise relative judgments, allowing the system to focus on the user's true intent. Experiments on the V3C dataset demonstrate significant improvements over baseline PicHunter, especially for targets initially ranked poorly.

## Method Summary
The method decomposes user perception into multiple sub-perceptions, each represented by an independent embedding space (CLIP4Clip, ITV, BLIP). A predictive user model estimates which sub-perceptions align with user feedback by analyzing pairwise relative judgments between video pairs. The model uses a Transformer-based encoder to process query, history, and feature differences, outputting confidence scores per sub-perception. During interaction, a soft Bayesian update incorporates these scores, and search space pruning (top 5,000 candidates) is applied to improve robustness. Diverse display strategies are employed to avoid local optima and improve feedback quality.

## Key Results
- Over 60% success rate in moving targets from ranks 10-50 to rank-1 within 7 iterations
- Over 40% success rate even for targets initially ranked between 1,000-5,000
- Significantly outperforms baseline PicHunter in all tested scenarios
- Robustness improves with search space pruning and diverse display strategies

## Why This Works (Mechanism)
The approach works by explicitly modeling the gap between user perception and machine similarity. By decomposing user perception into multiple sub-perceptions, each represented by an independent embedding space, the system can adapt to individual user preferences. The predictive user model filters out misaligned sub-perceptions using confidence scores derived from pairwise relative judgments, focusing only on those that match user feedback. This allows the system to converge more reliably to the target, even when initial rankings are poor.

## Foundational Learning
- **Known-Item Search (KIS):** Task of finding a specific, previously seen video from a large corpus using only a text query. *Why needed:* Defines the core problem space. *Quick check:* Can you describe a scenario where you'd use KIS?
- **Relevance Feedback:** Iterative process where users provide feedback on retrieved results to improve future rankings. *Why needed:* Core interaction mechanism. *Quick check:* How does relevance feedback differ from one-shot search?
- **Embedding Spaces:** Vector representations of videos (e.g., CLIP4Clip, ITV, BLIP) used for similarity computation. *Why needed:* Foundation for machine similarity. *Quick check:* What's the difference between text-to-video and video-to-video retrieval?
- **User Simulator:** Model that mimics human feedback for training and evaluation. *Why needed:* Enables scalable experimentation without human subjects. *Quick check:* How is majority voting used to simulate user feedback?
- **Predictive User Model:** Transformer-based model predicting which sub-perceptions align with user feedback. *Why needed:* Filters out misaligned sub-perceptions. *Quick check:* What input features does the predictive model use?
- **Search Space Pruning:** Strategy of focusing on top-N candidates to reduce noise and improve efficiency. *Why needed:* Improves robustness and computational efficiency. *Quick check:* What's the risk of aggressive pruning?

## Architecture Onboarding
- **Component Map:** Query -> Text-to-Video Retrieval -> Initial Probability Distribution -> Display Strategy -> User Feedback -> Predictive Model -> Confidence Scores -> Bayesian Update -> New Probability Distribution -> (Loop)
- **Critical Path:** Text-to-video retrieval → Display strategy (Greedy/Diverse) → User feedback simulation → Predictive model → Confidence scores → Soft Bayesian update → Re-ranking
- **Design Tradeoffs:** Multiple sub-perceptions improve robustness but increase computational cost; search space pruning improves efficiency but risks excluding the target; diverse display improves feedback quality but may increase interaction steps
- **Failure Signatures:** Target drift (pruning excludes target), convergence to local optima (greedy display shows similar videos), poor predictive model accuracy (misaligned sub-perceptions not filtered)
- **First Experiments:**
  1. Verify initial text-to-video retrieval ranks targets correctly for easy queries
  2. Test predictive model accuracy on simulated feedback data
  3. Evaluate search space pruning impact on target retention

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- User simulator based on majority voting may not accurately reflect real human feedback complexity
- Evaluation limited to synthetic queries rather than authentic user behavior
- Predictive model architecture and training details are underspecified
- Search space pruning risks permanently excluding the target from consideration
- Assumes user feedback correlates strongly with relative feature distances

## Confidence
- **Claims about improved success rates over PicHunter:** High (supported by experimental results)
- **Claims about robustness to inconsistent feedback:** Medium (depends on accuracy of user simulator and embedding features)
- **Claims about generalization to real users:** Medium (limited by simulated feedback and synthetic queries)

## Next Checks
1. Conduct user studies with real participants to compare the proposed method against PicHunter under authentic feedback conditions
2. Perform ablation studies to isolate the impact of the predictive user model, search space pruning, and diverse display strategies on overall performance
3. Test the method with additional embedding features and query types to assess robustness across different video characteristics and user intents