---
ver: rpa2
title: 'DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering
  in Low-Light Indoor Environments'
arxiv_id: '2512.24985'
source_url: https://arxiv.org/abs/2512.24985
tags:
- low-light
- image
- noise
- question
- llie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DarkEQA addresses the critical need for benchmarking vision-language
  models (VLMs) in low-light conditions, a real-world necessity often overlooked by
  existing EQA benchmarks. The core method involves creating a physics-based low-light
  image synthesis pipeline that models illumination degradation and sensor noise at
  the RAW sensor level, enabling controlled multi-level low-light conditions.
---

# DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments

## Quick Facts
- arXiv ID: 2512.24985
- Source URL: https://arxiv.org/abs/2512.24985
- Reference count: 40
- Primary result: VLMs experience significant performance degradation under low-light conditions, with sensor noise further compounding this decline

## Executive Summary
DarkEQA addresses the critical need for benchmarking vision-language models (VLMs) in low-light conditions, a real-world necessity often overlooked by existing EQA benchmarks. The core method involves creating a physics-based low-light image synthesis pipeline that models illumination degradation and sensor noise at the RAW sensor level, enabling controlled multi-level low-light conditions. This is paired with a deterministic rule-based QA generation process to ensure reproducibility and avoid data contamination. The benchmark evaluates VLMs under these degraded conditions, both with and without low-light image enhancement (LLIE) preprocessing.

## Method Summary
DarkEQA creates a physics-based low-light image synthesis pipeline operating in linear RAW space to simulate illumination degradation and sensor noise. The method uses a two-branch approach: one branch applies exposure scaling without noise injection, while the other incorporates four noise components (shot, read, row-pattern, quantization) with ISO-dependent statistics. A simplified ISP pipeline then renders these to sRGB. The benchmark employs rule-based QA generation from HM3D-Sem dataset frames to create ~9.4K multiple-choice questions across 5 question families. VLMs are evaluated at five degradation levels (L0-L5) with and without LLIE preprocessing.

## Key Results
- VLMs experience significant performance degradation under low-light conditions, with sensor noise further compounding this decline
- LLIE preprocessing can improve performance at severe degradation levels (L4-L5), it yields mixed or even negative results at moderate levels
- Color-related QA shows earliest/most severe degradation, indicating loss of chromatic information under low SNR
- VLM accuracy drops below blind-LLM baseline at L5, suggesting models ignore visual input and rely on priors

## Why This Works (Mechanism)

### Mechanism 1: Physics-Based Low-Light Degradation via RAW-Space Synthesis
- **Claim:** Modeling degradations in linear RAW space produces low-light images that more faithfully reflect real sensor behavior under illumination drop and noise
- **Mechanism:** The pipeline inverts the ISP to recover a camera-linear RAW Bayer representation, injects four physically-motivated noise components, applies exposure scaling in linear space, then re-renders via a simplified ISP to sRGB
- **Core assumption:** The unprocessing/inversion procedure sufficiently approximates the original sensor and ISP pipeline so that noise statistics injected in the synthetic RAW domain match real low-light capture characteristics
- **Evidence anchors:** [abstract] "visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise"; [Section III-A] Details the two-branch synthesis and EV levels L1-L5

### Mechanism 2: Controlled Separation of Illumination vs. Noise Factors
- **Claim:** Providing paired variants—noise-free EV-drop and EV-drop with sensor noise—enables attribution of VLM failure to illumination loss versus noise-driven SNR collapse
- **Mechanism:** For each source frame, synthesize two images: (i) linear EV scaling without noise injection and (ii) the same EV scaling after injecting four noise components in RAW
- **Core assumption:** The noise model captures the dominant noise sources under extreme low-light, and their injected statistics are representative across sensor gain settings
- **Evidence anchors:** [abstract] "DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations"

### Mechanism 3: LLIE Preprocessing with Level-Dependent Utility
- **Claim:** Low-light image enhancement preprocessing improves VLM performance at severe degradation levels (L4-L5) but can be neutral or detrimental at moderate levels (L1-L3)
- **Mechanism:** Apply a trained LLIE model as a fixed preprocessor to the synthesized low-light images before VLM inference
- **Core assumption:** The LLIE model's perceptual quality improvements correlate positively with VLM task performance for at least the severe-degradation regime
- **Evidence anchors:** [abstract] "LLIE preprocessing can improve performance at severe degradation levels (L4-L5), it yields mixed or even negative results at moderate levels"

## Foundational Learning

- **Concept:** Linear (camera) vs. non-linear (sRGB) color spaces and gamma
  - **Why needed here:** Low-light synthesis requires operating in linear space to correctly model EV scaling and additive noise before gamma re-encoding
  - **Quick check question:** If you multiply sRGB pixel values by 0.5 to simulate dimming, what physical assumption are you violating?

- **Concept:** Sensor noise taxonomy (shot, read, row, quantization) and SNR
  - **Why needed here:** The pipeline injects these four noise components with ISO-dependent statistics; understanding their origins helps diagnose failure patterns
  - **Quick check question:** Which noise component is signal-dependent vs. signal-independent, and why does SNR drop disproportionately as illumination decreases?

- **Concept:** ISP inversion ("unprocessing") for synthetic data generation
  - **Why needed here:** To synthesize realistic low-light RAW inputs from existing sRGB datasets, the pipeline must invert typical ISP operations
  - **Quick check question:** List three operations commonly applied by an ISP that must be inverted to recover a camera-linear RAW image

## Architecture Onboarding

- **Component map:** HM3D-Sem subset -> Frame extractor -> Multimodal renderer -> QA generator -> Low-light synthesis pipeline -> Evaluation harness
- **Critical path:** 1. Render well-lit frames with semantics/depth 2. Run rule-based QA generator 3. Apply low-light synthesis to generate paired L0-L5 variants 4. Optionally apply LLIE preprocessing 5. Query VLMs with multiple-choice prompts; compute accuracy
- **Design tradeoffs:** Rule-based vs. VLM-generated QA (annotation cost vs. contamination risk); Synthetic vs. real low-light capture (fidelity vs. scale); Noise-free vs. noise-injected branches (storage/compute vs. attribution)
- **Failure signatures:** VLM accuracy drops below blind-LLM baseline at L5; Color-related QA shows earliest/most severe degradation; LLIE preprocessing reduces accuracy at L1-L3
- **First 3 experiments:** 1. Run L0 evaluation across all VLMs to establish baseline; 2. Sweep L1-L5 with noise-free branch only to isolate illumination-drop impact; 3. Add noise-injected branch at L3 and L5 with and without LLIE preprocessing

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the underlying failure mechanisms causing current Low-Light Image Enhancement (LLIE) methods to degrade VLM performance at moderate degradation levels (L1-L3) while improving it at severe levels (L4-L5)?
- **Basis in paper:** [inferred] The paper observes that LLIE preprocessing yields mixed results, significantly improving accuracy at severe levels but causing performance drops at moderate levels
- **Why unresolved:** The paper demonstrates this phenomenon empirically but does not isolate whether the negative impact stems from artifact introduction, loss of semantic edge details, or distribution shifts that conflict with the VLM's training data
- **What evidence would resolve it:** An ablation study analyzing attention maps or feature extraction quality from VLMs when fed LLIE-processed images at different degradation levels

### Open Question 2
- **Question:** Can a "task-oriented" LLIE module be developed that consistently improves VLM reasoning across all low-light levels, rather than just severe ones?
- **Basis in paper:** [inferred] The authors conclude that "perceptual enhancement alone is insufficient" and explicitly state that "effective LLIE integration in VLMs requires task-oriented LLIE modules for VLM perception"
- **Why unresolved:** Current LLIE models are trained for visual quality (human preference), but the results show this heuristic fails for AI perception in moderately low light
- **What evidence would resolve it:** A new LLIE model trained with a loss function incorporating VLM embeddings or EQA accuracy, demonstrating positive gains across the L1-L5 spectrum

### Open Question 3
- **Question:** Does the performance degradation observed in the synthetic benchmark accurately predict failure rates in physical real-world low-light environments?
- **Basis in paper:** [explicit] The authors explicitly state that "mitigating a potential real-to-sim gap presents another important avenue for subsequent work"
- **Why unresolved:** While the synthesis pipeline is physics-based, the input images are from the HM3D-Sem dataset (rendered/sampled)
- **What evidence would resolve it:** A comparative evaluation of VLMs on DarkEQA versus a newly collected dataset of real robot navigation logs captured under identical Lux levels

## Limitations
- The physics-based low-light synthesis pipeline relies on assumptions about ISP inversion and noise modeling that may not fully capture real sensor behavior
- The rule-based QA generation may not fully represent the diversity of natural language queries encountered in real-world scenarios
- The evaluation focuses on multiple-choice accuracy without assessing the quality of VLM reasoning processes
- The study does not investigate the impact of other common degradation factors such as motion blur or compression artifacts

## Confidence
- **High confidence:** VLMs experience significant performance degradation under low-light conditions with sensor noise as a compounding factor
- **Medium confidence:** Attribution of failure modes between illumination loss and noise-driven SNR collapse
- **Medium confidence:** Level-dependent utility of LLIE preprocessing
- **Low confidence:** Generalizability of results to real-world sensor capture

## Next Checks
1. Implement real-world low-light capture validation using a representative sensor and ISP pipeline to verify the synthetic degradation model's accuracy
2. Extend evaluation to include task-oriented LLIE models optimized for VLM features rather than perceptual quality
3. Conduct ablation studies on the rule-based QA generation to quantify the impact of template diversity and validation constraints on model performance across degradation levels