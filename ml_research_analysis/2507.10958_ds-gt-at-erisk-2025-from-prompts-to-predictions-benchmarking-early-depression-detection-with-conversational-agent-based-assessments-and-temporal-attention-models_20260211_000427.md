---
ver: rpa2
title: 'DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression
  detection with conversational agent based assessments and temporal attention models'
arxiv_id: '2507.10958'
source_url: https://arxiv.org/abs/2507.10958
tags:
- depression
- symptoms
- task
- bdi-ii
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The DS@GT team participated in two eRisk 2025 challenges, applying
  LLM-based and machine learning approaches to detect depression. For the Pilot Task,
  they engineered prompts to guide diverse LLMs in conducting BDI-II-based assessments,
  producing structured JSON outputs and analyzing symptom indicators without ground-truth
  labels.
---

# DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models

## Quick Facts
- arXiv ID: 2507.10958
- Source URL: https://arxiv.org/abs/2507.10958
- Reference count: 40
- Ranked second overall in Task 2 with DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27

## Executive Summary
The DS@GT team participated in two eRisk 2025 challenges, applying LLM-based and machine learning approaches to detect depression. For the Pilot Task, they engineered prompts to guide diverse LLMs in conducting BDI-II-based assessments, producing structured JSON outputs and analyzing symptom indicators without ground-truth labels. For Task 2, they combined a voting classifier with engineered features and a LightGBM model enhanced with MentalRoBERTa embeddings and temporal attention. Their best submission ranked second overall, achieving DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

## Method Summary
For Task 2, the team developed a LightGBM classifier enhanced with MentalRoBERTa embeddings and a custom temporal attention layer. The model processes user posts through MentalRoBERTa to generate embeddings, applies linearly increasing temporal weights (0.1 to 1.0) with sparse attention over specific embedding dimensions (indices 15, 42, 127, 256, 512), and aggregates these weighted representations for classification. For the Pilot Task, they designed system prompts to guide LLMs through BDI-II assessments, enforcing structured JSON output and cross-model agreement analysis to mitigate individual model biases.

## Key Results
- Ranked second overall in Task 2 with DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27
- Achieved 0.90 P@10 for early detection with 1 writing, but performance dropped to 0.00 for 100 writings
- Demonstrated strong internal consistency with RÂ² = 0.91 between classification level and BDI score in Pilot Task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining pre-trained mental health embeddings with a custom temporal attention layer appears to improve early detection ranking (NDCG) compared to static feature engineering.
- **Mechanism:** The architecture assigns linearly increasing weights to recent posts (recency bias) while simultaneously applying a sparse attention matrix over MentalRoBERTa embedding dimensions to highlight specific "depression indicators" (e.g., indices 15, 42, 127).
- **Core assumption:** Depression signals are cumulative and dynamic; recent posts are more predictive of current risk, and specific embedding dimensions correlate with clinical symptoms.
- **Evidence anchors:**
  - [abstract]: "LightGBM model enhanced with MentalRoBERTa embeddings and temporal attention."
  - [section]: Page 4 describes the weighting scheme (0.1 to 1.0) and sparse attention matrix indices.
  - [corpus]: SINAI at eRisk 2025 (neighbor) also utilizes Transformers for contextual detection, supporting the embedding approach.
- **Break condition:** If the "depression indicator" dimensions in the sparse attention matrix do not actually correspond to semantic meaning in the specific dataset, or if users abruptly change posting behavior.

### Mechanism 2
- **Claim:** Constraining LLM outputs to a structured BDI-II-aligned JSON schema forces the model to map conversational cues to clinical categories, resulting in high internal consistency.
- **Mechanism:** The prompt enforces a state machine ("Gathering" -> "Consolidating") and requires the LLM to explicitly output `next_step_reasoning` and `bdi_scores` per turn. This chain-of-thought forcing aligns the model's latent reasoning with the Beck Depression Inventory structure.
- **Core assumption:** The LLM has sufficiently internalized the relationship between natural language expressions (e.g., "I feel tired") and clinical severity levels (BDI scores) during pre-training.
- **Evidence anchors:**
  - [abstract]: "Prompt design methodology aligned model outputs with BDI-II criteria."
  - [section]: Page 12 shows $R^2 = 0.91$ between classification level and BDI score, indicating strong internal logic.
  - [corpus]: INESC-ID @ eRisk 2025 (neighbor) explores prompt-based approaches, validating this as a active research vector.
- **Break condition:** If the user (persona) uses sarcasm, metaphors, or cultural idioms not well-represented in the LLM's training data, the mapping to BDI scores may fail.

### Mechanism 3
- **Claim:** Cross-model agreement analysis serves as a proxy for signal clarity, where low standard deviation across models identifies unambiguous symptoms.
- **Mechanism:** By running the same prompt across Claude, GPT-4o, and Gemini, the team filters signal from noise. High agreement (low std dev) on symptoms like "suicidal thoughts" suggests clear linguistic markers, while disagreement on "appetite changes" suggests ambiguous cues.
- **Core assumption:** Convergence across diverse model architectures implies a higher probability of correct inference than any single model's output.
- **Evidence anchors:**
  - [section]: Page 13 details standard deviation analysis; Figure 6 visualizes agreement rates.
  - [corpus]: *Systematic Evaluation of Machine-Generated Reasoning* (neighbor) highlights the need for quality control in LLM labeling, supporting the use of cross-validation.
- **Break condition:** If all models share a common bias (e.g., verbosity bias noted on Page 10 for Claude), consensus will reinforce error rather than truth.

## Foundational Learning

- **Concept: Beck Depression Inventory (BDI-II)**
  - **Why needed here:** This is the target schema for the Pilot Task. The system maps unstructured chat to 21 specific symptoms and a severity score (0-63). You cannot interpret the JSON outputs without understanding these categories.
  - **Quick check question:** If a user says "I can't get out of bed," which BDI-II domain (Sadness, Work Difficulty, or Fatigue) is most likely being activated?

- **Concept: Temporal Attention Weights**
  - **Why needed here:** In Task 2, the model prioritizes recent posts. Understanding that weights scale from 0.1 (oldest) to 1.0 (newest) is critical to debugging why the model flags a user at a specific timestamp.
  - **Quick check question:** If a user posts "I'm happy today" after 100 sad posts, how would the linear recency weight (0.1 to 1.0) affect the final aggregated embedding compared to a uniform weight?

- **Concept: Structured Output / JSON Schema**
  - **Why needed here:** The Pilot Task relies on the LLM outputting valid JSON for automated parsing. The prompt enforces this to extract `classification_suggestion` and `key_symptoms`.
  - **Quick check question:** If the LLM generates a conversational filler "Sure, here is the JSON:" before the opening brace `{`, will the parser likely succeed or fail?

## Architecture Onboarding

- **Component map:**
  - **Task 2 (Detection):** Data Loader -> Text Cleaner -> MentalRoBERTa (Embeddings) + Temporal Attention Layer -> LightGBM Classifier
  - **Pilot (Chat):** Simulator (ChatGPT Custom GPT) <-> Evaluator Agent (Claude/GPT/Gemini) -> JSON Parser -> Aggregator

- **Critical path:**
  - The **Prompt Engineering** in the Pilot Task is the highest leverage component; the entire assessment logic resides in the system prompt (Appendix A).
  - The **Sparse Attention Matrix** in Task 2 is the most fragile; manually selecting indices (15, 42, 127, 256, 512) requires validation.

- **Design tradeoffs:**
  - **Manual vs. API:** The team chose manual interaction with Custom GPTs (due to platform constraints), ensuring compliance but limiting reproducibility and scalability (approx. 8 hours of manual labor).
  - **Embeddings vs. Engineered Features:** MentalRoBERTa embeddings outperformed TF-IDF/LIWC on early ranking metrics (NDCG@10) but both failed to achieve high decision precision (P/R F1 = 0.20).

- **Failure signatures:**
  - **Arithmetic Hallucination:** LLMs struggle to sum 21 item scores into a total BDI score correctly (Table 13 shows 23% to 61% correctness). *Do not trust the LLM's final sum.*
  - **Verbosity Bias:** Claude tended to score higher (avg 28) than GPT-4o (avg 11), likely due to response length correlation (Page 10).

- **First 3 experiments:**
  1. **Offload Summation:** Remove BDI summation from the LLM prompt. Instead, ask for only the 21 item scores (0-3) and calculate the total in Python to eliminate arithmetic errors.
  2. **Automate Interaction:** Wrap the Custom GPT simulator in a headless browser (e.g., Playwright) or wait for an API to enable batch testing of 100+ personas to validate statistical significance.
  3. **Ablate Attention Indices:** In Task 2, test the sparse attention matrix by zeroing out indices 15, 42, etc., to verify they actually correspond to depression features rather than acting as random noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the temporal attention model's ranking performance collapse to zero when analyzing long conversation histories (100 writings)?
- Basis in paper: [inferred] Table 3 shows the LightGBM model achieves 0.90 P@10 for 1 writing but drops to 0.00 for 100 writings.
- Why unresolved: The authors note that basic temporal features may not model risk evolution, but they do not diagnose why the specific temporal attention mechanism fails as context length increases.
- What evidence would resolve it: Ablation studies analyzing how attention weights distribute over long sequences versus short ones.

### Open Question 2
- Question: What factors drive the significant variance in depression severity scores between different LLM families?
- Basis in paper: [inferred] Section 3.4.2 highlights that GPT-4o estimates an average BDI score of 11, while Claude estimates 28 for the same personas.
- Why unresolved: The authors identify the divergence but attribute it to opaque model reasoning and a lack of ground-truth data.
- What evidence would resolve it: Benchmarking models against personas with clinically verified depression levels to identify systematic scoring biases.

### Open Question 3
- Question: Can deep learning architectures capture latent linguistic markers of depression more effectively than the current feature-engineered approaches?
- Basis in paper: [explicit] Section 4 states the team plans to "experiment with deep learning models" to address the limitations of current methods in capturing "deeper semantic nuances."
- Why unresolved: The current methodologies (Voting Classifier, LightGBM) relied on explicit features and embeddings that underperformed.
- What evidence would resolve it: Comparative performance results of deep learning models (e.g., LSTMs or fine-tuned transformers) on the Task 2 dataset.

## Limitations

- The sparse attention mechanism relies on manually selected embedding indices (15, 42, 127, 256, 512) without systematic validation of their depression-relevance
- The prompt-based approach introduces significant variability due to LLM-specific biases, with arithmetic hallucination rates ranging from 23-61% correctness
- The temporal attention model's ranking performance degrades dramatically for long conversation histories (0.00 P@10 for 100 writings)

## Confidence

- **High Confidence:** The temporal attention weighting scheme (0.1 to 1.0) is straightforward and well-implemented. The choice of MentalRoBERTa embeddings for mental health-specific contextual understanding is methodologically sound and aligns with current research trends.
- **Medium Confidence:** The cross-model agreement analysis provides useful signal filtering, though its effectiveness is limited by the potential shared biases among LLMs. The overall system architecture for Task 2 appears robust, though the sparse attention indices require further validation.
- **Low Confidence:** The manual selection of sparse attention indices and the absence of systematic validation for their depression-relevance. The claim that low standard deviation across models indicates signal clarity assumes models don't share systematic biases.

## Next Checks

1. **Validate Sparse Attention Indices:** Perform ablation studies by systematically testing different combinations of embedding indices to determine whether the current selection (15, 42, 127, 256, 512) provides optimal performance or if they're arbitrary selections that happen to work.
2. **Cross-Platform Consistency:** Test the prompt-based assessment across additional LLM providers (Anthropic Claude, Google Gemini, OpenAI GPT-4o) with the same user personas to quantify model-specific biases and establish more robust cross-model agreement thresholds.
3. **External Dataset Validation:** Apply the Task 2 model architecture (MentalRoBERTa + temporal attention) to an independent depression detection dataset to verify that the manually selected attention indices generalize beyond the eRisk 2025 dataset.