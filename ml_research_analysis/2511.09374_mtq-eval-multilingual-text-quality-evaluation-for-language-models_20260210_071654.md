---
ver: rpa2
title: 'MTQ-Eval: Multilingual Text Quality Evaluation for Language Models'
arxiv_id: '2511.09374'
source_url: https://arxiv.org/abs/2511.09374
tags:
- text
- quality
- latn
- languages
- mtq-eval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MTQ-Eval, a novel framework for multilingual
  text quality evaluation that trains models to distinguish high- and low-quality
  text using Direct Preference Optimization (DPO). The method automatically generates
  synthetic quality preference data across 115 languages without human annotations
  by creating paired normal and degraded text samples.
---

# MTQ-Eval: Multilingual Text Quality Evaluation for Language Models

## Quick Facts
- **arXiv ID**: 2511.09374
- **Source URL**: https://arxiv.org/abs/2511.09374
- **Reference count**: 11
- **Primary result**: MTQ-Eval achieves significant improvements in multilingual text quality evaluation and downstream task performance using synthetic preference data and Direct Preference Optimization

## Executive Summary
This paper introduces MTQ-Eval, a novel framework for multilingual text quality evaluation that trains models to distinguish high- and low-quality text using Direct Preference Optimization (DPO). The method automatically generates synthetic quality preference data across 115 languages without human annotations by creating paired normal and degraded text samples. Comprehensive evaluations demonstrate that MTQ-Eval significantly improves text quality assessment performance compared to baseline approaches, achieving better Matthew's Correlation Coefficient, KL divergence, and F1 scores across both high- and low-resource languages. Additionally, the enhanced text quality evaluation capability translates to improved performance in downstream tasks including sentiment analysis and summarization, particularly for low-resource languages.

## Method Summary
MTQ-Eval trains multilingual quality evaluators using synthetic preference data generated from the Belebele dataset. For each of 115 languages, the method selects 20 human-translated passages and creates degraded versions by randomly shuffling 3-6 words per passage (excluding 7 languages where word order flexibility or tokenization makes shuffling ineffective). These create preference pairs: normal text rated 1 (good) versus shuffled text rated 0 (bad). The model is fine-tuned using Direct Preference Optimization with a binary classification objective to predict quality ratings. Training uses Llama-3.1-8B or Aya-Expanse-8B with LoRA (rank=64, alpha=128) for efficient adaptation. The resulting quality evaluator is then assessed on text quality benchmarks and shows transfer benefits to downstream tasks like sentiment analysis and summarization.

## Key Results
- MTQ-Eval achieves significant improvements in Matthew's Correlation Coefficient, KL divergence, and F1 scores compared to baseline approaches across multilingual text quality evaluation
- The framework demonstrates strong performance particularly for low-resource languages, with quality assessment capability translating to improved downstream task performance
- Model shows consistent gains in both quality evaluation metrics and downstream applications including sentiment analysis and summarization across 115 languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning from synthetic quality pairs enables multilingual quality discrimination without human annotations.
- Mechanism: Word shuffling disrupts grammatical structure and semantic flow while preserving lexical content, creating minimally-differentiated pairs where the model must learn structural rather than surface features. The binary preference signal (1=normal, 0=shuffled) trains the model to attend to coherence, fluency, and syntactic well-formedness.
- Core assumption: Degradation via word shuffling produces universally "low quality" text across typologically diverse languages, independent of language-specific word order flexibility.
- Evidence anchors: [section 2.1] "This straightforward technique, generally applicable across most languages, randomly rearranges a few words within a passage, disrupting the grammatical structure and semantic flow."

### Mechanism 2
- Claim: DPO's relative ranking objective provides more stable quality alignment than absolute scoring or classification.
- Mechanism: The log-ratio loss directly optimizes preference probability without requiring a separate reward model. By learning to rank good>bad rather than classify good/bad in isolation, the model develops calibrated quality representations that generalize across languages where absolute quality thresholds may differ.
- Core assumption: The preference signal from synthetic degradation transfers to naturalistic quality judgments in real evaluation tasks.
- Evidence anchors: [section 2.2] "DPO fine-tunes the model by increasing the likelihood of preferred responses while reducing the probability of less preferred ones."

### Mechanism 3
- Claim: Quality evaluation capability transfers to downstream tasks through improved linguistic representations.
- Mechanism: Learning to distinguish coherent from incoherent text strengthens internal representations of syntactic well-formedness and semantic coherence. These representations benefit generation and classification tasks, particularly for low-resource languages where models have weaker priors.
- Core assumption: Quality assessment training improves general linguistic competence, not just evaluation-specific behavior.
- Evidence anchors: [section 5.2] "MTQ-Eval performs as well as or better than SFT across all three evaluation dimensions, and particularly in low-resource categories."

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Core training method replacing RLHF; understanding the loss function is essential for debugging convergence.
  - Quick check question: Can you explain why DPO doesn't require a reward model and how the log-ratio loss encourages preference learning?

- Concept: **Text Quality Dimensions (Coherence, Fluency, Simplicity, Linguistic Acceptability)**
  - Why needed here: Defines what the model learns to evaluate; degradation must target these dimensions.
  - Quick check question: For a given degraded sample, which quality dimension(s) does word shuffling primarily affect?

- Concept: **Low-Resource Language Challenges**
  - Why needed here: Performance gains are largest for LR languages; understanding why helps predict failure modes.
  - Quick check question: Why might a quality evaluator trained on synthetic degradation help more for Nepali than English?

## Architecture Onboarding

- Component map: Belebele dataset (115 languages) -> Word shuffling degradation (3-6 words) -> Preference pair constructor (prompt + chosen/rejected) -> DPO training loop (Llama-3.1-8B or Aya-Expanse-8B with LoRA) -> Quality evaluation inference

- Critical path: Belebele extraction → word shuffling → preference pair formatting → DPO training → inference with quality evaluation prompt

- Design tradeoffs:
  - Word shuffling is language-agnostic but doesn't capture semantic errors or factual inaccuracies
  - Binary quality labels simplify training but lose granularity
  - 20 samples per language enables scale but may underfit for some languages
  - LoRA enables efficient training but may limit capacity for learning new representations

- Failure signatures:
  - Low MCC with high F1: Model memorizing position/degradation artifacts rather than quality
  - Strong HR performance, weak LR performance: Degradation not creating valid contrast for typologically different languages
  - Negative transfer to downstream tasks: Quality representations not generalizing (check for overfitting to synthetic patterns)
  - Script-specific failures (e.g., Orya, Tamil showing negative improvement): Tokenization or encoding issues

- First 3 experiments:
  1. **Validate degradation quality**: Manually inspect shuffled samples across 5-10 typologically diverse languages to confirm degradation produces noticeably lower quality. Flag languages where shuffling is ineffective.
  2. **Ablate training data size**: Train with 10/20/50 samples per language on a subset of 5 HR + 5 LR languages to verify 20 samples is sufficient before full training.
  3. **Probe representation change**: Compare hidden states for normal vs. degraded text before and after DPO on a held-out language to verify the model learns structural rather than surface features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would language-specific degradation methods (beyond simple word shuffling) affect MTQ-Eval's performance on agglutinative languages and languages with flexible word order?
- Basis in paper: [explicit] The authors acknowledge that "some languages (e.g., agglutinative or highly inflected languages) may not exhibit noticeable degradation from simple word shuffling" and excluded 7 languages where "word order plays lesser role and spaces do not function as reliable delimiters."
- Why unresolved: The uniform word-shuffling approach may not adequately create low-quality samples for all linguistic structures, potentially affecting training quality for underrepresented language types.
- What evidence would resolve it: A comparative study using language-specific perturbation techniques (e.g., morpheme shuffling for agglutinative languages, case-marking disruption for inflected languages) evaluated on previously excluded languages like Basque and Japanese.

### Open Question 2
- Question: Would scaling MTQ-Eval to larger model sizes (70B+ parameters) yield significant improvements for low-resource languages, or is performance primarily constrained by training data quality?
- Basis in paper: [explicit] The authors state "resource constraints prevented us from experimenting with larger models."
- Why unresolved: It remains unclear whether the observed lower performance on low-resource languages stems from model capacity limitations or fundamental issues with the synthetic data approach.
- What evidence would resolve it: Experiments comparing MTQ-Eval performance across model scales (8B, 70B, 405B) specifically analyzing the performance gap between high- and low-resource languages at each scale.

### Open Question 3
- Question: Can the MTQ-Eval framework be extended to capture semantic coherence and factual accuracy dimensions of text quality, which are not addressed by word-shuffling degradation?
- Basis in paper: [explicit] The authors acknowledge their degradation methods "do not capture other dimensions of low-quality text, such as semantic errors, factual inaccuracies, or nuanced linguistic issues."
- Why unresolved: Current training data only exposes models to syntactic/structural quality issues, leaving uncertainty about generalization to semantic and factual quality assessment.
- What evidence would resolve it: Evaluation of MTQ-Eval models on datasets containing semantic errors or factual inaccuracies (e.g., hallucinated content) without additional training, followed by experiments incorporating such degradation types into the training pipeline.

## Limitations

- Word shuffling degradation method may not create sufficient quality contrast for languages with flexible word order or non-space tokenization, systematically excluding certain language types from effective evaluation
- Binary quality annotation scheme limits the model's ability to capture nuanced quality gradations, potentially missing important distinctions between different levels of text quality
- Synthetic degradation only addresses syntactic/structural quality issues, leaving uncertainty about the model's ability to detect semantic errors, factual inaccuracies, or other nuanced linguistic problems

## Confidence

- **High Confidence**: The DPO training methodology is well-established, the implementation details are clearly specified, and the improvement on the MELA benchmark (MCC gains of 0.1-0.2) is substantial and reproducible given access to Belebele
- **Medium Confidence**: The claim that word shuffling produces universally degraded quality across 115 languages is plausible but untested across all target languages
- **Low Confidence**: The mechanism by which quality evaluation training improves downstream generation tasks is speculative and not causally established

## Next Checks

1. **Degradation Quality Validation**: Manually evaluate shuffled samples across 10-15 typologically diverse languages (including those with non-space tokenization like Thai and Lao) to quantify degradation severity and identify languages where shuffling fails to create clear quality contrast

2. **Transfer Mechanism Probing**: Compare internal representations of normal vs. degraded text before and after DPO training using probing tasks that test syntactic vs. semantic vs. discourse-level features to determine which quality dimensions the model actually learns

3. **Training Data Scaling Experiment**: Systematically vary training samples per language (10, 20, 50, 100) on a subset of 5 high-resource and 5 low-resource languages to determine the relationship between training data volume and quality evaluation performance, identifying potential data saturation points