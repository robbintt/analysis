---
ver: rpa2
title: 'PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval'
arxiv_id: '2504.20624'
source_url: https://arxiv.org/abs/2504.20624
tags:
- retrieval
- user
- sigir
- generation
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PaRT, a framework that enhances social chatbots
  with personalized real-time retrieval to enable proactive dialogues. The approach
  addresses the limitations of passive chatbots by integrating user profiles and dialogue
  context into large language models to refine user queries and identify underlying
  intents.
---

# PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval

## Quick Facts
- arXiv ID: 2504.20624
- Source URL: https://arxiv.org/abs/2504.20624
- Reference count: 40
- One-line primary result: 21.77% improvement in average dialogue duration vs passive baseline

## Executive Summary
This paper introduces PaRT, a framework that enhances social chatbots with personalized real-time retrieval to enable proactive dialogues. The approach addresses the limitations of passive chatbots by integrating user profiles and dialogue context into large language models to refine user queries and identify underlying intents. Guided by these refined intents, the system generates personalized dialogue topics that serve as queries to retrieve relevant passages from RedNote, a lifestyle-sharing platform. The retrieved content is then summarized and used to generate knowledge-grounded and engagement-optimized responses. PaRT has been deployed in a real-world production environment for over 30 days, achieving a 21.77% improvement in the average duration of dialogues.

## Method Summary
PaRT integrates user profiling, intent-guided query refinement, and retrieval-augmented generation to create proactive social chatbots. The system uses an LLM to classify user intent (Natural Transition, Explicit Retrieval, or Implicit Retrieval) and rewrites queries based on dialogue context and user profiles. These refined queries retrieve relevant passages from RedNote, which are summarized by another LLM before generating the final response. The framework uses Qwen2-7B for intermediate steps (refinement, summarization) and Qwen2-72B for final response generation, with SFT fine-tuning on 11,455 samples.

## Key Results
- 21.77% improvement in average dialogue duration compared to passive baseline
- Retrieval performance validated through P@k metrics showing LLM-rewritten queries outperform raw user input
- Generation quality maintained across Personalization, Informativeness, and Communication Skills (0-3 scale)

## Why This Works (Mechanism)

### Mechanism 1: Intent-Guided Query Refinement
If user queries are rewritten based on explicit context and implicit user profiles, retrieval relevance improves significantly compared to using raw user input. An LLM analyzes the dialogue context to classify intent into one of three categories (Natural Transition, Explicit Retrieval, Implicit Retrieval). It then rewrites the query to align with user preferences before searching the database. The core assumption is that the LLM can accurately detect subtle signals of "waning interest" or implicit intent better than a keyword-based matcher.

### Mechanism 2: Personalized Proactive Initiation
Proactively initiating dialogues using user profile data rather than waiting for user input appears to increase user engagement duration. The system utilizes a static question bank and stored user profiles (preferences/history) to generate "greetings" or topic shifts. This shifts the burden of sustaining conversation from the user to the system. The core assumption is that users prefer a chatbot that initiates topics over one that purely responds, and the stored profile data is current enough to be relevant.

### Mechanism 3: Summarized Retrieval-Augmented Generation (RAG)
Summarizing retrieved passages before feeding them to the generation model likely reduces noise and improves the "informativeness" of responses compared to using raw retrieved text. After retrieving top-k passages, a separate LLM step summarizes these notes based on the query. This filtered context is then used to generate the final response. The core assumption is that the summarization model successfully filters out irrelevant information without losing key details required for the response.

## Foundational Learning

**Retrieval-Augmented Generation (RAG)** - Why needed: PaRT relies on external knowledge (RedNote) to provide up-to-date information that static LLM weights cannot possess. Quick check: How does the system decide *when* to retrieve versus using internal knowledge?

**Prompt Chaining** - Why needed: The architecture does not use a single prompt. It chains "Intent Classification" -> "Query Rewriting" -> "Summarization" -> "Response Generation". Quick check: What is the risk of error propagation across this chain of LLM calls?

**User Profiling / Memory** - Why needed: Proactivity requires a memory of past interactions to avoid repetitive questions and personalize topic selection. Quick check: How does the system update the user profile over time (Section 2.1 mentions "continuously updated")?

## Architecture Onboarding

Component map: User Profiling -> Intent-Guided Query Refiner -> Retriever -> Summarizer -> Generator

Critical path: The **Intent-Guided Query Refiner** is the bottleneck; if the query rewrite is poor, the subsequent retrieval and generation will fail.

Design tradeoffs: The system uses a smaller model (7B) for intermediate steps (refining, summarizing) and a larger model (72B) for final generation to balance latency and quality.

Failure signatures:
- High latency: Cumulative latency from multi-step LLM calls (Refine -> Retrieve -> Summarize -> Generate)
- Hallucination: If the summarizer misses a key fact, the generator may invent details to bridge the gap

First 3 experiments:
1. Offline Retrieval Validation: Compare P@k of raw user queries vs. LLM-rewritten queries to validate the Refiner
2. Ablation on Retrieval Quantity: Test generation quality with k=1, 3, 5, 10 retrieved notes to find the "noise vs. info" threshold
3. Online A/B Testing: Deploy PaRT against a passive baseline in production to measure Average Dialogue Duration

## Open Questions the Paper Calls Out

Does increased dialogue duration directly correlate with higher user satisfaction, or does it potentially reflect user confusion or a failure to meet user needs efficiently? While the paper reports a 21.77% increase in duration, it does not provide data on user retention, explicit satisfaction ratings, or negative feedback rates. A longer duration could theoretically result from users struggling to end the conversation or the chatbot being verbose.

How robust is the intent-guided query refiner when retrieving from heterogeneous or noisy web sources compared to the curated RedNote platform? The system's retrieval performance (P@k) and summarization steps are optimized for a specific lifestyle domain. It is unclear if the query refinement logic holds up when the knowledge source contains conflicting information, significantly different jargon, or lower-quality content found on the open web.

What is the end-to-end latency overhead of the sequential PaRT pipeline, and how does this delay impact the perceived "real-time" nature of proactive interactions? The framework chains multiple LLM calls (intent refinement, summarization, generation) with an external retrieval step. Proactive chats require immediate engagement; significant delays in this complex pipeline could degrade user experience despite improved content quality.

## Limitations

- The lack of prompt templates for intent classification, query rewriting, and summarization requires prompt engineering during reproduction
- Reliance on RedNote as the retrieval corpus raises questions about generalizability to different domains or data sources
- No specific latency metrics are reported for the multi-step pipeline, despite mentioning latency-cost tradeoffs

## Confidence

High Confidence: The core architecture (intent classification → query rewriting → retrieval → summarization → generation) is clearly defined, and the reported 21.77% improvement in average dialogue duration is a concrete, measurable outcome.

Medium Confidence: The effectiveness of intent-guided query refinement and personalized proactive initiation is supported by the reported results, but the lack of prompt details and retrieval corpus specifics limits reproducibility.

Low Confidence: The claim about the effectiveness of the summarization step in reducing noise is weakly supported, as the paper does not provide evidence for this specific mechanism.

## Next Checks

1. Test the intent classification and query rewriting modules with a held-out validation set to ensure accurate intent detection and personalized query generation. Inspect rewritten queries for clarity and relevance.

2. Conduct an ablation study varying the number of retrieved passages (k=1, 3, 5, 10) to identify the optimal balance between information and noise. Measure Precision@k and generation quality.

3. Deploy the system on a different retrieval corpus (e.g., a web search API or a curated lifestyle-article database) to assess the robustness of the approach outside the RedNote domain.