---
ver: rpa2
title: Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits with Known
  Optimality Gap
arxiv_id: '2502.01067'
source_url: https://arxiv.org/abs/2502.01067
tags:
- algorithm
- arms
- sample
- bound
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the sample-memory-pass trade-offs for\
  \ pure exploration in multi-pass streaming multi-armed bandits (MABs) when the optimality\
  \ gap \u0394[2] is known a priori. Prior work showed that without knowing \u0394\
  [2], \u0398(log(1/\u0394[2])) passes are necessary and sufficient to achieve the\
  \ worst-case optimal sample complexity of O(n/\u0394\xB2[2]) with single-arm memory."
---

# Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits with Known Optimality Gap

## Quick Facts
- arXiv ID: 2502.01067
- Source URL: https://arxiv.org/abs/2502.01067
- Authors: Nikolai Karpov; Chen Wang
- Reference count: 40
- Primary result: Establishes tight bounds for pure exploration in streaming MABs with known optimality gap, showing Θ(log(n)/log log(n)) passes are necessary and O(log(n)) sufficient for instance-optimal sample complexity

## Executive Summary
This paper investigates the fundamental trade-offs between sample complexity, memory usage, and number of passes in streaming multi-armed bandit (MAB) problems when the optimality gap Δ[2] is known a priori. The authors address a key open problem in the field: determining how many passes are required to achieve instance-optimal sample complexity with sublinear memory. They establish both lower and upper bounds that are nearly tight, showing that Ω(log(n)/log log(n)) passes are necessary and O(log(n)) passes are sufficient to achieve O(∑i=2n 1/Δ²[i] · log(n)) sample complexity.

The paper introduces a novel lower bound framework for batched instance distributions and develops an elimination-based algorithm with geometrically increasing elimination gaps. The results provide a complete characterization of the sample-pass-memory trade-off for pure exploration in streaming MABs, generalizing classical multi-pass MAB algorithms to the streaming setting with known optimality gap.

## Method Summary
The paper presents two main results: a lower bound and an upper bound algorithm. For the lower bound, the authors develop a novel framework for batched instance distributions, constructing hard instances with two special arms per batch that demonstrate the Ω(log(n)/log log(n)) passes requirement. The upper bound algorithm uses an elimination-based approach where arms are eliminated in each pass based on geometrically increasing elimination gaps (ϵp = Δ[2] · n^(1-p/P) for the p-th pass). This algorithm generalizes classical multi-pass MAB algorithms to the streaming setting and achieves O(∑i=2n 1/Δ²[i] · log(n)) sample complexity with O(log(n)) passes and single-arm memory.

## Key Results
- Lower bound: Ω(log(n)/log log(n)) passes required for any algorithm achieving O(∑i=2n 1/Δ²[i] · log(n)) sample complexity with o(n/polylog(n)) memory
- Upper bound: O(∑i=2n 1/Δ²[i] · log(n)) sample complexity achievable with O(log(n)) passes and single-arm memory
- Sample-pass trade-off: P passes achieve O(∑i=2n 1/Δ²[i] · n^(2/P) · log(nP)) sample complexity
- Novel lower bound framework for batched instance distributions with two special arms per batch

## Why This Works (Mechanism)
The mechanism relies on carefully designed instance distributions and elimination strategies. The lower bound construction uses batched distributions where each batch contains two special arms with specific gap properties, forcing any algorithm to use many passes to distinguish between them. The upper bound algorithm exploits the known optimality gap by using geometrically increasing elimination gaps, allowing it to eliminate suboptimal arms efficiently across multiple passes while maintaining instance-optimal sample complexity.

## Foundational Learning
- Multi-armed bandits (MABs): Sequential decision-making problems where an agent selects arms to maximize cumulative reward
  - Why needed: Forms the fundamental problem setting being analyzed
  - Quick check: Can you explain the explore-exploit trade-off in MABs?

- Streaming algorithms: Algorithms that process data sequentially with limited memory
  - Why needed: The paper focuses on streaming MABs with sublinear memory
  - Quick check: What's the difference between single-pass and multi-pass streaming algorithms?

- Instance-optimal sample complexity: Sample complexity that depends on problem-specific parameters rather than worst-case bounds
  - Why needed: The paper aims to achieve instance-optimal rather than worst-case optimal complexity
  - Quick check: How does instance-optimal complexity differ from worst-case optimal complexity?

- Batched instance distributions: Distributions where arms are grouped into batches with specific properties
  - Why needed: Key technique for constructing lower bound instances
  - Quick check: Why would batching arms help in proving lower bounds?

## Architecture Onboarding

Component Map:
Streaming MAB Algorithm -> Passes -> Arm Elimination -> Sample Complexity
Lower Bound Framework -> Batched Distributions -> Special Arms -> Pass Lower Bound

Critical Path:
1. Initialize all arms as active
2. For each pass p = 1 to P:
   - Sample each active arm with geometrically increasing gaps ϵp = Δ[2] · n^(1-p/P)
   - Eliminate arms with statistically significant evidence of being suboptimal
3. Return the single remaining arm as the best arm

Design Tradeoffs:
- Memory vs. Passes: Single-arm memory requires more passes (O(log n)) vs. O(log log n) with linear memory
- Known vs. Unknown Δ[2]: Known gap enables instance-optimal complexity; unknown gap requires worst-case bounds
- Sample complexity vs. Passes: Trade-off parameterized by P in the upper bound

Failure Signatures:
- Suboptimal arms not eliminated: Insufficient sampling or incorrect gap estimates
- Best arm eliminated: Statistical error or gap estimation error
- Memory overflow: Algorithm not maintaining single-arm memory constraint

First Experiments:
1. Implement the upper bound algorithm and test on synthetic instances with varying gap configurations
2. Verify the lower bound by attempting to design algorithms that break the Ω(log(n)/log log(n)) pass requirement
3. Test the sample-pass trade-off by running the algorithm with different numbers of passes P

## Open Questions the Paper Calls Out
None explicitly stated in the provided information.

## Limitations
- Lower bound construction relies on specific batched distributions with two special arms per batch, which may not capture all hard instances
- Theoretical results assume known optimality gap Δ[2], with generalization to unknown Δ[2] only mentioned but not fully explored
- Practical implications and empirical validation are limited, focusing primarily on theoretical worst-case bounds
- The framework may not extend to all variations of streaming MAB problems beyond the specific pure exploration setting

## Confidence
High confidence in:
- Lower bound result (Ω(log(n)/log log(n)) passes required)
- Upper bound algorithm (O(∑i=2n 1/Δ²[i] · log(n)) sample complexity with O(log(n)) passes)
- Sample-pass trade-off characterization

Medium confidence in:
- Practical applicability of theoretical bounds
- Extension to unknown Δ[2] settings

## Next Checks
1. Implement the upper bound algorithm and test on synthetic instances to verify the claimed sample complexity and pass count empirically, particularly for different gap configurations.

2. Extend the lower bound framework to unknown Δ[2] settings to validate whether the log(1/Δ[2]) barrier can indeed be broken as suggested by the generalization discussion.

3. Analyze the algorithm's behavior on real-world datasets to assess practical performance beyond the theoretical worst-case bounds, especially focusing on memory efficiency and pass requirements.