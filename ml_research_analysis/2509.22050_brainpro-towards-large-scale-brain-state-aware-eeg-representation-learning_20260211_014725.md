---
ver: rpa2
title: 'BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning'
arxiv_id: '2509.22050'
source_url: https://arxiv.org/abs/2509.22050
tags:
- brainpro
- brain
- spatial
- learning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BrainPro, a large EEG model that explicitly
  models spatial interactions and learns brain-state-aware representations. It uses
  a retrieval-based spatial learning block to flexibly capture channel- and region-level
  dependencies across varying electrode montages, and a brain-state decoupling block
  with parallel encoders to disentangle shared and process-specific representations.
---

# BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning

## Quick Facts
- **arXiv ID**: 2509.22050
- **Source URL**: https://arxiv.org/abs/2509.22050
- **Reference count**: 40
- **Primary result**: BrainPro achieves SOTA performance on 9 BCI datasets (emotion, motor, etc.) with flexible spatial learning and brain-state-aware representations.

## Executive Summary
This paper introduces BrainPro, a large-scale EEG foundation model designed to learn brain state-aware representations with explicit spatial modeling. It addresses the challenges of heterogeneous electrode montages and task-specific EEG dynamics through a retrieval-based spatial learning block and a brain-state decoupling architecture with parallel encoders. Pre-trained on a large corpus of multi-task EEG data, BrainPro consistently outperforms both traditional and foundation-model baselines across diverse BCI datasets, demonstrating superior generalization and adaptability via flexible encoder fusion at fine-tuning.

## Method Summary
BrainPro uses a dual-loss pre-training strategy: region-aware masked signal reconstruction and brain-state decoupling with margin-based cosine loss. The model features one shared encoder and three state-specific encoders (affect, motor, other), each processing 10s EEG clips at 200 Hz. Spatial dependencies are captured via a retrieval-based block that maps any input montage to a universal 60-channel template using learnable channel/region filter banks. During fine-tuning, any subset of encoders can be concatenated for classification, enabling flexible adaptation to tasks with overlapping brain processes.

## Key Results
- Achieves SOTA balanced accuracy, kappa, and F1 across 9 public BCI datasets spanning 6 task types.
- Retrieval-based spatial learning enables consistent performance across heterogeneous electrode montages.
- Brain-state decoupling with parallel encoders improves generalization by learning disentangled shared and task-specific representations.
- Flexible encoder fusion at fine-tuning further boosts performance on multi-task benchmarks.

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-based spatial learning for montage generalization
A universal 60-channel template with learnable channel/region filter banks retrieves relevant spatial filters for any input montage via channel-name or coordinate matching, producing consistent spatial representations across datasets.

### Mechanism 2: Brain-state decoupling with parallel encoders
Separate shared and state-specific encoders learn disentangled representations via selective gradient updates and a margin-based cosine decoupling loss that penalizes high similarity between shared/state-specific and between different state encoders.

### Mechanism 3: Flexible encoder fusion at fine-tuning
Downstream tasks can select and concatenate outputs from any subset of encoders (shared + one or more state-specific), allowing mixture-of-experts adaptation without retraining the full architecture.

## Foundational Learning

- **EEG Signal Characteristics (Low SNR, Non-Stationarity, Montage Variability)**: BrainPro's design directly addresses these challenges—spatial retrieval for montage flexibility, masked reconstruction for denoising, and large-scale pre-training for robustness.
  - *Quick check*: Can you explain why fixed-channel models struggle with heterogeneous EEG datasets?

- **Self-Supervised Learning in Biosignals (Masked Modeling, Contrastive/Reconstruction Objectives)**: BrainPro uses masked signal reconstruction as its primary pre-training objective, similar to MAE in vision.
  - *Quick check*: What does the model learn to reconstruct, and why is masking applied?

- **Representation Disentanglement and Multi-Task Learning**: The brain-state decoupling block aims to separate shared and process-specific features.
  - *Quick check*: How does the decoupling loss differ from a standard contrastive loss, and what failure modes might it have?

## Architecture Onboarding

- **Component map**: Temporal CNN -> Retrieval-based Spatial Block -> Patchification & Token Embedding -> Transformer Encoder -> Parallel Encoders (Shared + 3 State-Specific) -> Reconstruction Head / Fine-tuning Head

- **Critical path**: Map input channels to universal template → apply masking → forward through all encoders (backprop only through active ones) → compute region-weighted reconstruction + decoupling loss → select encoder subset at fine-tuning and concatenate outputs for classification

- **Design tradeoffs**: Montage flexibility vs. template bias (SEED-based 60-channel template may not generalize to clinical montages); disentanglement vs. capacity (strong decoupling may limit shared encoder capacity); model size vs. efficiency (7.69M params, 531M FLOPs—heavier than EEGNet but lighter than EEGPT)

- **Failure signatures**: Spatial retrieval errors (channel mapping fails → misaligned spatial features); decoupling collapse (margin too small or loss weight insufficient → representation mixing); overfitting in encoder selection (small downstream data → encoder subset selection overfits)

- **First 3 experiments**: 1) Spatial retrieval validation: test on novel montage (e.g., 19-channel clinical) with/without proper channel mapping; 2) Decoupling ablation: compare variants with no decoupling, weaker margin, stronger margin on tasks requiring different brain states; 3) Encoder fusion analysis: systematically evaluate all encoder subsets on multi-task benchmark, measure consistency of gains

## Open Questions the Paper Calls Out

- Can finer-grained or dynamically discovered brain state encoders improve representation quality and downstream performance beyond the current coarse taxonomy (affect, motor, others)?
- How does selective gradient updating affect optimization stability when brain states are unequally represented in pre-training data?
- Can BrainPro be effectively compressed (via pruning, distillation, or parameter sharing) to support real-time, resource-constrained BCI deployment?
- How does incorporating multimodal neurophysiological signals (e.g., fNIRS, MEG, eye tracking) affect BrainPro's learned representations and cross-modal transfer capabilities?

## Limitations

- The 60-channel universal template based on SEED may not generalize to substantially different montages (e.g., clinical 19-channel setups).
- The three-state taxonomy (affect, motor, others) is coarse and may not capture finer distinctions like attention or memory processing.
- BrainPro remains computationally heavier than lightweight CNN-based models, potentially limiting real-time or resource-constrained BCI deployment.

## Confidence

- **High**: EEG foundation model improves downstream BCI performance vs. traditional methods (SOTA on 9 datasets).
- **Medium**: Brain-state decoupling improves generalization via disentangled representations (mechanism plausible but not directly validated).
- **Medium**: Retrieval-based spatial learning generalizes across montages (logic sound but empirical validation limited to similar setups).
- **Low**: Encoder fusion at fine-tuning consistently outperforms single-encoder adaptation (ablation in Appendix K, but not across all tasks).

## Next Checks

1. **Montage Generalization**: Evaluate BrainPro on a dataset with a substantially different montage (e.g., 19-channel clinical) compared to the SEED-based template. Compare performance with and without manual channel remapping to isolate the retrieval mechanism's contribution.

2. **Representation Disentanglement**: Visualize encoder outputs (e.g., t-SNE) for samples from affect, motor, and other tasks. Compute intra- and inter-state similarity matrices to directly assess whether the decoupling loss produces separable representations.

3. **Encoder Selection Robustness**: On a multi-task benchmark (e.g., emotion + motor), perform k-fold cross-validation over all encoder subsets. Report mean ± std of validation accuracy to test whether encoder fusion gains are consistent or due to overfitting on a single validation split.