---
ver: rpa2
title: Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute
  Constraints
arxiv_id: '2509.18382'
source_url: https://arxiv.org/abs/2509.18382
tags:
- reasoning
- performance
- safety
- compute
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how compute constraints affect the safety performance
  of reasoning models. It explores two methods: (1) Length Controlled Policy Optimization
  (LCPO), a reinforcement learning-based method enabling user-defined reasoning length
  control, and (2) weight quantization, which reduces compute demands and enables
  execution within user-defined inference time budgets.'
---

# Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints

## Quick Facts
- arXiv ID: 2509.18382
- Source URL: https://arxiv.org/abs/2509.18382
- Reference count: 5
- This paper studies how compute constraints affect safety performance of reasoning models using LCPO for reasoning length control and quantization to reduce compute demands.

## Executive Summary
This paper evaluates how compute constraints impact safety performance in reasoning models through two complementary approaches: Length Controlled Policy Optimization (LCPO) for reasoning length control and weight quantization for efficiency. The key finding is that quantized models can achieve comparable safety performance to full-precision models by generating more reasoning tokens within fixed compute budgets, effectively compensating for quantization-induced quality loss. Specifically, an 8-bit quantized model reasoning for 2048 tokens achieves similar safety performance to a full-precision model reasoning for 1024 tokens, but with a 16.4% lower compute budget.

## Method Summary
The method combines LCPO-based reinforcement learning with post-training quantization to create compute-efficient reasoning models that maintain safety performance. LCPO fine-tunes models using a composite reward function combining safety scores from Llama-Guard-3 and length penalties, with prompts augmented to specify target reasoning lengths. Models are then quantized using GPTQ to INT8 and INT4 weights (activations remain 16-bit). The approach is evaluated on L1-1.5B and L1-8B reasoning models across multiple benchmarks including GPQA, MATH500, AIME2025, and StrongReject (jailbreak queries), measuring both skill (pass@1) and safety (safe@1) performance across varying reasoning token budgets.

## Key Results
- Quantized models can match full-precision safety performance within fixed compute budgets by generating more tokens
- An 8-bit quantized model reasoning for 2048 tokens achieves similar safety performance to full-precision at 1024 tokens with 16.4% lower compute budget
- INT4 quantization causes catastrophic safety degradation (40%→10% safe@1 at 512 tokens)
- Both skill and safety performance scale positively with reasoning length across quantization levels (within bounds)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** INT8-quantized reasoning models can match full-precision safety performance within fixed compute budgets by generating more tokens.
- **Mechanism:** Quantization increases throughput (tokens/second), enabling more reasoning steps in the same wall-clock time. Additional tokens compensate for per-step precision loss through extended self-correction and decomposition.
- **Core assumption:** The quality-token relationship is sufficiently monotonic that ~2× tokens at ~0.6× per-token quality yields net parity.
- **Evidence anchors:**
  - [abstract] "an 8-bit quantized model reasoning for 2048 tokens achieves similar safety performance to a full-precision model reasoning for 1024 tokens, but with a 16.4% lower compute budget"
  - [section 5.4, Figure 4] Q8SL1-1.5B at 2048 tokens shows 1.4% lower safety than SL1-1.5B at 1024 tokens with comparable compute budget (19.08s vs 14.91s)
  - [corpus] "L1" paper establishes length-controllable reasoning as viable; corpus lacks direct safety-quantization studies.
- **Break condition:** INT4 quantization—safe@1 drops from 40%→10% at 512 tokens (Figure 3b), breaking compensation.

### Mechanism 2
- **Claim:** LCPO enables user-specified reasoning length control while improving safety through multi-objective RL fine-tuning.
- **Mechanism:** RL optimizes combined reward: (1) safety score from Llama-Guard-3 evaluator, (2) length penalty. Prompts augmented with "Think for n tokens" where n sampled 0–4000, teaching the model to budget its reasoning.
- **Core assumption:** Reward shaping avoids degenerate solutions (e.g., padding without reasoning) and preserves reasoning quality.
- **Evidence anchors:**
  - [abstract] "fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length"
  - [section 3.1] Reward combines safety reward + length penalty; training uses SafeChain dataset with random target lengths
  - [corpus] "L1" paper (Aggarwal & Welleck 2025) is LCPO source; no corpus evidence on safety-specific LCPO applications.
- **Break condition:** Target length below minimum required for task complexity—model cannot compress reasoning without quality loss.

### Mechanism 3
- **Claim:** Both skill and safety performance scale positively with reasoning length across quantization levels (within bounds).
- **Mechanism:** Extended CoT enables more thorough problem decomposition, error checking, and safety consideration before committing to outputs.
- **Core assumption:** Scaling holds across domains; no systematic inverse scaling.
- **Evidence anchors:**
  - [section 5.2.1] "an increase in reasoning length increases the performance of these models" on AIME, GPQA, LSAT
  - [section 5.3.2] "safety performance of the baseline and quantized models improves with an increase in reasoning length"
  - [corpus] "Inverse Scaling in Test-Time Compute" explicitly identifies tasks where longer reasoning degrades performance—mechanism breaks for counting-with-distractors and spurious-correlation tasks.
- **Break condition:** Inverse-scaling task types; diminishing returns beyond task-appropriate length (AIME flattens post-2048 tokens).

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Scaling**
  - Why needed here: Entire paper assumes understanding that longer intermediate reasoning traces improve output quality at compute cost.
  - Quick check question: Why does generating 2048 tokens of reasoning before answering differ from generating 512 tokens, in terms of both quality and latency?

- **Concept: Post-Training Quantization (PTQ)**
  - Why needed here: GPTQ, INT8 vs INT4, and weight-only quantization directly determine the tradeoffs being measured.
  - Quick check question: If INT8 weight quantization keeps activations at 16-bit, why might throughput improve but quality degrade slightly?

- **Concept: Reinforcement Learning with Composite Rewards**
  - Why needed here: LCPO combines safety + length rewards; understanding multi-objective RL prevents misinterpreting results as single-metric optimization.
  - Quick check question: If you reward both "be safe" and "use exactly N tokens," what failure modes could emerge during training?

## Architecture Onboarding

- **Component map:** Input Prompt + Length Instruction -> Base Model L1-Qwen-1.5B/8B -> LCPO Fine-tuning with SafeChain -> GPTQ Quantization (INT8/INT4) -> Evaluation with Llama-Guard-3

- **Critical path:** Start with L1 base model (length-controllable reasoner) → LCPO fine-tune on SafeChain with safety + length rewards → SL1 → Apply GPTQ INT8 quantization → Q8-SL1 (avoid INT4 per failure signature) → For deployment: set token budget based on target compute; use quantization if throughput gain > precision loss

- **Design tradeoffs:**
  - INT8 vs BF16: ~3–7% safety drop (Figure 3b), but ~1.6× throughput (Table 1: 69 vs 42 tokens/s for 1.5B)
  - INT4: Catastrophic—never viable for safety-critical deployment
  - Token budget allocation: Within fixed wall-clock budget, prefer quantized + more tokens over full-precision + fewer tokens

- **Failure signatures:**
  - INT4 quantization: safe@1 collapses (40%→10% at 512 tokens)
  - Short budgets (<1024 tokens): Underperformance across all configurations
  - Baseline L1 without safety fine-tuning: Poor safety despite skill competence
  - AIME scaling flattens post-2048 tokens

- **First 3 experiments:**
  1. Reproduce INT8 throughput vs safety tradeoff on StrongReject across {512, 1024, 2048} tokens, measuring safe@1 and reasoning time.
  2. Validate compute-equivalence hypothesis: fix wall-clock budget (~20s), compare SL1@1024 tokens vs Q8-SL1@~2048 tokens on both AIME and StrongReject.
  3. Ablate safety fine-tuning: compare L1 vs SL1 at matched token budgets to isolate LCPO + SafeChain contribution to safety gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the finding that quantized models can match full-precision safety performance by extending reasoning length hold for significantly larger models (e.g., 70B+ parameters)?
- **Basis in paper:** [inferred] The study is restricted to L1-1.5B and L1-8B models (Section 5.1).
- **Why unresolved:** Quantization error impacts different model sizes unevenly, and the capacity to "recover" performance via longer reasoning may not scale linearly with parameter count.
- **What evidence would resolve it:** Evaluating the LCPO and GPTQ trade-off on larger foundation models (e.g., Llama-3-70B).

### Open Question 2
- **Question:** Can Quantization-Aware Training (QAT) during the safety fine-tuning phase outperform Post-Training Quantization (PTQ) in maintaining safety under compute constraints?
- **Basis in paper:** [inferred] The paper notes performance drops in INT4 models and relies on PTQ (GPTQ) applied after training (Section 3.2).
- **Why unresolved:** Training with quantization awareness might allow the model to adapt its reasoning chain to low-precision constraints, potentially mitigating the safety degradation seen in INT4 models.
- **What evidence would resolve it:** A comparison of safety scores (safe@1) between GPTQ and QAT-trained models at equivalent bit-widths.

### Open Question 3
- **Question:** Is inference time (reasoning seconds) a consistent proxy for compute budget across different hardware architectures?
- **Basis in paper:** [inferred] The paper defines the compute budget as "reasoning time in seconds" based on throughput on A100 GPUs (Section 5.4).
- **Why unresolved:** Inference time is heavily dependent on memory bandwidth and kernel optimization, meaning the trade-off balance (e.g., 16.4% savings) might vanish or invert on different hardware (e.g., consumer GPUs or CPUs).
- **What evidence would resolve it:** Replicating the throughput and timing benchmarks on non-datacenter hardware or utilizing theoretical FLOPs instead of wall-clock time.

## Limitations
- The paper lacks precise LCPO reward function details and training hyperparameters, making exact replication challenging.
- Safety performance measured only on StrongReject (60 jailbreak queries) may not reflect real-world deployment scenarios.
- The catastrophic failure at INT4 quantization suggests results may not generalize to other quantization methods without extensive recalibration.

## Confidence
- **High confidence:** The throughput advantage of INT8 quantization (~1.6× tokens/s) and its correlation with safety performance improvements across reasoning lengths.
- **Medium confidence:** The safety-length scaling relationship (longer reasoning → better safety) is supported but may not generalize beyond StrongReject dataset.
- **Low confidence:** The exact LCPO implementation details (reward weights, training schedule) are insufficiently specified to reproduce the claimed safety improvements independently.

## Next Checks
1. **Validate compute-equivalence hypothesis:** Fix a 20-second wall-clock budget and compare SL1-1.5B@1024 tokens vs Q8-SL1-1.5B@~2048 tokens on StrongReject and AIME2025. Measure safe@1, pass@1, and actual reasoning time to verify the 16.4% compute budget reduction claim holds across both skill and safety domains.

2. **Ablate safety fine-tuning contribution:** Train L1-1.5B with LCPO length control but without SafeChain safety rewards, then compare against SL1-1.5B at matched token budgets (512, 1024, 2048 tokens). This isolates whether LCPO + SafeChain provides additive safety benefits beyond length control alone.

3. **Test inverse-scaling task vulnerability:** Evaluate Q8-SL1-1.5B on corpus-identified inverse-scaling tasks (counting-with-distractors, spurious-correlation) across 512-3600 token budgets. Verify whether the positive safety-length correlation holds or whether certain task types exhibit degradation with extended reasoning, as suggested by "Inverse Scaling in Test-Time Compute."