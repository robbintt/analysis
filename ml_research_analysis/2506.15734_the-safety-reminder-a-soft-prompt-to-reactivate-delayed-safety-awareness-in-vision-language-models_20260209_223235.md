---
ver: rpa2
title: 'The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness
  in Vision-Language Models'
arxiv_id: '2506.15734'
source_url: https://arxiv.org/abs/2506.15734
tags:
- safety
- prompt
- harmful
- soft
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a novel phenomenon termed "delayed safety
  awareness" in vision-language models (VLMs), where safety-aligned models initially
  generate harmful content under adversarial attacks but eventually recognize the
  associated risks and attempt self-correction. To address this, the authors propose
  Safety-Aware Soft Prompt Tuning (SAPT), which optimizes learnable soft prompts that
  are periodically injected during text generation to proactively reactivate the model's
  safety awareness.
---

# The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models

## Quick Facts
- arXiv ID: 2506.15734
- Source URL: https://arxiv.org/abs/2506.15734
- Authors: Peiyuan Tang; Haojie Xin; Xiaodong Zhang; Jun Sun; Qin Xia; Zijiang Yang
- Reference count: 40
- Primary result: Introduces SAPT, a soft prompt tuning method that reactivates delayed safety awareness in VLMs, reducing attack success rates while maintaining utility

## Executive Summary
This paper addresses a novel phenomenon in vision-language models (VLMs) called "delayed safety awareness," where safety-aligned models initially generate harmful content under adversarial attacks but eventually recognize associated risks and attempt self-correction. The authors propose Safety-Aware Soft Prompt Tuning (SAPT), which optimizes learnable soft prompts that are periodically injected during text generation to proactively reactivate the model's safety awareness. By leveraging hidden-state-based harmfulness detection and joint optimization of safety and utility objectives, SAPT significantly reduces attack success rates while preserving model performance on benign tasks, offering a practical solution for safer VLM deployment.

## Method Summary
SAPT optimizes learnable continuous tokens (soft prompts) that are periodically injected during autoregressive text generation. The method trains a safety state detector using logistic regression on hidden states from the VLM's final layer, which determines when to activate the soft prompt based on harmfulness probability. During training, the soft prompt and detector are jointly optimized using a combined loss function that balances harmful query refusal and benign query continuation. At inference, the system monitors hidden states every 16 tokens and injects the soft prompt when the detector's harmfulness probability exceeds a threshold of 0.9, effectively redirecting the generation toward safety-aware behavior.

## Key Results
- SAPT reduces attack success rates by 10-20% on FigStep and MMSafetyBench benchmarks compared to baselines
- The method maintains utility, with MM-Vet scores dropping only 3-5% for strongly-aligned models
- Detection accuracy reaches 90.2-93.2% across three VLMs with high recall (94.2-98.5%) but lower precision (87.3-90.8%)
- SAPT causes only a slight increase in refusal rates for benign tasks (~1% to ~5%)

## Why This Works (Mechanism)

### Mechanism 1: Interruption of Autoregressive Harmful Trajectory
- Claim: Injecting optimized soft prompts mid-generation may redirect the model from continuing harmful content toward refusal behavior.
- Mechanism: Autoregressive generation maximizes `p(yt|y<t, x)`, causing the model to complete whatever prefix it has started. By inserting soft tokens after an initial harmful prefix, the model receives new conditioning that prioritizes safety over completion. This effectively "resets" the generation context.
- Core assumption: The model's safety awareness can be triggered by additional context even after harmful generation has begun; this is not proven but supported by the observed "delayed safety awareness" phenomenon.
- Evidence anchors:
  - [abstract] "optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness"
  - [section 3.2] "Autoregressive Generation Facilitates Jailbreak Attack... once the model begins generating harmful content, the autoregressive objective drives the model to maintain consistency with the harmful prefix"
  - [corpus] Related work on inference-time defenses (DTR) similarly intervenes during generation, suggesting this is a plausible intervention point
- Break condition: If harmful content has already been emitted to users before the intervention point, the approach cannot retroactively prevent exposure.

### Mechanism 2: Hidden-State-Based Harmfulness Detection
- Claim: The final-layer hidden states of VLMs contain signal that correlates with whether current generation is harmful, enabling targeted intervention.
- Mechanism: A logistic regression classifier (`ŷ = σ(w^T hj + b)`) operates on hidden states at the soft prompt position. The classifier and soft prompt are trained jointly (`Ltotal = Lm + αLb + βLcls`), which may encourage the model to produce more distinguishable representations.
- Core assumption: Harmful and benign generation states are sufficiently separable in hidden space; this may not generalize across all attack types.
- Evidence anchors:
  - [section 3.4] "We implement the detector as a logistic regression classifier operating on the hidden states"
  - [Table 3] Detection accuracy 90.2-93.2% across three VLMs with high recall (94.2-98.5%) but lower precision (87.3-90.8%)
  - [corpus] No direct corpus evidence on hidden-state detection efficacy; related work focuses on input/output filtering rather than internal states
- Break condition: False positives cause unnecessary refusal of benign queries; false negatives allow attacks through. High recall but lower precision in Table 3 indicates over-sensitivity is the dominant failure mode.

### Mechanism 3: Dual-Objective Training for Safety-Utility Tradeoff
- Claim: Joint optimization on both harmful queries (with refusal targets) and benign queries (with continuation targets) encourages the soft prompt to be conditionally activated only when appropriate.
- Mechanism: `Lm` trains the soft prompt to produce refusal after harmful prefixes; `Lb` trains continuation after benign prefixes. The same soft prompt must satisfy both objectives, which may push the model toward context-dependent behavior rather than blanket refusal.
- Core assumption: A single soft prompt can encode context-conditional behavior; this relies on the detector accurately triggering only on harmful states.
- Evidence anchors:
  - [section 3.3] Formal loss definitions for `Lm` and `Lb`
  - [Table 4] Ablation: removing `Lb` causes utility drop (MM-Vet: 31.4 → 28.7) despite perfect safety, demonstrating the tradeoff
  - [corpus] AlphaAlign paper similarly addresses over-refusal issues in safety alignment, suggesting this is a known challenge
- Break condition: If `α` (benign loss weight) is too low, the model over-refuses; if too high, safety degrades. The paper uses fixed weights without systematic tuning analysis.

## Foundational Learning

- **Autoregressive Language Generation**
  - Why needed here: Understanding why harmful prefixes propagate into full harmful outputs is essential. The model predicts each token conditioned on all prior tokens; once "Sure, here's how to make a bomb" is generated, the model has strong incentive to complete coherently.
  - Quick check question: In a model generating "The steps to build a bomb are: 1.", why is the next token more likely to continue the list than to refuse?

- **Soft Prompt Tuning**
  - Why needed here: SAPT uses learnable continuous tokens (not discrete text) appended during generation. These are optimized via gradient descent rather than handcrafted, allowing the model to discover non-intuitive intervention patterns.
  - Quick check question: How does a 4-token soft prompt differ from inserting "Please be safe" as text tokens? (Hint: continuous vs. discrete embedding space)

- **Safety Alignment in Multimodal Models**
  - Why needed here: VLMs face attacks that exploit the visual modality to bypass text-only safety training. Understanding that alignment is not absolute but can be "delayed" or "reactivated" frames the problem correctly.
  - Quick check question: Why might an image-based jailbreak succeed when the same harmful text query alone would be refused?

## Architecture Onboarding

- **Component map**:
  Input (image + text) → Vision Encoder → Projector → LLM Backbone
                                                          ↓
                              Hidden States (every k tokens) → Safety Detector
                                                              ↓
                                           [if harmful score > θ] → Inject Soft Prompt
                                                              ↓
                                           Continue Generation (potentially redirected)

- **Critical path**:
  1. **Training**: Construct paired dataset (harmful queries + harmful prefixes + safe responses; benign queries + normal continuations). Train soft prompt and detector jointly with `Ltotal = Lm + αLb + βLcls` (α, β tuned).
  2. **Inference**: Generate tokens autoregressively. Every k tokens, extract hidden state, run detector. If harmfulness probability > θ (default 0.9), append soft prompt to current sequence and continue generation.

- **Design tradeoffs**:
  - **Soft prompt length**: 4 tokens chosen; shorter (2) may underfit, longer (8-16) may overfit and reduce utility (Figure 5 left). Not formally optimized.
  - **Detection interval**: 16 tokens balances safety (more frequent = earlier catch) vs. utility (more frequent = more false positives, Figure 5 right).
  - **Threshold θ**: 0.9 chosen; lower increases false positives, higher lets attacks through. No systematic sensitivity analysis provided.
  - **Detector complexity**: Logistic regression chosen for simplicity; more complex classifiers might improve precision but add latency.

- **Failure signatures**:
  - **Over-refusal**: Model rejects benign queries (refusal rate increases from ~1% to ~5% for Qwen2-VL in Table 2). Indicates detector threshold too low or soft prompt too aggressive.
  - **Undetected attacks**: Visual adversarial attacks with high perturbation (ϵ=128/255) still achieve 3-10% ASR (Table 1). Indicates hidden states for these cases are not well-separated.
  - **Utility degradation**: MM-Vet scores drop ~5% for strongly-aligned models (Qwen2-VL), suggesting safety and capability objectives can conflict.
  - **Latency overhead**: ~10% reduction in tokens/second (Table 5) due to periodic hidden state extraction and classification.

- **First 3 experiments**:
  1. **Validate delayed awareness on your target VLM**: Run target model on harmful benchmarks (FigStep or MMSafetyBench), record positions of refusal signals in outputs. Confirm that longer outputs show later refusal positions (replicate Figure 2 pattern).
  2. **Train detector-only baseline**: Train only the safety state detector (no soft prompt) and measure detection accuracy. This establishes whether hidden states in your model contain sufficient signal before investing in soft prompt optimization.
  3. **Ablation on soft prompt length and interval**: For your target model, sweep prompt lengths [2, 4, 8] and detection intervals [8, 16, 32]. Plot ASR vs. utility (MM-Vet or similar) to identify Pareto-optimal configuration before full deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is Safety-Aware Soft Prompt Tuning (SAPT) effective against text-based adversarial attacks, such as GCG, or is it limited to image-based jailbreaks?
- Basis in paper: [explicit] Section 6 (Limitations) states, "we have not evaluated our method against text-based adversarial attacks, such as GCG... It remains uncertain whether SAPT is effective against such attacks."
- Why unresolved: The current evaluation is restricted to image-based perturbations and typography attacks (FigStep, MMSafetyBench), leaving the model's robustness against optimization-based text attacks unknown.
- What evidence would resolve it: Evaluating the Attack Success Rate (ASR) of SAPT-defended models against text-only adversarial optimization methods like GCG on standard benchmarks.

### Open Question 2
- Question: How can the trade-off between increased safety and the over-sensitivity (high refusal rate) for benign queries be further minimized?
- Basis in paper: [explicit] Section 6 notes that "SAPT may make VLMs more sensitive, increasing the rejection rate for normal queries," and Table 2 shows a slight increase in refusal rates for benign tasks.
- Why unresolved: While SAPT preserves utility better than baselines, the mechanism of injecting safety prompts still introduces a risk of rejecting safe requests, which is a critical deployment hurdle.
- What evidence would resolve it: A modified training objective or inference strategy that reduces the False Positive Rate on benign datasets (like MM-Vet) to near-zero while maintaining high defense performance.

### Open Question 3
- Question: How can the threshold for the Safety State Detector be reliably optimized for diverse, real-world deployment scenarios?
- Basis in paper: [explicit] Section 6 highlights that "our method relies heavily on the Safety States Detector" and that "setting an appropriate threshold for the classifier can be challenging."
- Why unresolved: The current implementation uses a fixed threshold (0.9), but the optimal balance between detecting harm and maintaining flow likely varies significantly across different user contexts and input distributions.
- What evidence would resolve it: Development of an adaptive or self-calibrating threshold mechanism that dynamically adjusts based on the confidence distribution of hidden states during inference.

## Limitations

- **Temporal limitation**: The approach only prevents harmful continuation, not exposure of already-generated harmful content that may have been emitted before intervention.
- **Generalization gap**: The safety detector is trained on specific jailbreak patterns and may not generalize to novel attack strategies beyond the tested adversarial examples.
- **Deployment complexity**: The method requires maintaining additional components (soft prompt, detector) and adds inference-time overhead (~10% latency reduction), limiting practical deployment in latency-sensitive applications.

## Confidence

- **Medium confidence** - The "delayed safety awareness" phenomenon is well-demonstrated on the tested models but not established as universal across VLMs.
- **Medium confidence** - Hidden-state-based detection shows 90.2-93.2% accuracy but lacks validation for novel attack types beyond tested examples.
- **Low confidence** - Soft prompt optimization uses fixed hyperparameters without systematic tuning or sensitivity analysis.
- **Medium confidence** - Safety-utility tradeoff is demonstrated but doesn't explore the full Pareto frontier with systematic weighting scheme analysis.

## Next Checks

1. **Cross-model generalization test**: Apply the trained SAPT (optimized on one VLM) to a different VLM architecture to evaluate whether the soft prompt and detector generalize beyond the training model family.

2. **Novel attack robustness**: Design and test SAPT against completely new jailbreak strategies not represented in the training data to assess the detection mechanism's ability to generalize to unseen attack patterns.

3. **Pareto frontier exploration**: Systematically vary the detection threshold θ, soft prompt length, and interval parameters to map the complete safety-utility tradeoff space and identify optimal configurations for different deployment scenarios.