---
ver: rpa2
title: 'Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs'
arxiv_id: '2511.08274'
source_url: https://arxiv.org/abs/2511.08274
tags:
- query
- graph
- property
- character
- cypher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-Agent GraphRAG introduces a modular LLM agentic system for
  text-to-Cypher query generation over Labeled Property Graphs, addressing the underexplored
  potential of LPGs in GraphRAG workflows. The system employs iterative query refinement
  with schema-aware verification, entity extraction, and aggregated feedback to improve
  semantic and syntactic accuracy.
---

# Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs

## Quick Facts
- arXiv ID: 2511.08274
- Source URL: https://arxiv.org/abs/2511.08274
- Reference count: 17
- Primary result: Achieves 77.23% average accuracy on text-to-Cypher generation, outperforming linear baselines by up to 10.23%

## Executive Summary
Multi-Agent GraphRAG introduces a modular LLM agentic system for text-to-Cypher query generation over Labeled Property Graphs, addressing the underexplored potential of LPGs in GraphRAG workflows. The system employs iterative query refinement with schema-aware verification, entity extraction, and aggregated feedback to improve semantic and syntactic accuracy. Evaluated on CypherBench and IFC-based datasets, it achieves an average accuracy of 77.23%, outperforming linear-pass baselines by up to 10.23%. The approach demonstrates robustness in handling complex graph queries and real-world engineering data, enabling scalable natural language interfaces for property graph databases.

## Method Summary
The method employs a 7-agent LLM orchestration pipeline that generates, verifies, and iteratively refines Cypher queries from natural language questions. Agents include Query Generator (schema-grounded prompt), Named Entity Extractor, Verification Module (database lookup + LLM ranking), Instructions Generator, Feedback Aggregator, Query Evaluator (LLM critic), and Interpreter. The system executes up to 4 refinement iterations, with each failure triggering entity verification and aggregated feedback. Graph Database Executor (Memgraph) runs queries, and results are compared to ground truth via LLM-as-a-judge evaluation.

## Key Results
- Achieves 77.23% average accuracy across CypherBench and IFC datasets
- Outperforms linear-pass baselines by 6.79-10.23% across tested LLMs
- Demonstrates robust handling of complex graph queries and real-world engineering data
- Shows effectiveness in reducing hallucination through database-grounded entity verification

## Why This Works (Mechanism)

### Mechanism 1: Iterative Query Refinement with Aggregated Feedback
The system executes up to 4 refinement iterations where Feedback Aggregator consolidates signals from Query Evaluator and Verification Module into structured correction instructions. This multi-source feedback approach improves Cypher generation accuracy over single-pass baselines by 6.79–10.23% across tested LLMs.

### Mechanism 2: Database-Grounded Entity Verification Against Hallucination
Named Entity Extractor identifies node labels, property values, and relationship types in generated queries. Verification Module checks existence programmatically via auxiliary Cypher queries, proposing replacements using Levenshtein similarity + LLM semantic ranking when mismatches occur.

### Mechanism 3: Schema Formatting Aligned to Target Query Syntax
Graph schema is presented in Cypher-like syntax with sampled property values in Query Generator's system prompt. This token-level alignment between schema description and target output format reduces generation errors through better structural grounding.

## Foundational Learning

- **Concept: Labeled Property Graph (LPG) Model**
  - Why needed here: The system targets property graphs where both nodes and edges carry typed attributes, unlike RDF triples. Understanding this distinction is prerequisite to schema interpretation and query construction.
  - Quick check question: What structural features distinguish a labeled property graph from an RDF knowledge graph?

- **Concept: Cypher Query Language Fundamentals**
  - Why needed here: The pipeline generates, executes, and corrects Cypher; familiarity with MATCH patterns, OPTIONAL MATCH, WITH clauses, and EXISTS() semantics is required to debug agent outputs.
  - Quick check question: In Cypher, when would you use `OPTIONAL MATCH` instead of `MATCH`?

- **Concept: Multi-Agent LLM Orchestration**
  - Why needed here: The architecture coordinates 7+ specialized agents with stateful handoffs; understanding role separation and feedback loops is essential for modifying or extending the system.
  - Quick check question: What is the primary advantage of delegating distinct subtasks to separate agents versus using a single monolithic prompt?

## Architecture Onboarding

- **Component map:** Query Generator -> Graph DB Executor -> Query Evaluator -> (if not Accept) Named Entity Extractor -> Verification Module -> Instructions Generator -> Feedback Aggregator -> Query Generator (loop)

- **Critical path:** User question + schema → Query Generator → Cypher query → Graph DB Executor → result/error → Query Evaluator → grade + feedback → (if not Accept) extract entities → verify → generate fix instructions → aggregate feedback → regenerate (loop up to 4×) → if Accept: Interpreter → answer

- **Design tradeoffs:** 4-iteration cap balances accuracy vs latency; Memgraph backend provides low-latency execution but isn't Neo4j-compatible; LLM-as-judge evaluation offers flexibility but introduces subjectivity

- **Failure signatures:** Compositional queries with disjunctions requiring structurally distinct subqueries; symmetric relationships (e.g., `hasSpouse`) complicating schema validation; multi-intent questions causing semantic conflation in single query

- **First 3 experiments:** 1) Reproduce baseline vs agentic accuracy gap on CypherBench subset to validate claimed +10.23% improvement. 2) Ablate entity verification to isolate contribution of database-grounded hallucination detection. 3) Test on a custom property graph schema (e.g., your domain database) to assess generalization beyond benchmark datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit subgoal planning mechanisms improve accuracy on compositional queries involving disjunctions or structurally distinct subqueries?
- Basis in paper: Authors state the system "struggles with multi-intent questions that require decomposing and aligning distinct subgoals" and propose "incorporating explicit subgoal planning for compositional queries" as future work.
- Why unresolved: Current architecture processes queries holistically without intermediate decomposition, causing semantic conflation when sub-intents require independent resolution.
- What evidence would resolve it: Ablation study comparing current holistic generation against a planning-augmented variant on disjunctive and multi-intent CypherBench queries.

### Open Question 2
- Question: How does the system perform in multi-turn dialogue scenarios where follow-up questions reference prior query context?
- Basis in paper: Future work includes "extending this approach to multi-turn dialogue scenarios."
- Why unresolved: Current design treats each question independently; no mechanism persists conversational state or coreferences across turns.
- What evidence would resolve it: Evaluation on a multi-turn text-to-Cypher benchmark measuring accuracy degradation as dialogue depth increases.

### Open Question 3
- Question: What is the optimal iteration limit for the self-correction loop, and does performance plateau or degrade beyond four iterations?
- Basis in paper: The system caps refinement at four iterations without ablation; the trade-off between correction benefit and accumulated LLM error or cost remains unexamined.
- Why unresolved: No analysis examines whether additional iterations yield diminishing returns or introduce regression from over-correction.
- What evidence would resolve it: Systematic sweep of iteration limits (1–8) with per-iteration accuracy and error-type tracking across CypherBench domains.

### Open Question 4
- Question: Can larger domain-specific IFC–Cypher datasets improve generalization to unseen building models in digital construction applications?
- Basis in paper: Authors call for "developing larger domain-specific datasets, particularly for IFC-linked Cypher queries, to support the adoption of AI-driven solutions in digital construction."
- Why unresolved: Evaluation used only a single 10-question IFC sample; scalability to diverse building schemas and query complexity is untested.
- What evidence would resolve it: Training or few-shot evaluation on a multi-building IFC dataset with held-out models, reporting cross-schema transfer accuracy.

## Limitations
- Implementation code unavailable; GitHub repository URL points to placeholder
- Exact system prompts for each agent and LLM hyperparameters unspecified
- LLM-as-a-judge evaluation introduces subjectivity compared to deterministic verification
- Generalization to arbitrary property graph schemas beyond CypherBench and IFC datasets unproven
- 4-iteration cap represents arbitrary design choice without optimization analysis

## Confidence
- **High confidence**: The iterative feedback mechanism with aggregated corrections demonstrably improves accuracy over single-pass baselines (6.79–10.23% gain supported by benchmark results)
- **Medium confidence**: Database-grounded entity verification reduces hallucinations, though corpus support is limited and the mechanism's contribution relative to other factors needs isolation
- **Medium confidence**: Schema formatting in Cypher-like syntax improves generation quality, but the evidence is primarily based on author observation rather than controlled ablation studies

## Next Checks
1. Reproduce the baseline vs agentic accuracy gap on CypherBench subset to validate the claimed +10.23% improvement independently
2. Implement an ablation study removing the entity verification module to quantify its specific contribution to hallucination reduction
3. Test the system on a custom property graph schema from a different domain (e.g., academic collaboration network) to assess generalization beyond benchmark datasets