---
ver: rpa2
title: 'AdaTP: Attention-Debiased Token Pruning for Video Large Language Models'
arxiv_id: '2505.20100'
source_url: https://arxiv.org/abs/2505.20100
tags:
- video
- visual
- attention
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies and addresses attention bias in video large
  language models (Video LLMs), where global bias causes focus on the ends of visual
  token sequences and local bias concentrates attention on specific spatial positions
  across frames. The authors propose AdaTP, a training-free token pruning pipeline
  that integrates Global Debiasing and Local Debiasing modules to mitigate these biases.
---

# AdaTP: Attention-Debiased Token Pruning for Video Large Language Models

## Quick Facts
- arXiv ID: 2505.20100
- Source URL: https://arxiv.org/abs/2505.20100
- Reference count: 40
- Primary result: AdaTP achieves comparable performance to vanilla Video LLMs using only 27.3% of FLOPs by mitigating attention bias

## Executive Summary
This paper identifies and addresses two forms of attention bias in Video Large Language Models: global bias (over-focusing on sequence ends) and local bias (over-focusing on specific spatial positions across frames). The authors propose AdaTP, a training-free token pruning pipeline that combines Global Debiasing (segmenting videos by frame similarity and retaining text-relevant tokens) and Local Debiasing (reducing spatial redundancy within segments). Evaluated on LLaVA-OneVision-7B, AdaTP maintains performance while using only 27.3% of the FLOPs, outperforming state-of-the-art baselines on VideoMME, MLVU, and LongVideoBench benchmarks.

## Method Summary
AdaTP is a training-free token pruning pipeline that addresses attention bias in Video LLMs through two modules. The Global Debiasing module segments videos based on frame similarity and prunes tokens based on their correlation with text embeddings, ensuring text-relevant visual tokens are retained while reducing global bias toward sequence ends. The Local Debiasing module identifies spatial redundancy within each segment and reduces attention to frequently attended positions across frames. The pipeline introduces four hyperparameters (τs, τt, αboost, γcap) that control segmentation, text-relevance filtering, and spatial redundancy reduction. By combining these modules, AdaTP achieves comparable performance to the full model while significantly reducing computational cost.

## Key Results
- Achieves comparable performance to vanilla LLaVA-OneVision-7B using only 27.3% of FLOPs
- Outperforms state-of-the-art token pruning baselines on VideoMME, MLVU, and LongVideoBench benchmarks
- Maintains effectiveness across diverse video lengths and frame rates while being training-free

## Why This Works (Mechanism)
AdaTP works by explicitly addressing two identified attention biases in Video LLMs. Global bias causes the model to disproportionately focus on tokens at the beginning and end of visual sequences, while local bias causes concentration of attention on specific spatial positions across frames. By segmenting videos based on frame similarity and pruning tokens based on text relevance (Global Debiasing), then reducing spatial redundancy within segments (Local Debiasing), the method ensures that retained tokens carry both semantic relevance and spatial diversity. This selective retention preserves the model's ability to answer questions while eliminating computational waste from redundant or irrelevant tokens.

## Foundational Learning

**Visual Tokenization in Video LLMs**
Why needed: Understanding how videos are converted into token sequences that LLMs can process
Quick check: Verify that video frames are extracted at regular intervals and embedded into visual tokens compatible with text embeddings

**Attention Mechanisms in Vision-Language Models**
Why needed: Core to understanding how biases manifest in token processing
Quick check: Confirm that self-attention operates over concatenated visual-text sequences with causal masking

**Token Pruning Strategies**
Why needed: Essential for evaluating the novelty of AdaTP's approach
Quick check: Compare AdaTP's text-guided pruning against random or importance-based pruning methods

**Video Segmentation Based on Frame Similarity**
Why needed: Key to understanding Global Debiasing methodology
Quick check: Verify that frame similarity is computed using cosine distance or similar metric between frame embeddings

## Architecture Onboarding

**Component Map**: Video frames → Frame Embedding → Global Debiasing (segmentation + text relevance filtering) → Local Debiasing (spatial redundancy reduction) → Pruned Token Sequence → Video LLM

**Critical Path**: The most computationally intensive path is the attention computation over pruned tokens, which is reduced from full sequence to ~27.3% of original length

**Design Tradeoffs**: Training-free approach vs. potential performance gains from fine-tuning; fixed hyperparameters vs. adaptability to different video distributions

**Failure Signatures**: Performance degradation when text-relevant tokens are incorrectly pruned (Global Debiasing fails) or when spatial redundancy is not adequately reduced (Local Debiasing fails)

**First Experiments**:
1. Verify attention distribution visualization shows bias toward sequence ends and specific spatial positions in vanilla model
2. Confirm token count reduction from ~100% to ~27.3% after AdaTP application
3. Test that performance degradation is minimal (within 1-2%) compared to vanilla model on held-out validation set

## Open Questions the Paper Calls Out

**Open Question 1**: Does the identified attention bias phenomenon persist in Video LLMs significantly larger than 7B parameters, and does AdaTP maintain its efficiency-effectiveness trade-off at that scale?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "Due to computational resource constraints, our proposed method has not yet been validated on larger-scale Video LLMs."
- Why unresolved: The experiments were restricted to 0.5B and 7B models. As model size increases, attention patterns often change (e.g., attention sinks shift), potentially altering the bias or the effectiveness of the current debiasing strategy.
- What evidence would resolve it: Evaluation of AdaTP on models with 30B+ parameters (e.g., LLaVA-OneVision-72B or Yi-VL-34B) to verify if the 27.3% FLOPs retention rate still preserves performance.

**Open Question 2**: Can the four distinct hyperparameters (τs, τt, αboost, γcap) be automated or reduced without compromising the model's adaptability to different video dynamics?
- Basis in paper: [explicit] The authors acknowledge, "Our pipeline introduces a relatively large number of hyperparameters, which may increase its complexity," while noting that ablation studies show robustness but not independence from manual tuning.
- Why unresolved: While the method is training-free, the reliance on manual thresholds for segmentation and text-relevance limits its "plug-and-play" capability across diverse, unseen video distributions without potential tuning.
- What evidence would resolve it: A study showing the performance variance of AdaTP using a unified set of parameters across datasets with vastly different video lengths and frame rates (e.g., combining short-form Instagram Reels with long-form lectures).

**Open Question 3**: Is the observed "attention bias" an artifact of specific LLM architectures (e.g., decoder-only transformers) or rotary positional embeddings (RoPE), and does it apply to encoder-based or Mamba-based Video LLMs?
- Basis in paper: [inferred] The analysis is conducted exclusively on LLaVA-based architectures which utilize specific positional embeddings and attention mechanisms.
- Why unresolved: The bias towards sequence ends (global) and specific spatial positions (local) might be induced by the positional encoding or the causal mask inherent to the tested architectures; other architectures might not exhibit this bias, rendering AdaTP unnecessary or requiring modification.
- What evidence would resolve it: Analysis of attention distributions in non-transformer Video LLMs (e.g., those using state-space models) or encoder-decoder architectures to see if global/local biases exist.

**Open Question 4**: Does the performance improvement over the vanilla model stem from explicit semantic debiasing or implicit noise regularization?
- Basis in paper: [inferred] The results show AdaTP occasionally outperforming the vanilla model, which the authors attribute to eliminating redundant visual information. However, the exact mechanism for improvement (rather than just retention) is hypothesized but not isolated.
- Why unresolved: It is unclear if the "Global Debiasing" successfully identifies semantically relevant segments or simply acts as a regularizer by removing uninformative background noise that confuses the vanilla model.
- What evidence would resolve it: An ablation study replacing the text-guided global debiasing with random segment selection (controlling for token count) to isolate the contribution of semantic relevance vs. mere noise reduction.

## Limitations
- Not validated on Video LLMs larger than 7B parameters due to computational constraints
- Requires manual tuning of four hyperparameters, increasing complexity
- Effectiveness may be architecture-specific, limited to LLaVA-based decoder-only transformers

## Confidence
- High: The methodology is clearly described and the results are reproducible
- Medium: Claims about generalizability to larger models and different architectures
- Medium: The attribution of performance gains to specific debiasing mechanisms

## Next Checks
1. Validate attention bias patterns in Video LLMs with 30B+ parameters to assess scalability
2. Test AdaTP with automated hyperparameter selection on diverse video datasets
3. Implement ablation study with random token pruning to isolate semantic debiasing effects