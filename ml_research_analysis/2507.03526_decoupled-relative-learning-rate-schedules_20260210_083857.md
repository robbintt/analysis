---
ver: rpa2
title: Decoupled Relative Learning Rate Schedules
arxiv_id: '2507.03526'
source_url: https://arxiv.org/abs/2507.03526
tags:
- learning
- relative
- rate
- training
- rates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Decoupled Relative Learning Rate Schedules
  (RLRS), a method that assigns individual learning rate schedules to different components
  of Transformer models. Instead of using a uniform learning rate across all layers,
  RLRS tunes separate rates for Embedding, Unembedding, Attention, Feed-Forward, and
  in MoE models, Router and Expert layers.
---

# Decoupled Relative Learning Rate Schedules

## Quick Facts
- arXiv ID: 2507.03526
- Source URL: https://arxiv.org/abs/2507.03526
- Authors: Jan Ludziejewski; Jan Małaśnicki; Maciej Pióro; Michał Krutul; Kamil Ciebiera; Maciej Stefaniak; Jakub Krajewski; Piotr Sankowski; Marek Cygan; Kamil Adamczewski; Sebastian Jaszczur
- Reference count: 26
- Primary result: RLRS achieves up to 23% faster training and improved stability for large Transformer models by assigning individual learning rates to different components

## Executive Summary
This work introduces Decoupled Relative Learning Rate Schedules (RLRS), a method that assigns individual learning rate schedules to different components of Transformer models. Instead of using a uniform learning rate across all layers, RLRS tunes separate rates for Embedding, Unembedding, Attention, Feed-Forward, and in MoE models, Router and Expert layers. The relative rates are first optimized on small models and then transferred to larger ones, achieving up to 23% faster training and improved stability. The method works especially well for MoE architectures and scales effectively across model sizes, offering a practical and scalable approach to optimize large-scale neural networks.

## Method Summary
RLRS implements per-component learning rates by modifying the standard AdamW optimizer to accept different learning rates for each component group. Each component's effective learning rate is computed as η_t^m = η_base × λ_m(t), where λ_m interpolates between λ_start and λ_end via cosine scheduling. The method uses two hyperparameters per component (λ_start and λ_end) to define the relative learning rate curve, enabling simple transfer between model sizes. Small models are tuned using a local search algorithm with scaling factors {1/5, 2/3, 3/2, 5/1}, and the resulting λ values are transferred to larger models with only the base learning rate (η_base) retuned.

## Key Results
- Up to 23% faster training compared to uniform learning rates
- Improved training stability, especially for MoE models
- Successful transfer of hyperparameters from small to models up to 27× larger
- Larger gains observed for MoE architectures (23%) versus dense models (17%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different transformer components exhibit distinct weight update dynamics during training, requiring individualized learning rates.
- Mechanism: The paper observes that weight update magnitudes vary significantly across components (Figure 1 shows Attention ~90-120, Experts ~625-725, Router ~10-15, Embedding ~1060-1160). By assigning component-specific relative learning rates (λ_start, λ_end), each module receives gradient updates scaled to its actual training dynamics rather than a uniform rate that may be too aggressive for some and too conservative for others.
- Core assumption: Component-level heterogeneity in training dynamics persists throughout training and can be captured by start/end relative rates on a cosine schedule.
- Evidence anchors:
  - [abstract] "Traditional methods often apply a uniform learning rate across all network layers, potentially overlooking the unique dynamics of each part."
  - [Section 1, Page 1-2] Figure 1 visualization of differing weight update magnitudes across components.
  - [corpus] Weak corpus support; related work focuses on layer-wise decay (depth-wise) rather than component-type-wise schedules.

### Mechanism 2
- Claim: MoE models face conflicting stability-speed requirements that can be resolved through component-specific scheduling.
- Mechanism: Routers stabilize early in training (deterministic routing emerges), but global high LR causes instability; global low LR slows expert learning. RLRS assigns Router λ_start=0.6 (lower initially to delay premature stabilization) and Expert λ_start=0.3 → λ_end=1.125 (starts low for stability when router is random, increases as routing stabilizes). This decoupling allows aggressive expert learning late while maintaining router stability.
- Core assumption: Instabilities originate from specific components (router/experts) rather than the entire network; suppressing their LR early while boosting later improves overall convergence.
- Evidence anchors:
  - [Section 1, Page 2] "the Router often stabilizes early in training, leading to deterministic routing to the Experts"
  - [Section 4.1, Page 7] "Lowering the learning rate only at the beginning of the training (0.6) for the Router may mitigate instabilities... Increasing the relative learning rate at the end to 1, allow the model to benefit from the higher value"
  - [corpus] No direct corpus support for router/expert-specific LR scheduling in MoE; related work mentions MoE instability requiring global LR reduction.

### Mechanism 3
- Claim: Relative learning rate values tuned on small models extrapolate to larger models without retuning.
- Mechanism: The relative scaling factors (λ) capture component-specific dynamics that are architecture-dependent rather than scale-dependent. Small model tuning (e.g., MoE8×34M with 210M total params) finds optimal λ values that transfer to MoE8×906M (5.67B params, 27× larger), achieving 13.6% speedup. The base LR η_base must still be tuned per model size, but λ ratios remain stable.
- Core assumption: Component-level relative LR requirements are invariant to model scale; only absolute LR needs adjustment.
- Evidence anchors:
  - [abstract] "Hyperparameters of RLRS can be efficiently tuned on smaller models and then effectively reused on models up to 27× larger."
  - [Section 3.3, Page 5-6] Table 3 shows extrapolation results with 8.7%-19% speedup across model scales.
  - [corpus] Partial support from "Scaling and Transferability of Annealing Strategies" (arXiv:2512.13705), which examines LR schedule transferability but not component-wise decoupling.

## Foundational Learning

- Concept: Cosine Learning Rate Scheduling
  - Why needed here: RLRS builds directly on cosine schedules, defining λ_start and λ_end relative to the cosine curve's peak (η_base) and final (η_base × α_end) values. Understanding cosine decay is prerequisite to grasping how relative factors modify it.
  - Quick check question: If η_base = 0.001 and α_end = 0.1, what is the final learning rate before applying any relative factors?

- Concept: Mixture of Experts (MoE) Architecture
  - Why needed here: The paper shows RLRS achieves larger gains on MoE (23%) vs. dense models (17%). MoE introduces Router and Expert components with distinct training dynamics that benefit most from decoupled scheduling.
  - Quick check question: In a Token Choice MoE with 8 experts where 1 is activated per token, what role does the router play during forward pass?

- Concept: Hyperparameter Transfer/Extrapolation
  - Why needed here: The core practical contribution is tuning λ on small models (cheap) and transferring to large models (expensive). This requires understanding why certain hyperparameters scale while others don't.
  - Quick check question: If you tune λ values on a 34M parameter model, what additional hyperparameter must you still tune when scaling to a 906M parameter model?

## Architecture Onboarding

- Component map:
  Transformer Model
  ├── Embedding Layer      → λ_start=5.0, λ_end=0.6 (MoE/dense)
  ├── Attention Layers     → λ_start=1.0, λ_end=1.0 (MoE) or λ_end=0.2 (dense)
  ├── [Dense] Feed-Forward → λ_start=1.0, λ_end=0.6
  ├── [MoE] Router         → λ_start=0.6, λ_end=1.0
  ├── [MoE] Experts        → λ_start=0.3, λ_end=1.125
  └── Unembedding Layer    → λ_start=0.6/1.0, λ_end=0.4

- Critical path:
  1. Implement component-wise LR application in optimizer step (modify AdamW to accept per-parameter-group LRs)
  2. Define parameter group mapping (which parameters belong to which component)
  3. Implement cosine interpolation for each component using λ_start/λ_end values
  4. Small model tuning: run local search (Algorithm 2) with scaling factors {1/5, 2/3, 3/2, 5/1}
  5. Transfer λ values to target large model; tune only η_base

- Design tradeoffs:
  - **Complexity vs. generality**: Using only λ_start and λ_end (2 params per component) limits expressivity but enables extrapolation; more complex schedules might overfit to small model dynamics.
  - **Stability vs. speed**: Embedding λ_start=5.0 is aggressive and may cause early instability, but accelerates overall convergence; monitor for loss spikes in first 1% of training.
  - **Tuning cost vs. transfer distance**: Paper tested 27× scale transfer; larger jumps (100×+) may require partial retuning (Section 4.3 notes Embedding λ_start may need adjustment for larger models).

- Failure signatures:
  - Loss spikes in early training: Embedding λ_start too high; reduce toward 2.0-3.0
  - Router collapse (all tokens route to one expert): Router λ_start too low; increase toward 0.8-1.0
  - No improvement over baseline: Component parameter grouping incorrect; verify each module receives its intended λ
  - Large model underperforms small model transfer prediction: η_base not properly tuned for target scale

- First 3 experiments:
  1. **Validation on existing setup**: Apply paper's MoE λ values (Table 4) to a MoE8×113M model with η_base=2e-3; compare training loss curve against uniform LR baseline. Expected: 15-19% speedup to reach same loss.
  2. **Ablation on single component**: Hold all λ values at 1.0 except one component; test which component's λ provides most gain in MoE setting (hypothesis: Experts or Router based on Section 4.3 ablation results).
  3. **Transfer boundary test**: Train small model (e.g., 10M params) to find λ values, then transfer to 2×, 5×, 10× scales to measure degradation; identify at what scale factor retuning becomes necessary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a mathematically optimal extrapolation scheme be developed for transferring relative learning rate values to larger models?
- Basis in paper: [explicit] Section 2.1 states, "We leave the investigation of optimal extrapolation as future work," regarding the transfer of $\lambda$ values.
- Why unresolved: The authors currently use a direct transfer method (Algorithm 1) but acknowledge they "do not claim that $\lambda$ values are optimal for larger models."
- What evidence would resolve it: A theoretical framework or empirical law defining how relative rates should scale with model width/depth, outperforming direct transfer.

### Open Question 2
- Question: Can combining Relative Learning Rate Schedules (RLRS) with Tensor Programs eliminate the need to tune the base learning rate?
- Basis in paper: [explicit] Section 5.1 suggests that "by combining these two approaches, it may be possible to achieve a zero-shot transfer of RLRS."
- Why unresolved: The current method successfully transfers relative rates but still requires tuning the base learning rate ($\eta_{base}$) for each model size independently.
- What evidence would resolve it: Successful integration of RLRS with "muP" (maximal update parameterization) such that a single base learning rate transfers perfectly across scales.

### Open Question 3
- Question: Does RLRS improve efficiency and stability when applied to fine-tuning rather than pre-training?
- Basis in paper: [inferred] Section 5.2 hypothesizes the method "could be particularly applicable to fine-tuning scenarios" as a continuous alternative to freezing layers, but provides no experimental results.
- Why unresolved: All experiments in the paper focus on pre-training on the C4 dataset; the behavior of decoupled schedules during fine-tuning remains unexplored.
- What evidence would resolve it: Experiments applying RLRS to downstream tasks (e.g., GLUE) to see if it outperforms standard freezing techniques or uniform learning rates.

## Limitations

- The transfer of λ values from small to large models is empirically validated only up to 27× scale; extrapolation to 100×+ may require partial retuning.
- The method's effectiveness on non-GPT-like architectures and non-language modeling tasks remains untested.
- The theoretical justification for specific λ values is primarily empirical rather than derived from first principles.

## Confidence

- **High confidence**: The mechanism of component-specific learning rate heterogeneity (Mechanism 1) is well-supported by empirical weight update magnitude observations. The basic RLRS implementation showing improved training speed for MoE models is reproducible given the detailed component-specific λ values provided.
- **Medium confidence**: The transfer of λ values from small to large models (Mechanism 3) is supported by experiments up to 27× scale, but the scalability limits are not fully characterized. The claim that MoE benefits more than dense models is plausible but based on limited comparisons.
- **Low confidence**: The theoretical justification for specific λ values (especially the aggressive Embedding λ_start=5.0 and the asymmetric Expert schedule) is primarily empirical rather than derived from first principles. The paper doesn't explain why these particular values work best beyond observation.

## Next Checks

1. **Scale extrapolation boundary test**: Systematically train small models (10M, 34M, 113M parameters) to find optimal λ values, then transfer to 2×, 5×, 10×, 50×, and 100× larger models. Measure degradation in training speed and identify the exact scale factor at which partial retuning becomes necessary. This would quantify the true limits of the transfer claim.

2. **Dataset generalization test**: Apply the tuned RLRS values from C4 to different datasets (e.g., RedPajama, The Pile, or code-focused datasets). Measure whether the same λ values provide similar speedups or if dataset-specific tuning is required, revealing whether the component dynamics are dataset-invariant.

3. **Architecture robustness test**: Apply RLRS to non-GPT-like architectures (e.g., encoder-decoder models, Vision Transformers, or different MoE variants like Switch Transformers or Base Layer Selection). This would validate whether the component-wise scheduling approach generalizes beyond the specific Transformer decoder setup tested.