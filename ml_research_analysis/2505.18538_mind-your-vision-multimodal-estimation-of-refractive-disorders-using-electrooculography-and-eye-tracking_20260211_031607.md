---
ver: rpa2
title: 'Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography
  and Eye Tracking'
arxiv_id: '2505.18538'
source_url: https://arxiv.org/abs/2505.18538
tags:
- data
- refractive
- eye-tracking
- multimodal
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores passive refractive error estimation using eye
  movement data from electrooculography (EOG) and video-based eye tracking. LSTM models
  were trained to classify refractive power from unimodal (EOG or eye tracking) and
  multimodal configurations, evaluated under subject-dependent and subject-independent
  scenarios.
---

# Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking

## Quick Facts
- **arXiv ID:** 2505.18538
- **Source URL:** https://arxiv.org/abs/2505.18538
- **Reference count:** 40
- **Key outcome:** Multimodal LSTM achieves 96.2% accuracy in subject-dependent refractive power classification but only 8.9% in subject-independent scenarios.

## Executive Summary
This study explores passive refractive error estimation using eye movement data from electrooculography (EOG) and video-based eye tracking. LSTM models were trained to classify refractive power from unimodal (EOG or eye tracking) and multimodal configurations, evaluated under subject-dependent and subject-independent scenarios. The multimodal model achieved the highest average accuracy: 96.2% in the subject-dependent setting and 8.9% in the subject-independent setting. While the multimodal approach outperformed unimodal models in personalized estimation, generalization across individuals remained limited, with classification only marginally above chance. Statistical analysis confirmed multimodal superiority in personalized settings, but no significant differences were found in subject-independent scenarios. The findings highlight both the potential and limitations of eye movement-based refractive error estimation, suggesting multimodal integration as a promising direction for personalized, non-invasive vision screening.

## Method Summary
The study used 37 participants and simulated refractive errors with trial lenses ranging from -3.0D to +3.0D in 0.5D increments. Data was collected using Shimmer3 ExG (4-channel EOG at 512Hz, downsampled to 120Hz) and Pupil Core (video-based eye tracking at 120Hz). Eight fixation tasks (5s), eight pursuit tasks (1.43-2.92s), and one reading task (40s split into 8 segments) were performed per condition. Preprocessing included DC removal, Hanning windowing, low-pass filtering (50Hz), detrending, and Z-score normalization for EOG; and Hampel filtering, median filtering, and Z-score normalization for eye tracking. Features were concatenated (8 EOG + 93 eye-tracking = 101 total) and fed into a 4-layer LSTM with 512 hidden units. Subject-dependent performance was evaluated using 8-fold cross-validation per subject, while subject-independent performance used leave-one-subject-out evaluation.

## Key Results
- Multimodal LSTM achieved 96.2% accuracy in subject-dependent classification of refractive power
- Subject-independent performance was only 8.9%, barely above chance level (7.7%)
- Multimodal approach consistently outperformed unimodal models in personalized settings
- Statistical analysis confirmed multimodal superiority in subject-dependent scenarios but not in subject-independent ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inducing refractive errors via trial lenses alters eye movement dynamics and accommodative behavior sufficiently to be classified.
- **Mechanism:** The paper posits that variations in cornea/lens shape (simulated by trial lenses) affect accommodative behaviors and surrounding facial musculature. If the model classifies these conditions above chance, it implies that distinct diopter levels produce distinct signal signatures in EOG (voltage changes) and eye tracking (pupil/gaze geometry).
- **Core assumption:** Simulated blur via trial lenses triggers physiological responses analogous to actual refractive errors.
- **Evidence anchors:** Abstract states passive method using two eye movement recording techniques; section 1 suggests eye movement patterns differ between individuals with and without refractive errors.
- **Break condition:** If eye movements during blurred vision tasks remain statistically indistinguishable from normal vision across the population, the mechanism breaks.

### Mechanism 2
- **Claim:** Multimodal fusion improves classification robustness by combining the high sensitivity of EOG with the standardized geometric features of video-based eye tracking.
- **Mechanism:** EOG provides high temporal resolution of eye muscle electrical activity but is prone to signal drift and inter-subject variability. Video tracking provides spatial features but suffers from occlusion or light interference. Concatenating these features allows the LSTM to cross-reference temporal spikes with spatial verification, reducing noise-specific classification errors.
- **Core assumption:** Features from EOG and eye tracking are complementary, not merely redundant.
- **Evidence anchors:** Abstract shows multimodal model consistently outperforms unimodal models; section 4 discusses multimodal approach as most promising strategy.
- **Break condition:** If one modality's signal-to-noise ratio degrades severely, fusion may introduce noise that degrades performance below the best-performing unimodal model.

### Mechanism 3
- **Claim:** Temporal dependencies in eye movement signals, captured by LSTMs, are necessary to distinguish between static diopter classes.
- **Mechanism:** Refractive states are constant, but diagnostic data comes from time-series tasks. An LSTM processes sequences of feature vectors to learn how the eye transitions or stabilizes over time under specific blur conditions, rather than just classifying single frames.
- **Core assumption:** The "signature" of a specific refractive error is encoded in the temporal dynamics of the eye's reaction rather than just instantaneous position.
- **Evidence anchors:** Section 3.3 adopted 4-layer LSTM for capturing temporal dependencies; confusion matrices show specific misclassification patterns implying learned feature embeddings.
- **Break condition:** If the dataset contains insufficient sequence length or if the relevant physiological response is instantaneous, LSTM's recurrence provides no benefit over simpler dense networks.

## Foundational Learning

- **Concept:** Electrooculography (EOG) vs. Video-Oculography (VOG)
  - **Why needed here:** The system relies on fusing these two distinct sensing modalities. Understanding that EOG measures electrical potential and is sensitive to drift, while VOG measures light/geometry and is sensitive to lighting/occlusion, is critical for preprocessing and error analysis.
  - **Quick check question:** Does a user wearing sunglasses affect the EOG signal, the VOG signal, or both? (Answer: VOG primarily; EOG is unaffected by light but affected by electrode impedance.)

- **Concept:** Subject-Dependent vs. Subject-Independent Evaluation
  - **Why needed here:** The paper reveals a massive performance gap between these two scenarios (96% vs ~9%). Understanding that physiological signals are highly individualized explains why the "generalized" model failed and why personalization is the core successful mechanism here.
  - **Quick check question:** Why does normalizing data across all subjects often fail for biosignals compared to normalizing within a single subject's data?

- **Concept:** Diopters and Refractive Error
  - **Why needed here:** The model output is a 13-class classification of diopter values (-3.0D to +3.0D). Engineers need to understand that 0D is "normal," negative is myopia, and positive is hyperopia to interpret confusion matrices.
  - **Quick check question:** In the paper's confusion matrices, why might the model confuse -2.0D and +2.0D more than -2.0D and 0D? (Answer: Both represent significant blur, just at different focal depths, potentially driving similar eye movement behaviors.)

## Architecture Onboarding

- **Component map:** Shimmer3 ExG (4-channel EOG @ 512Hz) -> Preprocessing (outlier removal, Hanning window, DC removal, low-pass filter, detrend, Z-score) -> Downsample to 120Hz -> Eye tracking (120Hz) -> Preprocessing (unpack nested data, outlier removal, Hampel filter, median filter, Z-score) -> Trigger-based temporal alignment -> Concatenate features (8 EOG + 93 eye-tracking = 101 vector) -> 4-layer LSTM (512 hidden units) -> Fully Connected Layer -> 13-Class Softmax

- **Critical path:** The synchronization of triggers is the single point of failure for data alignment. The downsampling of EOG from 512Hz to 120Hz determines the temporal resolution of the fused vector. The LSTM layers must handle the sequence length of the trial segments.

- **Design tradeoffs:**
  * Unbalanced Features: 93 eye-tracking features vs 8 EOG features. The model may be biased toward video features unless weighting is applied.
  * Downsampling vs Upsampling: The authors chose to downsample high-res EOG to match lower-res Video. This loses high-frequency EOG detail but avoids introducing artifacts via upsampling video.
  * Unidirectional LSTM: Chosen for the model. It processes time forward only, which is computationally cheaper but may miss context from future eye positions within a trial.

- **Failure signatures:**
  * Chance-level Accuracy (~7.7%): Indicates the model is failing to generalize across subjects. The features learned are likely subject-specific anatomical traits rather than universal blur responses.
  * Symmetric Misclassification: Confusing positive (+2.0D) and negative (-2.0D) diopters. This suggests the model detects "blur magnitude" but fails to distinguish the "sign" of the error.

- **First 3 experiments:**
  1. Replicate Subject-Dependent Baseline: Train the LSTM on a single subject using 8-fold cross-validation to verify the 96% accuracy claim.
  2. Ablation Study (Modality): Retrain the model on the same subject using only EOG features and only Eye-tracking features. Compare confusion matrices to see which modality contributes to distinguishing the sign of the diopter.
  3. Normalization Stress Test: Take the "Subject-Independent" scenario and apply domain adaptation techniques to see if chance-level accuracy can be improved.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the significant disparity in feature dimensionality (93 eye-tracking vs. 8 EOG features) bias the multimodal model toward the eye-tracking modality?
- **Basis in paper:** The authors state that the imbalance may have limited the exploration of EOG's discriminative capacity and biased the multimodal configuration.
- **Why unresolved:** The current study utilized all extracted features without feature selection or balancing techniques.
- **What evidence would resolve it:** Re-evaluating performance after applying dimensionality reduction to the eye-tracking features or extracting deeper EOG features to balance input vector sizes.

### Open Question 2
- **Question:** Can personalized calibration strategies effectively mitigate inter-subject variability to improve subject-independent generalization?
- **Basis in paper:** The authors suggest investigating personalized calibration strategies, such as transforming features into shared latent representations, to address the lack of generalizability.
- **Why unresolved:** The subject-independent model performed only marginally above chance (8.882% accuracy), failing to generalize across the 37 participants.
- **What evidence would resolve it:** A study implementing subject-specific calibration tasks prior to classification to verify if statistical significance can be achieved in leave-one-subject-out scenarios.

### Open Question 3
- **Question:** How does the estimation framework perform when transferred to wearable, non-stationary equipment in real-world environments?
- **Basis in paper:** The authors note that the data was collected in a controlled lab with stationary equipment and identify improving wearability as a necessary direction for real-world applicability.
- **Why unresolved:** The current reliance on a chinrest and stationary cameras ensures data stability that does not exist in passive, mobile daily usage.
- **What evidence would resolve it:** Validation of the LSTM framework using data collected from mobile head-mounted devices during natural movement.

## Limitations
- The model fails to generalize across subjects, achieving only ~8.9% accuracy in subject-independent scenarios
- Simulated refractive errors via trial lenses may not fully capture the physiological responses of actual refractive disorders
- The significant feature imbalance (93 eye-tracking vs 8 EOG features) may bias the multimodal model toward eye tracking

## Confidence
- **High Confidence:** The multimodal model's superior performance in subject-dependent settings (96.2% accuracy) is well-supported by experimental results and statistical analysis
- **Medium Confidence:** The claim that multimodal fusion is "the most promising strategy" is supported within personalized context but not robustly validated for generalization
- **Low Confidence:** The broader claim that this approach enables "non-invasive vision screening" is overstated given near-chance performance in subject-independent scenarios

## Next Checks
1. **Domain Adaptation Experiment:** Apply domain adaptation techniques to the subject-independent scenario to determine if chance-level accuracy can be improved
2. **Physiological Plausibility Study:** Compare eye movement responses to simulated vs. actual refractive errors to validate the core assumption about trial lenses
3. **Feature Importance Analysis:** Perform an ablation study using SHAP or LIME to quantify contribution of each EOG and eye-tracking feature to classification