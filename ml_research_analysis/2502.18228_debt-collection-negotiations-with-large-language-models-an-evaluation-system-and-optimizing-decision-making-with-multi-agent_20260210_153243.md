---
ver: rpa2
title: 'Debt Collection Negotiations with Large Language Models: An Evaluation System
  and Optimizing Decision Making with Multi-Agent'
arxiv_id: '2502.18228'
source_url: https://arxiv.org/abs/2502.18228
tags:
- debtor
- debt
- financial
- negotiation
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses debt collection negotiations (DCN) by exploring
  the use of large language models (LLMs) for automation. The authors propose a comprehensive
  evaluation framework with 13 metrics across four aspects, revealing that LLMs tend
  to over-concede compared to human negotiators.
---

# Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent

## Quick Facts
- **arXiv ID:** 2502.18228
- **Source URL:** https://arxiv.org/abs/2502.18228
- **Reference count:** 37
- **Key outcome:** This paper addresses debt collection negotiations (DCN) by exploring the use of large language models (LLMs) for automation. The authors propose a comprehensive evaluation framework with 13 metrics across four aspects, revealing that LLMs tend to over-concede compared to human negotiators. To improve decision-making, they introduce the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules. Additionally, they apply post-training techniques like DPO with rejection sampling to optimize model performance. Experiments show that the MADeN framework and post-training methods significantly enhance negotiation outcomes, particularly in balancing debt recovery efficiency and debtor financial health.

## Executive Summary
This paper introduces a comprehensive evaluation framework and the MADeN multi-agent system to address the challenge of automating debt collection negotiations using large language models. The authors identify that LLMs tend to over-concede in negotiations, leading to suboptimal recovery rates. To mitigate this, MADeN incorporates planning and judging modules to guide the communication agent's decision-making. The paper also explores post-training optimization techniques, such as DPO with rejection sampling, to further improve model performance. The proposed methods demonstrate significant improvements in balancing creditor recovery and debtor welfare, as measured by the Comprehensive Collection Index (CCI).

## Method Summary
The paper proposes the MADeN framework, a multi-agent system for debt collection negotiations. It includes a Planning Agent that categorizes debtors and sets negotiation strategies, a Judging Agent that evaluates the rationality of actions, and a Communicating Agent that generates dialogue. The system is trained using Direct Preference Optimization (DPO) with rejection sampling on a synthetic dataset of 975 debt records. The evaluation uses 13 metrics, including Recovery Rate, Debtor's Health Index, and the Comprehensive Collection Index (CCI), to assess performance. The paper also explores the effectiveness of post-training techniques in optimizing model outcomes.

## Key Results
- The MADeN framework significantly improves negotiation outcomes, achieving a CCI of 0.814 compared to the human baseline of 0.840.
- LLMs without specialized architecture tend to over-concede, with repayment ratios below 95% and higher discount rates than human negotiators.
- Post-training techniques like DPO with rejection sampling enhance model performance, with DPO-MAG achieving a CCI of 0.768 and better Debtor's Health Index (DHI) than MADeN.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs systematically over-concede in debt negotiations relative to human baselines, reducing creditor recovery metrics while failing to proportionally improve debtor financial health.
- Mechanism: Baseline LLMs (without specialized architecture) optimize for agreement completion and conversational harmony, treating negotiation as a cooperative dialogue task rather than an adversarial allocation problem with asymmetric information. This alignment mismatch causes models to grant larger discounts, longer installment periods, and more lenient payment timelines than human collectors would accept.
- Core assumption: The human baseline (finance professionals) represents a rational recovery-maximizing strategy that appropriately balances creditor and debtor outcomes.
- Evidence anchors:
  - [abstract]: "Our experiments reveal that LLMs tend to over-concede compared to human negotiators."
  - [section 5.2]: "Most models tend to offer more generous concessions to debtors, both in repayment ratios and deadlines... all models except for the GPT series have repayment ratios below 95%, meaning they did not fully follow the prompt's guidelines."
  - [corpus]: Weak/missing - no direct corpus evidence on debt collection concession patterns; related work (EmoDebt, EvoEmo) focuses on emotional/strategic optimization but not baseline concession rates.
- Break condition: Over-concession effect disappears when: (1) debtor financial data is fully visible to creditor (eliminating information asymmetry), or (2) negotiation success metric rewards aggressive recovery regardless of debtor welfare.

### Mechanism 2
- Claim: Pre-negotiation debtor categorization by the Planning Agent reduces over-concession by constraining the action space before emotional or strategic pressure is applied.
- Mechanism: The Planning Agent maps debtor narratives to four discrete categories (based on current assets and future asset potential) and outputs a bounded negotiation framework. This front-loads strategic reasoning before the Communicating Agent engages with potentially manipulative debtor responses. By pre-committing to category-specific strategies (e.g., Category 2: high immediate payment, no discount), the system resists in-the-moment concession pressure.
- Core assumption: The four-category typology adequately captures the variation in debtor financial situations relevant to recovery outcomes.
- Evidence anchors:
  - [section 6.1]: "We categorize debtors into four distinct groups, each corresponding to different negotiation strategies and outcome spaces. This approach ensures that the model follows a consistent framework throughout the negotiation, avoiding deviations from the core objective."
  - [table 3]: "+ Planning" alone shows CRI increase (0.740→0.766) but DHI collapse (0.771→0.335), indicating strategy enforcement without flexibility harms debtor welfare.
  - [corpus]: No direct corpus evidence on category-based negotiation; related work (Advancing AI Negotiations) emphasizes scenario adaptation but not pre-commitment strategies.
- Break condition: Planning Agent's effectiveness degrades when: (1) debtor narratives are ambiguous or deliberately misleading, causing misclassification, or (2) real financial situations fall outside the four-category schema.

### Mechanism 3
- Claim: Real-time rationality evaluation by the Judging Agent corrects excessive concessions during negotiation, preserving both recovery efficiency and debtor health.
- Mechanism: The Judging Agent reviews each proposed creditor action against negotiation consistency principles (e.g., "if long installment granted, no discount needed"). It outputs corrections that the Communicating Agent integrates before responding to the debtor. This creates a deliberation loop that catches irrational concessions before they are committed, without requiring full re-negotiation.
- Core assumption: The Judging Agent's evaluation criteria (consistency, concession appropriateness) align with the true optimization objective (CCI maximization).
- Evidence anchors:
  - [section 6.1]: "The judging agent evaluates the debtor's decision after each round... It is set to be completely neutral and does not need to align with both sides."
  - [table 3]: "+ Judging" alone achieves highest CRI (0.840) but DHI remains moderate (0.648); combined MADeN achieves best balance (CRI=0.847, DHI=0.706).
  - [corpus]: No direct corpus evidence on judging mechanisms in negotiation; LLM-as-a-judge frameworks (referenced in paper) exist but not specifically for debt collection.
- Break condition: Judging loop fails when: (1) evaluation criteria are poorly specified in prompts, (2) turn-around latency is too slow for real-time conversation, or (3) judging becomes captured by either party's framing.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Both debtor and creditor agents use structured output (Thoughts, Dialogue, Action) to externalize reasoning. Understanding CoT is essential to debug why agents make specific decisions and to modify prompts effectively.
  - Quick check question: Given a negotiation transcript, can you identify where the creditor's "Thoughts" field fails to consider a relevant debtor constraint?

- **Multi-Agent Role Separation**
  - Why needed here: MADeN's effectiveness depends on three distinct roles (Communicating, Planning, Judging) with non-overlapping objectives. Engineers must understand why separation prevents single-agent optimization failures.
  - Quick check question: If you merged Planning and Judging into one agent, what specific failure mode would likely increase?

- **Direct Preference Optimization (DPO) with Rejection Sampling**
  - Why needed here: The paper shows DPO-MAG outperforms SFT-MAG, indicating preference-based alignment is superior for negotiation tasks. Understanding DPO is critical for reproducing or extending training results.
  - Quick check question: How does the "defective prompt" technique for generating negative samples differ from using random low-quality outputs?

## Architecture Onboarding

- **Component map:**
  Debtor Input (Ib + Ip) 
      ↓
  [Planning Agent] → Category + Strategy Framework
      ↓
  [Communicating Agent] ←→ [Judging Agent] (iteration loop)
      ↓
  Action Output (ask/reject/accept on 4 dimensions)

Training pipeline: Raw data → Multi-agent generation (MAG) → Rejection sampling (filter by CCI) → DPO training with negative samples

- **Critical path:**
  1. Debtor states financial hardship → Planning Agent classifies into Category 1-4
  2. Planning Agent outputs bounded negotiation space (e.g., "no discount, high immediate payment")
  3. Communicating Agent generates response → Judging Agent evaluates consistency
  4. If Judging rejects, Communicating revises before sending to debtor
  5. Loop until agreement on all 4 dimensions or max turns

- **Design tradeoffs:**
  - Planning-only: High recovery (CRI) but harms debtor health (DHI drops to 0.335)
  - Judging-only: Highest recovery (CRI 0.840) but moderate debtor protection
  - MADeN combined: Best balance (CCI 0.814 vs human 0.840)
  - DPO vs multi-agent: DPO-MAG achieves similar CCI (0.768) to MADeN with better DHI (0.632 vs 0.525), but requires training data curation

- **Failure signatures:**
  1. Creditor agrees to both high discount (≥10%) AND long installments (≥12 months) — indicates judging failure
  2. Recovery Rate < 90% with DHI < 0.6 — over-concession pattern
  3. Negotiation exceeds 10 turns without progress — poor planning categorization or debtor simulation mismatch
  4. L1D (hardest tier days) exceeds 20 — debtor trajectory forecast indicates unsustainable plan

- **First 3 experiments:**
  1. **Ablation test:** Run Planning-only, Judging-only, and full MADeN on 50 test samples; verify table 3 replication (CRI/DHI/CCI patterns)
  2. **Misclassification probe:** Manually assign incorrect categories to 20 debtors; measure CCI degradation to quantify Planning Agent sensitivity
  3. **Judging prompt stress test:** Weaken judging criteria (e.g., remove discount/installment consistency rule); confirm concession rates increase toward baseline LLM levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does incorporating complex, real-world financial variables—such as volatile cash flow and income fluctuations—alter the evaluation of LLM performance and the effectiveness of the MADeN framework?
- Basis in paper: [explicit] The Limitations section states that current research simplifies financial information to linear models with Gaussian noise, whereas real-world situations are more complex. The authors explicitly suggest that "future work should involve more detailed simulations of debtor information and comparisons with manually simulated debtors."
- Why unresolved: The current study relies on synthetic data generated via CTGAN and simplified linear estimates for daily surplus, which cannot capture the dynamic financial instability often seen in actual non-performing loan scenarios.
- What evidence would resolve it: Experiments comparing the current model's performance against a dataset enriched with high-granularity temporal financial data and manually simulated debtor behaviors.

### Open Question 2
- Question: Can traditional non-LLM automated decision models be integrated into the MADeN framework to provide superior planning and judging capabilities compared to the current prompt-based LLM approach?
- Basis in paper: [explicit] In the Limitations section, the authors note that their creditor multi-agent framework is relatively simple. They explicitly state: "We aim to integrate existing decision models to further optimize decision-making in the dialogue."
- Why unresolved: The current MADeN framework relies exclusively on LLM reasoning capabilities (Planning and Judging agents) without leveraging established mathematical optimization or decision theory models often used in finance.
- What evidence would resolve it: A hybrid system architecture where the Planning or Judging agents utilize external decision algorithms, benchmarked against the pure LLM implementation on the Comprehensive Collection Index (CCI).

### Open Question 3
- Question: Is the tendency of LLMs to "over-concede" in negotiations a generalizable trait across languages and cultural contexts, or is it specific to the Chinese language models and datasets evaluated in this study?
- Basis in paper: [inferred] The paper identifies that models tend to make "unsuitable concessions" potentially due to an "excessive focus on harmony." However, the experiments are restricted to Chinese language models (e.g., Qwen) and a Chinese financial context, where cultural norms regarding harmony may differ from Western contexts.
- Why unresolved: The experimental scope is monolingual and culturally specific; therefore, it remains unclear if this bias is inherent to the model architecture/training or influenced by the specific linguistic and cultural data.
- What evidence would resolve it: A cross-lingual replication of the study using Western-based models (e.g., Llama, GPT) on English-language debt negotiation scenarios to see if the "over-concession" gap between human and AI persists.

### Open Question 4
- Question: How does the performance of the MADeN framework degrade when the "Judging Agent" is exposed to adversarial or deceptive debtor tactics that were not present in the training or synthetic data distribution?
- Basis in paper: [inferred] The paper notes that debtors may "exaggerate their situation" and that the Judging Agent is set to be "completely neutral." However, the evaluation uses standard simulation rather than adversarial testing, leaving the robustness of the "Judging" module against sophisticated deception unverified.
- Why unresolved: The current evaluation assumes a standard distribution of debtor behaviors. It is unclear if the Judging Agent can maintain "decision rationality" when faced with novel or manipulative negotiation strategies designed to exploit the LLM's alignment toward helpfulness.
- What evidence would resolve it: An adversarial evaluation phase where human experts or red-team models attempt to manipulate the MADeN framework into irrational concessions.

## Limitations

- The synthetic dataset construction and the specific financial profiles of debtors are not fully disclosed, limiting reproducibility of the claimed performance improvements.
- The exact coefficients and noise parameters for the future economic prediction model are unspecified, making it difficult to verify the trajectory forecasting component.
- No direct empirical evidence is provided for the baseline concession rates of human negotiators, making the over-concession claim difficult to independently verify.

## Confidence

- **High confidence:** The MADeN framework architecture and its three-agent separation is clearly specified and theoretically sound
- **Medium confidence:** The 13-metric evaluation framework is comprehensive, though some metrics (like Asset Tier Variance) may have implementation dependencies
- **Low confidence:** The comparative advantage over human negotiators is asserted but relies on an inaccessible human baseline for validation

## Next Checks

1. Implement the synthetic data generator with the same distribution parameters and verify the statistical properties match the paper's reported ranges
2. Replicate the ablations (Planning-only, Judging-only, MADeN) on a small sample set to confirm the CRI/DHI/CCI patterns in Table 3
3. Test the Judging Agent's effectiveness by systematically weakening its criteria and measuring the increase in concession rates to quantify its contribution