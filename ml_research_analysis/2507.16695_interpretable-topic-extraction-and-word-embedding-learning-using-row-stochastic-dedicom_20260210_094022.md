---
ver: rpa2
title: Interpretable Topic Extraction and Word Embedding Learning using row-stochastic
  DEDICOM
arxiv_id: '2507.16695'
source_url: https://arxiv.org/abs/2507.16695
tags:
- topic
- word
- matrix
- words
- dedicom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a row-stochastic variation of the DEDICOM
  algorithm for interpretable topic extraction and word embedding learning. The method
  factorizes PMI matrices from text corpora into topic-specific word embeddings and
  interpretable topic relationships.
---

# Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM

## Quick Facts
- arXiv ID: 2507.16695
- Source URL: https://arxiv.org/abs/2507.16695
- Reference count: 40
- This paper introduces a row-stochastic variation of the DEDICOM algorithm for interpretable topic extraction and word embedding learning

## Executive Summary
This paper presents a novel approach to topic modeling that combines interpretable topic extraction with word embedding learning through a row-stochastic variation of the DEDICOM algorithm. The method factorizes Pointwise Mutual Information (PMI) matrices derived from text corpora into topic-specific word embeddings and interpretable topic relationships. By constraining the loading matrix to be row-stochastic, each word embedding becomes a probability distribution over latent topics, enabling semantic interpretation. The approach is trained via alternating gradient descent with Adam optimization and evaluated on semi-artificial documents combining Wikipedia articles.

## Method Summary
The proposed method factorizes PMI matrices from text corpora using a row-stochastic DEDICOM algorithm. The key innovation is constraining the loading matrix to be row-stochastic, ensuring that each word embedding represents a probability distribution over latent topics. The factorization takes the form of decomposing a PMI matrix into a diagonal matrix of topic weights, a row-stochastic loading matrix, and a topic-topic matrix. Training is performed through alternating gradient descent with Adam optimization, where one component is updated while others are held fixed. The approach is evaluated on semi-artificial documents created by combining Wikipedia articles, demonstrating successful recovery of thematic topics and meaningful word similarity based on cosine distance.

## Key Results
- Successfully recovers thematic topics (soccer, bees, Johnny Depp) from semi-artificial documents
- Learned embeddings capture both topic clusters and semantic relationships
- Outperforms competing methods (NMF, LDA, SVD) on interpretability and semantic coherence

## Why This Works (Mechanism)
The row-stochastic constraint on the loading matrix ensures that each word embedding represents a probability distribution over topics, making the factorization interpretable. The PMI matrix captures word co-occurrence statistics, which the DEDICOM factorization decomposes into topic-specific components. The alternating gradient descent with Adam optimization allows efficient learning of all three matrix components while maintaining the row-stochastic constraint through appropriate normalization.

## Foundational Learning
- Pointwise Mutual Information (PMI): Measures association between word pairs; needed to capture word co-occurrence statistics for factorization
- Quick check: PMI values should be positive for strongly associated words and negative for unrelated words
- DEDICOM decomposition: Factorizes matrices into interpretable components; needed to separate topic structure from word embeddings
- Quick check: The product of the three matrices should approximate the original PMI matrix
- Row-stochastic matrices: Matrices where each row sums to 1; needed to interpret word embeddings as probability distributions
- Quick check: Sum each row of the loading matrix to verify it equals 1

## Architecture Onboarding

### Component Map
PMI Matrix -> DEDICOM Factorization -> (Diagonal Topic Weights) x (Row-Stochastic Loading Matrix) x (Topic-Topic Matrix)

### Critical Path
1. Compute PMI matrix from corpus
2. Initialize DEDICOM components
3. Train via alternating gradient descent with Adam
4. Extract interpretable topics and word embeddings

### Design Tradeoffs
- Row-stochastic constraint vs. flexibility: Enables interpretability but may limit expressiveness
- Alternating optimization vs. joint optimization: Simpler implementation but may converge slower
- PMI-based factorization vs. direct co-occurrence: Captures statistical associations but requires more computation

### Failure Signatures
- Row-stochastic constraint violation: Indicates numerical instability or poor initialization
- Slow convergence: May suggest suboptimal learning rate or poor initialization
- Poor topic separation: Could indicate insufficient latent dimension or inadequate corpus size

### First Experiments
1. Test on simple synthetic corpus with known topics
2. Compare factorization quality on held-out word pairs
3. Evaluate topic interpretability through manual inspection

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks comparison to established word embedding methods on standard semantic similarity benchmarks
- Evaluation relies primarily on qualitative analysis of semi-artificial documents
- Semi-artificial test case may not represent challenges of real-world text corpora

## Confidence
- High confidence in mathematical formulation and implementation
- Medium confidence in interpretability claims due to limited quantitative evaluation
- Low confidence in generalizability to diverse real-world datasets

## Next Checks
1. Evaluate learned word embeddings on established semantic similarity benchmarks and compare against Word2Vec and GloVe
2. Test method on multiple diverse real-world text corpora to assess robustness across domains
3. Conduct ablation studies to quantify contribution of row-stochastic constraint compared to unconstrained DEDICOM