---
ver: rpa2
title: Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved
  Sample Complexity
arxiv_id: '2510.04189'
source_url: https://arxiv.org/abs/2510.04189
tags:
- have
- critic
- actor
- algorithm
- termi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces the first natural critic-actor algorithm
  with function approximation for the long-run average cost setting under inequality
  constraints. The method employs a three-timescale update scheme: average cost and
  actor on the fastest timescale, critic on a slower timescale, and Lagrange multiplier
  on the slowest timescale.'
---

# Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity

## Quick Facts
- **arXiv ID**: 2510.04189
- **Source URL**: https://arxiv.org/abs/2510.04189
- **Reference count**: 40
- **Key outcome**: First natural critic-actor algorithm for long-run average cost CMDPs with sample complexity bound $\tilde{O}(\epsilon^{-(2+\delta)})$ (improved to $\tilde{O}(\epsilon^{-2})$ with log-modified rates)

## Executive Summary
This paper introduces a natural critic-actor algorithm for constrained reinforcement learning in the average cost setting. The method employs a three-timescale update scheme with actor/policy updates on the fastest timescale, critic on a slower timescale, and Lagrange multipliers on the slowest timescale. Non-asymptotic convergence guarantees are established with sample complexity bounds matching state-of-the-art two-timescale algorithms. The approach uses natural gradient preconditioning for efficient policy updates and demonstrates competitive performance on Safety-Gym environments.

## Method Summary
The algorithm solves constrained MDPs with long-run average cost objective under inequality constraints. It employs three-timescale updates: average cost and actor parameters on the fastest timescale (step-size $a(n)$), critic value function on a slower timescale (step-size $b(n)$), and Lagrange multipliers on the slowest timescale (step-size $c(n)$). The critic uses TD(0) learning with linear function approximation, while the actor employs natural gradient updates preconditioned by the Fisher Information Matrix. The method handles constraints through Lagrangian relaxation with projection onto a bounded set. Modified learning rates with logarithmic factors achieve improved sample complexity of $\tilde{O}(\epsilon^{-2})$.

## Key Results
- Establishes finite-time convergence guarantees for natural critic-actor algorithm with sample complexity $\tilde{O}(\epsilon^{-(2+\delta)})$
- Achieves optimal sample complexity $\tilde{O}(\epsilon^{-2})$ with modified logarithmic learning rates
- Demonstrates competitive performance on Safety-Gym environments compared to C-AC, C-NAC, and C-CA baselines
- First natural critic-actor algorithm for long-run average cost setting with inequality constraints

## Why This Works (Mechanism)

### Mechanism 1: Timescale Separation for Iterate Stability
Reversing standard Actor-Critic timescales (Actor faster than Critic) stabilizes the system by approximating Value Iteration rather than Policy Iteration. The algorithm employs a three-timescale update: (1) Average cost/Actor (fastest), (2) Critic (slower), and (3) Lagrange multipliers (slowest). By updating the policy faster than the critic, the policy effectively sees the critic as quasi-static during its update, while the critic perceives the policy as near-equilibrium. This mimics the stability properties of Value Iteration.

### Mechanism 2: Natural Gradient Preconditioning
Preconditioning the policy gradient with the inverse Fisher Information Matrix ($G(t)^{-1}$) improves the efficiency of policy updates compared to standard gradients. The actor update scales the Temporal Difference (TD) error by $G(t)^{-1}\Psi_{s_n a_n}$. This effectively rotates the gradient direction to account for the curvature of the policy distribution, theoretically yielding larger consistent steps without increasing variance.

### Mechanism 3: Logarithmic Step-Size Annealing
Introducing logarithmic factors into polynomial learning rates improves sample complexity from $\tilde{O}(\epsilon^{-(2+\delta)})$ to the optimal $\tilde{O}(\epsilon^{-2})$. Standard polynomial rates $1/t^\nu$ leave a residual error $\delta$. The paper proposes modified rates (e.g., $a(t) \propto \frac{(\ln t)^{1/2}}{t^\nu}$). This subtle annealing slows the decay just enough to tighten the convergence bounds to match single-timescale rates while maintaining the stability of the two-timescale structure.

## Foundational Learning

- **Concept**: **Multi-timescale Stochastic Approximation**
  - **Why needed here**: The algorithm's stability relies entirely on the ODE method where fast dynamics track equilibria of slow dynamics.
  - **Quick check question**: Can you explain why the actor learning rate $a(t)$ must decay faster than the critic rate $b(t)$ for the "Critic-Actor" architecture to approximate Value Iteration?

- **Concept**: **Lagrangian Relaxation in CMDPs**
  - **Why needed here**: The algorithm handles inequality constraints (Safety RL) by converting them into penalties in the cost function via Lagrange multipliers ($\gamma$).
  - **Quick check question**: If the Lagrange multiplier $\gamma_k$ grows indefinitely, what does that imply about the constraint $G_k(\pi)$?

- **Concept**: **Linear Function Approximation & TD(0)**
  - **Why needed here**: The critic uses features $\phi(s)$ and TD error $\delta_n$ to learn value weights $v$.
  - **Quick check question**: Does Theorem 3 guarantee convergence to the true value function or to the best projection within the linear feature space?

## Architecture Onboarding

- **Component map**: Average Cost/Actor (fastest) -> Critic (slower) -> Lagrange Multipliers (slowest)
- **Critical path**:
  1. Sample transition $(s, a, s')$ and costs $(q, h_k)$
  2. Update $L_{t+1}$ (average cost) and $G_{t+1}$ (Fisher)
  3. Calculate TD error $\delta_t$
  4. Update Critic $v_{t+1}$
  5. Update Actor $\theta_{t+1}$
  6. Update Lagrange $\gamma_{t+1}$

- **Design tradeoffs**:
  - Optimality vs. Constraint Satisfaction: A high Lagrange learning rate $c(n)$ prioritizes safety but may degrade cost performance
  - Rate vs. Stability: The "Modified" version achieves $\tilde{O}(\epsilon^{-2})$ but requires strict adherence to the constant bounds; the standard version is more robust but slower

- **Failure signatures**:
  - Divergent Critic: If the matrix $A$ is not negative definite, $v$ explodes
  - Constraint Oscillation: If $c(n)$ is too large relative to $a(n)$, $\gamma$ will overshoot, causing the policy to alternate between risky and overly conservative behaviors

- **First 3 experiments**:
  1. Step-Size Ratio Validation: Run a grid search on $c_a/c_d$ to verify the inequality against the theoretical failure threshold
  2. Ablation on Modified Rates: Compare standard polynomial rates vs. log-modified rates to validate the claimed $\tilde{O}(\epsilon^{-2})$ improvement
  3. Safety-Gym Baseline: Reproduce the SafetyAntCircle experiment to verify constraint satisfaction against the unconstrained baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the finite-time convergence guarantees be extended to non-linear function approximation, such as neural networks?
- **Basis**: The analysis is strictly limited to linear function approximation (Section 3.3) and relies on bounded feature vectors (Assumption 1).
- **Why unresolved**: The proofs utilize linear properties of the operator $A$ and projection onto a convex set $C$, which do not translate directly to the non-convex landscapes of neural networks.
- **What evidence would resolve it**: A finite-time analysis for a neural network variant or theoretical bounds specifically addressing the approximation error in the non-linear regime.

### Open Question 2
- **Question**: Is the negative definiteness of matrix $A$ (Assumption 2) strictly necessary for convergence, or can it be relaxed?
- **Basis**: Assumption 2 states "The matrix $A$... is negative definite, with its largest eigenvalue given by $-\lambda_e < 0$."
- **Why unresolved**: This is a strong structural assumption required to prove the existence of $v^*$, but it may not hold for arbitrary feature maps or constrained settings, potentially limiting the applicability of the guarantees.
- **What evidence would resolve it**: A convergence proof under weaker assumptions (e.g., specific stability conditions) or an empirical study showing convergence even when the assumption is violated.

### Open Question 3
- **Question**: Can the optimal sample complexity of $\tilde{O}(\epsilon^{-2})$ be achieved without the logarithmic modification to the learning rates?
- **Basis**: The paper achieves the improved bound using step-sizes modified by $(\ln(t+1))^{1/2}$ (Section 4.3), implying standard step-sizes may be insufficient for this rate.
- **Why unresolved**: It is unclear if the log factors are a mathematical artifact of the proof technique or a fundamental requirement for stabilizing the three-timescale update scheme at that speed.
- **What evidence would resolve it**: A lower bound proof or a modified analysis showing the standard Robbins-Monro step-sizes can achieve $\tilde{O}(\epsilon^{-2})$.

## Limitations
- Theoretical analysis assumes uniform ergodicity, which may not hold for all Safety-Gym environments
- Modified logarithmic learning rates require careful tuning of constant ratio $c_a/c_d$ to satisfy stability inequality
- Fisher matrix preconditioning assumes the policy remains sufficiently stochastic; near-deterministic policies could cause numerical issues

## Confidence
- **High**: Sample complexity bounds (Theorem 6) and convergence analysis under stated assumptions
- **Medium**: Empirical performance claims on Safety-Gym; results appear competitive but limited to three environments
- **Medium**: Mechanism of timescale separation providing stability; theoretical justification is strong but practical sensitivity to learning rate ratios is not fully explored

## Next Checks
1. **Step-size sensitivity**: Systematically vary the ratio $c_a/c_d$ around the theoretical bound in Eq. 13 to identify the stability threshold
2. **Constraint violation monitoring**: During training, track constraint costs $G_k(\pi)$ relative to thresholds $\alpha_k$ to verify the Lagrange multiplier mechanism effectively enforces safety
3. **Fisher matrix conditioning**: Monitor the condition number of $G(n)$ throughout training to detect potential numerical instability as policies approach determinism