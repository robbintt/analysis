---
ver: rpa2
title: 'Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis:
  A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice'
arxiv_id: '2509.06854'
source_url: https://arxiv.org/abs/2509.06854
tags:
- joint
- hand
- images
- arthritis
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ARTSS, a deep learning framework that automates
  Total Sharp Score assessment in rheumatoid arthritis using hand X-ray images. The
  system employs ResNet50 for image orientation, U-Net for hand segmentation, YOLOv7
  for joint identification, and multiple models including Vision Transformer for TSS
  prediction.
---

# Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice

## Quick Facts
- arXiv ID: 2509.06854
- Source URL: https://arxiv.org/abs/2509.06854
- Reference count: 38
- Primary result: 99% joint identification accuracy and Huber loss of 0.87 for TSS prediction

## Executive Summary
This study presents ARTSS, a deep learning framework that automates Total Sharp Score assessment in rheumatoid arthritis using hand X-ray images. The system employs a four-stage pipeline: orientation correction with ResNet50, hand segmentation with U-Net, joint identification with YOLOv7, and TSS prediction with Vision Transformer. Evaluated on 970 patients, ARTSS achieved 99% joint identification accuracy and notably low Huber loss of 0.87 for TSS prediction, addressing challenges of joint disappearance and variable-length sequences through padding and masking techniques.

## Method Summary
ARTSS implements a four-stage pipeline to automate TSS scoring: image pre-processing with ResNet50 for orientation alignment, hand segmentation with U-Net, joint identification with YOLOv7 (11 joint classes per hand), and TSS prediction using Vision Transformer. The framework handles variable joint counts through padding and masking, enabling inclusion of severe RA cases with joint destruction. Models were trained on 970 hand X-ray images from 970 patients with 3-fold cross-validation, plus external testing on 291 subjects.

## Key Results
- 99% mean Average Precision for joint identification
- ViT achieved Huber loss of 0.87 for TSS prediction
- U-Net segmentation achieved 0.94 IoU
- Model outperformed multiple CNN architectures including VGG19 (MAE 2.99 vs 0.95)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing TSS prediction into sequential sub-tasks improves overall system accuracy
- Mechanism: The four-stage pipeline isolates distinct computational problems: orientation correction (ResNet50), anatomical segmentation (U-Net), joint localization (YOLOv7), and score regression (ViT/CNNs). Each stage is trained independently with task-specific loss functions, preventing error cascades from compounding.
- Core assumption: Errors at each stage can be independently minimized without joint optimization.
- Evidence anchors: Structured four-stage workflow described in abstract and Figure 1; similar staged approaches used in neighbor paper on Layer Separation.

### Mechanism 2
- Claim: Padding with masking enables processing of patients with joint disappearance
- Mechanism: The system identifies the maximum joint count across all patients, pads shorter sequences to uniform length, then applies masking during training so padded regions don't contribute to loss. This allows inclusion of severe RA cases typically excluded from studies.
- Core assumption: Masking effectively prevents the model from learning spurious patterns from padding tokens.
- Evidence anchors: Abstract mentions padding and masking techniques; Pages 7-8 describe implementation details.

### Mechanism 3
- Claim: Vision Transformer outperforms CNN architectures for TSS regression by capturing global joint relationships
- Mechanism: ViT's self-attention mechanism processes joint patches in parallel, potentially modeling inter-joint dependencies that CNNs with local receptive fields may miss. Huber loss provides robustness to score distribution imbalance.
- Core assumption: Joint relationships (not just individual joint appearance) contribute meaningfully to TSS prediction.
- Evidence anchors: ViT achieved MAE 0.95 vs VGG19's 2.99; Huber loss 0.87 vs 2.97 (Table 1, Page 11).

## Foundational Learning

- Concept: **U-Net architecture for semantic segmentation**
  - Why needed here: Stage II requires isolating hand anatomy from background before joint detection
  - Quick check question: Can you explain why skip connections help preserve spatial resolution in medical image segmentation?

- Concept: **YOLO object detection fundamentals**
  - Why needed here: Stage III must localize 11 distinct joint regions per hand with bounding boxes
  - Quick check question: How does YOLO's single-pass detection differ from two-stage detectors like Faster R-CNN, and why might speed matter for clinical deployment?

- Concept: **Huber loss for robust regression**
  - Why needed here: TSS distribution is imbalanced (more low scores); Huber loss reduces outlier influence
  - Quick check question: For what error threshold δ would Huber loss switch from quadratic to linear behavior, and why does this matter for skewed target distributions?

## Architecture Onboarding

- Component map: Input X-ray -> [ResNet50: 90° orientation] -> [U-Net: hand mask] -> [YOLOv7: 11 joint bboxes/hand] -> [Padding+Masking] -> [ViT: TSS regression] -> Score
- Critical path: Joint identification accuracy directly bounds TSS prediction—if YOLO misses joints, ViT receives incomplete inputs. The 99% joint detection accuracy is the gating metric.
- Design tradeoffs: ViT vs CNNs: ViT achieves best scores but requires more data; VGG19 is 3x worse but simpler. Pipeline vs end-to-end: Modular training is reproducible but may miss global optimizations. Including severe cases: Better generalizability but requires padding/masking complexity.
- Failure signatures: Missed joints in deformed hands (Page 13, Fig 4.C.II shows model missing joints); Orientation failures beyond 0-180° range (Page 14 limitation); Single-center training data limiting generalization.
- First 3 experiments: 1) Reproduce joint identification on the public dataset—verify 99% mAP before proceeding, 2) Ablation study: Train VGG19 vs ViT on identical folds to confirm performance gap, 3) Stress test: Evaluate on images with intentionally rotated hands outside the 0-180° training distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of pan-omics and radiomics data significantly improve the predictive accuracy of the ARTSS system compared to radiographic analysis alone?
- Basis in paper: The authors state in the conclusion that integrating multidimensional data (pan-omics) and high-throughput image data (radiomics) "may further enhance the accuracy and utility of automatic scoring systems."
- Why unresolved: The current study validates the framework using only hand X-ray images; it does not incorporate or test the additional biological or texture-based data layers suggested.
- What evidence would resolve it: A comparative study evaluating TSS prediction performance between the current vision-only model and a multi-modal model trained on combined imaging, genomic, and molecular data.

### Open Question 2
- Question: Can the ARTSS framework be effectively generalized to score foot joints, which are necessary for a complete Total Sharp Score assessment but were excluded from this study?
- Basis in paper: The authors identify the limitation that the study was "limited to hand radiographs, leaving it unclear whether the ARTSS system would perform equally well on the feet joint."
- Why unresolved: The current YOLOv7 and ViT models are trained specifically on hand anatomical structures (PI, PIP, MCP, wrist), which differ morphologically from foot joints.
- What evidence would resolve it: Training and evaluating the existing pipeline on a dataset of foot radiographs to determine if similar joint identification accuracy (99%) and TSS prediction error (Huber loss ~0.87) can be achieved.

### Open Question 3
- Question: How does the ARTSS system perform across multicenter datasets with variable imaging protocols and equipment different from the single-center data used for training?
- Basis in paper: The authors note the dataset originated from a single institution (Anhui University), which "may limit the generalizability of our findings," and explicitly recommend establishing a "public multicenter database."
- Why unresolved: While external testing was performed, the data source was likely homogenous regarding imaging parameters; distinct imaging protocols or scanner types in other centers may degrade the model's orientation or segmentation accuracy.
- What evidence would resolve it: External validation studies conducted on hand X-rays collected from diverse geographical locations and hospital systems to verify robustness against domain shift.

## Limitations

- Modular pipeline design prevents end-to-end optimization
- Key architectural details underspecified (ViT hyperparameters, masking implementation)
- Single-center training data limits generalizability
- Orientation correction assumes ±90° rotation only

## Confidence

- High confidence: Stage-wise decomposition approach is sound; segmentation and joint detection metrics are verifiable; masking for variable-length sequences is technically valid
- Medium confidence: ViT's superiority for TSS regression is supported by reported metrics but lacks architectural specification; 99% joint detection accuracy needs independent verification
- Low confidence: System robustness to diverse acquisition protocols; performance on multi-center data; handling of severe joint destruction beyond the current dataset

## Next Checks

1. Reproduce joint identification on the public dataset—verify the 99% mAP claim before proceeding to TSS prediction
2. Perform ablation study: train VGG19 vs ViT on identical folds to confirm the reported 3x performance gap
3. Stress test orientation correction on images rotated beyond the 0-180° training distribution to identify failure modes