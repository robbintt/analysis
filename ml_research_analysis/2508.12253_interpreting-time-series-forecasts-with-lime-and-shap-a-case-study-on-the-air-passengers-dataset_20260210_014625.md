---
ver: rpa2
title: 'Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the
  Air Passengers Dataset'
arxiv_id: '2508.12253'
source_url: https://arxiv.org/abs/2508.12253
tags:
- shap
- series
- arima
- lime
- xgboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for interpreting time-series
  forecasts using LIME and SHAP methods. The approach converts a univariate time series
  into a supervised learning problem with lagged features, rolling statistics, and
  seasonal encodings, then applies post-hoc explainability to gradient-boosted tree
  and ARIMA models.
---

# Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset

## Quick Facts
- arXiv ID: 2508.12253
- Source URL: https://arxiv.org/abs/2508.12253
- Reference count: 2
- XGBoost achieves slightly better accuracy than ARIMA (RMSE 12.97 vs 13.25, MAPE 5.21% vs 5.42%) on the Air Passengers dataset

## Executive Summary
This paper presents a unified framework for interpreting time-series forecasts using LIME and SHAP methods. The approach converts a univariate time series into a supervised learning problem with lagged features, rolling statistics, and seasonal encodings, then applies post-hoc explainability to gradient-boosted tree and ARIMA models. Using the Air Passengers dataset, the framework demonstrates that a small set of features—particularly the twelve-month lag and seasonal encodings—explain most forecast variance.

## Method Summary
The framework transforms a univariate time series into a supervised learning problem by creating lagged features, rolling statistics, and seasonal encodings. LIME and SHAP are then applied as post-hoc explainability methods to both gradient-boosted tree models (XGBoost) and ARIMA models. The Air Passengers dataset serves as the case study, where forecast performance is compared between XGBoost and ARIMA, and interpretability is analyzed through feature importance rankings and local explanations.

## Key Results
- XGBoost achieves slightly better accuracy than ARIMA (RMSE 12.97 vs 13.25, MAPE 5.21% vs 5.42%) on the hold-out period, though differences are not statistically significant
- SHAP and permutation importance analyses consistently identify the twelve-month lag as the dominant feature
- Local LIME explanations are stable across different kernel widths, with median R² of 0.86 for surrogate models

## Why This Works (Mechanism)
The framework works by leveraging established interpretability methods (LIME and SHAP) in a novel context—time series forecasting. By converting time series data into a supervised learning format with engineered features that capture temporal dependencies, the framework enables the application of these post-hoc explanation methods. The success stems from how well SHAP and LIME can decompose complex model predictions into interpretable feature contributions, while the feature engineering ensures that temporal patterns are properly represented.

## Foundational Learning
- **Time series to supervised learning conversion** (why needed: enables use of standard ML interpretability tools; quick check: verify lag features properly align with target variable)
- **SHAP values** (why needed: provide additive feature attribution for any model; quick check: ensure SHAP values sum to model output)
- **LIME local surrogate models** (why needed: approximate black-box model behavior in local regions; quick check: validate R² of surrogate models)
- **Permutation importance** (why needed: model-agnostic feature importance ranking; quick check: confirm importance scores correlate with model performance degradation)
- **Rolling statistics** (why needed: capture local trends and volatility; quick check: verify window sizes match expected patterns)
- **Seasonal encodings** (why needed: represent cyclical patterns explicitly; quick check: confirm encodings align with known seasonality)

## Architecture Onboarding

**Component map:** Time Series -> Feature Engineering -> Model Training -> Interpretability (LIME/SHAP) -> Feature Importance Analysis

**Critical path:** Feature engineering creates lagged features, rolling statistics, and seasonal encodings that feed into model training. The trained model's predictions are then explained using SHAP and LIME methods to generate feature importance rankings and local explanations.

**Design tradeoffs:** The framework prioritizes interpretability over model complexity, using relatively simple feature engineering and post-hoc explanation methods rather than building inherently interpretable time series models. This makes the approach broadly applicable but may miss some temporal dynamics.

**Failure signatures:** If SHAP values are inconsistent across similar forecast instances, or if LIME surrogate models have low R² scores, this indicates either model instability or inadequate feature representation. Poor feature importance rankings that don't align with domain knowledge suggest the feature engineering step needs refinement.

**3 first experiments:**
1. Test framework on multiple time series datasets with varying characteristics
2. Compare interpretability results when using different sets of engineered features
3. Evaluate stability of SHAP values under data perturbations

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to a single dataset (Air Passengers) with a fixed time horizon, constraining generalizability
- Comparison between XGBoost and ARIMA shows only marginal performance differences that are not statistically significant
- Interpretability analysis relies heavily on feature importance rankings without exploring variation across different forecast horizons

## Confidence

**High:** The methodology for converting time series to supervised learning problems and applying LIME/SHAP is technically sound

**Medium:** The feature importance conclusions are consistent but may not generalize beyond the Air Passengers dataset

**Low:** The claim that LIME surrogate models achieve "stable" local explanations based on median R² = 0.86 lacks sensitivity analysis across different forecast targets

## Next Checks
1. Test the framework on multiple time series datasets with varying characteristics (trend strength, seasonality patterns, noise levels) to assess generalizability
2. Conduct statistical significance testing for performance differences between XGBoost and ARIMA across multiple train-test splits
3. Perform robustness analysis of feature importance rankings by introducing controlled perturbations to the time series data and measuring stability of SHAP values