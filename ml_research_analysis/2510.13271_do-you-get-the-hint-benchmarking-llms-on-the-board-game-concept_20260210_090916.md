---
ver: rpa2
title: Do You Get the Hint? Benchmarking LLMs on the Board Game Concept
arxiv_id: '2510.13271'
source_url: https://arxiv.org/abs/2510.13271
tags:
- clues
- concept
- llms
- game
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Concept, a word-guessing board game, as a
  benchmark to evaluate large language models' (LLMs) abductive reasoning abilities
  in a natural language setting. The game involves one player providing clues from
  a predefined set to describe a concept, while another player must guess the concept.
---

# Do You Get the Hint? Benchmarking LLMs on the Board Game Concept

## Quick Facts
- arXiv ID: 2510.13271
- Source URL: https://arxiv.org/abs/2510.13271
- Reference count: 15
- LLMs struggle significantly with abductive reasoning in the board game Concept, with no model exceeding 40% success rate

## Executive Summary
This study introduces the board game Concept as a benchmark to evaluate large language models' (LLMs) abductive reasoning abilities in natural language settings. The game requires one player to provide hierarchical, colored clues from a predefined vocabulary to describe a concept, while another player must infer the target concept. The researchers collected game logs in English, Dutch, French, and Spanish, and tested seven state-of-the-art LLMs using both static and dynamic prompting approaches. Results show that LLMs struggle significantly compared to humans, particularly in interpreting strategic intent and updating hypotheses with new information. Performance is consistently worse in lower-resource languages, and even reasoning models do not outperform non-reasoning models.

## Method Summary
The study uses human game logs from Board Game Arena, filtered to exclude rounds with no clues. Two prompting modes are implemented: static (all final clues given at once) and dynamic (clues revealed step-by-step with incorrect guesses added incrementally). The hierarchical clue formatting groups clues by color with high-level pawns and low-level cubes. Models are queried with exact-match evaluation, using temperature=0.1 and max_output_tokens=10 for non-reasoning models, and temperature=1 with max_output_tokens=1000 for GPT-OSS-120B. The dataset is publicly available at https://huggingface.co/datasets/IneG/concept.

## Key Results
- LLMs struggle with abductive reasoning, with no model exceeding 40% success rate
- Dynamic prompting with sequential information updates degrades performance rather than improving it
- Model performance drops further in lower-resource languages (Dutch, French, Spanish) compared to English

## Why This Works (Mechanism)

### Mechanism 1
LLMs fail to infer the most plausible concept from incomplete, abstract clues despite natural language alignment. The guesser must search an open semantic space using associative skills and common sense to combine hierarchical clues into a coherent interpretation. LLMs struggle to meaningfully combine clues within the shared conceptual framework humans use.

### Mechanism 2
Dynamic prompting with sequential information updates degrades performance rather than improving it. In the dynamic setting, clues are updated iteratively and both human and LLM incorrect guesses are added to context. LLMs fail to update initial interpretations, often repeating previous incorrect answers even when explicitly informed.

### Mechanism 3
Abductive reasoning capabilities do not transfer robustly to lower-resource languages. All tested LLMs perform worse in Dutch, French, and Spanish than in English. Spanish yields slightly better results, potentially due to larger representation in training corpora.

## Foundational Learning

- **Abductive Reasoning**: Why needed here - The core task requires inferring the best explanation from incomplete evidence (clues) using background knowledge. Quick check - Given clues "Building" + "Metal" + "Blue/White/Red flag," can you infer "Eiffel Tower"?

- **Theory of Mind (ToM)**: Why needed here - Interpreting why Player 1 selected specific clues—and adapting when clues change—requires reasoning about another agent's intent. Quick check - If a clue-giver adds "Metal" after you guessed "Skyscraper," what might they be signaling?

- **Hierarchical Clue Interpretation**: Why needed here - Clues are organized by color and level; low-level clues must be interpreted relative to their high-level anchor of the same color. Quick check - If the green pawn is "Animal" and green cubes are "Stripes" and "Orange," what hierarchical inference is required?

## Architecture Onboarding

- **Component map**: Human game logs (Board Game Arena) -> filtered rounds -> static or dynamic prompt -> LLM guesser -> exact match evaluation (prec@k). Static uses final clue set; dynamic updates per step.

- **Critical path**: Prompt engineering (single example, hierarchical clue formatting) -> model inference (10 output tokens, temperature 0.1) -> normalization -> exact match. For dynamic: iterate steps until correct or exhausted.

- **Design tradeoffs**: Exact match is strict but reproducible; synonyms count as incorrect. Static prompting is simpler but misses ToM evaluation; dynamic is more realistic but degrades LLM performance. One-shot example balances guidance vs. cognitive overload.

- **Failure signatures**: (1) Repeating incorrect answers despite context updates; (2) Generating guesses inconsistent with top-level category (e.g., non-building when first clue is "Building"); (3) Reasoning models timing out before producing an answer.

- **First 3 experiments**:
  1. Run static vs. dynamic prompting on filtered English data to confirm gap; log where models repeat answers.
  2. Evaluate same models on Dutch/French/Spanish using translated prompts; compare delta vs. English.
  3. Ablate the one-shot example: test zero-shot vs. one-shot to measure example utility.

## Open Questions the Paper Calls Out

- Does implementing the game as a multi-agent setting, where diverse models guess sequentially and view each other's outputs, improve success rates compared to isolated agents?
- Is the model's ability to solve a concept directly correlated with the frequency of that concept in the model's pre-training data?
- Can supervised fine-tuning on human game logs significantly improve an LLM's capacity for dynamic hypothesis correction?

## Limitations

- Strict exact-match evaluation may underestimate model capability by not accounting for synonyms and multi-word concept variants
- Language-specific performance differences lack direct evidence linking them to training data frequency due to unavailable model documentation
- Static prompt may not fully capture Theory of Mind requirements as it provides all clues simultaneously

## Confidence

- High confidence: LLMs fail to correct initial hypotheses when given sequential information updates
- Medium confidence: Performance degradation in lower-resource languages correlates with training data frequency
- Low confidence: Static prompting fails to test abductive reasoning adequately

## Next Checks

1. Conduct a human baseline comparison using static prompts only to determine if the performance gap is due to reasoning difficulty or prompt design
2. Test models on a modified dynamic prompt that excludes incorrect guesses from context to isolate whether performance degradation stems from hypothesis revision failure or negative context interference
3. Analyze model attention patterns during incorrect answer repetition to determine if the failure is due to memory limitations or genuine inability to update interpretations