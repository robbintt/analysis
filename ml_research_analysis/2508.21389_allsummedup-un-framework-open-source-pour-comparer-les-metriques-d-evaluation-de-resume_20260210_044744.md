---
ver: rpa2
title: 'AllSummedUp: un framework open-source pour comparer les metriques d''evaluation
  de resume'
arxiv_id: '2508.21389'
source_url: https://arxiv.org/abs/2508.21389
tags:
- triques
- pour
- dans
- valuation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines reproducibility challenges in automatic summarization
  evaluation, comparing six metrics including classical approaches like ROUGE and
  LLM-based methods (G-Eval, SEval-Ex). Experiments reveal significant discrepancies
  between reported and observed performance, highlighting a trade-off: metrics with
  highest human alignment are computationally intensive and less stable across runs.'
---

# AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume

## Quick Facts
- arXiv ID: 2508.21389
- Source URL: https://arxiv.org/abs/2508.21389
- Reference count: 0
- Classical metrics (ROUGE, BERTScore) lack discriminative power for modern summarization systems

## Executive Summary
This paper addresses reproducibility challenges in automatic summarization evaluation by comparing six metrics, including classical approaches (ROUGE, BERTScore) and LLM-based methods (G-Eval, SEval-Ex). The study reveals significant discrepancies between reported and observed performance, highlighting a fundamental trade-off: metrics with highest human alignment are computationally intensive and less stable across runs. The authors introduce a unified open-source framework to facilitate fair comparison on standardized datasets, demonstrating that while some metrics achieve strong correlation with human judgments, their execution times vary dramatically. The work advocates for robust evaluation protocols including exhaustive documentation and methodological standardization.

## Method Summary
The authors developed a unified open-source framework called AllSummedUp that implements six summarization evaluation metrics consistently for fair comparison. The framework was applied to the SummEval dataset to evaluate performance, stability, and computational costs across metrics. The study compared classical metrics (ROUGE, BERTScore) with LLM-based approaches (G-Eval, SEval-Ex), examining their correlation with human judgments and their reproducibility across different runs. The implementation prioritized open-source alternatives even when it meant losing fidelity compared to original evaluation conditions, and included systematic testing of metric stability through repeated executions.

## Key Results
- LLM-based metrics show higher correlation with human judgments but require hours versus seconds for classical metrics
- Significant discrepancies exist between reported literature performance and observed experimental results
- Metrics exhibit instability across runs, particularly LLM-based methods affected by model randomness
- The framework successfully unified implementation of diverse metrics for standardized comparison

## Why This Works (Mechanism)
The framework addresses reproducibility by standardizing implementation across metrics, eliminating technical dependencies as a confounding factor. By using consistent evaluation conditions and open-source alternatives, the study isolates metric performance from implementation variations. The systematic comparison reveals that computational efficiency and human alignment are inversely related in current metrics, suggesting structural limitations in existing approaches.

## Foundational Learning
1. **ROUGE metrics** - why needed: Provides baseline n-gram overlap measurement; quick check: n-gram precision and recall values should match expectations
2. **BERTScore** - why needed: Captures semantic similarity through contextual embeddings; quick check: embedding dimensions should be 768 for base models
3. **LLM-based evaluation** - why needed: Attempts to capture human-like judgment through model reasoning; quick check: temperature and seed parameters should be documented
4. **Spearman correlation** - why needed: Measures monotonic relationship between metric scores and human judgments; quick check: correlation values should be between -1 and 1
5. **Computational complexity** - why needed: Critical for practical deployment of evaluation systems; quick check: execution time should scale linearly with input size
6. **Reproducibility testing** - why needed: Ensures reliability across different runs and environments; quick check: repeated executions should yield similar results

## Architecture Onboarding
Component map: Framework -> Metrics -> SummEval Dataset -> Evaluation Pipeline
Critical path: Data loading → Metric computation → Correlation calculation → Stability testing
Design tradeoffs: Open-source fidelity vs. proprietary accuracy, computational cost vs. alignment quality
Failure signatures: High variance across runs indicates instability; low correlation suggests poor human alignment
First experiments: 1) Run ROUGE metrics on sample text pairs; 2) Compare BERTScore outputs across different models; 3) Test LLM metric stability with fixed seeds

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can new evaluation metrics be developed to reconcile the structural trade-off between alignment with human judgment and computational efficiency?
- Basis in paper: [explicit] The conclusion advocates for the design of "new metrics, capable of reconciling methodological requirements, empirical robustness, and computational efficiency" to address the tensions identified.
- Why unresolved: The study identifies a "structural trade-off" where high-performing LLM metrics are unstable and computationally intensive, whereas efficient metrics like ROUGE lack discriminative power for modern systems.
- What evidence would resolve it: The creation of a metric that achieves high Spearman correlation with human judgments comparable to G-Eval, while maintaining the speed and stability of classical metrics.

### Open Question 2
- Question: Do the observed discrepancies between reported literature and experimental results persist across different evaluation datasets beyond SummEval?
- Basis in paper: [explicit] Section 5 notes the study relies exclusively on the SummEval dataset and states that "experiments on other corpora would be necessary to generalize our conclusions."
- Why unresolved: The reproducibility crisis highlighted (variability, technical dependencies) is demonstrated only on a single benchmark, leaving the generalizability of these findings unknown.
- What evidence would resolve it: Applying the AllSummedUp framework to additional datasets (e.g., CNN/DailyMail, NewsRoom) to verify if the same variability and correlation discrepancies occur.

### Open Question 3
- Question: To what extent does substituting proprietary models with open-source alternatives affect the fidelity of LLM-based evaluation metrics?
- Basis in paper: [explicit] Section 5 acknowledges that for G-Eval, the authors prioritized open-source alternatives, "even if it meant losing fidelity compared to the original evaluation conditions."
- Why unresolved: It remains unclear if the variability and performance drops observed are intrinsic to the evaluation method or a result of the specific model substitution.
- What evidence would resolve it: A direct comparison running G-Eval on both the original proprietary models (e.g., GPT-4) and the open-source substitutes (e.g., Gemma) on the same data to quantify the fidelity gap.

## Limitations
- Narrow focus on specific metrics and datasets limits generalizability to other summarization tasks or languages
- Performance discrepancies between observed and claimed results raise concerns about methodological transparency
- Instability in LLM-based metrics across runs affects reliability of evaluation outcomes

## Confidence
- Methodological framework feasibility: High
- Computational cost analysis: High
- Relationship between metric complexity and human alignment: Medium
- Generalizability of findings beyond SummEval: Low

## Next Checks
1. Replicate the stability tests across different hardware configurations to assess whether computational environment affects metric consistency
2. Expand evaluation to non-English datasets to verify cross-linguistic generalizability of the observed trade-offs
3. Conduct ablation studies removing specific technical dependencies to identify which factors most impact reproducibility