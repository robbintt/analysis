---
ver: rpa2
title: You Only Train Once
arxiv_id: '2506.04349'
source_url: https://arxiv.org/abs/2506.04349
tags:
- loss
- optimization
- which
- yoto
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces YOTO (You Only Train Once), a method for optimizing
  loss weight hyperparameters jointly with regular model parameters using standard
  gradient-based optimization. Instead of the traditional brute-force search for loss
  weights, YOTO treats them as learnable parameters within the model by parameterizing
  the composite loss with a softmax layer.
---

# You Only Train Once

## Quick Facts
- arXiv ID: 2506.04349
- Source URL: https://arxiv.org/abs/2506.04349
- Reference count: 40
- One-line primary result: YOTO jointly optimizes loss weights and model parameters in a single training run, outperforming grid search on monocular depth estimation and semantic segmentation tasks.

## Executive Summary
YOTO (You Only Train Once) introduces a novel method for optimizing loss weight hyperparameters jointly with regular model parameters using standard gradient-based optimization. Instead of traditional brute-force search for loss weights, YOTO treats them as learnable parameters within the model by parameterizing the composite loss with a softmax layer. This ensures positivity constraints and avoids degenerate gradients. A novel regularization loss is also introduced to promote uniformity among loss weights while maintaining boundedness. Experiments on monocular depth estimation (UniDepth) and semantic segmentation (CISS) show that YOTO consistently outperforms the best models found via grid search, achieving better generalization on unseen test data.

## Method Summary
YOTO optimizes loss weight hyperparameters jointly with model parameters by parameterizing the composite loss with a softmax layer. The method introduces a novel regularization loss on the learned hyperparameters to promote uniformity and boundedness. The softmax parameterization transforms loss weight hyperparameters into a differentiable layer, allowing for joint gradient-based optimization with model parameters. This creates a composite loss layer where gradients w.r.t. the hyperparameters can be computed via backpropagation, solving the degeneracy of a simple exponential parameterization.

## Key Results
- YOTO consistently outperforms the best grid-search models on unseen test data for both monocular depth estimation and semantic segmentation tasks.
- On nuScenes, YOTO improves the FA metric by 0.5% over grid search.
- The method is robust to initialization and stochasticity in training.
- YOTO achieves better generalization compared to traditional hyperparameter optimization techniques.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The softmax parameterization transforms loss weight hyperparameters into a differentiable layer, allowing for joint gradient-based optimization with model parameters.
- Mechanism: Instead of treating loss weights (λ) as fixed hyperparameters, they are treated as learnable parameters (µ) in log-space, passed through a softmax function to ensure positivity (λ > 0) and normalization (Σλ = 1). This creates a composite loss layer where gradients w.r.t. µ can be computed via backpropagation. The softmax normalization introduces competition between loss terms, allowing gradients to be positive or negative, which solves the degeneracy of a simple exponential parameterization where gradients are always positive, forcing all weights to collapse to zero.
- Core assumption: The composite loss function is differentiable with respect to both the model parameters and the loss weights.
- Evidence anchors: [abstract] "To this end, we leverage the differentiability of the composite loss formulation... parameterized with a softmax operation that satisfies the inherent positivity constraints... while avoiding degenerate empirical gradients." [section 3.3] "...we formulate the composite loss layer as Le = (Softmax(µ))T l(f(w))." [corpus] Weak direct evidence. Related work in hyperparameter optimization often focuses on bi-level or meta-learning approaches, making this direct gradient-based method distinct.
- Break condition: The loss function contains terms that are not differentiable with respect to the model parameters or the hyperparameters.

### Mechanism 2
- Claim: A novel regularization loss on the learned hyperparameters (µ) prevents unbounded growth and encourages a useful uniformity prior, stabilizing the optimization.
- Mechanism: The method introduces a regularization loss Lr(µ) composed of two terms: a negated entropy term and a softplus term. The negated entropy encourages the softmax weights (λ) to be more uniform, preventing a single loss from dominating. The softplus term (log(1+exp(µi))) penalizes large values of the exponents (µ), ensuring they remain bounded. This regularization acts similarly to weight decay for standard model parameters but is adapted for the unique properties of the softmax-parameterized hyperparameters.
- Core assumption: A uniform-ish distribution of loss weights is a reasonable inductive bias, and unbounded hyperparameters are detrimental to stable optimization.
- Evidence anchors: [abstract] "...defining a novel regularization loss on the learned hyperparameters, which models a uniformity prior among the employed losses while ensuring boundedness of the identified optima." [section 3.4] "We complete YOTO with a novel hyperparameter decay for regularization, which uses the gradient of a negated entropy term and a softplus term..." [corpus] Weak direct evidence. Standard HPO techniques may use priors, but this specific gradient-based regularization for loss weights is a unique contribution.
- Break condition: The problem requires one loss term to dominate others completely, which the uniformity prior would counteract.

### Mechanism 3
- Claim: Joint optimization of model parameters and loss weights in a single training run leads to models that generalize better than those with weights tuned via grid search.
- Mechanism: By making the loss weights λ dynamic and learnable, the model can adjust the emphasis on different loss terms throughout the training process. This allows the optimization to explore a richer joint parameter space (model weights w and hyperparameters µ) compared to a traditional bi-level approach where λ is fixed during a training run. The paper claims this joint exploration finds better optima that improve performance on unseen test data.
- Core assumption: The optimal weighting of loss terms is not static and the coupling between w and λ can be exploited for better generalization.
- Evidence anchors: [abstract] "...showing that it consistently outperforms the best grid-search model on unseen test data." [section 4.1, Table 1] "The YOTO model for UniDepth... consistently outperforms the best of the 9 grid-search models..." [corpus] Weak direct evidence. The corpus mentions related work in multi-objective HPO, but typically for validation objectives, not training losses.
- Break condition: The optimal loss weights are known a priori and constant, or the joint optimization is unstable and fails to converge.

## Foundational Learning

- **Gradient-Based Optimization & Backpropagation**
  - Why needed here: YOTO's core is extending backpropagation to compute gradients for a new set of parameters (the loss weights). Understanding the chain rule and how gradients flow from a loss function back through layers is essential.
  - Quick check question: If a loss L is a function of a model's output y, and y is a function of weights w, how is the gradient ∂L/∂w computed?

- **Composite Loss Functions & Regularization**
  - Why needed here: The method optimizes the weights of a composite loss (L = Σ λi * li). You must understand why multiple loss terms are used (e.g., reconstruction + regularization) and how their relative weighting affects the final model.
  - Quick check question: In a model with a loss L = λ1 * L_task + λ2 * L_penalty, what happens to the learned model if λ2 is set extremely high?

- **Softmax Function & its Properties**
  - Why needed here: The softmax function is used to parameterize the loss weights. You need to know that it converts a vector of real numbers into a probability distribution (values between 0 and 1, sum to 1).
  - Quick check question: What are the two key constraints satisfied by the output of a softmax function?

## Architecture Onboarding

- **Component map:**
  - Base Model f(w) -> Loss Vector l -> Hyperparameter Vector µ -> Softmax Layer -> Composite Loss Le -> Regularization Loss Lr -> Optimizer

- **Critical path:**
  1. Initialize µ with small initial weights for auxiliary losses.
  2. Forward: Compute base model output, individual losses l, weights λ = Softmax(µ), composite loss Le, and regularization loss Lr.
  3. Backward: Compute gradients ∇_w(Le + Lr) and ∇_µ(Le + Lr).
  4. Update: Apply optimizer step to both w and µ.

- **Design tradeoffs:**
  - Initialization (ε): A small ε starts with auxiliary losses having minimal impact, letting the model "choose" to increase them. Too small may cause slow learning of these terms.
  - Hyperparameter Decay (ρ): This new single hyperparameter controls the strength of the uniformity/boundedness prior. It trades off flexibility (letting weights go to extremes) vs. stability.
  - Learning Rate Scale: The softmax parameterization preserves the loss scale of the primary loss, meaning the original learning rate for the base model can often be reused without retuning.

- **Failure signatures:**
  - Weight Collapse: Without softmax normalization, gradients for µ are always positive, forcing all weights towards zero.
  - Unbounded Weights: Without the softplus regularization, µ values can explode, leading to NaN losses.
  - Poor Generalization: If ρ is poorly tuned, the model may overfit to a single loss term, hurting overall performance.

- **First 3 experiments:**
  1. Reproduce Baseline: On a simple multi-task problem, compare performance of grid search vs. YOTO. Verify that YOTO can find a comparable or better λ.
  2. Ablate Regularization (ρ): Run a sweep over ρ (e.g., [0.1, 1, 10, 100]). Plot final λ and validation performance to see its effect on weight distribution and model quality.
  3. Test Initialization Sensitivity: Run YOTO with different ε values (e.g., 1e-3, 1e-2, 1e-1). Check if the final converged weights and performance are consistent, as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the YOTO framework be extended to automatically optimize the learning rate alongside loss weights?
- **Basis in paper:** [explicit] The paper states: "Being able to also automatically optimize the learning rate is an exciting related possibility, which we do not address here but leave for future work."
- **Why unresolved:** The current softmax parameterization normalizes weights to sum to 1 specifically to preserve the learning rate scale; decoupling or jointly learning the rate requires overcoming this interdependence.
- **What evidence would resolve it:** A modified YOTO variant that successfully learns the learning rate α as a parameter while maintaining training stability and convergence speed comparable to or better than fixed-α baselines.

### Open Question 2
- **Question:** Can this gradient-based joint optimization approach be effectively adapted for non-continuous hyperparameters, such as architectural choices?
- **Basis in paper:** [explicit] The conclusion claims the method "opens new avenues for effective and efficient optimization of other types of HPs through standard GD," while the introduction distinguishes the method from discrete Neural Architecture Search (NAS).
- **Why unresolved:** The current method relies on a softmax layer designed for positive, continuous loss weights; architectural parameters are often discrete or categorical, presenting a different optimization landscape.
- **What evidence would resolve it:** Demonstrating that the differentiable parameterization logic of YOTO can be applied to categorical variables (e.g., using Gumbel-Softmax) to perform efficient architecture search without bi-level optimization.

### Open Question 3
- **Question:** How should the initialization parameter ε be set to ensure convergence to the optimal basin of attraction rather than a suboptimal local minimum?
- **Basis in paper:** [inferred] The sensitivity analysis (Figure 2) shows that different initializations converge to distinct optima (e.g., one retaining the invariance loss, one rejecting it), and the author notes these optima perform differently on different test sets.
- **Why unresolved:** While the method is robust in that it converges, the specific value of ε biases the model toward different local optima, and a principled strategy for selecting ε to maximize generalization is not established.
- **What evidence would resolve it:** A theoretical or empirical study identifying a "sweet spot" for ε that consistently avoids degenerate optima across diverse tasks and loss combinations.

## Limitations

- The core assumption that joint optimization of loss weights and model parameters improves generalization is not rigorously proven, though empirical results support it.
- The regularization hyperparameters (ε and ρ) require tuning, which could be a practical limitation.
- The method's scalability to more than two loss terms is not extensively tested, though it is claimed to be generalizable.

## Confidence

- High confidence in the technical mechanism of softmax parameterization and its ability to enable gradient-based optimization of loss weights.
- Medium confidence in the effectiveness of the regularization term for promoting uniformity and boundedness, as it is a novel contribution with limited direct evidence.
- Medium confidence in the claim of improved generalization over grid search, based on empirical results, but more extensive testing is needed.

## Next Checks

1. **Ablate the Regularization Term**: Run YOTO with ρ = 0 (no regularization) and compare the final λ distributions and model performance. This will isolate the effect of the uniformity and boundedness prior.
2. **Test Initialization Sensitivity**: Run YOTO with a wider range of ε values (e.g., [1e-4, 1e-3, 1e-2, 1e-1, 1]) and analyze the consistency of the final converged weights and performance. This will validate the claim of robustness to initialization.
3. **Compare to Alternative HPO Methods**: Benchmark YOTO against other hyperparameter optimization techniques (e.g., random search, Bayesian optimization) on the same tasks to quantify the improvement in efficiency and final performance.