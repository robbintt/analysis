---
ver: rpa2
title: Frequent subgraph-based persistent homology for graph classification
arxiv_id: '2512.24917'
source_url: https://arxiv.org/abs/2512.24917
tags:
- graph
- frequent
- homology
- persistent
- subgraph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel filtration method called Frequent
  Subgraph Filtration (FSF) for persistent homology on graphs. FSF is the first filtration
  constructed from frequent subgraph patterns mined across the entire dataset, capturing
  both pattern frequency and graph topology.
---

# Frequent subgraph-based persistent homology for graph classification

## Quick Facts
- arXiv ID: 2512.24917
- Source URL: https://arxiv.org/abs/2512.24917
- Authors: Xinyang Chen; Amaël Broustet; Guanyuan Zeng; Cheng He; Guoting Chen
- Reference count: 40
- Primary result: Introduces FSF, the first filtration constructed from frequent subgraph patterns mined across the entire dataset, capturing both pattern frequency and graph topology.

## Executive Summary
This paper introduces Frequent Subgraph Filtration (FSF), a novel filtration method for persistent homology on graphs that addresses the limitation of existing graph filtrations by mining frequent subgraph patterns across the entire dataset rather than processing graphs in isolation. The authors propose two graph classification approaches: FPH-ML, a machine learning model using FPH features, and FPH-GNN, a hybrid framework integrating FPH with graph neural networks. Theoretical properties of FSF are proven, and experimental results demonstrate competitive or superior accuracy compared to kernel-based and degree-based filtration methods, with relative performance gains ranging from 0.4% to 21% when integrated into GNNs.

## Method Summary
The method consists of two main components: an offline preprocessing pipeline and an online classification framework. During preprocessing, k-size frequent subgraph mining (k=4 default) is performed across the dataset using gSpan, filtering patterns by Minimum Node Image (MNI) support. For each graph, the method constructs simplicial complexes from embeddings of frequent patterns, computes persistent homology (H0, H1, H2) using GUDHI, and vectorizes the resulting persistence diagrams into 22 features (7 statistics per dimension plus total persistence). For FPH-ML, these features are used with an SVM classifier (RBF kernel, GridSearchCV). For FPH-GNN, FPH features are encoded via MLP, injected as a global token connected to top-K highest-degree nodes, and fused with GNN features through a gated mechanism using GCN/GIN backbones.

## Key Results
- FPH-ML achieves competitive or superior accuracy compared to kernel-based and degree-based filtration methods
- FPH-GNN yields relative performance gains ranging from 0.4% to 21% over GCN and GIN backbones
- Improvements of up to 8.2 percentage points across benchmarks
- Robustness to edge perturbations with bottleneck distances on the order of 10^-4 to 10^-2
- Budget-controlled mining preserves performance on some datasets (PROTEINS) but degrades on others (AIDS)

## Why This Works (Mechanism)

### Mechanism 1: Dataset-Level Topological Priors
FSF captures discriminative topology by anchoring the filtration to recurring substructures rather than generic graph properties. Standard filtrations process graphs in isolation, but FSF first mines frequent subgraphs across the entire dataset, ensuring the resulting persistence diagrams reflect topological features that are statistically significant to the dataset. This acts as a structural filter for noise, assuming the discriminative signal for graph classification is encoded in topological features that appear frequently across the dataset.

### Mechanism 2: Global Token Injection for GNN Context
Integrating FPH as a "global token" mitigates the over-squashing of global structural information in local message-passing GNNs. The global topological signature (FPH) is injected into the graph as a virtual node connected to the top-K highest-degree nodes, allowing global topological context to be distributed through the graph's backbone hubs. A gated fusion mechanism balances this global context against local features, assuming high-degree nodes serve as effective relays for distributing global topological context.

### Mechanism 3: Robustness via Frequency Filtering
FSF provides robustness to edge perturbations because the filtration relies on the existence of frequent patterns, which are resilient to minor structural noise. Small perturbations are unlikely to destroy frequent patterns or create new ones that satisfy the MNI threshold, so the resulting persistence diagram shifts minimally even if the underlying graph has changed slightly. This assumes noise is random/uncorrelated and does not systematically mimic the structure of frequent patterns.

## Foundational Learning

- **Persistent Homology & Filtration**: Core representation tracking birth and death of topological features (connected components H0, cycles H1, voids H2) in a sequence of complexes. Quick check: If a loop forms at filtration step 5 and is filled in at step 10, what are its birth and death times?

- **Frequent Subgraph Mining & MNI Support**: Defines the filtration order based on how frequently a subgraph pattern appears in the dataset. Quick check: Does a high MNI support threshold result in a more or less dense filtration initially?

- **Graph Neural Networks & Message Passing**: Understanding GNN limitations (over-smoothing/locality) explains why the global token mechanism is proposed. Quick check: Why might a standard GCN fail to distinguish two graphs with identical local degree distributions but different global cycle structures?

## Architecture Onboarding

- **Component map**: FSM Module (union of graphs -> gSpan mining -> MNI filtering) -> FSF Builder (pattern embeddings -> simplicial complexes) -> PH Computer (GUDHI -> persistence diagrams) -> Vectorizer (PDs -> 22 features) -> FPH-ML (SVC) or FPH-GNN (GNN + global token + gated fusion)

- **Critical path**: The construction of the simplicial complex K_t is the critical innovation. If the mapping of frequent subgraphs to simplices is incorrect (e.g., ignoring node labels or isomorphism), the theoretical guarantees break and the signal is lost.

- **Design tradeoffs**: Pattern size k vs. compute (exponential increase in FSM runtime), exact vs. budget FSM (exact is accurate but expensive; budget-controlled mining preserves performance on some datasets but risks losing discriminative patterns), top-K vs. full connection (connecting to all nodes introduces noise/dilution; top-K focuses through hubs).

- **Failure signatures**: High bottleneck distance suggests MNI threshold σ is too low (allowing noise to become "frequent"); FPH-GNN performing worse than vanilla GNN indicates global token injection is disrupting local message passing; empty or sparse persistence diagrams suggest pattern-graph compatibility issues or insufficient pattern size.

- **First 3 experiments**: 1) Robustness Check: Perturb 10% of edges in PROTEINS/NCI1 and verify bottleneck distance remains < 0.02; 2) Homology Dimension Ablation: Run FPH-ML using only H0, then only H1; verify ENZYMES benefits from H3 while AIDS relies on H0; 3) Scalability Budget: Run FPH-ML on PROTEINS with embedding budgets [5k, 10k, Exact] and verify 5k achieves comparable accuracy to Exact.

## Open Questions the Paper Calls Out

1. **End-to-end differentiable framework**: Can FSF be adapted into an end-to-end differentiable framework using learnable filtrations or neural estimators? The current FPH pipeline involves discrete, pre-computed mining steps decoupled from gradient-based optimization.

2. **Dynamic graph datasets**: How can the framework be extended to handle dynamic or evolving graph datasets efficiently? Current approach relies on static mining; re-mining frequent patterns from scratch upon graph updates is computationally prohibitive.

3. **Formal stability guarantees**: What are the formal stability guarantees of frequent subgraph-based filtrations? While empirical robustness is demonstrated, formal theoretical proofs linking graph perturbations to filtration stability are lacking.

4. **Topology-preserving approximate FSM**: Can topology-preserving approximate FSM methods balance efficiency with preservation of homological features? Exact mining is costly; current budget-controlled methods are heuristic and may discard topologically significant patterns.

## Limitations

- Critical hyperparameters (MNI support threshold σ, GNN hidden dimension H, number of layers L, learning rate, batch size, top-K parameter) are not fully specified in experimental results, creating uncertainty for faithful reproduction.

- Computational complexity analysis may not reflect practical limitations; budget-controlled mining shows mixed results (performance matches on PROTEINS but degrades on AIDS), suggesting dataset-dependent effectiveness.

- The assumption that discriminative topological signals are encoded in frequent patterns is untested; the paper doesn't explore scenarios where rare but highly discriminative substructures might be more important than frequent ones.

## Confidence

**High Confidence**: Theoretical framework and mathematical proofs (Propositions 4.2-4.4 on bounded homology, monotonicity, and isomorphism invariance) are well-established and clearly presented. Robustness mechanism is convincingly demonstrated through quantitative bottleneck distance measurements.

**Medium Confidence**: Empirical performance claims are supported by extensive experiments across multiple datasets, showing consistent improvements over baselines. However, lack of hyperparameter specifications and dataset-dependent effectiveness of budget mining introduce uncertainty about generalizability.

**Low Confidence**: Assumption that discriminative topological signals are encoded in frequent patterns is intuitive but untested. Paper doesn't explore scenarios where rare but highly discriminative substructures might be more important than frequent ones.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Reproduce the robustness experiment (Section 6.1) with varying MNI thresholds (σ = 3, 5, 10, 15) to determine sensitivity to this critical parameter. Measure both bottleneck distances under perturbation and classification accuracy across different σ values.

2. **Rare Pattern Ablation Study**: Design experiment to test core assumption by artificially creating modified dataset where discriminative signal is encoded in rare (low-frequency) patterns. Compare FPH performance against baseline that captures rare patterns.

3. **Computational Budget Scaling**: Conduct systematic scalability study across all datasets (PROTEINS, AIDS, NCI1, ENZYMES, DD) with embedding budgets [1k, 5k, 10k, 20k, Exact]. Plot accuracy vs. computational time to identify optimal budget-accuracy tradeoff curve.