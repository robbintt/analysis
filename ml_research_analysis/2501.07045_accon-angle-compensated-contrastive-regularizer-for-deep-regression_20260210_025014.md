---
ver: rpa2
title: 'ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression'
arxiv_id: '2501.07045'
source_url: https://arxiv.org/abs/2501.07045
tags:
- regression
- learning
- contrastive
- deep
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACCon, an angle-compensated contrastive regularizer
  for deep regression that addresses the challenge of capturing relationships among
  continuous labels in feature space. The method introduces angle compensation to
  refine distance-weighted negative cosine similarity in contrastive learning, projecting
  representations onto a semi-hypersphere that preserves label relationships.
---

# ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression

## Quick Facts
- arXiv ID: 2501.07045
- Source URL: https://arxiv.org/abs/2501.07045
- Reference count: 40
- Authors: Botao Zhao; Xiaoyang Qu; Zuheng Kang; Junqing Peng; Jing Xiao; Jianzong Wang
- One-line result: ACCon achieves up to 9.27% improvement in MAE and 4.63% improvement in G-means over state-of-the-art methods

## Executive Summary
This paper introduces ACCon, an angle-compensated contrastive regularizer that addresses the challenge of capturing relationships among continuous labels in deep regression. The method extends supervised contrastive learning by introducing angle compensation to refine distance-weighted negative cosine similarity, projecting representations onto a semi-hypersphere that preserves label relationships. ACCon demonstrates significant improvements on age estimation and semantic textual similarity tasks, particularly excelling in imbalanced regression scenarios and data-limited settings. The approach is theoretically justified and maintains plug-and-play compatibility with existing contrastive learning methods.

## Method Summary
ACCon extends supervised contrastive learning to regression by introducing angle compensation that linearizes the relationship between label distance and representation similarity. The method computes a compensation angle φ based on label differences and modifies the cosine similarity calculation to create distance-aware representations. The total loss combines standard regression loss with the angle-compensated contrastive loss, projecting features onto a semi-hypersphere rather than a full hypersphere to better preserve monotonic label structure. The approach is compatible with various backbone architectures and requires only minimal modifications to existing contrastive learning frameworks.

## Key Results
- Achieves up to 9.27% improvement in MAE and 4.63% improvement in G-means over state-of-the-art methods
- Excels in imbalanced regression tasks, showing significant gains in few-shot and medium-shot learning scenarios
- Maintains robust performance across various hyperparameter settings with γ ∈ [0.01, 100]
- Demonstrates superior data efficiency, maintaining performance with only 1/8 of training data

## Why This Works (Mechanism)

### Mechanism 1
Direct cosine weighting fails because cosine is nonlinear; angle compensation linearizes the relationship between label distance and representation similarity. The method replaces standard negative cosine similarity with a compensated version: `cos(θ̃) = z_i·z_m·cos(φ) - |sin(φ)|·√(1-(z_i·z_m)² + ε)`, where φ is the compensation angle derived from label distance. This ensures representations align on a semi-hypersphere proportionally to label differences. Core assumption: A linear negative correlation exists between label distances and representation similarities in well-learned regression models (hypothesized, not proven universally).

### Mechanism 2
Projecting to a semi-hypersphere (not full hypersphere) preserves monotonic label structure better than standard contrastive learning. Standard SupCon pushes all negative pairs to angle π uniformly. ACCon targets different angles per negative: `θ̃_i,m → π` while `θ̂_i,m → (y_m - y_i)/(max(Y) - min(Y))·π`. This creates ordered, continuous feature arrangements. Core assumption: Regression targets are inherently monotonic and should map to monotonic angular positions in feature space.

### Mechanism 3
Joint optimization of regression loss and angle-compensated contrastive loss improves data efficiency and handles imbalance better than regression loss alone. The total loss `L = L_reg + γ·L_ACCon` couples label prediction accuracy with structured representation learning. The contrastive component provides gradient signals that generalize across label space, reducing reliance on abundant samples at any specific label value. Core assumption: Feature structure learned via contrastive loss transfers to better regression generalization, especially for underrepresented label regions.

## Foundational Learning

- **Supervised Contrastive Learning (SupCon)**
  - Why needed here: ACCon extends SupCon from classification to regression; understanding how SupCon defines positive/negative pairs and its loss formulation is prerequisite to grasping the angle compensation modification.
  - Quick check question: Can you explain why SupCon treats all samples from the same class as positives and how this differs from self-supervised contrastive learning?

- **Cosine Similarity and Angular Distance on Hyperspheres**
  - Why needed here: The core insight is that cosine is nonlinear—understanding the relationship between cosine similarity, angular distance, and their derivatives is essential for implementing Eq. 7 correctly.
  - Quick check question: Given two L2-normalized vectors with cosine similarity 0.5, what is their angular distance? Why does the paper argue direct cosine weighting is suboptimal?

- **Imbalanced Regression and Long-tail Distributions**
  - Why needed here: A key claim is ACCon's effectiveness on imbalanced datasets; understanding what "many-shot," "medium-shot," and "few-shot" mean in regression contexts helps interpret Table 2 results.
  - Quick check question: How does label imbalance in regression differ from class imbalance in classification, and why might re-weighting alone be insufficient?

## Architecture Onboarding

- Component map: Input x → Augmentation (Aug1, Aug2) → Concatenated augmented batch → Feature Encoder f_θ(x) → Embedding z → Projection Layer g_ϕ(z) → L2-normalized representation z̃ ∈ S^{d_l-1} → Predictor h_ψ(z) → Prediction ŷ

- Critical path:
  1. **Bin assignment**: Partition label space into M bins; samples in same bin are positive pairs, different bins are negative pairs
  2. **Compensation angle computation**: For each anchor-negative pair, compute `φ = π(1 - (y_neg - y_anc)/(max(Y) - min(Y)))` using Eq. 6
  3. **Compensated cosine**: Compute `cos(θ̃_i,m)` via Eq. 7—this is the non-trivial implementation step involving the square root term and smoothing ε
  4. **Loss assembly**: Construct L_ACCon using Eq. 4 and Eq. 5, then combine with L_reg per Eq. 8

- Design tradeoffs:
  - **Projection dimension**: Higher dimensions (512, 1024) show better performance but increase memory; 128 is used for DIR datasets, 512 for Natural
  - **Two-stage vs. joint training**: Paper experimented with two-stage (pretrain then fine-tune) but found it inferior to joint optimization—convergence issues cited
  - **Before vs. after projection features**: Using features before projection layer yields compact features but weaker label-distance correlation; after projection provides better structure

- Failure signatures:
  - **Gradient explosion**: If ε is too small or omitted, `√(1 - (z_i·z_m)²)` can approach zero when embeddings are nearly identical—paper uses ε = 1e-6
  - **No improvement over vanilla**: Check if γ is effectively zero (weight coefficient not properly applied) or if bin granularity M is too coarse
  - **Worse performance on few-shot**: May indicate label space scaling issue—verify max(Y) and min(Y) are computed correctly across entire dataset, not just batch
  - **STS-B underperformance**: Paper notes limited data and weak augmentation for NLP tasks may limit gains—consider stronger text augmentations

- First 3 experiments:
  1. **Sanity check on synthetic data**: Generate 1D regression task with uniformly sampled labels; visualize t-SNE of learned representations. Expect: ACCon shows linear arrangement along a manifold; vanilla shows no clear structure. Verify Eq. 7 implementation produces expected angular separation.
  2. **Ablation on projection layer**: Train on AgeDB-Natural with three configs: (a) use features before projection, (b) remove projection layer entirely, (c) standard ACCon. Compare MAE and Pearson correlation between cos(θ) and label distance. Expected: (c) > (b) > (a) for both metrics.
  3. **Imbalance robustness test**: Create subsampled IMDB-WIKI versions at 25%, 50%, 75% of training data (preserving label distribution). Plot MAE vs. data fraction for Vanilla, RankSIM, and ACCon. Expected: ACCon's degradation curve is flatter, maintaining larger relative gains at lower data fractions as in Figure 4.

## Open Questions the Paper Calls Out

- **Dense Pixelwise Regression**: Can ACCon be optimized to handle dense pixelwise regression tasks without incurring prohibitive computational costs? Current methods require high computational resources for pixel-level predictions.

- **Self-supervised Regression**: Can the angle-compensated mechanism be adapted for self-supervised regression settings where labels are unavailable? Current methods require labeled training data, which may not be universally available.

- **Non-monotonic Label Spaces**: Does the hypothesis of a linear negative correlation between label distance and feature similarity hold for regression tasks with non-monotonic or complex manifolds? The method enforces a semi-hypersphere structure suitable for monotonic data; it remains unclear if this linear constraint distorts representations in domains where the relationship between target values and feature space is non-linear or multi-modal.

## Limitations

- **Dense Regression Tasks**: Current implementation is not suitable for dense pixelwise regression tasks due to high computational resource requirements for processing numerous sample pairs.

- **Self-supervised Learning**: The method requires labeled training data for calculating compensation angles, limiting its applicability to unsupervised scenarios.

- **Non-monotonic Distributions**: The linear negative correlation assumption may not hold for regression tasks with cyclic or non-linear label spaces, potentially limiting performance in certain domains.

## Confidence

- **High confidence**: The core mathematical formulation (Eq. 4-7) is sound and implementation appears correct. Empirical results on AgeDB and IMDB-WIKI are robust across multiple experiments.
- **Medium confidence**: Claims about angle compensation improving data efficiency are supported by experiments but may depend on specific data distributions. The superiority over vanilla methods is demonstrated but could be dataset-specific.
- **Low confidence**: The generalizability to NLP tasks (STS-B) is questionable given limited gains and lack of detailed augmentation strategies. The theoretical justification for why angle compensation specifically works better than alternative formulations remains incomplete.

## Next Checks

1. **Synthetic Data Validation**: Generate a 1D regression dataset with controlled label distributions (uniform, multimodal, cyclic) and visualize t-SNE embeddings for ACCon vs. vanilla methods. Verify that ACCon produces the expected ordered, continuous feature arrangements only for monotonic label spaces.

2. **Bin Sensitivity Analysis**: Systematically vary the number of label bins M (e.g., 10, 20, 50, 100) on AgeDB and measure both MAE performance and Pearson correlation between cosine similarity and label distance. Determine if there's an optimal binning strategy and whether the method is robust to this hyperparameter.

3. **NLP Task Enhancement**: Implement stronger text augmentation strategies (e.g., synonym replacement, back-translation, contextual augmentation) for STS-B and compare ACCon performance against vanilla methods. Quantify whether the limited gains are due to weak augmentations rather than fundamental limitations of the approach on NLP tasks.