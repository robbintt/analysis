---
ver: rpa2
title: Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right
  Data
arxiv_id: '2601.15158'
source_url: https://arxiv.org/abs/2601.15158
tags:
- chain
- lemma
- only
- data
- vertex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes theoretical foundations for the emergence
  of Chain-of-Thought reasoning in Transformers trained via outcome-based reinforcement
  learning. The authors analyze single-layer Transformers on a synthetic graph traversal
  task that requires multi-step reasoning, proving that despite training solely on
  final-answer correctness, policy gradient drives the model to converge to an efficient
  vertex-by-vertex traversal algorithm.
---

# Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data

## Quick Facts
- **arXiv ID**: 2601.15158
- **Source URL**: https://arxiv.org/abs/2601.15158
- **Reference count**: 40
- **Primary result**: Outcome-based RL with policy gradient provably drives single-layer Transformers to learn chain-of-thought reasoning algorithms, but only when training data includes sufficient mass of simple examples requiring few reasoning steps.

## Executive Summary
This paper establishes theoretical foundations for the emergence of Chain-of-Thought reasoning in Transformers trained via outcome-based reinforcement learning. The authors analyze single-layer Transformers on a synthetic graph traversal task that requires multi-step reasoning, proving that despite training solely on final-answer correctness, policy gradient drives the model to converge to an efficient vertex-by-vertex traversal algorithm. They characterize distributional properties required for this emergence, identifying that training distributions must place sufficient mass on "simple examples" requiring fewer reasoning steps. When simple examples are present, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes infeasible. Experiments with synthetic data and real-world Qwen-based models on mathematical reasoning tasks validate that theoretical findings carry over to practical settings, demonstrating that exposure to simple tasks during training is necessary for learning to solve complex tasks and that training on simple examples can improve performance on complex examples more than direct training on those complex examples.

## Method Summary
The paper analyzes single-layer linear Transformers trained via REINFORCE policy gradient on a chain identification task. The model uses one-hot embeddings, causal attention with learnable attention (A) and value (V) matrices, and softmax output. Training involves sampling trajectories through autoregressive decoding until reaching terminal tokens, computing binary rewards based on final answer correctness, and updating parameters via gradient ascent. For synthetic experiments, the model is trained on chains of varying lengths with different distributions emphasizing simple versus complex examples. For real-world validation, the approach is tested on Qwen 2.5 3B models solving mathematical reasoning problems with affine equations.

## Key Results
- Policy gradient converges to efficient chain-traversal reasoning algorithms despite training only on final-answer correctness
- Training distributions must have non-vanishing mass on simple examples (few reasoning steps) for polynomial-time learning; otherwise gradient magnitudes scale exponentially
- Models trained on short-chain examples can solve arbitrarily long chains through permutation-invariant generalization
- Qwen 2.5 3B models show that training on simple mathematical problems improves performance on complex problems more than direct training on complex problems

## Why This Works (Mechanism)

### Mechanism 1: Policy Gradient Exhibits Implicit Bias Toward Efficient Reasoning Algorithms
- Claim: When trained with outcome-based RL on the right data distribution, policy gradient converges to efficient chain-traversal reasoning rather than arbitrary valid solutions.
- Mechanism: The gradient flow dynamics show that α (forward-step logit) increases while β (backward-step logit) and γ (switch-step logit) decrease monotonically. This directional pressure arises because the long-jump absorbing chain contribution dominates the no-long-jump chain contribution in the derivative decomposition of the absorption probability.
- Core assumption: The base model has weak forward preference (p_fwd > p_bwd + ζ) and non-negligible switch probability (p_switch > υ) at initialization.
- Evidence anchors: [abstract] "policy gradient drives the Transformer to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex"; [Section 6.2, Theorem 2] convergence rate analysis; [corpus] related work on length generalization.

### Mechanism 2: Simple Examples Provide Tractable Gradient Signal While Hard Examples Contribute Negligibly
- Claim: Training distributions must place non-vanishing mass on simple examples (few reasoning steps) for polynomial-time learning; otherwise gradient magnitudes scale as exp(-Ω(n)).
- Mechanism: For easy examples (starting vertex within distance s of terminal), the gradient ∂θS scales polynomially in n. For hard examples (distance ≥ an), the no-long-jump probability decays as (1-q_min)^{an} ≤ e^{-cn}, causing gradient contributions to be exponentially suppressed.
- Core assumption: There exists q_min > 0 such that the one-step long-jump probability P_{LJ-abs}(x, e) ≥ q_min for all non-terminal states.
- Evidence anchors: [abstract] "When the training distribution places sufficient mass on these simpler examples, the Transformer learns a generalizable traversal strategy... when this mass vanishes, policy gradient learning becomes infeasible"; [Section 6.3, Theorem 3] exponential learning time proof.

### Mechanism 3: Permutation-Invariant Structure Enables Out-of-Distribution Generalization
- Claim: Models trained only on short-chain examples can solve arbitrarily long chains because the learned transition probabilities depend only on depth from terminal, not absolute vertex labels.
- Mechanism: Symmetric initialization and permutation-invariant data distribution preserve symmetry under gradient flow. The resulting attention parameters produce transition probabilities that are functions of relative position (depth d = n-i from terminal).
- Core assumption: The training distribution D_Q is invariant under vertex permutations and has ≥ s-simplicity mass c > 0.
- Evidence anchors: [Section 6.2] "the learned behavior applies under any test data distribution, thus it admits generalization out of distribution, including to complex examples possibly unseen during training"; [Section 7.1.2] experiments show perfect accuracy on longer test chains.

## Foundational Learning

- Concept: **Policy gradient with sparse rewards**
  - Why needed here: The paper analyzes gradient flow on the expected reward R(D, θ) = E[r(τ)] where r depends only on terminal tokens. Understanding the REINFORCE-style gradient estimator ∂θ log p(τ) · r(τ) is essential.
  - Quick check question: Can you derive why E[∂θ log p(τ) · f(τ)] = ∂θ E[f(τ)] for any function f?

- Concept: **Markov chain absorption probabilities**
  - Why needed here: The loss function is the same-side absorption probability Pr_v[X_τ = a_n] in a chain with terminals {a_n, b_n}. The analysis uses harmonic functions and source decompositions extensively.
  - Quick check question: For a birth-death chain on {0,1,...,N} with absorbing states {0,N}, can you write the probability of absorption at N starting from state i?

- Concept: **Softmax attention dynamics**
  - Why needed here: The Transformer's transition kernel is a softmax over logits parameterized by (α, β, γ). The derivative ∂θ P(x,y) = P(x,y)(∂θ o(x,y) - Σ_z P(x,z)∂θ o(x,z)) appears repeatedly.
  - Quick check question: Given softmax probabilities p_i = exp(ℓ_i)/Z, what is ∂ℓ_j p_i? What is the Fisher information term Σ_i p_i(∂ℓ_j log p_i)²?

## Architecture Onboarding

- Component map: One-hot embeddings → Attention layer (A, V matrices) → Softmax over final hidden state → Output distribution

- Critical path:
  1. Initialize A with small symmetric values ensuring p_fwd > p_bwd + ζ and p_switch > υ
  2. Sample trajectory via autoregressive decoding until terminal token
  3. Compute reward r(τ) = 1{terminal = correct answer}
  4. Update parameters via policy gradient: dA/dt = -∇_A L(D_Q, A)
  5. Monitor convergence by checking forward-probability p_fwd on held-out examples

- Design tradeoffs:
  - Linear vs. softmax attention: Linear enables closed-form analysis; softmax matches practical models but requires partition function Z_v bounds
  - Single-layer vs. multi-layer: Single-layer admits tractable analysis via Markov chain reduction; multi-layer may require different complexity-theoretic arguments
  - Output masking vs. free generation: Masking prevents degenerate solutions where model outputs edge tokens or repeats vertices; practical models may need softer constraints

- Failure signatures:
  - Learning stall: Gradient magnitudes decay as exp(-Ω(n)) when training only on long-chain examples (s-simplicity mass → 0)
  - Switch-heavy trajectories: If p_switch never decreases, model never commits to single-chain traversal
  - Overfitting to training chain lengths: If permutation invariance is broken (e.g., via position embeddings), depth-only generalization fails

- First 3 experiments:
  1. Replicate Figure 2: Train single-layer linear Transformers on chain identification with varying k (simplicity mass). Verify that test accuracy on hard examples correlates with k. Track gradient magnitudes over training.
  2. Probe transition probabilities: At checkpoints, compute p_fwd, p_bwd, p_switch as functions of depth d from terminal. Verify that p_fwd → 1 uniformly across depths as training progresses.
  3. Test out-of-distribution generalization: Train on chains with max length n=8, test on n=16. Measure accuracy and check whether trajectories follow chain-traversal pattern vs. random walks. Compare to model trained directly on n=16.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis is restricted to single-layer linear Transformers, differing significantly from practical multi-layer softmax Transformers used in experiments
- Proof assumes specific initialization conditions (p_fwd > p_bwd + ζ and p_switch > υ) that may not hold in practice
- Graph traversal task represents simplified reasoning compared to mathematical word problems, with unclear mechanistic connection to abstract reasoning

## Confidence
- **High confidence**: The impossibility result (Theorem 3) showing exponential learning times when s-simplicity mass vanishes is mathematically rigorous
- **Medium confidence**: The convergence proof (Theorem 2) for single-layer linear Transformers is sound but relies on strong assumptions
- **Medium confidence**: Experimental results with Qwen models demonstrate that simple examples help complex problem solving, but mechanism may differ from theoretical model

## Next Checks
1. Systematically vary initialization parameters (ζ, υ thresholds) and measure how often models converge to chain traversal versus degenerate solutions
2. Design training distributions with varying s-simplicity mass levels to empirically map learning time versus simplicity mass relationship
3. For Qwen experiments, examine attention patterns and value vectors during chain traversal to verify learned behavior matches theoretical vertex-by-vertex pattern