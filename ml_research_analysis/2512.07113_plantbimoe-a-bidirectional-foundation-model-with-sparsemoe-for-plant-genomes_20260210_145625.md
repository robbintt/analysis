---
ver: rpa2
title: 'PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes'
arxiv_id: '2512.07113'
source_url: https://arxiv.org/abs/2512.07113
tags:
- uni00000011
- uni00000013
- uni00000003
- plant
- plantbimoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PlantBiMoE, a lightweight foundation model
  for plant genome analysis that combines bidirectional Mamba and SparseMoE architectures.
  The model addresses limitations of existing plant genome models by capturing long-range
  dependencies and bidirectional strand information while maintaining computational
  efficiency through sparse parameter activation.
---

# PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes

## Quick Facts
- arXiv ID: 2512.07113
- Source URL: https://arxiv.org/abs/2512.07113
- Authors: Kepeng Lin; Qizhe Zhang; Rui Wang; Xuehai Hu; Wei Xu
- Reference count: 28
- Key outcome: PlantBiMoE achieves best average performance on 9 out of 11 genomic tasks and 20 out of 31 datasets in MPGB, outperforming AgroNT and PlantDNAMamba while using only 64M active parameters.

## Executive Summary
PlantBiMoE introduces a bidirectional foundation model for plant genome analysis that combines Mamba-based bidirectional processing with SparseMoE for computational efficiency. The model addresses limitations of existing plant genome models by capturing long-range dependencies and bidirectional strand information while maintaining computational efficiency through sparse parameter activation. PlantBiMoE was pre-trained on 42 plant species totaling 25.40B nucleotides and evaluated on the Modified Plants Genome Benchmark (MPGB), which includes 31 datasets across 11 genomic tasks.

## Method Summary
PlantBiMoE uses a 16-layer architecture with alternating BiMamba and SparseMoE layers, single-nucleotide tokenization (vocabulary size 12), and a 32,768 bp context window. The model is pre-trained via Masked Language Modeling on 42 plant species using AdamW optimization with mixed precision training. Downstream evaluation spans 11 genomic tasks including polyadenylation, splice sites, chromatin accessibility, and enhancer regions. The SparseMoE component reduces active parameters from 116M to 64M while maintaining modeling capacity through conditional computation.

## Key Results
- Achieves best average performance on 9 out of 11 genomic tasks in MPGB
- Outperforms AgroNT and PlantDNAMamba on 20 out of 31 datasets
- Maintains computational efficiency with only 64M active parameters (116M total)
- Demonstrates strong performance across diverse plant species and genomic functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional Mamba enables effective capture of structural dependencies across both DNA strands.
- Mechanism: BiMamba processes forward and reverse sequences through separate state space updates (h→t = Ah→t-1 + Bxt and h←t = Ah←t-1 + Bxt), then fuses outputs via element-wise addition (ht = y→t + y←t). This allows the model to integrate context from both directions without additional parameters.
- Core assumption: DNA strand information is bidirectional in nature, and capturing both forward and reverse dependencies improves representation of regulatory elements.
- Evidence anchors:
  - [abstract] "The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands."
  - [section II.B.1] Describes BiMamba's forward/reverse state updates and element-wise addition fusion strategy.
  - [corpus] Limited corpus evidence on BiMamba specifically; related work (JanusDNA, eccDNAMamba) also explores bidirectional modeling but with different architectures.

### Mechanism 2
- Claim: SparseMoE reduces active parameters while maintaining modeling capacity through conditional computation.
- Mechanism: A router assigns each token to the top-k experts based on learned routing scores (R = softmax(H̃Wr)). Only the selected experts' MLP subnetworks are activated per token, reducing active parameters from 116M to 64M while maintaining total model capacity.
- Core assumption: Different genomic regions (promoters, enhancers, coding sequences) benefit from specialized expert processing rather than dense computation.
- Evidence anchors:
  - [abstract] "SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity."
  - [section II.B.2] Describes Top-k routing, expert MLP structure with SiLU activation, and sparsity benefits.
  - [corpus] SparseMoE is established in general ML literature; corpus neighbors show MoE applied to genomics (CircFormerMoE) but limited plant-specific evidence.

### Mechanism 3
- Claim: Long context window (32,768 bp) combined with single-nucleotide tokenization captures long-range cis-regulatory interactions.
- Mechanism: Single-nucleotide tokenization (vocabulary size 12) preserves positional resolution. The 32,768 bp context window exceeds AgroNT (6,000) and PlantDNAMamba (2,000), enabling modeling of distal regulatory elements.
- Core assumption: Long-range genomic interactions (>2,000 bp) are functionally important and benefit from explicit context modeling rather than learned aggregation.
- Evidence anchors:
  - [abstract] "captures long-range dependencies and bidirectional strand information while maintaining computational efficiency."
  - [TABLE II] Shows PlantBiMoE context length (32,768) vs. competitors.
  - [corpus] Corpus shows long-context modeling trend in genomics (eccDNAMamba, HyenaDNA references), but optimal context length for plants remains unstudied.

## Foundational Learning

- **State Space Models (SSM/Mamba)**
  - Why needed here: BiMamba is the core sequence encoder; understanding selective state updates (A, B, C, D parameters) is essential for debugging gradient flow and memory efficiency.
  - Quick check question: Can you explain how Mamba's selective state update differs from LSTM's gated mechanism?

- **Mixture-of-Experts (MoE) Routing**
  - Why needed here: SparseMoE determines which experts process which tokens; understanding Top-k routing, load balancing, and expert specialization is critical for training stability.
  - Quick check question: What happens if the router collapses so all tokens route to a single expert?

- **DNA Language Modeling Fundamentals**
  - Why needed here: Tasks like polyadenylation, splice site detection, and chromatin accessibility require understanding why bidirectional context matters for double-stranded DNA.
  - Quick check question: Why might reverse-complement augmentation improve generalization for DNA sequence models?

## Architecture Onboarding

- **Component map:**
  Input DNA (L=32,768) → Tokenizer (single-nucleotide, vocab=12) → Embedding (D=512) → [Layer 1: BiMamba + SwiGLU] (odd layers) → [Layer 2: BiMamba + SparseMoE] (even layers, 8 experts, Top-1 routing) → ... (x16 layers alternating) → LM Head → MLM Loss (15% masking)

- **Critical path:** BiMamba's bidirectional state fusion (element-wise addition) → SparseMoE routing → expert MLP processing. Debugging should start here.

- **Design tradeoffs:**
  - Element-wise addition for fusion vs. gating/concatenation (chosen for simplicity, may sacrifice expressiveness)
  - Top-1 routing vs. Top-k (Top-1 minimizes compute but risks routing collapse)
  - Single-nucleotide tokenization vs. k-mer (preserves resolution but increases sequence length)

- **Failure signatures:**
  - Expert load imbalance (one expert receives >50% of tokens)
  - Gradient vanishing in long sequences (check SSM state updates)
  - Poor performance on short-input tasks (<500 bp) due to context mismatch
  - Masked token reconstruction accuracy plateaus early (indicates underfitting)

- **First 3 experiments:**
  1. **Routing analysis:** Log expert assignment distributions across different genomic regions (promoters vs. enhancers vs. coding). Verify specialization hypothesis.
  2. **Context ablation:** Test performance on MPGB tasks with reduced context windows (4,096, 8,192, 16,384 bp) to identify minimum effective context length.
  3. **Bidirectional fusion comparison:** Replace element-wise addition with concatenation + linear projection on one task; measure performance delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does PlantBiMoE underperform on Enhancer region and Core promoter tasks compared to baselines, and what architectural modifications could address these specific weaknesses?
- Basis in paper: [explicit] The authors state that "on the Enhancer region and Core promoter tasks, PlantBiMoE's performance is slightly lower than the best model" (AgroNT wins Enhancer region, PlantDNAMamba wins Core promoter).
- Why unresolved: No analysis or ablation is provided to explain these specific failures, despite strong performance on 9/11 other tasks.
- What evidence would resolve it: Task-specific error analysis, attention/SSM state visualization on enhancer and promoter sequences, and ablation of architectural components on these specific tasks.

### Open Question 2
- Question: What biological or sequence patterns do the individual experts in the SparseMoE layer specialize in, and do they correspond to meaningful genomic features?
- Basis in paper: [inferred] The paper introduces SparseMoE with 8 experts and Top-1 routing but provides no interpretability analysis of what each expert learns or whether specialization emerges.
- Why unresolved: Expert interpretability is critical for biological validation and understanding whether the sparse architecture captures meaningful structure rather than arbitrary partitioning.
- What evidence would resolve it: Visualization of expert activation patterns across different genomic regions (coding vs. non-coding, promoters vs. intergenic), correlation analysis between expert selection and sequence features.

### Open Question 3
- Question: How does the relative contribution of BiMamba versus SparseMoE affect performance, and is bidirectional modeling or sparse routing more critical for plant genome tasks?
- Basis in paper: [inferred] No ablation study isolates the contribution of each architectural innovation; the combined model is only compared against external baselines.
- Why unresolved: Without component-wise ablation, it remains unclear whether gains come from bidirectional modeling, sparse routing, or their interaction.
- What evidence would resolve it: Ablation experiments with unidirectional Mamba + SparseMoE, BiMamba without MoE (dense FFN), and dense bidirectional baselines across all MPGB tasks.

## Limitations

- SparseMoE architecture specifics: Total expert count and expert hidden dimensions are not explicitly specified, requiring assumptions from related work.
- Species diversity and task representativeness: The 42 plant species and 11 task categories may not fully capture the diversity of plant genomic functions.
- Context window necessity: The paper does not demonstrate whether 32,768 bp is necessary or optimal for the specific plant genomic tasks evaluated.

## Confidence

- **Model architecture effectiveness**: High - well-supported by ablation studies and comparison with established baselines
- **Computational efficiency claims**: Medium - parameter reduction is clear but detailed timing/comparison metrics are missing
- **Generalizability across plant species**: Medium - strong MPGB performance but limited species sampling and potential bias

## Next Checks

1. **Routing distribution analysis**: Log and analyze expert selection frequencies across different genomic regions (promoters, enhancers, coding sequences) during pre-training. This would verify whether the SparseMoE routing is actually learning meaningful specialization or collapsing to uniform token distribution.

2. **Context length ablation study**: Systematically evaluate PlantBiMoE performance on MPGB tasks using reduced context windows (4,096, 8,192, 16,384 bp) to identify the minimum effective context length and determine whether the 32,768 bp setting provides marginal or significant benefits for specific task types.

3. **Cross-species transfer validation**: Fine-tune the pre-trained PlantBiMoE on a held-out plant species not included in the original 42, then evaluate on the same MPGB tasks. This would directly test the model's ability to generalize beyond its training distribution and identify potential species-specific limitations.