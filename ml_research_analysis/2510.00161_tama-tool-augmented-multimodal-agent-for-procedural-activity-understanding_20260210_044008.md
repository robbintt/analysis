---
ver: rpa2
title: 'TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding'
arxiv_id: '2510.00161'
source_url: https://arxiv.org/abs/2510.00161
tags:
- tool
- answer
- question
- tools
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TAMA, a training-free agentic framework for
  procedural activity understanding that enables interleaved multimodal reasoning
  through multimedia-returning tools. The approach uses a vision-language model (VLM)
  as an agent to flexibly explore video content and instructions by calling tools
  that return either images or text.
---

# TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding

## Quick Facts
- arXiv ID: 2510.00161
- Source URL: https://arxiv.org/abs/2510.00161
- Reference count: 28
- Primary result: 14.1% improvement for GPT-5 over naive approach on ProMQA-Assembly dataset

## Executive Summary
TAMA introduces a training-free agentic framework for procedural activity understanding that uses a vision-language model (VLM) as an agent to perform interleaved multimodal reasoning through multimedia-returning tools. The framework enables flexible exploration of video content and instructions by calling tools that return either images or text, preserving visual information that would otherwise be lost in text-only grounding. Experiments on the ProMQA-Assembly dataset show that TAMA improves performance for certain models, particularly GPT-5 and MiMo-VL, with GPT-5 achieving a 14.1% improvement over the naive approach. Ablation studies confirm the effectiveness of multimedia-returning tools and agentic flexible tool selection.

## Method Summary
TAMA uses a VLM-based agent that interleaves thoughts with tool calls in a multi-turn conversation format. The agent can call four tools: sample_frame (extracts k frames from video), zoom_in (provides cropped image region), check_instruction (returns DOT-format text instructions or instruction images), and check_final_picture (shows target assembly image). Tool outputs can be images or text, enabling multimedia reasoning. The framework uses ReAct-style prompting for open-weight models and reasoning mode for proprietary models. Questions are answered through iterative thought-tool-output cycles until the model generates an answer or reaches the maximum turn limit.

## Key Results
- GPT-5 achieved a 14.1% improvement over the naive approach on ProMQA-Assembly
- Multimedia-returning tools showed clear advantages: GPT-5 mini improved 59.0→63.7, Gemini 2.5 Flash improved 48.2→52.4
- Agentic flexible tool selection outperformed all 6 workflow permutations for GPT-5 mini and MiMo-VL 7B
- All models except MiMo-VL 7B performed better than the naive approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multimedia-returning tools preserve visual information that would otherwise be lost in text-only grounding.
- **Mechanism:** When tools return images directly rather than captions, the VLM agent can perform its own visual analysis without information bottleneck.
- **Core assumption:** VLMs have sufficient visual reasoning capability to interpret returned images meaningfully.
- **Evidence anchors:**
  - [abstract]: "enables interleaved multimodal reasoning by making use of multimedia-returning tools"
  - [§5.1]: Ablation shows GPT-5 mini improves 59.0→63.7 and Gemini 2.5 Flash improves 48.2→52.4 when using multimedia vs text tools
  - [corpus]: Related work on GUI agents (Zhang et al., 2024) supports VLM-as-agent paradigm, but direct corpus evidence for multimedia tool superiority is weak
- **Break condition:** If the VLM lacks fine-grained visual discrimination, returned images provide no advantage over captions.

### Mechanism 2
- **Claim:** Agentic tool selection outperforms fixed workflow sequences by adapting information gathering to each question.
- **Mechanism:** The agent dynamically chooses which tools to call and in what order based on reasoning about the specific question, rather than following a predefined path.
- **Core assumption:** Models can reason about information needs and tool utility without explicit training.
- **Evidence anchors:**
  - [§5.2]: All 6 workflow permutations degraded performance for GPT-5 mini and MiMo-VL 7B compared to TAMA's flexible selection
  - [§4.3]: Different models show different tool usage patterns—GPT-5 heavily uses zoom_in while others don't
  - [corpus]: Workflow automation literature (LEGOMem) suggests modular procedural memory helps, but doesn't directly compare agentic vs workflow
- **Break condition:** If the model cannot reliably judge which tool is needed, flexible selection adds noise without benefit.

### Mechanism 3
- **Claim:** Interleaved multimodal reasoning enables iterative hypothesis refinement unavailable in single-pass prediction.
- **Mechanism:** The framework structures reasoning as alternating thought→tool_call→tool_output cycles, allowing the model to refine hypotheses based on new observations.
- **Core assumption:** The model maintains coherent reasoning state across turns and doesn't forget earlier observations.
- **Evidence anchors:**
  - [§1]: Single-pass approaches suffer from "errors in beginning processes...may be difficult to recover from"
  - [Table 3]: GPT-5 uses multiple zoom_in calls with different bounding boxes before concluding correctly
  - [corpus]: VideoAgent work references iterative grounding but primarily with text tools; direct corpus evidence for interleaved multimodal iteration is limited
- **Break condition:** If context length or attention degradation causes the model to lose track of earlier reasoning, iterative refinement fails.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) as agents vs tools**
  - Why needed here: TAMA's core design choice is using a VLM as the reasoning agent rather than a text-only LLM.
  - Quick check question: Can you explain why a VLM agent could directly reason about a returned image while a text LLM agent could not?

- **Concept: ReAct-style prompting and multi-turn tool use**
  - Why needed here: The framework implements iterative thought-action-observation cycles.
  - Quick check question: In a ReAct loop, what should the model do after receiving a