---
ver: rpa2
title: Online hand gesture recognition using Continual Graph Transformers
arxiv_id: '2502.14939'
source_url: https://arxiv.org/abs/2502.14939
tags:
- recognition
- gesture
- hand
- graph
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses online continuous hand gesture recognition
  using skeleton data, aiming to overcome limitations of segment-based methods in
  real-time scenarios. The authors propose a hybrid architecture combining Spatial
  Graph Convolutional Networks (S-GCN) for spatial feature extraction and a Transformer-based
  Graph Encoder (TGE) for temporal dependencies.
---

# Online hand gesture recognition using Continual Graph Transformers

## Quick Facts
- arXiv ID: 2502.14939
- Source URL: https://arxiv.org/abs/2502.14939
- Authors: Rim Slama; Wael Rabah; Hazem Wannous
- Reference count: 40
- Primary result: State-of-the-art accuracy on SHREC'21 with detection rate 0.916 and Jaccard index 0.853

## Executive Summary
This paper presents a hybrid architecture for online continuous hand gesture recognition using skeleton data. The system combines Spatial Graph Convolutional Networks (S-GCN) for spatial feature extraction with a Continual Transformer Graph Encoder (TGE) for temporal dependencies, addressing limitations of segment-based methods in real-time scenarios. A key innovation is the integration of continual learning mechanisms to adapt to evolving data distributions, enabling robust recognition in dynamic environments. The method achieves state-of-the-art performance on the SHREC'21 benchmark with a detection rate of 0.916 and a Jaccard index of 0.853, while maintaining a low false positive rate of 0.083.

## Method Summary
The approach uses a two-stage architecture: first, Spatial Graph Convolutional Networks extract spatial features from hand skeleton topology using normalized adjacency matrices and distance-based partitioning. Second, a Continual Transformer Graph Encoder processes temporal sequences using cached key-value memory buffers to reduce redundant computation while maintaining context. The system employs sliding window inference with per-class probability thresholds learned from validation data to filter false positives in unsegmented streams. Training uses Adam optimizer with cross-entropy loss and data augmentation including sliding-window fragments to help the model recognize partial gestures.

## Key Results
- Detection rate of 0.916 and Jaccard index of 0.853 on SHREC'21 benchmark
- False positive rate of 0.083, significantly outperforming existing methods
- State-of-the-art performance in online continuous hand gesture recognition
- Effective handling of continuous, unsegmented gesture sequences

## Why This Works (Mechanism)

### Mechanism 1
Graph convolution over hand skeleton topology captures spatial relationships that CNNs and RNNs miss. The S-GCN module constructs a normalized adjacency matrix from hand joint connectivity, then applies partitioned graph convolution using distance-based partitioning (root, direct neighbors, second-level neighbors). This aggregates features from each node with its neighbors weighted by learned edge importance masks. The hand's natural skeletal structure contains discriminative information for gesture classification that Euclidean coordinate representations alone do not expose.

### Mechanism 2
Continual attention with cached keys/values reduces redundant computation while preserving temporal context. The Continual TGE computes query, key, and value vectors for each incoming frame and stores the previous n computations in memory buffers (K_mem, V_mem). At time t, attention is computed as softmax(q_t · (k_t || K_mem) / √d_k) · (v_t || V_mem), enabling single-frame input with n-frame context. Temporal dependencies for gesture recognition operate within a bounded context window (n frames), beyond which older frames contribute minimally.

### Mechanism 3
Per-class probability thresholds learned from validation data reduce false positives in unsegmented streams. During validation, the system computes average classifier confidence for correctly predicted instances of each class C. During online inference, windows with P(C) < α(C) are relabeled as "No gesture" rather than the predicted class. False positive predictions have systematically lower confidence scores than true positives, and this pattern is consistent across validation and test distributions.

## Foundational Learning

- **Graph Convolutional Networks (GCNs)**: The S-GCN module assumes familiarity with adjacency matrices, degree normalization, and neighborhood aggregation. Quick check: Can you explain why D^(-1/2)AD^(-1/2) normalization prevents nodes with many neighbors from dominating feature aggregation?

- **Transformer Self-Attention**: The TGE module adapts standard transformer attention to graph sequences; understanding Q/K/V projections and scaled dot-product attention is prerequisite. Quick check: Why does the softmax in attention require scaling by 1/√d_k?

- **Online vs. Offline Recognition**: The paper distinguishes segment-based (offline) from continuous streaming (online) scenarios; the evaluation metrics and architecture choices depend on this distinction. Quick check: In online recognition, why is false positive rate often more critical than raw accuracy?

## Architecture Onboarding

- **Component map:** Input (3D skeleton sequence) → S-GCN (spatial features per frame, adjacency-based) → Positional Encoding (+PE) → Continual Transformer Graph Encoder (6 encoder layers, 8 heads each) → Global Average Pooling (graph nodes → sequence frames) → Fully Connected Classifier → Threshold Filter (per-class α(C)) → Output: gesture class or "No gesture"

- **Critical path:** The interaction between S-GCN output dimensions (γ × λ × d_model) and TGE input requirements. If spatial features are misshapen, positional encoding addition will fail silently or produce incorrect attention patterns.

- **Design tradeoffs:** Memory window size (n): Larger n captures longer dependencies but increases latency and memory; paper does not specify exact value. Number of encoder layers (6): Deeper = more representational capacity but slower inference. Sliding window stride (5 frames): Smaller stride = finer granularity but more computation per second.

- **Failure signatures:** High false positive rate on test data: Thresholds α(C) likely learned on unrepresentative validation data. Poor recognition of long-duration gestures: Memory window n may be too small. Inconsistent predictions across similar frames: Check that positional encoding is correctly added to spatial features.

- **First 3 experiments:** 1) Baseline spatial only: Run S-GCN without TGE on SHREC'21 to isolate spatial feature contribution to detection rate. 2) Memory window ablation: Test n ∈ {10, 20, 30, 50} frames to find context-length sweet spot for the 18 gesture classes. 3) Threshold sensitivity: Vary α(C) ± 0.1 from learned values and measure Jaccard index vs. false positive rate tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
How effectively can the proposed skeleton-based system be fused with EEG signals to maintain recognition performance in scenarios involving significant hand occlusion? The authors state in the Conclusion they plan to explore additional modalities such as EEG to complement skeleton-based recognition, especially in scenarios where hand occlusion is an issue. The current study relies exclusively on 3D skeleton data extracted from vision-based sensors; no experiments were conducted using bio-signals or multimodal fusion. Comparative experiments on a multimodal dataset containing occluded gestures would resolve this.

### Open Question 2
Can the system maintain its high detection rate and low latency when deployed for live, direct control of a robot in a collaborative task? The Conclusion outlines the future aim to integrate the hand gesture recognition system into human-robot interaction applications for direct control and collaborative tasks. The current results are derived from the offline evaluation of the SHREC'21 benchmark dataset, rather than a real-time robotic deployment where latency and safety are critical. A user study demonstrating the system controlling a robotic arm would resolve this.

### Open Question 3
Does the proposed "continual learning" mechanism actually mitigate catastrophic forgetting when adapting to new gesture classes, or is it optimized only for continuous inference on a fixed set of classes? The Abstract claims the mechanism enables "adaptability to evolving data distributions," but the Experiments section only validates the model on the static SHREC'21 dataset containing a fixed dictionary of 18 gestures. The paper evaluates "continual inference" but does not provide empirical evidence of "continual learning" (incrementally updating weights to learn new gestures without losing accuracy on old ones). Experiments where the model is sequentially trained on new gesture classes would resolve this.

## Limitations

- Inference window size and memory buffer size (n) are not specified, which critically affects temporal context and computational efficiency
- Hyperparameter details for regularization weights are mentioned but not quantified, potentially affecting model generalization
- Evaluation is limited to SHREC'21; performance on datasets with different skeleton topologies (e.g., NTU) remains untested

## Confidence

- **High confidence**: The spatial graph convolution mechanism (S-GCN) and its integration with hand skeleton topology is well-established in related literature
- **Medium confidence**: The continual attention mechanism (TGE) is novel in this context, with limited corpus validation; the claim of computational efficiency needs empirical verification
- **Medium confidence**: Threshold-based false positive reduction shows strong results on SHREC'21 but may not generalize to datasets with different gesture distributions or user populations

## Next Checks

1. **Memory window ablation**: Systematically vary the memory buffer size (n) from 10 to 50 frames to identify the optimal context window for the 18 gesture classes, measuring detection rate and false positive rate tradeoffs

2. **Cross-dataset transfer**: Evaluate the trained model on NTU or FPHA datasets without fine-tuning to assess generalization to different skeleton topologies and gesture vocabularies

3. **Real-time latency measurement**: Benchmark the complete pipeline (S-GCN + TGE + thresholding) on a standard CPU/GPU to verify the claimed real-time capability and identify bottlenecks in the continual attention mechanism