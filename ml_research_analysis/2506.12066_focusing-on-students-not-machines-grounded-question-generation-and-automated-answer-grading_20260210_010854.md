---
ver: rpa2
title: 'Focusing on Students, not Machines: Grounded Question Generation and Automated
  Answer Grading'
arxiv_id: '2506.12066'
source_url: https://arxiv.org/abs/2506.12066
tags:
- grading
- questions
- page
- dataset
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis develops an automated question generation and grading
  system for educational use. It introduces a visual layout-based document chunking
  method for PDFs that reduces context loss compared to fixed-window approaches.
---

# Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading

## Quick Facts
- arXiv ID: 2506.12066
- Source URL: https://arxiv.org/abs/2506.12066
- Reference count: 0
- Key result: Automated question generation and grading system with GPT-4o achieving weighted MAE of 0.21 and RMSD of 0.27

## Executive Summary
This thesis presents an automated educational system that generates questions from PDFs and grades student answers. The system introduces a visual layout-based document chunking method that preserves context better than traditional fixed-window approaches. Questions are generated based on learning objectives and Bloom's taxonomy using large language models (LLMs). The research creates ASAG2024, a benchmark dataset combining seven existing datasets with normalized grades. While GPT-4o shows the best grading performance, the system requires human oversight for high-stakes educational contexts due to residual error rates.

## Method Summary
The system employs visual layout analysis for document chunking, extracting context from PDFs based on their visual structure rather than fixed character windows. This approach aims to preserve semantic coherence and reduce context loss during processing. Question generation leverages learning objectives and Bloom's taxonomy to create pedagogically sound questions, with LLMs generating both questions and reference answers. For grading, the researchers compiled ASAG2024 by combining seven existing datasets and normalizing scores to a 0-1 scale. The system evaluates multiple LLM models for grading performance, with GPT-4o emerging as the top performer. Human oversight remains necessary due to the system's inability to achieve full autonomy in grading tasks.

## Key Results
- GPT-4o achieves the best grading performance with weighted MAE of 0.21 and weighted RMSD of 0.27
- Visual layout-based document chunking reduces context loss compared to fixed-window approaches
- No grading system reaches full autonomy, requiring human oversight in examination contexts
- Larger LLMs generally outperform specialized systems, which tend to overfit

## Why This Works (Mechanism)
The system leverages the strong reasoning capabilities of modern LLMs for both question generation and answer grading. By grounding questions in learning objectives and Bloom's taxonomy, the generated questions align with pedagogical frameworks. The visual layout-based chunking preserves semantic context from source documents, enabling more accurate question generation. The combination of multiple grading datasets into ASAG2024 provides diverse training data that helps models generalize across different question types and grading standards.

## Foundational Learning
- **Visual layout analysis**: Understanding document structure through spatial arrangement of elements; needed for preserving semantic context during chunking; quick check: test chunking consistency across differently formatted PDFs
- **Bloom's taxonomy**: Hierarchical framework for cognitive learning objectives; needed for generating questions at appropriate cognitive levels; quick check: validate question difficulty matches intended taxonomy level
- **LLM prompting techniques**: Strategic formulation of instructions to LLMs; needed for consistent question generation and grading; quick check: compare outputs across different prompt formulations
- **Benchmark dataset normalization**: Converting diverse grading scales to common metric; needed for meaningful model comparison; quick check: verify normalization preserves relative performance differences
- **Error metrics for grading**: MAE and RMSD for evaluating grading accuracy; needed for quantifying model performance; quick check: calculate metrics on held-out test data
- **Document semantic chunking**: Breaking text into meaningful segments; needed for efficient LLM processing while preserving context; quick check: measure information retention across chunking methods

## Architecture Onboarding

**Component map**: PDF processing -> Visual layout chunking -> Learning objective extraction -> Question generation -> Answer reference creation -> Student answer input -> Grading model -> Score output

**Critical path**: The core workflow flows from PDF document input through visual layout analysis and chunking, to learning objective identification, question generation, and finally to answer grading. The bottleneck typically occurs during document processing and chunking, as complex layouts may require additional parsing time.

**Design tradeoffs**: The system trades computational efficiency for context preservation by using visual layout analysis instead of simpler fixed-window chunking. This increases processing time but improves question relevance. Similarly, using larger general-purpose LLMs rather than specialized grading models sacrifices some domain specificity for better generalization across question types.

**Failure signatures**: 
- Poor chunking produces questions that reference incomplete or irrelevant document sections
- Inconsistent learning objective extraction leads to questions that don't align with intended educational goals
- LLM hallucination can generate factually incorrect questions or answers
- Grading bias emerges when models overfit to specific datasets or grading patterns
- Normalization artifacts may compress meaningful grade distinctions

**3 first experiments**:
1. Compare visual layout chunking against fixed-window chunking on document comprehension tasks
2. Test question generation quality with and without explicit Bloom's taxonomy prompts
3. Evaluate grading consistency across different LLM models using a standardized answer set

## Open Questions the Paper Calls Out
None

## Limitations
- PDF formatting inconsistencies may affect the reliability of visual layout-based chunking across different document sources
- LLM performance variability due to prompt engineering differences could impact reproducibility of question generation results
- The ASAG2024 benchmark may inherit biases from original datasets and lose grading nuance through normalization to 0-1 scale
- Current error rates (MAE 0.21, RMSD 0.27) indicate the system cannot operate autonomously in high-stakes educational settings

## Confidence
- Document chunking effectiveness: Medium confidence
- Question generation quality: Medium confidence  
- Grading benchmark reliability: Medium confidence
- Grading performance metrics: High confidence

## Next Checks
1. Test the document chunking approach on a diverse corpus of PDFs from different disciplines and institutions to evaluate robustness across formatting variations
2. Conduct a large-scale human evaluation comparing system-generated questions against expert-created questions for alignment with learning objectives and Bloom's taxonomy levels
3. Implement cross-validation testing of the grading system on held-out portions of the ASAG2024 dataset to assess generalization and potential overfitting to specific grading patterns