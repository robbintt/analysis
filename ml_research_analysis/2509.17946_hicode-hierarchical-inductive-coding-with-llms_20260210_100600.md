---
ver: rpa2
title: 'HICode: Hierarchical Inductive Coding with LLMs'
arxiv_id: '2509.17946'
source_url: https://arxiv.org/abs/2509.17946
tags:
- themes
- data
- labels
- page
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces HICode, an LLM-based pipeline for conducting
  deep, nuanced analysis over large-scale text corpora by scaling inductive coding
  methods from qualitative research. HICode consists of two modules: label generation,
  which produces fine-grained, research-question-relevant labels for each text segment
  using a tailored prompt, and hierarchical clustering, which groups labels into meaningful
  themes through iterative LLM prompting.'
---

# HICode: Hierarchical Inductive Coding with LLMs

## Quick Facts
- arXiv ID: 2509.17946
- Source URL: https://arxiv.org/abs/2509.17946
- Reference count: 28
- LLM-based pipeline for scaling inductive coding from qualitative research across large text corpora

## Executive Summary
HICode introduces a novel approach to scaling inductive coding methods from qualitative research by leveraging large language models. The system consists of two modules: label generation that produces fine-grained, research-question-relevant labels for text segments, and hierarchical clustering that groups these labels into meaningful themes through iterative LLM prompting. The approach is validated across three diverse datasets - Media Frames Corpus, Astro Queries, and ML Values - demonstrating strong alignment with human-constructed themes through both automated metrics and human evaluation.

## Method Summary
HICode employs a two-stage pipeline for inductive coding at scale. The first module uses LLM prompting to generate detailed, context-specific labels for individual text segments based on research questions. The second module performs hierarchical clustering where the LLM iteratively groups labels into themes, creating a multi-level thematic structure. This approach enables researchers to conduct deep qualitative analysis over large corpora while maintaining the nuanced understanding characteristic of traditional inductive coding methods.

## Key Results
- On Astro dataset, HICode achieved precision of 0.72 and recall of 0.74 in human evaluation, outperforming TopicGPT (precision 0.18, recall 0.33)
- On ML Values dataset, HICode achieved precision of 0.96 and recall of 0.57 at the most lenient threshold
- Case study on opioid litigation documents successfully surfaced themes like aggressive sales strategies and crisis management tactics at scale

## Why This Works (Mechanism)
HICode leverages LLMs' ability to understand context and generate semantically rich labels, then uses iterative prompting to create hierarchical theme structures that mirror human inductive coding processes. The approach scales qualitative analysis by automating the labor-intensive aspects of label generation and theme identification while preserving the nuanced understanding that makes inductive coding valuable.

## Foundational Learning
- Inductive coding principles: Essential for understanding how themes emerge from data rather than being predefined
- LLM prompting techniques: Critical for generating research-relevant labels and guiding hierarchical clustering
- Hierarchical clustering concepts: Needed to understand how themes can be organized at multiple levels of abstraction
- Qualitative research validation methods: Important for interpreting precision, recall, and human evaluation metrics
- Cross-domain dataset characteristics: Necessary for assessing generalizability claims

## Architecture Onboarding

Component Map: Label Generation -> Hierarchical Clustering -> Theme Validation

Critical Path: Text segments → LLM-generated labels → Iterative clustering prompts → Multi-level theme hierarchy → Validation against human themes

Design Tradeoffs: HICode prioritizes label granularity and thematic richness over speed, accepting longer processing times for more nuanced analysis. The iterative clustering approach trades computational efficiency for better alignment with human coding patterns.

Failure Signatures: Poor label generation leads to noisy or irrelevant themes; insufficient clustering iterations result in overly broad or fragmented themes; misalignment with research questions produces themes that miss key insights.

First Experiments:
1. Run HICode on a small dataset with known themes to verify label generation quality
2. Test hierarchical clustering depth sensitivity by varying iteration counts
3. Compare performance across different LLM models to assess robustness

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Validation relies on comparison against human-constructed themes rather than true ground truth, creating circularity
- Human evaluation conducted on only one of three datasets, leaving performance unverified by human judgment in other domains
- Manual case study annotations lack systematic rigor and inter-rater reliability measures
- Ablation studies test only three LLM variants without exploring prompt engineering variations

## Confidence
- High confidence in methodological description and implementation details
- Medium confidence in comparative performance claims against TopicGPT
- Medium confidence in robustness claims from ablation studies
- Low confidence in generalizability claims beyond tested datasets

## Next Checks
1. Conduct human evaluation on all three datasets to verify consistent performance across different domains
2. Implement inter-rater reliability assessment for manual case study annotations
3. Test HICode on at least two additional datasets from different domains to validate cross-domain applicability claims