---
ver: rpa2
title: Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided Prompting
  of Vision-Language Models
arxiv_id: '2503.02917'
source_url: https://arxiv.org/abs/2503.02917
tags:
- concepts
- retinal
- concept
- learning
- diseases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of few-shot and zero-shot retinal
  disease diagnosis using color fundus images. Current methods rely solely on image
  data and lack interpretability.
---

# Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided Prompting of Vision-Language Models

## Quick Facts
- **arXiv ID:** 2503.02917
- **Source URL:** https://arxiv.org/abs/2503.02917
- **Reference count:** 40
- **Primary result:** Achieves ~5.8% mAP improvement for 16-shot learning and ~2.7% for zero-shot detection using concept-guided prompt learning with CLIP.

## Executive Summary
This paper addresses the challenge of few-shot and zero-shot retinal disease diagnosis using color fundus images by incorporating interpretable disease concepts into vision-language models. The method extracts disease concepts from GPT models, validates them with experts, and uses them in concept-guided prompt learning to improve classification performance while providing interpretability. The framework achieves significant improvements over state-of-the-art methods and demonstrates effectiveness in distinguishing similar diseases and detecting novel diseases.

## Method Summary
The method uses a two-stage framework with frozen CLIP encoders. Stage 1 learns prompt tokens terminated with concept names to predict disease concepts from images using BCE loss. Stage 2 trains a Logistic Regression classifier to map concept predictions to disease categories. The concept bank is generated by querying GPT-3.5 with two prompt templates, retaining overlapping concepts that are validated by ophthalmologists. The approach is evaluated on an in-house dataset (4,535 images, 29 diseases) and RFMiD (3,200 images, 46 diseases) using 2/4/8/16-shot settings.

## Key Results
- Achieves approximately 5.8% mAP improvement for 16-shot learning compared to state-of-the-art prompt learning methods
- Demonstrates 2.7% mAP improvement for zero-shot detection of novel diseases
- Concept-based interpretability enables distinction between similar disease categories
- Ablation studies show optimal performance with [CONCEPT] token at END position and 32 learnable tokens

## Why This Works (Mechanism)

### Mechanism 1
Incorporating disease-specific concepts into prompt learning improves few-shot classification performance by providing domain knowledge anchors that guide visual feature alignment. Learnable prompt vectors terminated with concept names are optimized via BCE loss to align image features with textual concept embeddings, forcing the model to reason through clinically meaningful attributes rather than abstract feature correlations.

### Mechanism 2
A two-stage concept bottleneck architecture enables zero-shot detection of novel diseases by learning transferable concept-to-disease mappings. Stage 1 learns to predict concepts present in any image, while Stage 2 (Logistic Regression) learns linear relationships between concept presence and disease labels, allowing detection of novel diseases if their concepts were learned in Stage 1.

### Mechanism 3
LLM-generated concept banks validated by experts reduce manual annotation burden while maintaining clinical accuracy. Two prompt templates query GPT-3.5 for disease concepts, with overlapping concepts across prompts retained and validated by ophthalmologists, shifting expert role from creation to verification.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) with Contrastive Learning**
  - Why needed here: The entire architecture builds on CLIP's frozen encoders; understanding how image-text alignment works is prerequisite.
  - Quick check question: Can you explain why CLIP uses cosine similarity loss rather than cross-entropy for image-text matching?

- **Concept: Prompt Learning vs. Fine-tuning**
  - Why needed here: This work uses learnable prompt tokens (not encoder fine-tuning) to adapt CLIP; understanding parameter efficiency is critical.
  - Quick check question: What is the difference between CoOp (learnable prompts) and standard fine-tuning of a classification head?

- **Concept: Concept Bottleneck Models (CBMs)**
  - Why needed here: Stage 2 relies on CBM architecture where predictions flow through interpretable intermediate concepts.
  - Quick check question: Why might a CBM sacrifice accuracy compared to end-to-end models, and what does this work do to mitigate that?

## Architecture Onboarding

- **Component map:** Input Image → CLIP Image Encoder (frozen) → Visual Features (fm) → Cosine Similarity (fm * gm) → Concept Logits → Stage 2: Logistic Regression → Disease Prediction

- **Critical path:** Prompt token initialization → Concept loss backpropagation → Concept logit quality → Stage 2 classifier performance. If concept prediction accuracy is low (<70%), downstream disease classification degrades substantially.

- **Design tradeoffs:**
  - [CONCEPT] token position: END performs best (Table 3), likely due to contextual summary effect; START/MIDDLE degrade 1-2% mAP
  - Number of learnable tokens: Saturates at 32 tokens (Table 4); more tokens increase compute without gains
  - Stage 2 classifier: Logistic Regression outperforms MLP end-to-end training (Table 5) — suggests linear concept-to-disease mapping is sufficient and less prone to overfitting in few-shot regimes

- **Failure signatures:**
  - Low concept prediction accuracy on validation set → Prompt learning not converging; check learning rate or concept bank quality
  - High concept accuracy but low disease accuracy → Stage 2 overfitting; reduce classifier complexity or increase regularization
  - Novel disease detection fails → Concept bank missing key attributes; expand with ophthalmologist review

- **First 3 experiments:**
  1. Run vanilla CLIP zero-shot and CoOp without concepts on your dataset to establish floor performance.
  2. Have a domain expert review the GPT-generated concept list for your disease set; measure concept overlap across templates.
  3. With a fixed 16-shot setup, test [CONCEPT] at START, MIDDLE, END to confirm END optimal before full hyperparameter search.

## Open Questions the Paper Calls Out
- **Cross-domain applicability:** The method has "potential applicability to other medical image analysis domains" but was only evaluated on color fundus datasets, leaving generalization to other modalities untested.
- **Clinical utility validation:** While the method enhances "trustworthiness" and marks a step toward "real-world clinical implementation," no clinical user study measures how concept-based interpretability impacts diagnostic accuracy and confidence compared to black-box predictions.
- **Expert validation bottleneck:** The reliance on manual expert validation for GPT-generated concepts creates a potential bottleneck, but the paper does not explore automated filtering or verification mechanisms to reduce this dependency without compromising diagnostic safety.

## Limitations
- Performance critically depends on concept bank quality, with potential for hallucinations or irrelevant terms to create spurious correlations
- Generalizability to other medical imaging domains remains untested, limiting cross-domain validation
- Fixed two-stage architecture assumes linear concept-to-disease mappings, which may not hold for complex diseases with non-linear relationships

## Confidence
- **High confidence:** Few-shot classification improvements (5.8% mAP gain) are well-supported by controlled experiments across multiple shot settings and ablation studies
- **Medium confidence:** Zero-shot detection claims are supported by the in-house dataset but show significant performance gap on RFMiD (2.4% vs 5.8% in-house)
- **Medium confidence:** Interpretability claims are demonstrated through concept visualization but lack quantitative evaluation against clinical gold standards

## Next Checks
1. Conduct systematic expert review of all GPT-generated concepts to identify potential hallucinations or clinically irrelevant terms, measuring concept-to-disease mapping accuracy before model training.
2. Test the framework on a third retinal dataset or different medical imaging modality (e.g., chest X-rays) to validate concept transferability and identify domain-specific limitations.
3. Compare concept-based explanations against established clinical feature importance metrics or physician annotations to quantify the practical utility of interpretability claims.