---
ver: rpa2
title: 'Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference'
arxiv_id: '2510.13831'
source_url: https://arxiv.org/abs/2510.13831
tags:
- routing
- sparsity
- training
- token
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces informed routing to address the limitations
  of greedy routing in dynamic token-level computation allocation for large language
  models (LLMs). While existing methods make binary execute-or-skip decisions based
  on immediate token importance, informed routing replaces this with execute-or-approximate
  decisions using a lightweight feature forecaster (LFF) that estimates unit outputs
  before routing.
---

# Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference

## Quick Facts
- **arXiv ID:** 2510.13831
- **Source URL:** https://arxiv.org/abs/2510.13831
- **Reference count:** 12
- **Primary result:** Informed routing achieves better efficiency-performance trade-offs than SkipGPT by replacing binary execute-or-skip decisions with execute-or-approximate decisions using lightweight feature forecasting.

## Executive Summary
This paper introduces informed routing to address the limitations of greedy routing in dynamic token-level computation allocation for large language models (LLMs). While existing methods make binary execute-or-skip decisions based on immediate token importance, informed routing replaces this with execute-or-approximate decisions using a lightweight feature forecaster (LFF) that estimates unit outputs before routing. This allows routers to base decisions on token recoverability rather than just immediate impact, preserving model fidelity while reducing computation. The three-stage optimization includes LFF initialization, router training, and optional LoRA fine-tuning. Experiments on Llama models (3B and 8B) show that informed routing matches or surpasses strong baselines like SkipGPT across multiple sparsity levels, reducing training time by over 50% while achieving better efficiency-performance trade-offs. Analysis reveals that self-attention modules are highly "predictable" and can be effectively approximated by simple linear operations, validating the informed routing paradigm.

## Method Summary
Informed routing introduces a paradigm shift from binary execute-or-skip decisions to execute-or-approximate decisions in dynamic computation allocation for LLMs. The method employs a lightweight feature forecaster (LFF) that estimates unit outputs before routing, enabling routers to make decisions based on token recoverability rather than immediate impact. The three-stage optimization process includes: (1) LFF initialization where a lightweight module is trained to predict transformer unit outputs, (2) router training where the routing policy learns to choose between execution and approximation based on LFF predictions, and (3) optional LoRA fine-tuning to further improve performance. The approach leverages the insight that self-attention modules are highly predictable and can be effectively approximated by simple linear operations, allowing for significant computational savings without sacrificing model quality.

## Key Results
- Informed routing matches or surpasses SkipGPT baselines across multiple sparsity levels on Llama 3B and 8B models
- Training time reduced by over 50% compared to full execution while maintaining or improving performance
- Self-attention modules shown to be highly "predictable" and amenable to linear approximation
- Three-stage optimization process (LFF initialization, router training, optional LoRA fine-tuning) achieves superior efficiency-performance trade-offs

## Why This Works (Mechanism)
Informed routing works by fundamentally changing the decision-making framework from immediate token importance to token recoverability. Instead of asking "is this token important enough to execute?", the system asks "can we approximate this token's output without significant quality loss?". The lightweight feature forecaster provides estimated outputs that allow the router to make informed decisions about whether execution or approximation will better serve downstream performance. This shift is particularly effective because self-attention modules, despite their computational complexity, exhibit predictable behavior that can be captured by simpler linear approximations. The informed routing framework leverages this predictability to selectively apply approximations where they cause minimal harm, achieving computational savings while maintaining or improving model quality.

## Foundational Learning
- **Dynamic computation allocation** - The process of varying computational resources per token based on importance; needed to understand the efficiency gains, quick check: compare static vs dynamic approaches
- **Feature forecasting** - Predicting transformer unit outputs before execution; needed to grasp how informed decisions are made, quick check: examine LFF architecture and accuracy
- **Token recoverability** - The concept that some tokens can be approximated without significant quality loss; needed to understand the routing decision framework, quick check: analyze approximation quality vs computation savings
- **Self-attention predictability** - The observation that self-attention outputs follow predictable patterns; needed to understand why approximation works, quick check: validate predictability across different attention mechanisms
- **Three-stage optimization** - The sequential process of LFF initialization, router training, and fine-tuning; needed to understand the complete methodology, quick check: trace through each optimization stage
- **LoRA fine-tuning** - Low-Rank Adaptation as an optional refinement step; needed to understand performance optimization, quick check: compare results with and without LoRA

## Architecture Onboarding

**Component map:** Input tokens -> LFF (Lightweight Feature Forecaster) -> Router -> [Execute path OR Approximate path] -> Transformer units -> Output

**Critical path:** Input → LFF → Router → Decision → [Execution/Approximation] → Transformer units → Output

**Design tradeoffs:** The key tradeoff is between LFF complexity and routing accuracy versus computational overhead. Simpler LFFs reduce overhead but may lead to suboptimal routing decisions, while more complex forecasters improve accuracy but add computational cost. The three-stage optimization balances these competing factors.

**Failure signatures:** Poor routing decisions manifest as either excessive computation (overly conservative routing) or quality degradation (overly aggressive approximation). LFF inaccuracies lead to systematic routing errors, particularly in regions where token behavior is less predictable.

**First experiments:** (1) Ablation study varying LFF complexity to find optimal balance between accuracy and overhead, (2) Cross-task validation to test routing generalization beyond standard benchmarks, (3) Larger model scaling tests to assess performance on 20B+ parameter models.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to relatively small Llama models (3B and 8B parameters), raising scalability concerns
- Self-attention predictability findings require validation across diverse model architectures and attention mechanisms
- Feature forecasting overhead may become proportionally more significant in larger frontier models
- Limited comparison against other approximation strategies beyond SkipGPT baseline

## Confidence
- **High** confidence in the informed routing framework methodology and three-stage optimization process
- **Medium** confidence in empirical results due to limited model size range and narrow baseline comparisons
- **Low** confidence in generalizability of self-attention predictability findings given narrow experimental scope

## Next Checks
(1) Scaling experiments on larger models (20B+ parameters) to assess routing overhead and approximation quality at scale
(2) Cross-architecture validation testing informed routing on models with different attention mechanisms and architectural choices
(3) Ablation studies varying feature forecaster complexity to quantify trade-off between forecasting accuracy and computational overhead across different sparsity targets