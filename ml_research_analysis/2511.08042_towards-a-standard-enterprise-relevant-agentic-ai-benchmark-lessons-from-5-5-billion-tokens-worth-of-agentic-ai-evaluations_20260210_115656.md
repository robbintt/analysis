---
ver: rpa2
title: 'Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from
  5.5 billion tokens'' worth of agentic AI evaluations'
arxiv_id: '2511.08042'
source_url: https://arxiv.org/abs/2511.08042
tags:
- type
- name
- qwen3
- file
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'KAMI v0.1 evaluated 35 LLM configurations across 5.5 billion tokens
  to measure real-world agentic AI performance on enterprise tasks. The benchmark
  revealed a stark disconnect between traditional LLM rankings and practical capability:
  newer models like Qwen3 often underperformed older versions like Qwen2.5-72B on
  mundane but critical tasks like CSV analysis and database querying.'
---

# Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations

## Quick Facts
- arXiv ID: 2511.08042
- Source URL: https://arxiv.org/abs/2511.08042
- Reference count: 40
- Primary result: KAMI v0.1 found a stark disconnect between traditional LLM rankings and practical capability, with newer models underperforming on real-world tasks despite better static benchmark scores

## Executive Summary
This paper presents KAMI v0.1, an agentic AI benchmark that evaluated 35 LLM configurations across 5.5 billion tokens to measure real-world performance on enterprise tasks. The benchmark revealed significant discrepancies between traditional LLM rankings and practical capability, particularly showing that newer models like Qwen3 often underperformed older versions like Qwen2.5-72B on mundane but critical tasks. The study found that reasoning models showed poor cost-effectiveness, increasing tokens by 10-14x while improving accuracy modestly. Through contamination-resistant, dynamically randomized testing, the benchmark exposed fundamental limitations in how current models handle multi-step tool use and task execution in realistic enterprise scenarios.

## Method Summary
KAMI v0.1 uses the PICARD framework to generate contamination-resistant, dynamically randomized test variables and sandbox environments. The evaluation runs 570 test items per model (19 templates × 30 samples), repeated across 8 independent runs to capture stochastic behavior. The agentic server manages the inference loop with tools for Python execution, filesystem operations, and SQLite query execution. Models are evaluated on pooled accuracy, standard deviation across runs, RSE, 95% t-based CI, average time per conversation, and average tokens per conversation. The benchmark tests three fundamental categories: filesystem operations, text extraction, and database operations using CSV files and SQLite databases.

## Key Results
- Newer Qwen3 models underperformed older Qwen2.5-72B on practical tasks despite better static benchmark scores
- Reasoning models showed poor cost-effectiveness, increasing tokens by 10-14x for only modest accuracy gains
- FP8 quantization produced mixed results, with some models improving and others declining
- A "no output" tool feedback message caused capable models to abandon correct strategies and fail
- Standard deviation across runs revealed reliability issues not captured by average accuracy scores

## Why This Works (Mechanism)

### Mechanism 1: Contamination Resistance via Dynamic Randomization
- **Claim:** Static benchmarks suffer from data contamination (memorization), whereas dynamic randomization of test variables exposes true agentic capability gaps.
- **Mechanism:** The PICARD framework randomizes variables (e.g., specific line numbers, file names, monetary thresholds) and sandbox state for every test run. This prevents LLMs from retrieving memorized answers from training data, forcing them to execute genuine multi-step tool use (reading files, querying databases) to succeed.
- **Core assumption:** Models that fail dynamic tests but pass static ones rely primarily on pattern matching rather than reasoning.
- **Evidence anchors:** [Section 2.1] defines "agentic disconnect" and contamination issues; [Section 4.2.1] shows Qwen3 models outperforming Qwen2.5 on static benchmarks (BFCLv3) but failing KAMI's dynamic tasks.

### Mechanism 2: Tool Feedback-Induced Failure Loops
- **Claim:** Ambiguous or "silent" success messages from tools can cause capable models to hallucinate or abandon correct strategies.
- **Mechanism:** When a tool returns "Code executed successfully with no output," models may interpret "no output" as a failure. This triggers a strategy switch (e.g., abandoning Python code to manually "eyeball" text), which always fails for tasks requiring precise counting or extraction.
- **Core assumption:** The model weights the phrase "no output" more heavily than "successfully" when planning the next step.
- **Evidence anchors:** [Section 5.2.6] explicitly details the Q303 failure mode where models switched to manual counting due to the "no output" message.

### Mechanism 3: Asymmetric Cost of Reasoning Overhead
- **Claim:** Reasoning-optimized models (hybrid thinking) provide modest accuracy gains on mundane tasks at disproportionate computational cost.
- **Mechanism:** Reasoning models generate hidden "chain of thought" tokens before producing an output. For complex tasks, this improves logic. For mundane tasks (e.g., simple CSV filtering), this overhead increases token usage by 10-14x and latency by 4-6x without proportional accuracy gains, as the task complexity does not justify the reasoning budget.
- **Core assumption:** The latency/token cost is the primary bottleneck for enterprise deployment, rather than raw accuracy maximization.
- **Evidence anchors:** [Section 5.3] Table 13 shows Qwen3 4B increasing tokens 14x (900 to 13k) for only a ~13% accuracy boost.

## Foundational Learning

- **Concept:** **Benchmark Contamination (Data Leakage)**
  - **Why needed here:** Understanding why newer models (Qwen3) can score higher on leaderboards but fail real-world tests in this paper requires grasping that they may have memorized test answers.
  - **Quick check question:** If a model scores 90% on a public test set but 50% on the same task with randomized names/values, what is the likely cause?

- **Concept:** **Context Engineering (Tool Design)**
  - **Why needed here:** The paper highlights that tools are prompts. Understanding that error messages, tool names, and parameter descriptions are part of the "context window" is vital to debugging the failure modes in Section 5.2.6.
  - **Quick check question:** Does changing a tool's return message from "Success" to "Done" count as prompt engineering?

- **Concept:** **Statistical Reliability (Standard Deviation & Confidence Intervals)**
  - **Why needed here:** The paper rejects single-pass evaluation. You must understand why running a test 8 times (Section 3.3) and measuring standard deviation (Section 3.5.2) is necessary to distinguish a reliable agent from a lucky one.
  - **Quick check question:** Why is a model with 70% accuracy and ±1% std dev often better for enterprise than one with 75% accuracy and ±10% std dev?

## Architecture Onboarding

- **Component map:** PICARD Engine -> Agentic Server -> Sandbox
- **Critical path:** 1. Define task template (e.g., "Extract X from database") 2. Implement PICARD randomization (vary target X, table names, values) 3. Execute 8 independent runs to capture stochastic behavior 4. Analyze not just pooled accuracy, but Standard Deviation and t-CI intervals
- **Design tradeoffs:** Reasoning vs. Instruct (accuracy boosts but increases latency 4-6x and token cost 10-14x); Quantization (FP8) unpredictable; Centralized vs. Distributed Servers (easier to manage but risk "blast radius" failures)
- **Failure signatures:** "No Output" Spiral (agent repeatedly retries working tool or switches to manual counting); Schema Hallucination (agent guesses column names instead of using inspection tools); Hint Backfire (providing hints causes model to abandon correct instruction following)
- **First 3 experiments:** 1. Message Sensitivity Test (compare success rates when tool returns "Success" vs. "Code executed successfully with no output") 2. Reasoning ROI Test (calculate cost-per-percentage-point-accuracy-gain for Thinking mode ON vs OFF) 3. Reliability Stress Test (run same prompt 8 times with randomized variables; if Standard Deviation exceeds 5%, flag as unreliable)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimum sample size and run count required to establish statistically significant agentic reliability without incurring prohibitive evaluation costs?
- **Basis in paper:** [explicit] The authors state that "Finding a lower acceptable setting is one of the next design decisions to be made for v0.2," noting that the standard of 8 runs with 30 samples proved "too time-consuming and expensive" despite providing high total trial counts.
- **Why unresolved:** While the authors propose a two-stage evaluation approach (initial filtering with fewer runs, followed by intense evaluation of top candidates), they have not empirically validated the statistical power or reliability of lower-cost configurations.
- **What evidence would resolve it:** A sensitivity analysis comparing metric stability (Standard Deviation, RSE, t-CI) across various run/sample configurations to identify the optimal cost-accuracy trade-off threshold.

### Open Question 2
- **Question:** Does high performance on universal filesystem and SQL tasks reliably predict capability in specialized, vendor-specific enterprise environments?
- **Basis in paper:** [explicit] Section 3.2 posits that current fundamental tasks are "likely to generalize reasonably well" across specialized domains, but the authors explicitly list vendor-specific environments (e.g., Oracle, PostgreSQL, MongoDB) as a requirement for the "grand roadmap of KAMI."
- **Why unresolved:** The "agentic disconnect" observed between standard benchmarks and KAMI v0.1 suggests that capability does not always transfer between domains; it remains unproven whether proficiency in general agentic tasks translates to specialized administrative or database operations.
- **What evidence would resolve it:** The release of subsequent KAMI versions containing vendor-specific task categories, followed by a correlation analysis between v0.1 scores and performance on these specialized extensions.

### Open Question 3
- **Question:** How can context engineering strategies be designed to prevent the suppression of beneficial model behaviors, such as self-correction and data sampling?
- **Basis in paper:** [inferred] Section 5.2.3 details a case where "well-intentioned hints" designed to assist the model actually "completely suppressed" a self-correction mechanism (data sampling), leading to performance degradation. The authors explicitly call for "careful empirical validation of prompting strategies."
- **Why unresolved:** Current prompt engineering often relies on heuristics; the paper demonstrates that interventions targeting specific failure modes (e.g., schema assumptions) can inadvertently trigger negative "mental model shifts" (e.g., applying incorrect database conventions) that are difficult to predict.
- **What evidence would resolve it:** Systematic ablation studies on prompt variations across diverse models to identify heuristics that improve accuracy without disabling internal verification mechanisms.

## Limitations

- The evaluation framework relies on a proprietary PICARD system with unspecified implementation details for scoring types
- The study only evaluates models through 2024-2025, potentially missing newer architectures
- The randomization mechanism's contamination resistance depends on whether training data contains PICARD template logic
- The paper does not provide direct evidence of what specific training data enables the contamination behavior observed

## Confidence

**High Confidence**: The cost-effectiveness findings for reasoning models (10-14x token increase for modest accuracy gains) are supported by specific numerical evidence across multiple model families in Section 5.3.

**Medium Confidence**: The tool feedback-induced failure loops are clearly documented for Q303, but generalizability to other models and tasks remains uncertain without broader testing.

**Low Confidence**: The exact mechanism by which newer Qwen3 models underperform older versions on static benchmarks despite better dynamic performance requires further investigation.

## Next Checks

1. **Cross-Architecture Contamination Test**: Evaluate the same dynamic tasks on models from different companies (e.g., Anthropic, Google) to determine if contamination effects are universal or specific to certain training approaches.

2. **Tool Feedback Protocol Standardization**: Systematically vary tool return messages across different failure modes (not just "no output") to map the full parameter space of how message phrasing affects model behavior and develop standardized tool communication protocols.

3. **Extended Temporal Analysis**: Re-run the benchmark with models trained through 2026 to assess whether newer architectures have evolved to better handle dynamic, multi-step tasks while maintaining contamination resistance.