---
ver: rpa2
title: Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained
  Devices
arxiv_id: '2502.10239'
source_url: https://arxiv.org/abs/2502.10239
tags:
- fedspzo
- memory
- federated
- zero-order
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of federated fine-tuning large
  language models (LLMs) on resource-constrained edge devices, where high memory,
  communication, and computational demands pose significant barriers. The proposed
  method, Federated Split-Perturbation Zero-order Optimization (FedSPZO), leverages
  zero-order optimization to enable fine-tuning with inference-level memory requirements
  while preserving data privacy.
---

# Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2502.10239
- Source URL: https://arxiv.org/abs/2502.10239
- Authors: Mohamed Aboelenien Ahmed; Kilian Pfeiffer; Ramin Khalili; Heba Khdr; Jörg Henkel
- Reference count: 12
- One-line primary result: FedSPZO achieves 1.6-3× computation reduction vs state-of-the-art while maintaining inference-level memory and reducing upload communication by ~3 orders of magnitude

## Executive Summary
This paper addresses the challenge of federated fine-tuning large language models on resource-constrained edge devices where high memory, communication, and computational demands pose significant barriers. The proposed method, Federated Split-Perturbation Zero-order Optimization (FedSPZO), leverages zero-order optimization to enable fine-tuning with inference-level memory requirements while preserving data privacy. FedSPZO divides the model into two blocks and applies a different number of perturbations per block, reducing computational overhead by reusing intermediate outputs and improving gradient estimation accuracy.

## Method Summary
FedSPZO implements a federated learning framework for zero-order optimization of LLMs by splitting the model into two blocks (f₁ for early layers, f₂ for later layers) and applying asymmetric perturbation counts (P₁=2, P₂=8). The method uses central difference finite differences for gradient estimation, the seed trick for on-the-fly perturbation generation to eliminate activation storage, and transmits only scalar gradients and seeds to reduce communication. The server reconstructs client models using seeds and performs FedAvg aggregation. The approach is evaluated on SST2, RTE, WIC, and BOOLQ tasks using RoBERTa-large with prompt-based tuning.

## Key Results
- Achieves 1.6-3× reduction in computation overhead compared to state-of-the-art zero-order federated learning techniques
- Maintains inference-level memory requirements with less than 0.5% memory overhead compared to MeZO
- Reduces upload communication costs by transmitting only scalar gradients, achieving ~3 orders of magnitude improvement over parameter transmission
- Demonstrates minor accuracy degradation relative to first-order backpropagation methods while significantly reducing resource requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Splitting the model into two blocks with asymmetric perturbation counts reduces total computation while maintaining gradient estimation quality
- **Mechanism**: The model is divided into f₁ (early layers, larger) and f₂ (later layers, smaller, typically LM-Head). Block f₁ uses fewer perturbations (P₁=2), computing intermediate outputs ±yₗ that are reused across P₂=8 perturbations for f₂
- **Core assumption**: Later layers benefit more from multiple perturbations while spreading their improved gradient signal back through the shared loss differences
- **Evidence anchors**: [abstract] "divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence"; [Section 3.2.2, Eq. 7] "zo-fwFLOPs = 2×fw₁FLOPs × P₁ + 2×fw₂FLOPs × P₂"
- **Break condition**: If the second block contains substantial parameters (not just a small head), the computational advantage erodes

### Mechanism 2
- **Claim**: Zero-order optimization with central difference enables training with inference-level memory by eliminating activation storage
- **Mechanism**: Instead of backpropagation, gradients are estimated via finite differences: g = [L(θ+εz;B) - L(θ-εz;B)] / 2ε. The "seed trick" regenerates perturbation vectors z ~ N(0,I) on-demand from a pseudo-random seed
- **Core assumption**: The loss landscape of pre-trained LLMs is sufficiently smooth that noisy zero-order gradients converge with reasonable step counts
- **Evidence anchors**: [abstract] "enabling fine-tuning with inference-level memory requirements"; [Section 3.1, Eq. 1] Formal definition of scalar projected gradient
- **Break condition**: Very deep models or tasks requiring precise gradient information may exhibit slow convergence or accuracy degradation

### Mechanism 3
- **Claim**: Transmitting only scalar gradients and seeds reduces upload communication by ~3 orders of magnitude compared to parameter transmission
- **Mechanism**: Clients upload G (scalar gradients per step per block) and S (seeds used). Server reconstructs the exact model updates by regenerating perturbation vectors from seeds
- **Core assumption**: The server has sufficient compute to reconstruct all client models; pseudo-random generators are deterministic across client-server
- **Evidence anchors**: [Section 3.2.1] "uploaded only the vectors of projected gradients G and seeds S... independent of the model parameters"; [Section 4.3] "despite the fact that LoRA uploads only low-rank modules, it falls behind with about 3 order of magnitude gap to FedSPZO"
- **Break condition**: If client-server RNG implementations diverge, reconstruction fails

## Foundational Learning

- **Concept: Zero-Order Optimization (ZO-SGD)**
  - Why needed here: Core technique enabling gradient-free training. Understanding how finite differences approximate gradients explains both the memory savings and the noise/accuracy tradeoff
  - Quick check question: Given loss values L(θ+εz)=2.3 and L(θ-εz)=2.1 with ε=0.01, what is the scalar projected gradient g?

- **Concept: Federated Averaging (FedAvg)**
  - Why needed here: FedSPZO builds on FedAvg's round-based aggregation structure. The server-side reconstruction and averaging logic assumes familiarity with client sampling and model aggregation
  - Quick check question: In FedAvg with 10 sampled clients per round, how does the server produce θᵣ₊₁ from client models θᶜ?

- **Concept: Prompt-Based Fine-Tuning**
  - Why needed here: The paper notes ZO works effectively with prompt-based tuning, which constrains the optimization to a lower-dimensional space. Without this, ZO may struggle
  - Quick check question: How does prompt-based tuning differ from full fine-tuning in terms of trainable parameters?

## Architecture Onboarding

- **Component map**:
  - Client: Receives θᵣ, P₁, P₂, K → Runs K local steps → For each step: sample batch, run P₁ perturbations through f₁, reuse outputs for P₂ perturbations through f₂ → Compute scalar gradients g₁, g₂ → Store in G₁, G₂, S₁, S₂ → Upload G, S
  - Server: Broadcast θᵣ, P₁, P₂, K → Receive Gᶜ, Sᶜ from m clients → For each client: Reconstruct(θᵣ, K, Gᶜ, Sᶜ, μ, P₁, P₂) → Aggregate: θᵣ₊₁ = (1/m)Σθᶜ
  - Perturbation module: Reset RNG with seed s → Generate zᵢ ~ N(0,1) for each θᵢ → Apply in-place perturbation θᵢ ← θᵢ ± εzᵢ

- **Critical path**:
  1. Correct implementation of seed-based perturbation regeneration (must be byte-identical across client/server)
  2. Block split at the correct layer boundary (paper uses pre-LM-Head)
  3. Gradient accumulation across P₁ × P₂ combinations for g₁ (Eq. 5)
  4. Server reconstruction matching client update logic exactly

- **Design tradeoffs**:
  - P₁ vs P₂: Lower P₁ reduces computation but increases gradient noise for f₁; P₂=8 empirically chosen for RoBERTa-Large LM-Head
  - Block split position: Earlier split → larger f₁ activations to store; later split → more computation in f₂
  - Central vs forward difference: Central is more accurate but 2× forward passes per perturbation

- **Failure signatures**:
  - Accuracy plateaus below FedAvg by >5%: Check if perturbation scale ε is too large/small; verify prompt-based tuning is used
  - Server reconstruction diverges from client: RNG implementation mismatch; check seed handling, floating-point determinism
  - Memory exceeds expected: Verify activations are not being retained beyond the split layer; check that dropout is disabled during ZO steps
  - Communication cost unexpectedly high: Ensure seeds are not being sent if server-seeded variant is used; verify only scalars uploaded

- **First 3 experiments**:
  1. **Reproduce single-client ZO baseline**: Implement MeZO-style ZO-SGD on SST2 with RoBERTa-Large; verify memory matches inference-only (~1.4GB for context length 32) and accuracy within 2% of backprop
  2. **Validate split-perturbation in isolation**: Compare (P₁=2, P₂=8) vs (P=5 uniform) on same data; measure GFLOPs to reach target loss and confirm 1.6-3× reduction
  3. **End-to-end FL simulation**: 10 clients, 20% participation, 5 rounds; verify server reconstruction produces identical models to client-side; measure upload bytes (should be ~KB not MB/GB)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can FedSPZO be effectively adapted for inference-only accelerators utilizing lower numerical precision to bridge the remaining computational efficiency gap with backpropagation methods?
- **Basis in paper**: [explicit] The conclusion states: "For future work, we plan to explore zero-order on inference-only accelerators with lower precision"
- **Why unresolved**: The current evaluation relies on standard hardware and precision; the impact of quantization noise on the stability of zero-order gradient estimation in this split-block architecture is unknown
- **What evidence would resolve it**: Empirical results benchmarking FedSPZO on quantized models (e.g., INT8) or inference-optimized hardware showing sustained convergence and accuracy

### Open Question 2
- **Question**: How sensitive is the method's performance to the specific layer split point, and is the "head-only" split universally optimal for different model architectures?
- **Basis in paper**: [inferred] The paper fixes the split such that $f_2$ contains only the RoBERTaLM-Head but provides no ablation on how varying the size of $f_1$ versus $f_2$ affects the accuracy-computation trade-off
- **Why unresolved**: The optimal division of perturbations ($P_1, P_2$) likely depends on the relative size of the blocks, yet the methodology assumes a specific architectural cut
- **What evidence would resolve it**: An ablation study analyzing accuracy and FLOPs across various split ratios (e.g., 50/50, 75/25) and architectures

### Open Question 3
- **Question**: Does the computational reduction and convergence speed of FedSPZO scale effectively to multi-billion parameter LLMs beyond the RoBERTa-large (355M) model tested?
- **Basis in paper**: [inferred] Experiments are limited to RoBERTa-large; it remains unverified if the split-perturbation strategy mitigates the noise inherent in zero-order optimization as the parameter dimension ($d$) scales up dramatically
- **Why unresolved**: Zero-order optimization generally struggles with high dimensionality, and the benefits observed in a 355M parameter model may not translate to 7B+ models
- **What evidence would resolve it**: Evaluation of FedSPZO on contemporary large language models (e.g., Llama-2-7B) demonstrating sustained performance relative to baselines

## Limitations

- Learning rate (μ) and perturbation scale (ε) hyperparameters are unspecified, requiring empirical tuning that may affect accuracy and convergence speed
- Prompt templates for task alignment are referenced but not detailed, making exact reproduction of reported accuracy difficult
- Context length impacts memory usage at the split boundary, though the paper focuses on inference-level memory rather than activation storage concerns
- The effectiveness of split-perturbation depends on the LM-Head being a small module; models with larger final blocks may see reduced benefits

## Confidence

- **High confidence**: Memory reduction claims (inference-level memory, <0.5% overhead vs MeZO) - well-supported by ZO literature and the seed trick mechanism
- **Medium confidence**: Computation reduction (1.6-3× vs state-of-the-art) - based on GFLOP comparisons but dependent on unreported hyperparameter choices
- **Medium confidence**: Communication cost reduction (3 orders of magnitude) - theoretically sound but implementation details affect actual savings

## Next Checks

1. Reproduce single-client zero-order optimization baseline with RoBERTa-large on SST2 to verify memory matches inference-only (~1.4GB) and accuracy is within 2% of backpropagation
2. Implement split-perturbation strategy and compare (P₁=2, P₂=8) vs uniform P=5 on same data, measuring GFLOPs to target loss and confirming 1.6-3× reduction
3. Run end-to-end FL simulation with 10 clients, 20% participation, 5 rounds to verify server reconstruction produces identical models to client-side and measure upload bytes (should be ~KB not MB/GB)