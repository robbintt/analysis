---
ver: rpa2
title: Native LLM and MLLM Inference at Scale on Apple Silicon
arxiv_id: '2601.19139'
source_url: https://arxiv.org/abs/2601.19139
tags:
- cache
- apple
- caching
- inference
- silicon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces vllm-mlx, a framework for efficient LLM and
  MLLM inference on Apple Silicon. The framework leverages the unified memory architecture
  to provide higher throughput for text models (21-87% improvement over llama.cpp)
  and introduces content-based prefix caching to eliminate redundant vision encoding
  for multimodal models, achieving up to 28x speedup on repeated image queries.
---

# Native LLM and MLLM Inference at Scale on Apple Silicon

## Quick Facts
- arXiv ID: 2601.19139
- Source URL: https://arxiv.org/abs/2601.19139
- Authors: Wayner Barrios
- Reference count: 17
- Key outcome: vllm-mlx achieves 21-87% higher throughput for text models and 28x speedup for repeated image queries through unified memory architecture and content-based prefix caching

## Executive Summary
This paper introduces vllm-mlx, a framework enabling efficient LLM and MLLM inference on Apple Silicon through unified memory architecture optimization. The system achieves significant performance improvements over existing solutions like llama.cpp by leveraging Apple's MLX library and implementing continuous batching for throughput optimization. A novel content-based prefix caching mechanism eliminates redundant vision encoding for multimodal models, delivering up to 28x speedup on repeated image queries. The framework provides an OpenAI-compatible API for production deployment and scales effectively to 16 concurrent requests with 4.3x aggregate throughput improvement.

## Method Summary
The vllm-mlx framework implements continuous batching with K epochs to maximize throughput on Apple Silicon's unified memory architecture. For text models, it optimizes memory allocation and tensor operations using MLX's GPU kernels, achieving 21-87% throughput improvement over llama.cpp. For multimodal models, the framework introduces content-based prefix caching that computes visual embeddings once per unique image and reuses them for subsequent queries with the same content, eliminating redundant vision encoder computations. The system supports an OpenAI-compatible API interface and scales to multiple concurrent requests while maintaining performance benefits.

## Key Results
- 21-87% higher throughput for text models compared to llama.cpp across various configurations
- 28x speedup for multimodal models with repeated image queries using content-based prefix caching
- 4.3x aggregate throughput improvement at 16 concurrent requests through continuous batching

## Why This Works (Mechanism)
The unified memory architecture of Apple Silicon eliminates data transfer bottlenecks between CPU and GPU, enabling more efficient tensor operations. Continuous batching with K epochs allows the system to process multiple requests simultaneously while maintaining acceptable latency, maximizing hardware utilization. Content-based prefix caching leverages the observation that multimodal queries often reuse the same images, avoiding expensive vision encoder recomputation by storing and reusing visual embeddings.

## Foundational Learning

**Unified Memory Architecture**: Apple Silicon's shared memory pool between CPU and GPU that eliminates explicit data transfers. Why needed: Reduces memory copy overhead and enables seamless tensor operations across compute units. Quick check: Verify MLX operations don't require explicit memory transfers between CPU and GPU.

**Continuous Batching**: Processing multiple inference requests simultaneously in batches while maintaining acceptable per-request latency. Why needed: Maximizes hardware utilization and throughput on modern accelerators. Quick check: Monitor GPU utilization and latency distribution under varying batch sizes.

**Content-Based Prefix Caching**: Hashing visual content to detect duplicates and reuse precomputed embeddings instead of recomputing them. Why needed: Eliminates redundant computation in multimodal inference where images are often reused. Quick check: Measure cache hit rate and speedup factor with repeated queries.

## Architecture Onboarding

**Component Map**: Client API -> Request Queue -> Batching Engine -> MLX Backend -> Unified Memory -> GPU Execution -> Cache Store (for MLLM)

**Critical Path**: Request reception → Content hashing (MLLM) → Batch formation → MLX tensor operations → GPU execution → Response generation

**Design Tradeoffs**: The framework prioritizes throughput over single-stream latency, making it suitable for production APIs but potentially suboptimal for interactive applications requiring immediate responses.

**Failure Signatures**: Cache misses on visually similar but not identical images, batch formation delays under low request volume, memory pressure from large batch sizes.

**First Experiments**: 1) Benchmark single-model inference latency vs. llama.cpp baseline, 2) Test cache hit rate with repeated image queries, 3) Measure throughput scaling with concurrent request count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can speculative decoding be effectively implemented on Apple Silicon to improve single-stream latency?
- Basis in paper: [explicit] Under "Limitations and Future Work," the authors state: "Future directions include speculative decoding for improved single-stream latency."
- Why unresolved: The paper focuses on continuous batching for throughput optimization but does not implement speculative decoding, which targets a different objective (latency vs. throughput).
- What evidence would resolve it: Implementation and benchmarking of speculative decoding on Apple Silicon, measuring latency improvements against the baseline single-stream performance.

### Open Question 2
- Question: What throughput and latency characteristics can be achieved through distributed inference across network-connected Mac clusters?
- Basis in paper: [explicit] The authors list "distributed inference across multiple Apple Silicon devices via network-connected Mac clusters" as a future direction.
- Why unresolved: The current implementation is limited to single-device inference; network overhead, memory coherence, and load balancing for distributed Apple Silicon inference remain unexplored.
- What evidence would resolve it: Implementation of model or pipeline parallelism across multiple Macs with benchmarks measuring scaling efficiency under different network configurations.

### Open Question 3
- Question: Can the content-based prefix caching approach be extended to audio modalities with comparable speedup factors?
- Basis in paper: [explicit] The authors state they "plan to extend our caching approach to audio modalities for speech-enabled multimodal applications."
- Why unresolved: Audio encoders have different computational characteristics than vision encoders; whether audio embeddings benefit similarly from caching given different temporal structures is unknown.
- What evidence would resolve it: Implementation of audio content hashing and embedding caching with benchmarks across repeated audio queries.

### Open Question 4
- Question: What energy consumption trade-offs exist under different batching configurations on battery-powered Apple Silicon devices?
- Basis in paper: [explicit] The authors mention "energy profiling for battery-powered deployments" as a future direction.
- Why unresolved: The paper benchmarks only throughput and latency on M4 Max without measuring power draw, which is critical for laptop or mobile deployments.
- What evidence would resolve it: Power consumption measurements under various concurrency levels, batch sizes, and model configurations on battery-powered devices.

## Limitations

- Performance comparisons limited to single hardware configuration (M2 Ultra with 24 CPU threads and 64GB RAM)
- No comprehensive error analysis for content-based prefix caching edge cases
- Evaluation focuses on throughput metrics without latency distribution or power consumption analysis

## Confidence

- 21-87% throughput improvement over llama.cpp: High
- 28x speedup for content-based prefix caching: High
- 4.3x aggregate throughput at 16 concurrent requests: Medium

## Next Checks

1. Evaluate performance across multiple Apple Silicon configurations (M1, M2, M3 series with varying core counts and memory configurations) to establish generalization.
2. Conduct comprehensive latency percentile analysis (p50, p95, p99) under different concurrency levels to understand tail behavior.
3. Implement ablation studies to quantify the individual contributions of continuous batching versus content-based caching to overall performance improvements.