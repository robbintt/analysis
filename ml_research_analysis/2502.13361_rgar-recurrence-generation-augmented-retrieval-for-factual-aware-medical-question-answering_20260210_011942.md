---
ver: rpa2
title: 'RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical
  Question Answering'
arxiv_id: '2502.13361'
source_url: https://arxiv.org/abs/2502.13361
tags:
- knowledge
- retrieval
- rgar
- medical
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RGAR, a retrieval-augmented generation framework
  that improves medical question answering by jointly retrieving factual knowledge
  from Electronic Health Records (EHRs) and conceptual knowledge from medical corpora.
  RGAR uses a recurrent pipeline to iteratively refine queries through dual-end retrieval
  and factual knowledge extraction, enabling interaction between both knowledge types.
---

# RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering

## Quick Facts
- arXiv ID: 2502.13361
- Source URL: https://arxiv.org/abs/2502.13361
- Reference count: 40
- RGAR achieves state-of-the-art performance on three factual-aware medical benchmarks

## Executive Summary
This paper introduces RGAR, a retrieval-augmented generation framework that improves medical question answering by jointly retrieving factual knowledge from Electronic Health Records (EHRs) and conceptual knowledge from medical corpora. RGAR uses a recurrent pipeline to iteratively refine queries through dual-end retrieval and factual knowledge extraction, enabling interaction between both knowledge types. The framework demonstrates superior performance on medical QA benchmarks, with the Llama-3.1-8B-Instruct model outperforming larger models when enhanced with RAG.

## Method Summary
RGAR employs a dual-knowledge retrieval strategy that combines factual knowledge from Electronic Health Records with conceptual knowledge from medical corpora. The framework implements a recurrent pipeline that iteratively refines queries through dual-end retrieval and factual knowledge extraction. This approach enables interaction between both knowledge types, improving the overall generation quality for medical question answering tasks. The system is evaluated on three factual-aware medical benchmarks, demonstrating state-of-the-art performance.

## Key Results
- Achieves state-of-the-art performance on three factual-aware medical benchmarks
- Llama-3.1-8B-Instruct model outperforms GPT-3.5 with RAG enhancement
- Demonstrates effectiveness of extracting and leveraging factual knowledge from EHRs, particularly in scenarios with extensive EHR data

## Why This Works (Mechanism)
RGAR works by combining factual knowledge from EHRs with conceptual knowledge from medical corpora through an iterative refinement process. The dual-end retrieval system allows the model to access both structured patient data and broader medical knowledge, while the recurrent pipeline enables continuous query refinement. This approach addresses the challenge of factual accuracy in medical QA by grounding responses in verifiable patient information while maintaining access to comprehensive medical knowledge.

## Foundational Learning
- Electronic Health Records (EHRs): Structured patient data containing medical histories, diagnoses, and treatments. Needed to provide factual grounding for medical responses. Quick check: Verify EHR data includes patient identifiers, timestamps, and clinical codes.
- Medical Corpora: Collections of medical literature and documentation. Provides conceptual knowledge beyond individual patient cases. Quick check: Ensure corpus covers relevant medical specialties and recent publications.
- Retrieval-Augmented Generation (RAG): Combines information retrieval with language model generation. Enables access to external knowledge during response generation. Quick check: Validate retrieval accuracy and relevance to input queries.

## Architecture Onboarding

**Component Map:**
EHR Retriever -> Conceptual Retriever -> Factual Extractor -> Query Refiner -> Generator -> Response

**Critical Path:**
Query → EHR Retriever → Conceptual Retriever → Factual Extractor → Query Refiner → Generator → Response

**Design Tradeoffs:**
- Dual knowledge sources vs. single source: Trade complexity for improved factual accuracy
- Iterative refinement vs. single-pass retrieval: Trade latency for quality improvements
- EHR-specific vs. general medical knowledge: Trade scope for patient-specific relevance

**Failure Signatures:**
- Inaccurate EHR retrieval leading to incorrect patient-specific information
- Conceptual retriever missing relevant medical literature
- Factual extractor failing to identify key information from retrieved documents
- Query refiner getting stuck in local optima during iteration

**First Experiments:**
1. Test EHR retrieval accuracy on sample patient records with known outcomes
2. Validate conceptual retrieval performance on medical literature queries
3. Measure factual extraction accuracy from combined EHR and corpus results

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on factual accuracy and generation quality but lacks extensive comparison against other RAG methods beyond GPT-3.5
- Methodology for identifying and validating "factual" elements from EHRs is not fully detailed
- Limited discussion of data diversity and potential biases in EHR sources

## Confidence
- Claim: State-of-the-art performance on medical benchmarks - Medium
- Claim: Llama-3.1-8B-Instruct outperforms GPT-3.5 with RAG - Medium
- Claim: Effectiveness in extensive EHR scenarios - Low

## Next Checks
1. Conduct ablation studies to isolate the contribution of factual knowledge extraction versus other components of the RGAR framework
2. Test RGAR on additional medical QA benchmarks with varying EHR data volumes and types to validate performance claims in diverse scenarios
3. Perform human evaluation studies to assess the clinical relevance and accuracy of retrieved factual knowledge from EHRs