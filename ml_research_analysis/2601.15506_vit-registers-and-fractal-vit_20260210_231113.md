---
ver: rpa2
title: ViT Registers and Fractal ViT
arxiv_id: '2601.15506'
source_url: https://arxiv.org/abs/2601.15506
tags:
- tokens
- positional
- mask
- summary
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether attention masks can replace positional
  encodings in vision transformers by introducing "fractal ViT," which adds summary
  tokens and applies attention masks between regular and summary tokens. Experiments
  on ImageNet-1k with ViT-S/16 show that positional encoding of additional tokens
  is inconsequential and fractal masks do not improve performance.
---

# ViT Registers and Fractal ViT

## Quick Facts
- arXiv ID: 2601.15506
- Source URL: https://arxiv.org/abs/2601.15506
- Reference count: 23
- Key outcome: Experiments show fractal masks and summary tokens provide no benefit for image recognition tasks; positional encoding of additional tokens is inconsequential

## Executive Summary
This paper investigates whether attention masks can replace positional encodings in vision transformers by introducing "fractal ViT," which adds summary tokens and applies attention masks between regular and summary tokens. Experiments on ImageNet-1k with ViT-S/16 show that positional encoding of additional tokens is inconsequential and fractal masks do not improve performance. Models with 4-59 additional tokens show only marginal improvements (1-2 standard deviations) over baseline. The findings suggest that fractal masks and summary tokens are unhelpful for image recognition, unlike in language models, highlighting domain-specific limitations of these approaches.

## Method Summary
The paper tests whether attention masks between regular tokens and summary tokens can break permutation invariance without explicit positional encodings. The fractal ViT architecture adds summary tokens (smaller grids) to regular tokens and applies a hierarchical attention mask: regular tokens attend to all regular tokens and their assigned summary token; summary tokens attend to their assigned regular tokens; CLS token attends to all. The study compares ViT-S/16 with various positional encodings (sincos2d, learned, 2D-ALiBi, none) and different numbers of additional tokens (0, 4, 9, 17, 59), evaluating top-1 accuracy on ImageNet-1k after 90 epochs.

## Key Results
- Positional encoding of additional tokens is inconsequential for performance
- Fractal masks do not improve performance over baseline ViT with registers
- 4-59 additional tokens provide only marginal improvements (1-2 standard deviations) over baseline
- sincos2d positional encoding outperforms learned, 2D-ALiBi, and none for this task

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Attention Masking for Position Encoding
- Claim: Structured attention masks between summary tokens and regular tokens can theoretically break permutation invariance without explicit positional encodings.
- Mechanism: Assign k×k regular tokens to each summary token in a grid pattern. Summary tokens attend only to their assigned region; regular tokens attend back to their corresponding summary token. This creates a self-similar "fractal" structure that encodes spatial relationships through attention topology rather than additive position embeddings.
- Core assumption: Implicit positional information can be recovered from attention mask structure, analogous to how causal masks enable NoPE in language models.
- Evidence anchors: Abstract states fractal ViT breaks permutation invariance through attention masks; section 3 describes the mask construction.
- Break condition: Experiments show this does NOT improve performance over baseline ViT with registers.

### Mechanism 2: Register Tokens as Representation Buffers
- Claim: Additional tokens not tied to input can absorb outlier activations and improve training dynamics.
- Mechanism: Append learnable "register" or "summary" tokens to the token sequence. These tokens participate in attention but do not contribute to the output. They may function as attention sinks or representational scratchpads.
- Core assumption: High-norm outlier tokens that emerge during ViT training interfere with learned representations; additional buffer tokens can mitigate this.
- Evidence anchors: Section 2.2 references work on registers eliminating high-norm outlier token artifacts.
- Break condition: At ViT-S/16 scale on ImageNet-1k, adding 4-59 register/summary tokens yields only 1-2 standard deviation improvements over baseline.

### Mechanism 3: Positional Encoding Transfer from Language to Vision
- Claim: Positional encoding strategies successful in language models (NoPE, ALiBi, learned embeddings) have domain-specific effectiveness profiles.
- Mechanism: Language benefits from causal structure and sequential token relationships. Vision has 2D spatial structure with different symmetry properties (horizontal flip vs. full rotation invariance). Positional encoding must respect underlying data symmetries.
- Core assumption: Mechanisms explaining NoPE success in LMs should transfer to vision with appropriate mask design.
- Evidence anchors: Section 5 discusses why 2D-ALiBi works for satellite imagery but not ImageNet due to different symmetry requirements.
- Break condition: Direct transfer fails—2D-ALiBi underperforms sincos2d on ImageNet, and fractal masks provide no benefit.

## Foundational Learning

- Concept: **Permutation Invariance in Transformers**
  - Why needed here: Self-attention is permutation-invariant by default—swapping any two tokens in the sequence produces the same output. Positional encoding or attention masks must break this symmetry to encode spatial/sequential structure.
  - Quick check question: If you shuffle the patch tokens of a ViT input, would the model produce the same attention weights before positional encoding is added?

- Concept: **Attention Sinks and Outlier Tokens**
  - Why needed here: The paper builds on work showing high-norm tokens emerge in large ViTs and that registers can mitigate artifacts. Understanding why attention mass concentrates on certain tokens clarifies when/why additional tokens might help.
  - Quick check question: In StreamingLLM, why must the initial token's KV be retained even when it has no semantic content?

- Concept: **E(2) vs D4 Symmetry Groups**
  - Why needed here: The paper discusses why 2D-ALiBi works for satellite imagery but not ImageNet. E(2) equivariance (full translation + rotation) vs D4 (dihedral group of square symmetries) determines what positional encodings are appropriate.
  - Quick check question: Why might an architecture respecting E(2) symmetry be preferable to 2D-ALiBi for satellite imagery?

## Architecture Onboarding

- Component map: Input Image → Patch Embedding → [Regular Tokens (n×n)] + [Summary Tokens (n/k × n/k)] + [CLS Token] → Positional Encoding → Transformer Encoder with Optional Fractal Mask → [CLS Token] → Classification Head

- Critical path: Patch embedding → Token concatenation with registers/summary → Positional encoding addition → Attention computation (with optional mask) → CLS token extraction → Classification

- Design tradeoffs:
  - sincos2d positional encoding outperforms learned, 2D-ALiBi, and none for this task
  - Adding 4-59 additional tokens provides marginal gains (0.15-0.2% accuracy) vs architectural overhead
  - Fractal mask adds complexity without performance benefit
  - Summary tokens with fractal masks vs simple registers: no meaningful difference

- Failure signatures:
  - Model without positional encoding: 72.93% accuracy (vs 77.68% with sincos2d)—severe degradation
  - Fractal mask on regular tokens destroys performance (preliminary experiments mentioned in intro)
  - 2D-ALiBi underperforms on ImageNet despite success in CROMA for satellite imagery

- First 3 experiments:
  1. **Baseline replication**: Train ViT-S/16 with sincos2d positional encoding, 0 additional tokens, 224 resolution for 90 epochs. Verify ~76.9% top-1 accuracy.
  2. **Ablate positional encoding variants**: Compare sincos2d vs learned vs 2D-ALiBi vs none with 17 register tokens. Expect sincos2d > learned ≈ 2D-ALiBi >> none.
  3. **Token count sensitivity**: Add 4, 9, 17, and 59 register tokens to baseline. Measure whether accuracy gains (if any) scale with token count or plateau. Expect marginal 1-2 std dev improvements regardless of count.

## Open Questions the Paper Calls Out

- Question: Do fractal masks and summary tokens improve the performance of decoder-based or masked encoder language models?
  - Basis in paper: The authors suggest it may be worth bringing the idea of fractal masks and summary tokens back to the domain of language models to see if they help decoder models or masked encoder models.
  - Why unresolved: This study only evaluated the architecture on Vision Transformers (ViT) for image recognition, where it provided no benefit. It is unknown if the "attention sink" phenomenon in LLMs would interact more favorably with this structure.
  - What evidence would resolve it: Training transformer-based language models with the proposed fractal mask and summary token setup and comparing the perplexity or downstream task performance against standard baselines.

- Question: Are fractal patterns more useful for image generation tasks than for perception tasks?
  - Basis in paper: The conclusion hypothesizes that it is also possible that fractal pattern is more useful in generation than in perception, noting recent success in fractal generative models.
  - Why unresolved: The experiments were restricted to ImageNet-1k recognition (perception), where the method failed to improve upon the baseline.
  - What evidence would resolve it: Integrating the fractal attention mask and summary tokens into generative vision architectures (e.g., diffusion transformers) to evaluate if they improve sample quality or training efficiency.

- Question: Why does 2D-ALiBi positional encoding benefit satellite imagery models (like CROMA) but fail to improve standard ImageNet ViT baselines?
  - Basis in paper: The authors note that while CROMA shows benefits for 2D-ALiBi, the question on what makes the difference remains open given that their experiments on ImageNet-1k found sincos2d superior.
  - Why unresolved: The paper suggests potential causes like rotation invariance or contrastive objectives but provides no definitive evidence for the performance discrepancy.
  - What evidence would resolve it: Ablation studies testing 2D-ALiBi on ImageNet with contrastive learning objectives versus standard cross-entropy loss, or testing on datasets with higher rotational symmetry.

## Limitations

- Experimental scope limited to ViT-S/16 on ImageNet-1k, making generalization to larger models or different vision tasks uncertain
- Limited exploration of alternative mask designs, token counts, or initialization strategies means beneficial configurations may exist
- No investigation of whether fractal patterns might be more effective for image generation rather than perception tasks

## Confidence

- **High Confidence**: The core finding that fractal masks do not improve performance over baseline ViT with registers
- **Medium Confidence**: The conclusion that summary tokens with fractal masks are unhelpful for image recognition
- **Low Confidence**: The broader implication that fractal masks and summary tokens are fundamentally unsuitable for vision tasks

## Next Checks

1. **Scale Sensitivity Analysis**: Replicate experiments with larger ViT variants (ViT-B, ViT-L) to determine if the ineffectiveness of fractal masks and summary tokens persists across model scales, or if larger models show different sensitivity to these architectural modifications.

2. **Cross-Domain Generalization**: Test the ViT-S/16 architecture with fractal masks and summary tokens on datasets with different properties (e.g., satellite imagery where 2D-ALiBi succeeds, medical imaging with different symmetry requirements) to validate the domain-specific claims about positional encoding effectiveness.

3. **Alternative Mask Designs**: Systematically explore variations in fractal mask construction—different grid patterns, asymmetric attention flows, or hierarchical structures—to determine if the specific mask design in the paper is suboptimal rather than the concept itself being flawed.