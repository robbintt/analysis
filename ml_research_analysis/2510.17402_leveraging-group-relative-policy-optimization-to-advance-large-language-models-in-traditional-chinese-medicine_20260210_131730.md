---
ver: rpa2
title: Leveraging Group Relative Policy Optimization to Advance Large Language Models
  in Traditional Chinese Medicine
arxiv_id: '2510.17402'
source_url: https://arxiv.org/abs/2510.17402
tags:
- reasoning
- arxiv
- https
- grpo
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2510.17402
- Source URL: https://arxiv.org/abs/2510.17402
- Reference count: 0
- Primary result: Ladder-Score 0.803, Exact Match Accuracy 0.8623

## Executive Summary
This paper presents Ladder-base, a TCM-focused LLM fine-tuned with Group Relative Policy Optimization (GRPO) that achieves state-of-the-art performance on the TCM-Ladder benchmark. The model demonstrates strong diagnostic reasoning and factual consistency in Traditional Chinese Medicine contexts, outperforming existing models through verifiable reward signals and group-relative advantage estimation without requiring a learned value network.

## Method Summary
The authors fine-tuned Qwen2.5-7B-Instruct using GRPO with verifiable outcome rewards (correctness, formatting, tagging weighted 5:1:1) on the TCM-Ladder textual dataset (~52K entries, 21,326 QA pairs, 25,163 dialogues). Training used group size G=6, temperature 0.7, top-p 0.8, ε=0.2, KL penalty β=0.01, for 2 epochs on 2× NVIDIA A100 PCIe (80GB). The GRPO framework replaces learned reward models with ground-truth verification and normalizes advantages within sampled response groups.

## Key Results
- Ladder-Score: 0.803 (state-of-the-art on TCM-Ladder benchmark)
- Exact Match Accuracy: 0.8623
- Evaluated on 6 metrics: BLEU-4, ROUGE-L, METEOR, BERTScore, Ladder-Score, Exact Match Accuracy
- Trained exclusively on textual subset of TCM-Ladder dataset

## Why This Works (Mechanism)

### Mechanism 1
Group-relative advantage estimation provides stable optimization without requiring a learned value network. The policy samples G responses per prompt and computes advantages by normalizing rewards within the group (advantage = [reward − mean(rewards)] / std(rewards)). This converts absolute rewards into relative rankings, reducing variance from noisy scalar feedback. Break condition: If all sampled responses receive identical rewards (zero variance), advantage estimates become undefined or uninformative.

### Mechanism 2
Verifiable outcome rewards with weighted components reduce reward hacking compared to learned reward models. Rather than training a separate reward model, the system uses ground-truth verification: R = 1 if prediction matches ground truth, 0 otherwise. Components are weighted 5:1:1 (correctness:formatting:tagging) to prioritize answer accuracy while maintaining structure. Break condition: If tasks require subjective judgment without objective verification, the binary reward signal becomes inadequate or misleading.

### Mechanism 3
KL divergence constraints prevent catastrophic policy drift while allowing meaningful domain adaptation. The objective combines a clipped policy ratio with a KL penalty term (β = 0.01), constraining how far the fine-tuned policy can deviate from the reference model (Qwen2.5-7B-Instruct) while still optimizing for TCM-specific rewards. Break condition: If β is too high, the model cannot adapt; if too low, it may forget general capabilities or overfit to spurious reward signals.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: GRPO is explicitly framed as a PPO variant; understanding the clipping mechanism, policy ratio, and advantage estimation is prerequisite to grasping what GRPO modifies. Quick check: Can you explain why PPO clips the policy ratio and what problem this solves compared to vanilla policy gradient?

- **Reward hacking and reward model overoptimization**: The paper motivates GRPO's design by contrasting with learned reward models that can be gamed; understanding this failure mode clarifies why verifiable rewards are preferred. Quick check: What happens when a policy optimizes a proxy reward function that imperfectly correlates with the true objective?

- **KL divergence as a regularization constraint**: The β-weighted KL term is central to the GRPO objective; understanding how it trades off adaptation vs. preservation is critical for tuning. Quick check: If you observe training instability and rapid capability loss, which hyperparameter should you adjust first and in which direction?

## Architecture Onboarding

- Component map: Qwen2.5-7B-Instruct (reference and initialization) -> Policy model (fine-tuned copy) -> Reward computation (ground-truth verification) -> GRPO optimizer (TRL framework)
- Critical path: 1) Sample G responses per prompt from current policy (G=6) 2) Compute rewards for each response via verification 3) Normalize rewards within group to get advantages 4) Compute clipped loss with KL penalty 5) Backpropagate and update policy
- Design tradeoffs: Group size (G=6) - larger groups give better advantage estimates but increase compute; KL weight (β=0.01) - too restrictive limits domain adaptation, too permissive risks forgetting; Temperature (0.7) and top-p (0.8) - control response diversity during training, affect group variance
- Failure signatures: Zero variance in group rewards → undefined advantages; Rapid validation performance drop → KL constraint too weak or reward overfitting; High formatting/tagging scores but low correctness → reward hacking on proxy components
- First 3 experiments: 1) Group size ablation: Train with G ∈ {2, 4, 6, 8} on held-out subset; plot advantage variance and final Ladder-Score 2) KL weight sweep: Run β ∈ {0.001, 0.005, 0.01, 0.05}; monitor reference model KL divergence during training and downstream general capability vs. TCM task performance 3) Reward component ablation: Test alternative weightings (e.g., 3:1:1, 7:1:1) against 5:1:1 baseline

## Open Questions the Paper Calls Out

- **Multimodal extension**: Can the GRPO-based fine-tuning framework be effectively extended to integrate multimodal inputs such as tongue images, pulse data, and herb visuals? Current results are from textual-only training; resolution requires successful training and evaluation on visual and sensory components.

- **Clinical safety validation**: Does the model's performance translate to safety and reliability in real-world clinical environments over time? Current results are from standardized benchmarks; resolution requires longitudinal studies measuring safety incidents and expert agreement rates in clinical settings.

- **Ambiguity handling**: Does reliance on strict correctness-based reward function constrain the model's ability to handle inherent ambiguity of valid TCM diagnoses? The paper acknowledges TCM involves "ambiguous or multi-label decisions" yet uses binary reward signal; resolution requires expert review of high-probability "incorrect" responses for valid alternative reasoning.

## Limitations
- Unknown system prompt text and prompt formatting details
- TCM-Ladder benchmark access and license unclear; reward parser implementation details unknown
- Ladder-Score computation method not defined in paper

## Confidence
- Method reproducibility: Medium - Key hyperparameters specified but data access and prompt details unclear
- Claims validation: High - Results are from benchmark evaluation with multiple metrics
- Safety assessment: Low - No clinical validation or safety studies reported

## Next Checks
1. Verify TCM-Ladder benchmark data access and confirm 80/10/10 train/val/test split with exact preprocessing
2. Implement GRPO training with group size ablation (G=2,4,6,8) to validate advantage variance and Ladder-Score relationship
3. Conduct KL weight sweep (β=0.001, 0.005, 0.01, 0.05) monitoring both domain performance and general capability preservation