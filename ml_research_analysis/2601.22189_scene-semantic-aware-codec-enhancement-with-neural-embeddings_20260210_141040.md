---
ver: rpa2
title: 'SCENE: Semantic-aware Codec Enhancement with Neural Embeddings'
arxiv_id: '2601.22189'
source_url: https://arxiv.org/abs/2601.22189
tags:
- scene
- vmaf
- assembled
- video
- codec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of compression artifacts degrading
  perceptual quality in standard video codecs like H.264 and H.265. It proposes SCENE,
  a lightweight semantic-aware pre-processing framework that enhances perceptual fidelity
  by integrating semantic embeddings from a vision-language model (SigLIP 2) into
  an efficient convolutional architecture.
---

# SCENE: Semantic-aware Codec Enhancement with Neural Embeddings

## Quick Facts
- **arXiv ID:** 2601.22189
- **Source URL:** https://arxiv.org/abs/2601.22189
- **Reference count:** 0
- **Primary result:** Lightweight semantic-aware pre-processing framework that improves perceptual quality of compressed video streams via VLM embeddings and assembled convolutions

## Executive Summary
SCENE addresses compression artifact degradation in standard video codecs by introducing a semantic-aware pre-processing framework. The approach integrates vision-language model embeddings with an efficient convolutional architecture to enhance perceptual fidelity before encoding. Trained with a differentiable codec proxy, SCENE learns to mitigate artifacts from various codecs without modifying existing pipelines. The model operates in real-time at 36 fps for 1080p content and demonstrates improved objective and perceptual metrics on high-resolution benchmarks.

## Method Summary
SCENE is a lightweight pre-processing framework that enhances perceptual fidelity of compressed video streams. It combines SigLIP 2 vision-language embeddings with assembled convolutions to create a semantic-aware enhancement model. The architecture uses frozen SigLIP 2 embeddings to modulate the first of two assembled blocks, enabling content-aware filtering. Training employs a differentiable JPEG proxy to simulate compression artifacts while omitting motion compensation, allowing end-to-end optimization. During inference, the proxy is discarded and SCENE operates as a standalone pre-processor, maintaining real-time performance at 36 fps for 1080p content.

## Key Results
- Achieves 3.9% BD-rate reduction over AsConvSR baseline in VMAF when enhancing H.264/H.265 compressed video
- Improves VMAF scores while maintaining MS-SSIM performance on UVG 1080p test set
- Operates at ~36 fps for 1080p content with only 1.4M trainable parameters
- Demonstrates semantic guidance yields additional rate-quality gains in ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Semantic Prior Injection via VLM Embeddings
SCENE conditions enhancement on semantic embeddings to improve perceptual quality in salient regions without degrading structural fidelity. SigLIP 2 So400M extracts 1,152-dimensional embeddings encoding both high-level concepts and fine-grained spatial structures. A control module transforms these into channel-specific convolution coefficients via two 1×1 convolutions with ReLU, which modulate assembled convolution kernels to bias representational capacity toward perceptually meaningful structures.

### Mechanism 2: Channel-wise Assembled Convolution Adaptivity
The model independently computes coefficients per output channel, providing finer-grained content adaptation than global dynamic convolution. Each output channel c constructs its kernel K_c by linearly combining E=4 base kernels using channel-specific coefficients: K_c = Σ Coeff_c,i · k_i. Two assembled blocks operate sequentially—the first modulated by VLM features, the second by learned convolutional features—enabling semantic-to-spatial refinement.

### Mechanism 3: Proxy-Mediated Compression Awareness
A differentiable JPEG proxy enables the model to learn compression-robust enhancements that generalize to H.264/H.265. The proxy captures block-based transform–quantization distortions common to standard codecs while omitting motion-compensated prediction. Enhanced frames pass through this proxy before loss computation, exposing the model to compression artifacts during optimization.

## Foundational Learning

- **Vision-Language Model (VLM) Embeddings:** Understanding what semantic information SigLIP 2 encodes—and what it doesn't—is critical for diagnosing modulation failures. SigLIP 2 differs from CLIP by optimizing for dense predictions via decoder-based losses. *Quick check:* Given an input frame, can you predict whether the VLM embedding will prioritize faces over background foliage? What evidence supports your prediction?

- **Differentiable Proxies for Non-Differentiable Operations:** The JPEG proxy enables gradients to flow through simulated compression. Understanding proxy fidelity vs. real codec behavior determines when this approach will or won't work. *Quick check:* Why might a model trained with a JPEG proxy fail to generalize to AV1's in-loop filtering tools?

- **Rate-Distortion Tradeoffs and BD-Rate:** The paper reports BD-rate improvements in VMAF but slight MS-SSIM degradation. Understanding this tradeoff is essential for interpreting whether SCENE is "better" for a given deployment. *Quick check:* If MS-SSIM improves but VMAF degrades, what does this imply about the nature of the enhancement? When would this be acceptable?

## Architecture Onboarding

- **Component map:** Input Frame → Pixel Unshuffle (N=2) → Conv 3×3 → Assembled Block 1 ← SigLIP 2 → Control Module → Coefficients → Assembled Block 2 ← (coefficients from Block 1 features) → Conv 3×3 → Pixel Shuffle → Enhanced Frame

- **Critical path:** The semantic modulation path (SigLIP 2 → Control Module → Assembled Block 1 coefficients) is the primary novelty. If this path is ablated, SCENE reduces to AsConvSR baseline.

- **Design tradeoffs:** Proxy complexity favors faster training but may not capture motion-compensated artifacts in H.264/H.265 inter-frames. Only Block 1 uses VLM features; Block 2 uses learned features, limiting semantic influence. Perceptual losses improved VMAF scores but slightly degraded MS-SSIM; deployment context determines which matters.

- **Failure signatures:** If enhancement introduces unnatural sharpening, check VMAF_NEG metric and increase λ₁ weight for pre-proxy L1 loss. If semantic regions (faces, text) still degrade, verify SigLIP 2 is frozen and embeddings are correctly extracted. If bitrate increases substantially after enhancement, the model may be adding high-frequency content; increase λ_b (bitrate loss) weight.

- **First 3 experiments:**
  1. Ablate semantic path: Replace SigLIP 2 coefficients with random noise or zeros. Compare VMAF/MS-SSIM to isolate semantic contribution (paper shows ~3.9–5.8% BD-rate gap).
  2. Proxy fidelity test: Train with JPEG proxy, evaluate on actual H.264/H.265 compressed outputs at multiple CRF values. Plot proxy-to-real quality correlation to assess generalization.
  3. Coefficient visualization: Visualize assembled kernel coefficients for frames with clear foreground/background separation. Confirm higher coefficients in salient regions; if uniform, control module may not be learning meaningful modulation.

## Open Questions the Paper Calls Out

- **How can temporal modeling be integrated into the SCENE framework to leverage inter-frame correlations for artifact mitigation?** [explicit] The conclusion states that future work will "incorporate temporal modeling." The current architecture processes frames using spatial feature extraction without explicit mechanisms to utilize redundancy or motion information across video frames.

- **To what extent does the omission of motion-compensated prediction in the differentiable JPEG proxy limit the model's effectiveness against temporal codec artifacts?** [inferred] Section 2.3 acknowledges the proxy "omit[s] motion-compensated prediction," creating a potential gap between the training distortion model and real-world codec behavior.

- **Does extending VLM-based semantic conditioning to the second assembled block yield diminishing returns or significant improvements over the current single-block design?** [explicit] The authors note that "only the first assembled block was conditioned on VLM-derived semantic features," with future work aiming to "strengthen semantic conditioning."

## Limitations

- The JPEG proxy omits motion-compensated prediction, potentially limiting generalization to real codec artifacts involving temporal prediction and in-loop filtering.
- VMAF improvements come with slight MS-SSIM degradation, suggesting potential tradeoffs between perceptual preference and structural fidelity.
- AV1 compatibility shows bitrate increases that render standard BD-rate comparisons undefined, indicating potential conflicts with codec-specific psychovisual optimizations.

## Confidence

- **High confidence:** Assembled convolution architecture is well-specified and reproducible with clear equations and architecture diagrams.
- **Medium confidence:** Semantic modulation mechanism's effectiveness is demonstrated through ablation studies, but causal relationship between embedding content and enhancement quality remains underexplored.
- **Low confidence:** Generalization across diverse content types and compression scenarios beyond the UVG 1080p test set.

## Next Checks

1. **Proxy fidelity assessment:** Train the model with the JPEG proxy and evaluate on actual H.264/H.265 compressed outputs at multiple CRF values. Plot the correlation between proxy-simulated quality and real codec quality to quantify generalization gaps.

2. **Semantic coefficient analysis:** Visualize assembled kernel coefficients for frames with clear semantic structure (faces, text, natural scenes). Verify whether higher coefficients consistently align with salient regions and whether coefficient patterns correlate with enhancement quality.

3. **Cross-content robustness test:** Evaluate SCENE on diverse datasets beyond UVG (e.g., MCL-JCV, KoNViD-1k) and at multiple resolutions (480p, 720p, 1080p). Measure whether semantic gains persist across content types or degrade in low-texture regions.