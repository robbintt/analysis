---
ver: rpa2
title: Are generative AI text annotations systematically biased?
arxiv_id: '2512.08404'
source_url: https://arxiv.org/abs/2512.08404
tags:
- annotations
- gllm
- manual
- bias
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines potential bias in generative AI text annotations
  by comparing multiple GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) using
  different prompts against manual annotations of YouTube comments. The study finds
  that while GLLMs achieve reasonable F1 scores (0.45-0.73), they systematically differ
  from manual annotations in prevalence and downstream correlations.
---

# Are generative AI text annotations systematically biased?

## Quick Facts
- arXiv ID: 2512.08404
- Source URL: https://arxiv.org/abs/2512.08404
- Reference count: 5
- Primary result: GLLMs systematically differ from manual annotations, with better F1 scores correlating with greater bias

## Executive Summary
This paper investigates potential bias in generative AI text annotations by comparing multiple GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) against manual annotations of YouTube comments. The study finds that while GLLMs achieve reasonable F1 scores (0.45-0.73), they systematically differ from manual annotations in prevalence and downstream correlations. The key finding is that GLLMs show more agreement with each other than with manual annotations, suggesting systematic bias rather than random variation. Notably, better F1 scores correlate with greater bias, meaning selecting top-performing models could amplify rather than reduce differences from manual results.

## Method Summary
The study uses 2459 YouTube comments manually annotated for 5 concepts (political content, interactivity, rationality, incivility, ideology). Four GLLMs are tested with 5 different prompts each (Boukes, Simpa1, Para1, Para2, Jaidka), creating 20 annotator configurations. The evaluation compares F1/accuracy, prevalence, and downstream correlations with genre against manual annotations. A simulation framework tests 4 scenarios with 20 simulated annotators at 85% accuracy to distinguish random from systematic bias.

## Key Results
- GLLMs achieve F1 scores of 0.45-0.73 but systematically differ from manual annotations in prevalence
- GLLMs show more agreement with each other than with manual annotations, indicating systematic bias
- Better F1 scores correlate with greater bias, making model selection on traditional metrics potentially harmful

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GLLMs exhibit systematic rather than random annotation bias due to shared representational assumptions across models.
- Mechanism: Models converge on similar latent interpretations of classification boundaries, causing correlated errors that diverge consistently from human judgment. The paper demonstrates this through simulation: random 15% noise models show faster agreement curves than observed GLLM behavior, while systematic bias simulations matching modal GLLM annotations best fit the data.
- Core assumption: Shared training paradigms and architectural similarities cause models to develop aligned conceptual boundaries that differ from human operationalization.
- Evidence anchors:
  - [abstract] "display systematic bias in that they overlap more with each other than with manual annotations"
  - [section] Figure 3 analysis (pages 5-7): "GLLMs agree more than random chance would expect... while they also disagree more with the manual annotations than random chance would predict"
- Break condition: If models from fundamentally different training paradigms showed divergent bias patterns, shared training would not fully explain systematicity.

### Mechanism 2
- Claim: Higher F1 scores correlate with increased downstream bias, making model selection on traditional metrics potentially harmful.
- Mechanism: F1 optimization may reinforce model-specific decision boundaries that improve local classification accuracy while systematically shifting prevalence estimates and downstream correlations away from human benchmarks.
- Core assumption: The objective function alignment between F1 and model decision boundaries differs from alignment with human operationalization of concepts.
- Evidence anchors:
  - [abstract] "better F1 scores correlate with greater bias, meaning selecting top-performing models could amplify rather than reduce differences from manual results"
  - [section] Figure 5 (page 7): Positive correlation between macro F1/positive class F1 and normalized correlation coefficient difference for rationality
- Break condition: If F1-bias correlation were concept-dependent without pattern, selection optimization would not systematically amplify bias.

### Mechanism 3
- Claim: Prompt sensitivity interacts with model choice to produce prevalence shifts independent of accuracy metrics.
- Mechanism: Different operationalizations trigger distinct classification thresholds despite similar accuracy, with Jaidka prompts yielding higher rationality prevalence. Model architecture dominates prompt effects for Boukes variations.
- Core assumption: Models encode implicit threshold preferences that interact with prompt framing but remain stable within model families.
- Evidence anchors:
  - [abstract] "differ from manual annotations in terms of prevalence"
  - [section] Figure 1 (page 4): Jaidka prompts classify more replies as rational; Qwen2.5/GPT4o underestimate vs. Llama3.3:70b
- Break condition: If prompt randomization eliminated prevalence differences while maintaining accuracy, implicit thresholds would not be the mechanism.

## Foundational Learning

- Concept: Macro F1 vs. Positive Class F1
  - Why needed here: Paper shows both metrics correlate with bias differently; understanding this distinction is critical for interpreting Table 1 and Figures 5-6.
  - Quick check question: If a model achieves 0.85 accuracy on binary classification but 0.45 F1, what does this suggest about class imbalance and which F1 variant matters more?

- Concept: Prevalence Estimation in Annotation
  - Why needed here: Central finding that GLLMs systematically over/underestimate concept prevalence regardless of F1; this drives downstream correlation divergence.
  - Quick check question: Two annotators both achieve 80% accuracy on a 70/30 class distribution, but one produces 60% positive predictions and the other 75%. Which prevalence estimate is correct, and can F1 alone detect this difference?

- Concept: Systematic vs. Random Error in Simulation
  - Why needed here: Paper's Figure 3 simulation methodology distinguishes bias types through counterfactual annotator curves; understanding this is essential for interpreting the purple/blue line analysis.
  - Quick check question: If 20 annotators each have 85% accuracy and agree perfectly on 20% of cases, does this suggest random noise, systematic bias, or both? How would you distinguish?

## Architecture Onboarding

- Component map: YouTube comments + codebook definitions → 5 prompts → 4 GLLMs → binary labels → F1/accuracy/prevalence/correlation → simulation comparison
- Critical path: Prompt standardization → model inference → prevalence calculation → correlation with genre → simulation benchmarking
- Design tradeoffs: Single concept depth vs. multi-concept breadth; simplified correlation analysis vs. full modeling; simulation-based inference vs. theoretical assumptions
- Failure signatures: High accuracy with significant prevalence drift; strong inter-model agreement with manual divergence; F1-optimal model producing opposite downstream correlation
- First 3 experiments:
  1. Replicate prevalence divergence on held-out concepts to confirm generalizability beyond rationality
  2. Test prompt-model combinations across different text domains to evaluate domain sensitivity
  3. Introduce calibration step using small manual subset to estimate and correct prevalence bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What evaluation metrics can effectively detect systematic bias in LLM annotations where traditional F1 scores fail?
- Basis in paper: [explicit] The authors conclude that "traditional performance metrics failed to detect bias" and explicitly "recommend further research to propose more bias-sensitive metrics."
- Why unresolved: Current metrics like F1 assume errors are random or class-balanced, failing to capture the systematic nature of GLLM deviations from human ground truth.
- What evidence would resolve it: The development and validation of a metric that correlates more strongly with downstream result divergence than macro-F1 does.

### Open Question 2
- Question: What mechanisms drive the counter-intuitive positive correlation between model performance (F1) and systematic bias?
- Basis in paper: [explicit] The study finds that "better F1 scores correlate with greater bias," meaning standard model selection procedures might actively amplify errors.
- Why unresolved: The paper establishes the existence of this relationship but does not explain if it stems from model confidence, training data artifacts, or prompt optimization.
- What evidence would resolve it: A causal analysis identifying whether "high confidence" errors are the primary driver of the observed bias-F1 correlation.

### Open Question 3
- Question: How can prediction-powered inference or similar correction methods be adapted for large-scale datasets where human validation is impractical?
- Basis in paper: [explicit] The authors note that while existing methods can address bias, they "only work for small to medium size samples," leaving large-scale tasks vulnerable.
- Why unresolved: Validating large datasets with human annotators is cost-prohibitive, yet unvalidated GLLM annotations carry the risk of systematic divergence.
- What evidence would resolve it: A methodological framework that accurately debiases large-scale GLLM outputs using only a small, fixed sample of manual annotations.

## Limitations

- The study focuses on a single concept (rationality) in detail, with only 5 total concepts tested, limiting generalizability
- The analysis depends on the assumption that manual annotations represent "true" labels, though inter-annotator agreement data is not reported
- The simulation-based methodology relies on specific parameter choices (85% accuracy, 20 annotators) that may affect conclusions

## Confidence

- High Confidence: GLLMs show more agreement with each other than with manual annotations
- Medium Confidence: Correlation between F1 scores and bias amplification
- Medium Confidence: Prevalence differences across prompts and models

## Next Checks

1. Replicate the prevalence and correlation analysis across all five tested concepts to confirm the systematic bias pattern is not concept-specific
2. Test whether the F1-bias correlation holds when using different manual annotation baselines or when incorporating inter-annotator agreement as a reliability metric
3. Validate the simulation methodology by comparing simulated annotator agreement patterns against actual human annotation datasets with known inter-rater reliability