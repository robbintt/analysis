---
ver: rpa2
title: 'A Derandomization Framework for Structure Discovery: Applications in Neural
  Networks and Beyond'
arxiv_id: '2510.19382'
source_url: https://arxiv.org/abs/2510.19382
tags:
- learning
- function
- neural
- lemma
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a general framework for structure discovery\
  \ in neural networks and beyond. The key contribution is a derandomization lemma\
  \ showing that optimizing functions of the form Ex[g\u03B8(Wx+b)] + \u03BB||W||\xB2\
  \ leads to low-rank solutions at second-order stationary points."
---

# A Derandomization Framework for Structure Discovery: Applications in Neural Networks and Beyond

## Quick Facts
- **arXiv ID**: 2510.19382
- **Source URL**: https://arxiv.org/abs/2510.19382
- **Reference count**: 40
- **Primary result**: Derandomization framework showing low-rank solutions emerge at second-order stationary points for objectives of the form E_x[g_θ(Wx+b)] + λ||W||², with applications in neural networks and dimensionality reduction.

## Executive Summary
This paper introduces a general framework for structure discovery in neural networks and beyond. The key contribution is a derandomization lemma showing that optimizing functions of the form E_x[g_θ(Wx+b)] + λ||W||² leads to low-rank solutions at second-order stationary points. This result applies to neural networks of arbitrary depth and architecture, under any smooth loss function, with all parameters trainable (including biases), and using arbitrarily small regularization. The framework explains why gradient-based training often yields low-rank first-layer weights, a structure associated with improved generalization. Applications include deterministic approximations for MAXCUT and Johnson-Lindenstrauss embeddings, matching or improving upon existing guarantees.

## Method Summary
The paper develops a derandomization framework based on Stein's Lemma to analyze structure discovery in neural networks. The core technique shows that for objectives combining expected smooth loss with Frobenius norm regularization, second-order stationary points naturally exhibit low-rank structure. The framework handles arbitrary neural network architectures, smooth loss functions, and trainable biases. The analysis relies on properties of Gaussian inputs and twice-differentiable activations. Two optimization algorithms are provided: perturbed gradient descent (PGD) and Hessian Descent, with different convergence rates to ρ-SOSP.

## Key Results
- Optimizing E_x[g_θ(Wx+b)] + λ||W||² at second-order stationary points yields low-rank weight matrices
- Trainable biases enable structure discovery with arbitrarily small regularization, unlike frozen biases
- Applications to MAXCUT and Johnson-Lindenstrauss embeddings achieve deterministic approximations matching or improving randomized methods
- The framework explains implicit regularization in neural network training, leading to low-rank first-layer weights

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimizing objectives of the form E_x[g_θ(Wx+b)] + λ||W||²_F at second-order stationary points yields low-rank weight matrices.
- **Mechanism**: Stein's Lemma creates a coupling between ∂²f/∂b² and ∂f/∂W, expressing gradient conditions for W via expectations of second derivatives. At ρ-SOSP, the Hessian lower bound (λ_min ≥ -√(Kρ)) combined with approximate first-order stationarity forces ||W||_F ≤ ρ/(2λ - √(Kρ)).
- **Core assumption**: Input x ~ N(0, I_d) (Gaussian), g_θ is twice-differentiable with L-Lipschitz gradient and K-Lipschitz Hessian.
- **Evidence anchors**:
  - [abstract]: "derandomization lemma showing that optimizing functions of the form E_x[g_θ(Wx+b)] + λ||W||² leads to low-rank solutions at second-order stationary points"
  - [section]: Lemma 3.1 proof (Appendix A.1) derives the bound via Stein's Lemma and SOSP conditions
  - [corpus]: No direct corpus support; related papers focus on NN representations and optimization landscapes without this specific lemma
- **Break condition**: Non-Gaussian input distributions, non-smooth activations (e.g., exact ReLU requires smooth approximation ReLU_ι)

### Mechanism 2
- **Claim**: Training biases (not freezing them) enables structure discovery with arbitrarily small regularization.
- **Mechanism**: The bias b can adjust to satisfy the prediction target while W shrinks. The proof hinges on the interaction ∂²g_θ/∂b² affecting ∂f/∂W; with frozen b=0, achieving W=0 becomes suboptimal, requiring large λ to enforce.
- **Core assumption**: Biases are trainable parameters; objective is twice differentiable.
- **Evidence anchors**:
  - [abstract]: "with all parameters trainable (including biases), and using arbitrarily small regularization"
  - [section]: Pages 2-3, toy example with ReLU³(wx+b)-1 shows frozen biases require large λ; trained biases allow small λ
  - [corpus]: Weak/absent—neighbor papers do not address bias training effects on regularization requirements
- **Break condition**: Freezing biases; non-smooth objectives where second-order conditions don't apply

### Mechanism 3
- **Claim**: ρ-SOSP excludes high-rank saddle points that FOSP permits, enabling low-rank guarantees under weak regularization.
- **Mechanism**: FOSP only constrains ||∇f||, allowing high-rank stationary points. ρ-SOSP adds λ_min(∇²f) ≥ -√(Kρ), ruling out strict saddles. With small ρ, only low-rank local minima survive.
- **Core assumption**: Optimizer reaches ρ-SOSP (PGD converges in O(1/ρ²) iterations; Hessian Descent in O(1/ρ^1.5)).
- **Evidence anchors**:
  - [abstract]: "trained by any method that attains a second-order stationary point (SOSP), e.g. perturbed gradient descent (PGD)"
  - [section]: Definition 2.2 (Page 4), discussion on Page 3: "ρ-SOSP is more likely to be a local minimum than the corresponding approximate FOSP"
  - [corpus]: "Flat Channels to Infinity in Neural Loss Landscapes" discusses NN loss landscape structures but not this specific SOSP mechanism
- **Break condition**: First-order-only optimization that converges to strict saddles; non-Hessian-Lipschitz objectives

## Foundational Learning

- **Concept: Second-order stationarity (SOSP vs FOSP)**
  - Why needed here: The paper's guarantees depend critically on ρ-SOSP properties; understanding why first-order conditions are insufficient is essential.
  - Quick check question: Given a function with a strict saddle point where ∇f=0 but λ_min(∇²f)<0, would FOSP or ρ-SOSP exclude it?

- **Concept: Stein's Lemma for Gaussian expectations**
  - Why needed here: The core Lemma 3.1 proof uses Stein's Lemma to couple gradient expectations; this is the mathematical engine.
  - Quick check question: For x ~ N(0,I) and smooth g, what does Stein's Lemma say about E[x·g(Wx+b)] vs E[∇g(Wx+b)]?

- **Concept: Teacher-student framework**
  - Why needed here: Structure discovery is formalized via a teacher model defining a low-dimensional "principal subspace" that student weights should recover.
  - Quick check question: In a k-index teacher model y=h(⟨u₁,x⟩,...,⟨u_k,x⟩), what structure should student weights W exhibit at convergence?

## Architecture Onboarding

- **Component map**: Objective f(W,b;θ) = E_x[g_θ(Wx+b)] + λ||W||²_F -> Optimizer (PGD/Hessian Descent) -> Low-rank W at SOSP
- **Critical path**:
  1. Reformulate objective to match Eq. 1 form (decompose W into W_∥ + W_⊥)
  2. Set λ = (√(Kρ) + Δ)/2 for desired accuracy ε = ρ/Δ
  3. Run PGD for T = O(poly(L, log(d), ε⁻¹, Δ⁻¹)) iterations
  4. Verify ||W_⊥||_F < ε at convergence
- **Design tradeoffs**:
  - Smaller ρ → tighter rank bounds but more iterations (O(1/ρ²))
  - Smaller λ → needs smaller ρ to satisfy λ > √(Kρ)/2
  - Hessian Descent is faster (O(1/ρ^1.5)) but requires Hessian access
  - Smooth approximation parameter ι: larger → closer to ReLU but larger Lipschitz constants
- **Failure signatures**:
  - Weights remain high-rank: λ too small relative to ρ; increase λ or decrease ρ
  - Slow convergence: Hessian Lipschitz constant K is large (e.g., ι² terms for smooth ReLU)
  - Non-Gaussian data: Lemma 3.1 assumptions violated; no guarantees
- **First 3 experiments**:
  1. Single-index teacher recovery: Train 2-layer ReLU² student on y=tanh(θ·x)+noise with d=2, verify W aligns with θ (replicate Figure 3)
  2. Regularization sweep: Compare ||W_⊥||_F at convergence for λ ∈ {10⁻⁵, 10⁻³, 10⁻¹} with fixed ρ, confirm small λ suffices
  3. MAXCUT on small graph: Apply Algorithm 1 to smoothed MAXCUT objective (m=15 vertices), compare cut value to Goemans-Williamson randomized baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the derandomization framework be extended to non-Gaussian input distributions?
- **Basis in paper**: [explicit] "Our results rely on the assumption that the input distribution is Gaussian. Extending these findings to other distributions is an interesting avenue for future research."
- **Why unresolved**: The proof of Lemma 3.1 fundamentally relies on Stein's Lemma for Gaussian distributions to relate gradients w.r.t. W to Hessians w.r.t. b.
- **What evidence would resolve it**: A modified derandomization lemma for distributions with tractable integration-by-parts identities, or empirical validation on non-Gaussian data.

### Open Question 2
- **Question**: Can the structure discovery results be connected to explicit generalization guarantees?
- **Basis in paper**: [explicit] "It would be valuable to explore connections between our theoretical results and the learning and generalization guarantees that are observed in practice."
- **Why unresolved**: The paper focuses on landscape structure rather than optimization dynamics and sample complexity, requiring additional assumptions on data/hypothesis class.
- **What evidence would resolve it**: Sample complexity bounds linking low-rank recovery at SOSPs to generalization error in teacher-student settings.

### Open Question 3
- **Question**: Does standard SGD (without explicit perturbation) efficiently converge to ρ-SOSPs in this setting?
- **Basis in paper**: [explicit] "It is an open question whether this can be done efficiently, but empirical results on NNs strongly support this behavior" (footnote 1, regarding SGD with random initialization).
- **Why unresolved**: The paper uses PGD for theoretical guarantees; standard SGD almost surely avoids strict saddles but convergence rate guarantees are lacking.
- **What evidence would resolve it**: Polynomial-time convergence proofs for SGD on objectives of the form in Equation 2, or careful empirical timing studies.

## Limitations

- **Gaussian assumption**: The main theoretical results require Gaussian input distributions, limiting applicability to non-Gaussian data common in practice.
- **Twice-differentiability requirement**: The framework requires smooth activations, necessitating approximations for non-smooth functions like ReLU.
- **SOSP convergence**: While perturbation methods guarantee SOSP convergence, practical SGD efficiency for reaching SOSPs remains unproven.

## Confidence

- **Mechanism 1 (Low-rank at SOSP)**: High - The mathematical framework is rigorous and the proof is complete.
- **Mechanism 2 (Bias training advantage)**: Medium - The intuition is sound but empirical validation is limited.
- **Mechanism 3 (MAXCUT/JL applications)**: Low - Reliance on smoothed approximations may not capture discrete problem structure well.

## Next Checks

1. Test structure discovery with non-Gaussian inputs (e.g., uniform, Bernoulli) to assess Assumption 1 violation effects.
2. Compare convergence and final rank when training with vs without bias terms across multiple architectures.
3. Evaluate the smoothed MAXCUT and JL algorithms on discrete benchmarks against their randomized counterparts.