---
ver: rpa2
title: Explainable embeddings with Distance Explainer
arxiv_id: '2505.15516'
source_url: https://arxiv.org/abs/2505.15516
tags:
- distance
- image
- masks
- explainer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Distance Explainer addresses the challenge of explaining embedded
  vector spaces by adapting RISE saliency methods to explain the distance between
  two embedded data points. The method generates attribution maps by comparing masked
  versions of data points and filtering based on distance rankings.
---

# Explainable embeddings with Distance Explainer

## Quick Facts
- arXiv ID: 2505.15516
- Source URL: https://arxiv.org/abs/2505.15516
- Authors: Christiaan Meijer; E. G. Patrick Bos
- Reference count: 21
- Key outcome: Distance Explainer achieves low Average Sensitivity scores (0.04-0.06) and effectively identifies features contributing to similarity/dissimilarity between embedded points

## Executive Summary
Distance Explainer addresses the challenge of explaining embedded vector spaces by adapting RISE saliency methods to explain the distance between two embedded data points. The method generates attribution maps by comparing masked versions of data points and filtering based on distance rankings. Experiments with ImageNet and CLIP models demonstrate that the method effectively identifies features contributing to similarity or dissimilarity between embedded points while maintaining high robustness and consistency.

## Method Summary
Distance Explainer adapts RISE (Randomized Input Sampling for Explanation) saliency methods to explain distances in embedded vector spaces. The approach works by generating multiple masked versions of input data points, embedding them, and calculating distances between the masked embeddings. Attribution maps are created by aggregating masks that either increase or decrease the distance between the two original embedded points. The method uses a threshold-based selection strategy to filter masks based on their impact on the distance metric, with key parameters including mask quantity, selection strategy, and mask coverage percentage.

## Key Results
- Low Average Sensitivity scores (0.04-0.06) indicating high robustness against input perturbations
- Strong dependency on model parameters demonstrated through Model Parameter Randomization Test
- Optimal performance typically achieved with 1000 masks and 10% threshold selection strategy
- Effective identification of features contributing to similarity/dissimilarity between embedded points

## Why This Works (Mechanism)
Distance Explainer works by leveraging the relationship between input features and embedded distances through systematic masking. By creating multiple perturbed versions of input data and observing how these perturbations affect the distance between embedded points, the method can attribute importance to specific input regions. The threshold-based filtering ensures that only masks with meaningful impact on distance are considered, while the aggregation process creates comprehensive attribution maps that highlight both similarity and dissimilarity features.

## Foundational Learning

**RISE Saliency Methods**: Why needed - provides the core masking and attribution framework; Quick check - verify understanding of how random masks are applied and aggregated

**Embedded Vector Spaces**: Why needed - fundamental concept for understanding how data is represented and compared; Quick check - confirm knowledge of how embeddings capture semantic relationships

**Distance Metrics in High Dimensions**: Why needed - critical for understanding how similarity/dissimilarity is quantified; Quick check - ensure comprehension of different distance measures (Euclidean, cosine, etc.)

**Model Parameter Randomization Test**: Why needed - validation technique for assessing explanation sensitivity; Quick check - understand how randomizing model parameters affects explanation consistency

## Architecture Onboarding

Component Map: Input Images -> Mask Generation -> Embedding -> Distance Calculation -> Mask Filtering -> Attribution Map

Critical Path: The core pipeline involves generating masks, applying them to inputs, embedding masked versions, calculating distances, filtering masks based on threshold, and aggregating to create attribution maps.

Design Tradeoffs: Mask quantity vs. computational cost (1000 masks optimal), threshold percentage vs. explanation granularity (10% typical), mask coverage vs. feature preservation (10% standard).

Failure Signatures: Poor explanations may result from insufficient mask quantity, inappropriate threshold selection, or when embedded distances are insensitive to input perturbations.

First Experiments: 1) Test with varying mask quantities (100, 500, 1000, 2000) to find optimal balance, 2) Compare different threshold strategies (10%, 20%, 30%), 3) Validate on different embedding models (CLIP, ResNet, Vision Transformer).

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas for future research are implied by the limitations section.

## Limitations
- Limited validation to image data and CLIP models, raising questions about cross-domain applicability
- Reliance on RISE-based masking approaches may not capture all relevant features in complex embedded spaces
- Does not explore impact of different distance metrics or embedding dimensionalities on explanation quality

## Confidence

High: Claims about robustness due to low Average Sensitivity scores (0.04-0.06) and strong Model Parameter Randomization Test results

Medium: Effectiveness of 1000 masks and 10% threshold recommendation, as these parameters were empirically identified but may vary across embedding spaces

Low: Ability to explain all types of embedded relationships, as the approach is designed specifically for distance-based explanations and may not capture other semantic relationships

## Next Checks
1. Test Distance Explainer on non-image embedding spaces (text, audio, or graph embeddings) to assess cross-domain applicability
2. Compare Distance Explainer's explanations with human-annotated feature importance in controlled datasets to validate explanation quality
3. Conduct ablation studies on different distance metrics and embedding dimensionalities to understand parameter sensitivity beyond mask configuration