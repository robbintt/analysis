---
ver: rpa2
title: 'LILO: Bayesian Optimization with Interactive Natural Language Feedback'
arxiv_id: '2510.17671'
source_url: https://arxiv.org/abs/2510.17671
tags:
- utility
- feedback
- outcomes
- optimization
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LILO, a language-in-the-loop optimization
  framework that uses large language models to convert natural language feedback into
  scalar utilities for Bayesian optimization. Unlike traditional preference-based
  BO methods that rely on structured pairwise comparisons, LILO allows decision makers
  to provide rich, unstructured feedback in natural language, which the LLM translates
  into consistent utility signals.
---

# LILO: Bayesian Optimization with Interactive Natural Language Feedback

## Quick Facts
- **arXiv ID**: 2510.17671
- **Source URL**: https://arxiv.org/abs/2510.17671
- **Reference count**: 40
- **Primary result**: LILO uses LLMs to convert natural language feedback into scalar utilities for Bayesian optimization, outperforming conventional BO and LLM-only optimizers in feedback-limited regimes.

## Executive Summary
This paper introduces LILO, a language-in-the-loop framework that uses large language models to convert unstructured natural language feedback into scalar utilities for Bayesian optimization. Unlike traditional preference-based BO methods that rely on structured pairwise comparisons, LILO allows decision makers to provide rich, unstructured feedback in natural language, which the LLM translates into consistent utility signals. The method maintains the sample efficiency and uncertainty quantification of BO while providing a more intuitive interface for human decision makers. Experiments on synthetic and real-world optimization problems show that LILO outperforms both conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes.

## Method Summary
LILO operates by using an LLM to generate questions about preference trade-offs, which a decision maker answers in natural language. The LLM then labels pairwise preferences or generates scalar utility estimates from these responses. These noisy labels are fed into a Gaussian Process (GP) surrogate model that provides a probabilistic utility surface with calibrated uncertainty. The GP guides candidate selection through acquisition functions like LogNEI, enabling principled exploration-exploitation. The framework iteratively collects feedback, updates the GP models, and refines the optimization trajectory. LILO supports both pairwise preference learning and direct scalar utility estimation, and can incorporate prior knowledge expressed in natural language to accelerate convergence.

## Key Results
- LILO outperforms conventional BO baselines and LLM-only optimizers, especially in feedback-limited regimes (T=8 iterations).
- The framework successfully incorporates prior knowledge expressed in natural language, improving optimization performance.
- Pairwise preference labeling with the LLM yields more reliable utility estimates than direct scalar estimation in early optimization stages.

## Why This Works (Mechanism)

### Mechanism 1: Natural Language Feedback → Scalar Utility Translation
Converting natural language feedback into quantitative scalar utilities allows BO to operate on richer information than structured pairwise comparisons alone. The LLM interprets free-form feedback (e.g., rationales, trade-offs, constraints) and maps it to latent utility estimates via pairwise preference labeling or scalar scoring; these estimates are then used to train GP surrogate models. Core assumption: The LLM can faithfully and consistently translate qualitative human preferences into comparative or scalar signals that correlate with the true latent utility function.

### Mechanism 2: GP Smoothing of Noisy LLM Judgments
Using a Gaussian Process to model LLM-derived utilities provides uncertainty quantification and smooths over noise/instability in the LLM's raw predictions. The LLM labels pairwise preferences or scalar utilities; these noisy labels are fed into a pairwise or standard GP which provides a continuous, probabilistic utility surface with calibrated uncertainty, enabling principled acquisition functions. Core assumption: The LLM's preference judgments are noisy but systematic enough that a GP can model the underlying utility trend without being overwhelmed by labeling variance.

### Mechanism 3: Richer Information from NL vs. Pairwise Alone
Natural language feedback conveys more information about the utility function's global shape and trade-offs than localized pairwise comparisons alone. NL responses can express priorities, constraints, trade-off directions, and high-level goals, allowing the LLM (and hence the GP) to generalize across the outcome space more rapidly. Core assumption: The DM's natural language feedback is coherent and sufficiently informative about the utility structure; the LLM can extract and apply this information across outcomes.

## Foundational Learning

- **Concept: Gaussian Process (GP) surrogate models for BO**
  - Why needed here: GP provides probabilistic predictions and uncertainty estimates over the latent utility function, enabling principled exploration-exploitation via acquisition functions like LogNEI.
  - Quick check question: Given a small set of noisy utility labels, can you explain why a GP is preferable to a deterministic interpolator?

- **Concept: Preference learning with pairwise comparisons**
  - Why needed here: LILO uses pairwise preference labels derived by the LLM to train a pairwise GP (Chu & Ghahramani, 2005), which models the latent utility from comparative feedback.
  - Quick check question: Why might pairwise comparisons yield more reliable utility estimates than absolute scalar ratings (Phelps et al., 2015)?

- **Concept: In-context learning (ICL) in LLMs**
  - Why needed here: The LLM uses ICL to interpret feedback and label preferences based on the conversation history, without fine-tuning.
  - Quick check question: What are limitations of ICL for consistent utility estimation over many rounds (Brown et al., 2020; Falck et al., 2024)?

## Architecture Onboarding

- **Component map**: LLM Agent -> Feedback Dataset -> GP Proxy Models -> Acquisition Function -> Experimental Dataset
- **Critical path**: 1. Initial question generation → DM answers → build D^pf_0. 2. First batch of candidates (uniform or prior-guided) → evaluate black-box f → update D^exp. 3. Generate follow-up questions → DM answers → update D^pf. 4. LLM labels K pairwise preferences → fit/update GP models. 5. Optimize acquisition → select next candidates → repeat.
- **Design tradeoffs**:
  - Pairwise vs. scalar utility estimation: Pairwise more reliable but requires more LLM calls (K pairs).
  - Feedback batch size (B_pf): More feedback per round improves utility estimation but increases DM burden.
  - Prior knowledge inclusion: Can accelerate convergence but risks bias if prior is inaccurate.
  - GP pair selection strategy: EUBO-based selection vs. random; EUBO may improve proxy quality but not always optimization performance.
- **Failure signatures**:
  - Utility estimates plateau early: LLM judgments may be inconsistent; check labeling variance across LLM samples.
  - GP uncertainty poorly calibrated: Insufficient or highly noisy preference labels; consider increasing K or B_pf.
  - Optimization stagnates despite feedback: NL feedback may be uninformative; inspect Q&A logs for vague/contradictory responses.
- **First 3 experiments**:
  1. Run LILO on a synthetic environment (e.g., DTLZ2 + piecewise linear) with B_pf=2, log utility estimates vs. ground truth, and compare GP posterior to true utility surface.
  2. Ablate pairwise vs. scalar utility estimation on the same environment; record optimization trajectory and best-utility-at-n.
  3. Incorporate a domain prior in a semantically meaningful environment (e.g., Vehicle Safety) and measure improvement in starting utility and final convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid approaches that combine unstructured natural language with structured quantitative feedback effectively mitigate the limitations of in-context learning in utility estimation? The discussion section explicitly identifies this as a promising direction to "mitigate the limits of ICL in the utility estimation step."

### Open Question 2
What calibration strategies are required to prevent the LLM's pre-training priors from overshadowing the decision maker's specific preferences? The authors note that LLM estimators may encode preferences too strongly, risking "subtle bias toward the model's own world knowledge."

### Open Question 3
Does LILO maintain its reported sample efficiency when deployed with actual human decision makers rather than simulated agents? The primary experimental results utilize LLM-simulated feedback, while the Discussion calls for future work evaluating performance with human decision makers.

## Limitations
- LLM consistency and bias in translating natural language to utility estimates remain untested across different domains and DM personas.
- Performance in extremely feedback-limited regimes (<8 queries) is not validated; the T=8 iteration setup may not reflect real-world scenarios where only 2-3 feedback rounds are feasible.
- The method's scalability to high-dimensional spaces (d>10) and complex, non-linear utility functions is not explored; GP modeling complexity and LLM interpretability may degrade.

## Confidence
- **High confidence**: The core mechanism of using LLMs to convert NL feedback to pairwise preferences is technically sound and well-supported by the experimental results.
- **Medium confidence**: The advantage of LILO over preference-based BO in feedback-limited regimes is demonstrated but needs validation in more diverse, real-world environments with varying DM expertise.
- **Low confidence**: The robustness of LILO to noisy, vague, or contradictory NL feedback is not thoroughly tested; this is critical for practical deployment.

## Next Checks
1. **Validate LLM consistency**: Run LILO on the same feedback multiple times (e.g., 5 seeds) and measure variance in pairwise preference labels and resulting GP utility estimates. High variance would indicate instability.
2. **Stress-test feedback-limited performance**: Reduce T to 3-4 iterations and evaluate LILO's performance on a synthetic problem with known utility structure. Compare against preference-based BO to confirm LILO's advantage holds.
3. **Test real-world domain adaptation**: Apply LILO to a new, complex domain (e.g., drug discovery objective with multi-fidelity outcomes) and assess how well the LLM translates domain-specific NL feedback into utility signals without extensive prompt engineering.