---
ver: rpa2
title: Automated Repair of Ambiguous Problem Descriptions for LLM-Based Code Generation
arxiv_id: '2505.07270'
source_url: https://arxiv.org/abs/2505.07270
tags:
- repair
- problem
- description
- descriptions
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of automated repair of ambiguous
  natural language problem descriptions for LLM-based code generation. The authors
  find that ambiguous descriptions lead to incorrect code generation, even when input-output
  examples are provided.
---

# Automated Repair of Ambiguous Problem Descriptions for LLM-Based Code Generation

## Quick Facts
- arXiv ID: 2505.07270
- Source URL: https://arxiv.org/abs/2505.07270
- Reference count: 40
- Key outcome: Introduces SPECFIX, an automated approach that repairs ambiguous natural language problem descriptions for LLM code generation, improving Pass@1 by 30.9% on modified descriptions and 4.09% across all benchmarks.

## Executive Summary
This paper addresses the problem of ambiguous natural language problem descriptions that lead to incorrect code generation by LLMs, even when input-output examples are provided. The authors introduce SPECFIX, a novel approach that decomposes the task into two steps: first repairing the LLM's interpretation of the description by modifying the program distribution it generates, then refining the description via contrastive specification inference. Evaluation on three benchmarks with four LLMs shows SPECFIX modifies 43.58% of descriptions and improves code generation performance, with repairs that generalize across different models.

## Method Summary
SPECFIX operates through a two-phase approach. First, it samples programs from the LLM, partitions them into semantic clusters based on output behavior, and computes semantic entropy and example consistency metrics. If no cluster fully satisfies the embedded I/O examples, it repairs the highest-probability cluster using a self-refine program repair method. Second, it performs contrastive specification inference by prompting the LLM to rewrite the description, contrasting the selected (repaired) program behavior against rejected behaviors. The approach requires only embedded examples, no human intervention or external information, and uses iteration bound K=3 for the repair loop.

## Key Results
- SPECFIX modifies 43.58% of problem descriptions across benchmarks
- Improves Pass@1 by 30.9% on modified descriptions and 4.09% across all benchmarks
- Repairs generalize across models, improving other models' performance by 10.48%
- Base system has 3.23% Incorrect Modification Rate, reducible with human-in-the-loop mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition into program distribution repair and contrastive specification inference enables effective ambiguity resolution where direct prompting fails.
- Mechanism: Bypasses direct text manipulation by first intervening on the LLM's interpretation captured by program distribution, then mapping behavioral changes back to description.
- Core assumption: LLM understanding can be captured and guided by modifying program distribution statistics, and this change can be translated into natural language repair.
- Evidence anchors: Abstract states decomposition into simpler steps; section describes decomposing task into subtasks of repairing program distribution and mapping change via contrastive inference.

### Mechanism 2
- Claim: Reducing semantic entropy and maximizing example consistency in generated program distribution guides system toward less ambiguous and correct code.
- Mechanism: Uses semantic entropy (SE) to quantify uncertainty over semantic clusters and example consistency (EC) to measure fraction of I/O examples passed by clusters.
- Core assumption: Low semantic entropy correlates with less ambiguous descriptions, and programs satisfying examples are correct proxies for user intent.
- Evidence anchors: Abstract mentions reducing code generation uncertainty and aligning NL with examples; section describes using SE to lessen uncertainty and EC to align with examples.

### Mechanism 3
- Claim: Contrastive specification inference generates precise, minimal repairs by comparing desired program behavior against undesired one.
- Mechanism: Prompts LLM with selected program (positive) and rejected programs (negative) to rewrite description permitting only selected behavior.
- Core assumption: Providing contrasting behavioral examples is more effective for disambiguation than reasoning about abstract descriptions in isolation.
- Evidence anchors: Abstract mentions refining description via contrastive specification inference; section describes leveraging contrastive insight to highlight why outcome A occurs instead of B.

## Foundational Learning

- **Concept: Semantic Entropy**
  - Why needed here: Primary metric for quantifying ambiguity, measuring uncertainty over meanings of generated programs
  - Quick check question: If an LLM generates 20 programs that are syntactically different but produce identical outputs on all test inputs, is the semantic entropy high or low? (Answer: Low)

- **Concept: Example Consistency (EC)**
  - Why needed here: Novel metric quantifying how well generated program aligns with embedded I/O examples
  - Quick check question: A semantic cluster contains programs that pass 2 out of 5 embedded examples. What is its Example Consistency score? (Answer: 0.4)

- **Concept: Contrastive Specification Inference**
  - Why needed here: Final step translating repaired program into repaired natural language description by contrasting with original behavior
  - Quick check question: In contrastive inference prompt, what serves as "positive" and "negative" examples? (Answer: Selected program serves as positive, rejected programs as negative)

## Architecture Onboarding

- **Component map:**
  - 1. **Interpreter**: Samples N programs, generates test inputs, partitions into semantic clusters, computes SE and EC
  - 2. **Program Repairer**: Takes program from highest-probability cluster and I/O examples, produces repaired program via self-refine
  - 3. **Contrastive Inferencer**: Takes original description, selected program outputs, rejected program outputs, generates disambiguated description

- **Critical path:** Description → Interpreter → (if no EC=1 cluster) Program Repairer → Contrastive Inferencer → Repaired description

- **Design tradeoffs:**
  - Sample Size (N): Small N is computationally cheaper but risks missing rare interpretations; paper chooses N=20
  - Automation vs. Accuracy: Base system is fully automatic with 3.23% IMR; mitigation strategy lowers IMR but sacrifices autonomy for 10.8% of modifications

- **Failure signatures:**
  - Majority Voting Failure: Correct and incorrect clusters have similar probabilities, causing incorrect repair
  - Incorrect Examples: Embedded examples conflict with reference solutions, causing faulty repairs
  - Unsatisfiable Constraints: Description too vague for LLM to produce near-correct solutions, blocking pipeline

- **First 3 experiments:**
  1. Metric Ablation Study: Run SPECFIX against variants without semantic entropy or example consistency on benchmark
  2. Cross-Model Generalization Test: Repair descriptions with one model, measure Pass@1 of different model on repaired descriptions
  3. Mitigation Effectiveness Evaluation: Compare IMR and Pass@1 of automatic system vs. version with modified z-score mitigation

## Open Questions the Paper Calls Out

- **Open Question 1:** How can human users be efficiently integrated into the SPECFIX workflow to resolve ambiguities that automated methods cannot fix? (Section VI states future research will investigate human involvement)
- **Open Question 2:** Does automated ambiguity repair generalize to real-world software engineering contexts, such as interactive AI coding assistants? (Section VI notes studying ambiguities in real-world applications remains important)
- **Open Question 3:** Can ambiguity detection and repair be achieved using formal models or behavioral proxies when generated code cannot be executed in isolation? (Section VI suggests formal models could detect ambiguity instead of execution)

## Limitations

- Relies on embedded I/O examples as ground truth, assuming they are always correct and complete (less than 1% have inconsistent examples)
- Assumes semantic clusters from program outputs reliably capture semantic differences, which may fail for complex programs or infinite input spaces
- 3.23% Incorrect Modification Rate represents significant practical limitation requiring mitigation tradeoffs

## Confidence

- High confidence in mechanism of decomposing ambiguity resolution into program distribution repair and contrastive specification inference
- Medium confidence in specific metrics (semantic entropy and example consistency) as optimal measures for guiding repair
- Medium confidence in cross-model generalization claims

## Next Checks

1. **Human evaluation of repair quality**: Have human developers assess whether modified descriptions are clearer and more accurate than originals beyond measuring code generation Pass@1 scores
2. **Adversarial description test**: Systematically create deliberately ambiguous descriptions that exploit potential failure modes to test robustness boundaries
3. **Ablation of contrastive vs. direct prompting**: Compare SPECFIX against variant attempting direct description clarification without program distribution repair step