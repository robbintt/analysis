---
ver: rpa2
title: 'LLM-I: LLMs are Naturally Interleaved Multimodal Creators'
arxiv_id: '2509.13642'
source_url: https://arxiv.org/abs/2509.13642
tags:
- image
- tool
- arxiv
- generation
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLM-I, a framework that reframes interleaved
  image-text generation as a tool-use problem. It uses an LLM or MLLM as an agentic
  planner to orchestrate a toolkit of specialized visual tools, including online image
  search, diffusion-based generation, code execution, and image editing.
---

# LLM-I: LLMs are Naturally Interleaved Multimodal Creators

## Quick Facts
- arXiv ID: 2509.13642
- Source URL: https://arxiv.org/abs/2509.13642
- Authors: Zirun Guo; Feng Zhang; Kai Jia; Tao Jin
- Reference count: 11
- Primary result: Outperforms existing methods on interleaved image-text generation benchmarks by reframing the task as tool-use problem

## Executive Summary
This paper introduces LLM-I, a framework that reframes interleaved image-text generation as a tool-use problem rather than an end-to-end synthesis task. An LLM or MLLM serves as an agentic planner that orchestrates a toolkit of specialized visual tools (online search, diffusion-based generation, code execution, image editing) through structured `<imgen>` tags. The agent is trained via reinforcement learning with a hybrid reward system combining rule-based constraints and model-based quality judgments. The approach achieves state-of-the-art performance across four benchmarks and introduces a novel test-time scaling strategy for further improvements.

## Method Summary
LLM-I converts interleaved generation into a tool-selection problem where an LLM generates structured `<imgen>` tags containing JSON with tool type, description, and parameters. A parser intercepts these tags and dispatches calls to external tools, then replaces tags with results. The framework uses reinforcement learning with GRPO/GSPO and a hybrid reward system combining rule-based image count constraints with LLM and MLLM evaluators. Trained on a diverse dataset of ~4k samples using four model backbones, LLM-I demonstrates superior performance on OpenING, ISG, LLMI-Bench, and in-domain benchmarks. A test-time scaling strategy generates multiple candidates with parallel tool invocations and selector-based refinement.

## Key Results
- Achieves SOTA performance across four interleaved image-text generation benchmarks
- Demonstrates state-of-the-art results with both small (4B) and large (30B) models
- Shows significant improvements from test-time scaling strategy without additional training
- Ablation studies confirm the importance of hybrid reward composition and multi-tool flexibility

## Why This Works (Mechanism)

### Mechanism 1: Agentic Tool Orchestration
Converting interleaved generation from end-to-end synthesis to tool-selection enables handling diverse visual requirements (factual, synthetic, programmatic) that monolithic models cannot address. The LLM generates structured `<imgen>` tags that a parser dispatches to external tools, assuming LLMs possess emergent planning capabilities for implicit tool selection.

### Mechanism 2: Hybrid Reward Composition with Multiplicative Gating
Combining rule-based constraints with model-based quality judgments prevents reward hacking while maintaining semantic quality. Three reward components (R_rule, R_llm, R_mllm) combine with multiplicative gating where visual quality only counts if constraints are met, based on the assumption that rule-based rewards must gate model-based rewards to prevent avoiding error-prone tools.

### Mechanism 3: Test-Time Scaling via Parallel Candidate Enhancement
Generating multiple candidates with parallel tool invocations and selector-based refinement improves output quality without additional training. The four-stage pipeline (sample, select, enhance, polish) assumes candidate diversity through stochastic sampling combined with MLLM-based selection can identify higher-quality tool outputs than single-pass generation.

## Foundational Learning

- **Proximal Policy Optimization (PPO) and GRPO variants**: The paper uses GRPO for dense models and GSPO for MoE models; understanding value-free alternatives is essential for implementation. Quick check: Can you explain why GRPO avoids the need for a value model compared to PPO?

- **Structured generation and parsing**: The `<imgen>` tag mechanism requires reliable JSON-like generation and robust parsing; failures cascade to tool invocation errors. Quick check: How would you handle malformed JSON in an `<imgen>` tag during inference?

- **Tool-use prompting strategies**: The dataset uses implicit tool cues rather than explicit instructions; understanding how to design prompts that encourage tool reasoning without naming tools is critical. Quick check: What's the difference between "Generate a chart using Python" vs. "Visualize the quarterly trends" for implicit tool invocation?

## Architecture Onboarding

- **Component map**: Central Agent (LLM/MLLM backbone) -> Tag Parser -> Tool Executors (Search, Diffusion, Code, Edit) -> Reward System (Hybrid R_rule + R_llm + R_mllm) -> Test-Time Scaling (Selector, parallel execution, polisher)

- **Critical path**: User prompt → Agent generates response with `<imgen>` tags → Parser extracts tags → Dispatch to tool executors → Tools return images → Replace tags in output → During training: Compute hybrid reward → RL update (GRPO/GSPO) → During TTS: Generate n candidates → Filter → Select top-k → Enhance → Polish → Final selection

- **Design tradeoffs**: Tool diversity vs. simplicity (four tools provide flexibility but increase failure modes), Dataset size vs. quality (~4k samples prioritizes quality over quantity), TTS compute vs. latency (parallel tool calls minimize overhead, but polishing adds ~16s/iteration)

- **Failure signatures**: Tool call format errors (malformed `<imgen>` JSON causes parser failures), Reward hacking (agent avoids code/search tools), Image count violations (without R_rule gating, agent may over/under-generate images)

- **First 3 experiments**: 1) Baseline tool invocation: Test Qwen3-4B-Instruct with all four tools on LLMI-Bench without RL training; 2) Reward ablation: Train LLM-I-4B with R_rule only vs. full hybrid reward; 3) Tool subset analysis: Restrict trained LLM-I-4B to "only diffusion" vs. "only search"

## Open Questions the Paper Calls Out

### Open Question 1
Can the LLM-I agent generalize to orchestrate novel tools not present in the reinforcement learning training data without requiring fine-tuning? The Conclusion states this work "paves the way for future research into expanding the agent's toolkit," but experiments exclusively validate the specific four-tool set. Evaluating on tasks requiring new tools (e.g., 3D asset generation) would test zero-shot tool reasoning.

### Open Question 2
Does the computational overhead of test-time scaling (TTS), particularly the polishing stage, yield diminishing returns compared to simply scaling the backbone model size? Section 5.3 notes polishing is "most time-consuming" (~16s/it) but doesn't analyze cost-to-performance trade-offs. A comparative analysis of TTS-enhanced small models versus non-TTS large models would clarify efficiency.

### Open Question 3
To what extent does the hybrid reward system mitigate "reward hacking" where the model optimizes for evaluator heuristics rather than genuine semantic alignment? Section 3.2.1 notes explicit weighting to counteract aversion to error-prone tools, acknowledging potential bias. Qualitative analysis of failure cases where high automated reward scores mask factual errors would reveal limitations.

## Limitations
- Small training dataset (~4k samples) may limit generalization to diverse real-world scenarios
- Implicit tool cues could reduce reliability in complex cases requiring precise tool specification
- Test-time scaling assumes parallel tool execution is feasible, which may not hold with rate limits or cost constraints

## Confidence

- **High Confidence**: The mechanism of reframing interleaved generation as a tool-use problem is well-supported by architecture and ablation results showing baseline improvements
- **Medium Confidence**: The hybrid reward composition theory is internally consistent and supported by ablation studies, but specific weight configuration may not generalize
- **Low Confidence**: The test-time scaling strategy's scalability claims are promising but rely on ideal conditions; 16s polishing overhead could be prohibitive in latency-sensitive applications

## Next Checks

1. **Reward Sensitivity Analysis**: Systematically vary reward weights (w_rule, w_llm, w_mllm) across a grid search to identify robustness boundaries and test for overfitting to specific weight values

2. **Implicit vs. Explicit Tool Cue Comparison**: Create controlled experiment training the same model with explicit tool instructions versus implicit cues to measure differences in tool accuracy, output quality, and generation reliability

3. **Stress Test on Complex Multi-Turn Scenarios**: Evaluate LLM-I on prompts requiring nested tool interactions or multi-turn refinement to measure tool invocation accuracy and semantic coherence compared to reported single-turn benchmarks