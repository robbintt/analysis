---
ver: rpa2
title: 'Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation
  via Knowledge Injection'
arxiv_id: '2504.16429'
source_url: https://arxiv.org/abs/2504.16429
tags:
- code
- knowledge
- security
- codeguarder
- racg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses security risks in Retrieval-Augmented Code
  Generation (RACG), where poisoning of knowledge bases with malicious code can lead
  to insecure outputs. The proposed solution, CodeGuarder, shifts from retrieving
  only functional code examples to incorporating both functional code and security
  knowledge derived from real-world vulnerability databases.
---

# Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection

## Quick Facts
- arXiv ID: 2504.16429
- Source URL: https://arxiv.org/abs/2504.16429
- Reference count: 40
- Key outcome: Security rate improvements of 20.12% in standard RACG and 31.53%/21.91% under poisoning scenarios via dual retrieval of functional code and security knowledge

## Executive Summary
This paper addresses security risks in Retrieval-Augmented Code Generation (RACG) where poisoning of knowledge bases with malicious code can lead to insecure outputs. The proposed solution, CodeGuarder, shifts from retrieving only functional code examples to incorporating both functional code and security knowledge derived from real-world vulnerability databases. For each query, CodeGuarder decomposes it into fine-grained sub-tasks, retrieves relevant security knowledge, re-ranks it based on vulnerability susceptibility, and integrates it into the generation prompt. Evaluation shows CodeGuarder significantly improves code security rates without compromising functional correctness.

## Method Summary
CodeGuarder implements dual retrieval by first building a security knowledge base from CVE instances containing functionality descriptions, root causes with vulnerable code, and fixing patterns with secure code. For each user query, an LLM decomposes the query into sub-tasks, each representing a distinct semantic unit. Security knowledge is retrieved independently for each sub-task using jina-embeddings-v3 similarity search, then re-ranked by vulnerability susceptibility weights (based on frequency of CWE types in LLM-generated code). The top-k sub-tasks' knowledge is injected into the generation prompt alongside functional code examples. The system uses CyberSecEval benchmark (1,916 instances, 50 CWE types) for evaluation with an Insecure Code Detector achieving 96% precision.

## Key Results
- Security Rate (SR) improved by 20.12% in standard RACG conditions
- SR improvements of 31.53% and 21.91% under two distinct poisoning scenarios
- Generalization achieved with 75.54% security rate even without target-language knowledge base
- Maintains functional correctness with no significant degradation in similarity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit injection of structured security knowledge into prompts enables LLMs to generate more secure code, even when functional code examples are poisoned.
- Mechanism: CodeGuarder constructs a security knowledge base from CVE instances containing functionality description, root cause with vulnerable code example, and fixing pattern with secure code example. This knowledge is injected into the generation prompt alongside functional code, providing explicit security guidance that can override patterns from potentially malicious retrieved examples.
- Core assumption: LLMs can effectively utilize explicit security guidance to avoid insecure patterns, even when both poisoned functional examples and security knowledge are present in the same prompt.
- Evidence anchors:
  - [abstract]: "CodeGuarder... shifts the paradigm from retrieving only functional code examples to incorporating both functional code and security knowledge... achieving average improvements of 20.12% in standard RACG, and 31.53% and 21.91% under two distinct poisoning scenarios"
  - [section]: Figure 1 demonstrates a concrete case where injection of security knowledge about `sprintf` risks causes GPT-4o to switch to secure `snprintf` instead.
  - [corpus]: Related work "Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented Code Generation" (arXiv:2502.03233) establishes that a single poisoned sample can cause 48% of generated code to contain vulnerabilities.

### Mechanism 2
- Claim: Decomposing complex code generation queries into fine-grained sub-tasks enables retrieval of more precisely relevant security knowledge.
- Mechanism: An LLM decomposes the user query $Q$ into a list of sub-tasks $Q_d = [q_1, q_2, ..., q_n]$, each representing a distinct semantic unit. Security knowledge is retrieved independently for each sub-task rather than for the whole query, enabling targeted guidance for each security-critical operation.
- Core assumption: A single code generation task may involve multiple distinct security concerns at different code locations (e.g., memory allocation AND string copying), and addressing them individually yields better security outcomes than treating the query holistically.
- Evidence anchors:
  - [abstract]: "For each code generation query, a retriever decomposes the query into fine-grained sub-tasks and fetches relevant security knowledge"
  - [section]: Section 4.2.1 states "a single program may contain multiple vulnerabilities across different locations"; Figure 3 shows code with buffer overflow vulnerabilities at multiple locations. Table 11 ablation shows removing Query Decomposition drops SR from 76.36% to 68.61%.
  - [corpus]: No direct corpus evidence on query decomposition for security retrieval was found in neighboring papers.

### Mechanism 3
- Claim: Re-ranking retrieved security knowledge by vulnerability susceptibility under token budget constraints improves security outcomes by prioritizing high-risk guidance.
- Mechanism: Retrieved knowledge entries are weighted by the frequency of their associated vulnerability types in LLM-generated code (NULL pointer: 40.24%, buffer overflow: 25.53% from Table 1). Sub-tasks are ranked by aggregated weight, and only the top-k sub-tasks' knowledge is retained, focusing limited context window on the most likely vulnerabilities.
- Core assumption: LLMs exhibit systematic susceptibility patterns to certain vulnerability types, and prioritizing knowledge about high-frequency vulnerabilities yields better security improvements than uniform knowledge allocation given context window limits.
- Evidence anchors:
  - [abstract]: "To prioritize critical security guidance, we introduce a re-ranking and filtering mechanism by leveraging the LLMs' susceptibility to different vulnerability types"
  - [section]: Section 4.2.3 details the weighting formula; Table 11 shows removing KRF causes ~2.2% SR drop; Table 1 provides empirical vulnerability distribution from 310,531 code generation instructions across 13 LLMs.
  - [corpus]: The vulnerability distribution (Table 1) is derived from "How secure is AI-generated code: a large-scale comparison of large language models" (citation [41] in paper).

## Foundational Learning

- **Retrieval-Augmented Code Generation (RACG)**:
  - Why needed here: Standard RACG retrieves functional code examples to improve correctness but lacks security awareness, making it vulnerable to poisoning. Understanding this baseline is essential for appreciating why dual retrieval (functional + security) is novel.
  - Quick check question: Why does retrieving semantically similar code improve functional correctness but potentially harm security?

- **CWE/CVE Taxonomy and Security Knowledge Representation**:
  - Why needed here: The security knowledge base is structured around CVE instances and CWE classifications. Understanding the difference between a weakness type (CWE) and a specific vulnerability instance (CVE) is necessary for comprehending the knowledge extraction pipeline.
  - Quick check question: Given a CVE with a buffer overflow fixed by replacing `strcpy` with `strncpy`, what would appear in the "root cause" vs. "fixing pattern" dimensions?

- **Token Budget and Attention Dilution in Long Contexts**:
  - Why needed here: The re-ranking mechanism exists specifically because long prompts with excessive knowledge cause attention dilution and potential truncation. Understanding context window constraints motivates why filtering is necessary rather than simply injecting all retrieved knowledge.
  - Quick check question: If you have 20 sub-tasks each with 5 relevant security knowledge entries, what problem arises if you inject all 100 entries into the prompt?

## Architecture Onboarding

- **Component map**:
  Offline phase: CVE/ReposVul input → LLM extraction → structured knowledge tuples
  Online phase per query: Query Decomposer (LLM) → Knowledge Retriever (jina-embeddings-v3 similarity search) → Re-ranker/Filter (vulnerability frequency weighting) → Prompt Constructor → Code Generator (target LLM)
  Evaluation: CyberSecEval benchmark (1,916 instances, 50 CWEs) + Insecure Code Detector (96% precision)

- **Critical path**: User query → [Query Decomposition into sub-tasks] → [Per-sub-task embedding and retrieval] → [Weight aggregation and top-k filtering] → [Prompt assembly with security knowledge] → [LLM generation]

- **Design tradeoffs**:
  1. **k' (entries per sub-task) vs k (top sub-tasks)**: Table 12 shows larger models (DS-V3) benefit from k=7, smaller models (CodeLlama-13B) perform best at k=5. Default k'=2, k=5 balances robustness across model scales.
  2. **Functional code base source**: Using ReposVul fixed code for functional base K ensures no contamination but may not represent real-world code diversity in public repositories.
  3. **Language-specific vs cross-language generalization**: Section 6.4.2 shows 15.69%-21.26% improvements even without target-language knowledge, but 31.53% with it—a tradeoff between coverage and specificity.

- **Failure signatures**:
  1. **CWE coverage gaps**: CWE-79 (XSS) showed only 4.76% prevention in JavaScript due to missing language-specific knowledge (Section 7.2).
  2. **Token overflow**: Excessive k' and k cause context truncation and attention dilution (Section 7.4).
  3. **Small model instruction-following**: CodeLlama-13B shows smaller improvements (16.15%) vs. GPT-4o (20.28%), attributed to weaker instruction-following (Section 6.2).

- **First 3 experiments**:
  1. **Baseline security measurement**: Run CyberSecEval on your target LLM with standard RACG (no poisoning, no CodeGuarder) to establish baseline SR and identify which CWEs your model is most susceptible to.
  2. **Poisoning robustness test**: Inject 5 semantically similar vulnerable examples per query (Scenario I setup from Section 5.1.2) and measure whether CodeGuarder recovers security rate compared to poisoned baseline.
  3. **Ablation by component**: Run (a) without Query Decomposition, (b) without Re-ranking/Filtering, and (c) full CodeGuarder to determine which mechanism contributes most for your specific LLM and use case, following Table 11 methodology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the improved security rate detected by static analysis correlate with robustness against dynamic exploitation or runtime vulnerabilities?
- Basis in paper: [explicit] Section 7.5 (Threats to Validity) explicitly states that the primary measure relies on static analysis tools (Insecure Code Detector), which may not perfectly capture the true security posture or exploitability of the generated code.
- Why unresolved: Static analysis often yields false positives/negatives and lacks the runtime context required to confirm if a vulnerability is actually exploitable in a specific execution environment.
- What evidence would resolve it: An evaluation using dynamic application security testing (DAST) or fuzzing on the generated code to verify exploitability, rather than relying solely on pattern-based detection.

### Open Question 2
- Question: What is the computational latency overhead introduced by the query decomposition and re-ranking stages during real-time code generation?
- Basis in paper: [inferred] The methodology (§4.2) requires multiple steps: an LLM call for decomposition, embedding calculations for retrieval, and a re-ranking process, all of which add complexity beyond standard retrieval.
- Why unresolved: The evaluation focuses exclusively on security rates (SR) and functional similarity (Sim) without reporting inference time or resource consumption, leaving the practical efficiency for interactive coding assistants unknown.
- What evidence would resolve it: Benchmarks measuring the end-to-end latency (in milliseconds) for code completion tasks with CodeGuarder enabled versus a baseline RACG system.

### Open Question 3
- Question: How resilient is the framework if the security knowledge base itself is compromised with misleading or adversarial guidance?
- Basis in paper: [inferred] While the paper addresses poisoning of the functional code base ($K$), it implicitly assumes the security knowledge base ($S$) is trusted (§4.1). An attacker could target the integrity of $S$ to inject "poisoned" security advice.
- Why unresolved: The defense mechanism relies on the authority of the injected security knowledge; if that knowledge is manipulated to suggest insecure patterns as fixes, the "security course" could inadvertently teach the LLM to introduce vulnerabilities.
- What evidence would resolve it: An evaluation scenario where the security knowledge base contains noisy or adversarial examples to test if the LLM blindly follows incorrect instructions.

## Limitations
- Knowledge Base Coverage Gap: Effectiveness is fundamentally bounded by the diversity and language coverage of the CVE/ReposVul corpus, with clear blind spots for CWE types lacking real-world examples.
- Model-Specific Instruction-Following Capability: Significant performance variation across model scales (16.15% vs 20.28% improvement) suggests the approach may not generalize equally across different LLM architectures.
- Context Window and Attention Trade-offs: The assumption that prioritizing high-frequency vulnerability types optimizes security improvements within limited context may not hold for all code generation scenarios, particularly those involving novel or domain-specific security patterns.

## Confidence
- **High Confidence**: The core mechanism of security knowledge injection improving code security rates (20.12% standard RACG, 31.53%/21.91% poisoning scenarios) is well-supported by controlled experiments and ablation studies.
- **Medium Confidence**: The generalization claims—particularly the 75.54% security rate improvement without target-language knowledge—are promising but based on limited cross-language evaluation.
- **Low Confidence**: The assertion that query decomposition alone improves security by ~10% (Table 11) lacks independent validation and may be confounded by other factors.

## Next Checks
- Evaluate CodeGuarder on a synthetic vulnerability corpus specifically designed to test CWE types absent from the current security knowledge base to quantify the true upper bound of improvement possible through knowledge injection versus inherent limitations of corpus coverage.
- Test CodeGuarder with a diverse set of 5-10 LLMs spanning different architectures (decoder-only, encoder-decoder, mixture-of-experts) and instruction-following capabilities to validate whether the approach's effectiveness correlates with model size alone or depends on specific architectural features.
- Conduct a controlled experiment varying the ratio of security knowledge to functional code in prompts while holding total token count constant to reveal whether there's an optimal knowledge-to-code ratio that maximizes security improvements without attention dilution.