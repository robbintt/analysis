---
ver: rpa2
title: 'COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality
  Estimation'
arxiv_id: '2509.09014'
source_url: https://arxiv.org/abs/2509.09014
tags:
- captions
- dataset
- translation
- quality
- urdu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COCO-Urdu, a large-scale Urdu image-caption
  dataset derived from MS COCO. The dataset contains 59,000 images and 319,000 Urdu
  captions, created through stratified sampling and machine translation.
---

# COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation

## Quick Facts
- **arXiv ID:** 2509.09014
- **Source URL:** https://arxiv.org/abs/2509.09014
- **Authors:** Umair Hassan
- **Reference count:** 40
- **Primary result:** Introduces COCO-Urdu, the largest Urdu image-caption dataset (59K images, 319K captions), validated via a hybrid multimodal quality estimation framework.

## Executive Summary
This paper presents COCO-Urdu, a large-scale Urdu image-caption dataset derived from MS COCO. The dataset was created by translating 319,000 English captions to Urdu using SeamlessM4T v2, followed by a rigorous quality estimation pipeline combining semantic and visual metrics. A hybrid ensemble of COMET-Kiwi, BERTScore, and CLIP-based visual grounding is used to filter and refine low-quality translations. The resulting dataset achieves high automatic quality scores (BLEU 0.53, SacreBLEU 53, chrF 74) and is positioned as a resource to reduce language bias in multimodal research.

## Method Summary
COCO-Urdu was constructed by first applying iterative stratification to select a 50% subset of MS COCO images (59K) and their captions (319K). English captions were translated to Urdu using SeamlessM4T v2. A multimodal quality estimation framework then evaluated each caption using three metrics: COMET-Kiwi for translation quality, BERTScore with back-translation for semantic consistency, and a custom CLIP-based score for visual grounding. These were combined into a weighted hybrid score (0.4 COMET + 0.4 BERT + 0.2 CLIP). Captions scoring below 0.7 were iteratively refined using Qwen 14B to improve fluency and accuracy.

## Key Results
- COCO-Urdu contains 59,000 images and 319,000 Urdu captions, making it the largest Urdu captioning dataset to date.
- The hybrid quality estimation framework successfully filtered and refined low-quality translations, with 3,572 captions automatically improved.
- Automatic evaluation metrics show strong translation quality: BLEU 0.53, SacreBLEU 53, chrF 74.
- The methodology is positioned as a generalizable framework for creating high-quality datasets for other low-resource languages.

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Ensemble Quality Estimation
A weighted ensemble of semantic (COMET-Kiwi, BERTScore) and visual (CLIP) metrics provides a more robust signal for filtering low-quality machine translations than single-metric approaches. The combination covers distinct failure modes—fluency without accuracy and visual hallucinations—by aggregating normalized scores with weights (w_COMET=0.4, w_BERT=0.4, w_CLIP=0.2). Evidence comes from the paper's internal benchmarks, though specific validation of this weighting scheme for Urdu is limited.

### Mechanism 2: Relative Visual Grounding via Back-Translation
Using a relative CLIP score comparing back-translated text against original text mitigates bias from poorly grounded source captions. This approach isolates translation quality by penalizing captions that degrade alignment relative to the source, assuming back-translation preserves semantic content. The specific "relative" scoring mechanism is a unique engineering choice not widely validated in the provided corpus.

### Mechanism 3: Iterative LLM Refinement Loop
Targeted refinement of the bottom 1% of captions (by Hybrid Score) using Qwen 14B yields measurable improvements in aggregate dataset quality. The LLM acts as a corrector for fluency and semantic errors, removing the "long tail" of bad data. The efficacy depends on the LLM's Urdu capability, with the risk of introducing new hallucinations if the model "reward hacks" the quality metrics.

## Foundational Learning

- **Concept:** Stratified Sampling for Multi-Label Data
  - **Why needed here:** Ensures the 50% subset of MS COCO preserves the frequency of rare object classes.
  - **Quick check question:** Why is "iterative stratification" necessary when creating a subset of a multi-label dataset like MS COCO, compared to simple random sampling?

- **Concept:** Reference-Free Quality Estimation (QE)
  - **Why needed here:** Enables evaluation of 319,000 generated captions without human reference translations.
  - **Quick check question:** How does a metric like COMET-Kiwi estimate translation quality without a "gold standard" human translation to compare against?

- **Concept:** Cross-Modal Alignment (CLIP)
  - **Why needed here:** Detects if a translation is fluent but visually incorrect (hallucinations).
  - **Quick check question:** Why is cosine similarity between image and text embeddings a viable proxy for "correctness" in image captioning?

## Architecture Onboarding

- **Component map:** MS COCO (Images + English Captions) -> SeamlessM4T v2 (Translation) -> QE Evaluator (COMET-Kiwi, BERTScore with Back-Translator, CLIP) -> Qwen 14B (Refiner) -> COCO-Urdu Dataset.
- **Critical path:** Translation and subsequent QE calculation are bottlenecks, requiring forwarding passes through large transformer models for 319k samples.
- **Design tradeoffs:** Semantic metrics (COMET/BERT 0.8) are weighted higher than visual grounding (CLIP 0.2), prioritizing translation fidelity over visual fit. Automation is favored over human evaluation for scale.
- **Failure signatures:**
  - High BLEU, Low CLIP: Translation is linguistically accurate to a visually misaligned source.
  - Semantic Drift: Back-translation matches the image, but the actual Urdu text does not.
- **First 3 experiments:**
  1. Run the translation + QE pipeline on 100 random samples; manually verify captions flagged with HybridScore < 0.7 are genuinely low quality.
  2. Inspect CLIP-score distribution; identify cases where back-translation improves alignment and determine if improvements are real or artifacts.
  3. Take 50 low-scoring captions, run through Qwen 14B refiner, and compare "Before" and "After" using human judgment to ensure no hallucinations are introduced.

## Open Questions the Paper Calls Out

1. **Generalization to Other Low-Resource Languages:** Can the hybrid QE pipeline generalize effectively to other low-resource languages without language-specific threshold recalibration? The thresholds were set empirically for Urdu, and it is unclear if they transfer to languages with different structures.

2. **Bias in Machine-Generated References:** To what extent do machine-generated references (NLLB-3B) bias the reported benchmark metrics compared to human references? High scores may reflect similarity to NLLB-3B outputs, which may share systematic biases with the translation model.

3. **Cultural Nuances and Linguistic Constructs:** Does the QE pipeline adequately capture Urdu-specific cultural nuances and complex linguistic constructs? The validation relies heavily on semantic metrics, which may miss subtle cultural misalignments.

4. **Fine-Tuning vs. Zero-Shot Performance:** How do state-of-the-art multilingual vision-language models perform zero-shot on COCO-Urdu compared to models fine-tuned specifically on this dataset? The actual utility of the dataset for fine-tuning versus zero-shot benchmarking has not been established.

## Limitations

- **No Human Evaluation:** The paper lacks human evaluation data, particularly from native Urdu speakers, to validate caption accuracy, fluency, and cultural appropriateness.
- **Propagation of Source Bias:** The reliance on English source captions from MS COCO propagates any visual misalignment or cultural bias present in the original dataset.
- **Transparency in LLM Refinement:** The Qwen 14B refinement step lacks transparency in prompt engineering and may introduce stylistic artifacts without human oversight.

## Confidence

**High Confidence:**
- The technical pipeline for translation, QE computation, and refinement is clearly specified and reproducible.
- The hybrid QE framework combining COMET-Kiwi, BERTScore, and CLIP is a valid engineering approach.
- Reported metric scores are internally consistent with stated thresholds.

**Medium Confidence:**
- The effectiveness of the 0.4/0.4/0.2 metric weighting scheme for Urdu is based on internal benchmarks without comparison to alternatives.
- The CLIP-based relative scoring mechanism is innovative but not independently validated.
- The claim that this is the "largest" Urdu captioning dataset is plausible but not comprehensively surveyed.

**Low Confidence:**
- The quality of final captions regarding cultural appropriateness and avoidance of subtle biases cannot be assessed without human evaluation.
- The long-term utility of the dataset for Urdu-specific research is uncertain without validation of caption relevance to downstream tasks.

## Next Checks

1. **Human Quality Audit:** Recruit 3-5 native Urdu speakers to rate 200 random captions (100 pre-refinement, 100 post-refinement) on a 5-point scale for fluency, accuracy, and cultural appropriateness. Compare scores to automated metrics.

2. **Source Caption Bias Analysis:** Manually inspect 50 captions where CLIP score indicates poor visual grounding. Determine if the error originates from the MS COCO source or the Urdu translation.

3. **Downstream Task Validation:** Fine-tune a standard vision-language model (e.g., BLIP-2) on COCO-Urdu and evaluate on a small Urdu-specific image retrieval or VQA task. Compare performance to models trained on English MS COCO to assess the practical value of the Urdu adaptation.