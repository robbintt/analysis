---
ver: rpa2
title: 'Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery
  in Large Language Models'
arxiv_id: '2509.23108'
source_url: https://arxiv.org/abs/2509.23108
tags:
- imagery
- reasoning
- mental
- llms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study tested whether large language models (LLMs) can perform\
  \ mental imagery tasks traditionally thought to require visual imagery. Researchers\
  \ created 60 novel mental imagery tasks\u201448 newly designed items and 12 adapted\
  \ from prior work\u2014where participants followed written instructions to transform\
  \ imagined shapes and letters and identify the resulting object."
---

# Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.23108
- **Source URL:** https://arxiv.org/abs/2509.23108
- **Reference count:** 40
- **Primary result:** Top LLMs (GPT-5, o3/o3-Pro) outperformed humans on mental imagery tasks, suggesting language-based reasoning rather than pictorial processing.

## Executive Summary
This study investigates whether large language models can perform mental imagery tasks traditionally thought to require visual imagery. Researchers created 60 novel mental imagery tasks and tested both human subjects and multiple LLMs. The results showed that the best models significantly outperformed humans, while image-aided reasoning actually decreased performance in some cases. These findings challenge the assumption that mental imagery tasks require pictorial mental representations and suggest that language-based propositional reasoning can be sufficient for such tasks.

## Method Summary
The study employed 60 mental imagery instruction sets (48 novel, 12 from Finke et al. 1989) where participants followed written instructions to transform imagined shapes and letters and identify resulting objects. Human subjects (N=100) provided a baseline performance of 54.7%. SOTA LLMs (OpenAI o3/GPT-5, Claude, Gemini) were tested with reasoning tokens enabled. Models were evaluated using a weighted average of expert and crowd-sourced ratings comparing outputs to canonical visual labels. Performance was tested in both single-context (all items in one prompt) and multiple-context (separate prompts) formats.

## Key Results
- Top LLMs (GPT-5, o3, o3-Pro) significantly outperformed humans (67.0%, 64.1%, and 66.6% vs 54.7%, p < .00001)
- Image-aided reasoning generally decreased model performance, with o3 showing a significant drop when images were added
- Higher reasoning effort (more inference-time tokens) led to improved results
- Multimodal models showed more variable performance, with text-only approaches generally superior for these tasks

## Why This Works (Mechanism)

### Mechanism 1: Propositional Spatial Simulation
- **Claim:** LLMs solve visual imagery tasks by manipulating abstract, language-like symbols representing spatial relations rather than rendering mental pictures
- **Mechanism:** Models maintain a latent "state vector" representing current configuration, updating it using learned rules of spatial logic derived from text
- **Core assumption:** Spatial relationships can be fully captured and manipulated via discrete linguistic propositions
- **Evidence anchors:** Abstract suggests "language-based reasoning rather than pictorial processing"; discussion notes models "extract required spatial relations through textual token manipulation"

### Mechanism 2: Inference-Time Compute Scaling
- **Claim:** Performance scales with allocation of "reasoning tokens" allowing models to decompose and verify intermediate steps
- **Mechanism:** High reasoning effort generates hidden chain-of-thought sequences that simulate construction step-by-step
- **Core assumption:** Models possess capability to effectively self-correct during extended reasoning phase
- **Evidence anchors:** Abstract states "strongest performance when models allocate greater amounts of reasoning tokens"; results show higher reasoning effort improves performance

### Mechanism 3: Cross-Modal Interference
- **Claim:** Visual generation capabilities disrupt efficiency of pure language-based reasoning for this task type
- **Mechanism:** Image generation commits to specific pixel configurations early, potentially feeding errors back into linguistic reasoner
- **Core assumption:** Visual generation pipeline introduces more noise than purely linguistic simulation
- **Evidence anchors:** Abstract notes "adding image generation capabilities generally decreased model performance"; results show o3's performance decreased with images

## Foundational Learning

- **Concept:** **Propositional vs. Pictorial Representation**
  - **Why needed here:** Central debate the paper investigates—whether "thinking" about a shape differs from "seeing" it in mind's eye
  - **Quick check question:** Can you describe a "b" rotated 90 degrees left without visualizing, just by knowing its components and rules?

- **Concept:** **Aphantasia**
  - **Why needed here:** LLMs serve as model for "aphantasic" cognition—systems performing imagery tasks without visual experience
  - **Quick check question:** If someone can't visualize an apple but describes its shape from memory, are they using "imagery"?

- **Concept:** **Reasoning Effort (Inference Compute)**
  - **Why needed here:** Critical variable—paper shows "amount of thinking" (tokens generated) better predicts success than model size
  - **Quick check question:** Does a model perform better when answering immediately or when "thinking through" intermediate steps?

## Architecture Onboarding

- **Component map:** Input instructions → LLM (Reasoning Tokens + Temperature) → Text Label → Weighted expert/crowd evaluation
- **Critical path:** 
  1. Feed instructions with low temperature (0.1) for consistency
  2. Enable "High" reasoning effort for internal simulation
  3. Avoid image generation tools; rely on text-to-text pipeline
- **Design tradeoffs:**
  - Text-Only vs. Multimodal: Text-only superior for abstract compositional reasoning (64.1% vs 55.3% for o3 with images)
  - Reasoning vs. Speed: High reasoning effort increases latency/cost but causally linked to performance gap
- **Failure signatures:**
  - Hyper-Literalism: Outputs letter descriptions instead of object (e.g., "bd" vs "glasses")
  - Instruction Drift: Loses track of orientation/position across multi-step tasks
  - Visual Anchor Trap: Hallucinates features in generated images contradicting text instructions
- **First 3 experiments:**
  1. Baseline Validation: Run Finke et al. items on text-only vs reasoning model to verify "reasoning token" advantage
  2. Interference Test: Force high-performing model to generate intermediate images and measure accuracy drop
  3. Complexity Scaling: Create 6-step instruction sets to find "context break point" where reasoning models fail

## Open Questions the Paper Calls Out

1. **Mechanistic interpretability:** Can methods reveal whether LLMs use learned iconic representations or distinct spatial imagery capacities? The study relies on closed-weight models preventing direct inspection of internal mechanisms.

2. **Complexity scaling:** How does LLM performance scale when task complexity exceeds human working memory limits? Current experiment restricted complexity to accommodate human baselines.

3. **Strategy comparison:** Are aphantasic human strategies functionally similar to LLM processing? While aphantasics performed well, the study didn't analyze specific cognitive or algorithmic strategies employed by both groups.

## Limitations

- Model access and reproducibility: Specific model versions (e.g., `gpt-5-2025-08-07`) may not be publicly available, making exact reproduction difficult
- Evaluation methodology opacity: Exact rubric and criteria for scoring are not fully specified, creating uncertainty about replicating nuanced scoring system
- Uncontrolled variables: Paper doesn't fully account for differences in how models handle different types of spatial transformations

## Confidence

- **High Confidence (80-100%):** Core finding that top LLMs outperform humans is well-supported with statistical significance (p < .00001)
- **Medium Confidence (50-80%):** Claim that LLMs use propositional reasoning is well-supported by results but relies on inference rather than direct observation
- **Low Confidence (<50%):** Generalizability to more complex spatial reasoning tasks remains uncertain

## Next Checks

1. **Mechanistic probe experiment:** Use activation patching or interpretability techniques to examine whether models encode spatial relationships as discrete linguistic tokens or continuous spatial representation

2. **Transfer task validation:** Test whether models excelling at mental imagery also show superior performance on structurally similar but different spatial reasoning problems

3. **Human-LLM comparison protocol:** Conduct controlled study where humans receive same instructions with/without visual aids to determine whether performance gap reflects genuine representational differences