---
ver: rpa2
title: 'From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs'
arxiv_id: '2509.21984'
source_url: https://arxiv.org/abs/2509.21984
tags:
- spatial
- image
- position
- bias
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates spatial bias in large vision-language models
  (LVLMs), revealing that they produce inconsistent predictions when identical visual
  information is placed at different locations within an image. Through controlled
  probing experiments, the study demonstrates that this bias originates not from the
  vision encoder but from the unbalanced design of position embeddings in the language
  model component, specifically due to position embedding strategies like RoPE disrupting
  global information flow during cross-modal interaction.
---

# From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs

## Quick Facts
- **arXiv ID**: 2509.21984
- **Source URL**: https://arxiv.org/abs/2509.21984
- **Reference count**: 34
- **Primary result**: Proposes AGCI mechanism that reduces spatial bias in LVLMs by 40-70% through weighted global context injection, improving holistic visual understanding without retraining.

## Executive Summary
This paper investigates spatial bias in large vision-language models (LVLMs), revealing that they produce inconsistent predictions when identical visual information is placed at different locations within an image. Through controlled probing experiments, the study demonstrates that this bias originates not from the vision encoder but from the unbalanced design of position embeddings in the language model component, specifically due to position embedding strategies like RoPE disrupting global information flow during cross-modal interaction. To address this, the authors propose Balanced Position Assignment (BaPA), a simple mechanism that assigns identical position embeddings to all image tokens, promoting balanced integration of visual information. Extensive experiments show that BaPA enhances spatial robustness without requiring retraining and further boosts performance across diverse multimodal benchmarks when combined with lightweight fine-tuning. The analysis reveals that BaPA yields balanced attention, enabling more holistic visual understanding.

## Method Summary
The paper investigates spatial bias in LVLMs through a controlled probe task where identical visual content is placed at different positions within 3×3 composite images. The proposed AGCI mechanism computes a global context vector from all visual tokens, then injects this context back into each token weighted by (1−cosine_similarity) to promote balanced attention. This is implemented via LoRA fine-tuning with rank=8 and alpha=16 for 1-2 epochs. The probe dataset contains 10,000 LAION image-caption pairs with 8 distractors each, creating 90,000 samples at 840×840 resolution. Evaluation measures position-wise accuracy variance across grid positions, with downstream benchmarks including MMMU-Pro, ScienceQA, CRPE, HallusionBench, TextVQA, and POPE.

## Key Results
- Spatial bias in LVLMs causes 40-70% variance in predictions for identical visual content at different image locations
- AGCI reduces position-wise variance by 40-70% across multiple LVLM architectures
- BaPA mechanism improves downstream benchmark performance by 1-3% when combined with fine-tuning
- The bias originates from language model position embeddings rather than vision encoder

## Why This Works (Mechanism)
The spatial bias stems from unbalanced position embeddings in LLaVA-based architectures that create location-dependent attention patterns. When identical visual content appears at different positions, position embeddings cause the model to attend differently to the same features. AGCI addresses this by computing a global context vector and injecting it back into each token with weights inversely proportional to cosine similarity, effectively normalizing attention patterns. BaPA takes this further by assigning identical position embeddings to all image tokens, eliminating location-based bias at the source. This promotes balanced attention and more holistic visual understanding across the entire image.

## Foundational Learning
- **Vision-language integration**: How visual features from the encoder are transformed and integrated with language models through adapters/projectors. *Why needed*: Understanding this flow is crucial for identifying where spatial bias enters the system. *Quick check*: Verify that visual tokens pass through both vision and language components before final prediction.
- **Position embeddings**: Mathematical representations that encode spatial location information in transformer models, with strategies like RoPE affecting global context flow. *Why needed*: The core hypothesis attributes spatial bias to position embedding design. *Quick check*: Confirm that different position embedding strategies (absolute vs. relative) affect attention patterns differently.
- **Cross-modal attention**: The mechanism by which language models attend to visual tokens, creating the interaction between textual and visual information. *Why needed*: Spatial bias manifests through differential attention to identical features at different locations. *Quick check*: Measure attention weights for identical tokens placed at different positions.

## Architecture Onboarding

**Component Map**: Image tokens → Vision Encoder → Projector/Adapter → Position Embeddings → LLM → Output

**Critical Path**: The spatial bias originates in the position embedding stage where RoPE-based embeddings create location-dependent representations, disrupting the global context flow during cross-modal attention. This affects how the LLM attends to visual tokens regardless of their actual content.

**Design Tradeoffs**: Using identical position embeddings (BaPA) eliminates spatial bias but may reduce the model's ability to reason about absolute positions when that capability is genuinely needed. The weighting parameter λ in AGCI requires careful tuning - too high causes over-injection and degraded fine-grained recognition, too low provides insufficient bias mitigation.

**Failure Signatures**: If LoRA is applied to wrong modules (e.g., only language layers), AGCI adaptation fails completely. Incorrect λ selection causes either persistent spatial bias (λ too low) or degraded performance on position-sensitive tasks like TextVQA (λ too high).

**Three First Experiments**:
1. Implement AGCI on a small LVLM and measure position-wise variance reduction on the 3×3 grid probe task
2. Compare attention patterns for identical visual tokens at different positions with and without AGCI
3. Evaluate downstream performance on TextVQA to verify that position-sensitive tasks don't degrade with AGCI

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled 3×3 grid probe task may not generalize to real-world images with more complex spatial variations
- The mechanistic explanation that RoPE "disrupts global information flow" requires further theoretical validation
- Optimal λ values may vary across different LVLM architectures and task types

## Confidence

**High confidence**: The empirical demonstration that position embeddings create location-dependent attention patterns in LLaVA-based models, supported by systematic variance measurements across grid positions.

**Medium confidence**: The claim that AGCI improves holistic visual understanding through balanced attention, as downstream benchmark improvements could stem from general fine-tuning effects rather than spatial bias mitigation specifically.

**Low confidence**: The assertion that position embedding strategies like RoPE "disrupt global information flow" - this mechanistic explanation requires further theoretical or empirical validation beyond the presented correlation analyses.

## Next Checks

1. **Ablation of AGCI parameter λ**: Systematically evaluate AGCI performance across λ ∈ [0.1, 0.3, 0.5, 0.7, 0.9] on the probe task to determine sensitivity and identify optimal values for different LVLM architectures, establishing whether the reported λ=0.3/0.5 settings are robust choices.

2. **Cross-bias validation**: Test whether AGCI's improvements on the 3×3 grid probe task generalize to other spatial reasoning benchmarks with different spatial bias patterns (e.g., object detection at varying scales, reasoning about relative object positions in natural scenes) to verify that the mechanism addresses spatial bias broadly rather than optimizing for a specific task design.

3. **Vision encoder isolation test**: Conduct controlled experiments using the same positional injection strategy (AGCI) but with vision encoders from different architectures (e.g., SigLIP, EVA) to determine whether observed improvements are indeed due to language model position embedding issues rather than vision-language integration mechanisms.