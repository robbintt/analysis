---
ver: rpa2
title: On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order
  Optimization
arxiv_id: '2510.19953'
source_url: https://arxiv.org/abs/2510.19953
tags:
- fpxq
- gradient
- zeroth-order
- estimators
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the bias problem in zeroth-order optimization
  (ZOO) gradient estimators. Existing ZOO methods are inherently biased unless perturbation
  stepsizes vanish, which leads to poor variance control.
---

# On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization

## Quick Facts
- arXiv ID: 2510.19953
- Source URL: https://arxiv.org/abs/2510.19953
- Authors: Shaocong Ma; Heng Huang
- Reference count: 40
- Key outcome: Proposed unbiased gradient estimators eliminate bias in ZOO while maintaining O(d/ε⁴) optimal complexity for smooth non-convex objectives

## Executive Summary
This paper addresses the fundamental bias problem in zeroth-order optimization (ZOO) gradient estimators. Traditional ZOO methods are inherently biased unless perturbation stepsizes vanish, which leads to poor variance control and suboptimal convergence. The authors propose a novel family of unbiased gradient estimators by reformulating directional derivatives as telescoping series and sampling from carefully designed distributions. They derive four specific constructions (P1-P4 estimators) with varying numbers of function evaluations, providing both theoretical guarantees and practical improvements for ZOO applications.

## Method Summary
The authors reformulate directional derivatives as telescoping series and sample from carefully designed distributions to construct unbiased gradient estimators. They propose four specific constructions (P1-P4 estimators) that use different numbers of function evaluations. The P2-P4 estimators achieve variance scaling of O(d‖∇f(x)‖² + d³µ²) when optimal perturbation stepsizes and sampling distributions are chosen. The framework eliminates bias while maintaining the same variance as classical two-point estimators, enabling optimal O(d/ε⁴) complexity for smooth non-convex objectives.

## Key Results
- The P1 estimator can have infinite variance under certain conditions, making it unsuitable for practical use
- For P2-P4 estimators, variance scales as O(d‖∇f(x)‖² + d³µ²) with optimal parameters
- Achieves optimal complexity of O(d/ε⁴) gradient evaluations for smooth non-convex objectives
- Experiments show superior performance on synthetic tasks and language model fine-tuning with faster convergence and higher final accuracy

## Why This Works (Mechanism)
The proposed method works by reformulating directional derivatives as telescoping series, which allows for unbiased estimation through careful sampling from designed distributions. By breaking down the gradient estimation into a series of directional differences, the method eliminates the inherent bias present in traditional two-point estimators. The key insight is that while each term in the telescoping series is biased, their combination produces an unbiased estimate. The variance control is achieved through optimal choice of perturbation stepsizes and sampling distributions, balancing the trade-off between bias elimination and variance minimization.

## Foundational Learning
1. **Zeroth-order optimization (ZOO)** - optimization using only function evaluations without gradient information; needed because many real-world problems lack explicit gradients; quick check: verify gradient estimates converge to true gradients as samples increase
2. **Directional derivatives** - rate of change of function in specific directions; needed as building blocks for gradient estimation; quick check: ensure directional derivatives can be computed via finite differences
3. **Telescoping series** - series where consecutive terms cancel out; needed to reformulate gradient estimation problem; quick check: verify telescoping property holds for chosen perturbation distribution
4. **Variance scaling with dimension** - how estimator variance grows with problem dimension; needed to ensure scalability; quick check: verify O(d) scaling empirically on high-dimensional problems
5. **Bias-variance trade-off** - fundamental trade-off in estimator design; needed to understand why unbiased estimators are valuable; quick check: compare convergence speed of biased vs unbiased methods
6. **Smoothness parameter (µ)** - measures how differentiable the objective function is; needed for variance bound derivation; quick check: verify smoothness assumption holds for test problems

## Architecture Onboarding

**Component Map:**
Perturbation distribution design -> Telescoping series formulation -> Unbiased gradient estimation -> Variance optimization -> Convergence analysis

**Critical Path:**
The critical path flows from perturbation distribution design through telescoping series formulation to unbiased gradient estimation. The key bottleneck is choosing optimal perturbation distributions that minimize variance while maintaining unbiasedness. The convergence analysis depends on successfully achieving low variance through proper distribution design.

**Design Tradeoffs:**
The main design tradeoff is between the number of function evaluations and estimator quality. P1 requires minimal evaluations but suffers from infinite variance, making it impractical. P2-P4 require more evaluations but provide better variance control and practical utility. The choice of perturbation distribution involves balancing computational cost against variance reduction.

**Failure Signatures:**
- P1 estimator showing erratic or divergent behavior due to infinite variance
- Poor convergence rates indicating suboptimal perturbation distribution choice
- Numerical instability in telescoping series computation
- Dimension-dependent performance degradation suggesting variance scaling issues

**First 3 Experiments:**
1. Compare convergence rates of P2-P4 estimators vs classical two-point estimator on quadratic functions
2. Test variance scaling empirically as dimension d increases from 10 to 1000
3. Evaluate performance on non-convex logistic loss with varying condition numbers

## Open Questions the Paper Calls Out
None

## Limitations
- The P1 estimator's infinite variance property suggests potential instability in the framework
- Theoretical guarantees rely on specific assumptions about smoothness parameter µ and optimal perturbation distributions
- Experimental validation uses limited test problems, primarily synthetic tasks and language model fine-tuning
- The framework assumes smooth objectives and may not extend well to non-smooth or constrained optimization problems

## Confidence
- Theoretical variance bounds for P2-P4 estimators: High
- P1 estimator's infinite variance property: Medium (requires more rigorous proof)
- Practical performance improvements: Medium (limited experimental scope)
- Optimal complexity achievement: Medium (theoretical but needs broader validation)

## Next Checks
1. Test the P2-P4 estimators on high-dimensional problems (d > 1000) to verify if the O(d) variance scaling holds in practice and to assess computational feasibility
2. Compare the proposed estimators against recent biased ZOO methods with adaptive stepsizes to quantify the practical trade-off between bias and variance control
3. Evaluate the estimators on non-smooth and constrained optimization problems to determine the robustness of the telescoping series reformulation beyond the smooth objective assumption