---
ver: rpa2
title: On the Computational Complexity of Performative Prediction
arxiv_id: '2601.20180'
source_url: https://arxiv.org/abs/2601.20180
tags:
- point
- complexity
- points
- stable
- strategic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We established a sharp computational phase transition for performative\
  \ stability: computing an \u03F5-performatively stable point is PPAD-hard even when\
  \ \u03C1 = 1 + O(\u03F5), but becomes tractable when \u03C1 = 1 + O(\u03F5\u2074\
  ) via an ellipsoid-based algorithm with poly(d, log(1/\u03F5)) complexity. This\
  \ identifies an acute transition from intractability (equivalent to finding Nash\
  \ equilibria in general-sum games) to efficient computation."
---

# On the Computational Complexity of Performative Prediction

## Quick Facts
- arXiv ID: 2601.20180
- Source URL: https://arxiv.org/abs/2601.20180
- Reference count: 40
- Primary result: Sharp computational phase transition for performative stability

## Executive Summary
This paper establishes fundamental computational barriers in performative prediction, identifying a sharp phase transition between tractable and intractable regimes. The authors prove that finding an ϵ-performatively stable point is PPAD-hard even with minimal performative effects (ρ = 1 + O(ϵ)), but becomes computationally tractable when ρ = 1 + O(ϵ⁴) through an ellipsoid-based algorithm. This transition from PPAD-hardness to polynomial-time computability highlights the acute computational challenges in learning under distribution shifts induced by model deployment. The work also extends to strategic classification, proving PLS-hardness for finding strategic local optima.

## Method Summary
The authors employ complexity-theoretic reductions and algorithmic design to characterize the computational landscape of performative prediction. They establish PPAD-hardness through reductions from general-sum games, showing that finding performative stable points is computationally equivalent to finding Nash equilibria. For the tractable regime, they develop an ellipsoid-based algorithm that leverages the structure of the problem when performative effects are sufficiently small (ρ = 1 + O(ϵ⁴)). The algorithm achieves poly(d, log(1/ϵ)) complexity, contrasting sharply with the intractability in the regime where ρ = 1 + O(ϵ). Additionally, they prove unconditional query lower bounds requiring exponentially many empirical risk minimization (ERM) evaluations, and establish PLS-hardness for strategic classification problems.

## Key Results
- Sharp computational phase transition: PPAD-hard when ρ = 1 + O(ϵ), tractable when ρ = 1 + O(ϵ⁴)
- Ellipsoid-based algorithm with poly(d, log(1/ϵ)) complexity for small performative effects
- Exponential query lower bounds for ERM evaluations to find stable points
- PLS-hardness for finding strategic local optima in strategic classification

## Why This Works (Mechanism)
The computational hardness arises from the inherent game-theoretic structure of performative prediction, where the optimal model parameters and data distribution form a coupled system. When performative effects are minimal (ρ close to 1), the problem becomes equivalent to finding fixed points in general-sum games, which is PPAD-complete. The tractability threshold at ρ = 1 + O(ϵ⁴) emerges because the problem's geometry becomes sufficiently well-behaved for ellipsoid methods to converge efficiently. For strategic classification, the PLS-hardness stems from the non-convex optimization landscape created by strategic agents adapting their features to maximize utility given the classifier.

## Foundational Learning
- **Performative prediction**: Learning paradigm where model deployment changes the data distribution. Needed to understand modern ML systems where predictions influence outcomes. Quick check: Verify if the problem exhibits distribution shift upon model deployment.
- **PPAD complexity class**: Class of problems reducible to finding fixed points. Needed to establish computational hardness of finding stable points. Quick check: Confirm reduction from known PPAD-complete problems.
- **PLS complexity class**: Class of local optimization problems. Needed to characterize hardness of finding local optima in non-convex landscapes. Quick check: Verify PLS-completeness through appropriate reductions.
- **Ellipsoid method**: Polynomial-time algorithm for convex optimization. Needed to achieve efficient computation in the tractable regime. Quick check: Verify convexity and boundedness of the feasible region.
- **Nash equilibria**: Solution concept for general-sum games. Needed to establish PPAD-hardness via game-theoretic reductions. Quick check: Confirm equivalence between performative stability and game-theoretic equilibria.
- **Strategic classification**: Setting where agents adapt features based on classifier. Needed to extend results to strategic ML scenarios. Quick check: Verify strategic adaptation model satisfies problem assumptions.

## Architecture Onboarding

**Component map**: Performative prediction problem → Optimization algorithm → Stability verification → Query complexity analysis

**Critical path**: Problem formulation → Hardness reduction → Algorithm design → Query complexity proof → Strategic extension

**Design tradeoffs**: The paper trades off computational tractability against expressiveness of the performative model. By focusing on quadratic losses and affine distribution shifts, they achieve precise complexity characterizations but may miss behaviors in more general settings. The choice between PPAD-hardness results (general but intractable) and ellipsoid-based algorithms (restricted but efficient) represents a fundamental tradeoff in algorithm design for performative prediction.

**Failure signatures**: The ellipsoid-based algorithm may fail to converge when performative effects exceed the tractable threshold (ρ > 1 + O(ϵ⁴)). PPAD-hardness reductions indicate that no polynomial-time algorithm can find exact stable points in the hard regime, though approximate solutions might still be useful in practice. Strategic classification algorithms may get stuck in suboptimal local minima due to PLS-hardness.

**First experiments**:
1. Implement the ellipsoid-based algorithm and test convergence rates on synthetic performative prediction problems with varying ρ values
2. Empirically verify the PPAD-hardness reduction by attempting to solve the constructed general-sum games
3. Test strategic classification algorithms on benchmark datasets to observe PLS-hardness manifestations

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions beyond its technical contributions.

## Limitations
- Analysis assumes quadratic losses and affine distribution shifts, potentially limiting generalizability
- PPAD-hardness reduction relies on general-sum game constructions that may not directly translate to practical applications
- Query complexity lower bounds may not fully capture practical efficiency of heuristic approaches in specific domains

## Confidence

**Confidence Levels:**
- PPAD-hardness and tractability results: **High**
- Query complexity lower bounds: **Medium**
- Strategic classification PLS-hardness: **High**
- Phase transition sharpness: **Medium**

## Next Checks

1. Implement the ellipsoid-based algorithm and test convergence on synthetic and real-world performative prediction problems with varying ρ values
2. Empirically verify the query complexity lower bounds by comparing against practical optimization heuristics
3. Extend the PPAD-hardness reduction to alternative performative prediction formulations beyond quadratic losses