---
ver: rpa2
title: 'Federate the Router: Learning Language Model Routers with Sparse and Decentralized
  Evaluations'
arxiv_id: '2601.22318'
source_url: https://arxiv.org/abs/2601.22318
tags:
- federated
- client
- client-local
- router
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning routing policies for
  language model (LM) selection in a federated setting where clients possess private,
  fragmented evaluation data across heterogeneous query distributions and sparse model
  coverage. The authors propose federated training frameworks for both parametric
  MLP-Router and nonparametric K-Means-Router, enabling collaborative learning of
  routing policies from decentralized data without centralizing sensitive query-model
  evaluations.
---

# Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations

## Quick Facts
- arXiv ID: 2601.22318
- Source URL: https://arxiv.org/abs/2601.22318
- Reference count: 40
- Key outcome: Federated learning of LM routing policies from decentralized, heterogeneous, and sparse evaluation data while preserving privacy

## Executive Summary
This paper addresses the challenge of learning effective routing policies for language model selection in federated settings where clients possess private, fragmented evaluation data across heterogeneous query distributions and sparse model coverage. The authors propose federated training frameworks for both parametric MLP-Router and nonparametric K-Means-Router, enabling collaborative learning of routing policies without centralizing sensitive query-model evaluations. The work establishes theoretical convergence guarantees for federated optimization and demonstrates through experiments that federated routers consistently outperform client-local baselines while preserving data privacy.

## Method Summary
The paper introduces two federated learning frameworks for LM routing: federated MLP-Router and federated K-Means-Router. Both frameworks enable clients to collaboratively learn routing policies from decentralized evaluation data without sharing raw query-model performance records. The federated optimization approach aggregates local router updates across clients while maintaining privacy guarantees. The method addresses the challenge of sparse and heterogeneous evaluation data by leveraging cross-client collaboration to improve coverage and routing performance. Theoretical analysis establishes convergence guarantees for the federated optimization process, proving that federated training reduces routing suboptimality compared to client-local approaches by improving effective query and model coverage.

## Key Results
- Federated routers consistently outperform client-local baselines on both global and client-specific test distributions across two benchmarks
- Federated training reduces routing suboptimality by improving effective query and model coverage
- Adaptive personalization further improves robustness under extreme heterogeneity conditions

## Why This Works (Mechanism)
The federated approach works by enabling clients to collaboratively learn routing policies from their combined evaluation data while preserving privacy. By aggregating router updates across clients rather than raw evaluation data, the method leverages the collective knowledge of multiple clients to overcome individual limitations in query coverage and model availability. The theoretical convergence guarantees ensure that the federated optimization process will find effective routing policies, while the decentralized nature of the approach maintains privacy by design. The adaptive personalization mechanism allows routers to specialize for individual client distributions while still benefiting from cross-client knowledge transfer.

## Foundational Learning
- **Federated Learning**: Distributed machine learning where clients collaboratively train models without sharing raw data; needed to preserve privacy while leveraging distributed evaluation data
- **Language Model Routing**: Selecting optimal LMs for specific queries based on learned policies; critical for efficient multi-model deployment
- **Nonparametric vs Parametric Routing**: Different approaches to modeling routing policies; nonparametric methods use explicit examples while parametric methods learn generalized functions
- **Query-Distribution Heterogeneity**: Variation in query patterns across clients; fundamental challenge addressed by the federated approach
- **Model Coverage Sparsity**: Limited availability of evaluations across different model-query pairs; addressed through collaborative learning

## Architecture Onboarding

**Component Map**
Clients -> Local Router Training -> Federated Aggregation -> Global Router -> Routing Decisions

**Critical Path**
Query arrives at client → Client router evaluates routing policy → Selected LM executes query → Performance recorded locally → Federated aggregation updates global router

**Design Tradeoffs**
- Privacy preservation vs. routing accuracy: Federated approach sacrifices some accuracy for privacy guarantees
- Communication overhead vs. model quality: More frequent aggregation improves quality but increases communication costs
- Parametric vs. nonparametric complexity: MLP-Router requires less storage but may generalize less well than K-Means-Router

**Failure Signatures**
- Degraded routing performance when client heterogeneity is extreme
- Convergence issues when client participation is unstable
- Privacy breaches if aggregation mechanisms are compromised

**First Experiments**
1. Compare federated vs. client-local routing performance on synthetic benchmark with controlled heterogeneity
2. Evaluate convergence behavior under varying client participation rates
3. Test privacy guarantees by attempting to reconstruct evaluation data from router updates

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation primarily focuses on controlled synthetic data distributions rather than real-world scenarios
- Federated aggregation relies on assumptions about client participation that may not hold in practice
- Computational overhead of federated training and impact on end-to-end system performance are not thoroughly explored

## Confidence

**High Confidence:**
- Theoretical convergence guarantees for federated optimization
- Privacy preservation through data decentralization
- Effectiveness of federated training in improving coverage

**Medium Confidence:**
- Generalization of results to real-world heterogeneous distributions
- Scalability to large numbers of clients and models
- Practical feasibility of aggregation mechanisms

**Low Confidence:**
- Long-term stability of learned routing policies
- Impact on overall system latency
- Robustness to adversarial clients or Byzantine failures

## Next Checks
1. Conduct experiments on real-world LM routing datasets with naturally occurring query distributions and model coverage patterns to validate the approach beyond synthetic settings.
2. Implement and evaluate the federated aggregation mechanisms in a realistic federated learning environment with varying client participation rates and network conditions.
3. Perform ablation studies to quantify the trade-offs between routing accuracy, privacy preservation, and computational overhead in practical deployment scenarios.