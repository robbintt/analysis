---
ver: rpa2
title: Reactive Aerobatic Flight via Reinforcement Learning
arxiv_id: '2505.24396'
source_url: https://arxiv.org/abs/2505.24396
tags:
- aerobatic
- control
- flight
- trajectory
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning-based framework for
  achieving extreme aerobatic flight maneuvers with quadrotors. The key innovation
  is an end-to-end policy that directly maps drone states and aerobatic intentions
  to control commands, eliminating the traditional separation between trajectory optimization
  and tracking control.
---

# Reactive Aerobatic Flight via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2505.24396
- **Source URL**: https://arxiv.org/abs/2505.24396
- **Reference count**: 24
- **Primary result**: Autonomous continuous inverted flight through a moving gate achieved via end-to-end RL policy trained with automated curriculum learning and domain randomization

## Executive Summary
This paper presents a reinforcement learning framework for extreme aerobatic quadrotor flight, achieving the first autonomous continuous inverted flight through a moving gate. The approach eliminates the traditional separation between trajectory optimization and tracking control by training an end-to-end policy that directly maps drone states and aerobatic intentions to control commands. An automated curriculum learning strategy dynamically adjusts task difficulty during training, while domain randomization enables robust zero-shot sim-to-real transfer. Real-world experiments demonstrate superior performance compared to traditional methods, with position and attitude errors at aerobatic points of 0.35m and 10.2° respectively.

## Method Summary
The method trains a neural network policy using PPO to output body rates and thrust directly from state observations, bypassing traditional trajectory optimization. An automated curriculum strategy generates training states by backward integration from goal waypoints using differential flatness, progressively increasing difficulty based on the agent's proficiency. Domain randomization randomizes drag coefficients, control inputs, and simulates system latency by averaging network outputs over past intervals. The policy operates at 100 Hz, directly mapping relative waypoint positions, drone state, and last action to collective thrust and body rates.

## Key Results
- First autonomous continuous inverted flight through a moving gate demonstrated
- Position and attitude errors at aerobatic points: 0.35m and 10.2°
- Maximum tracking error: 0.63m position and 16.7° attitude even with gates moving at 3.4m/s
- Outperforms traditional modular pipelines in both accuracy and responsiveness

## Why This Works (Mechanism)

### Mechanism 1: End-to-End Control Mapping
Mapping aerobatic intentions directly to low-level control commands reduces latency and tracking error compared to traditional modular pipelines. Traditional methods separate trajectory optimization (planning) and tracking (control), introducing computational latency and model-reality discrepancies. By training a neural network to output body rates and thrust directly from state observations, the system bypasses SE(3) trajectory optimization, enabling reactive, high-frequency control (100 Hz).

### Mechanism 2: Automated Curriculum via Biased Resets
Dynamically adjusting training task difficulty via reset state selection enables stable convergence in sparse reward environments. Aerobatic rewards are sparse (only given upon passing a waypoint). The Biased Reset-Based Training Strategy uses states near the goal (estimated via differential flatness) early in training, progressively shifting to harder, more distant states as the agent improves.

### Mechanism 3: Latency-Aware Domain Randomization
Randomizing physical parameters and explicitly modeling system latency during training enables zero-shot sim-to-real transfer. The approach randomizes drag coefficients (±50%) and control inputs (±20%), and simulates latency by averaging network outputs over past intervals, forcing the policy to be robust to temporal misalignment.

## Foundational Learning

- **Differential Flatness**: Essential for generating the curriculum by computing backwards from goal states to create intermediate reset states for training. Quick check: Can you explain how to derive a quadrotor's attitude (quaternion) solely from its acceleration and velocity vectors?
- **Body Rate Control**: The policy outputs angular velocities rather than attitude or motor speeds, mimicking human pilot input. Quick check: Why might body rate control be more robust to sim-to-real gaps than direct motor speed control?
- **Quaternion Kinematics**: Aerobatic flight involves large attitude angles (e.g., inverted flight). Quick check: How do you measure the "attitude error" between two quaternions, as required by the reward function?

## Architecture Onboarding

- **Component map**: Observation vector → MLP (512 → 512 → 256 → 128) → Tanh output → Action vector (thrust + body rates) → PPO update
- **Critical path**: Generating Seed Points → Backward integration using Differential Flatness → Curriculum Buffer → Policy Update (PPO)
- **Design tradeoffs**: Action Space chosen as Collective Thrust + Body Rates over Motor Speeds for better sim-to-real robustness. Observation Frame uses relative body coordinates for waypoints to improve generalization.
- **Failure signatures**: Oscillation near waypoints indicates reward function smoothness penalty is weighted too low. Inverted flight crashes suggest latency randomization range didn't cover test hardware delay.
- **First 3 experiments**: 1) Static Overfit: Train on single static gate to verify network capacity. 2) Curriculum Ablation: Train without Algorithm 1 to test convergence. 3) Latency Sensitivity: Test deployed model with artificial latency injection.

## Open Questions the Paper Calls Out

- Can generative AI autonomously synthesize aerobatic intentions that surpass human-designed trajectories in complexity or efficiency? The conclusion states future work will utilize generative AI to autonomously generate aerobatic intention without human pre-specification.
- Can the policy maintain extreme aerobatic accuracy using only onboard visual-inertial state estimation? Current real-world validation relied on external Motion Capture System rather than onboard sensors.
- Does policy performance stabilize or degrade entirely when tracking aerobatic waypoints moving significantly faster than 3.4 m/s? Training capped velocities at 2.0 m/s, testing stopped at 3.4 m/s.

## Limitations

- Zero-shot sim-to-real transfer success depends heavily on specific randomization ranges that may not cover all real-world disturbances
- Automated curriculum assumes states closer to goal are always easier, which may not hold for maneuvers requiring precise energy management
- Latency-averaging approach represents a heuristic rather than principled solution to temporal misalignment

## Confidence

- **High**: End-to-end control mapping reduces latency compared to traditional pipelines
- **Medium**: Automated curriculum enables stable convergence in sparse reward environments
- **Medium**: Domain randomization enables robust zero-shot sim-to-real transfer

## Next Checks

1. **Curriculum Robustness**: Test the curriculum strategy on different aerobatic maneuvers (e.g., Immelmann turn) to verify backward integration generates feasible states across varied flight patterns
2. **Latency Sensitivity Analysis**: Systematically vary real-world latency outside trained range [4ms, 36ms] to quantify policy's robustness to temporal misalignment
3. **Reward Function Ablation**: Train with individual reward components disabled to quantify their contribution to stable aerobatic performance