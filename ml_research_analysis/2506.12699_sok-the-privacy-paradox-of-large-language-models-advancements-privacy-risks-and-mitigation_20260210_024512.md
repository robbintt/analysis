---
ver: rpa2
title: 'SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks,
  and Mitigation'
arxiv_id: '2506.12699'
source_url: https://arxiv.org/abs/2506.12699
tags:
- privacy
- data
- arxiv
- llms
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This SoK identifies four categories of privacy challenges in LLM\
  \ systems: training data, prompts, outputs, and agents. While prior work focused\
  \ on training data privacy, we highlight underexplored areas\u2014privacy risks\
  \ from user interactions and LLM outputs."
---

# SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation

## Quick Facts
- arXiv ID: 2506.12699
- Source URL: https://arxiv.org/abs/2506.12699
- Reference count: 40
- This SoK identifies four categories of privacy challenges in LLM systems: training data, prompts, outputs, and agents. While prior work focused on training data privacy, we highlight underexplored areas—privacy risks from user interactions and LLM outputs. We analyze the effectiveness of mitigation strategies, noting that many are nascent and face trade-offs between privacy and utility. The study calls for adaptive, unified frameworks to address the evolving privacy risks unique to LLMs.

## Executive Summary
This systematic literature review categorizes privacy challenges in LLM systems into four domains: training data, prompts, outputs, and agents. The analysis reveals that while training data privacy has received substantial attention, risks from user interactions and LLM outputs remain underexplored. The study evaluates existing mitigation strategies, finding that most face significant trade-offs between privacy protection and model utility. The authors argue for the development of adaptive, unified frameworks that can dynamically respond to evolving privacy threats across all four domains.

## Method Summary
The authors conducted a PRISMA-guided systematic review across ACM, IEEE Xplore, Springer, ScienceDirect, and Google Scholar databases. They identified 128 relevant papers published after 2022 through keyword searches including "privacy challenges in LLM OR ChatGPT," "privacy in LLM prompts," and "privacy in LLM agents." Papers were categorized into four privacy domains: training data (n=51), prompts (n=24), outputs (n=8), and LLM agents (n=24), with additional papers covering legal/copyright/bias (n=9), survey papers (n=14), copyright data (n=12), and responsible LLM (n=7). The review excluded papers from non-A/A*/B venues, security-only papers, and technical modifications without novel contributions.

## Key Results
- Privacy challenges in LLMs span four categories: training data, prompts, outputs, and agents, with each requiring distinct mitigation approaches
- Data memorization enables downstream privacy attacks including extraction and inference attacks on training data
- LLM agents autonomously interacting with external tools expand the attack surface beyond traditional privacy boundaries
- Most existing mitigation strategies face inherent trade-offs between privacy protection and model utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data memorization is a root cause that enables downstream privacy attacks (extraction, inference) in LLM training data.
- Mechanism: LLMs retain specific training data verbatim during training; adversaries then exploit this memorization via targeted prompts or gradient analysis to extract PII or infer membership.
- Core assumption: The model architecture and training process do not effectively suppress verbatim memorization of rare or sensitive sequences.
- Evidence anchors:
  - [abstract] The paper frames privacy challenges around training data where memorization and leakage are key.
  - [section 3.1.1] Explicitly states: "Data memorization is one of the primary causes of privacy risks, where models inadvertently retain and reproduce sensitive information from training data."
  - [corpus] Corpus papers on "Semantic Privacy in LLMs" and "Beyond Data Privacy" corroborate that memorization is a core concern; however, none provide a formal proof of elimination.
- Break condition: Mitigation (e.g., deduplication, differential privacy) reduces memorization to negligible levels, or the model is provably not overfit to sensitive data.

### Mechanism 2
- Claim: LLMs' inference capabilities can derive sensitive personal attributes from seemingly innocuous user prompts, bypassing traditional PII redaction.
- Mechanism: LLMs leverage contextual understanding and world knowledge to infer location, health status, or demographics from indirect cues (e.g., "hook turn" → Melbourne), even when direct PII is removed.
- Core assumption: The LLM's world knowledge and reasoning capacity are sufficient to connect non-sensitive contextual clues to sensitive attributes.
- Evidence anchors:
  - [section 3.2.2] Provides the "hook turn" example and cites studies (e.g., [107]) demonstrating high-accuracy attribute inference from anonymized text.
  - [abstract] Highlights "privacy risks from user interactions" as an underexplored area.
  - [corpus] Related work on "Semantic Privacy in LLMs" supports the idea that contextual inference is a distinct privacy vector beyond data privacy.
- Break condition: Anonymization techniques successfully obscure all inferable attributes, or the LLM lacks the knowledge/context to make the inference.

### Mechanism 3
- Claim: LLM agents' autonomous interaction with external tools and APIs expands the attack surface by propagating sensitive data beyond the user's control.
- Mechanism: A user prompt with sensitive data is processed by an agent that autonomously calls third-party tools, APIs, or other agents, potentially leaking data across systems without explicit user awareness or consent.
- Core assumption: The agent's autonomy is not sufficiently constrained by access controls, monitoring, or task-level privacy policies.
- Evidence anchors:
  - [section 3.4.3] Describes how agents "autonomously initiates queries in places without user awareness or inadvertently shares this information."
  - [figure 2] Illustrates the workflow of LLM agents interacting with external tools and other agents.
  - [corpus] Paper on "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents" directly addresses this agent-mediated data propagation risk.
- Break condition: Agent frameworks enforce strict data minimization, audit trails, and user-in-the-loop approvals for all external interactions.

## Foundational Learning
- Concept: **Differential Privacy (DP)**
  - Why needed here: DP is repeatedly cited as a mitigation for training data leakage and inference attacks, but the paper notes trade-offs in utility.
  - Quick check question: Why does adding noise to gradients (DP) potentially reduce an LLM's generation quality?
- Concept: **Membership Inference Attacks (MIA)**
  - Why needed here: MIA is a key attack vector for determining if data was in the training set, especially relevant for fine-tuned LLMs.
  - Quick check question: In an MIA, what does the attacker compare to decide if a sample was in the training set?
- Concept: **Contextual Integrity**
  - Why needed here: The paper argues that using data outside its original context (e.g., scraping social media for LLM training) violates privacy norms even if de-identified.
  - Quick check question: How does contextual integrity differ from traditional PII-based privacy definitions?

## Architecture Onboarding
- Component map: The LLM privacy landscape is divided into four interacting domains: (1) Training Data (memorization, attacks), (2) User Prompts (direct leakage, inference), (3) LLM Outputs (regurgitation, uncontrolled dissemination), and (4) LLM Agents (autonomous tool use, data propagation). Mitigations (e.g., NER, DP, unlearning) are applied at different stages (pre-training, training, inference).
- Critical path: Sensitive Data → Memorization/Ingestion → (Inference Time) User Prompt/Context → LLM Processing → Output or Agent Action → Leakage to Adversary/Third-Party. The most severe risks arise when memorized data is accessed via inference attacks or when agents autonomously exfiltrate data.
- Design tradeoffs: Privacy vs. Model Utility (DP reduces accuracy), Computational Overhead (FHE, ensembling), and User Friction (input sanitization, approval steps for agents). No single mitigation addresses all four challenge categories.
- Failure signatures: (1) NER-based redaction fails to catch inferable attributes (e.g., location from dialect), (2) DP-protected models still leak via sophisticated prompt engineering (jailbreaking), (3) Agents execute tasks that inadvertently include sensitive data in tool calls despite user intent.
- First 3 experiments:
  1. Test the effectiveness of deduplication on memorization rates in a small-scale LLM (measure via canary extraction).
  2. Compare attribute inference success rates from prompts sanitized with NER-only vs. an adversarial feedback-guided anonymizer.
  3. Simulate a multi-agent workflow with a mock third-party tool and audit the data payload passed to the tool under different access restriction policies.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can adaptive privacy mechanisms be designed to dynamically assess conversation context and adjust LLM responses in real-time to trigger enhanced protections like obfuscation or user alerts?
- Basis in paper: [explicit] The authors state that "future research should also focus on developing adaptive privacy mechanisms that dynamically assess conversation context and adjust LLM responses accordingly."
- Why unresolved: Current mitigation strategies are often static or isolated, lacking the capability to process context dynamically during multi-turn interactions.
- What evidence would resolve it: A framework demonstrating real-time context evaluation that successfully triggers specific privacy defenses (e.g., filtering, alerts) without significant latency or performance degradation.

### Open Question 2
- Question: How can defense mechanisms designed for static tasks (e.g., text classification) be effectively adapted to prevent sensitive information leakage in dynamic, generative LLM outputs?
- Basis in paper: [inferred] The paper notes that current output privacy methods (like instance obfuscation) are "designed mainly for static tasks" and "do not adequately address the generative nature of LLM outputs."
- Why unresolved: The generative and probabilistic nature of LLMs makes it difficult to apply rigid privacy controls used in classification tasks without stifling utility or fluency.
- What evidence would resolve it: Empirical validation of a generative-specific defense method that reduces PII leakage in open-ended text generation while maintaining semantic utility comparable to unprotected models.

### Open Question 3
- Question: How can lightweight, local LLMs be optimized to detect sensitive information that is inferable from contextual cues but missed by standard Named Entity Recognition (NER) tools?
- Basis in paper: [inferred] The authors highlight that "existing rule-based or NER-based redaction tools often prove inadequate" against LLM inference capabilities and suggest using smaller local LLMs as a potential, yet unrefined, solution.
- Why unresolved: Local models currently require "significant computational power" or domain-specific knowledge that users may lack, making them an impractical replacement for standard NER in many scenarios.
- What evidence would resolve it: A lightweight local model architecture capable of running on standard consumer hardware that outperforms NER in detecting inferable attributes (e.g., location from dialect) with high accuracy.

### Open Question 4
- Question: What architectural frameworks can minimize the high computational and human labor costs associated with supervision-based privacy protection in autonomous LLM agents?
- Basis in paper: [explicit] The paper concludes that "designing scalable, adaptive frameworks with minimal human involvement remains critical" for LLM agents, as current methods rely on "significant investment in human labor and computational resources."
- Why unresolved: Existing supervision methods (like ToolEmu or AgentMonitor) are effective but unsustainable at scale due to resource intensity and the "long-tail nature" of agent risks.
- What evidence would resolve it: An agent framework that autonomously enforces privacy constraints with performance metrics comparable to human-supervised systems but at a fraction of the computational cost.

## Limitations
- The classification of 128 papers into four privacy domains may be subjective given the overlapping nature of LLM privacy challenges
- The analysis does not provide quantitative benchmarks for comparing mitigation effectiveness across different privacy threat models
- The paper relies on reported findings without independent validation of attack efficacy or mitigation performance
- Analysis of emerging threats like agent-mediated data propagation lacks empirical studies demonstrating real-world exploit scenarios

## Confidence
- **High Confidence**: Data memorization as a root cause of training data privacy risks (supported by multiple prior studies and direct citations)
- **Medium Confidence**: User prompts enabling attribute inference beyond traditional PII (mechanism is well-theorized but effectiveness varies by inference task and context)
- **Medium Confidence**: LLM agents expanding attack surface through autonomous tool use (conceptually sound but limited empirical validation of real-world impact)
- **Low Confidence**: The assertion that current mitigations face "inherent trade-offs" between privacy and utility without quantifying these trade-offs across domains

## Next Checks
1. **Reproduce deduplication impact**: Measure memorization rates in a small LLM before and after deduplication using canary sequence extraction, comparing against claims in cited memorization studies
2. **Validate inference attack transferability**: Test whether context-based attribute inference (e.g., location from dialect) succeeds against prompts sanitized with NER-only vs. adversarial-guided methods using a standardized inference benchmark
3. **Audit agent data propagation**: Simulate a multi-agent workflow with mock external APIs and trace data payloads under different access restriction policies to quantify unintended data leakage