---
ver: rpa2
title: 'SymbioticRAG: Enhancing Document Intelligence Through Human-LLM Symbiotic
  Collaboration'
arxiv_id: '2505.02418'
source_url: https://arxiv.org/abs/2505.02418
tags:
- user
- retrieval
- symbioticrag
- document
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents SymbioticRAG, a framework addressing two core
  limitations in Retrieval-Augmented Generation systems: the inherently human nature
  of relevance determination and users'' difficulty formulating queries when unaware
  of knowledge gaps. The approach introduces a bidirectional learning relationship
  between humans and machines through interactive document exploration and personalized
  retrieval models based on user interactions.'
---

# SymbioticRAG: Enhancing Document Intelligence Through Human-LLM Symbiotic Collaboration

## Quick Facts
- **arXiv ID:** 2505.02418
- **Source URL:** https://arxiv.org/abs/2505.02418
- **Authors:** Qiang Sun; Tingting Bi; Sirui Li; Eun-Jung Holden; Paul Duuring; Kai Niu; Wei Liu
- **Reference count:** 7
- **One-line primary result:** Human-LLM collaboration framework significantly improves RAG relevance and user satisfaction across three scenarios.

## Executive Summary
SymbioticRAG addresses fundamental limitations in Retrieval-Augmented Generation systems by introducing a bidirectional learning relationship between humans and machines. The framework specifically targets users in the "unconscious incompetence" stage who struggle to formulate effective queries due to unawareness of knowledge gaps. Through interactive document exploration and personalized retrieval models based on user interactions, the system demonstrates substantial improvements over traditional RAG approaches. Evaluated across literature review, geological exploration, and education scenarios, SymbioticRAG achieved human-retriever distance reduction from 0.85-0.92 to 0.52-0.61 and user satisfaction score increases from 1.80-2.47 to 3.67-4.13 on a 5-point scale.

## Method Summary
The SymbioticRAG framework employs a two-level approach: Level 1 enables direct human curation of retrieved content through interactive source document exploration, while Level 2 (experimental) implements personalized retrieval using LLM-summarized user intention from interaction logs. The system processes documents using layout-aware detection (DocLayout-YOLO) to preserve bounding boxes, then extracts content through specialized modules (PaddleOCR for text, visual LLMs for figures/formulas, StructEqTable/Pix2Text for tables). User interactions are logged and fed to an LLM to generate intention summaries that augment subsequent queries. The architecture emphasizes simplicity, robustness, and precise source attribution through layout block preservation.

## Key Results
- Human-retriever distance decreased from 0.85-0.92 (NaïveRAG) to 0.52-0.61 (SymbioticRAG)
- User satisfaction scores increased from 1.80-2.47 to 3.67-4.13 on a 5-point scale
- The system will be made openly accessible to facilitate further research advancement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct human curation of retrieved content reduces the gap between algorithmic relevance and user-perceived relevance.
- **Mechanism:** Users view retrieved layout blocks within original document context via PDF viewer, then select/deselect blocks. This explicit selection creates labeled training data representing "true relevance" for personalized retrieval.
- **Core assumption:** Users can accurately judge relevance when given sufficient context, and their selections meaningfully represent information needs.
- **Evidence anchors:** Abstract mentions Level 1 enables direct human curation; results show human-retriever distance improvement from 0.85-0.92 to 0.52-0.61; related work on human-centered RAG evaluation supports utility-dimension frameworks.
- **Break condition:** If users cannot access source documents or selection granularity doesn't match mental models, curation quality degrades.

### Mechanism 2
- **Claim:** Query augmentation with LLM-summarized user interaction logs improves retrieval alignment by capturing latent user intent.
- **Mechanism:** User interaction logs (queries, clicks, selections, navigation, likes/dislikes) are fed to an LLM to generate "intention summary," then concatenated with raw query before embedding.
- **Core assumption:** Interaction patterns encode meaningful signal about user intent that can be distilled into natural language summaries; LLMs can reliably extract this signal.
- **Evidence anchors:** Abstract mentions Level 2 implementation via retriever strategy with LLM-summarized intention; section 3.3 describes feeding logs to LLM for intention summary generation; related papers focus on user profiles for re-ranking rather than interaction-log summarization.
- **Break condition:** If interaction logs are sparse, noisy, or dominated by exploratory behavior, intention summary may mislead retrieval.

### Mechanism 3
- **Claim:** Layout-aware document processing with preserved bounding boxes enables fine-grained retrieval and precise source attribution.
- **Mechanism:** Documents converted to PDF, processed page-by-page as images. DocLayout-YOLO detects layout blocks with bounding boxes; specialized modules extract content. Blocks are embedded individually, preserving positional information.
- **Core assumption:** Layout blocks are semantically coherent units appropriate for retrieval; bounding-box preservation adds value over merged/re-segmented representations.
- **Evidence anchors:** Section 3.1 emphasizes simplicity, robustness, and extensibility while prioritizing accurate document source tracking through layout block bounding boxes; popular open-source systems merge layout blocks losing positional information; hierarchical chunking approaches support importance of document structure awareness.
- **Break condition:** If specialized extraction modules produce low-quality output, retrieval degrades.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) fundamentals**
  - **Why needed here:** SymbioticRAG builds on standard RAG patterns and extends them with human-in-the-loop curation.
  - **Quick check question:** Can you explain why semantic similarity alone may fail to capture user-perceived relevance?

- **Concept: Four stages of competence (unconscious incompetence → conscious incompetence → conscious competence → unconscious competence)**
  - **Why needed here:** Framework explicitly targets users in "unconscious incompetence" stage who cannot formulate precise queries.
  - **Quick check question:** How would a RAG system's design differ if targeting "conscious incompetence" vs. "unconscious incompetence" users?

- **Concept: Document layout analysis and chunking**
  - **Why needed here:** System's retrieval unit is the layout block, not arbitrary text spans. Understanding layout detection, OCR, and specialized extraction is essential.
  - **Quick check question:** What information is lost when a table is converted to plain text vs. preserved as structured JSON?

## Architecture Onboarding

- **Component map:**
  1. Document Processing Pipeline: Input (PDF, DOCX, PPTX, images) → Anything2PDF → Layout Detection (DocLayout-YOLO) → OCR (PaddleOCR) + Table Extraction (StructEqTable/Pix2Text/visual LLM) + Formula Recognition (Pix2Text + visual LLM) + Figure Understanding (visual LLM) → JSON with bounding boxes, text, descriptions
  2. Retriever Module: Embedding (E5) → Vector DB → Retrieval strategies (NaïveRAG, LabelNaïveRAG, SymbioticRAG with intention summary) → Top-k layout blocks
  3. SymbioticRAG UI: PDF Viewer (annotated source display) + Chat component (multi-turn conversation, retrieval results table, LLM responses) + Staging area (human-selected blocks) + Report generation interface

- **Critical path:**
  1. Document upload and processing (dominant latency for large corpora; involves layout detection + multiple specialized extractors)
  2. Query submission → retrieval (top-k block selection) → LLM response generation
  3. User interaction (click, select/deselect blocks, regenerate) → interaction log capture → intention summary generation (Level 2)
  4. Human-on-the-loop validation (optional but recommended for high-quality extraction)

- **Design tradeoffs:**
  - Layout block granularity vs. retrieval precision: Fine-grained blocks improve precision but increase vector DB size and retrieval complexity
  - Human-on-the-loop validation vs. automation: Manual validation improves extraction quality but doesn't scale; best suited for domain-specific corpora with high stakes
  - Interaction log direct concatenation vs. LLM summarization: Direct concatenation caused retrieval to converge to already-viewed content; summarization abstracts intent but depends on LLM quality

- **Failure signatures:**
  - High human-retriever distance (>0.8): Indicates retriever consistently misses user-relevant content; diagnose by examining query quality, embedding model suitability, or chunk granularity
  - Low user satisfaction despite good retrieval metrics: May indicate UI friction, poor LLM response synthesis, or misalignment between retrieved content and user task context
  - Figure/formula extraction errors: Domain-specific notation not recognized; requires human-on-the-loop validation or specialized fine-tuning

- **First 3 experiments:**
  1. Baseline comparison: Run NaïveRAG vs. LabelNaïveRAG vs. SymbioticRAG on small corpus (10-20 documents) with 3-5 users; measure human-retriever distance and satisfaction scores
  2. Intention summary ablation: Compare retrieval quality with and without LLM-summarized intention augmentation on multi-turn conversations
  3. Domain-specific extraction validation: Process corpus with complex tables/figures; validate extraction outputs using human-on-the-loop interface; measure error rates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can user interaction data be utilized to build robust personalized retrieval models (Level 2) beyond experimental LLM-based intention summarization?
- **Basis in paper:** Paper states it focuses on "implementing the foundational feature and Level 1, experimenting and laying the groundwork for future Level 2 advancement."
- **Why unresolved:** Authors currently treat Level 2 as experimental stage using simple intention summaries, explicitly inviting "further advancement" to realize full personalized retrieval vision.
- **What evidence would resolve it:** Comparative evaluation showing fully trained Level 2 model significantly outperforms experimental intention-summary approach in Human-Retriever Distance metrics.

### Open Question 2
- **Question:** What specialized modeling approaches are required to improve figure understanding for high-complexity domains like geology compared to general-purpose visual LLMs?
- **Basis in paper:** Section 3.1 notes that while general visual LLMs are used initially, "effectiveness varies with domain complexity" and "more domain-specific solutions will be required."
- **Why unresolved:** Current system relies on general-purpose visual models which authors admit struggle with nuances of specialized scientific figures.
- **What evidence would resolve it:** Development of specialized extraction model demonstrating superior precision on geological or medical figures compared to current baseline.

### Open Question 3
- **Question:** How can raw user interaction logs be integrated into query embeddings without causing retrieval results to converge to previously viewed content?
- **Basis in paper:** Section 3.3 describes failed attempt to concatenate logs directly, resulting in content that "converges to specific contents which already exists inside the user logs."
- **Why unresolved:** Authors excluded this specific integration method from analysis because it inadvertently narrowed search space rather than enhancing it.
- **What evidence would resolve it:** New embedding strategy utilizing historical log data to broaden query context or improve relevance ranking without collapsing retrieval diversity.

## Limitations

- The effectiveness of LLM-summarized intention from interaction logs remains largely unproven, with only minimal related work in the literature
- Evaluation methodology relies on three independent evaluators per scenario, but inter-rater reliability metrics were not reported
- The paper claims Layout-aware processing provides superior source attribution, but comparative performance data against baseline chunkers is limited

## Confidence

- **High confidence:** The core architectural approach of human curation reducing human-retriever distance is well-supported by experimental results showing improvement from 0.85-0.92 to 0.52-0.61
- **Medium confidence:** The claim that Level 2 implementation (LLM-summarized intention) improves retrieval is supported by results but lacks sufficient methodological detail for full validation
- **Low confidence:** The superiority of layout-aware processing over traditional chunking approaches is asserted but not rigorously benchmarked against comparable systems

## Next Checks

1. **Intention summarization ablation study:** Compare retrieval quality with and without LLM-summarized intention augmentation on multi-turn conversations using standardized interaction log schema
2. **Inter-rater reliability assessment:** Calculate Cohen's kappa or similar metrics for three evaluators in each scenario to establish consistency and reliability of human-retriever distance measurements
3. **Layout processing benchmark:** Conduct head-to-head comparison between DocLayout-YOLO-based layout block retrieval and hierarchical chunking approaches on same document corpora, measuring both retrieval accuracy and source attribution precision