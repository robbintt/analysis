---
ver: rpa2
title: An Interpretable Benchmark for Clickbait Detection and Tactic Attribution
arxiv_id: '2509.10937'
source_url: https://arxiv.org/abs/2509.10937
tags:
- clickbait
- detection
- news
- headlines
- headline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of clickbait detection by introducing
  an interpretable two-stage framework that not only classifies headlines as clickbait
  or not, but also attributes them to specific linguistic manipulation tactics. A
  synthetic dataset was generated by augmenting real news headlines with ten predefined
  clickbait strategies using GPT-4.0, enabling controlled experimentation.
---

# An Interpretable Benchmark for Clickbait Detection and Tactic Attribution

## Quick Facts
- arXiv ID: 2509.10937
- Source URL: https://arxiv.org/abs/2509.10937
- Authors: Lihi Nofar; Tomer Portal; Aviv Elbaz; Alexander Apartsin; Yehudit Aperstein
- Reference count: 0
- Primary result: Fine-tuned BERT achieves 91% F1-score for clickbait detection and 84% F1-score for tactic attribution on synthetic dataset

## Executive Summary
This study addresses the challenge of clickbait detection by introducing an interpretable two-stage framework that not only classifies headlines as clickbait or not, but also attributes them to specific linguistic manipulation tactics. A synthetic dataset was generated by augmenting real news headlines with ten predefined clickbait strategies using GPT-4.0, enabling controlled experimentation. Models evaluated included a fine-tuned BERT classifier and large language models (GPT-4.0, Gemini 2.5 Flash) under zero-shot and few-shot prompting. The fine-tuned BERT achieved the highest performance with 91% F1-score for detection and 84% F1-score for tactic attribution. LLMs performed moderately, with few-shot prompting improving results. The work advances explainable AI for clickbait detection and supports efforts to enhance media integrity and user trust.

## Method Summary
The authors present a two-stage framework for clickbait analysis. First, a headline is classified as clickbait or not using either a fine-tuned BERT model or LLM prompting. Second, if classified as clickbait, the headline is analyzed to identify which of ten predefined linguistic tactics (e.g., curiosity gap, exaggeration) were used. The key innovation is generating a synthetic dataset by prompting GPT-4.0 to augment real news headlines with these tactics, creating labeled examples for both detection and attribution tasks. The framework is evaluated using precision, recall, and F1-scores for both stages, with the fine-tuned BERT model outperforming LLM baselines.

## Key Results
- Fine-tuned BERT achieves 91% F1-score for clickbait detection
- BERT model achieves 84% F1-score for multi-label tactic attribution
- LLM performance (GPT-4.0, Gemini 2.5 Flash) improves modestly from zero-shot (F1 0.40-0.46) to few-shot (F1 0.43-0.47)
- Synthetic data generation enables controlled experimentation with perfect tactic labels

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Task Decomposition
Separating detection from tactic attribution improves interpretability without substantially degrading detection performance. Stage 1 performs binary clickbait classification; Stage 2 predicts specific tactics via a dedicated multi-label classifier. This modular design allows each stage to optimize for its objective independently.

### Mechanism 2: Synthetic Data Generation with Controlled Tactic Injection
Generating clickbait variants by programmatically applying known tactics to real headlines creates a labeled dataset that supports both detection and attribution. Real non-clickbait headlines are augmented using GPT-4.0 with prompts specifying which tactic(s) to apply. Each generated headline is annotated with the exact tactics used, providing ground truth for supervised learning.

### Mechanism 3: Fine-Tuning Outperforms Zero-Shot and Few-Shot LLM Prompting
Task-specific fine-tuning of BERT yields substantially higher F1-scores than prompting GPT-4.0 or Gemini 2.5 Flash, even with few-shot exemplars. Fine-tuning adapts model weights directly to the target task distribution, whereas prompting relies on in-context learning without weight updates.

## Foundational Learning

- **Concept**: Multi-label classification
  - Why needed here: Tactic attribution requires predicting multiple applicable strategies per headline (e.g., "Curiosity Gap" + "Emotional Triggers"), not mutually exclusive classes.
  - Quick check question: Can a single headline be tagged with both "Exaggeration" and "Direct Appeals"? If yes, what loss function would you use?

- **Concept**: Transformer fine-tuning vs. in-context learning
  - Why needed here: Understanding the performance gap between fine-tuned BERT and prompted LLMs informs architecture and deployment choices.
  - Quick check question: What is updated during fine-tuning that is not updated during few-shot prompting?

- **Concept**: Synthetic data validation
  - Why needed here: The dataset is LLM-generated; evaluating its fidelity to real clickbait distributions is critical before trusting model performance claims.
  - Quick check question: What metrics or human evaluation protocols would you use to assess whether synthetic clickbait resembles authentic clickbait?

## Architecture Onboarding

- **Component map**: Real headlines -> GPT-4.0 augmentation with tactic prompts -> labeled synthetic dataset -> Stage 1 (Detection: BERT/LLM) -> Stage 2 (Attribution: BERT multi-label) -> Evaluation metrics

- **Critical path**: 1) Define tactic catalogue (10 strategies) 2) Generate synthetic dataset with GPT-4.0 3) Train/fine-tune BERT for detection 4) Train/fine-tune BERT for multi-label tactic attribution 5) Compare against LLM baselines

- **Design tradeoffs**: Fine-tuned BERT offers highest accuracy but requires labeled data and training infrastructure; LLM zero-shot requires no training but has lowest accuracy; LLM few-shot provides moderate accuracy gain without training but needs prompt engineering.

- **Failure signatures**: Low detection F1 with fine-tuned model suggests data quality or overfitting issues; low attribution precision indicates tactic overlap problems; LLM few-shot failure suggests poor prompt design or exemplar mismatch.

- **First 3 experiments**: 1) Reproduce detection results by fine-tuning BERT on synthetic dataset 2) Ablate synthetic vs. real data by training on different sources and comparing to real-world benchmarks 3) Vary few-shot exemplar count for GPT-4.0 to identify performance curves and diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can causal inference techniques effectively disentangle overlapping persuasive tactics to improve attribution precision?
- Basis in paper: [explicit] The authors state in the conclusion, "We plan to explore causal inference techniques to separate overlapping persuasive tactics and to enhance attribution precision."
- Why unresolved: The current framework relies on standard classification techniques which may struggle to differentiate when multiple manipulative strategies co-occur in a single headline.
- What evidence would resolve it: A modified framework incorporating causal layers that demonstrates higher F1-scores in multi-label attribution tasks compared to the current BERT-based baseline.

### Open Question 2
- Question: Does the framework maintain high performance when applied to organic, human-authored clickbait rather than synthetic data?
- Basis in paper: [inferred] The methodology relies exclusively on a synthetic dataset generated by GPT-4o; the paper does not validate the model on a ground-truth dataset of human-written clickbait.
- Why unresolved: Models trained on synthetic data often overfit to the specific linguistic patterns of the generator and may fail to generalize to the noisier, more diverse distribution of real-world clickbait.
- What evidence would resolve it: Benchmarking the fine-tuned BERT classifier on an external, human-labeled dataset to verify that the 91% F1-score for detection translates to real-world scenarios.

### Open Question 3
- Question: How does incorporating multimodal signals (images, social context) impact the accuracy of detection and the quality of explanations?
- Basis in paper: [explicit] The authors specify, "Future work will broaden the dataset to include multimodal signals such as images and social context to mirror real-world use better."
- Why unresolved: The current study is text-only, yet clickbait often relies on the discrepancy between a headline and an enticing image or social metadata, which the current model cannot process.
- What evidence would resolve it: Results from an extended architecture that processes image-text pairs, showing whether visual context reduces false positives or improves the identification of "Sensationalism."

## Limitations
- The study relies entirely on synthetically generated clickbait examples, raising questions about ecological validity and generalization to authentic clickbait.
- The ten predefined tactics may have ambiguous boundaries or frequent co-occurrence, potentially causing conflation in the multi-label classifier.
- LLM baseline comparisons lack detail on prompt engineering and exemplar selection, with modest improvements suggesting potential optimization gaps.

## Confidence
- **High Confidence**: The two-stage framework design is methodologically sound and the synthetic data generation approach is well-specified.
- **Medium Confidence**: The comparative performance against LLMs is credible but could benefit from more rigorous prompt engineering and ablation studies.
- **Low Confidence**: Generalization claims to real clickbait remain speculative without validation on authentic datasets.

## Next Checks
1. Evaluate the fine-tuned BERT model on a held-out set of real clickbait headlines from established benchmarks (e.g., Webis Clickbait Challenge 2017) to measure performance degradation and assess synthetic-to-real transfer.
2. Generate confusion matrices and per-class F1 scores for the tactic attribution stage to identify tactics with high confusion rates and determine if tactic definitions need refinement.
3. Systematically vary few-shot exemplar count (1, 3, 5, 10), prompt templates, and output formatting constraints for LLMs to identify optimal prompting strategies and determine if current results reflect fundamental LLM limitations or suboptimal configuration.