---
ver: rpa2
title: Ad-Hoc Human-AI Coordination Challenge
arxiv_id: '2506.21490'
source_url: https://arxiv.org/abs/2506.21490
tags:
- card
- human
- clue
- agents
- play
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Ad-Hoc Human-AI Coordination Challenge
  (AH2AC2) to evaluate how well AI agents can coordinate with humans in the cooperative
  card game Hanabi. The authors address the difficulty of human evaluation by developing
  human proxy agents trained on a large-scale dataset of human gameplay combined with
  regularised reinforcement learning to maintain human-like behaviour.
---

# Ad-Hoc Human-AI Coordination Challenge

## Quick Facts
- arXiv ID: 2506.21490
- Source URL: https://arxiv.org/abs/2506.21490
- Reference count: 40
- Primary result: Introduces AH2AC2 benchmark for human-AI coordination in Hanabi using human proxy agents and API-gated evaluation

## Executive Summary
This paper introduces the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to evaluate how well AI agents can coordinate with humans in the cooperative card game Hanabi. The authors address the difficulty of human evaluation by developing human proxy agents trained on a large-scale dataset of human gameplay combined with regularised reinforcement learning to maintain human-like behaviour. They open-source a limited dataset to encourage data-efficient methods and host proxy agents via an API to ensure fair, reproducible evaluation. The challenge includes two evaluation regimes: coordination with human proxies and action prediction on unseen human data. Baseline results show that zero-shot coordination methods like Off-Belief Learning outperform data-dependent approaches when limited human data is available, highlighting the difficulty of human-AI coordination.

## Method Summary
The AH2AC2 challenge evaluates AI agents' ability to coordinate with humans in Hanabi using human proxy agents. The methodology involves training BC policies on human gameplay data, then refining them through HDR-IPPO with KL regularization to maintain human-like behavior. Proxy agents are hosted via an API with pre-registration requirements to prevent overfitting. The benchmark includes two evaluation tracks: coordination games with proxy agents and action prediction on unseen human data. The dataset consists of 3,079 games (1,858 two-player, 1,221 three-player) from the AC2 dataset, with an open-sourced subset of 1,858 games.

## Key Results
- Human proxy agents reduce zero-score games from 70.92% to 0.27% in 3-player Hanabi compared to BC alone
- Zero-shot coordination methods like OBL outperform data-dependent approaches when limited human data is available
- DeepSeek-R1, even without fine-tuning, falls short of human-compatible performance in the benchmark
- The controlled API evaluation prevents overfitting to proxy agents while ensuring reproducible assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral cloning followed by KL-regularized reinforcement learning produces agents that maintain human-compatible conventions while improving robustness.
- Mechanism: BC policies capture human conventions from data but are brittle in unseen states. Adding IPPO with KL regularization (HDR-IPPO) allows the policy to generalize better while constraining deviation from human-like behavior. The loss function combines (1-λ)·L_IPPO + λ·D_KL(π_BC||π_HP), keeping the policy anchored to human strategies during optimization.
- Core assumption: Human gameplay data contains learnable, consistent conventions (H-Group conventions) that can be captured via supervised learning and preserved through constrained optimization.
- Evidence anchors:
  - [Section 4.2]: "BC alone struggles to generalise to unseen game states... we then refine them through regularised SP using IPPO"
  - [Section 5.1]: Human proxies reduced zero-score games from 70.92% to 0.27% in 3-player compared to BC alone
  - [Corpus]: Weak direct corpus evidence; related work (Hu et al., 2022; Bakhtin et al., 2022) supports KL-regularized approaches in other domains
- Break condition: If regularization weight λ is too low, policies diverge from human-like play; if too high, policies cannot improve generalization (see ablation in Appendix A.8)

### Mechanism 2
- Claim: API-gated evaluation with pre-registration prevents overfitting to proxy agents and ensures reproducible human-AI coordination assessment.
- Mechanism: Proxy agents are hosted behind a controlled API rather than released publicly. Participants must pre-register experiments and receive limited evaluation runs (1,000 games). This prevents iterative tuning to proxy behavior while maintaining standardized evaluation.
- Core assumption: Overfitting to fixed evaluation partners is a meaningful failure mode that would not transfer to real human coordination.
- Evidence anchors:
  - [Abstract]: "To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly"
  - [Section 4.1]: "This controlled access ensures consistency across submission and pre-registration of experiments is the gold standard for empirical science"
  - [Corpus]: No direct corpus comparison; mechanism is procedural rather than algorithmic
- Break condition: If proxy agents do not adequately represent human behavior diversity, passing the benchmark may not transfer to real human coordination

### Mechanism 3
- Claim: Zero-shot coordination methods like Off-Belief Learning can outperform data-dependent approaches even without human data access.
- Mechanism: OBL trains agents using counterfactual reasoning grounded in shared assumptions (no arbitrary conventions), producing policies that coordinate well with any partner following similar principles. In contrast, data-dependent methods (BC, BR-BC, HDR-IPPO) with limited data cannot sufficiently capture human variation.
- Core assumption: Humans play using grounded, interpretable reasoning patterns that OBL-style methods can discover without explicit human data.
- Evidence anchors:
  - [Section 6, Table 5]: OBL (L4) achieves 21.04 mean score in 2-player without human data, outperforming BR-BC (19.41) and HDR-IPPO (12.76)
  - [Section 6]: "Methods like OP fail to achieve successful human-AI coordination, and current approaches that leverage limited human data underperform compared to state-of-the-art ZSC algorithms"
  - [Corpus]: Related work on automatic curriculum design for ZSC (arxiv 2503.07275) suggests curriculum approaches may further improve zero-shot methods
- Break condition: If human behavior requires population-specific conventions beyond grounded reasoning, OBL's advantage may not hold across all partner types

## Foundational Learning

- Concept: Dec-POMDP (Decentralized Partially Observable Markov Decision Process)
  - Why needed here: Hanabi is formally a Dec-POMDP where each agent has local observations only; understanding this framing is required to reason about policy coordination under partial observability
  - Quick check question: Can you explain why joint policy optimization differs from independent policy learning in this setting?

- Concept: Behavioral Cloning with sequential models (LSTM)