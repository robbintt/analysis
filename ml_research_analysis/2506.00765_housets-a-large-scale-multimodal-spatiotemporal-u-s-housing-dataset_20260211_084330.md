---
ver: rpa2
title: 'HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset'
arxiv_id: '2506.00765'
source_url: https://arxiv.org/abs/2506.00765
tags:
- data
- housing
- year
- forecasting
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HouseTS is a large-scale multimodal spatiotemporal dataset for
  U.S. housing market analysis, covering over 6,000 ZIP codes across 30 metropolitan
  areas from 2012 to 2023.
---

# HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset

## Quick Facts
- arXiv ID: 2506.00765
- Source URL: https://arxiv.org/abs/2506.00765
- Reference count: 40
- Key outcome: HouseTS dataset enables long-horizon U.S. housing forecasting with 16 model families, showing simple linear models often outperform complex deep architectures when proper preprocessing is applied.

## Executive Summary
HouseTS is a large-scale multimodal dataset for U.S. housing market analysis, covering over 6,000 ZIP codes across 30 metropolitan areas from 2012 to 2023. The dataset integrates monthly housing indicators, points of interest, census data, and time-stamped aerial imagery under a unified schema. Experiments show that simple regularized baselines like DLinear and ARDL are highly competitive, while preprocessing—particularly z-score normalization—is critical for neural stability. Image-derived textual annotations provide structured neighborhood change descriptions with LLM-as-judge quality control and human verification.

## Method Summary
HouseTS preprocesses heavy-tailed housing data through log transformation followed by z-score normalization, then applies time-aware imputation. The dataset supports forecasting tasks with 16 model families including statistical, deep learning, and foundation models. Experiments systematically compare these models across different input window lengths and forecast horizons, with particular attention to preprocessing effects on neural model stability.

## Key Results
- Simple linear models (DLinear, ARDL) dominate the benchmark, challenging the necessity of complex deep architectures for ZIP-level house price forecasting
- Preprocessing—particularly z-score normalization—is critical for neural stability, with models like Informer failing catastrophically without proper scaling
- VLM-derived textual annotations with disagreement-aware quality control provide interpretable neighborhood change descriptions alongside tabular forecasting

## Why This Works (Mechanism)

### Mechanism 1: Preprocessing-Induced Neural Stability
- Claim: Log-transformation paired with z-score normalization is likely a primary determinant of stability and performance for deep learning models on heavy-tailed housing data, rather than model architecture complexity alone.
- Mechanism: Housing prices and POI counts are heavy-tailed and mixed-scale. Log transformation reduces skew, and z-score normalization centers the data, preventing ill-conditioned optimization and exploding gradients in neural networks (e.g., Transformers, LSTMs).
- Core assumption: The performance gains observed are not solely due to the inductive biases of the models but significantly driven by the input data distribution alignment.
- Evidence anchors: [abstract] "preprocessing—particularly z-score normalization—is critical for neural stability." [section] Table 4 shows Informer error dropping from 0.6481 to 0.0554 Log-RMSE when z-score is added.

### Mechanism 2: Linear Model Competitiveness (DLinear/ARDL)
- Claim: Simple, regularized linear models appear to capture the majority of the signal in long-horizon ZIP-level house price forecasting, challenging the necessity of complex deep architectures for this specific task.
- Mechanism: House price dynamics at the ZIP level often exhibit strong autocorrelation and linear trends. Regularized linear models (DLinear, ARDL) effectively capture these mappings without overfitting the noise inherent in high-dimensional, limited-history data.
- Core assumption: The dataset's temporal depth (2012–2023) is sufficient for deep models to shine, yet they fail to outperform linear baselines, suggesting the underlying signal-to-noise ratio favors simpler mappings.
- Evidence anchors: [section] Section 4.2 states "simple linear structure dominates this benchmark" and DLinear achieves lowest error.

### Mechanism 3: Disagreement-Aware VLM Annotation
- Claim: Using cross-model disagreement scores acts as a reliable proxy for annotation quality in vision-language pipelines, enabling scalable filtering of noisy image-derived textual data.
- Mechanism: Different VLMs (e.g., GPT, Gemini) produce varying outputs for ambiguous images. High variance (disagreement) in structured outputs signals low visual evidence or ambiguity, triggering human review or exclusion.
- Core assumption: Consensus among diverse models correlates with ground truth visual evidence more strongly than any single model's self-reported confidence.
- Evidence anchors: [section] Figure 5 & 6 show how deviation from consensus and disagreement index stratify sample reliability.

## Foundational Learning

- Concept: **Time-Series Normalization**
  - Why needed here: The paper explicitly identifies "neural stability" as a failure mode without proper scaling (Log + Z-score).
  - Quick check question: Why would applying standard scaling (z-score) *after* a log transform prevent a Transformer from diverging on housing price data?

- Concept: **Spatiotemporal Alignment**
  - Why needed here: HouseTS integrates monthly prices, annual census data, and yearly imagery.
  - Quick check question: How do you forward-fill annual census data to match a monthly target without inducing data leakage?

- Concept: **Autoregressive Distributed Lag (ARDL)**
  - Why needed here: It is cited as a top-performing baseline, often beating complex Deep Neural Networks (DNNs).
  - Quick check question: What does the success of ARDL imply about the linearity of housing price trends in this dataset?

## Architecture Onboarding

- Component map: Data Ingestion (Zillow/Redfin + Census + NAIP) -> Preprocessing Pipeline (Log transform -> Z-score -> Forward-fill imputation) -> Forecasting Backbone (Input Encoder -> Model -> Horizon Projection) -> VLM Branch (Multi-year Image -> VLM Annotator -> LLM-Judge -> Reliability Metadata)

- Critical path: The **Preprocessing Pipeline**. The paper demonstrates that skipping z-score normalization (Table 4) causes catastrophic failure in models like Informer (Log-RMSE > 0.6). This is the most brittle part of the architecture.

- Design tradeoffs:
  - **Model Complexity vs. Robustness**: Transformers (PatchTST, Informer) offer capacity but are brittle to scaling errors; DLinear/ARDL are robust but may miss non-linear cross-ZIP interactions.
  - **Graph vs. Non-Graph**: The paper notes Graph Neural Networks (STGCN) underperformed non-graph baselines, suggesting the simple kNN graph used may not capture the true spatial dependencies.

- Failure signatures:
  - **Exploding Gradients**: Log-RMSE > 0.6 usually indicates missing z-score normalization.
  - **High MAPE on Long Horizons**: AR models degrade significantly at H=12 (Table 3), indicating compound error accumulation.
  - **VLM Hallucination**: High self-reported uncertainty but low disagreement (or vice-versa) may indicate calibration issues in the image annotation pipeline.

- First 3 experiments:
  1. **Baseline Sanity Check**: Run DLinear on the raw data (log-only) vs. (log + z-score) to replicate the stability failure mode described in Section 4.2.
  2. **Horizon Ablation**: Compare ARDL vs. TimesFM (Foundation model) on H=3 vs. H=12 to quantify the "compounding error" degradation.
  3. **Annotation Reliability Check**: Filter the VLM annotations using the disagreement index (keep low disagreement) and verify if the text descriptions align with the "Ground Truth" census changes for a small subset of ZIP codes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can richer spatial priors or learned graph structures improve the performance of spatiotemporal GNNs on housing market data?
- Basis in paper: [explicit] Section 4.2 notes that graph-based approaches underperform relative to non-graph baselines, suggesting "richer or learned spatial priors may be necessary" beyond simple fixed geographic kNN graphs.
- Why unresolved: The benchmark only evaluated a fixed geographic adjacency, leaving the potential of semantic or adaptive graph structures unexplored.

### Open Question 2
- Question: To what extent can aligned multimodal features improve performance on tasks beyond direct forecasting, such as cross-city transfer or structured-missingness imputation?
- Basis in paper: [explicit] Section 3 lists cross-city transfer and imputation as tasks enabled by the schema, but explicitly states: "We leave a systematic study of these extensions to future work."
- Why unresolved: The current study restricts its evaluation to univariate and multivariate forecasting horizons.

### Open Question 3
- Question: What specific non-linear interactions in housing markets are deep models failing to capture that allow simple linear baselines to remain dominant?
- Basis in paper: [inferred] Section 4.2 highlights that "increasing model capacity does not necessarily translate into better generalization," but does not analyze *why* Transformer-based models fail to leverage the multivariate context better than ARDL.
- Why unresolved: The paper identifies the performance gap but does not perform feature attribution or interaction analysis to explain the failure of complex architectures.

## Limitations

- Preprocessing pipeline incompletely specified, lacking exact train/test split dates and census alignment protocols
- Foundation model baselines lack sufficient hyperparameter detail for fair comparison
- VLM annotation disagreement metric primarily validated internally without external benchmarks

## Confidence

- **High Confidence**: Preprocessing-induced neural stability, DLinear/ARDL competitiveness, VLM annotation pipeline feasibility
- **Medium Confidence**: Linear model superiority generalizability, spatial graph construction adequacy, foundation model relative performance
- **Low Confidence**: VLM disagreement as universal quality proxy, specific hyperparameter choices, exact data leakage prevention mechanisms

## Next Checks

1. **Preprocessing Ablation Replication**: Systematically test DLinear performance across preprocessing variants (raw, log-only, log+z-score) to confirm the stability threshold effects reported in Table 4
2. **Cross-Dataset Generalization**: Apply the best-performing HouseTS pipeline to a held-out metropolitan area not included in training to test geographic generalizability
3. **VLM Disagreement Validation**: Manually audit a stratified sample of high vs. low disagreement annotations to empirically validate the quality proxy assumption across different image types and neighborhoods