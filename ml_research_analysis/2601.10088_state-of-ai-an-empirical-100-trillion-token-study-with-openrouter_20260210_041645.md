---
ver: rpa2
title: 'State of AI: An Empirical 100 Trillion Token Study with OpenRouter'
arxiv_id: '2601.10088'
source_url: https://arxiv.org/abs/2601.10088
tags:
- usage
- open
- figure
- tokens
- share
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study analyzes over 100 trillion tokens of LLM usage data
  from OpenRouter to empirically understand how large language models are being used
  in practice. The research reveals several key findings: 1) A significant portion
  of usage (approximately 30%) is going to open-source models, with Chinese models
  like DeepSeek and Qwen showing particularly strong growth.'
---

# State of AI: An Empirical 100 Trillion Token Study with OpenRouter

## Quick Facts
- **arXiv ID:** 2601.10088
- **Source URL:** https://arxiv.org/abs/2601.10088
- **Reference count:** 5
- **Primary result:** 100T+ token analysis revealing programming dominance, open-source adoption, agentic inference emergence, and foundational cohort retention patterns

## Executive Summary
This empirical study analyzes over 100 trillion tokens of LLM usage data from OpenRouter to understand real-world adoption patterns. The research reveals that programming has become the dominant use case, representing over 50% of tokens, driven by workflow embedding in developer toolchains. Open-source models capture approximately 30% of usage, with Chinese models showing particularly strong growth. The study identifies emerging "agentic inference" patterns where models are used for multi-step reasoning and tool use rather than simple text generation. Retention analysis reveals "foundational cohorts" where early users demonstrate persistent engagement, suggesting certain workloads find strong model fit. Usage patterns vary significantly by geography, with Asia's share growing from 13% to 31% of total spend.

## Method Summary
The study uses anonymized request-level metadata from OpenRouter (Nov 2024–Nov 2025), including token counts, model identifiers, billing geography, and tool-calling flags. Content categorization was performed via GoogleTagClassifier on a 0.25% opt-in sample with confidence threshold 0.5. Analysis was conducted using SQL aggregations on the Hex platform, focusing on market share, category distribution, retention patterns, and geographic spread. The methodology relies entirely on metadata without access to raw prompt or completion content.

## Key Results
- Programming workloads dominate at over 50% of tokens, driven by IDE integrations and expanding context windows
- Open-source models achieve 30% market share through cost-quality tradeoffs for high-volume workloads
- "Foundational cohorts" show persistent retention, with early adopters maintaining 40% activity at Month 5
- Asia's geographic share grows from 13% to 31% of total spend, reflecting expanding global adoption
- Agentic inference emerges as a distinct pattern characterized by multi-step reasoning and tool use

## Why This Works (Mechanism)

### Mechanism 1: Open-Source Adoption Through Cost-Quality Tradeoffs
- Claim: Open-source models gain market share when they achieve sufficient quality at significantly lower cost, particularly for high-volume, less-regulated workloads.
- Mechanism: Competitive OSS releases create immediate usage spikes; users adopt for cost-sensitive tasks (roleplay, lightweight coding) where proprietary safety layers add friction without proportional value. Retention depends on continuous model iteration—stagnant OSS models lose share to actively maintained alternatives.
- Core assumption: Users evaluate models along multiple axes (capability, latency, price, trust) and select based on task-specific optimization rather than universal superiority.
- Evidence anchors: [abstract] "approximately 30% [of usage] is going to open-source models"; [section 3.1] "New entrants like Qwen's models... achieved production-scale adoption within weeks of release"
- Break condition: If proprietary models reduce prices dramatically or OSS model quality stagnates relative to frontier, the cost-quality arbitrage window narrows and OSS share growth slows.

### Mechanism 2: Programming Dominance Via Workflow Embedding
- Claim: Programming overtook other categories because LLMs became embedded in developer toolchains, creating compounding utility as context windows expanded.
- Mechanism: IDE integrations + increasing context lengths → developers pass entire codebases (20K+ tokens) for debugging/generation → higher per-request token volume reinforces category dominance. Programming workloads systematically require 3-4x longer prompts than other categories.
- Core assumption: Developer tool adoption is sticky; once integrated, usage scales with development activity rather than exploratory interest.
- Evidence anchors: [abstract] "Programming has become the dominant category of usage, representing over 50% of tokens in recent months"; [section 4.3] "programming workloads are the dominant driver of prompt token growth"
- Break condition: If code-generation quality plateaus or latency costs outweigh productivity gains, developers may reduce LLM reliance, causing programming share to stabilize or decline.

### Mechanism 3: Retention Through Workload-Model Fit (Glass Slipper Effect)
- Claim: Early user cohorts that find a model solving a previously impossible problem exhibit persistent retention, even when newer models emerge.
- Mechanism: First-mover advantage on specific capability inflection (e.g., reliable tool-use, reasoning fidelity) → users embed model into pipelines and habits → switching becomes costly technically and cognitively. Later cohorts arrive after fit is established and churn at higher rates.
- Core assumption: Capability advances are discontinuous; each breakthrough creates a transient "frontier window" where foundational cohorts form.
- Evidence anchors: [abstract] "Retention analysis shows 'foundational cohorts' where early users demonstrate persistent engagement"; [section 7.1] "the June 2025 cohort of Gemini 2.5 Pro... retain approximately 40% of users at Month 5"
- Break condition: If models converge rapidly on all capability dimensions, the "frontier window" shrinks, reducing opportunity to form foundational cohorts.

## Foundational Learning

- Concept: **Agentic inference** (multi-step reasoning with tool calls)
  - Why needed here: The paper documents a shift from single-pass text generation to orchestrated workflows involving planning, tool invocation, and extended context. Understanding this distinction is essential for interpreting the rising tool-call and sequence-length metrics.
  - Quick check question: Can you explain why a request with tool definitions but no tool invocation differs from one with a tool-call finish reason?

- Concept: **Cohort retention analysis**
  - Why needed here: The Glass Slipper effect relies on comparing retention curves across user cohorts (users who started in the same time window). Without understanding cohort segmentation, the difference between "foundational" and later cohorts appears meaningless.
  - Quick check question: Why might retention measured as "activity retention" (returning after any gap) show non-monotonic bumps compared to strict consecutive-month retention?

- Concept: **Token economics (prompt vs. completion, caching effects)**
  - Why needed here: Cost-analysis in Section 8 depends on understanding blended token rates, caching-driven effective price reductions, and why prompt tokens grew faster than completion tokens in programming workloads.
  - Quick check question: If a model caches prompt prefixes across repeated queries, how does this affect the "effective cost per 1M tokens" compared to list prices?

## Architecture Onboarding

- Component map: Data layer (OpenRouter metadata) -> Classification layer (GoogleTagClassifier on 0.25% sample) -> Analysis layer (Hex platform for SQL aggregations)

- Critical path:
  1. Understand metadata schema (Section 2.1) — what fields exist and what they do NOT contain
  2. Map classification taxonomy (Section 2.2) — how raw GoogleTagClassifier outputs roll up to analysis categories
  3. Trace each figure's data source and filtering logic (e.g., category analyses only available from May 2025)

- Design tradeoffs:
  - **Privacy vs. granularity**: Opt-in classification on 0.25% sample preserves privacy but limits statistical power for low-volume categories
  - **Billing geography vs. IP location**: Billing is more stable but may misrepresent actual user location (enterprise aggregation, third-party billing)
  - **Platform bias**: OpenRouter data reflects API users, not self-hosted small-model deployments (Section 3.2 notes small-model share decline may reflect self-hosting, not actual usage decline)

- Failure signatures:
  - Interpreting category trends before May 2025 (classification fields didn't exist)
  - Conflating "tokens routed through reasoning models" with "reasoning tokens in outputs" (Figure 10 caption)
  - Treating OSS market-share changes as absolute volume changes when they may reflect relative shifts

- First 3 experiments:
  1. Replicate Figure 19 (programming share over time) using raw token counts to verify the 11% → 50% growth claim; check for discontinuities around classification rollout.
  2. Cross-reference retention curves (Figure 25) against model release dates to test whether "foundational cohort" timing aligns with specific capability launches.
  3. Analyze tool-invocation rates by category to validate whether programming workloads drive agentic adoption as implied in Section 4.5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the high cost-per-token in the "Technology" category driven by high user willingness-to-pay (demand-side) or high inference costs (supply-side)?
- Basis in paper: [explicit] Section 8.1 explicitly asks: "One key question is whether this high price is driven by high user value (a 'demand-side' opportunity) or by a high cost-of-serving (a 'supply-side' challenge)."
- Why unresolved: The study analyzes aggregated billing metadata and cannot see internal provider costs or user utility metrics.
- What evidence would resolve it: A breakdown of inference compute requirements for "Technology" queries compared to user retention/satisfaction surveys regarding value.

### Open Question 2
- Question: To what extent does the self-hosting of small models distort the observed market share decline of small models on centralized APIs?
- Basis in paper: [explicit] Section 3.2 notes: "Small models are precisely the ones most commonly self-hosted, which means a meaningful portion of their real usage is invisible to any centralized API provider."
- Why unresolved: OpenRouter functions as an API aggregator and lacks visibility into on-premise or local deployments where small models are typically used.
- What evidence would resolve it: Correlating API usage data with download statistics from open-source repositories (e.g., HuggingFace) or local deployment telemetry.

### Open Question 3
- Question: At what point will agentic inference volume permanently exceed human-driven inference volume?
- Basis in paper: [explicit] The Conclusion states: "There are many reasons to believe that agentic inference will exceed, if it hasn't already, human inference."
- Why unresolved: The study uses proxies like tool calls and sequence length to infer agentic behavior but cannot definitively distinguish automated agent loops from complex human interactions.
- What evidence would resolve it: Tracking the ratio of distinct human-initiated sessions versus automated, session-less API calls over time.

### Open Question 4
- Question: Is the "Glass Slipper" (foundational cohort) effect a permanent lock-in or a transient state that breaks with major capability leaps?
- Basis in paper: [inferred] Section 7.1 hypothesizes that early fit creates "strong lock-in effects," yet also notes the "transient" nature of the frontier window.
- Why unresolved: The observational window (~13 months) is insufficient to determine if users who found a "glass slipper" fit will churn when a significantly superior frontier model appears.
- What evidence would resolve it: Longitudinal retention analysis tracking if foundational cohorts migrate when new state-of-the-art models (e.g., GPT-5) release.

## Limitations

- Reliance on OpenRouter metadata without access to prompt/completion content limits behavioral analysis depth
- Geographic attribution via billing location may misattribute enterprise users with centralized billing
- 0.25% opt-in sample for classification may underrepresent low-volume categories or niche use cases
- Small model market share decline may reflect self-hosting migration rather than reduced usage
- Cannot definitively distinguish legitimate agentic workflows from prompt injection attempts

## Confidence

- **High Confidence:** Programming category dominance (50%+ of tokens) and geographic expansion of Asia's share (13% to 31%) are supported by direct metadata evidence
- **Medium Confidence:** Open-source model adoption (30% share) and Chinese model growth are observed but may conflate actual usage shifts with platform migration effects
- **Low Confidence:** Precise drivers of agentic inference adoption are inferred from tool-call metadata but lack direct validation of actual reasoning quality or user satisfaction

## Next Checks

1. **Validate Classification Coverage**: Cross-validate the 0.25% sample classification against a larger sample to ensure category distributions are representative, particularly for low-volume categories that may be under-sampled.

2. **Geographic Attribution Audit**: Compare billing location attribution against a subset of IP-based location data to quantify potential geographic misattribution bias, especially for enterprise billing patterns.

3. **Self-Hosting Impact Assessment**: Analyze OpenRouter's small-model market share decline against public data on local model deployment trends to estimate the extent to which observed declines reflect migration rather than reduced usage.