---
ver: rpa2
title: 'Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech
  Codecs'
arxiv_id: '2511.16639'
source_url: https://arxiv.org/abs/2511.16639
tags:
- speech
- units
- discrete
- codec
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Codec2Vec introduces a self-supervised speech representation learning
  framework that operates exclusively on discrete units generated by neural audio
  codecs. Unlike conventional SSL models that process continuous audio signals, Codec2Vec
  leverages pre-computed discrete codec units, eliminating the need for online acoustic
  feature extraction.
---

# Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs

## Quick Facts
- **arXiv ID**: 2511.16639
- **Source URL**: https://arxiv.org/abs/2511.16639
- **Reference count**: 35
- **Primary result**: A self-supervised speech representation learning framework that operates on discrete codec units, achieving competitive SUPERB benchmark performance while reducing storage by 16.5× and training time by 2.3× compared to continuous-input models.

## Executive Summary
Codec2Vec introduces a self-supervised speech representation learning framework that operates exclusively on discrete units generated by neural audio codecs. Unlike conventional SSL models that process continuous audio signals, Codec2Vec leverages pre-computed discrete codec units, eliminating the need for online acoustic feature extraction. Using masked prediction objectives with various target derivation strategies, including reconstruction-based targets, iterative clustering, and online clustering, Codec2Vec learns contextualized speech representations directly from compressed audio data. Evaluated on the SUPERB benchmark, Codec2Vec achieves competitive performance compared to strong continuous-input baselines, demonstrating the feasibility of using discrete codec units for general-purpose speech processing. The framework also delivers substantial efficiency gains, reducing data storage requirements by up to 16.5× and accelerating pre-training by up to 2.3×, making it a scalable and resource-efficient approach for building speech foundation models.

## Method Summary
Codec2Vec processes discrete codec units (e.g., from DAC) as input, bypassing the need for acoustic feature extraction. It uses a BASE-sized Transformer encoder with a masked prediction objective, where masked spans of discrete tokens are predicted. The model supports three target derivation strategies: reconstruction-based (predicting original codec units), iterative clustering (k-means on latent representations), and online clustering (EMA-based teacher-student setup). The embeddings are initialized from the codec's codebook weights, and quantizer dropout is applied during training to improve robustness.

## Key Results
- Achieves competitive performance on SUPERB benchmark tasks compared to continuous-input baselines.
- Reduces data storage requirements by up to 16.5× (from 60.4 GB to 3.6 GB for LibriSpeech-960h).
- Accelerates pre-training by up to 2.3× by eliminating the convolutional feature extractor.
- Outperforms reconstruction-based targets with iterative and online clustering strategies on PR and ASR tasks.

## Why This Works (Mechanism)

### Mechanism 1: Discrete Input Efficiency
By replacing continuous waveforms with pre-computed discrete codec units, Codec2Vec eliminates the computationally expensive convolutional feature encoders typically required for raw audio. This reduces the data footprint and removes a major computational bottleneck from the training loop, yielding substantial efficiency gains.

### Mechanism 2: Contextualizing via Masked Prediction
Standard masked prediction objectives transfer effectively to the discrete audio domain, forcing the model to learn high-level contextual relationships rather than just smoothing local spectral continuity. This enables the model to capture semantic and phonetic patterns from compressed acoustic features.

### Mechanism 3: Target Abstraction via Clustering
Iterative and online clustering targets outperform direct codec reconstruction by forcing the model to group similar acoustic states, filtering out irrelevant acoustic noise and focusing on consistent linguistic units. This abstraction step helps the model learn more general representations than raw codec indices optimized for reconstruction.

## Foundational Learning

- **Neural Audio Codecs (RVQ)**: Understanding Residual Vector Quantization—how audio is compressed into multiple streams of discrete integers—is critical for interpreting the model's embedding layer. Quick check: Can you explain how a codec represents a single frame of audio using a tuple of indices from multiple codebooks?

- **Masked Language Modeling (MLM) Objective**: This is the training signal. Unlike contrastive learning, this relies on predicting masked tokens, treating speech like a text sequence. Quick check: How does predicting a discrete token differ mathematically from predicting a continuous vector?

- **K-Means Clustering & EMA**: The paper’s strongest results come from non-reconstruction targets. Understanding how targets are generated—either offline via K-means or online via EMA updates—is essential for reproducing the "Iterative" and "Online" variants. Quick check: In the "Online Clustering" mode, how does the "teacher" model generate targets for the "student" without external labels?

## Architecture Onboarding

- **Component map**: Input Layer (summation of codebook embeddings) -> Positional Encoding -> BASE Transformer Encoder -> Prediction Heads (Reconstruction or Clustering)

- **Critical path**: Data Preparation (extracting codec units is the bottleneck) -> Embedding Init (initializing with codec codebook weights yields gains) -> Target Derivation (implementing K-means or EMA module correctly is the "secret sauce")

- **Design tradeoffs**: Storage vs. Fidelity (16.5× storage efficiency but inherit codec quantization errors) vs. Training Speed vs. Info (2.3× faster but ASR may lag continuous SOTA)

- **Failure signatures**: Stagnant Loss (Reconstruction: loss drops but downstream performance poor) -> Codebook Collapse (Online: EMA parameters wrong, online codebook collapses) -> Codec Mismatch (Using EnCodec instead of DAC yields lower performance)

- **First 3 experiments**: 1. Codec Extraction Verification (extract units, decode back to audio, verify intelligibility) -> 2. Target Strategy Ablation (train three tiny models comparing raw reconstruction, fixed k-means targets, random targets) -> 3. Embedding Initialization Check (compare random vs. codec-codebook initialization on small pre-training run)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a neural audio codec be specifically optimized for self-supervised representation learning rather than reconstruction? The authors state identifying or developing the optimal codec remains an open research question.

- **Open Question 2**: How can SSL objectives be tailored for discrete input sequences to close the performance gap with continuous-input models on tasks like ASR? The authors note current mainstream SSL pre-training strategies were predominantly developed for continuous input signals.

- **Open Question 3**: To what extent does noise robustness observed in discrete codec units transfer to the SSL model, and how does error propagation from the codec affect performance in real-world noisy environments? The authors acknowledge they have not extensively evaluated performance in real-world noisy conditions.

## Limitations
- Generalization Gap to Non-LibriSpeech Domains: Validated exclusively on LibriSpeech-derived SUPERB tasks with limited cross-corpus validation.
- Codec Dependency: Superior performance demonstrated with DAC codec; EnCodec shows consistent underperformance.
- Limited Continuous Baseline Comparison: Shows competitive performance but lacks ablation studies isolating discrete-input advantage.
- Unspecified Quantizer Dropout: Exact implementation details and probability are unspecified.

## Confidence

**High Confidence**: Claims regarding computational efficiency gains (16.5× storage reduction, 2.3× training speedup) are directly supported by quantitative comparisons.

**Medium Confidence**: Claims about masked prediction effectiveness in discrete domain are supported by methodology and literature, though specific evidence for codec units is limited.

**Medium Confidence**: Claims about clustering-based targets outperforming reconstruction are well-supported by Table I results, though mechanism could benefit from deeper analysis.

**Low Confidence**: Claims about general-purpose applicability beyond LibriSpeech are weakly supported given narrow evaluation scope.

## Next Checks

1. **Cross-Codec Validation**: Reproduce pre-training using both DAC and EnCodec on identical subsets to quantify performance gap and identify whether framework or codec choice drives difference.

2. **Cross-Domain Generalization**: Evaluate Codec2Vec on non-LibriSpeech dataset (e.g., Common Voice or VoxCeleb) for at least two SUPERB tasks to assess domain transfer capability.

3. **Continuous Input Ablation**: Implement continuous-input version of Codec2Vec architecture (same Transformer, same objectives) to isolate contribution of discrete inputs versus other architectural innovations.