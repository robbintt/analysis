---
ver: rpa2
title: Population Synthesis using Incomplete Information
arxiv_id: '2510.00859'
source_url: https://arxiv.org/abs/2510.00859
tags:
- data
- population
- wgan
- training
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of training population synthesis
  models on incomplete microsample data, where missing values can occur due to privacy
  concerns or data collection constraints. The proposed method uses a Wasserstein
  Generative Adversarial Network (WGAN) with a mask matrix to represent missing values,
  allowing the model to learn effectively from incomplete training data.
---

# Population Synthesis using Incomplete Information

## Quick Facts
- arXiv ID: 2510.00859
- Source URL: https://arxiv.org/abs/2510.00859
- Reference count: 11
- Primary result: WGAN with mask matrix successfully trains on incomplete microsample data for population synthesis, matching performance of complete-data models

## Executive Summary
This study addresses the challenge of training population synthesis models on incomplete microsample data, where missing values can occur due to privacy concerns or data collection constraints. The proposed method uses a Wasserstein Generative Adversarial Network (WGAN) with a mask matrix to represent missing values, allowing the model to learn effectively from incomplete training data. The approach was validated using Swedish national travel survey data, comparing models trained on complete data versus those trained on datasets with varying levels of missing information.

## Method Summary
The method employs a WGAN-GP architecture with modifications to handle incomplete data through mask matrix multiplication. A binary mask Y (0=missing, 1=present) is multiplied with generator output before discriminator evaluation, constraining feedback to observed values only. The model includes regularization terms (RBD, RAD) to control the tradeoff between generating sampling zeros (desirable novel combinations) and structural zeros (implausible combinations). Training uses standard WGAN-GP hyperparameters with λbd=10 and λad=1 for regularization.

## Key Results
- Proposed method successfully generates synthetic populations that closely match the performance of models trained on complete data
- Attribute-level evaluation metrics (category coverage, TV complement, and category adherence) were high across all models
- Higher-dimensional joint distribution analysis demonstrated comparable performance despite some structural zero generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mask matrix multiplication enables WGAN training on incomplete data by constraining discriminator feedback to observed values only
- Mechanism: A binary mask Y (0=missing, 1=present) is multiplied with generator output before discriminator evaluation, ensuring the discriminator loss is computed only on non-missing attributes
- Core assumption: The generator will learn to produce reasonable imputations for missing positions through the joint distribution learned from observed positions
- Evidence anchors: [section 3] "The biggest difference from the original WGAN training algorithm is in line 7... where the matrix generated by the generator, G(Z) is multiplied by mask Y"

### Mechanism 2
- Claim: Regularization terms (RBD, RAD) control the tradeoff between generating sampling zeros and structural zeros
- Mechanism: RBD minimizes distance to nearest training sample, encouraging diversity. RAD penalizes average distance to training distribution, preventing drift
- Core assumption: The training sample represents a subset of feasible combinations; some unobserved combinations are valid (sampling zeros) while others are invalid (structural zeros)
- Evidence anchors: [section 3] Equations 5-7 define DIST, RBD, RAD with λbd=10, λad=1 hyperparameters

### Mechanism 3
- Claim: Wasserstein distance with gradient penalty provides stable training for tabular population synthesis
- Mechanism: WGAN-GP uses Wasserstein-1 distance with Lipschitz constraint enforced via gradient penalty (λgp=0.025)
- Core assumption: The latent space Z (128-dim standard normal) can be mapped to the complex joint distribution of 17 categorical attributes via neural network transformation
- Evidence anchors: [section 3] Equation 4 defines gradient penalty term; Table 3 shows λgp=0.025

## Foundational Learning

- Concept: **Wasserstein distance vs. JS divergence**
  - Why needed here: Understanding why WGAN-GP is used instead of vanilla GAN requires grasping that Wasserstein distance provides gradients even when distributions don't overlap, critical for sparse tabular data
  - Quick check question: If discriminator perfectly separates real from fake, what happens to gradients in vanilla GAN vs. WGAN?

- Concept: **Joint distribution estimation from marginals**
  - Why needed here: Population synthesis fundamentally learns P(X₁,...,Xₖ) from samples; missing data means learning from partial observations of this joint
  - Quick check question: Given attributes AGE, SEX, INCOME with 20% of INCOME missing, how would the mask approach differ from mean-imputation pre-processing?

- Concept: **Sampling zeros vs. structural zeros**
  - Why needed here: The key evaluation distinction in this paper. Sampling zeros = valid combinations absent from training sample; structural zeros = implausible combinations that shouldn't exist
  - Quick check question: If training data has no "age=65+, income=100k+" individuals but ground truth does, which zero type is this? What if the combination is logically impossible?

## Architecture Onboarding

- Component map: Latent Z (128-dim) → Generator (2 layers, 128 neurons) → X̂ (17 attrs) → Mask Y (binary) → X_masked → Discriminator (2 layers, 128 neurons) → score

- Critical path:
  1. Preprocess: Convert all attributes to categorical; create mask Y for missing positions
  2. Forward pass: Generate X̂, multiply by Y, concatenate with X_masked for discriminator
  3. Loss computation: Wasserstein loss + gradient penalty + RBD + RAD
  4. Training: n_d=5 discriminator updates per generator update

- Design tradeoffs:
  - **Mask vs. imputation**: Paper masks during training rather than pre-imputing; preserves uncertainty but requires modified architecture
  - **Regularization weights**: λbd=10, λad=1 favor diversity over strict adherence; may generate more structural zeros in exchange for better sampling zero coverage
  - **Network capacity**: 2 layers × 128 neurons is relatively small; sufficient for 17 attributes but may underfit more complex distributions

- Failure signatures:
  - **Category collapse**: If certain rare categories never appear in synthetic output (e.g., "4-körkort"), increase λad or reduce λbd
  - **Precision collapse**: If structural zeros dominate high-dimensional outputs, ground truth may have too few unique combinations
  - **Training instability**: If discriminator loss diverges, reduce learning rate from 0.01 or increase λgp

- First 3 experiments:
  1. **Baseline replication**: Train WGAN-GP on h-nomis (complete data) with Table 3 parameters; verify attribute-level metrics match Table 4 before proceeding
  2. **Ablation on missing rate**: Train models with r={10, 20, 30, 40}% missing on q=2 attributes; plot SRMSE trend to validate degradation is gradual (not cliff-like)
  3. **Zero analysis**: For 16k-dimensional joint, compute precision/recall vs. ground truth; if precision <0.01, verify this matches paper findings (Figure 4b shows precision ~0.0004-0.0008 for 7M-dim)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Conditional Tabular GANs (CT-GAN) be adapted to condition on aggregated marginal distributions to synthesize future populations, replacing traditional fitting techniques?
- Basis in paper: [explicit] The "Future works" section explicitly states the need to investigate CT-GAN for synthesizing populations for future scenarios
- Why unresolved: The current study only focused on the "starting solution" stage using WGAN; it did not integrate conditional generation mechanisms required for the fitting stage
- What evidence: A modified CT-GAN architecture that successfully generates synthetic populations matching specified future marginal constraints (e.g., R2 > 0.9 against target marginals) without using Iterative Proportional Fitting

### Open Question 2
- Question: How can the WGAN architecture or loss function be optimized to minimize structural zero generation in high-dimensional joint distributions with sparse ground truth data?
- Basis in paper: [inferred] The "Conclusion" and "Evaluation" sections identify the generation of numerous structural zeros as a primary limitation in high-dimensional cases (7M-dimensional joint)
- Why unresolved: The authors demonstrate the problem exists but do not propose or test architectural changes to restrict the generation of invalid combinations in high-dimensional spaces
- What evidence: Comparative experiments on the same dataset showing a statistically significant reduction in structural zero ratios (score_stz) and improved precision metrics without degrading the general sample or sampling zero scores

### Open Question 3
- Question: Does the mask-matrix training method maintain robustness when missingness is systematic (e.g., correlated with attribute values) rather than completely random?
- Basis in paper: [inferred] The experimental setup simulated missing data by replacing random rows ($r\%$) with NaN, whereas the introduction cites privacy concerns and data collection constraints, which typically result in non-random, systematic missingness
- Why unresolved: The validation is restricted to random missingness mechanisms; the model's ability to impute and generate accurate joint distributions when specific demographic groups are systematically under-reported remains unverified
- What evidence: Performance metrics (SRMSE, TV complement) derived from training datasets engineered with Missing At Random (MAR) and Missing Not At Random (MNAR) patterns compared against the current random missingness baseline

## Limitations

- The paper assumes missingness is random, which may not hold for real-world privacy or data collection constraints
- The small neural network architecture (2 layers, 128 neurons) may underfit complex joint distributions, particularly for high-dimensional sparse outputs
- Structural zero prevalence (precision <1%) appears inherent to the problem rather than a model failure, suggesting fundamental limitations for tabular synthesis in this regime

## Confidence

- **High confidence**: The masking mechanism enabling WGAN training on incomplete data is well-founded and directly implemented from established WGAN-GP literature
- **Medium confidence**: The regularization approach for controlling sampling vs. structural zeros works as described, though results suggest structural zeros may be unavoidable in high-dimensional sparse settings
- **Medium confidence**: Attribute-level metrics (coverage, TV complement, SRMSE) show comparable performance across models, validating the method's effectiveness

## Next Checks

1. Replicate baseline model (h-nomis, complete data) and verify attribute-level metrics match reported values before testing missing data scenarios
2. Systematically vary missing rate (10%, 20%, 30%, 40%) and track degradation patterns in joint distribution metrics to confirm gradual performance decline
3. Compare precision/recall on high-dimensional joint distributions against ground truth to determine if structural zero generation is inherent to the problem or can be mitigated through architectural changes