---
ver: rpa2
title: Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented
  Generation
arxiv_id: '2508.09755'
source_url: https://arxiv.org/abs/2508.09755
tags:
- question
- retrieval
- questions
- document
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a question-centric retrieval-augmented generation
  (RAG) framework for multihop question answering. The core innovation lies in decomposing
  complex multihop questions into single-hop subquestions using a large language model
  and transforming document chunks into answerable questions via Qwen3-8B, enabling
  semantically aligned retrieval through question-to-question similarity.
---

# Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.09755
- Source URL: https://arxiv.org/abs/2508.09755
- Authors: Seokgi Lee
- Reference count: 40
- Key outcome: Achieves 63.89 F1 average across HotpotQA, 2WikiMultiHopQA, and MuSiQue using question-centric RAG with GPT-4o

## Executive Summary
This paper introduces a question-centric retrieval-augmented generation (RAG) framework for multihop question answering. The core innovation transforms both queries and documents into answerable question formats—decomposing complex multihop questions into single-hop subquestions and converting document chunks into answerable questions via Qwen3-8B. These question-based representations are embedded and retrieved using question-to-question similarity, achieving superior semantic alignment compared to traditional document embedding approaches. Evaluated on three multihop QA benchmarks, the method consistently outperforms baseline RAG systems, with the best configuration achieving an average F1 score of 63.89.

## Method Summary
The approach uses a two-stage retrieval pipeline: (1) Query decomposition where an LLM breaks multihop questions into single-hop subquestions, and (2) Answerable Question Generation (AQG) where Qwen3-8B generates ~10-12 questions per 800-character document chunk. These questions are embedded using multilingual-e5-large and indexed. During inference, subquestions are embedded and used to retrieve AQGs, which are mapped back to source chunks. Retrieved chunks are then reranked against the original multihop query using a cross-encoder before being passed to a generator LLM. The unified inference approach (using the original question) outperforms sequential answering across all model variants.

## Key Results
- Best average F1 of 63.89 across three datasets using GPT-4o
- AQG-only embeddings improve MuSiQue F1 from 29.26 to 54.08 (GPT-4o)
- Decomposition strategy shows varying benefits: strong gains on complex datasets (MuSiQue, 2WikiMultiHopQA) but minimal/no impact on simpler HotpotQA
- LLaMA-3.1-8B performance degrades with k₂>7 in reranking, while GPT-4o and Gemini handle k₂=10-15

## Why This Works (Mechanism)

### Mechanism 1: Query Decomposition for Targeted Retrieval
Decomposing multihop questions into single-hop subquestions enables more precise retrieval of individual reasoning steps. An LLM parses complex queries into semantically independent subquestions, each targeting a distinct factual point. These are embedded and used to query the AQG index independently, broadening retrieval coverage. Break condition occurs when decomposition produces redundant or overlapping subquestions that retrieve identical chunks without adding coverage.

### Mechanism 2: Answerable Question Generation (AQG) as Document Proxies
Representing document chunks as sets of answerable questions improves semantic alignment with queries compared to raw document embeddings. Qwen3-8B generates ~10-12 questions per chunk, capturing explicit, implicit, and inferable content. These questions are embedded and indexed as retrieval proxies, transforming documents from narrative to interrogative form. Break condition occurs when generated questions are generic or fail to capture chunk-specific content.

### Mechanism 3: Cross-Encoder Reranking for Contextual Filtering
Reranking retrieved chunks against the original multihop query filters tangentially relevant but contextually unhelpful passages. After AQG-based retrieval via subquestions, a cross-encoder scores each candidate chunk against the original complex query. This filters valid-but-off-topic matches that individual subquestions may surface. Break condition occurs when the reranker is miscalibrated for the domain or when k₂ is too large for weaker models.

## Foundational Learning

- **Concept: Query-Document Semantic Mismatch**
  - Why needed here: The core motivation is that queries (short, interrogative) and documents (long, expository) occupy different regions in embedding space despite semantic alignment
  - Quick check question: Can you explain why a dense retriever might fail to match a question to a semantically relevant passage that uses different vocabulary or structure?

- **Concept: Multihop Reasoning Chains**
  - Why needed here: Understanding that answers require synthesizing facts across multiple documents—each step depends on prior retrieval
  - Quick check question: Given "Who was the advisor of the physicist who discovered the photoelectric effect?", can you identify the two retrieval steps required?

- **Concept: Bi-Encoder vs Cross-Encoder Retrieval**
  - Why needed here: The architecture uses bi-encoders (E5) for scalable retrieval and cross-encoders for precision reranking
  - Quick check question: Why is a cross-encoder too slow for initial retrieval over large corpora, but suitable for reranking a small candidate set?

## Architecture Onboarding

- **Component map**: Documents → Chunking (800 char, 200 overlap) → Qwen3-8B AQG (~10–12 questions/chunk) → E5 Embedding → Vector Index (AQG → chunk mapping) → Subquestion Retrieval → Cross-Encoder Reranking → Generator LLM → Answer

- **Critical path**: AQG quality determines retrieval quality. Poor question generation (generic, hallucinated, or sparse) propagates errors through the entire pipeline. Validate AQG output on sample chunks before scaling.

- **Design tradeoffs**:
  - k₂ (reranking scope): Model-dependent. GPT-4o and Gemini handle k₂=10–15 well; LLaMA degrades above 7
  - Decomposition vs original query: HotpotQA benefits less from decomposition (simpler structure); MuSiQue and 2WikiMultiHopQA benefit more
  - AQG-only vs AQG+document embeddings: AQG-only captures most signal; combining adds marginal gains (~1–2 F1 points)

- **Failure signatures**:
  - Zero AQG outputs: Numeric/formulaic chunks produce no questions—filter or handle separately
  - Redundant subquestions: Decomposition collapses into near-duplicates, retrieving same chunks repeatedly
  - Overly large k₂ for weaker models: LLaMA performance drops with more context
  - Generic questions: AQG produces "What is this text about?" style questions that don't distinguish chunks

- **First 3 experiments**:
  1. Validate AQG quality: Sample 20–50 chunks, manually inspect generated questions for coverage and specificity
  2. Ablate embedding source: Compare document-only, AQG-only, and combined embeddings on a held-out set
  3. Tune k₂ per model: Run grid search on k₂ ∈ {5, 7, 10, 12, 15} for your target LLM to find optimal reranking scope

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the question-centric transformation strategy generalize to domains outside of Wikipedia and to non-English languages?
- Basis in paper: The Limitations section explicitly states, "further validation on other domains and languages is necessary to generalize our findings"
- Why unresolved: The current study restricts evaluation to three specific Wikipedia-based benchmarks from LongBench
- What evidence would resolve it: Evaluation results on domain-specific corpora and multilingual multi-hop QA datasets

### Open Question 2
- Question: How can the framework be optimized to mitigate the increased inference latency caused by processing multiple decomposed subquestions?
- Basis in paper: The authors acknowledge, "the use of multiple subquestions increases the inference latency due to repeated retrieval and reranking steps"
- Why unresolved: While performance gains are demonstrated, the computational cost of repeated retrieval cycles is a noted practical drawback
- What evidence would resolve it: A comparative latency analysis or pruning/parallelization mechanism that maintains F1 scores while reducing per-query processing time

### Open Question 3
- Question: Is an adaptive decomposition strategy necessary to prevent performance degradation on simpler, bridge-type questions?
- Basis in paper: Table 3 shows that query decomposition actually lowers F1 scores for GPT-4o and Gemini on HotpotQA, attributed to its "relatively simpler structure"
- Why unresolved: The current static approach decomposes all queries, which appears to introduce unnecessary complexity for questions that don't require deep multi-hop isolation
- What evidence would resolve it: Development and testing of a "complexity-aware" switch that skips decomposition for simpler queries

## Limitations
- Decomposition quality depends on LLM capability—errors propagate through the pipeline
- AQG may fail on numeric/formulaic content, producing zero outputs for certain chunk types
- Cross-encoder reranking effectiveness is model-dependent (LLaMA degrades with larger k₂)
- Exact decomposition prompt template not provided, making precise replication challenging

## Confidence

- **High confidence**: AQG-only embeddings improve retrieval quality (consistent gains across all models and datasets)
- **Medium confidence**: Decomposition benefits vary by dataset complexity (clear gains on complex datasets, minimal on simpler ones)
- **Medium confidence**: k₂ tuning is model-dependent (LLaMA degrades with k₂>7 while GPT-4o handles k₂=10-15)

## Next Checks

1. **Prompt reproducibility**: Reconstruct decomposition prompt based on described functionality and test on 50 sample multihop questions to verify it produces semantically independent subquestions

2. **Chunk type robustness**: Systematically test AQG generation on different document types (narrative, technical, numeric, formulaic) to quantify coverage gaps and develop fallback strategies for zero-output cases

3. **Model transfer validation**: Evaluate the complete pipeline with a new LLM not used in original experiments (e.g., Claude-3 or Mistral) to assess whether k₂ tuning patterns and decomposition benefits generalize beyond the three tested models