---
ver: rpa2
title: Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for
  Cyber-Physical Systems Security
arxiv_id: '2506.22445'
source_url: https://arxiv.org/abs/2506.22445
tags:
- local
- learning
- global
- hierarchical
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces HAMARL, a Hierarchical Adversarially-Resilient\
  \ Multi-Agent Reinforcement Learning framework for securing Cyber-Physical Systems\
  \ against adaptive cyber threats. The framework combines hierarchical multi-agent\
  \ coordination\u2014with local agents protecting subsystems and a global coordinator\
  \ managing system-wide security\u2014with an adversarial training loop that continuously\
  \ adapts defenses against evolving attacks."
---

# Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security

## Quick Facts
- arXiv ID: 2506.22445
- Source URL: https://arxiv.org/abs/2506.22445
- Authors: Saad Alqithami
- Reference count: 7
- Primary result: HAMARL achieves F1 score ~0.80, MTTD ~500 steps, and FAR ~6.5% against adaptive cyber threats in CPS

## Executive Summary
This paper introduces HAMARL, a Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning framework designed to secure Cyber-Physical Systems against adaptive cyber threats. The framework combines hierarchical multi-agent coordination—with local agents protecting subsystems and a global coordinator managing system-wide security—with an adversarial training loop that continuously adapts defenses against evolving attacks. Experiments on a simulated industrial IoT testbed demonstrate HAMARL outperforms traditional multi-agent reinforcement learning and rule-based intrusion detection, achieving higher detection accuracy, faster mean time-to-detection, and lower false alarm rates while maintaining operational continuity under sophisticated attack scenarios.

## Method Summary
HAMARL employs a hierarchical MARL architecture where 8 local defender agents (using Graph Attention Networks) monitor individual PLC subsystems and a global coordinator (using a 3-layer MLP) manages system-wide defense strategies. The framework incorporates an adaptive attacker agent in a Markov Game framework, enabling adversarial co-evolution where defenders learn robust behaviors by competing against a continuously improving threat model. Training uses Proximal Policy Optimization with Generalized Advantage Estimation, with local rewards for true positives and false positives, and global rewards balancing uptime against compromised subsystems.

## Key Results
- Achieves F1 score of approximately 0.80, outperforming traditional MARL and rule-based baselines
- Detects intrusions with mean time-to-detection of approximately 500 steps
- Maintains low false alarm rate of approximately 6.5% while preserving operational continuity
- Demonstrates scalability with training time increasing linearly from 0.069h to 0.204h as agent count grows from 8 to 24

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Policy Factorization
The hierarchical structure enables scalable defense by decomposing the global security objective into independent local subsystem policies and a centralized coordination policy. The framework factorizes the joint policy such that local agents act on partial observations for immediate response, while the global coordinator acts on aggregated embeddings for system-wide strategy. This reduces the complexity of the joint action space.

**Core assumption**: Local agents can achieve near-optimal detection using only partial observations without full system state visibility.

**Evidence anchors**: The paper provides the factorization formula $\pi_{\Theta,\phi,\psi}(a | s) = [\prod \pi_{\theta_i}(a_i | \omega_i)] \pi_{\phi}(a_{global} | g)$ and cites related work supporting structured hierarchies for large-scale CPS optimization.

**Break condition**: If local anomalies require cross-subsystem context to distinguish from benign noise, local policies may produce excessive false positives due to information asymmetry.

### Mechanism 2: Adversarial Co-Evolution (Red Teaming)
Including an adaptive attacker agent in the training loop improves resilience against novel threats better than training against static rule-based attacks. The system is modeled as a Markov Game where the attacker learns to evade current defenses, forcing defenders to adapt via PPO to minimize regret. This drives the system toward a local Nash Equilibrium.

**Core assumption**: The simulated attacker's action space sufficiently spans the space of real-world cyber threats.

**Evidence anchors**: The paper describes the attacker receiving feedback and defenders learning robust behaviors to counter sophisticated threat patterns, noting that unlike prior work, HAMARL couples hierarchy with this adaptive adversary.

**Break condition**: If the attacker converges too quickly to a dominant strategy that defenders cannot counter, or if the attacker fails to explore the vulnerability space effectively, the training loop provides no signal.

### Mechanism 3: Bounded Compromise via Reward Asymmetry
The framework ensures operational continuity by mathematically bounding the ratio of compromised subsystems through asymmetric reward engineering. Theorem 2 posits that if the penalty cost incurred by defenders for a compromised subsystem is sufficiently high relative to the attacker's reward, defenders will prioritize aggressive recovery actions over passive monitoring.

**Core assumption**: The cost parameters in the reward function accurately reflect real-world operational impact of downtime vs. intrusion.

**Evidence anchors**: The paper provides the reward function $R = -0.1|Comp(t)| - 0.01 DOWNTIME + 0.2 UPTIME$ and the theoretical bound $\varrho^* < 1$ unless the attacker reward dwarfs defenders' ability.

**Break condition**: If the penalty for downtime outweighs the penalty for compromise, the coordinator may prefer leaving nodes compromised to maintaining uptime, breaking the resilience guarantee.

## Foundational Learning

- **Concept**: Markov Games (Stochastic Games)
  - **Why needed here**: This is the mathematical substrate. Unlike standard RL, HAMARL operates in a multi-agent environment where transition probability depends on joint actions of defenders and attacker.
  - **Quick check question**: Can you explain why a standard Q-learning agent would fail in an environment where the "goal posts" (attacker strategy) move every episode?

- **Concept**: Graph Attention Networks (GATs)
  - **Why needed here**: Local agents use GATs to process subsystem states. The "attention" mechanism is critical for weighting sensor inputs differently depending on threat context.
  - **Quick check question**: In a network of 8 PLCs, how does a GAT help an agent decide which neighboring PLC's status is relevant to its own security decision?

- **Concept**: Proximal Policy Optimization (PPO) with GAE
  - **Why needed here**: The paper uses PPO to update policies. Understanding the "clipping" (0.2) and GAE (λ=0.95) is necessary to diagnose training stability or divergence in the adversarial loop.
  - **Quick check question**: Why is PPO preferred over DQN in this specific multi-agent, continuous-adjacent control setting? (Hint: stability in policy updates).

## Architecture Onboarding

- **Component map**: Environment (Cyber-Battle-Sim) -> 8 Local Defenders (2-layer GATs) -> 1 Global Coordinator (3-layer MLP) -> 1 Attacker Agent

- **Critical path**: Environment steps forward; sensors generate data. Local agents receive partial observation ωᵢ and produce action aᵢ and an embedding. Global Coordinator concatenates embeddings (g) and issues global command. Attacker observes and acts. Reward Calculation is critical step.

- **Design tradeoffs**:
  - *Scalability vs. Training Time*: HAMARL scales linearly in training time but is significantly slower (2x-7x) than Non-Hierarchical MARL due to coordination overhead.
  - *Accuracy vs. False Alarms*: The system optimizes for low False Alarm Rate (~6.5%) but potentially at the cost of slightly lower Recall (~0.70) compared to theoretical max, preventing operational paralysis.

- **Failure signatures**:
  - *Coordination Collapse*: Local agents quarantine everything (high FP) while Global Coordinator does nothing. (Check: Local reward scaling vs Global penalty).
  - *Adversarial Overfitting*: Defenders beat the specific attacker policy in training but fail immediately when attacker changes seed or tactic. (Check: Diversity of attacker strategies).
  - *Instability*: Oscillating returns (loss spikes). (Check: PPO clipping parameter or learning rate 10⁻⁴).

- **First 3 experiments**:
  1. **Sanity Check (Static Threat)**: Run fixed "scan-and-dos" script against untrained network to establish baseline compromise ratio.
  2. **Ablation (No Hierarchy)**: Disable Global Coordinator (set MLP to NOOP) to measure performance drop in "unseen" vs "known" attack scenarios.
  3. **Scalability Stress Test**: Increase agents to 12 and 24 and plot inference latency to ensure real-time feasibility for target CPS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HAMARL framework maintain resilience when facing multiple, potentially colluding adversarial agents rather than the single adaptive attacker tested?
- Basis in paper: The conclusion explicitly identifies "extending adversarial training scenarios to include multiple or colluding attackers" as a necessary direction to expose vulnerabilities.
- Why unresolved: The current methodology models the threat landscape using a single attacker policy, which simplifies the coordination dynamics of complex, multi-vector assaults.
- What evidence would resolve it: Empirical results from simulations involving multiple independent or cooperative attacker agents, measuring detection rates and system uptime under coordinated attacks.

### Open Question 2
- Question: Can the computational overhead of hierarchical MARL be reduced to feasible levels for deployment on resource-constrained OT networks?
- Basis in paper: The authors list "substantial computational cost" as the foremost remaining challenge for practical adoption in standard industrial settings.
- Why unresolved: The current implementation requires significant processing power (demonstrated on M4 Max MacBook), which may exceed hardware capabilities of legacy industrial controllers.
- What evidence would resolve it: Successful deployment experiments utilizing lightweight policy distillation or federated training that demonstrate maintained security performance on low-power industrial hardware.

### Open Question 3
- Question: How effectively can defensive policies transfer across diverse CPS domains (e.g., from manufacturing to smart grids) without requiring extensive manual retuning?
- Basis in paper: The paper suggests "Transfer and meta-learning methods" as future work to reduce data overhead of deploying across diverse domains.
- Why unresolved: The current framework relies on domain-specific reward shaping and tuning to achieve high F1 scores, creating barriers to generalization.
- What evidence would resolve it: Benchmarks showing that policies pre-trained on manufacturing testbed can adapt to smart grid environment with minimal additional training episodes.

### Open Question 4
- Question: Does HAMARL satisfy formal industrial safety standards (e.g., IEC 62443) required for real-world critical infrastructure deployment?
- Basis in paper: The conclusion notes that "Real-world adoption necessitates demonstrable compliance with industrial standards" and fail-safe validations.
- Why unresolved: While the framework demonstrates statistical resilience, it lacks the formal verification required for safety-critical certification.
- What evidence would resolve it: A formal verification analysis proving that learned policies never violate specific safety constraints or require human-interpretable logic for audit purposes.

## Limitations
- The specific transition probabilities for attack actions and the exact embedding aggregation method are not mathematically defined, making faithful reproduction challenging
- The reward parameters appear tuned to the simulation rather than derived from real-world operational constraints, potentially limiting generalization
- The framework lacks formal verification required for safety-critical industrial certification and compliance with standards like IEC 62443

## Confidence
- **High Confidence**: The hierarchical policy factorization mechanism is well-defined and theoretically sound with clear mathematical formulation and consistent experimental validation
- **Medium Confidence**: The adversarial co-evolution mechanism shows strong conceptual merit, but achieving Nash equilibrium depends heavily on attacker exploration strategy and learning rate balance
- **Low Confidence**: The bounded compromise guarantee relies on reward parameters that may not generalize across different CPS configurations

## Next Checks
1. **Environment Fidelity Test**: Implement Cyber-Battle-Sim with explicit transition probabilities for each attack action and validate that the simulated threat model reasonably spans real-world CPS vulnerabilities
2. **Attacker Diversity Analysis**: Run adversarial training loop with multiple attacker seeds and compare defender performance to ensure robust, generalizable strategies rather than overfitting to a single attacker policy
3. **Real-time Feasibility Assessment**: Measure inference latency of the 8+1 agent system under operational conditions to verify the 500-step MTTD claim translates to sub-second response times required for CPS protection