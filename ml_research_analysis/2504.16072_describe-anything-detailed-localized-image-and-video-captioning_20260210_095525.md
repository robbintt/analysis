---
ver: rpa2
title: 'Describe Anything: Detailed Localized Image and Video Captioning'
arxiv_id: '2504.16072'
source_url: https://arxiv.org/abs/2504.16072
tags:
- image
- detailed
- localized
- region
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Describe Anything Model (DAM), a vision-language
  model designed for detailed localized captioning in images and videos. DAM addresses
  the challenge of generating precise descriptions for specific regions by employing
  a focal prompt that encodes high-resolution details and a localized vision backbone
  that integrates global context.
---

# Describe Anything: Detailed Localized Image and Video Captioning

## Quick Facts
- **arXiv ID:** 2504.16072
- **Source URL:** https://arxiv.org/abs/2504.16072
- **Reference count:** 40
- **Primary result:** DAM achieves state-of-the-art on seven benchmarks including LVIS 89.0% semantic similarity and Ref-L4 CIDEr 70.0%

## Executive Summary
This paper introduces the Describe Anything Model (DAM), a vision-language model designed for detailed localized captioning in images and videos. DAM addresses the challenge of generating precise descriptions for specific regions by employing a focal prompt that encodes high-resolution details and a localized vision backbone that integrates global context. To overcome data scarcity, the authors propose an SSL-based Data Pipeline (DLC-SDP) that leverages segmentation datasets and unlabeled web images. They also introduce DLC-Bench, a benchmark that evaluates captions based on predefined attributes rather than reference captions. DAM achieves state-of-the-art performance on seven benchmarks, including open-class keyword-level (LVIS 89.0% semantic similarity), phrase-level (Flickr30k Entities 22.6% BLEU@4), and detailed multi-sentence captioning (Ref-L4 38.7% BLEU@4, 70.0% CIDEr), as well as video captioning (HC-STVG 19.8% BLEU@4).

## Method Summary
DAM fine-tunes VILA 1.5 (3B/8B) with a localized vision backbone that processes both global context and focal crops using gated cross-attention adapters. The focal prompt crops regions with α=3 expansion and minimum 48px size. Training uses ~1.5M samples from DLC-SDP: Stage 1 (684k regions from segmentation datasets with human-annotated masks/keywords), Stage 2 (774k regions from SA-1B via SSL), and video data (94k regions from SA-V). The model mixes ShareGPT-4V for instruction-following to prevent catastrophic forgetting.

## Key Results
- State-of-the-art on seven benchmarks: LVIS semantic similarity (89.0%), PACO semantic IoU (73.2%), Flickr30k BLEU@4 (22.6%), Ref-L4 CIDEr (70.0%), DLC-Bench avg (67.3%), HC-STVG BLEU@4 (19.8%)
- Focal prompt with α=3 crop expansion and min 48px size enables high-resolution detail capture
- Localized vision backbone with gated cross-attention adapters improves performance from 42.4% to 67.3% on DLC-Bench

## Why This Works (Mechanism)
DAM addresses detailed localized captioning by encoding high-resolution details through focal prompts while maintaining global context via a localized vision backbone. The gated cross-attention adapters allow the model to attend to both global and regional features without forgetting pre-trained capabilities. The SSL-based data pipeline generates diverse training samples from unlabeled web images, addressing data scarcity for this task.

## Foundational Learning

**Focal Prompt Mechanism** - Crops regions with α=3 expansion and minimum 48px size to encode high-resolution details
- Why needed: Captures fine-grained visual information for detailed descriptions
- Quick check: Verify crop size ≥48px and expansion factor α=3 in implementation

**Localized Vision Backbone** - Processes full image and focal crop separately with gated cross-attention
- Why needed: Maintains global context while focusing on region-specific details
- Quick check: Confirm gated cross-attention adapters added to each transformer block

**SSL-based Data Pipeline** - Generates training samples from unlabeled web images using segmentation datasets
- Why needed: Addresses data scarcity for detailed localized captioning
- Quick check: Verify two-stage approach (684k human-annotated + 774k SSL-generated)

## Architecture Onboarding

**Component Map:** Input → Focal Crop + Mask → Localized Vision Backbone → Gated Cross-Attention → VILA LLM → Caption

**Critical Path:** Focal prompt → Localized vision backbone with mask embedding E_M → Gated cross-attention adapters → Caption generation

**Design Tradeoffs:** Uses gated cross-attention to balance global context with regional details, initialized to zero to prevent catastrophic forgetting of pre-trained capabilities

**Failure Signatures:** Degraded performance (42.4% vs 67.3%) without gated cross-attention; loss of VLM capabilities if new modules not initialized to zero

**First Experiments:** 1) Verify gated cross-attention initialization (E_M outputs zeros, γ^(l)/β^(l) initialized to zero) 2) Test focal prompt parameters (α=3, min 48px) 3) Measure performance gap with/without cross-attention integration

## Open Questions the Paper Calls Out
None

## Limitations
- Data pipeline opacity: Critical implementation details missing for Stage 2 SSL generation including specific VLM selection and filtering thresholds
- Scalability constraints: Requires 8 A100 GPUs for 14 hours of training
- Evaluation methodology shift: DLC-Bench's attribute-based evaluation differs from traditional reference-based metrics

## Confidence

**High confidence** in architectural design and reported performance gains based on ablation studies and state-of-the-art results across seven benchmarks.

**Medium confidence** in reproducibility of SSL-based Data Pipeline due to missing implementation details about rejection sampling thresholds and VLM selection.

**Medium confidence** in generalization claims due to DLC-Bench's fundamental methodological shift from reference-based evaluation.

## Next Checks

1. **Verify gated cross-attention initialization:** Confirm mask embedding E_M outputs zeros initially and γ^(l)/β^(l) scaling parameters in gated cross-attention adapters are properly initialized to zero.

2. **Reconstruct Stage 1 prompt engineering:** Test the exact prompt template from Table A.7 with different VLMs to assess sensitivity to annotation model choice.

3. **Validate cross-attention integration:** Implement localized vision backbone and measure performance gap between models with and without gated cross-attention (comparing against Tab 8's 42.4% vs 67.3% baseline).