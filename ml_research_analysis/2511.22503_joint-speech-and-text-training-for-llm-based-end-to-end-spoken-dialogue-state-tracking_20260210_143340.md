---
ver: rpa2
title: Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State
  Tracking
arxiv_id: '2511.22503'
source_url: https://arxiv.org/abs/2511.22503
tags:
- text
- speech
- training
- data
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-domain generalization
  in end-to-end spoken dialogue state tracking (DST), where models struggle to perform
  well on new domains due to data scarcity and the difficulty of collecting spoken
  training data for each domain. The proposed solution is to jointly train on available
  spoken DST data from source domains and unpaired textual DST data from target domains
  by augmenting an LLM-based spoken DST model with a text encoder.
---

# Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking

## Quick Facts
- **arXiv ID**: 2511.22503
- **Source URL**: https://arxiv.org/abs/2511.22503
- **Reference count**: 0
- **Key outcome**: Joint speech-text training with text encoder augmentation improves cross-domain spoken dialogue state tracking performance by 28.9-64.7% relative to baselines

## Executive Summary
This paper addresses the challenge of cross-domain generalization in end-to-end spoken dialogue state tracking (DST) by proposing a method to leverage unpaired textual data from target domains. The approach augments an LLM-based spoken DST model with a text encoder and jointly trains on spoken DST data from source domains and unpaired textual DST data from target domains. This enables the model to improve performance on spoken data from the target domain without requiring paired speech-text data, which is typically expensive to collect. The method shows consistent improvements across different LLM choices and scales well with model size.

## Method Summary
The proposed method involves augmenting an existing LLM-based spoken DST model with a text encoder module, then jointly training on two types of data: (1) spoken DST data from source domains where paired speech-text data is available, and (2) unpaired textual DST data from target domains. During training, the model learns to process both speech and text inputs while maintaining consistent dialogue state tracking outputs. The text encoder is integrated into the LLM architecture, allowing the model to leverage the abundant textual DST data to improve its spoken DST performance on target domains. This approach addresses the data scarcity problem in spoken DST by utilizing the much larger pool of available textual dialogue data.

## Key Results
- The proposed method consistently improves cross-domain DST performance across all tested scenarios
- Performance improvements range from 28.9% to 64.7% reduction in performance gaps compared to baselines
- Relative improvements become more pronounced as model size increases, demonstrating good scalability
- The approach is stable across different LLM choices, showing robustness to architecture variations

## Why This Works (Mechanism)
The method works by leveraging the complementary strengths of speech and text data in dialogue modeling. Textual data provides richer linguistic patterns and semantic representations that can be transferred to improve speech understanding. By jointly training on both modalities, the model learns shared representations that capture dialogue semantics more effectively. The text encoder augmentation allows the model to directly process textual inputs and align them with speech representations, creating a unified understanding of dialogue states across modalities. This cross-modal learning enables better generalization to new domains where spoken data is scarce but textual data may be available.

## Foundational Learning
- **Dialogue State Tracking (DST)**: The task of identifying and updating the user's goals and preferences throughout a conversation. Why needed: Forms the core task being improved; quick check: Can track slot values across dialogue turns.
- **Cross-domain Generalization**: A model's ability to perform well on tasks from domains not seen during training. Why needed: Addresses the fundamental challenge of applying models to new domains; quick check: Performance measured on target domains different from training domains.
- **LLM-based DST**: Using large language models as the foundation for dialogue state tracking systems. Why needed: Provides the base architecture that is augmented with text encoding capabilities; quick check: Model can process dialogue context and track states using LLM capabilities.
- **Unpaired Data Training**: Training models using data where inputs and outputs don't have direct correspondences across modalities. Why needed: Enables leveraging abundant textual data without requiring expensive paired speech-text data; quick check: Model trained on speech data for source domains and text data for target domains.
- **Text Encoder Augmentation**: Adding a text processing module to an existing speech-focused model architecture. Why needed: Enables the model to directly process and learn from textual data alongside speech data; quick check: Model can accept both speech and text inputs during training.
- **Cross-modal Learning**: Training models to process and integrate information from multiple input modalities. Why needed: Allows the model to leverage complementary information from speech and text to improve overall performance; quick check: Model shows improved performance compared to single-modality training.

## Architecture Onboarding

**Component Map**: Speech Encoder -> LLM Backbone -> Text Encoder -> Dialogue State Tracker -> Output

**Critical Path**: Speech/Text Input → Encoding → LLM Processing → State Prediction

**Design Tradeoffs**: The approach trades increased model complexity (adding text encoder) for improved cross-domain performance without requiring paired speech-text data. This avoids the need for expensive data collection while maintaining model performance.

**Failure Signatures**: Poor cross-domain performance if textual data from target domains is not representative of spoken patterns; degraded performance if text encoder integration disrupts speech processing capabilities; limited improvements if textual data is too dissimilar from spoken dialogue patterns.

**3 First Experiments**:
1. Baseline comparison: Evaluate standard LLM-based spoken DST without text encoder augmentation on cross-domain tasks
2. Joint training ablation: Compare joint speech-text training against separate training on each modality
3. Text encoder integration test: Verify that adding the text encoder doesn't degrade performance on source domain spoken data

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The approach assumes availability of unpaired textual data from target domains, which may not always be available in truly zero-resource scenarios
- Experiments are limited to English-language datasets (SpokenWOZ and MultiWOZ), limiting generalizability to other languages
- The range of model architectures and sizes tested is relatively narrow, though results show stability across tested choices
- Absolute performance levels on spoken data for target domains are not extensively characterized

## Confidence
- **Core claims**: Medium - Methodology is sound but limited evaluation scenarios reduce broader applicability confidence
- **Scalability claims**: Medium - Improvements shown across model sizes but limited set tested
- **Stability across LLM choices**: Medium - Supported by multiple models but range of architectures tested is narrow
- **Generalizability to non-English domains**: Low - Only English datasets evaluated
- **Performance in zero-resource scenarios**: Low - Method assumes unpaired textual data availability

## Next Checks
1. Evaluate the approach in truly zero-resource settings where no textual data is available for the target domain to assess effectiveness when key assumptions are violated
2. Test the approach on non-English datasets and diverse dialogue domains to establish cross-lingual and cross-domain robustness
3. Conduct ablation studies to quantify individual contributions of text encoder and joint training objective, and determine minimum textual data required for meaningful improvements