---
ver: rpa2
title: 'ScriptoriumWS: A Code Generation Assistant for Weak Supervision'
arxiv_id: '2502.12366'
source_url: https://arxiv.org/abs/2502.12366
tags:
- data
- weak
- supervision
- prompt
- labeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ScriptoriumWS, a code generation assistant
  for weak supervision that addresses the bottleneck of manually writing labeling
  functions. The authors propose using code-generation models (specifically OpenAI
  Codex) to automatically synthesize programmatic labeling functions through carefully
  designed prompts.
---

# ScriptoriumWS: A Code Generation Assistant for Weak Supervision

## Quick Facts
- **arXiv ID:** 2502.12366
- **Source URL:** https://arxiv.org/abs/2502.12366
- **Reference count:** 36
- **Primary result:** ScriptoriumWS-generated labeling functions achieve significantly higher coverage (e.g., improving from 40.5% to 100% on SMS dataset and 25.8% to 100% on Spouse dataset) while maintaining comparable accuracy to human-designed labeling functions.

## Executive Summary
This paper presents ScriptoriumWS, a code generation assistant that addresses the bottleneck of manually writing labeling functions in weak supervision. The system uses OpenAI Codex with carefully designed multi-tier prompts to automatically synthesize programmatic labeling functions. The approach significantly improves coverage compared to human-designed LFs while maintaining accuracy, with downstream end-model performance improving by 4-5% F1-score on benchmark datasets.

## Method Summary
ScriptoriumWS employs a multi-tier prompting strategy with OpenAI Codex to synthesize labeling functions for weak supervision. The system uses five prompt categories: General Prompts, Mission Statements, Human Heuristics, In-Context Labeling Function Exemplars, and In-Context Data Exemplars. Synthesized LFs are evaluated on coverage, accuracy, overlap, and conflict metrics, then aggregated via label models (Snorkel, Dawid-Skene, FlyingSquid, Majority Vote, or Weighted Majority Vote) to produce pseudolabels for training logistic regression end models on 6 text classification datasets from the WRENCH benchmark.

## Key Results
- ScriptoriumWS-generated LFs achieve comparable accuracy to human-designed LFs while providing significantly higher coverage (e.g., improving SMS coverage from 40.5% to 100% and Spouse from 25.8% to 100%).
- Synthesized LFs produce downstream end-model performance on par with human-designed ones, with F1-score improvements of 4.0% and 5.0% on SMS and Spouse datasets respectively.
- The synthesized LFs can be used complementarily with existing human-designed LFs to further improve end model performance by increasing the number of labeled examples.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-tier prompting strategies can guide code-generation models to synthesize high-quality labeling functions without manual programming.
- Mechanism: Prompts are structured in tiers—from minimal (general prompts with task description, function signature, labeling instructions) to richer (mission statements, human heuristics, in-context LF exemplars, data exemplars). Each tier adds priors that constrain the search space of possible programs, increasing alignment between synthesized LFs and task requirements.
- Core assumption: The code-generation model has internalized sufficient domain and programming knowledge from pretraining to map natural language specifications to executable labeling logic.
- Evidence anchors: [abstract] "The system employs a multi-tier prompting strategy that incorporates general prompts, mission statements, human heuristics, and in-context examples to guide model synthesis of high-quality LFs." [section 3.3] Describes five prompt categories.

### Mechanism 2
- Claim: Synthesized LFs achieve higher coverage than human-designed LFs while maintaining comparable accuracy because code-generation models are less conservative in pattern generalization.
- Mechanism: Human experts tend to write LFs with narrow heuristics (specific keywords, tight thresholds) to avoid false positives. Code-generation models, prompted with task descriptions, produce broader pattern-matching logic that labels more data points. The label model then resolves conflicts among overlapping LF outputs.
- Core assumption: The label model can effectively denoise conflicting labels from high-coverage, moderate-accuracy LFs.
- Evidence anchors: [abstract] "ScriptoriumWS-generated LFs achieve comparable accuracy to human-designed LFs while providing significantly higher coverage (e.g., improving SMS coverage from 40.5% to 100% and Spouse from 25.8% to 100%)." [table 1] Shows coverage comparisons.

### Mechanism 3
- Claim: Combining synthesized and human-designed LFs improves end-model performance by covering data points that human LFs miss while preserving high-precision human labels.
- Mechanism: Human LFs provide high-precision but low-coverage labels. Synthesized LFs fill the coverage gap. The union set is fed to the label model, which aggregates both sources. The end model is trained on the expanded labeled dataset.
- Core assumption: Human and synthesized LFs have complementary error patterns; conflicts between them are resolvable by the label model.
- Evidence anchors: [abstract] "The synthesized LFs can be used complementarily with existing human-designed LFs to further improve end model performance." [section 4.2] "We train the end model on the union of these two sets... the end-model performance improves due to the significant increase in labeled examples."

## Foundational Learning

- Concept: Programmatic Weak Supervision (PWS) and Label Models
  - Why needed here: ScriptoriumWS operates within the PWS framework; understanding how noisy LF outputs are aggregated via label models (Snorkel, Dawid-Skene, FlyingSquid) is essential to interpret why high-coverage LFs improve downstream performance.
  - Quick check question: Can you explain how a label model estimates LF accuracies without ground-truth labels?

- Concept: In-Context Learning with Large Language Models
  - Why needed here: The multi-tier prompting strategy leverages in-context learning—providing exemplars (LFs or data) in the prompt to guide code generation without fine-tuning.
  - Quick check question: What is the difference between providing in-context LF exemplars versus in-context data exemplars, and when would you choose one over the other?

- Concept: Labeling Function Properties (Coverage, Overlap, Conflict, Accuracy)
  - Why needed here: Evaluating synthesized LFs requires understanding these four metrics; the paper explicitly uses them to compare human vs. synthesized LFs.
  - Quick check question: Why might high overlap and conflict among LFs be beneficial rather than problematic in a weak supervision pipeline?

## Architecture Onboarding

- Component map: Prompt Constructor -> Code-Generation Model (Codex) -> LF Evaluator -> Label Model (Snorkel/DS/FS/MV/WMV) -> End Model (Logistic Regression)

- Critical path: Prompt design → LF synthesis → LF evaluation → Label model training → Pseudolabel generation → End model training → Evaluation

- Design tradeoffs:
  - Prompt richness vs. cost: More informative prompts (heuristics, exemplars) require human input, reducing automation benefits.
  - Coverage vs. accuracy: Higher-coverage LFs may introduce more conflicts; label model quality determines whether this tradeoff is favorable.
  - Synthesis-only vs. complementary: Using only synthesized LFs maximizes automation; combining with human LFs may improve performance but requires existing LF infrastructure.

- Failure signatures:
  - Low accuracy with high coverage: Synthesized LFs are too permissive; prompt lacks sufficient constraints.
  - High conflict with no label model improvement: LFs have correlated errors; diversifying prompt strategies may help.
  - Syntax errors in synthesized code: Prompt format unclear or model temperature too high.

- First 3 experiments:
  1. Baseline replication: Run ScriptoriumWS with general prompts only on one dataset (e.g., YouTube spam); measure LF coverage and accuracy against WRENCH human LFs.
  2. Prompt tier ablation: Add one tier at a time (mission statement → heuristics → LF exemplars → data exemplars); observe impact on LF metrics and downstream F1.
  3. Complementary combination test: Combine synthesized LFs with human LFs; measure end-model performance vs. each source alone to validate complementary claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific success and failure modes of different prompting strategies (e.g., general prompts vs. human heuristics) when synthesizing labeling functions?
- Basis in paper: [explicit] The authors conclude that their experimental results regarding prompting strategies were "inconclusive" and explicitly call for "A deeper analysis that elucidates the successes and failure modes of each prompting strategy."
- Why unresolved: While the paper demonstrates that ScriptoriumWS works, the results show high variance across datasets without a discernible pattern explaining why one prompting strategy outperforms another in a specific context.
- What evidence would resolve it: An ablation study correlating specific dataset characteristics (e.g., class balance, text length) with the performance of different prompt types, potentially using explainability tools to analyze the code generation process.

### Open Question 2
- Question: Can code-generation models effectively synthesize labeling functions for weak supervision in non-text domains, such as image or video classification?
- Basis in paper: [inferred] The paper notes in the Related Work section that Automated Weak Supervision is critical when feature spaces are too complex for humans, "such as in image and video domains," but the experimental evaluation is restricted entirely to text classification tasks.
- Why unresolved: The prompts and heuristics evaluated rely heavily on keywords and text processing (e.g., regex, string matching), leaving the efficacy of this approach for visual or high-dimensional data unproven.
- What evidence would resolve it: Experimental results applying ScriptoriumWS to computer vision datasets, evaluating the quality of generated code that must handle pixel data or visual embeddings rather than text strings.

### Open Question 3
- Question: How does the performance of ScriptoriumWS change when using open-source code generation models instead of OpenAI Codex?
- Basis in paper: [inferred] The methodology relies exclusively on OpenAI Codex, a large proprietary model, leaving the system's dependence on this specific architecture unexplored.
- Why unresolved: It is unclear if the high coverage and accuracy are results of Codex's specific capabilities or if lighter, open-source models (like CodeT5 or CodeGen) can achieve comparable results with similar prompting strategies.
- What evidence would resolve it: Benchmarks comparing LF quality (coverage, accuracy) and downstream model performance when substituting Codex with current open-source code generation alternatives.

## Limitations

- The paper relies heavily on OpenAI Codex, which may not be widely available or may have different performance characteristics than open alternatives.
- The exact prompt templates used across different datasets are not fully specified, making exact reproduction challenging.
- The claim that synthesized LFs are complementary to human LFs is based on experimental results but lacks theoretical grounding about when this complementarity holds.

## Confidence

- High confidence: The multi-tier prompting strategy framework is well-explained and mechanistically sound.
- Medium confidence: The claim that synthesized LFs achieve higher coverage while maintaining accuracy is supported by experimental results but may depend on specific dataset characteristics.
- Medium confidence: The complementary benefit of combining human and synthesized LFs is demonstrated experimentally but the generalizability across domains is uncertain.

## Next Checks

1. Test the same multi-tier prompting strategy with an open-source code generation model (e.g., CodeGen or CodeT5) to assess dependency on proprietary models.
2. Conduct ablation studies varying the number and quality of in-context exemplars to quantify their impact on LF performance.
3. Evaluate the system on a new dataset from a different domain (e.g., medical text classification) to test generalizability beyond the WRENCH benchmark tasks.