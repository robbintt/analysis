---
ver: rpa2
title: How to Protect Models against Adversarial Unlearning?
arxiv_id: '2507.10886'
source_url: https://arxiv.org/abs/2507.10886
tags:
- unlearning
- healing
- training
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adversarial unlearning, where
  malicious parties exploit unlearning mechanisms to degrade model performance. The
  authors propose a "healing" method that mitigates these effects by replacing unlearned
  samples with similar "spare" examples from a reserve set.
---

# How to Protect Models against Adversarial Unlearning?

## Quick Facts
- **arXiv ID:** 2507.10886
- **Source URL:** https://arxiv.org/abs/2507.10886
- **Authors:** Patryk Jasiorski; Marek Klonowski; Michał Woźniak
- **Reference count:** 40
- **One-line result:** Healing method restores model accuracy after unlearning attacks by replacing removed samples with similar "twin" examples.

## Executive Summary
This paper addresses adversarial unlearning, where malicious actors exploit unlearning mechanisms to degrade model performance. The authors propose a "healing" approach that mitigates these effects by replacing unlearned samples with similar "spare" examples from a reserve set. Experiments on MNIST, CIFAR-10, and AFHQ datasets show that healing significantly restores accuracy after unlearning attacks, with CIFAR-10 accuracy improving from 66.76% to 91.41% (FS) and 90.95% (IF) after N/2 epochs. The method is particularly effective against exact unlearning and provides resilience even when starting from severely degraded models.

## Method Summary
The healing method works by maintaining a reserve set of samples during initial training, then replacing unlearned samples with semantically similar "twins" from this reserve during the healing phase. After unlearning removes a sample z, the system identifies a similar replacement z* from the reserve set and fine-tunes the degraded model on the remaining data plus z*. The approach uses various similarity metrics (L2, Mahalanobis, cosine) in either raw pixel space or feature space. The method is tested with multiple unlearning techniques (Naive, SISA, Fisher, Influence) and shows significant accuracy recovery across all tested datasets.

## Key Results
- Healing improved CIFAR-10 accuracy from 66.76% to 91.41% (FS) and 90.95% (IF) after N/2 epochs, nearly matching or exceeding gold standard performance
- Feature-space similarity metrics (cosine, Mahalanobis) outperformed raw pixel metrics for identifying effective replacement samples
- Extended fine-tuning (N/2 epochs vs. 1 epoch) consistently produced better recovery across all datasets and starting points
- The method provides resilience even when models are severely degraded, though Fisher unlearning collapse remains challenging to recover from

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing unlearned samples with semantically similar "spare" examples during fine-tuning can recover model performance degraded by unlearning operations.
- **Mechanism:** When unlearning removes a sample z, the model loses not just that specific instance but potentially useful features learned from it. Healing identifies a similar replacement z* from a reserve set and fine-tunes on the remaining data plus z*, effectively filling the "knowledge gap" without reintroducing the specific removed information.
- **Core assumption:** The reserve set contains elements sufficiently similar to unlearned samples that they can serve as functional substitutes for maintaining learned representations.
- **Evidence anchors:** [abstract] "healing improved CIFAR-10 accuracy from 66.76% to 91.41% (FS) and 90.95% (IF) after N/2 epochs, nearly matching or exceeding gold standard performance"; [section 4] "Healing retrains the model using a set z* that consists of examples of instances 'similar' to the unlearned set z chosen from a 'spare' instance set"
- **Break condition:** If the reserve set is exhausted (no sufficiently similar replacements remain), or if the similarity metric d fails to capture task-relevant similarity, healing effectiveness degrades.

### Mechanism 2
- **Claim:** Fine-tuning duration directly impacts healing effectiveness, with longer adaptation periods producing more complete recovery.
- **Mechanism:** Extended fine-tuning (N/2 epochs vs. 1 epoch) allows the model to better integrate replacement samples and realign parameters across the loss landscape. The paper suggests this provides "more adaptation time on the healing data leads to a more complete recovery."
- **Core assumption:** The unlearned model's parameter space is not so corrupted that gradient-based optimization cannot find a path back toward gold-standard performance.
- **Evidence anchors:** [section 5.3.2] "extending the healing from 1 epoch to N/2 epochs yields better results. Across all datasets and starting points... the best accuracy achieved after N/2 epochs was consistently higher"; [section 5.3.2] "on CIFAR-10, starting from the Influence model, 1-epoch healing achieved the best accuracy just below Gold (-0.02pp), while 3-epoch healing surpassed it (+0.24pp)"
- **Break condition:** If healing fine-tuning continues too long, the model may overfit to the replacement samples or begin to relearn information that should remain forgotten.

### Mechanism 3
- **Claim:** Feature-space similarity metrics outperform raw pixel metrics for identifying effective replacement samples in complex models.
- **Mechanism:** Raw pixel distance captures superficial similarity but misses semantic relationships. Feature-space metrics (cosine similarity, Mahalanobis distance using model embeddings) capture how the model represents the data, yielding replacements that serve as better functional substitutes.
- **Core assumption:** The model's learned feature representations encode task-relevant semantic information that persists after unlearning.
- **Evidence anchors:** [section 5.3.2] "combining the remaining data with carefully chosen twin samples often led to the highest final accuracy... particularly effective using feature-based twins on MNIST and AFHQ"; [section 4] Describes cosine similarity and Mahalanobis distance in feature space as alternatives to raw pixel L2 distance
- **Break condition:** If the model's feature extractor is severely degraded by unlearning (as seen in Fisher unlearning collapse scenarios), feature-based similarity may become unreliable.

## Foundational Learning

- **Concept: Machine Unlearning Taxonomy**
  - **Why needed here:** The paper assumes familiarity with exact vs. approximate unlearning, and methods like SISA, Fisher, and Influence unlearning. Understanding these is essential to grasp what healing must repair.
  - **Quick check question:** Can you explain why SISA unlearning is computationally cheaper than naive retraining, and what trade-off it introduces?

- **Concept: Fisher Information Matrix and Influence Functions**
  - **Why needed here:** Fisher and Influence unlearning use second-order optimization to estimate parameter changes. Healing must work on models degraded by these approximations.
  - **Quick check question:** Why does the diagonal approximation of the Fisher matrix sometimes cause model collapse, as observed in Table 2?

- **Concept: Adversarial Threat Models in ML Systems**
  - **Why needed here:** The paper defines adversary capabilities (blind, output-aware, parameter-aware) that determine attack severity and required defense posture.
  - **Quick check question:** What additional capabilities does a "parameter-aware" adversary have compared to a "blind" adversary, and how might this affect healing requirements?

## Architecture Onboarding

- **Component map:** Initial training -> Reserve set creation -> Unlearning engine -> Healing controller -> Fine-tuning with twins -> Model recovery

- **Critical path:**
  1. During initial training, reserve k samples from D (General Spare Set) OR pre-identify twin pairs (Twins Strategy)
  2. Upon unlearning request for sample z, unlearning engine produces M'
  3. Healing controller finds z* = argmin d(z, z') from remaining reserve
  4. Fine-tune M' on Dremaining ∪ {z*} for 1 to N/2 epochs
  5. Remove z* from reserve set

- **Design tradeoffs:**
  - **Reserve set size vs. healing capacity:** Larger k provides more healing opportunities but reduces training data
  - **Twins vs. General Spare:** Twins require more data/computation upfront but provide guaranteed similar replacements; General Spare is simpler but may lack good matches
  - **Fine-tuning duration vs. computational cost:** N/2 epochs gives better recovery but increases healing latency

- **Failure signatures:**
  - Reserve set exhausted before unlearning requests complete
  - Model collapse after Fisher unlearning (Table 2: 10-33% accuracy) may be unrecoverable
  - Feature-space similarity unreliable if backbone feature extractor is severely damaged

- **First 3 experiments:**
  1. **Baseline susceptibility test:** Apply Fisher and Influence unlearning with random 5-30% removal on your dataset; measure if model collapses or degrades gracefully
  2. **Reserve set sizing:** Create reserve sets at 5%, 10%, 20% of training data; measure healing success rate as unlearning requests increase
  3. **Similarity metric comparison:** For a fixed unlearning scenario (25 targeted samples), compare healing effectiveness using raw-L2, feature-cosine, and feature-Mahalanobis similarity for twin selection

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the presence of benign unlearning requests from regular users mitigate or exacerbate adversarial unlearning attacks? The authors hypothesize that benign requests might "paradoxically protect the system," but state this interaction is "beyond the scope of our work" and requires extensive research.

- **Open Question 2:** How can the healing paradigm be effectively adapted for generative models where performance is harder to quantify than classification accuracy? The authors list "Studying analogies for problems other than classification, particularly generative models" as a critical direction for future work.

- **Open Question 3:** What is the minimal reserve set size required to guarantee robustness against specific adversarial capabilities? The authors suggest "Finding a minimal set of spare elements that provide some resistance" as a future direction.

- **Open Question 4:** Can learned similarity metrics (e.g., Siamese networks) outperform standard distance metrics in selecting optimal "twin" samples for healing? The paper notes that current selection relies on Euclidean or Mahalanobis distances, but suggests the "natural extension... may be based on Siamese neural networks."

## Limitations
- The reserve set strategy assumes sufficient similar examples exist in the spare set, which may fail for rare classes or complex domains
- Fisher unlearning collapse (accuracy dropping to 10-33%) represents a fundamental fragility where healing may be impossible regardless of technique
- The method's effectiveness against other unlearning approaches (SISA, Naive) appears more limited, suggesting the solution isn't universally applicable

## Confidence
**High confidence:** The core claim that healing can restore accuracy after targeted unlearning attacks on CIFAR-10 and MNIST, supported by specific numerical results (66.76% → 91.41% for Fisher unlearning).

**Medium confidence:** The mechanism explanation for why feature-space similarity metrics outperform pixel-space metrics, as the paper provides some evidence but lacks ablation studies isolating this effect.

**Medium confidence:** The claim about healing providing resilience against adversarial unlearning more broadly, as the evaluation focuses on specific attack patterns rather than comprehensive threat modeling.

## Next Checks
1. **Reserve set depletion testing:** Systematically evaluate healing performance as the reserve set becomes increasingly depleted (10%, 30%, 50% consumed) to identify the practical limits of the approach.

2. **Cross-domain generalization:** Apply the healing method to non-image datasets (text, tabular) to verify whether the similarity-based replacement strategy transfers across modalities.

3. **Adversarial healing resistance:** Design experiments where an attacker attempts to poison the reserve set or manipulate the similarity metric to degrade healing effectiveness, testing whether the defense is robust to counterattacks.