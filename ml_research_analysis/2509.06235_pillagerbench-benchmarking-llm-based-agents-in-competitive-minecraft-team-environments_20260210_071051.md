---
ver: rpa2
title: 'PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team
  Environments'
arxiv_id: '2509.06235'
source_url: https://arxiv.org/abs/2509.06235
tags:
- action
- cause
- effect
- team
- opponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PillagerBench introduces a novel benchmark for evaluating LLM-based\
  \ multi-agent systems in competitive team-vs-team scenarios within Minecraft, addressing\
  \ the gap in existing benchmarks that focus primarily on cooperative tasks. The\
  \ framework features two competitive scenarios\u2014Mushroom War and Dash & Dine\u2014\
  that test task allocation and strategic adaptation under dynamic, adversarial conditions."
---

# PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments

## Quick Facts
- arXiv ID: 2509.06235
- Source URL: https://arxiv.org/abs/2509.06235
- Reference count: 36
- Primary result: LLM-based multi-agent system outperforms baselines in competitive Minecraft team environments

## Executive Summary
PillagerBench introduces a novel benchmark for evaluating LLM-based multi-agent systems in competitive team-vs-team scenarios within Minecraft, addressing the gap in existing benchmarks that focus primarily on cooperative tasks. The framework features two competitive scenarios—Mushroom War and Dash & Dine—that test task allocation and strategic adaptation under dynamic, adversarial conditions. To tackle these challenges, the authors propose TactiCrafter, an LLM-based multi-agent system with a Tactics Module for strategy generation, a Causal Model for learning game mechanics, and an Opponent Model for inferring enemy strategies. Experimental results show TactiCrafter outperforms both random and Chain-of-Thought baselines across multiple metrics, including points scored (13.05), sabotage (1.55), points difference (-1.16), and win rate (46%). The benchmark and code are open-sourced to foster further research in competitive multi-agent AI.

## Method Summary
PillagerBench evaluates LLM-based multi-agent systems in competitive Minecraft scenarios through two team-vs-team games. Each team has two agents communicating via chat, operating in a 2-minute episode. The system uses Mineflayer API for Minecraft interaction and Docker for environment reproducibility. TactiCrafter implements a four-module architecture: Tactics Module generates high-level strategies, Causal Model builds causal graphs from observations, Opponent Model infers enemy tactics from chat, and Base Agents execute actions through iterative prompting. The benchmark runs agents against built-in opponents across multiple episodes, measuring points scored, sabotage effectiveness, points difference, and win rates.

## Key Results
- TactiCrafter achieves 13.05 average points and 46% win rate against competitive opponents
- Opponent modeling improves performance by 1.53 points against seen opponents but hurts generalization (-2.84 vs unseen)
- Self-play leads to overspecialization, with Dash & Dine performance degrading from +3 to -5.2 after 15 episodes
- GPT-4o outperforms Gemini 2.0 Flash and o3 Mini across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Causal-Graph-Guided Action Generation
- Claim: Maintaining an explicit causal graph enables LLM agents to reason about action preconditions and effects, improving task success in environments with complex dependencies.
- Mechanism: The Causal Model compiles observations into a directed acyclic graph encoding action→cause→effect triplets (e.g., "craftItem(bread)" requires ["wheat"]). This graph conditions the Tactics Module's strategy generation and Base Agents' code synthesis, grounding LLM reasoning in verified game mechanics.
- Core assumption: LLMs will correctly infer causal relationships from chat logs and inventory deltas without interventional verification (the intervention module was removed for latency).
- Evidence anchors:
  - [section IV-B]: "The Causal Model is responsible for compiling information about how the world works by creating a causal graph using LLM-based causal discovery."
  - [section IV-B]: "To work within PillagerBench's limited execution time, we opted to remove the intervention module... Instead we have to rely on identifying causal correlations from observations."
- Break condition: If inventory observations are incomplete or chat logs omit action details, causal edges will be missing or incorrect, propagating errors through tactics and execution.

### Mechanism 2: Opponent-Conditioned Strategy Adaptation
- Claim: Modeling opponent behavior from observable signals (chat messages, actions) enables tactics that counter specific enemy strategies, improving win rates against heterogeneous opponents.
- Mechanism: The Opponent Model extracts opponent chat logs after each episode, summarizes their tactics via few-shot CoT prompting, and feeds this to the Tactics Module. Updated tactics incorporate counter-strategies (e.g., sabotage specific crops an opponent relies on).
- Core assumption: Opponent agents broadcast their actions via chat or control-primitive logs that are visible to the observing team.
- Evidence anchors:
  - [section IV-C]: "The Opponent Model is responsible for figuring out what the opponent team is doing and summarizing it as tactics."
  - [table VII]: After 5 episodes against the same opponent, TactiCrafter achieves +1.53 avg point difference vs same opponent, but -2.84 vs different opponent, showing specialization.
- Break condition: If opponents operate silently or deceptively, inferred opponent tactics will be inaccurate, leading to ineffective counter-strategies.

### Mechanism 3: Self-Play-Driven Strategic Evolution with Overspecialization Risk
- Claim: Self-play generates diverse tactical experiences that can improve action efficiency, but risks overspecialization to self-generated strategies, degrading generalization to unseen opponents.
- Mechanism: TactiCrafter instances play against each other, logging events, updating causal graphs and opponent models. Tactics evolve across episodes (e.g., shifting from cookie-making to golden-carrot focus). However, learned strategies become tuned to the opponent instance, not the broader opponent distribution.
- Core assumption: The self-play opponent distribution is representative enough to transfer; mechanisms exist to decouple world knowledge from opponent-specific beliefs.
- Evidence anchors:
  - [section V-D]: "In self-play, TactiCrafter overfits to its own strategies, leading to worse performance when faced with a different opponent."
  - [figure 13]: Mushroom War improves from -8 to -1.8 after 15 self-play episodes; Dash & Dine degrades from +3 to -5.2.
- Break condition: Without resetting Opponent Models between opponent changes, the system carries over stale opponent-specific beliefs, actively harming performance.

## Foundational Learning

- **Causal Graphical Models (DAGs, intervention vs. observation)**
  - Why needed here: The Causal Model builds a DAG of game mechanics; understanding the difference between interventional and observational causal discovery is critical to interpret its limitations.
  - Quick check question: If you observe "inventory gained bread" after an agent executed code, can you infer which action caused it if multiple crafting actions were attempted?

- **Zero-Sum Game Theory (Nash equilibrium, minimax)**
  - Why needed here: The paper frames team-vs-team scenarios as zero-sum stochastic games; strategy optimization assumes adversarial utility.
  - Quick check question: In a zero-sum game where U_A = P_A - P_B, what is the Nash equilibrium condition for strategy s_A?

- **Chain-of-Thought Prompting (zero-shot vs. few-shot)**
  - Why needed here: TactiCrafter uses CoT across Tactics, Causal, and Opponent modules; the distinction affects when exemplars are available.
  - Quick check question: Which TactiCrafter module uses few-shot CoT, and what triggers the switch from zero-shot to few-shot?

## Architecture Onboarding

- **Component map:**
  Tactics Module -> Causal Model -> Base Agents; Opponent Model -> Tactics Module; Environment Bridge -> All components

- **Critical path:**
  Pre-game (causal graph + initial tactics) → Episode start → Base Agents generate code → Execute → Observe events → Post-game (update causal graph, opponent model, tactics) → Next episode

- **Design tradeoffs:**
  - Latency vs. accuracy: Intervention-based causal discovery removed; relies on observational correlation only
  - Specialization vs. generalization: Opponent model improves against seen opponents but harms transfer; self-play can degrade generalization
  - Token budget vs. completeness: Event deduplication cuts logs by 28%, but may discard repeated-failure signals

- **Failure signatures:**
  - Agents generate code with infinite loops repeating failed actions → event log bloat (mitigated by deduplication)
  - Causal graph missing edges (e.g., no feather after killing chicken) → base agents attempt infeasible actions
  - Opponent model stale after opponent switch → tactics target irrelevant sabotage

- **First 3 experiments:**
  1. Run TactiCrafter vs. "do nothing" opponent for 5 episodes; inspect causal graph growth and tactics evolution
  2. Ablate Opponent Model vs. aggressive built-in opponent; measure win-rate delta
  3. Execute self-play for 20 episodes; checkpoint every 5 episodes; evaluate each checkpoint against all built-in opponents to quantify overspecialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can competitive agents be trained via self-play to avoid overfitting to their own strategies while maintaining adaptability to novel opponents?
- Basis in paper: [explicit] The authors note that in self-play, "TactiCrafter overfits to its own strategies, leading to worse performance when faced with a different opponent," and explicitly ask how to "maintain adaptability while preventing detrimental overspecialization."
- Why unresolved: The current system struggles to distinguish between general world knowledge and opponent-specific specialization during extended self-play.
- What evidence would resolve it: An update protocol that successfully resets opponent specialization while retaining causal knowledge, showing improved performance against diverse unseen opponents after self-play.

### Open Question 2
- Question: Does integrating spatial reasoning and environmental states into the Causal Model improve agent performance in scenarios dependent on non-inventory variables?
- Basis in paper: [explicit] The authors state that because the causal model is based solely on inventory states, it "fails to represent interactions involving block states... we need more sophisticated methods that extend beyond inventory-based causal discovery to incorporate spatial reasoning."
- Why unresolved: The current implementation limits causal discovery to inventory items, ignoring mechanics involving block states, entity behaviors, or temporal dependencies.
- What evidence would resolve it: A comparative study showing TactiCrafter with a spatial-temporal causal graph outperforming the inventory-only version on tasks requiring environmental manipulation.

### Open Question 3
- Question: What is the optimal trade-off between LLM reasoning depth and response latency in real-time competitive environments?
- Basis in paper: [inferred] The paper notes o3-mini achieved lower win rates despite high sabotage scores, attributing performance issues to longer response times required for reasoning tokens in a "real-time benchmark."
- Why unresolved: While reasoning models generate deeper strategic thoughts, the computational delay reduces the number of actions agents can execute within the 2-minute time limit.
- What evidence would resolve it: A benchmark evaluation of models with similar intelligence but varying latencies, correlating win rates with reasoning time per token.

## Limitations

- Causal discovery relies on observational data without interventional verification, potentially missing complex causal relationships
- Self-play experiments reveal substantial overspecialization risks that degrade performance against unseen opponents
- Opponent modeling effectiveness depends on observable signals that may be limited or deceptive in competitive settings

## Confidence

- **High Confidence**: Basic framework architecture and component interactions are well-documented and empirically validated through ablation studies
- **Medium Confidence**: Specific performance improvements are contingent on particular built-in opponents and may not generalize broadly
- **Low Confidence**: Opponent Model's ability to accurately infer and counter opponent strategies from chat logs alone is questionable

## Next Checks

1. **Causal Graph Accuracy Validation**: Instrument the system to log all causal edges with confidence scores and verify against ground-truth action-effect relationships in controlled test cases. Measure precision and recall of the causal discovery mechanism.

2. **Opponent Model Generalization Test**: Run the self-play experiments for 50 episodes, saving checkpoints every 10 episodes. Test each checkpoint against 20 different built-in opponents (including novel strategies) to quantify the trade-off between specialization and generalization.

3. **Intervention vs. Observation Comparison**: Implement a lightweight intervention module that occasionally forces action execution and observes effects, then compare the causal graph accuracy and downstream task performance against the purely observational approach.