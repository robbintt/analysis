---
ver: rpa2
title: Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making
  in Vision Transformer
arxiv_id: '2503.14640'
source_url: https://arxiv.org/abs/2503.14640
tags:
- block
- attention
- daam
- proposed
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of interpreting how Vision Transformer
  (ViT) models make decisions by visualizing the attention flow through their internal
  blocks. The authors propose Dynamic Accumulated Attention Maps (DAAM), a novel method
  that decomposes the [class] token to extract spatial semantic information and combines
  it with channel importance coefficients to generate attention maps for each ViT
  block.
---

# Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer

## Quick Facts
- **arXiv ID**: 2503.14640
- **Source URL**: https://arxiv.org/abs/2503.14640
- **Authors**: Yi Liao; Yongsheng Gao; Weichuan Zhang
- **Reference count**: 40
- **Primary result**: DAAM achieves 56.45% IoU and 61.58% accuracy on DeiT-small-patch16, outperforming six CAM-based methods.

## Executive Summary
This paper addresses the problem of interpreting how Vision Transformer (ViT) models make decisions by visualizing the attention flow through their internal blocks. The authors propose Dynamic Accumulated Attention Maps (DAAM), a novel method that decomposes the [class] token to extract spatial semantic information and combines it with channel importance coefficients to generate attention maps for each ViT block. For self-supervised ViTs, dimension-wise importance weights are introduced. The attention flow is revealed by block-wise accumulation of these maps.

## Method Summary
DAAM extracts spatial semantic information from ViT's [class] token by computing element-wise multiplication of class attention weights with value matrices at each block (S_b = a^0_b ⊗ V_b). For supervised ViTs, it computes channel importance coefficients via gradient decomposition of classification scores. For self-supervised ViTs, it uses dimension-wise importance weights from memory bank similarities. Attention maps are generated by combining these spatial features with importance coefficients, then accumulated across blocks to reveal attention evolution. The method is evaluated on 9 ViT models using ImageNet, showing superior localization performance and passing sanity checks.

## Key Results
- DAAM achieves 56.45% IoU and 61.58% accuracy (IoU≥0.5) on DeiT-small-patch16, outperforming six CAM-based methods.
- Successfully visualizes attention evolution from intermediate to output blocks across 9 ViT architectures.
- Passes sanity checks, demonstrating sensitivity to model parameters when weights are randomized.
- Provides insights into incorrect classifications and reveals how inserted modules (XCA, LayerScale) affect attention formation.

## Why This Works (Mechanism)

### Mechanism 1: Spatial Feature Preservation via Element-wise Decomposition
The standard self-attention operation `softmax(QK^T/√d)V` sums over spatial positions, destroying localization information. DAAM computes `S_B = a^0_B ⊗ V_B` (element-wise multiplication of class attention vector with value matrix), yielding a `(N+1)×D` matrix where each of the `N+1` positions retains its spatial identity. This matrix is then weighted by channel importance coefficients and summed across channels to produce spatial attention maps.

### Mechanism 2: Channel Importance via Classification Score Decomposition
For supervised ViTs, the classification score `Y^c = X^0_B W^c + bias`. DAAM computes importance coefficients via chain rule: `C^c_B = W^c × ∂X^0_B/∂T_B`. These coefficients weight each channel's contribution to the final decision. After ReLU filtering (to exclude negative contributions), channels are combined: `L^c_B = Reshape(Σ_d ReLU(C^c_B) ⊗ S_B[1:])`.

### Mechanism 3: Block-wise Accumulation Reveals Attention Evolution
Each block produces an attention map `L^c_b`. Rather than visualizing each independently, DAAM accumulates: `L^c_DAAM = Σ^B_b=1 L^c_b`. This shows how early coarse attention refines into final discriminative regions.

## Foundational Learning

- **Concept: Self-attention mechanism and the [class] token role**
  - **Why needed here:** DAAM fundamentally manipulates the self-attention computation to extract spatial information. Without understanding how `Q, K, V` matrices interact and why the [class] token aggregates information for classification, the decomposition mechanism will be opaque.
  - **Quick check question:** Can you explain why the first row of the attention matrix (`a^0_B`) represents class-to-patch relationships, and why matrix multiplication `a^0_B × V_B` loses spatial indexing?

- **Concept: Gradient-based attribution methods (Grad-CAM family)**
  - **Why needed here:** DAAM's channel importance computation via score decomposition extends gradient-based attribution logic to transformers. Understanding how gradients flow through layers to weight feature importance provides the conceptual foundation.
  - **Quick check question:** Why does DAAM use `∂Y^c/∂T_B` rather than `∂Y^c/∂X^0_B` directly, and what does the intermediate `T_B = a^0_B V_B` representation enable?

- **Concept: Self-supervised learning with memory banks (DINO, KNN classifiers)**
  - **Why needed here:** For self-supervised ViTs, DAAM must compute importance without FC layer gradients. The dimension-wise importance weights derive from cosine similarity to memory bank features—a different paradigm requiring understanding of instance discrimination.
  - **Quick check question:** How does Eq. (13) compute dimension-wise importance from K-nearest neighbors, and why must the denominator normalize by `||cos(x^0_B, z_k)|| × ||x^0_B|| × ||z_k||`?

## Architecture Onboarding

- **Component map:** Decomposition Module -> Importance Coefficient Computer -> Map Generator -> Accumulator -> Upsampler
- **Critical path:**
  1. Forward pass through ViT, hooking MHSA outputs at each block to capture `(a^0_b, V_b)` pairs
  2. For target class c, compute `C^c_b` via one backward pass (or `w_d` via memory bank lookup for self-supervised)
  3. Generate `L^c_b` for each block, accumulate progressively
  4. Normalize (global max-min across all accumulated maps) and upsample

- **Design tradeoffs:**
  - Memory vs. granularity: Storing `S_b` for all blocks requires `B × (N+1) × D` floats. For ViT-B/16 on 224×224 images: 12 blocks × 197 tokens × 768 dim ≈ 1.8M floats per image
  - Supervised-only vs. universal: The dimension-wise importance weights for self-supervised models add complexity but extend applicability; if only working with supervised ViTs, this branch can be omitted
  - Accumulation vs. per-block: Accumulation shows evolution but may obscure block-specific contributions; consider also visualizing individual `L^c_b` for diagnostic purposes

- **Failure signatures:**
  - Uniform attention maps: Class attention `a^0_B` has near-uniform distribution—model isn't discriminating spatially, or decomposition isn't capturing the right signal
  - Early blocks dominate accumulation: If `L^c_1` overwhelms later blocks after normalization, check if channel importance coefficients are decaying too slowly through gradient computation
  - Self-supervised maps nonsensical: Memory bank may be poorly matched; verify KNN is returning meaningful neighbors (check similarity scores)

- **First 3 experiments:**
  1. Sanity check replication: Randomize weights from top block progressively (as in Fig. 6), verify attention maps degrade—if they don't, implementation is likely incorrect (not using model parameters)
  2. Single architecture baseline: Run DAAM on DeiT-small-patch16 with ImageNet validation subset (1000 images), compute IoU against bounding boxes using Eq. (15)—target ~56% IoU as reported in Table 3
  3. Attention flow visualization: Generate 12-block progression for 10 correctly classified images across 3 architectures (DeiT, DINO, T2T-ViT), qualitatively verify that attention concentrates on foreground objects progressively—compare against Grad-CAM baseline to confirm DAAM shows evolution while Grad-CAM doesn't

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the method be adapted to visualize attention flow in intermediate blocks where the [class] token is unavailable?
  - **Basis in paper:** [explicit] The authors state the method "currently cannot interpret and visualize the attention flow within intermediate blocks of a ViT where the [class] token is not available."
  - **Why unresolved:** The current decomposition module (Eq. 7-9) relies mathematically on the [class] token ($x_0$) to extract spatial semantic information.
  - **What evidence would resolve it:** A modified formulation utilizing the entire attention matrix ($A$) rather than the class attention row ($a^0$) to preserve spatial information for early layers (e.g., in CaiT/XCiT).

- **Open Question 2:** Can DAAM be integrated with iterative integration techniques to function as a model-agnostic tool for CNNs?
  - **Basis in paper:** [explicit] The conclusion proposes that "integrating DAAM with iterative integration-based explanation methods could enable its adaptation to CNN-based models."
  - **Why unresolved:** DAAM is designed for the Transformer's self-attention mechanism and token structure, which CNNs lack.
  - **What evidence would resolve it:** A successful extension of the accumulation logic to Convolutional Neural Networks, possibly by mapping convolutional features to a compatible iterative integration framework.

- **Open Question 3:** Can the visualized attention flow be formalized into a quantitative metric to guide the design of novel ViT architectures?
  - **Basis in paper:** [explicit] The introduction states future research will focus on "leveraging DAAM’s visual explanation maps to guide the design of novel model architectures."
  - **Why unresolved:** The paper provides qualitative visualizations of block contributions (e.g., block 6 vs. block 10) but does not define a quantitative rule for optimizing architectural efficiency based on this flow.
  - **What evidence would resolve it:** A study demonstrating improved model performance or efficiency by pruning or modifying blocks identified as "low contribution" by DAAM metrics.

## Limitations
- The method currently cannot interpret attention flow within intermediate blocks where the [class] token is unavailable, limiting analysis of early layer behavior.
- Performance relies on the assumption that class attention weights contain sufficient spatial discrimination information, which may not hold for all models or classes.
- The self-supervised dimension-wise importance computation requires careful memory bank implementation and may be sensitive to hyperparameter choices.

## Confidence

- **High confidence:** The decomposition mechanism (S_b = a^0_b ⊗ V_b) and accumulation strategy are mathematically sound and directly supported by the paper's equations and Fig. 2-5 visualizations.
- **Medium confidence:** The gradient decomposition for supervised importance coefficients relies on reasonable assumptions about feature attribution, but the exact autograd path through LayerNorm and FFN could affect results.
- **Medium confidence:** Localization performance claims (56.45% IoU, 61.58% accuracy) are supported by Table 3, but the evaluation methodology (thresholding, connected component extraction) has implementation-sensitive details.

## Next Checks

1. **Gradient path verification:** Implement and compare three variants of ∂X⁰_B/∂T_B computation—through FFN only, through FFN+LayerNorm, and through full block including residual connection. Measure how each affects IoU on DeiT-small-patch16 (5-10 image subset).
2. **Self-supervised sensitivity analysis:** Run DAAM on DINO-ViT-small-patch8 with varying K (5, 10, 20) in KNN memory bank, and measure IoU stability. If maps vary wildly with K, dimension-wise importance weights may be unreliable.
3. **Cross-architecture consistency:** Generate 12-block attention flows for the same 10 images across DeiT, T2T-ViT, and ViT-base. Verify that attention evolution patterns (coarse-to-fine refinement) are consistent across architectures, or identify architecture-specific failure modes.