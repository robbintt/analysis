---
ver: rpa2
title: 'Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation'
arxiv_id: '2511.10020'
source_url: https://arxiv.org/abs/2511.10020
tags:
- anomaly
- generation
- anomalies
- anomagic
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anomagic, a zero-shot anomaly generation
  method that leverages crossmodal prompts (combining visual and textual cues) to
  synthesize realistic anomalies without requiring any exemplar anomalies. The approach
  uses a crossmodal prompt encoding scheme to extract fine-grained conditions from
  anomaly-mask-caption triplets, then fine-tunes a pre-trained inpainting diffusion
  model with LoRA to generate anomalies guided by these conditions.
---

# Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation

## Quick Facts
- arXiv ID: 2511.10020
- Source URL: https://arxiv.org/abs/2511.10020
- Authors: Yuxin Jiang; Wei Luo; Hui Zhang; Qiyu Chen; Haiming Yao; Weiming Shen; Yunkang Cao
- Reference count: 12
- Primary result: State-of-the-art zero-shot anomaly generation using crossmodal prompts combining visual masks and textual descriptions

## Executive Summary
Anomagic introduces a novel approach to zero-shot anomaly generation that leverages crossmodal prompts combining visual masks with textual descriptions to synthesize realistic anomalies without requiring exemplar anomalies. The method employs a crossmodal prompt encoding scheme that extracts fine-grained conditions from anomaly-mask-caption triplets, then fine-tunes a pre-trained inpainting diffusion model with LoRA to generate anomalies guided by these conditions. A contrastive refinement strategy ensures precise alignment between generated anomalies and their masks. The approach is trained on AnomVerse, a dataset of 12,987 anomaly-mask-caption triplets automatically generated using multimodal large language models.

## Method Summary
Anomagic operates through a three-stage pipeline: first, it extracts fine-grained conditions from anomaly-mask-caption triplets using a crossmodal prompt encoding scheme; second, it fine-tunes a pre-trained inpainting diffusion model with LoRA to learn the generation of anomalies guided by these conditions; third, it employs a contrastive refinement strategy to ensure precise alignment between generated anomalies and their masks. The method is trained on AnomVerse, a synthetic dataset automatically generated using multimodal large language models, which contains 12,987 anomaly-mask-caption triplets covering diverse anomaly types and scenarios.

## Key Results
- Achieves state-of-the-art performance in generating realistic and diverse anomalies for zero-shot settings
- Improves downstream anomaly detection accuracy with F1 scores reaching 96.77% on the VisA dataset
- Demonstrates strong generalization to arbitrary user-defined prompts beyond the training distribution

## Why This Works (Mechanism)
The crossmodal approach works by leveraging complementary information from both visual masks (providing spatial location and shape) and textual descriptions (providing semantic context and fine-grained details). This dual conditioning allows the model to generate anomalies that are both spatially accurate and semantically coherent. The LoRA-based fine-tuning enables efficient adaptation of the pre-trained inpainting model to the anomaly generation task while preserving the model's ability to generate high-quality images. The contrastive refinement strategy further aligns the generated anomalies with their corresponding masks, ensuring precise localization and shape consistency.

## Foundational Learning
- Crossmodal prompt encoding: Why needed - to extract fine-grained conditions from combined visual and textual inputs; Quick check - verify that both mask and caption contribute meaningfully to the final generation
- LoRA fine-tuning: Why needed - to efficiently adapt pre-trained models to new tasks with minimal parameter changes; Quick check - confirm that LoRA adaptation preserves generation quality while enabling anomaly-specific capabilities
- Contrastive refinement: Why needed - to ensure precise alignment between generated anomalies and their spatial masks; Quick check - validate that refinement improves spatial accuracy without degrading visual quality

## Architecture Onboarding

Component map: Crossmodal Encoder -> LoRA Fine-tuning Module -> Diffusion Inpainting Model -> Contrastive Refinement -> Anomaly Generation

Critical path: The core workflow follows Crossmodal Encoder -> LoRA Fine-tuning Module -> Diffusion Inpainting Model, with Contrastive Refinement applied as a post-processing step to improve alignment quality.

Design tradeoffs: The method trades computational efficiency (through LoRA fine-tuning) for adaptation capability, and prioritizes zero-shot generalization over fine-tuning on specific target domains. The reliance on automatically generated captions introduces potential quality variations that must be managed.

Failure signatures: Common failure modes include spatial misalignment between generated anomalies and masks, semantic inconsistencies between generated anomalies and their textual descriptions, and reduced visual quality when prompts are too specific or unusual.

First experiments to run:
1. Generate anomalies using only visual masks without textual prompts to quantify the contribution of crossmodal information
2. Test the model's ability to generate anomalies for previously unseen object categories to evaluate true zero-shot generalization
3. Compare generation quality with and without the contrastive refinement step to measure its impact on alignment accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the main discussion and limitations sections.

## Limitations
- Reliance on automatically generated captions from multimodal large language models may introduce quality inconsistencies and reliability concerns
- Evaluation metrics, particularly Inception Score, may not fully capture the perceptual realism and diagnostic value of generated anomalies in practical scenarios
- Limited rigorous testing of generalization capabilities across diverse domain shifts and prompt variations

## Confidence

Major claim confidence assessments:
- Zero-shot anomaly generation performance: High
- Crossmodal prompt encoding effectiveness: Medium
- Generalization to arbitrary prompts: Medium
- Dataset quality and representativeness: Low

## Next Checks
1. Conduct a human perceptual study comparing Anomagic-generated anomalies against real anomalies to validate the claimed visual realism beyond automated metrics
2. Perform systematic ablation studies on the crossmodal prompt encoding scheme to quantify the contribution of visual versus textual components to final output quality
3. Test the method's performance on domain-shifted datasets not represented in the AnomVerse training data to evaluate true zero-shot generalization capabilities