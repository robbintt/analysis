---
ver: rpa2
title: An LLM-Powered Agent for Real-Time Analysis of the Vietnamese IT Job Market
arxiv_id: '2511.14767'
source_url: https://arxiv.org/abs/2511.14767
tags:
- data
- market
- agent
- online
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing reliable, real-time
  career guidance in Vietnam's rapidly evolving IT job market, where traditional reports
  are outdated and manual analysis of job postings is impractical. The authors propose
  an AI Job Market Consultant, a conversational agent that leverages an LLM-powered
  data pipeline and a tool-augmented AI agent to deliver deep, data-driven insights
  from labor market data.
---

# An LLM-Powered Agent for Real-Time Analysis of the Vietnamese IT Job Market

## Quick Facts
- arXiv ID: 2511.14767
- Source URL: https://arxiv.org/abs/2511.14767
- Authors: Minh-Thuan Nguyen; Thien Vo-Thanh; Thai-Duy Dinh; Xuan-Quang Phan; Tan-Ha Mai; Lam-Son Lê
- Reference count: 40
- One-line primary result: AI Job Market Consultant, a conversational agent leveraging LLM-powered data pipeline and tool-augmented AI, delivers deep, data-driven insights from real-time labor market data.

## Executive Summary
This paper addresses the challenge of providing reliable, real-time career guidance in Vietnam's rapidly evolving IT job market, where traditional reports are outdated and manual analysis of job postings is impractical. The authors propose an AI Job Market Consultant, a conversational agent that leverages an LLM-powered data pipeline and a tool-augmented AI agent to deliver deep, data-driven insights from labor market data. The system crawls job postings, uses an LLM to structure unstructured data, and employs semantic enrichment to label skills. The core AI agent, based on the ReAct framework, autonomously reasons, plans, and executes actions using a specialized toolbox for SQL queries, semantic search, and data visualization. The prototype collected and analyzed 3,745 job postings, demonstrating its ability to answer complex queries, generate on-demand visualizations, and provide personalized career advice.

## Method Summary
The system consists of an offline ETL pipeline and an online agent. The offline pipeline uses Playwright to crawl Vietnamese job portals, extracts raw text, and employs an LLM (Gemini 2.5 family) to parse unstructured postings into structured JSON. Semantic enrichment via embeddings and cosine similarity tags each job with top-10 skills from a 299-skill library. Data is stored in PostgreSQL with pgvector. The online agent uses the ReAct framework to handle natural language queries through iterative Thought-Action-Observation cycles, invoking tools for SQL queries, semantic search, visualization, and career advice. The agent synthesizes responses grounded in the database rather than parametric memory.

## Key Results
- Prototype collected and analyzed 3,745 job postings from Vietnamese portals (Jul 1–Aug 8, 2025)
- Demonstrated ability to answer complex queries and generate on-demand visualizations
- Showed more reliable, verifiable insights compared to conventional LLM chatbots through direct comparison
- Provided personalized career advice via semantic vector search

## Why This Works (Mechanism)

### Mechanism 1: LLM as Intelligent Structured Parser
Using an LLM to parse unstructured job postings into structured JSON enables downstream querying that rule-based parsers cannot achieve. Raw job descriptions are passed to an LLM with a prompt instructing it to act as an "expert HR specialist," extracting fields like `company_information`, `job_description`, and `job_requirements` into a predefined JSON schema. This transforms messy, inconsistent text into clean, queryable data.

### Mechanism 2: ReAct Framework for Multi-Step Reasoning with Tools
The ReAct (Reasoning + Acting) framework enables the agent to decompose complex natural language queries into iterated tool calls, improving reliability over single-pass LLM responses. The agent cycles through Thought → Action (tool call) → Observation (tool output) until the user's goal is met, enabling complex queries like "show top 10 skills" to be broken down into sequential tool executions.

### Mechanism 3: Mandatory Tool-Use for Grounded, Verifiable Responses
Forcing the agent to query an external database via tools (rather than relying on parametric memory) reduces hallucination and enables verifiable answers. The agent has no direct answer capability for data queries—it must invoke `query_database(sql_query: str)` or other tools. Responses are synthesized from retrieved data, making reasoning traceable and claims verifiable against source records.

## Foundational Learning

- **Concept: ReAct Agentic Framework**
  - Why needed here: The system's core reasoning engine relies on iterated Thought-Action-Observation cycles. Without understanding ReAct, you cannot debug why the agent selected wrong tools or loops indefinitely.
  - Quick check question: Given a user query "Compare Python vs. Java salaries," can you trace what tools the agent should call and in what order?

- **Concept: Vector Embeddings and Semantic Search**
  - Why needed here: Skill labeling and the Career Advisor Tool depend on embedding job requirements and comparing via cosine similarity. Without this, you cannot understand or modify the semantic enrichment pipeline.
  - Quick check question: If "React" and "React.js" map to different embedding vectors, how would you ensure both match the same canonical skill?

- **Concept: ETL Pipelines for Unstructured Data**
  - Why needed here: The offline pipeline (crawling → LLM structuring → semantic enrichment → DB load) is the foundation for all online queries. Without understanding ETL, you cannot diagnose stale or malformed data issues.
  - Quick check question: If the crawler fails on a new job portal with different HTML structure, at which ETL stage does the pipeline break, and how would you detect it?

## Architecture Onboarding

- **Component map:** Playwright crawler → raw job text → LLM parser (JSON structuring) → semantic enrichment (embeddings + skill labeling via cosine similarity) → PostgreSQL (with pgvector extension)
- **Online Agent:** User query (Streamlit UI) → ReAct agent (Gemini 2.5 family) → Toolbox (SQL queries, semantic search, visualization tools, career advisor) → Database → Response synthesis → UI rendering
- **Critical path:** Query parsing and intent classification by ReAct agent → Tool selection and parameter generation (e.g., SQL construction) → Tool execution against database → Observation synthesis into natural language + optional visualization
- **Design tradeoffs:** LLM-as-parser vs. rule-based offers higher flexibility but higher cost and potential inconsistency vs. deterministic but brittle rules; proprietary real-time DB vs. static training data enables verifiable, current insights but requires ongoing crawl/maintenance overhead vs. no infrastructure cost but stale/generic responses; tool-augmented vs. pure LLM enforces grounding and traceability but introduces latency and complexity (tool failures, SQL generation errors)
- **Failure signatures:** Agent loops indefinitely without completing Thought-Action-Observation cycles (tool missing or ambiguous intent); SQL generation errors return empty or incorrect results (schema mismatch, malformed query); Skill labeling misses obvious skills or produces false positives (embedding threshold issues, skill library gaps); Stale data responses (crawler stopped, pipeline not run recently); Visualization tool generates charts but agent fails to synthesize coherent explanation
- **First 3 experiments:** Validate LLM structuring accuracy: Sample 50 raw job postings, manually verify JSON extraction against ground truth; measure field-level precision/recall; Identify systematic error patterns (e.g., missing salary fields, misclassified requirements); Stress-test ReAct loop with adversarial queries: Submit 20 ambiguous or multi-intent queries; trace reasoning paths, count tool calls, identify failure modes (wrong tool, excessive iterations); Evaluate semantic enrichment coverage: Compare auto-labeled skills against manual annotations for 100 postings; compute skill match rate and analyze missed skills vs. false positives; Adjust cosine similarity threshold and re-test

## Open Questions the Paper Calls Out
None

## Limitations
- Vietnamese language processing: Exact embedding model and optimization for Vietnamese text are unspecified, potentially affecting skill labeling accuracy and semantic search quality
- Grounding reliability: System's reliability depends entirely on completeness and representativeness of crawled job postings, which may be biased or incomplete
- LLM parser consistency: Lacks validation metrics for parsing stage accuracy, with uncertainty about error rates in field extraction for Vietnamese job postings

## Confidence
- **High Confidence**: ReAct framework implementation for multi-step reasoning and tool use is well-documented and follows established patterns; comparison with conventional LLM chatbots on verifiability is clearly demonstrated
- **Medium Confidence**: ETL pipeline design is logically sound but lacks quantitative validation of each stage's accuracy; semantic enrichment depends on embedding model quality and skill library completeness
- **Low Confidence**: System's ability to handle complex, ambiguous queries is demonstrated through case studies but not systematically evaluated; paper doesn't address edge cases like multi-intent queries or tool selection failures

## Next Checks
1. Validate LLM structuring accuracy: Sample 50 raw job postings and manually verify JSON extraction against ground truth; measure field-level precision/recall to identify systematic error patterns and assess whether the LLM can consistently parse Vietnamese job posting formats
2. Stress-Test ReAct loop with adversarial queries: Submit 20 ambiguous or multi-intent queries that require combining multiple tools; trace reasoning paths, count tool calls, and identify failure modes like wrong tool selection or excessive iterations
3. Evaluate semantic enrichment coverage: Compare auto-labeled skills against manual annotations for 100 postings; compute skill match rate and analyze missed skills (gap in canonical library) versus false positives (threshold too low); adjust cosine similarity threshold and re-test to optimize coverage versus precision