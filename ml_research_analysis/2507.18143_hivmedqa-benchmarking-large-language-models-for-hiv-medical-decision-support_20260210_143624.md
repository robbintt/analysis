---
ver: rpa2
title: 'HIVMedQA: Benchmarking large language models for HIV medical decision support'
arxiv_id: '2507.18143'
source_url: https://arxiv.org/abs/2507.18143
tags:
- clinical
- medical
- question
- score
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# HIVMedQA: Benchmarking large language models for HIV medical decision support

## Quick Facts
- arXiv ID: 2507.18143
- Source URL: https://arxiv.org/abs/2507.18143
- Reference count: 40
- Primary result: General-purpose LLMs (Gemini 2.5 Pro, Claude 3.5 Sonnet) outperformed specialized medical models on HIV clinical decision support tasks.

## Executive Summary
This study benchmarks ten large language models on HIVMedQA, a dataset of 53+ medical questions spanning basic knowledge, USMLE-style problems, complex clinical vignettes, and cognitively biased scenarios. The authors find that general-purpose models often outperform medically fine-tuned ones, and introduce an LLM-as-a-judge framework (MedGPT) that better captures clinical accuracy than traditional lexical metrics. Key findings include susceptibility to cognitive biases and the inadequacy of standard evaluation methods for medical reasoning tasks.

## Method Summary
The HIVMedQA dataset contains 53+ medical questions across four categories, each with expert-validated gold-standard answers. Ten LLMs (7 general, 3 medical) were evaluated using two methods: an LLM-as-a-judge (MedGPT using GPT-4o) scoring responses on 1-5 scales for comprehension, reasoning, knowledge, bias, and harm; and a lexical metric (MedSynF1) measuring entity overlap with synonym expansion. Five inference iterations per question-model pair were conducted using default parameters and a unified system prompt. Models were ranked by MedGPT scores, with correlation analysis against MedSynF1.

## Key Results
- General-purpose models (Gemini 2.5 Pro, Claude 3.5 Sonnet) outperformed specialized medical models on HIV clinical decision support tasks.
- MedGPT evaluation captured clinical accuracy better than lexical overlap metrics, which often penalized correct answers for phrasing differences.
- Models showed performance drops on cognitively biased questions, demonstrating vulnerability to recency and status quo biases in clinical reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-as-a-judge evaluation captures clinical relevance and semantic accuracy better than lexical overlap metrics.
- Mechanism: The framework uses GPT-4o with a specific "senior clinician" persona (MedGPT) to score responses on a 1-5 scale across reasoning, comprehension, and safety. Unlike lexical matching (F1 score), which fails when synonyms or structural variations are used, the LLM-judge interprets the intent and medical validity of the answer.
- Core assumption: The judge model (GPT-4o) possesses sufficient medical knowledge and alignment to reliably approximate human expert evaluation.
- Evidence anchors:
  - [abstract] "evaluating responses using an LLM-as-a-judge proved more effective in capturing clinical accuracy than traditional lexical matching methods."
  - [section] Section 3.6 notes that while MedSynF1 penalizes correct answers for stylistic differences, MedGPT scores rephrased gold-standard answers near-perfectly (avg 4.87), confirming its sensitivity to quality over syntax.
  - [corpus] Neighbor paper *ART: Action-based Reasoning Task Benchmarking* supports the broader trend that standard benchmarks inadequately assess complex clinical actions, reinforcing the need for semantic evaluators.
- Break condition: If the judge model exhibits consistent bias towards verbose or confident-sounding but incorrect answers (sycophancy), the scoring validity degrades.

### Mechanism 2
- Claim: General-purpose foundation models can outperform medically fine-tuned models in complex clinical reasoning due to retained generalization capabilities.
- Mechanism: Specialized models (e.g., Meditron, Med42) are fine-tuned on narrower corpora (biomedical literature), which may lead to "forgetting" of general reasoning flexibility or overfitting to specific formats. Generalist models (e.g., Gemini, Claude) retain broader pattern matching and instruction-following capabilities essential for ambiguous clinical vignettes.
- Core assumption: The general-purpose models' pre-training data includes sufficient high-quality medical text to establish a robust baseline before instruction tuning.
- Evidence anchors:
  - [abstract] "Medically fine-tuned models did not always outperform general-purpose ones."
  - [section] Section 4 Discussion states that fine-tuning may impair general reasoning due to "forgetting" and that models like Med42 were aligned using general preference datasets rather than clinical expertise.
  - [corpus] Neighbor *Mind the Gap* reinforces that benchmarks often reflect specific high-income settings, suggesting specialized training might overfit to these specific distributions rather than general reasoning.
- Break condition: If the domain shift is extreme (e.g., rare tropical diseases not in general pre-training), fine-tuned models should theoretically regain the advantage.

### Mechanism 3
- Claim: Model performance is susceptible to cognitive bias framing in prompts, degrading clinical reliability.
- Mechanism: LLMs prioritize immediate context (recency in the prompt window) or statistically probable associations (frequency). When questions are modified to include misleading framing (e.g., "Recently, there was a patient with similar symptoms..."), models like NVLM-70B and Meditron anchor on this noise rather than clinical facts, dropping performance scores.
- Core assumption: The model's attention mechanism assigns high weight to the injected bias text, overriding the logical inference path required by the clinical vignette.
- Evidence anchors:
  - [abstract] "cognitive biases such as recency and status quo were observed."
  - [section] Section 3.5 details specific experiments where recency bias (mentioning a prior patient's diagnosis) caused performance drops (e.g., MedGPT score drop from 5 to 2.6).
  - [corpus] Corpus evidence on bias in this specific HIV context is weak; related papers focus on benchmark representativeness (*Mind the Gap*) rather than cognitive bias injection mechanisms.
- Break condition: If the model employs explicit "chain-of-thought" reasoning that forces it to verbalize evidence before concluding, it may self-correct and resist the bias.

## Foundational Learning

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** To evaluate open-ended medical answers where "correctness" is nuanced and lexical overlap (BLEU/ROUGE) is insufficient.
  - **Quick check question:** Does the evaluation metric penalize a correct answer for using a synonym (e.g., "antiretroviral therapy" vs. "ART")?

- **Concept: Cognitive Bias Injection**
  - **Why needed here:** To stress-test model robustness. Real clinical queries may contain misleading patient history or user framing; models must isolate relevant clinical data.
  - **Quick check question:** If a prompt mentions a rare diagnosis first, does the model disproportionately favor it despite contradictory symptoms?

- **Concept: Domain Specificity vs. Generalization**
  - **Why needed here:** To select the right model architecture. Understanding why "bigger" or "more medical" doesn't always mean "better" prevents over-engineering.
  - **Quick check question:** Why might a model trained only on medical textbooks fail to understand a colloquial description of symptoms?

## Architecture Onboarding

- **Component map:**
  - Dataset (HIVMedQA) -> Target Models (10 LLMs) -> Evaluator (MedGPT + MedSynF1)

- **Critical path:**
  1. Question Ingestion -> System Prompt Application ("You are a helpful... senior physician")
  2. Model Inference (5 iterations per question for consistency)
  3. Dual Evaluation: MedGPT scoring (1-5 on 5 dimensions) AND MedSynF1 calculation (Entity extraction + Synonym matching)
  4. Analysis: Correlating complexity (Categories 1-3) with performance degradation

- **Design tradeoffs:**
  - *Proprietary vs. Open:* Paper found proprietary models (Gemini, Claude) top-performing but opaque. Open models (Llama) offer auditability but lower reasoning scores.
  - *Evaluation Method:* MedGPT is semantic and nuanced but requires a strong judge model (cost/latency). MedSynF1 is deterministic but brittle to phrasing changes.

- **Failure signatures:**
  - *High F1 / Low MedGPT:* Model is guessing keywords without understanding context (unlikely in this dataset).
  - *Low F1 / High MedGPT:* Model gives a clinically correct answer using different terminology/phrasing than the gold standard (common).
  - *Category 4 Drop:* Model fails to ignore injected "recency" or "frequency" noise.

- **First 3 experiments:**
  1. **Baselining Judge Consistency:** Run MedGPT evaluation on the rephrased gold-standard answers to confirm the judge aligns with human expert quality (target score >4.5).
  2. **Bias Sensitivity Test:** Run Category 3 vs. Category 4 comparisons on a smaller open-source model (e.g., Llama 3.1-8B) to verify if the "recency bias" failure mode is reproducible.
  3. **Lexical vs. Semantic Gap:** Plot MedSynF1 vs. MedGPT-Knowledge scores for Gemini 2.5 Pro to visualize how often correct clinical reasoning results in low lexical overlap.

## Open Questions the Paper Calls Out

- **Open Question 1:** How closely do MedGPT (LLM-as-a-judge) scores correlate with human expert clinician evaluations of medical reasoning and safety?
  - **Basis in paper:** [explicit] The authors state, "Human validation is essential to confirm our findings. One key direction is to involve clinicians in evaluating the model-generated scores (e.g., MedGPT ratings) to assess alignment with expert judgment."
  - **Why unresolved:** The study validated MedGPT primarily against rephrased gold-standard answers and a lower-bound model (Llama 1B), but did not conduct a broad human expert evaluation to confirm the "judge" accurately reflects clinical nuance.
  - **What evidence would resolve it:** A correlation analysis comparing MedGPT scores against blinded ratings from a panel of infectious disease specialists for the same set of model responses.

- **Open Question 2:** Does performance on the HIVMedQA benchmark improve when models utilize retrieval-augmented generation (RAG) with specific clinical guidelines?
  - **Basis in paper:** [explicit] The authors note they "did not examine the potential benefits of incorporating clinical guidelines or knowledge bases through a retrieval-augmented generation (RAG) system," despite suggesting it as a method to enhance factual accuracy.
  - **Why unresolved:** The current benchmark evaluated models based on parametric knowledge and prompt engineering, without assessing external knowledge retrieval capabilities which are standard in clinical support tools.
  - **What evidence would resolve it:** A comparative study measuring MedGPT and factual accuracy scores of the same LLMs on HIVMedQA questions when equipped with a RAG mechanism versus their baseline performance.

- **Open Question 3:** To what extent does LLM assistance improve the diagnostic accuracy, speed, and confidence of junior versus senior clinicians in HIV management?
  - **Basis in paper:** [explicit] The paper suggests that "presenting clinicians with clinical questions both with and without LLM assistance would allow us to assess the added value of LLMs in terms of not only answer accuracy but also response speed and clinician confidence."
  - **Why unresolved:** This study evaluated LLMs in isolation ("curbside consult" simulation) but did not measure the efficacy of the human-AI hybrid team in a simulated clinical workflow.
  - **What evidence would resolve it:** Results from a randomized controlled trial where clinicians complete HIVMedQA vignettes with and without AI support, tracking changes in correct diagnosis rates and time-to-solution.

## Limitations
- The dataset's relatively small size (53 questions) limits generalizability to broader HIV care scenarios.
- The LLM-as-a-judge framework's reliability depends heavily on the assumed medical competence of GPT-4o, which remains untested against adversarial prompts or domain shift.
- The study did not assess the efficacy of human-AI hybrid teams in clinical workflows, focusing only on isolated LLM performance.

## Confidence
- **High confidence**: That lexical overlap metrics (F1) inadequately capture medical reasoning quality (supported by direct experimental comparison showing gold-standard answers receiving low MedSynF1 despite high MedGPT scores).
- **Medium confidence**: That general-purpose models can outperform fine-tuned medical models in complex reasoning (supported by Gemini 2.5 Pro ranking highest, but mechanism requires further validation with larger sample of fine-tuned models).
- **Low confidence**: That cognitive bias injection experiments conclusively demonstrate LLM vulnerability (supported by Category 4 performance drops, but corpus lacks detailed bias framing methodology or replication in other domains).

## Next Checks
1. Test MedGPT evaluation consistency by having multiple human clinicians independently score a subset of responses and comparing inter-rater reliability with LLM scores.
2. Expand bias injection experiments beyond recency and status quo to include confirmation bias and anchoring bias, measuring effect sizes across all 10 models.
3. Validate the MedSynF1 synonym dictionary by conducting a manual review of 50 random entity substitutions to confirm medical accuracy and coverage.