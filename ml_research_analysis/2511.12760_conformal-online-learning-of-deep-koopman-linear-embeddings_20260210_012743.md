---
ver: rpa2
title: Conformal Online Learning of Deep Koopman Linear Embeddings
arxiv_id: '2511.12760'
source_url: https://arxiv.org/abs/2511.12760
tags:
- learning
- online
- koopman
- prediction
- coloke
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conformal Online Learning of Koopman embeddings
  (COLoKe), a framework for adaptively updating Koopman-invariant representations
  from streaming data. COLoKe combines deep feature learning with multistep prediction
  consistency in the lifted space, where dynamics evolve linearly.
---

# Conformal Online Learning of Deep Koopman Linear Embeddings

## Quick Facts
- **arXiv ID:** 2511.12760
- **Source URL:** https://arxiv.org/abs/2511.12760
- **Authors:** Ben Gao; Jordan Patracone; Stéphane Chrétien; Olivier Alata
- **Reference count:** 40
- **Key outcome:** COLoKe combines deep feature learning with multistep prediction consistency and conformal thresholding to adaptively update Koopman embeddings from streaming data, achieving superior prediction accuracy and computational efficiency.

## Executive Summary
COLoKe introduces a framework for adaptively updating Koopman-invariant representations from streaming data by combining deep feature learning with multistep prediction consistency in the lifted space. The method employs a conformal-style mechanism that evaluates the consistency of the current Koopman model rather than just new states, triggering updates only when prediction error exceeds a dynamically calibrated threshold. This selective refinement approach balances expressive modeling with principled online adaptation for learning nonlinear dynamics.

The framework is evaluated on benchmark dynamical systems and compared to state-of-the-art online Koopman learning methods. COLoKe consistently outperforms existing approaches across both synthetic and real-world datasets, achieving lower prediction error while significantly reducing unnecessary updates. On the chaotic Lorenz system, it improves accuracy by nearly two orders of magnitude over baselines, demonstrating superior computational efficiency compared to offline training approaches.

## Method Summary
COLoKe introduces a framework for adaptively updating Koopman-invariant representations from streaming data by combining deep feature learning with multistep prediction consistency in the lifted space. The method employs a conformal-style mechanism that evaluates the consistency of the current Koopman model rather than just new states, triggering updates only when prediction error exceeds a dynamically calibrated threshold. This selective refinement approach balances expressive modeling with principled online adaptation for learning nonlinear dynamics.

## Key Results
- COLoKe consistently outperforms state-of-the-art online Koopman learning methods across synthetic and real-world datasets
- On the chaotic Lorenz system, COLoKe improves prediction accuracy by nearly two orders of magnitude over baselines
- The method achieves lower prediction error while significantly reducing unnecessary updates, demonstrating superior computational efficiency compared to offline training approaches

## Why This Works (Mechanism)
COLoKe works by maintaining a balance between model stability and adaptability through its conformal threshold mechanism. The method learns deep Koopman embeddings that map nonlinear dynamics into a linear space where predictions can be made efficiently. By evaluating the consistency of the current Koopman model rather than just new states, COLoKe avoids overfitting and only triggers updates when the model's prediction error exceeds a dynamically calibrated threshold. This selective updating strategy prevents the accumulation of errors that plague naive online learning approaches while maintaining the ability to adapt to changing dynamics.

## Foundational Learning

**Koopman Operator Theory**
*Why needed:* Provides the mathematical foundation for representing nonlinear dynamics as linear evolution in a lifted space
*Quick check:* Verify that the embedding preserves the semigroup property for the dynamics

**Deep Feature Learning**
*Why needed:* Enables learning of nonlinear embeddings that make dynamics approximately linear
*Quick check:* Confirm that the learned features improve linear predictability in the lifted space

**Conformal Prediction**
*Why needed:* Provides a principled framework for uncertainty quantification and threshold calibration
*Quick check:* Validate that the conformal threshold adapts appropriately to changing noise levels

**Online Learning**
*Why needed:* Enables continuous adaptation to streaming data without retraining from scratch
*Quick check:* Monitor prediction error stability during periods of no model updates

## Architecture Onboarding

**Component Map:**
Data Stream -> Deep Encoder -> Koopman Operator -> Prediction -> Conformal Evaluator -> Update Trigger

**Critical Path:**
The critical path for prediction accuracy is: Data Stream → Deep Encoder → Koopman Operator → Prediction. The conformal evaluator runs in parallel to monitor consistency but doesn't block the prediction pipeline.

**Design Tradeoffs:**
- Update frequency vs. prediction accuracy: More frequent updates improve accuracy but increase computational cost
- Embedding complexity vs. generalization: Deeper networks capture more complex dynamics but risk overfitting
- Threshold sensitivity vs. robustness: Lower thresholds enable faster adaptation but may cause unnecessary updates

**Failure Signatures:**
- Persistent prediction errors despite frequent updates indicate poor embedding quality
- Oscillating prediction errors suggest threshold calibration issues
- Sudden accuracy drops after updates may indicate catastrophic forgetting

**First Experiments:**
1. Test prediction accuracy on a simple linear system where Koopman theory is exact
2. Evaluate update frequency on a slowly varying dynamical system
3. Measure computational overhead of the conformal evaluation step

## Open Questions the Paper Calls Out
None

## Limitations
- Robustness of the conformal threshold mechanism under highly noisy real-world conditions remains uncertain
- Scalability to very high-dimensional state spaces and long-term prediction horizons has not been thoroughly tested
- Computational overhead of the conformal evaluation step could become prohibitive in resource-constrained settings

## Confidence

| Claim | Confidence |
|-------|------------|
| Prediction accuracy improvement over baselines | High |
| Reduced update frequency compared to naive online methods | High |
| Computational efficiency vs. offline methods | Medium |
| Theoretical guarantees of conformal mechanism | Medium |

## Next Checks

1. Test COLoKe on high-dimensional, real-world datasets (e.g., fluid dynamics or robotics) with varying noise profiles to assess robustness and scalability.

2. Conduct a systematic ablation study on the threshold calibration mechanism to quantify its impact on update frequency and prediction accuracy under different noise levels.

3. Evaluate the method's performance on long-term (>100 step) predictions and compare the accumulated error growth against both online and offline baselines.