---
ver: rpa2
title: Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking
arxiv_id: '2511.01208'
source_url: https://arxiv.org/abs/2511.01208
tags:
- reranking
- relevance
- documents
- document
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes contextual relevance and TS-SetRank for LLM-based
  document reranking. Contextual relevance captures the probability that a document
  is relevant given a query, marginalized over all batches it may appear in, accounting
  for both batch composition and document ordering.
---

# Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking

## Quick Facts
- arXiv ID: 2511.01208
- Source URL: https://arxiv.org/abs/2511.01208
- Reference count: 17
- Primary result: TS-SetRank improves nDCG@10 by 15-25% and 6-21% on BRIGHT and BEIR benchmarks respectively under fixed inference budgets

## Executive Summary
This paper introduces contextual relevance and TS-SetRank for efficient LLM-based document reranking. Contextual relevance captures how document relevance depends on both batch composition and document ordering, moving beyond static relevance judgments. TS-SetRank implements a two-phase Bayesian algorithm: initial uniform sampling for feedback, followed by Thompson sampling to adaptively select document batches that balance exploration and exploitation under fixed inference budgets.

## Method Summary
The method fine-tunes Qwen2.5-7B-Instruct using RLVR with GRPO on MS MARCO v2.1 samples. For reranking, TS-SetRank employs Beta-Bernoulli posteriors initialized at α=β=1 for each document. Phase I performs T_f uniform random batch samples (size b=10) to gather initial feedback. Phase II uses Thompson sampling for T−T_f rounds, selecting top-b documents based on sampled θ̃_i ~ Beta(α_i, β_i) from posteriors. Documents are ultimately ranked by posterior means θ̂_i = α_i/(α_i+β_i). The approach addresses the challenge of efficient reranking when LLM inference is expensive by adaptively focusing computation on documents most likely to be relevant.

## Key Results
- TS-SetRank achieves 15-25% nDCG@10 improvement over retrieval and reranking baselines on BRIGHT benchmark
- TS-SetRank achieves 6-21% nDCG@10 improvement over baselines on BEIR datasets
- Performance gains demonstrate the importance of modeling relevance as context-dependent rather than static

## Why This Works (Mechanism)
The method works by explicitly modeling that document relevance is context-dependent—both the other documents in a batch and their ordering affect judgment. TS-SetRank captures this through Beta-Bernoulli posteriors that track uncertainty for each document across different contexts. Thompson sampling then exploits this uncertainty to adaptively focus inference on documents whose relevance is still uncertain, rather than treating all documents equally. This balances exploration (learning about uncertain documents) with exploitation (confirming likely relevant documents), achieving better nDCG with fewer inference calls.

## Foundational Learning
- **Contextual relevance**: The probability a document is relevant given a query, marginalized over all possible batches it may appear in. Why needed: Static relevance judgments fail to capture how batch composition affects LLM judgments. Quick check: Compare nDCG when using batch-specific vs. document-specific relevance scores.

- **Thompson sampling**: A Bayesian approach that samples from posterior distributions to balance exploration and exploitation. Why needed: Enables adaptive selection of document batches based on uncertainty. Quick check: Track posterior variance over time to verify exploration-exploitation balance.

- **Beta-Bernoulli conjugacy**: The mathematical property that Beta distributions are conjugate priors for Bernoulli likelihoods, enabling efficient posterior updates. Why needed: Allows real-time updating of document relevance posteriors without expensive computations. Quick check: Verify posterior updates follow Beta(α+1, β) for positive feedback and Beta(α, β+1) for negative feedback.

## Architecture Onboarding

**Component map:**
MS MARCO v2.1 → Qwen2.5-7B-Instruct (GRPO fine-tuning) → TS-SetRank (Beta-Bernoulli posteriors) → Document batches → LLM reranker → Binary feedback → Posterior updates → Final ranking

**Critical path:**
Document retrieval → Initial uniform sampling (T_f rounds) → Thompson sampling (T−T_f rounds) → Posterior-based ranking

**Design tradeoffs:**
- Fixed batch size b=10 vs. adaptive batch size: Fixed size simplifies implementation but may not be optimal for all queries
- Binary vs. continuous relevance feedback: Binary enables efficient Beta-Bernoulli updates but loses granularity
- Number of uniform sampling rounds T_f: Too few leads to noisy posteriors, too many wastes inference budget

**Failure signatures:**
- Poor early performance if T_f too small (posterior estimates too noisy)
- High variance across runs due to LLM sampling variability
- Performance plateaus early if Thompson sampling fails to explore effectively

**First experiments:**
1. Run TS-SetRank with varying T_f values (10, 25, 50) to find optimal balance between exploration and exploitation
2. Compare nDCG@10 curves across different random seeds to assess variance
3. Implement ablation with random batch selection throughout to quantify benefit of Thompson sampling

## Open Questions the Paper Calls Out
- Can modeling structured dependencies between documents improve reranking performance in tasks like multi-hop QA compared to the current conditional independence assumption?
- Does training reranking models to output continuous relevance scores rather than binary judgments lead to more sample-efficient estimation of contextual relevance?
- To what extent does the higher nDCG@10 achieved by TS-SetRank translate into improved answer generation accuracy in end-to-end RAG pipelines?

## Limitations
- Missing implementation details for prompt templates and batch construction from MS MARCO v2.1
- Focus on batch size 10 without clear justification or exploration of scalability
- Binary relevance feedback loses granular information that could speed up convergence

## Confidence
- **High confidence**: Theoretical foundation and experimental results showing statistically significant nDCG improvements
- **Medium confidence**: Hyperparameter choices and implementation details require substantial trial and error due to missing specifications
- **Medium confidence**: Improvements demonstrated on BRIGHT and BEIR but generalizability to other collections remains untested

## Next Checks
1. **Ablation on batch size**: Systematically vary batch size b beyond 10 to determine optimal configuration and scalability
2. **Cross-collection generalization**: Evaluate TS-SetRank on additional benchmarks (TREC collections, other IR datasets) to assess broader applicability
3. **Computational efficiency analysis**: Measure wall-clock time and inference costs to quantify practical efficiency gains relative to fixed-batch approaches