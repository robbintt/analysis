---
ver: rpa2
title: 'SHAP values through General Fourier Representations: Theory and Applications'
arxiv_id: '2511.00185'
source_url: https://arxiv.org/abs/2511.00185
tags:
- shap
- fourier
- values
- feature
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes a rigorous spectral framework for analyzing\
  \ SHAP values using generalized Fourier expansions on discrete multi-valued domains\
  \ under product probability measures. The core method constructs an orthonormal\
  \ tensor-product basis and represents each SHAP attribution as a linear functional\
  \ of the model\u2019s Fourier coefficients, with explicit combinatorial weights\
  \ depending on feature interactions."
---

# SHAP values through General Fourier Representations: Theory and Applications

## Quick Facts
- arXiv ID: 2511.00185
- Source URL: https://arxiv.org/abs/2511.00185
- Reference count: 40
- One-line primary result: Establishes spectral framework for SHAP values using generalized Fourier expansions on discrete multi-valued domains under product probability measures.

## Executive Summary
This paper develops a rigorous spectral framework for computing SHAP values using generalized Fourier representations on discrete multi-valued domains under product probability measures. The method constructs an orthonormal tensor-product basis and represents each SHAP attribution as a linear functional of the model's Fourier coefficients, with explicit combinatorial weights depending on feature interactions. The framework provides both deterministic stability estimates and probabilistic convergence results for neural network limits, validated on a clinical stroke prediction dataset.

## Method Summary
The method builds an orthonormal tensor-product basis on discrete feature spaces under a product measure, computes Fourier coefficients for the trained model, and applies a closed-form formula to extract SHAP values as linear functionals of these coefficients. A sparse approximation strategy selects top-correlated modes up to a maximum interaction order, enabling massive computational savings over coalition enumeration. The approach is validated on a 3-layer neural network trained on clinical stroke prediction data, comparing Fourier-SHAP rankings against Kernel-SHAP.

## Key Results
- SHAP values can be computed as linear functionals of Fourier coefficients rather than by enumerating exponential coalitions
- Truncating high-frequency Fourier components introduces bounded, Lipschitz-continuous perturbations to SHAP values
- Finite-width neural network SHAP values converge to their infinite-width Gaussian process limits with quantifiable error bounds

## Why This Works (Mechanism)

### Mechanism 1
SHAP values decompose as linear functionals of Fourier coefficients using an orthonormal tensor-product basis. The explicit formula ϕᵢ(h; x*) = Σ_{k: kᵢ≠0} ĥ(k)Ψ_k(x*)/d(k) replaces O(2ⁿ) coalition evaluations with a sum over retained Fourier modes. This requires feature independence under the product measure μ.

### Mechanism 2
Spectral truncation introduces bounded perturbations to SHAP values. For sparse approximation h_S, the error |ϕᵢ(h; x*) − ϕᵢ(h_S; x*)| is controlled by the residual energy ‖r_S‖ and per-interaction weights w_k(i; x*). Low-frequency dominance in typical models ensures small truncation error.

### Mechanism 3
Finite-width neural network SHAP values converge to NNGP limits with bounds depending on Wasserstein-2 distance ε_N. The convergence E|ϕᵢ(h_N; x*) − ϕᵢ(h_{N,S}; x*)| combines spectral truncation error and distributional proximity to the infinite-width limit.

## Foundational Learning

- **Orthonormal tensor-product bases on discrete spaces**: Needed to construct {Ψ_k} enabling Parseval's identity and coefficient extraction. Quick check: For m₁=3 and m₂=4 possible values, what is the dimension of the full basis |I|, and how would you compute ĥ(k)?
- **Shapley values and the coalitional value function**: Needed to understand how the game-theoretic SHAP definition (equation 2.5) simplifies spectrally. Quick check: For 3 features, what is the combinatorial weight for feature i joining coalition S of size 1?
- **Gaussian processes as infinite-width neural network limits**: Needed for probabilistic results requiring understanding of NNGP kernel definitions and Wasserstein distance. Quick check: If pre-activations converge to GP with kernel K, what does the Karhunen-Loève expansion tell you about Fourier coefficients c_k?

## Architecture Onboarding

- Component map: Basis construction -> Fourier coefficient computation -> Sparse selection -> SHAP extraction
- Critical path: Precompute Fourier coefficients (expensive) -> Select sparse set S -> Compute SHAP values (fast, ~10⁻³s)
- Design tradeoffs: Higher interaction order d_max captures more complexity but increases memory; aggressive pruning speeds computation but may miss important effects
- Failure signatures: Ranking divergence from Kernel-SHAP indicates insufficient sparse basis; high per-frequency weights suggest important modes were discarded; memory explosion for high-cardinality features
- First 3 experiments:
  1. Reproduce stroke dataset validation with specified preprocessing and compare mean-abs-SHAP rankings to Kernel-SHAP
  2. Ablate interaction order d_max ∈ {1, 2, 3, 4} measuring SHAP correlation, runtime, and memory
  3. Test on synthetic model with known high-order interactions to characterize failure modes

## Open Questions the Paper Calls Out

- **Quantitative convergence rates for discrete domains**: The paper identifies lack of explicit rate decay formulas linking truncation set size to SHAP error for discrete feature spaces, despite establishing asymptotic convergence
- **Extension to non-product measures**: The current theory relies on tensor-product bases built upon feature independence; dependent features break orthogonality assumptions used in Theorem 3.1 proofs
- **SHAP behavior under stochastic perturbations**: The paper analyzes deterministic truncation and GP limits but doesn't characterize SHAP stability when the model itself is subject to random noise or stochastic perturbations

## Limitations
- Requires product measure μ (feature independence) for orthonormal basis construction and closed-form results
- Computational efficiency depends on spectral sparsity assumptions that may not hold for models with complex, high-order interactions
- NNGP convergence results depend on kernel diagonalizability assumptions that may not hold for all architectures

## Confidence

- **High confidence** in deterministic spectral decomposition (Theorem 3.1) and Lipschitz stability results
- **Medium confidence** in probabilistic NNGP convergence results (Theorems 3.4, 3.8) due to kernel diagonalizability assumptions
- **Medium confidence** in clinical stroke dataset validation given empirical nature and preprocessing sensitivity

## Next Checks

1. **Spectral sparsity test**: Construct synthetic models with controlled interaction orders (e.g., n-bit parity functions) and measure how Fourier-SHAP error scales with interaction order d versus Kernel-SHAP performance
2. **Measure dependence validation**: Apply Fourier-SHAP to datasets with known feature dependencies and compare against models that explicitly account for non-product measures
3. **NNGP convergence verification**: For fully connected networks of varying widths N, measure Wasserstein-2 distance ε_N between finite-width and infinite-width output distributions to verify Theorem 3.8's predicted convergence bounds