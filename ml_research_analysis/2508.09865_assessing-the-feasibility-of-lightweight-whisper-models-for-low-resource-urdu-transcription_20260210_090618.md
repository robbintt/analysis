---
ver: rpa2
title: Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu
  Transcription
arxiv_id: '2508.09865'
source_url: https://arxiv.org/abs/2508.09865
tags:
- urdu
- speech
- whisper
- error
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks lightweight Whisper models (Tiny, Base, Small)
  for Urdu speech recognition in low-resource settings without fine-tuning. Ten native
  Urdu speakers provided 36 audio samples, transcribed using pre-trained models and
  evaluated with Word Error Rate (WER).
---

# Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription

## Quick Facts
- arXiv ID: 2508.09865
- Source URL: https://arxiv.org/abs/2508.09865
- Reference count: 19
- Primary result: Whisper-Small achieved lowest WER (33.68%) for Urdu ASR without fine-tuning

## Executive Summary
This study benchmarks lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings without fine-tuning. Ten native Urdu speakers provided 36 audio samples, transcribed using pre-trained models and evaluated with Word Error Rate (WER). Whisper-Small achieved the lowest WER (33.68%), outperforming Base (53.67%) and Tiny (67.08%). Qualitative analysis revealed that Tiny often produced unintelligible or repetitive outputs, Base showed moderate performance with lexical distortions, and Small preserved sentence structure but still had phonetic and substitution errors. Results demonstrate that while Whisper-Small is the most promising lightweight model for Urdu ASR, significant improvements are needed for practical deployment.

## Method Summary
The study evaluated three pre-trained Whisper model sizes (Tiny: 39M, Base: 74M, Small: 244M parameters) on Urdu speech recognition without fine-tuning. Ten native Urdu speakers recorded 36 audio samples (4 voice notes each) using smartphones/laptops in quiet indoor settings. The audio (.wav, 16kHz) was transcribed using Hugging Face Transformers with CPU backend, and WER was computed via jiwer library after text normalization. The evaluation focused on zero-shot capabilities, measuring transcription accuracy across different model sizes.

## Key Results
- Whisper-Small achieved the lowest WER (33.68%), significantly outperforming Base (53.67%) and Tiny (67.08%)
- Whisper-Tiny frequently produced repetitive artifacts and unintelligible text, suggesting attention capacity limitations
- Whisper-Small preserved sentence structure but still exhibited phonetic accuracy and lexical coherence challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model parameter capacity directly constrains phonetic and lexical representation quality in low-resource languages
- Mechanism: Whisper-Small (244M parameters) provides deeper attention layers and greater representational capacity than Tiny (39M) or Base (74M), enabling better phoneme recognition and morphological disambiguation. The 6.2x parameter increase from Tiny to Small correlates with a 33.4 percentage point WER reduction.
- Core assumption: The performance gain stems primarily from architectural capacity rather than training data distribution differences across model sizes.
- Evidence anchors: [abstract] "Whisper-Small achieves the lowest error rates (33.68% WER), outperforming Tiny (67.08% WER) and Base (53.67% WER)"; [section 4.2] "Whisper-Tiny (39M parameters) has limited representational capacity and shallow attention layers. This leads to frequent breakdowns in transcription."
- Break condition: If WER does not improve monotonically with model size for a given language, the mechanism may reflect training corpus bias rather than pure capacity effects.

### Mechanism 2
- Claim: Zero-shot cross-lingual transfer degrades when pre-training corpus lacks linguistic representation of target language features
- Mechanism: Whisper's 680,000-hour multilingual pre-training enables baseline Urdu transcription, but phonetic complexity, dialectal diversity, and Arabic-origin vocabulary in Urdu create systematic substitution errors when target language traits diverge from pre-training distribution.
- Core assumption: Urdu's linguistic features (morphological complexity, Arabic-rooted terms) are underrepresented in Whisper's training corpus relative to high-resource languages.
- Evidence anchors: [section 1] "The Urdu language is not a well-studied language and its ASR training datasets are minimal"; [section 4.2] "The model's failure to generalize to formal Urdu or longer utterances suggests a mismatch between pretraining corpus distribution and the linguistic traits of Urdu"
- Break condition: If fine-tuning on Urdu data does not reduce substitution errors for Arabic-origin vocabulary, the issue may be architectural rather than distributional.

### Mechanism 3
- Claim: Temporal attention degradation in smaller models produces repetitive artifacts and syntactic incoherence in longer utterances
- Mechanism: Whisper-Tiny's shallow attention layers fail to maintain temporal coherence across longer sequences, resulting in repetitive syllable loops and complete loss of syntactic structure. This manifests as "buffer overflow" behavior where the model repeats tokens rather than advancing through the sequence.
- Core assumption: Repetitive artifacts stem from attention mechanism limitations rather than decoding strategy or tokenization issues.
- Evidence anchors: [abstract] "Qualitative analysis reveals persistent challenges in phonetic accuracy and lexical coherence, particularly for complex utterances"; [section 4.2] "In some cases, especially with Whisper-Tiny, the output included repeated syllables, suggesting issues in temporal attention or buffer overflow"; [Annex A] Complete failure case shows Whisper-Tiny producing looped repetition of the same token across entire output
- Break condition: If increasing model size does not reduce repetitive artifacts while maintaining identical architecture, the mechanism may involve decoding hyperparameters rather than attention capacity.

## Foundational Learning

- Concept: Word Error Rate (WER)
  - Why needed here: Primary evaluation metric quantifying transcription accuracy; understanding WER components (substitution, omission, insertion) is essential for diagnosing model failures.
  - Quick check question: If a model transcribes "ایک کالا کتا" (one black dog) as "ایک کالا" (one black), is this a substitution or omission error?

- Concept: Zero-shot inference
  - Why needed here: This study explicitly evaluates models without fine-tuning; understanding what pre-training enables vs. what requires adaptation is critical for interpreting results.
  - Quick check question: Why might a model trained on 680K hours of multilingual data still fail on a language with 230M speakers?

- Concept: Transformer encoder-decoder architecture for ASR
  - Why needed here: Whisper uses log-mel spectrograms as encoder input and generates text tokens autoregressively; understanding this pipeline is necessary for debugging transcription failures.
  - Quick check question: At which stage (encoder or decoder) would phonetic confusion most likely originate?

## Architecture Onboarding

- Component map: Audio (.wav, 16kHz) → Log-mel Spectrogram → Whisper Encoder → Decoder → Token Sequence → Urdu Text

- Critical path:
  1. Audio preprocessing: Ensure 16kHz sample rate, .wav format, normalization
  2. Model loading: Select size (Tiny/Base/Small) based on accuracy-latency tradeoff
  3. Inference: Run forward pass without gradient computation
  4. Post-processing: Normalize text (remove punctuation, unify spacing) before WER calculation

- Design tradeoffs:
  - Tiny (39M): Fastest inference, lowest memory; unsuitable for coherent Urdu output
  - Base (74M): Moderate speed; still high error rates (53.67% WER)
  - Small (244M): Best accuracy; requires more RAM but runs on CPU with 8GB

- Failure signatures:
  - Repetitive loops: Whisper-Tiny repeating same token → attention capacity exceeded
  - Phonetic substitution of Arabic-origin terms → pre-training corpus gap
  - Syntactic breakdown in long utterances → temporal attention degradation

- First 3 experiments:
  1. Replicate benchmark: Load Whisper-Small, run inference on provided dataset (GitHub available), verify ~33.68% WER baseline
  2. Utterance length analysis: Segment long audio clips into shorter units; measure WER reduction to isolate temporal attention effects
  3. Error taxonomy: Classify errors (substitution/omission/repetition/distortion) across models to identify which failure mode dominates each size tier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can fine-tuning lightweight Whisper models on curated Urdu datasets reduce WER and improve phonetic precision compared to zero-shot performance?
- Basis in paper: [explicit] The authors explicitly state that future research could "investigate the impact of fine-tuning these models on curated Urdu datasets to reduce WER and improve phonetic precision."
- Why unresolved: This study strictly evaluated the "zero-shot capabilities" of the models without any parameter updates or domain-specific training.
- What evidence would resolve it: A comparison of WER scores between the pre-trained models and their fine-tuned counterparts on the same Urdu speech corpus.

### Open Question 2
- Question: Can adaptation techniques such as noise augmentation or domain-specific lexicons effectively enhance the robustness of lightweight Urdu ASR in real-world environments?
- Basis in paper: [explicit] The Future Work section suggests that "exploring adaptation techniques such as noise augmentation or domain-specific lexicons could further enhance robustness in real-world environments."
- Why unresolved: The current methodology relied on clean recordings made in "quiet indoor settings," leaving the models' reaction to acoustic noise and specialized vocabulary untested.
- What evidence would resolve it: Benchmark results showing WER stability in models trained with augmented noise data or equipped with specialized vocabularies when exposed to noisy, real-world audio.

### Open Question 3
- Question: What architectural or attention-mechanism adjustments are required to prevent the repetitive artifacts and incoherence observed in ultra-lightweight models like Whisper-Tiny?
- Basis in paper: [inferred] The qualitative analysis notes that Whisper-Tiny often produced "repetitive artifacts" and "unintelligible" text, which the authors hypothesize may be due to "issues in temporal attention or buffer overflow."
- Why unresolved: The paper identifies the error pattern (repetitive loops) but does not test specific interventions to fix the underlying attention mechanism in the smallest model variant.
- What evidence would resolve it: An ablation study modifying the attention window or decoding strategy in Whisper-Tiny, resulting in a reduction of repetitive loops without degrading processing speed.

## Limitations

- The 36 audio samples represent a narrow domain of everyday speech without coverage of technical vocabulary, multiple dialects, or adverse acoustic conditions
- Zero-shot evaluation provides a useful baseline but doesn't establish the upper performance bound achievable through adaptation
- Qualitative analysis identifies failure patterns but lacks systematic error classification or comparison to traditional ASR approaches trained on Urdu-specific data

## Confidence

**High Confidence:** The monotonic WER improvement across model sizes (Tiny 67.08% → Base 53.67% → Small 33.68%) and the qualitative description of failure modes (repetitive artifacts in Tiny, lexical distortions in Base, phonetic errors in Small) are directly supported by the experimental results and error analysis presented.

**Medium Confidence:** The attribution of performance differences to parameter capacity versus pre-training corpus distribution represents reasonable inference but requires additional experiments. The claim that Whisper-Small is "most promising" for practical deployment assumes specific deployment constraints not explicitly validated.

**Low Confidence:** Assertions about Whisper's general inadequacy for low-resource languages based on this single language study, and the implied superiority of lightweight models over potential hybrid approaches, exceed the scope of the presented evidence.

## Next Checks

1. **Dialect and acoustic robustness testing:** Expand the evaluation corpus to include multiple Urdu dialects (Punjabi-influenced vs. Persian-influenced), age groups, and recording conditions (background noise, varying distances). Measure whether the size-performance hierarchy (Small > Base > Tiny) holds across these conditions, and identify specific acoustic or linguistic factors that break the pattern.

2. **Fine-tuning performance comparison:** Implement minimal supervised fine-tuning on a small Urdu corpus (e.g., 10-50 hours) for each model size. Compare zero-shot WER (current baseline) against fine-tuned WER to quantify the adaptation gap and determine whether smaller models benefit disproportionately from adaptation, potentially reversing the zero-shot hierarchy.

3. **Error taxonomy and correction feasibility:** Develop a systematic error classification framework (phonetic, lexical, syntactic, semantic) and annotate a subset of outputs. Measure error type distributions across model sizes and estimate the reduction in WER achievable through targeted post-processing or lightweight correction models, providing concrete guidance for deployment optimization.