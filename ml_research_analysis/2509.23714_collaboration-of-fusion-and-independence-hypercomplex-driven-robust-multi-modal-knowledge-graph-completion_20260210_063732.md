---
ver: rpa2
title: 'Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal
  Knowledge Graph Completion'
arxiv_id: '2509.23714'
source_url: https://arxiv.org/abs/2509.23714
tags:
- modality
- knowledge
- zhang
- m-hyper
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes M-Hyper, the first multi-modal knowledge graph
  completion (MMKGC) method operating in hypercomplex space. Existing MMKGC paradigms
  either fuse modalities with fixed strategies (losing modality-specific information)
  or keep modalities independent (missing cross-modal interactions).
---

# Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion

## Quick Facts
- **arXiv ID**: 2509.23714
- **Source URL**: https://arxiv.org/abs/2509.23714
- **Reference count**: 22
- **Primary result**: Proposes M-Hyper, achieving state-of-the-art MMKGC performance (41.25% MRR on DB15K) using biquaternion algebra

## Executive Summary
This paper introduces M-Hyper, the first multi-modal knowledge graph completion method operating in hypercomplex space. The key innovation addresses the fundamental tradeoff between modality fusion (which loses individual information) and independence (which misses interactions). By mapping three independent modalities and one fused joint modality to the four orthogonal bases of a biquaternion, M-Hyper simultaneously preserves both fused and independent representations. The method employs two novel modules: FERF for fine-grained entity representation factorization and R2MF for robust relation-aware modality fusion. Experiments on three MMKGC datasets show M-Hyper outperforms 18 baselines while maintaining computational efficiency and robustness to complex scenarios.

## Method Summary
M-Hyper operates on multi-modal knowledge graphs with structure, vision, and text modalities. The method first applies FERF to decompose each modality into modality-specific and task-specific components with a reconstruction loss. R2MF then performs relation-aware gated fusion of the three modalities to create a fourth "joint" modality, enhanced by noise-powered self-distillation. These four modalities are mapped to biquaternion bases (1, i, j, k) and combined via Hamilton product for pairwise interactions. The final score function uses both translation and rotation components to model complex relational patterns. Training combines reconstruction loss, distillation loss, triple loss, and N3 regularization with batch size 1000 using Adagrad optimizer.

## Key Results
- Achieves 41.25% MRR on DB15K, 37.02% on MKG-W, and 39.46% on MKG-Y
- Outperforms 18 baseline methods across all datasets
- All modalities positively impact performance, with joint modality showing largest contribution
- Demonstrates robustness to modality noise through noise-powered self-distillation
- Maintains computational efficiency compared to ensemble approaches

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Modality Separation via Biquaternion Bases
The model assigns three independent modalities (structure, vision, text) and one fused "joint" modality to the four orthogonal bases of a biquaternion. This allows simultaneous preservation of modality-specific information and cross-modal interaction without information loss typical of fixed fusion strategies. The Hamilton product models pairwise interactions between bases, ensuring all modalities influence the final score.

### Mechanism 2: Fine-grained Entity Representation Factorization (FERF)
FERF decomposes entity embeddings into "modality-specific" and "task-specific" subspaces. The modality-specific components retain unique features while task-specific components are learned for the downstream objective. A reconstruction loss forces task-specific parts to collaborate across modalities to reconstruct original modality information, enabling robustness when individual modalities are noisy.

### Mechanism 3: Noise-Powered Relation-aware Modality Fusion (R2MF)
R2MF calculates weighted sums of modalities using gates conditioned on relation queries, dynamically adapting to varying modality relevance. The noise-powered self-distillation trains a "student" network on noisy inputs to mimic a "teacher" on clean inputs, teaching gates to ignore irrelevant or corrupted features and improving robustness.

## Foundational Learning

- **Concept: Biquaternion Algebra**
  - **Why needed here:** Provides mathematical foundation using four orthogonal bases (1, i, j, k) with complex coefficients. The Hamilton product (non-commutative) allows modeling asymmetric relations.
  - **Quick check question:** How does the Hamilton product differ from a standard dot product in terms of interaction complexity?

- **Concept: Knowledge Graph Score Functions**
  - **Why needed here:** M-Hyper uses specific scoring function ϕ(h,r,t) = ⟨(Qh ⊕ Qr^T) ⊗ Qr^R, Qt⟩. Understanding translation (⊕) vs rotation (⊗) components is critical for superiority over translational baselines.
  - **Quick check question:** Why would rotation-based transformation be better for symmetric relations than translation-based?

- **Concept: Modality Fusion vs. Ensemble Paradigms**
  - **Why needed here:** M-Hyper positions as hybrid solution to dichotomy between early fusion (loses individual info) and ensemble (misses interactions). Attempts hybrid by keeping independent bases but fusing interactions mathematically.
  - **Quick check question:** What is specific drawback of "fixed fusion strategies" that M-Hyper tries to solve?

## Architecture Onboarding

- **Component map:** Raw embeddings -> FERF (modality-specific + task-specific) -> R2MF (gated fusion + noise distillation) -> Biquaternion assembly (4 bases) -> Score function (translation + rotation)
- **Critical path:** Success relies heavily on FERF module. If initial decomposition fails to isolate noise, subsequent biquaternion interactions will propagate noise across all bases.
- **Design tradeoffs:**
  - Complexity vs. Expressiveness: Biquaternions model complex relations but increase mathematical complexity and training instability
  - Robustness vs. Overfitting: Self-distillation improves robustness but requires tuning noise ratio β
- **Failure signatures:**
  - Gate Collapse: R2MF gates converge to near-zero variance, joint modality ignores input modalities
  - Quaternion Degradation: Rotation component learns identity mapping, reducing to simple translational model
- **First 3 experiments:**
  1. Ablation on FERF: Disable reconstruction loss to verify robustness gains from subspace decomposition
  2. Paradigm Comparison: Compare against M-Hyper-fusion (only joint) and M-Hyper-ensemble (only independent)
  3. Hyperparameter Sensitivity: Sweep noise rate β and embedding dimension d to identify stable operating region

## Open Questions the Paper Calls Out
- How to algebraically adapt M-Hyper for more than three independent modalities without fracturing unified biquaternion representation
- Whether freezing pre-trained feature extractors limits potential performance and whether end-to-end fine-tuning disrupts orthogonality of modality bases
- Whether strict enforcement of orthogonal modality bases hinders performance when modalities are semantically redundant or highly correlated

## Limitations
- Biquaternion algebra extension to more than four modalities remains theoretically challenging
- Pre-trained encoder freezing may limit feature optimization for specific task
- Orthogonality enforcement may discard useful correlated signals in semantically redundant modalities

## Confidence
- **Performance claims**: High confidence - clearly reported on standard metrics across three datasets with 18 baselines
- **Novelty of biquaternion approach**: Medium confidence - novel application but theoretical advantage needs more rigorous validation
- **Robustness claims**: Medium confidence - supported by experimental comparisons but mechanism could be more thoroughly isolated

## Next Checks
1. **Ablation on biquaternion necessity**: Compare M-Hyper against variant using standard vectors with same FERF+R2MF architecture
2. **Reconstruction loss sensitivity**: Systematically vary reconstruction loss weight to identify optimal balance
3. **Gate behavior analysis**: Visualize R2MF gate weights across different relations to verify dynamic adaptation