---
ver: rpa2
title: Georeferencing complex relative locality descriptions with large language models
arxiv_id: '2512.14228'
source_url: https://arxiv.org/abs/2512.14228
tags:
- georeferencing
- data
- locality
- descriptions
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies Large Language Models to automatically georeference\
  \ biological specimen collection records that contain complex, relative locality\
  \ descriptions (e.g., \u201C10 km north of Lake Wanaka, near Pipson Creek\u201D\
  ). Traditional georeferencing methods rely on place names and gazetteers, which\
  \ fail to capture spatial relationships embedded in such descriptions."
---

# Georeferencing complex relative locality descriptions with large language models

## Quick Facts
- **arXiv ID:** 2512.14228
- **Source URL:** https://arxiv.org/abs/2512.14228
- **Reference count:** 30
- **Key outcome:** Large Language Models achieve 65% accuracy within 10 km for georeferencing biological specimen records with complex relative locality descriptions, outperforming traditional methods.

## Executive Summary
This paper introduces an LLM-based approach for georeferencing biological specimen collection records that contain complex relative locality descriptions, such as "10 km north of Lake Wanaka, near Pipson Creek." Traditional georeferencing methods relying on place names and gazetteers fail to capture the spatial relationships embedded in such descriptions. The authors fine-tune a Mistral-7B LLM using quantized LoRA on biodiversity datasets from multiple regions and languages, employing a context manager prompting pattern. The fine-tuned model significantly outperforms baseline systems, achieving an average of 65% of records within 10 km of the true location across datasets, with the best result (New York) at 85% within 10 km and 67% within 1 km.

## Method Summary
The authors fine-tune Mistral-7B with QLoRA (4-bit quantization) using a Context Manager prompting pattern to georeference locality descriptions. The model is trained on GBIF occurrence records from New Zealand, USA, Australia, and Mexico, with hyperparameters including learning rate 2e-4, batch size 32, epochs 3, LoRA rank 32, and LoRA alpha 64. The Context Manager pattern structures prompts with Instruction + Context (State, Country) + Locality Description. Training is performed on a single NVIDIA A100 24GB GPU, and performance is evaluated using Haversine distance, with target metrics being percentage of predictions within 10 km and 1 km of ground truth coordinates.

## Key Results
- Fine-tuned LLM achieves 65% accuracy within 10 km and 33% within 1 km across all datasets
- Best performance on New York dataset: 85% within 10 km and 67% within 1 km
- Outperforms baseline GEOLocate and gazetteer-based methods significantly
- Robust to description length and performs well with multilingual data
- Model effectively uses quantitative spatial cues (e.g., "10 km north")

## Why This Works (Mechanism)
The approach works by leveraging LLMs' ability to understand complex spatial relationships and relative descriptions through fine-tuning on biodiversity data. The Context Manager prompting pattern provides geographic grounding by including state and country information alongside the locality description. The QLoRA fine-tuning allows efficient adaptation of the base model to the georeferencing task while preserving the model's general language understanding capabilities. The model learns to parse quantitative spatial cues and relative positions, which traditional gazetteer-based methods cannot handle effectively.

## Foundational Learning

**Large Language Models (LLMs):** Neural networks trained on massive text corpora that understand and generate human language, used here for understanding spatial relationships in text.
- *Why needed:* LLMs can interpret complex relative descriptions that traditional methods cannot parse
- *Quick check:* Verify the model can understand basic spatial relationships like "north of" or "near"

**Quantized LoRA (QLoRA):** A parameter-efficient fine-tuning method using low-rank adapters and 4-bit quantization for reduced memory usage
- *Why needed:* Enables fine-tuning large models (7B parameters) on limited GPU resources (24GB)
- *Quick check:* Confirm model trains successfully on A100 24GB GPU without memory errors

**Haversine Distance:** The great-circle distance formula used to measure geospatial error between predicted and actual coordinates
- *Why needed:* Provides accurate distance measurement accounting for Earth's curvature
- *Quick check:* Calculate distance between two known coordinates to verify implementation

**Context Manager Prompting:** A prompting pattern that provides additional context (state, country) alongside the locality description
- *Why needed:* Grounds the model's predictions geographically to prevent hallucinations
- *Quick check:* Test model with descriptions missing state/country context to observe performance drop

## Architecture Onboarding

**Component Map:** Mistral-7B (Base) -> QLoRA Adapter (Rank 32, Alpha 64) -> Fine-tuning (3 epochs) -> Context Manager Prompting -> Georeferencing Output

**Critical Path:** Data Preparation (GBIF records) -> Prompt Formatting (Context Manager) -> Model Fine-tuning (QLoRA) -> Coordinate Extraction -> Haversine Evaluation

**Design Tradeoffs:** The authors chose QLoRA over full fine-tuning to reduce memory requirements, enabling training on a single A100 24GB GPU. The Context Manager pattern was selected over simpler prompting to provide geographic grounding and prevent hallucinations. Mistral-7B was chosen for its strong performance-to-parameter ratio, though larger models might capture more complex patterns.

**Failure Signatures:** Geographic Hallucination (coordinates in wrong hemisphere), Format Non-compliance (model outputs explanation instead of coordinates), Low performance on large sparse regions (requires >100k training records)

**First Experiments:**
1. Test geographic hallucination by providing descriptions like "10 km south of the Arctic Circle" and verifying coordinates are in correct hemisphere
2. Verify format compliance by checking model outputs only coordinates without chain-of-thought explanations
3. Evaluate performance on a small, dense region (e.g., New Zealand) before scaling to larger regions

## Open Questions the Paper Calls Out

**Open Question 1:** Can spatially-informed Uncertainty Quantification (UQ) techniques, such as convex hull analysis on embeddings, be effectively integrated to provide reliable confidence scores for predicted coordinates? The paper identifies this as a limitation, noting the current model outputs single coordinates without uncertainty measures. Evidence would be a modified model that outputs error radii statistically correlating with actual distance errors in validation sets.

**Open Question 2:** Does integrating Retrieval-Augmented Generation (RAG) with external gazetteers significantly improve georeferencing performance in regions lacking specific training data? The study found limited transfer learning capabilities without local data. Evidence would be experiments showing a RAG-enhanced model maintains high accuracy in "unseen" regions without region-specific fine-tuning.

**Open Question 3:** To what extent does increasing LLM parameter size beyond 7B improve the granularity and accuracy of georeferencing for complex locality descriptions? The authors suggest larger models could capture more intricate patterns but were limited to 7B due to GPU constraints. Evidence would be comparative studies scaling to 13B or 70B parameters demonstrating significant reduction in median error distances.

## Limitations

- Performance significantly degrades on large, sparsely populated regions (e.g., Australia) requiring >100k training records
- Limited scalability to regions with <30k training records or where relative locality descriptions are rare
- Model lacks uncertainty quantification, making reliability assessment difficult
- 4-bit quantization may introduce numerical instability affecting fine-tuning effectiveness

## Confidence

- **High Confidence:** Model architecture (Mistral-7B + QLoRA) and Context Manager prompting pattern are clearly described and technically sound
- **Medium Confidence:** Training procedure and hyperparameters are detailed but minor deviations could affect reproducibility
- **Low Confidence:** Scalability claims for large, sparsely populated regions are weakly supported with limited evidence

## Next Checks

1. **Geographic Hallucination Test:** Validate model does not output coordinates in wrong hemisphere with descriptions like "10 km south of the Arctic Circle"
2. **Format Compliance Check:** Ensure model outputs only coordinates (no explanations) for complex descriptions like "near intersection of Route 66 and Mississippi River"
3. **Sparse Region Performance Test:** Evaluate on Northern Territory, Australia (<30k records) and compare to New York to confirm scalability limitations