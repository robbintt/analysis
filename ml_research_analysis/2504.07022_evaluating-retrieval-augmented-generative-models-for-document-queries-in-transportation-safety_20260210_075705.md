---
ver: rpa2
title: Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation
  Safety
arxiv_id: '2504.07022'
source_url: https://arxiv.org/abs/2504.07022
tags:
- information
- regulations
- llama
- transportation
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates generative Large Language Models (LLMs) for\
  \ retrieving regulatory information in hazardous materials transportation. Three\
  \ fine-tuned models\u2014ChatGPT, Google Vertex AI, and ORNL Retrieval Augmented\
  \ Generation (RAG) with LLaMA 2 and LLaMA 3\u2014were tested using 100 queries derived\
  \ from 40 regulatory documents."
---

# Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety

## Quick Facts
- arXiv ID: 2504.07022
- Source URL: https://arxiv.org/abs/2504.07022
- Reference count: 11
- Three fine-tuned models—ChatGPT, Google Vertex AI, and ORNL RAG with LLaMA 2/3—were tested using 100 queries from 40 regulatory documents, with RAG-augmented LLaMA models scoring highest (average 4.03).

## Executive Summary
This study evaluates generative Large Language Models (LLMs) for retrieving regulatory information in hazardous materials transportation. Three fine-tuned models—ChatGPT, Google Vertex AI, and ORNL Retrieval Augmented Generation (RAG) with LLaMA 2 and LLaMA 3—were tested using 100 queries derived from 40 regulatory documents. Qualitative ratings (1–5 scale) assessed accuracy, detail, and relevance, while semantic similarity measured response consistency. The RAG-augmented LLaMA models outperformed others, scoring an average of 4.03, compared to 3.41 for Vertex AI and 3.03 for ChatGPT. Semantic similarity analysis showed high consistency among RAG models (average >0.9) but greater variation with ChatGPT. Results highlight RAG’s potential for domain-specific applications, though occasional inconsistencies emphasize the need for further refinement.

## Method Summary
The study tested three fine-tuned generative models—ChatGPT, Google Vertex AI, and ORNL RAG with LLaMA 2 and LLaMA 3—using 100 queries derived from 40 regulatory documents in hazardous materials transportation. Performance was evaluated using qualitative ratings (1–5 scale) for accuracy, detail, and relevance, as well as semantic similarity to measure response consistency. The RAG-augmented LLaMA models demonstrated superior performance, with an average score of 4.03, compared to 3.41 for Vertex AI and 3.03 for ChatGPT. Semantic similarity analysis revealed high consistency among RAG models (>0.9) but greater variability with ChatGPT.

## Key Results
- RAG-augmented LLaMA models achieved the highest average score (4.03) among the tested models.
- Vertex AI scored 3.41, while ChatGPT scored 3.03, indicating lower performance in regulatory information retrieval.
- RAG models showed high semantic similarity (>0.9) in responses, demonstrating consistency, whereas ChatGPT exhibited greater variability.

## Why This Works (Mechanism)
The study demonstrates that retrieval-augmented generative models, particularly those fine-tuned with domain-specific data, can effectively retrieve and synthesize regulatory information. The integration of retrieval mechanisms with LLMs allows for more accurate and contextually relevant responses by grounding outputs in verified documents. The high semantic similarity among RAG responses indicates robustness in handling similar queries, while the lower performance of non-augmented models highlights the value of retrieval augmentation in domain-specific applications.

## Foundational Learning
- **Retrieval Augmented Generation (RAG)**: Combines retrieval of relevant documents with generative models to enhance response accuracy and relevance. *Why needed*: Ensures responses are grounded in verified regulatory texts. *Quick check*: Verify that retrieved documents directly address the query.
- **Semantic Similarity Analysis**: Measures the consistency of model responses by comparing semantic content. *Why needed*: Identifies variability in model outputs for similar queries. *Quick check*: Compare semantic similarity scores across different query types.
- **Qualitative Rating Scales (1–5)**: Used to assess accuracy, detail, and relevance of model responses. *Why needed*: Provides a structured way to evaluate performance in domain-specific tasks. *Quick check*: Ensure raters are trained and consistent in scoring.

## Architecture Onboarding
- **Component Map**: Regulatory documents → Query processor → RAG retriever → LLM generator → Output
- **Critical Path**: Document retrieval → Context embedding → Response generation → Quality assessment
- **Design Tradeoffs**: RAG models balance retrieval accuracy with generative fluency, but may struggle with ambiguous queries or incomplete document coverage.
- **Failure Signatures**: Inconsistent responses, reliance on outdated documents, or failure to address query nuances.
- **First Experiments**:
  1. Test RAG models with edge-case queries to assess robustness.
  2. Evaluate the impact of document recency on retrieval accuracy.
  3. Compare RAG performance with and without fine-tuning on domain-specific data.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relied on a relatively small sample of 100 queries derived from 40 regulatory documents, which may not capture the full diversity of real-world scenarios.
- Qualitative ratings (1–5 scale) introduce potential subjectivity, as scoring criteria were not fully detailed.
- Semantic similarity analysis does not directly assess factual correctness, which is critical in high-stakes safety applications.

## Confidence
- **High**: RAG models demonstrated superior performance (average score 4.03) compared to non-augmented models.
- **Medium**: Findings are based on a limited query and document set, raising questions about generalizability.
- **Low**: The claim that RAG models are "superior" for domain-specific applications lacks evidence of real-world deployment effectiveness.

## Next Checks
1. Expand Query and Document Diversity: Test models with a larger and more diverse set of queries and regulatory documents to assess generalizability and robustness across different scenarios.
2. Incorporate Factual Correctness Metrics: Develop and apply metrics to evaluate the factual accuracy of retrieved information, particularly in high-stakes safety contexts where errors could have serious consequences.
3. Real-World Deployment Testing: Conduct a pilot study to deploy the RAG models in a controlled real-world environment, monitoring their performance, reliability, and user satisfaction over time.