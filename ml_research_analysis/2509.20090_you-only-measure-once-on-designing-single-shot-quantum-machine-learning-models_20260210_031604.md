---
ver: rpa2
title: 'You Only Measure Once: On Designing Single-Shot Quantum Machine Learning Models'
arxiv_id: '2509.20090'
source_url: https://arxiv.org/abs/2509.20090
tags:
- quantum
- yomo
- inference
- shot
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces You Only Measure Once (Yomo), a quantum machine
  learning (QML) framework that achieves accurate inference with dramatically fewer
  measurements, down to the single-shot regime. Yomo replaces Pauli expectation-value
  outputs with a probability aggregation mechanism and introduces loss functions that
  encourage sharp predictions.
---

# You Only Measure Once: On Designing Single-Shot Quantum Machine Learning Models

## Quick Facts
- **arXiv ID:** 2509.20090
- **Source URL:** https://arxiv.org/abs/2509.20090
- **Reference count:** 40
- **Primary result:** Achieves accurate single-shot quantum inference via probability aggregation, bypassing shot-scaling limitations of expectation-based models.

## Executive Summary
This paper introduces You Only Measure Once (Yomo), a quantum machine learning framework that achieves accurate inference with dramatically fewer measurements, down to the single-shot regime. Yomo replaces Pauli expectation-value outputs with a probability aggregation mechanism and introduces loss functions that encourage sharp predictions. Theoretical analysis shows that Yomo avoids shot-scaling limitations inherent to expectation-based models, and experiments on MNIST and CIFAR-10 confirm that Yomo consistently outperforms baselines across different shot budgets and under depolarizing noise simulations. Yomo achieves single-shot inference with high accuracy (e.g., 90.52% on MNIST), reducing financial and computational costs of deploying QML and lowering the barrier to practical adoption.

## Method Summary
Yomo is a hybrid quantum-classical model that maps computational basis states directly to class labels, enabling single-shot inference. During training, a classical feature extractor processes images, which are then encoded into quantum states via angle encoding. A variational quantum neural network (QNN) with parameterized rotations and CNOT gates generates a quantum state whose basis state probabilities are aggregated into class probabilities through a pre-defined partition. The model is trained using exact state-vector simulation with a combined loss function (cross-entropy, probability sharpening, and entropy regularization) that encourages probability concentration on correct-class basis states. During inference, a single measurement directly yields a class label, bypassing the need for repeated measurements to estimate expectation values.

## Key Results
- Yomo achieves 90.52% test accuracy on MNIST with single-shot inference, compared to 27.25% for Vanilla QML.
- Single-shot accuracy consistently outperforms Vanilla baselines across all shot budgets ($N_{shot} \in \{1, 10, 100, \infty\}$).
- Theoretical analysis shows Yomo's error probability scales as $\exp(-2N(p-1/2)^2)$, avoiding the $O(1/\Delta^2)$ scaling that limits expectation-based models.

## Why This Works (Mechanism)

### Mechanism 1: Probability Aggregation Replaces Expectation-Value Estimation
- Claim: Mapping computational basis states directly to class labels enables single-shot inference, bypassing the shot-scaling bottleneck of Pauli expectation-value estimation.
- Mechanism: Partition the $2^{n_q}$ basis states into $K$ groups; each measured bitstring maps to a class via aggregation (Eq. 8). Training uses exact state-vector simulation to learn the mapping; inference requires only one measurement.
- Core assumption: The learned partition generalizes; the probability mass concentrates sufficiently on correct-class basis states during training.
- Evidence anchors:
  - [abstract] "Yomo replaces Pauli expectation-value outputs with a probability aggregation mechanism and introduces loss functions that encourage sharp predictions."
  - [Section 4] "A single measured bitstring can be directly mapped to a class label according to the pre-defined partition of basis states."
  - [corpus] Related work on learning quantum measurements (arXiv:2501.05663, 2505.13525) addresses measurement design but not single-shot inference explicitly.
- Break condition: If basis-state probabilities are near-uniform across classes (flat distribution), single-shot accuracy collapses; entropy loss must enforce peakedness.

### Mechanism 2: Loss-Driven Probability Sharpening During Training
- Claim: Combined loss (cross-entropy + sharpening + entropy regularization) trains the circuit to concentrate probability mass on correct-class basis states.
- Mechanism: $L_{\text{yomo}} = L_{\text{CE}} + \gamma L_{\text{PS}} + \omega L_E$; sharpening rewards predictions above threshold $\tau$ (Eq. 11); entropy term penalizes flat distributions (Eq. 12).
- Core assumption: Threshold $\tau$ is set appropriately (empirically $\tau=0.6$ optimal for single-shot); gradient flow through exact simulation is sufficient.
- Evidence anchors:
  - [Section 4] "L_PS encourages the model to push confident predictions further toward one-hot distributions."
  - [Section 6, Fig. 3(c-e)] "Single-shot accuracy peaks at $\tau=0.6$" and "inclusion of $L_{PS}$ clearly improves single-shot test accuracy."
  - [corpus] No direct corpus evidence on sharpening mechanisms in QML; this appears novel.
- Break condition: If $\tau$ is too low, early incorrect predictions are amplified; if too high, only already-confident predictions benefit, limiting training signal.

### Mechanism 3: Theoretical Shot-Complexity Separation from Expectation-Based Models
- Claim: Yomo's error probability scales as $\exp(-2N(p-1/2)^2)$, independent of margin $\Delta$ and Lipschitz constant $L$, avoiding the $O(1/\Delta^2)$ scaling that plagues expectation-based QML.
- Mechanism: Theorem 5.1 shows Yomo requires $N \geq \ln(1/\delta)/(2(p-1/2)^2)$ shots; Theorem 5.2 shows Vanilla requires $N \geq 8L^2/(\Delta^2) \cdot \ln(2K/\delta)$. As qubit count increases, $\Delta$ often shrinks exponentially, making Vanilla impractical.
- Core assumption: The trained model achieves $p > 1/2 + \Delta/(4L) \cdot \sqrt{\ln(1/\delta)/\ln(2K/\delta)}$ (Theorem 5.3); $p$ remains stable as system size grows.
- Evidence anchors:
  - [Section 5] "Yomo's requirement in Eq. 13 depends only on $p - 1/2$... enabling Yomo to sustain low-shot performance even in high-dimensional regimes."
  - [Section 6, Fig. 3(a,b)] "Vanilla suffers significant performance degradation as $n_q$ increases... Yomo shows no comparable performance decay."
  - [corpus] Single-shot QML theory (Recio-Armengol et al., 2025b) is cited but implementation pathways were absent before Yomo.
- Break condition: If training fails to achieve $p$ moderately above 0.5 (e.g., due to barren plateaus or noise), the theoretical advantage does not materialize.

## Foundational Learning

- **Concept: Expectation-value estimation via repeated measurements (shots)**
  - Why needed here: Vanilla QML relies on estimating $\langle O_k \rangle$ from many shots; understanding Hoeffding's $O(1/\sqrt{N})$ convergence clarifies why Yomo's avoidance is impactful.
  - Quick check question: Given a Pauli observable with eigenvalues in $[-1, 1]$, how many shots are needed to estimate its expectation within error $\epsilon$ with probability $\geq 1-\delta$?

- **Concept: Computational-basis measurement and bitstring-to-class mapping**
  - Why needed here: Yomo's inference maps a single bitstring $\phi \in \{0,1\}^{n_q}$ directly to a class via partition $S_k$; this is the core of single-shot prediction.
  - Quick check question: If $n_q = 4$ qubits and $K = 4$ classes, how many basis states are assigned to each class under the partitioning scheme in Section 4?

- **Concept: Entropy regularization and sharpening losses**
  - Why needed here: The combined loss $L_{\text{yomo}}$ uses entropy minimization and probability sharpening to ensure peaked distributions suitable for single-shot inference.
  - Quick check question: If all class probabilities are uniform ($p_k = 1/K$), what is the value of the entropy loss $L_E$? What does this imply for single-shot accuracy?

## Architecture Onboarding

- **Component map:**
  Classical feature extractor $f_{\theta_c}(x) \to z \in \mathbb{R}^{n_f}$ -> Angle encoding $V(z)$ -> QNN $U(\theta)$ -> Measurement -> Probability aggregation -> Class label

- **Critical path:**
  1. Train on classical simulator with exact state-vector probabilities (no shot noise)
  2. Optimize $\theta_c, \theta$ via Adam with gradients from autograd/parameter-shift
  3. Deploy: single-shot measurement $\to$ bitstring $\to$ class via pre-computed partition

- **Design tradeoffs:**
  - **Qubit count vs. shot efficiency**: More qubits increase expressivity but Vanilla degrades; Yomo remains stable (Fig. 3a,b)
  - **Circuit depth vs. noise**: Yomo achieves strong accuracy at low depth ($N_b = 5$); deeper circuits add noise without benefit
  - **Threshold $\tau$**: Low $\tau$ amplifies noise early; high $\tau$ gives weak signal; empirically $\tau = 0.6$ optimal for single-shot
  - **Train-on-classical, deploy-on-quantum**: Requires classical simulation during training (feasible up to ~25–35 qubits per Appendix discussion)

- **Failure signatures:**
  - Single-shot accuracy near random ($1/K$): probability distribution too flat; increase $\omega$ (entropy weight) or check gradient flow
  - Accuracy degrades sharply with noise: reduce circuit depth; Yomo is designed for low-depth deployment
  - Training loss plateaus early: check if $\tau$ too aggressive; reduce $\gamma$ or warm-up sharpening gradually

- **First 3 experiments:**
  1. **Reproduce MNIST single-shot result**: $n_q = 4$, $N_b = 5$, $\tau = 0.6$, compare Yomo vs. Vanilla at shots $\in \{1, 10, 100, \infty\}$; expect ~90% single-shot accuracy for Yomo vs. ~27% for Vanilla
  2. **Ablation on sharpening loss**: Train Yomo with and without $L_{\text{PS}}$; plot single-shot test accuracy over epochs (replicate Fig. 3e)
  3. **Noise robustness test**: Inject depolarizing noise with $p_1, p_2$ from Table 1 (e.g., Quantinuum H1-1 vs. IonQ Forte); compare Yomo single-shot vs. Vanilla 100-shot accuracy across hardware profiles

## Open Questions the Paper Calls Out

- **Can other quantum methods be reformulated to operate in a shot-efficient or even single-shot regime?**
  The paper explicitly asks whether VQE and QAOA can be adapted for shot-efficient operation, as these optimization tasks use fundamentally different cost functions than classification.

- **How can Yomo training scale beyond classical state-vector simulation limits?**
  The authors identify the need for advanced classical methods (tensor networks, sampling-based gradients) to train Yomo models on qubit counts exceeding ~25-35, where exact simulation becomes intractable.

- **At what qubit count does Yomo-based quantum inference become faster than classical simulation?**
  The paper suggests quantum inference with Yomo may surpass classical simulation in runtime for intermediate qubit regimes (25-35 qubits) but calls for systematic investigation of this crossover point.

- **How does restricted hardware connectivity impact Yomo performance?**
  The paper notes that real devices with limited qubit connectivity (requiring SWAP gates) may further degrade performance compared to the fully connected noise simulations used in the study.

## Limitations

- Training currently requires exact classical state-vector simulation, limiting scalability to ~25-35 qubits.
- The framework is specifically designed for classification tasks and may not directly extend to optimization problems like VQE or QAOA.
- Real-world hardware connectivity constraints and gate overhead (e.g., SWAP networks) could degrade the observed performance advantages.

## Confidence

- **High confidence**: Single-shot inference mechanism, empirical demonstration of superior accuracy at low shot counts versus Vanilla baselines, and basic theoretical separation of shot complexity.
- **Medium confidence**: The precise numerical advantage of sharpening loss (τ=0.6) across different problem domains, and robustness under realistic noise profiles.
- **Low confidence**: Sustained advantage in very high-dimensional quantum feature spaces (hundreds of qubits), and practical feasibility of training on exact simulator for such large systems.

## Next Checks

1. **Scale-up validation**: Test Yomo on larger datasets (e.g., ImageNet subset) to verify that the shot-complexity advantage scales with problem size and qubit count.
2. **Realistic noise characterization**: Implement noise models based on actual hardware specifications from multiple quantum computing vendors and validate single-shot accuracy under these conditions.
3. **Distribution sensitivity analysis**: Systematically vary class prior distributions and assess how Yomo's performance degrades compared to expectation-based models, quantifying the robustness of the probability concentration mechanism.