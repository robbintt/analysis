---
ver: rpa2
title: Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations
arxiv_id: '2505.12369'
source_url: https://arxiv.org/abs/2505.12369
tags:
- embedding
- queries
- geometric
- transitive
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GeometrE introduces a fully geometric approach to multi-hop reasoning\
  \ on knowledge graphs, addressing the interpretability gap in existing methods that\
  \ rely on neural networks for logical operations. The core innovation lies in mapping\
  \ all logical operations\u2014intersection, projection, and negation\u2014to pure\
  \ geometric transformations using box embeddings, eliminating the need for learnable\
  \ neural components."
---

# Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations

## Quick Facts
- arXiv ID: 2505.12369
- Source URL: https://arxiv.org/abs/2505.12369
- Authors: Fernando Zhapa-Camacho; Robert Hoehndorf
- Reference count: 30
- Primary result: Achieves up to 52.6 MRR on 1-hop queries in WN18RR, outperforming state-of-the-art methods

## Executive Summary
GeometrE introduces a fully geometric approach to multi-hop reasoning on knowledge graphs that eliminates learnable neural components for logical operations. The method maps all logical operations—intersection, projection, and negation—to pure geometric transformations using box embeddings, while introducing a transitive loss function that preserves the logical rule ∀a,b,c: r(a,b) ∧ r(b,c) → r(a,c) through order-preserving embeddings. Experiments on WN18RR-QA, NELL-QA, and FB15k-237 demonstrate superior performance across most query types, with the transitive loss function improving hypernym chain preservation from 0.32 to 0.88 Spearman correlation. While the negation approximation works well for some datasets, results on FB15k-237 indicate dataset-dependent limitations requiring future work on more robust negation modeling.

## Method Summary
GeometrE uses box embeddings to represent entities and relations, where each entity has a query box (center c, offset o) and answer point (center c', zero offset). Relation projections transform boxes using affine transformations with multiplicative and additive components. Logical operations are implemented as geometric transformations: intersection computes overlap boxes, projection applies relation-specific transformations, and negation uses set operations when boxes are disjoint. The method introduces a transitive loss function that enforces order-preserving embeddings along dedicated dimensions for transitive relations. Training uses margin-based ranking loss with negative sampling, plus transitive loss for maintaining idempotent transformations. Hyperparameters are grid-searched with dataset-specific optimal settings.

## Key Results
- Achieves 52.6 MRR on 1-hop queries in WN18RR, outperforming neural baselines
- Transitive loss function improves Spearman correlation for hypernym chains from 0.32 to 0.88
- Outperforms state-of-the-art methods across most query types on WN18RR and NELL-QA
- Negation approximation shows dataset-dependent limitations, particularly on FB15k-237

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Complex logical queries on knowledge graphs can be answered using purely geometric transformations without learnable neural components for logical operations.
- **Mechanism**: Map logical operations to fixed geometric transformations on box embeddings: Intersection computes overlap box via coordinate-wise min/max on corners, Projection applies affine transformation T_r with multiplicative and additive terms, Negation approximates via set operations when complement intersects another box.
- **Core assumption**: Geometric set operations on axis-aligned boxes sufficiently approximate first-order logic operations for knowledge graph queries.
- **Evidence anchors**: "GeometrE... does not require learning the logical operations and enables full geometric interpretability", "following the geometric definition of Equation 3 requires O(k · n) operations [vs] O(k · (n · d + L · d²)) for neural methods".
- **Break condition**: Negation approximation fails when boxes are not sufficiently disjoint; FB237 shows mixed results on negation queries.

### Mechanism 2
- **Claim**: The transitive logical rule ∀a,b,c: r(a,b) ∧ r(b,c) → r(a,c) can be preserved in embedding space through an order-preserving loss function and idempotent transformations.
- **Mechanism**: Assign dedicated dimension i per transitive relation, apply distordering to enforce ca > cb > cc along dimension i, optimize relation embedding ˆr_i → identity via L_ˆri.
- **Core assumption**: Transitive chains can be faithfully represented as ordered sequences along a single embedding dimension.
- **Evidence anchors**: "we introduce a transitive loss function and show that it can preserve the logical rule", Spearman correlation for hypernym chains improved from 0.32 → 0.88 with transitive loss.
- **Break condition**: When multiple transitive relations compete for the same dimension.

### Mechanism 3
- **Claim**: A composite distance function combining outside-distance and inside-distance can effectively score query-answer relationships while maintaining geometric interpretability.
- **Mechanism**: distbox(q,a) = distout + α · distin, where distout penalizes answers outside query box boundary and distin encourages answers toward box center without requiring exact match.
- **Core assumption**: Box containment with soft center-attraction provides sufficient signal for learning meaningful embeddings.
- **Evidence anchors**: GeometrE achieves 52.6 MRR on 1-hop WN18RR queries, outperforming neural baselines.
- **Break condition**: When query boxes collapse to zero volume or become too large to discriminate answers.

## Foundational Learning

- **Concept**: Box Embeddings (n-dimensional hyperrectangles)
  - **Why needed here**: Core representation for all entities, relations, and query results; every operation transforms boxes
  - **Quick check question**: Given two boxes B1=(l1,u1) and B2=(l2,u2), can you compute their intersection and determine if it's empty?

- **Concept**: First-Order Logic Queries in Disjunctive Normal Form
  - **Why needed here**: All queries are structured as DNF with anchor entities, bound variables, and target variables; determines computation graph
  - **Quick check question**: Can you identify the anchor entities, bound variables, and target variable in: ∃V1: r1(a,V1) ∧ r2(V1,V?)?

- **Concept**: Order Embeddings for Hierarchical Relations
  - **Why needed here**: Transitive relations require ordered embeddings; understanding this is essential for the transitive loss mechanism
  - **Quick check question**: If a > b > c in embedding dimension i, what does this imply for a transitive relation r(a,b) ∧ r(b,c)?

## Architecture Onboarding

- **Component map**: Entity initialization → Relation projection → Intersection/negation operations → Distance computation → Loss optimization
- **Critical path**: Entity embeddings (query box c, o; answer point c', 0) → Relation embeddings (4-tuple r1, r2, r3, r4) → Logical operators (geometric intersection, projection via T_r, negation approximation) → Distance scorer (distbox_tr combining containment and ordering constraints) → Loss functions (ranking loss L + transitive relation loss L_ˆri)
- **Design tradeoffs**: Multiplicative components (r1, r3) improve performance but reduce idempotency; separate vs shared answer embeddings: dataset-dependent; dimension allocation for transitive relations: one dimension per relation type limits scalability
- **Failure signatures**: Box collapse (all entities mapped to identical points), Negation approximation failure (poor performance on 2in, 3in, pni queries), Transitive ordering violation (Spearman correlation < 0.5 on known chains)
- **First 3 experiments**:
  1. Baseline intersection test: Train on 1p queries only, verify geometric intersection matches neural intersection performance on 2i queries
  2. Transitive loss ablation: Train with and without L_ˆri on WN18RR; compute Spearman correlation on held-out hypernym chains
  3. Negation boundary test: Evaluate on 2in/3in queries across datasets; identify where approximation breaks down by analyzing box overlap statistics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can negation be modeled more robustly across diverse knowledge graph structures without relying on disjoint-box approximations?
- Basis in paper: "The mixed performance on FB237 suggests that our negation approximation may not generalize equally across all knowledge graph structures, highlighting an important direction for future work on context-dependent negation modeling."
- Why unresolved: The current negation approximation assumes disjoint boxes and maps negation queries to positive query types, which works for WN18RR and NELL but underperforms on FB237.
- What evidence would resolve it: Development of a geometric negation operation that maintains competitive performance across all three benchmark datasets without requiring box disjointness assumptions.

### Open Question 2
- Question: Would transitioning to cone embeddings or alternative geometric representations provide a closed-form solution for negation while preserving the fully geometric interpretability?
- Basis in paper: "Future work should explore transitions to cones or alternative geometric representations that naturally support negation operations."
- Why unresolved: Box embeddings are fundamentally not closed under negation (complement of a box is not a box), forcing approximation strategies that compromise accuracy.
- What evidence would resolve it: A geometric embedding method using cones or another representation that natively supports negation, intersection, projection, and union with comparable or superior MRR scores.

### Open Question 3
- Question: Can alternative transformation types and loss functions beyond the identity-based idempotent transformation better capture diverse relational properties (symmetry, composition) in multi-hop reasoning?
- Basis in paper: "A promising direction for future research would be to investigate different transformation types and loss functions in alternative geometric spaces to further improve the representation of relational properties."
- Why unresolved: The current transitive loss enforces ordering along a single dimension using an identity-like transformation, but other relational patterns remain unexplored in the fully geometric framework.
- What evidence would resolve it: Extensions of GeometrE that successfully model symmetric, inverse, and general compositional relations with dedicated geometric constraints and corresponding performance improvements.

## Limitations

- Negation approximation shows dataset-dependent performance, particularly struggling on FB15k-237 queries where boxes cannot be effectively separated
- Transitive loss dimension allocation strategy (one dimension per relation) may not scale well to knowledge graphs with many transitive relations
- The method assumes boxes can be effectively separated for negation, but this may not hold universally across all knowledge graph structures

## Confidence

- **High confidence**: Geometric projection and intersection operations, standard ranking performance on 1p/2p/3p queries
- **Medium confidence**: Transitive loss effectiveness (validated on WN18RR hypernym chains but limited cross-dataset testing)
- **Low confidence**: Negation approximation mechanism (mixed results across datasets, no theoretical guarantees)

## Next Checks

1. Test negation approximation on synthetic datasets where box overlap statistics can be controlled
2. Evaluate transitive loss with shared vs dedicated dimensions across multiple transitive relations
3. Measure embedding collapse frequency during training as a failure diagnostic