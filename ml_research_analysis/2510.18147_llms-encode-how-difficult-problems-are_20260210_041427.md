---
ver: rpa2
title: LLMs Encode How Difficult Problems Are
arxiv_id: '2510.18147'
source_url: https://arxiv.org/abs/2510.18147
tags:
- difficulty
- qwen2
- performance
- arxiv
- probes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models solve complex problems yet fail on simpler
  ones, prompting investigation into whether they internally encode problem difficulty
  in ways that align with human judgment. The authors train linear probes across 60
  models and 60 token positions on mathematical and coding subsets of Easy2HardBench,
  measuring Spearman rank correlation between probe predictions and human-labeled
  difficulty.
---

# LLMs Encode How Difficult Problems Are

## Quick Facts
- arXiv ID: 2510.18147
- Source URL: https://arxiv.org/abs/2510.18147
- Reference count: 10
- Large language models encode problem difficulty in ways that align with human judgment

## Executive Summary
Large language models (LLMs) exhibit a paradox: they can solve complex problems while failing on simpler ones. This study investigates whether LLMs internally encode problem difficulty in a manner consistent with human judgment. Using linear probes across 60 models and 60 token positions on mathematical and coding subsets of Easy2HardBench, the authors find that human-labeled difficulty is strongly decodable from model activations (AMC: ρ≈0.88), scales with model size, and correlates positively with performance during reinforcement learning. In contrast, LLM-derived difficulty scores are weaker, show poor scaling, and degrade as models improve. Steering models toward "easier" representations reduces hallucination, shortens outputs, increases code generation, and improves accuracy.

## Method Summary
The authors train linear probes to decode problem difficulty from model activations across 60 different models and 60 token positions using mathematical and coding subsets of Easy2HardBench. They measure Spearman rank correlation between probe predictions and human-labeled difficulty, comparing human-derived difficulty signals with those automatically generated by LLMs. The study also examines how these difficulty signals evolve during reinforcement learning training using GRPO on Qwen2.5-Math-1.5B.

## Key Results
- Human-labeled difficulty is strongly decodable from model activations (AMC: ρ≈0.88)
- LLM-derived difficulty scores degrade as models improve, while human-difficulty probes strengthen during training
- Steering along the difficulty direction reduces hallucination, shortens outputs, increases code generation, and improves accuracy

## Why This Works (Mechanism)
Human annotations provide a stable, generalizable signal of problem difficulty that models can learn to recognize through linear probes. During reinforcement learning, models amplify this human-aligned difficulty representation, suggesting that human judgment captures aspects of difficulty that remain relevant across model improvements. Automated difficulty estimation methods become misaligned with true difficulty as models improve, indicating that they capture different aspects of the problem-solving process that do not generalize well.

## Foundational Learning
- **Linear probing**: A technique for measuring what information is encoded in model representations by training linear classifiers on top of frozen model activations. Why needed: To assess whether difficulty information is linearly separable in model activations. Quick check: High correlation (ρ≈0.88) between probe predictions and human difficulty labels.
- **Spearman rank correlation**: A non-parametric measure of monotonic relationship between two variables. Why needed: To evaluate alignment between predicted and human difficulty rankings without assuming linear relationships. Quick check: Consistent high correlations across multiple models and token positions.
- **GRPO (Group Relative Policy Optimization)**: A reinforcement learning algorithm that optimizes policy through group-relative advantage estimation. Why needed: To study how difficulty representations evolve during training when models improve. Quick check: Human-difficulty probes strengthen while LLM-difficulty probes weaken during training.

## Architecture Onboarding

Component map: Problem → Model activations → Linear probe → Difficulty prediction

Critical path: Input problem → Model forward pass → Activation extraction → Linear probe classification → Difficulty score

Design tradeoffs: Linear probes offer simplicity and interpretability but may miss nonlinear difficulty representations; using multiple token positions captures difficulty information at different processing stages but increases computational cost.

Failure signatures: Poor probe performance indicates difficulty information is not linearly separable in activations; degradation of LLM-derived difficulty signals suggests automated methods capture non-generalizable aspects of problem difficulty.

First experiments:
1. Evaluate probe performance on out-of-distribution problem sets beyond Easy2HardBench
2. Compare multiple automated difficulty estimation methods (entropy-based, self-consistency) against human annotations
3. Replicate steering experiments on larger models (7B+ parameters) to assess scalability

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on Easy2HardBench as the sole benchmark may limit generalizability to other problem domains
- Probe-based difficulty estimation may miss nonlinear relationships or deeper layer representations
- GRPO training results are limited to a single model size (1.5B) and dataset (Qwen2.5-Math)

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Human-labeled difficulty strongly decodable from activations (ρ≈0.88) | High |
| LLM-derived difficulty scores degrade with model improvement | Medium |
| Behavioral effects of steering toward "easier" representations | Medium |

## Next Checks
1. Test whether human-difficulty probes maintain strong performance on out-of-distribution problem sets beyond Easy2HardBench
2. Evaluate multiple automated difficulty estimation methods to determine if degradation with model improvement is consistent across approaches
3. Replicate steering experiments on larger models (7B+ parameters) to assess scalability of observed behavioral effects