---
ver: rpa2
title: AI Coding with Few-Shot Prompting for Thematic Analysis
arxiv_id: '2504.07408'
source_url: https://arxiv.org/abs/2504.07408
tags:
- coding
- passage
- passages
- refugees
- reviewers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel AI-assisted coding methodology for
  thematic analysis using large language models (LLMs), specifically GPT-3.5-Turbo.
  The method addresses the labor-intensive nature of traditional coding by combining
  few-shot prompting with structured Socratic questioning to improve the quality of
  machine-generated codes.
---

# AI Coding with Few-Shot Prompting for Thematic Analysis

## Quick Facts
- arXiv ID: 2504.07408
- Source URL: https://arxiv.org/abs/2504.07408
- Reference count: 40
- Method achieves F1 score of 0.82 and NPV of 0.97 for AI-generated codes

## Executive Summary
This paper introduces a novel AI-assisted coding methodology for thematic analysis using large language models, specifically GPT-3.5-Turbo. The approach addresses the labor-intensive nature of traditional coding by combining few-shot prompting with structured Socratic questioning to improve machine-generated code quality. The method clusters semantically similar passages and uses exemplar codes from these clusters as references during coding, enabling scalable and reproducible thematic analysis.

The methodology was evaluated on a corpus of 2,530 Malaysian news articles about refugees, with human reviewers assessing the AI-generated codes. The final version achieved an F1 score of 0.82 and a negative predictive value of 0.97, indicating high accuracy in identifying irrelevant material. Inter-rater reliability also improved substantially between iterations, demonstrating the method's effectiveness in producing consistent and interpretable codes.

## Method Summary
The methodology employs few-shot prompting with structured Socratic questioning to guide GPT-3.5-Turbo in generating thematic codes. Passages are first clustered by semantic similarity, then exemplar codes from these clusters serve as references during the coding process. The approach iterates through human review cycles where reviewers assess and refine AI-generated codes, with each iteration improving inter-rater reliability. The framework is designed to be reproducible and scalable for qualitative research while maintaining interpretive richness.

## Key Results
- Final model achieves F1 score of 0.82 for code accuracy
- Negative predictive value reaches 0.97 for identifying irrelevant material
- Inter-rater reliability improves substantially across iterative review cycles

## Why This Works (Mechanism)
The method leverages LLM's ability to recognize patterns across clustered semantic content while using few-shot exemplars to constrain and guide interpretation. Socratic questioning structures the prompt to elicit more nuanced, contextually appropriate codes rather than superficial keyword matches. The clustering approach ensures that similar content receives consistent treatment, while human iteration refines the model's understanding of domain-specific nuances in refugee-related discourse.

## Foundational Learning
**Few-shot prompting**: Providing examples within the prompt to guide LLM behavior; needed to establish coding patterns without extensive fine-tuning; quick check: test with 2-3 exemplar codes per cluster
**Socratic questioning**: Structured prompting technique that asks iterative clarifying questions; needed to elicit deeper analytical responses; quick check: validate question sequences produce consistent code types
**Semantic clustering**: Grouping similar passages based on meaning; needed to ensure consistent coding across related content; quick check: measure cluster coherence with topic modeling metrics

## Architecture Onboarding

**Component map**: News articles -> Semantic clustering -> Exemplar code extraction -> Few-shot prompting -> AI code generation -> Human review -> Code refinement -> Final codes

**Critical path**: Clustering -> Few-shot prompting -> Human iteration. This sequence ensures semantic consistency and quality control through each stage.

**Design tradeoffs**: GPT-3.5-Turbo vs. more capable models (better performance but higher cost); automated vs. human review (efficiency vs. interpretive depth); cluster size (larger clusters improve consistency but may miss nuance).

**Failure signatures**: Inconsistent coding across similar passages indicates clustering problems; overly generic codes suggest insufficient exemplar quality; high reviewer disagreement signals prompt ambiguity or domain complexity.

**First experiments**:
1. Test few-shot prompting with varying numbers of exemplars (1-5) on a small article subset
2. Validate clustering quality by measuring inter-passage similarity within clusters
3. Compare Socratic vs. standard prompting approaches on code specificity

## Open Questions the Paper Calls Out
None

## Limitations
- Single-case evaluation limits generalizability to other domains or languages
- GPT-3.5-Turbo is the only tested model, leaving performance with alternatives unknown
- Iterative human review introduces potential bias not systematically tracked

## Confidence

**High confidence**: Technical implementation of few-shot prompting with Socratic questioning methodology and basic performance metrics (F1=0.82, NPV=0.97)

**Medium confidence**: Scalability claims, as the 2,530-article corpus represents bounded deployment rather than truly large-scale application

**Medium confidence**: Reproducibility framework, given detailed methodology but potential variations in prompt engineering effectiveness

## Next Checks

1. **Cross-domain validation**: Apply methodology to qualitatively distinct corpora (medical literature, social media discourse, policy documents) to assess performance stability

2. **Multi-model comparison**: Evaluate same dataset using alternative LLMs (GPT-4, Claude, Llama) to identify model-specific performance variations

3. **Bias audit protocol**: Implement systematic tracking of human reviewer influence throughout iterations, including individual reviewer agreement rates and systematic biases