---
ver: rpa2
title: Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical
  Entity Linking
arxiv_id: '2505.19722'
source_url: https://arxiv.org/abs/2505.19722
tags:
- data
- llms
- entity
- dataset
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a three-step framework, RPDR, to tackle biomedical
  entity linking. The main issue is that traditional supervised methods require extensive
  annotated data, limiting their usage in low-resource scenarios, while closed-source
  LLMs, though powerful, pose stability issues and high economic costs.
---

# Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking

## Quick Facts
- arXiv ID: 2505.19722
- Source URL: https://arxiv.org/abs/2505.19722
- Reference count: 35
- Primary result: RPDR framework achieves 0.019-0.036 Acc@1 improvement over baselines in low-resource biomedical entity linking using synthetic training data

## Executive Summary
This paper addresses the challenge of biomedical entity linking in low-resource scenarios where annotated data is scarce. The proposed RPDR framework distills knowledge from closed-source LLMs to open-source models through a three-step process: candidate retrieval, synthetic training data generation via prompting, and knowledge distillation through fine-tuning. By generating training data from unannotated examples using GPT-3.5 Turbo or DeepSeek V3, the framework avoids the high costs and stability issues of direct closed-source LLM usage while achieving superior performance compared to traditional supervised methods.

## Method Summary
RPDR employs a three-step framework: First, a bi-encoder retrieves top-k candidate entities from a knowledge base using SapBERT (English) or fine-tuned Chinese-roberta-wwm-ext-large (Chinese). Second, closed-source LLMs (GPT-3.5-Turbo or DeepSeek V3) are prompted with few-shot examples to re-rank these candidates and generate synthetic training pairs from unannotated data. Third, an open-source LLM (BenTsao for Chinese, Llama2-7B for English) is fine-tuned using LoRA on the generated data. The framework achieves economic and stable biomedical entity linking by avoiding direct closed-source LLM calls during inference while leveraging their superior reasoning capabilities during training data generation.

## Key Results
- RPDR achieves 0.019 Acc@1 improvement over supervised fine-tuning on Aier dataset
- Domain-specific student models (BenTsao) outperform general LLMs by ~3% Acc@1 on medical data
- Synthetic training data from GPT-3.5 Turbo achieves 0.740 Acc@1 vs human-labeled data at 0.712 on Aier dataset
- Performance peaks at 1000-2000 generated samples before degrading due to noise accumulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic training data generated by closed-source LLMs can outperform human-annotated data for fine-tuning smaller models
- Mechanism: Closed-source LLMs leverage few-shot learning to re-rank candidates and generate training pairs. The student model learns to mimic this re-ranking behavior, capturing domain reasoning without explicit human annotation.
- Core assumption: The teacher LLM's re-ranking outputs are sufficiently accurate and consistent to serve as supervision signal.
- Evidence anchors:
  - [abstract] "By prompting a closed-source LLM to generate training data from unannotated data and fine-tuning an open-source LLM for re-ranking, we effectively distill the knowledge"
  - [section 4.6, Table 4] GPT-3.5 Turbo generated data achieves 0.740 Acc@1 vs human-labeled data at 0.712 on Aier dataset
  - [corpus] Limited direct evidence; neighbor papers discuss distillation in other contexts but not specifically synthetic data quality
- Break condition: If the closed-source LLM produces systematically biased or incorrect re-rankings for a specific domain, the student model will inherit these errors.

### Mechanism 2
- Claim: Bi-encoder retrieval combined with LLM-based re-ranking decomposes the entity linking problem into tractable sub-tasks
- Mechanism: The bi-encoder reduces the search space from N entities to k candidates via similarity scoring. The LLM re-ranker then performs contextual reasoning over this constrained set, which is more tractable than end-to-end linking.
- Core assumption: The retriever's top-k candidates contain the golden entity with high probability (high recall@k).
- Evidence anchors:
  - [section 3.1] "Entities with the highest similarity scores are selected as candidates"
  - [section 4.5] "By just prompting raw LLMs... we outperform all the baselines on both datasets"
  - [corpus] Neighbor papers on tool-routed LLMs and knowledge graphs similarly use retrieval-re-ranking decompositions
- Break condition: If retriever recall@k is low, re-ranking cannot recover the correct entity regardless of LLM capability.

### Mechanism 3
- Claim: Medical domain pre-training of the student model improves distillation effectiveness
- Mechanism: Models pre-trained on medical corpora (BenTsao for Chinese, Llama2-7B for English) have better initial representations for understanding biomedical mentions, requiring less adaptation during fine-tuning.
- Core assumption: Domain-specific pre-training provides semantic priors that generalize to the entity linking task.
- Evidence anchors:
  - [section 4.6, Table 5] BenTsao achieves 0.740 Acc@1 vs base models (Huozi 0.712, Llama-2 0.712, Alpaca-2 0.702)
  - [section 4.4] "BenTsao... is a model fine-tuned on the Chinese medical knowledge graph"
  - [corpus] Limited direct evidence in neighbors; corpus papers focus on general biomedical LLM evaluation
- Break condition: If domain pre-training introduces task-irrelevant biases or if the target domain differs significantly from pre-training data, benefits may diminish.

## Foundational Learning

- Concept: Bi-encoder vs Cross-encoder architectures
  - Why needed here: The retriever uses a bi-encoder (separate encoding of mention and entity) for efficient retrieval; understanding this distinction is essential for debugging retrieval quality.
  - Quick check question: Can you explain why bi-encoders are more efficient but potentially less accurate than cross-encoders for similarity scoring?

- Concept: Knowledge distillation and teacher-student training
  - Why needed here: The framework's core innovation is distilling closed-source LLM knowledge into open-source models via supervised fine-tuning on generated outputs.
  - Quick check question: What is the difference between distillation using soft probabilities vs. hard labels (generated text outputs)?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: The paper uses LoRA for parameter-efficient fine-tuning of open-source LLMs; understanding this is necessary for implementation and hyperparameter selection.
  - Quick check question: What are the key LoRA hyperparameters (rank, alpha, dropout) and how do they affect model capacity?

## Architecture Onboarding

- Component map:
  1. **Retriever (Bi-encoder)**: SapBERT (English) or Chinese-roberta-wwm-ext-large (Chinese) encodes mentions and entities; inner product computes similarity
  2. **Teacher LLM**: GPT-3.5-Turbo or DeepSeek V3 receives prompts with candidates and outputs re-ranked list
  3. **Student LLM**: BenTsao (Chinese) or Llama2-7B (English) fine-tuned on teacher outputs using LoRA
  4. **Knowledge Base**: ICD-10 (Aier) or SNOMED-CT/AMT (Ask A Patient)

- Critical path: Retriever candidate quality → Teacher prompt design → Generated data volume → Student fine-tuning hyperparameters
  - Assumption: The paper indicates an optimal data volume exists; too much generated data introduces noise (Figure 4 shows performance peak then decline).

- Design tradeoffs:
  - Teacher model choice: GPT-4 provides better re-ranking (0.808 Acc@1 on Aier) but higher cost; GPT-3.5 is more economical
  - Generated data size: 1000 samples used for Aier, 2000 for Ask A Patient; more data helps until noise accumulates
  - Student model initialization: Domain-specific pre-training helps but requires availability of such models

- Failure signatures:
  - Low retriever recall: Check if golden entity appears in top-k candidates
  - Poor teacher outputs: Manually inspect prompt responses for format violations or incorrect rankings
  - Student overfitting: Monitor validation performance during fine-tuning; early stopping if degradation occurs
  - Excessive generated data: Figure 4 shows performance decline with too many samples due to noise accumulation

- First 3 experiments:
  1. **Retriever validation**: Compute recall@k on validation set before re-ranking; ensure golden entity appears in candidates >90% of time
  2. **Prompt iteration**: Test different prompt formats (examples count, output format specification) on small subset; measure format compliance and accuracy
  3. **Data volume sweep**: Train student models with 250, 500, 1000, 2000 generated samples; identify optimal volume before noise degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework mitigate the performance degradation observed when scaling up the volume of LLM-generated training data?
- Basis in paper: [inferred] Section 4.6 (Ablation Studies) and Figure 4 show that model performance initially improves with more generated data but subsequently drops. The authors hypothesize this is due to the introduction of "wrong mention-candidates pairs" (noise) as data volume increases, but they do not propose a solution to filter this noise.
- Why unresolved: The paper identifies the non-monotonic relationship between data size and performance but treats it as an observation rather than a solved problem. The framework currently lacks a mechanism to validate or clean the "distilled" knowledge before the student model is fine-tuned.
- What evidence would resolve it: A modified RPDR framework that includes a data selection or filtering step (e.g., using ensemble disagreement or confidence thresholds) that results in monotonically increasing performance curves as generated data size increases.

### Open Question 2
- Question: Does the RPDR framework maintain its competitive advantage over traditional supervised methods in high-resource settings where abundant human-annotated data is available?
- Basis in paper: [inferred] The abstract and introduction explicitly frame the problem around "low-resource scenarios" and "transfer distortion." The experiments compare RPDR against baselines under limited data conditions (e.g., Aier dataset has only 309 training samples), leaving the "upper bound" performance in data-rich environments unexplored.
- Why unresolved: It is unclear if the error introduced by LLM hallucinations during data generation acts as a ceiling on performance, potentially causing the distilled model to underperform compared to a fully supervised model trained on thousands of high-quality human annotations.
- What evidence would resolve it: Experimental results on a standard biomedical entity linking dataset (e.g., UMLS) comparing RPDR against a state-of-the-art supervised baseline where both have access to the full training set (e.g., >10,000 annotated examples).

### Open Question 3
- Question: How does the framework's performance vary when deployed on open-source student models that lack domain-specific pre-training?
- Basis in paper: [inferred] Table 5 demonstrates that the domain-specific student model (BenTsao) outperforms general open-source models (Llama-2, Alpaca-2). However, the paper does not determine if the distillation process is sufficient to bridge this gap entirely for a general model, or if a medical pre-trained backbone is a strict requirement for the framework's success.
- Why unresolved: While the paper shows BenTsao is better, it does not analyze if the "distillation" step itself can imbue sufficient medical knowledge into a vanilla Llama-2 model to match BenTsao's performance, which is critical for applying this framework to new medical domains where a pre-trained medical LLM may not exist.
- What evidence would resolve it: A comparative analysis showing the convergence rates and final accuracies of a general-domain student model (e.g., standard Llama-2) versus a medical-domain student model when both are fine-tuned using the exact same generated dataset from the teacher model.

## Limitations
- Performance degradation occurs with excessive synthetic training data due to noise accumulation
- Domain-specific pre-trained student models are required for optimal performance, limiting applicability
- Framework effectiveness relies heavily on closed-source LLM quality and may not generalize across all biomedical domains

## Confidence
- **High Confidence**: The decomposition approach (retrieval + re-ranking) and LoRA fine-tuning methodology are well-established
- **Medium Confidence**: The effectiveness of synthetic data generated by closed-source LLMs, as results are shown on only two datasets with one being private
- **Medium Confidence**: The superiority of domain-specific pre-trained models (BenTsao) over general models for biomedical tasks

## Next Checks
1. Test RPDR framework on a publicly available biomedical entity linking dataset (e.g., NCBI disease corpus) to verify generalizability beyond the private Aier dataset
2. Conduct ablation studies varying synthetic data volume (25, 100, 500, 1000, 2000 samples) to confirm the inverted-U relationship between data volume and performance
3. Compare synthetic data quality by evaluating RPDR performance when using different teacher LLMs (GPT-3.5 vs GPT-4) on the same student model and dataset