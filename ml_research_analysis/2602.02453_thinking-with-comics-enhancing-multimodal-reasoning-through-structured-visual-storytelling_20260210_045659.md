---
ver: rpa2
title: 'Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual
  Storytelling'
arxiv_id: '2602.02453'
source_url: https://arxiv.org/abs/2602.02453
tags:
- reasoning
- visual
- comic
- comics
- thinking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thinking with Comics (TwC), a visual reasoning
  paradigm that uses comics as an intermediate medium positioned between static images
  and videos. Comics are leveraged for their ability to encode temporal structure,
  embedded text, and narrative coherence while avoiding the redundancy and high computational
  cost of videos.
---

# Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling

## Quick Facts
- arXiv ID: 2602.02453
- Source URL: https://arxiv.org/abs/2602.02453
- Reference count: 40
- Key outcome: 85.8% accuracy on MathVista, 99.4% on DocVQA, outperforming Thinking with Images and Thinking with Video

## Executive Summary
This paper introduces Thinking with Comics (TwC), a visual reasoning paradigm that uses comics as an intermediate medium between static images and videos. Comics are leveraged for their ability to encode temporal structure, embedded text, and narrative coherence while avoiding the redundancy and high computational cost of videos. The approach is instantiated through two paths: (1) end-to-end comic generation as the reasoning process, and (2) comics used as conditioning context for a vision-language model. Evaluated across reasoning tasks (MATH500, GSM8K, MathVista) and context understanding tasks (DocVQA, CulturalBench), TwC achieves 85.8% accuracy on MathVista and 99.4% on DocVQA, outperforming both Thinking with Images and Thinking with Video.

## Method Summary
The method uses comics generated by a vision model as an intermediate representation for reasoning. Two paths are explored: Path I generates comics end-to-end and extracts answers from the final panel, while Path II uses generated comics as context for a vision-language model to jointly reason. The optimal panel count is 4-6, and narrative style (particularly detective-style) significantly impacts performance. The approach is evaluated on reasoning benchmarks (MATH500, GSM8K, MathVista) and context understanding tasks (DocVQA, CulturalBench).

## Key Results
- 85.8% accuracy on MathVista benchmark
- 99.4% accuracy on DocVQA
- Detective-style narrative outperforms documentary-style by 28.5 percentage points on average
- 4-6 panels are Pareto-optimal; accuracy plateaus beyond this range

## Why This Works (Mechanism)

### Mechanism 1: Structured Visual Abstraction of Temporal Sequences
Comics compress temporal reasoning into discrete, information-dense panels that preserve causal structure while eliminating video redundancy. Sequential panels act as a selected subset of key states from an underlying latent trajectory, maximizing task-relevant information. The answer-relevant information function is approximately submodular, enabling near-optimal state selection with few panels.

### Mechanism 2: Textual Anchoring Reduces Visual Ambiguity
Embedded text (speech bubbles, narration) provides precise semantic cues that disambiguate purely visual representations. By the chain rule, the total information includes both visual and textual components, where text captures additional semantic information that visual representations alone cannot provide.

### Mechanism 3: Narrative Style as Visual System Prompt
Role-playing narrative frameworks (detective, documentary, slice-of-life) activate distinct reasoning pathways in the generation model. Narrative style constrains the latent reasoning trajectory by imposing genre-specific priors on panel composition, character roles, and logical flow.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: TwC extends CoT from text to visual domain; understanding step-by-step decomposition is prerequisite
  - Quick check question: Can you explain how intermediate reasoning steps in CoT improve final answer accuracy?

- **Mutual Information I(X;Y)**
  - Why needed here: The theoretical justification uses conditional mutual information to quantify information efficiency of comics vs. video
  - Quick check question: If I(a;v|q) grows sublinearly with video length T, what does this imply about frame redundancy?

- **Submodularity and Greedy Selection**
  - Why needed here: Panel selection is framed as maximizing a submodular set function; greedy selection gives (1-1/e) approximation
  - Quick check question: Why does submodularity guarantee that selecting K key frames from T >> K video frames retains most relevant information?

## Architecture Onboarding

- **Component map:**
  Input Question (q) → Image Generation Model (Gemini-3 Pro Image) → Generated Comic Panels (C) → ┌────────────┴────────────┐ → Path I: R(c_T) → Answer or Path II: F_φ(q, C) → Answer

- **Critical path:** Panel generation quality → Panel count scaling → Narrative style selection → Answer extraction (Path I) or VLM reasoning (Path II)

- **Design tradeoffs:**
  - Path I vs Path II: Path I is faster/cheaper but constrained by generator's implicit reasoning; Path II adds VLM overhead for explicit reasoning capacity
  - Panel count: 4-6 panels is Pareto optimal; fewer loses structure, more adds redundancy without accuracy gain
  - Narrative style: Detective-style best for logical/mathematical tasks; cultural style for context understanding

- **Failure signatures:**
  - Panel collapse: Model generates merged panels instead of distinct steps (addressed by comic structural prior)
  - Temporal disorder: Shuffled/deleted panels cause 3.5% accuracy drop—model relies on sequence, not isolated images
  - Answer extraction errors: Path I depends on reliable text extraction from final panel; 100% human-agreement in study but may not generalize

- **First 3 experiments:**
  1. Panel count ablation on held-out task: Test N ∈ {1, 2, 4, 6, 8} on a new benchmark (not MATH500) to verify 4-6 panel plateau generalizes
  2. Narrative style transfer test: Apply detective-style prompts to non-logical tasks (e.g., DocVQA) to identify style-task alignment boundaries
  3. Cross-generator validation: Replace Gemini-3 Pro Image with DALL·E 3 or Stable Diffusion to test if comics' advantage is generator-specific or paradigm-intrinsic

## Open Questions the Paper Calls Out

### Open Question 1
Can TwC's performance gains be reproduced with open-source or smaller-scale image generation models, or are they dependent on proprietary frontier models like Gemini-3 Pro Image? The approach relies exclusively on a single proprietary generative model; it remains unclear whether findings generalize.

### Open Question 2
To what extent does cultural or stylistic bias in the underlying generative model affect TwC's reliability across diverse cultural contexts? While CulturalBench results are reported, the paper does not analyze whether the generative model's training data biases comic depictions of specific cultures.

### Open Question 3
How can the optimal number of panels and narrative style be automatically selected for a given task without manual tuning? There is no mechanism for adaptive style/length selection; the scaling analysis is post-hoc rather than predictive.

### Open Question 4
Does the "textual anchoring" effect persist when comics contain visually plausible but factually incorrect embedded text (e.g., hallucinated reasoning steps)? The ablation on textual anchoring shows text improves accuracy, but does not examine whether incorrect text misleads reasoning or whether models can detect inconsistencies.

## Limitations
- The theoretical claims linking comics to information efficiency rely heavily on conditional mutual information arguments that are not empirically validated
- The narrative style effects, while impressive in magnitude, may be task-specific or generator-dependent
- The Path I answer extraction method achieves 100% human agreement but may not generalize to noisy real-world conditions

## Confidence

**High Confidence:** Panel count effects (4-6 panels optimal), temporal sequence importance (shuffled panels drop accuracy), basic superiority over Thinking with Images (consistent benchmark improvements)

**Medium Confidence:** Narrative style effects (strong results but limited ablation across diverse tasks), textual anchoring benefits (substantial gains but cross-task validation needed), Path I vs Path II tradeoff (architectural difference clear but real-world cost comparison lacking)

**Low Confidence:** Theoretical information-theoretic justification (elegant but untested), submodularity assumption for panel selection (unproven), generator independence (results tied to Gemini-3 Pro Image)

## Next Checks
1. **Cross-Generator Validation:** Test the full TwC pipeline using DALL·E 3 or Stable Diffusion instead of Gemini-3 Pro Image to determine if comics' advantage is paradigm-intrinsic or generator-specific
2. **Fine-Grained Temporal Reasoning:** Evaluate TwC on tasks requiring precise motion tracking (e.g., physics simulations) to identify the break condition for structured abstraction
3. **Real-World Cost Analysis:** Measure actual compute costs and inference times for Path I vs Path II across diverse tasks to validate the claimed cost-efficiency advantage