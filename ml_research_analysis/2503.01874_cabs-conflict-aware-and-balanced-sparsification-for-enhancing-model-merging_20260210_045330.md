---
ver: rpa2
title: 'CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging'
arxiv_id: '2503.01874'
source_url: https://arxiv.org/abs/2503.01874
tags:
- task
- cabs
- performance
- merging
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of parameter conflicts and weight
  imbalance in model merging using task vectors, which can significantly degrade the
  performance of merged models. The authors propose CABS (Conflict-Aware and Balanced
  Sparsification), a framework that combines two key strategies: Conflict-Aware Sparsification
  (CA) and Balanced Sparsification (BS).'
---

# CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging

## Quick Facts
- **arXiv ID**: 2503.01874
- **Source URL**: https://arxiv.org/abs/2503.01874
- **Reference count**: 40
- **Primary result**: CABS achieves 76.50 average score on Mistral-7B, surpassing the "ideal" virtual model (76.30) and outperforming the best baseline (76.02)

## Executive Summary
This paper addresses the problem of parameter conflicts and weight imbalance in model merging using task vectors, which can significantly degrade the performance of merged models. The authors propose CABS (Conflict-Aware and Balanced Sparsification), a framework that combines two key strategies: Conflict-Aware Sparsification (CA) and Balanced Sparsification (BS). CA eliminates parameter overlap between task vectors by applying sequential pruning with masks, ensuring non-overlapping weights. BS addresses weight distribution imbalance by applying n:m pruning to maintain an even distribution of weights across layers. The framework is evaluated across diverse tasks and model sizes, including Mistral-7B and RoBERTa-Base, demonstrating significant improvements over state-of-the-art methods.

## Method Summary
CABS operates on task vectors (τ = W_finetuned - W_base) and combines Conflict-Aware Sparsification with Balanced Sparsification. The method first applies n:m pruning to the first task vector (τA) to generate maskA. The second task vector (τB) is then masked with the inverse (1 - maskA) before its own pruning, ensuring retained weights occupy disjoint parameter positions. This guarantees orthogonality between task vectors. The Balanced Sparsification component partitions weight matrices into blocks of m consecutive elements and retains the n largest-magnitude weights per block, preventing local concentration of retained weights. After pruning, sparse task vectors are merged with the base model using rescaled coefficients (λ/(1-p) where p is sparsity) to recover information capacity.

## Key Results
- On Mistral-7B, CABS achieves an average score of 76.50, surpassing the "ideal" virtual model (76.30) and outperforming the best baseline (76.02)
- On RoBERTa-Base, CABS achieves a score of 81.70, outperforming the SOTA (79.88) by 1.82 points and the vanilla baseline (79.55) by 2.15 points
- CABS demonstrates consistent improvements across diverse tasks including ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K, and GLUE benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Sequential Masking for Zero Overlap
Eliminating parameter overlap between task vectors reduces interference during merging, enabling independent scaling of task contributions. CA applies sequential pruning where the first task vector (τA) is pruned to generate maskA. The second task vector (τB) is then masked with the inverse (1 - maskA) before its own pruning, ensuring retained weights occupy disjoint parameter positions. This guarantees orthogonality: ⟨τA, τB⟩F = 0, so the cross-term in ‖λAτA + λBτB‖²F vanishes.

### Mechanism 2: Block-Localized Pruning for Even Weight Distribution
Enforcing uniform retention density across weight blocks prevents local concentration that amplifies under global rescaling. BS partitions weight matrices into blocks of m consecutive elements and retains the n largest-magnitude weights per block. This prevents magnitude pruning's tendency to cluster retained weights, which would cause disproportionate influence after uniform λ rescaling.

### Mechanism 3: Rescaling Recovers Pruned Information Capacity
Sparse task vectors, when rescaled by 1/(1-p) where p is sparsity, recover near-original task performance. After pruning removes p fraction of weights, the remaining weights are multiplied by 1/(1-p) to preserve expected activation magnitudes. This compensation is nearly lossless for task-specific information.

## Foundational Learning

- **Task Vectors (τ = W_finetuned - W_base)**: Why needed: CABS operates entirely on task vectors; understanding them as directional updates in parameter space is prerequisite. Quick check: Given a base model and two fine-tuned variants, can you compute and interpret their task vectors?
- **Magnitude-Based Pruning**: Why needed: The paper's diagnosis starts with why magnitude pruning underperforms in merging; you need to know what it is. Quick check: If you zero out the smallest 90% of weights by absolute value, what happens to the weight distribution heatmap?
- **Frobenius Inner Product and Orthogonality**: Why needed: Section 4.3 derives orthogonality from non-overlap; this is the theoretical justification for CA. Quick check: If two sparse vectors have disjoint non-zero positions, what is their Frobenius inner product?

## Architecture Onboarding

- **Component map**: Input task vectors → CA Module (sequential pruning with mask propagation) → BS Module (block-wise n:m pruning) → Merge Step (W_final = W_base + Σ λ_i × (mask_i ⊙ τ_i))
- **Critical path**: 1) Compute task vectors from fine-tuned models; 2) For first task vector: apply BS → get maskA; 3) For each subsequent vector: compute remaining = τ ⊙ (1 - prior_masks) → apply BS → accumulate mask; 4) Merge with rescaled λ values
- **Design tradeoffs**: Higher sparsity → less overlap but requires larger λ; smaller n:m blocks → more uniform distribution but may lose globally important weights; pruning order affects task-specific scores but average performance is robust
- **Failure signatures**: Performance collapses at high sparsity without rescaling; overlap rate remains high if CA masking is skipped; grid search too coarse misses optimal λ
- **First 3 experiments**: 1) Reproduce overlap analysis comparing magnitude vs. random pruning at sparsity 0.5-0.9; 2) Ablate CA and BS independently at 75% sparsity; 3) Lambda sensitivity sweep at 90% sparsity with λ from 1.0 to 3.0

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that zero overlap is universally beneficial may not hold for tasks with complementary parameter usage patterns
- The BS block size selection (32:128) appears empirically chosen without theoretical justification
- The evaluation relies heavily on average scores across heterogeneous tasks, potentially masking task-specific degradation

## Confidence

- **High confidence**: The empirical results showing CABS outperforming baselines on Mistral-7B and RoBERTa-Base, and the basic mechanism of sequential masking to reduce overlap
- **Medium confidence**: The claim that zero overlap is universally beneficial, and that block-local n:m pruning is optimal for weight distribution
- **Low confidence**: The assumption that rescaling by 1/(1-p) provides near-lossless recovery at all sparsity levels, and that the chosen hyperparameters are globally optimal

## Next Checks

1. **Ablation of overlap thresholds**: Systematically vary the allowed overlap percentage (0%, 25%, 50%, 75%) in CA and measure average performance degradation to test whether zero overlap is truly optimal or if some overlap provides benefit through synergy.

2. **Extreme sparsity stress test**: Evaluate CABS performance at 95-99% sparsity with and without rescaling to determine the breaking point where 1/(1-p) compensation fails and identify alternative strategies for extreme compression.

3. **Joint optimization of CA and BS**: Implement a joint optimization framework where CA mask generation and BS block selection are optimized together rather than sequentially, to test whether the current two-stage approach is suboptimal.