---
ver: rpa2
title: 'UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making
  Systems via Multi-Dataset Integration'
arxiv_id: '2510.24166'
source_url: https://arxiv.org/abs/2510.24166
tags:
- planning
- trajectory
- driving
- learning
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-dataset integration
  for motion planning in autonomous vehicles. The key insight is that vehicle trajectory
  distributions and history-future correlations are consistent across different datasets,
  enabling cross-dataset learning.
---

# UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration

## Quick Facts
- arXiv ID: 2510.24166
- Source URL: https://arxiv.org/abs/2510.24166
- Authors: Xin Yang; Yuhang Zhang; Wei Li; Xin Lin; Wenbin Zou; Chen Xu
- Reference count: 40
- Primary result: Multi-dataset integration for motion planning enabled by consistent trajectory distributions and history-future correlations across datasets

## Executive Summary
This paper addresses the challenge of multi-dataset integration for motion planning in autonomous vehicles. The key insight is that vehicle trajectory distributions and history-future correlations are consistent across different datasets, enabling cross-dataset learning. The proposed UniPlanner framework achieves this through three innovations: a History-Future Trajectory Dictionary Network (HFTDN) that aggregates trajectory pairs from multiple datasets for retrieval-based planning guidance, a Gradient-Free Trajectory Mapper (GFTM) that learns universal history-future correlations while preventing shortcut learning through gradient isolation, and a Sparse-to-Dense (S2D) training paradigm that adaptively suppresses planning priors during training while fully utilizing them during inference. Extensive experiments on nuPlan benchmarks demonstrate UniPlanner's effectiveness, achieving 87.25 NR-CLS (+4.14%) and 71.38 NR-CLS (+3.63%) on Test14-random and Test14-hard benchmarks respectively, establishing a new paradigm for multi-dataset motion planning.

## Method Summary
UniPlanner integrates three key components to enable multi-dataset motion planning. The Gradient-Free Trajectory Mapper (GFTM) is pre-trained on Waymo and Lyft datasets to learn history-to-future trajectory mappings, with its feature extractors frozen to prevent shortcut learning when integrated into the main network. The main planning network uses a GameFormer backbone that processes scene context and receives GFTM's planning prior as auxiliary input, with S2D (Sparse-to-Dense) training applying adaptive dropout to these priors during training to improve robustness. The History-Future Trajectory Dictionary Network (HFTDN) maintains a trajectory dictionary constructed from multiple datasets and uses a guidance module to retrieve similar historical scenarios and provide explicit planning guidance via cross-attention. The entire pipeline is trained sequentially: GFTM pre-training on Waymo+Lyft (50 epochs), main network training on nuPlan (50 epochs with S2D), then HFTDN training with frozen main network (50 epochs).

## Key Results
- Achieves 87.25 NR-CLS (+4.14%) and 71.38 NR-CLS (+3.63%) on Test14-random and Test14-hard benchmarks respectively
- Demonstrates superior performance compared to existing multi-dataset integration methods
- Validates the core hypothesis that trajectory distributions and history-future correlations are consistent across different autonomous driving datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dataset integration for motion planning is enabled by consistent trajectory distributions and history-future correlations across datasets.
- Mechanism: While sensor data and map formats differ across datasets like Waymo, Lyft, and nuPlan, the fundamental physics of vehicle motion and driving patterns (curvature, yaw rate, velocity) remain consistent. This allows a model to learn a "universal" understanding of motion from combined data.
- Core assumption: The statistical distributions of motion types and temporal correlations between past and future trajectories are invariant across different geographical locations and collection platforms.
- Evidence anchors:
  - "we discover that vehicular trajectory distributions and history-future correlations demonstrate remarkable consistency across different datasets."
  - "Figure 1 shows remarkably consistent trajectory distributions across all datasets..." and "seven key transition patterns... consistently dominate across all examined datasets..."
  - No direct corpus paper refutes this invariance; UniTraj [16] is cited for prediction, but UniPlanner is positioned as the first for planning.
- Break Condition: If the target deployment environment has fundamentally different road infrastructure or traffic rules that drastically alter trajectory distributions, the transfer learning from multi-dataset priors may degrade or fail.

### Mechanism 2
- Claim: A Gradient-Free Trajectory Mapper (GFTM) enables the safe use of powerful historical trajectory priors without inducing harmful shortcut learning.
- Mechanism: GFTM is pre-trained on a large multi-dataset corpus to learn a mapping from history to future trajectories. Crucially, its feature extractors are frozen when integrated into the main planning network. This allows it to provide a "planning prior" feature without the main network learning to over-rely on or "cheat" with this easy signal.
- Core assumption: Shortcut learning in gradient-based planning networks is a primary cause of poor closed-loop performance, and a frozen pre-trained feature extractor can provide valuable, non-leaky information.
- Evidence anchors:
  - "Its gradient-free design ensures the introduction of valuable priors while preventing shortcut learning..."
  - "This design leverages the learned history-future correlations to transform historical trajectories into effective planning priors without creating harmful dependencies in the main network."
  - The problem of shortcut learning is corroborated by related work [17, 18, 27, 28].
- Break Condition: If the pre-training datasets do not adequately cover the motion patterns in the target domain, the frozen GFTM will provide a poor or irrelevant prior that fails to aid the planner.

### Mechanism 3
- Claim: A Sparse-to-Dense (S2D) training paradigm and a History-Future Trajectory Dictionary Network (HFTDN) further enhance robustness and leverage cross-dataset knowledge.
- Mechanism: S2D applies adaptive dropout to planning priors during training (sparse) to force robust feature learning but uses the full prior during inference (dense). The HFTDN uses a trajectory dictionary constructed from multiple datasets to retrieve similar historical scenarios and provide explicit guidance, which is injected into the main network via cross-attention after a separate, isolated training phase.
- Core assumption: Forcing the network to be robust to missing priors during training improves generalization, and explicit retrieval from a diverse dictionary is a superior way to handle multi-dataset knowledge compared to implicit learning alone.
- Evidence anchors:
  - "...Sparse-to-Dense (S2D) training paradigm that adaptively suppresses planning priors during training while fully utilizing them during inference."
  - "S2D employs phase-aware feature masking... HFTDN combines a History-Future Trajectory Dictionary with a Universal Dataset Trajectory Guide Module..."
  - Related principles of robust training are explored in RL sim-to-real transfer, but not in this specific sparse-to-dense formulation for dictionary-based planning.
- Break Condition: If the S2D dropout probability is misconfigured, it could overly hamper training. If the dictionary retrieval similarity metric or the HFTDN's isolated training process is flawed, the retrieved guidance could be counterproductive.

## Foundational Learning

- Concept: **Shortcut Learning in Imitation Learning**
  - Why needed here: The entire architecture is designed to mitigate this specific problem, where a planner exploits simple correlations in training data to minimize loss without learning the true driving logic, leading to catastrophic failures in new scenarios.
  - Quick check question: Can you explain why a model that achieves near-perfect open-loop error can still fail dramatically in a closed-loop simulation?

- Concept: **Temporal Correlations in Trajectory Data**
  - Why needed here: The paper's central hypothesis relies on the idea that past motion strongly predicts future motion. Understanding the strength and limitations of this assumption is critical for evaluating the method.
  - Quick check question: What are two driving scenarios where past trajectory might *not* be a good predictor of the immediate future (e.g., sudden emergency braking, lane change initiation)?

- Concept: **Retrieval-Augmented Generation (RAG) / k-Nearest Neighbors (k-NN)**
  - Why needed here: The HFTDN module is essentially a k-NN system for trajectories. Understanding how to retrieve relevant examples and aggregate their information is key to implementing and debugging this component.
  - Quick check question: In the HFTDN, how is the information from multiple retrieved future trajectories (top-K) aggregated into a single guidance vector?

## Architecture Onboarding

- Component map:
    - **GFTM (Gradient-Free Trajectory Mapper):** A pre-trained, frozen network that takes ego history as input and outputs a planning prior feature.
    - **Main Network:** A prediction-planning backbone (e.g., based on GameFormer) that processes scene context. It takes the GFTM's prior as an auxiliary input.
    - **S2D (Sparse-to-Dense):** A training-only module that applies adaptive dropout to the GFTM's output feature.
    - **HFTDN (History-Future Trajectory Dictionary Network):** A separate sub-network containing a trajectory dictionary and a guidance module. It takes scene context and retrieves/generates guidance, which is injected into the main network via cross-attention.
- Critical path: The ego vehicle's historical trajectory is fed into the GFTM to create a planning prior. This prior, modulated by S2D during training, is fed into the main network. In parallel, the HFTDN retrieves guidance from its dictionary based on history similarity and injects it into the main network. The main network's planner head produces the final trajectory.
- Design tradeoffs:
    - **GFTM Freezing:** Trading the flexibility of end-to-end fine-tuning for robustness against shortcut learning. The prior is fixed, which limits adaptation to out-of-distribution histories but ensures the main network learns robustly.
    - **HFTDN Isolation:** Training the HFTDN separately with a frozen main network prevents it from learning to "overwrite" the main network's features, but adds a complex multi-stage training pipeline.
    - **Dictionary Retrieval:** Using an explicit dictionary provides interpretable, instance-based reasoning, but its capacity is limited by the size and diversity of the dictionary compared to a purely parametric model.
- Failure signatures:
    - **Over-smoothed or Unresponsive Plans:** GFTM prior may be too dominant or generic, causing the planner to ignore critical real-time scene context.
    - **Erratic or Inconsistent Guidance:** Poorly formed dictionary or similarity metric in HFTDN could lead to retrieval of irrelevant or contradictory future trajectories.
    - **Degraded Performance on Novel Scenarios:** The entire system relies on cross-dataset invariance. Failure on a scenario type not represented in Waymo/Lyft/nuPlan would indicate a break in the core assumption.
- First 3 experiments:
  1.  **GFTM Ablation:** Train and evaluate the planner (1) with GFTM features end-to-end (gradient flow on), (2) with frozen GFTM features (UniPlanner style), and (3) without GFTM features. Compare closed-loop scores to quantify the impact of the gradient-free design on shortcut learning.
  2.  **HFTDN Retrieval Analysis:** For a set of test scenarios, visualize the top-K trajectory pairs retrieved by the HFTDN from the dictionary. Qualitatively assess if the retrieved futures are semantically similar to the ground truth.
  3.  **Cross-Dataset Validation:** Train the entire UniPlanner pipeline on one primary dataset (e.g., nuPlan) with auxiliary data from one other dataset (e.g., Waymo), then evaluate on a held-out portion of a *third* dataset (e.g., Lyft) to test the generalization of the learned multi-dataset priors.

## Open Questions the Paper Calls Out

- **Cross-Dataset Generalization**: The paper doesn't validate whether the learned multi-dataset priors generalize to datasets not included in the study (e.g., Argoverse, nuScenes). The analysis is descriptive rather than predictive about whether these patterns capture the full complexity of driving behaviors.

- **Long-Tail Scenario Coverage**: The K-means clustering used to construct the History-Future Trajectory Dictionary may filter out critical "long-tail" safety scenarios by selecting only representative centroids. It's unclear if the specific "outliers" removed during dictionary construction contained unique maneuver logic necessary for extreme edge cases.

- **Scalability Analysis**: The paper doesn't analyze how the computational complexity of the HFTDN's retrieval mechanism scales as the volume of integrated datasets grows from thousands to millions of hours. The current dictionary uses 3,909 representative pairs, but no analysis of retrieval latency or memory usage at larger scales is provided.

- **Cultural Robustness**: The identified "universal" history-future correlations may overfit to the specific infrastructure constraints of the US/Singapore datasets used. The authors assert that consistency stems from "road infrastructure constraints," but this assumption may fail in regions with less structured road discipline or non-lane-based traffic.

## Limitations

- The core hypothesis of cross-dataset invariance lacks external validation on additional datasets and driving environments beyond the three studied (Waymo, Lyft, nuPlan).
- The architectural innovations introduce significant complexity with limited ablation studies showing individual contributions of each component.
- The paper reports aggregate performance improvements but doesn't provide detailed breakdowns of which specific driving scenarios benefit most from multi-dataset integration versus which might be harmed by it.

## Confidence

- **High Confidence**: The implementation of the three-phase training pipeline (GFTM pre-training → main network training → HFTDN training) and the specific hyperparameter choices (learning rates, batch sizes, epoch counts) are clearly specified and reproducible.
- **Medium Confidence**: The core hypothesis that trajectory distributions and history-future correlations are consistent across datasets is plausible and internally supported, but requires external validation on additional datasets and driving environments.
- **Medium Confidence**: The gradient-free design of GFTM is a reasonable approach to prevent shortcut learning, but the paper provides limited empirical evidence comparing it against alternative regularization techniques or end-to-end fine-tuning approaches.
- **Low Confidence**: The relative contributions of S2D and HFTDN to the final performance are unclear due to insufficient ablation studies. The paper doesn't quantify how much each component adds versus the baseline or whether simpler approaches could achieve similar results.

## Next Checks

1. **Cross-Dataset Generalization Test**: Train UniPlanner on a combination of two datasets (e.g., Waymo + Lyft), then evaluate on a held-out third dataset (nuPlan) and compare against models trained only on the two training datasets. This directly tests whether the learned multi-dataset priors generalize to unseen data.

2. **Ablation Study on Individual Components**: Systematically disable each innovation (train GFTM end-to-end, remove S2D, remove HFTDN) and measure the impact on both open-loop trajectory error and closed-loop simulation performance. This quantifies the marginal benefit of each architectural choice.

3. **Behavioral Analysis on Critical Scenarios**: Identify specific driving scenarios where shortcut learning is most likely to occur (e.g., complex intersection navigation, emergency braking scenarios) and conduct targeted analysis of UniPlanner's behavior versus baseline models. This validates whether the gradient-free design actually prevents the most harmful forms of shortcut learning.