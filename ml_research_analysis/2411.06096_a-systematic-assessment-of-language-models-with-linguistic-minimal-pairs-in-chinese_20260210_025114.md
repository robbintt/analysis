---
ver: rpa2
title: A Systematic Assessment of Language Models with Linguistic Minimal Pairs in
  Chinese
arxiv_id: '2411.06096'
source_url: https://arxiv.org/abs/2411.06096
tags:
- chinese
- pairs
- minimal
- length
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZhoBLiMP, the largest linguistic minimal
  pair benchmark for Chinese with over 100 paradigms covering diverse syntactic phenomena.
  The authors train a suite of Chinese language models (Zh-Pythia) from scratch with
  varying parameters and tokenizers to study learning patterns.
---

# A Systematic Assessment of Language Models with Linguistic Minimal Pairs in Chinese

## Quick Facts
- **arXiv ID:** 2411.06096
- **Source URL:** https://arxiv.org/abs/2411.06096
- **Reference count:** 22
- **Primary result:** Introduces ZhoBLiMP (largest Chinese linguistic minimal pair benchmark) and SLLN-LP metric to mitigate length bias in LM acceptability judgments.

## Executive Summary
This paper presents ZhoBLiMP, the largest linguistic minimal pair benchmark for Chinese with over 100 paradigms covering 15 syntactic phenomena, and introduces Zh-Pythia, a suite of Chinese language models trained from scratch. The authors propose a new metric, sub-linear length normalized log-probabilities (SLLN-LP), to address length-related biases in minimal pair evaluations. Using this metric, they find that anaphor, quantifiers, and ellipsis remain challenging for language models even at 32B parameters, and demonstrate that SLLN-LP effectively reduces length bias across Chinese, English, Dutch, and Japanese benchmarks.

## Method Summary
The authors create ZhoBLiMP by adapting 118 paradigms from the English BLiMP benchmark, generating over 35k minimal pairs through template-based generation using shared vocabularies and syntactic rules. They train a suite of Chinese language models (Zh-Pythia) ranging from 14M to 1.4B parameters using Pythia architecture on 12GB of Chinese text. The key innovation is SLLN-LP, which normalizes sentence log-probabilities by dividing by the token count raised to the power of α (optimized to 0.5), addressing the bias where longer sentences are inherently penalized in traditional log-probability scoring.

## Key Results
- ZhoBLiMP contains 35k minimal pairs across 118 paradigms and 15 phenomena, making it the largest Chinese linguistic minimal pair benchmark.
- Anaphor, quantifiers, and ellipsis phenomena show poor performance even with 32B parameter models, suggesting these require pragmatic/discourse information beyond pure syntax.
- SLLN-LP (α=0.5) effectively reduces length-related bias in acceptability judgments across Chinese, English, Dutch, and Japanese benchmarks.
- U-shaped learning curves are observed for several phenomena, indicating complex learning dynamics during model training.

## Why This Works (Mechanism)
The paper's approach works by addressing a fundamental limitation in traditional LM evaluation: the bias toward shorter sentences in acceptability judgments. By normalizing log-probabilities using a sub-linear function of sentence length, SLLN-LP creates a more equitable comparison between sentences of different lengths. This is particularly important for Chinese, where tokenization can significantly affect length measurements. The metric ensures that acceptability judgments are based on grammaticality rather than sentence length, making minimal pair evaluations more reliable.

## Foundational Learning
- **Minimal pairs:** Sentence pairs differing by one grammatical element, used to test specific linguistic phenomena. Why needed: Provides controlled evaluation of syntactic competence. Quick check: Verify pairs differ by exactly one grammatical contrast.
- **Linking functions:** Methods to convert log-probabilities to acceptability scores. Why needed: Different functions can bias results differently. Quick check: Compare Mean LP vs SLLN-LP outputs on same data.
- **Tokenization impact:** How character/word-level tokenization affects length measurements. Why needed: Critical for accurate SLLN-LP normalization in Chinese. Quick check: Confirm tokenizer vocabulary size matches expected values.
- **U-shaped learning:** Performance that initially improves then declines before recovering. Why needed: Indicates complex learning dynamics in LMs. Quick check: Plot accuracy vs training steps for individual paradigms.

## Architecture Onboarding

**Component Map:** ZhoBLiMP benchmark -> Zh-Pythia models -> SLLN-LP metric -> Acceptability scoring

**Critical Path:** Template generation → Model training → Inference on minimal pairs → SLLN-LP calculation → Accuracy computation

**Design Tradeoffs:** The paper chooses Pythia architecture for reproducibility but sacrifices potential performance gains from newer architectures. Template-based generation ensures coverage but may miss edge cases compared to natural data.

**Failure Signatures:** Poor performance on specific paradigms may indicate tokenization issues rather than genuine linguistic difficulty. U-shaped learning curves suggest interference between different grammatical rules during training.

**First Experiments:**
1. Verify SLLN-LP implementation by reproducing accuracy scores for Zh-Pythia-160m on ZhoBLiMP test set.
2. Apply SLLN-LP to English BLiMP to confirm cross-linguistic effectiveness.
3. Test sensitivity of results to different α values (0.3-0.7 range).

## Open Questions the Paper Calls Out
The paper explicitly identifies several unresolved questions: What causes the U-shaped learning curves observed in Chinese LMs for phenomena like FCI Licensing and Topicalization? To what extent does the distinction between bare reflexive ziji and compound reflexive ta-ziji account for poor anaphora performance? Can performance on challenging phenomena be improved by augmenting training data with pragmatic and discourse context? What linguistic properties of Dutch cause mean probability (α=1.0) to be optimal versus sub-linear normalization for other languages?

## Limitations
- The 12GB Chinese corpus used for training is not explicitly confirmed as publicly released, blocking full training reproduction.
- The paper relies on a single α value (0.5) for SLLN-LP without exploring a wider range or providing principled justification.
- Results are based on a specific model suite and may not generalize to all Chinese LMs or linking functions.

## Confidence
- **High Confidence:** Construction of ZhoBLiMP benchmark, release of evaluation scripts, basic efficacy of SLLN-LP in reducing length bias.
- **Medium Confidence:** Claims about specific phenomena being "particularly challenging" based on single metric.
- **Medium Confidence:** Broader conclusion about careful consideration of linking functions, though this is sound principle.

## Next Checks
1. Replicate accuracy scores for Zh-Pythia-160m on ZhoBLiMP test set to verify SLLN-LP implementation.
2. Apply SLLN-LP (α=0.5) to English, Dutch, and Japanese BLiMP datasets to independently verify cross-linguistic effectiveness.
3. Systematically evaluate SLLN-LP performance across α values (0.3-0.7) on ZhoBLiMP subset to assess hyperparameter sensitivity.