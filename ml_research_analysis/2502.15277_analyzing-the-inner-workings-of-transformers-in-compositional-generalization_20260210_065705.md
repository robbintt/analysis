---
ver: rpa2
title: Analyzing the Inner Workings of Transformers in Compositional Generalization
arxiv_id: '2502.15277'
source_url: https://arxiv.org/abs/2502.15277
tags:
- generalization
- subnetwork
- syntactic
- compositional
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed the inner workings of Transformer models in
  compositional generalization tasks using subnetwork probing and causal analysis.
  The researchers trained a Transformer model on controlled synthetic datasets for
  machine translation and semantic parsing, then identified subnetworks that performed
  well in compositional generalization.
---

# Analyzing the Inner Workings of Transformers in Compositional Generalization

## Quick Facts
- arXiv ID: 2502.15277
- Source URL: https://arxiv.org/abs/2502.15277
- Authors: Ryoma Kumon; Hitomi Yanaka
- Reference count: 17
- Key outcome: Transformer models can learn non-compositional solutions for generalization tasks, relying on non-syntactic features rather than syntactic structure.

## Executive Summary
This study investigates how Transformer models handle compositional generalization tasks using subnetwork probing and causal analysis. The researchers trained Transformer models on synthetic datasets for machine translation and semantic parsing, focusing on two specific patterns of prepositional phrase attachment. Through concept scrubbing and analysis of trained subnetworks, they discovered that while the base models relied heavily on syntactic features, the better-generalizing subnetworks actually depended more on non-syntactic features, indicating a non-compositional solution to the problem.

## Method Summary
The researchers employed a comprehensive methodology combining subnetwork probing and causal analysis. They created controlled synthetic datasets using PCFG grammars with specific splits for training, testing, and compositional generalization evaluation. The Transformer models (3 encoder/decoder layers, 4 attention heads) were trained from scratch on machine translation and semantic parsing tasks. Subnetwork probing was performed using learnable masks trained via hard concrete distribution, while concept scrubbing (LEACE) was used to remove syntactic features layer-by-layer. The team conducted extensive causal analysis by measuring performance drops after feature removal and validated their results through linear probing classification tasks.

## Key Results
- Base Transformer models relied heavily on syntactic features for compositional generalization tasks
- Better-generalizing subnetworks depended on non-syntactic features, revealing a non-compositional solution
- Non-compositional solutions were acquired early in training and improved generalization performance more slowly than in-distribution performance

## Why This Works (Mechanism)
The study reveals that Transformers can achieve compositional generalization without actually learning compositional solutions. Instead of leveraging syntactic structure, the models find alternative pathways through non-syntactic features that happen to work for the specific generalization patterns tested. This suggests that Transformers' strong performance on compositional tasks may not stem from genuine compositional understanding but from finding shortcuts that bypass the need for structural reasoning.

## Foundational Learning
- PCFG generation: Why needed - Creates controlled synthetic datasets for compositional generalization testing; Quick check - Verify generated examples follow specified grammatical constraints
- Subnetwork probing: Why needed - Identifies model components responsible for generalization; Quick check - Confirm masks converge to sparse solutions
- Concept scrubbing: Why needed - Causal analysis of feature importance; Quick check - Validate syntactic feature removal through linear probing accuracy
- Hard concrete distribution: Why needed - Enables differentiable mask learning; Quick check - Monitor mask sparsity during training
- LEACE methodology: Why needed - Removes specific concepts while preserving others; Quick check - Ensure concept classification accuracy drops to near-zero
- Linear probing: Why needed - Validates concept removal effectiveness; Quick check - Confirm concept classifier performance on scrubbed representations

## Architecture Onboarding

**Component map:** Data Generation -> Transformer Training -> Subnetwork Probing -> Concept Scrubbing -> Causal Analysis

**Critical path:** The core experimental pipeline follows: synthetic data generation → base model training → subnetwork probing → concept scrubbing → causal analysis of feature importance

**Design tradeoffs:** The study uses synthetic PCFG data for controlled experiments, sacrificing ecological validity for experimental control. This enables precise manipulation of compositional patterns but may not generalize to natural language complexity.

**Failure signatures:** 
- Base model unexpectedly high generalization accuracy indicates train/test split contamination
- Subnetwork probing failure suggests incorrect mask training procedure or hyperparameter issues
- Concept scrubbing ineffectiveness points to improper LEACE implementation or concept definition problems

**3 first experiments to run:**
1. Generate synthetic data and verify train/generalization split separation
2. Train base Transformer and check baseline performance on both test and generalization sets
3. Apply concept scrubbing to validate syntactic feature removal through linear probing

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on synthetic PCFG datasets may not generalize to natural language complexity
- Focus on only two specific PP attachment patterns limits broader applicability
- Concept scrubbing methodology assumes LEACE effectively removes features without unintended side effects

## Confidence

**High confidence:** Core methodology and experimental results are well-documented and reproducible
**Medium confidence:** Broader implications for compositional generalization in Transformers given synthetic dataset limitations
**Low confidence:** Practical applicability to real-world compositional generalization challenges

## Next Checks
1. Reconstruct the SGET PCFG pipeline and verify generated examples match the paper's examples, ensuring correct train/generalization split separation
2. Replicate linear probing results to validate that concept scrubbing effectively removes syntactic features (targeting ~0% accuracy post-scrubbing)
3. Conduct ablation studies on subnetwork probing hyperparameters (β, λ) to confirm robustness of the non-compositional solution across different parameter settings