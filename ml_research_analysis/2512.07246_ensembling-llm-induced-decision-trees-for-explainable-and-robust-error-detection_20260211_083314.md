---
ver: rpa2
title: Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection
arxiv_id: '2512.07246'
source_url: https://arxiv.org/abs/2512.07246
tags:
- tree
- decision
- error
- data
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses error detection (ED) in tabular data by introducing
  an LLM-as-an-inducer framework comprising TreeED and ForestED. TreeED uses LLMs
  to generate decision trees with rule nodes, GNN nodes, and leaf nodes for interpretable
  ED, while ForestED ensembles multiple such trees using uncertainty-based sampling
  and an EM-based consensus algorithm to enhance robustness.
---

# Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection

## Quick Facts
- arXiv ID: 2512.07246
- Source URL: https://arxiv.org/abs/2512.07246
- Authors: Mengqi Wang; Jianwei Wang; Qing Liu; Xiwei Xu; Zhenchang Xing; Liming Zhu; Wenjie Zhang
- Reference count: 40
- Key outcome: ForestED achieves 16.1% average F1-score improvement over ZeroED baseline across seven datasets

## Executive Summary
This paper introduces an innovative LLM-as-an-inducer framework for error detection in tabular data, comprising TreeED and ForestED. TreeED leverages LLMs to generate interpretable decision trees with rule nodes, GNN nodes, and leaf nodes for explainable error detection, while ForestED enhances robustness through ensemble methods using uncertainty-based sampling and EM-based consensus algorithms. The approach demonstrates significant improvements in accuracy, explainability, and robustness compared to traditional and existing LLM-based error detection methods.

## Method Summary
The framework introduces TreeED, which uses LLMs to generate decision trees specifically for error detection tasks, incorporating interpretable components like rule nodes, GNN nodes, and leaf nodes. ForestED extends this by ensembling multiple TreeED models through uncertainty-based sampling and an EM-based consensus algorithm to improve robustness. The system operates on tabular datasets and focuses on generating both accurate predictions and interpretable explanations for detected errors.

## Key Results
- ForestED achieves 16.1% average F1-score improvement over strongest baseline (ZeroED)
- Significant performance gains across all seven tested datasets
- Demonstrates superior accuracy, explainability, and robustness compared to traditional and other LLM-based ED methods

## Why This Works (Mechanism)
The framework leverages LLMs' reasoning capabilities to generate interpretable decision trees that capture complex error patterns in tabular data. By using ensemble methods with uncertainty-based sampling and EM-based consensus, ForestED reduces individual model biases and improves overall robustness. The interpretable tree structure provides clear explanations for detected errors, addressing the traditional trade-off between accuracy and explainability in error detection systems.

## Foundational Learning
- **Decision Tree Induction**: LLMs are used to generate tree structures instead of traditional splitting algorithms; needed for creating interpretable error detection models; quick check: verify generated trees follow logical decision paths
- **GNN Nodes in Trees**: Graph Neural Network nodes are integrated into decision tree structures; needed to capture complex relational patterns in tabular data; quick check: validate GNN node outputs align with expected error patterns
- **EM-based Consensus Algorithm**: Expectation-Maximization algorithm is used for ensemble consensus; needed to aggregate multiple tree predictions robustly; quick check: monitor convergence of EM iterations across ensemble runs
- **Uncertainty-based Sampling**: Data points are selected for ensemble diversity based on prediction uncertainty; needed to ensure ensemble covers diverse error scenarios; quick check: verify uncertainty metrics correlate with actual error detection difficulty

## Architecture Onboarding
- **Component Map**: Tabular Data -> TreeED (LLM-induced trees) -> ForestED (ensemble) -> Error Detection Output
- **Critical Path**: Data preprocessing -> LLM tree induction (TreeED) -> Ensemble sampling (ForestED) -> Consensus prediction -> Error classification
- **Design Tradeoffs**: Interpretability vs. computational efficiency - more complex tree structures provide better explanations but increase inference time
- **Failure Signatures**: LLM hallucination leading to illogical tree structures; ensemble collapse when uncertainty sampling fails to capture diverse error patterns
- **First Experiments**: 1) Validate individual TreeED model accuracy on benchmark datasets; 2) Test ensemble performance with varying numbers of trees; 3) Evaluate explanation quality through human interpretability studies

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency of ForestED is not explicitly discussed, with potential substantial resource requirements for LLM-based tree induction
- Evaluation focuses primarily on tabular datasets, leaving performance on high-dimensional or unstructured data types unclear
- LLM reliability for generating meaningful decision trees may vary across different domain contexts

## Confidence
- High Confidence: Empirical improvements over ZeroED and other baselines on seven tested datasets are well-supported by F1-score metrics
- Medium Confidence: Explainability claims are reasonable given interpretable tree structure, though real-world validation of human-understandable explanations is limited
- Medium Confidence: Robustness claims through ensemble methods are theoretically sound but could benefit from more diverse failure mode testing

## Next Checks
1. Conduct computational efficiency benchmarking comparing TreeED/ForestED runtime against traditional ED methods on datasets of increasing size and complexity
2. Perform cross-domain validation on non-tabular datasets (e.g., time-series, text-based) to assess generalizability beyond current experimental scope
3. Design user studies to empirically validate that generated decision trees produce explanations genuinely interpretable and useful to domain experts in practical settings