---
ver: rpa2
title: 'Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models
  through Simulation and Task Decomposition'
arxiv_id: '2501.09056'
source_url: https://arxiv.org/abs/2501.09056
tags:
- agent
- question
- story
- theory
- mind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Decompose-ToM, an inference-time algorithm
  that improves Large Language Models' (LLMs) performance on Theory of Mind (ToM)
  reasoning tasks by decomposing them into recursive perspective simulations and knowledge-access
  subtasks. The method recursively simulates agents' perspectives and simplifies questions
  until reaching factual answers, requiring no additional training or extensive prompt
  tuning.
---

# Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition

## Quick Facts
- arXiv ID: 2501.09056
- Source URL: https://arxiv.org/abs/2501.09056
- Authors: Sneheel Sarangi; Maha Elgarf; Hanan Salam
- Reference count: 40
- Key outcome: Inference-time algorithm achieving up to 44.2% accuracy gains on higher-order ToM tasks without training

## Executive Summary
This paper introduces Decompose-ToM, an inference-time algorithm that improves Large Language Models' (LLMs) performance on Theory of Mind (ToM) reasoning tasks by decomposing them into recursive perspective simulations and knowledge-access subtasks. The method recursively simulates agents' perspectives and simplifies questions until reaching factual answers, requiring no additional training or extensive prompt tuning. Evaluated on Hi-ToM and FANToM datasets, Decompose-ToM achieved significant accuracy gains over baseline methods, with improvements of up to 44.2% on higher-order ToM tasks.

## Method Summary
Decompose-ToM is an inference-only algorithm that enhances LLM ToM reasoning through recursive decomposition. It operates in three phases: (1) Initialization - identifies the outermost agent, reframes the question, and sets up world state; (2) Simulation Processing - for each story statement, prompts the LLM to check if the agent knows it using world state, updates the state, and builds a filtered sub-story; (3) Question-Answering - recurses until the question becomes factual, then answers with chain-of-thought. The algorithm requires no training and uses dataset-specific instruction tuning.

## Key Results
- Achieved up to 44.2% accuracy gains on 4th-order ToM tasks compared to baselines
- Maintained performance across longer contexts, reducing accuracy gap between short and long stories from 7.57% to 0.9%
- Demonstrated consistent improvements across both open-source (Llama-3-8B, Llama-3-70B) and closed-source models (GPT-4o, Gemini-1.5-Flash)

## Why This Works (Mechanism)

### Mechanism 1: Recursive Perspective Simulation
The algorithm decomposes higher-order ToM questions into sequential single-agent simulations, reducing reasoning complexity at each step. It identifies the outermost agent in nested belief questions, simulates their perspective by filtering to only their known statements, then recursively processes the simplified question until factual. This transforms an n-th order ToM problem into n sequential 1st-order simulations.

### Mechanism 2: Granular Knowledge-Access Filtering
Per-statement knowledge filtering creates perspective-accurate context windows, reducing distractor interference. For each story statement, the LLM determines whether the target agent would know it based on location tracking and conversation participation. Unknown statements are excluded from the simulated perspective's story.

### Mechanism 3: Symbolic World State Tracking
Explicit world state representation improves knowledge-access judgments by providing structured location/conversation context. The algorithm maintains a text-based world state (e.g., "Living Room: [Agent A, Agent B]") that is both generated and updated by the LLM, reducing reliance on implicit context.

## Foundational Learning

- **Concept: Theory of Mind (ToM) Orders** - Understanding the distinction between 1st-order ("What does A think?") and 4th-order ("What does A think B thinks C thinks D thinks?") reasoning is essential to grasp why recursive decomposition is needed.
  - *Quick check:* Given "Where does Alice think Bob thinks the key is?", how many simulation steps does Decompose-ToM require?

- **Concept: Simulation Theory (Cognitive Psychology)** - The method draws from simulation theory—the idea that we predict others' mental states by simulating their perspective—which frames the algorithmic design philosophy.
  - *Quick check:* How does simulation theory differ from theory-theory as an account of ToM reasoning?

- **Concept: Inference-Time Algorithms vs. Training-Based Methods** - Understanding that Decompose-ToM requires no fine-tuning clarifies deployment constraints and computational cost tradeoffs.
  - *Quick check:* What are the computational cost implications of per-statement LLM calls at inference time?

## Architecture Onboarding

- **Component map:** Agent Identification Module → Question Reframing Module → World State Manager → Knowledge-Access Filter → Recursive Controller → Final QA Module

- **Critical path:** Agent ID → Question Reframe → [World State Init → For each chunk: Knowledge Filter + World State Update] → Recursive check (if still nested, repeat with next agent) → Final QA

- **Design tradeoffs:**
  - Accuracy vs. Cost: Per-statement filtering improves ToM accuracy but increases LLM calls linearly with story length and recursively with ToM order
  - Generality vs. Prompt Tuning: Dataset-agnostic but requires task-specific instruction tuning
  - Forward-Only Assumption: Assumes chronological stories; non-chronological narratives not handled

- **Failure signatures:**
  1. Snowballing errors: Early knowledge-access mistakes propagate through all subsequent simulations
  2. World state desynchronization: Missed agent exit corrupts all subsequent knowledge judgments using that state
  3. Agent ID failures: Misidentifying the target agent breaks the entire recursive chain

- **First 3 experiments:**
  1. Ablate knowledge filtering: Run Decompose-ToM without the knowledge-access step to isolate the contribution of perspective filtering
  2. Stress test non-chronological stories: Construct stories with retroactive knowledge to characterize the forward-only limitation
  3. Token cost profiling: Measure total LLM calls and token consumption per ToM order and story length

## Open Questions the Paper Calls Out

- **Non-chronological narratives:** How can the algorithm be adapted to handle stories where agent awareness is revealed retrospectively? The current implementation uses a simple forward pass and cannot update world states for prior events based on later revelations.

- **Computational efficiency:** How can the computational cost of per-statement processing be reduced? The approach is highly computationally expensive due to multiple LLM calls per sentence, and more efficient methods are needed.

- **Error mitigation in smaller models:** Can the "snowballing" of errors in smaller models during knowledge-awareness tasks be mitigated? Smaller models degrade on higher-order tasks because a single error in knowledge labeling propagates through recursive steps.

- **Automated chunking:** Can the system autonomously determine the atomic "information chunk" relevant to the ToM task? The current system defaults to sentences but could benefit from dynamically defining information chunks.

## Limitations

- Relies entirely on two proprietary benchmarks (Hi-ToM and FANToM) without external validation on real-world ToM tasks
- No ablation studies isolating the contribution of individual components (knowledge filtering vs. world state tracking vs. recursive decomposition)
- Acknowledged limitation: assumes chronological stories and cannot handle retroactive knowledge updates
- Computational cost not analyzed despite requiring multiple LLM calls per statement

## Confidence

- **High Confidence:** The recursive decomposition mechanism is clearly described and accuracy improvements on specific benchmarks are well-documented
- **Medium Confidence:** Claims about maintaining performance across longer contexts are supported by accuracy gap reduction, but analysis doesn't explore whether this is due to the algorithm or inherent model capabilities
- **Low Confidence:** Claims about generalizability across domains are not empirically validated beyond the two benchmark datasets; computational cost analysis is absent

## Next Checks

1. **Ablation study implementation:** Run Decompose-ToM with individual components disabled (no knowledge filtering, no world state tracking, non-recursive version) to quantify each mechanism's contribution to accuracy gains.

2. **Real-world ToM task evaluation:** Apply Decompose-ToM to human-annotated dialogue datasets (e.g., social media conversations, customer service transcripts) to assess performance beyond controlled benchmark environments.

3. **Error propagation analysis:** Systematically measure accuracy degradation at each recursive step for different ToM orders (1st through 4th) to quantify the "snowballing effect" and identify failure thresholds for different model sizes.