---
ver: rpa2
title: 'From What to Respond to When to Respond: Timely Response Generation for Open-domain
  Dialogue Agents'
arxiv_id: '2506.14285'
source_url: https://arxiv.org/abs/2506.14285
tags:
- time
- response
- dialogue
- event
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task called timely dialogue response
  generation, which requires dialogue agents to not only generate appropriate responses
  but also predict when to respond based on the temporal context of ongoing events.
  The authors construct a new benchmark dataset, TIMELY CHAT, and a large-scale training
  dataset (55K dialogues) by leveraging event knowledge from a temporal commonsense
  knowledge graph and synthesizing dialogues using a large language model.
---

# From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents

## Quick Facts
- arXiv ID: 2506.14285
- Source URL: https://arxiv.org/abs/2506.14285
- Authors: Seongbo Jang, Minjin Jeon, Jaehoon Lee, Seonghyeon Lee, Dongha Lee, Hwanjo Yu
- Reference count: 40
- Primary result: Introduced timely dialogue response generation task; TIMER model outperforms baselines on timing prediction and time-conditioned response generation

## Executive Summary
This paper introduces a novel task called timely dialogue response generation, which requires dialogue agents to not only generate appropriate responses but also predict when to respond based on the temporal context of ongoing events. The authors construct a new benchmark dataset, TIMELY CHAT, and a large-scale training dataset (55K dialogues) by leveraging event knowledge from a temporal commonsense knowledge graph and synthesizing dialogues using a large language model. They propose TIMER, a dialogue model trained with a multi-task objective to jointly predict time intervals and generate time-conditioned responses. Experiments show that TIMER outperforms prompting-based LLMs and fine-tuned baselines on both turn-level and dialogue-level evaluations, achieving significantly better performance in predicting appropriate delays and generating time-specific responses that align with the temporal context of events.

## Method Summary
The authors propose TIMER, a dialogue model trained with multi-task learning to jointly predict response timing and generate time-conditioned responses. The model takes as input a speaker turn, predicted time interval, and user utterance, formatted as `<SPK> s_t <TIME> τ_t <UTT> u_t`. The training objective combines response generation loss and timing prediction loss with equal weighting (λ=1.0). The model is fine-tuned on 55K synthetic dialogues generated from ATOMIC2020 event triplets and pseudo-labeled durations from GPT-3.5. TIMELY CHAT benchmark consists of 324 dialogues with 6.5 turns average. The model uses AdamW optimizer (lr=1e-4), 3 epochs, mixed precision training, and beam search inference (beam=3, top-p=0.95).

## Key Results
- TIMER achieves significantly better timing prediction than prompting-based LLMs and fine-tuned baselines on F1, FPR, and RMSLE metrics
- TIMER generates more time-specific responses as measured by G-Eval time-specificity scores (1-5 scale)
- TIMER shows superior performance on dialogue-level G-Eval coherence, delay-appropriateness, and time-specificity metrics

## Why This Works (Mechanism)
The method works by explicitly modeling temporal context through a multi-task learning framework that predicts both when to respond and what to say. By conditioning responses on predicted time intervals and training with synthetic event-driven dialogues, the model learns to ground responses in realistic temporal scenarios rather than always responding instantly.

## Foundational Learning
- Temporal commonsense knowledge graphs (why needed: provide event duration information; quick check: verify ATOMIC2020 filtering criteria)
- Multi-task learning for joint timing prediction and response generation (why needed: enables coordinated temporal and semantic understanding; quick check: confirm λ=1.0 weighting)
- Time-conditioned response generation (why needed: ensures responses align with temporal context; quick check: validate <TIME> token usage in training data)

## Architecture Onboarding

**Component Map:** Input → Tokenizer → Encoder → Multi-task Head (Timing + Response) → Decoder → Output

**Critical Path:** The multi-task head that jointly predicts timing intervals and generates responses is the critical component, as it directly addresses the core challenge of timely response generation.

**Design Tradeoffs:** The model trades immediate responsiveness for temporal appropriateness by introducing delays, which may reduce real-time interaction speed but increases naturalness and appropriateness of responses.

**Failure Signatures:** Over-prediction of delays (high FPR), time-agnostic responses that ignore temporal context, and failure to maintain coherence across multiple turns with varying time intervals.

**Three First Experiments:**
1. Validate the synthetic training data quality by manually checking 50-100 dialogues for temporal coherence
2. Test timing prediction accuracy on TIMELY CHAT benchmark with beam=3, top-p=0.95
3. Compare TIMER 3B against base model fine-tuning on turn-level timing metrics

## Open Questions the Paper Calls Out
### Open Question 1
Can modeling response timing as continuous time ranges (e.g., "2-6 hours") rather than discrete values improve the granularity and realism of timely dialogue response generation?
Basis: Section 8 states the current discrete formulation could be generalized to continuous ranges for more fine-grained control.

### Open Question 2
How does the correlation between human-likeness in timely responses and overall user satisfaction manifest in longer, multi-session interactions across diverse social contexts?
Basis: Section 8 notes potential for exploring longer interactions to analyze correlation between human-likeness and user experience.

### Open Question 3
Do task-specific training architectures beyond standard fine-tuning and in-context learning yield significant improvements in joint prediction of timing and response generation?
Basis: Section 8 suggests more task-specific training approaches could enhance performance beyond the current multi-task objective.

### Open Question 4
How robust is the TIMELY CHAT benchmark to distribution shift when applied to event types and temporal norms not well-represented in MC-TACO and ATOMIC20²⁰ knowledge sources?
Basis: The datasets are constructed from specific temporal commonsense knowledge graphs, potentially limiting generalization to culturally-specific timing norms.

## Limitations
- The TIMER 3B architecture is not explicitly specified, creating reproduction ambiguity
- Synthetic training data may not capture full diversity of real human conversational timing patterns
- Evaluation relies heavily on GPT-4 as proxy for human judgment without direct human studies

## Confidence
**High Confidence** in methodology and experimental design; **Medium Confidence** in synthetic data generation and task formulation; **Low Confidence** in claims of achieving "human-like" interactions without direct human validation.

## Next Checks
1. Determine and test with the exact 3B-parameter architecture used (likely by contacting authors)
2. Manually sample and verify 50-100 synthesized dialogues from the training set for quality
3. Deploy TIMER in small-scale real conversation setting (5-10 participants, 30-minute sessions) to measure actual timing prediction accuracy versus synthetic benchmarks