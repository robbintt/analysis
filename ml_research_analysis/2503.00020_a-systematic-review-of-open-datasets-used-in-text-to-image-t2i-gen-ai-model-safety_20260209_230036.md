---
ver: rpa2
title: A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model
  Safety
arxiv_id: '2503.00020'
source_url: https://arxiv.org/abs/2503.00020
tags:
- prompts
- datasets
- dataset
- harm
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic review of open datasets used in
  text-to-image (T2I) generative AI model safety research. The study analyzes 119,561
  prompts from eight key datasets, categorizing them using the AIR 2024 taxonomy and
  examining their syntactic and semantic diversity.
---

# A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety

## Quick Facts
- **arXiv ID**: 2503.00020
- **Source URL**: https://arxiv.org/abs/2503.00020
- **Reference count**: 40
- **Primary result**: Systematic review analyzing 119,561 prompts across eight datasets reveals significant safety research gaps in T2I models, including 49% sexual content focus and major coverage deficiencies for other harm types

## Executive Summary
This systematic review examines eight open datasets used in text-to-image generative AI model safety research, analyzing 119,561 prompts through the AIR 2024 taxonomy. The study reveals significant imbalances in harm type coverage, with 49% of prompts focusing on sexual content while other critical areas like discrimination, misinformation, and self-harm remain underrepresented. The research also identifies substantial linguistic bias (98.4% English prompts), synthetic data artifacts, and inconsistent labeling practices across datasets.

The findings provide crucial insights for researchers selecting datasets and highlight critical gaps that future safety research must address. By characterizing dataset composition and limitations, the study enables more informed approaches to developing safer T2I models and identifying areas requiring additional attention in safety evaluations.

## Method Summary
The study employs a systematic review methodology to analyze open datasets used in T2I generative AI model safety research. Researchers examined 119,561 prompts across eight key datasets, categorizing them using the AIR 2024 taxonomy framework to assess harm type coverage. The analysis included syntactic and semantic diversity evaluations, with particular attention to linguistic bias, synthetic data artifacts, and labeling consistency. The systematic approach ensures comprehensive coverage of available datasets while maintaining methodological rigor in identifying patterns and limitations across the safety research landscape.

## Key Results
- 49% of analyzed prompts focus on sexual content, revealing significant imbalance in harm type coverage
- 98.4% of prompts are in English, indicating substantial linguistic bias in safety datasets
- Major gaps identified in coverage of discrimination, misinformation, and self-harm harm types
- Synthetic data artifacts and inconsistent labeling practices found across multiple datasets

## Why This Works (Mechanism)
The systematic review methodology effectively identifies patterns and limitations in T2I safety datasets by applying standardized categorization frameworks and comprehensive quantitative analysis. The AIR 2024 taxonomy provides a structured approach to classifying harm types, while the large sample size (119,561 prompts) ensures statistical significance in identifying coverage gaps and biases. The combination of syntactic and semantic analysis reveals both surface-level patterns and deeper structural issues in dataset composition, enabling researchers to understand the full scope of safety research limitations.

## Foundational Learning
**AIR 2024 Taxonomy**: Classification framework for categorizing different types of harms in AI systems. Why needed: Provides standardized vocabulary for comparing and analyzing safety concerns across datasets. Quick check: Can the taxonomy accommodate emerging harm types not yet identified?

**Synthetic Data Artifacts**: Patterns or artifacts introduced during dataset generation that don't reflect natural prompt distributions. Why needed: Understanding these helps identify potential model biases and evaluation limitations. Quick check: Are artifacts consistent across different synthetic generation methods?

**Linguistic Bias Analysis**: Assessment of language distribution and cultural representation in datasets. Why needed: Ensures safety models perform equitably across different linguistic and cultural contexts. Quick check: Does prompt meaning change when translated across languages?

## Architecture Onboarding
**Component Map**: Datasets -> AIR 2024 Taxonomy Categorization -> Syntactic/Semantic Analysis -> Safety Gap Identification -> Research Implications
**Critical Path**: Dataset Selection → Prompt Extraction → Taxonomy Classification → Bias Analysis → Gap Assessment → Recommendations
**Design Tradeoffs**: Comprehensive coverage vs. focused depth, systematic methodology vs. flexible adaptation, quantitative rigor vs. qualitative insights
**Failure Signatures**: Overrepresentation of certain harm types, linguistic homogenization, synthetic artifact dominance, inconsistent labeling patterns
**Three First Experiments**:
1. Replicate analysis with expanded dataset pool (15-20 datasets) to verify pattern persistence
2. Conduct multilingual parallel analysis focusing on non-English prompts
3. Empirical model testing to correlate dataset limitations with actual safety performance

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including how to develop more balanced datasets that adequately represent diverse harm types, strategies for addressing linguistic and cultural biases in safety evaluations, methods for detecting and mitigating synthetic data artifacts, and approaches for standardizing labeling practices across different datasets and research groups.

## Limitations
- Analysis covers only eight datasets, potentially missing important patterns in unanalyzed datasets
- English-language bias (98.4%) may limit generalizability to multilingual contexts and cultural perspectives
- Taxonomy-based categorization may not capture emerging or nuanced harm types
- Focus on prompt-level analysis may miss higher-level patterns in dataset composition and generation methods

## Confidence
**High Confidence** - Quantitative findings regarding dataset composition (119,561 prompts analyzed, 49% sexual content focus) and linguistic bias (98.4% English) are well-supported by systematic methodology and transparent reporting.

**Medium Confidence** - Characterization of synthetic data artifacts and labeling inconsistencies is supported but may vary in severity across different datasets and contexts. Implications for safety model limitations are reasonable inferences requiring further validation.

**Low Confidence** - Broader implications for future research directions are more speculative and depend on evolving landscape of T2I safety research and emerging harm categories.

## Next Checks
1. **Dataset Expansion Validation**: Replicate the analysis using a broader set of T2I datasets (minimum 15-20) to verify whether identified patterns of harm type imbalance and linguistic bias persist across a more representative sample.

2. **Multilingual Extension Study**: Conduct parallel analysis of non-English prompts (focusing on major languages like Chinese, Spanish, Arabic) to assess whether safety concerns and harm patterns identified in English datasets hold across linguistic and cultural contexts.

3. **Model Impact Correlation**: Test actual T2I models trained on analyzed datasets to empirically measure how identified dataset limitations (harm type imbalance, linguistic bias, labeling inconsistencies) manifest in model outputs and safety performance.