---
ver: rpa2
title: 'Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with
  Adaptive Subspace Switching'
arxiv_id: '2602.01233'
source_url: https://arxiv.org/abs/2602.01233
tags:
- arxiv
- low-rank
- gradient
- training
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lotus, a method for efficient LLM training
  that addresses the memory-computation trade-off in large-scale models. Lotus improves
  upon GaLore by using randomized low-rank gradient projection combined with an adaptive
  subspace switching strategy, which dynamically updates the low-rank basis based
  on unit-gradient displacement rather than fixed intervals.
---

# Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching

## Quick Facts
- **arXiv ID**: 2602.01233
- **Source URL**: https://arxiv.org/abs/2602.01233
- **Reference count**: 0
- **Primary result**: 40% memory reduction and 30% training time savings over GaLore with better perplexity and GLUE scores

## Executive Summary
Lotus addresses the memory-computation trade-off in LLM training through randomized low-rank gradient projection combined with adaptive subspace switching. The method improves upon GaLore by using power-iteration-based randomized SVD for faster gradient projection and a path-efficiency criterion that triggers subspace updates only when needed. Theoretical analysis shows the adaptive policy achieves convergence in fewer iterations than fixed-interval switching. Empirical results demonstrate 40% memory savings and 30% training time reduction on LLaMA pre-training (C4 dataset) and RoBERTa fine-tuning (GLUE tasks) while maintaining or improving model quality.

## Method Summary
Lotus modifies gradient projection in low-rank optimization by replacing exact SVD with power-iteration-based randomized SVD for computational efficiency. The key innovation is adaptive subspace switching triggered by path-efficiency (ρ_t), which measures how well projected gradients align with ideal displacement directions. When ρ_t falls below threshold γ and minimum interval T_min has passed, the low-rank subspace is recomputed. This approach avoids the compute waste of premature switching and the performance degradation of delayed switching in fixed-interval methods. The method stores optimizer states in low-rank form, achieving significant memory savings while maintaining training stability and convergence speed.

## Key Results
- 40% memory reduction compared to GaLore through low-rank optimizer state compression
- 30% training time savings achieved by efficient randomized SVD and adaptive switching
- Better model quality: LLaMA-60M pre-training perplexity improves from 34.00 to 33.75; RoBERTa GLUE average score improves from 87.24 to 87.28
- Adaptive switching achieves convergence in fewer iterations than fixed-interval approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Randomized power-iteration SVD approximates full SVD sufficiently for gradient projection while reducing computational cost.
- **Mechanism**: Power iteration amplifies dominant singular vectors while dampening others, allowing efficient approximation of the top-r subspace without computing full SVD.
- **Core assumption**: Top-r singular vectors capture essential gradient structure; approximation error doesn't harm convergence.
- **Evidence anchors**: Paper states "Lotus uses a power-iteration-based randomized SVD to markedly accelerate the gradient projection step"; related work (GaLore 2, SubTrack++) also explores SVD alternatives.

### Mechanism 2
- **Claim**: Path-efficiency metric (ρ_t) based on unit-gradient displacement signals when the current subspace is exhausted.
- **Mechanism**: Computes ρ_t = ‖ΣP_k·ĝ‖₂ / ‖Σĝ‖₂; when ρ_t falls below threshold γ, gradients are canceling or oscillating—indicating subspace staleness.
- **Core assumption**: Unit-norm gradients follow stable trajectories; displacement degradation indicates subspace staleness rather than optimization noise.
- **Evidence anchors**: "When ρ_t ≈ 1, gradients remain confined within a narrow directional cone"; Theorem 3.2 claims N_ada < N_fix.

### Mechanism 3
- **Claim**: Adaptive switching achieves convergence in fewer iterations than fixed-interval switching.
- **Mechanism**: Fixed intervals may switch prematurely or belatedly; adaptive switching times updates to when they provide marginal benefit.
- **Core assumption**: Relationship between path-efficiency and convergence rate is monotonic—lower ρ_t reliably predicts slower progress.
- **Evidence anchors**: "Theoretical analysis shows that the adaptive policy leads to faster convergence with fewer iterations"; proof sketch provided but limited empirical validation.

## Foundational Learning

- **Concept**: Low-rank matrix decomposition (SVD and approximations)
  - **Why needed here**: Lotus projects gradients onto low-rank subspaces; understanding what SVD extracts is essential for debugging projection quality.
  - **Quick check question**: Can you explain why power iteration amplifies dominant singular vectors while dampening others?

- **Concept**: Optimizer state memory in Adam (momentum + variance)
  - **Why needed here**: 40% memory savings come from storing low-rank optimizer states; understanding what Adam stores clarifies what's being compressed.
  - **Quick check question**: For a weight matrix W ∈ R^(m×n), how many floats does Adam store beyond the weights themselves?

- **Concept**: Gradient projection geometry (subspace, projection matrix, residual)
  - **Why needed here**: Path-efficiency measures how well projected gradients approximate full gradients; understanding projection as P = QQ^T helps interpret ρ_t.
  - **Quick check question**: If projection matrix P has rank r < d, what is the dimension of the null space where gradient components are lost?

## Architecture Onboarding

- **Component map**: Full-rank gradient G_F → [Randomized Low-Rank Projector] → Low-rank gradient G_lr → [Adam optimizer on low-rank space] → Low-rank update ΔW_lr → [Project back to full space: Q·ΔW_lr·Q^T] → Weight update ΔW

- **Critical path**: The path-efficiency calculation (normalizing gradients, computing displacement, checking threshold every η steps) determines switching timing. Incorrect implementation causes compute waste or convergence degradation.

- **Design tradeoffs**:
  - **Rank r**: Lower saves memory but may lose gradient information. Paper uses r/d_model ratios of 0.25–0.5.
  - **Threshold γ (0.005-0.02)**: Lower = fewer switches, higher = more responsive but more SVD calls.
  - **Verifying gap η (25-100 steps)**: Lower = finer monitoring but more overhead; too low may trigger on noise.

- **Failure signatures**:
  - Perplexity plateaus early with frequent subspace switches → γ too high or η too low (noise triggering)
  - Training divergence after long stable period → subspace never switching (γ too low)
  - Memory usage higher than expected → projection matrices not being freed between switches
  - Training time not improving over GaLore → rSVD not actually being used (fallback to full SVD)

- **First 3 experiments**:
  1. **Baseline replication**: Pre-train LLaMA-60M on C4 with paper's hyperparameters (r=128, γ=0.01, η=50); verify ~33.75 perplexity and measure actual memory/time vs GaLore.
  2. **Ablation by component**: Run (a) full SVD + fixed switching, (b) rSVD + fixed switching, (c) rSVD + adaptive switching to quantify each contribution.
  3. **Hyperparameter sensitivity**: Sweep γ ∈ {0.005, 0.01, 0.02} and η ∈ {25, 50, 100} on a smaller model to characterize the stable operating region before scaling up.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical foundation gaps: Theorem 3.2's proof sketch lacks rigor and depends on assumptions about gradient path stability that may not hold in practice
- Hyperparameter sensitivity: Narrow operating regime without sensitivity analysis; optimal γ and η may require extensive tuning for new models
- Reproducibility concerns: Critical implementation details omitted including power-iteration count and exact path-efficiency computation timing

## Confidence
- **High confidence**: 40% memory reduction claim is well-supported by mathematical framework of low-rank optimization state compression
- **Medium confidence**: 30% training time improvement depends on implementation quality of randomized SVD efficiency
- **Low confidence**: Convergence theory for adaptive switching is weakly supported; theorem stated but not thoroughly proven or empirically validated

## Next Checks
- **Check 1**: Reproduce RoBERTa-Base GLUE fine-tuning results (target: 87.28 average score) with rank=4 and specified hyperparameters (γ=0.01, η=50); verify memory savings by measuring optimizer state sizes
- **Check 2**: Conduct hyperparameter sensitivity study on small LLaMA model, sweeping γ ∈ {0.005, 0.01, 0.02} and η ∈ {25, 50, 100} while monitoring perplexity, memory usage, and switch frequency
- **Check 3**: Implement ablation study comparing full SVD/fixed, rSVD/fixed, and rSVD/adaptive switching on LLaMA-60M pre-training to isolate contribution of each innovation