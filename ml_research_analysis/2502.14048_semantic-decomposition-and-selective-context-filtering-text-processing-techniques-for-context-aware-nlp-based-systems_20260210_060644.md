---
ver: rpa2
title: Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques
  for Context-Aware NLP-Based Systems
arxiv_id: '2502.14048'
source_url: https://arxiv.org/abs/2502.14048
tags:
- context
- arxiv
- systems
- input
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two novel text processing techniques\u2014\
  Semantic Decomposition and Selective Context Filtering\u2014designed to improve\
  \ the integration and performance of Large Language Models (LLMs) in context-aware\
  \ systems. Semantic Decomposition sequentially breaks down input prompts into structured,\
  \ hierarchical schemas, enabling systems to parse and process natural language commands\
  \ more effectively."
---

# Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques for Context-Aware NLP-Based Systems

## Quick Facts
- arXiv ID: 2502.14048
- Source URL: https://arxiv.org/abs/2502.14048
- Reference count: 34
- This paper introduces two novel text processing techniques—Semantic Decomposition and Selective Context Filtering—designed to improve the integration and performance of Large Language Models (LLMs) in context-aware systems.

## Executive Summary
This paper introduces two novel text processing techniques—Semantic Decomposition and Selective Context Filtering—designed to improve the integration and performance of Large Language Models (LLMs) in context-aware systems. Semantic Decomposition sequentially breaks down input prompts into structured, hierarchical schemas, enabling systems to parse and process natural language commands more effectively. Selective Context Filtering uses embedding-based methods to systematically remove irrelevant context segments, enhancing coherence and reducing computational overhead. Experiments using synthetic datasets and benchmark datasets show that these techniques improve consistency in schema adherence and context selection, with smaller models and vector-based filtering sometimes outperforming larger models. The work offers practical methods to enhance LLM-based system pipelines, though optimal context management and further evaluation are identified as future research directions.

## Method Summary
The authors propose two complementary techniques for improving context-aware NLP systems. Semantic Decomposition uses a hierarchical schema classification approach where inputs are sequentially classified through a predefined structure (query, command, informative, conversational, miscellaneous) using structured outputs enforced by Context-Free Grammars (CFGs). Each classification level can optionally generate a re-summarized prompt for downstream stages. Selective Context Filtering employs embedding-based similarity checks to remove irrelevant context segments before passing information to the LLM, reducing noise and improving coherence. The evaluation uses synthetic datasets (SynPrompt, SynAsst) and real datasets (OASST1, OASST2) with the Exponential Consistency Index (ECI) metric, testing across different context injection strategies and model configurations.

## Key Results
- Semantic Decomposition achieved high consistency scores (ECI up to 0.914 for 3-sentence prompts) when using raw context rather than pre-decomposed context
- Embedding-based context filtering outperformed LLM-based filtering (0.700-0.754 accuracy vs 0.602-0.631) while being more efficient
- Smaller models (gpt-4o-mini) sometimes outperformed larger models (gpt-4o) on consistency metrics, though the mechanism remains unexplained

## Why This Works (Mechanism)

### Mechanism 1: Schema-Constrained Hierarchical Classification
Sequentially decomposing prompts through a predefined hierarchical schema improves reasoning consistency by bounding the LLM's classification space. The LLM acts as a pseudo-classifier at each schema depth level, using structured outputs enforced by Context-Free Grammars (CFGs) to guarantee parseable results. At each level, the input is categorized into one of several predefined classes; if the class is non-terminal, decomposition continues deeper. Optional re-summarization at each stage increases semantic density.

### Mechanism 2: Embedding-Based Context Pruning
Removing low-relevance context segments via embedding similarity reduces noise and improves output coherence. Each context segment is converted to a vector embedding and compared against a reference embedding using similarity metrics. Segments below a threshold are filtered out before being passed to the LLM, reducing token count and potential distractions.

### Mechanism 3: Context Density Optimization
There exists an optimal level of context injection; both over- and under-provisioning degrades consistency. Experiments showed that passing raw (non-pre-decomposed) context often yielded higher ECI scores than injecting decomposed context at every stage. The 3-sentence prompt subset achieved the highest scores, suggesting a "sweet spot" in context density.

## Foundational Learning

- **Context-Free Grammars (CFGs) for Structured Outputs**
  - Why needed here: CFGs enforce that LLM outputs conform to predefined schemas (e.g., enums, nested structures), making them machine-parseable.
  - Quick check question: Given an enum `{query, command, informative}`, how does a CFG guarantee the LLM outputs only one of these values?

- **Embedding Similarity Metrics**
  - Why needed here: Selective Context Filtering relies on comparing vector embeddings to assess relevance.
  - Quick check question: If you have two text segments, what cosine similarity threshold would you use to decide if they're "relevant" to each other? Why?

- **Transformer Attention Scaling**
  - Why needed here: Understanding why context filtering matters—attention cost grows quadratically with sequence length, and irrelevant tokens dilute attention.
  - Quick check question: Why does the self-attention mechanism in transformers scale as O(n²), and what does this imply for long-context inputs?

## Architecture Onboarding

- **Component map:**
  Raw Input Prompt -> Semantic Decomposition Engine (hierarchical schema classifier, CFG-constrained) -> Decomposed Structured Prompt -> Context Store (conversation history + RAG retrieval) -> Selective Context Filter (embedding-based similarity thresholding) -> Filtered Context + Decomposed Prompt -> LLM Inference (gpt-4o-mini or equivalent) -> Structured Output (validated against schema)

- **Critical path:**
  1. Define your hierarchical schema with all leaf categories and negative fallbacks.
  2. Implement Semantic Decomposition as a depth-first classifier using structured outputs.
  3. Build the context store and implement embedding-based filtering.
  4. Inject filtered context + decomposed prompt into the LLM.
  5. Validate output against schema; route to downstream handlers.

- **Design tradeoffs:**
  - Pre-decomposed vs. raw context: Paper shows raw context often yields better consistency—test both for your domain.
  - LLM-based vs. embedding-based filtering: Embedding-based (text-embedding-3-small) achieved higher accuracy (0.700–0.754) than LLM-based (0.602–0.631) but lacks nuanced reasoning.
  - Model size: Smaller models (gpt-4o-mini) sometimes outperformed larger ones—mechanism is unexplained; validate on your workload.

- **Failure signatures:**
  - Schema missing fallback category → pipeline crashes on unexpected inputs.
  - Over-filtering → LLM generates generic or irrelevant responses.
  - Under-filtering → decoherence, hallucination, or off-topic outputs.
  - Ambiguous schema labels → inconsistent classification across runs.

- **First 3 experiments:**
  1. Build a 2-level schema for your domain (e.g., input type → sub-type). Test Semantic Decomposition on 50 real user prompts; compute ECI across 5 runs per prompt.
  2. Implement embedding-based context filtering on a sample conversation log (10+ turns). Compare filtering accuracy against human-labeled relevance.
  3. A/B test context injection levels (raw, pre-decomposed, filtered-only) on a downstream task (e.g., response generation); measure consistency and coherence.

## Open Questions the Paper Calls Out

### Open Question 1
Why do smaller models (e.g., gpt-4o-mini, text-embedding-3-small) consistently outperform larger models on Semantic Decomposition and Selective Context Filtering tasks? The authors report this counterintuitive finding but do not investigate its causes. Controlled experiments varying model scale while controlling for training data, architecture, and prompting; analysis of attention patterns or embedding distributions across model sizes would help resolve this.

### Open Question 2
What is the optimal amount of context tokens to inject at each stage of evaluation to maximize consistency? The authors hypothesize that "a balance needs to be achieved in how much context tokens needs to be passed on during all stages of evaluation" after finding that neither full pre-decomposed context injection nor minimal context performed best. Ablation studies systematically varying context token counts and measuring ECI scores across different input lengths would provide clarity.

### Open Question 3
How do these techniques generalize to non-OpenAI models and open-source LLMs? The conclusion states future work includes "testing on a wider set of models." All experiments used only gpt-4o, gpt-4o-mini, and OpenAI embedding models. Benchmarking Semantic Decomposition and Selective Context Filtering on diverse model families (e.g., Llama, Mistral, Claude) using the same ECI-based evaluation protocol would address this limitation.

## Limitations

- Synthetic datasets may not fully capture real-world complexity and diversity of natural language inputs
- No end-to-end system performance evaluation—only intermediate consistency and filtering accuracy were measured
- Authors cannot explain why smaller models sometimes outperformed larger ones, leaving a critical gap in understanding

## Confidence

- **High confidence**: The mechanism of using CFG-constrained hierarchical classification to improve reasoning consistency (supported by direct quotes and ECI measurements).
- **Medium confidence**: The effectiveness of embedding-based context pruning for coherence improvement (supported by accuracy metrics but lacks direct coherence measurement).
- **Low confidence**: The claim about optimal context density and why smaller models outperformed larger ones (these are hypothesized but not empirically explained).

## Next Checks

1. **Schema Completeness Testing**: Systematically generate and test inputs that fall outside the defined schema categories to measure pipeline failure rates and identify necessary fallback mechanisms.
2. **Embedding Threshold Optimization**: Conduct parameter sweeps across different cosine similarity thresholds (0.3-0.8) on real conversation datasets to identify domain-specific optimal values for context filtering.
3. **End-to-End Performance Evaluation**: Implement the complete system pipeline and measure downstream task performance (response quality, task completion rate) rather than just intermediate consistency metrics, using human evaluation to validate coherence improvements.