---
ver: rpa2
title: Rethinking On-policy Optimization for Query Augmentation
arxiv_id: '2510.17139'
source_url: https://arxiv.org/abs/2510.17139
tags:
- query
- retrieval
- dense
- retriever
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares two paradigms for query augmentation
  in information retrieval: simple prompt-based expansion and reinforcement learning
  (RL)-based query rewriting. Through extensive experiments across evidence-seeking,
  ad hoc, and tool retrieval tasks, the authors find that training-free prompt-based
  methods often match or exceed the performance of more complex RL-based approaches,
  especially when using powerful LLMs.'
---

# Rethinking On-policy Optimization for Query Augmentation

## Quick Facts
- arXiv ID: 2510.17139
- Source URL: https://arxiv.org/abs/2510.17139
- Authors: Zhichao Xu; Shengyao Zhuang; Xueguang Ma; Bingsen Chen; Yijun Tian; Fengran Mo; Jie Cao; Vivek Srikumar
- Reference count: 22
- Primary result: Simple prompt-based query expansion often matches or exceeds RL-based methods, with OPQE achieving state-of-the-art NDCG@10 of 58.1 on ad hoc retrieval

## Executive Summary
This paper challenges the assumption that reinforcement learning (RL)-based query augmentation is superior to simpler methods by systematically comparing prompt-based expansion with RL-based query rewriting across evidence-seeking, ad hoc, and tool retrieval tasks. The authors find that training-free prompt-based methods often perform on par with or better than RL approaches, particularly when using powerful LLMs. This motivates their proposed On-policy Pseudo-document Query Expansion (OPQE), which trains an LLM to generate pseudo-documents optimized for retrieval performance, consistently outperforming both standalone prompting and RL-based rewriting.

## Method Summary
The paper introduces On-policy Pseudo-document Query Expansion (OPQE), a hybrid approach that reformulates query augmentation as pseudo-document generation rather than query rewriting. OPQE uses an LLM policy to generate document-like expansions that are optimized through reinforcement learning with retrieval-based rewards. The method employs KL-regularized PPO training, with the policy constrained to produce pseudo-documents that better align with dense retriever training objectives. Unlike standard RL approaches that rewrite queries, OPQE leverages the LLM's document generation capabilities while maintaining optimization power through on-policy learning.

## Key Results
- Simple prompt-based methods match or exceed RL-based query augmentation across evidence-seeking, ad hoc, and tool retrieval tasks
- OPQE consistently outperforms both standalone prompting and standard RL methods
- On ad hoc retrieval with dense retrievers, OPQE achieves state-of-the-art average NDCG@10 of 58.1
- RL training shows no benefit for dense retrieval when optimized for sparse retriever characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-document generation improves retrieval by transforming asymmetric matching into symmetric matching
- Mechanism: When an LLM generates a pseudo-document answering a query, dense retrievers compare document-to-document embeddings rather than query-to-document. This better aligns with contrastive pretraining objectives that train on document pairs.
- Core assumption: Dense retrievers trained on symmetric similarity tasks perform better when queries are reformulated as documents.
- Evidence anchors:
  - [abstract] "simple, training-free query augmentation often performs on par with, or even surpasses, more expensive RL-based counterparts, especially when using powerful LLMs"
  - [section 4.5] "it transforms asymmetric query-document retrieval into a symmetric document-document task, which better aligns with the training objectives of many dense retrievers"
- Break condition: If the dense retriever was trained specifically on asymmetric query-document pairs rather than symmetric document pairs, pseudo-document generation may not provide this advantage.

### Mechanism 2
- Claim: RL optimization can degrade dense retrieval performance when optimized for sparse retriever characteristics
- Mechanism: RL policies trained with retrieval rewards may learn to produce keyword-dense outputs that improve BM25's term matching but result in embeddings that cluster poorly in semantic space.
- Core assumption: The reward signal from sparse retrievers reinforces lexical features that are suboptimal for dense embedding models.
- Evidence anchors:
  - [abstract] Notes RL methods assumed superior but systematically underperform simple prompting
  - [section 4.4] "RL training shows no benefit [for dense retriever], with average Completeness@10 scores (29.9 for +3B RL, 29.4 for +7B RL) failing to surpass the base score of 29.6"
- Break condition: If reward functions are specifically designed for dense retriever characteristics (e.g., embedding similarity rather than lexical overlap), this degradation should not occur.

### Mechanism 3
- Claim: OPQE's hybrid approach provides "warm start" initialization that accelerates and stabilizes RL training
- Mechanism: By constraining the policy to generate pseudo-documents rather than arbitrary query rewrites, OPQE leverages the LLM's pre-existing document generation capabilities at initialization, starting from a higher baseline reward than standard RL methods.
- Core assumption: The LLM's parametric knowledge produces reasonable pseudo-documents even before RL fine-tuning.
- Evidence anchors:
  - [abstract] OPQE "merging the generative richness of prompting with RL's optimization power"
  - [section 5.2] "In Fig. 1a and Fig. 1c (FEVER and TriviaQA with BM25), the OPQE policy (blue line) starts with a significantly higher average reward than the standard RL policy (red line)"
- Break condition: If the LLM has poor domain knowledge for the target corpus, initial pseudo-documents will be low-quality and the warm-start advantage diminishes.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation for text generation**
  - Why needed here: RL-based query augmentation frames generation as sequential token prediction with retrieval-based rewards
  - Quick check question: Can you map the state, action, policy, and reward components from Equation 1 in the paper?

- **Concept: KL-regularized reinforcement learning**
  - Why needed here: The objective function includes a KL divergence penalty to prevent the policy from drifting too far from the reference model, maintaining fluency
  - Quick check question: What would happen to generated query quality if β (the KL coefficient) were set to zero?

- **Concept: Sparse vs. dense retrieval paradigms**
  - Why needed here: The paper shows that query augmentation effectiveness varies dramatically between BM25 (sparse, term-matching) and Contriever/E5 (dense, semantic)
  - Quick check question: Why might a query optimized for BM25's term frequency scoring perform poorly when embedded by a dense retriever?

## Architecture Onboarding

- **Component map:**
  Original Query → [LLM Policy πθ] → Augmented Query/Pseudo-doc → [Retriever Environment] → Reward Signal

- **Critical path:**
  1. Prompt template design (different for standard RL vs. OPQE)
  2. Reward function selection (task-specific: Hit@20 for evidence-seeking, NDCG@10 for ad hoc)
  3. PPO hyperparameters (learning rate 1e-6, batch size 128, temperature 0.6 as reported)

- **Design tradeoffs:**
  - Standard RL (query rewriting): More flexible output format but lower initial reward and potential retriever mismatch
  - OPQE (pseudo-document): Constrained output format but higher baseline and better dense retriever alignment
  - Prompting-only: Zero training cost but no task-specific optimization; performance ceiling limited by LLM capability

- **Failure signatures:**
  - RL policy generates malformed outputs (format reward = 0): Check prompt template enforcement
  - Reward plateaus early: Insufficient exploration or learning rate too low
  - Dense retriever performance degrades during RL training: Policy learning keyword-biased outputs; consider OPQE reformulation

- **First 3 experiments:**
  1. Replicate SPQE baseline with GPT-4o-mini on FEVER dataset using the provided prompt template (Figure 4) to establish a no-training reference point
  2. Train standard RL policy (DeepRetrieval style) on FEVER train set, logging reward curves to compare initialization points
  3. Train OPQE policy on same data using pseudo-document prompt template (Figure 5), comparing reward trajectory convergence speed against standard RL

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of OPQE's pseudo-document generation mechanism lacks direct ablation studies confirming the symmetric matching hypothesis
- The observed degradation of RL methods on dense retrievers may be specific to the reward function design rather than an inherent limitation of RL approaches
- The warm start initialization advantage of OPQE is demonstrated through reward trajectories but lacks quantitative analysis of convergence speed or sample efficiency

## Confidence
- Claim: Simple prompt-based methods match or exceed RL-based augmentation: High (strong experimental backing across multiple tasks)
- Claim: Pseudo-document generation improves dense retrieval through symmetric matching: Medium (supported by performance patterns but lacks direct mechanism verification)
- Claim: RL optimization degrades dense retrieval when optimized for sparse characteristics: High (clear degradation patterns observed)
- Claim: OPQE provides warm start initialization: Medium (reward trajectory visualizations show difference but lack quantitative comparison)

## Next Checks
1. **Mechanism verification**: Conduct an ablation study where dense retrievers are trained specifically on asymmetric query-document pairs rather than symmetric document pairs, then measure whether OPQE's advantage disappears, directly testing the symmetric matching hypothesis.

2. **Reward function exploration**: Design and test dense-retriever-specific reward functions (e.g., based on embedding similarity or contrastive loss) for standard RL query rewriting, to determine whether the observed degradation is inherent to RL or merely a consequence of using sparse-retriever-optimized rewards.

3. **Sample efficiency analysis**: Measure and compare the number of training samples and compute resources required for standard RL, OPQE, and prompt-based methods to reach various performance thresholds, providing quantitative evidence for the warm start and optimization power claims.