---
ver: rpa2
title: 'DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections'
arxiv_id: '2508.15851'
source_url: https://arxiv.org/abs/2508.15851
tags:
- question
- table
- document
- what
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocHop-QA, a large-scale benchmark for multi-hop
  question answering over multimodal, multi-document scientific corpora. Unlike prior
  datasets limited to single documents or unimodal text, DocHop-QA integrates text,
  tables, and structural layout features from PubMed articles without predefined hyperlinks.
---

# DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections

## Quick Facts
- arXiv ID: 2508.15851
- Source URL: https://arxiv.org/abs/2508.15851
- Authors: Jiwon Park; Seohyun Pyeon; Jinwoo Kim; Rina Carines Cabal; Yihao Ding; Soyeon Caren Han
- Reference count: 25
- Key outcome: Large-scale benchmark for multi-hop QA over multimodal, multi-document scientific corpora

## Executive Summary
DocHop-QA introduces a novel benchmark for multi-hop question answering over multimodal document collections, addressing the challenge of reasoning across text, tables, and layout features from multiple scientific articles. The dataset contains 11,379 QA pairs generated through an LLM-driven pipeline guided by 11 scientific reasoning concepts, specifically designed for PubMed biomedical articles. The benchmark evaluates both discriminative tasks (bounding box prediction, entity index extraction) and generative tasks (structured and free-form answer generation), revealing strong performance in layout-aware retrieval while highlighting challenges in multimodal alignment and long-context reasoning.

## Method Summary
The authors developed an LLM-driven pipeline to automatically generate multi-hop QA pairs from a corpus of 2M PubMed articles, filtered to 20K document pairs. The process involves concept-guided question generation using fine-tuned Qwen2-Instruct models, hybrid document pairing based on keyword overlap and TF-IDF similarity, and dual-pattern answer retrieval (fan-hop for semantic aggregation, chain-hop for logical chaining). The benchmark supports both discriminative tasks (bounding box index prediction, XML entity index extraction) and generative tasks (structured and free-form answer generation), with evaluation across 11 scientific reasoning concepts.

## Key Results
- Strong performance in layout-aware retrieval: F1 scores up to 12.35 for bounding box prediction
- Good structured generation: F1 scores up to 14.06 for XML entity index extraction
- Challenges in multimodal alignment: Adding visual inputs often degrades generative performance (Qwen Z drops from 14.06→7.92 with images)
- Long-context reasoning difficulties: Answer snippets average 3–4 sections apart, requiring models to maintain coherence across long spans

## Why This Works (Mechanism)

### Mechanism 1: Concept-Guided Question Generation Pipeline
Structured reasoning concepts enable scalable, diverse multi-hop QA generation without manual annotation. The 11 concepts (e.g., Problem-Solution-Limitation) provide semantic scaffolds for LLMs to instantiate with document-specific content, creating logically coherent multi-hop questions requiring cross-document synthesis. The approach assumes these concepts derived from 90 seed QA pairs sufficiently capture scientific reasoning patterns.

### Mechanism 2: Hybrid Similarity-Based Document Pairing
Combining keyword overlap with TF-IDF similarity identifies document pairs supporting meaningful multi-hop reasoning. YAKE extracts keywords → TF-IDF computes title/abstract similarity → Pairs selected if ≥1 keyword overlap AND TF-IDF cosine ≥0.3. The approach assumes semantically similar documents contain complementary information enabling multi-hop inference.

### Mechanism 3: Dual-Pattern Answer Retrieval (Fan-Hop vs. Chain-Hop)
Different reasoning patterns require distinct retrieval strategies: semantic aggregation (fan-hop) vs. logical chaining (chain-hop). Fan-hop uses weighted score (0.2·TF-IDF + 0.8·BERTScore) for top-5 snippets without explicit linking. Chain-hop requires keyword-filtered paragraphs → logically linked tables → referenced documents, requiring cross-document citation following.

## Foundational Learning

- **Multi-Hop Reasoning**: Combining information from 2+ sources to answer questions that can't be resolved by any single source.
  - Why needed here: DocHop-QA explicitly requires cross-document, cross-modal synthesis (e.g., "What proteins are involved in pathway X, and what experiments validate their roles?" requires methods + results + tables from multiple papers).
  - Quick check question: Given two documents—one describing a protein's function, another detailing experimental validation—can you trace the reasoning steps needed to answer "Is protein X a valid drug target?"

- **Visually Rich Document (VRD) Understanding**: Processing documents with complex layouts, tables, figures, and positional cues beyond plain text.
  - Why needed here: DocHop-QA uses PubMed articles with multi-column layouts, embedded tables, and structural hierarchy (sections/subsections).
  - Quick check question: How would you extract the relationship between a table in "Results" and its discussion in "Conclusion" when they span different pages?

- **Long-Context Processing**: Handling documents with 1,500–2,500 tokens, 10+ pages, and 20+ subsections.
  - Why needed here: Section distance analysis shows answer snippets average 3–4 sections apart, requiring models to maintain coherence across long spans.
  - Quick check question: A transformer with 512-token context window receives a 2,000-token document—what information is likely lost, and how does this affect multi-hop reasoning?

## Architecture Onboarding

- **Component map**: Document Corpus → Document Selector → Question Generator → Answer Matcher → QA Dataset
- **Critical path**: Document pairing (Algorithm 1) → Question generation (fine-tuned models + prompts) → Post-processing filters → Answer matching (pattern-specific retrieval) → Quality assurance (LLM + human evaluation)
- **Design tradeoffs**:
  - TF-IDF vs. dense embeddings: Chose TF-IDF for efficiency and robustness to domain terminology
  - Separate vs. unified question models: Separate models per type produce higher validity but increase training overhead
  - Deterministic vs. generative table questions: TS/TSL use deterministic caption insertion for controllability; PST uses LLM generation for flexibility
- **Failure signatures**:
  - Low F1 scores (12.35–23.24) on Tasks 1–2 indicate spatial grounding and long-context challenges
  - Adding images to generative tasks often *degrades* performance, suggesting modality alignment issues
  - One-shot prompting reduces performance on long documents due to token budget consumption
- **First 3 experiments**:
  1. **Baseline retrieval test**: Run TF-IDF-only vs. hybrid (TF-IDF+BERTScore) retrieval on 100 sampled Paragraph-Oriented QA pairs; measure precision@5 and semantic coherence manually.
  2. **Document pairing ablation**: Test different TF-IDF thresholds (0.2, 0.3, 0.4, 0.5) on document pair quality; evaluate whether higher thresholds improve or harm multi-hop reasoning depth.
  3. **Modality alignment probe**: Compare Qwen2.5-VL performance on text-only vs. image-only vs. text+image inputs for 50 Table-Oriented questions; identify where visual features help vs. hurt.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cross-modal fusion strategies be redesigned to prevent the performance degradation observed when visual inputs are added to generative QA tasks?
- Basis in paper: In the results for Task 3, the authors explicitly note that "Visual input degrades performance due to layout misalignment" and list "modality interaction challenges" as a key issue.
- Why unresolved: Current models like InternVL2 and Qwen2.5-VL appear to struggle with integrating text and layout features effectively, resulting in lower F1 scores when images are provided compared to text-only inputs.
- What evidence would resolve it: A model architecture or training objective that achieves higher F1 scores on the Structured Generative Answering task when using image inputs than when using text alone.

### Open Question 2
- Question: What specific pre-training tasks are required to improve the structured output formatting capabilities of vision-language models for index prediction?
- Basis in paper: The authors state that InternVL2 "struggles with consistent output formatting, underscoring the need for task-specific pre-training" in the Task 3 analysis.
- Why unresolved: Generic instruction tuning appears insufficient for the precise entity-index generation required by DocHop-QA, leading to poor performance on structured generative tasks.
- What evidence would resolve it: The application of a specific pre-training regime that significantly closes the performance gap between generative models (e.g., InternVL2) and discriminative baselines on the XML Entity Index Extraction task.

### Open Question 3
- Question: To what extent do current retrieval biases towards frequently occurring entity positions inhibit performance on long-context, multi-hop reasoning?
- Basis in paper: In the Task 2 results, the paper notes that "DocHop-QA exposes label bias" and that "predicted indices tend to skew to the more frequent entity indices."
- Why unresolved: It is unclear if the low F1 scores (approx. 23%) are due to the complexity of the multi-hop reasoning or the inability of models to overcome positional priors in long documents.
- What evidence would resolve it: An ablation study or model architecture that normalizes for entity position frequency, resulting in a more uniform distribution of correctly predicted indices across document sections.

## Limitations
- Domain Generalization Risk: The 11 question concepts derived from PubMed biomedical articles may not generalize to other scientific domains.
- Pairing Strategy Validity: Similarity-based document pairing doesn't guarantee reasoning-relevant connections; effectiveness lacks systematic evaluation.
- Chain-Hop Fragility: Performance depends on explicit table citations and may degrade when citations are incomplete or ambiguous.

## Confidence
- **High Confidence**: Dataset size and diversity, question generation pipeline reproducibility, document pairing methodology
- **Medium Confidence**: Concept taxonomy comprehensiveness, similarity-based pairing effectiveness, chain-hop vs. fan-hop logic
- **Low Confidence**: Multimodal integration benefits, long-context reasoning capability, model generalization beyond biomedical domain

## Next Checks
1. **Cross-Domain Concept Validation**: Apply the 11 question concepts to non-biomedical corpora (e.g., physics, computer science papers) and measure concept applicability rates and question validity scores.
2. **Document Pairing Ablation Study**: Systematically test different TF-IDF thresholds (0.1, 0.2, 0.4, 0.5) and alternative similarity metrics to quantify impact on multi-hop reasoning quality.
3. **Chain-Hop Robustness Test**: Evaluate chain-hop performance on documents with varying levels of citation completeness (fully cited, partially cited, uncited tables) to identify failure modes and inform retrieval strategy improvements.