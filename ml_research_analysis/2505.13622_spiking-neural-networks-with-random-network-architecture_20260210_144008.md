---
ver: rpa2
title: Spiking Neural Networks with Random Network Architecture
arxiv_id: '2505.13622'
source_url: https://arxiv.org/abs/2505.13622
tags:
- network
- should
- neural
- training
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RanSNN, a novel spiking neural network architecture
  that simplifies training by randomizing hidden layer weights while only training
  the output layer. This approach leverages the representational power of high-dimensional
  random projections, significantly reducing training complexity compared to traditional
  SNN training methods.
---

# Spiking Neural Networks with Random Network Architecture

## Quick Facts
- arXiv ID: 2505.13622
- Source URL: https://arxiv.org/abs/2505.13622
- Reference count: 40
- Primary result: RanSNN achieves >100x training speedup with comparable accuracy to surrogate gradient methods

## Executive Summary
This paper introduces RanSNN, a novel spiking neural network architecture that dramatically simplifies training by randomizing hidden layer weights while only training the output layer. The approach leverages the representational power of high-dimensional random projections, achieving comparable accuracy to traditional surrogate gradient methods while reducing training complexity by over 100x. The framework demonstrates robustness across different random weight distributions and hyperparameters, validated on benchmark tasks including MNIST, Fashion-MNIST, Kuzushiji-MNIST, and Extended MNIST.

## Method Summary
RanSNN is a spiking neural network where all hidden layer weights are randomly initialized and kept fixed during training, while only the final readout layer weights are learned. Input images are converted to Poisson spike trains and processed through LIF neurons with random weights. The spike outputs from the final hidden layer are accumulated over time steps to form a static feature vector, which is then linearly mapped to output classes. Training involves only updating the readout layer using cross-entropy loss, avoiding the complexity of computing gradients through non-differentiable spiking mechanisms.

## Key Results
- Training speedups of over 100x compared to surrogate gradient methods
- Comparable accuracy to surrogate gradient methods on MNIST, Fashion-MNIST, Kuzushiji-MNIST, and Extended MNIST
- Performance robust to different random weight distributions and hyperparameters
- Limitations emerge on more complex tasks where fixed random projections cannot capture necessary features

## Why This Works (Mechanism)

### Mechanism 1: Linearizing the SNN Training Problem through Readout-Only Training
By fixing hidden layer weights randomly and only training the readout layer, RanSNN avoids complex gradient computation through non-differentiable spike mechanisms. Random projections in high-dimensional spaces combined with spiking neuron dynamics create sufficient representational capacity for target tasks.

### Mechanism 2: Temporal Spike Accumulation for Decoding
Summing spike trains over time steps converts temporal spike representations into static vectors suitable for linear classification. This treats temporal integration as feature extraction rather than something to be learned.

### Mechanism 3: Balanced Random Weight Initialization for Effective Information Propagation
Random weights must be drawn from zero-mean distributions with sufficient variance to preserve information. Zero-mean ensures balanced excitatory/inhibitory inputs, while adequate variance enables effective information flow through the network.

## Foundational Learning

- **Concept: Spiking Neural Networks and LIF Neurons** - Understanding how LIF neurons integrate inputs over time and generate discrete spike events is essential for comprehending information propagation in RanSNN.
  - Quick check: Can you explain how a LIF neuron's membrane potential evolves over time, and what triggers it to emit a spike?

- **Concept: Random Projection Theory and Extreme Learning Machines** - Understanding why random projections preserve information in high-dimensional spaces explains why fixed random weights in hidden layers suffice for RanSNN.
  - Quick check: What property of high-dimensional random projections allows them to preserve distance relationships between data points?

- **Concept: Poisson Rate Coding** - Understanding how continuous input values transform into stochastic spike sequences is necessary for the input preprocessing step in RanSNN.
  - Quick check: How does Poisson rate encoding convert a pixel intensity value into a spike train?

## Architecture Onboarding

- **Component map:** Input Data (784) → Poisson Encoding → Random Weights (784→2000) → LIF Hidden Layer (2000 neurons) → Spike Accumulation → Trainable Readout (2000→10/62) → Output

- **Critical path:**
  1. Normalize pixel values → Generate Poisson spike trains
  2. For each timestep: update membrane potentials using LIF equations, generate spikes
  3. Accumulate spike counts across all timesteps
  4. Apply trainable linear mapping
  5. Compute cross-entropy loss
  6. Update only readout weights using Adam optimizer

- **Design tradeoffs:**
  - Hidden layer width vs. accuracy: Wider layers improve accuracy but increase cost
  - Training efficiency vs. task complexity: Fast training but may lose features in complex tasks
  - Leak rate β vs. spike generation: Higher β enables easier spike triggering
  - Time steps vs. performance: More timesteps don't guarantee better results

- **Failure signatures:**
  - Biased weight distributions cause degradation in performance
  - Insufficient weight variance prevents effective information propagation
  - Task complexity mismatch where readout cannot linearly separate features
  - Convolutional incompatibility where weights have specific structural meanings

- **First 3 experiments:**
  1. Baseline MNIST classification comparing accuracy and training time against surrogate gradient method
  2. Hyperparameter sensitivity across multiple datasets (vary β, hidden width, time steps)
  3. Random weight distribution robustness (uniform vs. normal, centered vs. biased)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can RanSNN maintain high performance on complex, non-image or deep-feature-reliant tasks without losing critical information during temporal summation? The current readout mechanism collapses temporal dimension into scalar count, potentially discarding temporal dynamics required for sophisticated reasoning.

- **Open Question 2:** Is it possible to adapt the random weight initialization strategy for architectures where connection weights have specific structural meanings, such as Convolutional SNNs? The random projection theory relies on high-dimensional dense connections, which may destroy spatial inductive bias necessary for translation invariance.

- **Open Question 3:** Why does increasing the number of time steps fail to steadily improve network performance, and how can decoding efficiency be optimized? The authors suggest a trade-off between retaining input features and processing effect, but the exact mechanism causing performance saturation is not analyzed.

## Limitations

- Limited to tasks where output layer training suffices, may lose features in complex scenarios
- Cannot handle convolutional architectures where weights have specific structural meanings
- Performance depends on representational capacity of fixed random projections

## Confidence

- **High confidence:** The claim of >100x training speedup is well-supported by experimental results
- **Medium confidence:** The claim of "comparable accuracy" is supported but lacks extensive statistical significance testing
- **Low confidence:** Generalizability to more complex tasks beyond benchmark datasets remains unproven

## Next Checks

1. Run multiple trials with different random seeds to establish confidence intervals for accuracy claims and verify robustness
2. Test RanSNN on more complex datasets (e.g., CIFAR-10) to validate limitations with task complexity
3. Systematically vary the number of time steps beyond 25 to verify performance plateau and identify optimal decoding windows