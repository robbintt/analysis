---
ver: rpa2
title: Selecting and Combining Large Language Models for Scalable Code Clone Detection
arxiv_id: '2510.15480'
source_url: https://arxiv.org/abs/2510.15480
tags:
- clone
- llms
- recall
- code
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates scalable code clone detection using Large
  Language Models (LLMs). The authors first identified and filtered 76 unique LLMs
  down to 9 suitable candidates for large-scale clone detection.
---

# Selecting and Combining Large Language Models for Scalable Code Clone Detection

## Quick Facts
- arXiv ID: 2510.15480
- Source URL: https://arxiv.org/abs/2510.15480
- Reference count: 40
- Key outcome: No single best-performing LLM for clone detection; model performance highly dataset-dependent

## Executive Summary
This paper investigates scalable code clone detection using Large Language Models (LLMs) by systematically evaluating 76 unique LLMs, narrowing them to 9 suitable candidates for large-scale detection. The authors analyze performance on public industrial datasets (BigCloneBench and Company datasets) and a private commercial dataset, finding that model effectiveness depends heavily on dataset characteristics. The study reveals that smaller embedding sizes and tokenizer vocabularies positively impact recall, and demonstrates that LLM ensembling improves precision on large-scale datasets but can degrade performance on smaller ones. CodeT5+110M achieves 39.71% precision on the commercial dataset, twice that of previously used CodeBERT.

## Method Summary
The study employs a Scalable Source Code Detection (SSCD) architecture that converts source code into fixed-length embeddings using pre-trained encoder LLMs, then performs approximate nearest neighbor (ANN) search to identify clone pairs. Code is parsed at the function level with a maximum sequence length of 128 tokens. The authors evaluate multiple models (CodeT5+110M, CuBERT, SPTCode) on BigCloneBench and Company datasets, measuring recall and precision. Ensembling is performed using normalization methods (min-max, z-score) and aggregation methods (sum, max, rrf). The pipeline includes parsing code into functions, generating embeddings via LLM inference, performing kANN search, and optionally combining results from multiple models using ensemble techniques.

## Key Results
- CodeT5+110M, CuBERT, and SPTCode were top performers across evaluated datasets
- Smaller embedding sizes (256) and tokenizer vocabularies (32k) correlated with improved recall
- Ensembling showed statistically significant improvements on large datasets (p=0.00004) but degraded performance on smaller datasets
- CodeT5+110M achieved 39.71% precision on the commercial dataset, twice that of CodeBERT
- Min-max and z-score normalization with sum/max aggregation proved most effective for ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing embedding dimensions and tokenizer vocabulary size correlates with improved recall in code clone detection, provided the model is sufficiently pre-trained on relevant code domains.
- Mechanism: Smaller embedding spaces reduce semantic dilution where distinct code concepts drift apart in high-dimensional noise. Smaller vocabularies force tokenizers to use more frequent sub-word units, likely reducing fragmentation of identifiers and increasing overlap in token sequences for functionally similar code snippets, which aids in learning structural patterns.
- Core assumption: The training data quality compensates for the reduced capacity of smaller embeddings/vocabularies.
- Evidence anchors: [abstract] "Analysis revealed that smaller embedding sizes, smaller tokenizer vocabularies... positively impacted recall"; [section] "Table 3... OLS regression analysis... Embedding Size: Coefficient = -6.41, p=0.013... Tokenizer Vocabulary Size: Coefficient = -8.12, p=0.014."

### Mechanism 2
- Claim: Ensembling LLMs improves clone detection precision on large-scale datasets (millions of LOC) but degrades performance on smaller datasets.
- Mechanism: In large search spaces, individual models produce high false positive rates; intersecting or aggregating ranked lists acts as a voting mechanism that filters out noise while reinforcing signal. In small datasets, the candidate lists are short and dense, providing insufficient diversity for voting to help, making the ensemble susceptible to the weaker model's errors.
- Core assumption: The constituent models have heterogeneous error profiles (i.e., they make different mistakes).
- Evidence anchors: [abstract] "While ensembling showed mixed results on smaller datasets, it provided statistically significant improvements on larger datasets"; [section] "Table 16... For Company-C/C++ [small], ensembling is significantly worse... for BCB13 [large], ensembles consistently outperform... p=0.00004."

### Mechanism 3
- Claim: Approximate Nearest Neighbor (ANN) search on fixed-length code embeddings preserves detection effectiveness while enabling sub-quadratic scalability.
- Mechanism: By converting code into vector representations via the LLM's encoder, the comparison of code pairs becomes a geometric distance problem. Using ANN indices reduces the retrieval complexity from O(NÂ²) to approximately O(N log N), making million-line codebase analysis feasible.
- Core assumption: The geometric distance in the embedding space reliably maps to semantic similarity (functional equivalence).
- Evidence anchors: [section] "Section 2.3... generated code embeddings are compared to each other using k approximate nearest neighbor (kANN) algorithms... efficiently"; [abstract] "CodeT5+110M achieved 39.71% precision... on the commercial large-scale dataset."

## Foundational Learning

- **Concept: Code Embeddings (Vectorization)**
  - Why needed here: The entire SSCD architecture relies on converting source code into numerical vectors. Understanding that the LLM's job is to map "semantically similar" code to geometrically close vectors is prerequisite to understanding why distance-based search works for clone detection.
  - Quick check question: If two functions have identical functionality but different variable names, would you expect their embeddings to have a low or high cosine distance?

- **Concept: Tokenization and Vocabulary**
  - Why needed here: The paper identifies vocabulary size as a critical predictor of recall. You must understand how a tokenizer breaks code into chunks (tokens) to see why a larger vocabulary might fragment code patterns while a smaller one might generalize better.
  - Quick check question: If a tokenizer has a vocabulary size of 50,000 vs 32,000, which one is more likely to treat `compute_fast` and `compute_slow` as completely distinct tokens rather than sharing a root `compute`?

- **Concept: Reciprocal Rank Fusion (RRF)**
  - Why needed here: This is a specific normalization method highlighted as effective for ensembling. Unlike averaging scores, RRF uses the rank of results.
  - Quick check question: If Model A ranks a clone pair #1 and Model B ranks it #100, does RRF give more weight to the #1 rank compared to a simple average of the raw scores?

## Architecture Onboarding

- **Component map:** Parser -> Inference Engine -> Indexer -> Aggregator
- **Critical path:** Embedding Generation (most resource-intensive) -> Top-K Search (setting global top K and similarity threshold)
- **Design tradeoffs:** Recall vs. Precision (higher global top K increases recall but more false positives); Ensemble Cost (running 2 models doubles inference time/memory, only pays off for large datasets >1M LOC); Embedding Size (larger embeddings capture more nuance but may hurt recall compared to smaller ones for this specific task)
- **Failure signatures:** "Non-norm/Average" Failure (using raw similarity scores and averaging results in significantly lower precision); Small Dataset Ensemble Drop (applying ensembles to repositories <100k LOC yields worse results than single best model); Type-4 Blindness (if LLM hasn't been fine-tuned for semantic similarity, expect near-zero recall for functional clones)
- **First 3 experiments:**
  1. Baseline Retrieval: Run CodeT5+110M on a sample (e.g., 10k functions) and plot Precision-Recall curves at different similarity thresholds to calibrate the cutoff.
  2. Vocabulary Impact: Compare tokenization output of high-vocab (Roberta_50k) vs low-vocab (Roberta_32k) models on obfuscated clones to verify the paper's claim about fragmentation.
  3. Ensemble Validation: Combine best model with structurally different one (Encoder-Decoder + Encoder-Only) on large dataset, testing z-score_sum vs non-norm_average to quantify negative synergy risk.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the number of models in an ensemble beyond two affect clone detection performance and resource consumption?
- Basis: [explicit] The authors limited their study to pairwise ensembles and list "Ensembling More Than Two Models" as a specific avenue for future work to understand how effects change.
- Why unresolved: The complexity and computational cost of testing larger combinations were avoided in this study, leaving the potential for "synergy" in larger groups unknown.

### Open Question 2
- Question: Do the observed negative correlations between LLM characteristics (embedding size, vocabulary) and recall persist in languages other than C/C++ and Java?
- Basis: [explicit] The authors explicitly state that future work should assess whether "observed patterns... persist in broader, multilingual contexts" like Python or JavaScript.
- Why unresolved: The regression analysis was based solely on C/C++ and Java datasets; different syntactic structures in other languages may alter the importance of tokenization or context length.

### Open Question 3
- Question: Can dynamic weighting schemes (e.g., Learning to Rank) outperform the static aggregation methods (sum/max) identified as effective in this study?
- Basis: [explicit] The paper concludes that future work could "adapt advanced ranking-fusion techniques or dynamic weighting schemes" to optimize performance on heterogeneous datasets.
- Why unresolved: The current study only tested static normalization and aggregation; it is unknown if a meta-model could learn to weigh specific LLMs better for specific clone types.

### Open Question 4
- Question: Is the significant negative correlation between tokenizer vocabulary size and recall a generalizable trend or an artifact of the small sample size?
- Basis: [inferred] While the regression analysis showed a negative coefficient for vocabulary size, the authors noted the sample size was small (27 samples) and the finding was counter-intuitive, suggesting it needs buttressing.
- Why unresolved: The regression model used only 9 models across 3 datasets, which risks overfitting and may not capture the true global relationship between model size and performance.

## Limitations

- The study focuses on static code analysis using pre-trained embeddings, potentially missing dynamic or behavioral clone patterns that only manifest during execution
- Relies heavily on function-level granularity, potentially missing clones that span multiple functions or files
- Commercial dataset evaluation lacks public availability for independent verification
- Does not explore cross-language clone detection capabilities or examine how model performance varies across different programming paradigms

## Confidence

**High Confidence:** The empirical finding that model performance varies significantly across datasets and that no single best-performing LLM exists across all scenarios. The observation that CodeT5+110M, CuBERT, and SPTCode consistently perform well is supported by direct quantitative evidence.

**Medium Confidence:** The mechanism linking smaller embedding sizes and tokenizer vocabularies to improved recall, while supported by regression analysis (p-values of 0.013 and 0.014), requires further validation across diverse codebases to confirm generalizability beyond the studied datasets.

**Medium Confidence:** The ensemble effectiveness claims show statistically significant improvements on large datasets (p=0.00004 for BCB13) but mixed results on smaller datasets. The confidence is medium because the size-dependent performance inversion needs additional validation with more diverse dataset sizes.

## Next Checks

1. **Cross-Paradigm Validation:** Test the top-performing models (CodeT5+110M, CuBERT, SPTCode) on datasets spanning different programming paradigms (object-oriented, functional, procedural) to verify the robustness of the observed performance patterns across language families.

2. **Dynamic vs Static Comparison:** Implement a hybrid approach that combines static embeddings with dynamic execution traces to assess whether the integration improves Type-4 (semantic) clone detection recall, particularly for cases where static analysis alone fails.

3. **Ensemble Diversity Analysis:** Systematically measure the correlation between individual model predictions across different architectural families to quantify whether the observed ensemble benefits stem from true heterogeneous error profiles or other factors like dataset-specific performance variations.