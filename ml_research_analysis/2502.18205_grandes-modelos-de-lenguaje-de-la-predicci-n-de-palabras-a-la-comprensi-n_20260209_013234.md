---
ver: rpa2
title: "Grandes modelos de lenguaje: de la predicci\xF3n de palabras a la comprensi\xF3\
  n?"
arxiv_id: '2502.18205'
source_url: https://arxiv.org/abs/2502.18205
tags:
- para
- como
- modelos
- lenguaje
- palabras
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines large language models (LLMs) by tracing their
  historical evolution, from rule-based systems to statistical methods, and finally
  to neural models. It explains the technical foundations, including dense word embeddings,
  transformer architectures, and massive datasets, and illustrates how LLMs generate
  text via probabilistic prediction.
---

# Grandes modelos de lenguaje: de la predicción de palabras a la comprensión?

## Quick Facts
- arXiv ID: 2502.18205
- Source URL: https://arxiv.org/abs/2502.18205
- Authors: Carlos Gómez-Rodríguez
- Reference count: 0
- Primary result: Examines LLM evolution from rule-based to neural models, explains technical foundations, and discusses capabilities, limitations, and the debate around true language understanding.

## Executive Summary
This paper traces the historical evolution of language models from rule-based systems to statistical methods and neural architectures. It explains the technical foundations of modern LLMs including transformer models, dense word embeddings, and massive training datasets. The paper illustrates how LLMs generate text via probabilistic prediction and highlights the emergence of sophisticated capabilities like factual accuracy and task versatility through model scaling. It concludes by discussing inherent limitations such as hallucinations and model opacity, emphasizing that while LLMs offer significant utility, their limitations around factual reliability and transparency must be carefully managed.

## Method Summary
The paper demonstrates LLM concepts through experiments using Spanish Bible text for Markov chain generation and comparing outputs from GPT-2 Spanish and BLOOM models on factual queries. The methodology involves building k-gram frequency tables for Markov chains with k=1,2,3,10, and generating text continuations using both models for prompts about the Torre de Hércules. The paper compares factual accuracy between smaller and larger models while illustrating how sampling parameters affect output quality.

## Key Results
- LLMs generate text by predicting continuations probabilistically from training distributions using neural networks that handle large context sizes
- Capabilities such as factual accuracy and question-answering emerge spontaneously from model scaling rather than explicit training objectives
- Instruction tuning shapes model behavior to align with human preferences but doesn't add new capabilities beyond those already present in the base model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate text by predicting continuations probabilistically from training distributions
- Mechanism: Given a context of k tokens, the model estimates a probability distribution over possible next tokens, then samples from this distribution. This extends Markov chain principles but uses neural networks to handle much larger k values without data sparsity.
- Core assumption: Patterns in training text are sufficiently representative of desired outputs for a given prompt.
- Evidence anchors:
  - [section 3.1] "una cadena de Markov de orden k es un modelo que va atravesando una serie de estados, de tal modo que la probabilidad de cada nuevo estado depende de los k estados anteriores"
  - [section 3.2] "los modelos de lenguaje neuronales nos permiten usar tamaños de contexto de cientos de palabras sin que aparezca el problema de la escasez de datos"
  - [corpus] No direct corpus corroboration for this specific mechanistic claim.
- Break condition: If your task requires outputs not statistically represented in training data (e.g., novel reasoning chains, recent facts), prediction quality degrades or hallucinates.

### Mechanism 2
- Claim: Capabilities such as factual accuracy and question-answering emerge spontaneously from model scaling
- Mechanism: As model size and training data increase, the model develops better internal representations that implicitly encode world knowledge and task patterns, even though training only optimizes for next-token prediction. This was unexpected by researchers.
- Core assumption: Larger models can learn more complex implicit representations that generalize beyond mere pattern matching.
- Evidence anchors:
  - [section 3.3] "lo realmente sorprendente... es que simplemente escalando el modelo... se consigue que empiece a decir la verdad"
  - [section 4] "se trata de habilidades emergentes: capacidades que surgen de manera aparentemente espontánea cuando el sistema alcanza una determinada escala"
  - [corpus] Weak corpus evidence; related papers discuss LLM capabilities but not the emergence mechanism directly.
- Break condition: If emergent capabilities are actually measurement artifacts (as some research suggests), scaling alone may not reliably produce desired behaviors.

### Mechanism 3
- Claim: Instruction tuning shapes model behavior without adding new capabilities
- Mechanism: Supervised fine-tuning on instruction-response pairs and reinforcement learning from human feedback (RLHF) bias the model's output distribution toward responses humans prefer, making capabilities more consistent rather than creating them.
- Core assumption: Human evaluators can reliably identify desirable outputs, and their preferences generalize.
- Evidence anchors:
  - [section 3.3] "el ajuste de instrucciones... no sirve para añadir capacidades... solo hace que esa capacidad se aproveche de forma más consistente"
  - [section 4] "el ajuste de instrucciones sirve para dar más consistencia a los modelos y hacerlos más previsibles"
  - [corpus] Corpus neighbor on legal AI discusses RAG optimization for hallucination reduction, indirectly supporting that post-hoc methods mitigate but don't eliminate core limitations.
- Break condition: If alignment techniques conflict with capability expression, models may refuse valid requests or become inconsistent.

## Foundational Learning

- Concept: Probability distributions and sampling
  - Why needed here: Understanding that LLMs don't "choose" words deterministically but sample from learned distributions is essential for interpreting outputs and debugging unexpected generations.
  - Quick check question: If a model assigns probability 0.7 to token A and 0.3 to token B, what happens if you always select the highest-probability token?

- Concept: Word embeddings as vector representations
  - Why needed here: The paper emphasizes that dense vector representations enable flexibility by encoding semantic similarity, unlike discrete token matching in Markov chains.
  - Quick check question: Why would "avión" and "aeroplano" have similar embeddings, and why does this matter for generalization?

- Concept: The distinction between training objective and emergent behavior
  - Why needed here: LLMs are trained only to predict next tokens, yet exhibit behaviors (reasoning, translation) not explicitly trained—this gap is central to understanding both capabilities and limitations.
  - Quick check question: If an LLM was never trained to translate, why might it still perform translation reasonably well?

## Architecture Onboarding

- Component map:
  - Tokenizer -> Embedding layer -> Transformer backbone -> Output head -> Sampling strategy

- Critical path:
  1. Pre-training on massive text corpora (terabytes, trillions of tokens) to learn next-token prediction
  2. Instruction tuning (supervised + RLHF) to align output preferences with human intent
  3. Deployment with sampling parameters (temperature, top-k, top-p) that affect output diversity

- Design tradeoffs:
  - Larger context k improves coherence but increases compute cost quadratically for standard attention
  - Higher temperature increases creativity but reduces factual reliability
  - RLHF improves alignment but may introduce evaluator biases or reduce capability expressiveness

- Failure signatures:
  - Hallucinations: Plausible-sounding but false outputs—inherent to probabilistic generation, not a bug
  - Refusal loops: Over-aligned models that reject benign requests
  - Context exhaustion: Long inputs that exceed context window, causing truncation or quality degradation
  - Repetition: Sampling parameters that cause the model to get stuck in loops

- First 3 experiments:
  1. Generate multiple completions for the same prompt at different temperature settings (e.g., 0.0, 0.7, 1.2) to observe how sampling affects diversity vs. coherence.
  2. Test factual accuracy by prompting for information about obscure topics, then comparing against ground truth to estimate hallucination rates.
  3. Compare raw model outputs (before instruction tuning, if accessible) against the instruction-tuned version to verify the paper's claim that tuning improves consistency but not capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs genuinely understand language in some meaningful sense, or do they merely simulate coherent responses through statistical pattern matching?
- Basis in paper: [explicit] The title poses this question directly, and Section 4 extensively discusses the debate between the "stochastic parrots" view (Bender et al., 2021) and evidence suggesting world modeling capabilities (Li et al., 2023; Patel & Pavlick, 2022).
- Why unresolved: Competing evidence exists—the Othello experiment shows emergent world representations, yet the fundamental architecture remains probabilistic text prediction without intentionality or consciousness. No consensus definition of "understanding" exists.
- What evidence would resolve it: Agreed-upon criteria for understanding in artificial systems; conclusive evidence of internal world models or their absence; or theoretical frameworks distinguishing genuine comprehension from sophisticated pattern matching.

### Open Question 2
- Question: What mechanisms cause emergent abilities (like factual accuracy and task versatility) to appear spontaneously when models scale beyond certain thresholds?
- Basis in paper: [explicit] The paper states: "¿por qué y cómo estos modelos las desarrollan? Lo cierto es que, a día de hoy, no lo sabemos en absoluto" (Why and how do these models develop them? Today, we don't know at all).
- Why unresolved: These abilities were unexpected discoveries, not designed outcomes. While research attempts explanations (Schaeffer et al., 2024; Du et al., 2024), a complete theoretical account remains elusive.
- What evidence would resolve it: Predictive theories specifying which abilities emerge at what scales; mechanistic interpretability revealing internal computational processes; or systematic characterization of the scaling phenomena.

### Open Question 3
- Question: Are hallucinations an eliminable problem or an intrinsic, unavoidable limitation of the LLM paradigm?
- Basis in paper: [explicit] The paper cites Xu et al. (2024) and Banerjee et al. (2024) describing hallucinations as "innate," "inevitable," and permanent, while acknowledging ongoing mitigation efforts through RAG and instruction tuning.
- Why unresolved: Hallucinations derive from the core training objective—generating plausible continuations rather than verified truths. Mitigation techniques reduce but cannot guarantee elimination.
- What evidence would resolve it: Either demonstration of architectures achieving zero hallucination rates on factual queries, or theoretical proof that perfect factual accuracy is incompatible with probabilistic language modeling.

## Limitations

- The paper's claims about emergent capabilities from scaling rely heavily on theoretical arguments without systematic empirical validation across diverse tasks and domains.
- The discussion of hallucination and factual reliability is well-grounded but lacks quantitative error bounds or systematic evaluation protocols to measure these limitations.
- The mechanistic explanation for how implicit representations form during scaling remains somewhat hand-wavy, with the paper acknowledging that researchers don't fully understand why emergent abilities appear.

## Confidence

- **High confidence**: Claims about technical foundations (transformer architecture, sampling mechanisms, the distinction between pre-training and instruction tuning) are well-supported by established literature and clear mechanistic explanations.
- **Medium confidence**: Claims about emergent capabilities from scaling are supported by the paper's arguments and some evidence, but remain controversial in the broader research community with alternative interpretations.
- **Medium confidence**: Claims about limitations like hallucination and factual unreliability are accurate, but the paper doesn't provide quantitative bounds or systematic evaluation frameworks for these limitations.

## Next Checks

1. **Test scaling limits empirically**: Generate text continuations using the same prompt across multiple model sizes (small GPT-2, BLOOM variants) and quantify factual accuracy differences to verify the emergence claims. Document when and how factual accuracy breaks down.

2. **Systematic hallucination evaluation**: Design a test suite of factual claims (mixing true and false statements) to measure hallucination rates across different model sizes and sampling parameters. Compare against ground truth to establish error bounds.

3. **Sampling parameter sensitivity analysis**: Systematically vary temperature, top-k, and top-p parameters for the same model and prompt to quantify the tradeoff between output diversity and factual reliability. Document the point at which quality degradation becomes unacceptable.