---
ver: rpa2
title: High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized
  Model Predictive Control
arxiv_id: '2512.11705'
source_url: https://arxiv.org/abs/2512.11705
tags:
- bayesian
- cost
- closed-loop
- neural
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of optimizing dense high-dimensional\
  \ neural network\u2013parameterized model predictive control (MPC) cost functions,\
  \ where conventional Gaussian process surrogates degrade in performance as dimensionality\
  \ grows. The core method idea is to use Bayesian neural networks (BNNs) and infinite-width\
  \ BNNs as surrogate models within Bayesian optimization, replacing the standard\
  \ Mat\xE9rn-kernel Gaussian process baseline."
---

# High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control

## Quick Facts
- arXiv ID: 2512.11705
- Source URL: https://arxiv.org/abs/2512.11705
- Reference count: 4
- One-line primary result: BNN-based surrogates, especially infinite-width BNNs, outperform Matérn GP surrogates in dense high-dimensional MPC parameter spaces, maintaining strong performance even beyond 1000 parameters.

## Executive Summary
This work addresses the challenge of optimizing dense high-dimensional neural network–parameterized model predictive control (MPC) cost functions, where conventional Gaussian process surrogates degrade in performance as dimensionality grows. The core method idea is to use Bayesian neural networks (BNNs) and infinite-width BNNs as surrogate models within Bayesian optimization, replacing the standard Matérn-kernel Gaussian process baseline. Through simulation on a cart-pole system, the results show that BNN-based surrogates—especially infinite-width BNNs—achieve faster and more reliable convergence of the closed-loop cost than Matérn GPs as the number of parameters increases. Infinite-width BNNs maintain strong performance even beyond 1000 parameters, while Matérn GPs lose effectiveness and approach random-search behavior in high dimensions.

## Method Summary
The paper presents a Bayesian optimization framework for learning neural-network-parameterized MPC cost functions. The method replaces traditional Matérn-kernel Gaussian process surrogates with Bayesian neural networks (BNNs) and infinite-width BNNs to handle dense high-dimensional parameter spaces. The closed-loop evaluation uses a linearized cart-pole system with parameterized NN-augmented stage costs, evaluated over 80 time steps with a horizon of 10. Three surrogate models are compared: Matérn 5/2 GP, finite-width BNN with MCMC inference, and infinite-width BNN via analytical NNGP kernel. The acquisition function is logarithmic Expected Improvement, and experiments run for 50 iterations across 21 independent seeds per configuration.

## Key Results
- BNN-based surrogates achieve faster and more reliable convergence of closed-loop cost than Matérn GPs as parameter space dimensionality increases
- Infinite-width BNNs maintain strong performance even beyond 1000 parameters, while Matérn GPs approach random-search behavior in high dimensions
- Finite-width BNNs perform well up to ~500 parameters but become computationally prohibitive at higher dimensions due to MCMC inference costs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BNN-based surrogates scale more favorably than Matérn-kernel GPs in dense high-dimensional parameter spaces.
- **Mechanism:** The computational cost of BNN inference scales primarily with the number of network parameters rather than cubically with the number of data points, as is the case for GPs.
- **Core assumption:** The neural network architecture induces an inductive bias that better captures the structure of dense high-dimensional MPC cost functions compared to generic Matérn kernels.
- **Evidence anchors:** [abstract] "BNN-based surrogates—especially infinite-width BNNs—achieve faster and more reliable convergence... while Matérn GPs lose effectiveness and approach random-search behavior in high dimensions"

### Mechanism 2
- **Claim:** Infinite-width BNNs (I-BNNs) combine the analytical tractability of GPs with architecture-specific kernel structure.
- **Mechanism:** As the number of hidden units per layer → ∞, the prior over network functions converges to a GP. The resulting kernel is determined recursively by network depth, activation functions, and prior variances, yielding a GP that captures compositional structure absent from generic Matérn kernels.
- **Core assumption:** The I-BNN kernel's compositional structure provides better coverage of the true cost function geometry in dense parameter spaces.
- **Evidence anchors:** [Section 2.2.3] "In this limit, the prior over functions induced by the network converges to a Gaussian process... the kernel k_IBNN is determined by the network architecture, activation functions, prior variances σ²_w and σ²_b, and depth L"

### Mechanism 3
- **Claim:** Dense high-dimensional MPC parameterizations lack the low-dimensional latent structure that standard high-dimensional BO methods exploit.
- **Mechanism:** Neural-network-parameterized cost functions produce parameter spaces where nearly every parameter influences closed-loop behavior significantly. Methods relying on additive decompositions, random embeddings, or latent representations fail here because no sparsity exists to exploit.
- **Core assumption:** The NN-parameterized cost induces a genuinely dense parameter-response mapping without dominant low-dimensional subspaces.
- **Evidence anchors:** [Section 1] "In contrast to common high-dimensional Bayesian optimization benchmarks, these problems lack latent low-dimensional structure or sparsity"

## Foundational Learning

- **Concept: Gaussian Process Regression**
  - **Why needed here:** GPs are the standard surrogate baseline. Understanding mean/variance predictions, kernel selection, and marginal likelihood is essential to grasp why Matérn kernels degrade in high dimensions.
  - **Quick check question:** Given a GP posterior, how do the predictive mean and variance change as you move away from observed data points?

- **Concept: Bayesian Neural Network Inference**
  - **Why needed here:** Finite-width BNNs require approximate posterior inference (MCMC, variational, Laplace). Understanding the trade-offs between inference quality and computational cost explains their limited scalability beyond ~500 parameters.
  - **Quick check question:** Why does placing a prior over network weights induce epistemic uncertainty in predictions, and how is this typically approximated?

- **Concept: Model Predictive Control with Parameterized Costs**
  - **Why needed here:** The closed-loop cost G(θ) depends implicitly on MPC parameters through system dynamics. Understanding the feedback loop between cost design and closed-loop behavior is prerequisite to framing the optimization problem.
  - **Quick check question:** If you change the stage cost weights in MPC, how does this affect the resulting control policy and closed-loop trajectory?

## Architecture Onboarding

- **Component map:** Cart-pole dynamics -> MPC controller with NN-parameterized cost -> Closed-loop cost evaluation -> Surrogate model (GP/BNN/I-BNN) -> Acquisition function (log-EI) -> Parameter selection -> Repeat

- **Critical path:** Surrogate model expressiveness -> acquisition function quality -> parameter selection efficiency -> closed-loop cost convergence

- **Design tradeoffs:**
  - **Matérn GP:** Cheap kernel evaluations, simple hyperparameters; fails beyond a few hundred dimensions
  - **Finite-width BNN:** Strong performance up to ~500 dimensions; MCMC becomes prohibitive at scale
  - **I-BNN:** Best scaling beyond 1000 dimensions; requires kernel computation via recursive covariance propagation

- **Failure signatures:**
  - BO best-cost curve flattens early and matches random search baseline (Matérn GP at high dim)
  - Per-iteration time grows unacceptable (finite-width BNN with MCMC at high dim)
  - Surrogate uncertainty estimates become uninformative (kernel mismatch)

- **First 3 experiments:**
  1. **Low-dimensional validation (nθ ≈ 50–100):** Compare Matérn GP, finite BNN, and I-BNN on simple quadratic cost; verify all surrogates outperform random search.
  2. **Mid-range scaling (nθ ≈ 300–600):** Introduce NN-augmented cost with 2 hidden layers, 10–20 neurons; observe where Matérn GP begins to degrade relative to BNN variants.
  3. **High-dimensional stress test (nθ > 1000):** Use 2 hidden layers, 30+ neurons; compare only I-BNN vs. Matérn GP vs. random; confirm I-BNN retains performance advantage.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do infinite-width BNN surrogates maintain their performance advantage on more complex or highly nonlinear dynamical systems beyond the cart-pole benchmark?
- **Open Question 2:** Can I-BNN surrogates be integrated with stability and safety constraints during closed-loop learning?
- **Open Question 3:** How does the computational cost of I-BNN surrogates scale compared to Matérn GPs at very high dimensions with large datasets?
- **Open Question 4:** Do the findings transfer to real-world experimental setups with noise, model-plant mismatch, and hardware constraints?

## Limitations

- Results are confined to a single cart-pole benchmark, limiting generalizability to more complex or nonlinear systems
- No comparison against alternative high-dimensional BO strategies (e.g., additive models, random embeddings) to isolate surrogate-specific benefits
- Computational scaling of I-BNN surrogates at very high dimensions with large datasets remains unexplored

## Confidence

- **Mechanism 1 (BNN scaling):** High – supported by theory and experimental evidence
- **Mechanism 2 (I-BNN kernel expressiveness):** Medium – theoretically sound, but lack of corpus comparison introduces uncertainty
- **Mechanism 3 (dense parameterization):** Medium – justified in the paper, but untested against alternatives

## Next Checks

1. Test on a nonlinear or higher-dimensional dynamical system (e.g., double pendulum or quadrotor) to assess scalability and robustness
2. Compare I-BNN against at least one other high-dimensional BO method (e.g., additive GP or random embedding) to isolate surrogate-specific benefits
3. Perform a sensitivity analysis of acquisition function choice (e.g., PI, UCB) to determine robustness of the observed gains