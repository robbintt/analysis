---
ver: rpa2
title: Gradient Descent with Provably Tuned Learning-rate Schedules
arxiv_id: '2512.05084'
source_url: https://arxiv.org/abs/2512.05084
tags:
- most
- degree
- learning
- functions
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how to tune hyperparameters in gradient-based\
  \ optimization, especially in multi-task settings where optimal parameters vary\
  \ across tasks. The main technical contribution is showing that for a broad class\
  \ of non-convex, non-smooth functions\u2014including piecewise-polynomial and piecewise-Pfaffian\
  \ functions\u2014the dual cost function (number of steps to convergence as a function\
  \ of step-size) has only a logarithmic number of pieces in the function complexity."
---

# Gradient Descent with Provably Tuned Learning-rate Schedules

## Quick Facts
- arXiv ID: 2512.05084
- Source URL: https://arxiv.org/abs/2512.05084
- Authors: Dravyansh Sharma
- Reference count: 28
- Key outcome: Shows that for non-convex, non-smooth functions (piecewise-polynomial and piecewise-Pfaffian), the dual cost function has only logarithmic pieces in function complexity, yielding sample complexity bounds for tuning hyperparameters that match convex function bounds up to logarithmic factors.

## Executive Summary
This paper establishes sample complexity bounds for tuning hyperparameters in gradient-based optimization, particularly learning rates, when training across multiple tasks. The key insight is that for a broad class of non-convex, non-smooth functions—including neural networks with ReLU, sigmoid, and tanh activations—the dual cost function (iterations to convergence as a function of step-size) has only polynomially many pieces relative to function complexity. This enables rigorous sample complexity bounds for learning optimal hyperparameters via empirical risk minimization. The results extend to tuning multiple hyperparameters simultaneously, including learning rate schedules, initialization scales, and momentum parameters.

## Method Summary
The paper analyzes gradient descent as a learning problem where the goal is to select hyperparameters (primarily step-size η) that minimize the number of iterations to convergence across tasks drawn from a distribution. The core technique is to show that for piecewise-polynomial and piecewise-Pfaffian functions, the dual cost function ℓ(η, x, f) (number of iterations to converge) has bounded pseudo-dimension. This is achieved by proving that each iterate x_i is expressible as a piecewise-polynomial/Pfaffian function of η with controlled degree and piece count, using an amortized counting argument to prevent exponential blowup. Classical learning theory then converts the pseudo-dimension bound into sample complexity guarantees. The analysis extends to momentum-based optimization and validation loss optimization.

## Key Results
- For piecewise-polynomial functions with degree ≤ ∆ and p polynomial boundaries, the dual cost function has O(∆^H) pieces, yielding pseudo-dimension O(H log ∆)
- Sample complexity for learning optimal step-size is Õ(H³/ε²) for (ε, δ)-uniform convergence, matching convex function bounds up to logarithmic factors
- Extension to Pfaffian functions (sigmoid, tanh) preserves bounded sample complexity with pseudo-dimension O(q²d²H² + qdH log(∆+M))
- Learning rate schedules requires Õ(H⁴/ε²) samples (Theorem 4.1), while tuning multiple hyperparameters (step-size and momentum) scales with γ^(1/2) dependence (Theorem 4.4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual cost function has only O(∆^H) pieces for piecewise-polynomial functions, enabling bounded pseudo-dimension.
- Mechanism: Each GD iterate x_i is expressible as a piecewise-polynomial function of η. Since ||∇f(x_i)||² is also piecewise-polynomial with bounded degree and pieces, the convergence condition creates finitely many intervals where cost is constant. An amortized counting argument prevents exponential blowup across H iterations.
- Core assumption: The function f is piecewise-polynomial with degree ≤ ∆ and p polynomial boundaries; gradient descent never lands exactly on boundaries.
- Evidence anchors:
  - [section 3.1, Theorem 3.1] "for any fixed 1≤i≤H, xi = gi(η) where each coordinate g(j)_i is a polynomial with degree at most ∆^{i−2}"
  - [section 3.1] "using Lemma 2.2, since each function ℓ_{x,f} is O(∆^H)-monotonic, we get a bound on the pseudo-dimension of L of O(H log ∆)"
- Break condition: Functions with unbounded oscillations or non-piecewise structure (e.g., sine/cosine activations per Remark 1) violate the piecewise-polynomial/Pfaffian assumption.

### Mechanism 2
- Claim: Pseudo-dimension O(H log ∆) implies Õ(H³/ε²) sample complexity for uniform convergence when tuning step-size.
- Mechanism: Classical learning theory (Theorem 2.1) connects pseudo-dimension to (ε,δ)-uniform convergence. Since Pdim(L) = O(H log ∆) for polynomial functions, the sample complexity follows from the pseudo-dimension bound via Lemma 2.2.
- Core assumption: The loss function class has range in [0,H] and finite pseudo-dimension.
- Evidence anchors:
  - [section 2, Theorem 2.1] "M = O((H/ε)² (Pdim(F) + log(1/δ)))"
  - [section 3.1] Sample complexity Õ(H³/ε²) "asymptotically matching the previous bounds for convex and smooth functions"
- Break condition: Infinite pseudo-dimension would break the uniform convergence argument; occurs if dual cost function has infinitely many pieces.

### Mechanism 3
- Claim: Extension to Pfaffian functions (sigmoid, tanh activations) preserves bounded sample complexity with polynomial dependence on chain length q and dimension d.
- Mechanism: GD iterates remain Pfaffian functions under Pfaffian objectives. Khovanskii's theorem bounds connected components of Pfaffian constraint sets, yielding 2^{O(q²d²H²)} intervals—still finite, giving Pdim(L) = O(q²d²H² + qdH log(∆+M)).
- Core assumption: f is Pfaffian with chain length ≤ q, degree ≤ ∆, and Pfaffian degree ≤ M.
- Evidence anchors:
  - [section 3.2, Theorem 3.3] "m = O((H²/ε²)(q²d²H² + qdH log(∆+M) + log(1/δ)))"
  - [section 3.2, Definition 2-3] Pfaffian chain definitions enabling the inductive construction
- Break condition: Non-Pfaffian activations (periodic trigonometric functions) violate the Khovanskii bound assumptions.

## Foundational Learning

- Concept: **Pseudo-dimension**
  - Why needed here: Primary complexity measure for uniform convergence over real-valued function classes; determines sample complexity of hyperparameter tuning.
  - Quick check question: Can you explain why pseudo-dimension (not VC-dimension) is needed for real-valued loss functions?

- Concept: **Pfaffian functions and chains**
  - Why needed here: Enables theoretical guarantees for networks with sigmoid/tanh activations beyond piecewise-polynomial ReLU cases.
  - Quick check question: What property of Pfaffian chains ensures bounded connected components in constraint sets?

- Concept: **Dual cost function in data-driven algorithm design**
  - Why needed here: The analysis examines convergence steps as a function of hyperparameters, not the primal loss landscape.
  - Quick check question: Why does piecewise-monotonicity of the dual cost enable pseudo-dimension bounds while Lipschitzness fails for non-convex functions?

## Architecture Onboarding

- Component map:
  - Instance space Π: (initial point x ∈ ℝ^d, function f) pairs defining optimization problems
  - Cost function ℓ(η, x, f): Number of GD iterations until ||∇f|| < θ (or H if non-converging)
  - Function class F: Piecewise-polynomial or piecewise-Pfaffian objectives with bounded degree/pieces
  - Learning pipeline: Sample tasks from distribution D → select η minimizing empirical cost → uniform convergence guarantees

- Critical path:
  1. Identify function class structure (polynomial degree ∆, boundary count p, Pfaffian parameters if applicable)
  2. Apply appropriate theorem (3.1 for polynomial, 3.2 for piecewise-polynomial, 3.3 for Pfaffian)
  3. Collect Õ(H³/ε²) task samples from target distribution
  4. Tune η via empirical risk minimization on sampled tasks

- Design tradeoffs:
  - Polynomial vs. Pfaffian: Polynomial bounds have O(H log ∆) pseudo-dimension; Pfaffian adds O(q²d²H²) term—use polynomial analysis when activations permit
  - Single η vs. schedule: Full schedule requires Õ(H⁴/ε²) samples (Theorem 4.1) vs. Õ(H³/ε²) for fixed step-size
  - Convergence speed vs. final quality: Theorem 5.1 bounds validation loss optimization with additional dependence on validation function complexity

- Failure signatures:
  - Sample complexity scales linearly with layers L through ∆ (exponential in L for deep networks)—bounds may be loose for very deep nets
  - Assumes GD never lands on piece boundaries; may fail with pathological initialization
  - Convergence threshold θ must be fixed; varying θ changes the dual cost structure

- First 3 experiments:
  1. **Validation on shallow ReLU networks**: Sample tasks (random quadratic or low-degree polynomial objectives), compare empirically optimal η vs. theoretically predicted sample requirements
  2. **Schedule ablation**: Test Õ(H⁴) bound for learning rate schedules against Õ(H³) for fixed step-size on identical task distributions
  3. **Activation comparison**: Compare polynomial (ReLU) vs. Pfaffian (sigmoid) sample complexity predictions on controlled optimization problems to validate the q²d²H² overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bounds be extended to stochastic gradient descent (SGD), where gradients are computed on random mini-batches rather than the full dataset?
- Basis in paper: [inferred] The paper analyzes only full-batch gradient descent (Algorithm 1), while SGD is the dominant method in practice for training neural networks. The analysis relies heavily on the deterministic structure of gradient computations.
- Why unresolved: The randomness in SGD mini-batch sampling fundamentally changes the dual cost function structure, as the same step-size can produce different convergence behavior across runs. The piecewise-polynomial/Pfaffian structure of iterates may not hold with stochastic gradients.
- What evidence would resolve it: A theoretical analysis showing bounded pseudo-dimension for the expected cost function under mini-batch sampling, or alternatively, a lower bound showing that stochasticity fundamentally changes the sample complexity landscape.

### Open Question 2
- Question: Do lower bounds on sample complexity exist that match the Õ(H³/ϵ²) upper bounds for tuning the learning rate in non-convex settings?
- Basis in paper: [inferred] The paper provides upper bounds matching prior convex results up to logarithmic factors, but does not establish that these bounds are tight.
- Why unresolved: Lower bounds in data-driven algorithm design require constructing hard instance distributions where any learning algorithm provably requires many samples. The non-convex, non-smooth setting may have fundamentally different hardness than convex settings.
- What evidence would resolve it: A formal lower bound construction showing Ω(H³/ϵ²) samples are necessary for some distribution over piecewise-polynomial functions, or an improved upper bound showing the current analysis is loose.

### Open Question 3
- Question: Can the analysis be extended to networks with periodic activation functions like sine (used in SIRENs and Fourier feature networks)?
- Basis in paper: [explicit] "However, period trigonometric functions like sine and cosine are not Pfaffian (although non-periodic inverse trigonometric functions, like arctan are)."
- Why unresolved: The Pfaffian function framework is central to the analysis, and periodic functions fall outside this class because their derivatives cannot be expressed as polynomials of finite degree in the function itself.
- What evidence would resolve it: An alternative mathematical framework that handles periodic functions while still bounding the number of pieces in the dual cost function, or a proof that periodic activations lead to fundamentally different (potentially exponential) sample complexity.

### Open Question 4
- Question: What is the computational complexity of finding near-optimal hyperparameters given the sample complexity bounds, and can efficient algorithms be designed?
- Basis in paper: [inferred] The paper establishes sample complexity (how many tasks are needed) but does not address the computational cost of actually optimizing hyperparameters once samples are obtained. The dual cost function has O(Δ^H) pieces, suggesting potential computational barriers.
- Why unresolved: Even with sufficient samples, finding optimal hyperparameters requires searching over a space with exponentially many pieces in the worst case. The relationship between statistical and computational complexity is not addressed.
- What evidence would resolve it: A polynomial-time algorithm for finding ε-optimal hyperparameters, or a computational hardness result showing that hyperparameter optimization is NP-hard even when sample complexity is bounded.

## Limitations
- The analysis fundamentally assumes piecewise-polynomial or piecewise-Pfaffian structure, excluding periodic activations like sine/cosine
- Bounds scale exponentially with network depth through the degree parameter ∆, potentially limiting practical applicability to very deep networks
- No empirical validation is provided, and constants hidden in O(·) notation are unspecified

## Confidence
- Mechanism 1 (dual cost structure): High - The polynomial induction and piecewise counting argument are rigorously proven with explicit degree bounds
- Mechanism 2 (pseudo-dimension bounds): High - Direct application of classical learning theory to the established function class complexity
- Mechanism 3 (Pfaffian extension): Medium - While the Khovanskii-based counting is established, the careful chain construction for GD iterates requires verification

## Next Checks
1. Implement symbolic verification for H=5 on simple polynomial objectives to confirm the piecewise structure of ||∇f(x_i)||² and the O(∆^H) piece bound
2. Construct explicit Pfaffian chains for GD iterates on sigmoid activations and verify the O(q²d²H²) connected components bound via Khovanskii's theorem
3. Conduct empirical comparison of predicted vs. actual sample complexity on controlled optimization tasks with known optimal η across varying ∆ and H parameters