---
ver: rpa2
title: Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic
  Constraints
arxiv_id: '2505.07883'
source_url: https://arxiv.org/abs/2505.07883
tags:
- latent
- event
- probabilities
- embeddings
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incoherent probability estimates
  from large language models (LLMs), where probabilities assigned to complementary
  events often fail to sum to one, violating the axioms of probability theory. The
  authors propose an unsupervised method to recover coherent probabilities directly
  from LLM embeddings by enforcing axiomatic constraints through a modified variational
  autoencoder (VAE).
---

# Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints

## Quick Facts
- arXiv ID: 2505.07883
- Source URL: https://arxiv.org/abs/2505.07883
- Reference count: 40
- Key outcome: Reduces incoherence from 0.13 to 0.02 by recovering coherent probabilities from LLM embeddings via axiomatic constraints

## Executive Summary
This paper addresses the problem of incoherent probability estimates from large language models (LLMs), where probabilities assigned to complementary events often fail to sum to one, violating the axioms of probability theory. The authors propose an unsupervised method to recover coherent probabilities directly from LLM embeddings by enforcing axiomatic constraints through a modified variational autoencoder (VAE). The VAE is trained to both reconstruct the original embeddings and predict embeddings of complementary events by selectively modifying a subset of latent variables, with a Gaussian prior enforcing the additive rule for complementary events. Results show that probabilities recovered from LLM embeddings exhibit significantly greater coherence than probabilities directly elicited from the models, while maintaining comparable accuracy.

## Method Summary
The authors propose an unsupervised method that uses a modified variational autoencoder to recover coherent probabilities from LLM embeddings. The approach involves training a VAE on embeddings extracted from the last token of the final layer of Gemma-2-9b-instruct. The VAE learns to reconstruct these embeddings while also predicting embeddings of complementary events by negating a specific latent variable (z^(1)). This enforces the additive rule P(A) + P(¬A) = 1 in the latent space. The method is evaluated on dice-related events where true probabilities are known, demonstrating reduced incoherence and maintained accuracy compared to directly elicited probabilities.

## Key Results
- Reduces incoherence from 0.13 to 0.02 on the training set
- Maintains comparable accuracy to direct elicitation (MSE 0.059 vs 0.060)
- Shows stronger alignment with true probabilities (Pearson's r = 0.83 vs 0.80)
- Ablation study confirms two-step training is essential for interpretable latent representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing a sign-flip transformation on a specific latent variable forces the model to encode event probabilities as log-odds, satisfying the additive axiom of probability theory.
- **Mechanism:** The model assumes that if latent variable $z^{(1)}$ represents the log-odds of event $A$, then $-z^{(1)}$ represents the log-odds of the complementary event $\neg A$. By swapping $z^{(1)}$ to $-z^{(1)}$ while keeping other latent variables constant, the decoder is forced to generate the embedding for the complementary event. This structural constraint ensures $P(A) + P(\neg A) = 1$.
- **Core assumption:** The relationship between an event and its complement is linear and fully invertible in the log-odds latent space (i.e., $\log \frac{p}{1-p} \to -\log \frac{p}{1-p}$).
- **Evidence anchors:**
  - [Section 3] "The modification operation consists of negating the first latent variable... $T(z) = [-z^{(1)}, z^{(-1)}]$."
  - [Section 3] "Flipping the sign of $z^{(1)}$, we enforce that the recovered probabilities satisfy the additive rule... $e^{z^{(1)}}/(1+e^{z^{(1)}}) + e^{-z^{(1)}}/(1+e^{-z^{(1)}}) = 1$."
  - [corpus] No direct corpus support for this specific sign-flip log-odds mechanism; related papers focus on general uncertainty or event detection.
- **Break condition:** If the relationship between complementary embeddings requires complex non-linear transformations beyond a simple sign flip (e.g., if $e$ and $\neg e$ share almost no manifold structure), the decoder will fail to reconstruct valid embeddings from the modified latent vector.

### Mechanism 2
- **Claim:** Interleaved training (reconstruction + prediction) disentangles probability information from semantic context within the embeddings.
- **Mechanism:** The VAE is trained on two interleaved objectives: (1) reconstructing the original embedding $e$ (Standard VAE) and (2) predicting the complementary embedding $\neg e$ from the modified latent code. This forces the model to isolate the "probability" factor into the modifiable variable $z^{(1)}$ and the "semantic content" into the stable variables $z^{(2:10)}$, preventing the probability info from mixing with other features.
- **Core assumption:** The embeddings contain "stable" semantic features that are invariant between an event and its complement (e.g., "dice," "rolling") which can be isolated by the encoder.
- **Evidence anchors:**
  - [Section 3] "Step 1... learn to reconstruct the original LLM embeddings... Step 2... predict the embeddings of complementary events."
  - [Appendix G] Ablation study shows that removing Step 2 results in high polysemanticity and incoherent probabilities (Incoherence $\approx 0.30$).
  - [corpus] "Causal Graph based Event Reasoning" discusses identifying causal connections, which parallels the need here to isolate specific causal factors (probability) from embeddings, though the method differs.

### Mechanism 3
- **Claim:** A centered Gaussian prior ($\mu=0$) on the latent space acts as a regularizer for axiomatic compliance.
- **Mechanism:** By imposing a prior $p(z) = \mathcal{N}(0, I)$ and weighting the KL-divergence term (using $\beta$-VAE), the model pushes the latent log-odds toward 0 (probability 0.5) unless the data provides strong evidence otherwise. This prevents "probability drift" and ensures the distribution of beliefs is centered around uncertainty, aligning with the formal properties of coherent probabilities.
- **Core assumption:** A neutral bias (probability 0.5) is the appropriate default state for the latent variable representing belief.
- **Evidence anchors:**
  - [Page 6] "The centered isotropic Gaussian prior... effectively enforces the axiomatic constraint... sum of the log odds of the two events equaling zero."
  - [Page 6] "Stricter regularization can be imposed by increasing the weight of the divergence... more strongly enforcing adherence."
  - [corpus] Weak support; corpus papers discuss uncertainty estimation generally but do not link Gaussian priors to axiomatic recovery specifically.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs) & Reparameterization**
  - **Why needed here:** The entire architecture relies on an "extended VAE." Understanding the Evidence Lower Bound (ELBO), the role of the encoder ($q_\phi$), decoder ($p_\theta$), and the reparameterization trick (sampling $\epsilon$) is required to implement Algorithm 1.
  - **Quick check question:** Can you explain why the loss function includes both a Reconstruction Loss (MSE) and a KL Divergence term, and what happens if you scale the KL term (the $\beta$ parameter)?

- **Concept: Probability Coherence & Dutch Books**
  - **Why needed here:** The problem statement is not just "accuracy" but "coherence." You must understand why $P(A) + P(\neg A) \neq 1$ constitutes a rationality failure (incoherence) exploitable via arbitrage, distinct from simply being "wrong" about the true probability.
  - **Quick check question:** If an LLM outputs $P(\text{Rain}) = 0.8$ and $P(\text{No Rain}) = 0.4$, why is this fundamentally different (and worse) than outputting $0.6$ and $0.4$ when the true probability is $0.2$ and $0.8$?

- **Concept: Disentangled Representation Learning**
  - **Why needed here:** The goal is to force specific semantic meaning (probability) into a single dimension $z^{(1)}$. Understanding the trade-offs between entangled (dense) and disentangled (interpretable) representations helps explain why the ablation study (standard VAE) failed.
  - **Quick check question:** Why does a standard autoencoder typically fail to map a single latent dimension to a specific human-interpretable concept like "probability"?

## Architecture Onboarding

- **Component map:**
  - Input embeddings $e$ (dim $d$) -> Encoder (3-layer MLP) -> $\mu, \sigma$ -> Sample $z$ -> Transformation $T(z)$ (sign flip) -> Decoder (3-layer MLP) -> $\tilde{e}$ -> Probability (Sigmoid on $z^{(1)}$)

- **Critical path:**
  1. Extract Gemma/Llama embeddings $\to$ **Encoder** $\to$ Sample $z$.
  2. **Training Branch A:** $z \to$ **Decoder** $\to$ Reconstruct $e$ (Loss $L_1$).
  3. **Training Branch B:** $z \to$ **Sign Flip** $\to$ **Decoder** $\to$ Predict $\neg e$ (Loss $L_2$).
  4. **Inference:** $e \to$ Encoder $\to z^{(1)} \to \text{Sigmoid} \to P_{\text{recovered}}$.

- **Design tradeoffs:**
  - **Latent Dimension ($k$):** Paper uses 10. Too few dimensions may fail to capture semantic complexity (bad reconstruction); too many may lead to polysemanticity or dead neurons.
  - **$\beta$ Value:** Set to 5. Increasing this enforces coherence more strictly but may degrade reconstruction accuracy or "underfit" the embedding structure.
  - **Supervision:** This method is *unsupervised* regarding true probabilities (it uses axioms only). A Linear Probe (Supervised) achieves near-perfect training accuracy (MSE < 0.0001) but fails catastrophically on the test set (MSE 0.46), whereas this method generalizes.

- **Failure signatures:**
  - **High Incoherence (> 0.10):** Indicates the interleaved training (Step 2) is not effectively forcing the sign-flip constraint, or the model is ignoring the KL prior.
  - **Negative Correlation with True Probability:** Observed in earlier layers (e.g., Layer 25) or bad seeds. Indicates the model is learning an inverse relationship or "anti-probability" feature (e.g., representing "impossibility" rather than likelihood).
  - **Reconstruction Collapse:** If the decoder cannot reconstruct $e$ from $z$, the latent space is too compressed or the learning rate is too high.

- **First 3 experiments:**
  1. **Baseline Check:** Prompt the raw LLM (Gemma-2-9b) for $P(A)$ and $P(\neg A)$ separately to verify high incoherence ($\approx 0.13$) exists in your target domain.
  2. **Ablation Sanity Check:** Train a standard $\beta$-VAE (Step 1 only) on the embeddings. Verify that the latent space is uninterpretable and the "most negative correlation" latent variable performs poorly (high incoherence).
  3. **Layer Sweep:** Extract embeddings from different layers (e.g., Layer 41 vs. Layer 25). Verify if the final layer contains the most coherent probability signal (correlation should be positive and high in later layers).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this method be extended to enforce more complex axiomatic constraints, such as Bayes' rule for conjunctive events?
- **Basis in paper:** [explicit] The authors state in Section 6.1 that future work could explore implementing constraints for the relationship between conjunctive and constituent events via Bayes' rule.
- **Why unresolved:** The current methodology only enforces the additive rule for complementary events (A vs. not-A) and has not been tested on conditional or joint probability structures.
- **What evidence would resolve it:** Successful recovery of coherent conjunctive probabilities (e.g., P(A and B)) from embeddings that satisfy P(A and B) = P(A)P(B|A).

### Open Question 2
- **Question:** Do causal interventions on the identified latent variables induce more coherent probability judgments in the model's text output?
- **Basis in paper:** [explicit] Section 6.1 identifies "editing neuron activations based on the learned latent variables" as a natural next step to establish a causal relationship.
- **Why unresolved:** The current study focuses on decoding probabilities from embeddings (analysis) rather than modifying the model's generative process to fix incoherent outputs (intervention).
- **What evidence would resolve it:** Demonstrating that modifying the specific latent variable $z(1)$ changes the LLM's text response to output mathematically coherent probabilities.

### Open Question 3
- **Question:** Can this axiomatic constraint framework be generalized to other modalities, such as enforcing rotational invariance in multimodal LLMs?
- **Basis in paper:** [explicit] The authors suggest in Section 6.1 that the approach could be extended to other constraints, specifically citing rotational invariance for image processing in multimodal models.
- **Why unresolved:** The experiments were restricted to text-based dice events, and it is unknown if the VAE-based constraint enforcement transfers to visual or structural symmetries.
- **What evidence would resolve it:** Application of the method to vision-language models where the learned latent space enforces consistent representations of objects regardless of rotational orientation.

## Limitations
- The evaluation is restricted to a narrow domain (dice probability questions) where ground truth probabilities are known, limiting generalizability to broader event types.
- The method assumes a specific geometric relationship between complementary event embeddings that may not hold for all event types.
- The method requires access to embeddings and training infrastructure that may not be available for all LLM architectures.

## Confidence
- **High Confidence:** The coherence improvement (incoherence reduction from 0.13 to 0.02) and correlation with true probabilities (r=0.83) are well-supported by the experimental results.
- **Medium Confidence:** The two-step training approach's superiority over standard VAEs is demonstrated, but the specific choice of hyperparameters (β=5, k=10) and their optimality for different domains remains uncertain.
- **Medium Confidence:** The assumption that log-odds are encoded in a single latent dimension through the sign-flip mechanism is plausible but not rigorously proven across diverse event types.

## Next Checks
1. **Cross-Domain Generalization:** Test the method on non-dice event types (e.g., medical diagnosis probabilities, weather forecasting) to verify if the sign-flip log-odds mechanism generalizes beyond symmetric probability spaces.
2. **Latent Space Analysis:** Visualize and analyze the distribution of z(1) values across different probability ranges to confirm the log-odds interpretation and identify any systematic biases or clustering patterns.
3. **Ablation on Transformation Type:** Replace the sign-flip transformation with alternative operations (e.g., negation plus scaling, non-linear transformations) to test whether the specific sign-flip is critical or if other transformations could achieve similar coherence improvements.