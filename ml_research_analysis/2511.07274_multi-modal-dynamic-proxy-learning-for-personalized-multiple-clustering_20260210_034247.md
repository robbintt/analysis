---
ver: rpa2
title: Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering
arxiv_id: '2511.07274'
source_url: https://arxiv.org/abs/2511.07274
tags:
- clustering
- candidate
- color
- multi-dproxy
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-DProxy introduces a multi-modal dynamic proxy learning framework
  that addresses the limitations of static semantic rigidity and inflexible feature
  fusion in existing multiple clustering methods. By combining gated cross-modal fusion,
  dynamic candidate management, and dual-constraint proxy optimization, it generates
  personalized clusterings aligned with user interests.
---

# Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering

## Quick Facts
- arXiv ID: 2511.07274
- Source URL: https://arxiv.org/abs/2511.07274
- Reference count: 19
- Primary result: State-of-the-art personalized multiple clustering with perfect NMI/RI on Fruit dataset

## Executive Summary
Multi-DProxy introduces a multi-modal dynamic proxy learning framework that addresses the limitations of static semantic rigidity and inflexible feature fusion in existing multiple clustering methods. By combining gated cross-modal fusion, dynamic candidate management, and dual-constraint proxy optimization, it generates personalized clusterings aligned with user interests. Experiments across seven publicly available benchmarks demonstrate state-of-the-art performance, achieving perfect NMI and RI scores on the Fruit dataset and significant improvements over baselines on others.

## Method Summary
The method initializes learnable proxy vectors using a placeholder token, then applies L layers of gated cross-modal fusion through bidirectional cross-attention with sigmoid-gated residuals. Dual-constraint optimization minimizes cross-modal alignment loss, user interest constraints (proxy-centroid MSE), and concept discrimination constraints (contrastive learning). Every R epochs, the system performs K-means clustering on proxy embeddings to score and prune the candidate word set, iteratively refining proxies through clustering feedback. The final output uses K-means on the fused visual-textual representations.

## Key Results
- Achieves perfect NMI and RI scores on the Fruit dataset
- Demonstrates significant improvements over baseline methods across all seven benchmark datasets
- Ablation studies confirm the effectiveness of each component: Gated Cross-Modal Fusion, Dual-Constraint Optimization, and Dynamic Candidate Management

## Why This Works (Mechanism)

### Mechanism 1: Gated Cross-Modal Fusion
Dynamic, gated fusion between visual and textual representations produces discriminative joint representations that better capture user-specified clustering perspectives than static fusion. Bidirectional cross-attention enables visual features to modulate text representations and vice versa, while sigmoid-gated residuals adaptively recalibrate feature contributions. Theoretical analysis shows gradients prioritize visual features proportional to their discriminative power.

### Mechanism 2: Dual-Constraint Proxy Optimization
Enforcing both user interest constraints (semantic alignment with domain concepts) and concept constraints (cluster discrimination) produces proxies that capture user intent while maintaining cluster separability. Proxies are computed as semantic-weighted combinations of LLM-generated candidate embeddings, with adaptive scheduling that progressively strengthens semantic constraints during training.

### Mechanism 3: Dynamic Candidate Management
Iteratively refining candidate words through clustering feedback filters dataset-irrelevant LLM suggestions and adapts proxies to emergent data structures. Every R epochs, the system clusters proxies, computes cluster centroids, scores each candidate by average similarity to centroids, and retains top-K candidates with highest alignment scores.

## Foundational Learning

- **Cross-Modal Alignment (CLIP-style)**: Needed to ensure visual and textual features share semantic space using frozen CLIP encoders and cross-modal alignment loss. Quick check: Why does cosine similarity outperform Euclidean distance for cross-modal alignment?
- **Proxy-Based Classification**: Needed because learnable proxy vectors serve as cluster representatives, enabling gradient-based cluster optimization. Quick check: How do learnable proxies differ from traditional K-means centroids?
- **Multi-Head Cross-Attention**: Needed because gated fusion uses multi-head scaled dot-product attention enabling bidirectional information flow between visual and textual modalities. Quick check: Why use cross-attention rather than self-attention for multi-modal fusion?

## Architecture Onboarding

**Component map:**
Image → CLIP Vision Encoder → v_i
User Interest u → GPT-4 → Initial Candidates C (2^β×M words)
"A [object] with [u] of *" → CLIP Text Encoder → t_i
"*" token → Text Encoder → Base Proxy w'_i

Gated Cross-Modal Fusion (L layers):
[V, T] → Bidirectional Cross-Attention → Sigmoid-Gated Residual → LayerNorm + FFN
→ V^L, T^L → Adaptive Fusion (λ-weighted) → Fused Features F

Dual-Constraint Optimization:
Proxies W + Candidates C → Semantic Weighted Combination
→ L_u (user interest) + L_c (concept discrimination)

Dynamic Candidate Management (every R epochs):
Proxy K-means → Centroids → Candidate Scoring → Top-K Selection

Output: K-means on F

**Critical path:** Candidate initialization → Gated fusion quality → Proxy optimization convergence → Dynamic refinement stability

**Design tradeoffs:**
- R (update interval): Larger = stability/slower adaptation; smaller = faster adaptation/potential instability
- Initial candidates (2^β×M): Larger = better coverage/more noise; smaller = faster convergence/coverage risk
- Temperature parameters: Control attention/constraint sharpness; too low = peaked distributions; too high = weak discrimination

**Failure signatures:**
- Early NMI/RI plateau: α(t), β(t) schedules mismatch dataset
- Proxy collapse: Concept discrimination loss too weak
- User interest misalignment: Dynamic management not converging
- Modality dominance (λ→extreme): Temperature τ needs adjustment

**First 3 experiments:**
1. **Baseline validation:** Run on Fruit dataset with defaults (R=200, τ_α=0.2, σ=0.2, E=1000). Verify near-perfect NMI/RI as reported—confirms implementation correctness.
2. **Fusion ablation:** Compare full Multi-DProxy vs. w/o-GFusion (concatenation only) across all datasets. Quantifies fusion contribution.
3. **Candidate evolution tracking:** Monitor candidate scores over update cycles on a dataset with irrelevant initial LLM candidates. Visualizes filtering behavior.

## Open Questions the Paper Calls Out

### Open Question 1
How can the Dynamic Candidate Management module be adapted for true unsupervised settings where the target number of clusters (M) is unknown, given its current dependency on M for proxy clustering? The framework currently requires the ground-truth cluster count as a hyperparameter to perform candidate pruning and centroid calculation, limiting applicability to fully unsupervised discovery.

### Open Question 2
To what extent does the quality of the initial candidate set generated by the LLM impact the convergence of the dynamic feedback loop? While the ablation study removes the dynamic component entirely, it does not test the system's resilience to adversarial or highly noisy initialization, particularly whether the K-means centroid feedback loop can bootstrap itself from poor initial candidates.

### Open Question 3
How does the Gated Cross-Modal Fusion mechanism perform on multi-modal data where the visual and textual modalities are less aligned than in the CLIP pre-training data? The benchmark datasets are relatively well-aligned with CLIP's pre-training distribution, but the paper does not address domains where visual features are abstract or textual descriptions are highly technical.

## Limitations
- Relies heavily on quality of LLM-generated initial candidates with no clear failure mode analysis
- Requires ground-truth cluster count M as hyperparameter, limiting true unsupervised discovery
- Performance on out-of-distribution data where visual-textual alignment is weak is untested

## Confidence
- **High Confidence:** Gated cross-modal fusion mechanism; dual-constraint optimization framework; basic training pipeline
- **Medium Confidence:** Dynamic candidate management effectiveness; overall method superiority; hyperparameter sensitivity claims
- **Low Confidence:** Generalization to datasets with minimal visual-textual alignment; performance with poor quality LLM candidates; robustness to extreme hyperparameter settings

## Next Checks
1. **Extreme candidate failure test:** Initialize Multi-DProxy with completely irrelevant LLM candidates and measure convergence behavior and final clustering quality.
2. **Modality dominance analysis:** Systematically vary temperature τ to force λ→0 (text-only) and λ→1 (vision-only) extremes, measuring performance degradation.
3. **Long-term stability monitoring:** Run training for 2000+ epochs to observe whether dynamic candidate management maintains stability or exhibits drift over extended periods.