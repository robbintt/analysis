---
ver: rpa2
title: A Retrospect to Multi-prompt Learning across Vision and Language
arxiv_id: '2511.00191'
source_url: https://arxiv.org/abs/2511.00191
tags:
- learning
- prompt
- empl
- multi-prompt
- coop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a principled retrospect for vision-language
  multi-prompt learning by extending the constant modality gap phenomenon to learnable
  prompts and justifying the superiority of multi-prompt augmentation for vision-language
  transfer, both empirically and theoretically. The authors propose an Energy-based
  Multi-prompt Learning (EMPL) approach that generates multiple prompt embeddings
  by drawing instances from an energy-based distribution implicitly defined by Vision-Language
  Models (VLMs).
---

# A Retrospect to Multi-prompt Learning across Vision and Language

## Quick Facts
- arXiv ID: 2511.00191
- Source URL: https://arxiv.org/abs/2511.00191
- Reference count: 40
- Primary result: Energy-based multi-prompt learning generates prompt embeddings that balance in-domain accuracy and out-of-domain generalization while being parameter-efficient

## Executive Summary
This paper provides a principled retrospect for vision-language multi-prompt learning by extending the constant modality gap phenomenon to learnable prompts and justifying the superiority of multi-prompt augmentation for vision-language transfer. The authors propose Energy-based Multi-prompt Learning (EMPL), which generates multiple prompt embeddings by drawing instances from an energy-based distribution implicitly defined by VLMs. This method achieves a balance between in-domain and out-of-domain open-vocabulary generalization while being parameter-efficient. Comprehensive experiments demonstrate EMPL's effectiveness across three tasks: base-to-new generalization, cross-domain generalization, and cross-dataset transfer learning.

## Method Summary
EMPL extends constant modality gap analysis to learnable prompts and uses energy-based distributions to generate multiple prompt embeddings. The method employs Stochastic Gradient Langevin Dynamics (SGLD) to sample from an energy-based distribution where energy is derived from contrastive scores between image and text embeddings. The training objective combines standard prompt learning with uncertainty modeling over unseen classes, creating a negative correlation between in-domain and out-of-domain prompt distributions. EMPL uses 2-layer SGLD sampling and meta-learning with mixed seen/unseen classes per batch to optimize learnable context vectors while keeping encoders frozen.

## Key Results
- EMPL consistently outperforms single-prompt baselines and enhances multi-prompt methods across base-to-new generalization, cross-domain generalization, and cross-dataset transfer learning
- The method achieves improved generalization to unseen classes while maintaining strong performance on in-domain data
- Multi-prompt augmentation progressively reduces both individual-level and class-level modality gaps between image and text embeddings

## Why This Works (Mechanism)

### Mechanism 1
Multi-prompt learning reduces the modality gap between image and text embeddings more effectively than single-prompt approaches. The constant modality gap phenomenon manifests as a systematic offset between embedding distributions, and multiple learnable prompts create richer representation space that bridges this gap. Prompt augmentation progressively reduces both individual-level and class-level modality gaps.

### Mechanism 2
Single-prompt learning creates cross-modal non-identifiability where images with mutually exclusive concepts cannot be distinguished using their exclusive attributes. Multi-prompt augmentation provides multiple geometric views that collectively resolve this identifiability crisis by allowing proper discrimination of mutually exclusive attributes.

### Mechanism 3
Energy-based prompt distributions enable simultaneous in-domain accuracy and out-of-domain generalization through uncertainty-aware prompt sampling. EMPL treats prompt generation as sampling from an energy-based distribution where low energy corresponds to high compatibility, and the meta-learning objective includes uncertainty modeling over unseen classes to balance exploitation and exploration.

## Foundational Learning

- **Concept**: Contrastive Learning and Modality Alignment
  - Why needed here: EMPL builds on CLIP's contrastive pre-training where image and text encoders are trained to maximize similarity for matched pairs
  - Quick check question: Can you explain why contrastive learning creates a modality gap rather than perfectly overlapping embedding spaces?

- **Concept**: Soft Prompting and Learnable Context Vectors
  - Why needed here: The paper extends CoOp-style learnable prompts to multi-prompt settings where context vectors replace hand-crafted templates
  - Quick check question: Given a class name "dog," how would you construct h_v(dog) using learnable context vectors?

- **Concept**: Energy-Based Models and Langevin Dynamics
  - Why needed here: EMPL formulates prompt generation as sampling from an EBM distribution using SGLD with added noise to gradient descent
  - Quick check question: In Eq. 9, why are Gaussian noise terms added to gradient updates rather than using standard gradient descent?

## Architecture Onboarding

- **Component map**: Frozen CLIP image encoder f(·) and text encoder h(·) -> Learnable context vectors v -> Energy function E_ϕ(X,H; T_i) -> SGLD sampler -> Meta-learning loop

- **Critical path**: Initialize context vectors v -> Sample task T_i with mixed seen/unseen classes -> Execute SGLD sampling to generate prompt embeddings H -> Compute EMPL loss with cross-entropy on observed classes + uncertainty on unseen classes -> Update v via backpropagation

- **Design tradeoffs**: 2-layer SGLD is optimal (0.048→0.278 sec/iter); increasing unseen class ratio improves new-class generalization but plateaus quickly; higher λ favors out-of-domain exploration at cost of in-domain accuracy

- **Failure signatures**: SGLD divergence (NaN losses or collapsed embeddings), over-exploration (excessive λ causes poor in-domain performance), modality gap persistence (base method's prompts already have large gaps), inference latency (triple training time vs. CoOp)

- **First 3 experiments**:
  1. Replicate Figure 2 on your target dataset to measure individual and class modality gaps for CLIP, single-prompt CoOp, and multi-prompt variants
  2. On held-out validation set, sweep λ ∈ {0.01, 0.05, 0.1, 0.5, 1.0} to find in-domain/OOD tradeoff sweet spot
  3. Visualize prompt embedding trajectories during SGLD sampling for a few sample images to check convergence

## Open Questions the Paper Calls Out

- **Open Question 1**: Can sophisticated analytical studies in learnability and optimization theory fully explain why and how multi-prompt learners outperform single-prompt approaches? The authors state that despite identifying cross-modal non-identifiability, "why and how multi-prompt learners work out, still requiring for more sophisticated analytical studies with respect to learnability and optimization theories."

- **Open Question 2**: How can prompt diversity be explicitly quantified and optimized to maximize generalization in multi-prompt learning? The authors identify prompt diversity as "the key why multi-prompt learners outperform single-prompt learners" and call for research on prompt diversity.

- **Open Question 3**: Can the high inference cost of Energy-based Multi-Prompt Learning be reduced to match single-prompt methods without sacrificing generalization? The authors acknowledge that EMPL currently incurs triple the time cost of CoOp for training and double for inference.

## Limitations

- Theoretical analysis assumes constant modality gaps, but empirical validation only demonstrates gap reduction trends without establishing whether this assumption holds for learned prompts across diverse tasks
- Energy-based distribution formulation relies on CLIP's contrastive scores as proxy energy, but alternative formulations might yield different prompt distributions
- Meta-learning objective's unseen class vocabulary source remains unspecified, which could significantly impact generalization performance

## Confidence

- **High confidence**: The empirical superiority of multi-prompt augmentation over single-prompt baselines (observed consistently across all three task types)
- **Medium confidence**: The modality gap reduction mechanism (gap measurements support the trend but don't prove causal relationship to improved performance)
- **Medium confidence**: The energy-based prompt generation approach (SGLD sampling works in practice but theoretical convergence guarantees are not established)

## Next Checks

1. Systematically vary the number of prompts and measure how modality gap magnitude and directional consistency change across different task domains to validate the constant-gap assumption's applicability limits
2. Replace CLIP contrastive scores with alternative energy formulations (calibrated confidence scores, entropy-based measures) to test whether the specific energy function choice drives the observed performance gains
3. Compare EMPL performance using different unseen class vocabulary sources (task-specific vs. generic vs. random) to determine whether vocabulary selection strategy affects the negative correlation between in-domain and out-of-domain generalization