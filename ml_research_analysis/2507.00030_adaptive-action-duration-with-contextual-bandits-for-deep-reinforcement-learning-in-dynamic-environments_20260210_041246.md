---
ver: rpa2
title: Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning
  in Dynamic Environments
arxiv_id: '2507.00030'
source_url: https://arxiv.org/abs/2507.00030
tags:
- action
- learning
- duration
- contextual
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes integrating contextual bandits with Deep Q-Networks
  (DQN) to adaptively select action durations in reinforcement learning, addressing
  the limitation of static action repetition rates in existing DRL methods. The approach
  augments DQN with a contextual bandit module that learns to choose optimal action
  repetition rates based on state contexts, enabling finer-grained temporal control.
---

# Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments

## Quick Facts
- arXiv ID: 2507.00030
- Source URL: https://arxiv.org/abs/2507.00030
- Reference count: 12
- Primary result: Bandit-DQN achieves 15% higher scores than DFDQN in Seaquest and 10% higher in Enduro

## Executive Summary
This paper addresses a fundamental limitation in deep reinforcement learning: the inability to adaptively select action durations in dynamic environments. Traditional DRL methods use static action repetition rates, which can be suboptimal for scenarios requiring different temporal resolutions. The authors propose integrating contextual bandits with Deep Q-Networks (DQN) to learn optimal action repetition rates based on state contexts, enabling finer-grained temporal control. Their Bandit-DQN architecture demonstrates significant improvements over static duration baselines and the Dynamic Frameskip DQN across five Atari 2600 games.

## Method Summary
The proposed approach augments standard DQN with a contextual bandit module that learns to select optimal action repetition rates. The bandit module takes state features as input and outputs a distribution over possible durations, which are then used to repeat actions before the next DQN update. The bandit is updated using rewards derived from DQN's Q-value changes, creating a synergistic relationship where the bandit learns when to use short durations for quick-reflex scenarios versus long durations for sustained actions. Critically, the architecture shares convolutional features between the DQN and bandit components, reducing computational overhead while maintaining distinct decision-making capabilities.

## Key Results
- Bandit-DQN achieves 15% higher scores than DFDQN in Seaquest and 10% higher in Enduro
- Overall improvements across all five tested Atari games compared to static duration baselines
- Demonstrates adaptive duration selection: 60% short durations (1-5 frames) for Space Invaders versus 45% long durations (8-11 frames) for Enduro

## Why This Works (Mechanism)
The method works by introducing a hierarchical temporal abstraction where a contextual bandit module learns state-dependent action repetition rates. This allows the agent to dynamically adjust its decision frequency based on environmental demands - using shorter durations for fast-paced scenarios requiring quick reflexes and longer durations for situations where sustained actions are beneficial. The bandit module's reward signal, derived from Q-value changes, provides a natural metric for evaluating whether the chosen duration was effective in maximizing returns.

## Foundational Learning

**DQN (Deep Q-Networks)**
- Why needed: Standard reinforcement learning baseline that estimates action values
- Quick check: Can estimate Q-values for discrete action spaces using convolutional neural networks

**Contextual Bandits**
- Why needed: Framework for making decisions based on context without full state transitions
- Quick check: Can learn to select among discrete actions given contextual information

**Action Repetition**
- Why needed: Reduces computational cost by executing same action multiple frames
- Quick check: Can be parameterized by repetition count to control temporal abstraction

**Reward Shaping**
- Why needed: Guides learning by providing meaningful feedback signals
- Quick check: Can use Q-value differences as implicit reward for duration selection

**Feature Sharing**
- Why needed: Reduces computational overhead while maintaining information flow
- Quick check: Can share early convolutional layers between different decision modules

## Architecture Onboarding

**Component Map**
ConvNet Features -> DQN Module; ConvNet Features -> Bandit Module -> Duration Selection -> Action Repetition -> Environment -> Rewards

**Critical Path**
State → Shared Convolutional Features → Both DQN and Bandit → Action Selection with Duration → Environment Step → Reward Calculation → Q-value Update

**Design Tradeoffs**
- Shared features reduce computation but may limit bandit's specialized learning
- Q-value difference rewards are implicit but may be noisy in sparse-reward environments
- Fixed duration set limits adaptability but keeps bandit problem tractable

**Failure Signatures**
- Bandit selects extreme durations consistently (overfitting to specific scenarios)
- Q-value changes are too small to provide meaningful bandit rewards
- Shared features don't capture bandit-specific information

**First Experiments**
1. Test bandit duration selection on a simple gridworld with known optimal durations
2. Compare Q-value changes as rewards versus explicit handcrafted rewards
3. Measure computational overhead of shared versus separate feature extraction

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Atari 2600 games, may not generalize to continuous control tasks
- Bandit module's reliance on Q-value differences could be unstable in sparse-reward scenarios
- Computational overhead and sample efficiency impact were not thoroughly analyzed

## Confidence

**High confidence**: The core methodology of integrating contextual bandits with DQN is technically sound and the experimental framework is appropriate for the Atari benchmark.

**Medium confidence**: The claim that adaptive durations outperform static baselines is supported by the data, but the magnitude of improvement may vary with different hyperparameters or reward scales.

**Low confidence**: The generalization of these results to non-Atari environments or real-world robotics applications remains speculative.

## Next Checks

1. Test the Bandit-DQN architecture on continuous control tasks from the MuJoCo or PyBullet suites to assess generalization beyond discrete action spaces.

2. Conduct ablation studies varying the state representation for the bandit module to determine sensitivity to feature sharing assumptions.

3. Measure and report the computational overhead and sample efficiency impact of the bandit module compared to baseline DQN implementations.