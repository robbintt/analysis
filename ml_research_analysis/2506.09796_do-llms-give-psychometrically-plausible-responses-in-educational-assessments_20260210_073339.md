---
ver: rpa2
title: Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?
arxiv_id: '2506.09796'
source_url: https://arxiv.org/abs/2506.09796
tags:
- qwen2
- item
- response
- test
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether responses from 18 instruction-tuned
  LLMs are psychometrically plausible compared to human responses on multiple-choice
  educational assessments. Using classical test theory (CTT) and item response theory
  (IRT), the authors analyze response distributions across two datasets covering reading,
  U.S.
---

# Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?

## Quick Facts
- **arXiv ID**: 2506.09796
- **Source URL**: https://arxiv.org/abs/2506.09796
- **Reference count**: 23
- **Primary result**: Zero-shot LLMs are not sufficiently psychometrically plausible for educational assessment piloting

## Executive Summary
This paper evaluates whether responses from 18 instruction-tuned LLMs are psychometrically plausible compared to human responses on multiple-choice educational assessments. Using classical test theory (CTT) and item response theory (IRT), the authors analyze response distributions across two datasets covering reading, U.S. history, and economics. They apply temperature scaling to calibrate LLM probabilities to human response distributions and compute KL divergence, CTT item facility correlations, and IRT-based correlations. Results show that while larger models achieve higher accuracy, they are overly confident and poorly model distractor distributions, with modest CTT correlations (0.3-0.5 for some reading subsets) and generally low IRT correlations.

## Method Summary
The study extracts first-token logits for A/B/C/D options from 18 instruction-tuned models (0.5B–72B), applies cyclic permutation debiasing (4 orderings per item), and optimizes temperature scaling per model-subject-grade combination to minimize KL divergence with human response distributions. Evaluation uses three metrics: KL divergence between LLM and human response distributions, Pearson correlation between LLM correct-answer probability and CTT item facility, and Pearson correlation between LLM probability and expected probability from IRT (3-parameter logistic model) for average test-taker (θ=0). The analysis compares results against two baselines (uniform distractor probability and oracle) and examines domain differences across reading, history, and economics items.

## Key Results
- Larger models achieve higher accuracy but remain poorly calibrated to human response variance
- Temperature scaling partially aligns distributions but cannot fix fundamental distractor modeling failures
- CTT correlations are modest (0.3-0.5 for some reading subsets) while IRT correlations are generally low
- History items show negative IRT correlations in 4th grade, indicating inverted difficulty-response relationships
- Model-model correlations are much higher than model-human correlations after calibration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temperature scaling can partially align LLM response distributions with human distributions, but cannot fix fundamental mismatches in distractor modeling.
- **Mechanism:** Increasing softmax temperature redistributes probability mass from dominant options to alternatives. The paper optimizes temperature per model and subject-grade combination to minimize KL divergence. Larger models (70B+) require temperatures up to 20-30 due to extreme overconfidence.
- **Core assumption:** Assumes human response distributions are the appropriate target and that calibration on the same data provides a reasonable upper bound.
- **Evidence anchors:** [abstract] "their response distributions can be more human-like when calibrated with temperature scaling"; [Section 4.4] "In preliminary experiments, we found that most LLMs (especially very large ones) tend to be overly confident"
- **Break condition:** Temperature scaling cannot recover information about which distractors humans find attractive—it only smooths existing probabilities. When the model assigns near-zero probability to a high-distractor option, no temperature adjustment fixes this.

### Mechanism 2
- **Claim:** Psychometric plausibility correlates with subject domain, with reading comprehension showing higher alignment than history or economics.
- **Mechanism:** Reading comprehension items rely on text understanding that may overlap with LLM training objectives. History and economics require long-term memory retrieval and multimodal reasoning (images converted to alt-text), where LLMs diverge from human cognition.
- **Core assumption:** The difference is due to cognitive process alignment rather than data artifacts.
- **Evidence anchors:** [abstract] "LLMs tend to correlate better with humans in reading comprehension items compared to other subjects"; [Section 6] "reading comprehension in LLMs is more comparable to humans than other abilities such as long-term memory retrieval"
- **Break condition:** When items require domain knowledge not well-represented in training data, or when multimodal content is converted to text descriptions, plausibility degrades sharply.

### Mechanism 3
- **Claim:** Model scale improves accuracy but not psychometric plausibility; models converge to similar (non-human) response patterns regardless of size.
- **Mechanism:** Larger models achieve higher mode accuracy but remain poorly calibrated to human response variance. After temperature scaling, model-model correlations are much higher than model-human correlations, suggesting all models learn similar systematic biases.
- **Core assumption:** Instruction-tuning produces consistent response patterns across model families that diverge from human test-taking behavior.
- **Evidence anchors:** [Section 5.1] "LLM responses become more similar to the human distribution with increasing model size" for KL divergence, but "only a small number of very large models...managed to significantly outperform the OracleBaseline"
- **Break condition:** Scaling alone does not produce human-like uncertainty; fine-tuning on human response distributions may be required.

## Foundational Learning

- **Concept:** Classical Test Theory (CTT) — Item Facility and Item Discrimination
  - **Why needed here:** CTT provides the baseline correlation metric (item facility vs. LLM correct-answer probability). Without understanding facility as "proportion correct," you cannot interpret the 0.3-0.5 correlations.
  - **Quick check question:** If an item has facility 0.8, what proportion answered correctly? If a model assigns 0.9 probability to the correct answer, should the correlation be positive or negative?

- **Concept:** Item Response Theory (IRT) — 3-Parameter Logistic Model
  - **Why needed here:** The paper uses IRT to compare LLM probabilities against expected probabilities for an "average" test-taker (θ=0). The ICC comparison is stricter than CTT and reveals where models violate psychometric expectations.
  - **Quick check question:** In the 3PL model, what do parameters a, b, and c represent? If c=0.25 for a 4-option item, what does that imply about guessing?

- **Concept:** KL Divergence and Probability Calibration
  - **Why needed here:** KL divergence quantifies distribution mismatch; temperature scaling is the calibration method. Understanding why OracleBaseline (uniform distractor probability) is hard to beat reveals the distractor-modeling failure.
  - **Quick check question:** If a human distribution is [0.6, 0.2, 0.15, 0.05] and LLM output (after softmax) is [0.95, 0.03, 0.01, 0.01], will increasing temperature increase or decrease KL divergence?

## Architecture Onboarding

- **Component map:** Input Processing -> Response Extraction -> Debiasing -> Calibration -> Evaluation
- **Critical path:** Extract first-token logits correctly—Wang et al. (2024) show instruction-tuned models may have token-text mismatches. Apply cyclic permutation debiasing before any analysis. Calibrate temperature separately for each population subset (grade/subject). Compare against both baselines (Uniform, Oracle) to contextualize results.
- **Design tradeoffs:** Zero-shot vs. fine-tuning (paper tests zero-shot only); temperature scaling vs. other calibration (other methods are future work); model size (larger models need more aggressive calibration but remain poor at distractor modeling); dataset limitations (NAEP items with images converted to alt-text; CMCQRD lacks usable IRT parameters).
- **Failure signatures:** Negative IRT correlations (history items): Model more confident on difficult items—fundamentally inverted behavior. KL divergence barely below OracleBaseline: Model cannot distinguish which distractors are attractive. High mode accuracy but low psychometric correlation: Correct answers don't imply human-like reasoning. Model-model correlations >> model-human correlations: Models share systematic biases.
- **First 3 experiments:** Reproduce KL divergence results on a single subject (e.g., NAEP reading grade 8) with 2-3 model sizes. Verify temperature optimization converges and that OracleBaseline is hard to beat. Ablate cyclic permutation: Compare with/without debiasing on a 50-item subset to quantify ordering sensitivity. Pilot a simple fine-tuning intervention: Fine-tune a small model (e.g., Llama-3.2-1B) on human response distributions from one subject; compare CTT/IRT correlations to zero-shot baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning LLMs on human response distributions improve psychometric plausibility for educational assessment piloting beyond temperature scaling?
- **Basis in paper:** [explicit] The authors state: "Fine-tuning on human response distributions could be a promising direction for future research" after noting that temperature scaling and prompting techniques had "very limited" success.
- **Why unresolved:** The study only tested zero-shot prompting with temperature scaling. Fine-tuning approaches remain unexplored despite being a logical next step given the poor distractor modeling and low correlations observed.
- **What evidence would resolve it:** Experiments fine-tuning LLMs on datasets with human response distributions, then evaluating CTT/IRT correlations and KL divergence on held-out items.

### Open Question 2
- **Question:** Can multimodal LLMs achieve better psychometric plausibility on history and economics items that involve images?
- **Basis in paper:** [explicit] The authors state: "Future work could explore whether multimodal models are more successful with these item types" when discussing the discrepancy between reading vs. history/economics results.
- **Why unresolved:** The study used text-only LLMs, but many NAEP items contain images that had to be converted to alternative text, potentially affecting model performance on these subjects.
- **What evidence would resolve it:** Comparison of text-only vs. multimodal LLMs on image-containing items, with analysis of whether multimodal models reduce negative correlations observed in history items.

### Open Question 3
- **Question:** What calibration methods beyond temperature scaling could better align LLM distractor distributions with human response patterns?
- **Basis in paper:** [explicit] The authors mention: "In this study, we use temperature scaling to optimize the response distributions, leaving other calibration methods as future work."
- **Why unresolved:** Temperature scaling could not fix the core problem that LLMs are "bad at predicting which incorrect answer options humans are likely to be distracted by."
- **What evidence would resolve it:** Systematic comparison of alternative calibration methods (e.g., Platt scaling, isotonic regression, fine-tuning on soft labels) measuring KL divergence on distractor probabilities specifically.

## Limitations
- Results based on two specific educational assessment datasets (NAEP and CMCQRD) that may not represent broader educational domains
- Temperature scaling applied to same data used for evaluation provides an upper bound that may overestimate real-world performance
- Study uses simple prompt template without exploring more sophisticated approaches like persona prompting or chain-of-thought reasoning

## Confidence
- **High Confidence**: Larger models achieve higher accuracy but remain poorly calibrated to human response distributions; consistency of negative IRT correlations in history items; model-model correlations exceed model-human correlations
- **Medium Confidence**: Domain-specific differences (reading vs. history/economics) may reflect dataset artifacts rather than fundamental cognitive differences due to image conversion and selection criteria
- **Low Confidence**: Conclusion that zero-shot LLMs are "not sufficiently psychometrically plausible" is based on current methodology without exploring alternative approaches like fine-tuning

## Next Checks
- **Check 1**: Replicate KL divergence results on a held-out subset of NAEP items (20% validation set) with temperature scaling optimized on remaining data to test upper-bound calibration
- **Check 2**: Implement simple fine-tuning experiment where a small model (Llama-3.2-1B) is fine-tuned on human response distributions from one subject; compare CTT and IRT correlations against zero-shot baseline
- **Check 3**: Conduct ablation study comparing cyclic permutation debiasing against alternative ordering strategies on 50-100 items to quantify debiasing contribution to reported results