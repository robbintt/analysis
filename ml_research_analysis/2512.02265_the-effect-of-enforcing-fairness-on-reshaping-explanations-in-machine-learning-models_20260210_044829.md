---
ver: rpa2
title: The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning
  Models
arxiv_id: '2512.02265'
source_url: https://arxiv.org/abs/2512.02265
tags:
- fairness
- across
- mitigation
- feature
- rankings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how fairness constraints in machine learning
  models affect the stability of feature importance rankings in clinical settings.
  The authors analyze three healthcare datasets (pediatric UTI, AFib bleeding risk,
  and COMPAS recidivism) using four model types (logistic regression, random forest,
  XGBoost, and neural networks).
---

# The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models

## Quick Facts
- **arXiv ID**: 2512.02265
- **Source URL**: https://arxiv.org/abs/2512.02265
- **Reference count**: 28
- **Primary result**: Enforcing fairness reduces equalized odds disparities but destabilizes feature importance rankings in clinical ML models

## Executive Summary
This study examines how fairness constraints affect the stability of feature importance rankings in machine learning models used for clinical decision-making. The authors analyze three healthcare datasets using four different model types, applying exponentiated gradient reduction to enforce equalized odds fairness. Their investigation reveals that while fairness mitigation successfully reduces disparities across demographic groups, it also introduces significant instability in feature importance rankings derived from Shapley values. This trade-off between fairness and explanation stability has important implications for deploying ML models in healthcare settings where both equitable outcomes and interpretable decisions are critical.

## Method Summary
The authors evaluate fairness interventions across three healthcare datasets (pediatric UTI, AFib bleeding risk, and COMPAS recidivism) using logistic regression, random forest, XGBoost, and neural networks. They implement equalized odds fairness through exponentiated gradient reduction, measuring performance using accuracy, recall, F1-score, and disparity metrics. Feature importance stability is assessed by comparing pre- and post-mitigation Shapley value rankings using Spearman's correlation coefficient. The analysis examines both aggregate effects across all samples and subgroup-specific impacts, particularly focusing on racial disparities in the UTI and COMPAS datasets.

## Key Results
- Fairness mitigation reduces equalized odds disparities across all datasets and models
- Performance trade-offs vary by dataset, with AFib models showing minimal degradation while UTI models experience substantial drops in recall and F1 scores
- Feature importance rankings become significantly less stable after mitigation, especially in complex models and non-Black subgroups
- The instability in explanations is most pronounced when models are constrained to achieve near-zero disparity

## Why This Works (Mechanism)
Fairness constraints alter the optimization landscape of machine learning models, forcing them to make trade-offs between group-specific performance metrics. When enforcing equalized odds, models must balance true positive and false positive rates across demographic groups, which often requires redistributing decision boundaries. This redistribution affects which features the model relies on most heavily, leading to changes in feature importance rankings. The exponentiated gradient reduction method specifically adjusts model parameters iteratively to satisfy fairness constraints while minimizing performance loss, but this process inherently modifies the model's learned representations and decision logic, resulting in the observed instability in explanations.

## Foundational Learning
- **Shapley values**: Why needed - To quantify feature importance contributions in a theoretically sound way; Quick check - Verify that feature contributions sum to the model prediction
- **Equalized odds**: Why needed - To ensure fair treatment across demographic groups in classification tasks; Quick check - Calculate TPR and FPR parity between groups
- **Exponentiated gradient reduction**: Why needed - To transform any binary classifier into one satisfying fairness constraints; Quick check - Confirm the reduction algorithm converges to feasible solutions
- **Spearman's correlation**: Why needed - To measure rank-based similarity of feature importance orderings; Quick check - Values close to 1 indicate stable rankings, values near 0 indicate instability

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Model Training -> Fairness Mitigation -> Explanation Generation -> Stability Analysis

**Critical Path**: The most sensitive sequence is Model Training -> Fairness Mitigation -> Explanation Generation, as each step builds upon the previous one's outputs. The fairness mitigation step is particularly critical because it directly transforms the model's decision boundaries and feature dependencies.

**Design Tradeoffs**: The primary tradeoff is between fairness satisfaction and explanation stability. More aggressive fairness constraints (near-zero disparity) lead to greater explanation instability but better fairness metrics. Simpler models (logistic regression) show more stable explanations but may struggle to achieve strong fairness guarantees compared to complex models.

**Failure Signatures**: When fairness mitigation fails, you'll observe high disparity metrics alongside unstable feature rankings (low Spearman correlation). When explanation stability is prioritized over fairness, disparity metrics remain high while feature rankings stay consistent. The optimal operating point depends on the specific clinical context and stakeholder priorities.

**First Experiments**:
1. Compare Spearman correlation of Shapley rankings before and after fairness mitigation on a single dataset/model combination
2. Calculate equalized odds disparity metrics for the baseline and mitigated models
3. Measure performance degradation (accuracy, recall, FPR) when enforcing different levels of fairness constraints

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on Shapley value-based explanations, limiting generalizability to other explanation methods
- Equalized odds may not capture all relevant fairness definitions for clinical decision-making
- The COMPAS dataset's smaller sample size (7,214) compared to UTI (47,585) may affect the stability of observed effects
- Performance degradation in the UTI dataset raises questions about alternative fairness formulations or model architectures

## Confidence
- **High**: General trend that fairness mitigation reduces equalized odds disparities and affects feature importance stability
- **Medium**: Specific magnitude of performance trade-offs, given limited datasets and clinical context variability
- **Low**: Claims about relative instability across model types, as comparison involves only four architectures

## Next Checks
1. Replicate the analysis using alternative explanation methods (e.g., LIME, integrated gradients) to assess whether Shapley-specific properties drive the observed instability
2. Test additional fairness constraints beyond equalized odds, including demographic parity and individual fairness formulations
3. Evaluate the effect of different sample sizes and class imbalance levels on the relationship between fairness constraints and explanation stability