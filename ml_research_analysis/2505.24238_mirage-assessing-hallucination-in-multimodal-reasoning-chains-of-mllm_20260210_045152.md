---
ver: rpa2
title: 'MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM'
arxiv_id: '2505.24238'
source_url: https://arxiv.org/abs/2505.24238
tags:
- reasoning
- hallucination
- step
- claim
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIRAGE, a benchmark specifically designed
  to assess reasoning-induced hallucinations in multimodal large language models (MLLMs).
  Unlike prior benchmarks, MIRAGE isolates reasoning errors by focusing on questions
  where visual perception is accurate but logical reasoning fails.
---

# MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM

## Quick Facts
- arXiv ID: 2505.24238
- Source URL: https://arxiv.org/abs/2505.24238
- Authors: Bowen Dong; Minheng Ni; Zitong Huang; Guanglei Yang; Wangmeng Zuo; Lei Zhang
- Reference count: 40
- Primary result: MIRAGE benchmark isolates reasoning hallucinations from perception errors in MLLMs; Logos method reduces logical hallucinations through curriculum reinforcement fine-tuning

## Executive Summary
This paper introduces MIRAGE, a benchmark specifically designed to assess reasoning-induced hallucinations in multimodal large language models (MLLMs). Unlike prior benchmarks, MIRAGE isolates reasoning errors by focusing on questions where visual perception is accurate but logical reasoning fails. The benchmark includes 1,329 questions annotated with multi-level reasoning chains, steps, and claims, along with auxiliary hints. It introduces three evaluation metrics—accuracy, factuality, and LLMs hallucination score (LHS)—to comprehensively assess hallucinations at different reasoning levels. Experimental results reveal that model scale, training data quality, and training stages significantly influence logical, fabrication, and factual hallucinations, but show limited improvement in spatial hallucinations, highlighting weaknesses in visual reasoning. To address these challenges, the paper proposes Logos, a method combining curriculum reinforcement fine-tuning and collaborative hint inference, which effectively reduces logical hallucinations and improves answer accuracy. Logos establishes a strong baseline for future research on multimodal reasoning reliability.

## Method Summary
MIRAGE constructs a benchmark of 1,329 questions where three open-source MLLMs consistently generate accurate image descriptions but produce incorrect reasoning chains. The benchmark employs a two-stage curation process to isolate reasoning hallucinations from perception-induced errors. Logos, the proposed mitigation method, combines curriculum reinforcement fine-tuning (CRFT) with collaborative hint inference (CHI). CRFT uses Group Relative Policy Optimization (GRPO) with online reward filtration across multiple training stages, progressively increasing difficulty by filtering questions based on accuracy rewards. CHI provides structured topic-specific and question-specific hints during inference, but only improves performance on CRFT-trained models, not base models. The method achieves state-of-the-art results on MIRAGE while highlighting the persistent challenge of spatial reasoning hallucinations.

## Key Results
- MIRAGE achieves 73.7% reasoning chain accuracy through automated annotation (O3-mini + DeepSeek-R1) at ~$22 cost versus ~$200 for O1-only or ~200 person-hours for manual annotation
- Model scaling (3B→72B) and pretraining quality significantly reduce logical, fabrication, and factual hallucinations but show minimal improvement on spatial hallucinations (29-37% across all models)
- Logos with k=1 stage CRFT improves MIRAGE accuracy by +5.8 over vanilla GRPO, while removing KL-divergence provides additional +2.1 to +6.1 accuracy gains
- CHI provides +1.4 accuracy improvement on CRFT-trained models but only +0.2 on base models, confirming training-dependent hint receptivity

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Isolated Benchmark Construction
MIRAGE isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. The two-stage curation process ensures perceptual accuracy while maintaining reasoning difficulty. This isolation enables targeted diagnosis of multimodal reasoning failures without confounding perception errors.

### Mechanism 2: Curriculum Reinforcement Fine-Tuning (CRFT) with Online Reward Filtration (ORF)
CRFT progressively increases training difficulty while dynamically filtering zero-gradient samples to improve convergence toward logic-consistent reasoning chains. The method operates in stages with ORF discarding samples where all G sampled responses receive identical rewards, preventing gradient degradation. This curriculum approach shapes reasoning behavior more effectively than uniform training.

### Mechanism 3: Collaborative Hint Inference (CHI)
CHI reduces reasoning complexity at inference time by providing external LLM-generated hints. The method generates topic-specific and question-specific hints that are prepended to inputs during inference. Crucially, CHI improves performance only after CRFT training, indicating that optimized models develop the capacity to leverage external guidance effectively.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** GRPO is the core RL algorithm underlying Logos. It samples multiple responses per question and computes advantages relative to the group mean, eliminating the need for a separate value model. Understanding GRPO is essential to grasp how Logos shapes reasoning behavior without expensive value-function training.
  - **Quick check question:** Given 8 sampled responses with rewards [1, 0, 1, 0, 1, 0, 0, 0], what is the advantage of the first response? (Answer: A = (1 - mean([1,0,1,0,1,0,0,0])) / std([1,0,1,0,1,0,0,0]) = (1 - 0.375) / 0.518 ≈ 1.205)

- **Concept: Multi-Granular Evaluation Metrics**
  - **Why needed here:** MIRAGE introduces three metrics—accuracy (final answer), factuality (step/claim correctness), and LLMs Hallucination Score (LHS, whole-chain quality). These metrics reveal that different hallucination types respond differently to scaling and training, informing targeted mitigation strategies.
  - **Quick check question:** If a model achieves 80% step factuality but only 40% final accuracy, what does this suggest about its error pattern? (Answer: The model likely makes critical errors in late reasoning steps or fails to properly integrate intermediate results into the final answer.)

- **Concept: Curriculum Learning for RL Fine-Tuning**
  - **Why needed here:** CRFT adapts curriculum learning to reinforcement learning by dynamically adjusting data difficulty across training stages. This differs from standard RL fine-tuning, which processes all data uniformly. Understanding this progression helps diagnose training dynamics and design stage-specific hyperparameters.
  - **Quick check question:** In CRFT's second stage, why does retaining questions with average reward < 0.5 improve over simply using all remaining data? (Answer: Questions where the model already achieves >50% accuracy provide diminishing gradient signal; harder questions force the model to refine reasoning for edge cases.)

## Architecture Onboarding

- **Component map:** MIRAGE dataset (1,329 questions with 3-tier annotations) -> Evaluation pipeline (accuracy scorer, factuality matcher, LHS judger ensemble) -> CRFT orchestrator (stage-wise data filtration) -> GRPO optimizer (policy πθ, reward function R = Rfmt + Racc, advantage normalization) -> ORF filter (discards uniform-reward samples) -> CHI module (question classifier + hint generator LLM ϕ) -> Optimized policy πθ for response generation

- **Critical path:** 1. Data preparation: Filter MIRAGE questions to create training subset (13K K12 math + 1K text-only from LIMO) 2. Stage 1 CRFT: Train on questions where policy samples ≥1 correct chain; freeze visual encoder 3. Stage k CRFT (k=1 optimal per ablation): Retrain on harder subset (accuracy < 0.5) 4. Inference with CHI: Classify question, generate hints, prepend to input, sample response 5. Evaluation: Compute accuracy, factuality (Fstep, Fclaim), and LHS across MIRAGE test set

- **Design tradeoffs:**
  - GRPO vs. PPO: GRPO eliminates value model overhead but requires more samples per iteration (G=8). Table 11 shows GRPO outperforms PPO by +5.8 MIRAGE accuracy, justifying the sample cost.
  - KL-divergence weight: Removing KL-divergence (vs. 1e-3 or 1e-2) improves MIRAGE accuracy by +2.1 to +6.1 points (Table 13), suggesting reasoning MLLMs diverge significantly from base distributions—a tradeoff favoring hallucination reduction over distribution preservation.
  - Curriculum stages: More stages (k=2,3) yield only marginal gains (+0.1 MIRAGE accuracy) over k=1 (Table 15), indicating diminishing returns from finer difficulty gradation versus added training complexity.
  - Annotation automation: Two-stage automated annotation (O3-mini initialization + DeepSeek-R1 refinement) achieves 73.7% reasoning chain accuracy at ~$22 cost versus ~$200 for O1-only or ~200 person-hours for manual annotation (Table 18).

- **Failure signatures:**
  - Spatial hallucinations persist: Neither model scaling (3B→72B) nor CRFT significantly reduces spatial hallucination rates (29-37% across Qwen variants, Table 6-7). This indicates current architectures lack fundamental visual reasoning capabilities for complex spatial transformations.
  - Training-free methods degrade performance: Self-reflection and visual inference chains reduce accuracy and LHS on base 7B models (Table 2), suggesting insufficient reasoning capacity to leverage introspective prompts.
  - ORF starvation: If ORF is too aggressive, it may discard most samples in late CRFT stages, causing training instability. Monitor discard rate; if >80%, loosen filtration criteria or increase G.
  - Hint over-reliance: Excessive question-specific hints may cause models to ignore visual input, leading to context hallucinations. Limit hints to 2-3 sentences and validate against ground-truth descriptions.

- **First 3 experiments:**
  1. **Baseline establishment:** Evaluate target base model (e.g., Qwen2.5-VL-7B) on MIRAGE to measure initial accuracy, factuality, and LHS. Compute per-hallucination-type rates using the detection pipeline (Fig. 10 prompt) to identify dominant error patterns.
  2. **CRFT ablation:** Train Logos-7B with (a) vanilla GRPO (no curriculum), (b) CRFT without ORF, (c) full CRFT+ORF. Compare convergence speed, final MIRAGE accuracy, and hallucination type distributions to quantify each component's contribution (expected: CRFT +2.0 accuracy, ORF +2.9 accuracy per Tables 14, 12).
  3. **CHI transfer test:** Apply CHI (topic + question hints) to both CRFT-trained and base models. Verify that (a) trained models gain +1-2 MIRAGE accuracy, (b) base models show <0.5 gain, confirming training-dependent hint receptivity. If base models improve substantially, revisit the assumption that training creates unique hint-leverage capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can spatial hallucinations in MLLMs be mitigated through architectural innovations rather than scaling, given that current models show no effective improvement on spatial reasoning despite increases in model scale, data scale, and improved pretraining?
- Basis in paper: [explicit] "Current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities" and "Spatial hallucination does not significantly reduced, which indicates that current MLLMs still show weak visual reasoning capabilities."
- Why unresolved: The paper demonstrates that spatial hallucinations persist across different model sizes (3B-72B) and pretraining data quality (Qwen2-VL vs Qwen2.5-VL), suggesting scaling alone is insufficient. The Logos method also fails to reduce spatial hallucinations significantly.
- What evidence would resolve it: Experiments with specialized visual reasoning architectures (e.g., explicit spatial representation modules, neuro-symbolic spatial reasoners) showing significant reductions in spatial hallucination rates compared to standard MLLM architectures.

### Open Question 2
- Question: Does disabling the KL-divergence term in reinforcement fine-tuning provide consistent benefits across different base models, or is it specific to reasoning-tuned models with larger distribution gaps from their base models?
- Basis in paper: [inferred] The paper removes the KL-divergence term stating "reasoning models have a non-negligible distribution gap with base models" and Appendix D shows performance degrades as KL-divergence weight increases, but only tests on Qwen2.5-VL-7B.
- Why unresolved: Only one base model is tested, and the relationship between KL-divergence sensitivity and the base model's inherent reasoning capabilities remains unexplored.
- What evidence would resolve it: Systematic experiments across multiple base model families (e.g., InternVL, LLaMA-Vision, Phi-3.5-Vision) with varying pre-existing reasoning strengths, measuring performance sensitivity to KL-divergence regularization.

### Open Question 3
- Question: Why do training-free hallucination mitigation methods (self-reflection, visual inference chains) degrade performance on models with insufficient reasoning capabilities rather than providing marginal improvements?
- Basis in paper: [explicit] "Training-free methods like self-reflection [16] and visual inference chain [85] generally degrade both accuracy and LHS on base models without sufficient reasoning capabilities... highlighting their limitations."
- Why unresolved: The paper documents the counterintuitive finding but does not explain the mechanism—whether it's increased exposure to hallucinated content, cascading errors, or computational overhead disrupting reasoning.
- What evidence would resolve it: Ablation studies analyzing intermediate reasoning states before and after training-free interventions, combined with error propagation analysis to identify whether hallucinations compound or whether the methods introduce new failure modes.

## Limitations

- Benchmark isolation assumption may not be absolute, as the paper acknowledges this boundary is not perfect and automated curation may include samples with subtle perceptual errors influencing reasoning chains
- Training data generalization is limited, as Logos trains on K12-level math problems (13K samples) and small LEMO subset (~1K), which may not fully represent MIRAGE's broader reasoning complexity
- Evaluation methodology relies heavily on LLM judges, introducing potential judge bias and variability without full specification of inter-judge agreement rates or selection criteria for reference chains

## Confidence

**High Confidence**: The experimental findings that model scale (3B→72B) and training stages significantly improve logical, fabrication, and factual hallucination reduction are well-supported by ablation studies and consistent across multiple Qwen variants. The CRFT method's superiority over vanilla GRPO (+5.8 MIRAGE accuracy) and effectiveness of removing KL-divergence (+2.1 to +6.1 accuracy) are directly evidenced in Tables 11 and 13.

**Medium Confidence**: The claim that spatial hallucinations resist both scaling and training is strongly supported by the data (29-37% across all models) but lacks mechanistic explanation. The CHI method's training-dependent effectiveness is demonstrated through ablation (trained +1.4 accuracy vs base +0.2), but the underlying reason why training creates hint-receptivity remains speculative.

**Low Confidence**: The assertion that MIRAGE "isolates" reasoning hallucinations from perception-induced errors is the most uncertain claim, as the paper itself acknowledges this isolation is imperfect. The benchmark construction methodology provides strong evidence but cannot definitively prove complete separation of these hallucination types.

## Next Checks

1. **Spatial Reasoning Investigation**: Conduct targeted experiments on MIRAGE's spatial taxonomy using models with enhanced visual processing (e.g., Gemini-2.0-Flash-Exp) to determine whether spatial hallucination persistence is architecture-dependent or fundamental to current MLLM designs.

2. **Cross-Dataset Generalization Test**: Evaluate Logos-trained models on external multimodal reasoning benchmarks (e.g., MMMU, VQAv2) to assess whether CRFT's gains transfer beyond MIRAGE's specific question distribution or represent overfitting to the curated dataset.

3. **Judge Agreement Analysis**: Implement inter-annotator agreement metrics (Cohen's kappa) for LHS scoring across the three LLM judges and validate against human annotations on a subset of 100 samples to quantify evaluation reliability and potential bias in hallucination detection.