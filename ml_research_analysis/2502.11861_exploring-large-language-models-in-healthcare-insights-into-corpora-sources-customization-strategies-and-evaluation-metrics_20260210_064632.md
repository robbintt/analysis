---
ver: rpa2
title: 'Exploring Large Language Models in Healthcare: Insights into Corpora Sources,
  Customization Strategies, and Evaluation Metrics'
arxiv_id: '2502.11861'
source_url: https://arxiv.org/abs/2502.11861
tags:
- llms
- evaluation
- studies
- metrics
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review identified significant gaps in the fairness
  and reliability of healthcare Large Language Models (LLMs). Most studies relied
  on unverified or unstructured data sources, such as web-crawled content and synthetic
  datasets, leading to potential biases from geographic, cultural, and socio-economic
  factors.
---

# Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics

## Quick Facts
- **arXiv ID:** 2502.11861
- **Source URL:** https://arxiv.org/abs/2502.11861
- **Reference count:** 40
- **Primary result:** Most healthcare LLM studies rely on unverified data sources, leading to fairness and reliability gaps in clinical applications.

## Executive Summary
This scoping review systematically analyzed 61 healthcare LLM studies to identify gaps in corpus sources, customization strategies, and evaluation metrics. The review found that 90% of studies used unverified data sources like web-crawled content, potentially introducing geographic, cultural, and socioeconomic biases. Most evaluation metrics focused on task-specific accuracy without standardized frameworks for clinical applicability. The authors propose developing tiered corpus architectures with vetted sources and comprehensive evaluation systems addressing clinical semantic accuracy, safety, and fairness.

## Method Summary
The review followed JBI/PRISMA-ScR framework across PubMed/MEDLINE, Embase, Scopus, and Web of Science databases, plus arXiv preprints. Search terms included "large language model", "GPT", "LLaMA", "medicine", and "healthcare". Studies from 2021-2024 were included if they focused on LLMs (excluding GPT-2) for question-answering tasks in healthcare. Data extraction categorized corpus types (4 categories), base models, customization techniques (6 categories), and evaluation metrics. The minimum viable reproduction requires executing the search strategy, deduplicating and screening results, then performing full-text analysis to extract data according to the defined schema.

## Key Results
- 90% of studies used unverified data sources like web-crawls and synthetic datasets
- Most evaluation metrics were limited to task-specific accuracy and F1 scores without standardized clinical frameworks
- Combining multiple data sources improved model performance but increased complexity in data management
- There is a critical need for evidence-based clinical guidelines integration and fairness-aware evaluation systems

## Why This Works (Mechanism)
The review's systematic approach reveals fundamental gaps in healthcare LLM development by categorizing data sources, techniques, and metrics across a representative sample. The methodology exposes how unverified data sources create bias cascades that propagate through customization strategies to evaluation outcomes. By identifying the predominance of task-specific accuracy metrics over clinical applicability measures, the review highlights why current LLMs struggle with real-world deployment despite strong benchmark performance.

## Foundational Learning
- **Corpus Source Classification:** Understanding verified vs. unverified data sources is critical because healthcare applications require evidence-based, bias-minimized training data. Quick check: Verify data source credibility through peer-reviewed validation or established clinical databases.
- **Customization Technique Taxonomy:** Different techniques (fine-tuning, RAG, prompt engineering) have distinct data requirements and bias amplification patterns. Quick check: Match customization method to data characteristics and task requirements.
- **Evaluation Metric Limitations:** Task-specific accuracy doesn't capture clinical semantic accuracy or safety concerns. Quick check: Supplement accuracy metrics with clinical validity assessments and fairness audits.

## Architecture Onboarding
- **Component Map:** Corpus Sources → Customization Techniques → Evaluation Metrics → Clinical Deployment
- **Critical Path:** Verified data sources → Evidence-based fine-tuning → Standardized clinical evaluation → Real-world validation
- **Design Tradeoffs:** Performance vs. bias mitigation (larger unverified datasets may improve metrics but increase bias), complexity vs. interpretability (complex customization may reduce transparency), comprehensiveness vs. standardization (broad metrics may lack actionable insights)
- **Failure Signatures:** High benchmark accuracy with poor clinical performance, geographic/cultural bias in outputs, safety incidents in deployment
- **First Experiments:** 1) Audit current corpus sources for verification status, 2) Implement bias detection on existing training data, 3) Pilot standardized clinical evaluation framework on one model

## Open Questions the Paper Calls Out
The review identifies several critical open questions that need addressing: How can tiered corpus architectures be practically implemented to balance performance and bias mitigation? What standardized frameworks can evaluate clinical semantic accuracy beyond task-specific metrics? How can dynamic weighting strategies be developed to handle multiple data source types effectively? What mechanisms can ensure fairness across diverse patient populations in LLM outputs?

## Limitations
- Temporal drift may affect the prevalence statistics as the field rapidly evolves beyond 2024
- Classification of "unverified" data sources may be ambiguous in edge cases without full inclusion criteria
- Exclusion of non-English studies and non-QA tasks may underestimate broader LLM applications in healthcare

## Confidence
- **Prevalence statistics (90% unverified data):** Medium - potential temporal drift and search term limitations
- **Classification methodology:** High - systematic and transparent approach
- **Proposed solutions (tiered architectures):** Medium - lacks specific implementation guidance
- **Clinical applicability findings:** High - well-supported by evidence

## Next Checks
1. Execute updated search across the same databases with date range clamped to 2021-2024 to verify current prevalence statistics
2. Audit one healthcare LLM study to classify corpus sources according to the paper's taxonomy and verify the "unverified" designation criteria
3. Pilot the proposed standardized clinical evaluation framework on a small sample of existing healthcare LLM implementations