---
ver: rpa2
title: Towards Human Engagement with Realistic AI Combat Pilots
arxiv_id: '2509.26002'
source_url: https://arxiv.org/abs/2509.26002
tags:
- agents
- combat
- learning
- human
- vr-f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework enabling real-time human
  interaction with AI-controlled fighter jets in realistic 3D air combat scenarios.
  The approach combines Multi-Agent Reinforcement Learning (MARL) trained in a custom
  JSBSim-based environment with deployment into VR-Forces via the IEEE DIS protocol.
---

# Towards Human Engagement with Realistic AI Combat Pilots

## Quick Facts
- arXiv ID: 2509.26002
- Source URL: https://arxiv.org/abs/2509.26002
- Reference count: 16
- Primary result: Novel framework enabling real-time human-AI air combat teams with 80% win rate

## Executive Summary
This paper presents a novel framework enabling real-time human interaction with AI-controlled fighter jets in realistic 3D air combat scenarios. The approach combines Multi-Agent Reinforcement Learning (MARL) trained in a custom JSBSim-based environment with deployment into VR-Forces via the IEEE DIS protocol. The system supports three distinct control policies (Attack, Engage, Defend) selected by a commander policy, achieving an 80% win rate in 10-vs-10 combat scenarios. The integration allows mixed human-AI teams and creates new opportunities for tactical training and exploration of innovative combat strategies while maintaining human oversight in safety-critical domains.

## Method Summary
The framework trains MARL agents in a custom JSBSim-based environment using MA-PPO with Actor-Critic networks under the CTDE paradigm. Three specialized control policies (attack, engage, defend) are learned and selected by a commander policy. Trained policies deploy to VR-Forces via IEEE DIS protocol, supporting real-time human-AI interaction through cockpit controllers. The system uses self-play with curriculum learning, transitioning from 1-vs-1 to 10-vs-10 scenarios.

## Key Results
- 80% win rate achieved in 10-vs-10 air combat scenarios against mixed-strategy opponents
- Hierarchical policy selection enables strategic adaptation across attack, engage, and defend modes
- Real-time human-AI teaming demonstrated through VR-Forces integration with cockpit interface support

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical policy selection enables strategic adaptation and improved multi-agent coordination.
- Mechanism: A commander policy (πc) selects among three specialized control policies—attack (πa), engage (πe), defend (πd)—allowing agents to shift behavior based on tactical context rather than committing to a single strategy.
- Core assumption: Distinct behavioral modes are learnable and sufficiently disjoint that switching between them yields better outcomes than any single monolithic policy.
- Evidence anchors:
  - [Section 2]: "Each agent further learns a commander policy πc that decides which control policy to activate... agents with πc achieve an 80% win rate in 10-vs-10 air combats."
  - [Corpus]: "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning" supports hierarchical frameworks for identifying effective Courses of Action in combat scenarios.
- Break condition: If control policies overlap significantly in behavior, or if switching overhead introduces latency that disrupts coordination, hierarchical selection may not outperform flat policies.

### Mechanism 2
- Claim: Simulation-to-simulation transfer via DIS protocol preserves trained agent behavior across environments with differing physics fidelity.
- Mechanism: Agents train in JSBSim (100Hz flight dynamics) and deploy to VR-Forces via standardized DIS Protocol Data Units, with two transfer modes: (i) simulate locally in JSBSim and transmit state, or (ii) transmit actions and let VR-Forces simulate dynamics.
- Core assumption: The action space and state representations are sufficiently aligned between environments that policy transfer does not require domain adaptation.
- Evidence anchors:
  - [Section 3]: "Two types of DIS messages are supported... simulate the agent's dynamics locally in JSBSim and transmit the updated state to VR-F... or transmit the computed action to VR-F."
  - [Corpus]: Limited direct corpus support for DIS-based transfer; related work (e.g., BVR Gym, JSBSim+Gymnasium approaches) focuses on training environments rather than cross-simulator deployment.
- Break condition: If JSBSim and VR-Forces dynamics diverge significantly (e.g., different stall behavior, thrust modeling), agents trained in one may fail in the other without recalibration.

### Mechanism 3
- Claim: Centralized Training with Decentralized Execution (CTDE) enables coordination during training while permitting independent agent operation at deployment.
- Mechanism: During training, critics access global state information; at execution, each agent runs its policy independently using only local observations, scaling to varying m-vs-n scenarios.
- Core assumption: Global information during training yields policies that generalize to decentralized execution without performance collapse.
- Evidence anchors:
  - [Section 2]: "CTDE paradigm... with global information during training while allowing independent agent execution in any m-vs-n combat scenario."
  - [Corpus]: "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics" notes MARL enables coordination but deployment is constrained by explainability—suggesting CTDE is established but not fully solved for sensitive contexts.
- Break condition: If agents develop implicit reliance on global state (e.g., via credit assignment artifacts), decentralized execution may underperform, particularly in asymmetric or partially observable scenarios.

## Foundational Learning

- Concept: Markov Games (Multi-Agent MDPs)
  - Why needed here: The paper models air combat as a Markov Game with tuple (N, S, {A_i}, P, ρ, {R_i}, γ); understanding joint actions and transition dynamics is prerequisite to grasping MARL formulation.
  - Quick check question: Can you explain how a joint action a_t = (a¹_t, ..., aⁿ_t) affects state transitions differently than a single-agent MDP?

- Concept: Actor-Critic with MA-PPO
  - Why needed here: Training uses MA-PPO (Multi-Agent Proximal Policy Optimization) with actor-critic networks; understanding advantage estimation and policy updates is essential for debugging training.
  - Quick check question: What role does the critic play in variance reduction during policy gradient estimation?

- Concept: IEEE DIS Protocol (Distributed Interactive Simulation)
  - Why needed here: The integration layer relies on DIS PDU serialization over UDP/IP; without this, deployment to VR-Forces is impossible.
  - Quick check question: What are the tradeoffs between transmitting state vs. transmitting actions via EntityStatePdu in a real-time simulation?

## Architecture Onboarding

- Component map: Training Environment -> Policy Architecture -> Communication Layer -> Deployment Target -> In-Development
- Critical path:
  1. Train agents in JSBSim environment with curriculum learning + self-play
  2. Export trained policies (actor networks for πa, πe, πd, πc)
  3. Initialize DIS connection (UDP socket, receiver thread)
  4. On state update: parse PDU → select action via πc(πa, πe, πd) → encode and send PDU to VR-F
  5. Human users engage via VR-F cockpit/controllers; AI agents respond in real-time

- Design tradeoffs:
  - Local JSBSim simulation vs. VR-F dynamics: JSBSim offers higher fidelity (100Hz) but requires state synchronization; VR-F simulation reduces sync overhead but may introduce physics divergence.
  - Self-play vs. human feedback: Self-play yields competitive agents but may produce unrealistic maneuvers; future imitation learning aims to incorporate pilot feedback.
  - Centralized vs. decentralized critic: CTDE enables coordination but risks over-reliance on global state during training.

- Failure signatures:
  - Agents circle without engaging: Likely reward shaping issue or insufficient curriculum progression
  - Sudden agent freezes in VR-F: DIS PDU parsing failure or UDP packet loss; check socket buffer and PDU filter conditions
  - Win rate drops at deployment: Physics mismatch between JSBSim and VR-F; verify action space alignment
  - Commander policy oscillates rapidly between πa/πe/πd: Insufficient temporal smoothing or conflicting reward signals

- First 3 experiments:
  1. **Baseline validation**: Run 10-vs-10 self-play in JSBSim-only mode; verify 80% win rate replicates with commander policy against mixed-strategy opponents.
  2. **DIS latency test**: Measure round-trip time from JSBSim state generation → DIS PDU → VR-F state update; confirm <100ms for real-time engagement.
  3. **Single human vs. AI trial**: Deploy one human-controlled entity in VR-F against a single AI agent; log action trajectories and compare to expected πa/πe/πd behavior profiles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid algorithms combining Multi-Agent Reinforcement Learning (MARL) with supervised imitation learning significantly improve model realism and performance compared to pure self-play?
- Basis in paper: [explicit] The authors state an intention to "go beyond self-play to imitation learning and investigate hybrid algorithms that combine MARL with supervised learning techniques."
- Why unresolved: The current system relies primarily on self-play and curriculum learning; the integration of human behavioral data for supervised learning remains a planned future step.
- What evidence would resolve it: A comparative study showing convergence rates and win rates of hybrid models versus baseline MARL models, alongside qualitative assessments of "realism" by human pilots.

### Open Question 2
- Question: How do air combat strategies and trajectories differ when evolved purely by AI compared to those emerging from human-AI collaboration?
- Basis in paper: [explicit] The authors explicitly propose to "explore how air combat strategies evolve when purely driven by AI compared to human-AI collaboration, with a focus on imaginative trajectories."
- Why unresolved: While the system supports interaction, the specific analysis of strategic divergence and the discovery of novel, non-human intuitions in these distinct teaming configurations has not yet been conducted.
- What evidence would resolve it: Tactical analysis of trajectory data from AI-only teams versus mixed human-AI teams, identifying specific maneuvers that deviate from established human doctrine.

### Open Question 3
- Question: Can MARL agents be trained directly within VR-Forces using a split architecture while maintaining sufficient iteration rates for effective learning?
- Basis in paper: [explicit] The authors note they are "developing a system that trains MARL agents directly within VR-F" using a split architecture, targeting an iteration rate of about 10 Hz.
- Why unresolved: Current training occurs in a custom JSBSim environment (100 Hz); the feasibility and stability of training within the more complex VR-Forces environment at lower rates are under investigation.
- What evidence would resolve it: Demonstration of successful policy convergence within the VR-Forces integration, benchmarked against the performance of agents trained in the high-frequency JSBSim environment.

## Limitations

- Physics transfer gap between JSBSim (100Hz) and VR-Forces environments lacks systematic validation
- Critical training hyperparameters (network architectures, learning rates, batch sizes) remain unspecified
- Limited evaluation of human-AI collaboration quality and tactical effectiveness

## Confidence

- **High Confidence**: Hierarchical policy framework (commander selecting attack/engage/defend) and CTDE training approach are technically sound and well-supported by established MARL literature.
- **Medium Confidence**: 80% win rate claim is based on 10-vs-10 self-play scenarios but lacks cross-validation against human pilots or alternative opponent strategies.
- **Low Confidence**: Claims about real-time human engagement and innovative combat strategy exploration are largely aspirational given limited empirical evaluation of mixed human-AI teams.

## Next Checks

1. **Cross-Environment Validation**: Conduct controlled experiments comparing agent performance in JSBSim-only vs. DIS-to-VR-Forces deployment to quantify physics transfer impact.

2. **Human Factors Evaluation**: Run user studies with experienced pilots operating alongside AI agents to assess collaboration quality, trust, and tactical effectiveness.

3. **Policy Robustness Testing**: Evaluate agent performance across varying scenario complexities (3-vs-3, 5-vs-5, asymmetric engagements) to verify hierarchical policy selection generalizes beyond 10-vs-10 baseline.