---
ver: rpa2
title: Question Generation for Assessing Early Literacy Reading Comprehension
arxiv_id: '2507.22410'
source_url: https://arxiv.org/abs/2507.22410
tags:
- question
- generation
- reading
- comprehension
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YourBench4Edu, a framework for generating
  comprehension questions tailored to K-2 English learners. The system adapts YourBench
  to early literacy education by processing learning materials, summarizing them,
  and generating diverse question types at varying difficulty levels.
---

# Question Generation for Assessing Early Literacy Reading Comprehension

## Quick Facts
- arXiv ID: 2507.22410
- Source URL: https://arxiv.org/abs/2507.22410
- Reference count: 0
- Primary result: YourBench4Edu framework generates K-2 reading comprehension questions with MAP@N scores exceeding prior work on Rouge-L F1 metrics

## Executive Summary
This paper introduces YourBench4Edu, a framework for generating comprehension questions tailored to K-2 English learners. The system adapts YourBench to early literacy education by processing learning materials, summarizing them, and generating diverse question types at varying difficulty levels. It uses language models to create single-shot or multi-hop questions based on segmented text chunks. Evaluation on the FairytaleQA dataset using Llama-3.3-70B-Instruct, Qwen3-235B-A22B, and QwQ-32B models showed strong performance, with MAP@N scores surpassing prior work in Rouge-L F1 and maintaining competitive results in BERTScore F1. The approach supports autonomous AI-driven English instruction by enabling educators to quickly generate assessment materials and adapt content to learner proficiencies.

## Method Summary
YourBench4Edu processes learning materials through a four-stage pipeline: ingestion (normalizing PDF/HTML/MD to markdown via LLM), summarization (chunking text, summarizing each piece, and integrating into structured summaries), segmentation (creating question-generation chunks by length or sentence similarity), and question generation (prompting LLM with chunk context, summaries, question types, and target difficulty). For evaluation, the pipeline identifies the most relevant chunk given ground-truth Q&A pairs and generates questions with ground-truth answers as additional constraints. The framework supports both single-shot questions (from one chunk) and multi-hop questions (synthesizing information across multiple chunks) to create difficulty scaling for K-2 learners.

## Key Results
- QwQ-32B model achieved 0.573 MAP@10 Rouge-L F1, outperforming Llama-3.3-70B-Instruct and Qwen3-235B-A22B
- Strong MAP@N scores across all N values (1, 3, 5, 10) on FairytaleQA validation and test sets
- Competitive BERTScore F1 results despite weaker performance compared to Rouge-L F1
- Demonstrated ability to generate diverse question types (true-false, factual, analytical) at various difficulty levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical text processing (ingestion → summarization → segmentation) enables comprehensive material coverage for question generation.
- **Mechanism:** The pipeline first normalizes input formats to markdown, then chunks documents and generates intermediate summaries per chunk before integrating them. This creates structured representations that preserve content coverage while making long documents tractable for LLM processing. Segmentation then subdivides text based on length or sentence similarity to create granular units for targeted question generation.
- **Core assumption:** Summarization preserves the pedagogically relevant information needed for assessment questions; chunk boundaries align with meaningful semantic units.
- **Evidence anchors:**
  - [abstract] "Our method ensures complete coverage of the underlying material"
  - [section] "The summarization component chunks each ingested text document into pieces, prompts a language model to summarize each piece, and finally prompts the model to integrate the list of summaries into one well-structured text."
  - [corpus] Weak direct evidence—neighbor papers focus on question difficulty estimation and personalization, not chunking strategies specifically.
- **Break condition:** If summarization loses key details that appear in mid-chunk positions, or if chunk boundaries split related concepts, generated questions may miss critical comprehension targets.

### Mechanism 2
- **Claim:** Distinguishing single-shot and multi-hop question generation creates controllable difficulty scaling for early literacy assessment.
- **Mechanism:** Single-shot questions draw from one chunk (lower cognitive load), while multi-hop questions synthesize information across multiple chunks (higher cognitive load). This architectural choice maps directly to Bloom's taxonomy-style difficulty progression without requiring explicit difficulty classifiers.
- **Core assumption:** K-2 learners' comprehension can be validly assessed through both literal (single-chunk) and inferential (cross-chunk) questions; the chunk granularity is appropriate for the target age group.
- **Evidence anchors:**
  - [abstract] "generate a large diversity of question types at various difficulty levels"
  - [section] "each single-shot question is based on only one chunk, while each multi-hop question is raised based on multiple chunks"
  - [corpus] Neighbor paper "Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty" suggests LLMs can estimate difficulty, but does not validate the single-shot/multi-hop distinction specifically.
- **Break condition:** If multi-hop questions require working memory exceeding K-2 developmental capacity, or if chunks are too granular/coarse, the difficulty calibration fails.

### Mechanism 3
- **Claim:** Constrained LLM generation with predefined question types and ground-truth answer anchoring produces educationally valid assessments.
- **Mechanism:** The system provides the LLM with (1) chunked context, (2) summaries, (3) predefined question types (true-false, factual, analytical), (4) difficulty targeting instructions, and (5) ground-truth answers during evaluation. These constraints reduce hallucination and ensure answerability while maintaining generation flexibility.
- **Core assumption:** LLMs can follow pedagogical constraints reliably; ground-truth answer anchoring does not artificially inflate metric scores without improving educational validity.
- **Evidence anchors:**
  - [abstract] "adapts to the learner's specific proficiencies, and can generate a large diversity of question types"
  - [section] "the ground-truth answer is provided as an additional instruction in the question generation prompt"
  - [corpus] "Generating Reading Comprehension Exercises with Large Language Models for Educational Applications" corroborates LLM potential for adaptive content creation but notes quality/reliability concerns remain.
- **Break condition:** If constraint specification becomes overly rigid, output diversity suffers; if too loose, questions may drift from educational objectives or become unanswerable from source text.

## Foundational Learning

- **Concept: MAP@N (Mean Average Precision at N)**
  - **Why needed here:** The primary evaluation metric for comparing generated questions against reference questions. Understanding this metric is essential for interpreting the performance claims (e.g., QwQ-32B achieving 0.573 MAP@10 Rouge-L F1).
  - **Quick check question:** If a system generates 10 candidate questions and only the 7th matches the reference, how would this affect MAP@10 versus MAP@1?

- **Concept: Rouge-L F1 vs. BERTScore F1**
  - **Why needed here:** The paper uses both metrics—Rouge-L captures lexical overlap (exact n-gram matching), while BERTScore captures semantic similarity via embeddings. The discrepancy between strong Rouge-L results and competitive (not best) BERTScore results suggests the generated questions differ lexically but remain semantically aligned.
  - **Quick check question:** Why might a paraphrased question score lower on Rouge-L but higher on BERTScore compared to the reference?

- **Concept: Dialogic Reading**
  - **Why needed here:** The pedagogical foundation of this work. Understanding that the goal is not just question generation but supporting adult-child conversational interactions around text clarifies why question diversity and difficulty adaptation matter.
  - **Quick check question:** How does a true-false question versus an analytical question serve different functions in dialogic reading?

## Architecture Onboarding

- **Component map:** Input (PDF/HTML/MD) → Ingestion → Summarization → Segmentation → Question Generation → Output: Question-Answer pairs

- **Critical path:** Summarization quality → Segmentation appropriateness → Question generation prompt design. Errors propagate forward; poor summarization cannot be recovered by better prompting.

- **Design tradeoffs:**
  - **Chunk granularity:** Smaller chunks = more precise targeting but risk fragmenting concepts; larger chunks = better context but may overwhelm LLM context windows and reduce question specificity.
  - **Model selection:** The paper shows QwQ-32B (smaller) outperforming Llama-3.3-70B-Instruct and Qwen3-235B-A22B on Rouge-L, suggesting model size alone doesn't determine quality—prompt compatibility and fine-tuning history may matter more.
  - **Evaluation mode:** Providing ground-truth answers during generation (as done for validation) vs. answer-agnostic generation (production mode) represents a capability/evaluation gap.

- **Failure signatures:**
  - Questions unanswerable from source text (hallucination)
  - All generated questions clustering around one chunk (coverage failure)
  - Difficulty mismatch with K-2 level (e.g., vocabulary, sentence complexity)
  - High Rouge-L but low human educational utility (metric gaming)

- **First 3 experiments:**
  1. **Baseline coverage test:** Run YourBench4Edu on a single FairytaleQA story; manually verify that generated questions span all major narrative events (not just the beginning or most prominent chunks).
  2. **Difficulty calibration check:** Generate 20 questions each at "easy" and "hard" settings; have an educator blind-rate appropriateness for K-2 learners to validate that system-controlled difficulty translates to pedagogical difficulty.
  3. **Ablation on summarization:** Compare question quality (Rouge-L/BERTScore) with vs. without the summarization component to quantify its contribution to the pipeline.

## Open Questions the Paper Calls Out

- **Question 1:** How well do automated metrics (Rouge-L F1, BERTScore F1) correlate with human judgments of question quality and pedagogical appropriateness for K-2 learners?
  - **Basis in paper:** [inferred] The paper relies entirely on automated metrics for evaluation, with no human evaluation by educators or testing with actual K-2 students to validate that generated questions are developmentally appropriate or effective for assessment.
  - **Why unresolved:** Automated metrics capture surface-level similarity to reference questions but cannot assess whether questions are comprehensible to young children, appropriately scaffolded, or pedagogically sound.
  - **What evidence would resolve it:** A human evaluation study where educators rate generated questions on clarity, age-appropriateness, and alignment with literacy learning objectives.

- **Question 2:** How does question generation quality differ when ground-truth answers are unavailable (the intended real-world use case)?
  - **Basis in paper:** [inferred] The validation methodology explicitly modifies the pipeline to use ground-truth answers as additional instruction, which differs from the claimed use case where educators generate questions from arbitrary materials without pre-existing QA pairs.
  - **Why unresolved:** The paper does not report results using the vanilla pipeline without ground-truth guidance, leaving unclear whether the strong results generalize to the actual deployment scenario.
  - **What evidence would resolve it:** A comparison experiment evaluating question quality between the modified pipeline (with ground truth) and the standard pipeline (without ground truth) on the same source materials.

- **Question 3:** Can the framework accurately calibrate question difficulty to specific proficiency levels within the K-2 range?
  - **Basis in paper:** [inferred] The abstract claims adaptation to "learner's specific proficiencies" and "various difficulty levels," but the evaluation does not assess whether difficulty distinctions are valid or whether questions match target proficiency levels.
  - **Why unresolved:** No analysis is provided on whether questions labeled for different difficulty levels are measurably distinct or appropriately challenging for different student subgroups.
  - **What evidence would resolve it:** A study where K-2 students at different reading levels attempt generated questions, with analysis of success rates across difficulty tiers.

- **Question 4:** Does question type (factual, analytical, true-false, single-shot, multi-hop) significantly affect generation quality or student learning outcomes?
  - **Basis in paper:** [inferred] The paper claims "a large diversity of question types" but reports only aggregate metrics, without disaggregating performance by question type or analyzing which types are most suitable for early literacy assessment.
  - **Why unresolved:** Different question types may have different validity for assessing comprehension in young learners, but this remains unexamined.
  - **What evidence would resolve it:** Fine-grained evaluation results broken down by question type, plus pedagogical analysis of which types best capture reading comprehension skills.

## Limitations

- The evaluation relies on ground-truth answers during generation, which may not reflect real-world deployment where answers must be derived from text, creating a capability/evaluation gap.
- The paper focuses on automated metrics rather than human evaluation of educational effectiveness with K-2 learners, leaving the actual pedagogical impact unvalidated.
- The chunk-based segmentation assumes semantic units align with fixed-length or similarity-based boundaries without systematic analysis of whether this optimally captures comprehension-relevant content for early literacy.

## Confidence

- **High confidence:** The hierarchical processing pipeline (ingestion → summarization → segmentation) is technically sound and the MAP@N metric calculation methodology is correctly implemented. The performance improvements over baseline systems are statistically measurable.
- **Medium confidence:** The claim that multi-hop questions appropriately scale difficulty for K-2 learners is theoretically justified but lacks direct developmental validation. The assumption that chunk boundaries align with meaningful comprehension units is reasonable but untested.
- **Low confidence:** The practical effectiveness of the system for actual early literacy instruction is not demonstrated—the evaluation is limited to automated metrics on a specific dataset without human evaluation of educational quality or classroom integration.

## Next Checks

1. **Ground-truth-free evaluation:** Re-run the question generation pipeline without providing ground-truth answers in prompts to assess real-world performance and identify any degradation in quality or increase in hallucination.
2. **K-2 educator validation:** Have early childhood literacy educators review 50 randomly sampled questions across difficulty levels to assess age-appropriateness, answerability, and alignment with early literacy learning objectives.
3. **Cross-dataset generalization:** Test the trained pipeline on an independent early literacy comprehension dataset (e.g., children's storybook questions not from FairytaleQA) to evaluate whether the approach generalizes beyond the specific corpus used for development.