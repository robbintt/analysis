---
ver: rpa2
title: Self-Supervised Inductive Logic Programming
arxiv_id: '2507.16405'
source_url: https://arxiv.org/abs/2507.16405
tags:
- examples
- poker
- learning
- negative
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a self-supervised learning setting for Inductive
  Logic Programming (ILP) and presents Poker, a new MIL system that learns from positive
  and unlabelled examples while automatically generating new positive and negative
  examples during learning. The key innovation is the use of Second-Order Definite
  Normal Forms (SONFs) as a maximally general second-order background theory, eliminating
  the need for problem-specific background theories and negative examples.
---

# Self-Supervised Inductive Logic Programming

## Quick Facts
- arXiv ID: 2507.16405
- Source URL: https://arxiv.org/abs/2507.16405
- Reference count: 8
- Key outcome: Introduces Poker, a MIL system using SONFs as maximally general background theory for self-supervised learning from positive and unlabelled examples

## Executive Summary
This paper presents a novel self-supervised learning framework for Inductive Logic Programming (ILP) that eliminates the need for hand-crafted negative examples and problem-specific background theories. The Poker system uses Second-Order Definite Normal Forms (SONFs) as a maximally general background theory, automatically generating positive and negative examples during learning. The key innovation is detecting contradictions between examples accepted by the same hypothesis, enabling iterative refinement of learned programs. Experimental results show Poker successfully learns Context-Free and L-System grammars with increasing accuracy as more automatically generated examples are provided.

## Method Summary
The paper introduces a self-supervised learning setting for Inductive Logic Programming (ILP) and presents Poker, a new MIL system that learns from positive and unlabelled examples while automatically generating new positive and negative examples during learning. The key innovation is the use of Second-Order Definite Normal Forms (SONFs) as a maximally general second-order background theory, eliminating the need for problem-specific background theories and negative examples. Poker's algorithm detects contradictions between examples accepted by the same hypothesis to iteratively refine its learning. Experiments show that Poker's performance improves with increasing numbers of automatically generated examples, achieving high accuracy in learning Context-Free and L-System grammars, while a baseline system without negative examples over-generalizes. This work demonstrates the feasibility of self-supervised ILP using general background theories and automatic example generation.

## Key Results
- Poker successfully learns Context-Free and L-System grammars without hand-crafted negative examples
- System performance improves monotonically with increasing numbers of automatically generated examples
- Poker outperforms baseline MIL system without negative examples, which over-generalizes
- SONFs serve as effective maximally general background theories, eliminating need for problem-specific background knowledge

## Why This Works (Mechanism)
Poker works by leveraging contradictions between examples accepted by the same hypothesis. When a hypothesis H accepts both a positive example e+ and a negative example e-, the contradiction indicates that H is too general and needs refinement. The system iteratively generates new examples and uses this contradiction detection mechanism to progressively narrow down the hypothesis space. By using SONFs as background theory, Poker operates with a maximally general framework that doesn't require domain-specific knowledge, making it truly self-supervised.

## Foundational Learning
- **Inductive Logic Programming (ILP)**: Machine learning paradigm where logical programs are learned from examples and background knowledge. Why needed: Provides the theoretical foundation for learning relational representations. Quick check: Can you explain the difference between propositional and relational learning?
- **Second-Order Definite Normal Forms (SONFs)**: A generalization of normal forms to second-order logic that provides maximally general background theories. Why needed: Enables truly self-supervised learning without problem-specific background knowledge. Quick check: Can you derive a SONF for simple arithmetic predicates?
- **Multi-Example Learning (MIL)**: Learning framework where multiple examples are processed simultaneously rather than individually. Why needed: Enables detection of contradictions between accepted examples. Quick check: How does MIL differ from traditional ILP's single-example approach?
- **Herbrand Semantics**: Logical framework where constants are drawn from a finite set and interpretations are defined over Herbrand base. Why needed: Provides the semantic foundation for evaluating logical hypotheses. Quick check: Can you construct the Herbrand base for a simple predicate logic program?
- **Contradiction Detection**: Mechanism for identifying when a hypothesis accepts both positive and negative examples. Why needed: Drives the iterative refinement of hypotheses in Poker's learning algorithm. Quick check: How would you implement contradiction detection for a set of accepted examples?

## Architecture Onboarding

**Component Map**: User Input -> SONF Background Theory -> Hypothesis Generation -> Example Acceptance -> Contradiction Detection -> Refinement Loop -> Learned Program

**Critical Path**: The learning algorithm iteratively generates hypotheses from the SONF background theory, accepts examples, detects contradictions, and refines the hypothesis space until convergence.

**Design Tradeoffs**: Uses maximally general background theories (SONFs) at the cost of potentially generating many candidate hypotheses; trades manual negative example creation for automatic generation through contradiction detection.

**Failure Signatures**: System may over-generalize if insufficient contradictions are detected; performance degrades with poorly chosen unlabelled examples; computational complexity increases with larger Herbrand bases.

**First Experiments**:
1. Verify contradiction detection works by creating a simple hypothesis that accepts both positive and negative examples
2. Test hypothesis generation from a basic SONF with a small set of predicates
3. Evaluate learning performance on a toy grammar with manually verified ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the derivation of Second-Order Definite Normal Forms (SONFs) be automated?
- Basis in paper: [explicit] The paper states in the Framework section: "Derivation of a SONF is a process of abstraction that we do not currently know how to automate."
- Why unresolved: Currently, the SONFs (C-GNF and LNF) used in the experiments were manually derived by the author based on existing normal form definitions or manual inspection of code.
- What evidence would resolve it: An algorithm capable of generating a suitable SONF for a given class of problems or predicate definitions without manual intervention.

### Open Question 2
- Question: How does Poker perform on diverse domains outside of grammar learning?
- Basis in paper: [explicit] The Conclusion states: "Our empirical results are restricted to grammar learning. Future work should investigate Poker's application to more diverse domains."
- Why unresolved: The experimental evaluation was limited to Context-Free Languages (CFLs) and L-System grammars, leaving the system's efficacy on other logic programming tasks unproven.
- What evidence would resolve it: Successful application and evaluation of Poker on non-grammatical tasks, such as program synthesis, planning, or general knowledge graph completion.

### Open Question 3
- Question: How does the generality of unlabelled examples affect Poker's learning performance?
- Basis in paper: [explicit] The Experiments section notes: "In preliminary experiments we find that the effect of (b), unlabelled examples, on Pokers' learning performance depends on the generality of the unlabelled examples. This merits more thorough investigation."
- Why unresolved: The paper focused on the number of automatically generated examples (k), but did not rigorously test how the scope or distribution of user-provided unlabelled examples (E?) impacts convergence.
- What evidence would resolve it: Ablation studies analyzing learning accuracy and speed when varying the "generality" (e.g., breadth of the search space) of the unlabelled set E?.

### Open Question 4
- Question: What are the theoretical bounds on Poker's computational efficiency?
- Basis in paper: [explicit] The Conclusion mentions: "Our theoretical results can be extended with further proofs... of its computational efficiency."
- Why unresolved: While the paper proves that accuracy increases monotonically, it does not provide a complexity analysis regarding time or space requirements for the hypothesis generation and labelling loop.
- What evidence would resolve it: A theoretical analysis of the algorithm's complexity or empirical scaling laws demonstrating performance as the size of the Herbrand base increases.

## Limitations
- Experimental scope is limited to grammar learning tasks, raising questions about generalizability to other domains
- Computational complexity and runtime performance analysis is lacking
- Reliance on contradiction detection may not scale efficiently to larger or more complex hypothesis spaces
- Manual derivation of SONFs remains a bottleneck for broader application

## Confidence
- High confidence: The core algorithmic contribution (contradiction-based refinement using SONFs) is well-founded and clearly presented
- Medium confidence: Experimental results are internally consistent but limited in scope
- Low confidence: Claims about scalability and general applicability to diverse ILP problems

## Next Checks
1. Test Poker on diverse ILP benchmarks beyond grammar learning, including relational databases and planning problems
2. Conduct systematic analysis of runtime complexity and scalability with increasing numbers of generated examples
3. Compare Poker's performance against state-of-the-art ILP systems using the same self-supervised framework to isolate the impact of SONFs