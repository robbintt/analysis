---
ver: rpa2
title: 'GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic
  Supervision, and Reinforced MLLM-based Solutions'
arxiv_id: '2509.21050'
source_url: https://arxiv.org/abs/2509.21050
tags:
- geometric
- qwen2
- reasoning
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of Referring Expression Comprehension
  (REC) for geometric problems, designed to assess models' ability to localize geometric
  elements based on natural language queries. To support this task, the authors present
  GeoRef, a benchmark dataset built from existing geometric problem corpora, featuring
  diverse, high-quality annotations and queries.
---

# GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions

## Quick Facts
- arXiv ID: 2509.21050
- Source URL: https://arxiv.org/abs/2509.21050
- Reference count: 40
- Primary result: GRPO fine-tuning with verify-and-regenerate mechanism achieves state-of-the-art accuracy on GeoRef benchmark for geometric referring expression comprehension.

## Executive Summary
This paper introduces the task of Referring Expression Comprehension (REC) for geometric problems, designed to assess models' ability to localize geometric elements based on natural language queries. To support this task, the authors present GeoRef, a benchmark dataset built from existing geometric problem corpora, featuring diverse, high-quality annotations and queries. Due to the lack of annotated training data, they generate a large-scale synthetic dataset using a structured geometric formal language, ensuring broad coverage and mathematical consistency. The paper explores two fine-tuning approaches—Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)—and proposes a verify-and-regenerate mechanism that improves accuracy by refining incorrect predictions using contextual reasoning history. Experiments show that GRPO significantly outperforms SFT, and the enhanced model achieves state-of-the-art results. Models trained on GeoRef also demonstrate measurable improvements on downstream geometric reasoning tasks, highlighting the broader value of REC as a foundation for multimodal mathematical understanding.

## Method Summary
The paper formulates the REC task for geometry and builds GeoRef, a benchmark with 3,776 test questions across 392 images. To overcome the lack of training annotations, a synthetic dataset is generated using Penrose-style formal language templates, yielding 2,184 images and 29,815 QA pairs. Two fine-tuning methods are explored: SFT with LoRA (rank 64) and GRPO (via VLM-R1 framework), both using 8 generations per prompt. A key innovation is the verify-and-regenerate mechanism, which corrects errors by leveraging reasoning history. The base model is Qwen2.5-VL-7B, trained for 500 steps with a learning rate of 1e-5 and cosine scheduler. GRPO significantly outperforms SFT, and the enhanced model generalizes to downstream geometric reasoning tasks.

## Key Results
- GRPO fine-tuning with verify-and-regenerate mechanism achieves the highest accuracy on the GeoRef benchmark.
- Models trained on GeoRef show improved performance on downstream geometric reasoning tasks.
- Synthetic data generation using structured geometric formal language enables effective training without human annotations.

## Why This Works (Mechanism)
The paper demonstrates that geometric referring expression comprehension can be effectively learned through a combination of synthetic data generation and advanced fine-tuning methods. By using a structured geometric formal language, the synthetic dataset ensures broad coverage and mathematical consistency, enabling models to learn robust spatial and relational reasoning. The GRPO approach, combined with the verify-and-regenerate mechanism, allows the model to iteratively refine its predictions, improving accuracy by leveraging reasoning history. This approach addresses the scarcity of annotated geometric data and establishes REC as a valuable foundation for multimodal mathematical understanding.

## Foundational Learning
- **Geometric formal language**: Needed to generate diverse, consistent synthetic training data; quick check: ensure synthetic data covers all geometric object types and relations.
- **Referring expression comprehension (REC)**: Core task of localizing geometric elements from natural language; quick check: evaluate on varied query types (positional, shape, relational).
- **Reinforcement learning for VLLMs**: GRPO optimizes model outputs via reward shaping; quick check: monitor reward variance and KL divergence during training.
- **Regex-based evaluation**: Matches model outputs to predefined answer sets; quick check: log raw outputs and verify answer set coverage.
- **LoRA fine-tuning**: Efficiently adapts large vision-language models; quick check: compare with full fine-tuning on a small subset.
- **Verify-and-regenerate mechanism**: Iteratively refines predictions using reasoning history; quick check: compare accuracy with and without verification.

## Architecture Onboarding
- **Component map**: GeoRef dataset -> Synthetic data generation -> Base model (Qwen2.5-VL-7B) -> LoRA fine-tuning -> GRPO training -> Verify-and-regenerate mechanism -> Evaluation
- **Critical path**: Synthetic data generation -> GRPO fine-tuning with verify-and-regenerate -> Evaluation
- **Design tradeoffs**: Synthetic data vs. human annotations (coverage vs. realism); GRPO vs. SFT (performance vs. stability); verify-and-regenerate (accuracy vs. latency).
- **Failure signatures**: Regex matching fails due to varied answer formats; GRPO training destabilizes due to reward variance; overfitting to annotation conventions (e.g., "O" for circle centers).
- **First experiments**: 1) Reconstruct synthetic data generation pipeline using Penrose or similar; 2) Implement SFT baseline with LoRA and 3-shot prompt; 3) Implement GRPO with VLM-R1 and test reward shaping.

## Open Questions the Paper Calls Out
None.

## Limitations
- Key GRPO hyperparameters (KL penalty, clip ratio) are not fully specified, limiting reproducibility.
- The synthetic data generation pipeline (Penrose templates) is not released, requiring reconstruction.
- Evaluation relies on regex matching, which may not fully capture semantic correctness of geometric descriptions.

## Confidence
- High: Core experimental design (benchmark construction, task framing, SFT baseline) is fully described.
- Medium: Overall performance claims due to incomplete specification of GRPO hyperparameters and lack of released synthetic data generation code.
- Medium: Effectiveness of verify-and-regenerate mechanism, since prompt templates and orchestration logic are only partially specified.

## Next Checks
1. Reimplement the synthetic data generation pipeline using publicly available geometric formal languages (e.g., Penrose) and validate coverage and annotation quality against reported statistics.
2. Reproduce GRPO fine-tuning using open-source frameworks (e.g., VLM-R1), experimenting with KL penalty and clip ratio settings to identify the most stable configuration.
3. Conduct an ablation study on the verify-and-regenerate mechanism by comparing model performance with and without verification, using the same prompt templates and answer sets.