---
ver: rpa2
title: 'Embedding Generative AI into Systems Analysis and Design Curriculum: Framework,
  Case Study, and Cross-Campus Empirical Evidence'
arxiv_id: '2511.17515'
source_url: https://arxiv.org/abs/2511.17515
tags:
- accessibility
- students
- data
- groups
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAGE addresses the gap in AI pedagogy for systems analysis by embedding
  GenAI into curriculum to develop critical orchestration skills rather than passive
  acceptance. Implemented across four Australian universities with 18 student groups,
  SAGE trains students to systematically accept, modify, or reject AI outputs.
---

# Embedding Generative AI into Systems Analysis and Design Curriculum: Framework, Case Study, and Cross-Campus Empirical Evidence

## Quick Facts
- **arXiv ID**: 2511.17515
- **Source URL**: https://arxiv.org/abs/2511.17515
- **Reference count**: 19
- **Primary result**: SAGE framework trains students to systematically accept, modify, or reject AI outputs in systems analysis, addressing passive acceptance through documented reasoning and scaffolded skill development across four Australian universities.

## Executive Summary
This study introduces SAGE, a framework for embedding generative AI into systems analysis and design curriculum to develop critical orchestration skills rather than passive acceptance. Implemented across four Australian universities with 18 student groups, SAGE trains students to systematically evaluate AI outputs through structured documentation and scaffolded skill development. The research reveals that while most groups (84%) demonstrated selective judgment, none achieved the highest "Critical Synthesizer" competency level, suggesting current scaffolding may not support expert-level orchestration. Key findings include the fragility of accessibility awareness across abstraction layers and systematic gaps in students' ability to identify AI translation errors.

## Method Summary
The SAGE framework employs a two-stage progression: scaffolded tutorials (Weeks 1-6) followed by experimental assessments (Weeks 8, 10, 12). Students work with the GreenHarvest Smart Kiosk case study using standardized ChatGPT-4 prompts for requirements generation, DFD translation, and heuristic evaluation. Three tasks with controlled scaffolding variation require students to create pre-AI baselines, systematically document accept/modify/reject decisions with 50-75 word justifications, and identify minimum 8 errors in AI-generated DFDs. The framework measures orchestration patterns across three competency dimensions: Analytical Deconstruction, Contextual Application, and Reflective Synthesis.

## Key Results
- Accessibility awareness collapsed dramatically across abstraction layers: 85% explicit in requirements but dropping to 10% in architectural diagrams without scaffolding
- 55% of groups struggled with AI misclassification of system boundaries, 45% missed data management errors, and 55% overlooked exception handling
- Most groups (84%) showed selective judgment rather than passive acceptance, but none achieved Critical Synthesizer status
- Justification quality correlated strongly with synthesis evidence (ρ=0.84, p=0.0004), indicating reasoning capability develops with structured practice

## Why This Works (Mechanism)

### Mechanism 1: Structured Documentation Forces Explicit Reasoning
Mandated decision matrices with confidence ratings and source attributions create cognitive friction that prompts students to articulate why they accepted, modified, or rejected each AI suggestion. This externalizes metacognitive processes that would otherwise remain implicit.

### Mechanism 2: Layer-Specific Scaffolding Addresses Transfer Failure
Accessibility awareness operates as stimulus-dependent rather than internalized expertise. When environmental cues are removed, competency expression suppresses dramatically, then recovers when cues are reintroduced.

### Mechanism 3: Baseline-Comparison Creates Reference Expectations
Pre-AI baseline artifacts establish explicit expectations against which students evaluate AI outputs. This creates controlled comparison conditions that reveal both AI systematic failures and student evaluation capabilities.

## Foundational Learning

- **Formal Modeling Notation (DFD semantics):** Students must understand data flow diagram conventions—boundary classification, data store CRUD operations, exception pathway representation—to identify systematic AI translation errors.
  - *Quick check:* Can you explain why external entities cannot directly access data stores, and what process must mediate such interactions?

- **Requirements Prioritization and Synthesis:** Students need frameworks for evaluating competing stakeholder needs, technical feasibility, and business value to synthesize human and AI-generated requirements.
  - *Quick check:* When presented with an AI-generated requirement that conflicts with a human-authored requirement addressing accessibility needs, what criteria would you use to resolve the conflict?

- **Contextual Design Judgment:** Students must distinguish between universal usability principles (which AI often applies correctly) and context-specific constraints (which AI may miss).
  - *Quick check:* An AI suggests hover states for interface elements. Under what conditions would you reject this recommendation, and what alternative would you propose?

## Architecture Onboarding

- **Component map:** SAGE Framework -> Two-Stage Progression -> Scaffolded tutorials (Weeks 1-6) -> Experimental assessments (Weeks 8, 10, 12) -> Three Competency Dimensions -> Analytical Deconstruction, Contextual Application, Reflective Synthesis -> Measurement Instruments -> Product Backlog Synthesis Table, DFD Correction Log, AI Feedback Evaluation Matrix

- **Critical path:** Pre-AI baseline -> Standardized AI prompts -> Mandated distribution constraints -> Explicit justification documentation

- **Design tradeoffs:** Mandated response distributions prevent passive acceptance but may distort authentic decision-making patterns; detailed structured templates generate research-grade data but increase marking burden; layer-specific scaffolding addresses transfer failures but requires more preparation.

- **Failure signatures:** Accessibility suppression at architecture layer (10% visibility despite 85% at requirements and 90% at interface); boundary reasoning failures (55% of groups); state management omissions (45%); exception handling gaps (55%); competency ceiling with no Critical Synthesizer groups achieved.

- **First 3 experiments:**
  1. Requirements Synthesis Pilot: Product Backlog Synthesis Table with 8 human-authored stories, minimum 5 AI stories (3 accepted/modified, 2 rejected), and 2 hybrid stories.
  2. DFD Translation Error Detection: Structured Process Description Template, AI translation to Level-0 DFD, identification of 8+ errors with classification and corrections.
  3. Context Calibration Test: Wireframe design for specific context, AI heuristic evaluation, Evaluation Matrix with 4-3-3 distribution.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can specific pedagogical interventions enable students to achieve "Critical Synthesizer" status?
- **Open Question 2:** Does accessibility suppression at the architectural layer result from lack of prompting or fundamental conceptual difficulty?
- **Open Question 3:** Do student orchestration patterns remain consistent when mandatory response distributions are removed?
- **Open Question 4:** Do SAGE results generalize to undergraduate cohorts or domestic student populations?

## Limitations
- Cross-university implementation introduces uncontrolled variability in institutional contexts, prior student experience, and grading standards
- Absence of detailed inter-rater reliability metrics creates uncertainty about the objectivity of competency assessments
- Forced response distributions may artificially constrain natural decision-making patterns

## Confidence
- **High confidence:** Accessibility U-curve finding and justification quality correlation are robustly demonstrated
- **Medium confidence:** Competency dimension framework shows face validity but lacks convergent validity testing
- **Low confidence:** Claims about superiority of baseline-comparison approaches lack comparative evidence

## Next Checks
1. Implement crossover design comparing uniform versus layer-specific scaffolding across abstraction layers
2. Remove mandated response distributions in controlled subset to observe natural decision-making patterns
3. Conduct follow-up assessments 3-6 months post-course to measure retention of orchestration practices