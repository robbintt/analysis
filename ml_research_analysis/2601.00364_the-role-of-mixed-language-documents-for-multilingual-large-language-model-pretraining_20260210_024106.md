---
ver: rpa2
title: The Role of Mixed-Language Documents for Multilingual Large Language Model
  Pretraining
arxiv_id: '2601.00364'
source_url: https://arxiv.org/abs/2601.00364
tags:
- data
- bilingual
- language
- cross-lingual
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the role of bilingual data
  in multilingual large language model pretraining. The authors create a monolingual
  corpus by removing all multilingual documents from standard web data, then pretrain
  models from scratch to compare translation performance against models trained on
  the full multilingual corpus.
---

# The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining

## Quick Facts
- arXiv ID: 2601.00364
- Source URL: https://arxiv.org/abs/2601.00364
- Reference count: 11
- Primary result: Removing 2% bilingual data from pretraining corpus drops translation BLEU by 56% while cross-lingual QA and reasoning tasks remain stable

## Executive Summary
This study systematically investigates how bilingual documents affect multilingual LLM pretraining by removing them from standard web data and retraining from scratch. The authors find that translation performance depends critically on parallel documents (14% of bilingual data) which provide token-level cross-lingual alignment, while cross-lingual understanding and reasoning tasks rely primarily on sentence-level semantic alignment that emerges from monolingual exposure. Removing bilingual data causes translation to fail through two compounding modes: language generation failure (~55% source-language passthrough) and semantic underspecification (loss of fine-grained details like temporal markers and lexical precision).

## Method Summary
The authors built a bilingual detection pipeline using entropy-based filtering followed by LLM classification to identify parallel, code-switching, and miscellaneous bilingual documents. They created four pretraining corpora from FineWeb-Edu/FineWeb2: full multilingual data, monolingual-only (bilingual removed), monolingual+parallel, and monolingual+code-switching. Four 1.35B decoder-only transformers were pretrained from scratch (34K steps, ~143B tokens each) on English paired with German, Spanish, and French. Translation performance was measured with BLEU on WMT14/16 and FLORES-101, while cross-lingual QA used XQuAD/MLQA and reasoning used XNLI, HellaSwag, ARC, and other benchmarks. Layer-wise alignment probing measured lexical (word-level) and sentence-level cross-lingual alignment.

## Key Results
- Removing bilingual data (only 2% of corpus) causes translation BLEU to drop by 56% while cross-lingual QA and reasoning tasks remain within 1-2% of baseline
- Parallel documents (14% of bilingual data) almost fully restore translation performance (91% recovery), whereas code-switching data (72% of bilingual data) provides minimal benefit
- Translation failure occurs through two compounding modes: 55% failure to generate target language and severely degraded semantic quality with loss of fine-grained information
- Lexical-level cross-lingual alignment (measured by word P@1) drops 13-21% at middle transformer layers when parallel data is removed, while sentence-level alignment remains robust (<2% drop)

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Cross-Lingual Alignment from Parallel Data
Parallel documents enable translation by providing systematic token-level correspondences that cannot emerge from monolingual data alone. Parallel data (14% of bilingual documents) contains paragraph/word-aligned translations. During pretraining, the model learns explicit bidirectional mappings between tokens across languages. This creates lexical-level alignment in middle transformer layers (layers 6–12) where word representations become cross-lingually aligned. The model learns both "what generates what" (translation direction) and precise lexical equivalents. Core assumption: Translation requires precise token-to-token correspondences; approximate semantic overlap is insufficient. Evidence anchors: [abstract]: "parallel data almost fully restores translation performance (91% of the unfiltered baseline)"; [section 5.1, Table 5]: MONOWEB+PARALLEL achieves 20.2 BLEU vs. FINEWEB's 22.3 (91% recovery); [section 5.4, Table 8]: MONOWEB shows "sharp 13–21% degradation in lexical-level alignment" at layers 6–12; [corpus]: Related work on parallel corpora (arxiv:2503.04797) confirms importance for MT in low-resource settings.

### Mechanism 2: Sentence-Level Semantic Alignment from Monolingual Pretraining
Cross-lingual understanding and reasoning depend on sentence-level semantic alignment that emerges from monolingual exposure without document-level co-occurrence. The model learns language-agnostic semantic representations by seeing similar concepts expressed across languages in separate contexts. Sentence-level alignment (measured by P@1 on parallel sentence representations) is preserved in later layers (~layer 23) and enables semantic reasoning across languages without requiring the model to know which words correspond. Core assumption: High-level semantic concepts are sufficiently universal that their representations naturally align across languages given sufficient monolingual data. Evidence anchors: [abstract]: "cross-lingual understanding and reasoning rely primarily on sentence-level alignment, explaining their robustness to bilingual data removal"; [section 5.3, Table 9]: Understanding/reasoning tasks show stability "within 1–2% of the baseline" across all configurations; [section 5.4, Table 8]: MONOWEB "preserves robust sentence-level alignment (<2% drop from FineWeb)"; [corpus]: Weak corpus evidence—no direct prior work on monolingual emergence of sentence-level cross-lingual alignment.

### Mechanism 3: Two-Fold Translation Collapse Without Parallel Data
Translation failure compounds through (1) language generation failure and (2) semantic underspecification. Without parallel data, models fail to learn target-language generation conditioned on source input (~55% source-language passthrough). For outputs that do generate in the target language, the model preserves only coarse-grained propositional structure while systematically losing fine-grained information (temporal, explanatory context, lexical precision). Core assumption: Translation requires both language selection capability and precise lexical correspondence. Evidence anchors: [section 5.2]: "two compounding failure modes: (1) 56% failure to generate target language...and (2) severely degraded translation quality"; [section 5.2, Table 4]: Examples showing information loss: "immediately" → omitted; "kids" → "Familie" [family] instead of "Kinder"; [section 5.2, Table 6]: MONOWEB produces German in ~45% of cases vs. FINEWEB's ~86%; [corpus]: No direct corpus evidence for this specific two-fold failure decomposition.

## Foundational Learning

- **Concept: Token-Level vs. Sentence-Level Alignment**
  - Why needed here: The paper's central finding hinges on this distinction—translation needs token alignment, reasoning needs only sentence alignment.
  - Quick check question: For a sentiment classifier trained multilingually, which alignment type matters more?

- **Concept: Parallel vs. Code-Switching vs. Comparable Corpora**
  - Why needed here: The paper shows parallel (systematic alignments) and code-switching (organic mixing) have vastly different utility. Confusion between these leads to ineffective data curation.
  - Quick check question: A Wikipedia article with an embedded Spanish quote in an English article—is this parallel or code-switching?

- **Concept: Cross-Lingual Transfer Without Explicit Supervision**
  - Why needed here: Explains why models can perform cross-lingual reasoning despite largely monolingual pretraining.
  - Quick check question: Why might a model trained on separate English and French monolingual data still transfer reasoning skills across languages?

## Architecture Onboarding

- **Component map**: Entropy filter (Stage 1) -> LLM classifier (Stage 2) -> Pretraining corpus -> Model -> Evaluation
- **Critical path**: 1. Sample 60B tokens/language from FineWeb-Edu/FineWeb2 2. Apply entropy filter → ~5% candidates 3. LLM classify → 2% verified bilingual (14% parallel, 72% code-switching, 14% misc) 4. Build four corpus variants 5. Pretrain from scratch (34K steps, ~143B tokens, ~6,144 A100-hours/model) 6. Evaluate + layer-wise alignment probing
- **Design tradeoffs**: Entropy threshold: Lower τ → higher recall but more LLM cost; Model scale: 1.35B findings may not scale to 7B+ (acknowledged limitation); Language scope: Latin-script pairs only—typologically distant languages untested; Tokenizer: Llama-2 vocab may affect subword sharing across languages
- **Failure signatures**: Translation: 55%+ source passthrough; semantic underspecification (temporal/lexical details lost); Cross-lingual QA: ~10% drop on generative tasks (XQuAD), stable on retrieval (MLQA); Reasoning: <4% variation across configurations; Layer probing: Lexical P@1 drops 13–21% at layers 6–12; sentence P@1 stable (<2%)
- **First 3 experiments**: 1. Validate detection pipeline: Manually verify 100 documents per category for classification accuracy and alignment quality 2. Minimum parallel data threshold: Incrementally add parallel data (1%, 5%, 10% of bilingual corpus) to MONOWEB; plot BLEU recovery curve 3. Layer-wise alignment probe: Extract representations at each layer; compute P@1 for sentence pairs vs. MUSE word pairs to confirm granularity divergence

## Open Questions the Paper Calls Out

**Open Question 1**: Does the critical dependence on parallel data for translation persist in larger parameter models (e.g., 7B+), or does increased model capacity reduce the need for explicit token-level alignment? Basis in paper: [explicit] The authors note in the Limitations section that they "pretrained only 1.35B-parameter models and did not pretrain larger models such as 7B, which may exhibit different sensitivity to bilingual data." Why unresolved: It is unknown if the severe BLEU drop (56%) observed in small models when removing bilingual data would be mitigated by the emergent capabilities often found in larger models. What evidence would resolve it: Pretraining models with 7B or more parameters on the MonoWeb dataset and comparing the translation performance delta against the 1.35B baseline.

**Open Question 2**: Is the sufficiency of monolingual data for reasoning tasks applicable to low-resource languages or language pairs with different scripts and high typological distance? Basis in paper: [explicit] The authors state their experiments "focus on major languages within the Latin script family, leaving open questions about the impact of bilingual data on typologically distant or low-resource languages." Why unresolved: The finding that sentence-level alignment persists without bilingual data was only validated on English, German, French, and Spanish; it may not generalize to languages where shared lexical roots are absent. What evidence would resolve it: Replicating the MonoWeb ablation study on language pairs involving non-Latin scripts (e.g., English-Japanese) or low-resource languages (e.g., English-Yoruba).

**Open Question 3**: How do finer-grained distinctions in parallel data—such as domain specificity, register, or alignment quality—influence the degree of translation recovery? Basis in paper: [explicit] The authors categorize data broadly but acknowledge that "finer-grained distinctions, such as domain, register, or sentence-level alignment quality, may further influence cross-lingual learning." Why unresolved: The study treats the "Parallel" category as a monolith; the relative importance of high-quality dictionary data versus noisy aligned web text remains unclear. What evidence would resolve it: Conducting targeted ablations where specific sub-types of parallel data (e.g., only medical, only colloquial) are reintroduced to the monolingual corpus.

## Limitations

- Scale generalizability: Findings based on 1.35B models may not extend to 7B+ models which could have different capacity for cross-lingual learning
- Language family constraints: All experiments use Latin-script Indo-European languages; results may not generalize to typologically distant languages with different scripts
- LLM classification quality: Bilingual detection pipeline relies on Llama-3.3-70B-Instruct without providing classification accuracy metrics or validation of parallel data quality

## Confidence

**High confidence** in the core finding that removing bilingual data causes translation performance to drop by 56% while cross-lingual understanding remains stable.

**Medium confidence** in the mechanism that parallel data specifically restores translation through token-level alignment.

**Medium confidence** in the claim that code-switching data contributes minimally to translation performance.

## Next Checks

1. **Scale validation experiment**: Train the same model architectures at 7B parameters to verify whether the translation collapse pattern (55% language passthrough, semantic underspecification) persists at larger scales, or whether increased capacity mitigates the need for parallel data.

2. **Typological diversity test**: Extend the monolingual ablation experiment to include a typologically distant language pair (e.g., English-Chinese or English-Arabic) to determine whether sentence-level semantic alignment can emerge without document-level co-occurrence in non-Latin scripts.

3. **Code-switching granularity analysis**: Conduct a fine-grained analysis of code-switching patterns to identify specific characteristics (frequency, language switching points, domain distribution) that distinguish effective from ineffective code-switching for translation.