---
ver: rpa2
title: 'Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management'
arxiv_id: '2512.21567'
source_url: https://arxiv.org/abs/2512.21567
tags:
- memory
- uncertainty
- arxiv
- decision
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reframes LLM memory management as a sequential decision-making
  problem under uncertainty. The proposed Decision-theoretic Agent Memory (DAM) framework
  explicitly models memory operations (read, write, delete) as decisions that shape
  future retrieval quality, incorporating value estimation and uncertainty quantification
  to anticipate delayed and uncertain utility.
---

# Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management

## Quick Facts
- arXiv ID: 2512.21567
- Source URL: https://arxiv.org/abs/2512.21567
- Reference count: 9
- This paper reframes LLM memory management as a sequential decision-making problem under uncertainty.

## Executive Summary
This paper reframes LLM memory management as a sequential decision-making problem under uncertainty. The proposed Decision-theoretic Agent Memory (DAM) framework explicitly models memory operations (read, write, delete) as decisions that shape future retrieval quality, incorporating value estimation and uncertainty quantification to anticipate delayed and uncertain utility. DAM decomposes the process into a read policy for immediate access and a hierarchical write policy that generates candidate operations paired with value and uncertainty estimates, arbitrated by an aggregate policy. Rather than introducing a new algorithm, DAM provides a principled theoretical structure that clarifies the limitations of heuristic memory systems and enables future development of uncertainty-aware memory management. The framework is demonstrated through an example scenario and contrasted with traditional heuristic approaches.

## Method Summary
DAM formalizes memory management as a sequential decision problem where operations (read, add, delete) are treated as actions within a Markov Decision Process. The framework decomposes the problem into a read policy for immediate retrieval and a hierarchical write policy that generates candidate operations with associated value estimates (V^o) and uncertainty scores (Σ^o). An aggregate policy then arbitrates among these proposals based on their predicted long-term utility and risk. The approach aims to overcome the limitations of heuristic methods by explicitly modeling the delayed and uncertain nature of memory utility, enabling risk-sensitive decisions such as preventing irreversible deletions when model confidence is low.

## Key Results
- DAM provides a principled theoretical framework that reframes memory management as sequential decision-making under uncertainty
- The framework explicitly models delayed and uncertain utility through value functions and uncertainty quantification
- DAM demonstrates how hierarchical decomposition can reduce the combinatorial complexity of joint read-write optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit value estimation enables anticipation of delayed memory utility that heuristic thresholds cannot capture.
- Mechanism: Value functions V^o predict expected discounted future contributions from memory operations, allowing the system to retain currently irrelevant information with high predicted future value.
- Core assumption: The value function can be learned or approximated sufficiently to guide decisions (unproven; the paper identifies credit assignment as an open challenge).
- Evidence anchors:
  - [abstract] "candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk"
  - [Section 2.4.2] "These operation-specific value functions act as critics that assess long-term impact beyond immediate outcomes"
  - [corpus] Weak direct evidence; related work (Mem-α, Memory-R1) explores RL-based memory but without unified value-theoretic framing.
- Break condition: If credit assignment fails—i.e., the signal connecting a memory decision to a distant outcome remains too sparse—the value function provides no better signal than heuristics.

### Mechanism 2
- Claim: Hierarchical decomposition reduces the combinatorial complexity of joint read-write optimization into tractable sub-policies.
- Mechanism: Separate sub-policies propose candidate add/delete operations; an aggregate policy arbitrates among proposals using value and uncertainty signals, rather than jointly optimizing all operations.
- Core assumption: Sub-policies generate sufficiently diverse and non-redundant proposals; the aggregate policy can resolve conflicts without exhaustive search.
- Evidence anchors:
  - [abstract] "DAM decomposes the process into a read policy for immediate access and a hierarchical write policy that generates candidate operations paired with value and uncertainty estimates"
  - [Section 2.4] "Direct optimization over the full joint memory action space is typically intractable due to both combinatorial structure and severely delayed feedback"
  - [corpus] Git Context Controller and Nemori implement modular memory systems but do not formalize hierarchical arbitration.
- Break condition: If sub-policies propose conflicting or overlapping operations that the aggregate policy cannot resolve coherently, the decomposition introduces more noise than structure.

### Mechanism 3
- Claim: Explicit uncertainty quantification enables risk-sensitive suppression of irreversible operations.
- Mechanism: An uncertainty estimator Σ^o produces epistemic uncertainty scores; the aggregate policy can veto or deprioritize high-uncertainty deletions, preventing catastrophic memory loss.
- Core assumption: Epistemic uncertainty can be calibrated from LLM outputs (e.g., ensemble disagreement, semantic entropy); the paper flags this as an open challenge.
- Evidence anchors:
  - [Section 2.3] "By treating uncertainty as a first-class signal, DAM can distinguish between actions that may be high-impact but poorly understood"
  - [Section 3] "By treating epistemic uncertainty as a first-class signal, the aggregate policy can implement conservative behaviors, such as inhibiting irreversible deletion operations when model confidence is low"
  - [corpus] Weak direct evidence; corpus papers do not systematically address uncertainty quantification in memory.
- Break condition: If uncertainty estimates are miscalibrated (over- or under-confident), the aggregate policy either blocks useful operations or allows harmful ones.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and sequential decision-making under uncertainty
  - Why needed here: DAM formalizes memory management as a sequential decision problem with states S_t, actions A_t, transitions T, and cumulative objectives. Understanding Bellman-style optimization is prerequisite.
  - Quick check question: Can you explain why memory utility is modeled as a delayed, path-dependent quantity rather than an immediate reward?

- Concept: Value function approximation and credit assignment
  - Why needed here: The framework relies on estimating V^o, which aggregates discounted future contributions. Understanding temporal credit assignment is essential for implementing critics.
  - Quick check question: Given a deletion at t=0 and a hallucination at t=100, how would you assign credit to the deletion decision?

- Concept: Epistemic vs. aleatoric uncertainty
  - Why needed here: DAM uses epistemic uncertainty (model-based, reducible) to arbitrate irreversible operations. Distinguishing this from aleatoric noise is critical for calibration.
  - Quick check question: If an ensemble of LLMs disagrees on whether a memory should be deleted, is this epistemic or aleatoric uncertainty, and how should it influence the deletion decision?

## Architecture Onboarding

- Component map: State S_t -> (Read policy π_read, Write sub-policies π_add/π_delete) -> Value/Uncertainty estimators -> Aggregate policy π_agg -> Action A_t -> Transition T -> Updated State S_{t+1}

- Critical path: Implement a minimal π_add and π_delete with stub value/uncertainty estimators → implement π_agg with threshold-based arbitration → replace stubs with learned critics.

- Design tradeoffs:
  - Computational cost: Evaluating value/uncertainty for every candidate operation at each step may be prohibitive; consider heuristic gating to limit proposals.
  - Asymmetry: Rigorous evaluation applies only to write operations; read is treated as transient optimization.
  - Joint read-write coupling: π_read determines what information π_write sees; optimizing them independently risks instability.

- Failure signatures:
  - Memory bloat: π_add proposes too many additions; aggregate policy fails to prune low-value entries.
  - Catastrophic forgetting: Σ^delete underestimates uncertainty; irreversible deletions remove critical context.
  - Stagnation: Σ^delete overestimates uncertainty; system becomes overly conservative and never deletes.

- First 3 experiments:
  1. Implement a simple threshold-based π_agg that uses fixed V^o thresholds and Σ^o caps; test on a long-horizon conversational task with known critical memories.
  2. Replace fixed thresholds with learned V^o using offline behavioral cloning or inverse RL from expert-labeled memory traces.
  3. Inject synthetic distribution shift (e.g., change user preferences mid-session) and evaluate whether Σ^o-based arbitration prevents premature deletion of stale-seeming but still-relevant memories.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can value functions (V^o) be effectively approximated given the extreme sparsity and delay of feedback connecting a memory decision to a downstream outcome?
- Basis in paper: [explicit] The section "The Credit Assignment Problem in Value Estimation" highlights the difficulty of learning from sparse, delayed signals (e.g., a deletion at t=0 causing an error at t=100).
- Why unresolved: Standard optimization struggles with long time horizons, and the framework currently prescribes the need for value functions without providing a learning algorithm to overcome the temporal gap.
- What evidence would resolve it: Demonstration of a scalable method (e.g., inverse reinforcement learning or trajectory synthesis) that accurately predicts long-term utility without relying on dense immediate rewards.

### Open Question 2
- Question: Which forms of epistemic uncertainty estimation provide the most reliable signal for arbitrating memory operations without causing system paralysis or brittleness?
- Basis in paper: [explicit] The section "Calibrating Epistemic Uncertainty" questions how to balance risk, noting that underestimation causes errors while overestimation leads to stagnation.
- Why unresolved: Obtaining calibrated uncertainty from LLMs is notoriously difficult, yet the framework relies on these estimates (Σ^o) to veto high-risk, irreversible actions like deletion.
- What evidence would resolve it: Comparative studies showing which uncertainty metrics (e.g., ensemble disagreement vs. semantic entropy) correlate best with actual decision risk in memory maintenance tasks.

### Open Question 3
- Question: How can the read (π_read) and write (π_write) policies be jointly optimized when they induce different types of state changes (transient vs. persistent)?
- Basis in paper: [explicit] The section "Joint Optimization of Read and Write Policies" notes that while these policies are structurally distinct, they are functionally coupled.
- Why unresolved: Optimizing two coupled policies where one modifies a transient context and the other modifies a persistent store creates a complex, potentially destabilizing control problem.
- What evidence would resolve it: A stable joint training algorithm that improves both retrieval relevance and memory maintenance simultaneously without catastrophic forgetting or oscillation.

### Open Question 4
- Question: What specific evaluation protocols are required to decouple the assessment of memory management policies from the underlying LLM's reasoning capabilities?
- Basis in paper: [explicit] The section "Policy-Oriented Evaluation" states that existing benchmarks conflate memory decision quality with LLM reasoning capability.
- Why unresolved: Validating the DAM framework requires isolating the predictive validity of value estimates (V^o) and the calibration of uncertainty (Σ^o), which current downstream benchmarks do not measure.
- What evidence would resolve it: The creation and adoption of diagnostic benchmarks that specifically score the accuracy of predicted long-term utility and risk estimates independent of final task output.

## Limitations
- Value function learning: The framework assumes V^o can be learned to guide long-term decisions, but the paper explicitly identifies credit assignment for delayed memory utility as an open challenge.
- Uncertainty estimation: While epistemic uncertainty is central to DAM's risk-sensitive operation, no specific methods for calibration or validation are proposed.
- Hierarchical decomposition stability: The decomposition into separate sub-policies assumes proposals are non-redundant and the aggregate policy can resolve conflicts, but no analysis of failure modes is provided.

## Confidence

### Claim Clusters
- **Theoretical framework validity** (High): The MDP formalization and decomposition are internally consistent and build on established decision theory.
- **Practical utility** (Low): No empirical results or ablation studies demonstrate the framework's effectiveness in real memory management scenarios.
- **Implementation tractability** (Medium): While the framework suggests a path forward, critical components (V^o, Σ^o) lack concrete specifications.

## Next Checks
1. Implement a minimal DAM prototype using heuristic value/uncertainty estimators and evaluate on a synthetic memory task with known critical memories requiring delayed retrieval.
2. Conduct an ablation study comparing DAM's proposed value-based retention against fixed-threshold heuristic baselines across varying horizon lengths.
3. Test uncertainty calibration by injecting controlled distribution shifts and measuring whether Σ^o-based arbitration prevents premature deletion of relevant but contextually stale memories.