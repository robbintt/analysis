---
ver: rpa2
title: 'TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue
  Agent'
arxiv_id: '2601.18700'
source_url: https://arxiv.org/abs/2601.18700
tags:
- tool
- user
- uni00000013
- support
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TEA-Bench introduces the first interactive benchmark for evaluating
  tool-enhanced emotional support dialogue agents. It features realistic emotional
  scenarios, an MCP-style tool environment, and process-level metrics that jointly
  assess emotional support quality and factual grounding.
---

# TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent

## Quick Facts
- arXiv ID: 2601.18700
- Source URL: https://arxiv.org/abs/2601.18700
- Reference count: 40
- Primary result: First interactive benchmark for evaluating tool-enhanced emotional support dialogue agents with realistic scenarios and process-level metrics

## Executive Summary
TEA-Bench introduces the first interactive benchmark for evaluating tool-enhanced emotional support dialogue agents. It features realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess emotional support quality and factual grounding. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. The dataset TEA-Dialog, derived from high-quality tool-enhanced dialogues, demonstrates that supervised fine-tuning improves in-distribution support but generalizes poorly.

## Method Summary
TEA-Bench evaluates tool-augmented emotional support dialogue agents through interactive multi-turn conversations where agents can optionally invoke external tools for factual grounding. The benchmark uses 81 constructed scenarios with latent spatiotemporal attributes, a tool environment of 31 MCP tools across 7 categories, and an LLM-based hallucination detection module. Agents are evaluated on TEA-Scores (5 dimensions: Diversity, Fluency, Humanoid, Information, Effectiveness) and factuality metrics. The evaluation uses simulated users with hallucination-aware reactions and LLM-as-judge scoring with human validation.

## Key Results
- Tool augmentation consistently improves emotional support quality and reduces hallucination across all nine evaluated LLMs
- Performance gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally
- Supervised fine-tuning on TEA-Dialog improves in-distribution support but generalizes poorly, increasing hallucination rates on out-of-distribution scenarios

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Grounding for Hallucination Reduction
- Claim: External tool access enables factual grounding that reduces hallucination in emotional support responses
- Mechanism: Tools provide verifiable external context (time, location, weather, nearby places) that the agent can retrieve before generating responses containing factual claims
- Core assumption: Agents can successfully identify when external information is needed and correctly integrate tool outputs into natural language responses
- Evidence: Across all models, tool augmentation consistently reduces hallucination-related metrics, with both the proportion of hallucinated factual content and the hallucination rate decreasing under the with tool setting

### Mechanism 2: Capacity-Dependent Tool Utilization Efficiency
- Claim: Tool augmentation benefits scale with model capability; stronger models achieve more with fewer tool calls
- Mechanism: Stronger models demonstrate selective tool usage—they invoke tools only when necessary and extract maximum utility per call
- Core assumption: Model capability correlates with ability to judge when tools are needed, select appropriate tools, and synthesize tool outputs into emotionally appropriate responses
- Evidence: Stronger models achieve substantial performance gains with relatively fewer tool calls, indicating more precise and effective tool usage, while mid-capability models rely on more frequent tool invocations

### Mechanism 3: Supervised Fine-Tuning with Limited Distribution Coverage
- Claim: SFT on high-quality tool-enhanced dialogues improves in-domain performance but exposes generalization vulnerabilities
- Mechanism: Training on TEA-Dialog teaches models effective tool usage patterns for familiar scenario types
- Core assumption: The training set captures representative tool usage patterns that should transfer to new scenarios
- Evidence: Performance on in-distribution scenarios is significantly higher than on out-of-distribution scenarios for both models, with hallucination rates increasing markedly on OOD scenarios after fine-tuning

## Foundational Learning

- **Emotional Support Conversation (ESC) dual-support framework**
  - Why needed: TEA-Bench evaluates both affective support (empathy, emotional validation) and instrumental support (actionable guidance)
  - Quick check: Can you explain why affective-only responses might undermine user trust in ESC scenarios?

- **Hallucination in grounded dialogue systems**
  - Why needed: The paper's Hallucination Detection Module identifies ungrounded factual claims
  - Quick check: If an agent says "a nearby park" after calling a map tool, is this hallucination? Why or why not?

- **Model Context Protocol (MCP) tool interface**
  - Why needed: TEA-Bench exposes tools via MCP, which standardizes tool invocation
  - Quick check: How does scenario-aware execution ensure reproducibility for time-sensitive tools?

## Architecture Onboarding

- **Component map**: TEA-Scenarios -> Tool Environment -> Agent -> HDM -> User Simulator -> TEA-Evaluator -> TEA-Scores
- **Critical path**: Load scenario with latent attributes → Agent receives user utterance, optionally invokes tools → HDM evaluates response → User simulator reacts → Dialogue continues until termination or 15-turn cap → Complete dialogue evaluated
- **Design tradeoffs**: Simulated users enable scalable benchmarking but may not capture real user diversity; scenario-aware tool execution ensures reproducibility but limits temporal realism
- **Failure signatures**: Tool over-reliance by mid-capability models; hallucination cascade in SFT-trained models on OOD scenarios; grounding-utility mismatch where weaker models achieve factuality gains without TEA-Score improvements
- **First 3 experiments**:
  1. Baseline capability assessment: Run your model on TEA-Bench in both "without tool" and "with tool" settings
  2. Tool usage pattern analysis: Log all tool invocations across dialogue stages and compare distribution against expected patterns
  3. User type robustness check: Evaluate separately on action-oriented vs. emotion-oriented scenarios

## Open Questions the Paper Calls Out

- How can training strategies be designed to improve the generalization of tool-augmented emotional support agents to out-of-distribution scenarios without increasing hallucination rates?
- Can explicit "tool suppression" training improve the performance of weaker models when supporting emotion-oriented users?
- How do tool-augmented support strategies impact long-term user trust and sustained emotional regulation beyond the 15-turn interaction horizon?

## Limitations

- The evaluation focuses on short to medium-length interactions, leaving long-term dynamics such as sustained emotional support and trust formation unverified
- The simulated user model may not fully capture the diversity of real user behaviors and emotional responses
- The benchmark's scenario distribution may not fully represent the diversity of real-world emotional support needs

## Confidence

- Mechanism 1: High - Strong empirical evidence across all nine LLMs with clear metrics
- Mechanism 2: High - Detailed analysis of tool usage patterns and performance correlations
- Mechanism 3: Medium - In-distribution improvements are clear, but OOD generalization issues need further investigation

## Next Checks

1. Reproduce baseline results on your target model to establish capability tier and compare against Table 1 baselines
2. Analyze tool invocation patterns to determine if your model follows expected usage patterns or exhibits over-reliance
3. Test user type robustness by evaluating separately on action-oriented vs. emotion-oriented scenarios to identify potential empathy-tooling mismatches