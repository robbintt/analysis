---
ver: rpa2
title: 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency
  at Matched Compute'
arxiv_id: '2511.02309'
source_url: https://arxiv.org/abs/2511.02309
tags:
- reasoning
- sequential
- parallel
- across
- chains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing parallel self-consistency
  paradigm for test-time scaling in LLM reasoning. The authors systematically compare
  sequential iterative refinement with parallel independent chains under matched compute
  budgets across five state-of-the-art models and three reasoning benchmarks.
---

# The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute

## Quick Facts
- arXiv ID: 2511.02309
- Source URL: https://arxiv.org/abs/2511.02309
- Authors: Aman Sharma; Paras Chopra
- Reference count: 32
- Primary result: Sequential reasoning with inverse-entropy voting outperforms parallel self-consistency across all tested configurations

## Executive Summary
This paper challenges the prevailing parallel self-consistency paradigm for test-time scaling in LLM reasoning. Through systematic comparison under matched compute budgets, the authors demonstrate that sequential iterative refinement achieves superior accuracy over parallel independent chains in 95.6% of configurations. The key innovation is inverse-entropy weighted voting, which leverages token-level logprobs to weight answers by model confidence, establishing it as the universally superior aggregation strategy across both sequential and parallel approaches.

## Method Summary
The study compares sequential and parallel reasoning strategies under matched compute budgets across five state-of-the-art models and three reasoning benchmarks. Sequential reasoning employs iterative refinement where each step builds on previous outputs, while parallel reasoning generates independent chains simultaneously. The authors introduce inverse-entropy weighted voting that uses Shannon entropy from token-level logprobs to weight answers by model confidence. Efficiency analysis identifies optimal configurations, and ablation studies examine diversity characteristics between approaches.

## Key Results
- Sequential reasoning outperforms parallel approaches in 95.6% of configurations with accuracy gains up to 46.7%
- Inverse-entropy weighted voting achieves optimal performance in 97% of sequential configurations and 100% of parallel configurations
- 6-chain configurations identified as optimal, achieving 13.8 accuracy points per 1K tokens
- Parallel methods show greater semantic diversity while sequential methods demonstrate superior lexical diversity

## Why This Works (Mechanism)
Sequential reasoning leverages iterative error correction and context accumulation, allowing models to refine their reasoning through multiple passes. Each refinement step benefits from accumulated context and error correction, creating a compounding advantage over independent parallel sampling. Inverse-entropy weighting exploits the relationship between token-level uncertainty and answer quality, with lower entropy indicating higher confidence and better answers.

## Foundational Learning
- **Shannon entropy in logprobs**: Quantifies uncertainty at token level; needed to weight answers by confidence; quick check: compare entropy distributions across correct vs incorrect answers
- **Compute-matching methodology**: Ensures fair comparison between sequential and parallel approaches; needed to isolate reasoning strategy effects; quick check: verify FLOPs calculations across different step counts
- **Iterative refinement dynamics**: Shows how context accumulation improves reasoning; needed to understand sequential advantage; quick check: track intermediate reasoning quality across refinement steps

## Architecture Onboarding

**Component Map**
Input -> Token Generation -> Logprob Extraction -> Entropy Calculation -> Answer Weighting -> Voting Aggregation -> Final Output

**Critical Path**
Token generation → logprob extraction → entropy calculation → weighted voting → answer selection

**Design Tradeoffs**
- Sequential: Higher accuracy through refinement but increased latency
- Parallel: Lower latency but sacrifices accuracy and cannot benefit from iterative improvement
- Inverse-entropy: Requires logprob access but provides universal voting improvement

**Failure Signatures**
- Low entropy with incorrect answers suggest overconfident but wrong reasoning
- High entropy with correct answers indicate uncertainty despite valid reasoning
- Suboptimal chain count leads to either insufficient exploration or wasted compute

**First Experiments**
1. Test inverse-entropy voting on a small dataset with known entropy distributions
2. Compare single-step vs multi-step sequential reasoning on simple problems
3. Validate compute-matching calculations across different model configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scope limited to mathematical reasoning tasks, may not generalize to all domains
- Compute-matching assumes uniform cost assumptions that may not reflect real-world deployment constraints
- Logprob-based confidence estimates may not capture all forms of model uncertainty

## Confidence

| Claim | Confidence |
|-------|------------|
| Sequential reasoning superiority (95.6% win rate) | High |
| Inverse-entropy voting as universally optimal | Medium |
| 6-chain configuration as optimal | Medium |

## Next Checks
1. **Domain Generalization Test**: Apply sequential and parallel methods to non-mathematical reasoning tasks including code generation, scientific reasoning, and multi-modal problems to assess robustness across domains

2. **Temporal and Resource Constraints**: Evaluate performance under strict latency budgets and heterogeneous compute distributions to validate real-world applicability beyond idealized compute-matching

3. **Scaling and Architecture Sensitivity**: Test inverse-entropy voting and sequential advantages across larger model families (beyond 8B parameters) and different architectural variants to assess scalability limits