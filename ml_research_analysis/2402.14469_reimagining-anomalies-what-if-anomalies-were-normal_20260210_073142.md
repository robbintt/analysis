---
ver: rpa2
title: 'Reimagining Anomalies: What If Anomalies Were Normal?'
arxiv_id: '2402.14469'
source_url: https://arxiv.org/abs/2402.14469
tags:
- normal
- anomaly
- each
- anomalies
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for generating counterfactual
  explanations (CEs) for image anomaly detection models. The core idea is to train
  a generator to produce multiple alternative modifications of each anomaly, each
  capturing a different concept of anomalousness, while ensuring these modifications
  are perceived as normal by the anomaly detector.
---

# Reimagining Anomalies: What If Anomalies Were Normal?

## Quick Facts
- **arXiv ID:** 2402.14469
- **Source URL:** https://arxiv.org/abs/2402.14469
- **Reference count:** 40
- **Primary result:** Novel counterfactual explanation method for image anomaly detection that generates semantically meaningful modifications making anomalies appear normal

## Executive Summary
This paper introduces a method for generating counterfactual explanations (CEs) for image anomaly detection models by training a generator to produce multiple alternative modifications of each anomaly, each capturing a different concept of anomalousness while ensuring these modifications are perceived as normal by the anomaly detector. The approach goes beyond traditional feature-attribution methods by addressing what changes would make an anomaly appear normal, providing interpretable explanations that reveal detector biases. The method is evaluated across multiple image datasets and three state-of-the-art anomaly detection models, showing that generated counterfactuals are realistic, capture disentangled concepts, and successfully expose limitations in existing detectors.

## Method Summary
The method trains a generator to transform anomalous images into counterfactuals that appear normal to a pre-trained anomaly detector. The generator takes an input image, a target anomaly score, and a concept index, producing modifications through a minimax game with discriminator and concept classifier networks. The loss function combines GAN objectives, cycle consistency, reconstruction, anomaly score alignment, and concept classification. At inference, the generator produces multiple counterfactuals for each anomaly by varying the concept index, each representing a different way to make the anomaly appear normal while minimizing perceptual changes.

## Key Results
- Generated counterfactuals appear normal to detectors (AuROC close to 50% on most datasets)
- Counterfactuals are realistic with FID scores similar to anomalies
- Concept classifier accuracy exceeds 90% in most cases, indicating successful disentanglement
- Method reveals biases in detectors, such as supervised classifiers overfitting to specific anomaly characteristics

## Why This Works (Mechanism)

### Mechanism 1
The generator learns to produce counterfactuals that appear normal to the anomaly detector by optimizing a targeted loss that directly minimizes the anomaly score. The L_ϕ loss forces the generator output to achieve target anomaly score α via continuous binary cross-entropy. When α=0 at inference, the generator is pressured to produce samples the detector classifies as normal. Core assumption: the anomaly detector provides gradient signals that meaningfully guide the generator toward semantically normal outputs. Break condition: if φ is uncalibrated or produces near-constant scores, the L_ϕ gradient becomes uninformative and counterfactuals collapse.

### Mechanism 2
Disentangled concepts emerge from the concept classifier, which forces the generator to produce semantically distinct modifications for each concept index. The concept loss trains the classifier to correctly identify which concept produced a given counterfactual, creating pressure for different concepts to differ meaningfully. Core assumption: the number of concepts K is appropriately chosen and concepts are learnable from the data distribution. Break condition: if K exceeds the intrinsic dimensionality of normal variations, concepts become redundant or degenerate to noise patterns.

### Mechanism 3
Minimal modifications emerge from reconstruction and cycle-consistency losses that penalize large deviations from the original anomaly. The L1 norm is assumed to correlate with perceptual minimality, with large pixel-wise changes considered undesirable. Core assumption: the L1 norm correlates with perceptual minimality. Break condition: when anomalies and normal samples differ fundamentally, minimal pixel changes cannot bridge the gap and the generator may over-correct.

## Foundational Learning

- **Concept:** GAN adversarial dynamics
  - Why needed here: The generator and discriminator engage in minimax optimization; understanding this equilibrium is essential for debugging training instability.
  - Quick check question: Can you explain why the discriminator must be trained to convergence before the generator can improve?

- **Concept:** Anomaly scoring in deep AD (DSVDD, OE, HSC)
  - Why needed here: The counterfactual generator must interface with different detector architectures; knowing what φ(x) represents determines how to interpret L_ϕ.
  - Quick check question: For DSVDD, what does a low anomaly score indicate about the sample's position in the embedding space?

- **Concept:** Conditional generation and modulation
  - Why needed here: The generator must accept multiple conditioning signals (target score α, concept k); understanding conditional batch normalization helps debug conditioning failures.
  - Quick check question: How would you verify that the generator is actually using α as a conditioning signal rather than ignoring it?

## Architecture Onboarding

- **Component map:** Generator G (Wide ResNet encoder-decoder) -> Discriminator D (ResNet) and Concept Classifier R (ResNet) -> Anomaly Detector φ (pre-trained, frozen)

- **Critical path:**
  1. Pre-train φ on normal data (with OE if applicable)
  2. Freeze φ; initialize G, D, R
  3. Alternate updates: D→G→R following GAN training schedule
  4. At inference, pass α=0 to produce normal counterfactuals

- **Design tradeoffs:**
  - λ_ϕ too high → counterfactuals look normal but are unrealistic (adversarial artifacts)
  - λ_rec/λ_cyc too high → generator becomes conservative, fails to reach normal scores
  - λ_r too low → concepts collapse; too high → excessive diversity at realism cost
  - K selection: Too few concepts miss explanation modes; too many fragment into noise

- **Failure signatures:**
  - Mode collapse: All counterfactuals converge to same output → increase λ_r, check R training
  - Over-correction: Counterfactuals bear no resemblance to original → increase λ_rec
  - Non-normal counterfactuals: CF AuROC >> 50% → check φ is frozen, increase λ_ϕ
  - No disentanglement: Concept accuracy near random → inspect R architecture, ensure it sees (x, x̄) not just x̄

- **First 3 experiments:**
  1. Replicate Colored-MNIST setup (cyan+one normal); verify CF AuROC ≈ 50%, concept accuracy >90%, and visually inspect that one CE changes digit while the other changes color.
  2. Ablate L_con loss on same setup; confirm concept accuracy drops to ~50% and CEs become visually similar across k.
  3. Test on a detector with poor AUROC (e.g., DSVDD on CIFAR-10); observe whether counterfactual quality degrades proportionally, validating the dependency on detector quality.

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal number of categorical concepts (K) be determined automatically rather than requiring manual, problem-specific tuning? The current implementation fixes K as a hyperparameter, and the authors note that balancing objectives in an unsupervised setting becomes more difficult with limited diversity or an incorrect choice of K. A validation metric or optimization loop that dynamically adjusts K based on the stability of the concept classifier or the disentanglement quality without ground-truth supervision would resolve this.

### Open Question 2
Can unsupervised prompt learning replace text-conditional guidance in diffusion models to generate more generic and semantically relevant counterfactuals? The current diffusion-based approach relies on fixed text prompts (e.g., the normal class label), which may limit flexibility or bias the explanation toward specific linguistic features rather than purely visual normality. A comparison of counterfactual quality and diversity between fixed text-prompted generation and a model trained with learnable, unsupervised conditioning vectors would resolve this.

### Open Question 3
How can advancements in the field of adversarial examples be leveraged to prevent the generator from falling into local optima or over-correcting anomalies? The authors observe that the generator sometimes makes unnecessary changes (over-correction) or creates local optima where both color and digit are changed simultaneously. Demonstration that specific regularization techniques or attack strategies derived from adversarial robustness literature result in lower perturbation sizes while maintaining the "normality" of the counterfactuals would resolve this.

### Open Question 4
How can the framework be stabilized to generate meaningful counterfactuals even when the underlying anomaly detector is weak or struggles to distinguish anomalies? If the anomaly detector assigns low anomaly scores to anomalies during training, the generator lacks the necessary gradient signal to transform the image, resulting in failures of explanation. An architectural modification or auxiliary loss that decouples the generator's performance from the detector's classification accuracy would resolve this.

## Limitations

- Method effectiveness is tightly coupled to the quality of the underlying anomaly detector, with poor detectors producing less meaningful counterfactuals
- Approach assumes minimal modifications in pixel space correspond to semantic changes, which may not hold for fundamental class differences
- Method inherits GAN training instability risks including mode collapse and convergence to adversarial rather than semantically normal outputs

## Confidence

- **High:** Core mechanism of using targeted anomaly score optimization works as described
- **Medium:** Disentanglement claims, as concept classifier accuracy is high but qualitative interpretability varies
- **Low:** Generalization to complex real-world anomaly distributions with fundamental class differences

## Next Checks

1. Test counterfactual generation when anomaly detector has near-random performance (AUROC ~50%) to verify the generator doesn't produce arbitrary outputs
2. Evaluate whether the minimal modification assumption holds by comparing counterfactuals against ground-truth semantic changes on controlled datasets
3. Assess sensitivity to concept number K by systematically varying it and measuring concept classifier performance and visual quality degradation