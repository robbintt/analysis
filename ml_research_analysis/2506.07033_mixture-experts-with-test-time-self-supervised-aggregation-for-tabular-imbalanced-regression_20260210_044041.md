---
ver: rpa2
title: Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced
  Regression
arxiv_id: '2506.07033'
source_url: https://arxiv.org/abs/2506.07033
tags:
- test
- regression
- expert
- imbalanced
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of imbalanced regression in tabular
  data, where traditional methods struggle with continuous label boundaries and varying
  test distributions. The authors propose MATI (Mixture Experts with Test-Time Self-Supervised
  Aggregation), a novel approach featuring two key innovations: (1) Region-Aware Mixture
  Expert training using Gaussian Mixture Models to synthesize and train region-specific
  experts, and (2) Test-Time Self-Supervised Expert Aggregation that dynamically adjusts
  expert weights based on test data features without requiring test labels.'
---

# Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression

## Quick Facts
- arXiv ID: 2506.07033
- Source URL: https://arxiv.org/abs/2506.07033
- Reference count: 40
- Primary result: 7.1% average MAE improvement over existing methods on imbalanced regression tasks

## Executive Summary
This paper addresses the challenge of imbalanced regression in tabular data, where continuous label boundaries and varying test distributions create difficulties for traditional methods. The authors propose MATI, a novel approach that combines region-aware mixture expert training with test-time self-supervised aggregation. The method achieves significant improvements by creating specialized experts for different target regions and dynamically adjusting their weights during inference without requiring test labels.

## Method Summary
MATI operates in two phases: (1) Region-Aware Mixture Expert Training uses GMM to partition target space, then synthesizes and trains specialized experts on each region using SMOGN oversampling; (2) Test-Time Self-Supervised Expert Aggregation dynamically adjusts expert weights by minimizing prediction consistency between perturbed views of unlabeled test samples. The approach handles continuous label boundaries through statistical partitioning and adapts to unknown test distributions through the consistency-based aggregation mechanism.

## Key Results
- 7.1% average MAE improvement across four datasets and three test distribution types
- Superior performance on inverse test distributions where tail regions become majority
- Maintains robustness across balanced, normal, and inverse test distributions
- Effective handling of continuous label boundaries through region specialization

## Why This Works (Mechanism)

### Mechanism 1: Region-Aware Specialization via Statistical Partitioning
The paper uses GMM to partition continuous target space into N components characterized by μ_n and σ_n. A synthesizer then oversamples data within each region's range to train specialized experts. This creates models focused on "few-shot" regions that would be drowned out in global training. The approach assumes the data distribution can be approximated by a mixture of Gaussians, and statistical boundaries define valid expertise regions.

### Mechanism 2: Test-Time Aggregation via Prediction Consistency
Expert weights are dynamically calibrated for unseen test distributions by minimizing the prediction gap between perturbed views of unlabeled samples. The method assumes that experts with genuine expertise produce stable predictions under perturbation. During inference, two views are generated via corruption, and weights are optimized to minimize MSE between aggregated predictions, enforcing that contributing experts demonstrate stability.

### Mechanism 3: Distributional Robustness via Decoupled Optimization
By decoupling expert training from test-time adaptation, the system achieves robustness against distribution shifts. Unlike methods that reweight loss functions globally, MATI isolates skills in frozen experts and uses the test-time loop as a selector. This allows pivoting between distributions - on normal sets, head-region experts are weighted highly; on inverse sets, weights shift to tail-region experts without retraining.

## Foundational Learning

- **Concept: Gaussian Mixture Models (GMM)**
  - Why needed here: Partitions continuous target space to define expertise regions using μ (mean) and σ (variance)
  - Quick check question: If a target variable is bimodal (ages 20 and 60), how many Gaussian components should AIC ideally select to prevent a single expert from learning an average of the two peaks?

- **Concept: Self-Supervised Consistency Regularization**
  - Why needed here: Enables test-time adaptation by learning weights from prediction consistency between perturbed views, not labels
  - Quick check question: In regression, why is minimizing (ŷ₁ - ŷ₂)² a suitable proxy for accuracy without ground truth labels?

- **Concept: Imbalanced Regression (SMOGN/SMOTE)**
  - Why needed here: Generates training data for experts by injecting Gaussian noise to synthesize samples in minority regions
  - Quick check question: Why does the paper use SMOGN for the "Region-Aware" synthesizer rather than standard random oversampling?

## Architecture Onboarding

- **Component map:** Input → GMM Router → Synthesizer → Expert Bank → Test-Time Loop → Weighted Aggregation
- **Critical path:** The Test-Time Self-Supervised Loop is where architecture differs from standard ensembles, handling forward pass → optimize weights → forward pass for each unlabeled test batch
- **Design tradeoffs:**
  - Number of Experts (N): Too few re-introduces imbalance inside experts; too many cause data starvation
  - Perturbation Ratio (r): Performance drops if r > 0.4; requires careful tuning between stability signal and data utility
- **Failure signatures:**
  - Weight Collapse: Aggregation weights converge to one-hot vectors if one expert marginally more stable
  - MAE Degradation on Normal Split: Inverse gains come at cost of worse normal split performance
- **First 3 experiments:**
  1. Region Purity Check: Train experts using GMM partitioning, evaluate each expert on all regions to verify Expert n performs best on Region n
  2. Perturbation Sensitivity Sweep: Run test-time aggregation with corruption rates r ∈ [0.1, 0.7] on validation split, plot performance vs. corruption rate
  3. Distribution Shift Stress Test: Construct artificial inverse test set where tail classes are majority, compare MATI against Vanilla TabNet on few-shot regions

## Open Questions the Paper Calls Out
- Can a unified framework handle both imbalanced regression and classification tasks in tabular data without task-specific modifications?
- How can multi-modal features be effectively incorporated to improve representation learning for few-shot regions in imbalanced regression?
- How does test-time computational overhead scale with dataset size and expert count, and can the aggregation process be accelerated?

## Limitations
- Limited evaluation on datasets with extreme target skew (>10) or multimodal distributions poorly captured by GMM
- Multiple hyperparameters (N, r, GMM covariance type, α) significantly impact performance without comprehensive sensitivity analysis
- Test-time self-supervised loop introduces computational overhead not present in standard inference, with no reported inference time comparisons

## Confidence
- **High Confidence (8-10/10):** GMM-based expert specialization mechanism is well-supported by theoretical arguments and empirical results
- **Medium Confidence (5-7/10):** Test-time aggregation mechanism's theoretical justification is sound but practical effectiveness depends heavily on perturbation function quality
- **Low Confidence (1-4/10):** Claims about robustness to unknown test distributions rely on synthetic splits that may not capture real-world distribution shifts

## Next Checks
1. **Extreme Imbalance Stress Test:** Evaluate MATI on datasets with target skew >10 (income prediction, rare event forecasting) to test GMM partitioning and expert specialization limits
2. **Perturbation Ablation Study:** Systematically vary corruption ratio r and perturbation method to identify sensitivity of test-time aggregation performance
3. **Cross-Dataset Transferability:** Train experts on one dataset, evaluate test-time aggregation on different dataset with similar target distribution characteristics to assess generalization