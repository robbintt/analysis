---
ver: rpa2
title: 'Agnostics: Learning to Code in Any Programming Language via Reinforcement
  with a Universal Learning Environment'
arxiv_id: '2508.04865'
source_url: https://arxiv.org/abs/2508.04865
tags:
- code
- training
- zhang
- language
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agnostics introduces a language-agnostic post-training pipeline\
  \ that enables reinforcement learning for low-resource programming languages. The\
  \ core innovation is judging code solely by its externally observable behavior\u2014\
  input/output\u2014using a single universal verifier that works for any language."
---

# Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment

## Quick Facts
- arXiv ID: 2508.04865
- Source URL: https://arxiv.org/abs/2508.04865
- Authors: Aleksander Boruch-Gruszecki; Yangtian Zi; Zixuan Wu; Tejas Oberoi; Carolyn Jane Anderson; Joydeep Biswas; Arjun Guha
- Reference count: 40
- Primary result: Agnostics enables RL post-training in any programming language by judging code solely on I/O behavior, achieving SOTA results on MultiPL-E and LiveCodeBench for models up to 16B parameters.

## Executive Summary
Agnostics introduces a language-agnostic post-training pipeline that enables reinforcement learning for low-resource programming languages. The core innovation is judging code solely by its externally observable behavior—input/output—using a single universal verifier that works for any language. Agnostics reformulates language-specific datasets into this I/O format, applies a short language-specific configuration to specify compilation and execution, and uses reinforcement learning with verifiable rewards (RLVR) in a robust sandbox environment. Applied to Lua, Julia, R, OCaml, and Fortran, Agnostics improves Qwen-3 4B performance to levels rivaling 16B–70B models and sets new state-of-the-art pass@1 results for models up to 16B parameters on MultiPL-E and LiveCodeBench. The method scales to larger and diverse model families, works on easier datasets like MBPP, and significantly reduces fundamental programming mistakes in generated code. Agnostics makes RL post-training in any programming language as simple as editing a short YAML file, eliminating per-language engineering bottlenecks.

## Method Summary
Agnostics is a post-training pipeline that applies reinforcement learning with verifiable rewards to improve coding performance in low-resource programming languages. The system reformulates programming tasks to specify behavior via standard input/output pairs and uses a single universal verifier to test solutions written in any language. It employs Group-Relative Policy Optimization (GRPO) with a robust execution sandbox to handle compilation, execution, timeouts, and resource limits. The approach requires only a short language-specific configuration file and works by sampling candidate solutions, verifying them in a sandboxed environment, and updating the model based on verifiable binary rewards (pass/fail). Agnostics successfully scales to models up to 8B parameters and demonstrates significant improvements across multiple low-resource languages.

## Key Results
- Agnostics improves Qwen-3 4B performance to levels rivaling 16B–70B models on low-resource languages
- Sets new state-of-the-art pass@1 results for models up to 16B parameters on MultiPL-E and LiveCodeBench
- Reduces fundamental programming mistakes (syntax errors, function misuse) while potentially increasing logic flaws
- Works on easier datasets like MBPP and scales to larger and diverse model families
- Makes RL post-training as simple as editing a short YAML configuration file per language

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If code correctness is judged solely by externally observable I/O behavior, a single verifier can test programs written in any language.
- **Mechanism:** The system reformulates programming tasks to specify behavior via standard input/output pairs. A universal verifier executes a candidate program, feeds it the specified inputs, and compares its actual outputs to expected outputs.
- **Core assumption:** Correctness for a significant class of programming tasks can be fully captured by I/O behavior, and the program's internal structure is irrelevant for verification.
- **Evidence anchors:** [abstract]: "The key idea is to judge code solely by its externally observable behavior, so a single verifier can test solutions written in any language." [section 3]: "Our central idea is to have tasks ask for programs which behave in a particular way."
- **Break condition:** This mechanism fails for tasks where correctness cannot be specified by discrete I/O examples (e.g., GUI interactions, programs with side effects, or non-deterministic outputs).

### Mechanism 2
- **Claim:** If existing unit-test datasets can be automatically reformulated into a language-agnostic I/O format, the cost of creating training data for new programming languages is dramatically reduced.
- **Mechanism:** The authors use a capable LLM (e.g., Qwen3-32B) to translate problems from a unit-test format into a standardized I/O format, producing task descriptions, input/output format specifications, and concrete I/O test cases in JSON.
- **Core assumption:** A sufficiently capable LLM can accurately translate the semantics of a unit-test problem into an equivalent I/O specification without human intervention.
- **Evidence anchors:** [abstract]: "Concretely, we (i) use an LLM to rewrite existing unit-test datasets into an I/O format." [section 3.1 & Appendix B]: Successfully converted 776/974 MBPP problems in under one hour.
- **Break condition:** The mechanism breaks down if the LLM mistranslates the problem semantics, leading to incorrect I/O specifications that provide a faulty training signal.

### Mechanism 3
- **Claim:** If reinforcement learning with verifiable rewards (RLVR) is applied using Group-Relative Policy Optimization (GRPO) and a robust execution sandbox, a model can improve its coding ability in a target language from minimal initial competency.
- **Mechanism:** The GRPO algorithm samples a group of candidate solutions per problem. Each solution is executed and verified in a sandboxed environment (e.g., an OCI container). Binary rewards (1 for passing all tests, 0 otherwise) are converted to group-relative advantages, which guide policy updates.
- **Core assumption:** The model has a non-zero probability of generating a correct solution (even if very low), allowing GRPO to bootstrap learning. The sandbox is secure and correctly judges program behavior.
- **Evidence anchors:** [abstract]: "applies a short configuration file per language, and uses Group-Relative Policy Optimization with a robust language-agnostic execution sandbox." [section 3.3]: Details the GRPO objective function and sandbox design.
- **Break condition:** Learning stalls if the initial pass rate is exactly zero for all problems in a batch (no positive reward signal). The approach also fails if the sandbox is insecure and allows programs to tamper with the verifier.

## Foundational Learning

- **Concept:** Reinforcement Learning with Verifiable Rewards (RLVR)
  - **Why needed here:** The core training loop uses RLVR, where the reward is not a learned model's opinion but a deterministic, verifiable outcome (did the code pass the tests?). Understanding this is crucial to grasp why the system avoids reward hacking common with learned reward models.
  - **Quick check question:** How does a verifiable reward differ from a reward model, and what advantage does it offer for training code LLMs?

- **Concept:** Group-Relative Policy Optimization (GRPO)
  - **Why needed here:** GRPO is the specific RL algorithm used. It computes advantages by comparing a solution's reward to the mean and standard deviation of rewards from other solutions in the same group (prompt). This stabilizes training without needing a separate value model.
  - **Quick check question:** In GRPO, how is the advantage (`Â_i`) calculated for a candidate solution `i`? Why is normalization by group statistics important?

- **Concept:** Sandboxed Code Execution
  - **Why needed here:** To train safely, the system must execute untrusted, model-generated code. The sandbox provides isolation (containers), resource limits (CPU, memory, output size), and timeouts to prevent infinite loops or system crashes from affecting the training infrastructure.
  - **Quick check question:** List three safety measures the Agnostics sandbox employs to handle potentially malicious or buggy code. Why is a fixed-size read buffer used for program output?

## Architecture Onboarding

- **Component map:** Data Reformulation Module -> Language Configuration -> Trainer (GRPO Engine) -> Universal Execution Sandbox -> Training Datasets

- **Critical path:**
  1. **Prepare Data:** Run the reformulation module on a source dataset (e.g., MBPP) → Agnostics-format dataset
  2. **Create Language Config:** Write a YAML for the target language (e.g., `lua.yaml`) with install, execute, and prompt fields
  3. **Launch Training:** The trainer samples prompts, prepends `L.prompt`, and generates candidate solutions
  4. **Verify & Update:** The sandbox executes each candidate. Rewards are computed. GRPO updates the model policy
  5. **Iterate:** Repeat for all batches across the dataset

- **Design tradeoffs:**
  - **I/O Format vs. Unit Tests:** I/O is universal but may not suit all problem types (e.g., library design). Unit tests are expressive but require language-specific harnesses
  - **Sandbox Security vs. Speed:** Using warm containers and a RAM disk improves speed but requires careful management to prevent state leakage between runs
  - **Minimal Prompt Prefix vs. Detailed Instructions:** A minimal prefix (e.g., "Use Lua 5.1") works if the model has ~5% accuracy. Detailed instructions help bootstrap from near-zero but risk overfitting to the prompt style

- **Failure signatures:**
  - **Low or Flat Rewards:** If pass@1 stays near zero, the model cannot generate any valid solution. Check the language config and prompt prefix for basic errors
  - **Sandbox Timeouts/Crashes:** If containers frequently crash, a candidate program may be exploiting a resource limit. Review timeout values and output buffer size
  - **Model Overfitting to Prompt Prefix:** If performance drops when the prefix is removed (during evaluation), the model learned to rely on instructions
  - **Data Mistranslation:** If analysis shows many logical errors, the I/O reformulation might be flawed. Manually inspect reformulated problems

- **First 3 experiments:**
  1. **Sanity Check with a Known Language:** Configure and train a small model (e.g., Qwen-3 4B) on a single language (e.g., Lua) using Ag-Codeforces-X. Verify that pass@1 on a held-out set improves compared to the base model
  2. **Ablate the Prompt Prefix:** Train two models for a low-accuracy language (e.g., OCaml): one with the detailed LLM-generated prefix and one with a minimal prefix. Compare learning curves to measure the prefix's impact on bootstrapping
  3. **Error Taxonomy Analysis:** After training, classify bugs in the model's outputs before and after training (using the paper's taxonomy). Verify that fundamental errors (syntax, function misuse) decrease while logic flaws may increase, as observed in the paper

## Open Questions the Paper Calls Out

- **Question:** How does Agnostics training scale to models significantly larger than 8B parameters?
  - **Basis in paper:** [explicit] "We see no reason to expect the approach to not work for models larger than 8B, although our experiments are limited by available compute."
  - **Why unresolved:** The authors only experimented up to 8B models due to compute constraints; scaling behavior at 30B–70B+ scales remains empirically untested.
  - **What evidence would resolve it:** Training runs on models ≥30B parameters showing comparable relative improvements on Ag-LiveCodeBench-X and MultiPL-E.

- **Question:** What is the minimum model capacity required for Agnostics training to yield improvements?
  - **Basis in paper:** [inferred] "We found that Agnostics training with Ag-Codeforces-X does not improve two smaller models: Qwen 3 1.7B and Llama 3.2 3B Instruct, perhaps due to the problems being too difficult."
  - **Why unresolved:** The paper does not systematically characterize the failure mode—whether it is model size, prior training, or problem difficulty that causes smaller models to fail.
  - **What evidence would resolve it:** Ablation experiments varying model size, task difficulty (e.g., using MBPP-X vs Codeforces-X), and pretraining quality to identify the critical factors.

- **Question:** Can Agnostics be extended to problem formulations beyond stdin/stdout I/O, such as unit-test-based verification or API-style tasks?
  - **Basis in paper:** [explicit] "In this paper, we limited ourselves to working with tasks asking for programs which read data from the standard input, compute a unique answer, and write it to the standard output."
  - **Why unresolved:** The authors deliberately scoped to I/O for simplicity; the pipeline may require different extraction, execution, or reward designs for other task formulations.
  - **What evidence would resolve it:** Extending the verifier to support function-level unit tests or REST API mocking, then evaluating whether similar improvements hold.

## Limitations

- Training code has not been released, making faithful reproduction impossible without substantial engineering effort
- Performance improvements measured against base models that already have some exposure to these languages through pretraining
- Effectiveness for truly "zero-resource" languages with no pretraining exposure remains unproven
- Only tested on 5 languages, all of which have some pretraining data and community presence
- Datasets are synthetic reformulations of existing datasets, not naturally occurring low-resource language datasets

## Confidence

- **High Confidence:** The core mechanism of I/O-based verification working across languages
- **Medium Confidence:** The LLM-based dataset reformulation
- **Medium Confidence:** GRPO with sandbox achieving learning from minimal initial accuracy
- **Low Confidence:** Generalization to arbitrary low-resource languages

## Next Checks

1. **Dataset Quality Audit:** Manually inspect 50 randomly sampled problems from Ag-MBPP-X and Ag-Codeforces-X to verify that the LLM reformulation preserved problem semantics accurately.

2. **Zero-Resource Language Test:** Apply the full Agnostics pipeline to a truly low-resource language (e.g., COBOL or a niche domain-specific language) that has minimal or no representation in the pretraining corpus.

3. **Sandbox Security Validation:** Design and run adversarial programs targeting each sandbox safety mechanism (timeouts, memory limits, output size limits). Document any successful escapes or resource exhaustion attacks.