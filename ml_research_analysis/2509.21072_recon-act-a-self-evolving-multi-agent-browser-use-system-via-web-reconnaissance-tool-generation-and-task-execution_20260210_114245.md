---
ver: rpa2
title: 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance,
  Tool Generation, and Task Execution'
arxiv_id: '2509.21072'
source_url: https://arxiv.org/abs/2509.21072
tags:
- tool
- tools
- action
- wang
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Recon-Act, a self-evolving multi-agent framework
  designed to improve the performance of browser-use agents on complex, long-horizon
  tasks. The system addresses the challenge of disordered action sequencing and excessive
  trial-and-error in unfamiliar web environments.
---

# Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution

## Quick Facts
- arXiv ID: 2509.21072
- Source URL: https://arxiv.org/abs/2509.21072
- Reference count: 9
- Primary result: Achieves 36.48% overall success rate on VisualWebArena, surpassing previous methods.

## Executive Summary
Recon-Act introduces a self-evolving multi-agent framework to address disordered action sequencing and trial-and-error inefficiencies in complex browser-use tasks. The system employs a dual-team structure: a Reconnaissance Team that compares successful and failed trajectories to infer remedies, and an Action Team that executes tasks using dynamically generated tools. Tools are abstracted from remedial strategies and registered in real-time, enabling a closed-loop data-tools-action-feedback pipeline. Experiments demonstrate state-of-the-art performance, with a 36.48% success rate overall and 39.27% on the Shopping subdomain, while improving adaptability to unseen websites.

## Method Summary
The Recon-Act framework uses a dual-team multi-agent system operating at Level 3 autonomy (with human-in-the-loop intervention). The Reconnaissance Team, consisting of a Human Analyst and an LLM Coder, compares failed trajectories against successful ones to infer root causes and generate generalized tools (code or hints). The Action Team, comprising an LLM Master, a Human Tool Manager, and an LLM Execution Agent, executes tasks using these tools. The system dynamically registers tools in two modes: Decision (authoritative action) and Hint (advisory guidance). This creates a self-evolving loop where failures trigger tool generation, which in turn improves future task execution.

## Key Results
- Achieves 36.48% overall success rate on VisualWebArena benchmark.
- Surpasses previous methods, with 39.27% success rate on the Shopping subdomain.
- Improves adaptability to unseen websites and reduces trial-and-error inefficiencies.

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Trajectory Analysis
- **Claim:** Comparing erroneous and successful trajectories step-by-step allows the system to infer specific remedies for disordered action sequencing.
- **Mechanism:** The Analyst compares a failed trajectory against a successful one to identify the exact step of divergence and propose a remediation strategy.
- **Core assumption:** A library of successful trajectories exists for reference; failure modes are consistent enough to generalize.
- **Evidence anchors:** [abstract] Contrastive analysis infers remedies; [section 3.1] Step-level comparison identifies root causes.
- **Break condition:** No successful reference trajectory exists in the training set.

### Mechanism 2: Generalized Tool Abstraction (Hint vs. Decision)
- **Claim:** Abstracting remedies into generalized tools (code or hints) bypasses trial-and-error in future iterations.
- **Mechanism:** The Coder translates the Analyst's remedy into executable code or a hint, registered in Decision-mode (authoritative) or Hint-mode (advisory).
- **Core assumption:** The Coder generates valid, generalizable code/hints.
- **Evidence anchors:** [abstract] Tools expressed as hints or rule-based codes; [section 3.2] Two registration modes.
- **Break condition:** Tools are overly narrow and fail to generalize beyond specific contexts.

### Mechanism 3: Level-Based Human-AI Collaboration (Level 3)
- **Claim:** Decoupling high-variance reasoning (Analysis/Tool Management) from execution stabilizes the self-evolution pipeline.
- **Mechanism:** Human-operated Analyst and Tool Manager handle complex tasks, while model-driven agents manage execution.
- **Core assumption:** Human intervention is superior to LLMs at root-cause analysis and code merging.
- **Evidence anchors:** [abstract] Level 3 with limited human-in-the-loop; [table 2] Human roles for Analyst and Tool Manager.
- **Break condition:** Human analytical capacity becomes a bottleneck at scale.

## Foundational Learning

- **Concept: Trajectory Divergence Analysis**
  - **Why needed here:** Pinpoint the exact action index where a successful path and a failed path split in a log file.
  - **Quick check question:** Can you identify the specific step where a "go to cart" action turned into a "logout" action?

- **Concept: Tool Registration & Routing**
  - **Why needed here:** Dynamically add functions to the Action Team's API.
  - **Quick check question:** How does a function signature differ when designed for a "Hint" (advisory) vs. a "Decision" (imperative) execution flow?

- **Concept: Browser State Representation (SOM/Accessibility Tree)**
  - **Why needed here:** Agents rely on Set-of-Marks (SOM) or accessibility trees to interact with the DOM.
  - **Quick check question:** If a "Buy" button lacks a unique ID, how would a tool locate it?

## Architecture Onboarding

- **Component map:** Reconnaissance Team (Human Analyst, LLM Coder) -> Action Team (LLM Master, Human Tool Manager, LLM Execution Agent) -> Shared Tool Archive, Browser Context (Playwright).

- **Critical path:** 1. Execution Agent fails a task. 2. Analyst compares fail vs. success trace → defines remedy. 3. Coder generates code → Tool Manager validates/registers. 4. Master calls new tool in next run.

- **Design tradeoffs:**
  - **Decision vs. Hint Tools:** Decision tools are robust but brittle; Hint tools are flexible but require smart interpretation.
  - **Level 3 Autonomy:** Trading speed/scale for reliability by keeping Humans in Analyst/Tool Manager roles.

- **Failure signatures:**
  - **Tool Proliferation:** Many narrow tools instead of general tools.
  - **Branch Conflicts:** New tool logic breaks existing tool logic.
  - **Context Ambiguity:** Master calls a tool on the wrong page type.

- **First 3 experiments:**
  1. **Mock the Analyst:** Feed 3 manually written "Analyst Reports" and verify the Coder generates valid Python tools.
  2. **Tool Router Test:** Register one "Hint" and one "Decision" tool; verify Master selects appropriately.
  3. **Trace Contrast:** Run a task with and without a "PriceSorter" tool to measure step reduction.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the "Analyst" and "Tool Manager" components be fully automated to progress beyond Level 3 without compromising tool quality?
  - **Basis:** Section 6 highlights the need to strengthen reasoning and coding competence for higher autonomy.
  - **Why unresolved:** Current LLM/VLM limitations require human-in-the-loop intervention for reliability.
  - **Evidence needed:** Successful execution with fully LLM-driven Analyst and Tool Manager achieving comparable performance.

- **Open Question 2:** Can the reconnaissance module generalize to a broader range of heterogeneous web environments?
  - **Basis:** Section 6 notes the module performs well only on a fixed set of websites.
  - **Why unresolved:** Reliance on site-specific tool creation limits robustness to diverse structures.
  - **Evidence needed:** Evaluation on a benchmark with significantly more diverse website architectures.

- **Open Question 3:** Can random-walk-style self-exploration replace human-authored trajectories for autonomous training data generation?
  - **Basis:** Section 6 proposes random-walk exploration to reduce dependence on supervised training sets.
  - **Why unresolved:** Autonomously generating high-quality successful trajectories remains unproven.
  - **Evidence needed:** Demonstration of successful learning using training data generated through autonomous exploration.

## Limitations

- **Human Bottleneck:** Current Level 3 implementation relies heavily on human Analyst and Tool Manager, limiting scalability.
- **Model Dependency:** Claims of GPT-5-Chat performance may not be reproducible with publicly available models like GPT-4o.
- **Generalization Uncertainty:** Tools may overfit to specific website structures, reducing effectiveness on unseen environments.

## Confidence

- **Contrastive Analysis Mechanism:** Medium confidence; strong theoretical grounding but lacks independent validation.
- **Tool Generation Pipeline:** Medium confidence; dependent on proprietary model access and unopen-sourced prompts.
- **Self-Evolving Claim:** Low confidence in current Level 3 form; true autonomy requires advancing beyond human-in-the-loop stages.

## Next Checks

1. **Ablation on Human Roles:** Remove human Analyst and Tool Manager; measure performance drop when LLM agents handle analysis and tool registration.
2. **Tool Generalization Test:** Test a tool generated for one website on structurally similar but unseen sites to measure true generalization.
3. **Model Access Verification:** Confirm whether GPT-4o can reproduce the tool generation quality claimed for GPT-5-Chat.