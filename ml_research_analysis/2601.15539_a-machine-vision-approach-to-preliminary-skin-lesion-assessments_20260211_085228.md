---
ver: rpa2
title: A Machine Vision Approach to Preliminary Skin Lesion Assessments
arxiv_id: '2601.15539'
source_url: https://arxiv.org/abs/2601.15539
tags:
- lesion
- skin
- learning
- features
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a machine vision system for preliminary skin
  lesion assessment combining the ABCD dermoscopy rule with machine learning classification.
  A custom three-layer CNN trained from scratch on 1,000 dermoscopic images achieved
  78.5% accuracy and 86.5% recall, outperforming traditional ML classifiers (59.5%
  accuracy, 61.0% recall) and transfer learning with EfficientNet-B0 (4% recall).
---

# A Machine Vision Approach to Preliminary Skin Lesion Assessments

## Quick Facts
- arXiv ID: 2601.15539
- Source URL: https://arxiv.org/abs/2601.15539
- Reference count: 8
- Primary result: Custom 3-layer CNN achieved 78.5% accuracy and 86.5% recall on binary skin lesion classification, outperforming both handcrafted features and transfer learning from ImageNet

## Executive Summary
This study develops a machine vision system for preliminary skin lesion assessment by combining the ABCD dermoscopy rule with machine learning classification. The research demonstrates that purpose-built lightweight convolutional neural networks can outperform both traditional handcrafted feature approaches and transfer learning from unrelated domains when working with small, specialized medical datasets. A custom three-layer CNN trained from scratch on 1,000 dermoscopic images achieved 78.5% accuracy and 86.5% recall, significantly surpassing traditional ML classifiers (59.5% accuracy, 61.0% recall) and transfer learning with EfficientNet-B0 (4% recall).

## Method Summary
The study uses a balanced 1,000-image subset from the HAM10000 dataset (224×224 resolution) for binary classification of dermoscopic skin lesions. Three preprocessing variants are tested: median filtering (3×3), Gaussian filtering (3×3, σ=1.0), and flat averaging (3×3). The pipeline includes two parallel branches: (1) traditional ML with ABCD features extracted via Otsu segmentation and clinical scoring rules, and (2) a custom CNN architecture with three convolutional blocks (16→32→64 filters, 3×3 kernels, ReLU, 2×2 maxpool) followed by a fully connected layer. Models are evaluated using accuracy, recall, precision, F1, and ROC-AUC metrics.

## Key Results
- Custom 3-layer CNN achieved 78.5% accuracy and 86.5% recall
- Traditional ML classifiers with ABCD features reached 59.5% accuracy and 61.0% recall
- Transfer learning with EfficientNet-B0 failed with 4% recall due to domain shift
- Median filtering proved optimal for preprocessing, preserving edge information better than Gaussian or flat averaging
- Custom CNN outperformed traditional ML by 19 points in accuracy and 25 points in recall

## Why This Works (Mechanism)

### Mechanism 1: ABCD Feature Dimensionality Reduction
Reducing complex lesion morphology to five numerical features creates an information bottleneck that limits classification accuracy. The ABCD pipeline compresses high-resolution images into Asymmetry, Border, Color, Dermoscopic Structures, and Total Dermoscopy Score, discarding subtle textural and spatial patterns that distinguish malignant from benign lesions. Traditional classifiers plateau at ~60% accuracy regardless of algorithm choice, indicating the feature space limitation rather than model limitation.

### Mechanism 2: Domain Shift in Transfer Learning
Transfer learning from ImageNet to dermoscopic imagery fails due to domain shift between natural and medical images. EfficientNet-B0 learned features optimized for distinguishing everyday objects (cars, animals, furniture) that do not correspond to diagnostically relevant structures in skin lesions. With only 1,000 training images, the model defaulted to the majority class, achieving only 4% recall.

### Mechanism 3: Lightweight CNN Adaptation
Purpose-built lightweight CNNs can outperform both handcrafted features and large pretrained models on small, domain-specific medical datasets. The custom 3-layer CNN (16→32→64 filters) learns domain-relevant features directly from pixels without carrying irrelevant prior knowledge. With ~50x fewer parameters than EfficientNet-B0, it avoids overfitting while capturing patterns beyond ABCD encoding.

## Foundational Learning

- **Otsu's Thresholding for Binary Segmentation**
  - Why needed here: Separates lesion from healthy skin by finding the optimal intensity threshold that maximizes inter-class variance. Required for all downstream ABCD feature extraction.
  - Quick check question: Given a bimodal histogram with lesion pixels (dark) and background (light), what threshold maximizes between-class variance?

- **Transfer Learning and Domain Shift**
  - Why needed here: Understanding why ImageNet pretraining failed helps avoid costly mistakes when adapting models to medical imaging. Features learned on natural images (edges, textures, object parts) may have no correspondence to diagnostic structures in dermoscopy.
  - Quick check question: Why might a model that excels at distinguishing dogs from cats fail to distinguish benign from malignant lesions?

- **Recall vs. Precision in Clinical Screening**
  - Why needed here: The paper prioritizes recall (86.5%) because false negatives (missed malignancies) carry severe consequences. A model with 80% precision but 4% recall (EfficientNet-B0) is clinically dangerous despite high precision.
  - Quick check question: In a cancer screening application, would you prioritize minimizing false positives or false negatives? How does this differ from spam detection?

## Architecture Onboarding

- **Component map**: Input Image (224×224×3) → Preprocessing (Median Filter 3×3) → Branch A: ABCD Pipeline (Otsu Threshold → Mask → A/B/C/D extraction → TDS calculation → Traditional ML classifiers) OR Branch B: Custom CNN (Conv(16) → Conv(32) → Conv(64) → FC(64) → 1) → Binary Output (Benign/Malignant)

- **Critical path**: Preprocessing (median filter) → Segmentation (Otsu + morphological ops) → Feature extraction OR CNN forward pass → Classification. Median filtering choice directly impacts edge preservation for both branches.

- **Design tradeoffs**:
  - Handcrafted vs. Learned: ABCD features offer clinical interpretability (dermatologists can verify each score) but sacrifice accuracy. CNN offers 19-point accuracy gain but is a black box.
  - Pretrained vs. Scratch: EfficientNet-B0 provides rich features but requires large domain-matched datasets. Custom CNN requires more training effort but adapts to small specialized data.
  - Filter choice: Median preserves edges better than Gaussian (better for B-score gradient analysis) but may retain some noise.

- **Failure signatures**:
  - Transfer learning collapse: Model predicts one class almost exclusively (4% recall, 80% precision → defaults to majority). Indicates domain shift + insufficient data to adapt.
  - ABCD bottleneck: Traditional classifiers plateau at ~60% accuracy regardless of algorithm choice. Indicates feature space limitation, not model limitation.
  - Overfitting in custom CNN: Training accuracy >> validation accuracy with small datasets. Monitor with held-out test set.

- **First 3 experiments**:
  1. Reproduce preprocessing comparison: Train the custom CNN on median-filtered vs. Gaussian-filtered vs. flat-average images. Expect median to achieve highest recall based on paper results.
  2. Validate domain shift hypothesis: Fine-tune EfficientNet-B0 with progressively larger subsets (1K, 5K, 10K images) to identify the data threshold where transfer learning becomes viable.
  3. Hybrid architecture test: Concatenate ABCD features (5 dimensions) with CNN flattened features before the final classification layer to test whether interpretability and accuracy can coexist.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid ensemble architecture combining handcrafted ABCD features with CNN pattern recognition simultaneously achieve higher accuracy than the standalone custom CNN and maintain clinical interpretability? The authors propose that "hybrid ensemble architectures combining ABCD features with CNN pattern recognition could combine interpretability and learning power simultaneously."

### Open Question 2
Does domain-specific pre-training on dermoscopic images significantly improve transfer learning performance compared to ImageNet pre-training? The paper notes EfficientNet-B0 failed due to "domain shift" and suggests "domain-specific transfer learning using models pretrained on medical imaging datasets" as a solution.

### Open Question 3
How does the relative performance of lightweight custom CNNs versus large pre-trained models change as the dataset size scales to the full HAM10000 dataset? The authors suggest "increasing the dataset size to the entirety of the HAM10000 dataset... would significantly improve the generalizability."

## Limitations

- Limited dataset size (1,000 images) restricts generalizability of findings to larger datasets
- Binary classification approach doesn't address specific skin cancer subtypes or their clinical nuances
- ABCD feature extraction pipeline may not capture all clinically relevant patterns beyond the four criteria
- Custom CNN with only three layers may not scale well to more complex lesion types or larger datasets

## Confidence

- High confidence: Comparative performance metrics (78.5% vs 59.5% accuracy, 86.5% vs 61.0% recall) are well-documented and reproducible. Domain shift explanation for transfer learning failure is mechanistically sound.
- Medium confidence: Claim that custom CNNs outperform transfer learning on small medical datasets needs validation on larger datasets. Optimal preprocessing choice (median filtering) is supported but not extensively tested.
- Low confidence: Assertion that handcrafted features create an irreducible information bottleneck may not hold for more sophisticated feature engineering approaches.

## Next Checks

1. **Scale-up validation**: Evaluate the custom CNN and transfer learning approaches on the full HAM10000 dataset (10,000+ images) to test scalability claims and determine if transfer learning becomes viable with more data.

2. **Cross-dataset testing**: Test the trained models on an independent dermoscopic dataset (e.g., ISIC 2019) to assess generalization beyond the training domain.

3. **Feature importance analysis**: Use SHAP or similar techniques to identify which CNN features drive malignancy predictions and compare them to ABCD features to quantify the information bottleneck more precisely.