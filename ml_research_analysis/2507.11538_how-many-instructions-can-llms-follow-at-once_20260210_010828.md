---
ver: rpa2
title: How Many Instructions Can LLMs Follow at Once?
arxiv_id: '2507.11538'
source_url: https://arxiv.org/abs/2507.11538
tags:
- instruction
- instructions
- performance
- ratio
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IFScale is a benchmark measuring how instruction-following performance
  degrades as instruction density increases from 10 to 500 instructions. The benchmark
  uses keyword-inclusion instructions for business report generation, with performance
  measured by exact word matches.
---

# How Many Instructions Can LLMs Follow at Once?

## Quick Facts
- arXiv ID: 2507.11538
- Source URL: https://arxiv.org/abs/2507.11538
- Reference count: 23
- Primary result: IFScale benchmark reveals instruction-following performance degrades across 20 models as instruction density increases from 10 to 500, with best models achieving only 68% accuracy at maximum density.

## Executive Summary
This paper introduces IFScale, a benchmark designed to systematically measure how large language models handle increasing instruction density. The benchmark presents models with 10-500 keyword-inclusion constraints while asking them to generate a business report, measuring exact-word match accuracy. Testing 20 state-of-the-art models across seven providers reveals three distinct degradation patterns (threshold, linear, exponential), universal primacy effects showing attention limitations, and systematic error shifts from modification to omission under cognitive load. The best-performing models achieved only 68% accuracy at 500 instructions, highlighting fundamental limitations in instruction-following capabilities.

## Method Summary
The IFScale benchmark uses 500 business keywords extracted from SEC 10-K filings, filtered by Zipf frequency, lemmatized, deduplicated, and ranked by perplexity. For each instruction density (N ∈ {10, 20, ..., 500}), the method samples N keywords randomly, constructs prompts requiring exact-word inclusion, and evaluates outputs via case-insensitive regex matching. The evaluation counts omissions (no match) and modifications (≥80% prefix match), computes accuracy, error type ratios, primacy effect (comparing first vs. last third error rates), and standard deviation across 5 random seeds. Models are evaluated via OpenRouter API with default parameters and retry logic for refusal responses.

## Key Results
- Best models achieved only 68% accuracy at 500 instructions, with performance varying dramatically across architectures
- Three distinct degradation patterns emerged: threshold (robust until critical density), linear, and exponential decay
- Universal primacy effect observed: error rates consistently increased in later instruction thirds, indicating attention limitations
- Systematic error shift from modification to omission as cognitive load increased, with modification errors becoming less frequent

## Why This Works (Mechanism)
The benchmark's artificial task design creates controlled conditions to isolate instruction-following performance from other capabilities. By requiring exact keyword inclusion in a business report context, it creates a measurable task where success is binary and quantifiable. The systematic scaling from 10 to 500 instructions allows observation of performance degradation patterns that would be difficult to detect in naturalistic settings. The use of business keywords from SEC filings provides domain-specific vocabulary while maintaining task relevance.

## Foundational Learning
- **Keyword extraction from SEC filings**: Needed to create domain-specific vocabulary; quick check: verify 500 keywords have Zipf frequency ≥1.0
- **Perplexity ranking**: Needed to order keywords by difficulty; quick check: confirm ranking correlates with actual model difficulty
- **Regex matching for evaluation**: Needed for exact-word accuracy measurement; quick check: test regex patterns on sample outputs
- **Primacy effect calculation**: Needed to identify attention limitations; quick check: partition errors into thirds and compare rates
- **Error type classification**: Needed to understand failure modes; quick check: validate 80% prefix threshold with human review
- **Random seed stratification**: Needed for consistent difficulty across densities; quick check: compare keyword difficulty distributions across seeds

## Architecture Onboarding
**Component Map**: Keyword extraction -> Vocabulary filtering -> Density sampling -> Prompt construction -> Model API call -> Output evaluation -> Error analysis
**Critical Path**: Vocabulary creation → Density scaling → Prompt generation → Model evaluation → Performance analysis
**Design Tradeoffs**: Binary exact-match evaluation vs. semantic similarity (simplicity vs. nuance); keyword inclusion vs. complex constraints (measurability vs. realism); 5 seeds vs. more extensive sampling (computational cost vs. statistical power)
**Failure Signatures**: Models outputting keyword lists instead of coherent reports; token budget exhaustion at high densities; systematic shift from modification to omission errors
**First Experiments**:
1. Test threshold decay pattern by running models at densities just below and above suspected critical points
2. Validate primacy effect by analyzing error distribution across instruction thirds for each model
3. Characterize error type shift by computing modification-to-omission ratios across instruction densities

## Open Questions the Paper Calls Out
**Open Question 1**: Do the observed degradation patterns generalize to complex semantic constraints and non-English languages?
The authors acknowledge the study is limited to "English-language, business-domain instruction following" and explicitly call for "cross-lingual performance" investigations and exploring "instruction types beyond simple constraints."

**Open Question 2**: Does high-density instruction following actively degrade core task performance (coherence) independent of output token length?
The authors "raise questions around whether core task performance may degrade" and note that the observed coherence drop in reasoning models might be partially explained by their "reluctance to generate a large amount of output tokens."

**Open Question 3**: What specific architectural or training mechanisms differentiate models exhibiting threshold decay from those with linear or exponential decay?
The authors list "investigate the complete degradation mechanisms underlying our observed patterns" as a primary direction for future work.

## Limitations
- Artificial task design may not generalize to natural instruction-following scenarios
- Binary exact-match evaluation fails to capture semantic relevance or partial credit
- 5-seed evaluation may be insufficient to characterize performance variability at extreme densities
- Reliance on OpenRouter's default parameters without systematic hyperparameter tuning

## Confidence
**High Confidence**: The three distinct degradation patterns observed across models are well-supported by systematic analysis across 20 models and seven providers. The universal primacy effect showing consistent error rate increases in later instruction thirds demonstrates robust empirical findings.

**Medium Confidence**: The classification of error types and their shift under cognitive load is supported by the data, though the 80% prefix threshold represents an arbitrary cutoff that could affect results.

**Low Confidence**: Extrapolation of performance trends beyond tested densities carries significant uncertainty, as models may exhibit non-linear failure modes at extreme instruction counts not captured in current evaluation.

## Next Checks
1. Evaluate the same models on IFScale using instruction types beyond keyword inclusion (e.g., logical constraints, formatting requirements) to determine if degradation patterns hold across instruction categories.

2. Systematically vary maximum token limits while maintaining instruction density to isolate whether performance degradation stems from attention limitations versus token budget constraints.

3. Implement semantic similarity metrics (e.g., BLEU, ROUGE, embedding-based similarity) alongside exact-match evaluation to assess whether models achieve semantic compliance even when failing exact keyword inclusion.