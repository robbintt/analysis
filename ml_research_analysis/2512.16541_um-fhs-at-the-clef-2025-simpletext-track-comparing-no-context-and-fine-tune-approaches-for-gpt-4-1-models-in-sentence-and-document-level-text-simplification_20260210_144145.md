---
ver: rpa2
title: 'UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune
  Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification'
arxiv_id: '2512.16541'
source_url: https://arxiv.org/abs/2512.16541
tags:
- gpt-4
- sentence
- task
- sentences
- simplification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper compares no-context prompt-based and fine-tuned approaches
  for text simplification using GPT-4.1 models across sentence and document levels.
  Two strategies were tested: a prompt engineering approach and fine-tuned models
  on provided training data.'
---

# UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification

## Quick Facts
- arXiv ID: 2512.16541
- Source URL: https://arxiv.org/abs/2512.16541
- Reference count: 32
- Primary result: GPT-4.1-mini with prompt engineering achieved best sentence-level simplification (SARI 43.34, FKGL < 8), while fine-tuning showed inconsistent benefits

## Executive Summary
This paper compares no-context prompt-based and fine-tuned approaches for text simplification using GPT-4.1 models across sentence and document levels. The authors tested two strategies: a prompt engineering approach and fine-tuned models on provided training data. For sentence-level simplification, the no-context gpt-4.1-mini achieved the best SARI score of 43.34 with Flesch-Kincaid Grade Level below 8, meeting NIH plain language guidelines. Fine-tuned models showed inconsistent performance, with gpt-4.1-nano-ft failing to generate outputs at the sentence level. For document-level simplification, gpt-4.1 achieved the highest SARI (43.83) on the aligned dataset, while gpt-4.1-mini led on the larger unaligned set with SARI of 42.13 and FKGL of 7.56. Fine-tuning offered minimal benefit except in one document-level case, and cost-efficiency varied significantly across models. Results highlight model sensitivity to task granularity and prompt complexity.

## Method Summary
The study employed GPT-4.1 models (gpt-4.1, gpt-4.1-mini, gpt-4.1-nano) with two approaches: no-context prompt-only and fine-tuned on gpt-4.1-mini and gpt-4.1-nano. Training used ~4.8M tokens (sentence-level) and ~2.1M tokens (document-level) from Cochrane-auto dataset. The fine-tuning hyperparameters included epochs=3, batch_size=1, LR_multiplier=2, and random_seed=69517706. Prompt templates were provided in appendices. The task required scientific text simplification at sentence-level (Task 1.1) and document-level (Task 1.2), targeting 8th-grade reading level per NIH guidelines. Evaluation used SARI (primary), BLEU, FKGL (target <8), and compression ratio metrics.

## Key Results
- GPT-4.1-mini with no-context achieved best sentence-level simplification (SARI 43.34, FKGL < 8)
- Fine-tuned models showed mixed results, with gpt-4.1-nano-ft failing to generate outputs at sentence level
- GPT-4.1 achieved highest SARI (43.83) on aligned document dataset, while gpt-4.1-mini led on unaligned set (SARI 42.13, FKGL 7.56)
- Fine-tuning offered minimal benefit except in one document-level case
- Cost-efficiency varied significantly across models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mid-sized models (GPT-4.1-mini) using prompt engineering (no-context) generalize better for scientific simplification than fine-tuned variants.
- **Mechanism:** The pre-trained weights of mid-sized models already encode sufficient linguistic knowledge and "plain language" patterns. Fine-tuning on a limited or specific dataset (Cochrane-auto) may introduce over-fitting or "catastrophic forgetting" of general simplification capabilities, degrading performance on diverse test sets.
- **Core assumption:** The pre-training data for `gpt-4.1-mini` included a non-trivial amount of accessible or simplified text corpora.
- **Evidence anchors:**
  - [abstract]: "...gpt-4.1-mini model with no-context demonstrated robust performance... while the fine-tuned models showed mixed results..."
  - [section]: Table 3 shows `gpt-4.1-mini` (SARI 42.13) outperforming `gpt-4.1-mini-ft` (SARI 39.16) on the larger unaligned dataset.
  - [corpus]: Related work (arXiv:2501.03857) suggests LLMs struggle with document-level simplification, reinforcing that base model scaling and prompting are critical.
- **Break condition:** This mechanism likely fails if the target simplification requires highly domain-specific terminology not present in the pre-training data, necessitating fine-tuning for knowledge injection.

### Mechanism 2
- **Claim:** Fine-tuning smaller models (`nano`) on strict formatting constraints causes generation failures (null outputs).
- **Mechanism:** Smaller models have reduced capacity to simultaneously satisfy complex reasoning (simplification) and strict structural constraints (exact sentence alignment). Fine-tuning amplifies this tension, causing the model to default to refusal or empty generation when it cannot confidently satisfy the formatting rule.
- **Core assumption:** The failure to generate outputs is a result of constraint conflict rather than a purely computational error.
- **Evidence anchors:**
  - [abstract]: "...gpt-4.1-nano-ft failing to generate outputs at the sentence level."
  - [section]: "The fine-tuned gpt-4.1-nano model frequently failed to generate the desired correct number of sentences when constrained by rule-based prompting."
  - [corpus]: Corpus evidence on `nano` specific failures is weak; general literature suggests smaller models are more sensitive to instruction complexity.
- **Break condition:** If the output format constraint is relaxed (e.g., "summarize generally" vs. "return exactly N sentences"), the nano model would likely resume generation, albeit with lower quality.

### Mechanism 3
- **Claim:** Task granularity determines optimal model scale; reasoning-heavy tasks benefit from larger models, while extraction-heavy tasks favor efficient mid-sized models.
- **Mechanism:** Document-level simplification on aligned data (high reasoning requirement to maintain coherence) leverages the full capacity of `gpt-4.1` (SARI 43.83). In contrast, sentence-level or large-scale unaligned tasks favor `gpt-4.1-mini` because the marginal reasoning gain from the larger model is outweighed by the mid-sized model's efficiency and robustness against noise.
- **Core assumption:** The 217-document unaligned set contained noisier or more diverse data where "over-thinking" by the largest model offered no advantage.
- **Evidence anchors:**
  - [section]: Table 2 shows `gpt-4.1` dominating aligned document tasks; Table 3 shows `gpt-4.1-mini` dominating the unaligned set.
  - [section]: "Results highlight model sensitivity to task granularity and prompt complexity."
  - [corpus]: General consistency with findings in arXiv:2508.11816 regarding LLM planning in simplification.
- **Break condition:** If the document length exceeds the context window of the mid-sized model but fits the larger model, the scaling advantage reverts to the larger model.

## Foundational Learning

- **Concept:** **SARI (Sentence Alignment-based Replacement Input)**
  - **Why needed here:** This is the primary metric for success in the paper. Understanding that SARI measures how well the *edit operations* (add, delete, keep) match a reference is crucial for interpreting why "compression" alone isn't enough.
  - **Quick check question:** If a model deletes all complex words, does it achieve a high SARI score? (Answer: No, SARI rewards correct preservation of simple terms and accurate deletions).

- **Concept:** **FKGL (Flesch-Kincaid Grade Level)**
  - **Why needed here:** The paper sets a specific success criterion (FKGL < 8). You must understand that this is a proxy for "readability" based on syllables and sentence length, not semantic accuracy.
  - **Quick check question:** A model outputs very short, choppy sentences. Does FKGL go up or down? (Answer: Down, indicating "easier" reading, potentially gaming the metric).

- **Concept:** **Zero-Shot vs. Fine-Tuning (API Context)**
  - **Why needed here:** The paper compares these explicitly. You need to distinguish between passing a prompt to a frozen model (Zero-Shot/No-Context) vs. updating model weights via the OpenAI API (Fine-Tuning).
  - **Quick check question:** Which approach allows the model to learn a new output format (like a Python list) without it appearing in the input prompt? (Answer: Fine-Tuning).

## Architecture Onboarding

- **Component map:**
  - Cochrane Abstracts (Source) -> Sentence Segmentation -> OpenAI API (Models: `4.1`, `4.1-mini`, `4.1-nano`) + System Prompts (Appendices C-F) -> SARI/BLEU/FKGL computation against Reference Summaries

- **Critical path:**
  1.  **Prompt Engineering:** Start with the System Prompt defined in Appendix C & D. This is the highest-impact variable (the paper shows No-Context beats FT).
  2.  **Constraint Enforcement:** Ensure the "N elements" rule in the prompt is handled; if using `nano` or fine-tuning, implement a fallback/retry logic as these models failed to generate outputs.
  3.  **Cost Management:** Route `gpt-4.1` only for complex document-level reasoning; use `gpt-4.1-mini` for sentence-level or high-volume tasks.

- **Design tradeoffs:**
  - **gpt-4.1-mini (No-Context):** Best balance of cost/reliability. Tradeoff: Slightly lower reasoning capability than full `4.1` on aligned documents.
  - **Fine-Tuning:** High engineering cost (~$24/run). Tradeoff: Inconsistent performance and potential generation failures (observed in `nano-ft`).
  - **gpt-4.1 (Base):** Highest reasoning. Tradeoff: Higher latency and cost; underperformed on noisy/unaligned data compared to mini.

- **Failure signatures:**
  - **Empty List Return:** `gpt-4.1-nano-ft` returning `[]` or empty strings when confused by alignment constraints.
  - **Metric Gaming:** Generating very low FKGL (simple words) but low SARI (deleting essential facts).

- **First 3 experiments:**
  1.  **Reproduce "Mini" Baseline:** Run the prompt from Appendix C/D against `gpt-4.1-mini` with a sample of 10 Cochrane abstracts. Verify FKGL < 8.
  2.  **Stress Test Alignment:** Feed `gpt-4.1-nano` a list of 20 sentences to verify if it maintains the exact count, testing the "constraint brittleness" hypothesis.
  3.  **Cost/Benefit Analysis:** Run the same document-level task on both `gpt-4.1` and `gpt-4.1-mini` and calculate the cost delta per 1% SARI improvement to determine operational budget requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what specific conditions does fine-tuning provide a consistent advantage over prompt engineering for GPT-4.1 models in text simplification?
- **Basis in paper:** [explicit] The authors note that fine-tuning offered minimal benefit except in one case and "needs further investigation to assess their overall utility."
- **Why unresolved:** Results were mixed; fine-tuning improved SARI for `gpt-4.1-nano` in one document task but caused failures in others.
- **What evidence would resolve it:** Ablation studies comparing fine-tuning against few-shot prompting across varied dataset sizes and domain specificities.

### Open Question 2
- **Question:** Why do smaller models (e.g., `gpt-4.1-mini`) frequently outperform larger models like `gpt-4.1` in biomedical simplification tasks?
- **Basis in paper:** [explicit] The authors observe that `gpt-4.1-mini` outperformed `gpt-4.1` and state this aligns with previous work, warranting "further investigation."
- **Why unresolved:** The paper establishes the phenomenon (better SARI/FKGL for mini) but does not investigate the architectural or training reasons behind it.
- **What evidence would resolve it:** Comparative analysis of attention mechanisms or error types between mini and standard model sizes on complex biomedical syntax.

### Open Question 3
- **Question:** What mechanism causes fine-tuned nano models to fail entirely at generating outputs when constrained by strict formatting rules?
- **Basis in paper:** [inferred] The paper reports `gpt-4.1-nano-ft` failed to generate outputs at the sentence level, suggesting a sensitivity to the "strict input-output alignment" rule.
- **Why unresolved:** The paper notes the failure but does not determine if it stems from model capacity, hyperparameters (epochs/LR), or prompt rigidity.
- **What evidence would resolve it:** Error analysis of `gpt-4.1-nano-ft` log probabilities when processing the "exact number of elements" constraint.

## Limitations

- The conclusions about fine-tuning effectiveness are limited by testing only nano and mini models, making it unclear whether larger models would exhibit similar performance degradation
- The exact nature of training data preprocessing for fine-tuning is not fully specified, particularly how aligned vs. unaligned distinction was handled during model training
- The failure mode of the nano model (returning empty outputs) is not thoroughly analyzedâ€”whether this represents fundamental model limitation or artifact of specific constraint formatting remains unclear

## Confidence

- **High Confidence:** The finding that prompt-based approaches with mid-sized models outperform fine-tuned variants on sentence-level simplification and large unaligned document sets
- **Medium Confidence:** The assertion that task granularity determines optimal model scale, though the mechanism linking reasoning requirements to model size is largely inferred
- **Low Confidence:** The explanation for why fine-tuned nano specifically failed to generate outputs, as the paper observes the failure but provides limited analysis of the underlying cause

## Next Checks

1. Test whether relaxing the strict N-to-N sentence alignment constraint in the prompt enables nano-ft to generate outputs, distinguishing between fundamental model limitations and constraint brittleness
2. Conduct ablation studies on the fine-tuning data to determine whether performance degradation stems from the specific Cochrane-auto dataset characteristics or fine-tuning methodology itself
3. Evaluate the same models on a held-out validation set with varying levels of domain specificity to test the hypothesis about pre-training data coverage for general vs. specialized simplification tasks