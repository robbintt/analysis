---
ver: rpa2
title: Annotation-Free Reinforcement Learning Query Rewriting via Verifiable Search
  Reward
arxiv_id: '2507.23242'
source_url: https://arxiv.org/abs/2507.23242
tags:
- query
- rl-qr
- retrieval
- rewriting
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RL-QR introduces an annotation-free reinforcement learning framework
  for query rewriting in Retrieval-Augmented Generation systems. It synthesizes index-aligned
  queries and uses verifiable search rewards derived from NDCG scores, eliminating
  the need for human-annotated data.
---

# Annotation-Free Reinforcement Learning Query Rewriting via Verifiable Search Reward

## Quick Facts
- arXiv ID: 2507.23242
- Source URL: https://arxiv.org/abs/2507.23242
- Reference count: 9
- RL-QR framework achieves up to 3.9× improvement on lexical retrievers and 3.5× on semantic retrievers for visual documents (MTEB VIDORE V2).

## Executive Summary
RL-QR introduces an annotation-free reinforcement learning framework for query rewriting in Retrieval-Augmented Generation systems. It synthesizes index-aligned queries and uses verifiable search rewards derived from NDCG scores, eliminating the need for human-annotated data. The method trains query rewriters to adapt queries to the representation space of specific retrievers, enabling universal applicability across text and multi-modal unstructured documents. Experiments show substantial gains: up to 3.9× improvement on lexical retrievers and 3.5× on semantic retrievers for visual documents (MTEB VIDORE V2), with consistent 5%–10% improvements on MS MARCO v2.1 and internal industrial datasets. RL-QR effectively "unlearns" chat-oriented behaviors, focusing instead on retrieval intent, and dynamically adjusts query verbosity based on index requirements.

## Method Summary
RL-QR is a two-step framework that first generates index-aligned synthetic queries using a high-capacity LLM, then trains a query rewriter via GRPO using NDCG scores as verifiable rewards. The synthetic queries are constructed so their answers depend entirely on specific documents, creating a ground truth for reward calculation. The rewriter is trained to optimize retrieval performance for specific retriever types (lexical, semantic, multimodal), with the reward function combining retrieval accuracy and a penalty for conversational verbosity. The approach eliminates human annotation costs while achieving significant improvements across multiple benchmark datasets.

## Key Results
- Up to 3.9× improvement on lexical retrievers and 3.5× on semantic retrievers for visual documents (MTEB VIDORE V2)
- Consistent 5%–10% improvements on MS MARCO v2.1 and internal industrial datasets
- RL-QR effectively "unlearns" chat-oriented behaviors, realigning model outputs from instruction following to index-oriented retrieval
- 4B RL-QR model outperforms 14B base model, demonstrating efficiency of specialized training

## Why This Works (Mechanism)

### Mechanism 1: Index-Aligned Synthetic Reward Generation
RL-QR eliminates the need for human-annotated query-document pairs by synthesizing training data where the ground truth is structurally guaranteed. The system uses a high-capacity LLM to generate a question for a document such that the answer depends entirely on that document. During Reinforcement Learning, the reward is calculated by feeding the rewritten query into the retriever and measuring if it ranks the source document at the top (using NDCG). If the rewritten query retrieves the source document, the reward is high; otherwise, it is low.

### Mechanism 2: Behavioral Unlearning for Retrieval Intent
The framework shifts the LLM's prior from "conversational instruction following" to "keyword-based search optimization." Standard LLMs tend to expand queries into verbose explanations. The RL loop penalizes this verbosity if it lowers the NDCG score, forcing the model to output concise, search-optimized strings.

### Mechanism 3: Retriever-Specific Adaptation
A single rewriter policy is not universal; the system learns distinct rewriting strategies (refinement vs. expansion) conditioned on the retriever type. When the retriever is Lexical (BM25), the policy learns to repeat keywords and refine structure to maximize term overlap. When the retriever is Semantic (Embedding), the policy learns to expand the query to align with the semantic space of the index embeddings.

## Foundational Learning

- **Concept: Normalized Discounted Cumulative Gain (NDCG)**
  - Why needed: This is the mathematical heart of the "verifiable reward." You must understand that NDCG measures ranking quality—it rewards the model if the correct document appears at the very top of the search results, not just "somewhere in the list."
  - Quick check: If a rewritten query retrieves the correct document at position 5 instead of position 1, does the NDCG score increase, decrease, or stay the same? (Answer: It would be lower, though positive; NDCG discounts gains for lower ranks).

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: The paper uses this specific RL algorithm to update the model. It relies on generating a group of outputs and comparing their rewards relative to each other (Advantage) rather than using a separate "critic" model. This reduces memory overhead.
  - Quick check: In GRPO, how is the "Advantage" of a specific query output calculated? (Answer: By comparing its reward to the average reward of the other outputs generated in the same group/rollout).

- **Concept: Lexical vs. Semantic Retrieval**
  - Why needed: The paper proves the rewriter adapts its strategy based on this distinction. You need to know that Lexical (BM25) counts exact word matches, while Semantic measures meaning/vector similarity.
  - Quick check: Why would repeating a word 3 times help a Lexical retriever but potentially confuse a Semantic retriever? (Answer: Lexical relies on term frequency; Semantic relies on vector norms where keyword stuffing might not align with the document's semantic vector).

## Architecture Onboarding

- **Component map:** Source Index -> Query Synthesizer -> Trainee Rewriter -> Retriever -> Reward Engine
- **Critical path:** The Index-Aligned Query Synthesis is the most fragile step. If the synthesizer generates a question that is too generic or ambiguous, the Retriever will fail to find the specific document, providing a "false negative" reward signal to the RL agent, which harms training.
- **Design tradeoffs:**
  - Single-Rewriter vs. Per-Retriever: The paper suggests training separate rewriters for different retrievers (Lexical vs. Semantic) rather than one universal model.
  - Annotation Cost vs. Compute Cost: You save on human labeling hours but trade it for GPU hours (synthesizing queries + RL rollouts).
  - Model Size: The 4B model with RL-QR often outperforms the 14B base model, suggesting training a smaller specialized model is more efficient than prompting a larger generalist.
- **Failure signatures:**
  - "Chat Collapse": The rewriter ignores the prompt and outputs conversational filler (e.g., "Sure, here is the information..."). Check if the penalty term is active.
  - "Over-Optimization/Hallucination": The rewriter starts hallucinating keywords that don't exist in the query to "trick" the retriever (rare with NDCG, but possible if rewards are sparse).
  - "Training Instability": Loss spikes. Check the reward normalization; GRPO relies on group variance, so if all queries in a batch get 0 reward, the advantage calculation can fail.
- **First 3 experiments:**
  1. Metric Validation: Run the vanilla base model vs. the RL-QR model on a sample of MS MARCO. Verify that the RL-QR model shortens queries and increases NDCG.
  2. Retriever Ablation: Train two separate rewriters—one for BM25 and one for Semantic. Compare their outputs on the same query. You should see the BM25 rewriter repeating keywords and the Semantic rewriter expanding context.
  3. Synthesis Quality Check: Manually inspect 10 synthetic queries. Confirm that the "Answer" provided by the synthesizer is actually found in the "Source Document" and not in the model's parametric memory.

## Open Questions the Paper Calls Out
- Extending the framework to multi-turn conversational retrieval where context history significantly influences query intent.
- Incorporating richer reward signals from downstream generation quality to improve alignment beyond NDCG-based retrieval rewards.
- How the assumption that synthetic queries have a "single retrieval target" limits the model's efficacy in complex, multi-hop reasoning scenarios.
- Whether the requirement to train distinct rewriters for each retriever is necessary, or if a single universal rewriter could generalize effectively.

## Limitations
- The approach requires training separate rewriters for each retriever type and index, limiting scalability across dynamic retrieval systems.
- Performance heavily depends on the quality of the synthetic query generator; if questions are answerable from general knowledge, the reward signal becomes noisy.
- The paper does not provide exact GRPO hyperparameters or the GroupComputation formula, affecting reproducibility.

## Confidence
- **High Confidence:** The mechanism of using NDCG as a verifiable reward for index-aligned synthetic queries is well-grounded and clearly explained. The reported improvements are substantial and consistent.
- **Medium Confidence:** The behavioral unlearning claim is supported by qualitative examples and ablation studies, but the exact contribution of the penalty term versus the RL reward signal is not isolated.
- **Medium Confidence:** The retriever-specific adaptation claim is supported by performance differences across retriever types, but direct evidence showing how the rewriter's strategy changes between lexical and semantic retrievers is not provided.

## Next Checks
1. **Synthesis Quality Audit:** Manually inspect 50 synthetic queries to verify that the "answer" is strictly derived from the "source document" and not from general knowledge. Quantify the leakage rate.
2. **Retriever-Specific Output Analysis:** Generate 20 rewritten queries using the same base query but different retrievers (lexical vs. semantic). Compare the output strategies (e.g., keyword repetition vs. expansion) to confirm the model adapts its rewriting style.
3. **Dynamic Index Test:** Re-index a subset of the corpus (e.g., remove 10% of documents) and evaluate the rewriter's performance. Measure performance degradation to assess the model's sensitivity to index changes.