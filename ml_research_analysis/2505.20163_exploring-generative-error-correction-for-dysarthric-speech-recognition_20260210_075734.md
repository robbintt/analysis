---
ver: rpa2
title: Exploring Generative Error Correction for Dysarthric Speech Recognition
arxiv_id: '2505.20163'
source_url: https://arxiv.org/abs/2505.20163
tags:
- speech
- dysarthric
- hypotheses
- recognition
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of accurately transcribing dysarthric
  speech, which remains difficult for modern ASR systems due to irregular articulation,
  atypical prosody, and inconsistent speaking rates. The proposed two-stage framework
  combines a Whisper ASR model with a generative error correction (GER) stage using
  FlanT5.
---

# Exploring Generative Error Correction for Dysarthric Speech Recognition

## Quick Facts
- arXiv ID: 2505.20163
- Source URL: https://arxiv.org/abs/2505.20163
- Reference count: 0
- Achieves 6.40% WER and 92.47 SemScore on development set with proposed pipeline

## Executive Summary
This work addresses the challenge of accurately transcribing dysarthric speech using a two-stage framework that combines Whisper ASR with a generative error correction (GER) stage. The approach generates multiple ASR hypotheses and uses a fine-tuned FlanT5 model to select and refine the most accurate transcription. Experiments on the Speech Accessibility Project dataset demonstrate significant improvements over standard ASR, particularly for structured utterances and digital assistant commands, though single-word recognition remains problematic.

## Method Summary
The pipeline uses Whisper LARGE-V2 fine-tuned on dysarthric speech data to generate 20 N-best hypotheses via beam search, then applies diversity-based selection to retain 5 distinct hypotheses. These hypotheses are processed by FlanT5-XL/XXL with LoRA fine-tuning, which leverages linguistic knowledge to identify and correct errors. The system is trained on a combination of SAPC, TORGO, and VoxPopuli datasets with data augmentation including MUSAN noise injection and time stretching.

## Key Results
- Development set: 6.40% WER, 92.47 SemScore
- TEST-2 set: 12.89% WER (6% gap from dev set)
- GER improves SemScore from 83.91 to 90.32 on dev set
- Single-word utterances show 63.08% WER with no GER improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: General-purpose ASR models capture dysarthric speech acoustic patterns in their N-best hypotheses even when top-ranked output is incorrect
- Mechanism: Beam search decoding generates multiple candidate transcriptions ranked by confidence, with correct transcription often appearing in lower-ranked positions
- Core assumption: Acoustic encoder extracts meaningful representations from dysarthric speech, with decoding/ranking as primary failure point
- Evidence anchors:
  - WER improves from 12.37% (N=1) to 11.71% (N=5), supporting acoustic capacity in top hypotheses
  - Correct transcription appears as hypothesis #2 despite errors in #1

### Mechanism 2
- Claim: LLM-based error correction leverages linguistic knowledge and cross-hypothesis pattern analysis to identify correct transcriptions
- Mechanism: FlanT5 receives 5 diverse hypotheses via structured prompt and uses sequence-to-generation to produce refined output
- Core assumption: Errors in ASR hypotheses are partially systematic and can be disambiguated through linguistic context
- Evidence anchors:
  - GER improves SemScore from 83.91 to 90.32 on dev set
  - Related work shows LLMs as post-processing modules for disordered speech ASR error correction

### Mechanism 3
- Claim: Diversity-based hypothesis selection preserves genuinely different interpretations while maintaining computational efficiency
- Mechanism: Algorithm retains top hypothesis, then iteratively selects candidates maximizing minimum edit distance to previously selected ones
- Core assumption: Correct transcription is more likely to appear in a diverse hypothesis set than in a set of near-duplicates
- Evidence anchors:
  - Selected hypotheses show substantial lexical variation (play/pet/player/pick), enabling correct selection
  - Designed to select hypotheses representing genuinely different interpretations

## Foundational Learning

- Concept: Beam search and N-best lists in ASR
  - Why needed here: Understanding that ASR confidence scores don't guarantee correctness; lower-ranked hypotheses contain viable alternatives
  - Quick check question: If you only examine the top-1 ASR hypothesis, what information are you potentially discarding?

- Concept: Sequence-to-sequence text correction with LLMs
  - Why needed here: GER stage uses FlanT5 as a conditional text generator that maps noisy hypotheses to clean transcriptions
  - Quick check question: Why might a sequence-to-sequence model be preferable to a classification model for error correction?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: Authors adapt FlanT5 with ~1% trainable parameters while keeping base model frozen
  - Quick check question: What are the memory and storage tradeoffs between LoRA and full fine-tuning?

## Architecture Onboarding

- Component map: Audio → Whisper LARGE-V2 (fine-tuned) → 20 hypotheses → Diversity Selection → 5 diverse hypotheses → FlanT5 XL/XXL with LoRA → Final transcription

- Critical path:
  1. ASR beam search (N=20) must include correct transcription in top-K
  2. Diversity selection must preserve the correct candidate
  3. GER model must identify and output the correct transcription from the 5 inputs

- Design tradeoffs:
  - Model scale vs. returns: Scaling GER from 3B→11B shows diminishing returns on test sets
  - N-best size vs. efficiency: Most gains achieved by N=5; N=20 adds 0.11% WER improvement only
  - Fine-tuning vs. zero-shot: LARGE-V3 unstable when fine-tuned; LARGE-V2 preferred for adaptation

- Failure signatures:
  - Single-word utterances: 63.08% WER, no GER improvement—model bias toward complete utterances
  - Whisper LARGE-V3 fine-tuning: Produces repetitive patterns and incomplete utterances
  - Development-test gap: ~6 percentage point WER gap suggests generalization challenges

- First 3 experiments:
  1. Run zero-shot Whisper LARGE-V3 on dysarthric speech data with N=20 beam search; compute WER for N=1 vs. N=5 vs. N=20 to verify N-best gains
  2. Manually examine whether correct transcriptions appear in positions 2-5 of N-best list for error cases
  3. Provide GER model with ground truth included in the 5-hypothesis set; if GER still fails, linguistic reasoning mechanism is broken

## Open Questions the Paper Calls Out

- What specialized architectures or training constraints are needed to correct the strong bias against isolated word recognition in dysarthric ASR?
- What are the optimal fine-tuning strategies for Whisper Large-V3 to prevent the "repetitive pattern artifacts" observed during dysarthric speech adaptation?
- How can adaptation techniques be improved to close the generalization gap between development data and unseen dysarthric test speakers?

## Limitations

- Generalizability across different dysarthric speech patterns shows ~6% WER gap between development and test sets
- Single-word utterance failure with 63.08% WER and no GER improvement indicates systematic bias toward complete phrases
- Diversity selection algorithm lacks rigorous validation for dysarthric speech domain

## Confidence

- High Confidence: Fine-tuned Whisper LARGE-V2 outperforms zero-shot LARGE-V3 for dysarthric speech recognition
- Medium Confidence: N-best expansion (N=5) provides substantial WER improvement, but mechanism explanation is speculative
- Low Confidence: Diversity-based selection algorithm meaningfully improves GER performance lacks strong empirical support

## Next Checks

1. Evaluate the complete pipeline on dysarthric speech datasets from different sources to quantify the development-test gap and assess cross-etiology performance

2. Replace the edit distance-based diversity selection with random hypothesis selection and measure GER performance impact

3. Create controlled test cases where ground truth is included in the 5-hypothesis input to GER to measure GER's ability to identify correct transcription when guaranteed to be present