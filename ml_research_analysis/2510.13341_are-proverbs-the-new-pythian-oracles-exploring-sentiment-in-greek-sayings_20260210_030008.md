---
ver: rpa2
title: Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings
arxiv_id: '2510.13341'
source_url: https://arxiv.org/abs/2510.13341
tags:
- proverbs
- sentiment
- emotion
- proverb
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the sentiment and emotion of Greek proverbs\
  \ using NLP methods, addressing the challenge of their multidimensionality\u2014\
  where a single proverb can convey multiple, coexisting sentiments and emotions depending\
  \ on interpretation. Departing from an annotated dataset of 345 Greek proverbs,\
  \ we expand the analysis to local dialects and employ LLMs to predict sentiment\
  \ and emotion in both labeled and unannotated datasets of approximately 11k geographically\
  \ attributed proverbs."
---

# Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings

## Quick Facts
- arXiv ID: 2510.13341
- Source URL: https://arxiv.org/abs/2510.13341
- Reference count: 16
- Primary result: LLMs can effectively capture the multidimensional sentiment and emotion in Greek proverbs, with GPT-5mini achieving sentiment F1 scores up to 0.76 and Krikri-8B excelling in emotion prediction (micro F1 = 0.30)

## Executive Summary
This study explores the sentiment and emotion of Greek proverbs using NLP methods, addressing the challenge of their multidimensionality—where a single proverb can convey multiple, coexisting sentiments and emotions depending on interpretation. Departing from an annotated dataset of 345 Greek proverbs, we expand the analysis to local dialects and employ LLMs to predict sentiment and emotion in both labeled and unannotated datasets of approximately 11k geographically attributed proverbs. We propose a multi-label annotation framework and a novel evaluation approach that treats sentiment and emotion analysis as both multi-label and polarity-aware tasks. Our findings show that LLMs can effectively capture this multidimensionality, with GPT-5mini achieving the highest sentiment F1 scores (micro F1 = 0.76 zero-shot, 0.74 probabilistic) and Krikri-8B excelling in emotion prediction (micro F1 = 0.30 zero-shot, 0.30 probabilistic). However, models tend to overpredict positive emotions like happiness and anger, often at the expense of subtler emotions such as fear and sadness. Regional analysis reveals that anger and surprise are the dominant emotions across Greece, though local variation is significant and can be obscured by aggregation. This work establishes a foundation for understanding the complex emotional landscape of proverbs and highlights the importance of accounting for interpretation variability in NLP evaluations.

## Method Summary
The study employs a multi-label annotation framework for Greek proverbs, using expert annotators to label 345 proverbs with sentiment (positive, negative, neutral) and emotion (7 inner-ring Willcox categories). Dialectal proverbs are normalized to Standard Greek and geographically attributed. LLMs (GPT-5mini, Krikri-8B, Mistral-7B, Llama3-70B) are used for zero-shot and probabilistic prediction on both labeled and unannotated datasets (~11k proverbs). Predictions are converted to multi-hot vectors using a 0.3 threshold. Geographic visualization compares regional vs. sub-regional emotion distributions.

## Key Results
- GPT-5mini achieves highest sentiment F1 scores (micro F1 = 0.76 zero-shot, 0.74 probabilistic)
- Krikri-8B excels in emotion prediction (micro F1 = 0.30 zero-shot, 0.30 probabilistic)
- Models overpredict positive emotions like happiness and anger, underpredicting subtler emotions such as fear and sadness
- Regional analysis shows anger and surprise dominate across Greece, but local variation is significant and can be obscured by aggregation

## Why This Works (Mechanism)

### Mechanism 1: Multi-label Annotation Framework for Semantic Variability
- Claim: Proverbs inherently support multiple, coexisting emotional interpretations, which conventional single-label sentiment analysis fails to capture
- Mechanism: Multi-label annotation with explicit polarity tracking captures two distinct phenomena: (a) annotators assigning multiple labels to the same instance (multilabel conflict), and (b) annotators disagreeing on polarity (polarity disagreement). The framework treats both as valid multidimensionality rather than noise.
- Core assumption: Low inter-annotator agreement reflects genuine semantic ambiguity in proverbs, not annotation error or unclear guidelines
- Evidence anchors:
  - [abstract]: "addressing the challenge of their multidimensionality—where a single proverb can convey multiple, coexisting sentiments and emotions depending on interpretation"
  - [section 4.1]: Table 2 shows unimodal items have α=0.67 (positive) while polarity disagreement items have α=0.09, confirming that disagreement clusters around genuinely ambiguous proverbs
  - [corpus]: Limited corpus support; related work (BRoverbs, Krikri) focuses on LLM understanding benchmarks, not annotation methodology for figurative language
- Break condition: If low agreement stems from confusing annotation guidelines rather than proverb ambiguity, multi-label framework overfits to noise

### Mechanism 2: Probabilistic Prompting Captures Model Uncertainty
- Claim: Instructing LLMs to output probability distributions over sentiment/emotion classes better captures proverb multidimensionality than single-label prediction, particularly for emotion classification
- Mechanism: Probabilistic prediction prompting (Zp) provides percentage-based examples explicitly covering all sentiment/emotion classes with varying intensities. Predictions are converted to multi-hot vectors using a 0.3 threshold, allowing models to express genuine uncertainty.
- Core assumption: Model confidence distributions correlate meaningfully with human annotation variability and are well-calibrated
- Evidence anchors:
  - [abstract]: GPT-5mini achieves "0.76 zero-shot, 0.74 probabilistic" for sentiment, showing comparable performance
  - [section 4.2]: "Overall, the probabilistic approach can be more meaningful for emotion prediction" with GPT-5mini emotion improving from micro F1 0.28 (zero-shot) to 0.46 (probabilistic)
  - [corpus]: No corpus evidence specifically validates probabilistic prompting for figurative language sentiment
- Break condition: If model probability outputs are poorly calibrated (don't reflect true uncertainty), probabilistic prompting provides no benefit over zero-shot

### Mechanism 3: Regional Aggregation Tradeoffs
- Claim: Aggregating proverb emotions over large geographic areas loses meaningful local variation; fine-grained geographic analysis reveals that regional generalizations mask significant sub-regional differences
- Mechanism: Dialectal proverbs are geographically tagged, sentiment/emotion classified via LLMs, then visualized at both regional and sub-regional levels. Comparison reveals aggregation artifacts.
- Core assumption: Dialectal proverbs accurately reflect regional emotional tendencies and geographic attribution is reliable
- Evidence anchors:
  - [abstract]: "Regional analysis reveals that anger and surprise are the dominant emotions across Greece, though local variation is significant and can be obscured by aggregation"
  - [section 5]: "Kefalonia where the emotion is 'happy' while the emotion for the greater area of Ionian Islands is 'anger'" demonstrates the aggregation problem
  - [corpus]: Pavlopoulos et al. (2024) provides the geolocated proverb foundation; Dimakis et al. (2025) notes that normalization "ignores deeper dialectal attributes," suggesting attribution may be imperfect
- Break condition: If proverbs with same dialect have spread widely, or if historical dialect patterns don't reflect current regional characteristics, geographic conclusions are unreliable

## Foundational Learning

- Concept: Multi-label vs Multi-class Classification Evaluation
  - Why needed here: Conventional sentiment analysis assumes single polarity per instance. This paper requires multi-label F1 (micro/macro) and multi-hot vector encoding. Without this, you'll misinterpret the evaluation metrics.
  - Quick check question: Given gold labels [positive, neutral] and predicted labels [positive, negative], what are the per-instance precision, recall, and F1?

- Concept: Probabilistic/Soft Label Thresholding
  - Why needed here: The paper converts model percentage outputs (summing to 100) to discrete labels using a 0.3 threshold with fallback to highest-scoring class. Understanding this conversion is essential for reproducing results.
  - Quick check question: Given a model output "Positive: 28%, Negative: 45%, Neutral: 27%" with threshold 0.3, what labels are assigned? What if the threshold were 0.5?

- Concept: Krippendorff's Alpha Interpretation
  - Why needed here: The paper interprets low α (0.312 global for sentiment, 0.074-0.326 for emotions) as evidence of proverb multidimensionality, not annotation failure. This inverted interpretation is central to their methodology.
  - Quick check question: Why might low inter-annotator agreement be a feature rather than a bug in cultural/figurative language analysis? What would high agreement suggest about the proverbs?

## Architecture Onboarding

- Component map:
  Standard Greek proverbs + Dialectal proverbs (Hellenic Folklore Research Centre) → Multi-label annotation (12 expert annotators) → Dialect normalization → LLM inference (GPT-5mini, Krikri-8B, Mistral-7B, Llama3-70B) → Multi-label evaluation (micro/macro F1, MSE) → Geographic visualization (regional vs. sub-regional)

- Critical path:
  1. Annotated dataset (345 proverbs) → Model benchmarking → Select best model per task (GPT-5mini for sentiment, Krikri-8B for emotion)
  2. Best model (Krikri-8B) → Scale to 11k geolocated proverbs → Regional emotion distribution → Geographic visualization
  3. Threshold calibration (0.3) is the key hyperparameter for probabilistic-to-discrete conversion

- Design tradeoffs:
  - **Model selection by task**: GPT-5mini best for sentiment (micro F1 0.76), Krikri-8B best for emotion (micro F1 0.30)—no single model dominates both
  - **Zero-shot vs Probabilistic**: Zero-shot slightly better for sentiment; probabilistic significantly better for emotion (GPT-5mini: 0.28→0.46)
  - **Geographic granularity**: Fine-grained preserves local variation (Kefalonia=happy) but risks sparse data; regional aggregation (Ionian Islands=anger) masks local anomalies

- Failure signatures:
  - **Emotion overprediction**: Models consistently overpredict "happy" and "angry" at expense of "fearful," "sad," "disgusted" (see Figure 3: 110 "happy" misclassified as "angry")
  - **Positive sentiment underprediction**: "Positive" is rarest label and consistently confused with neutral/co-occurring labels (Table 6)
  - **Neutral overprediction in probabilistic mode**: Probabilistic prompting shifts errors toward neutral (Table 6b: 172 negative→neutral misclassifications)
  - **Aggregation masking**: Regional averages hide local outliers; always check sub-regional variation before concluding regional patterns

- First 3 experiments:
  1. **Replicate annotation on small sample (20-30 proverbs)** to validate multi-label methodology and measure your own inter-annotator agreement; compare to paper's α=0.312 benchmark
  2. **Threshold sensitivity analysis**: Vary the 0.3 probability threshold (test 0.2, 0.3, 0.4, 0.5) and measure impact on micro F1 for both sentiment and emotion; paper notes this as future work
  3. **Geographic holdout validation**: Hold out entire regions from in-context examples and test whether dialectal patterns generalize; this tests the assumption that dialectal features correlate with regional sentiment patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal probability threshold for converting LLM probabilistic predictions into discrete multi-label sentiment and emotion annotations?
- Basis in paper: [explicit] The authors state in the Conclusion: "Future work will take a deeper look at the probabilistic approach and examine threshold sensitivity to achieve optimal results."
- Why unresolved: The current study used an arbitrary 0.3 threshold for multi-label assignment, but systematic exploration of how different thresholds affect performance across sentiment and emotion tasks was not conducted.
- What evidence would resolve it: A threshold-sensitivity analysis showing performance metrics (micro/macro F1) across a range of threshold values (e.g., 0.1–0.5) on held-out data.

### Open Question 2
- Question: How should polarity conflicts (annotators selecting opposite single-label polarities) be formally integrated into NLP evaluation frameworks for proverb sentiment analysis?
- Basis in paper: [explicit] The Limitations section states: "polarity conflicts are treated here as a multi-label phenomenon, although in our methodology we note that they ideally should be handled differently... integrating it into evaluation remains an open challenge."
- Why unresolved: Current evaluation treats all disagreements uniformly as multi-label instances, but polarity disagreement (α = 0.09–0.15) differs fundamentally from multilabel conflict (α = 0.37), requiring distinct handling.
- What evidence would resolve it: A comparison of evaluation metrics that separately weight polarity-disagreement vs. multilabel-conflict instances against human judgments of annotation quality.

### Open Question 3
- Question: To what extent are the geolocated proverbs in this study still actively used and culturally relevant in contemporary Greek communities?
- Basis in paper: [explicit] The Limitations section notes: "we do not know the extent to which these proverbs are actively used today, leaving their current relevance somewhat uncertain."
- Why unresolved: The proverbs originate from historical collections, and without contemporary usage data, findings about regional emotional landscapes may reflect archived rather than living cultural expression.
- What evidence would resolve it: Sociolinguistic survey data or corpus analysis of contemporary Greek media/social platforms measuring proverb frequency and contextual usage by region.

## Limitations
- The 345-proverb annotation dataset represents a relatively small sample size for establishing robust sentiment/emotion patterns
- Model performance for emotion classification remains modest (micro F1=0.30), with consistent overprediction of "happy" and "angry" emotions
- Geographic analysis depends on dialect normalization quality and geographic attribution accuracy, which are not independently verified

## Confidence
- **High confidence**: The multi-label annotation framework successfully captures proverb multidimensionality; geographic aggregation does mask local variation
- **Medium confidence**: LLMs can effectively capture proverb sentiment/emotion multidimensionality; probabilistic prompting provides meaningful benefits for emotion prediction
- **Low confidence**: Regional sentiment/emotion patterns reflect genuine geographic variation; dialect normalization preserves proverb meaning accurately

## Next Checks
1. **Cross-cultural validation**: Apply the multi-label annotation framework to proverbs from other languages/cultures to test whether low inter-annotator agreement is a universal feature of figurative language or specific to Greek proverbs.

2. **Threshold sensitivity analysis**: Systematically vary the 0.3 probability threshold (test 0.2, 0.3, 0.4, 0.5) to determine optimal classification performance and assess model calibration quality across sentiment and emotion tasks.

3. **Geographic holdout experiment**: Hold out entire geographic regions from in-context examples and test whether dialectal patterns generalize, directly testing the assumption that dialectal features correlate with regional sentiment patterns.