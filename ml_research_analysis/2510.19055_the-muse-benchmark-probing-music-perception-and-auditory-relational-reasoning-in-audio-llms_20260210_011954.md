---
ver: rpa2
title: 'The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning
  in Audio LLMS'
arxiv_id: '2510.19055'
source_url: https://arxiv.org/abs/2510.19055
tags:
- tasks
- audio
- musical
- music
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MUSE Benchmark to evaluate fundamental
  music perception and auditory relational reasoning in multimodal large language
  models. The benchmark comprises 10 tasks grounded in music cognition research, testing
  invariant musical representations across pitch, rhythm, and harmony.
---

# The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS

## Quick Facts
- arXiv ID: 2510.19055
- Source URL: https://arxiv.org/abs/2510.19055
- Reference count: 0
- Four state-of-the-art models tested against human participants on 10 music cognition tasks

## Executive Summary
The MUSE Benchmark introduces a rigorous evaluation framework for assessing music perception and auditory relational reasoning in multimodal large language models. Testing four leading models against 200 human participants including expert musicians, the benchmark reveals significant performance gaps, with human experts consistently outperforming models on tasks requiring abstract relational reasoning. The findings demonstrate that current audio LLMs rely on surface acoustic features rather than invariant musical representations, and that simple prompting strategies cannot bridge this fundamental gap.

## Method Summary
The MUSE Benchmark comprises 10 tasks grounded in music cognition research, testing invariant musical representations across pitch, rhythm, and harmony. Four state-of-the-art models (Gemini Pro/Flash, Qwen2.5-Omni, Audio-Flamingo 3) were evaluated alongside 200 human participants including expert musicians. The benchmark tests both basic perception tasks and advanced relational reasoning tasks, using chain-of-thought prompting and few-shot learning conditions to probe model capabilities. Performance was measured against human baselines and analyzed for patterns of success and failure across different musical dimensions.

## Key Results
- Human experts consistently outperformed all models on relational reasoning tasks, with performance gaps of 15-40% in advanced tasks
- Qwen and Audio-Flamingo 3 performed at or near chance levels on complex tasks like Melody Shape and Pitch Shift Detection
- Chain-of-Thought prompting proved inconsistent and often detrimental to model performance
- Gemini Pro achieved competitive results on basic perception tasks but failed on abstract relational reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current audio LLMs rely on surface-level acoustic features (timbre, tempo) for classification, not abstract relational representations.
- Mechanism: Audio LLMs are trained to map audio-text pairs via contrastive/objective learning. The training data likely emphasizes surface co-occurrences (e.g., "piano" + specific timbre), rather than explicit musical structure labels (e.g., "modulated from C to G major"). As a result, models learn to associate statistical acoustic patterns with labels, failing to form invariant representations of underlying musical structure (pitch intervals, harmonic function).
- Core assumption: Current large-scale audio-text datasets lack dense annotations of musical structure and relational reasoning.
- Evidence anchors:
  - [abstract]: "current evaluations may obscure fundamental weaknesses in relational reasoning."
  - [section 1]: "...they may succeed by learning surface co-occurrences (e.g., timbre or tempo cues) rather than the relations that constitute musical structure."
  - [corpus]: Weak direct evidence in corpus. SonicBench (arXiv:2601.11039) probes physical attributes like pitch/loudness, suggesting a related but distinct perceptual bottleneck.
- Break condition: If training data were to include structured, symbolic musical annotations (e.g., MIDI-aligned harmonic analyses) alongside audio, this mechanism predicts improved performance on relational tasks.

### Mechanism 2
- Claim: LLMs' textual reasoning capabilities (as elicited by CoT) do not reliably transfer to or improve auditory perception, which is handled by a separate audio encoder.
- Mechanism: Multimodal LLMs have a frozen or partially-trained audio encoder that feeds into a text-based LLM. The LLM's reasoning capabilities are powerful in the text domain. CoT prompting forces the LLM to generate verbal reasoning steps. However, this verbal reasoning operates on the (often imperfect or incomplete) *representation* of the audio provided by the encoder. If the encoder fails to capture a relational feature (e.g., pitch shift), the LLM cannot reason about it, regardless of the prompt. The verbal chain may therefore be confabulation based on partial or incorrect information.
- Core assumption: The audio encoder's representation is the primary bottleneck for relational reasoning.
- Evidence anchors:
  - [section 4.3]: "CoT only produced a dramatic improvement in one case... More frequently, CoT either had a negligible effect or actively harmed performance."
  - [section 4.3]: "Analysis of Gemini Pro's CoT logs reveals that the model often sounds correct while reasoning incorrectly... the precise off-beat counts were correct in only 4/37."
  - [corpus]: Audio-CoT (arXiv:2501.07246) explores this, suggesting CoT can work, but the MUSE results show it is unreliable, pointing to a fragility in the cross-modal grounding.
- Break condition: If CoT prompts were used to guide *iterative attention* over the audio spectrogram or representation, performance might improve. As tested, it operates on a fixed representation.

### Mechanism 3
- Claim: A model's "in-context learning" from few-shot examples is not equivalent to human learning from training, which involves internalizing abstract rules.
- Mechanism: Human musical training creates a generative mental model of musical rules (e.g., tonal hierarchies, rhythmic grammar). This allows for robust generalization. LLM in-context learning is a form of conditioned generation. Providing more few-shot examples gives the model more statistical evidence to condition its output distribution. For simple pattern matching, this helps. For complex abstract rules that are not already present in the pre-training data, simply showing more examples without a clear verbalizable rule may not provide enough signal to shift the model's behavior reliably.
- Core assumption: The abstract musical rules required for advanced tasks are not sufficiently captured in the pre-training corpus.
- Evidence anchors:
  - [section 4.4]: "...providing models with more examples is an unreliable proxy for such training. The models performance seems more dependent on their pre-trained capabilities..."
  - [figure 2]: Shows inconsistent effect of more shots on model accuracy, contrasting with a consistent positive effect of human musical training.
  - [corpus]: No direct corpus evidence on this specific comparison. This mechanism is primarily derived from the paper's novel analysis.
- Break condition: This mechanism would break if models were shown to successfully meta-learn abstract rules from in-context examples for domains where they had no prior knowledge.

## Foundational Learning

- Concept: **Relational Reasoning**.
  - Why needed here: The entire MUSE benchmark is designed to test this. It is the ability to understand relationships *between* elements (e.g., between two notes forming an interval, between chords in a progression) rather than identifying elements in isolation. It is the core deficit identified in current models.
  - Quick check question: Can the model identify a melody after it has been transposed to a different key (Pitch Shift Detection), or does it only recognize the original audio snippet?

- Concept: **Invariant Representations**.
  - Why needed here: This is the core technical challenge the paper identifies. An invariant representation captures the essential structure of an input despite transformations. A "melody identity" representation should be invariant to key transposition, tempo changes, or timbre changes. The study shows models lack this.
  - Quick check question: If you change an input's surface features (e.g., play a melody on a different instrument), does the model's internal representation of the melody's core identity remain stable?

- Concept: **In-Context Learning (ICL)**.
  - Why needed here: The paper evaluates ICL as a proxy for human learning. Understanding ICL is crucial for interpreting why few-shot prompting failed to consistently improve performance. ICL is the ability of a model to learn from examples provided in the prompt at inference time, without weight updates.
  - Quick check question: When you provide a few input-output examples in the prompt, does the model's accuracy on a new, related problem improve? Does it plateau or decline with more examples?

## Architecture Onboarding

- Component map: Raw Audio -> Audio Encoder -> Audio Embeddings -> Projector -> LLM Input -> LLM -> Text Response
- Critical path: The performance breakdown on MUSE tasks suggests a failure in the first two stages (creating useful embeddings) for complex relational features.
- Design tradeoffs:
  - **Generality vs. Specificity**: Models like Gemini and Qwen are general-purpose. They may trade deep, domain-specific reasoning (like advanced music theory) for broad competence across speech, sound, and music.
  - **Unified vs. Modular**: The paper tests unified multimodal models (e.g., Gemini) vs. potentially more modular systems. The results show even the best unified models have specific, critical gaps, suggesting modularity or specialized training might be required for expert-level performance.
- Failure signatures:
  - **Chance-Level Performance**: Consistent ~25-50% accuracy on tasks (like Qwen on Melody Shape, Audio Flamingo 3 on most tasks). This indicates the model fundamentally cannot process the required feature (e.g., pitch direction). It's not just a little wrong; it's performing randomly.
  - **Detrimental CoT**: Performance drops when using Chain-of-Thought prompting. This suggests the verbal reasoning is not grounded in the auditory input and creates spurious patterns that mislead the model.
  - **Flat Learning Curve**: Increasing few-shot examples (1, 2, 4, 8, etc.) does not improve accuracy on advanced tasks. This indicates the model cannot generalize the rule from the examples, unlike humans.
- First 3 experiments:
  1. **Diagnostic Probe of Encoder Representations**: Before the LLM, probe the audio encoder's embeddings using linear probes for key musical features (pitch class, interval, chord quality). This isolates the encoder's contribution to the failure.
  2. **CoT with Symbolic Verification**: Run the CoT condition again, but for each reasoning step, force the model to make a simple, verifiable binary judgment (e.g., "Is the second note higher or lower?"). This tests if the reasoning chain is logically sound at each step.
  3. **Symbolic Augmentation Experiment**: Augment the training/fine-tuning data with a small amount of symbolically-aligned audio (e.g., from a dataset with MIDI annotations). Re-evaluate on MUSE to test if a small amount of grounded data can induce the formation of invariant representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training paradigm shifts are required for models to internalize invariant musical representations?
- Basis in paper: [explicit] The conclusion argues that bridging the human-machine gap requires "fundamental changes in model architecture" rather than scaling or prompting.
- Why unresolved: Current models succeed on surface classification but fail on relational reasoning (e.g., pitch-shift detection), suggesting their pre-training objectives miss structural invariances.
- What evidence would resolve it: A model trained with a novel objective (e.g., hierarchical predictive coding) matching human expert performance on MUSE Advanced tasks.

### Open Question 2
- Question: Why is Chain-of-Thought (CoT) prompting detrimental to non-linguistic perceptual reasoning?
- Basis in paper: [explicit] The authors found CoT "unreliable, often detrimental" (Section 4.3), failing to connect textual steps to auditory features.
- Why unresolved: It remains unclear if textual reasoning creates interference or if models simply lack the grounded representations to support articulated logic.
- What evidence would resolve it: Mechanistic interpretability studies mapping the cross-attention interference between language generation and audio processing layers.

### Open Question 3
- Question: Why does increasing few-shot examples fail to replicate the consistent learning improvements seen in human musical training?
- Basis in paper: [explicit] Section 4.4 highlights a "fundamental divergence" where models show no significant improvement with more examples, unlike humans.
- Why unresolved: In-context learning appears insufficient for acquiring abstract rules (e.g., key modulation) that humans internalize through training.
- What evidence would resolve it: Identifying a prompting mechanism where accuracy correlates significantly with the number of provided examples on abstract reasoning tasks.

## Limitations

- The benchmark focuses on relatively short, controlled musical stimuli (typically 3-5 seconds) rather than extended musical structure comprehension
- The study relies on a relatively small set of four commercial models, potentially missing architectural variations that could perform better on musical tasks
- The human expert baseline consists of 10 individuals whose musical training backgrounds may vary, introducing potential variance not fully characterized in the analysis

## Confidence

**High Confidence Claims:**
- Current audio LLMs show consistent and substantial performance gaps compared to human experts on music perception tasks
- Models fail to demonstrate invariant representations across musical transformations
- Chain-of-Thought prompting provides inconsistent and often negative effects

**Medium Confidence Claims:**
- The performance deficits stem from inadequate musical representations in the audio encoder rather than LLM reasoning capabilities
- Simple prompting strategies (CoT, few-shot examples) cannot bridge the performance gap
- The specific architectural components (audio encoder, projector) are the primary bottlenecks

**Low Confidence Claims:**
- The exact nature of the invariant representations humans use versus what models learn
- Whether alternative prompting strategies or architectural modifications could yield substantial improvements
- The extent to which performance on these controlled tasks generalizes to real-world music understanding

## Next Checks

1. **Encoder Representation Analysis**: Conduct ablation studies that extract and probe the intermediate representations from each model's audio encoder, testing whether these embeddings contain the necessary information for musical relational reasoning before they reach the LLM. This would isolate whether the bottleneck is in feature extraction or cross-modal integration.

2. **Long-form Music Comprehension Test**: Extend the benchmark to include 30-60 second musical excerpts requiring tracking of musical structure over time, including thematic development, key changes, and complex rhythmic patterns. This would test whether the observed deficits persist in more ecologically valid musical contexts.

3. **Symbolic Grounding Intervention**: Conduct a controlled experiment where models are fine-tuned on a small corpus of audio paired with symbolic musical annotations (MIDI, chord symbols, structural labels), then re-evaluated on MUSE. This would test whether introducing grounded musical structure during training can induce the formation of invariant representations.