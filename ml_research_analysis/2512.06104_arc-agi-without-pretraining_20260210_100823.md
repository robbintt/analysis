---
ver: rpa2
title: ARC-AGI Without Pretraining
arxiv_id: '2512.06104'
source_url: https://arxiv.org/abs/2512.06104
tags:
- puzzle
- compressarc
- puzzles
- training
- tensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CompressARC is a 76K-parameter model that solves 20% of ARC-AGI-1
  evaluation puzzles without pretraining by minimizing description length (MDL) at
  inference time. It learns only from the target puzzle itself, without using any
  training data.
---

# ARC-AGI Without Pretraining

## Quick Facts
- arXiv ID: 2512.06104
- Source URL: https://arxiv.org/abs/2512.06104
- Authors: Isaac Liao; Albert Gu
- Reference count: 40
- Primary result: 76K-parameter model solves 20% of ARC-AGI-1 evaluation puzzles without pretraining via MDL compression

## Executive Summary
CompressARC is a 76K-parameter model that solves 20% of ARC-AGI-1 evaluation puzzles without pretraining by minimizing description length (MDL) at inference time. It learns only from the target puzzle itself, without using any training data. The method frames ARC-AGI as a code-golfing problem, optimizing neural network weights to compress puzzle data into weight matrices that can be hard-coded into a program. This approach achieves strong generalization under extreme data limitations, suggesting MDL as an alternative path to intelligence beyond conventional pretraining.

## Method Summary
The method optimizes a 76K-parameter equivariant neural network to minimize description length by combining KL divergence (measuring latent bits) with cross-entropy reconstruction loss. The model uses a multitensor architecture with directional cummax layers that are equivariant to rotations, flips, and color permutations. All learning occurs at inference time via 2000 steps of Adam optimization per puzzle, with no pretraining on any external data. The loss function encourages the model to learn the underlying rule by finding the most compressible representation of the puzzle demonstration pairs.

## Key Results
- Solves 20% of ARC-AGI-1 evaluation puzzles (pass@2 accuracy)
- Achieves 34.75% accuracy on training set after 2000 optimization steps
- Operates with zero pretraining data, learning only from target puzzle demonstrations
- 76K parameters total with ~20 minutes computation per puzzle at inference

## Why This Works (Mechanism)

### Mechanism 1
Minimizing Description Length (MDL) functions as a proxy for discovering underlying generative rules by forcing the model to compress puzzle data into efficient latent representations. The VAE-style loss combines KL divergence (measuring bits needed to encode the puzzle) with reconstruction loss, encouraging learning of statistical regularities rather than pixel noise. The core assumption is that correct solutions require fewer bits to specify than arbitrary grids.

### Mechanism 2
Inference-time learning on single samples allows overfitting to individual puzzle domains, eliminating need for universal priors from massive datasets. The model learns weights specifically to compress provided examples of one puzzle, turning general learning into curve-fitting for specific logical rule instances. This works when demonstration pairs contain sufficient information to define the hidden rule uniquely.

### Mechanism 3
Architectural equivariance reduces search space by hard-coding symmetry assumptions into the network. The model uses multitensor structure and directional cummax layers that are inherently equivariant to rotations, flips, and color swaps, freeing weights to focus on unique puzzle logic rather than learning basic symmetries.

## Foundational Learning

- **Variational Autoencoders (VAEs) & ELBO**: CompressARC's loss is structurally a VAE objective. Understanding reconstruction loss vs KL divergence tension is essential for grasping why the model learns rules rather than memorizes. Quick check: If KL weight were zero, would CompressARC still generalize? (No, it would likely memorize training examples).

- **Solomonoff Induction / Kolmogorov Complexity**: The paper frames its solution as approximating the shortest program that outputs the dataset. This explains why compression theoretically links to intelligence and prediction accuracy. Quick check: Why is finding absolute shortest program intractable, and how does CompressARC approximate it?

- **Group Equivariance in Deep Learning**: The architecture processes transformed inputs to produce correspondingly transformed outputs. This design principle is core to custom layers. Quick check: If you rotate input grid 90 degrees, how should output logits of equivariant network change?

## Architecture Onboarding

- **Component map**: Input Multitensor -> Decoding Layer (samples z) -> Residual Backbone (equivariant layers) -> Linear Heads (predict grids) -> Optimizer (Adam at inference)
- **Critical path**: Multitensor format and Directional Cummax layer. Multitensor reasons about "color" independent of "position." Cummax propagates information across grid spatially for tasks like "extend line to wall."
- **Design tradeoffs**: Heavy engineering vs generality (hand-crafted for ARC-AGI grids/colors/symmetry), compute vs data efficiency (trades 0 pretraining data for 20 min optimization per puzzle)
- **Failure signatures**: Posterior collapse (KL drops to zero early, preventing rule learning), long-range dependency failure (struggles with iteration/long-range transport requiring more than local operations)
- **First 3 experiments**: 1) Ablate the Latent - replace learned z with fixed random vector to verify program storage, 2) Visualize the Multitensor - project active tensors (PCA) to confirm correlation with human concepts like "row index," 3) Stress Test Equivariance - rotate/flip test puzzles and verify outputs rotate/flip accordingly

## Open Questions the Paper Calls Out

### Open Question 1
Can joint compression via weight sharing between puzzles improve solution accuracy by shortening overall program description? The paper proposes sharing θ between all puzzles via LoRA or hypernetworks but didn't explore due to engineering constraints. Testing would require parallel puzzle processing.

### Open Question 2
Would adding regularization by compressing network weights θ improve generalization? The paper notes neglecting θ compression is "somewhat reckless" given the bits it contributes, suggesting L2 regularization might help. No implementation was done.

### Open Question 3
Can tropical convolution or alternative convolution-like layers enable shape copying operations currently lacking? Tropical convolution showed promise on toy puzzles but failed on real ARC-AGI data. Standard convolutions amplify noise.

### Open Question 4
Could scheduled KL floor prevent posterior collapse and improve training consistency? Forcing KL above zero might help, but network may not learn fast enough for static floor to help. Scheduling dynamics remain unexplored.

## Limitations
- Architecture is heavily hand-crafted for ARC-AGI's grid-based domain, raising questions about generalizability to non-grid domains
- 2000-step inference optimization is fragile and sensitive to initialization, with posterior collapse issues preventing reliable rule learning
- ~20 minutes per puzzle trades data efficiency for compute efficiency, limiting practical deployment compared to single-forward-pass models

## Confidence
- **High**: MDL framing and Occam's razor connection is theoretically sound; clear demonstration of solving simple ARC puzzles through compression
- **Medium**: 20% evaluation accuracy claim is credible given ablation studies, though training set performance (34.75%) suggests possible overfitting concerns
- **Low**: Claim that MDL provides "alternative feasible way to produce intelligence" extends beyond empirical results without establishing broader generalizability

## Next Checks
1. **Posterior collapse stress test**: Systematically vary initialization strategies and monitor per-tensor KL dynamics across 50+ puzzles to document failure rates and identify architectural modifications preventing early collapse
2. **Cross-domain generalization**: Apply same MDL optimization approach to non-ARC domains like text-based reasoning or continuous control tasks to measure data efficiency when architectural equivariance assumptions don't hold
3. **Compute-efficiency comparison**: Benchmark wall-clock time and energy consumption against pretrained models (LLMs, CNNs) on equivalent ARC-AGI accuracy levels, including single-instance and batch-processing scenarios for practical deployment assessment