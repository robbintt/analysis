---
ver: rpa2
title: A Survey on Vision-Language-Action Models for Autonomous Driving
arxiv_id: '2506.24044'
source_url: https://arxiv.org/abs/2506.24044
tags:
- arxiv
- driving
- autonomous
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides the first comprehensive overview of Vision-Language-Action
  (VLA) models for autonomous driving (VLA4AD), consolidating over 20 representative
  models and tracing their evolution from explanatory language overlays to reasoning-centric
  agents. The work formalizes the architectural building blocks of VLA4AD systems,
  covering multimodal inputs (visual, sensor, and language), core modules (vision
  encoders, language processors, action decoders), and output formats (low-level actions
  to trajectory planning).
---

# A Survey on Vision-Language-Action Models for Autonomous Driving

## Quick Facts
- **arXiv ID:** 2506.24044
- **Source URL:** https://arxiv.org/abs/2506.24044
- **Reference count:** 40
- **Key outcome:** First comprehensive overview of Vision-Language-Action models for autonomous driving, consolidating over 20 representative models and tracing their evolution from explanatory language overlays to reasoning-centric agents.

## Executive Summary
This survey provides the first comprehensive overview of Vision-Language-Action (VLA) models for autonomous driving (VLA4AD), consolidating over 20 representative models and tracing their evolution from explanatory language overlays to reasoning-centric agents. The work formalizes the architectural building blocks of VLA4AD systems, covering multimodal inputs, core modules, and output formats. It also reviews key datasets and benchmarks which jointly assess driving safety, accuracy, and explanation quality. The survey identifies four evolutionary stages and highlights how each stage progressively closes the loop between perception, language understanding, and control.

## Method Summary
The survey systematically categorizes VLA4AD architectures into four evolutionary stages: Explanatory Language Models, Modular VLA4AD, End-to-End VLA4AD, and Reasoning-Augmented VLA4AD. It reviews over 20 representative models, analyzing their architectural components (vision encoders, language processors, action decoders), training paradigms (imitation learning, reinforcement learning, multi-stage curricula), and evaluation protocols. The survey also examines key datasets and benchmarks such as nuScenes, BDD-X, Bench2Drive, and Impromptu VLA, which jointly assess driving safety, accuracy, and explanation quality.

## Key Results
- VLA4AD models evolve through four stages, progressively closing the loop between perception, language understanding, and control
- Training paradigms include imitation learning, reinforcement learning, and multi-stage curricula with joint optimization of trajectory regression and language modeling
- Current evaluation protocols measure closed-loop driving, open-loop prediction, language competence, and robustness across multiple benchmarks
- Open challenges include real-time efficiency, formal verification, multimodal alignment, and multi-agent social complexity

## Why This Works (Mechanism)

### Mechanism 1: Semantic Prior Injection via Foundation Models
- **Claim:** VLA models appear to handle long-tail scenarios better than traditional E2E systems by injecting common-sense priors from pre-trained foundation models.
- **Mechanism:** Pre-trained Vision-Language Models (VLMs) (e.g., CLIP, LLaMA) map visual inputs and language commands into a shared embedding space learned from internet-scale data. This allows the system to associate abstract concepts (e.g., "ambulance," "yield") with visual features even if they are rare in the specific driving dataset.
- **Core assumption:** The semantic relationships learned from general internet data transfer effectively to the driving domain without significant domain shift.
- **Evidence anchors:** Large-scale multimodal pretraining equips models with commonsense associations (e.g., siren â†’ yield) that task-specific labels often miss.

### Mechanism 2: Closed-Loop Policy Unification
- **Claim:** Unifying perception, planning, and control into a single differentiable policy mitigates error propagation found in modular pipelines.
- **Mechanism:** Instead of cascading distinct modules (detection -> prediction -> planning), VLA models map sensor streams and language instructions directly to control signals or waypoints in one forward pass. This allows joint optimization where perception features are tuned specifically for the control objective.
- **Core assumption:** The network has sufficient capacity and the training data has enough coverage to learn the complex, direct mapping from high-dimensional inputs to safety-critical actions.
- **Evidence anchors:** Contrasts E2E driving with modular stacks that suffer from "information fragmentation" and "upstream errors propagate without correction."

### Mechanism 3: Chain-of-Thought (CoT) Grounding
- **Claim:** Explicit reasoning steps (verbalizing rationale) improve planning reliability and interpretability in complex interactions.
- **Mechanism:** The model generates intermediate text tokens (reasoning chains) before decoding action tokens. This "thinking" step forces the model to attend to relevant scene context and explicitly resolve ambiguities (e.g., right-of-way) before committing to a trajectory.
- **Core assumption:** The generated reasoning accurately reflects the decision logic and is not a post-hoc hallucination.
- **Evidence anchors:** Reasoning-Augmented VLA models like Impromptu VLA "align CoT with action" and learn to "verbalise its decision path before acting."

## Foundational Learning

- **Concept: Transformer Cross-Attention / Multimodal Fusion**
  - **Why needed here:** VLA architectures rely on fusing visual embeddings and linguistic embeddings. Understanding how cross-attention layers align these distinct modalities is critical for debugging "hallucinations" or misalignment.
  - **Quick check question:** Can you explain how a cross-attention layer allows a text token (e.g., "stop sign") to query a visual feature map to find its location?

- **Concept: Behavior Cloning (BC) & Distribution Shift**
  - **Why needed here:** Most VLA models are trained via Supervised Imitation Learning (BC). One must understand why BC fails in novel states (covariate shift) to appreciate the need for techniques like DAgger, Reinforcement Learning, or the "Action-dreaming" mentioned in Section 4.3.
  - **Quick check question:** Why does purely imitating an expert's actions often lead to failure when the robot makes a small mistake and enters a state never seen in the training data?

- **Concept: Tokenization of Continuous Actions**
  - **Why needed here:** VLA models must output continuous control signals (steering, throttle) using LLM backbones designed for discrete text. Understanding how continuous trajectories are discretized into "action tokens" is key to implementing the output head.
  - **Quick check question:** How can a vocabulary of discrete text tokens represent a continuous, high-frequency steering trajectory?

## Architecture Onboarding

- **Component map:** Multi-view Cameras, LiDAR (optional), Text Instructions (Tokens) -> Vision Encoder (e.g., CLIP ViT, DINOv2) + LLM (e.g., LLaMA, Vicuna) + Adapter (projects visual features into LLM embedding space) -> Action Decoder (LLM itself, Diffusion Head, or separate MLP head) -> Trajectory waypoints (BEV) or Low-level Control (steering/throttle) + Text Explanation

- **Critical path:** The Vision-to-Action Alignment is the most brittle link. Ensuring the LLM actually "sees" the traffic light in the projected features before generating the "stop" token is the primary implementation challenge.

- **Design tradeoffs:**
  - *Autoregressive (LLM) vs. Diffusion Decoders:* LLMs offer unified reasoning/planning but are slower. Diffusion heads handle multi-modal distributions better but are harder to train.
  - *Modular vs. E2E:* Modular allows debugging of intermediate steps; E2E is faster and potentially more robust but a "black box."

- **Failure signatures:**
  - **Hallucination:** The model describes an object (e.g., "blue car") that is not in the visual input.
  - **Latency Spike:** Processing all visual tokens for every frame drops the rate below the required 10-30 Hz.
  - **Command Misinterpretation:** The model follows the syntax of a command but ignores safety constraints (e.g., "Turn left" -> Turns left immediately into traffic).

- **First 3 experiments:**
  1. **Sanity Check (Open-Loop):** Implement a frozen CLIP + LLaMA pipeline. Feed images and ask VQA questions about traffic light states. Verify visual grounding (BLEU/Accuracy).
  2. **Control Tokenization (Closed-Loop Sim):** Fine-tune a small adapter on top of the LLM to output discretized steering angles in a simple CARLA environment. Test instruction following ("Turn left at intersection").
  3. **CoT Ablation:** Compare the driving performance of a model prompted with "Directly output action" vs. "Describe the scene, then output action." Check for improvements in corner-case reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement learning training paradigms effectively blend driving rewards with language fidelity?
- **Basis in paper:** Section 6.1 states, "A key open question is how to blend driving rewards with language fidelity: current work often sidesteps the issue by freezing the LLM... leaving joint gradients over text and control largely unexplored."
- **Why unresolved:** Optimizing for both trajectory safety and text fluency simultaneously risks gradient interference, and current methods typically freeze the LLM to focus solely on control penalties.
- **What evidence would resolve it:** A training framework that demonstrates successful joint optimization of control accuracy and linguistic quality without performance degradation.

### Open Question 2
- **Question:** How can language-conditioned policies be formally verified to ensure safety and social compliance?
- **Basis in paper:** Section 7 notes that "formal verification and 'socially-compliant' driving policies remain largely unexplored" despite the potential for LLMs to hallucinate hazards.
- **Why unresolved:** Pure end-to-end neural networks struggle to provide hard safety guarantees, and current logic-based vetoes are only a preliminary step.
- **What evidence would resolve it:** The development of neuro-symbolic frameworks where a neural VLA outputs a structured action program verifiable by a symbolic safety kernel.

### Open Question 3
- **Question:** How should autonomous vehicles exchange intent through a constrained yet flexible "traffic language" in multi-agent settings?
- **Basis in paper:** Section 7 asks, "How should AVs exchange intent in a constrained yet flexible 'traffic language'?" while noting the lack of robustness against malicious messages.
- **Why unresolved:** Scaling from pairwise coordination to dense traffic raises unresolved issues regarding communication protocols, trust, and security.
- **What evidence would resolve it:** A standardized V2V communication ontology that remains robust against adversarial attacks and bandwidth constraints.

### Open Question 4
- **Question:** How can heterogeneous sensor modalities (LiDAR, radar, HD-maps) be fused with vision and language in a principled, temporally consistent manner?
- **Basis in paper:** Section 7 states, "A principled, temporally consistent fusion of heterogeneous modalities is still missing," noting that current VLA4AD work is camera-centric.
- **Why unresolved:** Existing fusion methods often lack the ability to unify temporal state and diverse spatial data with linguistic context seamlessly.
- **What evidence would resolve it:** A unified architecture that demonstrates superior performance in long-horizon reasoning by effectively integrating multi-sensor temporal histories with language prompts.

## Limitations

- **Cross-dataset generalization:** Most VLA4AD models are evaluated primarily on their training domains with limited ablation on transfer to unseen cities or cultures.
- **Real-world robustness:** Limited quantification of performance drop when models trained in simulation are deployed on real roads.
- **Evaluation standardization:** Proposed benchmarks are nascent and metrics for joint driving+language competence are still evolving.

## Confidence

- **High confidence:** The historical evolution of VLA4AD from modular to unified architectures is well-supported by the surveyed literature and clear architectural descriptions.
- **Medium confidence:** The mechanisms by which foundation models inject semantic priors and unify perception-planning-control are logically sound but empirically unverified across diverse scenarios.
- **Low confidence:** Claims about CoT grounding significantly improving reliability are based on a small set of recent models requiring more rigorous testing.

## Next Checks

1. **Cross-dataset robustness test:** Retrain a representative VLA4AD model (e.g., EMMA) on nuScenes and evaluate on an unseen dataset (e.g., BDD or Argoverse) to quantify transfer performance drop.
2. **Real-world closed-loop latency:** Deploy a lightweight VLA4AD model on a real vehicle or high-fidelity simulator with strict timing budgets to measure actual inference latency and identify bottlenecks.
3. **CoT reasoning fidelity audit:** Implement a human-in-the-loop evaluation where experts rate the logical consistency and safety relevance of generated reasoning chains versus actual actions in complex driving scenarios.