---
ver: rpa2
title: On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order
  Optimization
arxiv_id: '2510.19953'
source_url: https://arxiv.org/abs/2510.19953
tags:
- fpxq
- gradient
- zeroth-order
- estimators
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the bias problem in zeroth-order optimization
  (ZOO) gradient estimators. Existing ZOO methods are inherently biased unless perturbation
  stepsizes vanish, which leads to poor variance control.
---

# On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization

## Quick Facts
- arXiv ID: 2510.19953
- Source URL: https://arxiv.org/abs/2510.19953
- Reference count: 40
- Primary result: Proposes unbiased ZOO gradient estimators with O(d/ε⁴) complexity for smooth non-convex objectives

## Executive Summary
This paper addresses the fundamental bias problem in zeroth-order optimization (ZOO) gradient estimators, which are inherently biased unless perturbation stepsizes vanish. The authors propose a novel family of unbiased gradient estimators by reformulating directional derivatives as telescoping series and sampling from carefully designed distributions. They derive four specific constructions (P1-P4 estimators) with varying numbers of function evaluations, achieving optimal complexity for smooth non-convex objectives while eliminating the bias inherent in standard two-point estimators.

## Method Summary
The method reformulates the limit definition of directional derivatives as absolutely convergent telescoping series, enabling unbiased estimation without requiring vanishing stepsizes. Four specific estimators (P1-P4) are constructed by sampling from optimized distributions over the series terms. The variance is controlled through functional optimization of perturbation stepsizes and sampling probabilities, achieving bounds that match classical two-point estimators. Stochastic gradient descent using these estimators achieves O(d/ε⁴) gradient evaluations for smooth non-convex objectives.

## Key Results
- P1 estimator can have infinite variance under certain conditions, making it unsuitable for practical use
- P2-P4 estimators achieve variance scaling of O(d‖∇f(x)‖² + d³µ²) with optimal perturbation stepsizes
- Optimal complexity of O(d/ε⁴) gradient evaluations achieved for smooth non-convex objectives
- Experiments show superior performance on synthetic tasks and language model fine-tuning with faster convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating the directional derivative as an absolutely convergent telescoping series enables unbiased gradient estimators without requiring perturbation stepsize to vanish.
- **Mechanism:** The method expresses the limit definition of a directional derivative as a weighted sum of finite differences. By defining a probability distribution {p_n} over the terms of this series, the expectation of the sampled terms mathematically equals the true gradient, circumventing the approximation error inherent in standard finite-difference methods.
- **Core assumption:** The objective function f is second-order continuously differentiable with Lipschitz continuous gradients, and the perturbation sequence {μ_n} is summable to ensure absolute convergence of the series.
- **Evidence anchors:**
  - [abstract] "...reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions..."
  - [section 2.1] Eq. (4) and (5) demonstrate the conversion of the limit into an expectation; Proposition 2.1 establishes the absolute convergence condition.
  - [corpus] Weak direct validation in provided corpus; relies on internal theoretical derivation.
- **Break condition:** The assumption of absolute convergence fails (e.g., non-smooth objectives), causing the expectation representation to become invalid or order-dependent (Riemann series theorem).

### Mechanism 2
- **Claim:** The variance of the unbiased estimator can be controlled to match the optimal order of classical biased two-point estimators by solving a functional optimization problem for the perturbation stepsizes {μ_n} and sampling probabilities {p_n}.
- **Mechanism:** The variance of the estimator depends on a term ϱ = Σ(μ_{n+1}-μ_n)²/p_n. The authors derive an analytical solution that minimizes this term, effectively decoupling the "unbiased" property from "high variance" by ensuring the sampling distribution concentrates probability mass efficiently relative to the step size decay.
- **Core assumption:** The sampling distribution {p_n} and step size sequence {μ_n} can be optimized jointly; specifically, μ_n → 0 as n → ∞ and Σμ_n < ∞.
- **Evidence anchors:**
  - [section 3.2] Theorem 3.2 explicitly defines the lower bounds for ϱ and φ and the conditions for equality (optimal choice).
  - [key outcome] "...variance scales as O(d‖∇f(x)‖² + d³µ²)... matching classical two-point estimators while eliminating bias."
  - [corpus] No direct comparison in corpus; mechanism is specific to this paper's theoretical contribution.
- **Break condition:** If the sequence {μ_n} decays too slowly or the distribution {p_n} is chosen poorly (e.g., generic uniform), the variance term ϱ explodes, rendering the estimator impractical despite being unbiased.

### Mechanism 3
- **Claim:** Stochastic Gradient Descent (SGD) utilizing the proposed unbiased estimators achieves the optimal function query complexity of O(d/ε⁴) for smooth non-convex objectives.
- **Mechanism:** Because the estimator is unbiased and its variance is optimally bounded (Mechanism 2), the convergence analysis follows standard SGD complexity arguments. The "cost" of zeroth-order information is isolated to the dimension-dependent constant and variance scaling, preserving the optimal convergence rate relative to the accuracy threshold.
- **Core assumption:** The stochastic gradients are generated via the P2, P3, or P4 estimators with optimally tuned step sizes; P1 is excluded due to infinite variance.
- **Evidence anchors:**
  - [abstract] "...SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives."
  - [section 3.3] Corollary 3.5 derives the O(d/ε⁴) bound from the variance properties.
  - [corpus] Unverified in provided neighbors; standard ZOO complexity is cited as baseline.
- **Break condition:** The objective is non-smooth or noisy (stochastic function evaluations) without adjustments, as the theoretical guarantees rely on smoothness (Lipschitz gradients) and noiseless evaluations (paper notes limitations in noisy setups).

## Foundational Learning

- **Concept:** **Zeroth-Order Optimization (ZOO) & Finite Differences**
  - **Why needed here:** This paper is fundamentally a correction to standard ZOO methods. You must understand that standard two-point estimators (f(x+μv)-f(x))/μ v are biased approximations of ∇f(x) unless μ → 0, which causes numerical instability.
  - **Quick check question:** Why does the standard two-point gradient estimator become biased when the perturbation step size μ is finite?

- **Concept:** **Telescoping Series & Absolute Convergence**
  - **Why needed here:** The paper's core theoretical leap is treating a derivative limit as a convergent infinite series. Understanding why absolute convergence is required (to prevent Riemann rearrangement issues) is critical for validating the "unbiased" claim.
  - **Quick check question:** If a series is convergent but not absolutely convergent, can you rearrange its terms to sum to any real number? Why does this matter for defining an expectation?

- **Concept:** **Bias-Variance Trade-off in Gradient Estimation**
  - **Why needed here:** The paper claims to break the traditional trade-off where unbiasedness requires tiny steps (low bias, high variance). Understanding this tension is necessary to appreciate the contribution of the optimal distribution {p_n}.
  - **Quick check question:** In classical ZOO, if you decrease μ to reduce bias, what typically happens to the variance of the estimator?

## Architecture Onboarding

- **Component map:** Inputs -> Sequence Generator -> Sampler -> Estimator Logic -> Aggregator -> Optimizer

- **Critical path:** The implementation hinges on the **Sequence Generator**. You cannot simply pick arbitrary μ_n. The relationship p_n = (μ_n - μ_{n+1})/μ_1 must hold exactly to achieve the optimal variance bound (Theorem 3.2).

- **Design tradeoffs:**
  - **P4 (4 evaluations)**: Lowest variance, highest computation per step
  - **P2 (2-3 evaluations)**: Requires careful tuning of φ (variance term) and may have higher variance than P3/P4
  - **Geometric vs. Zipf**: Geometric (Exponential decay) is easier to implement but parameter c must be tuned. Zipf (Polynomial decay) offers parameter-agnostic variance bounds for P3/P4 but requires calculating zeta function constants

- **Failure signatures:**
  - **Exploding Gradients:** If using **P1**, expect infinite variance; gradients will be unstable
  - **Numerical Underflow:** If μ_n decays too fast (e.g., Geometric with small c), function evaluations f(x+μ_n v) become indistinguishable from f(x) due to floating-point precision
  - **Bias Re-introduction:** Truncating the series sampling (not sampling large n) re-introduces bias. The paper suggests truncating only where p_n < 10⁻³

- **First 3 experiments:**
  1. **Variance Verification:** Implement P4 with Geometric sampling on a quadratic function. Plot Mean Squared Error (MSE) of the gradient estimate against the number of samples. Compare against a standard two-point estimator to confirm variance scales as O(d).
  2. **Bias Test:** Run the estimator on a non-convex logistic regression task. Verify that the expected value of the estimate converges to the true gradient (computed via autodiff) as sample count increases, validating the "unbiased" property.
  3. **Convergence Benchmark:** Fine-tune a small transformer (e.g., OPT-125m) on a classification task. Compare P3 vs. Standard Two-Point ZOO using identical function evaluation budgets to validate the claimed speedup.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees depend critically on absolute convergence of telescoping series, which requires smooth objectives with Lipschitz continuous gradients
- Infinite variance result for P1 under certain conditions raises questions about stability of entire estimator family with suboptimal parameter choices
- Theoretical O(d/ε⁴) complexity for SGD relies on standard convergence theory but lacks direct experimental verification

## Confidence

- **Theoretical unbiasedness guarantee (High):** The mathematical derivation from telescoping series is rigorous and well-established
- **Optimal variance scaling (Medium):** While the variance bounds are theoretically derived, empirical validation against the claimed O(d||∇f(x)||² + d³µ²) scaling is limited
- **O(d/ε⁴) complexity for SGD (Low):** This relies on standard SGD convergence theory applied to the proposed estimators, but direct experimental verification of this complexity bound is not provided

## Next Checks

1. **Numerical stability analysis:** Implement the P1 estimator on a quadratic function and systematically vary parameters to verify when variance becomes infinite, confirming the theoretical prediction.

2. **Non-smooth objective testing:** Apply the estimators to a piecewise linear objective (e.g., L1-regularized regression) to verify whether the unbiasedness property breaks down outside the smoothness assumptions.

3. **Ablation on truncation:** Run experiments truncating the series sampling at different thresholds (e.g., p_n < 10⁻² vs 10⁻³) to empirically measure the bias introduced and validate the paper's recommendation.