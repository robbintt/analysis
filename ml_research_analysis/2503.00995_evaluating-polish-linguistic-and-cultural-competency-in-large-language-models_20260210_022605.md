---
ver: rpa2
title: Evaluating Polish linguistic and cultural competency in large language models
arxiv_id: '2503.00995'
source_url: https://arxiv.org/abs/2503.00995
tags:
- polish
- language
- questions
- uni00000013
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Polish linguistic and cultural competency
  benchmark, a novel evaluation dataset consisting of 600 manually crafted questions
  across six categories (history, geography, culture & tradition, art & entertainment,
  grammar, and vocabulary) designed to assess large language models' knowledge of
  Polish cultural context and language nuances. The evaluation employs a deterministic
  rule-based system with binary scoring for each question.
---

# Evaluating Polish linguistic and cultural competency in large language models

## Quick Facts
- arXiv ID: 2503.00995
- Source URL: https://arxiv.org/abs/2503.00995
- Reference count: 40
- Primary result: Commercial models outperform open models in Polish cultural competency evaluation

## Executive Summary
This paper introduces a novel benchmark for evaluating Polish linguistic and cultural competency in large language models, consisting of 600 manually crafted questions across six categories. The evaluation framework employs a deterministic rule-based system with binary scoring to assess models' knowledge of Polish cultural context and language nuances. Experiments with over 30 commercial and open-weight models reveal significant performance differences, with commercial models achieving up to 83% overall accuracy compared to the best open model at 62%.

The benchmark demonstrates that specialized Polish models like Bielik-2.3 can achieve competitive performance despite fewer parameters, emphasizing the value of language-specific training approaches. The study provides valuable insights into the current state of multilingual cultural understanding in large language models and highlights the persistent gap between commercial and open models in handling cultural context beyond dominant languages.

## Method Summary
The benchmark employs a deterministic rule-based evaluation system where each of the 600 manually crafted questions receives binary scoring (correct/incorrect). Questions span six categories: history, geography, culture & tradition, art & entertainment, grammar, and vocabulary. The evaluation framework is designed to test both linguistic competence and cultural knowledge specific to Polish context. Multiple models are evaluated including both commercial offerings and open-weight alternatives, with results aggregated across all categories to provide overall performance metrics.

## Key Results
- Commercial models (e.g., Gemini-Exp-1206) achieve 83% overall accuracy, significantly outperforming open models (best at 62%)
- Specialized Polish models like Bielik-2.3 match performance of much larger multilingual models despite fewer parameters
- Substantial performance gap exists between commercial and open models in handling Polish cultural context

## Why This Works (Mechanism)
The benchmark works by providing a standardized, manually curated evaluation framework that tests both linguistic and cultural knowledge specific to Polish context. The deterministic scoring system ensures reproducibility and consistency across model evaluations, while the diverse question categories capture different aspects of cultural competency from historical facts to language nuances.

## Foundational Learning
- Binary scoring systems - Why needed: Provide clear, reproducible evaluation metrics; Quick check: Verify scoring consistency across evaluators
- Manual question crafting - Why needed: Ensure cultural relevance and accuracy; Quick check: Validate questions with native speakers
- Deterministic evaluation - Why needed: Enable fair comparison across diverse model architectures; Quick check: Test with known correct/incorrect responses

## Architecture Onboarding

### Component Map
Question Database -> Scoring Engine -> Model Interface -> Results Aggregator -> Analysis Dashboard

### Critical Path
Question selection and validation -> Model API integration -> Deterministic scoring execution -> Performance aggregation -> Comparative analysis

### Design Tradeoffs
- Binary vs. graded scoring: Binary provides clarity but may miss nuanced understanding
- Manual vs. automated question generation: Manual ensures cultural accuracy but introduces human bias
- Rule-based vs. model-based evaluation: Rule-based ensures consistency but may miss context-dependent correctness

### Failure Signatures
- Binary scoring may misclassify partially correct responses as incorrect
- Cultural bias in question selection may favor certain knowledge domains
- API limitations may affect model performance independently of actual capability

### First Experiments
1. Test scoring consistency across multiple evaluators on sample questions
2. Evaluate model performance variance across different API configurations
3. Compare binary scoring results with human-annotated partial credit assessment

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Binary scoring system may not capture nuanced or partial knowledge
- Manual question creation introduces potential human bias in question selection
- Benchmark's Polish-specific focus limits generalizability to other languages/cultures

## Confidence

| Claim | Confidence |
|-------|------------|
| Commercial models outperform open models | High |
| Specialized models can match larger models despite fewer parameters | High |
| Binary scoring provides reliable evaluation | Medium |
| Results generalize to other cultural contexts | Low |

## Next Checks
1. Replicate evaluation with a graded scoring system to capture partial knowledge and compare results with binary scoring
2. Test model performance across multiple cultural contexts to assess generalization patterns and identify universal vs. culture-specific challenges
3. Implement uncertainty-aware evaluation metrics to better understand model confidence in cultural knowledge domains and correlate with performance patterns