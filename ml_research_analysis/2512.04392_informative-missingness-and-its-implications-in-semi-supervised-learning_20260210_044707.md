---
ver: rpa2
title: Informative missingness and its implications in semi-supervised learning
arxiv_id: '2512.04392'
source_url: https://arxiv.org/abs/2512.04392
tags:
- labelled
- missingness
- data
- information
- mechanism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses semi-supervised learning (SSL) where labels
  are missing, not randomly, but informatively based on feature uncertainty or class
  membership. The core method uses a finite mixture model with an expectation-maximisation
  (EM) framework, jointly modelling both the data and the missingness mechanism.
---

# Informative missingness and its implications in semi-supervised learning
## Quick Facts
- arXiv ID: 2512.04392
- Source URL: https://arxiv.org/abs/2512.04392
- Reference count: 9
- Primary result: Correctly specified informative missingness mechanisms can yield better classifiers than fully labelled data.

## Executive Summary
This paper demonstrates that in semi-supervised learning, labels may be missing in an informative way, either based on feature uncertainty (MAR) or class membership (MNAR). By jointly modelling both the data and the missingness mechanism via finite mixture models and EM, the missing-label indicators themselves can carry additional information. Theoretical analysis shows that when the missingness is informative, this can outweigh the loss from missing labels, leading to a classifier with smaller expected error than one trained on a fully labelled sample.

## Method Summary
The core method employs a finite mixture model with an EM framework to jointly model the observed data and the missingness mechanism. Two missingness mechanisms are considered: MAR, where label absence depends on feature uncertainty (e.g., entropy near decision boundaries), and MNAR, where it depends on class membership. The analysis uses information-theoretic measures to quantify the added information from missing-label indicators (I(miss)_PC) and demonstrates conditions under which this yields a net information gain.

## Key Results
- Informative missingness can result in a total information gain such that partially labelled data yield a classifier with smaller expected error than fully labelled data.
- The information gain is largest under moderate class overlap, sparse labelled data, and strongly informative missingness mechanisms.
- Popular SSL heuristics like entropy minimisation and consistency regularisation can be interpreted as MAR-type mechanisms.

## Why This Works (Mechanism)
The missing-label indicators are themselves informative when missingness depends on feature uncertainty or class membership. This information is quantified as I(miss)_PC and can compensate for the loss of information due to missing labels, resulting in a net gain when the missingness mechanism is correctly specified.

## Foundational Learning
- **Finite mixture models**: Why needed - to represent heterogeneous data distributions; Quick check - verify convergence of EM algorithm.
- **Expectation-maximisation (EM)**: Why needed - to handle incomplete data and missingness; Quick check - monitor log-likelihood increase per iteration.
- **Information-theoretic measures**: Why needed - to quantify information gain from missing-label indicators; Quick check - ensure I(miss)_PC > 0 under informative missingness.
- **Semi-supervised learning assumptions**: Why needed - to justify joint modelling of labels and missingness; Quick check - confirm mechanism matches data generation process.

## Architecture Onboarding
- **Component map**: Data features -> Mixture model parameters -> Missingness mechanism -> Label indicators -> Classifier
- **Critical path**: Joint EM optimisation of mixture and missingness models, followed by classification using inferred parameters
- **Design tradeoffs**: Correct mechanism specification vs. flexibility; computational cost of joint EM vs. simpler heuristics
- **Failure signatures**: Mis-specified missingness mechanism leads to degraded performance; poor convergence when class overlap is extreme
- **First experiments**: 1) Simulate MAR mechanism with varying uncertainty; 2) Simulate MNAR mechanism with class-dependent missingness; 3) Compare classification error with/without informative missingness

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends critically on correct specification of the missingness mechanism.
- Theoretical results assume finite mixture models; extension to complex, high-dimensional, or non-linear data is unexplored.
- No empirical validation on real-world datasets provided.

## Confidence
- Theoretical framework reproducibility: High
- Practical generalisation under correct assumptions: High
- Robustness to mechanism misspecification: Medium
- Applicability to complex real-world data: Medium

## Next Checks
1. Test the framework on real-world semi-supervised datasets with known label acquisition patterns to assess robustness to mechanism misspecification.
2. Compare classification error when the true missingness mechanism is correctly specified versus misspecified, to quantify sensitivity.
3. Extend simulations to higher-dimensional, non-linear data structures to evaluate performance beyond the finite mixture model assumptions.