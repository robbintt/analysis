---
ver: rpa2
title: 'Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge'
arxiv_id: '2506.18998'
source_url: https://arxiv.org/abs/2506.18998
tags:
- self-knowledge
- llms
- arxiv
- task
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a previously overlooked link between memorization
  and self-knowledge deficits in LLMs, showing how reliance on memorized solutions
  artificially inflates models' confidence in their reasoning capabilities. A novel
  framework is introduced that generates self-validated, feasible tasks and evaluates
  consistency of feasibility assessments under minor, logically coherent perturbations.
---

# Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge

## Quick Facts
- **arXiv ID:** 2506.18998
- **Source URL:** https://arxiv.org/abs/2506.18998
- **Reference count:** 40
- **Key outcome:** All tested LLMs show >45% feasibility judgment inconsistency under perturbations, revealing memorization-driven overconfidence in self-knowledge.

## Executive Summary
This paper identifies a previously overlooked link between memorization and self-knowledge deficits in LLMs, showing how reliance on memorized solutions artificially inflates models' confidence in their reasoning capabilities. A novel framework generates self-validated, feasible tasks and evaluates consistency of feasibility assessments under minor, logically coherent perturbations. The study reveals that all tested LLMs—including GPT-4o and Claude—flip feasibility judgments over 45% of the time, with science and medicine domains showing the highest instability. The introduced MIRAGE and SKEW metrics quantify this memorization-driven overconfidence and inconsistent self-knowledge.

## Method Summary
The study employs a three-stage pipeline: Task Generation (LLMs generate self-deemed-feasible STEM tasks at temperature=1.0), Perturbation (three sequential modules: ontology replacement via Gemini 1.5 Flash, instruction translation to German/Spanish/French, and data perturbation with ~15% numerical variation), and Classification (perturbed tasks fed back to original LLM at temperature=0.0 for feasibility judgments). Human spot-checks verify feasibility preservation and logical equivalence across perturbations. MIRAGE measures infeasibility rates of perturbed tasks, while SKEW measures pairwise disagreement in feasibility assessments.

## Key Results
- All tested LLMs (GPT-4o, Claude 3.7 Sonnet, Mistral Large 24.11, DeepSeek-V3) show >45% feasibility judgment inconsistency under perturbations
- Science and medicine domains exhibit highest MIRAGE scores (GPT-4o=0.85, Mistral=0.96)
- Models lack stable internal metrics for reasoning capability, relying instead on memorized patterns
- Memorization-driven overconfidence poses risks for critical applications requiring accurate self-knowledge

## Why This Works (Mechanism)

### Mechanism 1: Memorization-to-Confidence Transfer
LLMs derive self-knowledge confidence from recognizing memorized problem-solution patterns rather than assessing genuine reasoning capability. When models recognize problem formats or terminology from training data, pattern-matching circuits activate high-confidence outputs, misattributed to reasoning ability.

### Mechanism 2: Perturbation-Sensitivity as Memorization Diagnostic
Minor, logically coherent perturbations to task formulations cause feasibility judgment flips specifically when models rely on memorization rather than reasoning. Three perturbation types—ontology replacement, instruction translation, and data perturbation—disrupt surface-level pattern matching while preserving logical structure.

### Mechanism 3: Standardized Domain Amplification
Domains with highly standardized terminology and problem formats (science, medicine) exhibit amplified memorization-driven overconfidence due to higher pattern repetition in training corpora. Standardized jargon creates stronger pattern-frequency signals during training.

## Foundational Learning

- **Concept: Self-knowledge in LLMs (metacognitive calibration)**
  - Why needed: The paper evaluates whether models accurately assess their own capability boundaries
  - Quick check: Can you explain why a model claiming 100% confidence on a task it cannot actually solve represents a self-knowledge failure?

- **Concept: Memorization vs. generalization distinction**
  - Why needed: The core hypothesis depends on distinguishing "recalling a memorized solution" from "applying learned reasoning patterns"
  - Quick check: If a model solves a math problem correctly but fails an isomorphic problem with different numbers and variable names, which failure mode does this indicate?

- **Concept: Consistency as an evaluative signal**
  - Why needed: MIRAGE and SKEW metrics measure inconsistency across perturbations, not direct accuracy
  - Quick check: Why might a model show high accuracy on a benchmark but still exhibit poor self-knowledge according to the SKEW metric?

## Architecture Onboarding

- **Component map:** Task Generation -> Perturbation Pipeline (Ontology -> Translation -> Data) -> Classification -> Metrics Computation
- **Critical path:** Task generation → Human spot-check for feasibility → Perturbation → Human verification that difficulty preserved → Classification → Metric computation
- **Design tradeoffs:** Self-generated tasks ensure models have no plausible deniability about feasibility but introduce selection bias; three perturbation types provide broader coverage but complicate attribution; human spot-checks ensure quality but limit scalability
- **Failure signatures:** Perturbation breaks logical coherence, model generates trivially easy tasks, language-specific failure modes from translation
- **First 3 experiments:**
  1. Baseline replication with GPT-4o to verify MIRAGE ≈0.79 and SKEW ≈0.51
  2. Ablation by perturbation type to determine which drives majority of flips
  3. Control task set from low-standardization domain to test domain-amplification hypothesis

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the memorization-driven self-knowledge inflation effect persist in non-STEM domains or tasks lacking clear instruction-data separation?
- **Open Question 2:** Can specific training-time interventions successfully decouple memorization confidence from genuine reasoning self-knowledge?
- **Open Question 3:** Does multimodal input integration stabilize self-knowledge boundaries or introduce new vectors for memorization-driven overconfidence?

## Limitations
- Causal mechanism linking pattern familiarity to confidence calibration remains inferential
- Human spot-check validation steps limit scalability and introduce potential selection bias
- ~15% numerical perturbation rule is insufficiently specified for domains with precise constants

## Confidence
- **High Confidence:** All tested LLMs show >45% feasibility judgment inconsistency under perturbations (directly measurable)
- **Medium Confidence:** Memorization is the primary driver of feasibility flip rates (methodology sound but alternatives possible)
- **Low Confidence:** Standardized terminology in science/medicine domains directly amplifies memorization-driven overconfidence (requires corpus frequency analysis)

## Next Checks
1. **Perturbation Ablation Study:** Run three separate conditions—ontology-only, translation-only, data-only—to determine which perturbation type drives the majority of feasibility flips
2. **Control Domain Comparison:** Generate tasks from low-standardization domains (creative writing, open-ended design) and compare MIRAGE scores against science/medicine
3. **Corpus Frequency Analysis:** Analyze training corpus data to quantify frequency of standardized problem formats in science/medicine versus other domains