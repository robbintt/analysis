---
ver: rpa2
title: Cross Knowledge Distillation between Artificial and Spiking Neural Networks
arxiv_id: '2507.09269'
source_url: https://arxiv.org/abs/2507.09269
tags:
- data
- knowledge
- distillation
- snns
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving Spiking Neural
  Networks (SNNs) performance by leveraging knowledge from Artificial Neural Networks
  (ANNs) using cross-modality and cross-architecture knowledge distillation. The proposed
  Cross Knowledge Distillation (CKD) method bridges the gap between RGB images and
  event-based DVS data, and between ANN and SNN architectures.
---

# Cross Knowledge Distillation between Artificial and Spiking Neural Networks

## Quick Facts
- arXiv ID: 2507.09269
- Source URL: https://arxiv.org/abs/2507.09269
- Reference count: 32
- Primary result: Achieved SOTA 97.13% accuracy on N-Caltech101 and 40.20% on CEP-DVS using cross-modality KD

## Executive Summary
This paper addresses the challenge of improving Spiking Neural Networks (SNNs) performance by leveraging knowledge from Artificial Neural Networks (ANNs) using cross-modality and cross-architecture knowledge distillation. The proposed Cross Knowledge Distillation (CKD) method bridges the gap between RGB images and event-based DVS data, and between ANN and SNN architectures. It employs a semantically similar replacement module to gradually transition from RGB to DVS data, and an indirect phased knowledge distillation approach to transfer knowledge from a well-performing ANN teacher to an SNN student. Experimental results demonstrate the effectiveness of CKD, achieving a new state-of-the-art top-1 accuracy of 97.13% on N-Caltech101 and a competitive accuracy of 40.20% on CEP-DVS.

## Method Summary
The method involves two key components: (1) a semantically similar replacement module that gradually transitions training data from RGB to DVS through a non-linear probability function, creating a hybrid data stream, and (2) an indirect phased knowledge distillation approach that transfers knowledge from a pre-trained ANN teacher to an SNN student using logits-level distillation combined with feature-level domain alignment via CKA. The SNN student has two parallel streams (static/hybrid and dynamic) that share weights, with domain alignment constraining their feature distributions at each timestep. Training includes a phased switching mechanism that stops KD at an optimal point when the hybrid stream becomes DVS-dominated.

## Key Results
- Achieved new state-of-the-art top-1 accuracy of 97.13% on N-Caltech101 dataset
- Achieved competitive accuracy of 40.20% on CEP-DVS dataset
- Demonstrated effectiveness of gradual modality transition versus direct substitution
- Showed that SNNs can perform on par with ANNs for image classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradual modality transition from RGB to DVS data during training enables more stable cross-modal knowledge transfer than direct substitution.
- **Mechanism:** The semantically similar replacement module uses a non-linear probability function (Preplace) that increases from 0 to 1 across training batches/epochs. Early training exposes the SNN to RGB-rich inputs (benefiting from ANN knowledge), then progressively shifts to DVS-rich inputs (the target modality). This creates a hybrid data stream that bridges the distribution gap between dense RGB and sparse DVS data.
- **Core assumption:** RGB and DVS data from the same category share sufficient semantic similarity to serve as meaningful proxies during intermediate training stages.
- **Evidence anchors:** [abstract] "leverages semantic similarity and sliding replacement to mitigate the cross-modality challenge"; [section III.A.2] "replaces static data with DVS data by a non-linear probability function Preplace, which gradually increases from 0 to 1"; [corpus] Related work on ANN-to-SNN distillation uses progressive alignment but typically without modality transition.
- **Break condition:** If RGB and DVS representations diverge significantly within categories, the semantic bridge weakens and early-stage knowledge may not transfer effectively.

### Mechanism 2
- **Claim:** Logits-level distillation combined with feature-level domain alignment enables more flexible cross-architecture transfer than direct feature matching.
- **Mechanism:** The framework decouples knowledge transfer into two levels: (1) logits-based KD from ANN teacher to SNN hybrid stream (relaxed layer correspondence), and (2) feature-level domain alignment via CKA between RGB and DVS streams within the SNN. The domain alignment loss (LDA) uses learnable timestep weights (θt) and operates at each timestep, then averages. This indirect approach lets the ANN's logits knowledge be "corrected" by the cross-modality transfer module before reaching the final DVS stream.
- **Core assumption:** ANNs can provide useful semantic priors through logits even when their feature representations don't align with SNN's temporal spike-based architecture.
- **Evidence anchors:** [section III.B.1] "our framework appropriately relaxes the constraints on architecture, thereby reducing the costs of computation and training"; [section III.A.1] "domain-alignment loss is employed to constrain the feature distribution differences of the two streams in SNN"; [corpus] Head-Tail-Aware KL Divergence and Efficient Logit-based KD similarly emphasize logits-level distillation for SNNs.
- **Break condition:** If the ANN teacher's logits contain modality-specific biases, these may transfer spurious correlations that domain alignment cannot fully correct.

### Mechanism 3
- **Claim:** Phased distillation switching prevents negative transfer when the hybrid stream becomes dominated by DVS data.
- **Mechanism:** As training progresses, the replacement module increases DVS ratio in the hybrid stream. Since the original intent is for the static stream to serve as an ANN-to-SNN bridge, continuing KD when the stream is DVS-dominated contradicts the design. The switching function γ(e) uses a sigmoid-controlled transition to smoothly reduce KD weight from 1 to 0 at an appropriate training stage.
- **Core assumption:** There exists an optimal switching point where KD has provided maximum benefit but further distillation would either be ineffective or harmful due to modality mismatch.
- **Evidence anchors:** [section III.B.2] "This phenomenon contradicts our initial intent... Therefore, it is crucial to stop the distillation process at a certain point"; [Fig. 4 ablation] Shows performance sensitivity to eth and k values; [corpus] No direct corpus evidence on phased KD switching for cross-modal SNN training.
- **Break condition:** If switching occurs too early, SNN may not fully absorb ANN knowledge; if too late, the modality-mismatched distillation may degrade learned DVS representations.

## Foundational Learning

- **Concept: Leaky Integrate-and-Fire (LIF) Neurons**
  - **Why needed here:** The SNN student uses LIF neurons that accumulate spikes over time with leaky decay (τ). Understanding Eq. 1-3 is essential for grasping why domain alignment operates at each timestep and why temporal averaging matters.
  - **Quick check question:** Given τ=0.5, if a neuron receives no input for 3 timesteps, what fraction of its initial membrane potential remains? (Answer: 0.5³ = 0.125)

- **Concept: Centered Kernel Alignment (CKA)**
  - **Why needed here:** CKA measures representation similarity between networks and is the core metric in the domain-alignment loss (LDA). It enables quantitative comparison of feature distributions across modalities within the same SNN.
  - **Quick check question:** If CKA between RGB and DVS features is 0.85, what is the domain-alignment loss contribution for that timestep? (Answer: 1 - 0.85 = 0.15)

- **Concept: Knowledge Distillation Temperature (τ)**
  - **Why needed here:** The KD loss (Eq. 9) uses softmax temperature to soften probability distributions. This controls how much emphasis is placed on the teacher's "dark knowledge" (relative probabilities of incorrect classes) versus the hard labels.
  - **Quick check question:** Does higher temperature produce softer (more uniform) or harder (more peaked) probability distributions? (Answer: Softer)

## Architecture Onboarding

- **Component map:**
  ANN Teacher (WRN101_2) → SNN Student with two streams:
  Static/Hybrid Stream ← RGB input → DVS input (probabilistic replacement)
  └── Domain Alignment (CKA-based LDA)
  Dynamic Stream ← DVS input only
  └── Final classification (TET loss, Lcls-e)
  
  Key: Both streams share weights in the SNN backbone

- **Critical path:**
  1. Pre-train ANN teacher on RGB Caltech101 (HSV format, 20 epochs from ImageNet checkpoint)
  2. Initialize SNN with shared weights for both streams
  3. Training loop per batch:
     - Sample RGB-DVS paired data (same category)
     - Apply replacement probability to determine hybrid stream input
     - Forward pass through ANN (static only) and SNN (both streams)
     - Compute LDA between static/hybrid and dynamic stream features at each timestep
     - Compute LKD between ANN and SNN static/hybrid logits (if γ(e) > 0)
     - Compute classification losses (Lcls-s, Lcls-e using TET)
     - Backpropagate combined loss
  4. Phase switching: Monitor epoch, reduce γ(e) per Eq. 10

- **Design tradeoffs:**
  - **Logits vs Feature KD:** Authors chose logits to avoid strict layer correspondence, reducing architecture coupling but potentially losing fine-grained feature-level guidance. Feature KD slightly underperformed in ablations.
  - **Switching steepness (k):** N-Caltech101 benefits from swift switching (k=100, eth≈150), while CEP-DVS prefers gradual (k=0.001, eth≈90). Assumption: Dataset complexity and modality gap size determine optimal strategy.
  - **Timestep allocation:** N-Caltech101 uses T=10, CEP-DVS uses T=6. More timesteps capture more temporal dynamics but increase computational cost.

- **Failure signatures:**
  - **Early stagnation:** If accuracy plateaus well below baseline (~93%), likely causes are (a) replacement probability too aggressive, (b) domain alignment weight β too low, or (c) switching epoch eth too early.
  - **Late-stage collapse:** If accuracy degrades after epoch 150+, KD switching may be too delayed (γ(e) still high when hybrid stream is DVS-dominated).
  - **Class-specific failure:** t-SNE visualization showing cluster overlap indicates insufficient domain alignment; consider increasing β or adjusting CKA timestep weights.

- **First 3 experiments:**
  1. **Baseline reproduction:** Train SNN without CKD (only Lcls-e) on N-Caltech101 to establish ~93% baseline. Verify VGGSNN architecture, T=10 timesteps, and TET loss implementation.
  2. **Ablation on eth:** Fix k=100, sweep eth ∈ {100, 125, 150, 175, 200} on N-Caltech101. Expect peak near 150 based on paper's ablation.
  3. **Modality gap analysis:** Train with Preplace forced to 0 (RGB-only hybrid stream) vs 1 (DVS-only hybrid stream). Measure (a) training stability and (b) final accuracy to quantify the benefit of gradual transition. Expect RGB-only to underperform on DVS test set due to modality mismatch.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Cross Knowledge Distillation (CKD) framework be effectively adapted for complex video-based tasks, such as action recognition, where temporal dynamics are significantly more critical than in static-to-dynamic image classification?
- **Basis in paper:** [explicit] The authors state that their replacement module "builds a more gradual hybrid modal data stream" and that "this advantage will become more evident in the future, especially in action recognition with more temporal information and video-based detection tasks."
- **Why unresolved:** The current experiments are limited to N-Caltech101 and CEP-DVS, which primarily involve converting static images to event streams or relatively simple classification, leaving the efficacy of CKD on high-temporal-resolution video tasks unproven.
- **What evidence would resolve it:** Successful application of the CKD framework to standard video action recognition datasets (e.g., DVS128 Gesture or video datasets converted to events) showing performance improvements over standard SNN training methods.

### Open Question 2
- **Question:** Why do state-of-the-art logit-based knowledge distillation methods (e.g., DKD, DIST) negatively impact the CKD framework's domain-alignment module, and can a specialized loss function be designed to avoid this conflict?
- **Basis in paper:** [inferred] The authors note that alternative KD methods "exert relatively excessive influence on the static stream... adversely affecting our domain-alignment module," forcing them to use "vanilla KD," which suggests an unresolved incompatibility between advanced distillation losses and cross-modality alignment.
- **Why unresolved:** The paper identifies the performance drop when using advanced KD methods but does not provide a theoretical analysis or solution for why these methods disrupt the feature distribution alignment required for cross-modality transfer.
- **What evidence would resolve it:** An analysis of the gradient interactions between advanced KD losses and the domain-alignment loss (CKA), or the proposal of a modified distillation loss that improves upon vanilla KD without destabilizing the feature alignment.

### Open Question 3
- **Question:** How can the optimal parameters for the phase switching function (eth and k) be determined a priori for a new dataset, given that the optimal strategy differs significantly between N-Caltech101 and CEP-DVS?
- **Basis in paper:** [inferred] The ablation study reveals that N-Caltech101 performs best with a "swift" switching strategy (k=100), whereas CEP-DVS requires a "slow" strategy (k=0.001), indicating that the transition hyperparameters are highly dataset-sensitive and lack a universal configuration.
- **Why unresolved:** The paper relies on grid search to find these values and does not establish a heuristic or theoretical rule for setting these parameters based on dataset characteristics (e.g., dataset size, class similarity, or event density).
- **What evidence would resolve it:** Identification of specific dataset properties (e.g., temporal density, semantic overlap) that correlate with the optimal switching speed, or the development of an adaptive scheduling mechanism that removes the need for manual hyperparameter tuning.

## Limitations
- Key architectural details remain underspecified, including exact SNN layer configurations, optimizer parameters, and LIF neuron hyperparameters
- The semantic similarity assumption between RGB and DVS data may not hold for motion-dependent classes where static RGB lacks temporal information
- The phased switching mechanism introduces dataset-specific hyperparameters (eth, k) whose optimal values may not generalize beyond tested datasets

## Confidence

- **High Confidence:** The core CKD methodology and its implementation details are well-documented and reproducible, supported by ablation studies and SOTA comparisons
- **Medium Confidence:** The theoretical justification for gradual modality transition and phased KD switching is sound, though the optimal switching strategy appears dataset-dependent rather than universally optimal
- **Low Confidence:** The semantic similarity assumption between RGB and DVS data across all categories lacks thorough validation, and the generalizability of the eth/k hyperparameters to other datasets remains untested

## Next Checks

1. **Semantic Bridge Validation:** Test the replacement module's effectiveness by measuring cross-modal alignment at different Preplace values (0, 0.5, 1) using CKA or similar metrics to verify semantic similarity degrades gracefully rather than abruptly

2. **Hyperparameter Transferability:** Apply the learned eth and k values from N-Caltech101 and CEP-DVS to a third dataset (e.g., DVS128 Gesture) to assess whether the switching strategy generalizes or requires dataset-specific tuning

3. **Negative Transfer Analysis:** Systematically evaluate accuracy degradation when KD switching occurs too early or too late to establish the boundaries of the safe switching window and quantify the risk of negative transfer