---
ver: rpa2
title: 'STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop
  Multi-Agent Framework'
arxiv_id: '2506.01531'
source_url: https://arxiv.org/abs/2506.01531
tags:
- formula
- reasoning
- mathematical
- data
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STORM-BORN, a challenging mathematical derivations
  dataset curated from cutting-edge academic papers using a human-in-the-loop multi-agent
  framework. The dataset addresses limitations in existing math datasets by focusing
  on complex reasoning processes with human-like approximations and heuristic cues,
  rather than simple numerical computations or formal proofs.
---

# STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework

## Quick Facts
- arXiv ID: 2506.01531
- Source URL: https://arxiv.org/abs/2506.01531
- Reference count: 39
- Even advanced models like GPT-o1 solve fewer than 5% of STORM-BORN problems compared to 95% on GSM8K

## Executive Summary
STORM-BORN introduces a challenging mathematical derivations dataset curated from cutting-edge academic papers using a human-in-the-loop multi-agent framework. The dataset addresses limitations in existing math datasets by focusing on complex reasoning processes with human-like approximations and heuristic cues, rather than simple numerical computations or formal proofs. The multi-agent system extracts mathematical expressions, generates questions, retrieves answers, and refines them through human expert evaluation, resulting in 100 high-quality problems. Fine-tuning on the dataset yields significant improvements: 7.84% accuracy gain for LLaMA3-8B and 9.12% for Qwen2.5-7B on numerical reasoning tasks, demonstrating strong generalization despite the dataset not containing explicit numerical problems.

## Method Summary
The STORM-BORN dataset was created using a six-agent pipeline: Math Expression Extractor, Query Draft Agent, Answer Retriever Agent, Context Collector Agent, Question Refiner Agent, and Answer Filter Agent. The system first filters arXiv papers from May 2023–Oct 2024 with OpenReview scores above "weak accept" and ≥5 occurrences of derivation-related keywords. It then generates 2,000 synthetic samples, which domain experts select down to 100 problems based on reasoning density, problem clarity, derivation correctness, and problem difficulty. The final dataset is used for benchmarking and fine-tuning LLMs, with evaluation on GSM8K, MATH, and AIME using 0-shot and few-shot settings.

## Key Results
- GPT-o1-Pro solves fewer than 5% of STORM-BORN problems versus ~95% on GSM8K, demonstrating the dataset's superior difficulty
- Fine-tuning on STORM-BORN improves LLaMA3-8B accuracy by 7.84% and Qwen2.5-7B by 9.12% on numerical reasoning tasks
- The dataset demonstrates strong generalization to standard benchmarks despite not containing explicit numerical problems
- Human experts curated 100 problems from 2,000 generated samples based on reasoning density and derivation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-agent framework can decompose complex tasks into manageable sub-tasks, improving output quality over single-agent approaches for data generation.
- Mechanism: The paper's "STORM" framework delegates specific roles (extraction, drafting, retrieval, refinement) to specialized agents. This prevents prompt overload, allowing each agent to focus on a single objective. The sequential workflow ensures each step is addressed before moving to the next.
- Core assumption: The task is too complex for a single model to handle reliably in one pass, and that error propagation between agents can be managed.
- Evidence anchors:
  - [section] "Why not single-agent? ... The task is inherently complex and involves multiple steps... By employing a multi-agent system, we can decompose the task into smaller, more manageable components..." (Page 4).
  - [corpus] Related work (e.g., 'Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning') supports the general principle of collaborative decomposition.
- Break condition: If the prompts for any single agent become as complex as the original single-agent task, or if the handoff between agents loses critical information, the quality will degrade to or below the single-agent baseline.

### Mechanism 2
- Claim: Curation from expert-level sources (academic papers) with human-like reasoning (approximations, heuristics) creates data that improves generalization to standard numerical reasoning tasks.
- Mechanism: The dataset focuses on "human-like derivations" from recent, high-quality papers. This data contains reasoning-dense steps and heuristics. Fine-tuning on this style of reasoning is hypothesized to transfer to other math tasks, like numerical reasoning (MATH, GSM8K), even without explicit numerical examples in the training set.
- Core assumption: The reasoning patterns required for complex derivation are a superset or a strong foundation for those needed for simpler numerical tasks.
- Evidence anchors:
  - [abstract] "...demonstrating strong generalization despite the dataset not containing explicit numerical problems."
  - [section] "Fine-tuning on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B)..." (Page 7, Table 1).
  - [corpus] Related work like 'Nemotron-Math' and 'MathFimer' also emphasize reasoning density and long-context traces.
- Break condition: If the derivations are too abstract or disconnected from calculation (e.g., purely topological proofs with no algebraic manipulation), transfer to numerical tasks would likely fail.

### Mechanism 3
- Claim: Combining LLM-based generation with rigorous, expert human-in-the-loop (HITL) evaluation is a viable path to creating ultra-challenging, high-quality benchmarks.
- Mechanism: The system generates a large pool of synthetic candidates (2,000 samples). Human experts then act as a high-precision filter, selecting the top 100 most difficult and well-formed problems based on criteria like "reasoning density" and "derivation correctness."
- Core assumption: Human experts can reliably identify "reasoning density" and "problem clarity" and are willing to perform this curation task at scale.
- Evidence anchors:
  - [abstract] "...human-in-the-loop multi-agent framework... We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems."
  - [section] "Human Expert Selection... This process was guided by the following five core principles..." (Page 5-6).
  - [corpus] The principle is foundational to dataset creation.
- Break condition: If human annotators lack the necessary domain expertise or consistency, the resulting curated dataset will be noisy or biased towards problems that appear difficult but are actually ill-posed.

## Foundational Learning

- Concept: **Human-in-the-loop (HITL) Machine Learning**
  - Why needed here: This entire dataset relies on HITL for quality control. Understanding the trade-off (annotation cost vs. data quality) is critical.
  - Quick check question: What is the primary trade-off when adding a human-in-the-loop to a data generation pipeline?
    - (Answer: Reduced speed and increased cost per sample in exchange for higher data quality, consistency, and reliability).

- Concept: **Multi-agent Systems**
  - Why needed here: The paper proposes this as an alternative to single-agent LLM usage. Understanding why tasks are decomposed and how agents communicate is key.
  - Quick check question: In the STORM pipeline, why is a multi-agent system preferred over a single agent for this task?
    - (Answer: To decompose a complex, multi-step task with a long prompt into smaller, focused sub-tasks, improving instruction following and output quality).

- Concept: **Transfer Learning in Mathematical Reasoning**
  - Why needed here: A core claim is that training on derivation data improves performance on numerical reasoning tasks. This is a form of transfer learning.
  - Quick check question: The paper claims fine-tuning on STORM-BORN improves performance on GSM8K/MATH. What property of STORM-BORN data makes this transfer possible?
    - (Answer: It focuses on "reasoning-dense" steps and human-like heuristics, teaching the model generalizable reasoning patterns rather than just numerical calculation).

## Architecture Onboarding

- Component map: Math Expression Extractor -> Query Draft Agent -> Answer Retriever Agent -> Context Collector Agent -> Question Refiner Agent -> Answer Filter Agent -> Human Expert Selection
- Critical path: The data flow starts with selecting high-quality arXiv papers -> extracting formulas -> drafting questions -> retrieving answers from the paper -> collecting context -> refining the question to be self-contained -> filtering the answer. The human experts are the final quality gate.
- Design tradeoffs: The biggest trade-off is quality vs. quantity. The pipeline generates 2,000 samples but curates only 100, prioritizing difficulty and reliability over scale. Another trade-off is cost vs. automation; they use powerful models (GPT-o1-Pro) for complex agents but rely on expensive human labor for final selection.
- Failure signatures:
  - Context Loss: If the Context Collector fails to capture all necessary background info, the "self-contained" question will be unsolvable.
  - Hallucination in Retrieval: If the Answer Retriever agent generates an answer instead of extracting it from the source text, the data becomes unreliable.
  - Annotation Bottleneck: If the human experts are inconsistent in applying the five principles, the dataset will have variable quality.
- First 3 experiments:
  1. Reproduce the Full Pipeline: Re-implement the 6-agent system using an available LLM API and a single, open-access paper. Compare the generated "self-contained" question against the paper's content to verify extraction fidelity.
  2. Agent Ablation (Single vs. Multi): Build a simplified, single-agent version of the pipeline with the same paper. Compare the quality (completeness, clarity) of the output from the single-agent vs. the multi-agent system.
  3. Transfer Learning Test: Fine-tune a small, open-source model (e.g., Tiny-LLaMA or LLaMA-7B) on the full 2,000-sample generated set (without human curation) and evaluate on a standard numerical reasoning benchmark like a subset of MATH.

## Open Questions the Paper Calls Out

The paper explicitly identifies several open questions:

1. **Can advanced LLMs replace human experts in evaluating "reasoning density" and "derivation correctness" for complex mathematical data?**
   - The paper states that automated evaluation of data quality remains challenging and currently relies on manual inspection by mathematicians, suggesting future work could employ LLMs to automate this process.

2. **Does training on informal, "human-like" mathematical derivations improve a model's ability to generate formal, verifiable proofs (e.g., in Lean or Isabelle)?**
   - The paper contrasts STORM-BORN with formal theorem proving datasets and demonstrates transfer to numerical reasoning, but leaves open whether heuristic, approximation-heavy nature helps or hinders rigid syntax of formal proof assistants.

3. **Can the multi-agent generation pipeline be improved to yield high-quality samples without the current 95% rejection rate (2,000 down to 100)?**
   - The methodology describes generating 2,000 synthetic samples that human experts then rigorously filter down to 100, suggesting significant inefficiency or noise in the agent-based generation phase.

## Limitations

- Extreme data scarcity: The reported fine-tuning gains are achieved with only 100 curated problems, raising concerns about statistical significance and overfitting.
- Human-in-the-loop bottleneck: The expert curation process creates a significant scaling bottleneck and lacks inter-annotator agreement metrics.
- Potential dataset contamination: The evaluation on benchmarks like MATH and GSM8K may be affected by overlap with the academic sources used for STORM-BORN generation.

## Confidence

- **High Confidence:** The multi-agent framework architecture and its role in decomposing complex tasks is well-specified and theoretically sound. The benchmark difficulty claim (GPT-o1-Pro <5% accuracy on STORM-BORN vs ~95% on GSM8K) is directly measurable.
- **Medium Confidence:** The transfer learning claim that fine-tuning on derivation data improves numerical reasoning performance is supported by results, but the small dataset size (100 problems) raises concerns about generalizability.
- **Low Confidence:** The scalability and reproducibility of the human-in-the-loop curation process. Without detailed annotation guidelines or evidence that this process can be applied to larger datasets, the practical utility remains uncertain.

## Next Checks

1. **Dataset Contamination Audit:** Conduct a systematic check for overlap between STORM-BORN source papers and problems in standard benchmarks (MATH, GSM8K, AIME). Calculate the exact percentage of benchmark problems that may have been influenced by the same academic literature used to generate STORM-BORN.

2. **Scaling Experiment:** Implement the full 2,000-sample generation pipeline without human curation and evaluate the resulting dataset on standard benchmarks. Compare performance gains against the 100-sample curated version to quantify the trade-off between data quality and quantity.

3. **Fine-tuning Robustness Test:** Perform ablation studies on the fine-tuning experiments by varying the number of training samples (e.g., 10, 25, 50, 75, 100 problems) and measuring performance degradation. This would reveal whether the reported gains are robust to dataset size reduction or primarily driven by overfitting to the 100 curated problems.