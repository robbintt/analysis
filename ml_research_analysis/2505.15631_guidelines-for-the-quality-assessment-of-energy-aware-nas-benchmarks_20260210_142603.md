---
ver: rpa2
title: Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks
arxiv_id: '2505.15631'
source_url: https://arxiv.org/abs/2505.15631
tags:
- power
- energy
- consumption
- meter
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the reliability of energy measurements in
  the EA-HAS-Bench energy-aware neural architecture search benchmark. The authors
  analyze three key design principles for such benchmarks: reliable power measurements,
  a wide range of GPU usage, and holistic cost reporting.'
---

# Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks

## Quick Facts
- **arXiv ID:** 2505.15631
- **Source URL:** https://arxiv.org/abs/2505.15631
- **Reference count:** 40
- **Primary result:** This study evaluates the reliability of energy measurements in the EA-HAS-Bench energy-aware neural architecture search benchmark.

## Executive Summary
This study evaluates the reliability of energy measurements in the EA-HAS-Bench energy-aware neural architecture search benchmark. The authors analyze three key design principles for such benchmarks: reliable power measurements, a wide range of GPU usage, and holistic cost reporting. Their experiments reveal that Nvidia SMI produces poor correlation with external power meter measurements due to low sampling rates in certain epochs, while NVML provides more accurate results. They find that EA-HAS-Bench exhibits a narrow GPU usage range (146-305W) and propose a calibration method to improve holistic energy reporting using Code Carbon, reducing maximum inaccuracy from 10.3% to 6.6%. The study highlights the importance of reliable measurement tools and calibration experiments for developing trustworthy energy-aware benchmarks.

## Method Summary
The study evaluates energy measurement reliability in EA-HAS-Bench by comparing internal tools (SMI, NVML, Code Carbon) against an external power meter ground truth. The authors sample 500 architectures for validation and 20 for main analysis, training RegNet models on TinyImageNet sequentially on a single GPU. Power is logged via SMI (10Hz), pyNVML, Code Carbon (100ms interval), and RAPL. The "ground truth" is calibrated by measuring idle and stressed system states ($P_{busy} = 811W$, $P_{idle} = 783W$) to correct holistic tools.

## Key Results
- Nvidia SMI shows poor correlation with external power meters due to low sampling rates in short epochs, while NVML provides more accurate results
- EA-HAS-Bench exhibits a narrow GPU usage range of 146-305W, utilizing only ~40% of GPU capacity
- Code Carbon calibration using empirically measured $P_{busy}$ bounds reduces maximum inaccuracy from 10.3% to 6.6%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct library polling (NVML) provides more reliable energy estimates than CLI wrappers (SMI) for short-duration training epochs.
- **Mechanism:** The Nvidia SMI CLI introduces overhead that interferes with the sampling loop, effectively reducing the sampling rate during short kernel executions. This results in sparse data capture (fewer than 10 samples in 39% of epochs), failing to register high-power states. Direct NVML calls avoid this overhead, maintaining consistent sampling frequency and capturing transient power spikes.
- **Core assumption:** The observed sampling dropouts in SMI are intrinsic to the tool's overhead rather than external system latency.
- **Evidence anchors:**
  - [abstract] "Nvidia SMI produces poor correlation... due to low sampling rates in certain epochs, while NVML provides more accurate results."
  - [section V] "In these epochs, there are no more than 10 power samples measured... we observed in subsequent experiments that it is due to the use of SMI on top of pyNVML."
  - [corpus] Corpus indicates "Kernel-Level Energy-Efficient NAS" uses proxy metrics like FLOPs; this paper mechanism argues that *direct* measurement is feasible but tool-dependent.
- **Break condition:** If epochs are sufficiently long (e.g., full training cycles), SMI aggregates converge with power meter readings (Pearson 0.99), breaking the need for high-frequency NVML polling.

### Mechanism 2
- **Claim:** Calibrating "uncaptured" system power ($P_{busy}$) significantly reduces error in holistic energy reporting tools like Code Carbon.
- **Mechanism:** Tools like Code Carbon rely on static assumptions for non-GPU components (e.g., assuming 3W per 8GB of memory). These assumptions often underestimate total node power. By empirically measuring the system's idle ($P_{idle}$) and stressed ($P_{busy}$) baselines, one can derive a dynamic offset that accounts for uncaptured motherboard/fan losses, reducing maximum inaccuracy from 10.3% to 6.6%.
- **Core assumption:** The "uncaptured" power is relatively constant or bounded within $P_{idle}$ and $P_{busy}$ during the training workload.
- **Evidence anchors:**
  - [abstract] "...propose a calibration method to improve holistic energy reporting using Code Carbon, reducing maximum inaccuracy from 10.3% to 6.6%."
  - [section IV] "We thus need to determine a $P_{busy}$ that approximates the uncaptured power consumption... calculating large prime numbers with a $P_{busy}$ of 811 W is a good lower bound."
  - [corpus] "Fuzz Smarter... generates significant carbon footprints"; accurate holistic measurement is a prerequisite for the carbon accounting described in related Green AI work.
- **Break condition:** If the workload induces highly variable CPU/memory loads outside the calibrated "busy" bounds, the linear correction may over- or under-compensate.

### Mechanism 3
- **Claim:** A wide range of GPU utilization is required for a benchmark to effectively differentiate energy-efficient architectures.
- **Mechanism:** The EA-HAS-Bench search space (RegNet on H100) yields a narrow power band (146â€“305W), utilizing only ~40% of the GPU's capacity. This saturation limits the benchmark's ability to act as a surrogate for diverse hardware scenarios (e.g., low-power IoT vs. high-power clusters), effectively flattening the energy landscape.
- **Core assumption:** Architectures that are efficient at 300W scale similarly to efficiency at 50W or 700W (an assumption challenged by the narrow range).
- **Evidence anchors:**
  - [abstract] "We show a narrow usage range... narrowing down even further when using all four GPUs."
  - [section VI.C] "The range of different power draws on the GPU is relatively narrow... This is a sign that the RegNet search space is not ideal for the H100 GPU."
  - [corpus] "EARL: Energy-Aware Optimization... for Pervasive AI" emphasizes on-device constraints; a benchmark with a 146W floor cannot model these pervasive/IoT scenarios effectively.
- **Break condition:** If the benchmark is used only to rank relative efficiency within the same high-power hardware class, the narrow range may be sufficient.

## Foundational Learning

- **Concept: Power Sampling vs. Integration**
  - **Why needed here:** The paper highlights a failure mode where sampling rate ($10\text{Hz}$) misses transient loads. Understanding the difference between instantaneous power draw and integrated energy (Joules) over time is critical for debugging measurement tools.
  - **Quick check question:** Does your measurement tool report the average power over the last second (SMI default) or instantaneous draw at the query time?

- **Concept: System Power Composition ($P_{total} = P_{comp} + P_{static} + P_{loss}$)**
  - **Why needed here:** Simply measuring GPU power ($P_{GPU}$) ignores the "holistic cost" (CPU, Memory, Cooling). The paper isolates $P_{busy}$ to correct holistic tools that rely on incomplete component sums.
  - **Quick check question:** When reporting energy, have you included the Power Usage Effectiveness (PUE) factor or measured the wall-socket drain directly?

- **Concept: Surrogate Benchmarking**
  - **Why needed here:** The goal is to avoid full training costs by querying a pre-trained model. If the data used to train the surrogate (collected via faulty SMI) is noisy, the surrogate inherits that inaccuracy.
  - **Quick check question:** Is the surrogate modeling the validation accuracy, the energy cost, or a multi-objective Pareto front?

## Architecture Onboarding

- **Component map:**
  - External Power Meter (ZES ZIMMER LMG450) -> Ground Truth measurements
  - 4x NVIDIA H100 GPUs, 2x Intel Xeon Platinum, 2TB Mem -> Subject Node
  - NVML (GPU), PyRAPL (CPU/Mem), Code Carbon (Aggregator + Estimator), Nvidia SMI (Control) -> Software Stack

- **Critical path:**
  1. Define workload (RegNet on TinyImageNet)
  2. Select measurement tool (SMI vs. NVML)
  3. **Calibration Phase:** Run idle + stress (Firestarter/Prime Calc) to determine $P_{idle}$ and $P_{busy}$ offsets
  4. **Data Collection:** Sample architecture power over epochs
  5. **Post-processing:** Apply calibration bounds to raw measurements

- **Design tradeoffs:**
  - **SMI vs. NVML:** SMI is easier to script but suffers from sampling interference. NVML requires Python bindings (pyNVML) but offers reliable high-frequency polling.
  - **Accuracy vs. Accessibility:** External power meters are most accurate but require physical access; Code Carbon is software-only but requires calibration ($P_{busy}$ bounds) to be trustworthy.

- **Failure signatures:**
  - **Bimodal Distribution:** Seeing power readings stuck in distinct "low" and "high" clusters with no intermediate values (indicative of SMI sampling dropouts)
  - **Constant Offset:** Energy readings that track the ground truth perfectly but are consistently lower by a fixed margin (indicative of missing $P_{busy}$ component)
  - **Floor Saturation:** A benchmark where no architecture consumes less than 100W (fails to represent low-power contexts)

- **First 3 experiments:**
  1. **SMI vs. NVML Race:** Run a 10-second high-intensity training kernel. Measure simultaneously with SMI (loop) and pyNVML (loop). Compare sample counts and correlation to identify sampling dropout.
  2. **Determine $P_{busy}$:** Run a CPU stress test (e.g., calculating large primes or Firestarter) and measure wall power. Subtract RAPL (CPU) and NVML (GPU) to derive the uncaptured system overhead ($P_{busy}$).
  3. **Code Carbon Validation:** Run a full training job with Code Carbon and an external meter (if available) or derived ground truth. Apply the $P_{busy}$ correction to Code Carbon's output and verify if the % error drops toward the target (6.6%).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do the measurement discrepancies in Nvidia SMI and the narrow GPU usage range observed on NVIDIA H100s generalize to other hardware architectures, particularly consumer-grade GPUs or non-NVIDIA accelerators?
- **Basis in paper:** [explicit] The authors state in Section VII that evaluating only one type of GPU introduces bias and that "Testing on additional hardware is out of scope of this study, but would be worthwhile investigating in future work."
- **Why unresolved:** The entire study was restricted to a single node configuration with four NVIDIA H100 GPUs, leaving the behavior of other hardware unknown.
- **What evidence would resolve it:** Comparative data showing correlation coefficients between SMI/NVML and external power meters across different GPU models (e.g., consumer RTX series or AMD Instinct accelerators).

### Open Question 2
- **Question:** What specific computational load provides the optimal bounds for $P_{busy}$ to minimize the inaccuracy of offline calibration in holistic energy reporting tools like Code Carbon?
- **Basis in paper:** [explicit] The authors note in Section VI.D and Section VII that while they propose a calibration method, "precisely defining what load gives the best bounds for this correction will be explored within future work."
- **Why unresolved:** The paper demonstrates that "prime number calculation" provides better bounds than "Firestarter" or "Idle" states, but does not conclusive identify the optimal standard load.
- **What evidence would resolve it:** Empirical validation comparing the error rates of various stress-test loads (e.g., LINPACK, stress-ng) against external power meter ground truth to minimize the maximum inaccuracy below the achieved 6.6%.

### Open Question 3
- **Question:** How can energy-aware NAS benchmarks be constructed to support search spaces that are device-agnostic and transferable to hardware-constrained devices such as IoT endpoints?
- **Basis in paper:** [explicit] Section VII states that "In the future, energy-aware NAS benchmarks should sample from a more device-agnostic search space and provide transferability towards hardware-constrained devices."
- **Why unresolved:** Current benchmarks like EA-HAS-Bench rely on specific search spaces (RegNet) and server-grade hardware usage profiles (narrowing further with multi-GPU use), which may not translate to smaller devices.
- **What evidence would resolve it:** The design and evaluation of a surrogate benchmark where energy predictions trained on high-end hardware data accurately correlate with actual energy consumption on low-power IoT devices.

## Limitations

- The narrow power range finding may be hardware-specific to H100 GPUs rather than a fundamental limitation of the RegNet search space
- The calibration method relies on a single static $P_{busy}$ measurement that may not capture dynamic workload variations
- The study lacks external power meter validation for the main results, reducing confidence in absolute energy measurements

## Confidence

- **High confidence:** NVML provides more reliable measurements than SMI due to clear empirical evidence of sampling dropouts in SMI during short epochs
- **Medium confidence:** The claim about EA-HAS-Bench's inadequacy for energy-aware NAS due to narrow power range, as it relies on assumptions about power-efficiency scaling across hardware contexts
- **Medium confidence:** The Code Carbon calibration method, as it relies on a single static measurement that may not generalize across different workloads

## Next Checks

1. **Hardware Generalization Test:** Repeat the power measurement study on multiple GPU architectures (e.g., A100, RTX 4090) to verify whether the narrow power range finding is hardware-specific or a fundamental limitation of the RegNet search space.

2. **Dynamic Calibration Validation:** Implement a time-varying $P_{busy}$ correction based on CPU/memory utilization patterns during training, and measure whether this improves accuracy beyond the static 6.6% error bound.

3. **Search Space Expansion:** Modify EA-HAS-Bench to include architectures with known low-power variants (e.g., MobileNet, EfficientNet) and verify whether the power range expands to better represent diverse hardware scenarios.