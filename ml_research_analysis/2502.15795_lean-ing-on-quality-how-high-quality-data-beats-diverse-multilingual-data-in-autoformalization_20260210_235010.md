---
ver: rpa2
title: 'Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data
  in AutoFormalization'
arxiv_id: '2502.15795'
source_url: https://arxiv.org/abs/2502.15795
tags:
- language
- data
- dataset
- theorem
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of autoformalization\u2014\
  translating informal mathematical statements into formal Lean4 proofs\u2014by introducing\
  \ a novel methodology that prioritizes data quality over quantity. The authors leverage\
  \ backtranslation with hand-curated prompts to enhance mathematical capabilities\
  \ of language models, particularly addressing the scarcity of labeled formal-informal\
  \ data."
---

# Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization

## Quick Facts
- arXiv ID: 2502.15795
- Source URL: https://arxiv.org/abs/2502.15795
- Reference count: 5
- High-quality synthetic data improves autoformalization performance on benchmarks like ProofNet, outperforming pretrained models using minimal tokens

## Executive Summary
This paper introduces a novel methodology for autoformalization that prioritizes data quality over quantity by leveraging backtranslation with hand-curated prompts to enhance mathematical capabilities of language models. The approach addresses the scarcity of labeled formal-informal data by generating high-quality synthetic data through three variations: on-the-fly backtranslation, distilled backtranslation with few-shot amplification, and line-by-line proof analysis integrated with proof state information. The method demonstrates that high-quality synthetic data can significantly outperform extensive multilingual fine-tuning approaches on benchmarks like ProofNet while using only 1/150th of the tokens, offering a promising direction for resource-efficient mathematical formalization.

## Method Summary
The authors propose a backtranslation-based approach to autoformalization that generates high-quality synthetic training data from existing formal proofs. The method employs hand-curated prompts to guide language models in translating formal Lean4 proofs back into informal mathematical statements, creating pairs that can be used to train autoformalization systems. Three primary variations are explored: on-the-fly backtranslation that generates data dynamically during training, distilled backtranslation that combines few-shot amplification techniques for improved quality, and line-by-line proof analysis that incorporates proof state information for enhanced context. This approach leverages the abundance of formal proofs to create synthetic informal-formal pairs, addressing the fundamental challenge of limited labeled data in mathematical formalization tasks.

## Key Results
- High-quality synthetic data improves autoformalization performance on ProofNet benchmark
- Outperforms pretrained models using minimal tokens compared to traditional approaches
- Surpasses MMA fine-tuning on ProofNet with only 1/150th of the tokens required

## Why This Works (Mechanism)
The approach works by generating high-quality synthetic training data through backtranslation, which addresses the fundamental scarcity of labeled formal-informal pairs in mathematical domains. By using hand-curated prompts, the method guides language models to produce informal statements that accurately capture the mathematical essence of formal proofs. The three variations provide different strategies for data generation: on-the-fly backtranslation offers flexibility and freshness, distilled backtranslation with few-shot amplification improves consistency and quality, and line-by-line proof analysis with proof state information provides contextual awareness. This combination allows the system to learn effective mappings between informal and formal mathematical representations while requiring significantly fewer resources than traditional multilingual fine-tuning approaches.

## Foundational Learning
- Backtranslation: Technique of translating from target to source language to generate synthetic training data; needed because labeled formal-informal pairs are scarce in mathematical domains
- Proof state information: Context about current proof goals and assumptions; needed to provide relevant context for accurate formalization
- Few-shot amplification: Technique using multiple prompts with the same input to improve output quality; needed to enhance the reliability of synthetic data generation
- Lean4 formal proof structure: Understanding the syntax and semantics of formal proofs in Lean4; needed as the target representation for autoformalization
- Mathematical informal-formal mapping: The relationship between natural language mathematics and formal proof representations; needed to guide the backtranslation process

## Architecture Onboarding

Component Map:
Backtranslation Prompts -> Language Model -> Synthetic Data -> Training Pipeline -> Autoformalization Model

Critical Path:
The critical path involves generating synthetic data through backtranslation, which requires running the language model with curated prompts on existing formal proofs. This synthetic data then feeds directly into the training pipeline for the autoformalization model. The efficiency and quality of this data generation process directly impact the performance of the final model.

Design Tradeoffs:
The approach trades computational overhead during training (for on-the-fly backtranslation) against the quality and consistency of pre-generated synthetic data. Using hand-curated prompts provides better control over output quality but requires domain expertise and careful prompt engineering. The choice between different backtranslation variations involves balancing flexibility, quality, and computational requirements.

Failure Signatures:
Poor quality synthetic data will manifest as noisy or incorrect formal-informal pairs, leading to degraded autoformalization performance. Overfitting to synthetic data patterns rather than genuine mathematical understanding may occur if the backtranslation process produces overly consistent or biased outputs. Failure to capture mathematical nuance in the informal statements will result in models that struggle with complex or novel proof structures.

First Experiments:
1. Generate a small batch of synthetic data using different prompt variations and evaluate quality manually
2. Compare autoformalization performance on a subset of ProofNet using synthetic data versus no additional training data
3. Test the impact of including versus excluding proof state information in the backtranslation process

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on ProofNet benchmark with limited testing on diverse mathematical domains
- Hand-curated prompts are not described in sufficient detail for reproducibility across different mathematical topics
- Exact computational costs of backtranslation process and few-shot amplification steps are not quantified
- Limited evidence for generalizability beyond the specific benchmark tested
- Comparison to MMA fine-tuning does not address potential differences in model architectures

## Confidence
Medium - The experimental results are internally consistent and demonstrate clear improvements over baseline approaches on the tested benchmark. However, the limited scope of evaluation, lack of detailed methodology for key components, and absence of comprehensive resource analysis prevent High confidence in the broader claims about generalizability and efficiency.

## Next Checks
1. Evaluate the backtranslation approach across multiple mathematical domains and proof styles beyond ProofNet to assess generalizability
2. Conduct a detailed computational resource analysis comparing the full pipeline costs (including backtranslation and few-shot amplification) against multilingual fine-tuning approaches
3. Perform ablation studies to isolate the contributions of each component (on-the-fly backtranslation, distilled backtranslation, line-by-line proof analysis) to the overall performance gains