---
ver: rpa2
title: Surrogate Benchmarks for Model Merging Optimization
arxiv_id: '2509.02555'
source_url: https://arxiv.org/abs/2509.02555
tags:
- merging
- surrogate
- optimization
- benchmarks
- hyperparameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SMM-Bench, a surrogate benchmark for optimizing
  hyperparameters in model merging. The key idea is to construct predictive models
  that estimate the performance of merged LLMs from hyperparameter configurations,
  enabling efficient algorithm development and comparison without expensive actual
  merging evaluations.
---

# Surrogate Benchmarks for Model Merging Optimization

## Quick Facts
- arXiv ID: 2509.02555
- Source URL: https://arxiv.org/abs/2509.02555
- Reference count: 2
- One-line primary result: Introduces SMM-Bench, a surrogate benchmark with predictive performance (R² scores up to 0.962) for optimizing hyperparameters in model merging

## Executive Summary
This paper introduces SMM-Bench, a surrogate benchmark for optimizing hyperparameters in model merging. The key idea is to construct predictive models that estimate the performance of merged LLMs from hyperparameter configurations, enabling efficient algorithm development and comparison without expensive actual merging evaluations. The authors define two search spaces for parameter-space (PS) and data-flow-space (DFS) merging, collect datasets using random sampling, CMA-ES, and TPE, and train LightGBM surrogate models to predict accuracy on Japanese mathematics tasks. Results show strong predictive performance (R² scores up to 0.962) and successful simulation of optimization algorithm behaviors. The benchmark allows rapid evaluation of algorithms like Sep-CMA and DE in minutes instead of days, demonstrating its practical utility for advancing model merging optimization research.

## Method Summary
The method constructs surrogate benchmarks for two model merging paradigms: parameter-space (PS) merging using layer-wise task arithmetic weights (64 continuous variables) and data-flow-space (DFS) merging using layer selection plus scaling factors (32 categorical + 63 continuous variables). Training data is collected through actual model merges evaluated on Japanese mathematics benchmarks (gsm8k-ja and MGSM) using random sampling, CMA-ES, and TPE strategies. LightGBM models are trained with Optuna-optimized hyperparameters and 5-fold CV ensemble averaging. The surrogate takes hyperparameter vectors as input and predicts accuracy scores, enabling rapid evaluation of optimization algorithms without expensive actual merging.

## Key Results
- Surrogate models achieve R² scores up to 0.962 and Kendall's Tau of 0.791-0.883 on held-out test sets
- Behavioral simulation successfully replicates optimization trajectories for random search, CMA-ES, and TPE
- Sep-CMA outperforms DE in optimization speed when evaluated on SMM-Bench-PS
- DFS surrogates show tendency to overestimate performance despite high correlation with true values
- Full optimization runs complete in minutes rather than days compared to actual merging evaluations

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Model Fidelity Through Gradient Boosting
- LightGBM surrogate models can accurately predict merged LLM performance from hyperparameter configurations by learning hierarchical feature interactions from diverse sampling trajectories
- Core assumption: The performance landscape is sufficiently smooth and locally consistent for tree-based regression to generalize
- Evidence anchors: R² = 0.950 (PS/gsm8k-ja), 0.962 (DFS/gsm8k-ja); Kendall's Tau = 0.791-0.883
- Break condition: Surrogate accuracy degrades when extrapolating far beyond training sample ranges

### Mechanism 2: Behavioral Simulation via Multi-Strategy Data Coverage
- Surrogate benchmarks replicate relative algorithm rankings and optimization trajectories by spanning uniform coverage and high-performance regions
- Core assumption: Algorithm trajectories are primarily determined by the shape of the response surface rather than stochastic noise
- Evidence anchors: Optimization trajectories on surrogate benchmarks qualitatively match true benchmark curves for random, CMA-ES, and TPE
- Break condition: When DFS surrogates overestimate absolute performance or ranking inversions occur

### Mechanism 3: Search Space Parameterization Alignment
- Separate surrogate models for PS (continuous) and DFS (mixed categorical-continuous) spaces preserve structural constraints essential to each merging paradigm
- Core assumption: The defined search spaces contain the optima relevant to practical model merging
- Evidence anchors: PS uses 64 continuous variables for layer-wise task arithmetic weights; DFS combines 32 categorical + 63 continuous variables
- Break condition: If optimal configurations lie outside the bounded search ranges

## Foundational Learning

- Concept: **Task Arithmetic and Layer-wise Merging**
  - Why needed here: PS benchmark optimizes per-layer weighting factors; understanding how task vectors combine explains why 64 hyperparameters emerge from two 32-layer models
  - Quick check question: Can you explain why layer-wise merging increases the hyperparameter count from 2 (model-level) to 64 (layer-level)?

- Concept: **Surrogate Benchmarks vs. Tabular Benchmarks**
  - Why needed here: SMM-Bench uses regression models rather than exact lookup tables; this enables coverage of continuous, high-dimensional spaces
  - Quick check question: What is the fundamental trade-off between surrogate benchmark flexibility and prediction certainty compared to tabular benchmarks?

- Concept: **CMA-ES and TPE for Data Collection**
  - Why needed here: Sampling strategies determine which regions of the search space are well-represented; combining random, evolutionary, and Bayesian approaches ensures both exploration and exploitation coverage
  - Quick check question: Why might training surrogates only on random samples still yield useful behavioral simulation?

## Architecture Onboarding

- Component map: Data Collection Pipeline -> Surrogate Training Pipeline -> Benchmark Interface -> Evaluation Protocol
- Critical path: Define merging scenario → Collect training pairs via actual merges → Train LightGBM surrogates with Optuna tuning → Validate on held-out test → Deploy as benchmark API
- Design tradeoffs: PS collected 133K samples vs. DFS 41K samples; separate surrogates per dataset doubles maintenance but preserves task-specific surfaces; 5-fold CV ensemble reduces variance but increases latency
- Failure signatures: R² < 0.8 or Kendall's Tau < 0.6 indicates poor landscape capture; systematic overestimation suggests training data bias; algorithm ranking inversions indicate ordinal structure loss
- First 3 experiments: 1) Replicate surrogate training and verify metrics match Table 1; 2) Run random search, CMA-ES, TPE on surrogate and compare trajectories to Figure 1; 3) Benchmark Sep-CMA vs. DE and verify speed ranking consistent with Figure 2

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the surrogate models maintain high predictive accuracy when applied to tasks beyond Japanese mathematics or architectures other than Mistral-7B?
  - Basis in paper: SMM-Bench is constructed exclusively using Japanese mathematics datasets and specific Mistral-7B variants
  - Why unresolved: Unclear if surrogate models generalize to other domains or different base models without collecting new, expensive datasets
  - What evidence would resolve it: Constructing and evaluating surrogate models for diverse tasks and different base models to verify consistent R² scores

- **Open Question 2**: How can the systematic performance overestimation in the DFS search space be mitigated to ensure reliable algorithm comparison?
  - Basis in paper: Section 3 states "the performance prediction on the search space for DFS merging tends to overestimate" despite high correlation
  - Why unresolved: The paper identifies the bias but does not investigate causes or propose correction methods
  - What evidence would resolve it: Applying bias correction techniques to DFS surrogate and demonstrating alignment with true benchmark values

- **Open Question 3**: Does the efficiency and accuracy of SMM-Bench scale effectively when merging more than two source models?
  - Basis in paper: Search space design explicitly limits setup to "two source models" for both PS and DFS merging
  - Why unresolved: Increasing source models exponentially increases hyperparameter dimensionality, potentially degrading surrogate quality
  - What evidence would resolve it: Extending data collection and surrogate training to scenarios involving three or more models and measuring predictive fidelity

## Limitations

- Surrogate accuracy may degrade for configurations far outside the convex hull of training data, particularly in DFS extrapolation beyond sampled ranges
- DFS surrogates systematically overestimate performance, potentially leading to unreliable absolute performance predictions
- The benchmark's transferability to tasks beyond Japanese mathematics and architectures other than Mistral-7B remains unproven

## Confidence

- **High confidence**: Surrogate model training methodology (LightGBM + Optuna + CV ensemble) and data collection framework (multiple sampling strategies)
- **Medium confidence**: Behavioral simulation fidelity claims—qualitative trajectory matching is demonstrated but systematic ranking inversions are not extensively validated
- **Medium confidence**: Search space parameterization optimality—theoretically motivated but empirical validation of practical optimality is limited

## Next Checks

1. **Out-of-distribution extrapolation test**: Evaluate surrogate predictions for configurations systematically varied beyond training ranges and compare against actual merged model performance

2. **Algorithm ranking stability analysis**: Run diverse optimization algorithms on both surrogate and true benchmarks, compute Kendall's Tau for ranking preservation, and identify any ranking inversions

3. **Transferability validation**: Train surrogates using only random samples versus full multi-strategy datasets and compare behavioral simulation quality and prediction accuracy