---
ver: rpa2
title: Incentivizing Strong Reasoning from Weak Supervision
arxiv_id: '2505.20072'
source_url: https://arxiv.org/abs/2505.20072
tags:
- reasoning
- student
- teacher
- qwen2
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the reasoning capabilities of large
  language models (LLMs) can be effectively incentivized using supervision from significantly
  weaker models, rather than relying on expensive reinforcement learning or high-quality
  demonstrations. The proposed Weak-to-Strong Reasoning (W2SR) paradigm trains stronger
  student models using long chain-of-thought trajectories generated by much weaker
  teacher models.
---

# Incentivizing Strong Reasoning from Weak Supervision

## Quick Facts
- arXiv ID: 2505.20072
- Source URL: https://arxiv.org/abs/2505.20072
- Reference count: 40
- Primary result: Weak-to-Strong Reasoning (W2SR) achieves up to 94.34% of RL reasoning gains using supervision from much weaker models

## Executive Summary
This paper challenges the conventional wisdom that strong reasoning capabilities in large language models require expensive reinforcement learning or high-quality demonstrations from equally capable teachers. The authors propose Weak-to-Strong Reasoning (W2SR), a paradigm where stronger student models are trained on chain-of-thought trajectories generated by significantly weaker teacher models. Experiments across diverse mathematical and general reasoning benchmarks demonstrate that weak supervision can recover most of the reasoning gains achieved by reinforcement learning, with the strongest student models outperforming both their teachers and RL baselines. The approach offers a scalable, cost-effective alternative for enhancing reasoning in LLMs.

## Method Summary
W2SR uses a two-stage pipeline: first, weak teacher models (including Qwen2.5-0.5B/1.5B/7B/14B/32B variants) generate long chain-of-thought trajectories for a dataset using greedy decoding; second, student models are fine-tuned via supervised learning on these trajectories using Negative Log-Likelihood loss. The method is evaluated against RL baselines trained with GRPO, with performance measured using Pass@1 accuracy and Reasoning Gap Recovered (RGR) metric. Three variants are explored: W2SR (all trajectories), W2SR-P (correct-answer trajectories only), and W2SR-N (incorrect-answer trajectories only).

## Key Results
- W2SR achieves up to 94.34% of RL's reasoning gains while requiring 25x less training time
- Students trained with W2SR consistently outperform both their weak teachers and vanilla students
- Teacher reasoning ability is more important than model size for effective supervision
- Even incorrect reasoning traces provide pedagogically valuable supervision for strong students

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Process Supervision Over Outcome
- Claim: Structurally coherent reasoning traces provide effective supervision even when the final answer is incorrect.
- Mechanism: Strong student models can identify and correct computational errors in weak teacher's reasoning while retaining sound logical structure.
- Evidence: W2SR-N (trained only on incorrect traces) significantly outperforms vanilla students, showing 43.77% vs 30.71% avg Pass@1 for Qwen2.5-Math-7B student.
- Break condition: Student lacks capacity to recognize and correct teacher's reasoning errors.

### Mechanism 2: Elicitation of Latent Reasoning Circuits
- Claim: Weak supervision can unlock dormant reasoning capabilities in stronger student models.
- Mechanism: Weak teacher's CoT traces provide scaffolding that teaches students how to structure and deploy reasoning.
- Evidence: W2SR students acquire knowledge beyond base model's priors, suggesting activation of latent abilities.
- Break condition: Student's pre-existing knowledge base is insufficient for the reasoning task.

### Mechanism 3: Structure Over Scale in Supervision
- Claim: Explicit step-by-step reasoning process in teacher is more critical than teacher's overall model size or accuracy.
- Mechanism: Smaller Reasoner models (fine-tuned for CoT) provide better supervision than larger Non-Reasoner models.
- Evidence: 1.5B Reasoner teacher produces better students than 32B Non-Reasoner teacher, demonstrating structure > scale.
- Break condition: Reasoner teacher outputs are verbose or incoherent rather than truly structured.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) vs. Reinforcement Learning (RL)
  - Why needed: W2SR uses simple SFT as replacement for costly RL, understanding this trade-off is fundamental
  - Quick check: Why is RL more expensive than SFT for reasoning? (Answer: RL requires thousands of GPU-hours, complex reward engineering)

- Concept: Chain-of-Thought (CoT) Prompting and Distillation
  - Why needed: Core unit of supervision is "long CoT trajectory"; understanding good CoT traces is essential
  - Quick check: What makes a "Reasoner" model distinct from "Non-Reasoner"? (Answer: Reasoner is explicitly fine-tuned to generate long, step-by-step reasoning traces)

- Concept: Weak-to-Strong Generalization
  - Why needed: Central paradigm challenging intuition that teacher must be more capable than student
  - Quick check: What does RGR metric measure? (Answer: How much of RL's gain is recovered by weak supervision)

## Architecture Onboarding

- Component map: Data Distillation Engine -> Filtering Pipeline -> SFT Training Loop -> Evaluation Suite
- Critical path: Generation of high-quality CoT traces from weak teacher is most critical step
- Design tradeoffs:
  - Teacher Choice: Smaller, cheaper "Reasoner" model better than larger "Non-Reasoner"
  - Data Filtering: W2SR-P provides best performance but W2SR (all traces) is simpler
  - Cost vs. Performance: W2SR-P provides 25x training speedup over RL with comparable performance
- Failure signatures:
  - Minimal or negative gain suggests teacher is not true "Reasoner" and doesn't generate structured CoT
  - Performance plateau indicates limit to what weak supervision can provide
- First 3 experiments:
  1. Train identical students using small Reasoner vs large Non-Reasoner teacher to validate structure > scale
  2. Train students using all traces, only correct traces, and only incorrect traces to quantify imperfect supervision value
  3. Establish RGR baseline by training strong student with RL, then comparing W2SR performance

## Open Questions the Paper Calls Out

- Can W2SR effectively incentivize reasoning in non-mathematical domains such as commonsense reasoning, scientific QA, or legal analysis? (All experiments focus on mathematical reasoning)
- What adaptive filtering techniques can optimally prioritize pedagogically valuable reasoning trajectories, including incorrect but structurally informative traces? (Current variants use simple correctness-based filtering)
- How can ensembles of multiple weak teachers be effectively aggregated to provide richer supervision signals for student reasoning? (Current experiments use single-teacher supervision)
- Can the W2SR paradigm be extended to multi-modal reasoning or tool-augmented settings? (All experiments involve text-only mathematical reasoning)

## Limitations

- Experiments use teacher models from same family (Qwen2.5) and single RL methodology (GRPO), raising generalizability questions
- Paper doesn't explore whether weak teacher's errors follow systematic patterns that could mislead students
- Claims about incorrect traces being pedagogically valuable depend heavily on student model's pre-existing reasoning capabilities

## Confidence

- **High**: Core finding that structured reasoning traces from weak teachers significantly improve student performance
- **Medium**: Claim that reasoning ability trumps model size in supervision effectiveness
- **Low**: Assertion that even incorrect reasoning traces are pedagogically valuable

## Next Checks

1. Test W2SR across model families by using weak teacher from one architecture (e.g., Llama) to train student from another (e.g., Qwen)
2. Investigate impact of systematic teacher errors by intentionally introducing consistent logical flaws in teacher CoT traces
3. Compare W2SR performance against alternative weak supervision methods like outcome-only distillation or process-only supervision without final answers