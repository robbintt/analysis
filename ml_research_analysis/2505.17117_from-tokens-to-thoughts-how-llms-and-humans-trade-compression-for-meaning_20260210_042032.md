---
ver: rpa2
title: 'From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning'
arxiv_id: '2505.17117'
source_url: https://arxiv.org/abs/2505.17117
tags:
- rosch
- human
- semantic
- llms
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how LLMs and humans balance compression
  and semantic meaning when organizing knowledge into concepts. Using an Information
  Bottleneck framework, the authors compare 40+ LLMs against classic cognitive benchmarks
  (Rosch, 1975; McCloskey & Glucksberg, 1978) to evaluate categorical alignment, internal
  semantic structure, and compression-meaning trade-offs.
---

# From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning

## Quick Facts
- **arXiv ID**: 2505.17117
- **Source URL**: https://arxiv.org/abs/2505.17117
- **Reference count**: 40
- **Primary result**: LLMs achieve mathematically "optimal" compression-meaning balance compared to human categories, sacrificing semantic nuances for statistical efficiency

## Executive Summary
This study investigates how large language models (LLMs) and humans balance compression and semantic meaning when organizing knowledge into concepts. Using an Information Bottleneck framework, the authors compare 40+ LLMs against classic cognitive benchmarks to evaluate categorical alignment, internal semantic structure, and compression-meaning trade-offs. The research reveals that while LLMs broadly align with human categories at coarse levels, they fail to capture fine-grained semantic distinctions like item typicality. Most strikingly, LLMs achieve mathematically "optimal" compression-meaning balance (lower L scores) compared to humans, suggesting they prioritize statistical efficiency over semantic richness. This fundamental difference reveals that artificial intelligence systems excel at compression but sacrifice the semantic nuances essential for human-like understanding.

## Method Summary
The authors employ the Information Bottleneck (IB) framework to analyze how LLMs and humans trade off compression against semantic meaning when forming concepts. They evaluate 40+ LLMs against classic cognitive benchmarks including Rosch's basic-level categories (1975) and McCloskey & Glucksberg's typicality judgments (1978). The study measures categorical alignment using Adjusted Mutual Information (AMI), evaluates internal semantic structure through typicality correlations, and quantifies the compression-meaning trade-off using L scores that combine compression and semantic loss. Training dynamics are analyzed by examining semantic processing patterns across different network layers throughout model development, revealing how concepts evolve from early formation to architectural reorganization.

## Key Results
- LLMs broadly align with human categories at coarse levels (AMI ≈ 0.55) but fail to capture fine-grained semantic distinctions like item typicality (Spearman correlations ρ < 0.2)
- LLMs achieve mathematically "optimal" compression-meaning balance (lower L scores) compared to human categories, suggesting prioritization of statistical efficiency over semantic richness
- Encoder models surprisingly outperform much larger decoder models in human alignment despite smaller scales, indicating architectural advantages beyond model size
- Training dynamics reveal rapid initial concept formation followed by architectural reorganization, with semantic processing migrating from deep to mid-network layers

## Why This Works (Mechanism)
The Information Bottleneck framework provides a mathematical lens for understanding how systems balance compression (reducing information to essential elements) against semantic preservation (maintaining meaningful distinctions). LLMs, trained on vast statistical patterns, naturally optimize for compression efficiency, which leads to mathematically superior L scores. However, this optimization comes at the cost of semantic richness - the nuanced, context-dependent understanding that characterizes human cognition. The framework reveals that human conceptual organization, while less mathematically efficient, preserves the semantic complexity necessary for flexible, context-sensitive reasoning that current LLMs cannot replicate.

## Foundational Learning
- **Information Bottleneck Theory**: Explains how systems balance compression against information preservation; needed to quantify the trade-off between statistical efficiency and semantic meaning
- **Conceptual Structure**: Understanding how humans organize knowledge into categories and subcategories; needed as the benchmark for evaluating LLM performance
- **Semantic Typicality**: Measures how well items fit within categories (e.g., robin as a typical bird vs. penguin); needed to evaluate fine-grained semantic distinctions
- **Categorical Alignment Metrics**: Tools like Adjusted Mutual Information to compare organizational structures; needed to quantify how well LLMs match human categorization
- **Model Architecture Differences**: Understanding how encoder vs. decoder designs affect semantic processing; needed to explain why smaller encoders sometimes outperform larger decoders
- **Training Dynamics Analysis**: Tracking how semantic processing evolves during model development; needed to understand the developmental trajectory of conceptual understanding

## Architecture Onboarding

**Component Map**: Data -> Embedding Layer -> Transformer Blocks (Encoder/Decoder) -> Classification Head -> Output

**Critical Path**: Input tokens → Embedding projection → Multi-head attention + feed-forward layers → Layer normalization → Output representation → Semantic extraction

**Design Tradeoffs**: Encoder models prioritize bidirectional context and semantic preservation, while decoder models optimize for autoregressive generation and compression efficiency. This architectural difference explains why encoders show better human alignment despite smaller scale - they maintain richer semantic representations at the cost of generation efficiency.

**Failure Signatures**: LLMs show mathematically optimal compression but fail to capture human-like semantic nuances, particularly in typicality judgments and contextual flexibility. They optimize for statistical patterns rather than conceptual understanding, leading to efficient but semantically impoverished representations.

**3 First Experiments**:
1. Measure AMI scores between LLM embeddings and human categories across multiple benchmark datasets
2. Calculate L scores to quantify compression-meaning trade-offs for different model architectures
3. Track semantic processing patterns across layers during training to identify developmental trajectories

## Open Questions the Paper Calls Out
The study highlights fundamental questions about the nature of conceptual understanding: Can we develop models that preserve semantic richness while maintaining computational efficiency? What architectural modifications would enable LLMs to capture the contextual, cultural, and experiential factors that shape human conceptual organization? How can we quantify and incorporate the "inefficiencies" that support cognitive flexibility and analogical reasoning?

## Limitations
- Static embeddings and frozen checkpoints may not capture dynamic semantic shifts during active reasoning or conversation
- The Information Bottleneck framework may oversimplify the complex, multi-layered nature of human conceptual organization
- Focus on categorical structure potentially overlooks other crucial aspects like analogical reasoning, metaphor comprehension, and embodied experience

## Confidence
- **High confidence**: LLMs demonstrate superior mathematical compression efficiency compared to human categories (L scores analysis)
- **Medium confidence**: LLMs broadly align with human categories at coarse levels but miss fine-grained semantic distinctions (AMI and typicality results)
- **Medium confidence**: Encoder models show better human alignment than decoder models despite smaller size (architectural differences finding)
- **Lower confidence**: Training dynamics showing semantic processing migration from deep to mid-layers (based on specific architectural analysis)

## Next Checks
1. Conduct human behavioral experiments testing whether humans show varying degrees of semantic compression across different cognitive tasks and contexts, to determine if LLM-human divergence represents fundamental difference or task-specific optimization
2. Test additional cognitive benchmarks beyond Rosch's basic-level categories and McCloskey & Glucksberg's typicality judgments, including conceptual combination, metaphor understanding, and cross-modal concept formation
3. Implement dynamic evaluation protocols where models process concepts in conversational or reasoning contexts rather than through static embeddings, to assess whether contextual processing affects compression-meaning trade-off