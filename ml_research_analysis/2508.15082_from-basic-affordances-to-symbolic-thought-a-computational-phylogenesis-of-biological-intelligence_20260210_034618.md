---
ver: rpa2
title: 'From Basic Affordances to Symbolic Thought: A Computational Phylogenesis of
  Biological Intelligence'
arxiv_id: '2508.15082'
source_url: https://arxiv.org/abs/2508.15082
tags:
- relations
- mapping
- object
- memory
- multi-place
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the fundamental question of what distinguishes\
  \ human symbolic thought from the cognitive abilities of other animals. The authors\
  \ propose that two kinds of hierarchical integration\u2014multi-place predicates\
  \ and structure mapping\u2014are minimal requirements beyond dynamic binding to\
  \ achieve symbolic cognition."
---

# From Basic Affordances to Symbolic Thought: A Computational Phylogenesis of Biological Intelligence

## Quick Facts
- arXiv ID: 2508.15082
- Source URL: https://arxiv.org/abs/2508.15082
- Authors: John E. Hummel; Rachel F. Heaton
- Reference count: 0
- Primary result: Both multi-place predicates and structure mapping are necessary (and jointly minimal) for basic symbolic thought

## Executive Summary
This paper addresses what distinguishes human symbolic thought from the cognitive abilities of other animals by proposing that two hierarchical integration mechanisms—multi-place predicates and structure mapping—are minimal requirements beyond dynamic binding for achieving symbolic cognition. Through 17 simulations of cognitive architectures with varying capabilities, the authors systematically test performance on tasks requiring dynamic binding, multi-place predicates, structure mapping, or combinations thereof. The results demonstrate that architectures possessing both capabilities successfully solve symbolic tasks while those lacking either fail, supporting the hypothesis that both capacities are necessary for basic symbolic thought.

## Method Summary
The study uses the LISA neurocomputational architecture to test four cognitive architectures created by crossing two design choices: (1) multi-place predicates vs single-place predicates only, and (2) structure mapping with learning rate µ=0.9 vs no mapping (µ=0). The architectures are tested on four tasks: Dynamic Binding Only (DBO), Relational Only (RO), Mapping Only (MO), and Relational + Mapping (R&M). Each task presents a Perception analog with propositions and a Memory analog that must infer correct relational inferences. Success is measured by whether the "Affordance" semantic unit fires in synchrony with the "Critical" object unit, out of synchrony with non-affordance and noncritical units.

## Key Results
- Architectures with both multi-place predicates and structure mapping (R&M) successfully solved all symbolic tasks
- Architectures lacking either capability (DBO, RO, MO) failed on tasks requiring the missing component
- Even with "double time" processing, MO architecture failed RO task, demonstrating hierarchical structure is essential
- Structure mapping learning (µ=0.9) successfully overcame semantic biases in MO and R&M tasks

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Binding via Neural Synchrony
Dynamic binding is necessary but not sufficient for symbolic thought. Neurons representing bound elements fire in synchrony (40-80 Hz gamma) while out of synchrony with unbound elements, enabling on-the-fly composition and destruction of role-filler bindings without affecting the representations themselves. The format of bindings matters for systematicity; roles and arguments must remain identifiable across different binding contexts. Evidence includes synchrony observations in monkey, cat, and insect brains that nonetheless lack symbolic thought. Break condition: if static conjunctive coding replaces dynamic binding, the architecture loses systematicity.

### Mechanism 2: Multi-place Predicates (Hierarchical Integration)
Integrating multiple role-bindings into multi-place predicates is necessary for representing allocentric relations. Multiple role-filler bindings (e.g., "larger+X", "smaller+Y") are composed under shared proposition units that fire at slower theta/beta frequencies (~10 Hz), creating unified relational structures. Single-place predicates (with fixed arguments like "hand") cannot capture relations where all arguments vary freely. Evidence: RO and R&M architectures succeeded on relational tasks; MO and DBO failed even with "double time" processing. Break condition: if architecture only forms single-place predicates, it cannot represent whether object X fits inside object Y when neither argument is fixed.

### Mechanism 3: Structure Mapping with Incremental Learning
Computing systematic one-to-one correspondences between structures is necessary for using symbol systems. Driver structure activates propositions one at a time in working memory; Hebbian learning tracks which recipient elements activate together, with 1:1 constraints accumulating across processing cycles. Mapping must be incremental (capacity-limited) rather than all-at-once to match human working memory constraints. Evidence: MO and R&M succeeded on mapping-only task; learning visible between iterations 270-380 in time traces. Break condition: if mapping learning rate (µ) is zero, architecture cannot overcome semantic biases to find correct correspondences.

## Foundational Learning

- **Concept: Dynamic Binding vs. Static Coding**
  - Why needed here: Core distinction between symbol systems and associative networks; without this, you cannot understand why LISA uses synchrony
  - Quick check question: Can you explain why representing "red-circle" via a single "red-circle" neuron differs from synchronizing separate "red" and "circle" neurons?

- **Concept: Role-Filler vs. Argument-Predicate Relations**
  - Why needed here: Multi-place predicates require understanding that "larger-than(X,Y)" is not equivalent to "large(X) + small(Y)"
  - Quick check question: Why can "hand-distance(glass)" be reduced to single-place while "fits-inside(box, drawer)" cannot?

- **Concept: Working Memory Capacity Limits in Reasoning**
  - Why needed here: Explains why LISA's incremental mapping algorithm matters; structures cannot be compared all-at-once
  - Quick check question: If you could map entire structures simultaneously, what constraint would that violate in human cognition?

## Architecture Onboarding

- **Component map:**
  - Semantic units (distributed) -> Bottom layer; shared type representations
  - Object/Predicate units (localist) -> Token representations per analog
  - Role-binding units (SPs) -> Conjunctive storage; oscillate via inhibitory exchange
  - Proposition units -> Integrate role-bindings hierarchically; slower temporal integration
  - Mapping connections -> Learned Hebbian links between driver and recipient

- **Critical path:**
  1. Perception analog (driver) activates proposition → role-binding wins inhibitory competition → fires with bound semantics
  2. Semantic overlap activates recipient propositions in Memory
  3. Mapping connections learn (if µ > 0) over multiple cycles
  4. Learned mappings bias subsequent activations toward correct correspondences
  5. Affordance semantic unit fires in synchrony with Critical object = successful inference

- **Design tradeoffs:**
  - Hand-coded vs. learned semantics: Paper uses hand-coding for experimental control; Wilner & Hummel (2017) offers automatic generation but less interpretable
  - CWSG enabled vs. disabled: Disabled here for fair comparison across architectures; enabling allows analogical inference
  - "Cat" vs. "Balint's" versions: Both fail RO task; relations in memory without perception cannot compensate

- **Failure signatures:**
  - DBO/RO on MO task: Affordance fires with Noncritical object due to semantic distraction (no mapping to overcome bias)
  - MO/DBO on RO task: Fails to bind Affordance to Critical object despite "double time" (no hierarchical structure)
  - All non-R&M on R&M task: Hybrid failures; neither component compensates for missing other
  - Initial iterations in MO: Wrong inference visible before mapping learning completes (~270 iterations)

- **First 3 experiments:**
  1. Run all four architectures on DBO task (basic affordance) to verify baseline—expect 100% success; confirms dynamic binding works
  2. Run RO task with "Balint's" variant to test prediction: having relations in memory but not perception yields same failure as "Cat" variant
  3. Extend R&M task with CWSG enabled to observe whether analogical inference emerges from same mapping mechanism; compare to Hummel & Holyoak (2003) results

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation-based evidence provides proof-of-concept but doesn't establish applicability to actual neural implementations in biological systems
- Controlled feature set eliminates semantic cues, but real-world concepts may have richer feature structures affecting binding and mapping performance
- Hand-coded semantic features rather than learned representations limit ecological validity

## Confidence
- **High confidence:** Dynamic binding is necessary but insufficient for symbolic thought (supported by computational results and neural synchrony evidence)
- **Medium confidence:** Both multi-place predicates AND structure mapping are jointly necessary for basic symbolic thought (supported by simulation results but not yet validated in biological systems)
- **Low confidence:** These architectural requirements explain why symbolic thought evolved only once (speculative extension beyond current evidence)

## Next Checks
1. Replicate the simulation results using learned semantic representations rather than hand-coded features to test robustness of the binding and mapping requirements
2. Implement a comparative simulation testing whether architectures with partial capabilities (e.g., learned mapping without hierarchical predicates) show intermediate performance on symbolic tasks
3. Conduct behavioral experiments with human participants on modified versions of these tasks to verify that working memory capacity limits and semantic biases match the computational predictions