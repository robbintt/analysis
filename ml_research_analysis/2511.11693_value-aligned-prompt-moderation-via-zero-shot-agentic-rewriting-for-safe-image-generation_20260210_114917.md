---
ver: rpa2
title: Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image
  Generation
arxiv_id: '2511.11693'
source_url: https://arxiv.org/abs/2511.11693
tags:
- safe
- image
- prompts
- safety
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VALOR, a zero-shot LLM-guided framework that
  addresses safety risks in text-to-image generation by combining multi-granular detection
  (word, semantic, value levels), intention disambiguation, and dynamic prompt rewriting.
  Experiments show that VALOR achieves up to 100% reduction in unsafe content generation
  while maintaining high output quality, with safety rates exceeding 92.7% on harmful
  datasets and superior performance over baselines in terms of FID (15.89), CLIP (27.69),
  and LPIPS (0.8289) metrics.
---

# Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation

## Quick Facts
- arXiv ID: 2511.11693
- Source URL: https://arxiv.org/abs/2511.11693
- Reference count: 26
- The paper introduces VALOR, a zero-shot LLM-guided framework that addresses safety risks in text-to-image generation by combining multi-granular detection (word, semantic, value levels), intention disambiguation, and dynamic prompt rewriting. Experiments show that VALOR achieves up to 100% reduction in unsafe content generation while maintaining high output quality, with safety rates exceeding 92.7% on harmful datasets and superior performance over baselines in terms of FID (15.89), CLIP (27.69), and LPIPS (0.8289) metrics.

## Executive Summary
This paper presents VALOR, a novel framework for ensuring safe text-to-image (T2I) generation by addressing the critical challenge of prompt moderation. VALOR leverages a Large Language Model (LLM) to perform intention disambiguation and dynamic prompt rewriting, thereby mitigating the generation of unsafe or harmful content. The framework employs a multi-granular detection approach, analyzing prompts at word, semantic, and value levels to identify potential risks. The approach aims to balance safety with output quality, addressing the limitations of existing defenses that often compromise creativity or incur high computational costs.

## Method Summary
The VALOR framework employs a zero-shot, agentic LLM-guided approach to prompt moderation. It integrates multi-granular detection modules to analyze prompts at word, semantic, and value levels. Intention disambiguation is performed to distinguish between harmful and benign contexts (e.g., medical or educational). If a prompt is flagged as unsafe, the LLM dynamically rewrites it to align with safety guidelines while preserving the user's intent. The framework is designed to be adaptable and does not rely on task-specific fine-tuning, making it applicable across various T2I models.

## Key Results
- VALOR achieves up to 100% reduction in unsafe content generation.
- Safety rates exceed 92.7% on harmful datasets.
- Superior performance over baselines in terms of FID (15.89), CLIP (27.69), and LPIPS (0.8289) metrics.

## Why This Works (Mechanism)
The paper argues that existing safety measures in T2I systems are insufficient due to the nuanced nature of unsafe content and the limitations of static filtering. VALOR addresses these issues by leveraging the reasoning capabilities of LLMs to perform dynamic, context-aware prompt rewriting. The multi-granular detection allows for a more comprehensive analysis of prompts, while intention disambiguation ensures that benign uses of potentially risky terms are not blocked. This approach balances safety with output quality and user intent preservation.

## Foundational Learning
- **Multi-Granular Detection**: Analyzing prompts at word, semantic, and value levels allows for a more nuanced identification of unsafe content. Why needed: Single-level detection is insufficient for capturing the complexity of harmful prompts. Quick check: Evaluate the contribution of each detection level to overall safety rates.
- **Intention Disambiguation**: Distinguishing between harmful and benign contexts prevents over-blocking of legitimate prompts. Why needed: Many prompts contain terms that can be used in both harmful and safe contexts. Quick check: Measure the precision of intention disambiguation in edge cases.
- **Dynamic Prompt Rewriting**: Rewriting unsafe prompts to align with safety guidelines while preserving user intent. Why needed: Static filtering can stifle creativity and block legitimate uses of potentially risky terms. Quick check: Assess the quality of rewritten prompts compared to the original.
- **Zero-Shot Learning**: Adapting to various T2I models without task-specific fine-tuning. Why needed: Existing safety measures are often model-specific and require retraining. Quick check: Evaluate VALOR's performance across different T2I models.
- **LLM Reasoning**: Leveraging LLM's reasoning capabilities for context-aware decision making. Why needed: Understanding the intent behind a prompt is crucial for accurate safety assessment. Quick check: Compare the performance of different LLM backbones.

## Architecture Onboarding
- **Component Map**: Input Prompt -> Word-Level Detection -> Semantic-Level Detection -> Value-Level Detection -> Intention Disambiguation -> LLM Rewriting -> Safety Verification -> Output
- **Critical Path**: Input Prompt -> Intention Disambiguation -> LLM Rewriting -> Safety Verification
- **Design Tradeoffs**: Balancing safety with output quality and user intent preservation. Serial LLM inference adds latency but enables context-aware rewriting.
- **Failure Signatures**: High false negative rates with ambiguous intents, reliance on predefined taxonomies for value-level detection, and computational overhead from LLM rewriting.
- **First Experiments**: 1) Evaluate VALOR's performance on a dataset of known harmful prompts. 2) Compare VALOR's safety rates against existing baselines. 3) Assess the impact of different LLM backbones on VALOR's performance.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the specific reasoning capability and parameter size of the backbone Large Language Model (LLM) affect the stability of VALOR's False Negative Rate (FNR) and its ability to handle ambiguous intents?
- Basis in paper: [inferred] Table 1 reveals substantial variance in performance across different LLMs; specifically, Qwen1.5-1.8B-Chat exhibits a significantly higher FNR (up to 33.3%) compared to Deepseek or Llama (often <5%), suggesting the framework's reliability is highly sensitive to the choice of the rewriter model.
- Why unresolved: The paper establishes that LLMs are effective but does not determine the minimal model capacity or the specific alignment training properties required to consistently resolve the "Intention Ambiguity" challenges described in the Introduction.
- What evidence would resolve it: A comparative analysis plotting model parameter counts or specific benchmark scores (e.g., reasoning benchmarks) against VALORâ€™s FNR to identify the critical threshold for reliable semantic disambiguation.

### Open Question 2
- Question: To what extent does the Value-Level Detection module, which relies on cosine similarity to predefined lists of sensitive locations and acts, generalize to novel or culturally specific norm violations?
- Basis in paper: [inferred] The Method section defines the Value-Level Detection (Eq. 6) using specific sets of locations $L$ (Table 11) and acts $A$ (Table 12), implying a dependence on the coverage of these static taxonomies to detect "violations of social norms."
- Why unresolved: While the framework detects known violations effectively (high SAFE scores), the reliance on pre-defined lists and cosine similarity thresholds ($\tau_v$) may struggle with implicit context violations or cultural nuances that lack explicit lexical overlap with the database.
- What evidence would resolve it: Performance evaluation on a "wild" dataset of emerging social controversies or cross-cultural scenarios that contain value mismatches not explicitly cataloged in the current location and act taxonomies.

### Open Question 3
- Question: Can the computational overhead introduced by the serial LLM-rewriting and verification steps be optimized to support real-time, high-throughput deployment without negating the safety benefits?
- Basis in paper: [inferred] Figure 8 shows that adding the LLM component increases the average time cost per prompt by 0.5 to 2.5 seconds, which the authors deem "acceptable," though this creates a significant latency disparity compared to standard T2I generation.
- Why unresolved: The paper critiques existing defenses for "incurring high costs," yet the zero-shot agentic approach inherently adds serial inference latency, raising questions about its scalability in commercial applications requiring instant responsiveness.
- What evidence would resolve it: System throughput benchmarks (requests per second) under load, or the demonstration of optimization techniques (e.g., speculative decoding, distillation) that reduce the LLM overhead while maintaining the safety alignment.

## Limitations
- Safety effectiveness metrics (92.7%+) rely on internal benchmarks without independent verification.
- Quality preservation claims (FID/CLIP/LPIPS) lack comparative baselines for context.
- Intention disambiguation accuracy is supported by qualitative claims without quantitative validation.

## Confidence
- Safety effectiveness (92.7%+): Medium - based on internal benchmarks without independent verification
- Quality preservation (FID/CLIP/LPIPS): Low - no comparative baselines provided
- Intention disambiguation accuracy: Low - qualitative claims without quantitative validation

## Next Checks
1. Conduct cross-cultural safety testing with diverse annotator pools to assess robustness against cultural variations in unsafe content perception
2. Perform ablation studies isolating each detection level (word, semantic, value) to quantify individual contribution to overall safety rates
3. Implement red-teaming exercises using established prompt injection techniques to test framework resilience against adversarial attacks