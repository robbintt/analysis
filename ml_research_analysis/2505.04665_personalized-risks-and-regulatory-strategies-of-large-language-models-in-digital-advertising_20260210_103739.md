---
ver: rpa2
title: Personalized Risks and Regulatory Strategies of Large Language Models in Digital
  Advertising
arxiv_id: '2505.04665'
source_url: https://arxiv.org/abs/2505.04665
tags:
- privacy
- data
- user
- advertising
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies personalized risks and regulatory strategies
  for large language models in digital advertising. The authors propose a BERT-based
  personalized advertising recommendation method combined with local model training
  and encryption technology to protect user privacy.
---

# Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising

## Quick Facts
- arXiv ID: 2505.04665
- Source URL: https://arxiv.org/abs/2505.04665
- Authors: Haoyang Feng; Yanjun Dai; Yuan Gao
- Reference count: 19
- One-line primary result: BERT-based personalized advertising with local training achieves 15.50% CTR, 4.20% CR, and 0% privacy breach probability while maintaining 9.5/10 user satisfaction

## Executive Summary
This paper proposes a BERT-based personalized advertising recommendation system that combines semantic embedding of ad content with local model training and encryption to protect user privacy. The approach uses bidirectional attention to capture richer contextual semantics in ad copy than traditional methods, achieving superior click-through and conversion rates. By training models locally and only uploading derived parameters (user tags), the system eliminates raw data exposure to servers while maintaining high recommendation quality.

## Method Summary
The method involves three core components: (1) BERT-based semantic embedding of ad content using bidirectional attention to generate context-aware ad representations, (2) local model training on-device to generate user portraits without exposing raw behavioral data, and (3) encrypted transmission of only derived user tags to ad servers for matching. The system aggregates user behavior, interest, temporal, and device signals into a unified portrait, then matches BERT-embedded ads against this profile. Local training ensures privacy by keeping all raw user data on-device while still enabling effective personalization through parameter-only uploads.

## Key Results
- BERT model achieves 15.50% click-through rate and 4.20% conversion rate, outperforming other methods
- Local model training with privacy protection results in zero privacy breach incidents and 0% privacy breach probability
- User satisfaction scores reach 9.5/10 while maintaining privacy protection
- The approach effectively balances advertising effectiveness with privacy preservation

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Semantic Embedding for Ad-User Matching
- Claim: BERT's bidirectional attention captures richer contextual semantics in ad copy than unidirectional or keyword-based methods, improving click-through and conversion rates.
- Mechanism: The self-attention mechanism computes Query, Key, and Value vectors for each token, then generates attention weights via softmax over dot-product similarities. This produces context-aware token representations that aggregate into a fixed-dimensional ad embedding via the [CLS] token. These embeddings enable semantic similarity matching between ads and user interest profiles.
- Core assumption: Bidirectional context modeling captures ad semantics more effectively than collaborative filtering or content-based keyword matching.
- Evidence anchors:
  - [abstract] "the BERT model achieves a click-through rate of 15.50% and conversion rate of 4.20%, outperforming other methods"
  - [section 3.2.2] Describes token embedding, position encoding, multi-layer Transformer encoder, and [CLS] pooling for ad semantic vectors
  - [corpus] SeqUDA-Rec paper (FMR=0.53) supports sequential user behavior modeling for personalized content marketing
- Break condition: If ad copy is extremely short or semantically ambiguous, attention weights become noisy and embedding quality degrades.

### Mechanism 2: Local Training with Parameter-Only Upload for Privacy Preservation
- Claim: Training models locally and uploading only derived parameters (user tags) eliminates raw data exposure to servers.
- Mechanism: User behavioral data stays on-device. Local model training generates user tags/portraits locally. Only aggregated tags are transmitted to the ad server for matching—raw logs, browsing history, and personal identifiers never leave the device.
- Core assumption: Adversaries cannot reconstruct sensitive user data from uploaded model parameters or tags alone.
- Evidence anchors:
  - [abstract] "Local model training with privacy protection measures results in zero privacy breach incidents and a 0% privacy breach probability"
  - [section 3.3] "Through local training, user data does not need to be uploaded to the server, which can effectively avoid the risk of data leakage"
  - [corpus] Privacy Preserving Conversion Modeling paper (FMR=0.51) addresses similar clean-room approaches but doesn't validate local-only training
- Break condition: If parameters/tags leak information via membership inference or reconstruction attacks, privacy guarantees fail.

### Mechanism 3: Multi-Signal User Portrait Construction
- Claim: Combining behavioral, interest, temporal, and device signals improves recommendation relevance over single-signal approaches.
- Mechanism: Aggregates user behavior data (browsing, search, purchase), interest profiles, social relationships, time-of-day patterns, and device type into a unified user portrait. The BERT embedding of ad content is matched against this portrait for ranking.
- Core assumption: Historical multi-dimensional user signals predict future ad engagement.
- Evidence anchors:
  - [section 3.2.1] Lists "user behavior data, interest data, and social relationship data" plus "advertising content, structure, language style"
  - [section 4.4, Table 1] Shows how User ID + interests + time + device correlate with clicks/conversions (e.g., U001 "Tech, Gadgets" → clicks and converts on "Smartphone Promotion")
  - [corpus] Guarding Digital Privacy paper discusses user profiling tradeoffs but doesn't provide comparative performance data
- Break condition: If user interests shift rapidly or data is sparse, portraits become stale and mismatched.

## Foundational Learning

- Concept: **Transformer Self-Attention (Q/K/V mechanism)**
  - Why needed here: Core to understanding how BERT generates context-aware ad embeddings.
  - Quick check question: Given a 10-token ad copy, how many attention weights does a single head compute for one token?

- Concept: **Embedding Pooling Strategies ([CLS] vs. mean pooling)**
  - Why needed here: Determines how variable-length ad text becomes a fixed-size vector for classification.
  - Quick check question: Why might [CLS] token pooling preserve sequence-level semantics better than mean pooling?

- Concept: **Local vs. Cloud Training Tradeoffs**
  - Why needed here: Central to the privacy argument; affects deployment architecture decisions.
  - Quick check question: What specific data elements would be transmitted in cloud training that are NOT transmitted in local training?

## Architecture Onboarding

- Component map:
  - Data Ingestion Layer: Collects user behavior logs, ad content, social data
  - Preprocessing Pipeline: Cleaning, normalization, feature selection, train/test split
  - BERT Encoder: Token embedding → Position encoding → Multi-layer Transformer → [CLS] pooling
  - Classification Head: Sigmoid output for click probability prediction
  - Local Training Module: On-device model updates, no raw data export
  - Encryption Layer: Data encryption for any transmitted parameters
  - Tag Generation & Upload: Derives user tags locally, uploads only tags to ad server
  - Ad Matching Engine: Server-side matching of tags to ad inventory

- Critical path:
  1. Collect and preprocess user + ad data locally
  2. Generate BERT embeddings for ad content
  3. Train classification model on-device using local user engagement data
  4. Extract user tags from local model
  5. Upload tags (encrypted) to ad server
  6. Server matches tags to candidate ads and returns ranked list

- Design tradeoffs:
  - **Local vs. cloud training**: Privacy (local wins) vs. computational scale (cloud wins)
  - **BERT-base vs. BERT-large**: Embedding quality vs. inference latency on device
  - **Tag granularity**: Fine-grained tags improve matching but increase privacy surface area

- Failure signatures:
  - Low CTR despite good embeddings → Feature mismatch or stale user portraits
  - Privacy breach in local setup → Parameter leakage or insecure tag transmission
  - High inference latency → Model too large for target device; consider distillation
  - Poor conversion on high clicks → Ad creative mismatch, not targeting problem

- First 3 experiments:
  1. **Baseline comparison**: Replicate CTR/CR comparison (BERT vs. RF vs. Content-based vs. Random) on your own dataset to validate performance claims
  2. **Privacy stress test**: Simulate adversarial attacks (membership inference, parameter reconstruction) on uploaded tags to verify 0% breach claim holds under attack
  3. **Ablation on user portrait features**: Remove one signal category at a time (behavior, interest, time, device) to measure contribution to CTR/CR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed local training framework maintain effectiveness when applied to diverse application domains beyond the specific advertising scenarios tested?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that due to data limitations, "their effectiveness in other application fields needs to be further verified."
- Why unresolved: The current experimental results are derived from specific public and simulated ad datasets, limiting the generalizability of the findings to other industries.
- What evidence would resolve it: Validation of the method on cross-industry datasets (e.g., e-commerce, healthcare) to verify if high click-through rates and privacy standards are maintained.

### Open Question 2
- Question: How does the method scale regarding computational efficiency and model performance when expanding to significantly larger datasets?
- Basis in paper: [explicit] The conclusion suggests, "Future research can further expand the data set to verify the wide applicability of the proposed method."
- Why unresolved: The study relies on limited experimental data, leaving the robustness of the proposed BERT-based approach on massive, industrial-scale datasets unproven.
- What evidence would resolve it: Performance benchmarks on terabyte-scale industrial data measuring latency, training time, and accuracy retention.

### Open Question 3
- Question: Is local training of a computationally intensive model like BERT feasible on standard consumer devices without causing significant performance degradation?
- Basis in paper: [inferred] The paper proposes "local model training" to ensure privacy but does not analyze hardware constraints or computational overhead for the end-user device.
- Why unresolved: Running large language models locally is resource-heavy; practical implementation on mobile or low-power clients may be limited by battery and processing power.
- What evidence would resolve it: Benchmarks of CPU usage, memory consumption, and inference time on standard consumer mobile hardware during the local training process.

## Limitations
- Privacy protection claims rely on experimental observations rather than cryptographic proofs, with no validation against sophisticated reconstruction attacks
- Model performance claims are based on unspecified "public ad recommendation datasets" without detailed data characteristics or cross-domain validation
- The encryption method is mentioned but not specified, leaving potential implementation vulnerabilities unaddressed
- Local training feasibility on consumer devices with resource constraints is not analyzed

## Confidence
- **BERT semantic embedding performance claims (CTR 15.50%, CR 4.20%)**: Medium confidence - Performance metrics are reported but dataset details and baseline implementations are unspecified
- **Privacy protection guarantees (0% breach probability)**: Low confidence - Claims are experimental observations without formal security proofs or attack resistance validation
- **User satisfaction scores (9.5/10)**: Low confidence - Satisfaction measurement methodology and sample size are not disclosed
- **Local training privacy mechanism**: Medium confidence - The architectural approach is sound but security guarantees are unproven

## Next Checks
1. **Adversarial privacy stress test**: Implement membership inference and parameter reconstruction attacks on the uploaded user tags to verify that privacy guarantees hold under realistic threat models

2. **Cross-dataset reproducibility**: Replicate the CTR/CR performance on at least two different public advertising datasets (e.g., Criteo, Avazu) with documented preprocessing and hyperparameter settings

3. **Real-world deployment simulation**: Conduct a pilot deployment with actual user feedback collection to validate the 9.5/10 user satisfaction score and identify any performance degradation in production conditions