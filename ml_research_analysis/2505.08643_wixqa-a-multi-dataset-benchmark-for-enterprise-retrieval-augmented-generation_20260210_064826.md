---
ver: rpa2
title: 'WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation'
arxiv_id: '2505.08643'
source_url: https://arxiv.org/abs/2505.08643
tags:
- answer
- datasets
- answers
- information
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'WixQA is a multi-dataset benchmark suite for evaluating Retrieval-Augmented
  Generation (RAG) systems in enterprise settings. It includes three KB-grounded QA
  datasets derived from Wix.com customer support: WixQA-ExpertWritten (200 real user
  queries with expert-authored, multi-step answers), WixQA-Simulated (200 expert-validated
  QA pairs distilled from user dialogues), and WixQA-Synthetic (6,221 LLM-generated
  QA pairs systematically derived from Wix articles).'
---

# WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.08643
- Source URL: https://arxiv.org/abs/2505.08643
- Authors: Dvir Cohen; Lin Burg; Sviatoslav Pykhnivskyi; Hagit Gur; Stanislav Kovynov; Olga Atzmon; Gilad Barkan
- Reference count: 40
- Primary result: WixQA is a multi-dataset benchmark suite for evaluating Retrieval-Augmented Generation (RAG) systems in enterprise settings with three KB-grounded QA datasets derived from Wix.com customer support.

## Executive Summary
WixQA is a multi-dataset benchmark suite for evaluating Retrieval-Augmented Generation (RAG) systems in enterprise settings. It includes three KB-grounded QA datasets derived from Wix.com customer support: WixQA-ExpertWritten (200 real user queries with expert-authored, multi-step answers), WixQA-Simulated (200 expert-validated QA pairs distilled from user dialogues), and WixQA-Synthetic (6,221 LLM-generated QA pairs systematically derived from Wix articles). The benchmark addresses the need for domain-specific, multi-document QA evaluation in enterprise environments, where users often require procedural guidance and complex information synthesis. WixQA uniquely features multi-article dependencies in ExpertWritten and Simulated datasets, requiring retrieval and synthesis from multiple sources. A comprehensive baseline evaluation using BM25 and E5 dense retrieval with multiple generation models (Claude 3.7, Gemini 2.0 Flash, GPT-4o, GPT-4o Mini) shows that dense retrieval consistently improves context recall for complex queries, though overall performance indicates significant room for advancement in enterprise RAG systems. The datasets and associated 6,221-article Wix knowledge base are publicly available under MIT license.

## Method Summary
WixQA evaluates RAG systems using three datasets: ExpertWritten (200 real user queries with expert-authored answers), Simulated (200 expert-validated QA pairs from user dialogues), and Synthetic (6,221 LLM-generated pairs from Wix articles). The 6,221-article Wix knowledge base is indexed using BM25 or E5-large-v2 dense embeddings. The FlashRAG pipeline retrieves top-5 documents per query, then generates answers using LLMs (Claude 3.7, Gemini 2.0 Flash, GPT-4o, GPT-4o Mini). Evaluation uses F1, BLEU, ROUGE metrics plus LLM-judge metrics (Context Recall, Factuality) with GPT-4o as judge.

## Key Results
- Dense semantic retrieval (E5) improves context recall by 8-12 percentage points over BM25 for complex, multi-article enterprise queries
- 27% of ExpertWritten answers and 14% of Simulated answers require synthesis from multiple KB articles, creating unique retrieval challenges
- Performance varies significantly across datasets: Synthetic (F1 0.42-0.59) > ExpertWritten (F1 0.36-0.43) > Simulated (F1 0.21-0.30), reflecting different query complexities and information densities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense semantic retrieval improves context recall for complex, multi-article enterprise queries compared to keyword-based retrieval.
- Mechanism: E5 dense embeddings capture semantic relationships between user queries and procedural content that lexical overlap (BM25) misses, particularly when query vocabulary differs from KB article terminology. This is critical when answers require synthesizing information distributed across multiple documents.
- Core assumption: Semantic similarity in embedding space correlates with information relevance for procedural QA tasks.
- Evidence anchors:
  - [section 4.4]: "The E5 dense retriever consistently outperforms BM25 on Context Recall, particularly for the ExpertWritten and Simulated datasets requiring multi-article synthesis."
  - [tables 3-5]: E5 achieves 0.81 Context Recall vs. BM25's 0.73 on ExpertWritten; 0.67 vs. 0.55 on Simulated.
  - [corpus]: Related work on Domain-Specific Data Generation Framework for RAG Adaptation (FMR=0.60) supports domain-specific retrieval improvements.
- Break condition: If queries contain highly specific technical terminology not well-represented in general embedding models, dense retrieval may underperform.

### Mechanism 2
- Claim: Multi-article dependencies in enterprise QA create retrieval and synthesis challenges that single-document benchmarks don't capture.
- Mechanism: When answers require information from 2+ KB articles (27% of ExpertWritten, 14% of Simulated), the RAG system must: (1) retrieve multiple relevant documents within the top-k cutoff, and (2) synthesize information across retrieved passages without losing procedural coherence. Standard retrieval metrics that optimize for single-document relevance may miss this requirement.
- Core assumption: Users require complete procedural guidance that cannot be satisfied by partial information from a single source.
- Evidence anchors:
  - [section 3.2]: "27% [of ExpertWritten answers] involve more than one article... reflecting realistic enterprise support scenarios where information is often distributed across documents."
  - [table 2]: Multi-article percentages explicitly tracked across datasets.
  - [corpus]: Weak corpus evidence for multi-article synthesis specifically; related benchmarks like MultiDoc2Dial address conversational but not procedural multi-doc synthesis.
- Break condition: If retrieval top-k is too small or synthesis capability is weak, multi-article queries will fail even with good individual document retrieval.

### Mechanism 3
- Claim: Dataset construction methodology (expert-written vs. simulated vs. synthetic) creates different difficulty levels that stress different RAG components.
- Mechanism: ExpertWritten contains authentic user queries with complex information needs (median 19 query tokens, 172 answer tokens), requiring both retrieval accuracy and generation quality. Simulated queries are distilled to be concise (12 query tokens, 50 answer tokens) but may have mismatched information density. Synthetic queries are generated directly from single articles, making retrieval nearly deterministic (Context Recall 0.95-0.97) but testing generation fidelity.
- Core assumption: Query-answer complexity distributions affect which RAG pipeline component becomes the bottleneck.
- Evidence anchors:
  - [section 4.4]: "Performance differs markedly across datasets, with Synthetic yielding the highest scores, followed by ExpertWritten, and then Simulated proving the most challenging."
  - [table 5 vs tables 3-4]: Synthetic shows F1 0.42-0.59 vs. ExpertWritten 0.36-0.43 vs. Simulated 0.21-0.30.
  - [corpus]: Domain-Specific Data Generation Framework paper (FMR=0.60) discusses synthetic data quality tradeoffs.
- Break condition: Synthetic data may not generalize to real user query distributions; expert validation doesn't guarantee retrieval-friendly phrasing.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) pipeline architecture
  - Why needed here: WixQA evaluates end-to-end RAG systems; understanding the two-stage retrieve-then-generate flow is essential for interpreting benchmark results.
  - Quick check question: Can you explain why Context Recall (retrieval quality) and Factuality (generation quality) are evaluated separately?

- Concept: Sparse vs. Dense Retrieval (BM25 vs. E5)
  - Why needed here: The baseline comparison shows dense retrieval improves Context Recall by 8-12 percentage points on complex queries; understanding *why* helps predict when each approach is appropriate.
  - Quick check question: What type of query vocabulary mismatch would cause BM25 to fail but E5 to succeed?

- Concept: Multi-document QA and information synthesis
  - Why needed here: Enterprise support queries often require combining information from multiple KB articles; this is a core differentiator of WixQA from single-document benchmarks like SQuAD.
  - Quick check question: If 27% of queries require 2+ articles and you retrieve top-5, what's the minimum retrieval precision needed to guarantee all relevant articles are retrieved?

## Architecture Onboarding

- Component map:
  - Knowledge Base (6,221 Wix articles) -> Retrieval Module (BM25 or E5-large-v2, top-k=5) -> Generation Module (LLM: Claude 3.7, Gemini 2.0 Flash, GPT-4o, GPT-4o Mini) -> Evaluation (F1, BLEU, ROUGE, Context Recall, Factuality)

- Critical path:
  1. KB indexing (chunking strategy not specified in paperâ€”assumption: article-level)
  2. Query encoding and retrieval (top-5 documents)
  3. Context assembly (max 50,000 input tokens)
  4. Generation (max 1,024 output tokens, temperature=0)
  5. Evaluation against ground truth

- Design tradeoffs:
  - **Dataset choice**: ExpertWritten for realistic complexity vs. Synthetic for scale (6,221 pairs) vs. Simulated for concise procedural accuracy
  - **Retrieval method**: E5 improves recall (+8-12%) but adds latency and compute vs. BM25
  - **Top-k selection**: Higher k improves multi-article coverage but increases context noise
  - **Evaluation**: LLM-judge metrics (Context Recall, Factuality) correlate with human judgment but introduce judge-model bias

- Failure signatures:
  - **Low Context Recall + High Factuality**: Retrieval failing; generation doing well with limited context
  - **High Context Recall + Low Factuality**: Retrieval working; generation hallucinating or failing to synthesize
  - **Simulated dataset underperformance**: Suggests difficulty with concise, distilled query formulations
  - **Large gap between n-gram metrics (F1, BLEU) and Factuality**: Generated answers semantically correct but lexically different

- First 3 experiments:
  1. **Baseline replication**: Run BM25 + GPT-4o on ExpertWritten; verify Context Recall ~0.73 and Factuality ~0.83 match reported results.
  2. **Retrieval ablation**: Compare top-k=5 vs. top-k=10 on multi-article queries (subset of ExpertWritten); measure Context Recall improvement vs. noise impact on generation.
  3. **Cross-dataset generalization**: Train/fine-tune retrieval model on Synthetic (6,221 pairs), evaluate on ExpertWritten; assess whether synthetic data improves real-query retrieval.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies heavily on LLM-based evaluation (GPT-4o as judge) which introduces potential bias from the judge model itself
- Synthetic dataset (6,221 pairs) may not generalize well to real user query distributions despite systematic generation methodology
- Paper does not specify article-level vs. chunk-level KB indexing, which could significantly impact retrieval performance

## Confidence
- High confidence: Dense retrieval (E5) consistently outperforms BM25 on context recall for complex queries
- Medium confidence: Multi-article dependencies are a critical challenge for enterprise RAG systems
- Medium confidence: Dataset construction methodology creates predictable difficulty differences

## Next Checks
1. Replicate the BM25 baseline on ExpertWritten dataset and verify the reported Context Recall of 0.73 and Factuality of 0.83
2. Run retrieval ablation study comparing top-k=5 vs. top-k=10 specifically on the multi-article subset (27% of ExpertWritten) to quantify the tradeoff between coverage and noise
3. Train a retrieval model on the Synthetic dataset (6,221 pairs) and evaluate its performance on ExpertWritten to assess cross-dataset generalization capability