---
ver: rpa2
title: 'Trust Me on This: A User Study of Trustworthiness for RAG Responses'
arxiv_id: '2601.14460'
source_url: https://arxiv.org/abs/2601.14460
tags:
- response
- trust
- explanations
- responses
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated how different explanation types affect
  user trust in retrieval-augmented generation (RAG) responses. A controlled two-stage
  user study was conducted where participants compared pairs of responses (one higher
  quality than the other), first without explanations and then with one of three explanation
  types: source attribution, factual grounding, or information coverage.'
---

# Trust Me on This: A User Study of Trustworthiness for RAG Responses

## Quick Facts
- **arXiv ID**: 2601.14460
- **Source URL**: https://arxiv.org/abs/2601.14460
- **Authors**: Weronika Łajewska; Krisztian Balog
- **Reference count**: 0
- **Primary result**: Explanations significantly increase users' ability to identify higher-quality RAG responses (165/300 vs. 84/300 without explanations)

## Executive Summary
This study investigates how different explanation types affect user trust in retrieval-augmented generation (RAG) responses through a controlled two-stage user experiment. Participants compared pairs of responses (one higher quality than the other), first without explanations and then with one of three explanation types: source attribution, factual grounding, or information coverage. The results show that explanations significantly increased users' selection of higher quality responses, with factual grounding producing the largest shift in preference. The study highlights that user trust judgments are influenced not just by objective response quality but also by clarity, actionability, and users' own background knowledge, revealing the need for context-aware explanation strategies in RAG systems.

## Method Summary
The study used a within-subject two-stage design with 21 MTurk workers. Participants first judged response pairs without explanations, then with explanations. The experiment used 30 queries from TREC CAsT '22 dataset, with response pairs generated using the GINGER pipeline [11]. Three explanation types were tested: source attribution showing supporting passages, factual grounding with inline statement-to-source links, and information coverage revealing missing topic facets. Each worker completed HITs with 2 queries per explanation type, and data quality was ensured through qualification tasks and control questions.

## Key Results
- Explanations significantly increased users' selection of higher quality responses (165/300 with explanations vs. 84/300 without)
- Factual grounding produced the largest preference shift (27/300 total shifts), followed by source attribution (23/300) and information coverage (19/300)
- Source attribution was most effective in factual contexts but less valued in subjective questions
- User trust judgments were influenced by clarity, actionability, and users' own background knowledge, not just objective response quality

## Why This Works (Mechanism)
The study demonstrates that explanations provide users with concrete signals to evaluate RAG response quality beyond the response text itself. Different explanation types work through distinct mechanisms: source attribution leverages credibility through traceability, factual grounding enables direct verification of claims, and information coverage reveals potential gaps in completeness. The effectiveness of each mechanism depends on context, with users valuing different aspects based on query type and their own knowledge. This suggests that trust is not solely determined by response quality but by the user's ability to assess quality through available signals.

## Foundational Learning
- **RAG system architecture**: Understanding how retrieval and generation components interact is essential for interpreting how explanations relate to the underlying system behavior. Quick check: Can identify retrieval vs. generation components in a RAG pipeline.
- **User trust calibration**: Users don't have perfect ability to judge response quality without external signals. Quick check: Can explain why users might trust a response that is actually low quality.
- **Explanation design principles**: Different explanation types serve different purposes and work better in different contexts. Quick check: Can match explanation type to appropriate use case (factual vs. subjective queries).
- **Controlled user study methodology**: Within-subject designs can isolate the effect of interventions while controlling for individual differences. Quick check: Can explain the advantage of showing both versions to same user.
- **Data quality validation in crowd work**: Qualification tasks and control questions are necessary to ensure reliable data from crowd workers. Quick check: Can calculate acceptable accuracy threshold for control questions.

## Architecture Onboarding

**Component Map**
User -> Response Pair (High/Low Quality) -> Explanation Module -> Trust Judgment

**Critical Path**
Query Selection → Response Generation (GINGER) → Explanation Generation → User Evaluation → Trust Measurement

**Design Tradeoffs**
- Static vs. dynamic explanations: Study used static explanations rather than allowing users to request them on-demand
- Binary quality distinction vs. continuous quality: Simplified to high vs. limited quality for clarity
- Single-session vs. longitudinal study: Captured immediate reactions rather than evolution of trust over time

**Failure Signatures**
- Users ignoring explanations entirely (especially for subjective questions)
- Users being confused by explanation format or content
- Users trusting explanations more than their own judgment inappropriately

**Three First Experiments**
1. Test explanation effectiveness with interactive system where users can request explanations on-demand
2. Vary explanation prominence (inline vs. separate section) to test visibility effects
3. Test explanation effectiveness across different domains (factual vs. opinion-based topics)

## Open Questions the Paper Calls Out
**Open Question 1**: How does user trust in RAG systems evolve and calibrate with prolonged exposure to explanations over time? The study captured immediate reactions rather than longitudinal effects of habituation or learning.

**Open Question 2**: To what extent does a user's prior background knowledge systematically moderate the effectiveness of different explanation types? The study inferred knowledge effects from post-hoc comments rather than objective pre-screening of expertise levels.

**Open Question 3**: How should explanation strategies be optimally adapted for subjective versus factual contexts to maintain user engagement? The study identified that users ignore sources for subjective questions but did not test alternative, tailored explanation strategies for those specific contexts.

**Open Question 4**: Do the observed trust benefits of explanations generalize to expert populations or high-stakes professional domains? The study relied on crowd workers, and professional users in high-stakes fields may exhibit different trust behaviors than the general crowd population.

## Limitations
- Controlled setting limits generalizability to real-world RAG use with static response pairs rather than interactive systems
- Binary quality distinction oversimplifies the continuous nature of response quality
- Crowd workers may not represent typical end users of RAG systems
- Study did not explore how explanations affect trust over time or with repeated exposure

## Confidence

**High confidence**: The core finding that explanations increase users' ability to identify higher-quality responses (165/300 with explanations vs. 84/300 without) is statistically robust and well-supported by the experimental design.

**Medium confidence**: The relative effectiveness of different explanation types (factual grounding > source attribution > information coverage) is supported but may vary with different query types or user populations.

**Medium confidence**: The contextual dependency of explanation effectiveness (source attribution working better for factual queries) is plausible but based on limited query samples per category.

## Next Checks

1. Conduct a follow-up study testing the same explanation types in an interactive RAG system where users can request explanations on-demand rather than receiving them automatically.

2. Perform a controlled replication with full implementation details of the GINGER pipeline and exact explanation generation templates to verify reproducibility of the quality distinction mechanism.

3. Design a longitudinal study tracking how user trust in RAG responses evolves over multiple sessions with repeated exposure to explanations, measuring both selection accuracy and trust calibration.