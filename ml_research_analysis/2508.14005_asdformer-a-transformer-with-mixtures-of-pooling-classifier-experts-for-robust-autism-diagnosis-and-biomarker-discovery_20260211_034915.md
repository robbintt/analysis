---
ver: rpa2
title: 'ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust
  Autism Diagnosis and Biomarker Discovery'
arxiv_id: '2508.14005'
source_url: https://arxiv.org/abs/2508.14005
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000014
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses autism spectrum disorder (ASD) diagnosis by
  modeling functional connectivity in brain networks using functional MRI (fMRI).
  The authors propose ASDFormer, a transformer-based architecture with a Mixture of
  Pooling-Classifier Experts (MoE) decoder to capture ASD-related neural signatures.
---

# ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery

## Quick Facts
- arXiv ID: 2508.14005
- Source URL: https://arxiv.org/abs/2508.14005
- Reference count: 30
- Primary result: Achieves 81.17% AUC on ABIDE dataset for ASD classification

## Executive Summary
This paper addresses autism spectrum disorder (ASD) diagnosis by modeling functional connectivity in brain networks using functional MRI (fMRI). The authors propose ASDFormer, a transformer-based architecture with a Mixture of Pooling-Classifier Experts (MoE) decoder to capture ASD-related neural signatures. The model leverages expert specialization and attention mechanisms to identify critical brain regions and their interactions. Evaluated on the ABIDE dataset, ASDFormer achieves state-of-the-art performance with an AUC of 81.17%, accuracy of 74.60%, sensitivity of 82.55%, and specificity of 66.09%. The approach also provides interpretable insights into connectivity disruptions, particularly in networks like the default mode and sensorimotor systems, aligning with established ASD biomarkers.

## Method Summary
ASDFormer uses a transformer encoder to process functional connectivity matrices derived from resting-state fMRI data. The encoder employs multi-head self-attention to capture complex interactions between brain regions, while positional encoding preserves spatial relationships. A Mixture of Pooling-Classifier Experts (MoE) decoder follows, where specialized experts independently pool features and classify based on learned responsibilities. This architecture allows for both global pattern recognition and local specialization in identifying ASD-related neural signatures. The model is trained on the ABIDE dataset, which includes multi-site fMRI data from individuals with and without ASD.

## Key Results
- Achieves 81.17% AUC, 74.60% accuracy, 82.55% sensitivity, and 66.09% specificity on ABIDE dataset
- Outperforms existing methods including BrainNetCNN, VGGNet, ResNet, and SFMixer
- Identifies key disrupted networks including default mode, salience, and sensorimotor systems as critical biomarkers

## Why This Works (Mechanism)
The transformer architecture excels at capturing long-range dependencies and complex interactions in functional connectivity data. The self-attention mechanism allows the model to weigh the importance of different brain region interactions dynamically, while the MoE decoder enables specialized processing of distinct neural patterns associated with ASD. This combination allows ASDFormer to identify both global connectivity disruptions and localized anomalies that characterize the disorder. The attention weights provide interpretable insights into which brain regions and connections are most relevant for classification, aligning with established neurobiological understanding of ASD.

## Foundational Learning
1. **Functional Connectivity**: Measures temporal correlations between brain regions - needed for capturing neural network dynamics; quick check: verify correlation matrices are properly computed from fMRI time series
2. **Transformer Architecture**: Uses self-attention to model relationships between all brain regions simultaneously - needed for capturing complex, non-local interactions; quick check: confirm attention scores are normalized and interpretable
3. **Mixture of Experts**: Combines multiple specialized classifiers that learn different aspects of the data - needed for capturing heterogeneous ASD presentations; quick check: verify gating network properly assigns responsibilities to experts
4. **Positional Encoding**: Embeds spatial information about brain regions into the model - needed because transformers lack inherent sequence awareness; quick check: ensure encoding preserves anatomical relationships
5. **Attention Interpretability**: Uses attention weights to identify important brain regions - needed for clinical validation and biomarker discovery; quick check: correlate attention patterns with known ASD-related networks

## Architecture Onboarding

**Component Map**: fMRI connectivity matrix -> Transformer Encoder -> MoE Decoder -> Classification

**Critical Path**: Functional connectivity extraction → Transformer self-attention → MoE pooling/classification → Output probabilities

**Design Tradeoffs**: The model prioritizes interpretability and biomarker discovery over pure classification performance, which explains the moderate specificity. The MoE decoder adds complexity but enables specialization for heterogeneous ASD presentations. The transformer architecture trades computational efficiency for superior capture of complex brain interactions.

**Failure Signatures**: 
- Low specificity suggests potential overfitting to ASD-positive patterns
- Attention weights may highlight spurious correlations rather than true biomarkers
- Performance degradation across sites indicates sensitivity to acquisition protocols
- Imbalanced sensitivity/specificity ratio suggests model bias toward positive predictions

**First Experiments**:
1. Test on held-out sites to assess cross-site generalization
2. Compare attention-based interpretations with established neurobiological networks
3. Evaluate performance on balanced datasets to address potential class imbalance issues

## Open Questions the Paper Calls Out
None

## Limitations
- Modest sample size (N=755) across multiple sites introduces potential heterogeneity
- Lower specificity (66.09%) compared to sensitivity (82.55%) suggests higher false positive rates
- Exclusive focus on resting-state fMRI limits applicability to task-based paradigms
- Lack of detailed cross-validation and site-specific performance metrics

## Confidence

**High confidence**: Transformer architecture effectiveness and overall classification performance metrics

**Medium confidence**: Biomarker discovery claims and interpretability of specific brain regions

**Medium confidence**: Generalizability across diverse populations and clinical settings

## Next Checks

1. Conduct external validation on independent, multi-site ASD cohorts with varying demographics and scanner protocols to assess true generalizability
2. Perform ablation studies systematically removing individual components (attention mechanisms, MoE decoder) to quantify their specific contributions to performance
3. Implement controlled experiments comparing attention-based interpretations with alternative explainability methods (e.g., SHAP values, counterfactual explanations) to validate the identified biomarkers