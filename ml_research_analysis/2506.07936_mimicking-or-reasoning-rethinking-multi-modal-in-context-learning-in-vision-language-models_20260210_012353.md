---
ver: rpa2
title: 'Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language
  Models'
arxiv_id: '2506.07936'
source_url: https://arxiv.org/abs/2506.07936
tags:
- reasoning
- answer
- arxiv
- shot
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines the assumption that vision-language models
  (VLMs) can perform genuine multimodal in-context learning (MM-ICL). Under various
  controlled conditions, the authors find that VLMs often fail to learn meaningfully
  from demonstrations, relying instead on shallow heuristics such as copying or majority
  voting.
---

# Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models

## Quick Facts
- arXiv ID: 2506.07936
- Source URL: https://arxiv.org/abs/2506.07936
- Reference count: 40
- This paper re-examines the assumption that vision-language models (VLMs) can perform genuine multimodal in-context learning (MM-ICL). Under various controlled conditions, the authors find that VLMs often fail to learn meaningfully from demonstrations, relying instead on shallow heuristics such as copying or majority voting. To strengthen the test, they propose a new MM-ICL with Reasoning pipeline that augments demonstrations with rationales, but even with this enhancement, models show limited sensitivity to shot count, retrieval method, and rationale quality. The results suggest that current VLMs lack true MM-ICL capabilities and do not effectively leverage demonstration-level information as intended.

## Executive Summary
This paper critically examines whether vision-language models (VLMs) can genuinely perform multimodal in-context learning (MM-ICL) by learning from demonstrations. The authors find that VLMs often rely on shallow heuristics like copying or majority voting rather than truly understanding and applying demonstration-level information. To address this, they propose an enhanced MM-ICL with Reasoning pipeline that includes rationales for each demonstration, but results show limited improvement, suggesting deeper architectural limitations in current VLMs.

## Method Summary
The authors conduct a systematic evaluation of VLMs' MM-ICL capabilities under various controlled conditions. They first assess whether models can learn from demonstrations without additional support, then propose an enhanced pipeline (MM-ICL with Reasoning) that includes rationales for each demonstration. The study tests multiple shot counts, retrieval methods, and rationale qualities to evaluate model performance. Experiments are conducted on both synthetic and real-world datasets, with comparisons across different VLM architectures to assess generalizability.

## Key Results
- VLMs often fail to learn meaningfully from demonstrations, instead relying on shallow heuristics like copying or majority voting.
- The proposed MM-ICL with Reasoning pipeline shows limited improvement, indicating that current VLMs lack true MM-ICL capabilities.
- Models demonstrate minimal sensitivity to shot count, retrieval method, and rationale quality, suggesting fundamental limitations in cross-modal integration during context processing.

## Why This Works (Mechanism)
The paper does not explicitly detail a mechanism for why MM-ICL works in VLMs, as the primary finding is that current VLMs do not effectively perform MM-ICL. Instead, the authors highlight the need for architectures with tighter cross-modal fusion and reasoning-aware retrieval strategies to overcome these limitations.

## Foundational Learning
- Multimodal in-context learning (MM-ICL): The ability of VLMs to learn from demonstrations across both visual and textual modalities. Why needed: To understand the core capability being evaluated and its significance in advancing VLM reasoning.
- Shallow heuristics: Strategies like copying or majority voting that VLMs rely on instead of genuine learning. Why needed: To recognize the limitations of current VLMs in processing demonstration-level information.
- Cross-modal fusion: The integration of visual and textual information within a model. Why needed: To identify the architectural gaps that prevent effective MM-ICL.
- Reasoning-aware retrieval: Strategies for selecting demonstrations based on reasoning path consistency rather than input similarity. Why needed: To explore potential solutions for improving MM-ICL in reasoning-intensive tasks.
- Interleaved-modal Chain-of-Thought (CoT): A reasoning approach that alternates between visual and textual steps. Why needed: To investigate whether multimodal reasoning can enhance demonstration learning.

## Architecture Onboarding
Component map: Input Retrieval -> Demonstration Augmentation (with Rationale) -> Context Processing -> Output Generation
Critical path: The pipeline's critical path involves retrieving relevant demonstrations, augmenting them with rationales, and processing the combined context to generate outputs.
Design tradeoffs: Balancing the complexity of demonstration augmentation (e.g., adding rationales) with the model's ability to process and utilize the additional information effectively.
Failure signatures: Overreliance on shallow heuristics, minimal sensitivity to shot count or retrieval method, and limited improvement even with enhanced demonstrations.
Three first experiments:
1. Test the proposed pipeline on a broader range of task types to assess generalizability.
2. Conduct ablation studies varying model architecture, size, and pretraining objectives to determine MM-ICL limitations.
3. Perform qualitative analysis of model attention patterns to understand the mechanisms underlying observed behavior.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can reasoning-aware retrieval strategies overcome the limitations of input-based similarity retrieval in reasoning-intensive MM-ICL tasks?
- Basis in paper: The authors explicitly propose exploring "reasoning-aware retrieval strategies" in the Limitations and Future Direction section.
- Why unresolved: The study found that multimodal retrievers fail for reasoning models because input similarity does not correlate with reasoning path similarity, often causing them to underperform compared to random selection.
- What evidence would resolve it: A retrieval method that selects demonstrations based on reasoning path consistency, resulting in significant performance gains over random baselines for VLM reasoners.

### Open Question 2
- Question: Do architectures with tighter cross-modal fusion enable VLMs to effectively utilize demonstration-level information for compositional reasoning?
- Basis in paper: The authors list "architectures with tighter cross-modal fusion" as a specific future direction to address the lack of true MM-ICL capabilities.
- Why unresolved: Current models rely on shallow heuristics (copying, majority voting) rather than learning from demonstrations, suggesting a failure in cross-modal integration during context processing.
- What evidence would resolve it: A VLM with enhanced fusion mechanisms showing positive scaling trends (performance increasing with shot count) on out-of-distribution reasoning benchmarks.

### Open Question 3
- Question: Does reasoning with multiple modalities, such as interleaved-modal Chain-of-Thought, enhance a VLM's ability to learn from in-context demonstrations?
- Basis in paper: The authors suggest future work should explore "reasoning with multiple modalities (interleaved-modal CoT)" in the Limitations and Future Direction section.
- Why unresolved: The current "MM-ICL with Reasoning" pipeline relies on text-only rationales, which may be insufficient for conveying visual task methodologies to the model.
- What evidence would resolve it: A pipeline allowing demonstrations to include interleaved visual and textual reasoning steps, leading to significant performance improvements over text-only rationale baselines.

## Limitations
- The study relies on specific synthetic and real-world datasets, raising questions about generalizability across domains and task types.
- The definition and quality of "reasoning" rationales are not rigorously validated, leaving ambiguity about their meaningfulness.
- The analysis focuses on performance metrics without deep qualitative investigation into the actual mechanisms VLMs employ when processing demonstrations.

## Confidence
High: The core finding that VLMs often fail to learn meaningfully from demonstrations and instead rely on shallow heuristics is well-supported by controlled experiments across multiple conditions and model types.
Medium: The claim that current VLMs lack true MM-ICL capabilities is plausible but requires further validation across broader task domains and model architectures to rule out task-specific or model-specific limitations.
Low: The assertion that the proposed reasoning pipeline would substantially improve MM-ICL is weakly supported, as the results show limited improvement even with enhanced demonstrations, suggesting the underlying issue may be more fundamental than can be addressed through simple prompt engineering.

## Next Checks
1. Test the same experimental protocol across a wider range of task types and domains to assess generalizability of the findings.
2. Conduct ablation studies varying model architecture, size, and pretraining objectives to determine whether MM-ICL limitations are universal or model-dependent.
3. Perform detailed qualitative analysis of model attention patterns and internal representations when processing demonstrations to understand the actual mechanisms underlying the observed behavior.