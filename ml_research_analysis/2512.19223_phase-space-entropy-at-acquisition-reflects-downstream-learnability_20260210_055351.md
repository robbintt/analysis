---
ver: rpa2
title: Phase-space entropy at acquisition reflects downstream learnability
arxiv_id: '2512.19223'
source_url: https://arxiv.org/abs/2512.19223
tags:
- entropy
- periodic
- random
- mask
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce a phase-space entropy measure that captures\
  \ how acquisition policies alter the joint spatial and spectral structure of signals.\
  \ By defining a Husimi-based, band-normalized entropy change, they show that smaller\
  \ |\u2206SB| corresponds to better downstream learnability across vision, MRI, and\
  \ MIMO."
---

# Phase-space entropy at acquisition reflects downstream learnability

## Quick Facts
- **arXiv ID**: 2512.19223
- **Source URL**: https://arxiv.org/abs/2512.19223
- **Reference count**: 40
- **Primary result**: A phase-space entropy measure predicts downstream learnability of subsampled signals across vision, MRI, and MIMO without requiring model training.

## Executive Summary
This paper introduces a phase-space entropy measure that captures how acquisition policies alter the joint spatial and spectral structure of signals. By defining a Husimi-based, band-normalized entropy change, the authors show that smaller |ΔSB| corresponds to better downstream learnability across vision, MRI, and MIMO. Periodic sampling increases entropy via coherent spectral folding, while random sampling preserves it. The approach enables zero-training acquisition design, such as selecting MRI masks without model training, and is validated with over-the-air MIMO measurements.

## Method Summary
The method computes the phase-space entropy change by first generating a Husimi density via Gaussian-windowed short-time Fourier transform of the acquired signal. The local spectral power is normalized over the Nyquist band to create a probability density, from which Shannon entropy is calculated at each spatial location. The global entropy SB is aggregated (uniform or energy-weighted), and the change ΔSB is computed relative to a reference protocol. The magnitude |ΔSB| serves as a proxy for downstream reconstruction difficulty, with smaller values indicating better-preserved signal structure for learning tasks.

## Key Results
- Periodic subsampling systematically increases phase-space entropy due to coherent spectral folding, while random sampling preserves it in expectation
- The magnitude of entropy change |ΔSB| consistently ranks sampling geometries and predicts downstream reconstruction quality without requiring model training
- The metric discriminates between sampling geometries better than conventional distortion proxies and matches designs tuned by conventional criteria across vision, MRI, and MIMO modalities

## Why This Works (Mechanism)

### Mechanism 1
Periodic subsampling systematically increases phase-space entropy (ΔSB > 0) relative to a fully sampled reference. Periodic masking in the spatial domain acts as a convolution in the spectral domain, creating a convex mixture of shifted spectral copies within the Nyquist band. Because Shannon entropy is strictly concave, the entropy of a mixture is strictly greater than the mixture of entropies, reflecting a loss of structural distinctiveness. This holds when the signal is not "lattice-invariant" and the shifted spectral copies are not identical.

### Mechanism 2
Randomized subsampling preserves phase-space entropy in expectation (ΔSB ≈ 0). Random masks break the phase coherence of the sampling lattice. While the instantaneous Wigner distribution fluctuates, the smoothed Husimi density effectively averages to the original density plus a small isotropic pedestal. This avoids the systematic, directed "folding" of energy that increases disorder, limiting perturbations to stochastic fluctuations of order O(1/√N). This assumes large N and mask independence.

### Mechanism 3
The magnitude of band-entropy change (|ΔSB|) serves as a modality-agnostic proxy for downstream reconstruction difficulty. The metric quantifies the total disruption of joint space-frequency organization. In MRI, coefficients are removed rather than folded, typically decreasing entropy (ΔSB < 0). In Vision, entropy typically increases. By using the magnitude |ΔSB|, the framework unifies "folding" (additive noise-like) and "removal" (subtractive loss) into a single "distance-to-reference" scale that correlates with the information preserved for the learner.

## Foundational Learning

- **Concept: Joint Time-Frequency (Phase-Space) Representations**
  - **Why needed here**: The paper rejects purely spectral or spatial analysis in favor of a combined view to capture local structural changes
  - **Quick check question**: How does a spectrogram differ from a global Fourier transform, and why does the paper prefer the Husimi (smoothed) version over the raw Wigner distribution?

- **Concept: Aliasing as Coherent Folding**
  - **Why needed here**: Understanding why periodic sampling is "bad" requires viewing aliasing not just as error, but as a deterministic mixing operation that increases uncertainty
  - **Quick check question**: If you sample a high-frequency sine wave at too low a rate, why does it appear as a lower frequency wave, and how does this "folding" differ from random noise?

- **Concept: Jensen's Inequality for Concave Functions**
  - **Why needed here**: This is the mathematical engine proving that mixing signals increases entropy
  - **Quick check question**: Why is the entropy of a mixture of two probability distributions always greater than or equal to the average of their individual entropies?

## Architecture Onboarding

- **Component map**: Signal Field I(x) -> Husimi Transform -> Band-Normalization -> Entropy Computer -> Comparator
- **Critical path**: The definition of the Instrument Kernel (Φ) determines the resolution tradeoff in phase space. The selection of the Reference Protocol (Iref) is equally critical; a noisy reference renders the metric unstable.
- **Design tradeoffs**:
  - Window Size (σ): Too small captures noise/insufficient frequency resolution; too large smears local structure
  - Sign handling: Vision/MIMO (folding) generally increases entropy (ΔS > 0), while MRI (removal) decreases it (ΔS < 0). The system must use |ΔSB| for ranking across these distinct failure modes
- **Failure signatures**:
  - Rank Reversal: Using too small a window on MRI data causes reversal of rankings
  - Saturation: In aggressive undersampling, conventional proxies compress to zero discriminability, while |ΔSB| maintains dynamic range
  - Reference Mismatch: If the reference Iref is biased or acquired under different conditions, ΔSB measures setup error rather than acquisition quality
- **First 3 experiments**:
  1. Implement a simple 1D or 2D periodic vs. random mask on a standard image. Compute ΔSB and verify that Periodic yields ΔSB > 0 and Random yields ΔSB ≈ 0
  2. Systematically vary the Gaussian window size (Φ) on the same dataset. Plot ΔSB vs. window size to identify the "stability plateau"
  3. Train a simple autoencoder or classifier on data acquired with two different masks (one high ΔSB, one low). Verify that Low ΔSB correlates with lower reconstruction loss or higher accuracy before tuning model hyperparameters

## Open Questions the Paper Calls Out
- How can the phase-space entropy metric be extended with band- or region-weighting to quantify information preservation for downstream tasks that depend disproportionately on specific structural features?
- Can |ΔSB| be utilized in a closed-loop sensing system to dynamically maintain acquisition quality under shifting conditions?
- Does the phase-space entropy framework accurately predict learnability for deterministic sampling patterns (e.g., compressed sensing designs) that lie outside the standard periodic or random mask models?
- How sensitive is the acquisition principle to noise or bias in the reference protocol Iref, and can the metric be stabilized against reference mismatch?

## Limitations
- The framework assumes the signal does not possess inherent lattice invariance; if the signal's spectral structure matches the sampling mask periodicity, the entropy increase from folding may vanish
- The energy-weighted aggregation in Vision experiments is underspecified, creating ambiguity in reproducing exact entropy rankings across scales
- Reference protocol selection critically affects ΔSB stability; noisy or mismatched references will dominate the metric

## Confidence
- **High Confidence**: The mathematical mechanism linking periodic sampling to entropy increase via Jensen's inequality and the strict concavity of Shannon entropy
- **Medium Confidence**: The preservation of entropy under random sampling in expectation. While the theoretical averaging argument is sound, practical deviations could reduce its reliability
- **Medium Confidence**: |ΔSB| as a modality-agnostic proxy for downstream learnability. The metric shows strong empirical correlations, but edge cases could cause failures that are not well-characterized

## Next Checks
1. Systematically degrade the reference signal (add synthetic noise, use a different acquisition modality) and measure the stability of ΔSB rankings for periodic vs. random masks
2. Design an experiment where a narrow, high-importance frequency band is removed. Measure |ΔSB| and downstream performance to test if the metric can fail when critical, low-entropy features are lost
3. Generate random masks with varying degrees of spatial correlation and measure both |ΔSB| and downstream reconstruction quality to determine the threshold at which "random" sampling ceases to preserve entropy