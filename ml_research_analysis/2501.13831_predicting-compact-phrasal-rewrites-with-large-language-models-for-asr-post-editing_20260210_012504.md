---
ver: rpa2
title: Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post
  Editing
arxiv_id: '2501.13831'
source_url: https://arxiv.org/abs/2501.13831
tags:
- span
- phrase
- representations
- target
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two compact phrasal rewrite representations
  for efficient text rewriting using large language models. The span representation
  encodes edits as numerical spans with target phrases, while the phrase pair representation
  uses source-target phrase pairs with optional dilation for context.
---

# Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing

## Quick Facts
- arXiv ID: 2501.13831
- Source URL: https://arxiv.org/abs/2501.13831
- Authors: Hao Zhang; Felix Stahlberg; Shankar Kumar
- Reference count: 19
- This paper introduces two compact phrasal rewrite representations for efficient text rewriting using large language models. The span representation encodes edits as numerical spans with target phrases, while the phrase pair representation uses source-target phrase pairs with optional dilation for context. A target-only representation uses only target phrases with surrounding context words. Applied to ASR post-editing on LibriSpeech, the target-only representation achieves 3.0 WER with 6 tokens average output length, closing 57-54% of the accuracy gap between the span representation and full rewrite while retaining 80-78% of the span representation's efficiency.

## Executive Summary
This paper introduces compact phrasal rewrite representations designed to improve the efficiency of large language models for ASR post-editing tasks. The authors propose three representations: span-based, phrase pair-based, and target-only formats that reduce output length while maintaining accuracy. The target-only representation achieves significant efficiency gains with minimal accuracy loss, demonstrating a promising approach for practical deployment of LLMs in text rewriting applications.

## Method Summary
The paper presents three compact phrasal rewrite representations for text rewriting using LLMs. The span representation encodes edits as numerical spans with target phrases, the phrase pair representation uses source-target phrase pairs with optional dilation for context, and the target-only representation uses only target phrases with surrounding context words. These representations are evaluated on the LibriSpeech ASR post-editing task, comparing their accuracy and efficiency against full rewrite approaches.

## Key Results
- Target-only representation achieves 3.0 WER with 6 tokens average output length
- Closes 57-54% of the accuracy gap between span representation and full rewrite
- Retains 80-78% of the span representation's efficiency

## Why This Works (Mechanism)
The compact representations work by constraining the LLM's output space to only the necessary information for rewriting. By encoding edits as spans, phrase pairs, or target phrases, the models avoid generating full sentences and instead focus on the minimal changes needed. The target-only approach is particularly efficient because it eliminates redundant source phrase encoding while using context words to maintain disambiguation.

## Foundational Learning
- **ASR post-editing**: Correcting errors in automatic speech recognition output
  *Why needed*: The task demonstrates practical application of compact rewriting
  *Quick check*: Verify WER improvements on LibriSpeech dataset
- **Phrasal rewrite representations**: Structured formats for encoding text edits
  *Why needed*: Enables efficient LLM output generation
  *Quick check*: Compare output lengths across different representations
- **Span encoding**: Using numerical positions to indicate edit locations
  *Why needed*: Reduces output verbosity compared to full phrase pairs
  *Quick check*: Measure accuracy vs efficiency trade-offs
- **Dilation in phrase pairs**: Including context words around source phrases
  *Why needed*: Helps disambiguate edits in ambiguous contexts
  *Quick check*: Evaluate impact of dilation on accuracy
- **Target-only representation**: Encoding only target phrases with context
  *Why needed*: Maximizes efficiency by removing redundant source encoding
  *Quick check*: Compare WER and output length with other representations

## Architecture Onboarding

**Component Map**: Input text -> Span/Phrase Pair/Target-only encoder -> LLM -> Compact rewrite output -> Application

**Critical Path**: Text input → Compact representation encoding → LLM generation → Output parsing → Text rewriting

**Design Tradeoffs**: Accuracy vs efficiency balance, with target-only favoring efficiency while span representation prioritizing accuracy

**Failure Signatures**: Increased WER when context words are insufficient for disambiguation, reduced efficiency when representations are too verbose

**First Experiments**:
1. Compare WER and output length across all three representations on LibriSpeech
2. Test impact of dilation parameter on phrase pair representation accuracy
3. Evaluate context word requirements for target-only representation effectiveness

## Open Questions the Paper Calls Out
The paper acknowledges that the evaluation is limited to a single dataset and task, raising questions about performance on different domains or languages. The generalizability of the compact phrasal rewrite representations beyond the LibriSpeech ASR post-editing domain remains uncertain.

## Limitations
- Evaluation limited to single dataset (LibriSpeech)
- Limited scope to ASR post-editing task only
- Subjective assumptions about acceptable accuracy-efficiency trade-offs
- Efficiency metrics based on output length may not reflect real-world computational costs

## Confidence
High: The experimental results are well-documented and demonstrate clear improvements in efficiency while maintaining reasonable accuracy.
Medium: The narrow scope of evaluation and lack of cross-domain validation limit broader claims about general applicability.

## Next Checks
1. Evaluate the compact phrasal rewrite representations on additional ASR datasets and languages to assess cross-domain and cross-linguistic generalizability.
2. Compare the computational efficiency and latency of the target-only representation against full rewrite approaches in a production-like environment.
3. Conduct a user study to determine whether the 6-token average output length meets practical usability requirements for downstream tasks.