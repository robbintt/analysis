---
ver: rpa2
title: 'Attention on Multiword Expressions: A Multilingual Study of BERT-based Models
  with Regard to Idiomaticity and Microsyntax'
arxiv_id: '2505.06062'
source_url: https://arxiv.org/abs/2505.06062
tags:
- attention
- idioms
- msus
- language
- mwes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how fine-tuning BERT-based models affects
  their attention patterns towards idioms and microsyntactic units (MSUs) across six
  Indo-European languages. By extending Jang et al.'s attention analysis methodology,
  the authors examine pre-trained versus fine-tuned models on syntactic and semantic
  tasks.
---

# Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax

## Quick Facts
- arXiv ID: 2505.06062
- Source URL: https://arxiv.org/abs/2505.06062
- Reference count: 11
- Primary result: Fine-tuning BERT-based models redistributes attention toward task-relevant Multiword Expressions, with syntactic tasks increasing lower-layer attention to microsyntactic units and semantic tasks distributing attention more evenly across layers.

## Executive Summary
This study investigates how fine-tuning BERT-based models affects their attention patterns toward idioms and microsyntactic units (MSUs) across six Indo-European languages. By extending Jang et al.'s attention analysis methodology, the authors examine pre-trained versus fine-tuned models on syntactic and semantic tasks. Results show that syntactic fine-tuning increases attention to MSUs in lower layers, aligning with syntactic processing, while semantic fine-tuning distributes attention more evenly across layers for idioms. Attention to both MWE types generally decreases after fine-tuning, especially in higher layers. The study reveals that attention mechanisms adapt during fine-tuning to better reflect linguistic properties of the target task and language.

## Method Summary
The study fine-tunes 24-layer monolingual BERT-large models (BERT, GBERT, RobBERT, HerBERT, ruBERT, Liberta) on four tasks per language: syntactic (dependency relation, POS tagging) and semantic (NER, topic classification). Models are fine-tuned for 10 epochs with a minimum F1 threshold of 0.75. Attention matrices are extracted for MWE-containing sentences, averaging across 16 attention heads per layer. The analysis computes layer-wise attention percentages directed to MWE tokens, comparing pre-trained versus fine-tuned models across six Indo-European languages.

## Key Results
- Syntactic fine-tuning increases attention to microsyntactic units in lower layers (3-4), while semantic fine-tuning distributes attention more evenly across layers for idioms
- Pre-trained models show uniform attention to MWEs across middle and upper layers; fine-tuning creates sharper attention peaks
- Cross-linguistic differences observed: Germanic languages show more uniform patterns while Slavic languages display more varied attention distributions
- Attention to both MWE types generally decreases after fine-tuning, especially in higher layers

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on syntactic tasks increases attention to microsyntactic units in lower layers, while semantic tasks distribute attention more evenly across layers. Task-specific gradients during fine-tuning reweight attention heads to prioritize linguistic features relevant to the downstream objective—syntactic relations for DepRel/POS, semantic integration for NER/Topic.

### Mechanism 2
Pre-trained models allocate uniform attention to MWEs across middle and upper layers; fine-tuning creates sharper attention peaks. Pre-training optimizes for general-purpose representation (uniform attention), while fine-tuning specializes attention distributions toward task-relevant tokens.

### Mechanism 3
Morphological complexity influences attention distribution patterns across languages. Languages with richer morphology (Slavic) require more varied attention allocation, while less morphologically complex languages (Germanic) show more uniform patterns.

## Foundational Learning

- **Multiword Expressions (MWEs)** — idioms and microsyntactic units
  - Why needed here: Understanding that MWEs are non-compositional units requiring specialized processing is essential to interpret attention findings.
  - Quick check question: Can you explain why "spill the beans" cannot be understood by composing the meanings of "spill" + "beans"?

- **BERT layer hierarchy (syntactic → semantic)**
  - Why needed here: The paper's interpretation relies on lower layers encoding syntax and upper layers encoding semantics.
  - Quick check question: Which layers would you expect to show higher attention peaks for part-of-speech tagging versus topic classification?

- **Attention head averaging**
  - Why needed here: The methodology averages attention across heads per layer; results depend on this aggregation choice.
  - Quick check question: What information might be lost when averaging 16 attention heads into a single matrix per layer?

## Architecture Onboarding

- **Component map**: Input sentences → Monolingual BERT-large → Layer-wise attention matrices → Task-specific classifiers → F1 evaluation
- **Critical path**: Load pre-trained monolingual BERT model → Extract baseline attention matrices for MWE-containing sentences → Fine-tune on task (10 epochs, F1 > 0.75) → Re-extract attention matrices from fine-tuned model → Compute layer-wise attention percentages directed to MWE tokens
- **Design tradeoffs**: Head averaging simplifies analysis but may obscure head-specific specialization; Monolingual models enable controlled comparison but limit cross-lingual transfer insights; Dataset size varies (701–7000 samples), potentially affecting fine-tuning quality
- **Failure signatures**: Uniform attention after fine-tuning (no peaks) → task gradients not reaching attention layers; Attention drops across all layers for Russian → possible overfitting or language-specific tokenization issues; Inconsistent patterns across languages → dataset quality variation or model architecture differences
- **First 3 experiments**: 
  1. Replicate attention extraction pipeline: Load pre-trained BERT-large, pass MWE sentences, verify head-averaged attention matrix extraction matches paper's methodology.
  2. Single-task fine-tuning validation: Fine-tune on POS tagging (smallest dataset), confirm F1 > 0.75 and attention peaks emerge in lower layers.
  3. Cross-check MSU attention: Verify that MSU sentences show peak attention at layers 3–4 in pre-trained model before any fine-tuning.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do BERT-based models fine-tuned on tasks requiring deep semantic processing (e.g., summarization, paraphrase detection) exhibit stronger or more distinct attention shifts toward idiomatic expressions compared to the shallow semantic tasks (NER, Topic Classification) tested in this study?
- **Open Question 2**: Are the observed attention adaptation patterns for MWEs specific to the encoder-only BERT architecture, or do they generalize to decoder-based (e.g., GPT) or encoder-decoder (e.g., T5) transformer architectures?
- **Open Question 3**: Is the consistent decrease in attention to MWEs observed in the Russian models a result of dataset idiosyncrasies, language-specific morphological complexity, or the specific pre-training corpus used?
- **Open Question 4**: How does the attention allocation to Microsyntactic Units (MSUs) differ in Germanic languages compared to the Slavic languages tested, given that the current study only analyzed MSUs in Slavic contexts?

## Limitations
- Fine-tuning hyperparameters (learning rate, batch size, optimizer) are unspecified, potentially affecting attention redistribution patterns
- Cross-linguistic differences may be confounded by dataset size variations and language-specific tokenization effects
- Head averaging methodology obscures potential head-specific attention patterns that could provide more granular insights
- The correlation between morphological complexity and attention patterns requires more systematic validation across additional language families

## Confidence
- **High confidence**: Pre-training produces uniform attention distributions across middle/upper layers; fine-tuning creates sharper attention peaks for MWEs
- **Medium confidence**: Syntactic fine-tuning increases lower-layer attention to MSUs while semantic fine-tuning distributes attention more evenly for idioms
- **Low confidence**: Morphological complexity directly determines attention distribution patterns across languages (Germanic vs. Slavic)

## Next Checks
1. **Hyperparameter sensitivity analysis**: Re-run fine-tuning with varying learning rates (1e-5 to 5e-5) and batch sizes (16/32) to verify that attention pattern changes are robust to optimization settings
2. **Head-level attention analysis**: Decompose averaged attention matrices to examine whether specific attention heads show specialized MWE processing rather than uniform layer-wide patterns
3. **Cross-linguistic replication**: Test the morphological complexity hypothesis by extending analysis to agglutinative languages (Finnish, Turkish) and isolating effects of morphological richness from other linguistic factors