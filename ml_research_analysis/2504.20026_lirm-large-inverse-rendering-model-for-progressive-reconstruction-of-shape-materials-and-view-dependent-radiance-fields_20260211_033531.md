---
ver: rpa2
title: 'LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape,
  Materials and View-dependent Radiance Fields'
arxiv_id: '2504.20026'
source_url: https://arxiv.org/abs/2504.20026
tags:
- reconstruction
- lirm
- images
- rendering
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large Inverse Rendering Model (LIRM) addresses the challenge of
  reconstructing high-quality 3D shape, materials, and view-dependent radiance fields
  from sparse multi-view images. The core method introduces three key innovations:
  a progressive update model that refines reconstructions with additional views without
  increasing GPU memory, a hexa-plane neural SDF representation for better texture
  detail recovery, and neural directional encoding to handle view-dependent effects.'
---

# LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields

## Quick Facts
- **arXiv ID:** 2504.20026
- **Source URL:** https://arxiv.org/abs/2504.20026
- **Reference count:** 40
- **Primary result:** LIRM achieves state-of-the-art 3D reconstruction from sparse multi-view images, producing relightable assets in 0.3 seconds with superior quality to optimization-based methods on real-world benchmarks.

## Executive Summary
LIRM (Large Inverse Rendering Model) addresses the challenge of reconstructing high-quality 3D shape, materials, and view-dependent radiance fields from sparse multi-view images. The core innovation is a progressive update model that refines reconstructions with additional views without increasing GPU memory consumption. LIRM introduces a hexa-plane neural SDF representation for better texture detail recovery and neural directional encoding to handle view-dependent effects. Trained on a large-scale dataset with a coarse-to-fine scheme, LIRM achieves state-of-the-art results, producing relightable 3D content comparable to optimization-based methods while requiring only 0.3 seconds per reconstruction.

## Method Summary
LIRM is a transformer-based inverse rendering model that predicts 3D shape (SDF), PBR materials (albedo, metallic, roughness), and view-dependent radiance fields from sparse multi-view images. The model uses a hexa-plane representation (6 orthogonal planes dividing space into 8 octants) instead of standard tri-plane to better isolate opposing surface textures. A neural directional encoding mechanism predicts multiple feature panoramas to capture high-frequency view-dependent effects. The progressive update module allows incremental refinement by feeding previous iteration's output tokens back into the transformer with new image tokens. Training follows a 3-stage coarse-to-fine scheme from 128 to 384 to 512 resolution using 24-block transformer on 64 H100 GPUs.

## Key Results
- LIRM produces relightable 3D assets in 0.3 seconds, significantly faster than optimization-based methods (2+ hours)
- Achieves state-of-the-art PSNR, SSIM, and LPIPS scores on synthetic benchmarks (GSO, ABO, DTC)
- On real-world Stanford-ORB benchmark, matches or exceeds optimization-based methods in geometry and relighting accuracy
- Outperforms optimization methods on highly specular objects where competitors fail

## Why This Works (Mechanism)

### Mechanism 1: Token-Based State Fusion for Progressive Refinement
The model treats predicted 3D representation tokens as persistent state, fusing them with new view information via self-attention without increasing memory. When new views arrive, previous iteration's output tokens are concatenated with positional encodings and fused via an MLP before being fed into the transformer with new image tokens.

### Mechanism 2: Hexa-Plane Spatial Partitioning
Replaces standard tri-plane with 6 planes dividing the bounding box into 8 octants, separating features from opposing object surfaces. This prevents texture bleeding between front and back faces that occurs in tri-plane representation when both sides contain different complex textures.

### Mechanism 3: Multi-Panorama Neural Directional Encoding (NDE)
Predicts multiple feature panoramas and uses an MLP to select which panorama to sample based on reflection direction. This discrete set of reflection environments approximates complex light transport and near-field effects better than low-frequency Spherical Harmonics.

## Foundational Learning

- **Signed Distance Functions (SDF) & Volume Rendering**: Understanding how SDF values convert to density is critical for extracting surfaces and rendering opacity. *Quick check: How does the β parameter in density conversion affect surface geometry sharpness?*
- **Inverse Rendering & Material Decomposition**: The model separates appearance into albedo, metallic, roughness, and radiance fields. *Quick check: Why does the model input background images alongside masked object images?*
- **Transformer Cross-Attention / Tokenization**: LIRM attends between image tokens and 3D structural tokens. *Quick check: In the update module, does the transformer treat image tokens and hexa-plane tokens as distinct modalities or a single sequence?*

## Architecture Onboarding

- **Component map**: Tokenizer (images+rays -> tokens) -> 24-block Transformer -> Decoders (5 heads: SDF, albedo, metallic, roughness, color) -> Renderer (volume ray marching)
- **Critical path**: The Update Loop. Implement Eq 5 carefully—input at step m is MLP_fuse(Prev_Tokens, Pos_Encoding), not just new images.
- **Design tradeoffs**: Hexa-plane uses more memory but reduces texture ambiguity; higher NDE count captures more reflections but increases overhead; coarse-to-fine training is mandatory for memory efficiency.
- **Failure signatures**: "Floater" artifacts (SDF bias issues), texture leakage (hexa-plane query logic broken), static specularities (NDE not utilized).
- **First 3 experiments**: 1) Overfit single glossy textured object to validate high-frequency detail representation. 2) Ablate update steps—feed 4 views, render; feed 4 more, render; compare PSNR. 3) Relighting check—render under novel lighting; if object remains white/gray, material decomposition has failed.

## Open Questions the Paper Calls Out

1. **High-frequency mirror reflections**: LIRM-NDE cannot handle sharp mirror reflections because it requires reconstructing the full 3D scene from sparse observations. Evidence would be successful mirror reflection reconstruction on Stanford-ORB or similar benchmarks.

2. **Fine texture detail limitations**: Failure to recover detailed textures (e.g., small text) may be a network capacity issue rather than fundamental architectural limitation. Evidence would be scaling law analysis showing improved texture sharpness with larger models.

3. **Dynamic scene adaptation**: The progressive update mechanism was demonstrated for adding static objects but remains untested for general dynamic scenes with non-rigid motion. Evidence would be successful application on dynamic scene benchmarks with temporal consistency.

## Limitations

- Cannot handle high-frequency mirror reflections requiring reconstruction of the full reflected 3D scene
- Struggles to recover fine texture details (e.g., small text) potentially due to network capacity constraints
- Performance relies heavily on proprietary training dataset and extensive computational resources

## Confidence

- **High Confidence**: Progressive update mechanism works as described (clear implementation details and ablation in Figure 4)
- **Medium Confidence**: Hexa-plane representation provides measurable improvements over tri-plane (supported by Figure 4 but could benefit from more ablation studies)
- **Medium Confidence**: Neural directional encoding captures view-dependent effects better than SH (qualitative results in Figure 6 support this, but quantitative metrics are limited)

## Next Checks

1. **Cross-Dataset Generalization**: Test LIRM on public datasets (CO3D or NeRD) to verify performance without proprietary training data.

2. **Memory-Efficient Ablation**: Compare memory usage and reconstruction quality of LIRM's update mechanism against baseline approaches that concatenate all input views at once.

3. **Material Decomposition Verification**: Perform controlled experiments with known lighting conditions to verify changing lighting in reconstructed scenes produces physically accurate results according to predicted material parameters.