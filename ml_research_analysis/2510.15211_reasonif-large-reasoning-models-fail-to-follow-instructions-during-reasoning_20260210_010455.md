---
ver: rpa2
title: 'ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning'
arxiv_id: '2510.15211'
source_url: https://arxiv.org/abs/2510.15211
tags:
- reasoning
- instruction
- lrms
- instructions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReasonIF, a benchmark for evaluating whether
  large reasoning models follow user instructions during their reasoning traces, not
  just in final answers. The authors find that state-of-the-art LRMs exhibit significantly
  lower instruction-following compliance in reasoning traces compared to main responses,
  with the highest instruction-following score remaining below 0.25.
---

# ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning

## Quick Facts
- arXiv ID: 2510.15211
- Source URL: https://arxiv.org/abs/2510.15211
- Authors: Yongchan Kwon; Shang Zhu; Federico Bianchi; Kaitlyn Zhou; James Zou
- Reference count: 40
- One-line primary result: LRMs exhibit significantly lower instruction-following compliance in reasoning traces compared to main responses, with the highest instruction-following score remaining below 0.25

## Executive Summary
This paper introduces ReasonIF, a benchmark for evaluating whether large reasoning models follow user instructions during their reasoning traces, not just in final answers. The authors find that state-of-the-art LRMs exhibit significantly lower instruction-following compliance in reasoning traces compared to main responses, with the highest instruction-following score remaining below 0.25. They demonstrate that reasoning instruction adherence degrades further as task difficulty increases, and explore two mitigation strategies: multi-turn reasoning and reasoning instruction finetuning (RIF). RIF using synthetic data improves GPT-OSS-20B's instruction-following score from 0.11 to 0.27, showing measurable but incomplete progress toward better instruction compliance during reasoning.

## Method Summary
The paper creates a benchmark with 300 samples from 5 datasets (GSM8k, AMC, AIME, GPQA-Diamond, ARC-Challenge) evaluating 6 verifiable instruction types (multilingual, word limit, disclaimer, JSON formatting, uppercase, remove commas). Models generate reasoning traces and answers, which are programmatically checked for instruction compliance. For RIF, GPT-OSS-20B is fine-tuned on 953 synthetic samples using full-parameter SFT with learning rate 5e-6, max_length 8192, and epochs 0.25-1.0. Synthetic data is created by sampling GPT-OSS-20B on seed prompts plus random instructions, then transforming reasoning traces using rule-based approaches for simple constraints and GPT-4o for complex ones.

## Key Results
- LRMs achieve instruction-following scores (IFS) below 0.25 in reasoning traces, compared to much higher scores in main responses
- Instruction-following compliance degrades as task difficulty increases (correlation > 0.76)
- Multi-turn reasoning improves IFS by 16.6% on average
- RIF improves GPT-OSS-20B's IFS from 0.11 to 0.27, though accuracy drops slightly (0.77 → 0.73) due to distribution shift

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Reasoning Models (LRMs) fail to follow instructions in reasoning traces because their training objectives prioritize final answer correctness over process adherence.
- **Mechanism:** The paper posits that standard Reinforcement Learning (RL) pipelines with verifiable rewards optimize for the correctness of the final answer (the "main response") but fail to provide explicit rewards or penalties for the format or content of the intermediate "reasoning trace." Consequently, the model learns to solve the problem but not to constrain its internal thought process according to user instructions.
- **Core assumption:** The reasoning behavior is a result of the reward structure in the training pipeline, specifically the lack of "verifiable rewards" for reasoning trace compliance.
- **Evidence anchors:**
  - [abstract] "LRMs' poor reasoning IF performance may be attributed to their training pipeline, where reinforcement learning with verifiable reward is deployed... while little attention is paid to their reasoning traces."
  - [section 5] Mentions that RL training augments reasoning capability but ignores trace adherence.
  - [corpus] The corpus neighbor "LIFEBench" supports the general difficulty models face with explicit length instructions, a subset of the reasoning constraints tested here.
- **Break condition:** If RL training were modified to include process-based rewards for instruction adherence in the trace, this specific failure mode should theoretically decrease.

### Mechanism 2
- **Claim:** Multi-turn reasoning improves instruction compliance by forcing the model to reflect on its previous trace against explicit constraints.
- **Mechanism:** In a single-turn setting, the model generates the trace and answer atomically. In a multi-turn setting, if the trace fails a programmatic check, the model receives feedback ("Your previous output... did not follow the instructions"). This forces the model to attend to the constraint tokens in the prompt that it previously ignored, effectively conditioning the next generation on the failed constraint.
- **Core assumption:** The model has sufficient self-correction capabilities to rewrite the reasoning trace without degrading the logical path to the answer.
- **Evidence anchors:**
  - [abstract] "Multi-turn reasoning... improves IFS by 16.6% on average."
  - [section 4.2 (RQ3)] "For the multi-turn conversation... a reflection prompt is followed only when the first reasoning does not follow instructions."
  - [corpus] The corpus neighbor "Sparse Activation Editing for Reliable Instruction Following" suggests that identifying and editing instruction-relevant neurons is effective; multi-turn feedback may act as a coarse-grained activation editor for these constraints.
- **Break condition:** If the instruction requires a fundamental change in reasoning logic (e.g., "solve this math problem without using variable X") rather than just formatting, simple feedback may fail to correct the reasoning path.

### Mechanism 3
- **Claim:** Reasoning Instruction Finetuning (RIF) improves compliance by exposing the model to a distribution of reasoning traces that explicitly satisfy constraints.
- **Mechanism:** Standard LRMs see few examples of constrained reasoning. RIF uses synthetic data where reasoning traces are post-processed (e.g., by LLMs or rule-based transformations) to strictly adhere to instructions (e.g., "Uppercase only"). Fine-tuning on this data aligns the model's next-token prediction probabilities to favor compliant reasoning patterns.
- **Core assumption:** The synthetic data quality is high enough that the model learns the *format* without learning artifacts or errors introduced by the data generation process.
- **Evidence anchors:**
  - [abstract] "RIF improves the IFS of GPT-OSS-20B from 0.11 to 0.27."
  - [section 4.2 (RQ4)] Describes the "mixed rule-based and LLM-based approach" for creating synthetic compliant traces.
  - [corpus] Corpus signals regarding "Training with Pseudo-Code" suggest that structuring the training data with explicit logic/rules aids instruction following; RIF applies this to the reasoning trace specifically.
- **Break condition:** If the synthetic data transformation distorts the logical coherence of the reasoning (e.g., truncating words to meet a limit breaks the logic), the model may learn compliance at the cost of reasoning accuracy.

## Foundational Learning

- **Concept: Reasoning Trace vs. Main Response**
  - **Why needed here:** The paper's central thesis relies on distinguishing between the hidden "thinking" tokens (Trace) and the final output (Response). Standard evaluation only checks the latter; ReasonIF evaluates the former.
  - **Quick check question:** Can you identify which part of a model's output the `ginst-checker` function evaluates in the ReasonIF benchmark?

- **Concept: Verifiable Instructions**
  - **Why needed here:** To automate the evaluation of reasoning traces (without using another LLM as a judge), the paper uses instructions that can be programmatically checked (e.g., JSON format, Word count, specific phrases).
  - **Quick check question:** Why is "Respond in Chinese" a verifiable instruction, whereas "Think creatively" is not?

- **Concept: Supervised Fine-Tuning (SFT) on Synthetic Data**
  - **Why needed here:** Understanding RIF requires knowing how a base model is adapted using a new dataset. In this context, the "teacher" model or rule system creates compliant traces to teach the "student" model.
  - **Quick check question:** Why might training a model on 100% compliant reasoning traces lead to a drop in accuracy on hard reasoning benchmarks (Out-of-Distribution shift)?

## Architecture Onboarding

- **Component map:** Input (Question + Instruction) -> LRM Core (Generates <reasoning> trace and <answer>) -> Checker (Programmatic verification) -> Mitigation Layer (Optional multi-turn feedback or RIF adapter)
- **Critical path:**
  1. **Data Synthesis (for RIF):** Transforming existing reasoning traces to be instruction-compliant (using GPT-4o + rules)
  2. **Evaluation:** Running the 300-sample ReasonIF benchmark through the model
  3. **Verification:** Applying the specific `ginst-checker` for each instruction type (e.g., checking uppercase, counting words)
- **Design tradeoffs:**
  - **Accuracy vs. Compliance:** RIF improves Instruction Following Score (IFS) but may cause a slight drop in reasoning accuracy (0.77 -> 0.73) due to distribution shift
  - **Cost vs. Performance:** Multi-turn reasoning improves IFS (16.6%) but doubles/quadruples inference cost (generating tokens multiple times)
  - **Rule-based vs. LLM-based Synthesis:** Rules are perfect for simple constraints (Uppercase); LLMs are needed for semantic constraints (Multilingual) but risk hallucination
- **Failure signatures:**
  - **Format Collapse:** Models achieving 0.0 IFS on "JSON formatting" or "Uppercase only" in reasoning, despite high response IFS (Fig 3)
  - **Difficulty Degradation:** IFS dropping as task difficulty increases (correlation > 0.76 for most models), meaning the model "forgets" instructions on hard math problems
  - **Word Limit Heuristic Failure:** In multi-turn, models may satisfy "Word limit" by just stopping early or summarizing rather than reasoning efficiently
- **First 3 experiments:**
  1. **Reproduce the Gap:** Run a supported LRM (e.g., GPT-OSS-20B or DeepSeek-R1) on the ReasonIF dataset and calculate IFS for *Reasoning* vs. *Response* to confirm the <0.25 score
  2. **Ablate Difficulty:** Test the model on GSM8k (easy) vs. AIME (hard) while holding the instruction constant to observe the drop in IFS as difficulty rises
  3. **RIF Pilot:** Fine-tune a small model (e.g., GPT-OSS-20B) on ~50 synthetic samples where reasoning traces are forcibly converted to Uppercase, then verify if IFS for "Uppercase only" rises from 0.0

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does providing explicit feedback on instruction compliance in multi-turn reasoning unexpectedly improve task accuracy, even without answer-level corrections?
- **Basis in paper:** [explicit] The authors observe accuracy improvements in the second iteration of multi-turn reasoning and state, "A thorough investigation of this effect is intriguing, but it is beyond the scope of this work and is left for future research."
- **Why unresolved:** The mechanism is currently hypothesized to be related to the model's exposure to prior reasoning steps, but this has not been verified.
- **What evidence would resolve it:** Ablation studies isolating the specific factors (e.g., reasoning length, self-correction signals) that contribute to accuracy gains in multi-turn settings.

### Open Question 2
- **Question:** How does the ability to follow instructions within reasoning traces affect the performance of LRMs when embedded as components of agentic systems?
- **Basis in paper:** [explicit] In the Limitations section, the authors state, "it is crucial to understand how an LRM's reasoning IF capability affects the model performance when it is embedded as a component of an agentic system."
- **Why unresolved:** The current study evaluates models in a standard chat setting rather than within complex, multi-step agentic workflows.
- **What evidence would resolve it:** Benchmarks that measure instruction following during reasoning within tool-use or multi-step planning tasks.

### Open Question 3
- **Question:** How do LRMs handle reasoning instruction following when multiple constraints are imposed simultaneously or when instructions require subjective judgment?
- **Basis in paper:** [explicit] The authors identify the focus on "single-constraint, easy-to-verify instructions" as a limitation, noting that real-world applications often require "multiple instructions simultaneously" or subjective styling (e.g., "polish this text"), which remains "an important future topic."
- **Why unresolved:** The ReasonIF benchmark is designed strictly for single, verifiable constraints (e.g., JSON formatting, uppercase) and does not test compound or subjective requirements.
- **What evidence would resolve it:** An extension of the ReasonIF benchmark to include multi-constraint prompts and subjective instruction types evaluated by strong LLMs or human annotators.

## Limitations
- The instruction types evaluated (6 verifiable types) represent a narrow slice of possible constraints, potentially missing semantic or contextual instructions that might be more challenging
- The use of synthetic data for RIF introduces uncertainty about whether the transformations preserve logical coherence, and the paper doesn't provide detailed error analysis of where reasoning breaks down post-transformation
- The benchmark's 300 samples across 5 datasets may not capture the full diversity of reasoning tasks, and the evaluation methodology relies on programmatic checking that might miss nuanced compliance issues

## Confidence
- **High Confidence**: The fundamental observation that IFS in reasoning traces (typically <0.25) is dramatically lower than in final responses is well-supported by the systematic evaluation across multiple models and datasets
- **Medium Confidence**: The RIF approach showing improvement from 0.11 to 0.27 IFS is promising but requires more extensive validation
- **Low Confidence**: The mechanism explaining why RL training ignores reasoning traces—specifically that verifiable rewards don't incentivize trace compliance—is plausible but not definitively proven

## Next Checks
1. **Error Analysis of RIF Transformations**: Manually examine 50 post-transformation reasoning traces from the RIF dataset to verify that logical coherence is preserved and that the synthetic compliant traces are genuinely valid reasoning, not just surface-level compliant text
2. **Cross-Domain Generalization Test**: Evaluate the RIF-improved model on completely different reasoning benchmarks (e.g., MATH, Codeforces) to determine if instruction-following improvements transfer beyond the ReasonIF benchmark distribution
3. **Ablation of Multi-Turn Effectiveness**: Test whether multi-turn reasoning improves IFS specifically through constraint awareness (by including sham feedback about non-existent constraints) versus through actual reasoning correction, to isolate the mechanism driving the 16.6% improvement