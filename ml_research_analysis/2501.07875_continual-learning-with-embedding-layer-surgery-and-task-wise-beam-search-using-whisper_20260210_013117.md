---
ver: rpa2
title: Continual Learning with Embedding Layer Surgery and Task-wise Beam Search using
  Whisper
arxiv_id: '2501.07875'
source_url: https://arxiv.org/abs/2501.07875
tags:
- languages
- language
- arxiv
- token
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in multilingual ASR
  by focusing on the token embedding lookup table, which previous continual learning
  methods overlooked. The authors propose Embedding Layer Surgery, which creates separate
  copies of token embeddings for each new language, and Task-wise Beam Search, which
  maintains multiple language hypotheses during decoding to improve LID and ASR accuracy.
---

# Continual Learning with Embedding Layer Surgery and Task-wise Beam Search using Whisper

## Quick Facts
- arXiv ID: 2501.07875
- Source URL: https://arxiv.org/abs/2501.07875
- Reference count: 0
- Key outcome: Proposed method reduces average WER of pre-trained languages from 14.2% to 11.9% compared to Experience Replay

## Executive Summary
This paper addresses catastrophic forgetting in multilingual ASR by focusing on the token embedding lookup table, which previous continual learning methods overlooked. The authors propose Embedding Layer Surgery, which creates separate copies of token embeddings for each new language, and Task-wise Beam Search, which maintains multiple language hypotheses during decoding to improve LID and ASR accuracy. Evaluated on Whisper adapted to 10 unseen languages from Common Voice, their method reduces the average WER of pre-trained languages from 14.2% to 11.9% compared to Experience Replay, without compromising performance on new languages.

## Method Summary
The proposed approach consists of two main components: Embedding Layer Surgery and Task-wise Beam Search. Embedding Layer Surgery creates separate copies of token embeddings for each new language task, preventing interference between languages during continual learning. Task-wise Beam Search maintains multiple language hypotheses during decoding, improving language identification and overall ASR accuracy by selecting the most probable hypothesis across languages.

## Key Results
- Average WER reduction from 14.2% to 11.9% on pre-trained languages compared to Experience Replay
- Method prevents catastrophic forgetting while maintaining performance on new languages
- Effective evaluation on 10 unseen languages from Common Voice dataset

## Why This Works (Mechanism)
The approach addresses catastrophic forgetting by isolating token embeddings for each language task, preventing interference during training. Task-wise Beam Search improves language identification by maintaining multiple hypotheses, allowing the model to select the most appropriate language-specific representation during decoding.

## Foundational Learning
- Catastrophic forgetting: Neural networks forget previously learned tasks when trained on new ones; critical to understand for continual learning approaches
- Language identification (LID): Automatic detection of language in speech; necessary for multilingual ASR systems
- Beam search decoding: Heuristic search algorithm for finding most likely sequence; quick check: understand how multiple hypotheses improve LID
- Token embedding lookup tables: Store word representations; why needed: these are often overlooked in continual learning
- Experience replay: Rehearsal method storing past examples; quick check: baseline for comparing forgetting prevention methods

## Architecture Onboarding
**Component map:** Whisper ASR -> Embedding Layer Surgery -> Task-wise Beam Search -> Multilingual ASR
**Critical path:** Input speech → ASR encoder → Task-specific embeddings → Beam search decoding → Output transcript
**Design tradeoffs:** Separate embedding copies increase memory usage but prevent forgetting; beam search adds computation but improves accuracy
**Failure signatures:** Performance degradation on pre-trained languages indicates forgetting; poor LID accuracy suggests beam search issues
**First experiments:** 1) Test embedding isolation on single language pairs, 2) Evaluate beam search impact on decoding latency, 3) Measure memory overhead for multiple embedding copies

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to more diverse language families or different ASR architectures beyond Whisper
- Memory requirements could become prohibitive with many languages or extremely large vocabularies
- Computational overhead of Task-wise Beam Search during inference not quantified

## Confidence
**High confidence:** The technical implementation of Embedding Layer Surgery and Task-wise Beam Search is clearly described and appears sound
**Medium confidence:** The claim that focusing on token embeddings addresses catastrophic forgetting more effectively than previous methods, based on limited comparisons
**Low confidence:** Generalizability of results to other ASR architectures and very large-scale multilingual scenarios

## Next Checks
1. Test the approach on a more diverse set of languages spanning different language families and scripts to verify robustness
2. Measure and report the computational overhead of Task-wise Beam Search during inference, including latency and memory usage
3. Compare against additional continual learning baselines (EWC, SI, etc.) to establish relative performance advantages