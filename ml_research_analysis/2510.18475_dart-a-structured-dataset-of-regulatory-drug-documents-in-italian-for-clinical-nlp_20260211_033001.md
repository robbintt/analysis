---
ver: rpa2
title: 'DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical
  NLP'
arxiv_id: '2510.18475'
source_url: https://arxiv.org/abs/2510.18475
tags:
- drug
- regulatory
- clinical
- dart
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DART introduces the first structured dataset of Italian Summary
  of Product Characteristics (RCP), addressing the scarcity of regulatory text resources
  in non-English languages. Built via automated retrieval, semantic segmentation,
  and LLM-based summarization, it provides structured access to key pharmacological
  domains such as indications, ADRs, and DDIs.
---

# DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP

## Quick Facts
- arXiv ID: 2510.18475
- Source URL: https://arxiv.org/abs/2510.18475
- Reference count: 37
- Primary result: LLM-enhanced DDI detection recall improves from 0.229 to 0.843 using structured Italian RCP summaries

## Executive Summary
DART introduces the first structured dataset of Italian Summary of Product Characteristics (RCP), addressing the scarcity of regulatory text resources in non-English languages. Built via automated retrieval, semantic segmentation, and LLM-based summarization, it provides structured access to key pharmacological domains such as indications, ADRs, and DDIs. The dataset includes over 16,000 RCPs and 95 million tokens. Validation through an LLM-driven DDI checker shows strong performance: models enhanced with DART summaries (e.g., LLaMA-3.1-8B + DART) achieve 0.843 recall, significantly outperforming standalone LLMs and web-based tools, demonstrating the value of structured regulatory data for clinical NLP.

## Method Summary
DART constructs a structured dataset of Italian RCPs through automated retrieval from AIFA using undocumented REST APIs, followed by text extraction with PyMuPDF, regex-based semantic segmentation of standardized sections (04.1-04.8), and LLM-based summarization using LLaMA-3.1-405B via NVIDIA NIM API. The pipeline filters out 5,473 documents lacking required sections, resulting in 16,029 RCPs totaling 95M+ tokens. Validation uses an LLM-as-judge DDI checker with few-shot prompting, achieving 0.843 recall on a 100-example test set.

## Key Results
- Dataset of 16,029 Italian RCPs with 95M+ tokens from AIFA regulatory documents
- DDI checker with LLaMA-3.1-8B + DART summaries achieves 0.843 recall, 0.786 accuracy
- Manual review shows 95% factual consistency in LLM-generated summaries
- Significantly outperforms standalone LLMs (Recall 0.229) and web-based tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured regulatory summaries improve LLM-based DDI detection recall significantly
- Mechanism: Condensing heterogeneous RCP sections into 450-word summaries reduces semantic dispersion, enabling LLMs to focus on clinically relevant pharmacological concepts without exceeding context windows
- Core assumption: Key interaction information survives the summarization compression without critical loss
- Evidence anchors:
  - [abstract] "models enhanced with DART summaries (e.g., LLaMA-3.1-8B + DART) achieve 0.843 recall, significantly outperforming standalone LLMs"
  - [section 4.1 Table 4] LLaMA-3.1-8B + DART: Recall 0.843 vs standalone Recall 0.229
  - [corpus] MUDI dataset (arxiv 2506.01478) supports multimodal DDI understanding but lacks Italian regulatory focus; corpus evidence for Italian-specific regulatory grounding is limited
- Break condition: Summarization may omit rare or context-specific drug interactions, as acknowledged in paper limitations

### Mechanism 2
- Claim: Standardized RCP section headers enable reliable semantic segmentation via regex
- Mechanism: Italian RCPs follow EU-mandated numbering conventions (04.1 Therapeutic Indications, 04.5 Interactions, 04.8 Undesirable Effects), allowing pattern-matching extraction with high accuracy
- Core assumption: AIFA documents consistently follow regulatory formatting standards
- Evidence anchors:
  - [abstract] "semantic segmentation of regulatory sections"
  - [section 3.1.2] "A robust regular expression was designed to recognize both the numerical and textual components of the headers... over 97% of expected sections were correctly identified and segmented"
  - [corpus] Weak corpus evidence for Italian-specific segmentation benchmarks; this appears novel
- Break condition: Non-standard formatting, OCR failures, or scanned PDFs without text layers cause extraction failures (4.1% excluded)

### Mechanism 3
- Claim: Low-temperature LLM decoding with few-shot prompting preserves factual accuracy in clinical summarization
- Mechanism: Temperature 0.2 reduces stochasticity, while few-shot examples anchor output to regulatory tone and domain terminology, minimizing hallucination
- Core assumption: The summarization model has sufficient pharmacological knowledge to accurately condense technical content
- Evidence anchors:
  - [section 3.3] "A manual review of 100 generated summaries showed a high degree of factual consistency (95%) and minimal hallucination"
  - [section 3.3] "LLaMA 3.1–405B... with a low-temperature setting (0.2) to ensure high consistency"
  - [corpus] Case-Based Reasoning paper (arxiv 2505.23034) supports context-enhanced LLM approaches for DDI but doesn't address temperature/summarization specifically
- Break condition: Prompt design sensitivity; expert validation protocol still under development

## Foundational Learning

- Concept: Summary of Product Characteristics (SmPC/RCP) structure
  - Why needed here: DART's entire pipeline depends on understanding that RCPs have standardized sections (04.x clinical, 05.x pharmacological, 06.x pharmaceutical) with predictable numbering
  - Quick check question: Can you identify which RCP section contains drug-drug interaction information without reading the full document?

- Concept: LLM temperature and its effect on factual consistency
  - Why needed here: The summarization module relies on low-temperature decoding (0.2) to minimize hallucination—understanding why this matters is critical for reproducing results
  - Quick check question: What happens to output variance when temperature approaches 1.0 vs 0.1?

- Concept: Drug-drug interaction severity classification
  - Why needed here: The DDI checker outputs four levels (Absent, Minor, Moderate, Major); understanding clinical implications is essential for evaluating system performance
  - Quick check question: Why would a pharmacovigilance system prioritize recall over precision for DDI detection?

## Architecture Onboarding

- Component map:
  - AIFA Spider -> Text Extractor -> Summarizer -> DDI Checker

- Critical path: RCP retrieval → Section segmentation → Summarization → DDI inference. The summarization step is the bottleneck that enables lightweight models to compete with proprietary systems.

- Design tradeoffs:
  - Excluded 5,473 documents (25.45%) to maintain structural quality
  - Binary classification consolidation (Minor/Moderate/Major → Interaction) trades granularity for tool compatibility
  - Section 05.0 (Pharmacological Properties) removed due to missing data

- Failure signatures:
  - PDFs without embedded text layers (4.1%)—requires OCR fallback (under development)
  - Non-standard section formatting causing regex misses
  - API endpoint changes (unofficial AIFA APIs may break without notice)

- First 3 experiments:
  1. Reproduce DDI detection results: Run LLaMA-3.1-8B + DART on the 100-example test set, verify ~0.84 recall
  2. Ablate summarization: Compare DDI performance with full RCP text vs. summaries to quantify compression benefit
  3. Test temperature sensitivity: Generate summaries at temp 0.0, 0.2, 0.5, 1.0 and measure factual consistency on 20 samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implementation of the planned OCR modules affect the retrieval success rate for the 4.1% of RCPs previously excluded due to missing embedded text layers?
- Basis in paper: [explicit] Section 3.1.2 states that OCR modules are "currently under development" to handle scanned documents excluded from the current version.
- Why unresolved: The current pipeline strictly filters out rasterized PDFs, creating a gap in drug coverage that automated text recognition aims to solve.
- What evidence would resolve it: Performance metrics (e.g., Word Error Rate) and the successful integration rate of the excluded documents following OCR deployment.

### Open Question 2
- Question: To what extent does the proposed expert-based validation protocol reduce hallucinations compared to the current manual review of LLM-generated summaries?
- Basis in paper: [explicit] Section 3.3 notes that while a manual review showed 95% consistency, an "expert-based validation protocol is currently under development."
- Why unresolved: Current validation relies on limited manual review, whereas clinical deployment requires rigorous expert verification to mitigate risks in sensitive domains like drug safety.
- What evidence would resolve it: A comparative study measuring factual consistency and hallucination frequency between the current LLM outputs and those verified by the expert protocol.

### Open Question 3
- Question: Does the high recall performance of the DDI checker persist when evaluated on a larger, more diverse test set beyond the 100-example benchmark?
- Basis in paper: [inferred] The validation in Section 4.1 relies on a "manually annotated test set comprising 100 examples," which is a small sample for a dataset of 16,000+ documents.
- Why unresolved: A small test set may not capture the variance of rare drug-drug interactions or the full linguistic complexity of the corpus, limiting the generalizability of the 0.843 recall score.
- What evidence would resolve it: Evaluation results on a significantly expanded test set (e.g., 1,000+ pairs) showing consistent precision and recall metrics across different therapeutic classes.

### Open Question 4
- Question: How can DART be operationally integrated with observational databases like the National Pharmacovigilance Network (RNF) to validate predicted interactions against real-world evidence?
- Basis in paper: [explicit] Section 2 claims the dataset fosters a "bidirectional loop" between automated reasoning and real-world evidence, but the paper only demonstrates the regulatory text extraction side.
- Why unresolved: The mechanism for linking structured RCP data to spontaneous reporting systems remains conceptual rather than implemented in the current work.
- What evidence would resolve it: A working pipeline that successfully cross-references DART-extracted interactions with RNF adverse event reports to confirm clinical relevance.

## Limitations

- Pipeline depends on unofficial AIFA API endpoints that may change without notice, creating a single point of failure for data collection
- Manual review of 100 summaries shows 95% factual consistency, but sample size is relatively small for 16,000+ document corpus
- DDI evaluation based on 100 manually annotated examples, representing a limited validation sample for such a large dataset

## Confidence

- **High Confidence**: The core technical achievement of creating the first structured Italian RCP dataset, the pipeline architecture (extraction → segmentation → summarization), and the performance improvement of DART-enhanced models over standalone LLMs
- **Medium Confidence**: The generalizability of results to other non-English regulatory domains, the robustness of regex-based segmentation across all AIFA documents, and the sufficiency of 450-word summaries for capturing all clinically relevant information
- **Low Confidence**: The long-term sustainability of the data collection pipeline given reliance on undocumented APIs, and the completeness of DDI coverage given the acknowledged limitations of summarization compression

## Next Checks

1. **Reproduce the 0.843 recall result**: Run the LLaMA-3.1-8B + DART pipeline on the 100-example test set using the provided implementation, verifying the published performance metrics match

2. **Ablate the summarization component**: Compare DDI detection performance using full RCP text versus DART summaries on the same test set to quantify the exact benefit of the 450-word compression

3. **Stress test API reliability**: Attempt to reproduce the data collection pipeline with current AIFA endpoints, documenting any changes in API structure, rate limiting, or data availability that would affect reproducibility