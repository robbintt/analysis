---
ver: rpa2
title: 'MEL: Legal Spanish Language Model'
arxiv_id: '2501.16011'
source_url: https://arxiv.org/abs/2501.16011
tags:
- legal
- spanish
- language
- texts
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed MEL, a Spanish legal language model based
  on XLM-RoBERTa-large, pre-trained on Spanish legal documents including BOE and congressional
  records. MEL outperforms existing multilingual and domain-specific models on Spanish
  legal text classification tasks.
---

# MEL: Legal Spanish Language Model

## Quick Facts
- **arXiv ID:** 2501.16011
- **Source URL:** https://arxiv.org/abs/2501.16011
- **Reference count:** 23
- **Primary result:** MEL achieves F1 of 0.8025 on multilingual Eurlex dataset, outperforming Legal-XLM-RoBERTa-Large (0.7933) and other baselines

## Executive Summary
MEL is a Spanish legal language model developed through continued pre-training of XLM-RoBERTa-large on a curated corpus of Spanish legal documents including BOE and congressional records. The model demonstrates superior performance on Spanish legal text classification tasks compared to both general multilingual models and existing legal-specific models. MEL achieves state-of-the-art results on the multilingual Eurlex dataset (F1 0.8025) and a private multiclass legal corpus (F1 0.9260), while also showing improved sample efficiency during fine-tuning.

## Method Summary
MEL was developed through continued masked language modeling pre-training on a 92.7 GB Spanish legal corpus consisting of BOE documents, congressional records, court rulings, and other legal texts. The pre-training process used whole-word masking and ran for 2 epochs on an NVIDIA A100 80GB GPU, taking approximately 13.9 days. The model was fine-tuned on downstream classification tasks using standard Huggingface Trainer with specific hyperparameters including learning rate of 1e-4, AdamW optimizer, and batch size of 64 (achieved through gradient accumulation).

## Key Results
- MEL achieves F1 score of 0.8025 on the multilingual Eurlex dataset, outperforming Legal-XLM-RoBERTa-Large (0.7933) and XLM-RoBERTa-Large (0.7962)
- On a private multiclass legal corpus, MEL achieves F1 of 0.9260, significantly outperforming all baseline models
- MEL demonstrates superior sample efficiency, achieving F1 of 0.8812 with limited training data versus 0.7803 for Legal-XLM-RoBERTa-Large

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continued pre-training on Spanish legal texts reweights token embeddings and attention patterns toward domain-specific terminology
- **Evidence:** MEL outperforms Legal-XLM-RoBERTa-Large on Multieurlex (F1 0.8025 vs 0.7933)
- **Core assumption:** Base model representations are transferable to legal domain
- **Break condition:** May reduce cross-lingual transfer capabilities

### Mechanism 2
- **Claim:** Rigorous Spanish-only corpus cleaning yields higher-quality training data than multilingual scraping
- **Evidence:** Language filtering at 95% confidence and manual handling of autonomous community bulletins
- **Core assumption:** Noise reduction correlates with downstream performance
- **Break condition:** May remove relevant multilingual legal terminology

### Mechanism 3
- **Claim:** Domain-adapted pre-training improves sample efficiency during fine-tuning
- **Evidence:** MEL achieves higher F1 at epoch 1 and strong performance with limited data samples
- **Core assumption:** MLM pre-training transfers to classification tasks
- **Break condition:** May not transfer to structurally dissimilar tasks

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - **Why needed:** MEL uses whole-word masking as its pre-training objective
  - **Quick check:** Given "El **[MASK]** del Estado," what contextual signal helps predict "Boletín"?

- **Concept: Domain Adaptation via Continued Pre-training**
  - **Why needed:** MEL extends XLM-RoBERTa-large rather than training from scratch
  - **Quick check:** Why use continued pre-training rather than direct fine-tuning?

- **Concept: Encoder-Only Architectures for Classification**
  - **Why needed:** MEL argues encoder-only models offer better quality-size tradeoffs for classification
  - **Quick check:** What output representation from an encoder would you use for multilabel classification?

## Architecture Onboarding

- **Component map:** XLM-RoBERTa-large (24 transformer layers, 1024 hidden dim, 16 attention heads) -> MLM head -> Task-specific classifiers
- **Critical path:** Corpus collection (BOE PDFs, congressional records) -> OCR with PDFPlumber -> Preprocessing (language filtering, whitespace cleaning, 512-token chunking) -> Continued pre-training (2 epochs, MLM objective) -> Fine-tuning on downstream tasks
- **Design tradeoffs:** Monolingual specialization enables deeper domain adaptation but sacrifices cross-lingual transfer; encoder-only architecture provides better quality-size tradeoff but no generative capabilities
- **Failure signatures:** Training/eval loss diverging (check learning rate warmup, data corruption); poor few-shot performance (verify corpus language filtering); low F1 on specific legal subtopics (check corpus coverage)
- **First 3 experiments:** 1) Fine-tune MEL on Multieurlex Spanish split to verify F1 ≈ 0.80; 2) Pre-train on 50% of corpus to measure degradation vs. full model; 3) Evaluate on token classification if annotated data available

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MEL perform on token or span classification tasks such as Named Entity Recognition (NER) within the Spanish legal domain? The paper explicitly states that token or span classification tasks remain unevaluated due to lack of annotated texts.
- **Open Question 2:** Can MEL effectively transfer its domain knowledge to generative downstream tasks such as summarization or question answering? The paper lists testing MEL on diverse downstream tasks such as question answering and summarization as future work.
- **Open Question 3:** How does MEL compare to modern generative Large Language Models (LLMs) on Spanish legal text classification tasks? The paper discusses LLMs but excludes them from experimental comparison, arguing encoder-only models offer better trade-off for non-generative tasks.

## Limitations
- The private multiclass corpus (9 classes, 2389 training samples) cannot be independently verified, making performance assessment difficult
- Comparison against Legal-XLM-RoBERTa-Large uses different model sizes, potentially confounding architectural differences with pre-training effects
- The paper doesn't provide ablation studies on corpus size to determine if smaller, higher-quality corpora would suffice

## Confidence
- **High confidence:** MEL outperforms general multilingual models (XLM-RoBERTa-large, RoBERTalex) on Spanish legal classification tasks, supported by public benchmark results
- **Medium confidence:** MEL's superiority over Legal-XLM-RoBERTa-Large, though comparisons involve different model scales and private corpus results cannot be verified
- **Low confidence:** Sample efficiency claims, demonstrated only on private corpus with limited data splits and no ablation studies on pre-training duration

## Next Checks
1. **Independent reproduction on public benchmarks:** Re-implement MEL's pre-training procedure and evaluate on the publicly available Spanish portion of Multieurlex to verify the 80.25% F1 claim
2. **Controlled ablation study:** Pre-train MEL for 1 epoch versus 2 epochs on the same corpus, and compare performance on Multieurlex to quantify computational investment
3. **Cross-lingual transfer evaluation:** Test MEL's performance on multilingual legal classification tasks (e.g., English or French legal texts) to assess whether monolingual specialization limits cross-lingual capabilities