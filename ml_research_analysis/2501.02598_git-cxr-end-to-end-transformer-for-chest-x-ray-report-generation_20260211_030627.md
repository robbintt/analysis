---
ver: rpa2
title: 'GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation'
arxiv_id: '2501.02598'
source_url: https://arxiv.org/abs/2501.02598
tags:
- reports
- report
- learning
- curriculum
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GIT-CXR, an end-to-end transformer approach
  for generating radiology reports from chest X-ray images. The method employs curriculum
  learning based on report length, incorporates context, uses multi-view images, and
  adds a multi-label classification head.
---

# GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation

## Quick Facts
- arXiv ID: 2501.02598
- Source URL: https://arxiv.org/abs/2501.02598
- Reference count: 34
- End-to-end transformer for chest X-ray report generation achieves state-of-the-art clinical accuracy (F1-macro, F1-micro, F1-examples-averaged) and NLG metric METEOR on MIMIC-CXR-JPG

## Executive Summary
This paper introduces GIT-CXR, an end-to-end transformer approach for generating radiology reports from chest X-ray images. The method employs curriculum learning based on report length, incorporates patient context, uses multi-view images, and adds a multi-label classification head. Experiments on the MIMIC-CXR-JPG dataset show state-of-the-art results on clinical accuracy metrics and the NLG metric METEOR, with competitive performance on BLEU and ROUGE-L. Ablation studies confirm that all components contribute to performance, with curriculum learning having the most significant impact.

## Method Summary
GIT-CXR uses a transformer-based architecture with a ViT image encoder and autoregressive decoder. The model processes multi-view chest X-ray images (AP, PA, LATERAL, LL) through a shared encoder, adding temporal embeddings to differentiate views. Patient context (indication and history) is concatenated and fed to the decoder as prefix tokens. A curriculum learning strategy samples training data by report length, progressively exposing the model to longer reports. An auxiliary multi-label classification head predicts 14 pathologies from the encoder output.

## Key Results
- State-of-the-art F1-examples-averaged (0.506), F1-macro (0.418), and F1-micro (0.565) on MIMIC-CXR-JPG
- Best METEOR score (0.287) among end-to-end approaches
- Curriculum learning shows significant improvement for reports >75 tokens
- Multi-view input improves F1-micro from 0.500 to 0.538 (single-view) and from 0.536 to 0.565 (multi-view)

## Why This Works (Mechanism)

### Mechanism 1: Curriculum Learning by Report Length
Training with progressively longer reports improves generation quality, particularly for clinically complex long reports. The dataset is split into bins by target report length, with sampling weights favoring shorter reports early and longer bins later. This allows the model to first learn basic sentence structures before tackling longer, multi-finding reports.

### Mechanism 2: Multi-View Image Embedding Fusion
Combining multiple X-ray views (AP, PA, LATERAL, LL) in a single embedding improves clinical accuracy over single-view inputs. Each image passes through the shared vision encoder separately; distinct temporal embeddings differentiate them. Embeddings are concatenated before decoding, giving the text decoder access to complementary anatomical perspectives.

### Mechanism 3: Clinical Context Conditioning
Providing patient history and indication as context improves report relevance and factual completeness. The "indication" and "history" fields are concatenated, tokenized, and prepended to the target report in the decoder input. This conditions generation on prior clinical intent.

## Foundational Learning

- **Vision Transformer (ViT) encoders**: Understand attention over spatial patches to debug what visual regions influence generation. Can you explain how a ViT splits an image into patches and processes them as a sequence?
- **Autoregressive text decoding with cross-attention**: The report is generated token-by-token with cross-attention to image embeddings. What happens to error accumulation if you generate long sequences without curriculum or scheduled sampling?
- **Multi-label classification with class imbalance**: The auxiliary classification head predicts 14 pathologies with severe class imbalance. Why would macro-F1 be much lower than micro-F1 when rare classes have poor recall?

## Architecture Onboarding

- **Component map**: Images -> ViT encoder -> per-view embeddings (+ temporal embeddings) -> concatenate -> cross-attention -> decoder -> report
- **Critical path**: 1) Image(s) → encoder → per-view embeddings → (+ temporal embeddings) → concatenate; 2) Context tokens → decoder prefix; 3) Concatenated image embedding → cross-attention in decoder; 4) Decoder generates report token-by-token
- **Design tradeoffs**: CL + classifier incompatibility (short reports skew pathology distribution early); MV adds ~2× encoder compute with modest gains; report truncation at 192 tokens clips long tail
- **Failure signatures**: Short, generic reports ("no acute findings"); high precision, low recall on rare pathologies; BLEU drops sharply for long reports
- **First 3 experiments**: 1) Baseline replication: Train GIT-CXR (SV+C) without curriculum learning; 2) Curriculum learning ablation: Add CL (10 bins, 25% sampling) and compare METEOR and F1 on long reports; 3) Multi-view with limited data: Train MV variant on subset with ≥2 views

## Open Questions the Paper Calls Out

### Open Question 1
How can curriculum learning (CL) and the auxiliary multi-label classification head be reconciled to function synergistically rather than detrimentally? The authors observe that the classification head is "not compatible with the curriculum learning method," suggesting a modified training schema or loss function that allows the model to utilize both CL and the classifier to achieve scores exceeding the current GIT-CXR (MV+C+CL) baseline.

### Open Question 2
Can a domain-specific curriculum learning strategy outperform the current text-length-based approach? The authors acknowledge using a simple NLP-inspired method and state: "a different curriculum learning method designed specifically for medical imaging could improve the results even further." A curriculum based on visual features or pathology density that yields higher clinical accuracy than the length-based curriculum.

### Open Question 3
Does the effectiveness of the proposed GIT-CXR architecture and curriculum learning generalize to smaller or structurally different radiology datasets? The authors list as a limitation: "First, we only used a single dataset for testing our proposed method." Experimental results demonstrating maintained performance gains on alternative public radiology datasets.

## Limitations

- Curriculum learning and auxiliary classification head are incompatible, limiting multi-task benefits
- All experiments conducted on single institutional dataset (MIMIC-CXR-JPG), raising generalizability concerns
- Multi-view improvements assume access to paired views, which may not be available in real-world deployment

## Confidence

- **High Confidence**: Multi-view image fusion provides measurable improvements in clinical accuracy metrics (F1-micro, F1-macro) over single-view inputs
- **Medium Confidence**: Curriculum learning by report length effectively addresses degradation in generation quality for long reports
- **Medium Confidence**: Clinical context conditioning (history + indication) improves both NLG and clinical accuracy metrics
- **Low Confidence**: The auxiliary multi-label classification head adds clinically meaningful signal to the generation process

## Next Checks

1. **Curriculum Learning on Long Reports**: Train GIT-CXR (SV+C) with and without curriculum learning on a subset of MIMIC-CXR-JPG containing only reports >100 tokens. Measure METEOR and F1-micro to confirm the ~3-5 pp improvement claimed for long reports.

2. **Multi-View Dependency on View Availability**: Filter MIMIC-CXR-JPG to studies with exactly 2 views (AP/PA + Lateral/LL). Train both single-view and multi-view variants on this subset. Compare performance to determine if multi-view gains persist when view availability is consistent.

3. **Context Ablation with Incomplete Context**: Randomly mask 50% of context fields (indication/history) in MIMIC-CXR-JPG. Train GIT-CXR with context on this corrupted dataset. Compare to a version trained without context to quantify the sensitivity to context quality.