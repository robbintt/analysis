---
ver: rpa2
title: Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations
  of Explainable AI
arxiv_id: '2505.21589'
source_url: https://arxiv.org/abs/2505.21589
tags:
- gaze
- dataset
- direction
- image
- animal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a fundamental limitation in current explainable
  AI (XAI) methods: their reliance on pixel-based saliency attribution, which fails
  to capture perceptual ambiguity in visual data. The authors introduce a novel dataset
  of animal-based optical illusions where one animal is hidden within another, annotated
  with gaze direction, eye coordinates, and bounding boxes.'
---

# Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI

## Quick Facts
- arXiv ID: 2505.21589
- Source URL: https://arxiv.org/abs/2505.21589
- Authors: Carina Newen; Luca Hinkamp; Maria Ntonti; Emmanuel Müller
- Reference count: 6
- Key outcome: Demonstrates that pixel-based XAI methods fail on ambiguous images while concept-based features (gaze direction, eye coordinates) improve accuracy by over 20% across ResNet and VGG architectures

## Executive Summary
This paper addresses a fundamental limitation in current explainable AI (XAI) methods: their reliance on pixel-based saliency attribution, which fails to capture perceptual ambiguity in visual data. The authors introduce a novel dataset of animal-based optical illusions where one animal is hidden within another, annotated with gaze direction, eye coordinates, and bounding boxes. They demonstrate that existing XAI methods like Grad-CAM, Integrated Gradients, PipNet, and ACE fail to provide meaningful explanations for these ambiguous images due to their pixel-level focus. The paper introduces gaze direction and eye coordinates as generalizable visual concepts that significantly improve model accuracy on ambiguous data, with improvements of over 20% across multiple architectures (ResNet and VGG) when these features are included. The dataset and source code are made publicly available, providing a foundation for future research on concept-based explainability in ambiguous visual settings.

## Method Summary
The authors create Ambivision, a dataset of synthetic animal-based optical illusions where two animals share overlapping visual features but differ in gaze direction and eye position. Models are trained on four dataset variants: raw images, images with gaze arrows, images with eye markers, and images with random markings. Pre-trained ImageNet classifiers (ResNet18/34/50, VGG13/16) are fine-tuned using Adam optimizer with learning rates 1e-4, 1e-5, and 5e-6 for up to 1000 epochs. Gaze vectors are incorporated by drawing arrows directly onto training images. Performance is evaluated using classification accuracy where either animal class is accepted as correct, and XAI methods (Grad-CAM, LIME, PipNet, ACE) are applied to visualize feature importance.

## Key Results
- Pixel-based XAI methods (Grad-CAM, Integrated Gradients, PipNet, ACE) produce nearly identical explanations for both animal classes in ambiguous images
- Incorporating gaze direction and eye coordinates as explicit features improves classification accuracy by over 20% across ResNet and VGG architectures
- ACE and similar automatic concept extraction methods fail to discover gaze direction and eye position as important concepts because they rely on pixel-level segmentation
- Random annotation control shows no improvement, confirming that only specific concept features (gaze, eye) drive the accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel-based saliency attribution methods fail to distinguish between overlapping classes when visual features are shared, producing near-identical explanations for different predictions.
- Mechanism: Grad-CAM, Integrated Gradients, and similar methods compute gradient-based importance over spatial regions. When two animals share overlapping pixel regions (e.g., cheetah fur and eagle feathers rendered similarly), these methods highlight the same areas regardless of which class is being explained, because the attribution is computed from the same input pixels.
- Core assumption: Ambiguous images contain shared pixel regions that contribute to multiple valid interpretations; the decision boundary depends on abstract interpretation rather than spatially distinct features.
- Evidence anchors:
  - [abstract] "existing XAI methods like Grad-CAM, Integrated Gradients, PipNet, and ACE fail to provide meaningful explanations for these ambiguous images due to their pixel-level focus"
  - [Page 2] "If you were to use any current XAI algorithm to generate explanations of why it is a rabbit or a duck, the explanations could look exactly the same"
  - [corpus] Related work on visual illusions in vision models exists (e.g., "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases"), but corpus evidence does not directly validate this specific failure mode for XAI methods.

### Mechanism 2
- Claim: Incorporating abstract visual concepts (gaze direction vectors, eye coordinates) as explicit input features improves classification accuracy on ambiguous data by over 20%.
- Mechanism: The authors inject gaze direction arrows or eye markers directly into training images. The network learns to associate these concept-level annotations with class labels. During inference, the presence (or inferred presence) of these cues disambiguates overlapping classes that would otherwise be confused.
- Core assumption: Gaze direction and eye position are generalizable across the animal domain and correlate consistently with class identity in the dataset.
- Evidence anchors:
  - [abstract] "improvements of over 20% across multiple architectures (ResNet and VGG) when these features are included"
  - [Page 7, Figure 7] Shows accuracy curves comparing "Direction," "No Direction," "Only Eyes," and "Random" annotation conditions across ResNet18/34/50 and VGG13/16
  - [corpus] No direct corpus validation for gaze-as-concept on ambiguous images; this appears novel.

### Mechanism 3
- Claim: Automatic concept extraction methods (e.g., ACE) that rely on pixel-level segmentation fail to discover abstract concepts like gaze direction.
- Mechanism: ACE segments images into regions, clusters similar segments, and assigns importance scores. Because gaze direction is a relational concept (a vector, not a region), it does not correspond to any single segment and is not extracted.
- Core assumption: Meaningful concepts for ambiguous visual reasoning are relational or abstract rather than region-based.
- Evidence anchors:
  - [Page 4] "ACE fails to find the two general concepts that we suggest work well: gaze direction and the eyes. We argue that this is because, again, ACE employs segmentation on a pixel level"
  - [Page 4, Figure 6] Shows ACE extracting meaningless segments (dots on wings) rather than gaze or eye concepts
  - [corpus] Corpus does not provide direct evidence for or against ACE's failure on ambiguous data.

## Foundational Learning

- Concept: **Perceptual ambiguity in visual data**
  - Why needed here: Understanding that a single image can support multiple valid interpretations (e.g., rabbit vs. duck) is prerequisite to grasping why pixel-based methods fail and why concept-level features matter.
  - Quick check question: Can you explain why two humans might label the same image differently without either being wrong?

- Concept: **Concept-based vs. pixel-based explainability**
  - Why needed here: The paper's central argument is that explanations should reference abstract concepts (gaze, eye position) rather than pixel importance. Understanding this distinction is necessary to evaluate the proposed approach.
  - Quick check question: If a model's explanation highlights the same pixels for class A and class B, what information is missing from that explanation?

- Concept: **Gaze direction as a semantic feature**
  - Why needed here: The paper operationalizes gaze direction as a normalized 2D vector [dx, dy] combined with eye coordinates. Understanding how this is computed and annotated is necessary to reproduce or extend the work.
  - Quick check question: Given an eye position at [100, 150] and a head direction vector [0.6, 0.8], how would you compute a gaze vector with length 50 pixels?

## Architecture Onboarding

- Component map:
  Dataset variants (raw, with arrows, with eyes, random) -> Pretrained ImageNet models (ResNet18/34/50, VGG13/16) -> Fine-tuning with ADAM -> Evaluation with concept features and XAI methods

- Critical path:
  1. Load dataset variant (with or without direction arrows/eye markers)
  2. Fine-tune pretrained ResNet or VGG on Ambivision
  3. Evaluate accuracy on ambiguous classification (both classes allowed as correct, or single class depending on experiment)
  4. Apply XAI method (Grad-CAM, LIME, PipNet, ACE) to visualize what features the model uses

- Design tradeoffs:
  - **Injection method**: Authors tried concatenating gaze coordinates at the softmax layer (via MLP) but found no improvement; drawing arrows directly into images worked. Assumption: visual integration allows the convolutional layers to learn spatial relationships.
  - **Dataset size**: ~200 images is small; the authors argue optical illusions are inherently difficult to generate. Tradeoff: depth of annotation vs. scale.
  - **Generalization**: Eye position alone sometimes outperformed gaze direction; the dataset ensures unique (eye, gaze) pairs but not unique gaze alone.

- Failure signatures:
  - Accuracy stuck near random (50% for binary ambiguous choice) when training without concept annotations
  - XAI methods highlighting shared regions identically for both classes
  - ACE extracting low-semantic segments (e.g., wing dots) instead of gaze/eye concepts
  - Random annotation control shows no improvement, confirming concept specificity

- First 3 experiments:
  1. **Baseline check**: Train ResNet18 on raw Ambivision images (no annotations) and measure accuracy. Expect near-chance performance on ambiguous pairs.
  2. **Concept ablation**: Train separate models with (a) gaze arrows, (b) eye markers only, (c) random markings. Compare accuracy curves to isolate which concept contributes most.
  3. **XAI failure visualization**: Apply Grad-CAM and LIME to the baseline model. Verify that explanations for class A and class B are near-identical, confirming the pixel-attribution limitation claimed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can concept-based XAI methods be developed that capture abstract perceptual concepts like gaze direction without relying on pixel-level segmentation?
- Basis in paper: [explicit] The authors explicitly state that ACE and similar automatic concept-based methods "fail to find the two general concepts that we suggest work well: gaze direction and the eyes" because "ACE employs segmentation on a pixel level and derives concepts from there."
- Why unresolved: Current concept extraction relies on clustering pixel-based segmentations, which cannot capture abstract relational concepts like direction vectors or perceptual organization.
- What evidence would resolve it: Development of a concept extraction method that can automatically identify gaze direction and eye position as important concepts without explicit annotation.

### Open Question 2
- Question: What determines perceptual preference when neither gaze nor eye coordinates serve as distinguishing features in ambiguous images?
- Basis in paper: [explicit] The authors generated images where "the gazes are completely shared and do not help in distinguishing which is the correct feature" and ask: "Is there a more prominent answer on which one is seen with a higher likelihood, and what is it dependent on? The outer one? Do we prefer the color black? Is it the animal whose head 'looks complete'?"
- Why unresolved: The study intentionally constrained ambiguity to be resolvable via gaze/eye position; images lacking these cues remain unexplored.
- What evidence would resolve it: Systematic evaluation of classification preferences on illusion variants controlling for figure-ground, color, completeness, and rotation.

### Open Question 3
- Question: Do the performance gains from gaze and eye annotations generalize to real-world ambiguous visual data beyond synthetic optical illusions?
- Basis in paper: [inferred] The dataset is entirely synthetic (ChatGPT-generated), and the authors acknowledge this as a limitation. Safety-critical domains like autonomous driving and medical diagnostics are mentioned as motivation but not evaluated.
- Why unresolved: All experiments use artificially constructed illusions with controlled ambiguity; no validation on natural ambiguous imagery.
- What evidence would resolve it: Evaluation of gaze/eye concept integration on real-world ambiguous datasets (e.g., medical imaging with ambiguous lesions, camouflaged animals, or adversarial examples).

## Limitations

- Dataset scale: Only ~200 synthetic images, limiting generalizability and statistical power
- Generalization uncertainty: Effectiveness of gaze/eye concepts on non-animal or non-illusion data remains unproven
- Safety domain gap: No evaluation on real-world ambiguous visual data in safety-critical applications

## Confidence

- **High Confidence**: The failure of pixel-based XAI methods on ambiguous images is well-demonstrated through multiple experiments and visualizations. The 20%+ accuracy improvements with concept features are reproducible given the dataset.
- **Medium Confidence**: The generalizability of gaze direction and eye position as semantic concepts across different illusion types and animal categories. The paper provides strong internal evidence but limited external validation.
- **Low Confidence**: Claims about ACE's fundamental inability to extract abstract concepts are based on a single segmentation method. Alternative concept extraction approaches might succeed where ACE fails.

## Next Checks

1. **Dataset Expansion**: Generate and evaluate model performance on a 10× larger Ambivision dataset to assess whether improvements scale with data diversity.
2. **Cross-Domain Generalization**: Test gaze/eye concept effectiveness on non-animal optical illusions (e.g., Rubin vase, geometric illusions) to validate cross-category applicability.
3. **Alternative Concept Methods**: Apply modern concept-based XAI approaches (e.g., ConceptSHAP, ACE variants with different segmentation strategies) to determine if gaze direction can be extracted without direct annotation.