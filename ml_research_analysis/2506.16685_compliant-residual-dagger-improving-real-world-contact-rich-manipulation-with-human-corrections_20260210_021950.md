---
ver: rpa2
title: 'Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation
  with Human Corrections'
arxiv_id: '2506.16685'
source_url: https://arxiv.org/abs/2506.16685
tags:
- policy
- correction
- data
- residual
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving real-world robot
  policies for contact-rich manipulation tasks through human corrections. It introduces
  Compliant Residual DAgger (CR-DAgger), which consists of two key components: a Compliant
  Intervention Interface that enables humans to provide smooth, delta-based corrections
  without interrupting robot execution, and a Compliant Residual Policy that learns
  from corrections while incorporating force feedback.'
---

# Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections

## Quick Facts
- arXiv ID: 2506.16685
- Source URL: https://arxiv.org/abs/2506.16685
- Reference count: 33
- Key outcome: Improves contact-rich manipulation success rates by 64% using human corrections

## Executive Summary
This paper introduces Compliant Residual DAgger (CR-DAgger), a system that improves real-world robot policies for contact-rich manipulation tasks through human corrections. The approach combines a Compliant Intervention Interface that enables smooth delta-based corrections during robot execution with a Compliant Residual Policy that learns from these corrections while incorporating force feedback. The method demonstrates significant performance improvements across four contact-rich tasks - book flipping, belt assembly, cable routing, and gear insertion - while requiring minimal correction data (50-100 demonstrations).

## Method Summary
CR-DAgger addresses the challenge of improving real-world robot policies for contact-rich manipulation tasks through human corrections. The system consists of two key components: a Compliant Intervention Interface that enables humans to provide smooth, delta-based corrections without interrupting robot execution, and a Compliant Residual Policy that learns from corrections while incorporating force feedback. The interface allows operators to physically interact with the robot and provide delta corrections that are learned by a residual policy, which is added to the original policy to improve performance. The method demonstrates significant improvements on four contact-rich tasks while requiring minimal correction data.

## Key Results
- Achieved 64% improvement in base policy success rates across four contact-rich tasks
- Demonstrated effective learning with only 50-100 human correction demonstrations
- Outperformed both retraining-from-scratch and finetuning approaches
- Successfully handled tasks including book flipping, belt assembly, cable routing, and gear insertion

## Why This Works (Mechanism)
The system works by enabling continuous human intervention during robot execution through a compliant interface that doesn't interrupt task flow. Humans provide delta-based corrections that capture local adjustments needed for successful task completion. These corrections, combined with force feedback, train a residual policy that learns to handle contact-rich scenarios where the base policy struggles. The force feedback integration allows the system to adapt to different physical interactions and generalize across similar tasks with varying object properties.

## Foundational Learning
- **Delta-based corrections**: Learning local adjustments rather than complete trajectories - needed for efficient learning from minimal demonstrations, quick check: compare learning efficiency with full trajectory corrections
- **Force feedback integration**: Using tactile information to guide policy corrections - needed for handling contact-rich scenarios, quick check: evaluate performance with and without force sensing
- **Residual policy learning**: Adding learned corrections to existing policies rather than replacing them - needed for preserving base policy capabilities while improving weaknesses, quick check: measure performance degradation when residual is removed
- **Human-in-the-loop training**: Real-time human intervention during execution - needed for capturing expert knowledge in dynamic scenarios, quick check: compare with offline demonstration learning
- **Impedance control**: Enabling safe human-robot physical interaction - needed for practical implementation, quick check: verify safety across different intervention forces
- **Trajectory smoothing**: Ensuring smooth robot motion during corrections - needed for maintaining task execution quality, quick check: measure trajectory jerk with and without smoothing

## Architecture Onboarding
- **Component map**: Human operator -> Compliant Intervention Interface -> Force-torque sensor -> Residual Policy Network -> Combined Policy -> Robot execution
- **Critical path**: Human correction → Delta calculation → Residual policy training → Policy combination → Task execution
- **Design tradeoffs**: Real-time intervention capability vs. system complexity, minimal correction data vs. learning completeness, force feedback fidelity vs. computational overhead
- **Failure signatures**: Poor calibration leading to unstable corrections, insufficient force feedback causing task failures, residual policy overfitting to specific correction patterns
- **First experiments**: 1) Baseline task success rate without corrections, 2) Single-task improvement with corrections, 3) Cross-task generalization validation

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The Compliant Intervention Interface requires precise calibration between human operator and robot impedance parameters
- The system assumes stationary human operators, limiting applicability to mobile or distributed supervision
- Claims about delta-based corrections generalizing better than full trajectory demonstrations need more systematic ablation studies
- The method may be overfitting to specific force patterns observed during correction, limiting true generalization

## Confidence
- **High**: The compliant residual policy structure and its integration with force feedback is technically sound
- **Medium**: Generalization claims across tasks and objects, particularly for the belt assembly task
- **Medium**: The assertion that 50-100 correction demonstrations suffice for significant improvement
- **Low**: The claim that delta-based corrections inherently outperform other correction modalities without comprehensive comparison

## Next Checks
1. Conduct ablation studies varying human intervention frequency and magnitude to quantify the trade-off between correction effort and performance gain
2. Test policy robustness to operator skill level and intervention style variations through experiments with multiple untrained human operators
3. Evaluate zero-shot generalization to novel object geometries and material properties not present in the correction dataset