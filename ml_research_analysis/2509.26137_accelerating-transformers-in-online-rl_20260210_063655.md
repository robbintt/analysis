---
ver: rpa2
title: Accelerating Transformers in Online RL
arxiv_id: '2509.26137'
source_url: https://arxiv.org/abs/2509.26137
tags:
- transformer
- training
- stage
- algorithm
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to accelerate transformer training
  in online reinforcement learning by using a simpler, more stable model (the "Accelerator")
  to generate trajectories for pretraining the transformer. The Accelerator interacts
  with the environment independently while simultaneously training the transformer
  through behavior cloning.
---

# Accelerating Transformers in Online RL

## Quick Facts
- arXiv ID: 2509.26137
- Source URL: https://arxiv.org/abs/2509.26137
- Reference count: 40
- Primary result: Stabilizes and accelerates transformer training in online RL by 2x using a two-stage "Accelerator" framework, reducing replay buffer needs to 10-20k

## Executive Summary
This paper addresses the instability of training transformer-based policies in online reinforcement learning by introducing a two-stage training framework. The method uses a simpler, more stable "Accelerator" model to generate trajectories for pretraining the transformer via behavior cloning. This pretraining stage provides a stable initialization before switching to standard online RL fine-tuning. The approach enables stable transformer training, reduces required training time by up to a factor of two, and decreases replay buffer size from the typical 1 million to just 10-20 thousand transitions.

## Method Summary
The method employs a two-stage training process. In Stage 1, an "Accelerator" policy (typically an MLP or LSTM) interacts with the environment using standard RL algorithms like TD3 or SAC. During this interaction, state-action trajectories are collected and used to pretrain a transformer policy via behavior cloning. In Stage 2, the pretrained transformer takes over as the actor, and online RL continues with the transformer as the policy network. The key insight is that the accelerator provides stable, expert-like trajectories for initialization, allowing the transformer to avoid the instability typically associated with training from scratch in online RL settings.

## Key Results
- Achieves stable transformer training in online RL where vanilla transformers fail
- Reduces training time by up to a factor of two compared to standard approaches
- Decreases required replay buffer size from 1 million to 10-20 thousand transitions
- Works effectively for both state-based and image-based tasks in MDP and POMDP settings

## Why This Works (Mechanism)

### Mechanism 1: Stable Pretraining via Expert Trajectories
The simpler, more stable "Accelerator" policy generates demonstration trajectories that serve as expert data for behavior cloning. This bypasses the instability of training transformers directly with online RL from scratch by providing a stable, supervised learning signal during the critical initialization phase.

### Mechanism 2: Two-Stage Training for Transfer
Stage 1 establishes foundational weights through stable BC pretraining, while Stage 2 switches to online RL for task-specific optimization. This separation allows the transformer to refine its policy based on environmental rewards and correct distributional shifts.

### Mechanism 3: Reduced Memory Requirements
Because the transformer has learned a reasonable policy from demonstrations, it requires less exploration during fine-tuning. The replay buffer only needs to store recent, relevant experience for refinement rather than a vast, diverse history for initial learning.

## Foundational Learning

**Behavior Cloning (BC)**
- Why needed: Core supervised learning algorithm used in first stage to train transformer from Accelerator's trajectories
- Quick check: Can you explain how MSE loss can train a policy to mimic an expert's actions?

**Actor-Critic Reinforcement Learning (TD3/SAC)**
- Why needed: Foundational RL algorithms used to train the Accelerator and re-used for transformer's fine-tuning
- Quick check: What are the roles of "actor" and "critic" networks in actor-critic algorithms?

**Transformer Architecture as Policy**
- Why needed: Target model being trained; understanding sequential nature and context window is crucial
- Quick check: How does a transformer's attention mechanism process past states to predict next action?

## Architecture Onboarding

**Component map:**
- Accelerator Policy (π_ϕ) -> Accelerator Critic (Q_ϕ) -> Replay Buffer (B) -> Trajectory Buffer (T) -> Transformer Policy (π_θ)

**Critical path:**
1. Train Accelerator with TD3, store transitions in B
2. Build state-action sequences from π_ϕ interactions, store in T
3. Train Transformer on T via Behavior Cloning (Stage 1)
4. Switch to pretrained π_θ as actor (Stage 2)
5. Continue training using TD3 with π_θ as actor and pre-trained Q_ϕ as critic

**Design tradeoffs:**
- Accelerator complexity: Simpler models train faster but provide weaker expert signal
- Optional gradient ascent: Can provide boost but not guaranteed, adds complexity
- Trajectory buffer size: Needs to be large enough for stable BC batch but smaller than typical replay buffer

**Failure signatures:**
- Unstable Accelerator: Poor-quality data in T leads to bad transformer policy
- Catastrophic forgetting in Stage 2: High learning rate or large distributional shift causes unlearning
- Small buffer failure in Stage 2: Buffer < 10k may lack diverse data for effective fine-tuning

**First 3 experiments:**
1. Vanilla Transformer baseline: Train transformer from scratch using online TD3
2. Stage 1 only (ablation): Train Accelerator and pretrain transformer without Stage 2
3. Full pipeline with varying buffer sizes: Test 10k, 15k, 20k, and 1M replay buffers

## Open Questions the Paper Calls Out

**Open Question 1:** How can an automatic switching mechanism be designed to optimally terminate the pretraining stage without manual monitoring? The authors note the absence of automatic switching as a limitation requiring human monitoring.

**Open Question 2:** Under what specific conditions does the additional gradient ascent on the critic function improve acceleration stability? Results show mixed effectiveness across environments, with PullCube benefiting while PushCube showed no improvement.

**Open Question 3:** Why does the method exhibit higher training stability in image-based environments compared to their vector-based counterparts? This counter-intuitive result (vector-based PullCube less stable than image-based version) remains unexplained.

## Limitations
- No automatic mechanism for stage transition, requiring manual monitoring
- Optional gradient ascent technique shows inconsistent benefits across environments
- Implementation details like exact transition triggers and Transformer positional encoding are underspecified

## Confidence
- **High:** Core claim of stable transformer training through pretraining is well-supported by experimental results
- **Medium:** Claims about reduced training time and smaller replay buffers are supported but could benefit from more extensive ablation studies
- **Low:** Contribution of optional gradient ascent and sensitivity to accelerator architecture choices remain uncertain

## Next Checks
1. Implement and test multiple stage transition triggers (fixed steps vs reward plateau) to quantify their impact on final performance
2. Systematically compare MLP vs LSTM accelerators across MDP and POMDP environments to determine architecture impact
3. Conduct finer-grained analysis of replay buffer sizes (5k, 10k, 15k, 20k, 50k) to establish minimum viable buffer size for stable fine-tuning