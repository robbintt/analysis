---
ver: rpa2
title: 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal
  LLMs in Multi-Turn Dialogues'
arxiv_id: '2510.17722'
source_url: https://arxiv.org/abs/2510.17722
tags:
- video
- arxiv
- answer
- wang
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MT-Video-Bench is a new benchmark for evaluating multimodal large\
  \ language models (MLLMs) in multi-turn video dialogues. It tests six core capabilities\u2014\
  object reference, memory recall, content summary, answer refusal, topic shifting,\
  \ and proactive interaction\u2014across 1,000 dialogues spanning diverse video domains."
---

# MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues

## Quick Facts
- arXiv ID: 2510.17722
- Source URL: https://arxiv.org/abs/2510.17722
- Reference count: 37
- 20 state-of-the-art models tested; top performer achieves only 76.95% overall accuracy

## Executive Summary
MT-Video-Bench introduces a comprehensive evaluation framework for multimodal large language models (MLLMs) in multi-turn video dialogues. The benchmark tests six core capabilities—object reference, memory recall, content summary, answer refusal, topic shifting, and proactive interaction—across 1,000 dialogues spanning diverse video domains. Results reveal that even leading models struggle with interactivity tasks and exhibit significant performance drops as scene complexity increases, highlighting substantial room for improvement in long-range temporal reasoning and adaptive conversational engagement.

## Method Summary
MT-Video-Bench employs a semi-automatic pipeline: videos are processed using PySceneDetect for scene splitting, Gemini-2.5-Flash for caption-based scene merging, and 2 FPS frame extraction with quality filtering. Object detection uses YOLOv11, with cross-frame tracking through an object memory bank. Multi-turn dialogues are generated using Gemini-2.5-Pro with task-specific prompts, followed by dual-annotator human verification. Evaluation uses checklist-based assessment with 5 yes/no questions per QA pair, judged by Gemini-2.5-Flash, achieving high human agreement (Spearman 95.25%).

## Key Results
- Top-performing models achieve only 76.95% overall accuracy across all tasks
- Answer refusal capability remains a universal bottleneck at 57.74% even for leading models
- Performance degrades by 13% as scene count increases from 1-5 to 20+ scenes
- Significant performance gaps between golden context and self-predicted context settings reveal cascade error propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Golden context evaluation isolates model reasoning capability from cascading error propagation
- Mechanism: By providing curated dialogue history rather than model self-predicted context, the benchmark separates intrinsic reasoning ability from error accumulation across turns
- Core assumption: Models' self-predicted context contains compounding errors; golden context accurately represents ideal dialogue history
- Evidence anchors: [abstract] performance discrepancies and limitations; [section 4.3] significant gap between golden and self-predicted settings reveals cascade effect

### Mechanism 2
- Claim: Checklist-based decomposition enables granular, automatable assessment of complex multi-turn responses
- Mechanism: Each QA pair generates 5 yes/no verification questions through atomic fact decomposition with fact reversal; automated evaluator judges responses against these checklists
- Core assumption: Complex answers can be meaningfully decomposed into atomic verifiable facts; automated evaluators reliably approximate human judgment
- Evidence anchors: [section 3.4] Gemini-2.5-Flash constructs 5 yes/no questions per QA pair; [section 4.5] high level of agreement with human evaluators

### Mechanism 3
- Claim: Scene complexity scaling reveals long-range temporal dependency limitations in current architectures
- Mechanism: Systematically varying scene counts (1-5 to 20+ scenes) exposes how models degrade when maintaining coherent representations across temporally distant segments
- Core assumption: Scene count is a valid proxy for temporal reasoning complexity; object continuity across scenes represents real-world cross-scene demands
- Evidence anchors: [abstract] significant performance drops as scene complexity increases; [section 4.3] consistent performance decline for top-tier models suggests struggle with long-range temporal dependencies

## Foundational Learning

- Concept: **Cross-modal grounding in multi-turn context**
  - Why needed here: Object Reference requires tracking pronoun antecedents across dialogue history and video frames simultaneously
  - Quick check question: Given "the man in the blue shirt" mentioned in Round 2, can the model correctly identify whom "he" refers to in Round 5?

- Concept: **Context window management and error propagation**
  - Why needed here: Cascade effect shows early errors corrupt downstream reasoning; understanding context accumulation and degradation is essential
  - Quick check question: If a model hallucinates an object in Round 3, how does this affect its Round 6 response when summarizing all discussed entities?

- Concept: **Interactive dialogue policies (refusal, topic shifting, proactivity)**
  - Why needed here: Interactivity tasks require conversational judgment—knowing when to refuse unanswerable queries, adapt to topic changes, and proactively engage
  - Quick check question: When a user abruptly shifts from video content to an unrelated topic (e.g., Apollo 11), should the model answer the new topic or redirect to video content?

## Architecture Onboarding

- Component map: Video Input → Frame Extraction → Object Detection + Caption Generation → Object Memory Bank → Scene Merging → Multi-turn Dialogue Generation → Human Verification → Checklist Generation → Automated Judging → Accuracy Aggregation

- Critical path:
  1. Frame selection quality directly impacts object detection accuracy and downstream scene coherence
  2. Human verification ensures task-capability alignment—misaligned dialogues invalidate evaluation
  3. Context setting (golden vs. self-predicted) determines whether measuring reasoning or robustness

- Design tradeoffs:
  - 128 frames at 720p balances temporal coverage and compute cost; smaller models saturate while larger models benefit from higher resolutions
  - Checklist-based vs. open-ended evaluation: Checklists enable automation but may miss nuanced response quality
  - Single-scene vs. cross-scene dialogues: Cross-scene increases ecological validity but introduces scene-merging complexity

- Failure signatures:
  - Visual-temporal hallucinations: Model fabricates events not in video (e.g., claiming a person drinks juice when video ends before tasting)
  - Inter-turn interference: Model conflates Round 1 and Round 2 descriptions
  - Conversational passivity: Model fails to proactively engage with open-ended, curiosity-prompting questions

- First 3 experiments:
  1. Baseline evaluation: Run target model on MT-Video-Bench with golden context, 128 frames at 720p; report per-task accuracy to identify weakest capability
  2. Context sensitivity analysis: Compare golden vs. self-predicted context vs. no context for 50-dialogue subset; quantify cascade effect magnitude
  3. Scene complexity scaling: Evaluate on dialogues grouped by scene count (1-5, 6-10, 11-20, 20+); plot degradation curve to determine where temporal reasoning breaks down

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MLLMs be improved to recognize knowledge boundaries and refuse unanswerable queries without hallucinating, given that Answer Refusal remains a universal bottleneck even for top-tier models?
- Basis in paper: Authors identify AR as a "universal bottleneck" where "even the strongest models struggle to effectively recognize knowledge boundaries"
- Why unresolved: Current models lack reliable self-assessment mechanisms for distinguishing answerable from unanswerable video-grounded queries
- What evidence would resolve it: Demonstrated improvement in AR accuracy through new training objectives or architectural modifications

### Open Question 2
- Question: What mechanisms can mitigate the "cascade effect" where early dialogue errors accumulate and lead to conversation collapse under self-predicted context?
- Basis in paper: Authors observe "a severe error propagation issue" with significant gaps between golden and self-predicted context settings
- Why unresolved: Models lack robust error recovery and self-correction capabilities in extended multi-turn interactions
- What evidence would resolve it: Reduced performance gap between golden and self-predicted contexts, or dialogue-level consistency metrics showing error containment

### Open Question 3
- Question: How well do dialogues generated by Gemini-2.5-Pro generalize to real human-AI video conversations across diverse user populations?
- Basis in paper: The semi-automated pipeline relies entirely on Gemini-2.5-Pro for multi-turn dialogue generation, which may embed model-specific conversational patterns
- Why unresolved: No validation comparing synthetic dialogues to authentic human-AI video conversations
- What evidence would resolve it: Comparative analysis between benchmark performance and performance on crowdsourced human-generated dialogues

## Limitations
- Benchmark relies on Gemini-2.5-Flash for both dialogue generation and automated evaluation, creating potential circularity concerns
- Human verification quality control protocols remain underspecified, limiting reproducibility of the curation process
- Scene complexity as a proxy for real-world temporal reasoning demands lacks validation for semantic coherence equivalence

## Confidence
- High confidence in core findings about multi-turn reasoning limitations and cascade effects
- Medium confidence in checklist-based evaluation validity (high human agreement but may miss nuanced quality)
- Low confidence in ecological validity of scene complexity as a proxy for real-world temporal reasoning demands

## Next Checks
1. Conduct ablation study varying the ratio of positive to negative checklist questions to test automated evaluator robustness against question phrasing bias
2. Implement cross-validation using multiple automated evaluators (different model families) to assess consistency of checklist scoring
3. Perform qualitative analysis of top-performing model responses on hardest dialogues (multi-scene, multi-turn) to determine whether failures stem from genuine reasoning deficits or artifacts of the scene-merging process