---
ver: rpa2
title: 'Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context
  Learning'
arxiv_id: '2511.05532'
source_url: https://arxiv.org/abs/2511.05532
tags:
- harmful
- spam
- text
- benign
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using in-context learning with foundation models
  for personalized harmful content detection across toxicity, spam, and negative sentiment.
  It unifies binary, multi-class, and multi-label tasks, enabling lightweight personalization
  via simple prompt-based interventions without retraining.
---

# Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning

## Quick Facts
- **arXiv ID:** 2511.05532
- **Source URL:** https://arxiv.org/abs/2511.05532
- **Reference count:** 40
- **Primary result:** Personalized harmful content detection via ICL with strong cross-task generalization and lightweight user adaptations.

## Executive Summary
This paper introduces a unified In-Context Learning (ICL) framework for detecting harmful content (toxicity, spam, negative sentiment) across binary, multi-class, and multi-label tasks. The key innovation is enabling lightweight personalization through prompt-based interventions—no retraining required. By injecting user-provided examples or definitions into the prompt context, the model dynamically adjusts its behavior, effectively "unlearning" or "expanding" concepts. Experiments show the approach matches or exceeds fine-tuned baselines, particularly when augmented with label definitions or rationales, and demonstrates strong generalization to noisy, real-world data.

## Method Summary
The method leverages foundation models (Llama-3.1-8B, Mistral-7B, Qwen2-7B) with ICL to perform unified harmful content detection. The system uses a retriever to sample demonstrations from a training pool, which are then assembled into prompts alongside task descriptions (Level 1: concise; Level 2: with label definitions). The model predicts labels conditioned on this prompt context. Personalization is achieved by injecting user-defined examples or definitions into the prompt, enabling dynamic "blocking" or "unblocking" of categories without updating model weights. Rationale augmentation further improves robustness on noisy data.

## Key Results
- ICL matches or exceeds fine-tuned baselines on benchmark datasets (F1 ~0.97 on TextDetox with 128 shots).
- Random retrieval outperforms semantic similarity at high shot counts (F1 0.918 at 192 shots).
- Adding label definitions (Level 2) significantly reduces false positive rates in multi-task settings.
- Rationale augmentation drops FPR from 0.224 to 0.019 on wild Mastodon data.
- Personalization succeeds with as few as one user example or definition.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Personalization via "contextual unlearning" or concept expansion by injecting user-provided examples into the prompt context.
- **Mechanism:** The model conditions its predictions on the immediate prompt context ($D_k$) rather than fixed weights. By introducing re-labeled examples (e.g., labeling "toxic" text as "benign") or new definitions, the model shifts its decision boundary for the specific inference session without updating global parameters.
- **Core assumption:** The foundation model's instruction-following capability is strong enough to override its pre-trained associations when presented with explicit, in-context contradictions.
- **Evidence anchors:**
  - [Section 7.2] Shows that re-labeling toxic shots as benign successfully "unblocks" the category with high success rates, particularly when using Level 2 prompt definitions.
  - [Section 7.1] Demonstrates that adding just 1-2 examples of a new harmful category stabilizes and improves detection, even without prior training on that category.
  - [Corpus] Weak direct evidence; neighbor papers focus on translation or general moderation, lacking specific data on "contextual unlearning" via re-labeling.
- **Break condition:** If the pre-trained model has a very strong prior (high $k_1$ general knowledge) and the personalization signal is sparse (low $k_2$), the model may revert to its default behavior, failing to respect the user's specific overrides.

### Mechanism 2
- **Claim:** Demonstration diversity (via Random retrieval) outperforms semantic similarity as the number of shots increases.
- **Mechanism:** At low shot counts, semantic retrieval helps by providing relevant prototypes. However, at high shot counts (e.g., 192), random sampling provides a more diverse coverage of the input space, preventing the model from overfitting to the specific nuances of the query and enabling better generalization across heterogeneous harmful content types.
- **Core assumption:** The beneficial effect of demonstration diversity scales with the number of shots, eventually surpassing the benefit of query-relevance.
- **Evidence anchors:**
  - [Section 5.2] Figure 6 and Table 3 show Random retrieval achieving the highest F1 (0.918) at 192 shots, outperforming Balanced Semantic strategies.
  - [Section 6.2] Confirms that Random retrieval stabilizes performance at higher shot counts in multi-class settings.
  - [Corpus] No direct contradiction found in neighbors; consistent with general ICL literature cited (e.g., retrieval strategies).
- **Break condition:** If the dataset is extremely sparse or the query is out-of-distribution, random retrieval at low shot counts may fail to provide relevant prototypes, causing performance to collapse compared to semantic retrieval.

### Mechanism 3
- **Claim:** Augmenting prompts with explicit rationales (reasoning chains) reduces false positive rates in noisy, real-world data.
- **Mechanism:** In noisy environments, raw labels may be ambiguous or inconsistent. Providing a natural language rationale forces the model to align its prediction with a semantic explanation, filtering out spurious correlations and reducing the likelihood of over-prediction (false alarms).
- **Core assumption:** The generated or curated rationales are high-quality and accurately reflect the decision logic required for the task.
- **Evidence anchors:**
  - [Section 8.4] Table 5 shows that adding reasons to wild data demos drops the False Positive Rate from 0.224 to 0.019 and boosts precision from 0.714 to 0.966.
  - [Abstract] States that "augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data."
  - [Corpus] Not explicitly confirmed in neighbor papers; corpus focuses on detection models rather than rationale-augmented prompting.
- **Break condition:** If the rationales are hallucinated, inconsistent, or contradictory, they may confuse the model, leading to degraded performance or increased inference latency without accuracy gains.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** This is the engine of the proposed system. Unlike fine-tuning, ICL adapts the model dynamically using the prompt ($t, D_k$), which is the only way to achieve the lightweight personalization described in the paper.
  - **Quick check question:** Can you explain how the model computes $y_{query}$ differently in a 0-shot vs. a 48-shot setup?

- **Concept: False Positive Rate (FPR) vs. Recall**
  - **Why needed here:** The paper explicitly identifies high FPR as a critical failure mode in multi-task settings (Section 5.2) and uses it as a primary metric to evaluate the success of "unblocking" (Section 7.2) and rationale augmentation (Section 8.4).
  - **Quick check question:** Why might a model have high Recall but poor utility for a user, and how does reducing FPR help?

- **Concept: Prompt Engineering (Level 1 vs. Level 2)**
  - **Why needed here:** The paper demonstrates that simply changing the task description granularity (adding definitions) significantly impacts performance, specifically reducing FPR.
  - **Quick check question:** What is the structural difference between a Level 1 and Level 2 prompt description, and when should you prefer one over the other?

## Architecture Onboarding

- **Component map:** Raw text query ($x_{query}$) -> Retriever (selects $D_k$ from Training Pool) -> Prompt Builder (assembles prompt with Task Description and demonstrations) -> Inference Engine (vLLM with foundation model) -> Output Parser (extracts structured labels).
- **Critical path:** The **Prompt Builder** is the most critical component. A failure here (e.g., missing label definitions, incorrect formatting of demonstrations) directly breaks the ICL mechanism, causing the model to revert to default behavior or hallucinate outputs.
- **Design tradeoffs:**
  - **Random vs. Semantic Retrieval:** Random is computationally cheaper and better at high shot counts; Semantic is better for low-resource, specific queries.
  - **Reason Augmentation:** Massively improves robustness (Precision 0.96+) but increases prompt token count and latency.
- **Failure signatures:**
  - **Catastrophic Generalization:** Model over-predicts "harmful" on benign inputs (High FPR). *Fix:* Switch to Level 2 prompts or add rationales.
  - **Personalization Drift:** Model ignores user overrides (unblocking). *Fix:* Increase $k_2$ (personal examples) relative to $k_1$ (general knowledge) or use explicit definitions.
  - **Confusion in Multi-Class:** "Negative" sentiment often confused with "Benign" or "Toxic". *Fix:* Use Fine-grained Balanced Semantic retrieval.
- **First 3 experiments:**
  1. **Baseline Reproduction (Multi-Task Binary):** Implement the Random Retrieval strategy with 48 and 192 shots using Llama-3.1-8B on the benchmark dataset. Verify if F1 > 0.91 and identify the FPR gap compared to single-task models.
  2. **Personalization Stress Test (Unblocking):** Take the best baseline model and attempt to "unblock" Toxicity by injecting 8 toxic examples labeled as "benign" ($k_2=8$). Measure if the success rate exceeds 0.90 as claimed in Section 7.2.
  3. **Wild Data Robustness:** Evaluate the baseline on the Mastodon dataset without rationales. Then, switch to Rationale-Augmented prompts. Verify if the FPR drops significantly (target: < 0.02) as per Section 8.4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality, faithfulness, and potential bias of automatically generated rationales impact the robustness of harmful content detection in In-Context Learning (ICL)?
- Basis: [explicit] The discussion section notes that the "evaluation of reason-augmented prompts focuses solely on classification outcomes without systematically assessing the quality, faithfulness, or potential biases of the generated explanations."
- Why unresolved: While the paper demonstrates that adding rationales improves robustness on wild data, it treats the rationale generation (via DeepSeek-V3) as a black box, leaving the sensitivity of the model to poor or misleading rationales unknown.
- What evidence would resolve it: Ablation studies substituting high-quality human rationales with low-quality or contradictory generated rationales to measure the resulting degradation in classification F1-scores.

### Open Question 2
- Question: Does the ICL framework’s ability to generalize and personalize degrade when the taxonomy of harm is expanded to include more fine-grained categories (e.g., specific threats vs. insults) or distinct concepts like misinformation?
- Basis: [explicit] The authors state: "Future research should incorporate a broader taxonomy... [and] explore finer-grained subdivisions within each category."
- Why unresolved: The study limits evaluation to a tripartite scheme (toxicity, spam, negative sentiment), and it is unclear if the prompt context window or model capacity can handle the semantic ambiguity of a significantly larger, multi-class label space.
- What evidence would resolve it: Experiments applying the unified ICL framework to datasets with high-cardinality label sets, such as the full Jigsaw taxonomy or misinformation benchmarks, comparing performance against the tripartite baseline.

### Open Question 3
- Question: Is the personalization mechanism (blocking/unblocking) effective in multilingual or code-switching contexts where definitions of harm may differ culturally?
- Basis: [explicit] The limitations section explicitly notes: "our experiments are restricted to English, leaving open questions about the effectiveness of ICL in multilingual or code-switching scenarios."
- Why unresolved: The semantic retrieval strategies and prompt definitions are validated only on English datasets (TextDetox English subset, UCI, SST2), leaving the cross-lingual transfer of personalized definitions untested.
- What evidence would resolve it: Evaluation of the personalized ICL setup on multilingual versions of the TextDetox dataset or code-switching social media data to measure if user-defined blocking rules transfer across language boundaries.

## Limitations
- **Hallucination Risk in Rationale Augmentation:** The paper relies on DeepSeek-V3 to generate rationales for noisy real-world data, but does not validate their accuracy or consistency, introducing potential for model bias or hallucination.
- **Contextual Unlearning Generalization:** The "unblocking" mechanism shows high success rates, but the paper does not explore whether this effect generalizes to more subtle or context-dependent overrides, such as cultural variations in harmful content definitions.
- **Resource Constraints for High-Shot ICL:** While the paper demonstrates strong performance with 192 shots, the token limits of current models (e.g., 32K for Llama-3.1-8B) may restrict the feasibility of this approach for longer texts or higher shot counts in real-world deployments.

## Confidence
- **High Confidence:** Single-task and multi-task ICL performance on benchmark datasets. The paper provides clear experimental results with strong F1 scores (e.g., 0.97 on TextDetox) and direct comparisons to fine-tuned baselines.
- **Medium Confidence:** Personalization via contextual unlearning and the effectiveness of Random retrieval at high shot counts. While the results are promising, the mechanisms rely on assumptions about the model's instruction-following capability and demonstration diversity that may not hold in all contexts.
- **Low Confidence:** Rationale augmentation for wild data robustness. The paper demonstrates a significant drop in FPR, but the reliance on externally generated rationales introduces potential for hallucination or inconsistency, which is not fully validated.

## Next Checks
1. **Rationale Quality Audit:** Manually review a subset of the rationales generated by DeepSeek-V3 to assess their accuracy and consistency. If hallucinations are detected, explore alternative methods for rationale generation or validation.
2. **Cross-Domain Unblocking Test:** Test the unblocking mechanism on a dataset with more subtle or context-dependent overrides (e.g., cultural or regional variations in harmful content definitions). Measure whether the model can respect these nuanced user preferences without explicit re-labeling.
3. **Token Efficiency Analysis:** Evaluate the impact of increasing prompt token count (e.g., Level 2 descriptions, rationales) on inference latency and cost. Identify the optimal balance between prompt richness and computational efficiency for real-world deployment.