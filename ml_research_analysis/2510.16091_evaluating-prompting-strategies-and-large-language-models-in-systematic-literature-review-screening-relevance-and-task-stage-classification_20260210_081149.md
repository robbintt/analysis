---
ver: rpa2
title: 'Evaluating Prompting Strategies and Large Language Models in Systematic Literature
  Review Screening: Relevance and Task-Stage Classification'
arxiv_id: '2510.16091'
source_url: https://arxiv.org/abs/2510.16091
tags:
- prompting
- screening
- prompt
- precision
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how prompting strategies and large language
  models (LLMs) influence automated screening in systematic literature reviews. Six
  models (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku,
  Llama-4-Maverick) are tested under five prompt types (zero-shot, few-shot, chain-of-thought,
  CoT-few-shot, self-reflection) across relevance classification and six SLR stage
  tasks.
---

# Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification

## Quick Facts
- arXiv ID: 2510.16091
- Source URL: https://arxiv.org/abs/2510.16091
- Reference count: 40
- Primary result: Chain-of-thought with few-shot prompting balances precision-recall most reliably in automated SLR screening; zero-shot maximizes recall for sensitivity-first passes.

## Executive Summary
This study investigates how prompting strategies and large language models influence automated screening in systematic literature reviews (SLRs). Six prominent LLMs—GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku, and Llama-4-Maverick—are evaluated under five prompt types across relevance classification and six SLR stage tasks. The findings show that chain-of-thought with few-shot prompting delivers the most reliable precision-recall balance, while zero-shot prompting maximizes recall for high-sensitivity screening passes. GPT-4o and DeepSeek-Chat-V3 perform robustly overall, with GPT-4o-mini offering competitive accuracy at a lower cost. The study concludes by recommending a staged workflow: deploy low-cost models with structured prompts for initial screening and escalate only borderline cases to higher-capacity models.

## Method Summary
The study evaluates six LLMs across five prompting strategies (zero-shot, few-shot, chain-of-thought, CoT-few-shot, self-reflection) on relevance classification and six SLR stage tasks. Models are tested for precision-recall trade-offs, with cost analysis included to guide model selection. The experimental setup involves structured SLR datasets and benchmarks automated screening against established relevance criteria.

## Key Results
- Chain-of-thought with few-shot prompting provides the best precision-recall balance.
- Zero-shot prompting maximizes recall, ideal for high-sensitivity screening.
- GPT-4o and DeepSeek-Chat-V3 show robust overall performance; GPT-4o-mini is cost-effective and competitive.

## Why This Works (Mechanism)
The study demonstrates that structured prompting strategies—particularly chain-of-thought with few-shot examples—guide LLMs to reason more thoroughly, reducing false positives while maintaining high recall. Zero-shot prompts, by contrast, encourage broader inclusion, increasing recall at the expense of precision. The inclusion of cost analysis grounds model selection in practical deployment constraints.

## Foundational Learning
- **Prompting strategies**: Why needed—different tasks benefit from distinct reasoning patterns; Quick check—evaluate precision-recall trade-offs for each prompt type.
- **LLM selection**: Why needed—model capabilities and costs vary; Quick check—benchmark model performance and cost per task.
- **Relevance classification**: Why needed—core to SLR automation; Quick check—validate model accuracy against gold-standard relevance labels.
- **SLR task staging**: Why needed—optimizes resource use; Quick check—simulate staged workflow on diverse datasets.

## Architecture Onboarding
- **Component map**: Dataset → Preprocessing → Model-Prompt Pairing → Screening → Relevance Classification → Cost Analysis
- **Critical path**: Dataset → Model-Prompt Pairing → Screening → Relevance Classification
- **Design tradeoffs**: Precision vs. recall, cost vs. accuracy, model capacity vs. deployment speed
- **Failure signatures**: Over-inclusivity (self-reflection), low recall (high-precision prompts), cost overruns (unoptimized model selection)
- **First experiments**: 1) Benchmark prompt strategies on a held-out dataset. 2) Simulate staged workflow with cost tracking. 3) Validate self-reflection prompt performance with alternative formulations.

## Open Questions the Paper Calls Out
None

## Limitations
- Conclusions based on a single dataset and six models, limiting generalizability.
- Cost analysis excludes deployment overhead, latency, and model availability constraints.
- Self-reflection strategy underperformance is noted but not deeply explored.
- Assumes clear delineation between first-pass and escalation cases, which may not hold in practice.

## Confidence
- **High**: Precision-recall trade-off findings for CoT-few-shot and zero-shot strategies; relative model performance rankings (GPT-4o, DeepSeek, GPT-4o-mini).
- **Medium**: Cost-effectiveness recommendations; staged workflow guidance.
- **Low**: Generalization of self-reflection strategy underperformance; impact of deployment factors on cost claims.

## Next Checks
1. Test the proposed staged workflow on a separate, larger SLR dataset to confirm robustness across domains.
2. Evaluate self-reflection prompts with revised formulations to isolate whether the poor performance is prompt- or model-dependent.
3. Conduct a deployment simulation incorporating API latency, rate limits, and real-world error costs to validate cost-effectiveness claims.