---
ver: rpa2
title: Vision Language Models are Confused Tourists
arxiv_id: '2511.17004'
source_url: https://arxiv.org/abs/2511.17004
tags:
- image
- country
- cultural
- perturbation
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CONFUSEDTOURIST, a new benchmark for evaluating
  the robustness of Vision-Language Models (VLMs) to cultural concept mixing. The
  key innovation is the use of adversarial image perturbations that insert conflicting
  geographical cues (flags and landmarks) alongside a target cultural item (e.g.,
  cuisine, attire, or music).
---

# Vision Language Models are Confused Tourists

## Quick Facts
- **arXiv ID:** 2511.17004
- **Source URL:** https://arxiv.org/abs/2511.17004
- **Reference count:** 40
- **One-line primary result:** VLMs suffer substantial accuracy drops when faced with adversarial image perturbations that insert conflicting geographical cues alongside cultural items.

## Executive Summary
This work introduces CONFUSEDTOURIST, a new benchmark for evaluating the robustness of Vision-Language Models (VLMs) to cultural concept mixing. The key innovation is the use of adversarial image perturbations that insert conflicting geographical cues (flags and landmarks) alongside a target cultural item (e.g., cuisine, attire, or music). The evaluation suite contains 5,451 images covering 243 unique cultural items from 57 countries, with perturbations applied at two difficulty levels using both image stacking and generative methods. Results show that all 14 tested VLMs, including proprietary and open-source models, suffer substantial accuracy drops when faced with these perturbations, with the presence of flags causing the most disruption.

## Method Summary
The study creates a benchmark dataset of 5,451 images covering 243 cultural items across 57 countries in three categories (cuisine, attire, music). Adversarial perturbations are applied through image stacking (overlaying flags/landmarks at ≤20% size) and generative methods using Gemini-2.5-Flash-Image. Cultural items are paired using semantic similarity (mE5 embeddings + cosine similarity) or geographic distance (Haversine distance). Models are evaluated using multi-target accuracy (substring match against multiple valid labels) and distraction likelihood (percentage of wrong predictions matching the adversarial country). Interpretability analysis examines attention heatmaps and reasoning traces to understand failure mechanisms.

## Key Results
- All 14 tested VLMs show significant accuracy drops when exposed to cultural concept mixing perturbations
- Flag-based perturbations cause the most severe degradation (up to 18.4% accuracy drop) compared to landmarks (up to 6.9%)
- Generative perturbations are more effective than simple image stacking in causing model confusion
- Model failures are driven by attention shifts toward adversarial cues, diverting focus from the intended cultural item
- All models exhibit fallback preferences toward culturally over-represented countries (India, China, Japan) when uncertain

## Why This Works (Mechanism)

### Mechanism 1: Attention Diversion by Salient Geographic Cues
VLMs exhibit systematic attention shifts toward adversarial geographic cues (especially flags), diverting focus from target cultural items. The visual attention mechanism is disproportionately captured by high-salience, semantically unambiguous symbols like flags. When a flag appears alongside a cultural item, the model's cross-attention heads prioritize the flag's visual tokens over the item's intrinsic features, causing the final prediction to reflect the flag's country rather than the actual cultural object.

### Mechanism 2: Text-Token-Driven Visual Grounding Bias
Specific text tokens (system tokens, geographic references, category terms, and certain stopwords) disproportionately drive visual attention allocation, overriding intrinsic object features. The cross-modal attention layers weight certain text tokens more heavily when computing attention over image patches. Tokens like "country" or category-specific terms cause the model to seek geographic or categorical markers in the image. When adversarial flags or landmarks are present, these tokens amplify attention to those regions.

### Mechanism 3: Training Distribution Fallback Bias
VLMs exhibit a fallback preference toward culturally over-represented countries in their training data, causing systematic misattribution even without adversarial cues. Training data for VLMs contains unequal representation of cultural content. When uncertain, models default to predicting countries with higher representation (e.g., India, China, Japan). This is amplified under perturbation: as model confidence drops, the probability mass shifts toward these high-prior countries rather than being uniformly distributed.

## Foundational Learning

- **Concept:** Cross-modal attention in transformer-based VLMs
  - Why needed here: The paper's interpretability analysis relies on understanding how text tokens weight image regions; the mechanism of attention shift cannot be understood without grasping how vision and language tokens interact in attention layers.
  - Quick check question: Given a text query "identify the country of origin" and an image with a food item plus a flag in the corner, which image patches would receive higher attention weights if the model attends to the flag?

- **Concept:** Adversarial robustness and distribution shift
  - Why needed here: The benchmark tests VLMs under a specific form of distribution shift (conflicting cultural cues); understanding why simple image stacking causes failure requires knowing how models behave outside their training distribution.
  - Quick check question: If a model trained only on images with single cultural cues is tested on images with two conflicting cues, what type of robustness failure would you expect, and how would you measure it?

- **Concept:** Cultural proxies in multimodal evaluation (semantic similarity vs. geographic distance)
  - Why needed here: The benchmark pairs items using description-based (semantic) and geographic proximity; understanding why these pairings are "hard" or "easy" requires knowing how cultural relatedness can be operationalized.
  - Quick check question: For two dishes—Indonesian lemper (rice wrapped in banana leaf) and Japanese onigiri (rice ball)—would they form a "hard" pair under semantic similarity, geographic distance, or both? Why?

## Architecture Onboarding

- **Component map:** Context Crawler → Cultural Item Pool (cuisine/attire/music, 243 items, 57 countries) → Adversarial Pairing (description-based: mE5 embeddings + cosine similarity; geographic: Haversine distance) → Image Perturbation Module: - Image Stacking: Φ(I_ori, {∇(I_flag), ∇(I_landmark)}) → fixed placement (flag: top-right, landmark: bottom-left) - Generative: Ψ(I_ori, {I_flag, I_landmark}, prompt) → Gemini-2.5-Flash-Image with constrained prompt → Evaluation Metrics: - Multi-Target Accuracy (substring match against multiple valid labels) - Distraction Likelihood: P(adversarial_country | wrong_prediction) → Interpretability Layer: Attention heatmap extraction, reasoning trace analysis

- **Critical path:** 1. Pair creation (determines difficulty: "hard" pairs = semantically/geographically close items) 2. Perturbation type (flag > landmark in impact; generative > stacking in effectiveness) 3. Metric computation (accuracy drop + distraction likelihood reveal both overall and targeted failure)

- **Design tradeoffs:** Image stacking vs. generative perturbation: Stacking is simpler and reproducible; generative is more realistic but introduces generation artifacts and quality variance (avg quality score: 4.49/5). Substring matching vs. exact match: Substring allows alternative names but may over-count partial matches. Single-turn prompt vs. multi-turn: Single-turn isolates grounding ability but may not reflect real-world usage.

- **Failure signatures:** Flag-induced attention collapse: Attention heatmap shows >50% weight on flag region; prediction matches adversarial country. Country fallback: Wrong prediction is a high-frequency country (India, China, Japan) unrelated to either the item or adversarial cue. Reasoning trace contamination: Model explicitly mentions flag/landmark in reasoning before identifying object ("I see a flag... so it must be Country X").

- **First 3 experiments:** 1. Reproduce the image-stacking perturbation on 10 samples: Take a cultural item image, overlay a foreign flag at 20% size in the top-right corner, and query your VLM. Compare accuracy against the baseline (unperturbed image) to confirm the flag-induced drop. 2. Token ablation study: Modify the prompt to remove geographic references (e.g., change "country from which this originates" to "describe this object's origin without using surrounding context") and measure whether attention shifts back to the item. 3. Distraction likelihood calibration: For a model with known fallback bias, compute P(adversarial_country | wrong) across 50 samples. If >60% of errors match the adversarial country, the model is attention-diverted; if errors cluster around fallback countries, the model is prior-biased.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial training with culturally conflicting cues improve VLM robustness without degrading baseline cultural recognition performance?
- Basis in paper: The authors state "VLMs must develop greater cultural robustness to achieve reliable multimodal understanding across diverse cultural contexts" and "underscore the urgent need for more culturally robust multimodal understanding."
- Why unresolved: The paper only diagnoses the vulnerability through benchmarking; no mitigation strategies are proposed or tested.
- What evidence would resolve it: Training VLMs on augmented datasets with conflicting cultural cues, then evaluating on both CONFUSEDTOURIST and unperturbed cultural benchmarks to measure robustness-accuracy tradeoffs.

### Open Question 2
- Question: Why do flag-based perturbations cause significantly greater performance degradation than landmark-based perturbations?
- Basis in paper: The paper reports "the presence of the flag caused a decline of up to 18.4%, whereas applying the landmark perturbation resulted in a minor drop of only up to 6.9%," but does not provide a mechanistic explanation.
- Why unresolved: Interpretability analysis shows attention shifts toward adversarial cues broadly, but does not isolate why flags specifically are more distracting than landmarks.
- What evidence would resolve it: Controlled studies varying flag visual properties (color complexity, recognizability, spatial frequency) paired with fine-grained attention analysis to identify which features drive the disparity.

### Open Question 3
- Question: Can architectural modifications to attention mechanisms reduce susceptibility to distractor cues while preserving cultural grounding ability?
- Basis in paper: The attention analysis reveals that specific token types (system, geo-related, stopwords) disproportionately drive visual attention toward adversarial cues. Prompt ablation showed "inconsistent" improvements, suggesting language-side intervention alone is insufficient.
- Why unresolved: The paper demonstrates the problem exists at the attention level but does not explore whether architectural changes could provide more stable solutions.
- What evidence would resolve it: Experiments with attention regularization, object-centric attention mechanisms, or multi-head attention constraints evaluated on the CONFUSEDTOURIST suite.

## Limitations

- The benchmark relies on a curated dataset of 243 cultural items, which may not represent the full diversity of global cultural concepts.
- The perturbation methods use fixed visual placements (flags top-right, landmarks bottom-left) that could be optimized further.
- The interpretability analysis focuses on attention heatmaps, which are known to be imperfect proxies for model reasoning.
- The inconsistency in prompt ablation results across categories suggests that the token-attention mechanism may not be universally applicable.

## Confidence

- **High Confidence:** The core finding that VLMs suffer accuracy drops under cultural concept mixing is well-supported by consistent results across 14 models and multiple perturbation types. The benchmark methodology is clearly specified and reproducible.
- **Medium Confidence:** The attention diversion mechanism is supported by heatmaps and accuracy drops, but the causal link between attention shifts and prediction errors could be strengthened with additional intervention studies. The token-driven attention bias finding shows promise but lacks consistent replication across categories.
- **Low Confidence:** The training distribution fallback hypothesis relies on correlation between prediction frequency and assumed training data prevalence, but direct evidence of training data composition is not provided.

## Next Checks

1. **Attention Intervention Validation:** Design an experiment where attention weights on adversarial regions are artificially suppressed during inference, then measure whether accuracy on perturbed images improves while baseline accuracy remains stable.

2. **Cross-Cultural Generalization:** Expand the benchmark to include cultural items from underrepresented regions and test whether the flag-induced attention collapse persists across all cultural contexts, particularly for cultures with lower training data prevalence.

3. **Alternative Perturbation Patterns:** Test whether moving adversarial cues to different spatial locations (center, bottom-right) or using different visual transformations (color inversion, transparency) produces similar or different effects, to determine if the vulnerability is tied to specific visual patterns or general distractor presence.