---
ver: rpa2
title: Pushing the Boundaries of State Space Models for Image and Video Generation
arxiv_id: '2502.00972'
source_url: https://arxiv.org/abs/2502.00972
tags:
- arxiv
- generation
- preprint
- image
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scalability of State Space Models (SSMs)
  for high-resolution image and long video generation. To address SSMs' inherent causality
  limitations, the authors build a large-scale diffusion model hybridizing Hydra bidirectional
  SSMs with self-attention (HTH), totaling 5 billion parameters.
---

# Pushing the Boundaries of State Space Models for Image and Video Generation

## Quick Facts
- **arXiv ID:** 2502.00972
- **Source URL:** https://arxiv.org/abs/2502.00972
- **Reference count:** 40
- **Primary result:** 5B parameter HTH model generates 2K images and 360p 8-second videos with performance comparable to Transformer baselines while offering faster inference

## Executive Summary
This paper investigates the scalability of State Space Models (SSMs) for high-resolution image and long video generation. To address SSMs' inherent causality limitations, the authors build a large-scale diffusion model hybridizing Hydra bidirectional SSMs with self-attention (HTH), totaling 5 billion parameters. Their model generates up to 2K resolution images and 360p 8-second videos (16 FPS) by adapting a text-to-image model to video through selective temporal scanning in certain layers. Experimental results demonstrate that HTH produces high-fidelity, text-aligned outputs and temporally consistent, dynamic videos, achieving performance comparable to state-of-the-art Transformer-based models while offering faster inference for long sequences.

## Method Summary
The paper presents HTH, a 5B parameter diffusion model combining Hydra bidirectional SSMs with self-attention blocks in a 30:3 ratio. The architecture processes 2x2 patches through 33 blocks (30 Hydra, 3 self-attention) with cross-attention for text conditioning. Video adaptation is achieved by switching 40% of Hydra blocks from spatial to temporal scanning patterns. Training proceeds in four stages: 256p T2I, 1K T2I fine-tuning, 180p T2V mixed training, and 360p T2V fine-tuning, using a 3D VAE adapted from MAGVIT-v2 and FLAN-T5-XXL text encoder.

## Key Results
- Generates 2K resolution images with FID scores competitive with Transformer baselines
- Produces 360p 8-second videos (16 FPS) with improved temporal consistency over spatial-only scanning
- Achieves strong text-image alignment (CLIP-Score) while maintaining efficient long-sequence processing
- Demonstrates object composition limitations compared to Transformers (GenEval 0.58 vs DiT 0.63)

## Why This Works (Mechanism)

### Mechanism 1: Hybrid SSM-Attention Token Mixing
Alternating Hydra bidirectional SSMs with sparse self-attention layers balances sequence efficiency with global dependency modeling. Hydra blocks process tokens through sub-quadratic quasiseparable matrices (O(n) complexity), while self-attention blocks interspersed every 10 layers provide global token interaction. Long-range global dependencies can be adequately captured by periodic attention layers rather than dense attention.

### Mechanism 2: Bidirectional Scanning for N-Dimensional Visual Data
Alternating horizontal and vertical raster scans across layers expands token receptive fields to address SSMs' inherent locality bias. Each Hydra block applies a directional scan (horizontal or vertical), and alternating scans across consecutive blocks enables tokens to accumulate context from both spatial dimensions. 2D spatial continuity can be approximated by interleaved 1D scans across layers.

### Mechanism 3: Selective Temporal Scanning for Video Adaptation
Converting spatial-major scans to temporal-major scans in specific layers enables T2I models to learn temporal evolution without adding new parameters. For video, 40% of Hydra blocks switch from (Horizontal, Vertical) to (Temporal-Horizontal, Temporal-Vertical) scanning, processing all tokens at the same temporal position before moving spatially. Temporal consistency can be learned by forcing select layers to prioritize time-axis token relationships.

## Foundational Learning

- **Selective State Space Models (Mamba/S4)**
  - Why needed: HTH builds on Mamba-2 framework; understanding discretization (∆t, At, Bt, Ct), semiseparable matrices, and selective state propagation is prerequisite to comprehending Hydra's modifications.
  - Quick check: Can you explain how input-dependent ∆t controls state preservation vs. new information integration?

- **Diffusion Transformer (DiT) Architecture**
  - Why needed: HTH follows DiT's denoising framework (patch embedding, timestep conditioning, iterative denoising); deviations assume familiarity with the baseline.
  - Quick check: What is the computational complexity of self-attention in a standard DiT block, and how does patch size affect token sequence length?

- **Quasiseparable Matrix Structure**
  - Why needed: Hydra's expressiveness claim rests on quasiseparable matrices being "strictly more expressive than existing addition-based bidirectional SSMs."
  - Quick check: How does combining forward and reverse semiseparable matrices create a quasiseparable matrix, and what property does this preserve?

## Architecture Onboarding

- **Component map:** Text encoder (FLAN-T5-XXL) → cross-attention → LayerNorm → 33 blocks [10 Hydra → SA → 10 Hydra → SA → 10 Hydra → SA → 10 Hydra] → LayerNorm → FFN → LayerNorm → FFN → LayerNorm → FFN

- **Critical path:**
  1. Implement Hydra block via Mamba-2 (bidirectional scan, quasiseparable matrix multiplication)
  2. Integrate with standard self-attention using FlashAttention-2
  3. Implement scan pattern switching logic (H/V for images; TH/TV/HT/VT for video)
  4. Multi-stage training: 256p T2I → 1K T2I → 180p T2V → 360p T2V

- **Design tradeoffs:**
  - 30:3 Hydra:Attention ratio optimizes for efficiency but trades off object composition quality (GenEval 0.58 vs DiT 0.63)
  - Simple sinusoidal positional encoding vs. learned/RoPE: Faster but may be suboptimal for SSM spatial continuity
  - Direct timestep addition (not AdaLN) saves parameters but reduces conditioning flexibility

- **Failure signatures:**
  - Structure-incoherent objects in complex scenes → insufficient global dependency modeling
  - Temporal flickering at higher resolutions → temporal receptive field exhaustion
  - Checkerboard artifacts in zero-shot higher-res generation → HTH should avoid this via locality bias

- **First 3 experiments:**
  1. Ablation on Hydra:Attention ratio: Compare 30:3 vs 20:13 vs 10:23 on GenEval object composition metrics
  2. Scan pattern validation: Train small-scale (1B param) models with (a) horizontal-only, (b) alternating H/V, (c) 4-direction scanning
  3. Temporal scanning ablation for video: Compare full temporal-major vs. selective (40%) temporal-major vs. spatial-only scanning on VBench metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative scanning strategies (e.g., diagonal, spiral, or randomized scans) compare to standard horizontal and vertical bidirectional raster scans?
- Basis: Page 5 states experimenting with various scanning patterns "falls outside the scope of this paper"
- Resolution: Comparative study on HTH architecture using diverse scanning patterns, measuring FID/CLIP scores on high-resolution benchmarks

### Open Question 2
- Question: What conditioning mechanisms can effectively mitigate the semantic understanding limitations of SSMs regarding entity composition and structural coherence?
- Basis: Appendix A notes inferior semantic understanding and structural coherence compared to Transformers
- Resolution: Implementation of advanced conditioning techniques resulting in GenEval scores comparable to state-of-the-art Transformer baselines

### Open Question 3
- Question: Can specific positional encoding schemes be designed to better align with the inductive biases of SSMs?
- Basis: Appendix A identifies standard positional encodings may be suboptimal for SSM spatial continuity
- Resolution: Ablation studies replacing absolute sinusoidal embeddings with relative or continuous positional encodings

## Limitations

- Object composition quality trade-off: The 30:3 hybrid ratio demonstrably sacrifices object-level composition quality (GenEval 0.58 vs DiT baseline 0.63)
- Temporal resolution scaling limits: Selective temporal scanning shows clear benefits at 180p but degrades at 360p (Subject Consistency drops from 95.71 to 91.36)
- Internal data dependency: Relies on proprietary "internal data" making it impossible to validate whether performance gains stem from architectural innovations or data advantages

## Confidence

- **High confidence:** Diffusion framework integration, HTH architecture specifications, video adaptation through temporal scanning patterns, and reported evaluation metrics on standard benchmarks
- **Medium confidence:** Claims about SSM efficiency advantages and comparative speed benefits relative to Transformers
- **Low confidence:** Claims about avoiding Transformer limitations without direct empirical comparison, and assertions about quasiseparable matrix expressiveness without mathematical proof

## Next Checks

1. **Temporal scanning ablation at multiple resolutions:** Compare full temporal-major, selective (40%), and spatial-only scanning across 180p, 360p, and 480p video generation to quantify scaling limits and identify optimal temporal-major layer percentage.

2. **Hybrid ratio optimization study:** Systematically vary the Hydra-to-attention ratio (10:23, 20:13, 30:3) on GenEval and PartiPrompts benchmarks to precisely characterize the efficiency-quality frontier.

3. **Cross-modal alignment validation:** Conduct human evaluation studies comparing text-video semantic alignment between HTH and Transformer baselines at equivalent resolutions, measuring whether temporal consistency improvements preserve or enhance text-to-video semantic coherence.