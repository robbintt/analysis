---
ver: rpa2
title: Bringing together invertible UNets with invertible attention modules for memory-efficient
  diffusion models
arxiv_id: '2504.10883'
source_url: https://arxiv.org/abs/2504.10883
tags:
- invertible
- https
- memory
- diffusion
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of high memory consumption in
  diffusion models when applied to 3D medical imaging data, such as MRI and CT scans,
  which limits training to multi-GPU setups and increases energy usage. The proposed
  solution is an Invertible Diffusion Model (IDM) that combines invertible U-Nets
  with invertible attention modules.
---

# Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models

## Quick Facts
- arXiv ID: 2504.10883
- Source URL: https://arxiv.org/abs/2504.10883
- Reference count: 40
- Primary result: Memory-efficient diffusion model for 3D medical imaging achieving 15% peak memory reduction with comparable PSNR (25.01 T1, 25.54 T2)

## Executive Summary
This paper introduces an Invertible Diffusion Model (IDM) that combines invertible U-Nets with invertible attention modules to address the high memory consumption challenges of diffusion models when applied to 3D medical imaging data. The architecture allows activations to be reconstructed during backpropagation rather than stored, making memory usage independent of dataset dimensionality. The model achieves comparable image quality metrics to state-of-the-art models while significantly reducing memory requirements, though at the cost of increased computational overhead.

## Method Summary
The IDM architecture uses invertible layers throughout the network core, with non-invertible "bookend" layers only at the start and end to capture complex data distributions. It extends invertible attention to 3D tensors using a spatial checkerboard pattern that allows global dependencies to be captured without the standard memory penalty. The model employs learnable invertible upsampling and downsampling layers, and uses standard layers only at the boundaries to handle channel expansion. During backpropagation, activations are recomputed on-demand via inverse operations rather than retrieved from memory, trading compute for memory efficiency.

## Key Results
- Achieved PSNR of 25.01 for T1 MRI and 25.54 for T2 MRI on BraTS2020 dataset
- Reduced peak memory consumption by 15% during training compared to baseline
- Performed 4.61x more FLOPs than PatchDDM-3D but used significantly less energy per parameter
- Maintained stable peak temperatures around 61°C compared to 62°C for baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reconstructing activations during the backward pass decouples memory usage from network depth, reducing peak consumption.
- Mechanism: The architecture utilizes invertible layers (bijective mappings) where inputs x can be mathematically derived from outputs y (x = f⁻¹(y)). During backpropagation, instead of retrieving stored activations, the model recomputes them on-demand via an inverse pass.
- Core assumption: The computational overhead of recomputing activations does not outweigh the memory benefits, and the inverse operations are numerically stable.
- Evidence anchors: [abstract]: "architecture allows for activations to be reconstructed during backpropagation rather than stored"; [section 4]: "reconstruct activations rather than storing them... during backpropagation at the cost of an increase in computations required."

### Mechanism 2
- Claim: Extending invertible attention to 3D tensors allows the model to capture global dependencies without the standard memory penalty of storing attention matrices.
- Mechanism: The model adapts the "iTrans" module using a spatial checkerboard pattern extended to depth. It splits the input tensor, processes one part with attention weights derived from the other, and combines them such that the concatenation operation is reversible.
- Core assumption: The checkerboard split preserves enough information density to accurately model complex 3D anatomical structures.
- Evidence anchors: [abstract]: "introduces non-invertible layers only at the start and end... to capture complex data distributions"; [section 2.2]: "To extend this work to the 3D space, we modify the iTrans model to include a depth dimension."

### Mechanism 3
- Claim: Non-invertible "bookend" layers are required to handle the channel expansion necessary for learning complex data distributions.
- Mechanism: Because strictly invertible down/up-sampling constrains channel expansion (to maintain bijectivity), the model adds standard (non-invertible) layers at the input and output. These layers store activations, accepting a fixed memory overhead to grant the model greater expressive capacity.
- Core assumption: The memory cost of storing activations for these few bookend layers is negligible compared to the savings in the deep invertible core.
- Evidence anchors: [abstract]: "introduces non-invertible layers only at the start and end of the network to capture complex data distributions."; [section 4]: "we introduce non-invertible non-linearly increasing downsample and upsample layers at the start and end... leading to a slight increase in memory consumption for much greater model power."

## Foundational Learning

- Concept: **Reversible Residual Layers (Invertibility)**
  - Why needed here: The core memory savings depend on the ability to re-calculate inputs from outputs perfectly. Without understanding the constraints of invertible functions (e.g., maintaining input/output dimensions), implementation will fail.
  - Quick check question: If a layer halves the spatial dimensions but doubles the channels, is it strictly invertible without storing additional state?

- Concept: **3D Convolution & Volumetric Data**
  - Why needed here: The paper specifically targets the memory explosion inherent in 3D MRI/CT scans (240 × 240 × 155). Standard 2D intuition regarding batch sizes and memory often fails in 3D contexts.
  - Quick check question: How does the memory requirement scale when moving from a 64² image to a 64³ volume with the same channel count?

- Concept: **Diffusion Process (Forward/Reverse)**
  - Why needed here: The invertible U-Net serves as the denoising agent (U(xₜ, t)). Understanding the noise schedule (βₜ) and the objective (predicting noise ε) is required to integrate the custom architecture.
  - Quick check question: In a standard DDPM, does the U-Net predict the clean image or the noise component at timestep t?

## Architecture Onboarding

- Component map: Input -> Non-invertible downsampling blocks -> Invertible U-Net with 3D iTrans Attention -> Non-invertible upsampling blocks -> Output
- Critical path: Implementing the custom autograd.Function for the invertible attention and convolutions. The backward pass must explicitly call the inverse logic (f⁻¹) rather than relying on automatic differentiation of the forward pass to save memory.
- Design tradeoffs: The model trades Compute for Memory. Expect a ~4.6x increase in FLOPs (recomputation cost) for a 15% reduction in peak memory. Additionally, the strictly invertible core may limit model expressivity compared to fully unconstrained architectures, resulting in lower SSIM scores.
- Failure signatures:
  - Model Collapse: Generating pure noise or uniform values, likely caused by insufficient channel expansion in the invertible layers (solved by the non-invertible bookends).
  - OOM (Out of Memory): Occurring not in the deep layers, but in the initial non-invertible expansion blocks if they are made too large.
  - Numerical Instability: Divergence during training if the invertible inverse operations accumulate floating-point errors.
- First 3 experiments:
  1. Baseline Memory Profile: Run a forward/backward pass on a standard 3D U-Net vs. the Invertible U-Net (without attention) on a dummy 64³ volume to verify the theoretical memory reduction.
  2. Bookend Ablation: Remove the non-invertible start/end layers to confirm the hypothesis that the model fails to generate coherent images without them.
  3. Attention Scaling: Benchmark the 3D invertible attention module specifically, as extending the "checkerboard" pattern to 3D is a novel constraint that may introduce unexpected latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the architectural design of IDM be refined to improve structural fidelity (SSIM) in generated medical images without compromising memory efficiency?
- Basis in paper: [explicit] The authors note that IDM exhibits significantly lower SSIM scores (0.48) compared to SOTA models like CycleGAN (0.98), stating this "lack of fine-grained structural fidelity... may limit IDM’s utility in medical imaging applications."
- Why unresolved: The current implementation prioritizes pixel-wise accuracy (PSNR) and memory savings, but the authors do not offer a solution for the loss of structural details caused by the invertible constraints.
- What evidence would resolve it: A variation of the model that achieves competitive SSIM scores (e.g., >0.90) on the BraTS2020 dataset while maintaining the 15% reduction in peak memory consumption.

### Open Question 2
- Question: Can IDM's inverse prediction capabilities be utilized to create a robust classifier for distinguishing between real and diffusion-generated medical images?
- Basis in paper: [explicit] The authors state, "we hope to exploit the inverse prediction capabilities of IDM to predict the noise that a random image came from... enabling the creation of a classifier."
- Why unresolved: The paper proposes this application as a future direction but provides no experimental validation or theoretical framework for how the inverse pass would function as a discriminator.
- What evidence would resolve it: A demonstration of a classifier trained using IDM's inverse mappings that successfully differentiates between real MRI scans and those synthesized by the model.

### Open Question 3
- Question: Can computational efficiency techniques, such as patch-based training, be integrated into IDM to reduce the high FLOP count associated with reconstructing activations during backpropagation?
- Basis in paper: [explicit] The authors acknowledge that IDM performs 4.61x more FLOPs than PatchDDM-3D and explicitly list applying ideas from "Patch Diffusion" (reference [51]) to reduce computational power as a future work goal.
- Why unresolved: The current method trades memory for computation (re-computing activations), leading to longer training times; the feasibility of combining invertible logic with patch-based efficiency remains untested.
- What evidence would resolve it: A hybrid implementation that reduces the FLOP count to be comparable with PatchDDM-3D while preserving the memory independence of the invertible architecture.

## Limitations
- SSIM scores significantly lower than state-of-the-art models (0.48 vs 0.98), indicating loss of structural fidelity
- 4.61x increase in FLOPs compared to baseline, trading memory for computational efficiency
- Implementation complexity due to custom invertible operations and 3D attention extensions

## Confidence
- Memory efficiency claims: High - well-supported by reversible network theory and demonstrated 15% reduction
- PSNR performance claims: Medium - comparable results reported but direct ablation studies against baseline missing
- Energy efficiency claims: Medium - temperature measurements support but may not fully capture computational overhead
- Architectural details: Low - unspecified parameters (bookend layer configurations, 3D iTrans specifics) limit exact reproduction

## Next Checks
1. **Memory Profiling Validation**: Profile memory usage layer-by-layer during training to verify that only the non-invertible bookend layers store activations while the invertible core reconstructs them during backpropagation.

2. **Ablation Study**: Train the model without the non-invertible start/end layers to empirically confirm the claim that these are necessary for capturing complex data distributions, as the paper suggests the model would fail without them.

3. **Compute-Memory Tradeoff Analysis**: Measure actual training time and energy consumption on different GPU architectures to validate that the 4.61x increase in FLOPs translates to acceptable performance given the memory savings, particularly for multi-modal medical imaging workflows.