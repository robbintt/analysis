---
ver: rpa2
title: 'MAPS: Preserving Vision-Language Representations via Module-Wise Proximity
  Scheduling for Better Vision-Language-Action Generalization'
arxiv_id: '2511.19878'
source_url: https://arxiv.org/abs/2511.19878
tags:
- maps
- arxiv
- pretrained
- language
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAPS improves Vision-Language-Action (VLA) models by preserving
  pretrained Vision-Language Model (VLM) representations during action fine-tuning.
  It addresses generalization loss in VLAs caused by catastrophic forgetting when
  adapting pretrained VLMs to robotics tasks.
---

# MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization

## Quick Facts
- **arXiv ID:** 2511.19878
- **Source URL:** https://arxiv.org/abs/2511.19878
- **Reference count:** 40
- **Primary result:** Improves VLA generalization by up to +30% using module-wise regularization that preserves pretrained VLM representations during action fine-tuning

## Executive Summary
MAPS addresses catastrophic forgetting in Vision-Language-Action (VLA) models by preserving pretrained Vision-Language Model (VLM) representations during action fine-tuning. The method uses Module-Wise Proximity Scheduling to apply stronger regularization to visual components (DINOv2, SigLIP) while allowing more flexible adaptation of language layers. This prevents drift from pretrained priors while enabling task-specific grounding. Across multiple benchmarks including SimplerEnv, CALVIN, LIBERO, and real-world robot evaluations, MAPS consistently improves both in-distribution and out-of-distribution performance without additional parameters or data.

## Method Summary
MAPS implements layer-wise proximity scheduling on top of SPD regularization. The model is ordered as DINOv2→SigLIP→Bridge→Language layers, with regularization strength λ_k decaying linearly from λ_max for early vision layers to 0 for late language layers. After standard Adam updates, MAPS checks if the gradient opposes the displacement from initialization (c_t < 0). If so, it projects weights back toward their initial values using module-specific λ_k. Non-pretrained modules (action head, proprio projector) receive λ_k=0. The method operates as an optimizer wrapper that intercepts weight updates to apply these projections.

## Key Results
- Consistently improves OOD generalization by 5-30% across SimplerEnv, CALVIN, and LIBERO benchmarks
- Preserves pretrained visual features (DINOv2) more strongly than language layers, matching empirical sensitivity hierarchy
- Achieves gains without additional parameters, data, or architectural changes
- Outperforms freezing and uniform regularization baselines in both ID and OOD settings

## Why This Works (Mechanism)

### Mechanism 1
Constraining low-level visual modules (DINOv2, SigLIP) closer to pretrained weights than language layers preserves geometric priors necessary for generalization. MAPS applies linearly decaying proximity weight λ_k, starting high for early visual layers and approaching zero for late language layers. Visual encoders capture generalizable spatial/structural features in pretraining that are easily disrupted by sparse action data, whereas language layers require more flexibility to ground task-specific semantics.

### Mechanism 2
A "soft" projection-based constraint (MAPS) outperforms "hard" freezing by allowing limited, controlled drift rather than binary constraint. Instead of freezing weights (Δθ = 0), MAPS projects updated weights back toward initialization θ_0 based on deviation ratio r_t. If gradient direction opposes displacement from θ_0 (gradient-displacement correlation c_t < 0), the model is pulled back. The optimal parameters for downstream action tasks lie within a relatively small radius around pretrained initialization.

### Mechanism 3
Uniform regularization strength across all modules fails to account for differing sensitivity of visual vs. language components to fine-tuning. MAPS decouples regularization schedule across architecture stack, replacing single global hyperparameter with module-specific schedule. This allows deviation radius to expand differently for each module. The sensitivity of a layer to fine-tuning correlates inversely with its depth in visual processing hierarchy.

## Foundational Learning

- **Catastrophic Forgetting**
  - Why needed here: MAPS is designed as counter-measure to loss of spatial reasoning and world knowledge when VLMs are fine-tuned on sparse robotics data
  - Quick check question: Why does fine-tuning a large pretrained model on a small dataset often hurt performance on out-of-distribution inputs?

- **L2-Regularization vs. L2-SP**
  - Why needed here: The paper builds upon L2-SP (Regularization toward initialization) rather than standard weight decay (toward zero)
  - Quick check question: In L2-SP formulation, what point does regularization term pull weights toward?

- **VLA Architecture Stacking**
  - Why needed here: MAPS requires ordering model into stack L (DINOv2 → SigLIP → Language)
  - Quick check question: In standard VLA, which component is responsible for spatial geometry (DINOv2) versus semantic alignment (SigLIP)?

## Architecture Onboarding

- **Component map:** Pretrained VLM (DINOv2 + SigLIP + LLM) → Action Head (trained from scratch) → MAPS Wrapper (around optimizer)
- **Critical path:**
  1. Identify module boundaries in your VLA (Vision Encoders vs. LLM)
  2. Store initial weights θ_0
  3. Define linear decay schedule for λ_k based on layer depth
  4. Inside optimizer loop: compute unconstrained update θ̃_t
  5. Check condition c_t < 0; if true, project θ̃_t toward θ_0 using module-specific λ_k
- **Design tradeoffs:**
  - λ_max Selection: High λ_max stabilizes OOD generalization but may slow convergence on ID tasks
  - Linear vs. Constant: Paper finds Linear scheduling provides better stability than Constant or Cosine
- **Failure signatures:**
  - ID performance drops significantly: Regularization too strong (λ_max too high)
  - OOD performance matches baseline: Regularization too weak or applied uniformly
  - Grasping fails: Visual encoder drifting too far; check if DINOv2 λ correctly set to λ_max
- **First 3 experiments:**
  1. Baseline Check: Fine-tune VLA on target dataset with standard Adam (no MAPS) to establish ID/OOD baseline
  2. Scheduler Ablation: Implement MAPS with Linear schedule vs. Constant λ to verify benefit of Module-Wise decay
  3. Hyperparameter Sweep: Sweep λ_max ∈ [0.5, 1.0, 2.0] to find stability-plasticity sweet spot for your robot data scale

## Open Questions the Paper Calls Out

### Open Question 1
Can optimal module-wise proximity schedule be automatically learned or adapted per task, rather than using fixed linear schedule? The paper notes that "freezing effects are not universally consistent" and "some configurations improve one benchmark but harm another, indicating that freezing introduces task-dependent inductive biases." MAPS uses manually designed linear decay schedule with single hyperparameter λ_max that must be tuned per configuration.

### Open Question 2
How does MAPS interact with parameter-efficient fine-tuning methods like LoRA, which the paper uses for OpenVLA-OFT but does not analyze? Table 8 shows LoRA (r=32) was used for OpenVLA-OFT experiments, but the paper does not discuss whether MAPS' regularization operates on LoRA adapters or base weights.

### Open Question 3
Why does linear scheduler outperform cosine and constant alternatives, and does this finding hold across different training durations and dataset scales? "We find that in general, project-strength modulation offers ID and OOD benefits compared to regular robust-finetuning. We also find that the linear scheduler offers most stable ID performance and greatest gains in OOD." The scheduler comparison is limited to LIBERO with specific projection strength values.

## Limitations
- Effectiveness critically depends on correct layer boundary identification and hyperparameter tuning of λ_max
- Module-wise scheduling assumes fixed hierarchy (DINOv2 > SigLIP > Language) that may not generalize to all VLM architectures
- Linear decay schedule is empirically chosen over alternatives without exhaustive ablation

## Confidence

- **High:** MAPS consistently improves OOD performance across multiple benchmarks when properly configured
- **Medium:** Mechanism explaining why visual layers need stronger preservation than language layers is theoretically sound but lacks ablation studies isolating each component's contribution
- **Medium:** Claim that MAPS works "without additional parameters or data" is accurate for method itself, though requires careful tuning of existing hyperparameters

## Next Checks

1. **Layer Boundary Sensitivity:** Systematically vary layer partitioning between DINOv2, SigLIP, and language components to test robustness of module-wise hierarchy assumption

2. **Alternative Decay Schedules:** Compare linear decay against exponential, cosine, and step-wise schedules to verify specific choice is optimal beyond empirical selection

3. **Architecture Transfer:** Apply MAPS to VLA architectures with different visual backbones (e.g., CLIP-based) to test whether DINOv2-first preservation principle generalizes beyond tested setup