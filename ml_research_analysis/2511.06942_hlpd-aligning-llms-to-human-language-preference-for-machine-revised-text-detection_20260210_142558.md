---
ver: rpa2
title: 'HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text
  Detection'
arxiv_id: '2511.06942'
source_url: https://arxiv.org/abs/2511.06942
tags:
- uni00000013
- text
- uni00000003
- hlpd
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HLPD addresses the challenge of detecting machine-revised text
  under black-box conditions, where source LLMs are unknown. It introduces Human Language
  Preference Optimization (HLPO) to align a scoring model with human writing styles
  by training on paired human-written and machine-revised texts.
---

# HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection

## Quick Facts
- **arXiv ID:** 2511.06942
- **Source URL:** https://arxiv.org/abs/2511.06942
- **Reference count:** 17
- **Primary result:** HLPD achieves 15.11% relative AUROC improvement over ImBD on GPT-series revisions and strong multilingual and iterative revision performance.

## Executive Summary
HLPD addresses the challenge of detecting machine-revised text when the source LLM is unknown. It introduces Human Language Preference Optimization (HLPO) to align a scoring model with human writing styles by training on paired human-written and machine-revised texts. This alignment makes the model more sensitive to human-like language, improving detection accuracy. Evaluated using an adversarial multi-task framework with diverse prompts and advanced LLMs, HLPD achieves strong performance: 15.11% relative AUROC improvement over ImBD on GPT-series revisions, 45.56% over Fast-DetectGPT, and 5.53% over ImBD on texts from advanced LLMs. It also outperforms baselines in multilingual and iterative revision scenarios. The method shows robustness and adaptability across tasks and languages, with ablation studies confirming the effectiveness of its optimization strategy. Additionally, HLPD's scoring model demonstrates potential in evading commercial detectors through adaptive humanization.

## Method Summary
HLPD detects machine-revised text by aligning a scoring model to prefer human writing styles over machine revisions. The method uses Human Language Preference Optimization (HLPO), a variant of Direct Preference Optimization, which maximizes the reward margin between original human-written text and its machine-revised counterpart using a linear contrastive loss. A dynamic variance-aware scheduler adjusts the temperature coefficient during training to prevent margin saturation and improve stability on limited data. Detection is performed using Human Language Preference Conditional Probability Curvature (HLP-CPC), which measures the probability curvature of text under the aligned scoring model. The method is evaluated on diverse datasets including XSum, SQuAD, WritingPrompts, PubMedQA, and WikiText, showing strong generalization across tasks and languages.

## Key Results
- 15.11% relative AUROC improvement over ImBD on GPT-series revisions
- 45.56% improvement over Fast-DetectGPT on advanced LLM revisions
- Strong performance in multilingual and iterative revision scenarios

## Why This Works (Mechanism)

### Mechanism 1: Preference Alignment via Contrastive Loss
HLPO aligns the scoring model to prefer human text over machine-revised text using a linear contrastive loss. This forces the model to sharpen its distribution around human stylistic features rather than overfitting to specific LLM artifacts. The method assumes human writing possesses consistent stylistic patterns distinct from machine revisions. Evidence shows HLPO outperforms standard training methods by significant margins, and alignment training significantly alters text detectability.

### Mechanism 2: Conditional Probability Curvature Inversion
The aligned scoring model exhibits a reversal of standard probability curvature found in base models. While machine text typically has higher log-probability in standard models, the HLPO-aligned model assigns highest probability to human text. This creates positive curvature for human text and negative curvature for machine-revised text, enabling detection through curvature differences. The mechanism assumes perturbations can reveal probability landscape curvature without semantic drift.

### Mechanism 3: Dynamic Variance-Aware Optimization
A dynamically adjusted temperature coefficient in the loss function prevents margin saturation and improves training stability. The scheduler monitors variance of the reward margin over a sliding window, decreasing temperature when variance is high (noisy signal) and increasing it when variance is low (confident signal). This acts as a gradient regulator, improving performance by 1-8% over static variants.

## Foundational Learning

### Direct Preference Optimization (DPO)
**Why needed:** HLPO is a modification of DPO. Understanding how DPO replaces explicit reward models with implicit rewards derived from policy and reference model log-probabilities is essential.
**Quick check:** How does removing the sigmoid activation (changing to linear loss) affect gradient magnitude for well-classified pairs?

### Conditional Probability Curvature
**Why needed:** The detection metric relies on comparing log-probability of text against perturbed versions. Understanding why this curvature exists is key to understanding why alignment changes the detection boundary.
**Quick check:** Why does machine-generated text typically exhibit negative curvature in standard pre-trained models?

### Style Transfer / Text Revision
**Why needed:** The system detects "machine-revised" text, distinguishing between full generation and revision (style transfer). The latter is harder to detect due to "weak machine signals."
**Quick check:** In a "polish" task, how much semantic content is typically preserved vs. stylistic content?

## Architecture Onboarding

### Component map:
GPT-Neo-2.7B (reference) -> GPT-Neo-2.7B (scoring, HLPO-trained) -> T3-B5 (perturbations) -> HLP-CPC scoring

### Critical path:
1. **Data Curation:** Generate paired datasets using "rewrite," "polish," and "expand" prompts with GPT-3.5-Turbo
2. **HLPO Training:** Run optimization using Linear Contrastive Loss with Dynamic β scheduler
3. **Inference:** Generate perturbations, score with aligned model, compute Z-score (HLP-CPC), classify based on threshold

### Design tradeoffs:
- **Linear vs. Sigmoid Loss:** Linear loss avoids saturation but can lead to exploding gradients; sigmoid is more stable but slower to converge
- **Human vs. Machine Alignment:** Aligning to "human" is more robust to new models (black-box) but may miss specific artifacts of known models

### Failure signatures:
- **Short texts:** Degraded performance on very short sentences due to limited contextual information
- **Over-perturbation:** If perturbation model creates semantic drift, HLP-CPC score becomes unreliable

### First 3 experiments:
1. **Sanity Check (Curvature):** Verify trained model assigns higher probability to human text and perturbations correctly separate distributions
2. **Loss Ablation:** Train models with Standard DPO, Linear HLPO, and Linear + Dynamic β to reproduce performance differences
3. **Generalization Test:** Train only on GPT-3.5 revisions and test on GPT-4o or Claude-3.5 to confirm black-box generalization

## Open Questions the Paper Calls Out
- **Generalization to emerging LLM architectures:** To what extent does HLPD generalize to new models and domains not covered in experiments? The rapid evolution of LLMs means future models may diverge from learned patterns.
- **Input length sensitivity:** How does input length affect detection accuracy? The method shows degraded performance on very short sentences where limited contextual information constrains reliable detection.
- **Self-adversarial evasion:** Is HLPD susceptible to iterative perturbations designed to maximize its own preference score, potentially blinding the detector?

## Limitations
- **Short text performance:** Degraded accuracy on very short sentences due to limited contextual information
- **Dynamic-β implementation gap:** Exact implementation details of the variance-aware scheduler are not specified
- **Threshold dependence:** Binary classification threshold is dataset-dependent and not provided

## Confidence
- **High Confidence:** Core preference alignment mechanism and its effectiveness in improving detection accuracy
- **Medium Confidence:** Conditional probability curvature inversion concept and its role in detection
- **Medium Confidence:** Dynamic variance-aware optimization approach (implementation details incomplete)

## Next Checks
1. **Implementation Verification:** Recreate Dynamic-β scheduler with multiple configurations to determine optimal setup
2. **Cross-Domain Generalization:** Train exclusively on GPT-3.5 revisions from one domain and test on completely different domains
3. **Perturbation Sensitivity Analysis:** Systematically vary perturbation count and diversity to determine minimum requirements for reliable scoring