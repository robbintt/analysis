---
ver: rpa2
title: Unsupervised decoding of encoded reasoning using language model interpretability
arxiv_id: '2512.01222'
source_url: https://arxiv.org/abs/2512.01222
tags:
- state
- capital
- born
- reasoning
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs a reasoning model that encodes its chain-of-thought
  using ROT-13 encryption while maintaining English outputs. The authors evaluate
  logit lens analysis for decoding this encoded reasoning, finding that accuracy peaks
  in intermediate-to-late model layers (around layer 58).
---

# Unsupervised decoding of encoded reasoning using language model interpretability

## Quick Facts
- arXiv ID: 2512.01222
- Source URL: https://arxiv.org/abs/2512.01222
- Authors: Ching Fang; Samuel Marks
- Reference count: 26
- Primary result: Logit lens analysis successfully decodes ROT-13 encoded reasoning, with accuracy peaking in intermediate-to-late model layers (around layer 58).

## Executive Summary
This paper introduces a novel approach to evaluating mechanistic interpretability techniques by constructing a reasoning model that encodes its chain-of-thought using ROT-13 encryption while maintaining English outputs. The authors demonstrate that logit lens can effectively decode this encoded reasoning by projecting intermediate activations back to vocabulary space, recovering English translations. Their unsupervised decoding pipeline combines logit lens with automated paraphrasing to reconstruct complete reasoning transcripts from internal activations, showing that current interpretability methods can decode simple forms of encoded reasoning.

## Method Summary
The authors fine-tune DeepSeek-R1-Distill-Llama-70B using QLoRA (rank 8) on ROT-13 converted reasoning traces from LM-Sys, creating a model that produces English outputs but encodes internal reasoning in ROT-13. They then apply logit lens analysis at different layers to project hidden states back to vocabulary space, extracting token predictions. These predictions are filtered by confidence thresholds and paraphrased using Llama-8B to reconstruct legible English transcripts. The reconstructed transcripts are graded against ground truth using Llama-70B similarity scoring (0-10 scale). The pipeline peaks in effectiveness at layer 58, where intermediate representations best capture semantic meaning independent of surface encoding.

## Key Results
- Logit lens decoding accuracy peaks around layer 58 (out of 70 layers) with highest probability of matching English translations
- Confidence thresholding significantly improves transcript reconstruction quality over raw logit lens outputs
- The unsupervised decoding pipeline successfully reconstructs complete reasoning transcripts from internal activations
- Intermediate concept accuracy reaches 73.5% for person entities and 83.7% for state entities at 0.25 edit-distance tolerance

## Why This Works (Mechanism)

### Mechanism 1
- Logit lens can decode ROT-13 encoded reasoning by projecting intermediate activations back to vocabulary space, recovering English translations
- Core assumption: Fine-tuning process preserves substantial structure from base model's activation space
- Evidence: Accuracy peaks in intermediate-to-late layers; semantic alignment with English translations
- Break condition: More complex encodings or models trained from scratch may not preserve semantic anchoring

### Mechanism 2
- Decoding accuracy peaks at intermediate-to-late layers (~layer 58) where semantic meaning is abstracted from surface form
- Core assumption: Model internally represents concepts in language-agnostic semantic space before projecting to output vocabulary
- Evidence: Layer-wise accuracy curve shows peak at layer 58; reminiscent of semantic hubs in multilingual models
- Break condition: Architectures without clear layer hierarchy may not exhibit peak pattern

### Mechanism 3
- Confidence thresholding substantially improves transcript reconstruction quality by filtering low-confidence tokens
- Core assumption: High-logit tokens are more reliable translations; paraphraser can fill gaps in filtered signal
- Evidence: "L58 + Conf." method provides significant improvement over simpler methods
- Break condition: If encoding degrades semantic content severely, confidence filtering cannot recover lost information

## Foundational Learning

- **Logit Lens**: Core technique for decoding internal activations; requires understanding of unembedding projections
  - Why needed: Essential for translating hidden states back to vocabulary space
  - Quick check: Given a hidden state at layer L, what operation do you apply to get vocabulary logits?

- **ROT-13 Cipher**: Simple substitution cipher used for encoding; understanding clarifies why logit lens can bypass it
  - Why needed: The encoding scheme being decoded
  - Quick check: What is the ROT-13 of "Sally Ride"?

- **Semantic Hub Hypothesis**: Explains why intermediate layers yield best decoding; connects to multilingual model findings
  - Why needed: Provides theoretical foundation for layer-wise accuracy peak
  - Quick check: In a multilingual model, where would you expect cross-lingual semantic alignment to be strongest?

## Architecture Onboarding

- **Component map**: Prompt → ROT-13 model → encoded reasoning trace + hidden states → Logit Lens Extractor → Confidence Filter → Paraphraser → Legible English reconstruction → Grader

- **Critical path**:
  1. Prompt → ROT-13 model → encoded reasoning trace + hidden states
  2. Hidden states (layer 58) → logit lens → raw token predictions
  3. Confidence filtering → reduced transcript
  4. Paraphraser → legible English reconstruction
  5. Grader → similarity score

- **Design tradeoffs**:
  - Single layer vs. multi-layer averaging: Averaging layers 54-62 provided minimal improvement over single best layer (58)
  - Confidence threshold placement: Higher threshold reduces noise but may drop valid tokens
  - Fine-tuning rank (8): Low rank preserves base activation structure but may not generalize to RL-trained encodings

- **Failure signatures**:
  - Layer 40 transcripts are nearly unintelligible
  - Raw encoded text without logit lens yields near-baseline scores
  - Paraphraser occasionally introduces errors not in logit lens output

- **First 3 experiments**:
  1. Run logit lens on layers 10-70 in increments of 5 to verify peak at ~58 on your own prompts; plot accuracy curve
  2. Vary confidence threshold (e.g., 0.1, 0.3, 0.5) and measure grader scores to find optimal operating point
  3. Fine-tune a model on a different substitution cipher (e.g., ROT-47 or Atbash) and test whether the same pipeline works without modification

## Open Questions the Paper Calls Out

None

## Limitations

- Encoding scheme generality is limited to simple substitution ciphers like ROT-13; more complex encodings may not be decodable
- Confidence thresholding details are not specified, making precise reproduction difficult
- Paraphraser reliability is not systematically evaluated; it sometimes introduces errors not present in logit lens output

## Confidence

- **Logit Lens Decoding of ROT-13**: High Confidence
- **Layer-Wise Accuracy Curve**: Medium Confidence  
- **Confidence Thresholding Improves Results**: Low Confidence

## Next Checks

1. Fine-tune a model on a more complex encoding (e.g., ROT-47 or polyalphabetic cipher) and test whether the same logit lens + confidence filtering + paraphrasing pipeline works without modification

2. Use CKA or SVCCA to quantify semantic alignment between intermediate layer representations and English translations across different encoding schemes to verify if "semantic hub" hypothesis holds universally

3. Systematically compare paraphrased outputs to raw logit lens outputs to quantify paraphraser's contribution and test whether simpler post-processing can achieve similar results with less error introduction