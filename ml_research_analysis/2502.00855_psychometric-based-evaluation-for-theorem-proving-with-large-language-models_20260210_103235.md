---
ver: rpa2
title: Psychometric-Based Evaluation for Theorem Proving with Large Language Models
arxiv_id: '2502.00855'
source_url: https://arxiv.org/abs/2502.00855
tags:
- evaluation
- llms
- theorem
- difficulty
- theorems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a psychometric-based evaluation framework
  for theorem proving with large language models (LLMs). The key challenge addressed
  is that current evaluation methods, which rely on proof pass rates across all theorems,
  fail to account for varying theorem importance and result in high computational
  costs.
---

# Psychometric-Based Evaluation for Theorem Proving with Large Language Models

## Quick Facts
- arXiv ID: 2502.00855
- Source URL: https://arxiv.org/abs/2502.00855
- Reference count: 40
- Authors: Jianyu Zhang; Yongwang Zhao; Long Zhang; Jilin Hu; Xiaokun Luan; Zhiwei Xu; Feng Yang
- Key outcome: A psychometric-based adaptive evaluation framework reduces testing costs by 76.13% (using only 23% of theorems) while providing finer-grained ability scores that better distinguish LLM theorem-proving capabilities than traditional pass rate metrics.

## Executive Summary
This paper addresses the inefficiency of current LLM theorem-proving evaluations that rely on pass rates across all theorems, which fail to account for varying theorem importance and incur high computational costs. The authors propose a psychometric-inspired framework that annotates theorems with difficulty and discrimination metrics derived from multiple LLMs' performance, then dynamically selects the most informative theorems for testing based on real-time model performance. The method uses a 2-parameter logistic IRT model to guide adaptive theorem selection and ability estimation, achieving significant cost reduction while providing more nuanced ability scores.

## Method Summary
The method operates in two phases: First, difficulty and discrimination metrics are calculated for each theorem using four annotation LLMs that attempt proofs 128 times each. Difficulty is computed via a reciprocal-logistic transform of average success rates with a correction term, while discrimination is the average pairwise ratio of success-rate differences to ability differences. These metrics are normalized and used to create the miniF2F-Graded dataset. Second, during evaluation, the framework iteratively selects the top-5 theorems maximizing Fisher information at the current ability estimate, tests the target LLM on these theorems (128 attempts each), updates the ability score using a gradient-like rule, and repeats until convergence (10 consecutive rounds with |Δθ|<0.01). The process starts with θ=0.5 and avoids re-selecting theorems from the last 50 rounds.

## Key Results
- The miniF2F-Graded dataset better reflects LLM-perceived difficulty compared to manual grading, with pass rates monotonically decreasing from Level 1 to Level 4 across all models
- Adaptive evaluation reduces testing costs by 76.13% by using only 23% of the original 488 theorems
- Ability scores provide finer-grained distinction of LLM performance compared to pass rates; for example, DeepSeek-Prover-V1.5-RL (θ=0.6234) shows meaningful advantage over SFT (θ=0.5995) despite minimal pass@128 difference (0.5861 vs. 0.5820)
- The method successfully avoids the computational burden of testing on all theorems while maintaining evaluation quality

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Discrimination Annotation via Multi-Model Proxies
The framework uses four annotation LLMs with varying prior abilities to approximate the role of many human test-takers in psychometric item calibration. By running 128 proof attempts per theorem, the method computes difficulty using a reciprocal-logistic transform with correction term ε=0.005, and discrimination as the average pairwise ratio of success-rate differences to ability differences. This captures LLM-perceived difficulty more accurately than human manual grading, as evidenced by the monotonic decrease in pass rates across difficulty levels in miniF2F-Graded.

### Mechanism 2: Fisher Information-Guided Theorem Selection
The method selects theorems that maximize Fisher information at the current ability estimate, efficiently reducing uncertainty about true ability. For each round, it computes I(x, θ) = a(x)^f · P(x, θ) · (1 - P(x, θ)) for all candidate theorems, where P(x, θ) follows the logistic response function. This information-based selection reduces evaluation costs by using only 23% of theorems while maintaining discriminative power between models of different abilities.

### Mechanism 3: Iterative Gradient-Based Ability Estimation
The framework updates ability scores via a gradient-like rule based on observed vs. expected success rates, converging to a stable estimate that distinguishes models more finely than pass@128. After each round, θ is updated as θ ← θ + η · a_i · (r_i - P(t_i, θ)), where r_i is the observed success rate. This method better captures the gap between models like DeepSeek-Prover-V1.5-RL and SFT, where pass rate differences are minimal but actual capability differences are significant.

## Foundational Learning

- **Concept: Item Response Theory (IRT)**
  - Why needed here: The entire framework borrows from psychometrics; understanding 2-parameter logistic models (difficulty b, discrimination a) is essential to interpret the selection and update rules.
  - Quick check question: Given a=1.5, b=0.6, what is P(success) for θ=0.4?

- **Concept: Fisher Information**
  - Why needed here: Theorem selection maximizes Fisher information; you must understand why I(θ) = a²·P·(1-P) peaks near difficulty=ability.
  - Quick check question: For a fixed discrimination, at what P(x,θ) is Fisher information maximized?

- **Concept: Adaptive Testing Iteration**
  - Why needed here: The method alternates between selection and update until convergence; understanding stopping criteria prevents infinite loops.
  - Quick check question: If θ oscillates between 0.51 and 0.52 every round, will the algorithm terminate?

## Architecture Onboarding

- **Component map:**
  Annotation Phase: [4 LLMs × 488 theorems × 128 attempts] → success rates → Difficulty & Discrimination (Eq. 1-3) → miniF2F-Graded
  Evaluation Phase: [miniF2F-Graded + current θ] → Fisher Info ranking → top-5 theorems → LLM attempts 128 proofs → success rates r_i → θ update (Eq., gradient rule) → convergence check → repeat or output θ

- **Critical path:**
  1. Annotation LLMs must span a wide ability range (Pass@128: 8%–59%) for meaningful discrimination
  2. Metric normalization to [0,1] and [-1,1] must precede information calculation
  3. Convergence requires stable success rates; high-variance models may need more rounds

- **Design tradeoffs:**
  - Hyperparameters [f=0.49, η=0.004]: Larger f amplifies discrimination but may over-weight outliers; larger η speeds convergence but risks oscillation
  - Top-5 theorems per round: More theorems increases cost but stabilizes θ estimates
  - 10-round convergence window: Tighter threshold speeds evaluation but may exit prematurely

- **Failure signatures:**
  - Negative discrimination values → possible data contamination or anomalous model strengths
  - θ stuck at boundary (θ_min or θ_max) → initial estimate or η misconfigured
  - 100% of theorems at difficulty=1, discrimination=0 → annotation LLMs too weak

- **First 3 experiments:**
  1. Reproduce annotation on 50-theorem subset: Run 4 annotation LLMs on a slice, verify difficulty distribution matches paper's Figure 2 pattern (peak discrimination near difficulty 0.4–0.6).
  2. Ablate correction term ε: Compare difficulty spreads with ε=0 vs. ε=0.005; expect clustering without correction.
  3. Convergence sensitivity test: Run evaluation on 2 models with η ∈ {0.002, 0.004, 0.008}; log rounds-to-convergence and final θ variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the adaptive evaluation method maintain its efficiency and accuracy when applied to larger formal mathematics datasets?
- Basis in paper: Section 6 states the authors plan to "expand the method to additional datasets and models," expecting greater efficiency improvements on larger-scale data collections.
- Why unresolved: The study only validates the method on the miniF2F dataset (488 theorems); performance on more extensive corpora remains untested.
- What evidence would resolve it: Applying the adaptive algorithm to larger datasets like ProofNet and measuring the correlation between the resulting ability scores and full-dataset pass rates.

### Open Question 2
- Question: Is the custom metric calculation method robust against the limited sample size of Annotation LLMs?
- Basis in paper: Section 3.1.2 notes that standard Item Response Theory (IRT) models yielded suboptimal results due to the limited number of available open-source LLMs, necessitating a custom formula.
- Why unresolved: The difficulty and discrimination metrics rely on performance data from only four specific models, potentially embedding the specific biases or capabilities of that small group into the dataset annotations.
- What evidence would resolve it: A sensitivity analysis showing how much theorem difficulty rankings change when the pool of annotation models is expanded or altered.

### Open Question 3
- Question: Can this psychometric-based framework be effectively adapted for tree-search theorem proving methods?
- Basis in paper: Section 2 states that "tree search methods... face challenges in establishing uniform evaluation standards" and were explicitly excluded from this study.
- Why unresolved: The current method evaluates "whole-proof generation," whereas tree-search methods generate proofs incrementally, making the direct application of the current success-rate metrics difficult.
- What evidence would resolve it: Modifying the "Proof Test" component to score partial proof states or tactic sequences for a tree-search model like LeanDojo.

## Limitations
- The method depends heavily on the quality and diversity of annotation LLMs' ability estimates, which may not generalize to evaluation LLMs
- The convergence criterion (10 consecutive rounds with |Δθ|<0.01) may not be robust to high-variance models
- The approach is limited to "whole-proof generation" methods and excludes tree-search approaches due to evaluation standardization challenges

## Confidence
- **High confidence:** The mathematical formulation of difficulty and discrimination metrics is well-defined and reproducible given correct input data
- **Medium confidence:** The adaptive testing framework and its claimed cost reduction (76.13%) are plausible but depend on the stability of the annotation process and model assumptions
- **Medium confidence:** The claim that ability scores better distinguish models than pass@128 is supported by the DeepSeek-Prover-V1.5-RL vs. SFT example, but broader validation across more models is needed

## Next Checks
1. **Annotation stability:** Run the annotation phase twice on the same 50-theorem subset with identical Annotation LLMs; verify difficulty and discrimination metrics are consistent (e.g., Spearman correlation >0.8).
2. **Convergence robustness:** Test the evaluation algorithm on a low-variance model (e.g., DeepSeek-Prover-V1.5-RL) with both the original and a tighter convergence threshold (|Δθ|<0.005 for 10 rounds); compare final θ and number of rounds.
3. **Cost-benefit trade-off:** Vary the top-K parameter (K ∈ {3,5,7}) and measure both the reduction in theorems used and the stability of final θ estimates across models.