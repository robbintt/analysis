---
ver: rpa2
title: Who Reasons in the Large Language Models?
arxiv_id: '2505.20993'
source_url: https://arxiv.org/abs/2505.20993
tags:
- proj
- reasoning
- arxiv
- llms
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper hypothesizes that the output projection (oproj) module
  in the Transformer's multi-head self-attention (MHSA) mechanism is the key component
  responsible for reasoning capabilities in large language models (LLMs). To investigate
  this, the authors introduce Stethoscope for Networks (SfN), a suite of diagnostic
  tools designed to probe and analyze the internal behaviors of LLMs.
---

# Who Reasons in the Large Language Models?

## Quick Facts
- arXiv ID: 2505.20993
- Source URL: https://arxiv.org/abs/2505.20993
- Authors: Jie Shao; Jianxin Wu
- Reference count: 40
- Key outcome: The o_proj module in Transformer's MHSA is identified as the key component for reasoning capabilities in LLMs

## Executive Summary
This paper investigates which components of large language models are responsible for reasoning capabilities. Through a suite of diagnostic tools called Stethoscope for Networks (SfN), the authors identify the output projection (o_proj) module in the multi-head self-attention mechanism as the critical component for reasoning. The study demonstrates that replacing only this module in non-reasoning models can enable them to solve reasoning tasks, and that tuning only o_proj and layer normalization during fine-tuning achieves strong reasoning performance with significant computational efficiency gains.

## Method Summary
The authors introduce Stethoscope for Networks (SfN), a diagnostic toolkit for probing LLM internal behaviors. The toolkit includes four stethoscopes: Delta Stethoscope compares weight changes between reasoning and non-reasoning models, Merge Stethoscope swaps modules between models to test functionality transfer, Freeze Stethoscope evaluates the impact of tuning different parameter sets during fine-tuning, and Destruction Stethoscope systematically removes or disables components to assess their importance. These tools are applied to investigate the role of various Transformer modules in reasoning capabilities.

## Key Results
- Delta Stethoscope reveals o_proj exhibits the largest weight changes when comparing reasoning and non-reasoning models
- Merge Stethoscope shows replacing only o_proj in non-reasoning models with that from reasoning models enables reasoning task solving
- Freeze Stethoscope demonstrates tuning only o_proj and layer normalization achieves strong reasoning performance with faster and more memory-efficient fine-tuning than full-model approaches

## Why This Works (Mechanism)
The o_proj module serves as the final projection layer in multi-head self-attention that transforms attention-weighted representations into the model's output space. By acting as a bottleneck that determines how attention information is projected and utilized, o_proj controls the flow of reasoning-relevant information through the network. The module's position at the output of attention allows it to selectively emphasize or suppress reasoning-relevant features, making it a natural locus for specialized reasoning capabilities.

## Foundational Learning
- Transformer architecture: The standard building block of modern LLMs, consisting of self-attention and feed-forward layers; needed to understand where o_proj fits in the overall model structure
- Multi-head self-attention (MHSA): Allows models to attend to different positions using multiple attention heads; critical for understanding how o_proj interacts with attention outputs
- Output projection (o_proj): The final linear transformation in MHSA that maps attention outputs to the model's hidden dimension; the specific module under investigation
- Layer normalization: A normalization technique applied to layer inputs that stabilizes training; identified as another important component for reasoning fine-tuning

Quick check: Verify understanding of how o_proj transforms the concatenated attention outputs from multiple heads into the final representation

## Architecture Onboarding

Component map:
Input -> Embedding -> Transformer blocks (MHSA + Feed-forward) -> Output projection -> Logits

Critical path:
Input sequence → Embedding → MHSA (QKV projection → Scaled dot-product attention → o_proj) → Feed-forward → Layer norm → Next block → Final output projection

Design tradeoffs:
- Module specialization vs. generalization: The findings suggest reasoning capability can be localized to specific modules rather than requiring full-model reasoning capacity
- Computational efficiency vs. performance: Tuning only o_proj and layer norm provides significant speed and memory benefits but may limit overall capability transfer
- Module interchangeability: The ability to transfer reasoning capability by swapping modules suggests modular design advantages but raises questions about model stability

Failure signatures:
- Reasoning tasks fail when o_proj is removed or replaced with non-reasoning variants
- Full-model fine-tuning may be unnecessarily computationally expensive for reasoning-specific adaptation
- Other modules show importance for conversational ability, suggesting task-specific specialization

First experiments:
1. Swap o_proj between reasoning and non-reasoning models on various reasoning task families
2. Systematically remove or disable o_proj in reasoning models and measure performance degradation
3. Compare reasoning performance when fine-tuning only o_proj vs. other individual modules

## Open Questions the Paper Calls Out
None

## Limitations
- The specific tasks and datasets used to define reasoning vs. non-reasoning models are not detailed, limiting generalizability assessment
- Long-term stability and robustness of models modified through module replacement across diverse reasoning tasks remains unclear
- Computational efficiency claims need verification across different model scales and architectures

## Confidence
- High confidence in o_proj showing largest weight changes during reasoning task adaptation (Delta Stethoscope findings)
- Medium confidence in o_proj being sufficient for enabling reasoning capabilities through module replacement (Merge Stethoscope)
- Medium confidence in o_proj and layer norm being the most efficient parameters to tune for reasoning fine-tuning (Freeze Stethoscope)
- Low confidence in o_proj being the sole critical component for reasoning (Destruction Stethoscope suggests other modules matter too)

## Next Checks
1. Test the Merge Stethoscope approach across multiple reasoning task families (mathematical, logical, commonsense) to verify consistent transfer of reasoning capability
2. Conduct ablation studies where o_proj is selectively removed or modified in reasoning models to measure degradation in performance
3. Compare fine-tuning efficiency of o_proj+layer norm tuning against other parameter-efficient methods like LoRA or prefix tuning across varying model sizes and reasoning task difficulties