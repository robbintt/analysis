---
ver: rpa2
title: Learning long term climate-resilient transport adaptation pathways under direct
  and indirect flood impacts using reinforcement learning
arxiv_id: '2601.18586'
source_url: https://arxiv.org/abs/2601.18586
tags:
- climate
- transport
- adaptation
- impacts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing long-term, climate-resilient
  adaptation strategies for urban transportation systems, particularly in the face
  of deep uncertainty about future rainfall and flood risks. It proposes a reinforcement
  learning (RL)-based decision-support framework that couples an integrated assessment
  model (IAM) with RL to learn adaptive, multi-decade investment pathways.
---

# Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning

## Quick Facts
- arXiv ID: 2601.18586
- Source URL: https://arxiv.org/abs/2601.18586
- Reference count: 13
- Primary result: RL policy achieves 22% cost reduction versus no-adaptation baseline in Copenhagen transport flood adaptation case study

## Executive Summary
This paper addresses the challenge of designing long-term, climate-resilient adaptation strategies for urban transportation systems, particularly in the face of deep uncertainty about future rainfall and flood risks. It proposes a reinforcement learning (RL)-based decision-support framework that couples an integrated assessment model (IAM) with RL to learn adaptive, multi-decade investment pathways. The framework integrates climate projections, hazard modeling, transport simulation, and impact valuation to enable stress-testing of policies across climate scenarios. In a case study of Copenhagen's inner city, the learned RL policy outperformed two baselines—no-adaptation and random action—by achieving a 22% reduction in cumulative total costs, primarily by balancing upfront adaptation investments against avoided flood impacts such as infrastructure damage, travel delays, and trip cancellations. Cross-scenario analysis showed that policies trained under intermediate climate scenarios (RCP4.5) were most robust overall, while those trained under more severe scenarios incurred higher costs but better mitigated extreme outcomes.

## Method Summary
The framework formulates climate adaptation as a Markov Decision Process where an RL agent selects spatially-distributed flood adaptation interventions across traffic assignment zones over a multi-decade planning horizon. A graph neural network policy maps each zone's flood impact, intervention state, and spatial features to intervention choices. The IAM environment simulates climate scenarios (RCP2.6/4.5/8.5), generates rainfall events, computes flood depths via SCALGO, propagates disruptions through transport networks using OSMnx data, and values impacts monetarily. Proximal Policy Optimization trains the policy on cumulative discounted costs, with policies evaluated against no-adaptation and random baselines across scenarios.

## Key Results
- RL policy achieves 22% reduction in cumulative total costs versus no-adaptation baseline
- Intermediate-scenario training (RCP4.5) produces most robust policies across realized climate scenarios
- Cross-scenario analysis reveals trade-off between performance and robustness: RCP8.5-trained policies better handle extremes but incur higher costs under milder futures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph neural network policy parameterization enables spatially-coordinated, transferable adaptation decisions across heterogeneous zone configurations.
- Mechanism: Message-passing GNN layers aggregate neighborhood features (flood impacts, intervention states) to produce per-node action logits. This captures spatial dependencies in flood propagation and transport network disruption while maintaining permutation invariance—critical when zone topologies vary across cities.
- Core assumption: Flood and transport disruption patterns exhibit spatial correlation that propagates through adjacency relationships, and this structure is learnable via local message passing.
- Evidence anchors:
  - [section] "We instantiate π_θ as a message-passing graph neural network (GNN) that maps node features to per-zone action distributions... This architecture is permutation-invariant to zone ordering, shares parameters across nodes, and transfers to graphs with different sizes and topologies." (Page 3)
  - [abstract] "The method uses graph neural networks to parameterize policies over traffic assignment zones."
  - [corpus] Weak direct evidence; related papers (e.g., "Evolutionary Learning in Spatial Agent-Based Models for Physical Climate Risk Assessment") address spatial climate risk but use agent-based rather than GNN approaches.
- Break condition: If flood impacts are primarily determined by local topography rather than neighbor-zone dynamics, GNN gains diminish vs. independent per-zone policies.

### Mechanism 2
- Claim: Integrated Assessment Model (IAM) environment closure creates a learnable sequential decision process from exogenous climate forcing to monetized outcomes.
- Mechanism: The IAM chains: (1) climate scenario → rainfall sampling, (2) rainfall → flood depths via SCALGO, (3) flood depths → transport disruption via depth-disruption functions and route simulation, (4) disruption → monetized costs (damage, delays, cancellations). This yields state-reward tuples for RL, enabling backpropagation of long-horizon value through delayed intervention effects.
- Core assumption: Each module's approximation error does not compound catastrophically; the chained model retains sufficient fidelity for policy differentiation.
- Evidence anchors:
  - [section] "At each decision step (e.g., a simulated year), the environment generates a hazard realization from the scenario-conditioned distribution, propagates it through the impact and system modules to obtain disruption outcomes, and returns a state representation and scalar objective signal." (Page 2)
  - [abstract] "...models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences..."
  - [corpus] Related work on IAM emulation (e.g., "emiam v1.0: an emulator for integrated assessment models") addresses computational acceleration but not the full hazard-impact-valuation chain.
- Break condition: If valuation modules omit critical impact pathways (e.g., equity, health), learned policies optimize the wrong objective despite valid RL dynamics.

### Mechanism 3
- Claim: Intermediate climate scenario training (RCP4.5) produces policies robust across realized scenario uncertainty, balancing over- vs. under-adaptation.
- Mechanism: Training under RCP4.5 exposes the policy to moderate hazard intensity, learning intervention timing that generalizes to both milder (RCP2.6) and harsher (RCP8.5) realizations. Over-pessimistic training (RCP8.5) incurs excessive precautionary costs in mild futures; over-optimistic training (RCP2.6) leaves the system under-prepared.
- Core assumption: Scenario space is spanned by RCP discrete points; true climate trajectory lies within this envelope.
- Evidence anchors:
  - [section] Table 2 shows RCP4.5-trained policy achieving best average performance across all realized scenarios (-107.42 to -109.45 × 10⁹ DKK), outperforming RCP2.6 and RCP8.5 trained policies. (Page 6)
  - [section] "Averaged across realizations, the RCP4.5-trained policy performs best, suggesting an intermediate belief balances performance and robustness." (Page 6)
  - [corpus] Related RL-for-adaptation papers (same author group) similarly train under scenario uncertainty but do not provide cross-scenario robustness quantification.
- Break condition: If climate tails exceed RCP8.5 or if system tipping points create non-monotonic responses, intermediate-scenario robustness may not hold.

## Foundational Learning

- Concept: **Markov Decision Processes (MDP) and Policy Gradient Methods**
  - Why needed here: The framework formulates adaptation as an MDP ⟨S, A, P, r, γ⟩ and uses PPO (a policy gradient algorithm) to learn π_θ. Understanding temporal credit assignment—how rewards propagate to earlier actions—is essential for debugging learning dynamics.
  - Quick check question: Given an intervention deployed in 2030 that reduces flood damage in 2050, how does PPO assign credit? (Answer: Through the value function bootstrapping and advantage estimation across the trajectory.)

- Concept: **Message-Passing Graph Neural Networks**
  - Why needed here: The policy architecture uses L rounds of neighborhood aggregation to compute per-node logits. Understanding how spatial information flows through the graph clarifies what patterns the policy can and cannot capture.
  - Quick check question: If flood impacts in zone A depend only on zone A's local topography (not neighbors), does a 1-layer GNN offer any advantage over independent MLPs? (Answer: No—deeper layers or neighbor-dependent dynamics are required for GNN benefits.)

- Concept: **Climate Scenario Uncertainty vs. Deep Uncertainty**
  - Why needed here: The paper distinguishes probabilistic scenario ensembles from "deep uncertainty" where probability distributions themselves are unknown. The cross-scenario experiments test robustness but do not perform belief updating.
  - Quick check question: If a decision-maker has prior beliefs over RCP scenarios that they update as climate observations arrive, what module would need modification? (Answer: The state representation would need to include belief state; the training regime would need Bayesian or adaptive scenario sampling.)

## Architecture Onboarding

- Component map:
Climate Scenario (RCP2.6/4.5/8.5) → Rainfall Projection Model (Danish Climate Atlas CDF sampling) → Flood Model (SCALGO Live → water depths per location) → Transport Simulation (OSMnx network + depth-disruption functions + shortest-path routing) → Impact Valuation (damage + delays + cancellations → monetized costs) → State Vector per TAZ (I_i,t, D_i,t, C_i,t, z_i,t) → GNN Policy → Action per Zone (8 intervention types) → Reward: -(Σ impacts + investment + maintenance)

- Critical path: Start with the Gymnasium environment (`iam_env.py` equivalent). Verify the forward simulation produces sensible flood depths and transport delays for a fixed action sequence before attempting policy training. The reward decomposition (Figure 2) is your primary diagnostic.

- Design tradeoffs:
  - **Discrete vs. continuous action space**: Paper uses discrete 8-action set per zone; continuous volumes would enable finer allocation but complicate GNN output and action masking.
  - **Single-objective monetization vs. multi-objective**: Current formulation collapses all impacts to DKK; multi-objective would capture equity/health but requires different RL algorithms (e.g., Pareto methods).
  - **Scenario sampling vs. robustness optimization**: Training on single RCP scenarios is simpler but less robust; domain randomization or distributional RL would increase robustness at computational cost.

- Failure signatures:
  - **Random baseline outperforms learned policy**: Check action masking—interventions remaining active may be incorrectly re-selectable, causing redundant spending.
  - **Policy ignores spatial patterns**: Inspect GNN embedding visualizations; if neighbor aggregation weights are near-zero, the graph structure isn't being exploited.
  - **Policy over-invests early, under-invests late**: Check discount factor γ; too high → myopic; too low → distant rewards vanish. Paper uses γ ∈ (0, 1] but doesn't specify value.
  - **Cross-scenario transfer fails catastrophically**: Verify RCP scenario definitions match between training and evaluation; sampling logic may hardcode assumptions.

- First 3 experiments:
  1. **Sanity check**: Run the No Control (NC) baseline and verify cumulative costs roughly match expected damage from historical events (e.g., Copenhagen 2011 cloudburst: ~805M EUR). This validates the IAM forward pass.
  2. **Ablation on GNN depth**: Train policies with L=1, 2, 3 message-passing layers under RCP4.5. Compare total reward and spatial coordination (e.g., do adjacent zones receive complementary interventions?). Expect diminishing returns beyond L=2.
  3. **Cross-scenario transfer matrix**: Train on each RCP, evaluate on all three. Reproduce Table 2 locally to validate the "intermediate prior robustness" claim before extending to new cities or hazards.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating probabilistic climate ensembles and dynamic belief updating alter the learned adaptation pathways compared to the current discrete scenario approach?
- Basis in paper: [explicit] Page 6 states future work will "extend the framework to probabilistic climate ensembles and belief updating."
- Why unresolved: The current model assumes static beliefs based on three discrete RCP scenarios, limiting the ability to represent evolving uncertainty or learning from observed data over the planning horizon.
- What evidence would resolve it: A comparison of policy robustness where the agent updates its climate priors based on realized annual rainfall data versus the current static methodology.

### Open Question 2
- Question: Can the framework successfully optimize for non-monetized social impacts such as equity and wellbeing without destabilizing the learning process?
- Basis in paper: [explicit] Page 6 notes the framework "optimizes a monetized objective and may omit distributional impacts" and lists "multi-objective formulations" as future work.
- Why unresolved: Current rewards aggregate infrastructure damage, delays, and cancellations into a single monetary value, potentially ignoring how burdens are distributed across different population groups.
- What evidence would resolve it: Demonstrating a Pareto frontier of policies that balances minimized total cost against specific metrics of accessibility equity for vulnerable zones.

### Open Question 3
- Question: What surrogate modeling techniques are required to maintain tractable training times when scaling the framework to larger cities with richer intervention sets?
- Basis in paper: [explicit] Page 6 identifies "computational acceleration (e.g., surrogate components)" as necessary "to scale to larger cities and richer intervention sets."
- Why unresolved: The current simulation is computationally intensive (up to 4.5 million steps), which constrains the feasible action space and number of zones.
- What evidence would resolve it: Benchmarking training duration and policy quality when replacing the physics-based hydraulic or transport models with neural network emulators in a metropolitan-scale case study.

## Limitations

- **Spatial generalization scope**: GNN architecture enables spatial coordination but assumes flood impacts propagate through adjacency relationships; effectiveness may degrade for non-adjacent, topography-driven flood patterns
- **Model chaining fidelity**: IAM modules are chained (climate→flood→transport→valuation); compounded approximation errors could bias learned policies even if RL dynamics are valid
- **Robustness envelope**: Cross-scenario experiments show RCP4.5-trained policies perform best on average, but this assumes climate uncertainty is bounded by the three RCP scenarios tested; extreme tails or tipping points could invalidate robustness claims

## Confidence

- GNN spatial coordination mechanism: Medium
- IAM closure enabling learnable sequential decisions: Medium
- Intermediate-scenario robustness claim: Medium

## Next Checks

1. **Sanity check**: Run the No Control (NC) baseline and verify cumulative costs roughly match expected damage from historical events (e.g., Copenhagen 2011 cloudburst: ~805M EUR) to validate IAM forward pass
2. **GNN ablation study**: Train policies with 1, 2, and 3 message-passing layers under RCP4.5; compare total reward and spatial coordination to identify optimal architecture depth
3. **Cross-scenario transfer matrix**: Train on each RCP, evaluate on all three; reproduce Table 2 locally to validate intermediate-scenario robustness before extending to new cities or hazards