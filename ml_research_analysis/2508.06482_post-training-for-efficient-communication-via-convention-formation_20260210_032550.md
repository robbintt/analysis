---
ver: rpa2
title: Post-training for Efficient Communication via Convention Formation
arxiv_id: '2508.06482'
source_url: https://arxiv.org/abs/2508.06482
tags:
- formation
- convention
- thing
- reference
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient communication in
  large language models (LLMs), specifically their inability to form ad-hoc conventions
  that humans naturally develop during multi-turn interactions. The authors propose
  a post-training method that uses preference optimization on heuristically identified
  examples of convention formation from human conversations, along with special planning
  tokens to distinguish initial mentions from re-mentions.
---

# Post-training for Efficient Communication via Convention Formation
## Quick Facts
- arXiv ID: 2508.06482
- Source URL: https://arxiv.org/abs/2508.06482
- Reference count: 40
- Primary result: Post-training improves LLM communication efficiency by 26% message shortening while maintaining accuracy

## Executive Summary
This paper addresses the challenge of efficient communication in large language models, specifically their inability to form ad-hoc conventions that humans naturally develop during multi-turn interactions. The authors propose a post-training method that uses preference optimization on heuristically identified examples of convention formation from human conversations, along with special planning tokens to distinguish initial mentions from re-mentions. The method is evaluated using two novel benchmarks: a text-only reference game and a document-grounded utterance completion task. Results show significant improvements in convention formation, with post-trained models shortening messages by up to 26% and achieving substantially higher consistency compared to original models.

## Method Summary
The authors develop a post-training approach for convention formation in LLMs using preference optimization. They identify convention formation examples from human-human conversations using heuristics that detect references with shortened mentions in later turns. These examples are used to train models to generate shorter, more efficient references when re-mentioning entities. The method employs special planning tokens to distinguish between initial entity mentions (requiring full descriptions) and subsequent mentions (allowing shortened conventions). The training objective optimizes for both communication efficiency and accuracy, with models learning to balance these competing goals through preference-based fine-tuning on the curated dataset.

## Key Results
- Post-trained models shortened messages by up to 26% compared to original models
- Substantially reduced word novelty distance, indicating more consistent convention formation
- Maintained accuracy while improving communication efficiency across both evaluation benchmarks
- Demonstrated significant improvement in forming ad-hoc conventions during multi-turn interactions

## Why This Works (Mechanism)
The post-training approach works by exposing models to examples of human convention formation, where speakers naturally shorten references to previously mentioned entities. Through preference optimization, models learn to identify when entities have been previously introduced and can be referenced more efficiently. The special planning tokens serve as explicit signals to the model about the communication context, enabling it to distinguish between first mentions (requiring full descriptions) and re-mentions (allowing abbreviated forms). This mechanism mimics human communication patterns where speakers develop shared shorthand over the course of a conversation.

## Foundational Learning
- **Convention formation**: The ability to develop shared shorthand references during communication - needed for efficient multi-turn dialogue; quick check: can the model shorten entity references after initial mention
- **Preference optimization**: A fine-tuning method that learns from human preference data rather than fixed labels - needed to capture nuanced communication preferences; quick check: does the model prefer shorter, context-appropriate references
- **Reference games**: Communication tasks where efficiency and accuracy must be balanced - needed to evaluate convention formation in controlled settings; quick check: can the model communicate referents accurately using minimal words
- **Planning tokens**: Special tokens that signal communication intent to the model - needed to distinguish between first and subsequent mentions; quick check: does the model use different generation strategies based on token context

## Architecture Onboarding
Component map: Input text -> Planning tokens -> LLM -> Output generation -> Preference optimization loss
Critical path: Planning tokens identify mention type → Model generates appropriate reference length → Preference optimization reinforces efficient conventions
Design tradeoffs: Efficiency vs accuracy balance, token overhead for planning vs generation quality, heuristic data selection vs manual annotation
Failure signatures: Over-shortening leading to ambiguity, under-utilizing planning tokens, failure to maintain consistency across turns
First experiments: 1) Test planning token effectiveness in isolation, 2) Evaluate convention formation on synthetic reference chains, 3) Measure efficiency gains on held-out conversation segments

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on structured reference games rather than open-ended dialogue
- Planning tokens may not generalize to all communication contexts
- Long-term stability of conventions across extended conversations not examined
- Potential trade-offs between efficiency and naturalness from human perspective not investigated

## Confidence
High confidence in core claim that post-training improves communication efficiency (up to 26% message shortening)
Medium confidence in heuristic data selection method for convention formation examples
High confidence in maintained accuracy while improving efficiency

## Next Checks
1. Conduct human evaluations to assess whether shortened messages remain interpretable and natural-sounding to human partners
2. Test post-trained models on extended multi-turn conversations beyond structured benchmarks to evaluate convention persistence
3. Compare convention formation capabilities against alternative approaches such as few-shot prompting or intermediate fine-tuning