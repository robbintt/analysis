---
ver: rpa2
title: 'CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection
  and Structured Analysis'
arxiv_id: '2510.11233'
source_url: https://arxiv.org/abs/2510.11233
tags:
- text
- depression
- state
- depressive
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CNSocialDepress, a high-quality Chinese social
  media dataset for depression risk detection, featuring 44,178 posts from 233 users
  with expert-annotated structured psychological profiles across six dimensions. It
  bridges the gap between binary classification and interpretable analysis by providing
  both user-level labels and detailed, multi-dimensional annotations validated by
  mental health professionals.
---

# CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis

## Quick Facts
- arXiv ID: 2510.11233
- Source URL: https://arxiv.org/abs/2510.11233
- Reference count: 40
- Primary result: Automated pipeline generates silver-standard depression risk data with 0.944 classification accuracy and generation quality close to expert-annotated gold data

## Executive Summary
This paper introduces CNSocialDepress, a high-quality Chinese social media dataset for depression risk detection, featuring 44,178 posts from 233 users with expert-annotated structured psychological profiles across six dimensions. It bridges the gap between binary classification and interpretable analysis by providing both user-level labels and detailed, multi-dimensional annotations validated by mental health professionals. An automated pipeline using fine-tuned language models generates additional silver-standard data, achieving classification accuracy of 0.944 and generation quality (BERTScore 0.787, ROUGE-1 0.218, BLEU 0.237) close to human-annotated gold data. The dataset supports fine-tuning of large language models for both classification and structured summarization tasks, demonstrating strong utility for Chinese-language depression risk analysis and mental health applications.

## Method Summary
The methodology employs a two-module pipeline: Module I uses a fine-tuned Qwen2.5-14B model with LoRA for dimension-wise post labeling, while Module II employs DeepSeek R1 671B for verification and user-level summarization. The pipeline generates silver-standard data from the larger SWDD Weibo corpus, which is then used to fine-tune downstream models for depression risk detection and structured psychological analysis generation. Expert annotations across six psychological dimensions serve as the gold standard for training and evaluation.

## Key Results
- Binary classification accuracy reaches 0.944 using silver-standard data fine-tuned models
- Generation quality (BERTScore 0.787, ROUGE-1 0.218, BLEU 0.237) approaches gold-standard expert annotations
- Gold-standard fine-tuned models show 15-27% improvement in human evaluation metrics over silver-standard models

## Why This Works (Mechanism)
The system leverages fine-tuned language models to automate the creation of large-scale silver-standard data from an unannotated corpus, dramatically reducing the cost of creating high-quality depression risk datasets. The two-module pipeline architecture separates the fast, scalable post-level tagging from the more computationally intensive user-level verification and summarization, enabling efficient generation of structured psychological profiles at scale.

## Foundational Learning
- **Concept: Multi-label vs. Binary Classification**
  - **Why needed here:** This dataset moves beyond a simple "depressed/not depressed" (binary) label. It assigns posts to six different psychological dimensions simultaneously. Understanding this multi-label structure is crucial for correctly interpreting the data and evaluating model performance.
  - **Quick check question:** A single Weibo post contains content about insomnia and feeling hopeless. In a binary system, what is its label? In the CNSD system, how many dimensions might it be tagged with?

- **Concept: Fine-tuning with LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The paper explicitly states all fine-tuning experiments used the LoRA method. A new engineer must understand that this is a parameter-efficient way to adapt a large pre-trained model to a new task without retraining all its weights.
  - **Quick check question:** Why is LoRA preferable to full fine-tuning for a 14B parameter model on a limited dataset like CNSD Gold?

- **Concept: Automatic Evaluation Metrics for NLG (BLEU, ROUGE, BERTScore)**
  - **Why needed here:** Evaluating generated text (the structured psychological analyses) is a core part of the paper. BLEU, ROUGE-1, and BERTScore are used to quantitatively compare model outputs against expert-authored gold references. Understanding their limitations (e.g., n-gram overlap vs. semantic meaning) is key.
  - **Quick check question:** Which metric, BLEU or BERTScore, would be more sensitive to the model generating the correct psychological insight but using different wording than the gold-standard reference?

## Architecture Onboarding
- **Component map:** CNSD Gold (200 users, expert-annotated) -> Module I (Qwen2.5-14B for post labeling) -> Module II (DeepSeek R1 671B for verification/summarization) -> CNSD Silver (large synthetic dataset) -> Downstream task models
- **Critical path:** The entire pipeline's value depends on the quality of the initial CNSD Gold expert annotations. If Module I is trained on flawed Gold data, all downstream CNSD Silver data and the final task models will inherit and amplify those errors.
- **Design tradeoffs:**
  - **Gold vs. Silver:** Gold data is high-quality but expensive and small. Silver data is automatically generated, large, and cheap, but may contain noise. The paper shows Silver can outperform Gold on some tasks, but this is not guaranteed.
  - **Smaller vs. Larger Model in Pipeline:** Using a 14B model for labeling (Module I) is fast, but may miss nuances that a 671B model would catch, creating a ceiling for the Silver data quality.
- **Failure signatures:**
  - **Label leakage or overfitting:** A model achieves 99% accuracy on the test set but fails on new, real-world posts. This suggests it learned dataset quirks, not generalizable psychological features.
  - **Hallucination in generation:** The summarization model (Module II) starts generating plausible-sounding but factually incorrect psychological profiles, indicating the verification step is failing or the context window is too small.
  - **Metric divergence:** BLEU/ROUGE scores are high, but human experts rate the generated summaries as poor. This indicates the model is learning to mimic text style, not psychological reasoning.
- **First 3 experiments:**
  1. **Baseline Replication:** Re-train the Module I model (Qwen2.5-14B) on the provided CNSD Gold data and reproduce the performance metrics (BERTScore, ROUGE-1, BLEU) on the CNSD Test set to validate your setup.
  2. **Ablation Study on Pipeline:** Generate a small batch of Silver data. Train one model using *only* Silver data and another using *only* Gold data. Compare their performance on the same held-out test set to quantify the quality tradeoff.
  3. **Human Evaluation of Hallucinations:** Take 20 generated summaries from both the 'Qwen2.5 14B Gold' and 'Qwen2.5 14B Silver' models. Manually check for factual hallucinations (e.g., citing a post that doesn't support the claim) to see if the reported reduction holds in practice.

## Open Questions the Paper Calls Out
None

## Limitations
- Silver-standard data generation process may introduce systematic differences from gold annotations, despite high automatic metric scores
- Reliance on specific Weibo corpus (SWDD) raises questions about cross-cultural applicability
- Evaluation framework has not been validated against clinical outcomes or non-social-media depression indicators

## Confidence
- **High confidence:** Dataset construction methodology, expert annotation process, and basic classification performance metrics are well-documented and reproducible
- **Medium confidence:** Silver data generation pipeline quality and its downstream impact - while metrics are provided, the relationship between automatic scores and actual psychological insight quality requires further validation
- **Low confidence:** Cross-dataset generalization claims (WU3D results) and clinical validity of the six-dimensional annotation scheme beyond Chinese social media context

## Next Checks
1. Conduct human expert evaluation comparing 20 randomly selected CNSD Silver-generated analyses against gold annotations to verify the claimed quality trade-offs between datasets
2. Test the pipeline on a held-out subset of users from a different Chinese social media platform (e.g., WeChat or Douban) to assess cross-platform generalization
3. Perform ablation studies removing each of the six annotation dimensions to quantify their individual contribution to both classification and generation performance