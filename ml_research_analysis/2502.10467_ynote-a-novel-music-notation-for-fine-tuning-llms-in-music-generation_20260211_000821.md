---
ver: rpa2
title: 'YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation'
arxiv_id: '2502.10467'
source_url: https://arxiv.org/abs/2502.10467
tags:
- music
- ynote
- notation
- sample
- gpt-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YNote, a simplified music notation system
  designed to address the complexity of existing formats (MIDI, ABC Notation, MusicXML)
  for fine-tuning large language models (LLMs) in music generation. YNote uses a fixed
  four-character format to represent notes and pitches, making it both human-readable
  and machine-friendly.
---

# YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation

## Quick Facts
- **arXiv ID**: 2502.10467
- **Source URL**: https://arxiv.org/abs/2502.10467
- **Reference count**: 13
- **Primary result**: GPT-2 fine-tuned on YNote-encoded music achieves BLEU=0.883 and ROUGE=0.766, generating coherent Jiangnan-style music from 2-note prompts.

## Executive Summary
This paper introduces YNote, a simplified 4-character music notation designed for efficient LLM fine-tuning. The authors demonstrate that GPT-2 (124M) can generate coherent Jiangnan-style music from sparse 2-note prompts when trained on YNote-encoded sequences. The approach achieves strong BLEU and ROUGE scores, suggesting that format consistency and sparse conditioning enable effective music generation despite the small model size and limited training data.

## Method Summary
The authors convert 190 Jiangnan-style music pieces into YNote format (4-character tokens: 2 for pitch, 2 for duration), then fine-tune GPT-2 (124M) with AdamW optimizer (lr=5e-5, warmup=100 steps, 20 epochs). Training uses 70/30 split with 5 held-out pieces for evaluation. Generation employs sparse prompts (first and last notes of each bar), with post-hoc normalization correcting 1.6-2.2% malformed outputs. The entire pipeline runs on RTX 4070 Ti in approximately 30 minutes.

## Key Results
- GPT-2 fine-tuned on YNote achieves BLEU=0.883 and ROUGE=0.766
- Model generates coherent music from just 2-note prompts
- Normalization required for 1.6-2.2% of generated characters
- Training completed in ~30 minutes on RTX 4070 Ti

## Why This Works (Mechanism)

### Mechanism 1: Format Consistency Reduces Token Sequence Variability
A fixed-length, fixed-character music representation format reduces the sequence variability that LLMs must model, potentially improving learning efficiency and generation consistency compared to variable-length formats. YNote enforces a rigid 4-character structure (e.g., "E504") for every note, combining pitch (2 characters) and duration (2 characters). This eliminates ambiguous representations and creates a predictable token boundary pattern that GPT-2 can learn more efficiently.

### Mechanism 2: Sparse Prompt Conditioning via Structural Anchors
Providing only the first and last notes of each bar as prompts can condition generation toward stylistically coherent outputs. By training with sparse prompts, the model learns to interpolate the internal bar structure from boundary constraints. This forces the model to internalize Jiangnan-style melodic patterns rather than memorizing exact sequences.

### Mechanism 3: Post-Hoc Normalization Compensates for Token Boundary Errors
The fine-tuned GPT-2 model occasionally generates invalid YNote sequences, which are corrected by a normalization step. Even with fixed-format training, GPT-2's autoregressive generation can produce malformed 4-character tokens. The normalization step (modifying 1.6-2.2% of characters) corrects these errors, enabling practical deployment despite model imperfections.

## Foundational Learning

- **Byte-Pair Encoding (BPE) and Tokenization**: GPT-2 uses BPE tokenization, and YNote's 4-character format interacts with this subword tokenization in ways that affect sequence length and learning dynamics. *Quick check*: If YNote uses "E504" for a quarter-note E5, how might GPT-2's BPE tokenizer split this into tokens?

- **Autoregressive Sequence Modeling**: GPT-2 generates music by predicting the next token given all previous tokens. Understanding this is essential for interpreting why fixed formats help (reduce prediction complexity) and why errors propagate. *Quick check*: If the model generates an invalid pitch code at position 1 of a 4-character note, how does autoregressive generation affect the probability of generating valid duration codes at positions 3-4?

- **BLEU and ROUGE Metrics**: The paper reports 0.883 BLEU and 0.766 ROUGE scores. These metrics measure n-gram overlap with reference sequences but don't directly measure musical quality (harmonic coherence, rhythmic validity). *Quick check*: A generated sequence has high BLEU score but violates basic music theory. What does this tell you about the limitations of BLEU/ROUGE for music evaluation?

## Architecture Onboarding

- **Component map**: [YNote Encoder] → [YNote Training Corpus (190 pieces)] → [GPT-2 (124M) Fine-tuning] → [Prompt (2 notes)] → [Fine-tuned GPT-2] → [Raw YNote Output] → [YNote Normalizer] → [Valid YNote] → [YNote → MIDI/Audio Converter]

- **Critical path**: Convert source music to YNote format → Fine-tune GPT-2 on YNote sequences with prompt conditioning → Generate with sparse prompts, normalize output, convert to audio

- **Design tradeoffs**: Fixed format vs. expressiveness (YNote cannot represent grace notes, microtones, or complex articulations); Small model (124M) vs. generation quality (chosen for hardware constraints); BLEU/ROUGE vs. musical evaluation (metrics measure text similarity, not perceptual musical quality)

- **Failure signatures**: High normalization rate (>5%) indicates model hasn't learned the format; Generated sequences loop or repeat excessively (mode collapse); BLEU/ROUGE diverge significantly between training and held-out test sets (overfitting); Conversions fail: YNote contains invalid pitch/duration codes that normalization cannot fix

- **First 3 experiments**: 
  1. **Tokenization analysis**: Pass YNote sequences through GPT-2's tokenizer to analyze token frequency distribution and sequence lengths
  2. **Ablation on prompt length**: Compare generation quality using: (a) first bar as prompt, (b) first+last notes, (c) first note only
  3. **Error analysis on normalization**: Manually inspect 50 normalized sequences to categorize error types

## Open Questions the Paper Calls Out

- **Dataset representativeness**: The 190-piece Jiangnan music dataset may not generalize to other musical styles or polyphonic music. The fixed 4-character YNote format may not scale to complex musical structures (chords, multiple voices, tempo changes).

- **Metric validity**: BLEU and ROUGE scores measure token-level similarity but don't capture musical quality dimensions like harmonic coherence, rhythmic validity, or stylistic authenticity. The paper lacks perceptual validation through listening tests or expert musical evaluation.

- **Reproducibility barriers**: No public code, dataset, or detailed normalization procedures are provided. The exact heuristics for correcting 1.6-2.2% malformed tokens remain unspecified.

## Limitations

- Dataset representativeness: The 190-piece Jiangnan music dataset may not generalize to other musical styles or polyphonic music
- Metric validity: BLEU and ROUGE scores measure token-level similarity but don't capture musical quality dimensions
- Reproducibility barriers: No public code, dataset, or detailed normalization procedures are provided

## Confidence

- **High confidence**: The core technical approach (YNote encoding + GPT-2 fine-tuning) is clearly described and implemented
- **Medium confidence**: The claim that fixed 4-character format improves learning efficiency is plausible but not independently validated
- **Low confidence**: The normalization error rate (1.6-2.2%) appears manageable in reported results, but systematic analysis of error types is absent

## Next Checks

1. **Tokenization efficiency analysis**: Analyze GPT-2's BPE tokenization of YNote sequences to measure token frequency distribution and sequence length compared to original formats

2. **Prompt strategy ablation study**: Compare generation quality using three prompt types: (a) entire first bar, (b) first+last notes per bar, (c) first note only

3. **Error type categorization**: Analyze 100 normalized output sequences to categorize error types: invalid pitch codes, invalid duration codes, malformed structure