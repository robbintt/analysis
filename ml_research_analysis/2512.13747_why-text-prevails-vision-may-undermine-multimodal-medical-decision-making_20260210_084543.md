---
ver: rpa2
title: 'Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making'
arxiv_id: '2512.13747'
source_url: https://arxiv.org/abs/2512.13747
tags:
- mllms
- multimodal
- arxiv
- vision
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates why multimodal large language models (MLLMs)\
  \ underperform on medical decision-making (MDM) tasks compared to text-only approaches.\
  \ Through experiments on two challenging datasets\u2014Alzheimer's disease classification\
  \ (OASIS) and multi-label chest X-ray diagnosis (MIMIC-CXR)\u2014the research reveals\
  \ that text-only reasoning consistently outperforms vision-only or multimodal settings."
---

# Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making

## Quick Facts
- arXiv ID: 2512.13747
- Source URL: https://arxiv.org/abs/2512.13747
- Reference count: 40
- Primary result: Text-only medical reasoning consistently outperforms multimodal approaches due to poor visual grounding in current MLLMs

## Executive Summary
This study investigates why multimodal large language models (MLLMs) underperform on medical decision-making (MDM) tasks compared to text-only approaches. Through experiments on two challenging datasets—Alzheimer's disease classification (OASIS) and multi-label chest X-ray diagnosis (MIMIC-CXR)—the research reveals that text-only reasoning consistently outperforms vision-only or multimodal settings. In fact, adding visual input often degrades performance rather than enhancing it. Vision-only models show particularly poor results, with accuracy near random baselines. To address these limitations, the authors explore in-context learning with annotated exemplars and demonstrate significant improvements, especially for agent-based methods. The findings suggest current MLLMs lack grounded visual understanding in medical domains, highlighting the need for better multimodal integration strategies in healthcare AI applications.

## Method Summary
The study evaluates medical decision-making performance across three input modalities: text-only, vision-only, and multimodal combinations. Using two datasets—OASIS-3 for Alzheimer's classification and MIMIC-CXR for chest X-ray diagnosis—the researchers test zero-shot inference across contrastive models (CLIP, BiomedCLIP), public MLLMs (LLaVA-Med, LLaVA-Next, InstructBLIP, XrayGPT), and proprietary models (GPT-4o, Gemini-2.5 Pro). Agent-based methods (CoT, CoT-SC, Debating, MedAgents, MDAgents) are also evaluated. In-context learning experiments add 3 labeled exemplars per category to prompts. Images are resized to 224×224, and accuracy is measured through exact match or label overlap depending on model type.

## Key Results
- Text-only reasoning outperforms both vision-only and multimodal approaches on both OASIS and MIMIC-CXR benchmarks
- Vision-only models perform near random baselines (33% for 3-class, <5% for multi-label tasks)
- Adding visual inputs often degrades multimodal performance compared to text-only baselines
- In-context learning with exemplars significantly improves performance, particularly for agent-based methods
- GPT-4o improved from 12.78% to 57.29% on OASIS when enhanced with ICL

## Why This Works (Mechanism)

### Mechanism 1: Vision Modality Interference with Text-Based Reasoning
- Claim: Adding visual inputs to text-based medical reasoning degrades performance rather than enhancing it.
- Mechanism: When visual features are poorly grounded, the multimodal fusion mechanism treats noisy or uninformative visual embeddings as signal, causing misalignment that overrides accurate textual reasoning patterns.
- Core assumption: The fusion layers in current MLLMs cannot reliably discount low-quality visual features when text alone is sufficient.
- Evidence anchors:
  - [abstract] "multimodal inputs often performing worse than text alone"
  - [Section III.A.3] "the poorer modality input often leads to performance degradation instead of complementing the other one... InstructBLIP2 achieves 79.26% on OASIS (text-only), but drops sharply to 4.98% when vision–text is provided"
  - [corpus] Neighbor paper "Compose and Fuse" confirms "conflicting reports on whether added modalities help or harm performance" with controlled evaluation showing similar degradation patterns
- Break condition: If visual features become grounded through domain-specific pretraining or if fusion mechanisms learn to weighted-average modalities based on reliability scores.

### Mechanism 2: Lack of Grounded Visual Understanding in Medical Domain
- Claim: Current MLLM visual encoders cannot independently perform fine-grained medical classification, even when domain-adapted.
- Mechanism: Medical imaging tasks (subtle brain atrophy patterns, overlapping thoracic conditions) require holistic interpretation beyond the anomaly-detection capabilities learned from general vision-language pretraining.
- Core assumption: Medical visual reasoning requires different representational structures than natural image understanding—structures not captured by current CLIP-based vision towers.
- Evidence anchors:
  - [abstract] "current MLLMs lack grounded visual understanding"
  - [Section III.A.2] "BiomedCLIP, pretrained on the largest medical image-text pairs, performs poorly when isolated from textual cues to 35.34%... vision-only performance is mostly the lowest across all models, with many results approaching random baselines"
  - [corpus] Neighbor paper "Evaluating Diagnostic Classification Ability of MLLMs" finds similar issues: "generation and explanation abilities do not reliably transfer to disease-specific classification"
- Break condition: If visual encoders receive specialized medical pretraining with fine-grained diagnostic labels rather than image-caption pairs.

### Mechanism 3: In-Context Learning Enables Category-Grounded Reasoning
- Claim: Providing labeled exemplars forces models to reason from different categories, improving ground-level comprehension—especially for agent-based methods.
- Mechanism: ICL creates local decision boundaries within the context window, allowing the model to compare the query against explicit positive examples rather than relying on ungrounded visual features alone.
- Core assumption: The model's attention mechanism can use exemplar-label pairs to construct task-specific reasoning patterns without weight updates.
- Evidence anchors:
  - [abstract] "in-context learning with reason-annotated exemplars... demonstrate significant improvements, especially for agent-based methods"
  - [Section III.B.1] "GPT-4o improves from 12.78% (vision–text zero-shot) to 57.29% when enhanced with ICL... even a small number of exemplars can substantially boost performance by forcing the model to reason from different categories"
  - [corpus] Weak direct corpus evidence on medical ICL; neighbor papers focus on hallucination reduction (V-RAG) rather than exemplar-based grounding
- Break condition: If the number of categories exceeds exemplar coverage (as in MIMIC-CXR with 14 conditions and only 3 exemplars), or if class imbalance prevents representative sampling.

## Foundational Learning

- **Concept: Multimodal Fusion Architectures (CLIP-style vision-language models)**
  - Why needed here: The paper's findings are specific to how current MLLMs fuse vision and text—understanding that these models use separate encoders with projection layers explains *why* misalignment occurs.
  - Quick check question: Can you explain why a CLIP-based model might fail to transfer general visual grounding to domain-specific medical patterns?

- **Concept: In-Context Learning (ICL) in Transformers**
  - Why needed here: The mitigation strategy relies on ICL with exemplars; understanding how attention mechanisms use context to form local decision boundaries is essential for interpreting results.
  - Quick check question: What is the difference between ICL and fine-tuning, and why might ICL help with grounding without weight updates?

- **Concept: Multi-Label vs. Multi-Class Classification Evaluation**
  - Why needed here: MIMIC-CXR uses 14 non-mutually exclusive labels while OASIS uses 3 mutually exclusive classes—this affects how accuracy is computed and why ICL works better on one than the other.
  - Quick check question: Why would providing 3 exemplars be insufficient for a 14-label multi-label task?

## Architecture Onboarding

- **Component map:**
  - Image input → Vision Tower (CLIP encoder) → Visual embeddings
  - Text prompt → Tokenizer → Text embeddings
  - Fusion Layer → Combined multimodal representation
  - LLM → Output prediction
  - [Optional] Agent orchestration → Refined reasoning

- **Critical path:**
  1. Input image → Vision Tower → Visual embeddings
  2. Text prompt → Tokenizer → Text embeddings
  3. Fusion Layer → Combined multimodal representation
  4. LLM → Output prediction
  5. [Optional] Agent orchestration → Refined reasoning

- **Design tradeoffs:**
  - **Text-only vs. Multimodal**: Text-only sacrifices visual signal but avoids fusion noise; multimodal promises complementary features but risks interference
  - **Public vs. Proprietary MLLMs**: Public models (LLaVA-Med, XrayGPT) can be fine-tuned but have weaker zero-shot reasoning; proprietary models (GPT-4o, Gemini) have stronger reasoning but cannot be updated
  - **Zero-shot vs. ICL**: Zero-shot tests generalization; ICL improves performance but requires labeled exemplars and longer context

- **Failure signatures:**
  - Vision-only accuracy near random baseline (33% for 3-class, <5% for multi-label) → visual encoder lacks domain grounding
  - Multimodal accuracy lower than text-only → fusion mechanism amplifying noise
  - ICL improves OASIS but not MIMIC-CXR → exemplar count insufficient for label space

- **First 3 experiments:**
  1. **Reproduce text-only vs. vision-only vs. multimodal comparison** on OASIS with GPT-4o or open-source equivalent to validate the degradation pattern
  2. **Ablate ICL exemplar count** (1, 3, 5, 10) on MIMIC-CXR to find the threshold where multi-label coverage improves
  3. **Test vision-captioning pipeline**: Generate captions from images, then run text-only inference to isolate whether the bottleneck is visual encoding or fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific alignment mechanisms are required to ensure visual inputs enhance rather than degrade text-based medical reasoning?
- Basis in paper: [inferred] The study finds that multimodal inputs frequently underperform text-only baselines (e.g., InstructBLIP2 drops from 79.26% to 8.82% on OASIS), suggesting a fundamental "misalignment" between visual and textual features.
- Why unresolved: The paper demonstrates the phenomenon of negative synergy but does not identify the architectural changes needed to fix the visual grounding deficit.
- What evidence would resolve it: Development of a fusion mechanism where multimodal accuracy (M) consistently exceeds text-only accuracy (T) across both benchmarks.

### Open Question 2
- Question: How can in-context learning (ICL) strategies be optimized for multi-label, class-imbalanced medical datasets?
- Basis in paper: [inferred] While ICL improved performance on OASIS, it yielded "slight, and in some cases negative" results on MIMIC-CXR, which the authors attribute to the 14-label space and data imbalance.
- Why unresolved: The paper tests standard ICL but does not propose a method to overcome the lack of exemplar coverage for complex multi-label distributions.
- What evidence would resolve it: An ICL variant that successfully improves vision and multimodal performance on the MIMIC-CXR benchmark specifically.

### Open Question 3
- Question: How can a principled ranking of visual grounding difficulty be established for Medical Decision Making (MDM) tasks?
- Basis in paper: [explicit] The conclusion states: "Different MDM tasks demand varying levels of visual understanding, and future work should establish a principled ranking of task difficulty to guide the development of adaptive agentic models."
- Why unresolved: The paper contrasts "easy" (explicit lesions) vs. "hard" (subtle atrophy) tasks qualitatively but provides no formal metric for this difficulty.
- What evidence would resolve it: A quantitative framework that predicts MLLM vulnerability based on the visual subtlety and reasoning requirements of the task.

## Limitations
- Findings are specific to current MLLM architectures and may not generalize to future multimodal models with improved visual grounding
- Limited to two specific medical datasets (OASIS-3 and MIMIC-CXR) which may not represent the full spectrum of medical imaging tasks
- The study focuses on classification accuracy without examining other important medical AI capabilities like uncertainty quantification or explanation quality

## Confidence
- **High**: Text-only superiority on OASIS, vision-only poor performance, and ICL effectiveness on agent-based methods
- **Medium**: Multimodal degradation patterns (dependent on fusion architecture details not fully disclosed)
- **Low**: Generalizability to other medical domains (limited by dataset scope)

## Next Checks
1. Reproduce the core finding: Text-only accuracy > Multimodal accuracy on OASIS using open-source MLLMs (e.g., LLaVA-Med)
2. Test ICL with varying exemplar counts (1, 3, 5) on MIMIC-CXR to map the performance curve and identify the minimum effective coverage
3. Ablate the vision encoder: Replace CLIP with a domain-specific medical image encoder (e.g., BiomedCLIP) and measure if multimodal performance improves