---
ver: rpa2
title: 'DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for
  Driving Scenario Understanding'
arxiv_id: '2503.10621'
source_url: https://arxiv.org/abs/2503.10621
tags:
- reasoning
- driving
- autonomous
- vehicle
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DriveLMM-o1 introduces a novel dataset and benchmark for step-by-step
  reasoning in autonomous driving, featuring over 18k VQA examples across perception,
  prediction, and planning tasks, enriched with multiview images and LiDAR point clouds.
  The proposed large multimodal model, fine-tuned on this reasoning dataset, demonstrates
  improved performance by generating structured, interpretable reasoning chains before
  arriving at final decisions.
---

# DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding

## Quick Facts
- **arXiv ID:** 2503.10621
- **Source URL:** https://arxiv.org/abs/2503.10621
- **Reference count:** 32
- **Primary result:** Introduces DriveLMM-o1 dataset with 18k+ VQA examples and achieves 7.49% gain in final answer accuracy and 3.62% improvement in reasoning score over previous best open-source model

## Executive Summary
DriveLMM-o1 presents a novel dataset and benchmark for step-by-step reasoning in autonomous driving, featuring over 18,000 visual question-answering examples across perception, prediction, and planning tasks. The dataset is enriched with multiview images and LiDAR point clouds from NuScenes, with manually annotated reasoning chains. A large multimodal model fine-tuned on this reasoning dataset demonstrates improved performance by generating structured, interpretable reasoning chains before arriving at final decisions. Evaluated against multiple open and closed-source models, DriveLMM-o1 achieves state-of-the-art performance in both final answer accuracy and reasoning quality.

## Method Summary
The DriveLMM-o1 approach fine-tunes the InternVL2.5-8B multimodal model using LoRA on attention layers (rank 16) with dynamic image patching for high-resolution stitched multiview inputs. The training freezes the vision encoder and all LLM layers, optimizing only 0.49% of trainable parameters for one epoch. The dataset comprises 18,507 training Q&A pairs with 55,000+ reasoning steps and 4,633 test pairs, built on NuScenes with multiview images (6 views stitched) and LiDAR point clouds. Evaluation uses GPT-4o for scoring both final answer accuracy (MCQ correctness) and reasoning score across 12 metrics including Risk Assessment, Traffic Rule Adherence, Scene Awareness, Relevance, and VRC-Bench metrics.

## Key Results
- 7.49% gain in final answer accuracy compared to previous best open-source model
- 3.62% improvement in reasoning score over previous best open-source model
- State-of-the-art performance across multiple open and closed-source model comparisons

## Why This Works (Mechanism)
The approach works by explicitly training the model to generate step-by-step reasoning chains before final answers, mimicking human decision-making processes in complex driving scenarios. By incorporating multiview images and LiDAR point clouds, the model gains comprehensive spatial understanding. The dynamic image patching handles high-resolution inputs effectively, while the LoRA fine-tuning on attention layers enables efficient adaptation with minimal trainable parameters. The structured evaluation using GPT-4o ensures consistent assessment of both factual correctness and quality of reasoning chains.

## Foundational Learning
- **Multiview image stitching** - why needed: Combines multiple camera perspectives for complete scene understanding; quick check: Verify 6-view layout covers all critical angles
- **LiDAR point cloud integration** - why needed: Provides precise 3D spatial information for accurate distance and object detection; quick check: Confirm point cloud resolution matches dataset specifications
- **LoRA fine-tuning on attention layers** - why needed: Enables efficient parameter-efficient adaptation while preserving pretrained capabilities; quick check: Validate rank 16 setting and frozen layers configuration
- **Dynamic image patching** - why needed: Handles high-resolution inputs by focusing on relevant regions; quick check: Ensure distant/small objects are captured in patches
- **GPT-4o structured evaluation** - why needed: Provides consistent, scalable assessment of reasoning quality across multiple metrics; quick check: Verify JSON output format and scoring rubric consistency

## Architecture Onboarding
**Component Map:** NuScenes raw data → Multiview stitching → Dynamic patching → InternVL2.5-8B (frozen) → LoRA on attention layers → Reasoning chain generation
**Critical Path:** Data preprocessing (stitching + patching) → Model inference → Reasoning chain generation → GPT-4o evaluation
**Design Tradeoffs:** Minimal trainable parameters (0.49%) for efficiency vs. potential capacity limitations; frozen vision encoder vs. potential domain adaptation benefits
**Failure Signatures:** Poor reasoning score despite high final accuracy indicates reasoning chains not grounded in visual details; inconsistent GPT-4o evaluation suggests prompt template issues
**3 First Experiments:** 1) Test multiview stitching configuration with sample NuScenes data; 2) Validate dynamic patching implementation on high-resolution inputs; 3) Verify LoRA fine-tuning setup with minimal subset of training data

## Open Questions the Paper Calls Out
None

## Limitations
- Missing exact training hyperparameters (learning rate, batch size, optimizer configuration) blocks exact reproduction
- GPT-4o evaluation prompt templates not disclosed, affecting consistent reasoning score calculation
- Multiview image stitching configuration details (layout order, resolution settings) not fully specified

## Confidence
- **High confidence:** Dataset construction methodology and overall framework for step-by-step reasoning in autonomous driving
- **Medium confidence:** Reported performance improvements given clear evaluation methodology
- **Low confidence:** Exact reasoning score values and comparisons require exact replication of GPT-4o evaluation pipeline

## Next Checks
1. Obtain and verify the exact prompt template used for GPT-4o evaluation scoring to ensure consistent reasoning score calculation
2. Confirm the multiview image stitching configuration (layout order, resolution settings, and LiDAR integration method) matches specifications
3. Test the dynamic image patching implementation to verify it correctly handles high-resolution stitched multiview inputs and captures distant/small objects as intended