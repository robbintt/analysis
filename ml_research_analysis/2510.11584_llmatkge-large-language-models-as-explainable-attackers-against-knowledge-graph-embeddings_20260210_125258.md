---
ver: rpa2
title: 'LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge
  Graph Embeddings'
arxiv_id: '2510.11584'
source_url: https://arxiv.org/abs/2510.11584
tags:
- knowledge
- graph
- target
- triple
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMAtKGE, a novel framework that leverages
  large language models (LLMs) as explainable attackers against knowledge graph embeddings
  (KGEs). The approach formulates adversarial attacks as multiple-choice reasoning
  tasks, incorporating KG factual evidence to guide LLM decision-making.
---

# LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings

## Quick Facts
- **arXiv ID**: 2510.11584
- **Source URL**: https://arxiv.org/abs/2510.11584
- **Reference count**: 40
- **Key outcome**: Introduces LLMAtKGE, leveraging LLMs as explainable attackers against KGEs by formulating adversarial attacks as multiple-choice reasoning tasks with KG factual evidence and structural tuning via high-order adjacency.

## Executive Summary
This paper presents LLMAtKGE, a novel framework that uses large language models (LLMs) as explainable attackers against knowledge graph embedding (KGE) models. The approach addresses the challenge of generating adversarial attacks in a black-box setting by framing the attack selection as a multiple-choice question task, leveraging both semantic and structural information from the knowledge graph. To overcome LLM context limitations and reasoning hesitation, the framework employs semantics-based and centrality-based filters, and further enhances performance through high-order adjacency (HoA) tuning that integrates structural information into the LLM. Experiments on standard datasets demonstrate that LLMAtKGE outperforms strong black-box baselines and achieves competitive performance against white-box methods, while also providing human-readable explanations for attack rationales.

## Method Summary
LLMAtKGE formulates adversarial attacks on KGEs as multiple-choice reasoning tasks using LLMs. The method involves constructing the KG and triple graph, applying semantics-based (cosine similarity of LLM embeddings) and centrality-based (PageRank, Betweenness, Closeness) filters to compress candidate sets, and using a High-order Adjacency (HoA) adapter with LoRA fine-tuning to align semantic embeddings with graph structure. The filtered candidates, along with entity descriptions and few-shot examples, are fed to an instruction LLM (Qwen3-8B) to select the attack target and generate explanations. The framework supports both deletion and addition attacks, with experiments conducted on WN18RR and FB15k-237 datasets.

## Key Results
- LLMAtKGE outperforms the strongest black-box baselines on both WN18RR and FB15k-237 datasets.
- The framework achieves competitive performance against white-box attack methods while generating human-readable explanations.
- High-order adjacency tuning improves retrieval accuracy by aligning semantic embeddings with graph topology, particularly for structurally relevant candidates.

## Why This Works (Mechanism)

### Mechanism 1: Discrete Reasoning Space via MCQ Prompting
Mapping the open-ended graph perturbation problem to a multiple-choice question (MCQ) format constrains the LLM's reasoning path and reduces output instability. The framework constructs a prompt where the LLM must select a target triple from a fixed list rather than generating one from scratch, preventing hallucination and forcing comparative evaluation.

### Mechanism 2: Hesitation Reduction via Candidate Filtering
Pruning the candidate set using semantics and centrality metrics prevents LLM "reasoning hesitation" and context window overflow. Large candidate sets cause the LLM to oscillate between options or exceed token limits, so the authors apply heuristics to compress the input context.

### Mechanism 3: Structural Injection via High-Order Adjacency (HoA) Tuning
Integrating multi-hop structural topology into the LLM embedding space improves retrieval of attack-relevant triples beyond simple semantic similarity. Standard LLM embeddings prioritize textual similarity, but HoA tuning aligns vector representations with graph topology to identify structurally relevant candidates.

## Foundational Learning

- **Concept**: Knowledge Graph Embeddings (KGE) & Link Prediction
  - **Why needed here**: You cannot attack what you do not understand. You must grasp how models like TransE or DistMult score triples $(s,r,o)$ to understand why removing an "inverse triple" reduces the confidence of a target prediction.
  - **Quick check question**: If a KGE model learns $s+r \approx o$ (TransE), how does removing an inverse triple $(o, r_{inv}, s)$ affect the embedding of $s$?

- **Concept**: Graph Centrality Metrics (PageRank, Betweenness, Closeness)
  - **Why needed here**: These metrics are the heuristics used to select "noisy" entities for addition attacks. Understanding what makes a node "central" helps predict which entities will maximize disruption when injected.
  - **Quick check question**: For an "addition attack" intended to confuse a model with irrelevant connections, would you prefer a node with high Closeness Centrality (close to all nodes) or high Betweenness Centrality (bridge node)?

- **Concept**: Parameter-Efficient Fine-Tuning (PEFT/LoRA)
  - **Why needed here**: The framework updates the LLM to understand graph structure (HoA) without retraining the entire model. Understanding LoRA is critical for implementing the "Adapter" in the filtering stage.
  - **Quick check question**: Why is LoRA preferred over full fine-tuning when integrating structural graph data into a pre-trained LLM?

## Architecture Onboarding

- **Component map**: Target Triple -> Retriever (1-hop or 3-hop neighbors) -> Filter (LLMEmbed + HoA Adapter + Centrality) -> Top-K Candidates -> Instruction LLM (Qwen3-8B) -> Selected Triple ID + Explanation

- **Critical path**: The HoA Adapter. If the adapter fails to align semantic embeddings with structural importance, the correct adversarial candidate will be filtered out before reaching the Instruction LLM.

- **Design tradeoffs**: 
  - Recall vs. Hesitation: Increasing K increases the chance of including the right attack target but causes the Instruction LLM to "hesitate" or exceed the context window.
  - Semantic vs. Structural: Pure semantic filtering fails on structural attacks; HoA tuning fixes this but adds training overhead.

- **Failure signatures**:
  - Reasoning Loop: If the LLM output contains repeated phrases like "Wait..." or "But...", the candidate set is likely too large or ambiguous.
  - Zero-Effect Attack: If the MRR/Hits@1 of the target KGE doesn't drop, the filter likely pruned the influential triple.
  - Inverse Miss: If the system fails to select the inverse triple $(o, r, s)$ for a deletion attack, the HoA tuning may be insufficient.

- **First 3 experiments**:
  1. Baseline Validation: Run "Random" and "Direct" attacks on WN18RR to calibrate upper/lower bounds of performance.
  2. Filter Ablation: Compare Semantics-only vs. HoA-tuned filtering on FB15k-237 to check recall improvement.
  3. Prompt Stress Test: Fix candidates and vary K (5, 10, 30) to observe "hesitation" vs. attack success rate.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can LLM-based attackers be adapted to improve generalizability on dense, open-domain knowledge graphs?
**Basis in paper**: [explicit] The authors observe in Section 5.2 that results on WN18RR are significantly better than on FB15k-237, stating explicitly that "LLMs exhibit limited generalizability to open-domain knowledge graphs."
**Why unresolved**: The paper attributes the lower performance on FB15k-237 to its richer neighborhood structure and the limited generalizability of LLMs to open-domain data, but does not propose a specific mechanism to overcome this structural density barrier.
**What evidence would resolve it**: A modification to the filtering or tuning mechanism that results in statistically significant performance gains on FB15k-237, achieving parity with or superiority over the results seen on lexical datasets like WN18RR.

### Open Question 2
**Question**: What methodological advancements are required to close the effectiveness gap between addition and deletion attacks?
**Basis in paper**: [explicit] Section 5.2 states that while addition attack performance improved, "overall effectiveness still lags behind that of deletion attacks," consistent with baseline observations.
**Why unresolved**: The paper successfully implements addition attacks via centrality-based filters and entity replacement, but the fundamental difficulty of generating effective poisoning triples remains an open challenge.
**What evidence would resolve it**: A framework variant where addition attacks achieve comparable Mean Reciprocal Rank (MRR) degradation to deletion attacks across standard benchmarks.

### Open Question 3
**Question**: How can structural tuning be optimized to improve attack transferability to additive KGE models like TransE?
**Basis in paper**: [explicit] Section 5.3 notes that improvement on the additive model TransE is "less pronounced," suggesting that Transformer-based LLMs are "inherently more difficult to transfer to additive models."
**Why unresolved**: While High-order Adjacency (HoA) tuning integrates structure, the inductive bias of the LLM (multiplicative attention) may misalign with the distance-based mechanisms of additive KGE models.
**What evidence would resolve it**: An adapter or tuning strategy specifically designed to map attention weights to translational distances, resulting in a significant increase in attack success rates specifically for TransE.

## Limitations
- The framework's effectiveness on knowledge graphs with different relation types, densities, or domain-specific semantics remains untested.
- The black-box assumption is partially relaxed as the method still requires access to KGE model embeddings and scoring functions for evaluation.
- The High-order Adjacency adapter relies on pre-trained semantic embeddings that may not generalize well across different knowledge graph structures.

## Confidence
**High Confidence**:
- MCQ prompting successfully constrains LLM reasoning to finite decision space
- Semantics and centrality filters effectively compress candidate sets while maintaining recall
- HoA adapter improves retrieval accuracy by aligning semantic embeddings with structural topology

**Medium Confidence**:
- Framework generates human-readable explanations that are faithful and useful
- LLMAtKGE achieves competitive performance against white-box methods

**Low Confidence**:
- "Reasoning hesitation" phenomenon is universally observable across different LLMs and candidate set sizes

## Next Checks
1. **Cross-Dataset Robustness Test**: Implement LLMAtKGE on a third KG (e.g., YAGO3-10 or biomedical KG) and evaluate whether HoA adapter parameters learned on WN18RR/FB15k-237 transfer effectively.
2. **Prompt Ablation Study**: Systematically remove components from the MCQ prompt to quantify marginal contribution of each element to attack success and explanation quality.
3. **White-Box vs. Black-Box Fidelity Analysis**: Implement gradient-based attack on same target triples and compare selected perturbation targets to quantify "information gap" that LLM reasoning compensates for.