---
ver: rpa2
title: Measuring Time Series Forecast Stability for Demand Planning
arxiv_id: '2508.10063'
source_url: https://arxiv.org/abs/2508.10063
tags:
- forecast
- data
- forecasting
- stability
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study measures forecast stability across state-of-the-art\
  \ time series models by quantifying model-induced stochasticity\u2014the variance\
  \ in forecasts from repeated runs with fixed inputs but different random seeds.\
  \ It uses coefficient of variation (CV) and RMSE as metrics, focusing on M5 and\
  \ Favorita datasets."
---

# Measuring Time Series Forecast Stability for Demand Planning
## Quick Facts
- arXiv ID: 2508.10063
- Source URL: https://arxiv.org/abs/2508.10063
- Authors: Steven Klee; Yuntian Xia
- Reference count: 14
- Key outcome: Ensemble models like AutoGluon achieve significantly higher forecast stability (CV ≤5%) compared to individual deep learning models (CV up to ~20%) while maintaining competitive accuracy

## Executive Summary
This study introduces a framework for quantifying forecast stability by measuring model-induced stochasticity—the variance in forecasts from repeated runs with identical inputs but different random seeds. Using coefficient of variation (CV) and RMSE as metrics, the research evaluates state-of-the-art time series models on M5 and Favorita datasets. The key finding demonstrates that ensemble approaches substantially reduce forecast variance without sacrificing accuracy, making them preferable for production systems where consistency is critical. The work establishes a foundation for understanding the stability-accuracy trade-off in demand planning applications.

## Method Summary
The authors measure forecast stability by running each model multiple times with fixed inputs but varying random seeds to isolate stochastic variability. They calculate the coefficient of variation (CV) as the primary stability metric, defined as the standard deviation divided by the mean forecast. The study compares individual deep learning models (DeepAR, N-BEATS, Transformer) against ensemble methods (AutoGluon, Weighted Average) across two benchmark datasets. Stability is evaluated alongside accuracy metrics (RMSE) to identify models that achieve both low variance and competitive prediction performance. The experimental design focuses on short to medium forecast horizons typical in retail demand planning.

## Key Results
- Ensemble models like AutoGluon achieve CV ≤5%, significantly lower than individual deep learning models reaching CV up to ~20%
- No significant accuracy trade-off: ensemble models maintain competitive RMSE while providing superior stability
- Model-induced stochasticity represents a meaningful source of forecast uncertainty that varies substantially across model architectures

## Why This Works (Mechanism)
Forecast stability depends on the inherent variance in model predictions when run under identical conditions with different random seeds. Deep learning models, particularly those using stochastic training procedures, exhibit higher sensitivity to initialization and training dynamics, leading to greater prediction variance. Ensemble methods reduce this instability by aggregating multiple model predictions, effectively averaging out individual model stochasticity. The coefficient of variation captures this relative variability, making it an appropriate metric for comparing stability across different forecast scales and magnitudes.

## Foundational Learning
**Coefficient of Variation (CV)**: A normalized measure of dispersion calculated as standard deviation divided by mean. Needed to compare variability across different scales; quick check: CV < 5% indicates low relative variability.
**Model-Induced Stochasticity**: Variance in model outputs due to random factors like initialization and training order. Needed to isolate inherent model uncertainty; quick check: compare CV across repeated runs with fixed inputs.
**Ensemble Methods**: Techniques that combine multiple model predictions to reduce variance. Needed for stability improvement; quick check: verify CV reduction when averaging predictions.
**RMSE vs. CV Trade-off**: Balance between absolute accuracy and relative stability. Needed for practical deployment decisions; quick check: plot accuracy vs. stability across model types.

## Architecture Onboarding
**Component Map**: Data Input -> Model Training -> Multiple Predictions -> CV Calculation -> RMSE Evaluation -> Model Comparison
**Critical Path**: Model training and prediction generation must be repeated multiple times with different seeds to capture stochastic variability accurately.
**Design Tradeoffs**: Individual models offer computational efficiency but higher variance; ensembles provide stability at increased computational cost. The choice depends on application requirements for consistency vs. resource constraints.
**Failure Signatures**: High CV with low RMSE indicates unstable but accurate predictions; high CV with high RMSE indicates both poor accuracy and stability.
**First Experiments**: 1) Run single model with 10 different seeds to establish baseline CV; 2) Compare CV across different forecast horizons (1-28 days); 3) Test stability sensitivity to data normalization methods.

## Open Questions the Paper Calls Out
The paper calls for further research on cycle-to-cycle stability and the impact of input perturbations. Additionally, it suggests investigating how stability metrics scale with longer forecast horizons and exploring the relationship between model architecture choices and stochastic sensitivity.

## Limitations
- Analysis focuses only on model-induced stochasticity, ignoring other stability factors like input perturbations and temporal drift
- CV thresholds for acceptable stability (≤5% for ensembles) lack theoretical grounding and may not reflect industry-specific requirements
- Results may not generalize to all demand planning contexts, particularly those with different seasonal patterns or structural breaks
- Evaluation limited to relatively short forecast horizons (1-28 days) without addressing longer-term stability

## Confidence
- **High Confidence**: Ensemble methods like AutoGluon demonstrate consistently lower forecast variance than individual deep learning models, with robust statistical significance
- **Medium Confidence**: The observed stability-accuracy trade-off is dataset-dependent and may vary across different demand planning contexts
- **Low Confidence**: Practical implications assume uniform planner preferences for stability over accuracy, which may vary by industry and use case

## Next Checks
1. Conduct sensitivity analysis by systematically perturbing input features (e.g., ±5% in historical demand values) to measure how forecast stability changes under realistic data uncertainty
2. Test stability metrics across longer forecast horizons (3-12 months) to determine if ensemble advantages persist for strategic planning applications
3. Implement A/B testing in a production environment where planners can choose between high-stability and high-accuracy forecasts to empirically validate assumed trade-off preferences