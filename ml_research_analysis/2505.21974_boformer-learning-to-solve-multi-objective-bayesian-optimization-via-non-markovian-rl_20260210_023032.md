---
ver: rpa2
title: 'BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian
  RL'
arxiv_id: '2505.21974'
source_url: https://arxiv.org/abs/2505.21974
tags:
- boformer
- learning
- functions
- mobo
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing acquisition functions
  for multi-objective Bayesian optimization that can plan over multiple steps rather
  than just optimizing one-step improvement. The key issue identified is the hypervolume
  identifiability problem, which arises from the non-Markovian nature of MOBO - the
  same observation can lead to different hypervolume improvements depending on history.
---

# BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL

## Quick Facts
- arXiv ID: 2505.21974
- Source URL: https://arxiv.org/abs/2505.21974
- Reference count: 0
- One-line primary result: BOFormer achieves superior hypervolume improvement in multi-objective Bayesian optimization by learning non-Markovian policies via transformer-based RL

## Executive Summary
This paper addresses the challenge of designing acquisition functions for multi-objective Bayesian optimization that can plan over multiple steps rather than optimizing one-step improvement. The key issue identified is the hypervolume identifiability problem, which arises from the non-Markovian nature of MOBO - the same observation can lead to different hypervolume improvements depending on history. The proposed solution, BOFormer, implements a generalized deep Q-network framework using transformer-based sequence modeling with Q-augmented observation representations that are domain-agnostic and memory-efficient. Extensive experiments demonstrate that BOFormer consistently outperforms both rule-based methods (like qNEHVI and JES) and learning-based alternatives (like FSAF and OptFormer) across various synthetic functions and real-world hyperparameter optimization tasks.

## Method Summary
BOFormer treats multi-objective Bayesian optimization as a non-Markovian reinforcement learning problem, where the agent learns a generalized Q-function over observation-action histories using transformer-based sequence modeling. The method introduces Q-augmented observation representations that combine posterior statistics with Q-values, avoiding the scalability issues of direct sequence modeling approaches. During training, a prioritized trajectory replay buffer stores complete trajectories, with a fraction collected using a demo policy (like qNEHVI) for efficient exploration. The transformer-based policy network takes the history and Q-augmented observations as input and outputs Q-values for all candidate actions, with a separate target network providing stable Q-estimates for bootstrapping.

## Key Results
- BOFormer consistently outperforms rule-based methods (qNEHVI, JES) and learning-based alternatives (FSAF, OptFormer) across synthetic benchmark functions
- The method demonstrates strong cross-domain transferability, maintaining performance across different problem dimensions and numbers of objectives without retraining
- Q-augmented observation representations enable domain-agnostic transfer while avoiding scalability issues of direct sequence modeling
- Demo-policy-guided exploration with qNEHVI significantly accelerates learning compared to epsilon-greedy exploration alone

## Why This Works (Mechanism)

### Mechanism 1: Addressing Hypervolume Identifiability via Non-Markovian RL
- Claim: Multi-objective BO is inherently non-Markovian because the same posterior distribution and best-observed values can yield different hypervolume improvements depending on the history of previous samples.
- Mechanism: BOFormer treats MOBO as a non-Markovian RL problem by learning a generalized Q-function over observation-action histories using transformers for sequence modeling. This allows the acquisition function to make decisions conditioned on the full trajectory rather than just the current posterior.
- Core assumption: Hypervolume improvement is history-dependent; approximating the optimal non-Markovian policy via learned Q-values yields better long-term planning than myopic one-step acquisition functions.
- Evidence anchors:
  - [abstract] "...the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the hypervolume identifiability issue, which results from the non-Markovian nature of MOBO problems."
  - [Section 1, Figure 1] The motivating example shows two scenarios where AF inputs at x₃ are identical but hypervolume improvements differ.
  - [corpus] Weak/no direct corpus evidence; related MOBO papers focus on acquisition function design rather than non-Markovian formulation.
- Break condition: If hypervolume improvement were approximately Markovian (e.g., under very smooth functions with negligible history dependence), the sequence modeling overhead may not justify gains.

### Mechanism 2: Q-Augmented Observation Representation
- Claim: Domain-agnostic observation representations that augment posterior statistics with Q-values enable cross-domain transfer without storing domain-sized embeddings.
- Mechanism: Each observation is represented as oₜ(x) = [μₜ⁽ⁱ⁾(x), σₜ⁽ⁱ⁾(x), yₜ⁽ⁱ⁾*, t] across objectives i, without including all domain points. Q-values from the target network are recursively incorporated into the history representation hₜ, providing prospective improvement signals without scaling with domain size.
- Core assumption: Posterior statistics plus Q-value history capture sufficient information for decision-making; the exact domain geometry can be abstracted away during training.
- Evidence anchors:
  - [Section 4.2] "Under this design, the representation is completely domain-agnostic and memory-efficient in the sense that it does not increase with the domain size."
  - [Section 4.1] Direct implementation storing all domain points is noted as impractical due to scalability issues.
  - [corpus] No direct corpus comparison on Q-augmented representations for MOBO.
- Break condition: For problems where domain geometry critically informs sampling (e.g., highly constrained or irregular domains), abstracting away domain structure may degrade performance.

### Mechanism 3: Demo-Policy-Guided Exploration with Off-Policy Learning
- Claim: Combining epsilon-greedy exploration with trajectories from a competent but suboptimal demo policy (e.g., qNEHVI) accelerates learning in sample-scarce BO regimes.
- Mechanism: During training, a fraction r_demo of trajectories is collected using the demo policy. A prioritized trajectory replay buffer stores complete trajectories; TD-errors aggregated over trajectories determine sampling priority. This enables off-policy reuse of high-quality exploration data.
- Core assumption: The demo policy explores regions near the Pareto front sufficiently well; off-policy learning from mixed behavior policies remains stable under the proposed loss function.
- Evidence anchors:
  - [Section 4.2] "To achieve efficient exploration, we propose to collect part of the training trajectories through a helper demo policy."
  - [Appendix D.2] Ablation shows qNEHVI demo policy improves hypervolume over no demo or NSGA-II demo.
  - [corpus] No direct corpus evidence on demo-policy-guided MOBO.
- Break condition: If the demo policy is systematically biased away from optimal regions, learned policy may inherit suboptimal priors; insufficient mixing with self-collected data could cause overfitting to demo behavior.

## Foundational Learning

- Concept: Hypervolume and Pareto Fronts
  - Why needed here: MOBO success is measured by hypervolume improvement toward an unknown Pareto front; understanding dominance and Pareto optimality is essential.
  - Quick check question: Given points A=(0.5, 0.8) and B=(0.7, 0.6) maximizing two objectives, does A dominate B?

- Concept: Gaussian Process Posterior Distributions
  - Why needed here: The GP provides μₜ(x) and σₜ(x) as inputs to the acquisition function; you must understand how posteriors update with observations.
  - Quick check question: After observing (x₁, y₁), how does the posterior variance at x₁ compare to a distant point x₂?

- Concept: Deep Q-Learning with Target Networks
  - Why needed here: BOFormer extends DQN to non-Markovian settings; you need to understand Q-learning, target networks, and TD-error minimization.
  - Quick check question: In standard DQN, why is a separate target network Q̄_θ used rather than the same network for bootstrapping?

## Architecture Onboarding

- Component map:
  - GP surrogate model -> provides μₜ(x), σₜ(x) for all x ∈ X at each step
  - Policy network Q_θ -> transformer-based; takes history hₜ and observation oₜ(x), outputs Q-values per action
  - Target network Q̄_θ -> frozen copy synchronized periodically; provides stable Q-estimates for bootstrapping
  - Prioritized trajectory replay buffer -> stores trajectories τ; samples based on aggregated TD-error
  - Demo policy (e.g., qNEHVI) -> external AF used during training for guided exploration

- Critical path:
  1. Training: Generate synthetic GP functions → collect trajectories (mixed demo/self policy) → store in buffer → sample batches → compute TD-loss over histories → update Q_θ
  2. Deployment: Initialize GP with observations → compute posteriors → build history hₜ → evaluate Q_θ(hₜ, oₜ(x)) for all x → select argmax → observe yₜ → update GP and repeat

- Design tradeoffs:
  - Sequence length w: Longer w captures more history but increases memory/computation. Paper uses w=31; w=1 (Markovian) underperforms
  - Demo rate r_demo: Higher r_demo improves sample efficiency but risks over-reliance on demo policy. Paper uses r_demo=0.01
  - Transformer depth/heads: Deeper models may overfit to training GP distributions. Paper uses 8 layers, 4 heads

- Failure signatures:
  - Loss divergence early in training: Often indicates missing temporal information or positional encoding; verify these are included
  - Poor cross-domain transfer: Check if embedding layer dimension matches new objective count; use transfer learning strategy from Appendix C.2
  - Inference too slow for real-time use: BOFormer inference ~14ms per step; if slower, check transformer implementation and batch processing

- First 3 experiments:
  1. Reproduce synthetic benchmark (e.g., Ackley-Rastrigin, K=2) with w=31 vs. w=1 to validate non-Markovian benefit; plot hypervolume over 100 steps
  2. Ablate demo policy: Train with no demo, qNEHVI demo, and NSGA-II demo; compare convergence speed and final hypervolume
  3. Cross-domain transfer test: Train on 2-objective GP functions, freeze transformer, retrain only embedding layer for 3-objective task; verify performance within 400 episodes as claimed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BOFormer scale effectively to very high-dimensional input domains without relying on Sobol grid approximations?
- Basis in paper: [explicit] Remark 4.2 states that using Sobol grids to approximate the maximum Q-value "could be a limitation in high-dimensional tasks," though Appendix D.3 shows preliminary results up to d=100.
- Why unresolved: The current approach relies on discrete candidate sampling via Sobol sequences, which becomes increasingly sparse in high dimensions, potentially missing optimal regions.
- What evidence would resolve it: Experiments demonstrating competitive hypervolume performance on problems with d>200, or development of alternative maximization strategies that do not rely on grid-based approximations.

### Open Question 2
- Question: What is the optimal strategy for selecting the demo policy in demo-policy-guided exploration?
- Basis in paper: [inferred] Appendix D.2 shows that using qNEHVI as the demo policy significantly outperforms NSGA2 and no demo policy, but no principled method for demo policy selection is provided.
- Why unresolved: The performance gains depend on the quality of the demo policy's trajectories, but the relationship between demo policy characteristics and BOFormer's learning efficiency remains uncharacterized.
- What evidence would resolve it: A systematic study correlating demo policy performance metrics with BOFormer's final hypervolume, or an adaptive demo policy selection mechanism.

### Open Question 3
- Question: How can the optimal window size (sequence length) be determined adaptively for different MOBO problem instances?
- Basis in paper: [inferred] Figure 4 demonstrates that window size w affects hypervolume performance, with w>1 outperforming w=1, but the fixed window size (w=31) is chosen empirically without theoretical justification.
- Why unresolved: The appropriate memory horizon likely depends on problem characteristics like the complexity of the Pareto front and the number of objectives, but no adaptive mechanism is proposed.
- What evidence would resolve it: Analysis showing correlation between optimal window size and problem properties (e.g., Pareto front complexity, number of objectives), or an adaptive window selection algorithm.

### Open Question 4
- Question: What are the theoretical convergence guarantees and regret bounds for BOFormer in the non-Markovian MOBO setting?
- Basis in paper: [inferred] The paper provides empirical validation but no theoretical analysis of the Generalized DQN framework's convergence properties or sample complexity guarantees.
- Why unresolved: The non-Markovian nature of MOBO combined with function approximation via Transformers makes standard RL theoretical tools difficult to apply directly.
- What evidence would resolve it: A formal analysis establishing regret bounds or PAC-style guarantees for BOFormer, possibly under assumptions about the GP surrogate model and Transformer approximation capacity.

## Limitations
- The current approach relies on Sobol grid approximations for candidate maximization, which may become problematic in very high-dimensional spaces
- No theoretical convergence guarantees or regret bounds are provided for the non-Markovian RL formulation
- The optimal demo policy selection strategy remains unclear, with empirical results showing qNEHVI outperforms alternatives but no principled guidance

## Confidence
- High confidence: The BOFormer architecture works as described and outperforms baselines on tested benchmarks
- Medium confidence: The hypervolume identifiability problem is correctly identified as the core challenge for MOBO acquisition functions
- Medium confidence: Q-augmented observations provide cross-domain transfer benefits without scaling issues
- Low confidence: Demo-policy-guided exploration with qNEHVI is the optimal choice for all MOBO domains

## Next Checks
1. Implement a Markovian variant of BOFormer (w=1) with identical architecture to isolate the non-Markovian contribution from other architectural choices
2. Test BOFormer on constrained multi-objective problems where domain geometry is critical to verify the domain-agnostic representation assumption
3. Compare demo policy choices (qNEHVI vs. NSGA-II vs. random) across diverse problem types to identify conditions where guided exploration helps or hurts