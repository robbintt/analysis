---
ver: rpa2
title: 'Fake & Square: Training Self-Supervised Vision Transformers with Synthetic
  Data and Synthetic Hard Negatives'
arxiv_id: '2509.02029'
source_url: https://arxiv.org/abs/2509.02029
tags:
- synthetic
- data
- learning
- negatives
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates synthetic data and synthetic hard negatives
  to enhance self-supervised vision transformers. The authors generate a synthetic
  clone of ImageNet-100 using diffusion models and create synthetic hard negatives
  in representation space to improve contrastive learning.
---

# Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives

## Quick Facts
- arXiv ID: 2509.02029
- Source URL: https://arxiv.org/abs/2509.02029
- Reference count: 34
- Primary result: Synthetic data + synthetic hard negatives achieve 82.12% top-1 accuracy on ImageNet-100 with DeiT-S

## Executive Summary
This paper investigates synthetic data and synthetic hard negatives to enhance self-supervised vision transformers. The authors generate a synthetic clone of ImageNet-100 using diffusion models and create synthetic hard negatives in representation space to improve contrastive learning. Their Syn2Co framework combines both approaches, training DeiT-S and Swin-T architectures. Results show synthetic data alone can achieve reasonable performance (79.02% top-1 accuracy for DeiT-S), though real data still provides advantages. Synthetic hard negatives further improve representations, with DeiT-S reaching 82.12% top-1 accuracy when using both synthetic components. Swin-T benefits primarily from synthetic negatives.

## Method Summary
The authors propose Syn2Co, a self-supervised learning framework that leverages synthetic data and synthetic hard negatives. They generate a synthetic clone of ImageNet-100 using diffusion models, creating approximately 130,000 synthetic images. For synthetic hard negatives, they identify the top-N hardest negatives from a memory queue and synthesize new negatives using interpolation, extrapolation, mixing, noise jittering, perturbation, or adversarial methods. The framework trains DeiT-S and Swin-T architectures using MoBY-style contrastive learning with InfoNCE loss, combining real and synthetic images as positives and augmenting negatives with synthetic hard examples.

## Key Results
- Synthetic data alone achieves 79.02% top-1 accuracy for DeiT-S on ImageNet-100
- Combining synthetic data and synthetic hard negatives achieves 82.12% top-1 accuracy for DeiT-S
- Swin-T architecture benefits primarily from synthetic hard negatives rather than synthetic data
- Performance plateaus when synthetic data fully replaces real data, indicating distribution shift limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion-generated synthetic images can serve as effective training substitutes or complements for contrastive learning.
- Mechanism: A diffusion model generates synthetic image clones that preserve class distribution and semantic content, which are then sampled alongside or instead of real images during contrastive pre-training. The synthetic data expands sample diversity without requiring additional real data collection.
- Core assumption: The diffusion model captures sufficient semantic structure from the original dataset to produce informative training examples (distribution alignment is adequate but not perfect).
- Evidence anchors:
  - [abstract] "leveraging synthetic data to augment sample diversity"
  - [section 3.3] "we leverage the diffusion model from [22] to generate clones of ImageNet-100. This approach results in a synthetic dataset of approximately 130,000 images"
  - [corpus] Related work SynCLR and Sariyildiz et al. show comparable findings for diffusion-generated training data
- Break condition: If synthetic images introduce systematic distribution shifts that dominate the semantic content, representation quality degrades.

### Mechanism 2
- Claim: Synthetic hard negatives generated in representation space improve discriminative feature learning.
- Mechanism: Given a query representation q, the top N hardest negatives are selected from a memory queue based on cosine similarity. A synthesis function F creates new synthetic negatives that interpolate between query and hard negatives, providing more challenging contrasts.
- Core assumption: Harder negatives push the model to learn finer semantic boundaries rather than superficial features.
- Evidence anchors:
  - [abstract] "generating synthetic hard negatives directly in the representation space to improve contrastive learning"
  - [section 3.4] "These hardest negatives serve as the basis for constructing synthetic contrasts that the model must learn to differentiate"
  - [corpus] SynCo [12] and related hard negative mixing work (Kalantidis et al.) provide precedent for representation-space synthesis
- Break condition: If hardness is too extreme or synthesis creates false negatives (semantically similar samples treated as negatives), gradients become conflicting and representation quality drops.

### Mechanism 3
- Claim: Architecture design determines optimal synthetic augmentation strategy (DeiT benefits from both synthetic components; Swin benefits primarily from synthetic negatives).
- Mechanism: DeiT's flat patch-based architecture appears more sensitive to data diversity and negative hardness, while Swin's hierarchical windowed design may already encode sufficient inductive bias, making it less reliant on synthetic data but still responsive to harder negatives.
- Core assumption: Architectural inductive biases interact differently with synthetic augmentation types.
- Evidence anchors:
  - [section 4.2] "Swin benefits primarily from synthetic negatives, achieving optimal performance with synthetic negatives alone. In contrast, DeiT effectively leverages both synthetic components"
  - [section 5] "architectural differences significantly impact how models leverage synthetic elements"
  - [corpus] Weak direct evidence—neighboring papers don't explicitly compare architecture-specific synthetic responses
- Break condition: This finding may not generalize beyond ImageNet-100 scale.

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE objective)**
  - Why needed here: The entire framework builds on MoBY-style contrastive learning where positives (augmented views of same image) are pulled together and negatives are pushed apart. Understanding Equation 1 is essential.
  - Quick check question: Can you explain why the temperature parameter τ affects negative hardness?

- Concept: **Vision Transformer Architectures (DeiT vs. Swin)**
  - Why needed here: The paper shows architecture-specific responses to synthetic augmentation. DeiT uses flat patch processing with distillation; Swin uses hierarchical windows with shifted attention.
  - Quick check question: What is the key structural difference between DeiT's global attention and Swin's windowed attention?

- Concept: **Hard Negatives in Contrastive Learning**
  - Why needed here: The core innovation is generating synthetic hard negatives. Understanding why hard negatives matter—providing informative gradients near decision boundaries—is prerequisite.
  - Quick check question: Why would "easy" negatives (very dissimilar to query) provide less useful learning signal than hard negatives?

## Architecture Onboarding

- Component map:
  - Real images X + Synthetic images Xs → augmentation T → views xq, xk
  - Online encoder fq + Target encoder fk → representations
  - Memory queue Q → Top-N hard selection → Synthesis function F → Synthetic negatives Qs
  - InfoNCE loss combining positives and negatives → gradient update

- Critical path:
  1. Generate synthetic ImageNet-100 clone (130K images) using Relay Diffusion
  2. Initialize MoBY framework with DeiT-S or Swin-T backbone
  3. For each batch: sample from X ∪ Xs, create augmented views, encode with fq and fk
  4. Select top N hard negatives from queue, synthesize L additional negatives
  5. Compute InfoNCE loss with combined negative set, update fq via backprop, fk via momentum

- Design tradeoffs:
  - Synthetic data ratio: More synthetic = more diversity but potential distribution shift (Figure 3 shows diminishing returns)
  - Hard negative count N: Larger N = more diverse synthesis candidates but higher computation
  - Synthetic negative percentage: DeiT shows sensitivity; requires tuning (Figure 4a)
  - Synthesis strategy choice: 6 options available; paper doesn't isolate best strategy per architecture

- Failure signatures:
  - Performance plateau or degradation when synthetic data fully replaces real data (distribution shift)
  - DeiT performance drops with incorrect hardness/proportion settings (sensitivity to hyperparameters)
  - No improvement from synthetic negatives if N is too small or synthesis function doesn't create meaningful variations
  - False negatives from synthesis may cause conflicting gradients (not directly measured)

- First 3 experiments:
  1. **Synthetic data ablation**: Train with 0%, 25%, 50%, 75%, 100% real data (rest synthetic) on ImageNet-100, measure linear probing accuracy to replicate Figure 3 curve.
  2. **Hard negative sweep**: Fix synthetic data at 50%, vary N ∈ {256, 512, 1024} and synthetic percentage 0-40%, plot accuracy surface separately for DeiT-S and Swin-T to confirm architecture-specific optima from Figure 4.
  3. **Component isolation**: Run three conditions—(a) synthetic data only, (b) synthetic negatives only, (c) both—to replicate Table 1 Syn1Co vs Syn2Co comparison and verify which component drives gains per architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms explain the divergent architecture-specific responses of DeiT and Swin transformers to synthetic hard negatives?
- Basis in paper: [explicit] The authors state "DeiT exhibits sensitivity to both hardness levels and proportions, while Swin maintains consistent performance gains across configurations" and note these "architecture-specific responses highlight the importance of considering transformer design when applying synthetic augmentation strategies."
- Why unresolved: The paper observes but does not investigate the underlying causes—whether differences stem from hierarchical vs. flat architectures, attention patterns, or training dynamics.
- What evidence would resolve it: Ablation studies isolating architectural components (attention mechanisms, patch embedding strategies, normalization layers) combined with analysis of representation space geometry during training.

### Open Question 2
- Question: Can diffusion-generated synthetic data overcome distribution shift limitations to serve as a complete substitute for real data in self-supervised pre-training?
- Basis in paper: [explicit] The discussion notes "diffusion models produce distribution shifts, limiting their effectiveness as complete substitutes" and that "synthetic images serve as valuable complements rather than complete substitutes for real data."
- Why unresolved: The paper demonstrates a performance gap between synthetic-only and real data training but does not characterize or quantify the specific distributional differences causing this gap.
- What evidence would resolve it: Quantitative analysis of distribution shift metrics (e.g., FID, classifier-based divergence) between real and synthetic data, combined with targeted improvements to generation pipelines addressing identified gaps.

### Open Question 3
- Question: Do synthetic data and synthetic hard negatives provide comparable benefits when scaling to larger datasets such as ImageNet-1K or multi-million image corpora?
- Basis in paper: [inferred] All experiments are conducted on ImageNet-100, a subset with ~130,000 images. The paper does not address whether synthetic augmentation strategies maintain their effectiveness when real data abundance increases substantially.
- Why unresolved: The relative value of synthetic data may diminish with larger real datasets, while synthetic negatives might show different scaling properties.
- What evidence would resolve it: Experiments replicating the Syn2Co framework on ImageNet-1K, ImageNet-21K, or web-scale datasets, comparing relative gains against the ImageNet-100 baseline.

## Limitations
- Distribution shift from synthetic data causes diminishing returns when fully replacing real data
- Architecture-specific findings may not generalize beyond ImageNet-100 scale
- No analysis of which synthetic hard negative synthesis strategy performs best per architecture

## Confidence
- **High confidence**: Synthetic data alone can achieve reasonable performance (79.02% top-1 for DeiT-S on ImageNet-100), and synthetic hard negatives improve representations (DeiT-S reaches 82.12% with both components)
- **Medium confidence**: Architecture-specific responses (DeiT vs. Swin) to synthetic augmentation are demonstrated but may not generalize beyond ImageNet-100 scale
- **Medium confidence**: Diffusion-generated synthetic images capture substantial semantic information, though distribution alignment is adequate but not perfect

## Next Checks
1. **Distribution shift quantification**: Measure representation distance (e.g., FID or LPIPS) between synthetic and real ImageNet-100 images to quantify distribution alignment quality
2. **Architecture generalization test**: Apply the same synthetic augmentation strategies to other vision transformer variants (e.g., PVT, ConViT) to test whether architecture-specific responses hold
3. **False negative impact measurement**: Evaluate the semantic similarity between synthetic negatives and their corresponding positives to quantify false negative introduction rate and its effect on downstream performance