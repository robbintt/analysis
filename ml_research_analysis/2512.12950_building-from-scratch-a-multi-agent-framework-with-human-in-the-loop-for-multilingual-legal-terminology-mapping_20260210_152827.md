---
ver: rpa2
title: 'Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for
  Multilingual Legal Terminology Mapping'
arxiv_id: '2512.12950'
source_url: https://arxiv.org/abs/2512.12950
tags:
- legal
- terminology
- term
- terms
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a human-AI collaborative framework for multilingual
  legal terminology mapping, addressing the challenge of accurate cross-lingual mapping
  for language pairs like Chinese and Japanese with many homographs. The framework
  employs a multi-agent system that integrates advanced LLMs and legal experts throughout
  the process, from document preprocessing and alignment to terminology extraction,
  mapping, and quality assurance.
---

# Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping

## Quick Facts
- arXiv ID: 2512.12950
- Source URL: https://arxiv.org/abs/2512.12950
- Reference count: 28
- Primary result: Human-AI collaborative multi-agent framework improves multilingual legal terminology mapping precision and scalability

## Executive Summary
This paper presents a multi-agent framework that combines advanced large language models (LLMs) with human legal experts to address the challenge of multilingual legal terminology mapping, particularly for Chinese-Japanese language pairs with many homographs. The framework systematically decomposes the terminology extraction pipeline into specialized AI agents for OCR, text segmentation, alignment, and extraction, while embedding human experts at critical quality control points. The approach leverages a dual-stream extraction strategy with English as a pivot language to handle Chinese-Japanese homographs, achieving improved coverage and consistency compared to traditional manual methods.

## Method Summary
The framework employs a four-stage pipeline: (1) preprocessing using OCR and text cleaning agents, (2) article alignment through embedding-based matching with reranking, (3) dual-stream bilingual term extraction with an auto-complete agent to fill missing language entries, and (4) standardization of extracted terms. Human experts validate outputs at alignment, mapping, and standardization stages. The system processes 35 Chinese statutes with official English and Japanese translations, extracting 353 unique trilingual legal terminology entries from 5,172 aligned article triplets.

## Key Results
- Dual-stream extraction with auto-complete achieves 44% more unique terms (353 vs 245) than single-stream approaches
- Human-in-the-loop reduces LLM hallucination rates from 1.1-7.0% to negligible levels through expert validation
- The framework improves precision and consistency while offering greater scalability than traditional manual terminology mapping

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Task Decomposition with Specialized Agents
- Claim: Decomposing the terminology mapping pipeline into specialized agents improves extraction coverage and reduces error propagation compared to monolithic automated approaches.
- Mechanism: The framework assigns distinct, repetitive tasks to specialized AI agents—PDF Parsing Agent uses GPT-4.1-mini for OCR, Style Quality Control Agent cleans text with larger models (GPT, Gemini, DeepSeek), Content Segmentation Agent combines rule-based Python scripts with LLM-based logical segmentation, and dedicated QA agents validate each stage before handoff.
- Core assumption: Task-specific prompting and model selection outperform single-model end-to-end pipelines for legal terminology extraction.
- Evidence anchors:
  - [abstract] "AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction"
  - [section 4.1] Figure 2 shows the four-agent preprocessing workflow with explicit feedback loops between Style/Segmentation QA agents and human Language Revisers
  - [corpus] TransLaw (arXiv:2507.00875) validates multi-agent approaches for legal translation, showing professional simulation benefits
- Break condition: If inter-agent communication overhead exceeds the accuracy gains from specialization, or if task boundaries become ambiguous (e.g., segmentation vs. extraction overlap causes redundant processing), the decomposition no longer provides value.

### Mechanism 2: Strategic Human-in-the-Loop at Quality-Critical Decision Points
- Claim: Human legal expert oversight at alignment validation, term mapping review, and standardization stages mitigates LLM hallucinations and jurisdiction-specific errors that automated systems cannot reliably detect.
- Mechanism: Human experts (Senior Translators, Legal Experts, Terminologists) are embedded at three critical junctions: (1) alignment validation where senior translators verify bilingual/trilingual correspondence, (2) term mapping where legal experts resolve ambiguities for Chinese-Japanese homographs, and (3) standardization where experts select canonical forms from variant translations.
- Core assumption: Human legal judgment remains superior for context-sensitive disambiguation, particularly for "false friends" like Chinese "裁判" (broad judgment/decision) vs. Japanese "裁判" (strictly judicial trial).
- Evidence anchors:
  - [abstract] "human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment"
  - [section 4.3-4.4] "Legal experts also play a critical role primarily in the term mapping and standardization stages... review and validate the candidate term mappings proposed by the Term Mapping Agent, resolve any ambiguities"
  - [section 5.6.4] Table 14 shows hallucination rates of 1.1-7.0% across models, demonstrating ongoing need for human correction
  - [corpus] Cross-Lingual Mental Health Ontologies paper (arXiv:2510.05387) similarly emphasizes human-in-the-loop validation for domain-specific terminology
- Break condition: If human review becomes a throughput bottleneck (e.g., inter-annotator agreement drops below usable thresholds as shown in Appendix A with ICC(2,1) values of 0.015-0.144), or if annotation costs exceed the value of error correction.

### Mechanism 3: Dual-Stream Extraction with Pivot Language Bridging
- Claim: Running parallel bilingual extraction streams (Chinese-English and Chinese-Japanese) combined with an auto-complete agent achieves higher trilingual term coverage than single-stream approaches.
- Mechanism: Two Bilingual Term Extraction Agents process aligned article pairs independently, each extracting source-target term pairs with context. An Auto-Complete Agent then identifies entries missing one target language and suggests completions using semantic context from available translations. English serves as a semantic pivot to disambiguate Chinese-Japanese homographs.
- Core assumption: English legal translations are sufficiently reliable and available to serve as a disambiguation bridge for Chinese-Japanese mapping where direct resources are limited.
- Evidence anchors:
  - [abstract] "language pairs like Chinese and Japanese, which share a large number of homographs with different meanings"
  - [section 4.3] "we propose a dual-stream extraction strategy... To address the issue of missing English or Japanese terms in trilingual legal terminology entries extracted from bilingual corpora, we have developed an intelligent Auto-complete Agent"
  - [section 5.1] Table 6 shows dual extractor with auto-complete achieves 353 unique terms vs. 245 for single extractor on Standardization Law (44% improvement)
  - [corpus] Limited direct corpus evidence for pivot-language mechanisms specifically; related work (MiLorE-SSL, arXiv:2601.20300) addresses multilingual scaling but not pivot strategies
- Break condition: If the pivot language introduces systematic translation drift (compounding errors through Chinese→English→Japanese chain), or if auto-completion hallucinates terms not grounded in source context (Section 5.6.4 shows this occurs at 0.6-6.7% rates).

## Foundational Learning

- **Concept: Legal Terminology as Concept Systems (Not Word Lists)**
  - Why needed here: The paper defines termbases as "concept-oriented" (Definition 4), and Chinese-Japanese legal terms require understanding that identical characters (homographs) can represent different legal concepts across jurisdictions. Without this, you'll misinterpret the extraction task as simple translation rather than cross-jurisdictional concept mapping.
  - Quick check question: Given the Chinese term "裁判" appearing in a statute, would you expect a single Japanese equivalent, and why would this assumption fail?

- **Concept: Multi-Agent Coordination Patterns (Pipeline with Feedback Loops)**
  - Why needed here: The framework uses sequential agents with quality control agents that can trigger re-processing. Understanding that QA agents provide "suggestions to human experts" rather than autonomous correction is critical for implementing the workflow correctly.
  - Quick check question: If the Segmentation Quality Control Agent detects an overly long text block, what action should it take—autonomous re-segmentation or human escalation?

- **Concept: LLM Hallucination Modes in Domain Extraction**
  - Why needed here: The paper identifies four failure categories (variants, redundancy, context mismatch, hallucinations) with concrete examples. Recognizing that GPT-4.1 hallucinates at 1.8-7.0% rates (Table 14) and that hallucinations include "fabrication of legal terms that lack statutory basis" (Section 5.6.4) is essential for designing evaluation protocols.
  - Quick check question: If an extracted term "real-name reporter" appears in your output but not in the source statute, which failure mode is this, and what validation step should catch it?

## Architecture Onboarding

- **Component map:**
  Input PDFs → [PDF Parsing Agent + Zerox/GPT-4.1-mini]
            → [Style QA Agent] → Markdown
            → [Content Segmentation Agent + Python rules + DeepSeek-v3]
            → [Segmentation QA Agent + DeepSeek-r1]
            → JSON article segments
            → [Article Alignment Agent + embeddings + reranker]
            → [Alignment QA Agent] → Trilingual Corpus
            → [Bilingual Term Extraction Agent ×2] (CN-EN, CN-JA branches)
            → [Auto-Complete Agent]
            → [Standardization Agent]
            → [5 Evaluation Agents] → Quality Scores
            → Human Expert Review → MLTDB

- **Critical path:** Segmentation quality → Alignment accuracy → Extraction coverage. Errors in segmentation (e.g., splitting mid-sentence) propagate to alignment failures, which then cause term extraction to miss context or extract spurious phrases.

- **Design tradeoffs:**
  - **Model selection:** GPT-4.1/Gemini-2.5-flash achieve highest extraction counts but higher hallucination rates (Table 14); Qwen3-8B is cost-effective but 68.1% success rate vs. 100% for larger models (Table 7)
  - **Automation depth:** The framework allows full automation but recommends 2-3 human experts per stage; removing humans speeds throughput but hallucinations and context mismatches accumulate
  - **Dual vs. single stream:** Dual extraction + auto-complete yields 44% more unique terms (Table 6) but doubles LLM API costs

- **Failure signatures:**
  - **Hallucinations:** Terms not present in source text (e.g., "helping commercial publicity" in Table 13); detect via source-text span verification
  - **Over-extraction:** LLM includes procedural phrases as "terms" (e.g., "result of its investigation and handling" in Table 12); detect via term length heuristics and legal concept validation
  - **Alignment drift:** Chinese article mapped to wrong Japanese article due to embedding similarity of generic legal language; detect via article number/structure heuristics

- **First 3 experiments:**
  1. **Single-law extraction baseline:** Run the full pipeline on one statute (e.g., Trade Union Law, 66 articles) using a single model (DeepSeek-v3), measure extraction count, hallucination rate, and human review time. This establishes component integration before scaling.
  2. **Model comparison on held-out law:** Process Standardization Law (54 articles) with 3 models (GPT-4.1-mini, Gemini-2.5-flash, DeepSeek-v3), compare unique terms extracted and duplicate rates using the 5-dimensional evaluation framework. This validates model selection tradeoffs.
  3. **Human annotation calibration:** Have 2 legal experts independently review 50 extracted term entries, measure inter-annotator agreement on "correct/incorrect" judgments and compare to LLM evaluation scores. This calibrates the automated evaluation agents against human standards before relying on them for filtering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed multi-agent framework be effectively generalized to diverse legal systems and language pairs outside the specific Sinosphere (Chinese-Japanese) context?
- Basis in paper: [explicit] Conclusion: "Questions remain about... how to generalize the workflow to other legal systems and languages."
- Why unresolved: The current study focused on Chinese, Japanese, and English, utilizing specific homograph handling and pivot strategies suitable for these languages, which may not transfer directly to morphologically different languages or distinct legal traditions.
- What evidence would resolve it: Successful application and evaluation of the framework on a distinct legal corpus (e.g., Civil Law vs. Common Law) involving non-CJK languages without significant architectural redesign.

### Open Question 2
- Question: What is the optimal method for defining and weighting the multi-dimensional evaluation criteria (Coverage, Consistency, Completeness, Professionalism, Translation Quality) to balance objectivity and context sensitivity?
- Basis in paper: [explicit] Discussion: "We expect that further empirical testing and input from other researchers will be needed to refine these rubrics."
- Why unresolved: The authors assigned weights based on their initial assessment of the Chinese/Japanese/English domains, and the LLM-based evaluations showed significant variance and bias compared to human raters.
- What evidence would resolve it: Empirical validation demonstrating a specific weighting configuration that correlates strongly with independent human expert rankings across multiple legal domains.

### Open Question 3
- Question: What mechanisms are required to effectively limit error propagation from upstream automated modules (such as OCR and document alignment) to the final terminology extraction stage?
- Basis in paper: [explicit] Conclusion: "Questions remain about... how to limit error propagation from automated modules."
- Why unresolved: The pipeline is sequential; failures in the PDF parsing or alignment agents necessitate manual intervention or automatic retries, potentially undermining the scalability claimed by the automated workflow.
- What evidence would resolve it: Ablation studies quantifying the impact of noise injection in early pipeline stages on final term extraction accuracy, or the demonstration of a robust feedback loop that auto-corrects upstream errors.

## Limitations
- Human-in-the-loop design introduces throughput constraints that may limit scalability for large-scale deployment
- Pivot language approach assumes English translations are sufficiently reliable, which may not hold for jurisdictions with poor translation quality
- Hallucination rates of 1.1-7.0% indicate ongoing reliability concerns that could propagate through the pipeline

## Confidence

- Multi-agent task decomposition effectiveness: **High** (supported by quantitative extraction improvements and established multi-agent legal benchmarks)
- Human-in-the-loop quality assurance: **Medium** (empirical evidence shows reduction in hallucinations but no comparative analysis with fully automated baselines)
- Pivot language disambiguation strategy: **Low-Medium** (mechanism described but limited empirical validation; potential for translation drift not quantified)

## Next Checks

1. **Human throughput benchmarking:** Measure actual human review time per term and inter-annotator agreement across 3 legal experts to quantify the scalability ceiling of the human-in-the-loop approach.
2. **Pivot language failure analysis:** Systematically inject controlled translation errors into English pivot texts and measure propagation rates to Chinese-Japanese term mapping accuracy to quantify the approach's brittleness.
3. **Model comparison on held-out jurisdiction:** Apply the framework to Chinese-Korean legal texts (another homograph-rich pair) to test whether the dual-stream + pivot approach generalizes beyond the Chinese-English-Japanese combination.