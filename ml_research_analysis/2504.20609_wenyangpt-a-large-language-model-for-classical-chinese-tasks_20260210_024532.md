---
ver: rpa2
title: 'WenyanGPT: A Large Language Model for Classical Chinese Tasks'
arxiv_id: '2504.20609'
source_url: https://arxiv.org/abs/2504.20609
tags:
- chinese
- classical
- tasks
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WenyanGPT, a large language model specifically
  designed for Classical Chinese tasks. The model is built by continuing pre-training
  and instruction fine-tuning on the LLaMA3-8B-Chinese model using a large-scale,
  high-quality Classical Chinese corpus.
---

# WenyanGPT: A Large Language Model for Classical Chinese Tasks

## Quick Facts
- arXiv ID: 2504.20609
- Source URL: https://arxiv.org/abs/2504.20609
- Authors: Xinyu Yao; Mengdi Wang; Bo Chen; Xiaobing Zhao
- Reference count: 24
- Primary result: WenyanGPT significantly outperforms general-purpose models on Classical Chinese tasks including NER (F1 > 90%), punctuation (F1 > 75%), and translation (BLEU up to 0.47).

## Executive Summary
This paper introduces WenyanGPT, a specialized large language model designed for Classical Chinese language tasks. The model is built by continuing pre-training LLaMA3-8B-Chinese on a 16GB Classical Chinese corpus, followed by instruction fine-tuning on 1.85M task-specific examples across six domains. The authors develop a novel framework for generating domain-specific instruction data and create WenyanBENCH, a comprehensive evaluation benchmark. Experimental results demonstrate that WenyanGPT achieves state-of-the-art performance on multiple Classical Chinese tasks, significantly outperforming both general-purpose and specialized models.

## Method Summary
The WenyanGPT model is trained in two stages: first, continued pre-training on a 16GB Classical Chinese corpus using LLaMA3-8B-Chinese as the base model; second, supervised fine-tuning on 1.85 million instruction examples generated through a four-stage pipeline (manual design → LLM expansion → testing/filtering → integration). The model is evaluated on WenyanBENCH, a benchmark containing 25,953 test samples across six task types: punctuation, POS tagging, NER, translation, word explanation, and reverse dictionary. Training uses a cosine learning rate scheduler with 0.1 warmup, batch size 16 for pre-training and 8 for fine-tuning (with gradient accumulation).

## Key Results
- WenyanGPT achieves F1 scores exceeding 90% on named entity recognition tasks
- The model reaches F1 scores above 75% on punctuation tasks
- Translation tasks show BLEU scores up to 0.47, demonstrating strong performance in Classical to Modern Chinese translation
- The model outperforms general-purpose models like Qwen2.5-7B and LLaMA3-8B-Chinese across all six evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Continued Pre-training
The 16GB Classical Chinese corpus exposes the base model to domain-specific vocabulary, syntax patterns, and cultural context absent from general training. This potentially enhances token-level representations for Classical Chinese linguistic features.

### Mechanism 2: Structured Instruction Data Generation
The four-stage pipeline (manual instruction design → LLM-based expansion and reverse reasoning → testing and filtering → integration) reduces low-quality pairs and increases instruction diversity, improving model alignment to task formats.

### Mechanism 3: Multitask Instruction Fine-Tuning
Training simultaneously on six different tasks may allow the model to learn shared representations (e.g., syntactic structure informing both punctuation and POS) that reinforce each task through cross-task generalization.

## Foundational Learning

- **Continued Pre-training (Domain Adaptation)**: Understanding how additional pre-training on specialized corpora modifies a frozen or partially-updated model is essential to interpret WenyanGPT's improvements. Quick check: Given a model pre-trained on general text, what hyperparameters would you adjust to avoid catastrophic forgetting during domain adaptation?

- **Instruction Fine-Tuning (Supervised Fine-Tuning)**: The paper's core contribution includes a novel instruction data construction pipeline; understanding standard SFT helps evaluate whether this pipeline offers measurable advantages. Quick check: How does instruction tuning differ from continued pre-training in terms of data format and training objective?

- **Evaluation Metrics for Understanding vs. Generation Tasks**: WenyanBENCH uses Precision/Recall/F1 for understanding tasks and BLEU/BERT-Score for generation tasks; selecting appropriate metrics is critical for fair model comparison. Quick check: For a Classical Chinese translation task, would BLEU alone capture semantic adequacy, or would auxiliary metrics be necessary?

## Architecture Onboarding

- **Component map**: LLaMA3-8B-Chinese base model -> Continued pre-training on 16GB corpus -> Instruction fine-tuning on 1.85M examples -> Evaluation on WenyanBENCH

- **Critical path**: 1) Corpus preparation (clean, deduplicate, format) -> 2) Continued pre-training on LLaMA3-8B-Chinese -> 3) Instruction data construction (manual → LLM expansion → filtering) -> 4) Supervised fine-tuning -> 5) Evaluation on WenyanBENCH using scripted metrics

- **Design tradeoffs**: Single epoch for both stages reduces overfitting risk but may underutilize data; LLM-generated instructions increase diversity but introduce potential noise; multitask training claims lack ablation studies to confirm benefits.

- **Failure signatures**: Character substitution errors (e.g., "杀" for "弑") suggest insufficient domain vocabulary acquisition; POS confusion between time words and nouns indicates tokenization or representation issues; over-explanation in generation tasks suggests alignment drift.

- **First 3 experiments**:
  1. Ablate continued pre-training: Fine-tune LLaMA3-8B-Chinese directly on instruction data without the 16GB corpus step; compare F1 on NER and BLEU on translation.
  2. Single-task baselines: Train separate models for each of the 6 tasks using identical instruction data; compare to multitask WenyanGPT.
  3. Instruction data quality audit: Sample 200 LLM-generated instruction pairs; manually annotate error rates and correlate with downstream task performance.

## Open Questions the Paper Calls Out

- How can the integration of multimodal data, specifically images of inscriptions and manuscripts, enhance the processing capabilities of Classical Chinese models? The current study focuses solely on text-based processing, and the specific benefits or methods of integrating visual data with Classical Chinese LLMs remain unexplored.

- What specific architectural or training optimizations are necessary to improve performance on long Classical Chinese texts with complex syntax? The current model faces challenges with context length and syntactic complexity, but the paper does not propose solutions for these specific issues.

- How can evaluation benchmarks be adapted to reliably include subjective tasks such as poetry generation? Current benchmarks rely on automated metrics which are insufficient for capturing the literary quality and aesthetic nuances of generated poetry.

## Limitations

- The paper lacks direct comparisons to specialized models trained specifically for Classical Chinese tasks, only comparing against general-purpose models.
- Multitask training benefits are claimed without ablation studies to confirm whether gains arise from task synergy or simply increased training data diversity.
- The instruction generation pipeline relies on LLM expansion without quantifying error rates or validating the filtering process's effectiveness.

## Confidence

- High confidence in technical implementation details and benchmark construction validity
- Medium confidence in performance improvements over general-purpose models
- Low confidence in multitask training benefits and instruction generation pipeline effectiveness

## Next Checks

1. Conduct an ablation study of continued pre-training by training an identical model architecture on the same instruction data without the 16GB Classical Chinese corpus pre-training step, then compare performance on WenyanBENCH.

2. Perform an instruction data quality audit by manually sampling and annotating 200 randomly selected instruction pairs from the final dataset, classifying errors by type and correlating error rates with downstream task performance degradation.

3. Compare single-task baselines by training six separate models, each fine-tuned only on one of the six task types using identical instruction data, then compare their performance against the multitask WenyanGPT to empirically test the claimed cross-task generalization benefits.