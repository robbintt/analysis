---
ver: rpa2
title: Annotation-Free Class-Incremental Learning
arxiv_id: '2511.19344'
source_url: https://arxiv.org/abs/2511.19344
tags:
- imagenet
- downstream
- learning
- clip
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles annotation-free class-incremental learning\
  \ (AFCIL), where models must learn new classes from unlabeled, sequentially arriving\
  \ data without storing exemplars. The authors propose CrossWorld-CL, which leverages\
  \ external world knowledge\u2014specifically, ImageNet\u2014as a stable auxiliary\
  \ source to guide learning in the absence of labels."
---

# Annotation-Free Class-Incremental Learning

## Quick Facts
- arXiv ID: 2511.19344
- Source URL: https://arxiv.org/abs/2511.19344
- Reference count: 40
- Primary result: Annotation-free class-incremental learning using external world knowledge to achieve up to +8.11% accuracy gains over CLIP baselines

## Executive Summary
This paper introduces CrossWorld-CL, a framework for annotation-free class-incremental learning (AFCIL) that leverages external world knowledge—specifically ImageNet—as a stable auxiliary source to guide learning in the absence of labels. The method retrieves semantically related ImageNet classes for each downstream task, aligns features across domains using dual supervision from DINO and CLIP, and introduces a privacy-preserving replay strategy using selected ImageNet samples instead of storing task-specific exemplars. Across four datasets (DTD, CIFAR-100, Oxford Pets, and Oxford Flowers), CrossWorld-CL consistently outperforms CLIP baselines and state-of-the-art continual learning methods.

## Method Summary
CrossWorld-CL operates in five stages: (1) retrieve top-K semantically related ImageNet classes per downstream class using CLIP text similarity, (2) expand class descriptions using GPT-3 and compute averaged CLIP text embeddings, (3) train lightweight adapters mapping DINO features to CLIP space using pseudo-labels (downstream) and ground-truth labels (ImageNet), (4) train visual prompts in CLIP encoder with cross-domain alignment losses, and (5) select top-k ImageNet samples per class as replay proxies. The framework uses frozen CLIP and DINO backbones with trainable visual prompts, LayerNorm parameters, and linear adapters to enable continual learning without exemplar storage.

## Key Results
- CrossWorld-CL achieves average accuracy gains of up to +8.11% over CLIP baselines
- The method demonstrates especially strong performance on later tasks where class confusion is highest
- Minimal forgetting (<1%) is achieved across datasets through the ImageNet replay strategy
- Oxford Pets shows negative forgetting in early tasks, indicating effective knowledge preservation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieving semantically related ImageNet classes provides auxiliary supervision that compensates for missing labels in downstream tasks.
- **Mechanism:** CLIP text embeddings compute cosine similarity between downstream class names and ImageNet class names. Top-K matching ImageNet samples form auxiliary dataset A_t, supplying labeled examples that share visual/semantic properties with unlabeled downstream categories.
- **Core assumption:** CLIP's pretrained text embeddings capture sufficient cross-domain semantic similarity to identify useful proxy classes.
- **Evidence anchors:** [abstract] "retrieves semantically related ImageNet classes for each downstream category"; [Section 3, Stage-1] Shows retrieval formula and construction of A_t; Figure 1 demonstrates coherent matches (e.g., "bottle" → "water bottle," "beer bottle")
- **Break condition:** If downstream classes have no semantically close ImageNet equivalents, retrieved samples may provide misleading gradients.

### Mechanism 2
- **Claim:** Aligning DINO visual features with CLIP semantic space enables reliable pseudo-label generation for unlabeled data.
- **Mechanism:** DINO produces object-centric visual features. Lightweight adapters (φ_D, φ_I) learn to project these into CLIP's text-embedded semantic space using dual supervision from pseudo-labels and ImageNet ground-truth labels. Cross-entropy losses train both adapters jointly.
- **Core assumption:** DINO's self-supervised representations transfer to semantic classification tasks when properly mapped.
- **Evidence anchors:** [Section 3, Stage-3] "This step allows the DINO representations to be semantically grounded in the CLIP space"; [Table 1] Ablation shows removing cross-domain alignment losses drops Task-4/Task-5 accuracy by 15-20%
- **Break condition:** If pseudo-labels from CLIP are systematically wrong, adapter training propagates errors.

### Mechanism 3
- **Claim:** Replaying semantically aligned ImageNet samples instead of downstream exemplars mitigates forgetting while preserving privacy.
- **Mechanism:** After cross-domain alignment, prompted CLIP encoder extracts embeddings from ImageNet auxiliary pool. Top-K samples per downstream class (by similarity to text prototypes) form replay set A_R. KL divergence between successive task prototypes regularizes semantic drift.
- **Core assumption:** ImageNet samples, once aligned, sufficiently approximate downstream class semantics to serve as rehearsal proxies.
- **Evidence anchors:** [Section 3, Stage-5] Describes replay construction and KL-based temporal distillation; [Section 4.3, Forgetting Measure] Reports minimal forgetting (often <1%) across datasets
- **Break condition:** If alignment is poor, replayed ImageNet samples reinforce incorrect associations.

## Foundational Learning

- **Vision-Language Models (CLIP architecture)**
  - Why needed here: CrossWorld-CL depends on CLIP's dual encoders for semantic similarity computation, text prototype initialization, and cross-domain alignment. Without understanding contrastive pretraining and shared embedding spaces, the retrieval and alignment mechanisms are opaque.
  - Quick check question: Can you explain why CLIP's image and text embeddings lie in a shared space, and how cosine similarity enables zero-shot classification?

- **Self-Supervised Visual Representation Learning (DINO)**
  - Why needed here: DINO provides the pseudo-label generation backbone. Its object-centric features differ from CLIP's global alignment—understanding this distinction is critical for grasping why dual supervision works.
  - Quick check question: How does DINO's self-distillation objective differ from contrastive methods like SimCLR, and why might it produce better object-level features?

- **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The entire framework addresses forgetting through replay and regularization. Understanding stability-plasticity tradeoffs, rehearsal buffers, and knowledge distillation is prerequisite to evaluating the replay strategy design.
  - Quick check question: What happens when a neural network trained on task A is fine-tuned on task B without any forgetting mitigation? Why does rehearsal help?

- **Prompt Tuning for Vision Transformers**
  - Why needed here: Stage-4 introduces learnable visual prompts into CLIP's image encoder. Understanding where prompts are injected, what they optimize, and why they preserve pretrained knowledge is essential.
  - Quick check question: Where are visual prompts inserted in a ViT architecture, and why does this require fewer trainable parameters than full fine-tuning?

## Architecture Onboarding

- **Component map:**
  Frozen CLIP ViT-B/32 image encoder f_I → Trainable visual prompts → Trainable CLIP image encoder f_Ip → CLIP text encoder f_t (frozen)
  Frozen DINO backbone f_D → Trainable adapter φ_D → CLIP semantic space
  ImageNet auxiliary pool → Trainable adapter φ_I → CLIP semantic space

- **Critical path:**
  1. **Stage 1:** Encode downstream and ImageNet class names with f_t; retrieve top-K ImageNet classes per downstream class via cosine similarity; construct A_t
  2. **Stage 2:** Query LLM for M descriptions per class; encode and average to initialize t_D; encode ImageNet class names to t_I
  3. **Stage 3:** Generate pseudo-labels for downstream data using f_I and t_D; train φ_D and φ_I via cross-entropy on (pseudo-labeled downstream, labeled ImageNet)
  4. **Stage 4:** Insert visual prompts into CLIP encoder; train f_Ip, t_D, t_I using DINO-generated pseudo-labels and four alignment losses (L_DD, L_II, L_ID, L_DI) plus KL distillation for t>1
  5. **Stage 5:** Extract ImageNet embeddings with f_Ip; select top-K samples per downstream class by similarity to t_D; store as A_R for next-task replay

- **Design tradeoffs:**
  - **ImageNet as world knowledge:** Transparent, reproducible, privacy-safe—but may lack coverage for specialized domains. Paper acknowledges this limitation.
  - **DINO over CLIP for pseudo-labels:** DINO's object-centric features better capture discriminative structure; CLIP optimized for global alignment, may miss fine-grained cues.
  - **Prompts vs. adapters:** Prompts preserve CLIP's generalization; adapters provide task-specific routing. CrossWorld-CL uses both: prompts in visual encoder, adapters on DINO features.
  - **Replay buffer size:** Table 3 shows k=1 to k=10 yields similar results (84-86% on Oxford Pets), suggesting semantic quality matters more than quantity.

- **Failure signatures:**
  - **Semantic drift:** If KL distillation weight λ_4 is too low, text prototypes shift excessively; early task accuracy degrades.
  - **Misaligned retrieval:** If CLIP text similarity retrieves irrelevant ImageNet classes, auxiliary supervision introduces noise; expect high variance in later-task performance.
  - **Pseudo-label collapse:** If CLIP confidence threshold is too low, φ_D trains on noisy labels; DINO adapter outputs become unreliable for Stage-4 supervision.
  - **Cross-domain mismatch:** Removing L_ID or L_DI causes 10-20% drops in Task-4/Task-5, indicating bidirectional alignment is necessary for long sequences.

- **First 3 experiments:**
  1. **Validate retrieval quality:** For a single task, manually inspect top-3 ImageNet classes retrieved for 10 random downstream classes. Compute precision@3 against human judgment. If <0.7, adjust similarity threshold or expand LLM descriptions.
  2. **Ablate alignment losses incrementally:** Train on 5-task split with only L_DD, then add L_II, then L_ID, then L_DI. Plot task-wise accuracy curves. Confirm cross-domain losses contribute to later-task stability.
  3. **Measure forgetting explicitly:** After training on all 5 tasks, evaluate each task's test set separately. Compute forgetting measure F(t) per Section 8. Compare against baseline (Continual CLIP) to quantify replay effectiveness.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- **Retrieval dependency:** Performance hinges on CLIP text embeddings finding semantically related ImageNet classes; no fallback mechanism for highly specialized downstream categories where ImageNet coverage is sparse.
- **LLM expansion fragility:** Quality of semantic expansions directly affects prototype initialization. Poorly written or ambiguous descriptions could lead to misaligned prototypes, propagating errors into adapter training and replay selection.
- **External knowledge assumptions:** Relying on ImageNet as "world knowledge" limits applicability to domains with different visual vocabularies (satellite imagery, industrial inspection).

## Confidence
- **High confidence:** Retrieval of semantically related ImageNet classes via CLIP similarity (Stage 1) is well-justified by established text-image alignment literature and clear experimental support.
- **Medium confidence:** Dual supervision with DINO pseudo-labels + ImageNet ground-truth for adapter training (Stage 3) is novel and theoretically sound, but the exact impact of pseudo-label noise is unclear without ablation studies.
- **Low confidence:** Privacy preservation claims are somewhat overstated—while ImageNet replay avoids storing raw downstream data, the external dataset itself may contain sensitive content.

## Next Checks
1. **Retrieval stress test:** For each downstream dataset, identify the 5 classes with lowest CLIP similarity to any ImageNet class. Manually assess whether retrieved ImageNet samples are truly semantically relevant. If <60% relevance, explore expanding LLM descriptions or using multiple external knowledge sources.
2. **Ablate replay size systematically:** Train on 5-task splits with k=1, k=3, k=5, k=10 replay images per class. Plot forgetting curves and average accuracy. Confirm whether semantic quality outweighs sheer quantity.
3. **Cross-domain transfer test:** Apply CrossWorld-CL to a domain with minimal ImageNet overlap (e.g., medical X-ray or satellite imagery using ImageNet as auxiliary). Measure retrieval quality and overall accuracy drop. If performance degrades >15%, investigate domain-adaptive retrieval mechanisms.