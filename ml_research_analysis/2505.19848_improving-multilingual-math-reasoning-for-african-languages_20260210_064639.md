---
ver: rpa2
title: Improving Multilingual Math Reasoning for African Languages
arxiv_id: '2505.19848'
source_url: https://arxiv.org/abs/2505.19848
tags:
- languages
- data
- language
- african
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates strategies for adapting large
  language models to African languages, focusing on mathematical reasoning tasks.
  The authors compare synthetic data generation (via persona-driven prompts) against
  machine translation of English math datasets, examining prompt masking, scaling,
  monolingual vs multilingual training, and continual pretraining.
---

# Improving Multilingual Math Reasoning for African Languages

## Quick Facts
- arXiv ID: 2505.19848
- Source URL: https://arxiv.org/abs/2505.19848
- Reference count: 33
- One-line primary result: Synthetic native-language data generation via persona-driven prompts achieves comparable performance to high-quality machine-translated data for African language math reasoning

## Executive Summary
This paper systematically evaluates strategies for adapting large language models to African languages, focusing on mathematical reasoning tasks. The authors compare synthetic data generation (via persona-driven prompts) against machine translation of English math datasets, examining prompt masking, scaling, monolingual vs multilingual training, and continual pretraining. Experiments using the Llama 3.1 8B model family show that synthetic native-language data performs comparably to high-quality translated data, with both outperforming translation of narrow-domain datasets. Notably, masking prompts during fine-tuning degrades performance, and multilingual training yields better results than monolingual models. The 30k-sample synthetic dataset trained model approaches or exceeds GPT-4 performance on several African languages.

## Method Summary
The authors fine-tune Llama 3.1 8B Instruct using DeepSpeed ZeRO-3 on 4x A100 80GB GPUs with mixed precision. They compare synthetic native-language data (AfriPersona-Instruct) generated via persona-driven prompts against translated English datasets (OpenMathInstruct, BigMath). Training runs for 2 epochs with effective batch size 128, learning rate 5e-5 with linear schedule and 3% warmup. Loss is computed over all tokens (instruction + response), and models are evaluated on AfriMGSM benchmark using GPT-4o-as-judge. The synthetic data pipeline involves text-to-persona and persona-to-persona generation using GPT-4o and Wikipedia/WURA articles, with deduplication via MinHash LSH.

## Key Results
- Multilingual training outperforms monolingual training across all nine African languages tested
- Synthetic native-language data (AfriPersona-Instruct) performs comparably to high-quality translated data (OpenMathInstruct)
- Narrow-domain translated datasets (BigMath) significantly underperform broader datasets
- Not masking prompts during fine-tuning improves performance in low-resource settings
- 30k synthetic samples trained model approaches or exceeds GPT-4 performance on several African languages

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Transfer via Multilingual Joint Training
Training a single model on multiple African languages simultaneously yields better performance than training separate monolingual experts. Joint multilingual training allows the model to leverage cross-lingual transfer, where shared representations and patterns learned from one language improve performance on others, even with limited data per language.

### Mechanism 2: Quality and Diversity of Synthetic Native-Language Data
Synthetic data generated directly in target low-resource languages using persona-driven prompts can be competitive with high-quality machine-translated data from English datasets. Persona-based prompts guide a strong LLM to generate diverse, contextually relevant math problems and solutions directly in the target language, avoiding translation artifacts and better capturing cultural context/linguistic nuances.

### Mechanism 3: Language Adaptation via Loss on Instruction Tokens
Computing the supervised fine-tuning loss over prompt/instruction tokens improves performance on mathematical reasoning for low-resource languages. Including instruction tokens in the loss provides an additional gradient signal that may force the model to better model the input language distribution, grounding its understanding of the question itself before generating an answer.

## Foundational Learning

**Cross-Lingual Transfer**
- Why needed here: This is the core principle explaining why multilingual training outperforms monolingual training
- Quick check question: If a model learns to solve a math problem in English, does it implicitly learn a solution structure that applies to Swahili?

**Supervised Fine-Tuning (SFT)**
- Why needed here: This is the primary training stage investigated
- Quick check question: What is the goal of SFT in a post-training pipeline for an already pre-trained model?

**Persona-Driven Data Synthesis**
- Why needed here: This is the specific method used to generate the "AfriPersona-Instruct" dataset
- Quick check question: How does generating a "persona" from a Wikipedia article help create a more diverse math problem?

## Architecture Onboarding

**Component map**: Base Model (Llama 3.1 8B Instruct) -> Data Sources (Synthetic Native Data + Translated Data) -> Training (SFT with loss on all tokens) -> Evaluation (LLM-as-a-Judge)

**Critical path**: The most effective path found is using Llama 3.1 8B Instruct, training on a multilingual mix of high-quality data (combining synthetic native-language data and high-quality translated data like OpenMathInstruct), computing loss over all tokens, and scaling data size to at least 30k samples.

**Design tradeoffs**:
- Synthetic vs. Translated Data: Synthetic native data can be more culturally aligned but may have linguistic errors. Translated data from broad sources is high quality but may have translation artifacts. Combining both yields the best results.
- Multilingual vs. Monolingual: Multilingual training is simpler (one model) and yields better performance via transfer. Monolingual models are specialists but perform worse due to lack of transfer.
- Prompt Masking: Masking prompts simplifies the loss but degrades performance in low-resource settings. Not masking adds complexity/signal but improves results.

**Failure signatures**:
- Training on translated data from a narrow/specialized source (e.g., BigMath) results in poor generalization (very low average score: 11.2)
- Continual pre-training on a general corpus (Lugha-Llama) followed by SFT on math data can underperform compared to SFT-only, likely due to domain mismatch
- Synthetic data generation produces "unnatural phrasing," "lexical inaccuracies," or "ambiguous" problems if the generator's language capability is insufficient

**First 3 experiments**:
1. Establish a multilingual baseline: Fine-tune Llama 3.1 8B Instruct on 30k samples from OpenMathInstruct across all 9 target languages. Do not mask the prompt. Evaluate on AfriMGSM.
2. Implement the persona-based synthesis pipeline: Build the "text-to-persona" and "persona-to-persona" pipeline using GPT-4o and Wikipedia data for one low-resource African language. Generate a few hundred synthetic math problems. Qualitatively audit them for linguistic quality.
3. Compare synthetic vs. translated data: Fine-tune separate Llama 3.1 8B Instruct models using (a) only synthetic native-language data (e.g., 10k samples) and (b) only translated data (10k samples). Compare their performance to quantify the trade-off for your specific target language.

## Open Questions the Paper Calls Out

**Open Question 1**: Would the observed benefits of synthetic data scaling and multilingual instruction tuning generalize to non-mathematical reasoning domains such as commonsense reasoning, reading comprehension, or scientific QA in African languages?

**Open Question 2**: Would domain-aligned continual pretraining (math-focused corpora) improve mathematical reasoning performance in African languages, compared to the general language pretraining used in Lugha-LLaMA?

**Open Question 3**: How would automated or human-in-the-loop verification of synthetic data correctness affect downstream model performance on mathematical reasoning tasks?

**Open Question 4**: How sensitive are the findings to variations in prompt templates, source tasks, or instruction styles for synthetic data generation?

## Limitations
- The synthetic data generation pipeline relies heavily on GPT-4o's language capabilities, which may not be consistently available or performant across all target African languages
- The evaluation methodology using LLM-as-a-judge introduces potential bias, as the same model (GPT-4o) is used both for data generation and evaluation
- The study focuses specifically on mathematical reasoning tasks, and findings may not directly transfer to other domains or reasoning types

## Confidence
**High Confidence**: The core finding that multilingual training outperforms monolingual training for African language math reasoning is well-supported by experimental results across all nine languages tested.

**Medium Confidence**: The effectiveness of not masking prompts during fine-tuning is demonstrated, but the mechanism could benefit from deeper analysis. The claim that this provides additional language modeling signal is plausible but not definitively proven.

**Medium Confidence**: The persona-driven synthetic data generation approach shows promise, but the quality assessment relies on automated evaluation. The paper acknowledges linguistic imperfections in generated data.

## Next Checks
1. Conduct controlled experiments isolating cross-lingual transfer effects by training models on disjoint language subsets and measuring performance transfer to unseen languages.
2. Implement human evaluation of synthetic math problems across the nine target languages, focusing on linguistic naturalness, cultural relevance, and mathematical clarity.
3. Evaluate the trained models on non-mathematical reasoning tasks in the same languages to determine whether the adaptations generalize beyond math reasoning.