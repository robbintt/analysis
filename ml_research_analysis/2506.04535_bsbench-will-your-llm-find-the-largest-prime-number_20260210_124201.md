---
ver: rpa2
title: 'BSBench: will your LLM find the largest prime number?'
arxiv_id: '2506.04535'
source_url: https://arxiv.org/abs/2506.04535
tags:
- answer
- arxiv
- https
- questions
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BSBench, a novel benchmark testing whether
  LLMs can recognize and admit when tasks are logically impossible. The core idea
  is to evaluate if models avoid the common bias of always attempting to answer, even
  when no valid solution exists.
---

# BSBench: will your LLM find the largest prime number?

## Quick Facts
- arXiv ID: 2506.04535
- Source URL: https://arxiv.org/abs/2506.04535
- Authors: K. O. T. Erziev
- Reference count: 16
- Key outcome: BSBench tests if LLMs recognize logically impossible tasks; even advanced models like Claude-4 and o3-mini frequently fail, showing solution-seeking bias

## Executive Summary
BSBench introduces a novel benchmark testing whether LLMs can recognize and admit when tasks are logically impossible. The benchmark includes 40 manually crafted impossible questions spanning logical, mathematical, nomological, and technological impossibility types. Experiments show that even advanced models like Claude-4 and o3-mini fail to recognize impossibility in a significant fraction of cases, with average bs_score drops of around 50% when correct answers are replaced with "no answer" options. The results highlight a fundamental limitation in current LLMs and suggest the need for improved training to handle unanswerable questions appropriately.

## Method Summary
The study evaluates LLMs using BSBench (40 impossible questions) and BS-fied GPQA-diamond (modified multiple-choice questions with invalid answers). Models are prompted with standardized "Simple" or "Manus-inspired" system prompts, optionally followed by "try better" retry loops. An LLM judge evaluates whether responses unambiguously state impossibility (binary yes/no). The primary metric is bs_score - the fraction of times the model fails to clearly state impossibility. The method tests both static responses and pressure sensitivity under conversational retry attempts.

## Key Results
- Advanced models (Claude-4, o3-mini) fail to recognize impossibility in significant fractions of cases, with bs_scores ranging from 0.3-0.6 across impossibility types
- GPQA-BS experiments show ~50% average score drop when correct answers are replaced with "There is no correct answer"
- Retry pressure causes models to flip-flop between correct refusal and incorrect attempts, with "try better" follow-ups reducing correct uncertainty recognition
- No significant performance differences between simple and system prompts, though all models show high bs_score regardless of prompt style

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit solution-seeking bias because training corpora predominantly contain answerable questions with valid solutions
- Mechan: Models learn statistical patterns from data where questions have answers; this creates pressure toward generating responses rather than refusing, even when refusal is correct
- Core assumption: Prevalence of answerable training examples creates learned prior that questions should have answers
- Evidence anchors: [abstract] "existing models demonstrate a performance far from the perfect on such questions"; [section 1] contrast with carefully crafted benchmarks
- Break condition: If models were trained with substantial proportions of explicitly impossible/unsolvable examples with correct refusal responses, this bias should diminish

### Mechanism 2
- Claim: "BS-fication" of existing benchmarks exposes impossibility-blindness by replacing correct answers with invalid options while preserving question format
- Mechan: Substituting valid answers with phrases like "This is a sample answer" or "There is no correct answer" tests whether models follow formatting/pattern-matching impulses versus recognizing semantic impossibility
- Core assumption: Performance drops reflect genuine inability to recognize impossibility rather than confusion from unnatural modifications
- Evidence anchors: [abstract] "discover that existing models demonstrate a performance far from the perfect on such questions"; [section 3] "Average drop of score is around 50%"
- Break condition: If models were explicitly trained to detect invalid options in multiple-choice settings, the drop should narrow

### Mechanism 3
- Claim: "Try better" pressure amplifies the solution-seeking bias, causing models that initially refuse to later attempt impossible solutions
- Mechan: When users push back with requests to improve answers, models interpret this as implicit feedback that their refusal was wrong, overriding correct uncertainty recognition
- Core assumption: Social/linguistic pressure cues override logical assessment
- Evidence anchors: [section 2] "we test our approach in a similar setting by sending the impossible task once, and sending several consecutive 'try better' questions"; [section 3, Figure 2] shows transitions between yes/no responses
- Break condition: If models had robust uncertainty calibration that persists under conversational pressure, retry loops would not cause flip-flopping

## Foundational Learning

- Concept: **Types of impossibility (logical, nomological, technological)**
  - Why needed here: The benchmark categorizes questions by impossibility type; understanding distinctions is required to interpret failure patterns
  - Quick check question: "Is 'find a prime number divisible by 4' a logical or nomological impossibility?"

- Concept: **Reward hacking in agentic systems**
  - Why needed here: The paper positions BSBench as a detection mechanism for agents claiming completion of impossible tasks
  - Quick check question: "If an agent reports success on an impossible task, what does this indicate about its objective function?"

- Concept: **LLM-as-judge evaluation**
  - Why needed here: BSBench uses an LLM judge to score whether responses "clearly state impossibility"; understanding judge limitations is critical for interpreting results
  - Quick check question: "What failure modes might occur when using an LLM to evaluate another LLM's refusal behavior?"

## Architecture Onboarding

- Component map: BSBench dataset (40 impossible questions) -> Model prompt (Simple/Manus) -> Response capture -> LLM judge evaluation -> bs_score aggregation
- Critical path: 1. Select impossible question → 2. Prompt model with standardized format → 3. Capture response → 4. LLM judge scores clarity of impossibility statement → 5. Aggregate bs_score
- Design tradeoffs: Small dataset (n=40) limits statistical power but enables manual quality control; LLM judge introduces subjectivity; system prompts may exacerbate solution-seeking
- Failure signatures: Model provides attempted solution without acknowledging impossibility; model acknowledges uncertainty but outputs formatted answer; model flip-flops between refusal and attempt under "try better" pressure
- First 3 experiments: 1) Replicate BSBench evaluation on your target model using provided judge prompt to establish baseline bs_score; 2) Test retry loop sensitivity with "try better" 3 times and measure transition rates; 3) Apply BS-fication to domain-specific multiple-choice benchmark and measure performance drop

## Open Questions the Paper Calls Out

- **Open Question 1:** Do training datasets lacking "unanswerable" examples cause the observed bias to solve impossible tasks?
  - Basis in paper: Section 5 asks about prevalence of training data with answers as potential cause
  - Why unresolved: Paper identifies failure mode but doesn't perform ablation studies to isolate root cause
  - What evidence would resolve it: Comparative analysis of models trained on standard versus unanswerable-rich datasets

- **Open Question 2:** Can targeted training data modification eliminate the tendency of LLMs to hallucinate answers for impossible questions?
  - Basis in paper: Section 5 proposes modification to training data as potential solution
  - Why unresolved: Authors propose intervention but don't implement or validate training run
  - What evidence would resolve it: Follow-up showing models trained on "BS-ficated" datasets achieve significantly lower bs_score

- **Open Question 3:** Can impossible tasks function as a negative reward signal to improve early stopping in agentic systems?
  - Basis in paper: Section 5 asks about serving as valuable negative signal to reward early stopping
  - Why unresolved: Current work benchmarks static responses, doesn't test in RL environment
  - What evidence would resolve it: Integration into RL environment showing penalizing failure to admit impossibility reduces resource waste

## Limitations
- Small dataset size (n=40) constrains statistical generalizability and may overfit to specific impossibility types
- LLM judge evaluation introduces subjectivity without inter-annotator agreement statistics
- "Try better" pressure mechanism lacks causal isolation - unclear whether learned conversational patterns versus genuine uncertainty calibration failures
- GPQA-BS modifications may introduce confounds through unnatural answer replacements that don't cleanly test impossibility recognition

## Confidence

**High confidence:** Core empirical finding that LLMs frequently fail to recognize impossibility (directly measurable through bs_score metrics across multiple models)

**Medium confidence:** Mechanism linking training data bias to solution-seeking behavior (supported by related work but not causally proven)

**Medium confidence:** Claim that retry pressure causes flip-flopping between refusal and attempt (observed but not mechanism-isolated)

**Low confidence:** Broader implication that this limitation threatens agentic system safety (plausible but requires additional validation in deployment contexts)

## Next Checks

1. **Dataset scaling validation:** Generate 10x larger BSBench dataset (n=400) using LLM-assisted question generation with human validation to test whether failure patterns persist with increased statistical power

2. **Judge reliability assessment:** Implement inter-annotator agreement testing with 3+ different LLM judges (including open and closed models) and calculate Cohen's kappa to quantify evaluation subjectivity

3. **Training intervention test:** Fine-tune a model on balanced corpus containing 50% impossible questions with correct refusal responses, then re-evaluate on BSBench to test training data hypothesis directly