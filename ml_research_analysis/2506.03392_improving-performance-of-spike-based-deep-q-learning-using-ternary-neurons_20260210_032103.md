---
ver: rpa2
title: Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons
arxiv_id: '2506.03392'
source_url: https://arxiv.org/abs/2506.03392
tags:
- spiking
- neurons
- ternary
- neuron
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of ternary spiking neurons to improve
  the representation capacity of spiking neural networks in deep Q-learning tasks.
  The authors mathematically analyze the performance degradation observed with existing
  ternary neuron models and hypothesize that gradient estimation bias during training
  is the underlying cause.
---

# Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons

## Quick Facts
- **arXiv ID:** 2506.03392
- **Source URL:** https://arxiv.org/abs/2506.03392
- **Reference count:** 40
- **Primary result:** Asymmetric ternary spiking neurons achieve 130% average improvement over binary spiking neuron baseline on 7 Atari games

## Executive Summary
This paper addresses the performance gap between spiking neural networks (SNNs) and artificial neural networks in deep Q-learning by introducing asymmetric ternary spiking neurons. The authors identify that symmetric ternary neurons suffer from gradient estimation bias during training, which degrades learning performance. They propose a novel asymmetric ternary model where positive and negative spike thresholds differ, enabling better gradient flow while maintaining the benefits of sparse spiking computation. The model is evaluated on seven Atari games using a deep spiking Q-learning network, demonstrating significant performance improvements over binary spiking baselines while avoiding common training stability issues.

## Method Summary
The method introduces an asymmetric ternary leaky integrate-and-fire (LIF) neuron that outputs spikes in the set {-1, 0, 1} with different thresholds for positive ($v_{th}^p=1$, fixed) and negative ($v_{th}^n=2$, trainable) spikes. The neuron integrates membrane potential over time and generates spikes when thresholds are crossed, with the negative threshold treated as a learnable parameter. The network uses standard DQN architecture (3 convolutional layers + 2 fully connected layers) with rate-based Bernoulli encoding for input pixels and accumulates spikes over a 20-timestep window to produce Q-values. Surrogate gradient learning enables backpropagation through the non-differentiable spiking mechanism.

## Key Results
- Achieves 130% average improvement over binary spiking neuron baseline on 7 Atari games
- Maintains training stability without gradient vanishing or exploding problems
- Outperforms symmetric ternary models that suffer from zero expected gradient during backpropagation
- Better aligns with biological evidence showing most brain neurons are excitatory rather than symmetrically balanced

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Asymmetric firing thresholds prevent the expected gradient from collapsing to zero during backpropagation.
- **Mechanism:** Standard ternary neurons use symmetric thresholds ($v_{th}^p = v_{th}^n$). Assuming a Gaussian-like distribution of membrane potentials, this symmetry causes positive and negative spike probabilities to cancel out ($p^+ = p^-$), driving the expected gradient $\mathbb{E}[\frac{\partial \tilde{s}_T}{\partial m(t)}]$ to zero. The proposed model decouples these thresholds ($v_{th}^p \neq v_{th}^n$), ensuring $p^+ \neq p^-$ and maintaining a non-zero gradient flow essential for Reinforcement Learning (RL).
- **Core assumption:** The subthreshold membrane potential follows a Gaussian-like distribution.
- **Evidence anchors:**
  - [Section 4.1]: "For ternary neurons with symmetric thresholds... the expected gradient becomes zero."
  - [Section 4.2]: "We propose a novel ternary spiking model... where $v_{th}^p \neq v_{th}^n$."
  - [Corpus]: Related work confirms the difficulty of training SNNs in RL due to non-differentiability, though specific asymmetric thresholds are not mentioned. (Corpus evidence weak for specific mechanism).

### Mechanism 2
- **Claim:** Trainable negative thresholds allow the network to dynamically balance representation capacity against gradient bias.
- **Mechanism:** The fixed positive threshold ($v_{th}^p=1$) anchors the network, while the negative threshold ($v_{th}^n$) is treated as a learnable parameter (`nn.Parameter`). This allows the optimizer to minimize gradient bias by finding an optimal ratio between excitatory and inhibitory firing probabilities, rather than forcing a static trade-off.
- **Core assumption:** The optimizer can successfully navigate the trade-off between information entropy (capacity) and gradient stability via standard gradient descent.
- **Evidence anchors:**
  - [Section 4.2]: "We propose fixing $v_{th}^p$ at a constant value and treating $v_{th}^n$ as a trainable parameter."
  - [Section 5 (Fig 3c/d)]: Shows the negative threshold evolving dynamically during training, diverging from the positive threshold.
  - [Corpus]: No direct corpus support for trainable thresholds in this specific context.

### Mechanism 3
- **Claim:** The architecture maintains "dynamic isometry," preventing vanishing/exploding gradients in deep recurrent structures.
- **Mechanism:** By maintaining the singular values of the network's input-output Jacobian close to 1, the asymmetric ternary spike distribution preserves the magnitude of gradients through layers. The paper mathematically derives that this model approximates the stability properties of ReLU networks better than symmetric ternary models.
- **Core assumption:** The firing rate $r$ is sufficiently low (sparse spiking) to satisfy the conditions of Lemma 4.6.
- **Evidence anchors:**
  - [Section 4.3]: "The asymmetric ternary spiking neuron achieves at least the dynamical isometry of ReLU activations."
  - [Section 5 (Training Dynamics)]: "Average gradient norm during training remains stable."
  - [Corpus]: General support for SNN training stability challenges exists, but specific "dynamic isometry" claims are unique to this paper.

## Foundational Learning

- **Concept:** **Surrogate Gradient Learning (SGL)**
  - **Why needed here:** Spiking neurons use a non-differentiable step function to generate spikes. SGL substitutes a smooth derivative (e.g., ATan, STE) during backpropagation to estimate gradients, which is the foundational technique enabling the training of the proposed DATSQN.
  - **Quick check question:** Can you explain why a surrogate function is needed only during the backward pass but not the forward pass?

- **Concept:** **The Bias-Variance Trade-off in RL Replay Buffers**
  - **Why needed here:** The paper argues that in RL (unlike CV/NLP), gradient bias is catastrophic because the training data (replay buffer) is generated *by* the network's own policy. A biased gradient leads to poor policies, generating poor data, creating a feedback loop of degradation.
  - **Quick check question:** How does the quality of data in a replay buffer differ between a static dataset (Supervised Learning) and an agent-generated dataset (RL)?

- **Concept:** **LIF Neuron Dynamics (Integrate-and-Fire)**
  - **Why needed here:** The model modifies the standard Leaky Integrate-and-Fire (LIF) equation. Understanding the basic cycle—membrane potential charging, threshold comparison, resetting—is required to understand how asymmetric thresholds alter the spike generation logic (Eq. 19).
  - **Quick check question:** In the standard binary LIF model, what happens to the information encoded in negative membrane potentials?

## Architecture Onboarding

- **Component map:** Input (Bernoulli encoder) -> 3 Conv layers -> 2 FC layers -> Asymmetric Ternary LIF Neurons -> Output (Linear layer summing spikes)

- **Critical path:**
  1. Implement Eq. (19) carefully: The neuron must handle two distinct thresholds ($v_{th}^p, v_{th}^n$).
  2. Initialize $v_{th}^p=1$ (fixed) and $v_{th}^n=2$ (trainable).
  3. Ensure the backpropagation pass uses the surrogate gradient function (Table 1) defined for the asymmetric domain.

- **Design tradeoffs:**
  - **Simulation Window ($T$):** The paper uses $T=20$ (shorter than standard 40). This reduces latency and energy but increases the challenge for the encoder to represent information accurately.
  - **Energy vs. Capacity:** Ternary spikes require 2-bit activations (vs 1-bit for binary). This incurs a slight memory/energy overhead per spike but reduces the number of time steps required to achieve the same representation capacity.

- **Failure signatures:**
  - **Symmetric Degradation:** If performance drops below the binary baseline, check if $v_{th}^n$ has converged to $v_{th}^p$ (symmetry), causing the "zero expected gradient" problem.
  - **Silent Neurons:** If $v_{th}^n$ becomes too large (negative), the inhibitory pathway may die out, effectively reverting the network to binary behavior without the performance gains.
  - **Instability:** If gradient norms explode, verify the surrogate gradient function is correctly clipped/applied in the asymmetric regions.

- **First 3 experiments:**
  1. **Sanity Check (Gradient Norms):** Train a symmetric ternary model vs. the asymmetric model on a simple task. Plot the average gradient norm to verify that the symmetric model's gradient collapses to near zero while the asymmetric model's remains stable (replicate Fig 2b).
  2. **Ablation on Trainability:** Compare three configurations: (A) Fixed symmetric thresholds, (B) Trainable *shared* threshold, and (C) Trainable *asymmetric* negative threshold. Verify that only (C) yields significant performance improvements over the binary baseline.
  3. **Threshold Evolution Tracking:** Monitor the value of $v_{th}^n$ during training on a complex Atari game (e.g., Breakout). Confirm if the network naturally drives the threshold to satisfy the biological "80/20" excitation/inhibition ratio mentioned in the Discussion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the asymmetric ternary neuron model improve performance in non-RL domains such as computer vision and natural language processing?
- Basis in paper: [explicit] Section 6 states the model is "broadly applicable" and "holds promise" for computer vision, NLP, and control systems, despite being tested only on Atari games.
- Why unresolved: The paper restricts experimental evaluation to Q-learning tasks; the gradient estimation bias hypothesis was specifically contextualized for RL data dependencies.
- What evidence would resolve it: Benchmarks of the DATSQN model on standard vision datasets (e.g., ImageNet) or language tasks compared to state-of-the-art binary SNNs.

### Open Question 2
- Question: How does the proposed neuron model interact with on-policy reinforcement learning algorithms like Policy Gradients?
- Basis in paper: [inferred] The analysis of gradient bias (Section 4.1) focuses on Q-learning and the quality of data in the replay buffer; it does not address on-policy learning dynamics.
- Why unresolved: The interplay between asymmetric thresholds and the variance/bias trade-off in on-policy gradient estimation remains unexplored.
- What evidence would resolve it: Evaluation of the asymmetric ternary model using Actor-Critic or PPO algorithms on standard control benchmarks.

### Open Question 3
- Question: Is the assumption of a Gaussian-like membrane potential distribution valid in deep network layers?
- Basis in paper: [inferred] Section 4.1 derives the Gaussian distribution assuming independent inputs to the *first* layer. It notes this assumption holds "mostly" but inputs to deeper layers are correlated spike trains.
- Why unresolved: If the Central Limit Theorem justification fails in deeper layers due to correlated spikes, the theoretical basis for the expected gradient calculation weakens.
- What evidence would resolve it: Empirical statistical analysis of membrane potential distributions in the final convolutional or fully connected layers of the trained network.

## Limitations

- The performance improvements are demonstrated only on Atari games, with claims about broader applicability remaining theoretical.
- The computational overhead of ternary spikes (2-bit vs 1-bit) may offset some energy efficiency gains in practical implementations.
- The learning dynamics of the trainable negative threshold parameter could potentially lead to instability if not properly constrained.

## Confidence

- **High Confidence:** The architectural implementation of asymmetric ternary LIF neurons (Eq. 19) and the mathematical derivation of gradient bias in symmetric models (Lemma 4.1) are well-supported by the paper's analysis.
- **Medium Confidence:** The empirical performance improvements on Atari games are demonstrated, but the ablation studies comparing symmetric vs asymmetric thresholds are not shown, leaving some uncertainty about the specific contribution of the asymmetry.
- **Low Confidence:** The claim about achieving "dynamic isometry" comparable to ReLU networks (Mechanism 3) lacks sufficient empirical validation beyond gradient norm monitoring.

## Next Checks

1. **Gradient Collapse Verification:** Implement a controlled experiment comparing gradient norms during training between symmetric ternary and asymmetric ternary models on a simple RL task to verify the claimed difference in gradient stability.
2. **Threshold Evolution Analysis:** Track the evolution of $v_{th}^n$ during training across multiple runs to confirm it converges to a stable value rather than oscillating or diverging, particularly in complex environments.
3. **Energy-Efficiency Tradeoff:** Measure the actual energy consumption (or computational overhead) of the ternary model compared to binary baselines, accounting for both the increased per-spike memory and reduced time steps.