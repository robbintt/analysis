---
ver: rpa2
title: 'HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space'
arxiv_id: '2509.22299'
source_url: https://arxiv.org/abs/2509.22299
tags:
- atomic
- expert
- pruning
- experts
- heapr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing Mixture-of-Experts
  (MoE) models by proposing a novel atomic expert pruning approach called HEAPr. Existing
  methods focus on coarse-grained expert-level pruning, which often leads to accuracy
  degradation.
---

# HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space

## Quick Facts
- arXiv ID: 2509.22299
- Source URL: https://arxiv.org/abs/2509.22299
- Reference count: 12
- Key outcome: Nearly lossless MoE compression at 20%-25% pruning ratios with 20% FLOPs reduction

## Executive Summary
HEAPr addresses the challenge of compressing Mixture-of-Experts (MoE) models by proposing a novel atomic expert pruning approach that decomposes experts into smaller, indivisible atomic experts and leverages second-order information to quantify their importance. Existing methods focus on coarse-grained expert-level pruning, which often leads to accuracy degradation. HEAPr overcomes this by transforming second-order information from expert parameters to atomic expert outputs, reducing space complexity from O(d^4) to O(d^2).

The key innovation is that HEAPr requires only two forward passes and one backward pass on a small calibration set to compute atomic expert importance, making it computationally efficient. Experiments on various MoE models demonstrate that HEAPr achieves nearly lossless compression at pruning ratios of 20%-25%, outperforming existing expert-level pruning methods while reducing FLOPs by approximately 20%.

## Method Summary
HEAPr decomposes each expert into smaller, indivisible atomic experts and uses Hessian-based second-order information to quantify atomic expert importance. The method transforms this information from parameter space to output space, reducing computational complexity from O(d^4) to O(d^2). By requiring only two forward passes and one backward pass on a small calibration set, HEAPr efficiently identifies and prunes less important atomic experts while preserving model accuracy. This fine-grained approach enables substantial model efficiency improvements compared to traditional expert-level pruning methods.

## Key Results
- Achieves nearly lossless compression at 20%-25% pruning ratios across various MoE models
- Reduces FLOPs by approximately 20% while maintaining model performance
- Outperforms existing expert-level pruning methods on Qwen-family models
- Demonstrates effectiveness on DeepSeekMoE-16B-Base, Qwen1.5-MoE-A2.7B-Chat, Qwen2-57B-A14B, and Qwen3-30B-A3B

## Why This Works (Mechanism)
HEAPr works by decomposing experts into atomic units and using second-order Hessian information to measure their contribution to model outputs. The transformation from parameter space to output space enables efficient computation of importance scores while maintaining accuracy. This approach captures fine-grained dependencies between atomic experts and model predictions, allowing for more precise pruning decisions compared to coarse-grained expert-level methods.

## Foundational Learning

**Mixture-of-Experts (MoE)**: A neural network architecture that activates different subsets of experts for different inputs, enabling conditional computation. Understanding MoE is crucial because HEAPr specifically targets the efficiency challenges in these models.

**Second-order optimization**: Uses curvature information (Hessian) to understand parameter sensitivity. Essential for HEAPr's importance quantification method, as it provides more nuanced importance signals than first-order gradients alone.

**Atomic expert decomposition**: Breaking down experts into smaller, indivisible units for finer-grained pruning. This concept is fundamental to HEAPr's approach, enabling more precise pruning than traditional expert-level methods.

**Pruning ratio**: The fraction of model components removed during compression. Critical for evaluating HEAPr's effectiveness in achieving compression goals while maintaining performance.

**FLOPs reduction**: Measuring computational efficiency improvements after pruning. Important metric for assessing HEAPr's practical impact on model deployment efficiency.

## Architecture Onboarding

**Component map**: Input -> Gate Mechanism -> Expert Selection -> Atomic Expert Processing -> Output Aggregation

**Critical path**: The gating mechanism determines which atomic experts process each input token, making gate design and atomic expert importance calculation central to HEAPr's effectiveness.

**Design tradeoffs**: HEAPr trades computational overhead of second-order calculations for more precise pruning decisions, enabling higher compression ratios with less accuracy loss compared to expert-level pruning.

**Failure signatures**: If atomic expert importance calculations are inaccurate, pruning may remove critical components leading to accuracy degradation. Over-pruning beyond 25% may cause significant performance drops.

**First experiments**:
1. Apply HEAPr to a small MoE model with known sensitivity patterns to verify atomic importance scoring accuracy
2. Compare HEAPr's pruning decisions against random pruning to validate the effectiveness of second-order information
3. Test HEAPr on a single expert decomposition to verify the transformation from parameter to output space works correctly

## Open Questions the Paper Calls Out

None

## Limitations

- Reliance on second-order Hessian information assumes accurate importance quantification across diverse architectures and tasks
- Experimental scope limited to Qwen-family models, with unverified effectiveness on other MoE architectures
- Evaluation focuses primarily on perplexity metrics, lacking downstream task performance assessment
- Hardware-specific inference benefits and real deployment constraints not empirically validated

## Confidence

- **High Confidence**: Mathematical formulation of atomic expert importance using Hessian-based second-order information and space complexity reduction from O(d^4) to O(d^2)
- **Medium Confidence**: "Nearly lossless compression" claims at 20%-25% pruning ratios supported by Qwen model experiments but requiring broader validation
- **Medium Confidence**: Superiority over existing expert-level pruning methods substantiated within experimental scope but may not generalize to all pruning scenarios

## Next Checks

1. Apply HEAPr to non-Qwen MoE architectures (DeepSeekMoE, GShard) to validate cross-architecture effectiveness and assess performance consistency across different gating mechanisms

2. Evaluate HEAPr-pruned models on diverse downstream tasks including reasoning benchmarks, code generation, and multilingual tasks to assess broader applicability beyond perplexity metrics

3. Measure actual inference latency, memory usage, and throughput on GPUs/TPUs to validate claimed efficiency gains under real deployment conditions, including effects on KV cache and batch processing