---
ver: rpa2
title: Diagnosing and Mitigating Modality Interference in Multimodal Large Language
  Models
arxiv_id: '2505.19616'
source_url: https://arxiv.org/abs/2505.19616
tags:
- modality
- multimodal
- interference
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modality interference in MLLMs occurs when spurious signals from
  non-essential modalities distort model decisions, particularly in unimodal diagnostic
  settings. The core issue stems from MLLMs' inability to fairly evaluate and integrate
  information across modalities, leading to inappropriate reliance on irrelevant modality
  signals.
---

# Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2505.19616
- **Source URL:** https://arxiv.org/abs/2505.19616
- **Reference count:** 40
- **Primary result:** Unified finetuning framework combining heuristic and adversarial perturbations with consistency regularization achieves Pareto-optimal performance, boosting VQA accuracy by over 14% while maintaining near-clean performance under modality interference.

## Executive Summary
Modality interference in Multimodal Large Language Models (MLLMs) occurs when non-essential modality signals distort model decisions, particularly in unimodal diagnostic settings. This work proposes a unified finetuning framework that combines heuristic and adversarial perturbation-based data augmentation with output-level consistency regularization between original and perturbed inputs. The method achieves robust performance across multiple MLLM architectures and scales, consistently improving both unimodal robustness and multimodal generalization while maintaining clean accuracy.

## Method Summary
The method fine-tunes MLLMs using a combined loss that includes standard supervised training, adversarial robustness, and consistency regularization. It employs modality-specific perturbation masking to restrict adversarial noise to task-irrelevant modalities, preventing semantic destruction. The framework mixes original samples with heuristic perturbations (misleading text/irrelevant images) and adversarial variants, enforcing output consistency between clean and perturbed inputs. The approach is tested on LLaVA-1.5-7B with frozen vision encoder and trained projector, using a batch size of 8, learning rate of 2e-5, for one epoch.

## Key Results
- Achieves Pareto-optimal performance across multiple MLLM architectures and scales
- Boosts VQA accuracy by over 14% while maintaining near-clean performance under modality interference
- Improves unimodal robustness and multimodal generalization simultaneously (Caltech-101 accuracy: 98.6% â†’ 98.4% under misleading text)
- Outperforms baselines including chain-of-thought prompting and reasoning-enhanced models

## Why This Works (Mechanism)

### Mechanism 1: Targeted Perturbation as Causal Intervention
Injecting noise into task-irrelevant modalities forces the model to isolate correct causal pathways rather than relying on spurious correlations. The method uses Modality-Specific Perturbation Masking combined with Projected Gradient Descent (PGD) in the embedding space, restricting perturbations to irrelevant modalities via a binary mask $M$ while applying PGD constraints $\|\delta\|_\infty \leq \epsilon$.

### Mechanism 2: Output-level Consistency Regularization
Enforcing predictive invariance between clean and perturbed inputs stabilizes the decision boundary against spurious signals. The framework minimizes divergence (KL or JS) between output distributions $P(A|x)$ and $P(A|\tilde{x})$, pushing latent representations toward robustness to introduced perturbations.

### Mechanism 3: Unified Multi-Task Mix (Heuristic + Adversarial)
Combining heuristic noise (simulating realistic messiness) with adversarial noise (simulating worst-case attacks) yields better out-of-distribution generalization. The training batch mixes "Origin Samples" (clean), "Perturbation-Augmented Samples" (heuristic noise), and adversarial variants, covering known failure modes and unseen vulnerabilities.

## Foundational Learning

- **Concept: Causal Intervention in Deep Learning (do-calculus)**
  - Why needed: Frames modality interference as a failure to identify causal dependencies, explaining why targeted noise intervention is necessary
  - Quick check: Can you explain why intervening on text (changing content) differs from observing correlation in standard datasets?

- **Concept: Projected Gradient Descent (PGD) in Continuous Spaces**
  - Why needed: Extends adversarial attacks from pixel space to multimodal embedding space, essential for understanding perturbation generation
  - Quick check: If PGD runs for 100 steps but loss doesn't increase, what does this imply about model robustness or step size $\alpha$?

- **Concept: Divergence Measures (JS vs. KL)**
  - Why needed: Paper ablates between symmetric (JS) and asymmetric (KL) measures for consistency regularization
  - Quick check: If $P$ is clean distribution and $Q$ is perturbed, why might $KL(P||Q)$ penalize "missed" predictions differently than $KL(Q||P)$?

## Architecture Onboarding

**Component map:** Data Loader -> Perturbation Engine -> Masking Module -> Main Model (LLM) -> Loss Aggregator

**Critical path:** The computational bottleneck is the Adversarial Generation Loop, requiring $N$ extra forward passes per batch. Efficient memory sharing between frozen perturbation LLM and trainable main LLM is crucial for resource management.

**Design tradeoffs:**
- Overhead vs. Robustness: 1.66x FLOPs increase theoretical; marginal OOD robustness gains must justify 1.5x training slowdown
- Masking Granularity: Coarse masks (whole modalities) are easier but may miss token-level interference; fine-grained masks require task-specific knowledge

**Failure signatures:**
- Semantic Destruction: High $\epsilon$ renders input unintelligible, causing model to learn random/unknown predictions
- Consistency Collapse: High $\lambda_{cons}$ prioritizes identical (wrong) answers over learning correct ones

**First 3 experiments:**
1. Perturbation Ablation: Compare heuristic-only vs. adversarial-only performance on validation set to identify dominant signal
2. Masking Validation: Train "No Mask" variant vs. "Modality-Specific Mask" on VQA task; expect "No Mask" to degrade due to semantic corruption
3. Hyperparameter Sensitivity ($\epsilon$ and $N$): Plot performance vs. perturbation strength to identify "Goldilocks" zone maintaining clean accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on synthetic interference patterns rather than real-world noisy distributions
- Scalability to extremely large models (>30B parameters) remains unexplored
- Computational overhead claims lack empirical runtime measurements and scaling analysis

## Confidence
- **High Confidence:** Core experimental results showing consistent robustness gains across multiple architectures and datasets
- **Medium Confidence:** Generalization claims to out-of-distribution scenarios based on limited test cases
- **Low Confidence:** Scalability projections to much larger models and precise computational overhead in production environments

## Next Checks
1. Evaluate method on datasets with naturally occurring multimodal interference (social media posts, product reviews with conflicting text-image information)
2. Test method on models beyond 13B parameters (30B-70B range) to verify scalability and identify new interference patterns
3. Measure actual wall-clock training time and memory usage across different hardware configurations to validate theoretical FLOPs estimate