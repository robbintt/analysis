---
ver: rpa2
title: 'You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in
  Weak Base Models'
arxiv_id: '2511.04902'
source_url: https://arxiv.org/abs/2511.04902
tags:
- reasoning
- learning
- training
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical limitation in label-free reinforcement
  learning for reasoning: smaller models with weak reasoning capabilities often fail
  to improve and may even collapse, as they cannot generate sufficiently long or diverse
  chain-of-thought reasoning to enable effective self-reflection. To address this,
  the authors propose CuMa, a curriculum-guided masked majority voting reinforcement
  learning approach that progressively introduces harder problems, masks rewards for
  no-majority rollouts, and uses curated synthetic data of predefined difficulty levels.'
---

# You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models

## Quick Facts
- arXiv ID: 2511.04902
- Source URL: https://arxiv.org/abs/2511.04902
- Authors: Shuvendu Roy; Hossein Hajimirsadeghi; Mengyao Zhai; Golnoosh Samei
- Reference count: 25
- Primary result: Smaller models with weak reasoning capabilities fail to improve with label-free RL and may collapse

## Executive Summary
This paper identifies a critical limitation in label-free reinforcement learning for reasoning: smaller models with weak reasoning capabilities often fail to improve and may even collapse, as they cannot generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection. To address this, the authors propose CuMa, a curriculum-guided masked majority voting reinforcement learning approach that progressively introduces harder problems, masks rewards for no-majority rollouts, and uses curated synthetic data of predefined difficulty levels. This method consistently improves performance across all model sizes (0.5B to 7B parameters) and reasoning strengths, achieving up to 32.8% on Math 500 for the 0.5B model, compared to collapse or degradation with existing methods.

## Method Summary
The authors propose CuMa (Curriculum-guided Masked Majority voting reinforcement learning) as a solution to the collapse problem in label-free RL for weak base models. The approach combines three key components: curriculum learning that progressively introduces harder problems, reward masking that only provides feedback when majority voting achieves consensus, and curated synthetic data organized by difficulty levels. The method aims to stabilize training by ensuring that weak models receive meaningful feedback only when their reasoning chains are sufficiently developed to reach majority agreement, while gradually building up their reasoning capabilities through structured difficulty progression.

## Key Results
- Small models (0.5B parameters) achieved up to 32.8% accuracy on Math 500 using CuMa, compared to collapse with existing methods
- CuMa consistently improved performance across all model sizes from 0.5B to 7B parameters
- Ablation studies confirmed that curriculum learning, reward masking, and curated data are each essential components

## Why This Works (Mechanism)
Weak base models struggle with label-free RL because they cannot generate diverse or long enough chain-of-thought reasoning to enable effective self-reflection. Without sufficient reasoning capability, these models fail to produce the variety of solutions needed for majority voting to identify correct approaches. CuMa addresses this by using curriculum learning to build reasoning capabilities progressively, masking rewards to prevent misleading feedback when reasoning is insufficient, and providing curated data at appropriate difficulty levels. This creates a stable learning environment where weak models can gradually develop the reasoning capacity needed to benefit from self-reflection and majority voting mechanisms.

## Foundational Learning
- Chain-of-thought reasoning: Sequential reasoning steps that models generate to solve problems; needed to understand how models approach reasoning tasks and why weak models fail
- Reinforcement learning from scratch: RL without pre-training on supervised data; quick check: verify understanding of policy gradient methods
- Majority voting in RL: Using consensus among multiple reasoning paths to determine rewards; needed to grasp how CuMa filters quality reasoning
- Curriculum learning: Training strategy that starts with easier tasks and progresses to harder ones; quick check: understand how difficulty progression affects learning
- Reward masking: Withholding rewards when no consensus is reached; needed to understand how CuMa prevents misleading feedback
- Model collapse: Degradation of model performance during training; quick check: identify symptoms and causes of collapse

## Architecture Onboarding
- Component map: Synthetic data generation -> Curriculum scheduler -> RL policy training with majority voting -> Performance evaluation
- Critical path: Data curation → Curriculum progression → Reward masking → Policy update
- Design tradeoffs: Complexity of three-component approach vs. consistent performance improvement; balancing difficulty progression vs. maintaining stable learning
- Failure signatures: Performance collapse, degradation of reasoning quality, inability to generate diverse solutions
- First experiments: 1) Baseline comparison without curriculum learning, 2) Test reward masking with random data, 3) Evaluate curated data without majority voting

## Open Questions the Paper Calls Out
None

## Limitations
- The definition of "weak base model" remains somewhat subjective and may vary across domains
- The complexity of the three-component approach makes it difficult to definitively attribute gains to specific mechanisms
- The majority voting mechanism assumes that majority often provides correct reasoning, which may not hold for all problem types

## Confidence
- High confidence: Small models with weak reasoning capabilities experience performance collapse during label-free RL training
- Medium confidence: The three proposed components (curriculum learning, reward masking, curated data) each contribute positively to performance
- Medium confidence: CuMa consistently outperforms existing methods across different model sizes

## Next Checks
1. Test CuMa on reasoning tasks outside mathematics to verify generalizability of the approach
2. Conduct ablation studies with alternative reward functions to isolate whether performance gains stem from reasoning improvement versus optimization stability
3. Evaluate whether pre-training smaller models on additional reasoning-specific data could achieve similar improvements without the complexity of CuMa