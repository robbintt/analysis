---
ver: rpa2
title: 'SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate
  Your Speech-LLM Development'
arxiv_id: '2503.23848'
source_url: https://arxiv.org/abs/2503.23848
tags:
- speech
- dialogue
- generation
- arxiv
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SpeechDialogueFactory is a production-ready framework for generating
  high-quality speech dialogue datasets to accelerate Speech-LLM development. The
  system addresses limitations in existing approaches by implementing a comprehensive
  three-stage pipeline: metadata generation with scenario-based initialization, dialogue
  scripting as an intermediate representation, and single-pass dialogue simulation
  with paralinguistic annotations.'
---

# SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate Your Speech-LLM Development

## Quick Facts
- **arXiv ID**: 2503.23848
- **Source URL**: https://arxiv.org/abs/2503.23848
- **Reference count**: 7
- **Key outcome**: Production-ready framework for generating high-quality speech dialogue datasets with 30+ speaker diversity

## Executive Summary
SpeechDialogueFactory is a comprehensive framework designed to generate high-quality speech dialogue datasets for Speech-LLM development. The system addresses limitations in existing approaches by implementing a three-stage pipeline that generates metadata, creates dialogue scripts, and simulates conversations with paralinguistic annotations. By leveraging LLM-based generation, advanced TTS models, and automated quality assessment, the framework produces dialogue data comparable to professional human recordings while significantly reducing costs and expanding speaker diversity from 2 to 30+ speakers.

## Method Summary
SpeechDialogueFactory implements a three-stage pipeline for dialogue generation: metadata generation with scenario-based initialization, dialogue scripting as an intermediate representation, and single-pass dialogue simulation with paralinguistic annotations. The system employs profile-driven speaker retrieval from Common Voice combined with advanced TTS models (COSY VOICE 2, FISH-TTS) that leverage emotional and speech rate control. Quality assessment occurs at both content and speech levels using LLM-based evaluation. The framework is released as an open-source toolkit with sample English and Chinese datasets.

## Key Results
- Generated dialogues achieve quality scores comparable to professional human recordings: 96.9±2.3 consistency, 99.7±0.5 coherence, and 91.6±1.8 naturalness
- Speech quality MOS of 3.38±0.14 with 30+ speaker diversity versus 2 speakers in human datasets
- Significant cost reduction compared to human annotation while maintaining professional-level quality

## Why This Works (Mechanism)
SpeechDialogueFactory succeeds by addressing the dual challenges of data quality and diversity in Speech-LLM development. The three-stage pipeline ensures systematic generation of coherent dialogues with appropriate paralinguistic features, while the profile-driven speaker retrieval from Common Voice provides diverse voice profiles. The combination of metadata-driven initialization and advanced TTS models with emotional and speech rate control enables the generation of natural-sounding speech across multiple speakers. LLM-based quality assessment provides automated validation of both content and speech quality.

## Foundational Learning
- **LLM-based dialogue generation**: Essential for creating coherent, contextually appropriate dialogue content at scale; quick check: evaluate generated dialogues for logical consistency and relevance to scenarios
- **Profile-driven speaker retrieval**: Critical for ensuring speaker diversity and authenticity; quick check: verify speaker profiles match target demographics and speaking styles
- **Advanced TTS with emotional control**: Enables natural-sounding speech with appropriate emotional tone; quick check: assess speech naturalness across different emotional states
- **Automated quality assessment**: Provides scalable validation of generated data quality; quick check: compare LLM-based metrics with human evaluations
- **Three-stage pipeline architecture**: Ensures systematic generation from metadata through final speech output; quick check: trace data flow through all pipeline stages
- **Paralinguistic annotation**: Captures non-verbal communication elements for more natural dialogue; quick check: verify paralinguistic features align with dialogue context

## Architecture Onboarding

**Component Map**: Metadata Generator -> Dialogue Scriptor -> Dialogue Simulator -> TTS Engine -> Quality Assessor

**Critical Path**: Metadata Generation → Dialogue Scripting → Dialogue Simulation → Speech Synthesis → Quality Assessment

**Design Tradeoffs**: The framework prioritizes diversity and scalability over perfect naturalness, accepting minor quality compromises for significant gains in speaker variety and cost reduction. The use of LLM-based evaluation trades off some accuracy for speed and scalability compared to human evaluation.

**Failure Signatures**: Quality degradation may occur when LLM context is insufficient for complex scenarios, when speaker profiles lack appropriate emotional range, or when TTS models struggle with uncommon pronunciations. Automated assessments may overestimate quality due to evaluation bias.

**First 3 Experiments**:
1. Generate a small dataset with 5 speakers across 3 scenarios to validate the complete pipeline functionality
2. Compare LLM-based quality scores with human evaluations for a subset of generated dialogues
3. Test the framework's ability to generate dialogues in a new language beyond the provided English and Chinese samples

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-based evaluation may introduce bias favoring generated content over professional assessment
- Current implementation limited to English and Chinese, reducing cross-linguistic applicability
- Computational resource requirements for multi-stage generation may be substantial

## Confidence

**High confidence**: Speaker diversity improvements (30+ speakers vs. 2), technical implementation of the three-stage pipeline, and metadata-driven approach effectiveness

**Medium confidence**: Quality metrics comparisons with human data relying on automated evaluation rather than standardized linguistic benchmarks

**Medium confidence**: Speech synthesis quality improvements evaluated through single-speaker crowd-sourced assessments rather than diverse professional voice actors

## Next Checks

1. Conduct professional linguistic evaluation of generated dialogues using standardized protocols to validate LLM-based quality assessments
2. Test framework performance across additional languages beyond English and Chinese to assess cross-linguistic generalizability
3. Benchmark computational efficiency and resource requirements against actual professional human annotation workflows in real production environments