---
ver: rpa2
title: 'CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation
  Models'
arxiv_id: '2512.02180'
source_url: https://arxiv.org/abs/2512.02180
tags:
- lead
- learning
- risk
- ptb-xl
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEF, a clinically-guided contrastive learning
  method for pretraining foundation models on electrocardiogram (ECG) data. Unlike
  prior self-supervised approaches that rely on generic data augmentations, CLEF incorporates
  clinically validated risk scores derived from patient metadata to guide the contrastive
  learning process.
---

# CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models

## Quick Facts
- **arXiv ID:** 2512.02180
- **Source URL:** https://arxiv.org/abs/2512.02180
- **Reference count:** 40
- **Primary result:** Achieves 3.1% AUROC improvement over self-supervised baselines on 18 ECG tasks

## Executive Summary
CLEF introduces a clinically-guided contrastive learning approach for pretraining ECG foundation models. Unlike generic self-supervised methods, CLEF leverages clinically validated risk scores derived from patient metadata to guide contrastive learning. By adaptively weighting negative pairs based on risk score dissimilarities and handling missing metadata explicitly, CLEF learns embeddings that better capture clinically meaningful relationships between ECGs. The method achieves strong performance across diverse datasets while requiring only unlabeled ECGs and routinely collected metadata.

## Method Summary
CLEF pretrains a ResNeXt1D backbone on single-lead ECGs from MIMIC-IV-ECG using a combined weighted contrastive loss and dissimilarity alignment loss. The key innovation is incorporating SCORE2 cardiovascular risk scores to guide contrastive learning - negative pairs are weighted by their clinical dissimilarity, and an additional loss aligns embedding similarity with risk score dissimilarity. The method handles missing metadata through explicit imputation and weighting schemes. Pretraining uses random single-lead selection and physiological noise augmentation, then fine-tunes on downstream tasks across multiple datasets.

## Key Results
- Outperforms strong self-supervised baselines by 3.1% average AUROC on classification tasks
- Reduces MAE by 2.9% on regression tasks across 18 evaluations
- Matches supervised state-of-the-art performance while using only unlabeled data
- Successfully transfers to single-lead ECGs from wearables

## Why This Works (Mechanism)
CLEF works by aligning the contrastive learning objective with clinically meaningful relationships between ECGs. Standard contrastive learning treats all negative pairs equally, but CLEF weights them by clinical dissimilarity - ECGs from patients with similar cardiovascular risk are considered more similar, regardless of their raw signal similarity. The dissimilarity alignment loss ensures that embeddings preserve these clinical relationships. By incorporating missingness-aware weighting, CLEF avoids over-relying on noisy imputed metadata while still leveraging available clinical information.

## Foundational Learning

**Risk Score Computation**
- *Why needed:* Provides clinically validated anchor points for contrastive learning
- *Quick check:* Verify risk scores spread across [0,1] range, not clustered at extremes

**Missing Data Weighting**
- *Why needed:* Prevents model from over-relying on imputed metadata values
- *Quick check:* Pairs with 4/7 missing variables should receive ~0.56 weight via exp(-0.57)

**Contrastive Learning with Adaptive Weights**
- *Why needed:* Enables clinically meaningful negative sampling rather than random
- *Quick check:* Similar-risk ECGs should have smaller contrastive distances

## Architecture Onboarding

**Component Map**
ResNeXt1D Backbone -> Clinical Risk Score Module -> Weighted Contrastive Loss -> Dissimilarity Alignment Loss -> Final Embedding

**Critical Path**
MIMIC-IV-ECG Data -> Single-Lead Selection + Augmentation -> ResNeXt1D Forward Pass -> Risk Score Computation -> Weighted Loss Calculation -> Embedding Update

**Design Tradeoffs**
- Using clinical risk scores vs. pure self-supervised learning: gains clinical relevance but requires metadata
- Single-lead vs. multi-lead pretraining: enables wearable compatibility but loses some information
- Adaptive weighting vs. uniform weighting: better clinical alignment but more complex implementation

**Failure Signatures**
- Risk score scaling errors: embeddings cluster incorrectly, poor downstream performance
- Missingness weighting collapse: model overfits to imputed metadata patterns
- Temperature sensitivity: improper Ï„ causes gradient instability or poor separation

**First Experiments**
1. Verify SCORE2 risk score calculation produces expected distributions
2. Test weighted contrastive loss with synthetic metadata patterns
3. Compare CLEF embeddings to random initialization on simple downstream task

## Open Questions the Paper Calls Out

**Cross-Modal Transferability**
Can the clinically-guided contrastive learning strategy be effectively transferred to other physiological signals like PPG or respiratory signals? The study only evaluated on ECG data, though the authors suggest potential applicability to other modalities.

**Metadata Variable Importance**
Which specific metadata variables contribute most significantly to representation quality? The pretraining dataset lacked complete metadata, preventing determination of variable importance.

**Risk Score Generalizability**
How robust is the framework to different clinical risk scores, especially for non-European populations? The study used only SCORE2, developed on European cohorts, without testing alternative risk stratification tools.

## Limitations

- SCORE2 risk scores were developed on European populations, limiting generalizability to other demographics
- Implementation requires specific ResNeXt1D gating mechanism details not fully specified
- Missing metadata handling through imputation may introduce bias if not carefully implemented

## Confidence

- **High Confidence:** Core contrastive learning framework with clinical guidance is well-defined and reproducible
- **Medium Confidence:** Performance improvements are demonstrated but exact reproducibility depends on resolving implementation details
- **Low Confidence:** Real-world generalization to noisy wearable data and non-European populations requires further validation

## Next Checks

1. **Risk Score Calculation Validation:** Implement SCORE2 calculation and verify output distributions match expected clinical ranges using synthetic test cases
2. **Missingness Weighting Implementation:** Create controlled test cases with known metadata completeness to verify $M_{ik}$ weights correctly penalize missing data
3. **Downstream Task Transferability:** Fine-tune on a held-out arrhythmia dataset not mentioned in the paper to validate generalization beyond reported evaluation sets