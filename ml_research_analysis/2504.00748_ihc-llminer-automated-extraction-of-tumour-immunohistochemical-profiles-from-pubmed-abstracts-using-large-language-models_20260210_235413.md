---
ver: rpa2
title: 'IHC-LLMiner: Automated extraction of tumour immunohistochemical profiles from
  PubMed abstracts using large language models'
arxiv_id: '2504.00748'
source_url: https://arxiv.org/abs/2504.00748
tags:
- data
- abstracts
- extraction
- ihc-tumour
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'IHC-LLMiner is an automated pipeline for extracting tumour immunohistochemical
  profiles from PubMed abstracts using large language models (LLMs). The system performs
  two subtasks: classifying abstracts as relevant or irrelevant for IHC studies, and
  extracting structured IHC-tumour profiles (tumour type, tumour site, and marker
  positivity rates) from relevant abstracts.'
---

# IHC-LLMiner: Automated extraction of tumour immunohistochemical profiles from PubMed abstracts using large language models

## Quick Facts
- **arXiv ID:** 2504.00748
- **Source URL:** https://arxiv.org/abs/2504.00748
- **Reference count:** 40
- **Key outcome:** Automated pipeline extracting IHC-tumour profiles from PubMed abstracts with 91.5% classification accuracy and 63.3% correct extraction rate.

## Executive Summary
IHC-LLMiner is a two-stage automated pipeline for extracting tumour immunohistochemical profiles from PubMed abstracts using large language models. The system first classifies abstracts as relevant or irrelevant for IHC studies (91.5% accuracy), then extracts structured IHC-tumour profiles (tumour type, tumour site, and marker positivity rates) from relevant abstracts (63.3% correct outputs). The pipeline uses task-specific fine-tuning with LoRA to adapt open-source models, demonstrating that Gemma-2 outperforms GPT-4 in both accuracy and inference speed while being more cost-effective. Extracted profiles are normalized to UMLS concepts and validated against PathologyOutlines reference data, enabling automated knowledge base development for IHC research and clinical applications.

## Method Summary
The pipeline processes PubMed abstracts through two stages: classification and extraction. Classification uses fine-tuned Gemma-2 9B to identify relevant IHC studies with 91.5% accuracy. Extraction uses the same model with LoRA adaptation to generate markdown tables containing tumour type, site, and marker positivity rates. LoRA training (learning rate 1e-4, 3 epochs, 4× A100 GPUs) enables efficient domain adaptation with limited annotations. Extracted entities are normalized to UMLS concepts using fine-tuned SapBERT. The system processes 107,759 abstracts, identifying 30,481 relevant ones containing 50 marker-specific studies.

## Key Results
- Gemma-2 fine-tuned achieved 91.5% accuracy and 91.4 F1 score for abstract classification, outperforming GPT-4 by 9.5% accuracy with 5.9× faster inference
- IHC-tumour profile extraction reached 63.3% Correct outputs with the same fine-tuned model, significantly outperforming baseline LLMs (2.0-13.3% Correct)
- The pipeline extracted profiles from 30,481 relevant abstracts, filling gaps in existing IHC knowledge bases
- Extracted profiles showed strong concordance with PathologyOutlines reference data (22/50 markers had quantitative values; 18/50 qualitative labels matched)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-specific fine-tuning with LoRA enables domain adaptation while preserving pre-trained knowledge, yielding higher extraction accuracy than prompting alone.
- **Mechanism:** LoRA introduces small trainable rank-decomposition matrices while freezing base weights. This allows the model to learn IHC-specific patterns (e.g., "X/Y" notation for positivity rates, tumour-site-marker triples) without catastrophic forgetting or requiring large annotated datasets.
- **Core assumption:** The pre-trained LLM already encodes sufficient biomedical language understanding; only task-specific output formatting needs adaptation.
- **Evidence anchors:** Page 8-9: "LoRA adapts the model for task-specific learning by introducing small, trainable rank-decomposition matrices while keeping the pre-trained model weights frozen." Table 5: Gemma-2 improved from 2.0% to 63.3% Correct outputs after fine-tuning; Phi-3 improved from 13.3% to 43.9%. Limited corpus evidence on LoRA for biomedical extraction specifically; related work on transfer learning for drug relation extraction (DREaM) supports domain adaptation benefits.
- **Break condition:** If annotated data is too sparse or conceptually diverse, LoRA may underfit; expansion to new markers or tumour types without retraining may degrade performance.

### Mechanism 2
- **Claim:** A two-stage cascade (classification → extraction) reduces error propagation and computational cost compared to end-to-end extraction.
- **Mechanism:** Stage 1 filters irrelevant abstracts (reviews, meta-analyses, non-IHC studies) with 91.5% accuracy. Stage 2 operates only on 28% of the corpus (30,481/107,759), concentrating extraction capacity on actionable content and reducing false positives from irrelevant text.
- **Core assumption:** Classification errors are largely independent of extraction errors; precision at Stage 1 sufficiently limits downstream noise.
- **Evidence anchors:** "From an initial dataset of 107,759 abstracts...the classification task identified 30,481 relevant abstracts." Page 6: Classification criteria explicitly exclude review articles to prevent data duplication. Weak corpus evidence directly comparing cascade vs. end-to-end for biomedical extraction; general NLP literature supports cascaded approaches for precision-critical tasks.
- **Break condition:** If Stage 1 recall is too low, valuable IHC profiles are permanently lost; if Stage 1 precision is poor, extraction noise dominates.

### Mechanism 3
- **Claim:** Structured markdown output format combined with UMLS normalization converts unstructured text into queryable, interoperable knowledge.
- **Mechanism:** The model generates markdown tables with explicit columns (tumour type, tumour site, marker positivity). SapBERT then maps extracted terms to UMLS concepts via embedding-space nearest-neighbor matching, enabling cross-study aggregation and comparison to reference databases.
- **Core assumption:** SapBERT embeddings generalize to IHC-specific terminology; UMLS coverage is sufficient for tumour types and markers.
- **Evidence anchors:** Page 9: "During inference, we used Euclidean distance within the embedding space to align extracted terms with their closest UMLS concepts." Table 7: 22/50 markers had quantitative values comparable to PathologyOutlines; 18/50 qualitative labels matched. No direct corpus evidence on SapBERT for IHC normalization; general entity alignment work supports embedding-based normalization.
- **Break condition:** If extracted terms are ambiguous or misspelled, SapBERT may map to incorrect UMLS concepts; rare tumour types or novel markers may lack UMLS entries.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** Understanding how a 9B-parameter model can be efficiently adapted with limited annotations (~364 training examples) without full fine-tuning costs.
  - **Quick check question:** Can you explain why freezing base weights and training only low-rank matrices might prevent overfitting on small datasets?

- **Concept: UMLS (Unified Medical Language System)**
  - **Why needed here:** The pipeline normalizes extracted entities to UMLS concepts to enable aggregation and comparison across studies.
  - **Quick check question:** What would happen if an extracted tumour type had no corresponding UMLS entry?

- **Concept: Generative vs. Discriminative Extraction**
  - **Why needed here:** This paper uses LLMs to generate structured tables, unlike BERT-based NER+relation extraction pipelines.
  - **Quick check question:** Why might a generative approach reduce annotation burden compared to training separate NER and relation extraction models?

## Architecture Onboarding

- **Component map:** PubMed Abstract Collection (Entrez e-utils) → Abstract Classification (Gemma-2 fine-tuned, binary) → IHC Profile Extraction (Gemma-2 + LoRA) → UMLS Normalization (SapBERT fine-tuned on UMLS 2024AB)

- **Critical path:** Classification accuracy → Extraction recall/precision → UMLS mapping quality. Errors compound downstream; a false negative at Stage 1 loses data permanently.

- **Design tradeoffs:**
  - Open-source Gemma-2 (9B) vs. GPT-4: Gemma-2 is 5.9× faster and cheaper at scale but requires fine-tuning infrastructure (4× A100 GPUs for LoRA training).
  - LoRA vs. full fine-tuning: LoRA is parameter-efficient but may underfit on complex tasks compared to full adaptation.
  - Markdown tables vs. JSON: Markdown is human-readable and aligns with LLM training data, but requires post-processing for programmatic use.

- **Failure signatures:**
  - **Low classification recall:** Many relevant abstracts marked "Exclude"; check if training data over-represents exclusion criteria.
  - **"Partially Correct" extraction:** Model omits stromal vs. epithelial distinctions or misreports percentages as counts; may need prompt refinement or more diverse training examples.
  - **UMLS mismatches:** Normalized concepts cluster incorrectly; check SapBERT embedding quality for rare terms.

- **First 3 experiments:**
  1. **Ablation on annotation size:** Train classification and extraction models with 100, 200, 400, 800 annotated abstracts to characterize data-efficiency curves.
  2. **Error analysis on "Partially Correct" outputs:** Manually categorize omission types (missing columns, wrong format, incomplete rows) to prioritize prompt or training adjustments.
  3. **Cross-dataset validation:** Apply the pipeline to a held-out marker set (e.g., 10 markers not in the original 50) to assess generalization to unseen IHC terminology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the volume of annotated training data beyond the current 364 samples significantly improve the generalizability and accuracy of the fine-tuned Gemma-2 model for IHC-tumour profile extraction?
- Basis in paper: [explicit] The Conclusion states that a "key limitation... is the reliance on a relatively small annotated dataset" and that "Expanding the number of annotated samples could further enhance the model's ability."
- Why unresolved: The study utilized a limited training set (364 abstracts for extraction) due to the resource-intensive nature of manual annotation by pathologists.
- What evidence would resolve it: A comparative study evaluating model performance (percentage of "Correct" outputs) as the training dataset size scales from hundreds to thousands of abstracts.

### Open Question 2
- Question: Can integrating generative LLMs into the normalisation step improve the accuracy of mapping extracted terms to UMLS concepts compared to the current SapBERT approach?
- Basis in paper: [explicit] The Conclusion suggests that "the normalisation step could benefit from integrating advanced LLMs, similar to the classification task, to further improve performance."
- Why unresolved: The pipeline currently relies on SapBERT for UMLS alignment, which relies on embeddings rather than the contextual generative capabilities used in the extraction phase.
- What evidence would resolve it: A benchmark comparison of entity linking accuracy between the current SapBERT implementation and an LLM-based normalisation method using the same dataset.

### Open Question 3
- Question: Can the IHC-LLMiner pipeline be effectively adapted to extract immunohistochemical profiles from other biomedical text sources, such as full-text articles or unstructured electronic health records (EHRs)?
- Basis in paper: [explicit] The Conclusion notes that "Future work can... [extend] the framework to additional biomedical datasets, further enhancing its applicability and impact."
- Why unresolved: The current study focused exclusively on PubMed abstracts; the system's performance on the longer, more complex narratives found in full texts or clinical notes remains untested.
- What evidence would resolve it: Evaluation of the pipeline's classification and extraction performance when applied to a corpus of full-text pathology reports or clinical EHR data.

## Limitations

- **Data annotation constraints:** The extraction model was trained on only 364 examples, raising concerns about robustness to diverse IHC reporting styles and new tumour types or markers.
- **Performance variability:** While classification achieves 91.5% accuracy, extraction correctness drops to 63.3%, with 34.7% "Partially Correct" outputs that may contain incomplete information.
- **UMLS normalization assumptions:** The system assumes SapBERT embeddings effectively map IHC-specific terminology to UMLS concepts, but rare tumour types or novel markers without UMLS entries would fail normalization.

## Confidence

- **High confidence (Level 3):** Task-specific fine-tuning with LoRA enables domain adaptation while preserving pre-trained knowledge, supported by clear quantitative improvements (Gemma-2: 2.0% → 63.3% Correct outputs; Phi-3: 13.3% → 43.9% Correct outputs) and established transfer learning literature.
- **Medium confidence (Level 2):** The two-stage cascade approach is sound with high classification accuracy (91.5%), but lacks direct comparison to end-to-end methods in this domain and assumes Stage 1 errors are independent of Stage 2 errors.
- **Medium confidence (Level 2):** The markdown + UMLS normalization approach is valid and supported by related work on entity alignment, but performance depends heavily on SapBERT's ability to handle IHC-specific terminology without direct validation.

## Next Checks

1. **Cross-dataset generalization test:** Apply the pipeline to a held-out marker set (e.g., 10 markers not in the original 50) to assess whether performance degrades on unseen IHC terminology and tumor types.

2. **Annotation efficiency characterization:** Systematically train classification and extraction models with varying annotation sizes (100, 200, 400, 800 examples) to quantify the data-efficiency curve and determine minimum viable annotation requirements.

3. **Error mode decomposition:** Manually categorize "Partially Correct" extraction outputs by error type (missing columns, wrong format, incomplete rows) to identify whether prompt refinement or additional training data would most improve performance.