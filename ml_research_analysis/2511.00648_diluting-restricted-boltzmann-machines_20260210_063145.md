---
ver: rpa2
title: Diluting Restricted Boltzmann Machines
arxiv_id: '2511.00648'
source_url: https://arxiv.org/abs/2511.00648
tags:
- pruning
- training
- trained
- networks
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether sparse neural networks can maintain
  strong generative performance by studying Restricted Boltzmann Machines (RBMs) under
  extreme pruning conditions. The authors trained RBMs on MNIST, then applied progressive
  pruning to both randomly initialized and fully trained networks, examining the impact
  on generation quality using multiple metrics.
---

# Diluting Restricted Boltzmann Machines

## Quick Facts
- arXiv ID: 2511.00648
- Source URL: https://arxiv.org/abs/2511.00648
- Reference count: 31
- Primary result: RBMs maintain generative quality with up to 80% pruning at initialization, but retraining fails to recover performance after post-training pruning

## Executive Summary
This paper investigates whether sparse neural networks can maintain strong generative performance by studying Restricted Boltzmann Machines (RBMs) under extreme pruning conditions. The authors trained RBMs on MNIST, then applied progressive pruning to both randomly initialized and fully trained networks, examining the impact on generation quality using multiple metrics. They found that RBMs can achieve high-quality generation even when up to 80% of connections are pruned before training, supporting the Lottery Ticket Hypothesis. However, performance degrades abruptly when pruning exceeds a critical threshold, suggesting a minimal core of essential connections. Crucially, networks cannot recover lost performance through retraining after additional pruning - retrained models consistently underperform reference models trained from scratch at equivalent sparsity levels. This demonstrates that initial training conditions create path dependencies that determine ultimate capabilities. The findings suggest that for sparse networks to work effectively, pruning should be implemented early in training rather than attempted afterward, and that the performance of a network is highly sensitive to its initial conditions.

## Method Summary
The study uses single-layer Restricted Boltzmann Machines with 784 visible units (28×28 MNIST pixels) and 400 hidden units, totaling 313,600 weights. Training employs Persistent Contrastive Divergence (PCD) with 1,000 Gibbs steps per iteration for 10,000 iterations, monitoring pseudo-likelihood for convergence. The experiments apply two types of pruning: initial pruning p0 (randomly setting a fraction of weights to zero before training) and additional pruning p (removing smallest-magnitude weights post-training). Three evaluation metrics assess generative quality: Second Moment Error (MSE of covariance matrices), Adversarial Accuracy Indicator Error (nearest-neighbor based), and auxiliary classifier confidence score Q plus digit diversity df. The study runs 15 independent replicas per condition to ensure statistical robustness.

## Key Results
- RBMs achieve high-quality generative performance even with up to 80% of connections pruned before training, confirming the existence of viable sub-networks
- A sharp phase transition occurs when pruning disrupts a minimal core of essential connections, with performance degrading abruptly rather than gradually
- Retrained networks remain constrained by parameters originally learned, performing worse than networks trained from scratch at equivalent sparsity levels
- The critical threshold follows a scaling law p* = p - 0.5·p0, where α = 0.5 balances initial and additional pruning effects

## Why This Works (Mechanism)

### Mechanism 1: Sub-network Sufficiency Under Initial Sparsity
Random initialization in over-parameterized networks creates a high probability that at least one sufficiently expressive sub-network exists. During training, gradients only update the remaining non-zero weights, causing the network to adapt its representation to the available connectivity. The remaining sub-network must have sufficient expressivity to capture the data distribution's relevant structure.

### Mechanism 2: Critical Threshold with Sharp Phase Transition
Information required for generation is distributed across a relatively small set of crucial weights. Below the threshold, redundancy compensates; above it, the network loses fundamental representational capacity. The critical threshold corresponds to disrupting a minimal spectral support (approximately 62 top eigenvalues observed in experiments).

### Mechanism 3: Path Dependency Constrains Retraining Recovery
Retraining starts from weights that were optimized for a different connectivity pattern. The pruning mask removes low-magnitude weights systematically, but the remaining weight values carry an "imprint" of the original training that retraining cannot erase. The loss landscape around the initially learned solution contains local minima that trap retraining.

## Foundational Learning

- Concept: Restricted Boltzmann Machines and Energy-Based Models
  - Why needed here: The entire paper operates within the RBM framework—bipartite graphs with visible/hidden units, Boltzmann distributions, and likelihood gradient training via Contrastive Divergence.
  - Quick check question: Can you explain why RBM gradients require approximating model expectations through sampling?

- Concept: Weight Pruning and Sparsification
  - Why needed here: The paper distinguishes between initial pruning (p0), additional post-training pruning (p), and their different effects on generative performance.
  - Quick check question: What is the difference between pruning at initialization versus pruning after training, and why might they have different outcomes?

- Concept: Principal Component Analysis and Spectral Methods
  - Why needed here: The paper uses PCA on both raw weights and generalized Ising model couplings to understand how different training histories create distinct parameter space regions.
  - Quick check question: Why might PCA of effective couplings reveal structure that PCA of raw weights misses?

## Architecture Onboarding

- Component map:
  Input Layer (784 visible units) -> Hidden Layer (400 hidden units) -> Weight Matrix (313,600 connections) -> Pruning Mask (binary mask) -> Training (PCD with 1,000 Gibbs steps) -> Evaluation (three metrics)

- Critical path:
  1. Initialize RBM weights randomly
  2. Apply initial pruning mask (p0 fraction set to zero permanently)
  3. Train with PCD, monitoring pseudo-likelihood
  4. Evaluate generative quality using all three metrics
  5. If studying degradation: apply additional pruning (p) to smallest remaining weights
  6. If studying retraining: continue training after additional pruning

- Design tradeoffs:
  - **Initial sparsity vs. performance**: Higher p0 slightly reduces quality but enables aggressive final sparsity (80% removal still viable)
  - **Retraining vs. training from scratch**: Retraining is computationally cheaper but yields worse models; fresh training at target sparsity performs better
  - **Metric choice**: Auxiliary classifier is fast and interpretable; EAAI and Second Moment Error are more theoretically grounded but costlier

- Failure signatures:
  - **Gradual quality degradation**: Normal when approaching threshold
  - **Sudden quality collapse to near-zero**: Critical threshold exceeded; core connections disrupted
  - **Retrained models underperforming reference models**: Path dependency effect; initial conditions cannot be overcome
  - **Loss of digit diversity (high df)**: Network generating limited digit classes, indicating mode collapse

- First 3 experiments:
  1. **Baseline sweep**: Train 15 replicas each at p0 ∈ {0, 0.2, 0.4, 0.6, 0.8}; evaluate with all three metrics to establish the quality floor under initial sparsity.
  2. **Threshold detection**: Starting from trained models at each p0, apply incremental additional pruning p and plot quality curves. Identify where curves collapse to establish the scaling law p* = p - 0.5·p0.
  3. **Retraining comparison**: For a target total sparsity (e.g., 85%), compare (a) train from scratch with p0=0.85 versus (b) train with p0=0.6, prune to 0.85, retrain. Use GIM coupling PCA to verify parameter space separation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed pruning resilience and retraining limitations generalize to deeper architectures, such as Deep Boltzmann Machines or convolutional networks?
- Basis in paper: The conclusion states, "Future research would explore whether these observations hold for more complex architectures or if alternative training strategies might overcome these constraints."
- Why unresolved: The study exclusively restricted its methodology to single-layer Restricted Boltzmann Machines.
- What evidence would resolve it: Replication of the scaling laws and phase transitions in deep networks or modern discriminative architectures.

### Open Question 2
- Question: Can alternative training strategies or regularization techniques overcome the "path dependency" that constrains retrained networks?
- Basis in paper: The conclusion explicitly asks if "alternative training strategies might overcome these constraints" preventing retrained models from matching reference performance.
- Why unresolved: Standard retraining failed to erase the weight configuration imprint inherited from early training.
- What evidence would resolve it: Identification of a training protocol that allows retrained networks to escape their initial basin and match reference model performance.

### Open Question 3
- Question: Is the empirically observed scaling factor (α=0.5) universal, or does it depend on dataset complexity and network size?
- Basis in paper: The authors identify a scaling law p* = p - 0.5 p0 that collapses performance curves, but they do not provide a theoretical derivation for why this specific value occurs.
- Why unresolved: The value was determined by fitting experimental curves on MNIST rather than through theoretical derivation.
- What evidence would resolve it: A theoretical framework predicting α or empirical analysis showing α varies with data dimensionality.

## Limitations
- The study's findings are based solely on the MNIST dataset, which may not generalize to more complex data distributions
- The 80% pruning threshold for initial sparsity is dataset-specific and may differ for other tasks
- The path dependency mechanism lacks direct mechanistic evidence beyond parameter space visualization

## Confidence

- High confidence: Sub-network sufficiency under initial sparsity (strong empirical support with consistent metrics)
- Medium confidence: Critical threshold with sharp phase transition (empirical evidence present but theoretical explanation limited)
- Medium confidence: Path dependency constraining retraining recovery (parameter space analysis supports claim, but alternative explanations possible)

## Next Checks

1. Test the initial sparsity threshold (80%) on alternative datasets (Fashion-MNIST, CIFAR-10) to verify generalizability of the sub-network sufficiency claim
2. Conduct ablation studies removing single critical weights to test whether the sharp threshold represents true information bottlenecks versus distributed redundancy
3. Implement a controlled retraining experiment where weights are reinitialized (rather than pruned) after initial training to isolate the effect of weight values versus connectivity patterns on path dependency