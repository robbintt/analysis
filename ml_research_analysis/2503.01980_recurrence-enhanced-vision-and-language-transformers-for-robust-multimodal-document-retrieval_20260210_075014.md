---
ver: rpa2
title: Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal
  Document Retrieval
arxiv_id: '2503.01980'
source_url: https://arxiv.org/abs/2503.01980
tags:
- layer
- input
- visual
- clip
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal document retrieval where both queries
  and documents consist of paired images and text. The proposed method, ReT, employs
  a novel Transformer-based recurrent cell that integrates features from multiple
  layers of visual and textual backbones, using sigmoidal gates inspired by LSTMs
  to control information flow across layers and modalities.
---

# Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval

## Quick Facts
- **arXiv ID:** 2503.01980
- **Source URL:** https://arxiv.org/abs/2503.01980
- **Reference count:** 40
- **Primary result:** ReT achieves significant improvements in R@5 and PR@5 on M2KR and M-BEIR multimodal document retrieval benchmarks.

## Executive Summary
This paper introduces ReT, a recurrent Transformer cell designed to enhance multimodal document retrieval by integrating features from multiple layers of visual and textual backbones. The method employs sigmoidal gates inspired by LSTMs to control information flow across layers and modalities, enabling dynamic feature selection. ReT outperforms state-of-the-art approaches on the M2KR benchmark and demonstrates superior performance on the M-BEIR benchmark. The method is further validated in a retrieval-augmented generation pipeline for knowledge-based visual question answering, where it consistently retrieves more accurate and contextually relevant documents.

## Method Summary
ReT processes multimodal queries and documents consisting of paired images and text. It employs a novel Transformer-based recurrent cell that integrates features from multiple layers of visual and textual backbones. The recurrent cell uses sigmoidal gates inspired by LSTMs to control information flow across layers and modalities. The model is trained with symmetric InfoNCE loss and produces multiple output tokens for fine-grained late interaction scoring. During inference, document embeddings are indexed using PLAID for efficient retrieval.

## Key Results
- ReT achieves significant improvements in R@5 and PR@5 metrics on the M2KR benchmark compared to state-of-the-art approaches
- The method demonstrates superior performance on the M-BEIR benchmark
- In a retrieval-augmented generation pipeline for knowledge-based visual question answering, ReT consistently retrieves more accurate and contextually relevant documents

## Why This Works (Mechanism)

### Mechanism 1: Multi-layer Feature Integration
Multi-layer feature integration improves retrieval of fine-grained visual details by preserving low-level information often lost in deeper network layers. The recurrent cell processes representations sequentially from shallow to deep layers, with shallow layers retaining low-level visual clues (e.g., jersey numbers, textures) that may be abstracted away in deeper layers. The forget gate selectively retains information from these earlier layers when needed for specific queries.

### Mechanism 2: Sigmoidal Gates for Dynamic Feature Selection
Sigmoidal gates inspired by LSTMs provide learnable, per-modality and per-layer control over information flow. At each layer, a forget gate controls retention of the candidate state from previous layers, while separate visual and textual input gates modulate how much new information enters from each modality's backbone at that layer. This allows the model to dynamically emphasize visual vs. textual features depending on query content.

### Mechanism 3: Fine-grained Late Interaction
Fine-grained late interaction via multiple output tokens provides more robust relevance scoring than single-vector representations. ReT produces k latent tokens for each query and document, and the relevance score is computed as the sum of maximum similarities between all query-document token pairs. This captures fine-grained local similarities rather than relying on single global similarity scores.

## Foundational Learning

- **Concept: Transformer Architectures & Multi-Head Self-Attention**
  - Why needed here: ReT is explicitly a "Transformer-based recurrent cell" using self-attention and cross-attention as core operations. Understanding how attention computes context-dependent weighted sums is essential to grasp how the model fuses information from hidden states and backbone features.
  - Quick check question: How does the self-attention operation in the recurrent cell use the hidden state h_l to compute the candidate state c_l?

- **Concept: Recurrent Neural Networks & LSTM Gating**
  - Why needed here: The paper explicitly draws inspiration from LSTM gating for its forget and input gates. Understanding the purpose of these gates (controlling information flow and retention over sequence steps, here corresponding to layers) is crucial for understanding ReT's core control mechanism.
  - Quick check question: In ReT, the "sequence" for the recurrent cell is the set of backbone layers. How does the forget gate's function differ when applied to layers compared to its traditional application to timesteps in an LSTM?

- **Concept: Contrastive Learning for Retrieval**
  - Why needed here: ReT is trained with symmetric InfoNCE loss, the standard training paradigm for modern neural retrieval. Understanding how this loss shapes embedding space to bring relevant pairs closer and push irrelevant pairs apart is fundamental to understanding how the model learns retrieval capability.
  - Quick check question: What key difference in ReT's encoder setup does the symmetric InfoNCE loss help optimize compared to using a single shared encoder for both queries and documents?

## Architecture Onboarding

- **Component map:**
  - Inputs: Multimodal Query/Document → CLIP Visual/Text Encoders (frozen or partially adapted via LoRA)
  - Core Processor: ReT Recurrent Cell (separate instances for query and document with distinct weights)
  - Iterates through L selected backbone layers
  - Each step: LayerNorm → Self-Attention → Parallel Cross-Attentions (Visual & Text)
  - Gating Module: Computes forget/input gates → Gated combination of candidate state and modality features
  - Output: Set of k latent tokens from final recurrent iteration, linearly projected to 128-dim
  - Similarity Scoring: Fine-grained late interaction (ColBERT-style MaxSim) between query and document token sets
  - Training: Symmetric InfoNCE loss with Adam optimizer

- **Critical path:**
  1. Feature Extraction: Extract intermediate representations from multiple layers of pre-trained visual and textual backbones
  2. Recurrent Fusion: For each selected layer, feed features into recurrent cell. Hidden state updates via self-attention and gated fusion with new layer's visual/textual features.
  3. Token Projection: Take final hidden state from recurrent cell and project to output token space
  4. Scoring & Loss: Compute fine-grained MaxSim score between query and document token sets, apply contrastive loss

- **Design tradeoffs:**
  - Recurrence scope: Ablation shows applying recurrence only on last 4 layers significantly outperforms first 4 layers
  - Output token count (k=32): Determines representation granularity and late-interaction scoring cost
  - Layer selection strategy: For mismatched backbone depths, uniformly sample equal layer counts

- **Failure signatures:**
  - Gate saturation: If qualitative analysis shows gates stuck near 0 or 1, the model isn't learning dynamic selection
  - Performance improvement without recurrence: If removing recurrence improves performance, multi-layer integration is adding noise
  - Modality underutilization: If visual input gates consistently near zero, model ignores visual information

- **First 3 experiments:**
  1. Reproduce single-cell ablation: Implement recurrent cell but configure to run only once on final layer outputs
  2. Visualize gate activations: During training, log average forget/visual input/textual input gate values across layers
  3. Vary output token count: Train models with k ∈ {16, 32, 64} on validation set to establish efficiency-performance tradeoff

## Open Questions the Paper Calls Out

- **Question:** Does the shared presence of specific document images across different training datasets introduce data leakage that biases the model against utilizing visual features in smaller datasets?
- **Question:** Is the multi-layer recurrent mechanism strictly necessary for encoding multimodal documents, or is a simpler cross-attention on the final layer sufficient?
- **Question:** Is the strategy of uniformly sampling or manually selecting layers for the recurrent cell optimal when processing vision-and-language backbones with mismatched depths?

## Limitations

- The requirement to extract and cache multi-layer backbone features creates substantial memory overhead during training
- The layer selection strategy for handling depth mismatches between visual and textual backbones is empirically chosen without theoretical justification
- Fine-grained late-interaction scoring scales quadratically with output token count, creating computational bottlenecks for large-scale deployments

## Confidence

- **High confidence:** The architectural design choices (LSTM-style gating, multi-layer feature integration, fine-grained late interaction) are well-justified by existing literature and ablation studies
- **Medium confidence:** The generalizability claims to other multimodal retrieval tasks and the effectiveness of the VQA-augmented retrieval pipeline need further validation on additional benchmarks
- **Low confidence:** The scalability analysis for very large document collections is limited, and the computational complexity implications for production deployment are not fully explored

## Next Checks

1. **Cross-domain robustness testing:** Evaluate ReT performance on document retrieval tasks from domains not represented in M2KR (e.g., medical literature with figures, scientific papers with equations and diagrams)

2. **Memory-efficient training validation:** Implement and benchmark alternative memory optimization strategies to determine if the DisCo configuration is essential

3. **Layer selection sensitivity analysis:** Systematically vary the layer sampling strategy for visual and textual backbones to identify whether the empirically chosen layers are optimal