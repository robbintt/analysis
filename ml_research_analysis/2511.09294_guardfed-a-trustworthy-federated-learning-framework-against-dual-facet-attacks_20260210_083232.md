---
ver: rpa2
title: 'GuardFed: A Trustworthy Federated Learning Framework Against Dual-Facet Attacks'
arxiv_id: '2511.09294'
source_url: https://arxiv.org/abs/2511.09294
tags:
- fairness
- learning
- guardfed
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of federated learning systems
  being vulnerable to attacks that compromise both model utility and fairness. The
  authors propose a novel threat model called Dual-Facet Attack (DFA) that simultaneously
  degrades predictive accuracy and group fairness through sensitive attribute manipulation
  and model perturbation.
---

# GuardFed: A Trustworthy Federated Learning Framework Against Dual-Facet Attacks

## Quick Facts
- arXiv ID: 2511.09294
- Source URL: https://arxiv.org/abs/2511.09294
- Reference count: 40
- Primary result: GuardFed achieves testing accuracies above 65% on COMPAS and 82% on ADULT datasets while maintaining low fairness metrics (ASPD and AEOD generally below 0.1) even under severe adversarial attacks including DFA

## Executive Summary
This paper addresses the critical vulnerability of federated learning systems to attacks that simultaneously compromise both model utility and fairness. The authors introduce a novel threat model called Dual-Facet Attack (DFA) that manipulates sensitive attributes and model parameters to degrade both predictive accuracy and group fairness. To counter this threat and similar attacks, they propose GuardFed, a self-adaptive defense framework that maintains a fairness-aware reference model and computes dual-perspective trust scores for clients based on utility deviation and fairness degradation, enabling selective aggregation of trustworthy updates.

## Method Summary
GuardFed operates by maintaining a reference model at the server using a small amount of clean data augmented with synthetic samples. During each communication round, clients submit their local model updates, which are evaluated against the reference model using dual-perspective trust scoring. This scoring mechanism assesses both the deviation in utility (accuracy loss) and fairness degradation by testing the updates on the reference dataset with known sensitive attributes. Clients whose updates maintain high trust scores across both dimensions have their contributions aggregated into the global model, while those showing significant degradation in either aspect are excluded. The framework is self-adaptive, continuously updating the reference model and trust thresholds based on observed performance across multiple rounds.

## Key Results
- Achieves testing accuracies above 65% on COMPAS and 82% on ADULT datasets
- Maintains fairness metrics (ASPD and AEOD) generally below 0.1 under various attack scenarios
- Outperforms existing robust FL methods across diverse non-IID and adversarial conditions
- Successfully defends against the proposed Dual-Facet Attack (DFA) and other common FL attacks

## Why This Works (Mechanism)
GuardFed's effectiveness stems from its dual-perspective trust scoring mechanism that simultaneously evaluates model updates for both utility preservation and fairness maintenance. By maintaining a reference model with known fairness properties, the framework can detect when client updates introduce bias or degrade accuracy. The selective aggregation process ensures that only updates meeting both criteria contribute to the global model, preventing the accumulation of biased or ineffective updates that could compromise the overall system.

## Foundational Learning

1. **Dual-Facet Attack (DFA)**
   - Why needed: To model realistic scenarios where adversaries target both accuracy and fairness simultaneously
   - Quick check: Verify that attacks can indeed degrade both metrics concurrently on test datasets

2. **Fairness Metrics (ASPD, AEOD)**
   - Why needed: To quantify group fairness in model predictions across sensitive attributes
   - Quick check: Ensure metrics capture disparate impact and equalized odds violations

3. **Reference Model Maintenance**
   - Why needed: To provide a consistent benchmark for evaluating client updates
   - Quick check: Validate that reference model remains representative of global data distribution

4. **Trust Scoring Mechanism**
   - Why needed: To objectively assess client updates for both utility and fairness
   - Quick check: Confirm scoring correlates with actual model performance degradation

## Architecture Onboarding

**Component Map:** Server -> Reference Model -> Trust Scoring -> Client Selection -> Global Model

**Critical Path:** The core workflow involves (1) server receiving client updates, (2) evaluating updates against reference model using dual-perspective trust scoring, (3) selecting trustworthy updates based on scoring thresholds, and (4) aggregating selected updates into global model.

**Design Tradeoffs:** The framework balances between model accuracy and fairness preservation by selectively aggregating updates. The 1% server data retention represents a tradeoff between having sufficient reference data for evaluation and maintaining client privacy. The dual-perspective scoring adds computational overhead but provides comprehensive protection against multifaceted attacks.

**Failure Signatures:** Performance degradation occurs when: (1) reference model becomes unrepresentative of global distribution, (2) trust scoring thresholds are too permissive or restrictive, (3) client attacks successfully evade detection by simultaneously manipulating utility and fairness metrics, or (4) server-side data is insufficient to capture data distribution complexity.

**First Experiments:**
1. Test GuardFed with varying reference data sizes (0.5%, 1%, 5%) to assess sensitivity
2. Evaluate performance on multi-class classification datasets to test generalizability
3. Implement adaptive adversaries targeting the trust scoring mechanism

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- Effectiveness depends heavily on the quality and representativeness of the small server-side reference dataset (1% of data)
- Assumes server trust for maintaining reference model and computing trust scores, not addressing server compromise scenarios
- Evaluation limited to binary classification tasks; generalizability to multi-class or regression tasks unexplored
- Does not investigate computational overhead at scale or sensitivity to different reference data distributions

## Confidence

- **High confidence** in overall framework design and experimental methodology
- **Medium confidence** in generalizability to different dataset sizes and distributions
- **Medium confidence** in robustness against sophisticated attack strategies beyond those currently considered

## Next Checks

1. Conduct experiments with varying sizes of reference data (0.5%, 2%, 5%) to assess sensitivity of GuardFed's performance to the amount of server-side data.
2. Evaluate GuardFed's performance on multi-class classification datasets (e.g., CIFAR-10, MNIST) to test its applicability beyond binary classification.
3. Implement and test GuardFed against adaptive adversaries that specifically target the trust scoring mechanism or attempt to poison the reference model.