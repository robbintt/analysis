---
ver: rpa2
title: Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR
arxiv_id: '2509.02522'
source_url: https://arxiv.org/abs/2509.02522
tags:
- pacs
- grpo
- performance
- policy
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse reward signals in
  reinforcement learning with verifiable rewards (RLVR) for mathematical reasoning
  tasks. The authors propose PACS, a novel framework that reformulates RLVR as a supervised
  learning problem by treating outcome rewards as labels and training a score function
  via cross-entropy loss.
---

# Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR

## Quick Facts
- **arXiv ID**: 2509.02522
- **Source URL**: https://arxiv.org/abs/2509.02522
- **Reference count**: 14
- **Primary result**: PACS achieves 59.78% pass@256 on AIME 2025, outperforming PPO by 13.32 and GRPO by 14.36 percentage points

## Executive Summary
This paper introduces PACS, a novel framework that reformulates reinforcement learning with verifiable rewards (RLVR) as a supervised learning problem. By treating outcome rewards as labels and training a score function via cross-entropy loss, PACS achieves implicit actor-critic coupling through a unified model update. The framework demonstrates state-of-the-art performance on mathematical reasoning benchmarks, achieving 59.78% pass@256 on AIME 2025 while maintaining healthy policy entropy for exploration.

## Method Summary
PACS addresses the sparse reward challenge in RLVR by transforming the problem into supervised learning. The method trains a single model to predict reward outcomes using cross-entropy loss on binary labels (1 for success, 0 for failure). This approach implicitly couples actor and critic updates within a single optimization step, eliminating the need for separate value networks or complex reward modeling. The framework uses a log-probability scaling factor β to control the influence of successful trajectories, with β=1 empirically optimal for mathematical reasoning tasks.

## Key Results
- Achieves 59.78% pass@256 on AIME 2025, improving by 13.32 and 14.36 points over PPO and GRPO
- Maintains healthy policy entropy (average ~1.5) for effective exploration
- Demonstrates consistent improvements across multiple mathematical reasoning benchmarks

## Why This Works (Mechanism)
The supervised learning reformulation enables stable training by providing dense gradients from binary reward labels. By treating rewards as supervised labels, the framework avoids the variance issues inherent in traditional actor-critic methods. The implicit coupling emerges from the unified optimization objective, where policy improvement and reward estimation are learned simultaneously through a single cross-entropy loss function.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed - framework for training agents on tasks with binary success/failure outcomes. Quick check - agent must learn from sparse binary feedback.
- **Actor-Critic Methods**: Why needed - balance between policy optimization and value estimation. Quick check - traditional RL requires separate actor and critic networks.
- **Supervised Learning Framework**: Why needed - provides stable gradients from sparse rewards. Quick check - cross-entropy loss enables effective learning from binary labels.
- **Log-Probability Scaling (β)**: Why needed - controls influence of successful trajectories. Quick check - β=1 empirically optimal for mathematical reasoning.

## Architecture Onboarding

**Component Map**: Policy Network -> Cross-Entropy Loss -> Score Function -> Reward Prediction

**Critical Path**: Input problem → Policy network generates solution → Binary reward label (0/1) → Cross-entropy loss computation → Model update with implicit actor-critic coupling

**Design Tradeoffs**: 
- Supervised learning vs. traditional RLVR (stability vs. flexibility)
- Single unified model vs. separate actor-critic networks (simplicity vs. specialization)
- Binary classification vs. value estimation (dense gradients vs. continuous supervision)

**Failure Signatures**:
- Performance degradation when rewards are not truly binary
- Overfitting to specific problem types without diversity
- Sensitivity to hyperparameter β in complex domains

**First Experiments**:
1. Test PACS on a simple verifiable reward task (e.g., arithmetic) to validate basic functionality
2. Compare training stability with traditional RLVR methods on the same task
3. Vary β parameter to observe sensitivity and identify optimal values

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can PACS effectively leverage dense or process-based rewards (e.g., from Process Reward Models) rather than solely binary outcome rewards?
- **Basis in paper**: The methodology explicitly restricts the reward function to binary values {0, 1}, framing the problem as binary classification
- **Why unresolved**: The cross-entropy loss formulation is tailored for discrete labels; it is unclear how the "implicit critic" would handle continuous intermediate supervision without architectural changes
- **What evidence would resolve it**: Experiments integrating PACS with a Process Reward Model (PRM) to validate if the implicit coupling improves sample efficiency on long-horizon reasoning tasks

### Open Question 2
- **Question**: Is there a theoretical justification for the optimal β value, or a mechanism to adaptively tune it for tasks of varying complexity?
- **Basis in paper**: Section 4.3 notes that AIME datasets exhibit "heightened sensitivity to variations in β," and that β=1 is empirically optimal, but attributes this to dataset difficulty rather than providing a theoretical basis
- **Why unresolved**: The paper establishes the sensitivity but does not derive why β=1 is optimal or how to select it without extensive ablation on new datasets
- **What evidence would resolve it**: A theoretical analysis linking the log-probability ratio scaling (β) to the variance of the advantage estimates and the signal-to-noise ratio in gradient updates

### Open Question 3
- **Question**: What specific theoretical properties of the RLOO estimator cause it to significantly outperform Dr. GRPO on high-difficulty benchmarks like AIME?
- **Basis in paper**: Appendix C.4 states: "We posit that this performance gap stems from a subtle algorithmic divergence... which is magnified under more challenging conditions," noting the gap is unclear despite similar objectives
- **Why unresolved**: The authors observe the performance divergence but leave the underlying cause—hypothesized to be "leave-one-out" robustness—as an open point of discussion rather than a proven theorem
- **What evidence would resolve it**: A variance/bias decomposition of the gradients for RLOO versus Dr. GRPO specifically in low-data, high-variance regimes (like AIME)

## Limitations
- Limited evaluation to mathematical reasoning domains, with unclear generalizability to other RLVR applications
- No computational overhead analysis compared to standard RLVR approaches
- Limited hyperparameter sensitivity analysis beyond β optimization

## Confidence

**High confidence**: The core methodology of treating RLVR as supervised learning with cross-entropy loss on verifiable rewards is technically sound and well-implemented

**Medium confidence**: The comparative performance gains over PPO and GRPO are significant but based on a single benchmark suite (mathematical reasoning)

**Medium confidence**: The claim of implicit actor-critic coupling through unified model updates is theoretically valid but lacks extensive ablation studies demonstrating necessity

## Next Checks
1. Test PACS on non-mathematical RLVR domains (e.g., code generation, logical reasoning) to verify domain transferability
2. Conduct ablation studies removing the supervised learning component to quantify the specific contribution of implicit coupling
3. Measure wall-clock training time and compute requirements compared to standard RLVR baselines across different model scales