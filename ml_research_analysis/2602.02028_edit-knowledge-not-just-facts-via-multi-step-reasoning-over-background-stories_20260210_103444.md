---
ver: rpa2
title: Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories
arxiv_id: '2602.02028'
source_url: https://arxiv.org/abs/2602.02028
tags:
- knowledge
- answer
- reasoning
- question
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of integrating new knowledge into
  large language models (LLMs) in a way that supports flexible multi-step reasoning.
  Existing knowledge editing methods focus on isolated facts and struggle with portability,
  meaning models fail to use updated information when solving complex, multi-hop reasoning
  tasks.
---

# Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories

## Quick Facts
- arXiv ID: 2602.02028
- Source URL: https://arxiv.org/abs/2602.02028
- Authors: Ya Gao; Kalle Kujanpää; Pekka Marttinen; Harri Valpola; Alexander Ilin
- Reference count: 40
- Primary result: 99.0% accuracy on multi-hop reasoning tasks using contextualized background stories

## Executive Summary
This paper addresses the limitations of existing knowledge editing methods that focus on isolated facts by proposing a novel approach for integrating new knowledge into LLMs through contextualized background stories and self-generated multi-hop reasoning questions. The method enables models to perform complex reasoning tasks by combining new and existing knowledge in a portable manner, rather than simply memorizing individual facts. Experiments demonstrate significant improvements over baseline methods, achieving near-perfect performance on challenging multi-step reasoning tasks while maintaining factual accuracy and knowledge portability.

## Method Summary
The proposed approach treats knowledge editing as a reasoning problem rather than fact memorization. New knowledge is integrated through background stories that provide context, while self-generated multi-hop questions require combining new facts with existing knowledge to arrive at answers. Training employs knowledge distillation where a teacher model demonstrates the desired reasoning behavior, and a student model learns to replicate this behavior without direct access to the new information. This setup encourages the student to internalize the reasoning patterns rather than simply memorizing answers, resulting in improved portability across different reasoning tasks.

## Key Results
- Achieves 99.0% accuracy on challenging multi-hop questions requiring multiple new facts
- Substantially outperforms baseline knowledge editing methods on MQuAKE-Story and MQuAKE-CF datasets
- Demonstrates effective knowledge internalization with preserved factual accuracy and locality while significantly improving portability

## Why This Works (Mechanism)
The approach works by shifting from isolated fact editing to contextual knowledge integration. By embedding new facts within background stories, the model gains richer semantic context that facilitates reasoning connections. The self-generated multi-hop questions force the model to practice combining information from multiple sources, building reasoning pathways rather than simple lookup capabilities. Knowledge distillation enables the model to learn reasoning patterns through observation rather than explicit instruction, which promotes more flexible knowledge application across different contexts and tasks.

## Foundational Learning
- **Knowledge Distillation**: Why needed - enables student models to learn reasoning patterns from teacher demonstrations; Quick check - verify distillation loss decreases during training
- **Multi-hop Reasoning**: Why needed - allows combining multiple pieces of information for complex problem-solving; Quick check - test model on questions requiring at least 3 reasoning steps
- **Background Story Integration**: Why needed - provides contextual framework for new knowledge that supports flexible application; Quick check - measure performance difference with and without story context
- **Self-generated Question Generation**: Why needed - creates training data that matches the complexity of target reasoning tasks; Quick check - validate generated questions actually require multi-step reasoning
- **Portability in Knowledge Editing**: Why needed - ensures updated knowledge can be applied across different reasoning contexts; Quick check - test model performance on novel reasoning tasks using updated knowledge
- **Teacher-Student Learning**: Why needed - enables knowledge transfer without direct access to new information; Quick check - compare student performance with teacher on held-out reasoning tasks

## Architecture Onboarding

**Component Map**: Background Stories -> Self-Generated Questions -> Teacher Model -> Knowledge Distillation -> Student Model -> Multi-hop Reasoning Tasks

**Critical Path**: The knowledge integration pipeline flows from contextualized background stories through question generation to teacher demonstration, then via distillation to student internalization, culminating in improved multi-hop reasoning performance.

**Design Tradeoffs**: The method prioritizes reasoning flexibility and portability over simple fact memorization, accepting higher computational costs for knowledge distillation in exchange for more robust knowledge application. The reliance on self-generated questions trades potential question quality control for scalability in creating diverse training data.

**Failure Signatures**: Poor performance indicates either insufficient background story context, inadequate question complexity, or ineffective knowledge distillation. Models may show memorization without reasoning (high accuracy on training questions but poor generalization) or fail to integrate new knowledge with existing knowledge (correct answers using only old or new information, but not both).

**3 First Experiments**:
1. Test baseline knowledge editing method on same multi-hop reasoning tasks for comparison
2. Evaluate student model performance with and without background story context
3. Measure portability by testing updated knowledge on novel reasoning tasks not seen during training

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to specific model sizes (32B and 70B parameters) and particular datasets, raising generalizability concerns
- Does not address computational costs associated with knowledge distillation training process
- Evaluation focuses primarily on accuracy metrics without extensive analysis of reasoning quality, potential hallucinations, or retention of previously learned knowledge
- Assumes self-generated multi-hop questions effectively capture complex reasoning patterns without external validation
- Does not explore long-term knowledge retention or performance degradation over time

## Confidence
- High confidence in the core technical approach and dataset creation methodology
- Medium confidence in the generalization claims beyond tested models and domains
- Medium confidence in the practical utility given computational cost considerations

## Next Checks
1. Test the approach on diverse model sizes (including smaller models) and different knowledge domains to assess scalability and domain transfer
2. Conduct ablation studies to isolate the contribution of self-generated questions versus background story context in achieving portability
3. Perform longitudinal studies measuring knowledge retention and reasoning performance over extended periods post-training