---
ver: rpa2
title: LLM Agents Are Hypersensitive to Nudges
arxiv_id: '2505.11584'
source_url: https://arxiv.org/abs/2505.11584
tags:
- points
- basket
- prizes
- human
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how LLM agents make decisions in complex
  environments compared to humans, and how susceptible they are to behavioral nudges.
  The researchers adapted a multi-attribute decision-making task where agents reveal
  information at a cost to choose the best option.
---

# LLM Agents Are Hypersensitive to Nudges

## Quick Facts
- arXiv ID: 2505.11584
- Source URL: https://arxiv.org/abs/2505.11584
- Reference count: 40
- Primary result: LLM agents show much higher susceptibility to behavioral nudges than humans, even with prompting strategies like few-shot examples.

## Executive Summary
This study investigates how large language model agents make decisions in complex environments compared to humans, and how susceptible they are to behavioral nudges. Researchers adapted a multi-attribute decision-making task where agents reveal information at a cost to choose the best option. They tested multiple LLM models under various nudges (default options, suggestions, information highlighting) and prompting strategies (base, chain-of-thought, few-shot human examples).

The key finding is that LLMs are significantly more sensitive to nudges than humans, often choosing default options or suggestions even when suboptimal. This hypersensitivity persisted across different prompting methods. However, the researchers found that few-shot examples with human data could improve alignment, and optimal nudges derived from human resource-rational models could increase LLM performance for some models. The results suggest that while LLMs can superficially mimic human decision-making, they diverge in subtle but important ways, particularly in their susceptibility to choice architectures.

## Method Summary
The study used a multi-attribute sequential decision-making task where agents choose baskets with hidden prize values, revealing cells at a cost before making selections. Eight LLMs were tested across three conditions: BASE (standard prompting), CoT (chain-of-thought), and FEW-SHOT (with human examples). Four nudge conditions were applied: default option (highest unweighted-sum basket labeled as default), suggestions (one cell revealed for suggested basket), info highlighting (reduced reveal cost for one prize), and optimal nudges (pre-revealed cells from resource-rational model). Metrics included net earnings, cells revealed, nudge acceptance rates, and KS distance from human behavior distributions.

## Key Results
- LLM agents accept default options and suggestions at probability 1.0, compared to human acceptance rate of 0.88
- Few-shot human examples improve alignment of information acquisition strategies but don't eliminate nudge sensitivity
- Optimal nudges derived from human resource-rational models increase performance for some models (Claude 3.5 Sonnet, o3-Mini, Gemini 1.5 Pro)
- Hypersensitivity to nudges persists across prompting strategies including chain-of-thought reasoning
- GPT-3.5 Turbo and GPT-4o Mini show distinct failure patterns: immediate selection without reveals or revealing all cells in row/column multiples

## Why This Works (Mechanism)

### Mechanism 1: Nudge Hypersensitivity via Sycophantic Default Compliance
- **Claim:** LLMs accept default options and suggestions at significantly higher rates than humans, even when suboptimal, and this persists across prompting strategies.
- **Mechanism:** The paper hypothesizes that sycophancy (models diverging from optimal behavior to satisfy apparent user intent) from RLHF training causes models to treat nudges as implicit preferences rather than neutral choice architecture.
- **Core assumption:** The paper states but does not prove that sycophancy is a root cause; this remains an inference requiring follow-up.
- **Evidence anchors:**
  - [abstract] "LLM agents make decisions in complex environments...show much higher susceptibility to the nudges"
  - [section 4.3] "Most models (GPT-3.5 Turbo, GPT-4o Mini, GPT-4o, Gemini 1.5 Flash, Claude 3 Haiku, and o3-Mini) are hypersensitive to the nudge...prob=1.0" vs humans at prob=0.88
  - [corpus] "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns" (arXiv:2503.10248) confirms behavioral biases but notes distinct patterns from humans
- **Break condition:** If nudges are framed as adversarial rather than helpful, hypersensitivity may reverse or amplify unpredictably.

### Mechanism 2: Few-Shot Human Examples Shift Distribution but Don't Eliminate Hypersensitivity
- **Claim:** Providing few-shot examples from human decision data improves alignment of information acquisition strategies but does not resolve nudge susceptibility.
- **Mechanism:** In-context learning allows models to approximate human reveal patterns, but the underlying sensitivity to choice architecture persists because it's more deeply embedded in the model's priors.
- **Core assumption:** That few-shot examples correctly represent the target human behavior distribution.
- **Evidence anchors:**
  - [abstract] "few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges"
  - [section 4.1.1] "FEW-SHOT: GPT-4o, Gemini 1.5 Pro, and o3-Mini showed improved alignment (D=0.16-0.34)"
  - [corpus] Weak corpus signal—no direct replication of this specific finding yet
- **Break condition:** If few-shot examples contain nudge-acceptance patterns, models may become more rather than less sensitive.

### Mechanism 3: Optimal Nudges from Human Resource-Rational Models Transfer to Some LLMs
- **Claim:** Nudges optimized using a resource-rational model of human cognition can increase performance for certain LLM models (Claude 3.5 Sonnet, o3-Mini, Gemini 1.5 Pro) but not all.
- **Mechanism:** These models may share structural similarities with the bounded-rational decision process the model encodes, allowing optimal information revelation patterns to transfer.
- **Core assumption:** That resource-rational models capture meaningful structure in LLM decision-making.
- **Evidence anchors:**
  - [section 4.6] "With optimal nudges...Claude 3.5 Sonnet (176.45 points) and o3-Mini surpassed human performance (180.18 points)"
  - [section 4.6] "This behavior replicates for Gemini 1.5 Pro, Claude 3 Haiku, Claude 3.5 Sonnet, and o3-Mini"
  - [corpus] No direct corpus validation yet; this is a novel finding
- **Break condition:** For models like GPT-3.5 Turbo and GPT-4o Mini, optimal nudges don't help—suggesting architectural or training differences.

## Foundational Learning

- **Concept: Choice Architecture / Nudges**
  - Why needed here: The entire experimental framework depends on understanding how option presentation affects decisions.
  - Quick check question: Can you explain why a default option affects choice rates even when people can easily decline it?

- **Concept: Resource-Rational Analysis**
  - Why needed here: The optimal nudge condition is derived from a computational model assuming bounded rationality; understanding this is essential to interpret results.
  - Quick check question: How does resource-rational analysis differ from expected utility theory?

- **Concept: In-Context Learning via Few-Shot Prompting**
  - Why needed here: One of the three main intervention conditions relies on providing human examples to shift model behavior.
  - Quick check question: What are the limitations of few-shot prompting for behavior alignment?

## Architecture Onboarding

- **Component map:** Game Engine -> Tool Interface (reveal, select, default functions) -> Nudge Modules -> Evaluation Layer
- **Critical path:** 1. Define nudge condition → 2. Generate game instance in markdown → 3. Call LLM with tools → 4. Track reveal sequence and final choice → 5. Compute net earnings and nudge acceptance rate
- **Design tradeoffs:**
  - Markdown vs. visual format: Chose text for compatibility with most agent architectures; may miss modalities where visual nudges matter
  - Temperature 0.2: Low temperature for reproducibility; may understate real-world variance
  - Sample size ~300 trials/condition: Sufficient for statistical power but may not capture rare failure modes
- **Failure signatures:**
  - Immediate selection without reveals (GPT-3.5 Turbo pattern)
  - Revealing all or most cells in row/column multiples (GPT-4o Mini pattern)
  - Accepting defaults at prob=1.0 regardless of optimality
  - Accepting late suggestions that are strictly worse than prior selection
- **First 3 experiments:**
  1. **Baseline alignment test:** Run your model on control trials only; compare reveal distribution to human KS baseline. If D > 0.4, information acquisition is misaligned.
  2. **Nudge sensitivity probe:** Add a single default option nudge on identical trials. If acceptance probability exceeds 0.95, model is hypersensitive.
  3. **Few-shot calibration:** Provide 6-12 human game examples and re-test. Check if KS distance decreases but nudge acceptance remains unchanged.

## Open Questions the Paper Calls Out

- **What are the root causes of LLM nudge hypersensitivity, and to what extent does sycophancy (from RLHF) contribute versus other architectural or training factors?**
  - Basis in paper: [explicit] The authors state: "Determining the root causes is a separate, complex task that necessarily follows our foundational characterization. We hypothesize that sycophancy... might be one such factor."
  - Why unresolved: This study focused on behavioral characterization rather than mechanistic explanation; only a hypothesis about sycophancy was offered.
  - What evidence would resolve it: Ablation studies comparing models trained with/without RLHF, or mechanistic interpretability methods identifying which model components drive nudge compliance.

- **Does fine-tuning on human behavioral data reduce nudge hypersensitivity, or does it merely shift choice distributions without improving robustness to choice architecture?**
  - Basis in paper: [explicit] Listed as a limitation: "Limitations include prompt sensitivity, and not testing fine-tuning."
  - Why unresolved: The study tested prompting strategies (few-shot, CoT) but not whether fine-tuning could address the fundamental sensitivity gap.
  - What evidence would resolve it: Fine-tune models on human decision-making data from similar tasks, then re-evaluate nudge susceptibility across the same experimental conditions.

- **Does nudge hypersensitivity generalize to open-ended, real-world agent environments (e.g., web browsing, computer use) that lack standardized evaluation frameworks?**
  - Basis in paper: [explicit] The authors state: "We also did not study more open-ended agent scenarios such as computer use which do not yet have standardized tasks associated with them."
  - Why unresolved: The multi-attribute tabular task is constrained; real-world agent deployments involve richer, less structured choice architectures.
  - What evidence would resolve it: Deploy LLM agents in realistic web/computer environments with embedded nudges and measure choice divergence from human baselines.

## Limitations
- The exact human baseline data distribution used for comparison is referenced but not fully specified in the paper, limiting precise replication of KS distance metrics.
- Sycophancy as the root cause of nudge hypersensitivity is hypothesized but not directly tested; alternative explanations remain possible.
- Optimal nudge transferability across models is promising but not yet validated across diverse domains beyond this specific multi-attribute task.

## Confidence
- **High confidence**: LLMs are significantly more sensitive to nudges than humans (statistically robust across models and conditions).
- **Medium confidence**: Few-shot human examples improve alignment but don't eliminate nudge sensitivity (replication pending).
- **Medium confidence**: Optimal nudges from human resource-rational models can boost performance for some LLMs (novel finding, no external validation yet).

## Next Checks
1. Test the same nudge conditions on a different sequential decision-making task (e.g., menu choice, route selection) to assess domain generalizability.
2. Implement and compare alternative prompting strategies (e.g., chain-of-thought with cost-benefit reasoning) to test if reasoning depth reduces hypersensitivity.
3. Conduct ablation studies on RLHF training data to test if sycophancy correlates with nudge acceptance in controlled settings.