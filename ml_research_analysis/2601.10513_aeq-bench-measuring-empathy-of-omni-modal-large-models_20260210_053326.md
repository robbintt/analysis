---
ver: rpa2
title: 'AEQ-Bench: Measuring Empathy of Omni-Modal Large Models'
arxiv_id: '2601.10513'
source_url: https://arxiv.org/abs/2601.10513
tags:
- audio
- human
- empathy
- response
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AEQ-Bench, a benchmark for evaluating empathy
  in omni-modal large models. It addresses the challenge of assessing both linguistic
  and paralinguistic empathy through novel context- and tone-variant settings.
---

# AEQ-Bench: Measuring Empathy of Omni-Modal Large Models

## Quick Facts
- arXiv ID: 2601.10513
- Source URL: https://arxiv.org/abs/2601.10513
- Reference count: 40
- Primary result: Introduces AEQ-Bench for evaluating empathy in omni-modal large models

## Executive Summary
AEQ-Bench is a novel benchmark designed to evaluate empathy in omni-modal large language models (OLMs), focusing on both linguistic and paralinguistic empathy. It introduces context- and tone-variant settings, requiring models to generate and judge empathetic responses based on audio and text inputs. The benchmark includes 1,885 instances from three datasets and highlights the challenges of assessing paralinguistic expressiveness. Experiments reveal that while OLMs outperform text-only models in naturalness and coherence, they struggle with fine-grained paralinguistic empathy evaluation, particularly when judged by audio-native models.

## Method Summary
AEQ-Bench evaluates empathy in omni-modal large models by testing their ability to generate and judge empathetic responses using audio and text inputs. It introduces context- and tone-variant settings to assess both linguistic and paralinguistic empathy. The benchmark includes 1,885 instances from three datasets, requiring models to generate and judge empathetic responses. Human judgment is incorporated to validate model outputs, with a focus on naturalness, coherence, and emotional expressiveness. The evaluation distinguishes between linguistic empathy (text-based) and paralinguistic empathy (tone and prosody-based).

## Key Results
- OLMs with audio output capabilities (e.g., GPT, Qwen-Omni) outperform text-only models in naturalness and coherence.
- Models align with human judgments on coarse-grained tasks but struggle with fine-grained paralinguistic expressiveness.
- OLM judges diverge from human perception in evaluating emotional prosody, highlighting the need for dedicated audio-native evaluators.

## Why This Works (Mechanism)
AEQ-Bench works by explicitly separating linguistic and paralinguistic empathy evaluation, enabling targeted assessment of omni-modal models' capabilities. The context- and tone-variant settings simulate real-world empathetic communication scenarios, requiring models to adapt to nuanced emotional cues. By incorporating both generation and judgment tasks, the benchmark provides a comprehensive evaluation of empathy, while human judgment ensures alignment with human perceptions of empathy.

## Foundational Learning

1. **Linguistic Empathy** - The ability to convey empathy through text-based communication. Why needed: Essential for evaluating text-only models and understanding the baseline of empathetic expression. Quick check: Assess model responses for coherence and emotional appropriateness.

2. **Paralinguistic Empathy** - The conveyance of empathy through tone, pitch, and prosody in speech. Why needed: Critical for evaluating omni-modal models' ability to express empathy beyond text. Quick check: Analyze audio outputs for naturalness and emotional expressiveness.

3. **Context-Variant Settings** - Scenarios where empathy expression adapts to different conversational contexts. Why needed: Reflects real-world empathetic communication, where context shapes emotional expression. Quick check: Evaluate model responses across varied contexts for consistency and appropriateness.

4. **Tone-Variant Settings** - Scenarios where empathy expression varies based on emotional tone. Why needed: Tests models' ability to modulate emotional expression to match the speaker's tone. Quick check: Assess audio outputs for alignment with the intended emotional tone.

## Architecture Onboarding

**Component Map:** Input (Audio + Text) -> Model (Generation/Judgment) -> Output (Empathetic Response/Judgment) -> Human Evaluation

**Critical Path:**