---
ver: rpa2
title: Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards
arxiv_id: '2512.21625'
source_url: https://arxiv.org/abs/2512.21625
tags:
- step
- entropy
- training
- length
- aime25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic investigation into how positive
  and negative samples affect RLVR training dynamics and behaviors. We find that positive
  samples sharpen existing correct reasoning patterns, while negative samples encourage
  exploration of new reasoning paths.
---

# Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards

## Quick Facts
- arXiv ID: 2512.21625
- Source URL: https://arxiv.org/abs/2512.21625
- Reference count: 40
- Key outcome: A3PO improves RLVR reasoning performance by adaptively shaping token-level advantages based on sample polarity

## Executive Summary
This paper systematically investigates how positive and negative samples affect Reinforcement Learning with Verifiable Rewards (RLVR) training dynamics. Through empirical analysis on mathematical reasoning tasks, the authors discover that positive samples primarily sharpen existing correct reasoning patterns while negative samples encourage exploration of new reasoning paths. Building on these insights, they propose A3PO (Adaptive and Asymmetric token-level Advantage shaping for Policy Optimization), which more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of this approach, with the optimal positive-to-negative advantage ratio empirically found to be 0.5.

## Method Summary
The paper proposes A3PO, an extension of DAPO that implements adaptive, asymmetric token-level advantage shaping. For positive samples (A>0), A3PO amplifies advantages of low-probability tokens (bottom 20% percentile), while for negative samples (A<0), it amplifies advantages of high-probability tokens (top 20%). The scaling factor decays linearly during training: max(ρ - α×step, 1), with ρ+=ρ-=2 and α+=α-=0.005. The method uses 300 training steps with batch size 512, mini-batch 32, and learning rate 1e-6. Implementation uses Verl pipeline with vLLM for rollouts and FSDP for training on 16×H200 GPUs. No KL or entropy loss is applied. The approach aims to balance the sharpening effect of positive samples with the exploration encouraged by negative samples.

## Key Results
- A3PO achieves improved average accuracy across five reasoning benchmarks compared to baseline DAPO
- The optimal positive-to-negative advantage ratio is empirically determined to be 0.5
- Token-level advantage shaping (bottom 20% for positives, top 20% for negatives) proves more effective than uniform scaling
- Different base models (math-specialized vs general pretraining) show varying responses to single-polarity training

## Why This Works (Mechanism)
A3PO works by recognizing that positive and negative samples serve distinct functions in RLVR training. Positive samples with high advantage values tend to reinforce existing correct reasoning patterns, leading to sharper, more confident predictions on tokens that are already likely. Negative samples with low advantage values, conversely, encourage exploration of alternative reasoning paths by focusing on tokens that were given high probability but led to incorrect outcomes. By adaptively shaping advantages at the token level—amplifying low-probability tokens in positive samples and high-probability tokens in negative samples—A3PO creates a more balanced training signal that both sharpens correct reasoning and explores alternatives.

## Foundational Learning
- **RLVR (Reinforcement Learning with Verifiable Rewards)**: RL applied to LLMs where rewards are binary outcomes from verifiable tasks. Needed to understand the specific training paradigm being analyzed. Quick check: Can the model distinguish between self-generated rollouts and ground truth solutions?
- **Sample polarity**: Classification of training samples as positive (A>0) or negative (A<0) based on their advantage values. Needed to understand the core dichotomy driving the analysis. Quick check: Does the distribution of A values show clear separation between positive and negative samples?
- **Token-level advantage shaping**: Modifying the advantage values at individual token positions rather than applying uniform scaling. Needed to understand how A3PO creates more precise training signals. Quick check: Are the bottom/top 20% tokens consistently located in semantically meaningful positions?
- **Adaptive scaling with decay**: Linearly decreasing the scaling factor over training steps to prevent over-amplification. Needed to understand the stability mechanism. Quick check: Does the scaling factor approach 1 before training completes?
- **Per-response vs global percentile ranking**: Whether token probability thresholds are computed per response or globally across all responses. Needed to understand implementation details affecting reproducibility. Quick check: Does changing this setting significantly affect training outcomes?
- **DAPO-Math dataset**: The specific dataset used for training, containing verifiable mathematical reasoning tasks. Needed to understand the evaluation context. Quick check: Can the dataset be accessed or is there a public equivalent?

## Architecture Onboarding

**Component Map:**
Verl RLHF Framework -> vLLM Rollout Engine -> FSDP Training Loop -> A3PO Advantage Shaping Module -> DAPO-Math Dataset

**Critical Path:**
Prompt Generation -> Rollout Sampling (8 responses) -> Reward Calculation -> Advantage Computation -> Token Probability Extraction -> Adaptive Advantage Shaping -> Policy Update

**Design Tradeoffs:**
- Per-token vs uniform advantage shaping: Token-level provides precision but increases computational complexity
- Static vs adaptive scaling: Adaptive allows dynamic adjustment but requires careful hyperparameter tuning
- Positive-only vs balanced polarity training: Balanced improves exploration but may slow convergence on already-correct patterns

**Failure Signatures:**
- Reward hacking with excessive response length reduction (single polarity training on base models)
- Training-inference mismatch causing catastrophic forgetting
- Performance degradation when ρ or α values are too aggressive
- Loss of diversity in generated responses with over-sharpening

**First Experiments:**
1. Run 50-step training with baseline DAPO on Qwen2.5-7B-Math to establish reference performance
2. Implement A3PO with adaptive scaling disabled (static ρ) to isolate the effect of token-level shaping
3. Train with only positive samples to reproduce the reward hacking failure mode and verify its absence in math-specialized models

## Open Questions the Paper Calls Out
- How do sample polarity dynamics transfer to vision-language models (VLMs) and multimodal reasoning tasks?
- Do positive and negative sample dynamics generalize to agent-based scenarios such as tool-use, search agents, or code execution?
- Is the optimal positive-to-negative advantage ratio (empirically found at 0.5) universal or dependent on model architecture, dataset, or task type?
- What mechanisms cause different base LLMs (math-specialized, general pretrained, distilled) to respond so differently to single-polarity training?

## Limitations
- Findings are derived from experiments on Qwen2.5-7B-Math and Qwen3-8B-Base models, raising generalizability concerns
- The DAPO-Math dataset used for training is not publicly available, limiting independent validation
- The study focuses exclusively on binary outcome rewards in mathematical reasoning tasks
- Adaptive advantage shaping relies on hyperparameters that may require task-specific tuning

## Confidence
- **High confidence**: The empirical finding that positive and negative samples serve distinct functions in RLVR training (sharpening vs exploration)
- **Medium confidence**: The proposed A3PO method's effectiveness, as results show improvements over baselines but depend on specific hyperparameter settings
- **Low confidence**: Claims about the mechanism of advantage amplification at the token level, as the paper provides limited theoretical justification

## Next Checks
1. **Ablation on token selection percentile**: Test A3PO with different percentile thresholds (10%, 30%, 50%) for token selection to determine sensitivity and optimal range
2. **Cross-model generalization**: Apply A3PO to a different model family (e.g., Llama, Mistral) on the same reasoning benchmarks to assess whether benefits transfer beyond Qwen models
3. **Dataset independence test**: Train A3PO on a publicly available RLVR dataset (e.g., RewardMath or RL-Coder) to verify that improvements are not specific to the proprietary DAPO-Math dataset