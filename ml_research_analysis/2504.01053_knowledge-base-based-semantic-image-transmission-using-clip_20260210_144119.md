---
ver: rpa2
title: Knowledge-Base based Semantic Image Transmission Using CLIP
arxiv_id: '2504.01053'
source_url: https://arxiv.org/abs/2504.01053
tags:
- semantic
- image
- transmission
- channel
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge-base (KB) assisted semantic communication
  framework for image transmission using CLIP. The core idea involves extracting 512-dimensional
  semantic features using CLIP, compressing them with a lightweight neural network,
  and transmitting them through a wireless channel.
---

# Knowledge-Base based Semantic Image Transmission Using CLIP

## Quick Facts
- **arXiv ID:** 2504.01053
- **Source URL:** https://arxiv.org/abs/2504.01053
- **Authors:** Chongyang Li; Yanmei He; Tianqian Zhang; Mingjian He; Shouyin Liu
- **Reference count:** 13
- **Primary result:** Proposes KB-assisted semantic communication using CLIP features, achieving higher semantic accuracy than traditional BPG+LDPC and SwinJSCC across various CBR and SNR conditions.

## Executive Summary
This paper introduces a knowledge-base assisted semantic communication framework for image transmission using CLIP. The approach extracts 512-dimensional semantic features using CLIP, compresses them with a lightweight neural network, and transmits them through a wireless channel. At the receiver, compressed features are reconstructed and matched against a KB using FAISS to retrieve the most semantically similar image. The framework prioritizes semantic accuracy over traditional metrics like PSNR, introducing a new evaluation paradigm for semantic-aware communication systems.

## Method Summary
The method extracts CLIP semantic embeddings from transmitted images, compresses them via a lightweight MLP encoder, transmits through AWGN or Rayleigh fading channels, then reconstructs and retrieves the most semantically similar image from a pre-built FAISS index. The system is trained per Channel Bandwidth Ratio (CBR) level with noise augmentation across multiple SNRs. Evaluation uses CIFAR100 with a per-category split between transmitted images and KB entries.

## Key Results
- Outperforms traditional BPG+LDPC and deep learning-based SwinJSCC in semantic transmission accuracy across various CBR and SNR conditions
- Achieves higher semantic accuracy while offering faster inference speed compared to SwinJSCC
- Demonstrates robustness to channel noise through CLIP's semantic feature space and KB-based reconstruction

## Why This Works (Mechanism)

### Mechanism 1
CLIP's 512-dimensional semantic embeddings provide inherent noise resilience compared to pixel-level representations. The embedding space clusters semantically similar images together, meaning small perturbations from channel noise shift the embedding within a semantic neighborhood rather than crossing category boundaries.

### Mechanism 2
Retrieval-based reconstruction from a shared KB achieves higher semantic accuracy than pixel-level reconstruction under noisy channels. Instead of reconstructing pixels, the receiver performs nearest-neighbor search in the KB using the reconstructed CLIP feature, returning a valid, high-quality image.

### Mechanism 3
Lightweight MLP encoder-decoder enables CBR-adaptive transmission with learned noise resilience. When CBR > 1/12, the network learns to add redundancy that improves noise resistance. Separate models are trained per CBR level, each exposed to multiple SNR conditions during training.

## Foundational Learning

### Concept 1: CLIP Joint Vision-Language Embedding Space
Why needed: CLIP encodes images into a 512-dim space where semantically similar images are close in L2 distance. Understanding this is essential for debugging retrieval failures.
Quick check: Given two images of different dog breeds, would you expect their CLIP embeddings to be closer or farther apart than a dog vs. a cat embedding? Why does this matter for KB retrieval under noise?

### Concept 2: Semantic vs. Pixel-Level Metrics
Why needed: The paper explicitly rejects PSNR/MS-SSIM in favor of semantic accuracy (category consistency). Practitioners must understand why traditional metrics fail.
Quick check: An image is transmitted and received with a PSNR of 18 dB but the retrieved image is semantically correct. Is this a successful transmission under this framework? What if PSNR is 35 dB but category is wrong?

### Concept 3: Channel Bandwidth Ratio (CBR) and Rate-Distortion Tradeoffs
Why needed: CBR directly controls the compression ratio and thus the bandwidth-accuracy tradeoff. Selecting the right CBR model for deployment conditions is a key engineering decision.
Quick check: For a low-bandwidth IoT sensor transmitting images over a noisy channel, would you select CBR = 1/48 or CBR = 1/3? What are the tradeoffs in terms of semantic accuracy, latency, and robustness?

## Architecture Onboarding

### Component Map
RGB Image → CLIP ViT Encoder → 512-dim Feature → MLP Encoder → k-dim Compressed → IQ Modulation → Channel → Demodulation → MLP Decoder → 512-dim Reconstructed → FAISS L2 Search against KB → Retrieved Image

### Critical Path
1. KB Construction (offline): Extract CLIP embeddings from all reference images; build FAISS index. Verify coverage of all expected categories.
2. Model Training per CBR: Train encoder-decoder MLP on training split with channel noise augmentation at multiple SNRs. Validate on held-out set.
3. Inference Pipeline: At transmitter, extract CLIP feature → encode → modulate → transmit. At receiver, demodulate → decode → FAISS retrieval → return image.

### Design Tradeoffs
| Decision | Options | Tradeoff |
|----------|---------|----------|
| CBR Selection | Lower (1/48) vs. Higher (1/3) | Lower CBR = less bandwidth but lower accuracy; higher CBR = more bandwidth, better noise resilience, potential over-redundancy |
| KB Size | Small (few images/category) vs. Large (many images/category) | Small KB = faster retrieval, less coverage; large KB = better matching, higher memory and retrieval latency |
| Channel Model Training | Single SNR vs. Multi-SNR | Single SNR = optimized for specific condition; multi-SNR = more robust generalization |

### Failure Signatures
- High semantic accuracy but wrong specific image: KB lacks fine-grained diversity
- Sharp accuracy drop at specific SNR threshold: Encoder-decoder overfitted to training SNR range
- Category confusion under moderate noise (e.g., wolf→dog): CLIP embedding space has small inter-class margin for visually similar categories
- Slow retrieval (>10ms): FAISS index not optimized; move to GPU or use IVF/PQ indexing for large KBs

### First 3 Experiments
1. Baseline Sanity Check: Transmit images with no compression (k=512, CBR=1/12) over AWGN at SNR=10 dB. Measure semantic accuracy.
2. CBR Sweep under Fixed SNR: Run inference at CBR ∈ {1/48, 1/24, 1/12, 1/6, 1/3} with SNR=0 dB. Plot semantic accuracy vs. CBR.
3. Cross-Channel Generalization: Train model on AWGN only; test on Rayleigh fading at matched SNR levels. Measure accuracy drop.

## Open Questions the Paper Calls Out

### Open Question 1
How can the Knowledge Base (KB) structure be optimized to handle large-scale or dynamic datasets while maintaining the low latency required for real-time semantic communication? The current experiments utilize FAISS on CIFAR100; it's unclear if the 1.2 ms retrieval speed remains constant when scaling to millions of diverse images.

### Open Question 2
Can advanced feature compression techniques improve transmission efficiency beyond the lightweight MLP approach without compromising the system's robustness to channel noise? The current compression uses a simple MLP, and results show performance drops at very low CBRs.

### Open Question 3
How can the framework be modified to handle "KB mismatch" scenarios where the transmitted image content is not present in the receiver's Knowledge Base? The current methodology assumes the receiver always possesses a relevant image to retrieve.

## Limitations
- No hyperparameter specification (loss function, optimizer, learning rate, batch size, epochs) makes reproduction challenging
- KB diversity and size not quantified, which affects retrieval reliability and could explain performance variance
- Channel model generalization limited to AWGN and Rayleigh fading, not covering real-world correlated fading conditions

## Confidence
- **High confidence**: Core mechanism of CLIP-based semantic embedding extraction and FAISS retrieval for KB-assisted semantic communication is well-founded and clearly described
- **Medium confidence**: Claim that lightweight MLP compression outperforms complex JSCC models is supported by CIFAR100 results but may not generalize to more complex datasets
- **Low confidence**: Assertion that MLP encoder-decoder is sufficient for this task is based on limited evidence; paper lacks ablation studies comparing MLP to more advanced architectures

## Next Checks
1. Train the encoder-decoder with multiple hyperparameter settings (loss types, learning rates) to determine which configurations reproduce the reported semantic accuracy trends
2. Systematically vary KB size and per-category image count to measure how retrieval accuracy and robustness to noise change
3. Evaluate the trained models on a dataset outside CIFAR100 (e.g., ImageNet subsets or real-world images) to assess whether the approach maintains semantic accuracy without retraining the KB or models