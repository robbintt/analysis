---
ver: rpa2
title: 'Don''t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation'
arxiv_id: '2505.16222'
source_url: https://arxiv.org/abs/2505.16222
tags:
- code
- evaluation
- bias
- biases
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically investigates biases in LLM-based code\
  \ evaluation, revealing that LLM judges are highly susceptible to superficial code\
  \ variations that should not affect correctness. We define six bias types\u2014\
  authority, self-declared correctness, variable renaming, reverse authority, misleading\
  \ tasks, and illusory complexity\u2014and demonstrate that all tested models, including\
  \ GPT-4o, show accuracy drops of up to 26.7 percentage points when exposed to these\
  \ biases."
---

# Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation

## Quick Facts
- arXiv ID: 2505.16222
- Source URL: https://arxiv.org/abs/2505.16222
- Reference count: 33
- Primary result: LLM judges show accuracy drops up to 26.7 percentage points when exposed to superficial code variations

## Executive Summary
This study systematically investigates biases in LLM-based code evaluation, revealing that LLM judges are highly susceptible to superficial code variations that should not affect correctness. We define six bias types—authority, self-declared correctness, variable renaming, reverse authority, misleading tasks, and illusory complexity—and demonstrate that all tested models, including GPT-4o, show accuracy drops of up to 26.7 percentage points when exposed to these biases. Positive biases (e.g., authority cues, self-declared correctness) lead to overestimating correctness, while negative biases (e.g., misleading tasks, reverse authority) cause correct code to be penalized. Test-case generation provides only marginal robustness improvements, highlighting the need for more robust evaluation methods.

## Method Summary
The study uses CodeNet dataset filtered to AtCoder Beginner Contest problems, extracting 200 problems per language (C++, Python, Java, JavaScript, Go) with paired correct/"Wrong Answer" solutions. All existing comments are stripped, then six bias types are injected: authority cues (expert comments), self-declared correctness (explicit "correct code" comments), variable renaming (24-character random strings), reverse authority (non-expert comments), misleading tasks (incorrect task descriptions), and illusory complexity (dummy functions). Evaluation uses Chain-of-Thought prompting with temperature=0.0, averaged over 3 trials for closed-source models. Robustness is measured via Mean Absolute Deviation (MAD) comparing accuracy between original and biased conditions.

## Key Results
- All six bias types cause significant accuracy degradation across all tested models (GPT-4o, GPT-4o-mini, Gemini-2.0-Flash, Claude-3.5-Sonnet, LLaMA-3.1-70B/8B)
- Self-declared correctness shows most pronounced effect, especially in open-source models (24.7% drop for LLaMA-3.1-70B)
- Variable name length and illusory complexity consistently induce positive bias, with accuracy improving for both correct and incorrect code as features increase
- Test-case generation provides only marginal robustness improvements
- Biases persist across five programming languages and are largely independent of model scale

## Why This Works (Mechanism)

### Mechanism 1: Authority Cue Hijacking
LLM judges grant undue trust to comments claiming expert authorship, bypassing rigorous code analysis. The model's training on expert-curated content creates a learned association between authority signals and higher-quality outputs. When present in input, these cues activate a "trust mode" that reduces scrutiny of actual logic. The bias stems from training data correlations between expertise markers and correctness.

### Mechanism 2: Self-Declared Correctness Anchoring
Explicit correctness claims in comments cause LLMs to skip or superficially perform logical verification. The presence of a "correct code" comment serves as a strong prior that anchors the final judgment. The model rationalizes around the anchor rather than falsifying it. This differs from authority bias by being a direct assertion rather than an inferred credential.

### Mechanism 3: Surface-Feature Misattribution (Variable Length & Illusory Complexity)
LLMs conflate code length, variable name length, and structural complexity with functional correctness. Longer variable names and additional code (even non-functional dummy functions) trigger a heuristic associating verbosity with sophistication. Figure 4 shows accuracy improves for both correct and incorrect code as variable length increases from 2 to 48 characters.

## Foundational Learning

- **Concept: Reference-Free Code Evaluation**
  - Why needed: The entire paper studies settings where LLMs assess correctness using only task description and code, without test cases or reference implementations
  - Quick check: If you had access to executable test cases, which bias types would become irrelevant?

- **Concept: Chain-of-Thought Prompting in Evaluation**
  - Why needed: The paper uses CoT prompting yet models still fail, revealing that reasoning traces can be rationalizations post-hoc rather than genuine verification
  - Quick check: If a model produces logically coherent reasoning but an incorrect final judgment, where did the process fail?

- **Concept: Positive vs. Negative Bias Directionality**
  - Why needed: The paper distinguishes biases that inflate scores (positive: authority, self-declared correctness, variable lengthening) from those that depress scores (negative: misleading task, reverse authority)
  - Quick check: If your benchmark shows inflated accuracy on incorrect code, which bias types should you audit for first?

## Architecture Onboarding

- **Component map:**
  - Bias Injection Module (comment-based: authority, reverse-authority, self-declared, misleading; code-transformation-based: variable renaming, illusory complexity) -> Evaluation Pipeline (Task description + code -> LLM judge with CoT -> correctness judgment) -> Robustness Metric (MAD)

- **Critical path:**
  1. Source human-written code from CodeNet, filtered to AtCoder Beginner Contest problems
  2. Strip existing comments to establish clean baseline
  3. Inject single bias type per sample
  4. Run evaluation with temperature=0.0, average over 3 trials for closed-source models
  5. Compute MAD per bias type per language

- **Design tradeoffs:**
  - Single-bias injection isolates individual effects but misses interaction effects when multiple biases co-occur
  - Comment removal preprocessing ensures fair comparison but may reduce ecological validity (real code has comments)
  - Reference-free setting reflects real constraint but excludes hybrid approaches with test generation

- **Failure signatures:**
  - Self-declared correctness: Look for judgment flips on incorrect code when "# correct code" comment added
  - Misleading task: Watch for reasoning that cites the misleading comment as evidence
  - Variable renaming: Compare accuracy when names go from semantic ("total_sum") to random 24-char strings

- **First 3 experiments:**
  1. Baseline audit: Run your current LLM judge on the paper's 200 Python problems with no bias injection; compare your base accuracy to reported ~84.7% (correct) / 63.1% (incorrect)
  2. Sensitivity probe: For 20 samples, test each bias type individually. Compute per-bias MAD. If self-declared correctness shows >10%p shift, your judge is highly vulnerable
  3. Mitigation pilot: Implement comment-stripping preprocessing + test-case generation. Measure MAD reduction. If <30% reduction, deeper architectural changes needed beyond prompting

## Open Questions the Paper Calls Out

- **Open Question 1:** Do superficial biases persist in reference-based evaluation settings, and does the presence of reference implementations mitigate or exacerbate them?
  - Basis: The Limitations section states it "remains an open question whether—and to what extent—the same forms of superficial bias identified here manifest in reference-based evaluation settings."
  - Why unresolved: The study focused exclusively on reference-free scenarios where LLM judges assess code without access to ground-truth implementations.
  - What evidence would resolve it: Replicating the bias experiments in a setting where the LLM judge is provided with a reference implementation for comparison.

- **Open Question 2:** How do language-specific stylistic conventions (e.g., Python indentation) influence LLM judge biases?
  - Basis: The Limitations section notes that the study "does not address language-specific biases," such as "Python-specific formatting practices such as indentation style or whitespace usage."
  - Why unresolved: The experimental design deliberately abstracted away from idiosyncratic stylistic conventions unique to specific programming languages.
  - What evidence would resolve it: Experiments that systematically vary language-specific stylistic elements while holding semantic correctness constant to measure their impact on evaluation scores.

- **Open Question 3:** Can the bias caused by "illusory complexity" be distinguished from bias caused simply by increased code length?
  - Basis: The Limitations section acknowledges a difficulty in "clearly distinguishing between biases originating solely from code length and those inherent to superficial biases."
  - Why unresolved: The method for generating illusory complexity (adding dummy functions) inevitably resulted in longer code, confounding the specific cause of the observed positive bias.
  - What evidence would resolve it: Experiments that control for code length while varying the complexity of the content, or vice versa, to isolate the variables.

## Limitations

- The study cannot determine whether observed effects stem from training data correlations or architectural inductive biases
- Variable renaming bias shows clear statistical significance, yet the mechanism remains speculative without ablation studies
- The misleading task bias relies on human validation of generated comments, introducing potential subjectivity
- Ecological validity is constrained by single-bias injection—real code likely contains multiple concurrent bias triggers

## Confidence

- **High confidence:** Authority and self-declared correctness bias mechanisms (clear statistical evidence, consistent across models and languages)
- **Medium confidence:** Variable renaming and illusory complexity biases (strong quantitative effects but speculative underlying mechanisms)
- **Low confidence:** Misleading task bias (human validation step introduces subjectivity; effects may not generalize beyond ABC problems)

## Next Checks

1. **Multi-bias interaction test:** Inject combined bias types (e.g., authority + variable lengthening) on 50 samples to measure synergistic effects beyond additive impacts
2. **Cross-dataset replication:** Apply bias injection to 100 samples from Codeforces Div2 problems to verify generalizability beyond ABC
3. **Mechanistic ablation:** Test variable renaming with fixed-length semantic names (24 characters each) versus random strings to isolate length effect from randomness effect