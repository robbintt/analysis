---
ver: rpa2
title: Evaluating Speech-to-Text Systems with PennSound
arxiv_id: '2504.05702'
source_url: https://arxiv.org/abs/2504.05702
tags:
- https
- speech
- whisper
- systems
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks 8 commercial and open-source speech-to-text
  systems on a random 10-hour sample from PennSound, the world's largest online poetry
  reading archive. Reference transcripts were created by trained annotators, and systems
  evaluated included AWS, Azure, Google, IBM, NVIDIA NeMo, Rev.ai, Whisper, and Whisper.cpp.
---

# Evaluating Speech-to-Text Systems with PennSound

## Quick Facts
- **arXiv ID:** 2504.05702
- **Source URL:** https://arxiv.org/abs/2504.05702
- **Reference count:** 0
- **Primary result:** 8 commercial and open-source speech-to-text systems benchmarked on PennSound poetry archive achieved WER below 15%, with Rev.ai best at 9.0% and Whisper at 9.5% (when hallucinations were suppressed).

## Executive Summary
This paper benchmarks 8 commercial and open-source speech-to-text systems on a random 10-hour sample from PennSound, the world's largest online poetry reading archive. Reference transcripts were created by trained annotators, and systems evaluated included AWS, Azure, Google, IBM, NVIDIA NeMo, Rev.ai, Whisper, and Whisper.cpp. All systems achieved word error rates below 15%, with Rev.ai performing best at 9.0% WER and Whisper showing strong performance at 9.5% WER (when hallucinations were avoided). AWS achieved the best diarization error rate at 14.5%. The results suggest that current speech-to-text technology is sufficiently accurate for effective information retrieval from audio collections like PennSound.

## Method Summary
The study evaluated 8 speech-to-text systems on a 10-hour random sample from PennSound, consisting of 100 audio clips each with ≥5 minutes of speech. Audio was preprocessed to 16 kHz mono WAV/FLAC using SoX with normalization. Reference transcripts were created by trained annotators. WER was calculated using NIST SCTK, and DER was measured for AWS, Azure, and Rev.ai. Whisper's hallucination behavior was controlled through specific runtime parameters.

## Key Results
- All 8 systems achieved WER below 15%, with Rev.ai best at 9.0% and Whisper at 9.5% (with hallucination suppression)
- AWS achieved the best diarization error rate at 14.5%
- Whisper hallucinations were successfully mitigated using `--word_timestamps=True` and `--hallucination_silence_threshold` parameters
- Commercial systems (Rev.ai, AWS, Azure, Google, IBM) outperformed open-source alternatives (NeMo, Whisper, Whisper.cpp) in overall accuracy

## Why This Works (Mechanism)

### Mechanism 1: Whisper Hallucination Control
Whisper generates spurious text during silent regions when running default configurations. Setting `--word_timestamps=True` combined with `--hallucination_silence_threshold` (values of 0.1 or 0.5 both effective) suppresses these insertions by forcing the model to align output with detected speech activity. Without suppression, FIC ranged from 27 to 1601 tokens; with options enabled, main trial showed no extraneous tokens beyond audio's end.

### Mechanism 2: WER Stability for System Ranking
WER computes normalized edit distance between reference and hypothesis transcripts via NIST's SCTK toolkit. Despite recording condition variability, aggregate WER across 100 samples produced consistent system rankings that aligned with qualitative difficulty assessments. High-WER files correlate with overlapping speech and unintelligible regions, validating that WER differences have practical consequences.

### Mechanism 3: Audio Preprocessing Normalization
SoX converts varied MP3 inputs to uniform WAV/FLAC format at 16kHz mono with `--norm=-1` peak normalization. This removes sampling rate and channel count as confounding variables while preserving sufficient fidelity for speech recognition. IBM's SNR estimates showed no clear explanatory power for WER variation, suggesting preprocessing succeeded in leveling baseline conditions.

## Foundational Learning

- **Concept: Word Error Rate (WER)**
  - Why needed here: Primary metric for all system comparisons; understanding its components (substitutions, insertions, deletions) is essential for diagnosing failure modes
  - Quick check question: If a system outputs "the cat sat" for reference "the cat sat on the mat," what is the WER? (Answer: 40% — 2 deletions out of 5 reference words = 0.4)

- **Concept: Speaker Diarization and DER**
  - Why needed here: Three systems (AWS, Azure, Rev.ai) provide speaker labels; DER quantifies how well they segment and attribute speech to speakers
  - Quick check question: If DER is 15%, what does that represent in terms of error components? (Answer: Combination of missed speech, false alarm speech, and speaker confusion relative to total speech time)

- **Concept: Hallucination in Sequence Models**
  - Why needed here: Whisper's tendency to generate plausible-but-fabricated text during silence is a deployment risk; understanding why it happens guides mitigation
  - Quick check question: Why would a speech model generate text during silence? (Answer: The language model component continues predicting probable token sequences when acoustic confidence drops, treating partial output as a prompt)

## Architecture Onboarding

- **Component map:**
  ```
  PennSound MP3s → SoX (16kHz mono normalize) → WAV/FLAC
                                            ↓
                ┌─────────────────────────────────────────┐
                │         STT Systems (8 evaluated)        │
                │  Cloud: AWS, Azure, Google, IBM, Rev.ai  │
                │  Local: NeMo, Whisper, Whisper.cpp       │
                └─────────────────────────────────────────┘
                          ↓                    ↓
                    Raw transcripts     Speaker labels (3 systems)
                          ↓                    ↓
                SCTK (WER scoring)    dscore (DER scoring)
  ```

- **Critical path:**
  1. Audio extraction from long recordings using SAD (Speech Activity Detector) to identify 5+ minute speech segments
  2. Human transcription by trained annotators using standardized guidelines (ground truth)
  3. Parallel STT processing across all systems with documented parameters
  4. Alignment and scoring via SCTK (WER) and dscore (DER for diarization-capable systems)

- **Design tradeoffs:**
  - **Rev.ai (9.0% WER)**: Best accuracy, but commercial API with per-minute costs and data privacy considerations
  - **Whisper (9.5% WER)**: Top open-source, requires hallucination parameter tuning, non-deterministic outputs across runs
  - **AWS**: Best diarization (14.52% DER), strong overall performer, commercial
  - **IBM (14.4% WER)**: Lowest accuracy in this benchmark, may have different strengths not captured by WER

- **Failure signatures:**
  - High IER (Insertion Error Rate) without corresponding WER increase → likely hallucination issue in Whisper
  - Sharp WER spikes across all systems on same file → check for overlapping speech or unintelligible regions
  - Inconsistent results across Whisper runs → inherent non-determinism; average multiple runs or fix random seed

- **First 3 experiments:**
  1. **Baseline replication**: Run Whisper-large-v3 on 5 sample files with and without `--hallucination_silence_threshold`; measure FIC and IER to calibrate your own threshold
  2. **Diarization stress test**: Select 3 multi-speaker recordings from your collection; compare AWS, Azure, and Rev.ai DER scores to determine if diarization quality is acceptable for speaker-attributed search
  3. **Error pattern analysis**: On your 5 highest-WER files, manually categorize errors (overlapping speech, accents, background noise, poetry-specific vocabulary) to determine if additional preprocessing or domain adaptation is needed before full-collection transcription

## Open Questions the Paper Calls Out

- **Question:** Can the automatic transcriptions generated for the 10-hour sample effectively support text-based document retrieval when scaled to the full 6,000-hour PennSound collection?
  - **Basis in paper:** The Future Work section states the next step is to "create automatic transcriptions of the full collection, and to implement a web-based front end for searching."
  - **Why unresolved:** The current study only evaluated transcription accuracy (WER) on a sample; it did not test the downstream utility of these transcripts for search and retrieval tasks on the entire archive.
  - **What evidence would resolve it:** Successful implementation of the planned search interface and user evaluation of retrieval performance across the full corpus.

- **Question:** How does time-mediated evaluation of word recognition compare to the single-segment scoring method used in the current analysis?
  - **Basis in paper:** The authors note in Future Work that they "will also evaluate time-mediated evaluation of word recognition" because current scoring eliminated insertion/deletion errors caused by timestamp mismatches.
  - **Why unresolved:** Reference transcripts were flattened into single segments for this study, removing the need for precise timestamp alignment during scoring.
  - **What evidence would resolve it:** Re-scoring the existing system outputs against the reference transcripts using metrics that penalize words recognized correctly but placed in the wrong time segment.

- **Question:** What are the optimal runtime configurations for Whisper to consistently minimize hallucinations without sacrificing accuracy in diverse audio conditions?
  - **Basis in paper:** The Discussion highlights that Whisper is an attractive system "only if hallucinations are avoided" and requires specific options (`word_timestamps`, `hallucination_silence_threshold`) to function safely.
  - **Why unresolved:** The paper notes that hallucinations fluctuate non-deterministically and that the "precise value had no obvious effect," suggesting the interaction between parameters and hallucination rates is not fully understood.
  - **What evidence would resolve it:** A systematic ablation study of Whisper parameters on the PennSound dataset to quantify the trade-offs between specific threshold settings, insertion error rates, and word error rates.

## Limitations

- The 10-hour sample size may not capture all failure modes present in PennSound's full 6,000+ hour collection
- The study focuses exclusively on poetry recordings, which have distinct acoustic properties that may not generalize to spontaneous speech or noisy environments
- Reliance on single reference transcripts introduces uncertainty in scoring, particularly for highly unintelligible regions

## Confidence

- **High confidence**: System ranking stability; Whisper hallucination mitigation mechanism; preprocessing normalization effectiveness
- **Medium confidence**: WER as sufficient metric for information retrieval; generalizability to non-poetry domains
- **Low confidence**: Long-term commercial API performance; impact of poetry-specific vocabulary on general-purpose system accuracy

## Next Checks

1. **Domain transfer validation**: Test top 3 systems (Rev.ai, Whisper, AWS) on a 2-hour sample of non-poetry speech (lectures, interviews, spontaneous conversation) from PennSound to quantify performance degradation outside poetry domain
2. **Diarization precision validation**: For AWS/Azure/Rev.ai, manually verify speaker attribution accuracy on 10 multi-speaker poetry recordings to determine if DER 14-16% translates to acceptable speaker attribution for search/browse applications
3. **Whisper runtime consistency validation**: Execute Whisper-large-v3 on 20 random files with `--word_timestamps True` and `--hallucination_silence_threshold 0.1` across 3 independent runs each; measure standard deviation in WER to quantify non-determinism impact on deployment planning