---
ver: rpa2
title: 'PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement'
arxiv_id: '2512.04532'
source_url: https://arxiv.org/abs/2512.04532
tags:
- video
- motion
- physical
- modeling
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling Video Large Language
  Models (Video LLMs) to better understand and reason about physical dynamics in videos.
  The key limitation is that current Video LLMs rely primarily on appearance-based
  matching and fail to model physical motion, leading to poor performance on tasks
  requiring understanding of acceleration, deceleration, and other physical behaviors.
---

# PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement

## Quick Facts
- arXiv ID: 2512.04532
- Source URL: https://arxiv.org/abs/2512.04532
- Reference count: 40
- This paper addresses the challenge of enabling Video LLMs to understand physical dynamics by disentangling motion from appearance and modeling with Neural ODEs.

## Executive Summary
PhyVLLM addresses the fundamental limitation of current Video LLMs that rely on appearance-based matching and fail to model physical motion, leading to poor performance on tasks requiring understanding of acceleration, deceleration, and other physical behaviors. The proposed solution explicitly incorporates physical motion modeling through a dual-branch encoder that disentangles visual appearance from object motion, a Neural ODE module that models physical dynamics over time, and a projection mechanism that enables a pretrained LLM to leverage these motion-aware representations. Experiments show PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning tasks (PhyBench benchmark) and general video understanding tasks.

## Method Summary
PhyVLLM processes videos through a dual-branch encoder that shares a ViT backbone but splits into separate appearance and motion processing streams. The motion branch uses temporal attention and is trained with an HSIC-based loss to minimize correlation with appearance features, ensuring true disentanglement. Physical dynamics are modeled using a Neural ODE that learns continuous motion evolution in latent space, trained via self-supervised prediction of future motion states. The resulting motion-aware representations are projected into the LLM's token space using special tokens (<motion>, <appearance>) and the LLM is adapted via LoRA while keeping the backbone frozen. The model is trained on instruction-tuning datasets and evaluated on synthetic PhyBench for physical reasoning.

## Key Results
- Achieves 79% accuracy on PhyBench physical reasoning benchmark versus 23% baseline
- Significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks
- Demonstrates the effectiveness of incorporating explicit physical modeling through Neural ODEs
- Shows self-supervised training can learn physical dynamics without costly annotations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disentangling motion from appearance prevents the model from relying on visual texture to infer dynamics, forcing the learning of actual physical trajectories.
- **Mechanism:** A dual-branch encoder processes the video: one branch (Appearance Encoder) extracts static features via a shallow MLP, while the other (Motion Encoder) uses temporal attention. An HSIC-based loss minimizes mutual information between the two feature sets, ensuring the motion branch cannot access appearance statistics.
- **Core assumption:** The model assumes that motion cues can be mathematically separated from appearance features and that minimizing correlation (via HSIC) results in a representation purely of physics, not texture.
- **Evidence anchors:**
  - [abstract] "PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder."
  - [section 3.2] "We define the appearance disentanglement loss as Lapp = HSIC(Fmot, Fapp) which enforces independence."
  - [corpus] Related work (VLIPP) confirms that standard models often fail to produce physically plausible video due to an inherent focus on appearance, supporting the need for explicit separation.
- **Break condition:** If the visual texture is the only reliable cue for motion (e.g., motion blur is the only indicator of speed), the disentanglement constraint may strip the motion encoder of necessary signal, leading to random guessing.

### Mechanism 2
- **Claim:** Modeling dynamics as a continuous differential equation (Neural ODE) enables the capture of latent physical quantities (like acceleration) that discrete frame differences miss.
- **Mechanism:** Instead of processing frames as discrete steps $t, t+1$, the model projects motion features into a latent state $z(t)$. A neural network $F_\theta$ learns the derivative $dz/dt$. By solving this integral, the model predicts future states $z(t+\tau)$, effectively learning the underlying "physics engine" of the video.
- **Core assumption:** Assumption: The latent motion features map linearly to a physical state space where temporal evolution follows a learnable, continuous differential function.
- **Evidence anchors:**
  - [abstract] "To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module."
  - [section 3.3] "Fθ learns to approximate this process... implicitly encoding both position-like and velocity-like information."
  - [corpus] "Physics-Guided Motion Loss" (neighbor paper) suggests frequency-domain priors improve motion; PhyVLLM instead uses continuous-time ODEs as its prior.
- **Break condition:** If the training data contains discontinuous cuts or non-physical edits, the ODE solver may fail to converge or learn a chaotic derivative function that cannot predict future states.

### Mechanism 3
- **Claim:** Projecting physics-aware representations into the LLM's token space allows the reasoning engine (LLM) to leverage continuous dynamics without altering its core weights.
- **Mechanism:** The output of the Neural ODE (future trajectory/predicted state) and the appearance features are projected via linear layers into the LLM's embedding dimension. These are injected at specific anchor tokens (e.g., `<motion>`). The LLM is then fine-tuned via LoRA, keeping the backbone frozen.
- **Core assumption:** The pretrained LLM possesses sufficient reasoning capacity to interpret abstract vector embeddings as "physical states" when presented in this specific token format.
- **Evidence anchors:**
  - [abstract] "The resulting motion-aware representations are projected into the token space of a pretrained LLM."
  - [section 3.4] "We insert the projected features at designated anchor positions using special tokens <motion> and <appearance>."
  - [corpus] Weak/missing direct corpus evidence for this specific token injection strategy in related work; this appears to be a novel contribution of this architecture.
- **Break condition:** If the projection layers are not aligned correctly, the LLM may treat the motion tokens as noise, ignoring them in favor of language priors or appearance tokens.

## Foundational Learning

- **Concept: Neural Ordinary Differential Equations (Neural ODEs)**
  - **Why needed here:** Standard Transformers (discrete steps) struggle with "continuous" physics (e.g., determining if a car is accelerating requires understanding rates of change between frames). Neural ODEs provide the mathematical framework for modeling these continuous derivatives.
  - **Quick check question:** Can you explain why an ODE solver (like Runge-Kutta) is different from a standard Recurrent Neural Network (RNN) cell in how it handles time steps?

- **Concept: Hilbert-Schmidt Independence Criterion (HSIC)**
  - **Why needed here:** This is the mathematical tool used to "disentangle" features. You need to understand that this measures statistical independence (correlation) between two variable sets to understand how the model isolates motion from texture.
  - **Quick check question:** If two variables are independent, does a low HSIC score guarantee they are physically unrelated, or just statistically uncorrelated?

- **Concept: Self-Supervised Prediction**
  - **Why needed here:** The paper notes that "collecting accurate annotations for physical attributes is costly." The model learns physics not by being told "this is acceleration," but by trying to predict the future frame features of the video.
  - **Quick check question:** How does the loss function $L_{phys}$ penalize the model if it fails to predict the future state of the object?

## Architecture Onboarding

- **Component map:**
  1. Input: Video Frames
  2. Shared Backbone: ViT Base (Patch extraction)
  3. Dual Branch: Appearance Encoder (MLP) || Motion Encoder (Transformer + Temporal Attention)
  4. Physics Core: Latent Projection → Neural ODE Solver (F_θ) → Readout Network
  5. LLM Interface: Linear Projectors → Token Embeddings (<motion>, <appearance>)
  6. Reasoning Engine: Frozen LLM (e.g., InternLM-7B) + LoRA adapters

- **Critical path:** The **Motion Encoder → Neural ODE → Physics Loss (L_phys)** path is the most critical. If the ODE cannot predict the next feature state, the model has failed to learn physics.

- **Design tradeoffs:**
  - **Frozen vs. Fine-tuned LLM:** The paper freezes the LLM backbone and uses LoRA. This trades potential maximum reasoning capability for efficiency and preservation of general multimodal knowledge.
  - **Synthetic vs. Real Data:** The authors introduce PhyBench (synthetic) because real-world physical annotations are too hard to get. This ensures clean evaluation but raises questions about "sim-to-real" gap.

- **Failure signatures:**
  - **Uniform Motion Bias:** The model defaults to predicting "uniform motion" for everything (seen in Table 1 for baselines). This indicates the ODE learned a linear derivative (dz/dt = constant) rather than complex acceleration.
  - **High Disentanglement Loss:** If Lapp (HSIC) does not decrease, the "motion" tokens still contain texture information, contaminating the physical reasoning.

- **First 3 experiments:**
  1. **Disentanglement Validation:** Visualize the t-SNE of features from the Motion Encoder vs. Appearance Encoder. Do they cluster differently?
  2. **Trajectory Prediction Check:** Run the ODE module in isolation. Feed it frames T0…T8 and ask it to generate features for T9…T11. Compare similarity (as in Figure 3).
  3. **Ablation on Losses:** Retrain with Lphys = 0 (no physics supervision). The model should perform similarly to standard Video LLMs on PhyBench (approx. 23% accuracy vs the full 79%).

## Open Questions the Paper Calls Out

- **Question:** Can the proposed framework scale to effectively reason about compound physical interactions (e.g., rotations, multi-body collisions) currently excluded from the PhyBench benchmark?
- **Basis in paper:** [explicit] The authors state that the PhyBench dataset "selects relatively simple physical interaction scenarios" and limits motion types to five specific categories. They explicitly note that the generation platform can simulate "more challenging reasoning environments" like rotation and compound collisions, but these are not included in the current evaluation.
- **Why unresolved:** The current evaluation is restricted to "canonical motion patterns" and "single type of motion" per video. It is unclear if the Neural ODE module, which models the evolution of a latent state, can capture the combinatorial complexity of interacting objects or rotational dynamics without architectural modifications.
- **What evidence would resolve it:** Evaluation results on an extended benchmark including videos with compound collisions, rotational dynamics, and multi-object interactions to test the scalability of the physics-guided modeling.

## Limitations

- **Synthetic evaluation bias:** The PhyBench benchmark is entirely synthetic, creating an unknown sim-to-real transfer gap that may limit real-world applicability.
- **Architecture specificity:** The highly specialized dual-branch design with HSIC disentanglement may not generalize well to diverse real-world video domains where physical dynamics are less explicit.
- **Black-box physical reasoning:** The model learns physical dynamics through self-supervised prediction rather than explicit physical laws, potentially learning statistically accurate but physically incorrect representations.

## Confidence

- **High confidence:** The disentanglement mechanism (HSIC loss, dual-branch architecture) is mathematically sound and directly addresses the stated problem of appearance-motion correlation in video models.
- **Medium confidence:** The Neural ODE approach for continuous dynamics modeling is theoretically valid, but the actual physical quantities learned (acceleration vs. just trajectory prediction) are not explicitly verified.
- **Medium confidence:** The performance gains on PhyBench are significant and well-documented, but the synthetic nature of the benchmark limits generalizability claims.

## Next Checks

1. **Real-world physical reasoning test:** Apply PhyVLLM to real video datasets with physical dynamics (e.g., sports videos, vehicle tracking) and compare performance against baselines on tasks requiring motion understanding beyond synthetic trajectories.

2. **Physical quantity extraction:** After the Neural ODE module, attempt to extract explicit physical quantities (velocity vectors, acceleration values) from the latent state z(t) and verify they correspond to ground-truth physical measurements in controlled experiments.

3. **Ablation on physical losses:** Systematically vary λ_phys and λ_app coefficients across multiple orders of magnitude to identify the optimal balance and confirm that both disentanglement and physics modeling are necessary for the performance gains, not just one component.