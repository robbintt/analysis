---
ver: rpa2
title: 'Beyond speculation: Measuring the growing presence of LLM-generated texts
  in multilingual disinformation'
arxiv_id: '2503.23242'
source_url: https://arxiv.org/abs/2503.23242
tags:
- texts
- machine-generated
- datasets
- dataset
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first empirical evidence of large language
  model (LLM)-generated content in real-world multilingual disinformation datasets.
  The authors developed a detection framework using two fine-tuned multilingual detectors,
  GemmaMultiDomain and GemmaGenAI, which were evaluated on benchmark datasets showing
  precision of 0.94-0.9985 and true positive rates of 0.5-0.74.
---

# Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation

## Quick Facts
- arXiv ID: 2503.23242
- Source URL: https://arxiv.org/abs/2503.23242
- Reference count: 24
- First empirical evidence of LLM-generated content in real-world multilingual disinformation datasets

## Executive Summary
This study provides the first empirical evidence of large language model (LLM)-generated content in real-world multilingual disinformation datasets. The authors developed a detection framework using two fine-tuned multilingual detectors, Gemma_MultiDomain and Gemma_GenAI, which were evaluated on benchmark datasets showing precision of 0.94-0.9985 and true positive rates of 0.5-0.74. The framework was applied to four real-world datasets spanning social media posts, election articles, and conflict-related news, revealing increasing prevalence of LLM-generated content over time and significant variation across languages and platforms.

## Method Summary
The authors developed a multilingual LLM detection framework using two fine-tuned detectors: Gemma_MultiDomain and Gemma_GenAI. These models were trained on diverse multilingual datasets and evaluated on benchmark datasets, achieving high precision (0.94-0.9985) and moderate true positive rates (0.5-0.74). The framework was then applied to four real-world multilingual disinformation datasets: MultiClaim (fact-checked social media posts), FakeNews (English election articles), USC-X (election-related Twitter posts), and FIGNEWS (news media narratives on the Israel-Gaza conflict). The study tracked LLM-generated content prevalence across time periods (2021-2023), languages, and platforms.

## Key Results
- LLM-generated content prevalence increased from 0.93% in 2021 to 1.85% in 2023 in MultiClaim dataset
- Significant language variation observed: Polish (4.7%) and French (4.2%) showed highest LLM presence
- Machine-generated content detected in both "true" and "false" labeled texts across all four datasets

## Why This Works (Mechanism)
The detection framework leverages fine-tuned multilingual models that capture language-specific patterns and anomalies characteristic of LLM generation. By training on diverse multilingual datasets, the models learn to identify subtle linguistic markers that distinguish machine-generated text from human-written content across different languages and domains.

## Foundational Learning
- Multilingual fine-tuning: why needed - enables detection across language barriers; quick check - test performance on held-out languages
- Domain adaptation: why needed - disinformation patterns vary by topic; quick check - compare detection rates across different content types
- Temporal analysis: why needed - LLM usage patterns evolve; quick check - track detection rates across different time periods
- Precision-recall tradeoff: why needed - balance false positives vs false negatives; quick check - evaluate at different confidence thresholds
- Cross-platform variation: why needed - different platforms have different linguistic patterns; quick check - compare detection rates across platforms
- Labeled vs unlabeled data: why needed - ground truth essential for validation; quick check - verify annotation consistency

## Architecture Onboarding
Component map: Input texts -> Preprocessing -> Gemma_MultiDomain detector -> Gemma_GenAI detector -> Aggregation -> Output scores
Critical path: Text input → Feature extraction → Detector inference → Confidence scoring → Classification decision
Design tradeoffs: Multilingual coverage vs detection accuracy, real-time processing vs comprehensive analysis, false positive tolerance vs false negative risk
Failure signatures: Low confidence scores across languages, inconsistent detection across similar texts, high false positive rates for certain linguistic patterns
First 3 experiments:
1. Test framework on synthetic LLM-generated texts with known provenance
2. Compare detection performance across different confidence threshold settings
3. Evaluate framework on texts known to contain human editing of LLM-generated content

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset bias toward "popular" multilingual content may exclude less visible disinformation campaigns
- Cannot distinguish legitimate LLM use (translation, editing) from malicious generation
- Detection framework may have language-specific biases affecting cross-language comparisons

## Confidence
- **High confidence** in technical methodology and detection framework performance metrics
- **Medium confidence** in temporal trends and platform/language variation findings
- **Medium confidence** in conclusions about LLM misuse for disinformation

## Next Checks
1. Cross-validate detection results using an independent LLM detection method on a subset of the same multilingual datasets
2. Conduct linguistic analysis comparing detected LLM-generated texts across languages to identify language-specific evasion techniques
3. Test framework on deliberately crafted disinformation samples employing known LLM evasion strategies (paraphrasing, human editing, hybrid generation)