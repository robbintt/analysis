---
ver: rpa2
title: 'VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning'
arxiv_id: '2510.01444'
source_url: https://arxiv.org/abs/2510.01444
tags:
- arxiv
- uncertainty
- reasoning
- branch
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of exploration in multimodal
  reinforcement learning with verifiable rewards (RLVR), where current methods treat
  visual inputs as deterministic and fail to distinguish between perceptual ambiguity
  and reasoning uncertainty. The authors propose DUPL, a dual-uncertainty guided policy
  learning approach that quantifies both perceptual uncertainty (via symmetric KL
  divergence between raw and noisy visual branches) and output uncertainty (via policy
  entropy).
---

# VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning

## Quick Facts
- arXiv ID: 2510.01444
- Source URL: https://arxiv.org/abs/2510.01444
- Reference count: 28
- Primary result: DUPL achieves up to 11.2% accuracy gains on visual math tasks and 7.1% on general-domain reasoning tasks

## Executive Summary
DUPL addresses the challenge of exploration in multimodal reinforcement learning with verifiable rewards (RLVR) by treating visual inputs as stochastic contexts rather than deterministic. The method quantifies both perceptual uncertainty (via symmetric KL divergence between raw and noisy visual branches) and output uncertainty (via policy entropy), integrating these into an uncertainty-driven feedback loop to shape advantage functions for targeted exploration. Implemented on Qwen2.5-VL models, DUPL consistently outperforms GRPO baselines across six benchmarks, demonstrating that grounding exploration in visual input uncertainty is an effective strategy for improving multimodal reasoning.

## Method Summary
DUPL builds on GRPO within the EasyR1 framework, using a dual-branch forward pass with raw and perturbed images. Perceptual uncertainty is quantified as symmetric KL divergence between policy distributions over the two branches, while output uncertainty is measured as policy entropy. These uncertainties are scaled and added to advantage functions to guide exploration, with dynamic branch prioritization that favors the noisy branch early in training and transitions to the raw branch for stable optimization. The method employs standard augmentations (flips, rotations, color jitter, Gaussian noise) and trains for 200 steps with specific hyperparameters.

## Key Results
- Achieves 11.2% accuracy improvement on visual math tasks compared to GRPO
- Delivers 7.1% improvement on general-domain reasoning tasks
- Consistently outperforms GRPO baselines across six benchmarks including MathVerse, MathVista, WeMath, HallusionBench, ChartQA, and LogicVista

## Why This Works (Mechanism)

### Mechanism 1
Symmetric KL divergence between raw and perturbed visual branches provides stable perceptual uncertainty signals. The symmetric KL (½[D_KL(p||q) + D_KL(q||q)]) quantifies policy sensitivity to visual perturbations, guiding exploration toward perceptually ambiguous states. This differs from forward KL which causes training instability with unbounded uncertainty growth.

### Mechanism 2
Output uncertainty (policy entropy) complements perceptual uncertainty by maintaining stochasticity in action space. Token-level entropy is computed for both branches and clipped by advantage magnitude before being added to shape the advantage function. This encourages exploration in textual output space independent of visual ambiguity.

### Mechanism 3
Dynamic branch prioritization enables curriculum learning from exploration to exploitation. Early training favors noisy branch advantages for broad exploration, while later training shifts to raw branch advantages for stable optimization. The decay schedule p_noi(s) = max(0, 1 - s/s_total) linearly transitions from 1 to 0 probability of using the noisy branch.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: DUPL builds directly on GRPO's advantage computation and clipped surrogate objective
  - Quick check question: Can you explain why GRPO normalizes rewards within a group before computing advantages?

- **Concept: Symmetric KL Divergence**
  - Why needed here: The paper shows forward KL causes training instability while symmetric KL remains stable
  - Quick check question: Why would forward KL(p||q) alone cause different training dynamics than symmetric KL when measuring perceptual uncertainty?

- **Concept: Stop-gradient Operators**
  - Why needed here: Guidance signals use stopgrad(u) to modulate update magnitude without affecting gradient propagation
  - Quick check question: What would happen if stopgrad were removed from the guidance signal computation?

## Architecture Onboarding

- **Component map:**
  Input (x_text, x_image)
       │
       ├─► Raw Branch ─► π_θ(·|x) ─► p ─┐
       │                                ├─► Symmetric KL ─► u_per ─► g_per
       └─► Noisy Branch ─► π_θ(·|x') ─► q ─┘
              (T: augment)              │
                                        ├─► Entropy ─► u_out ─► g_out
                                        │
                                   Rewards ─► A_raw, A_noi
                                        │
                                   Â_raw = A_raw + g_out^raw
                                   Â_noi = A_noi + g_out^noi + g_per
                                        │
                              Branch Selector (Bernoulli(p_noi))
                                        │
                                   Policy Update (GRPO objective)

- **Critical path:** The dual-branch forward pass is the computational bottleneck, requiring forward passes on both raw and perturbed images plus token-level KL divergence computation.

- **Design tradeoffs:**
  - Noise level (σ=0.4): Too low (σ=0.2) provides insufficient exploration; too high (σ=0.8) introduces excessive variance
  - Scaling factors (α_p=1.0, β_p=2.0, α_o=0.4, β_o=2.0): Control magnitude of uncertainty bonuses relative to base advantage
  - Perturbation composition: Standard augmentations probe different types of perceptual ambiguity

- **Failure signatures:**
  - Perceptual uncertainty growing unbounded → likely using forward KL instead of symmetric KL
  - Training accuracy plateauing early → p_noi decay may be too fast, or α_p too low
  - High variance in training rewards → noise level σ too high or β_p too permissive
  - No improvement over GRPO baseline → check guidance signals are actually being added to advantages

- **First 3 experiments:**
  1. Run DUPL with σ=0 (no noise) and verify it reduces to standard GRPO performance
  2. Compare σ ∈ {0.2, 0.4, 0.8} on validation set to verify 0.4 is optimal
  3. Remove perceptual uncertainty (g_per=0), then output uncertainty (g_out=0), then both, comparing degradation magnitudes

## Open Questions the Paper Calls Out

### Open Question 1
Can extending the dual-uncertainty paradigm to incorporate textual input perturbations improve multimodal reasoning, and how would textual and visual perturbations interact? The current implementation only perturbs visual inputs; extending to text presents a worthy direction.

### Open Question 2
Can learnable or task-adaptive perturbation generators outperform the fixed augmentation transformations used in DUPL? Standard augmentations may not expose task-specific perceptual ambiguities; learnable perturbations could target model-specific weaknesses more effectively.

### Open Question 3
How does DUPL generalize to dynamic visual modalities such as video, where temporal coherence adds additional complexity to perceptual uncertainty estimation? Video introduces temporal dependencies that complicate perturbation strategies.

### Open Question 4
Does dual-uncertainty guided policy learning transfer effectively to other MLLM architectures beyond Qwen2.5-VL, or is the effectiveness architecture-dependent? All experiments use Qwen2.5-VL, leaving cross-architecture robustness untested.

## Limitations

- The method currently treats textual input as fixed, limiting the scope of uncertainty-guided exploration to visual modalities only
- Effectiveness may be architecture-dependent, as all experiments use Qwen2.5-VL models without cross-architecture validation
- Training only spans 200 steps, raising questions about long-term stability and whether improvements persist over extended training

## Confidence

- **High Confidence**: Symmetric KL divergence reliably captures meaningful perceptual ambiguity versus forward KL instability
- **Medium Confidence**: Improvement magnitudes are compelling but require replication across different model architectures
- **Low Confidence**: Claim of consistent superiority across all six benchmarks needs more stress-testing on out-of-distribution data

## Next Checks

1. Implement DUPL on LLaVA-Next or GPT-4V and verify uncertainty-guided advantage mechanism maintains performance improvements
2. Systematically sweep noise levels (σ ∈ {0.2, 0.4, 0.6, 0.8}) and plot perceptual uncertainty magnitude vs. accuracy
3. Extend training to 500+ steps and monitor whether uncertainty-guided exploration prevents early plateauing or causes eventual divergence