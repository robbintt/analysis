---
ver: rpa2
title: 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards'
arxiv_id: '2510.08529'
source_url: https://arxiv.org/abs/2510.08529
tags:
- arxiv
- comas
- uni00000013
- uni00000048
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Co-Evolving Multi-Agent Systems (CoMAS),
  a novel framework for enabling LLM-based agents to improve autonomously through
  inter-agent interaction without external supervision. CoMAS uses three core components:
  interaction (collaborative solution proposal, evaluation, and scoring), reward formulation
  (LLM-as-a-judge mechanism deriving intrinsic rewards from discussion dynamics),
  and policy optimization (REINFORCE++ algorithm updating agent policies).'
---

# CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards

## Quick Facts
- arXiv ID: 2510.08529
- Source URL: https://arxiv.org/abs/2510.08529
- Reference count: 40
- Agents improve autonomously through inter-agent interaction without external supervision

## Executive Summary
CoMAS introduces a novel framework for enabling LLM-based agents to evolve autonomously through inter-agent interaction. The framework leverages interaction rewards derived from discussion dynamics to guide agent policy optimization, eliminating the need for external supervision. Through a combination of collaborative solution proposal, evaluation, and scoring mechanisms, agents can collectively improve their performance across various benchmarks. The approach demonstrates significant performance gains over untrained agents while establishing a new paradigm for self-evolving multi-agent systems.

## Method Summary
The CoMAS framework implements a three-stage process for agent self-evolution: interaction, reward formulation, and policy optimization. During interaction, agents collaboratively propose solutions, evaluate each other's contributions, and score the overall discussion quality. The reward formulation stage uses an LLM-as-a-judge mechanism to derive intrinsic rewards from these interaction dynamics, capturing the quality and effectiveness of agent collaborations. Finally, the REINFORCE++ algorithm updates agent policies based on these interaction-based rewards, enabling continuous improvement without external supervision. The framework supports multi-stage discussions and can scale with increasing numbers of diverse agents.

## Key Results
- CoMAS consistently outperforms untrained agents across multiple benchmarks
- Achieves absolute performance gains up to 2.20%, 3.66%, 19.80%, and 6.10% over baselines
- Ablation studies confirm interaction-based rewards prevent training collapse and reward hacking
- Framework shows promising scalability with increasing agent number and diversity

## Why This Works (Mechanism)
CoMAS leverages the collective intelligence of multiple agents through structured interaction mechanisms. By having agents evaluate and score each other's contributions, the system creates a self-reinforcing feedback loop where agents learn to improve their collaborative capabilities. The LLM-as-a-judge mechanism provides a sophisticated reward signal that captures nuanced aspects of discussion quality and solution effectiveness, going beyond simple task completion metrics. This approach enables agents to develop more refined communication strategies and problem-solving approaches through repeated interactions, leading to emergent behaviors that wouldn't be possible with individual agent training alone.

## Foundational Learning
- **Interaction Rewards**: Why needed - Provides intrinsic motivation for agents to improve collaboration without external supervision; Quick check - Verify reward signal correlates with actual performance improvement
- **LLM-as-a-Judge**: Why needed - Enables sophisticated evaluation of discussion quality and solution effectiveness; Quick check - Ensure judge consistency across different agent combinations
- **REINFORCE++ Algorithm**: Why needed - Optimizes agent policies based on interaction rewards while preventing collapse; Quick check - Monitor policy stability during training

## Architecture Onboarding

**Component Map**: Interaction Stage -> Reward Formulation -> Policy Optimization -> Agent Policy Update

**Critical Path**: The interaction stage serves as the foundation, generating the data necessary for reward formulation. The reward formulation stage then processes this interaction data to create meaningful signals, which are finally used in the policy optimization stage to update agent behaviors.

**Design Tradeoffs**: The framework balances between exploration (diverse agent interactions) and exploitation (focusing on successful collaboration patterns). The LLM-as-a-judge approach trades computational efficiency for more nuanced reward signals compared to simpler heuristic-based methods.

**Failure Signatures**: Training collapse occurs when agents stop learning meaningful collaboration patterns. Reward hacking manifests when agents manipulate the reward signal rather than genuinely improving. Performance plateaus indicate the framework may need additional diversity or complexity.

**First 3 Experiments**: 
1. Single-agent baseline comparison to establish baseline performance
2. Two-agent interaction to verify basic collaborative improvement
3. Multi-stage discussion with increasing agent numbers to test scalability

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Scalability constraints with larger agent pools may limit practical deployment
- Reward mechanism vulnerability to sophisticated manipulation attempts
- Domain generalization uncertainty across diverse problem types

## Confidence
- High: Core methodology of interaction-based rewards is technically sound
- Medium: Performance improvements over baselines are likely valid within tested benchmarks
- Low: Claims about preventing training collapse require further empirical validation

## Next Checks
1. Design adversarial agents to test the robustness of the LLM-as-a-judge reward mechanism
2. Apply CoMAS framework to three distinctly different domains to validate generalization
3. Conduct extended training runs (10x duration) to evaluate long-term stability