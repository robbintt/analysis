---
ver: rpa2
title: Why Trust in AI May Be Inevitable
arxiv_id: '2502.20701'
source_url: https://arxiv.org/abs/2502.20701
tags:
- knowledge
- explanation
- explainer
- trust
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining AI systems' decisions
  to human users. The authors model explanation as a search process through knowledge
  networks, where an explainer must find paths between shared concepts and the concept
  to be explained within finite time.
---

# Why Trust in AI May Be Inevitable

## Quick Facts
- arXiv ID: 2502.20701
- Source URL: https://arxiv.org/abs/2502.20701
- Reference count: 40
- One-line primary result: Explanation can fail even under theoretically ideal conditions due to finite time constraints, making trust a necessary but imperfect substitute.

## Executive Summary
This paper addresses why explanation of AI systems' decisions to human users can fail even when ideal conditions seem to be met. The authors model explanation as a search process through knowledge networks, showing that successful explanation requires not just shared knowledge but also finding the connection path within finite time. They demonstrate that it can be rational to terminate explanation attempts before discovering existing shared knowledge, especially when knowledge networks are large or overlap is small. This creates conditions where trust becomes a necessary substitute for explanation, with important implications for human-AI interaction as AI systems become more sophisticated at generating superficially compelling but potentially spurious explanations.

## Method Summary
The authors formalize explanation as a sequential search process through an Explainer's knowledge network to find nodes that overlap with an Explainee's network. They use analytical derivations based on Negative Hypergeometric distributions to calculate expected search times and Bayesian updating to model how beliefs about shared knowledge evolve during failed search attempts. The model assumes complete graphs for knowledge networks and explores how search termination decisions depend on expected benefits declining over time. Simulations compare different prior distributions (uniform vs. truncated normal) to demonstrate how belief dynamics affect explanation outcomes.

## Key Results
- Explanation can fail even when shared knowledge exists, actors are rational and motivated, and communication is perfect
- Rational termination of explanation attempts can occur before discovering existing shared knowledge
- Trust functions as a necessary substitute when explanation is impossible, but prevents knowledge integration and creates risks of misplaced trust

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation can fail even when shared knowledge exists, actors are rational and motivated, and communication is perfect.
- Mechanism: Explanation is formalized as a sequential search through the Explainer's knowledge network R to find nodes that also exist in the Explainee's network E (the overlap set K). Success requires discovering a bridging node connected to the target concept within finite time. The expected time to successful explanation is E(T) = N_R / (N_K + 1), meaning larger knowledge networks or smaller overlap sets increase search duration.
- Core assumption: Knowledge elements can be represented as nodes in a network with edges representing coherence relationships; the Explainer can only search locally through connected nodes.
- Evidence anchors:
  - [abstract] "explanation can fail even under theoretically ideal conditions - when actors are rational, honest, motivated, can communicate perfectly, and possess overlapping knowledge"
  - [section 4.1] Equation (2) shows expected time E(T) = N_R / (N_K + 1), establishing that "below a critical mass of shared knowledge, the explanation process is time-consuming"
  - [corpus] Weak corpus linkage; neighboring papers address explanation evaluation and trust but not the formal search failure mechanism.
- Break condition: If the overlap set K is empty (no shared knowledge), or if time constraints are relaxed to infinity, the failure mode disappears.

### Mechanism 2
- Claim: It can be rational to terminate explanation attempts before discovering existing shared knowledge.
- Mechanism: After each unsuccessful search step t, the Explainer updates beliefs about the size of K using Bayesian updating. The expected benefit of continuing E(B_t) = B × μ_Kt / (N_R - t) generally decreases over time. When E(B_t) falls below the cost c(t), rational termination occurs—even if K is non-empty.
- Core assumption: Explainer and Explainee have aligned incentives; the size N_R is known; prior beliefs about K follow a distribution that can be updated.
- Evidence anchors:
  - [abstract] "it can therefore be rational to cease attempts at explanation before the shared knowledge is discovered"
  - [section 4.2] Equations (5)-(6) show belief updating and expected benefit decline; Figure 1 demonstrates E(B_t) decreasing over time under uniform priors
  - [corpus] No direct corpus support for the Bayesian termination mechanism.
- Break condition: If prior variance relative to mean is very low (high confidence), E(B_t) may initially increase before declining, potentially extending search duration.

### Mechanism 3
- Claim: Trust functions as a necessary substitute when explanation is impossible, but is inferior because it prevents knowledge integration.
- Mechanism: When explanation terminates unsuccessfully, the Explainee must either reject the target proposition or accept it based on trust in the Explainer's competence or benevolence. Unlike successful explanation, trust-based acceptance does not create new connections in the Explainee's knowledge network, limiting future application, adaptation, and transmission of the knowledge.
- Core assumption: Trust and explanation are substitutable mechanisms for accepting propositions; knowledge integration requires network-level connection formation.
- Evidence anchors:
  - [abstract] "humans may default to trust rather than demand genuine explanations. This creates risks of both misplaced trust and imperfect knowledge integration"
  - [section 5.2.2] "acceptance of other's opinions for reasons having to do with trust will produce disjunct sub-graphs" whereas explanation "increases internal connectivity"
  - [corpus] "Most General Explanations of Tree Ensembles" notes XAI is "critical for attaining trust in AI systems" but assumes explanation precedes trust.
- Break condition: If independent verification mechanisms exist (e.g., track records of reliability), trust can be grounded outside the explanation process.

## Foundational Learning

- Concept: **Knowledge networks as graph structures**
  - Why needed here: The entire formal model depends on representing knowledge as nodes (concepts) connected by edges (coherence relations).
  - Quick check question: Can you explain why a fully connected graph represents the most favorable conditions for explanation search?

- Concept: **Sequential search with Bayesian belief updating**
  - Why needed here: Understanding why explanation fails requires tracking how expected benefits evolve as the search proceeds unsuccessfully.
  - Quick check question: After three failed search attempts, how would you update a uniform prior belief about the size of the overlap set K?

- Concept: **Bounded rationality as time constraints (not just capacity constraints)**
  - Why needed here: The paper distinguishes processing limits from finite-time limits; the latter persist even with infinite computational capacity.
  - Quick check question: Why might more knowledgeable explainers (larger N_R) derive lower expected benefits from initiating explanation?

## Architecture Onboarding

- Component map:
  - Explainer network R -> Search process -> Belief updater -> Termination decider
  - Explainee network E (initially private)
  - Overlap set K (nodes in both R and E)
  - Target node R_0 (concept to be explained)

- Critical path:
  1. Initialize prior belief distribution over K size {i: p_i0}
  2. Calculate E(B_1); if B_1 > c(1), begin search
  3. At each step t: select node, check K membership
  4. If node in K: explanation succeeds, benefit B realized
  5. If not: update beliefs via equation (5), recalculate E(B_t)
  6. If E(B_t) < c(t): rationally terminate (explanation fails)

- Design tradeoffs:
  - **Complete vs. partially connected graphs**: Complete graphs (assumed in analysis) represent best-case search; real knowledge networks are sparse and hierarchical, making search harder
  - **High vs. low prior variance**: Low variance (high confidence) sustains search longer but may be unjustified; high variance leads to faster termination
  - **Larger vs. smaller N_R**: More knowledgeable explainers face lower E(B_1) for the same K, paradoxically reducing explanation initiation

- Failure signatures:
  - **Silent failure**: K is non-empty but search terminates before discovery—no error signal, just non-completion
  - **Confidence trap**: High-confidence but wrong priors lead to prolonged but ultimately failed search
  - **Expertise paradox**: Domain experts with large N_R may never initiate explanation despite substantial overlap

- First 3 experiments:
  1. **Baseline simulation**: Implement the search process with uniform priors across varied N_R (100, 300, 500) and N_K values; measure termination rates and mean search duration before success or failure.
  2. **Variance sensitivity test**: Fix N_R = 300, N_K = 10; vary prior variance-to-mean ratios (0.5, 1.0, 2.0, 5.0); observe how E(B_t) dynamics change and whether low-variance priors extend successful search.
  3. **Sparse network extension**: Replace complete graph R with a small-world network (Watts-Strogatz); measure how path dependency and local search constraints affect explanation success rates compared to the complete-graph baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different search strategies (e.g., breadth-first versus depth-first) affect explanation success rates under varying network topologies?
- Basis in paper: [explicit] Section 5.3 explicitly calls for "a systematic investigation of search strategies (e.g., breadth-first versus depth-first) under varying network topologies."
- Why unresolved: The model assumes a complete graph, which eliminates path dependence. Real knowledge networks are sparse and hierarchical, making strategy choice consequential.
- What evidence would resolve it: Simulation studies comparing search strategies across networks with varying sparsity, modularity, and degree distributions; empirical studies of how humans actually navigate explanation attempts.

### Open Question 2
- Question: How do knowledge networks evolve dynamically through repeated explanation attempts, and how does this co-evolution affect future explainability?
- Basis in paper: [explicit] Section 5.3 notes that "successful explanations likely modify these networks by creating new links within and between components" and calls for "a dynamic analysis [to] formalize how learning and explainability co-evolve."
- Why unresolved: The model treats knowledge networks as static, but real explanations reshape understanding—successful explanations create new pathways that facilitate future explanations.
- What evidence would resolve it: Longitudinal studies tracking how explanation attempts reshape knowledge structures; formal models incorporating network plasticity.

### Open Question 3
- Question: How do collective search processes in multi-agent settings differ from dyadic explanation, and how might multiple explainers coordinate to bridge knowledge gaps?
- Basis in paper: [explicit] Section 5.3 states that "extending our framework to multi-agent settings would allow us to theorize how collective search processes differ from individual ones."
- Why unresolved: The model focuses on dyadic interaction, but organizations involve multiple potential explainers who may possess complementary knowledge overlaps.
- What evidence would resolve it: Experiments comparing explanation success in dyadic vs. group settings; computational models of coordinated multi-agent search.

## Limitations
- The model assumes knowledge can be perfectly represented as complete graphs, which may not reflect real-world knowledge structures
- Bayesian belief updating relies on actors knowing their own knowledge size N_R and being able to estimate the Explainee's overlap distribution
- The termination decision framework assumes cost c(t) is either constant or monotonic, but real-world explanation costs may have complex time dependencies

## Confidence
- **High confidence**: The mathematical framework for search time (Mechanism 1) and the basic Bayesian updating equations are well-specified and internally consistent
- **Medium confidence**: The rationality of termination (Mechanism 2) depends on specific assumptions about cost functions and prior distributions that may not generalize to all contexts
- **Medium confidence**: The distinction between trust and explanation (Mechanism 3) is conceptually sound but the claim that trust prevents knowledge integration requires empirical validation in real human-AI interactions

## Next Checks
1. **Empirical validation**: Test the model predictions against human-human explanation experiments where participants attempt to explain concepts to partners with known or manipulated knowledge overlap
2. **Alternative graph structures**: Extend the analysis to sparse and hierarchical knowledge networks to assess how realistic network topologies affect the failure conditions identified in the complete-graph model
3. **Cost function sensitivity**: Investigate how different cost function specifications (e.g., increasing marginal costs, fixed setup costs) affect the termination thresholds and the prevalence of silent explanation failures