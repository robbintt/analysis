---
ver: rpa2
title: Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation
arxiv_id: '2602.00413'
source_url: https://arxiv.org/abs/2602.00413
tags:
- guidance
- diffusion
- logp
- alignment
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning diffusion models
  and flow matching models with human preferences in text-to-image generation without
  requiring computationally expensive fine-tuning. The authors propose a novel framework
  that formulates alignment as sampling from reward-weighted distributions, decomposing
  the required score function (for diffusion) or velocity field (for flow matching)
  into the pre-trained quantity plus a reward-driven guidance term.
---

# Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation

## Quick Facts
- **arXiv ID:** 2602.00413
- **Source URL:** https://arxiv.org/abs/2602.00413
- **Reference count:** 40
- **Primary result:** Achieves comparable performance to fine-tuning-based models with one-step generation while reducing computational cost by at least 60%.

## Executive Summary
This paper addresses the challenge of aligning diffusion models and flow matching models with human preferences in text-to-image generation without requiring computationally expensive fine-tuning. The authors propose a novel framework that formulates alignment as sampling from reward-weighted distributions, decomposing the required score function (for diffusion) or velocity field (for flow matching) into the pre-trained quantity plus a reward-driven guidance term. For diffusion models, they identify an adversarial nature flaw in the guidance term that can introduce artifacts, and propose a fine-tuning-free approach using a lightweight guidance network with regularization to estimate the conditional expectation of the reward. For flow matching, they derive a training-free estimator that directly computes the guidance term without additional model modification. Experimental results show that their method achieves comparable performance to fine-tuning-based models with one-step generation while reducing computational cost by at least 60%, and improves generation quality for flow matching without additional training overhead.

## Method Summary
The paper introduces a fine-tuning-free alignment framework for diffusion models and flow matching models in text-to-image generation. For diffusion models, they train a lightweight guidance network (72MB) to estimate the conditional expectation of the reward, addressing the adversarial guidance problem through consistency regularization. For flow matching, they derive a training-free estimator using importance sampling that directly computes the velocity guidance term. The approach works by decomposing the aligned score/velocity into the pre-trained component plus a reward-driven guidance term, enabling inference-time alignment without modifying base model weights.

## Key Results
- Achieves comparable performance to fine-tuning-based models with one-step generation while reducing computational cost by at least 60%
- The lightweight guidance network (72MB) effectively mitigates adversarial artifacts in diffusion model guidance through consistency regularization
- Training-free flow matching alignment improves generation quality without additional training overhead
- Successfully aligns both SDXL-Turbo (diffusion) and SD3.5 Large Turbo (flow matching) with multiple reward functions including PickScore and HPSV2

## Why This Works (Mechanism)

### Mechanism 1: Reward-Weighted Distribution Decomposition
The framework decomposes the aligned score/velocity into pre-trained component plus conditional expectation of reward, enabling inference-time alignment without weight modification. This additive decomposition is mathematically proven and allows modifying behavior at inference without changing model weights.

### Mechanism 2: Adversarial Guidance Mitigation via Conditional Expectation Network
Direct gradient-based guidance for diffusion introduces adversarial artifacts because the gradient of conditional expectations can exploit high-dimensional vulnerabilities. The proposed solution uses a lightweight guidance network with consistency regularization to directly estimate the conditional expectation, avoiding the adversarial gradient structure.

### Mechanism 3: Training-Free Velocity Guidance for Flow Matching
Flow matching velocity guidance lacks the adversarial gradient structure present in diffusion, allowing a training-free solution. The aligned velocity can be computed directly from importance samples of the marginal distribution without additional model training.

## Foundational Learning

- **Score-based generative modeling (SDE/ODE formulation)**: Essential for understanding how generative models are represented via score functions (diffusion) or velocity fields (flow matching), and how these relate to log-density gradients. *Quick check:* Can you explain why the score function ∇x log p(x,t) appears in the reverse-time SDE for diffusion?

- **Bradley-Terry preference model**: Provides the mathematical foundation for modeling preferences as p(xw ≻ xl|y) = exp(r(xw,y))/(exp(r(xw,y)) + exp(r(xl,y))). *Quick check:* How does the temperature parameter β in πr ∝ πref · exp(r/β) control the strength of preference enforcement?

- **Importance sampling for conditional expectations**: Critical for the flow matching solution that converts E[f(x1)|xt,y] to expectations under p(x1|y) using importance weights. *Quick check:* When converting an expectation under p(x1|xt,y) to one under p(x1|y), what quantity must you account for in the importance weights?

## Architecture Onboarding

- **Component map:** Pre-trained model (SDXL-Turbo/SD3.5 Large Turbo) → Reward function (PickScore/HPSV2) → Guidance network hψ (diffusion only) → Consistency regularizer → Aligned score/velocity
- **Critical path:** 1) Train guidance network on preference dataset (583k pairs, ~10 epochs) using L_guidance + ηL_consistence; 2) Inference: Sample xT ~ N(0,I), compute guided score s = s_pretrained + ∇xt log hψ(xt,y,t), denoise to x0; 3) For flow matching: Skip training; at inference, compute importance-sampled velocity adjustment
- **Design tradeoffs:** One-step vs. multi-step generation; guidance network capacity (72MB found sufficient); regularization strength η controls tradeoff between guidance effectiveness and artifact suppression
- **Failure signatures:** Artifacts with jagged/noisy textures (adversarial guidance issue); no guidance effect (network underfitting or β too small); OOM during flow matching (reduce importance sampling count)
- **First 3 experiments:** 1) Reproduce guidance network training with SDXL-Turbo and Pick-a-Pic V1 dataset; 2) Ablate regularization strength η to observe artifact patterns; 3) Test flow matching transfer to SD3.5 Large Turbo with 4-step sampling

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The adversarial nature of guidance in diffusion models requires additional regularization, which adds complexity compared to flow matching's training-free approach
- The flow matching training-free estimator's computational efficiency depends heavily on the number of importance samples needed for stable estimation
- The framework assumes reward functions can be evaluated efficiently at inference time, which may not hold for complex reward models

## Confidence
- **High confidence**: The decomposition theorem for diffusion models and the identification of adversarial guidance artifacts are well-supported by mathematical proof and visual demonstrations
- **Medium confidence**: The training-free flow matching guidance is theoretically sound, but practical effectiveness depends on importance sampling quality
- **Medium confidence**: The 72MB guidance network architecture is claimed sufficient, but ablation studies only explore regularization strength, not network capacity

## Next Checks
1. **Test adversarial robustness**: Systematically vary guidance strength α beyond the fixed value of 1 to confirm artifacts scale as predicted and that regularization consistently mitigates them across different reward functions
2. **Flow matching scalability**: Evaluate the importance sampling approach with computationally expensive reward functions (e.g., CLIP-based rewards) to measure the trade-off between guidance quality and inference time
3. **Architecture sensitivity**: Conduct an ablation study varying guidance network capacity (from 10MB to 500MB) to determine if the 72MB design is truly optimal or simply sufficient for the tested scenarios