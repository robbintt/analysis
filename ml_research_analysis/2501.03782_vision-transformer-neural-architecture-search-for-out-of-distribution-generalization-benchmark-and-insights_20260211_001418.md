---
ver: rpa2
title: 'Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization:
  Benchmark and Insights'
arxiv_id: '2501.03782'
source_url: https://arxiv.org/abs/2501.03782
tags:
- accuracy
- generalization
- architectures
- search
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OoD-ViT-NAS, the first benchmark designed
  for Neural Architecture Search (NAS) to study Vision Transformer (ViT) architectures
  under Out-of-Distribution (OoD) shifts. The authors evaluate 3,000 diverse ViT architectures
  from three search spaces (Autoformer-Tiny/Small/Base) on 8 widely used OoD datasets,
  including ImageNet-C, ImageNet-A, ImageNet-O, ImageNet-P, ImageNet-D, ImageNet-R,
  ImageNet-Sketch, and Stylized ImageNet.
---

# Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights

## Quick Facts
- arXiv ID: 2501.03782
- Source URL: https://arxiv.org/abs/2501.03782
- Reference count: 40
- Key outcome: Introduces OoD-ViT-NAS benchmark for ViT NAS under OoD shifts, revealing embedding dimension's strong correlation with OoD accuracy while simple proxies outperform complex NAS methods.

## Executive Summary
This paper introduces OoD-ViT-NAS, the first benchmark for Neural Architecture Search (NAS) to study Vision Transformer (ViT) architectures under Out-of-Distribution (OoD) shifts. The authors evaluate 3,000 diverse ViT architectures on 8 widely used OoD datasets, revealing that ViT architecture design significantly impacts OoD accuracy, with up to 11.85% improvement observed for some OoD shifts. Surprisingly, In-Distribution (ID) accuracy is found to be a poor indicator of OoD accuracy, challenging the assumption that architectures optimized for ID performance generalize well to OoD scenarios.

## Method Summary
The benchmark evaluates 3,000 ViT architectures sampled from Autoformer-Tiny/Small/Base supernets on 8 OoD datasets using weight inheritance without individual retraining. Architectures are sampled from defined search spaces covering embedding dimension, depth, MLP ratio, and head count. Performance is measured using Top-1 Accuracy for ID and OoD, with Kendall τ correlations computed between architectural attributes and accuracy metrics. Training-free NAS proxies are benchmarked against simple complexity metrics (#Param, #Flops) to assess their effectiveness in predicting OoD performance.

## Key Results
- Embedding dimension shows strong positive correlation with OoD accuracy (Kendall τ = 0.65), while depth, MLP ratio, and head count show minimal or non-obvious effects
- ID accuracy poorly predicts OoD accuracy, with Kendall τ correlations remaining low across most datasets
- Simple proxies (#Param, #Flops) surprisingly outperform complex training-free NAS methods in predicting OoD accuracy
- The benchmark enables systematic evaluation of ViT architecture design choices for OoD generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing embedding dimension generally improves ViT OoD generalization, while depth, MLP ratio, and head count show minimal or non-obvious effects.
- Mechanism: Higher embedding dimensions enable ViTs to learn more high-frequency components (HFC) in input data. Models that preserve HFC better tend to generalize better to OoD shifts. This is validated through frequency filtering experiments where higher embedding dimensions show improved performance on HFC-filtered samples across multiple radius values.
- Core assumption: HFC learning is causally linked to OoD generalization, not merely correlated.
- Evidence anchors:
  - [abstract]: "increasing the embedding dimension of ViT architectures generally improves OoD generalization, while other architectural attributes like network depth, MLP ratio, and number of heads have minimal or non-obvious impacts."
  - [section 4.4]: Kendall τ correlation for embedding dimension (0.65) substantially exceeds depth (0.19), MLP ratio (0.09), and #Heads (0.07).
  - [corpus]: Weak/no direct corpus support for HFC mechanism specifically in ViT OoD contexts.

### Mechanism 2
- Claim: In-Distribution (ID) accuracy is a poor indicator of OoD accuracy for ViT architectures.
- Mechanism: Architectures optimized for ID performance exploit dataset-specific shortcuts and texture biases that do not transfer under distribution shift. Pareto-optimal architectures for ID accuracy exhibit sub-optimal OoD performance, suggesting the features learned for ID and OoD differ significantly.
- Core assumption: The disconnect is due to feature-level differences, not merely evaluation noise.
- Evidence anchors:
  - [abstract]: "In-Distribution (ID) accuracy is found to be a poor indicator of OoD accuracy, challenging the assumption that architectures optimized for ID performance generalize well to OoD scenarios."
  - [section 4.2]: Kendall τ correlations between ID and OoD accuracy remain low across most datasets; ImageNet-P shows highest correlation due to weaker OoD shift.

### Mechanism 3
- Claim: Existing training-free NAS proxies are largely ineffective at predicting ViT OoD accuracy, and simple proxies (#Param, #Flops) outperform complex methods.
- Mechanism: Training-free NAS proxies (e.g., gradient-based measures) capture training dynamics relevant to ID optimization but fail to encode structural properties that support OoD robustness. Parameter count and FLOPs indirectly correlate with embedding dimension, which has the highest architectural correlation with OoD accuracy.
- Core assumption: Simple proxies succeed because they indirectly track embedding dimension, not because raw capacity alone explains OoD robustness.
- Evidence anchors:
  - [abstract]: "Simple proxies like parameter count or FLOPs surprisingly outperform complex NAS methods in predicting OoD accuracy."
  - [table 2]: #Param (0.3600) and #Flops (0.3537) achieve higher Kendall τ correlation with OoD accuracy than AutoProx-A (0.3303) and DSS (0.3421).

## Foundational Learning

- Concept: **Vision Transformer (ViT) Architecture Components**
  - Why needed here: The paper analyzes how embedding dimension, depth, MLP ratio, and number of heads affect OoD generalization; understanding these components is prerequisite to interpreting results.
  - Quick check question: Can you explain how embedding dimension differs from MLP ratio in a ViT block, and which is fixed across layers in Autoformer search spaces?

- Concept: **Out-of-Distribution (OoD) Generalization vs. Robustness**
  - Why needed here: The paper evaluates OoD accuracy across 8 distinct shift types (corruptions, adversarial, style, sketch, diffusion-synthetic); conflating OoD generalization with adversarial robustness will lead to misinterpretation.
  - Quick check question: Name two OoD datasets from the benchmark that represent natural distribution shifts versus synthetic corruptions, and explain why ImageNet-P has higher ID-OoD correlation.

- Concept: **One-Shot NAS and Supernet Weight Inheritance**
  - Why needed here: The benchmark relies on Autoformer supernets where subnets inherit weights without retraining; results assume this provides reliable performance estimates.
  - Quick check question: What is the core efficiency assumption of One-Shot NAS, and what evidence in the paper supports or challenges its validity for OoD evaluation?

## Architecture Onboarding

- Component map:
  - **OoD-ViT-NAS Benchmark**: 3,000 architectures sampled from Autoformer-Tiny/Small/Base supernets, evaluated on 8 OoD datasets with ID accuracy, OoD accuracy, and AUPR (for ImageNet-O) metrics.
  - **Autoformer Search Space**: Defines embedding dimension, Q-K-V dimension, number of heads, MLP ratio, and depth; embedding dimension is fixed across layers while others vary.
  - **Evaluation Datasets**: ImageNet-C (15 corruptions, 5 severity levels), ImageNet-P (perturbations), ImageNet-A/O (adversarially filtered), ImageNet-R (renditions), ImageNet-Sketch, Stylized ImageNet, ImageNet-D (diffusion-synthetic).

- Critical path:
  1. Load pre-trained Autoformer supernets (Tiny/Small/Base).
  2. Sample 1,000 architectures per supernet using defined search space bounds.
  3. Evaluate each architecture on all 8 OoD datasets without retraining (weight inheritance).
  4. Compute Kendall τ correlations between architectural attributes, ID accuracy, and OoD accuracy.
  5. Benchmark training-free NAS proxies against simple baselines (#Param, #Flops).

- Design tradeoffs:
  - **Supernet vs. Standalone Training**: Supernet inheritance enables scaling to 3,000 architectures but may introduce ranking errors; the paper claims performance is "comparable or superior" but does not quantify OoD-specific ranking fidelity.
  - **Search Space Scope**: Autoformer excludes hybrid CNN-ViT architectures; results may not generalize to hierarchical designs like Swin (though Appendix 8 shows limited validation).
  - **OoD Dataset Coverage**: 8 datasets cover corruption, style, sketch, adversarial, and diffusion shifts, but do not include domain shift scenarios (e.g., different camera sensors, geographic regions).

- Failure signatures:
  - **Bimodal OoD Accuracy Distribution**: Indicates embedding dimension is a dominant factor; if your architecture search yields bimodal results, check embedding dimension clustering.
  - **High ID Accuracy, Low OoD Accuracy**: Pareto architectures for ID accuracy show this pattern; avoid using ID performance as the sole selection criterion.
  - **Training-free NAS Proxy Mismatch**: If NAS proxy rankings correlate with ID but not OoD accuracy (e.g., AutoProx-A: 0.4023 ID vs. 0.3303 OoD correlation), the proxy is not suitable for OoD-aware search.

- First 3 experiments:
  1. **Embedding Dimension Ablation**: Fix depth=12, MLP ratio=3.5, heads=6; vary embedding dimension across {320, 384, 448} and evaluate on ImageNet-C and ImageNet-R to replicate the positive correlation (target: Kendall τ ~0.65).
  2. **Training-free NAS Benchmark Reproduction**: Implement #Param, #Flops, SNIP, and AutoProx-A proxies on 100 sampled architectures; verify that simple proxies achieve higher Kendall τ with OoD accuracy than complex methods.
  3. **ID-OoD Correlation Check**: Compute Kendall τ between ID and OoD accuracy for all architectures in one search space; confirm low correlation (expect <0.5 for most datasets except ImageNet-P).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a training-free Neural Architecture Search (NAS) proxy be devised that consistently surpasses simple complexity metrics (e.g., #Params or #FLOPs) in predicting Vision Transformer (ViT) Out-of-Distribution (OoD) accuracy?
- Basis in paper: [explicit] Section 4.3 states that the observation of simple proxies outperforming complex methods "poses a challenge to the Training-free NAS research community: to devise a Training-free NAS that surpasses #Params or #Flops in OoD Acc prediction for ViT."
- Why unresolved: Current training-free proxies rely on metrics (like gradient flow) that correlate well with In-Distribution (ID) accuracy but fail to capture the robustness required for OoD generalization, resulting in lower correlation scores than naive parameter counts.
- What evidence would resolve it: The development of a new zero-cost proxy that achieves a statistically significant Kendall $\tau$ ranking correlation higher than the $0.36$ baseline established by parameter count on the OoD-ViT-NAS benchmark.

### Open Question 2
- Question: Do the benefits of increasing embedding dimensions and the observed minimal impact of network depth on OoD generalization hold for hierarchical ViT architectures (e.g., Swin Transformers) or hybrid CNN-ViT models?
- Basis in paper: [inferred] The study is restricted to the AutoFormer search space (Section 3), which consists of standard, non-hierarchical ViT blocks. The authors note in Section 4.4 that attributes like depth and MLP ratio have "minimal or non-obvious impacts," but this may be specific to the isotropic nature of the search space used.
- Why unresolved: The structural differences in hierarchical models (which have shifting embedding dimensions and patch merging) might alter the relationship between architectural attributes and OoD robustness observed in isotropic ViTs.
- What evidence would resolve it: Extending the OoD-ViT-NAS benchmark to include hierarchical search spaces (e.g., Swin or Pyramid ViT) to verify if the dominance of embedding dimension over depth persists.

### Open Question 3
- Question: Why do architectural attributes such as network depth and MLP ratio exhibit such weak and non-obvious correlations with OoD generalization compared to embedding dimension?
- Basis in paper: [inferred] In Section 4.4 and Appendix 17, the authors observe that increasing embedding dimension helps learn high-frequency components, but they find the impact of depth and MLP ratio to be "minimal" or "non-obvious" without providing a theoretical explanation for this disparity.
- Why unresolved: The paper empirically demonstrates the lack of correlation but does not fully explain the underlying mechanism—specifically, why increasing depth (which increases receptive field) does not yield the consistent OoD benefits seen in ID scenarios.
- What evidence would resolve it: A mechanistic interpretability study analyzing how variations in depth vs. embedding dimension affect the internal feature representations of ViTs when subjected to specific OoD corruptions (e.g., texture shifts vs. noise).

## Limitations

- The reliance on Autoformer supernets without retraining individual subnets may introduce ranking inaccuracies, particularly for OoD performance where architecture differences could be amplified
- The search spaces exclude hybrid CNN-ViT designs, potentially missing architectures that could perform better on certain OoD shifts
- While 8 OoD datasets are evaluated, they represent specific shift types but not broader domain shifts (geographical, temporal, sensor-based)

## Confidence

- **High Confidence**: The observation that embedding dimension correlates strongly with OoD accuracy (Kendall τ = 0.65) is robustly supported by empirical data and frequency filtering experiments.
- **Medium Confidence**: The claim that ID accuracy poorly predicts OoD accuracy is well-supported but may be dataset/search-space specific, as ImageNet-P shows higher correlation due to weaker OoD shift.
- **Low Confidence**: The mechanism linking embedding dimension to HFC learning requires further validation, as corpus evidence for this specific pathway in ViT OoD contexts is weak.

## Next Checks

1. Replicate the embedding dimension ablation study on a subset of architectures to verify the Kendall τ correlation remains above 0.6 across multiple OoD datasets.
2. Implement frequency filtering experiments to test whether HFC preservation mediates the embedding dimension-OoD generalization relationship.
3. Evaluate a small set of hybrid CNN-ViT architectures on the benchmark to assess whether excluding these designs limits the scope of the findings.