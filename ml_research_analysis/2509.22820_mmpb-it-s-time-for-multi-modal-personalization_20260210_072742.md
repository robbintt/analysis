---
ver: rpa2
title: 'MMPB: It''s Time for Multi-Modal Personalization'
arxiv_id: '2509.22820'
source_url: https://arxiv.org/abs/2509.22820
tags:
- image
- arxiv
- concept
- personalization
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces MMPB, the first large-scale benchmark designed\
  \ to evaluate the personalization capabilities of Vision-Language Models (VLMs).\
  \ It comprises 10,000 image-query pairs across 111 personalizable concepts spanning\
  \ four categories\u2014humans, animals, objects, and characters\u2014with humans\
  \ enriched by preference-grounded queries."
---

# MMPB: It's Time for Multi-Modal Personalization

## Quick Facts
- arXiv ID: 2509.22820
- Source URL: https://arxiv.org/abs/2509.22820
- Reference count: 40
- First large-scale benchmark for evaluating personalization capabilities in Vision-Language Models

## Executive Summary
MMPB introduces the first comprehensive benchmark designed to evaluate personalization capabilities in Vision-Language Models (VLMs). The benchmark addresses a critical gap in VLM evaluation by focusing on personalized interactions across 10,000 image-query pairs spanning 111 personalizable concepts. Through a three-stage evaluation protocol involving concept injection, multi-turn dialogue, and personalized querying, MMPB reveals significant challenges in VLM personalization, including consistency maintenance, preference handling, and visual cue utilization.

## Method Summary
The benchmark employs a three-stage evaluation protocol to assess VLM personalization capabilities. Stage one involves concept injection where personalizable concepts are introduced to models. Stage two uses multi-turn dialogue to establish context and preferences. Stage three tests personalized querying to evaluate how well models maintain consistency and apply learned preferences. The benchmark covers four concept categories (humans, animals, objects, characters) with humans specifically enriched by preference-grounded queries. Results are gathered from 23 widely used VLMs across both open- and closed-source architectures.

## Key Results
- VLMs struggle significantly with maintaining consistency over multi-turn dialogue sequences
- Top-ranked models on general benchmarks perform poorly on preference-grounded personalization tasks
- Safety-induced evasive responses particularly affect human-centric concept personalization
- Models demonstrate weak visual cue utilization for maintaining personalization across interactions

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic approach to evaluating personalization through real-world scenarios. By requiring models to maintain user preferences across multi-turn interactions and apply them to new queries, MMPB reveals fundamental limitations in current VLM architectures. The preference-grounded queries for human concepts create realistic personalization challenges that expose gaps in models' ability to integrate visual and textual information while maintaining user-specific contexts.

## Foundational Learning

### Vision-Language Model Fundamentals
Why needed: Understanding how VLMs integrate visual and textual inputs
Quick check: Verify model architecture includes both vision and language encoders

### Personalization Concept Learning
Why needed: Models must learn and maintain user-specific preferences
Quick check: Test model's ability to recall preferences after concept injection

### Multi-Turn Dialogue Management
Why needed: Maintaining context across conversational turns
Quick check: Evaluate consistency decay across dialogue sequences

## Architecture Onboarding

### Component Map
Data Collection -> Concept Injection -> Multi-turn Dialogue -> Personalized Querying -> Performance Evaluation

### Critical Path
Concept injection and preference establishment form the critical path, as errors here propagate through subsequent dialogue turns and query responses.

### Design Tradeoffs
The benchmark trades breadth of concept coverage for depth of personalization evaluation. The three-stage protocol prioritizes consistency assessment over rapid testing, potentially limiting scalability.

### Failure Signatures
Safety filter activations for human concepts, preference forgetting across dialogue turns, and visual cue neglect represent primary failure modes.

### First Experiments
1. Test concept recall after initial injection across different VLM architectures
2. Measure preference consistency decay over increasing dialogue turns
3. Compare open-source versus closed-source model performance on human-centric concepts

## Open Questions the Paper Calls Out
The paper acknowledges several open questions, including the impact of safety filters on personalization evaluation, the generalizability of results across more diverse concept categories, and the need for longitudinal testing to better understand consistency maintenance over extended interactions.

## Limitations
- Dataset covers only 111 personalizable concepts, potentially limiting real-world applicability
- Safety-filter-induced evasive responses may skew performance metrics for human-centric concepts
- Preference-grounded queries may introduce subjective bias in personalization assessment
- Multi-turn dialogue variability could affect result consistency

## Confidence
- High confidence in benchmark structural validity and identification of personalization challenges
- Medium confidence in generalizability due to limited concept diversity
- Medium confidence in comparative analysis between open- and closed-source models

## Next Checks
1. Expand concept diversity beyond current 111 categories to include niche entities and cross-cultural variations
2. Conduct ablation study isolating safety filter effects from genuine personalization failures
3. Implement longitudinal testing for consistency evaluation over extended dialogue sequences