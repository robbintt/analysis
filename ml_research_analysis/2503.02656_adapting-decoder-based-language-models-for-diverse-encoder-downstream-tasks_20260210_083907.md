---
ver: rpa2
title: Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks
arxiv_id: '2503.02656'
source_url: https://arxiv.org/abs/2503.02656
tags:
- gemma
- attention
- encoder
- pooling
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting decoder-only language
  models, such as Gemma, for encoder-only tasks like classification, regression, and
  ranking. The authors propose a systematic approach to convert decoder-based models
  into encoder architectures by exploring various pooling strategies, attention mechanisms,
  and hyperparameters like dropout.
---

# Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks

## Quick Facts
- arXiv ID: 2503.02656
- Source URL: https://arxiv.org/abs/2503.02656
- Reference count: 15
- Primary result: Direct adaptation of Gemma decoder models for encoder tasks without encoder-decoder pretraining, achieving competitive GLUE/SuperGLUE and MS MARCO ranking performance

## Executive Summary
This paper addresses the challenge of adapting decoder-only language models, such as Gemma, for encoder-only tasks like classification, regression, and ranking. The authors propose a systematic approach to convert decoder-based models into encoder architectures by exploring various pooling strategies, attention mechanisms, and hyperparameters like dropout. Their method involves task-specific pooling and MLP layers, bidirectional attention during fine-tuning, and dropout regularization. The adapted Gemma Encoder models are evaluated on GLUE and SuperGLUE benchmarks for classification and regression tasks, as well as the MS MARCO ranking benchmark. The results demonstrate that Gemma Encoder outperforms competitive baselines, achieving strong performance on diverse encoder-centric tasks despite not leveraging any encoder-decoder pretraining or up-training.

## Method Summary
The method adapts Gemma decoder models for encoder tasks by modifying attention mechanisms and adding task-specific components. During fine-tuning, the causal attention mask is replaced with bidirectional attention, allowing tokens to attend to all other tokens. A pooling layer (mean, last-token, or attention-based) converts variable-length sequences to fixed-dimensional vectors, followed by task-specific MLP projection layers. Dropout (10%) is applied to attention softmax and feed-forward network outputs to prevent overfitting on limited encoder task datasets. The architecture maintains pretrained Gemma weights while adding randomly initialized pooling and MLP components, with right-padding for sequence processing.

## Key Results
- Gemma Encoder achieves strong GLUE and SuperGLUE benchmark performance, outperforming T5 baselines despite lacking encoder-decoder pretraining
- Bidirectional attention is critical, improving GLUE average from 84.5 to 89.4 for the 2B model and from 87.5 to 90.9 for the 9B model
- Mean and last-token pooling strategies perform comparably to attention pooling on limited GLUE data, avoiding overfitting from randomly initialized attention parameters
- 10% dropout on attention and FFN outputs significantly improves generalization compared to no dropout regularization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Switching from causal to bidirectional attention during fine-tuning significantly improves encoder task performance, even when the base model was pretrained with causal masking only.
- **Mechanism**: Bidirectional attention allows each token to attend to all other tokens in the sequence, enabling the model to build holistic representations of the entire input rather than accumulating information unidirectionally. This matches the structural requirements of classification/regression/ranking tasks where the output depends on full context.
- **Core assumption**: The knowledge acquired during causal pretraining transfers to bidirectional processing without requiring bidirectional pretraining.
- **Evidence anchors**:
  - [abstract] "Ablations show bidirectional masking is critical"
  - [section 3.4.2] Table 6 shows 2B model improves from 84.5 (causal, mean pooling) to 89.4 (bidirectional) on GLUE average; 9B improves from 87.5 to 90.9
  - [corpus] Weak direct evidence; related work (Encoder-Decoder Gemma, T5Gemma 2) explores similar adaptation but without focused ablation on attention masking alone
- **Break condition**: If target task inherently requires autoregressive processing (e.g., next-token prediction), bidirectional attention would violate task structure.

### Mechanism 2
- **Claim**: Simple pooling strategies (mean, last-token) perform comparably to attention pooling on encoder tasks with limited fine-tuning data.
- **Mechanism**: With limited training examples (GLUE tasks have <1M samples), randomly initialized attention pooling heads lack sufficient data to learn meaningful attention weights. Parameter-free pooling methods (mean, last-token) avoid this underfitting risk while still extracting useful sequence representations from pretrained transformer outputs.
- **Core assumption**: The pretrained transformer has already learned to distribute relevant information across token positions such that simple aggregation suffices.
- **Evidence anchors**:
  - [section 3.4.1] Table 5: Mean pooling (89.4) and last-token (89.1) match or exceed attention pooling (89.0) for 2B model on GLUE average
  - [section A.4] "GLUE training sets are limited in size... inadequate for effective adaptation of a randomly initialized attention pooler with millions of parameters"
  - [corpus] No direct corpus evidence on pooling strategy comparisons
- **Break condition**: Large-scale fine-tuning or pre-finetuning scenarios may benefit from attention pooling's additional capacity.

### Mechanism 3
- **Claim**: Adding 10% dropout to attention softmax and feedforward outputs during fine-tuning improves generalization on encoder tasks.
- **Mechanism**: Encoder-only tasks use sentence-level supervision with deterministic loss functions on limited datasets, making them prone to overfitting on spurious features. Dropout regularizes by preventing co-adaptation of neurons, forcing robust feature learning.
- **Core assumption**: The benefit of regularization outweighs the reduction in model capacity during fine-tuning.
- **Evidence anchors**:
  - [section 3.4.3] Figure 2: Performance peaks at dropout=0.1, degrades at 0.15; 2B improves from 87.2 (no dropout) to 89.4 (0.1 dropout)
  - [section 2.3] "Encoder-only tasks... are more prone to overfitting compared to generative tasks... supervision signal operates at sentence or full-input-sequence level"
  - [corpus] No direct corpus evidence on dropout for encoder adaptation
- **Break condition**: Very large fine-tuning datasets or when task-specific regularization (weight decay, early stopping) already suffices.

## Foundational Learning

- **Concept: Causal vs. Bidirectional Attention**
  - Why needed here: Understanding how masking patterns constrain information flow is essential to grasp why bidirectional attention improves encoder performance.
  - Quick check question: If token 5 can only attend to tokens 1-4, what information cannot influence its representation?

- **Concept: Pooling for Sequence Representation**
  - Why needed here: Encoder tasks require converting variable-length token sequences into fixed-dimensional vectors—pooling is this conversion mechanism.
  - Quick check question: Why might averaging all token embeddings lose task-relevant information compared to learning which tokens matter?

- **Concept: Regularization in Transfer Learning**
  - Why needed here: Fine-tuning large models on small datasets creates overfitting risk; understanding dropout helps justify why it's reintroduced despite being omitted in pretraining.
  - Quick check question: If a model achieves 99% training accuracy but 70% validation accuracy, what regularization strategy might help?

## Architecture Onboarding

- **Component map**: Input tokens → Embedding layer (pretrained Gemma) → Transformer encoder (bidirectional attention, pretrained weights) → Pooler (randomly initialized: mean/last-token/attention) → MLP projection (randomly initialized) → Task logits
- **Critical path**: The attention mask configuration is the highest-impact change. It must be changed from causal (Gemma default) to bidirectional at fine-tuning time. Pooler and MLP are added on top with random initialization.
- **Design tradeoffs**: 
  - Pooling complexity vs. data size: Attention pooling adds parameters requiring more data; simple pooling is data-efficient
  - Dropout rate: 0.1 optimal; higher degrades performance
  - Padding side: Left vs. right padding shows no significant difference post-fine-tuning
- **Failure signatures**:
  - Using causal attention with mean pooling causes significant performance drop (2B: 89.4 → 84.5)
  - Dropout > 0.15 causes performance degradation
  - Attention pooling underperforms with limited training data
- **First 3 experiments**:
  1. **Attention masking ablation**: Compare causal vs. bidirectional attention on a single GLUE task (e.g., MNLI) with identical pooling (last-token) and dropout (0.1)
  2. **Dropout sweep**: Fine-tune with dropout rates [0, 0.05, 0.1, 0.15] on validation split to confirm 0.1 optimal for your specific task
  3. **Pooling comparison**: Compare mean, last-token, and attention pooling on your target task to determine if simple pooling suffices given your training data scale

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the relative efficacy of attention pooling versus simple pooling strategies change when fine-tuning decoder-based encoders on datasets significantly larger than the GLUE benchmark?
- **Basis in paper**: [explicit] Section 3.4.1 states that the superiority of simple pooling found in this study is specific to the limited GLUE finetuning context and that "a reevaluation of the relative efficacy of attention pooling and simple pooling methods is warranted" for large-dataset finetuning or pre-finetuning.
- **Why unresolved**: The authors note that GLUE training sets are too small (<1 million examples) to effectively train the millions of parameters introduced by randomly initialized attention poolers, leaving their potential on larger corpora unverified.
- **What evidence would resolve it**: Experiments comparing mean/last-token pooling against attention pooling on encoder tasks with large-scale training data (e.g., web-scale retrieval or classification datasets with >1M examples).

### Open Question 2
- **Question**: Would incorporating an intermediate encoder-style pre-training phase (such as masked language modeling) yield significant performance gains over the proposed direct fine-tuning adaptation?
- **Basis in paper**: [inferred] Section 3.3 emphasizes that Gemma Encoder matches T5 baselines "despite not utilizing any uptraining or encoder-decoder masked language modeling (MLM) pretraining."
- **Why unresolved**: While the paper proves that encoder pre-training is not strictly necessary for strong performance, it does not quantify the marginal utility or efficiency trade-offs of adding such a phase compared to the compute-heavy direct fine-tuning of large decoders.
- **What evidence would resolve it**: A comparative analysis of Gemma Encoder performance when initialized from a causally-masked model versus a model that underwent an intermediate "uptraining" phase with bidirectional masking.

### Open Question 3
- **Question**: Do the benefits of enabling bidirectional attention and adding task-specific dropout generalize to other decoder-only architectures with different pre-training objectives or attention mechanisms?
- **Basis in paper**: [inferred] The paper focuses exclusively on the Gemma family (2B and 9B). Section 2.1 limits the attention exploration to causal and bidirectional, while Section 1 frames the problem generally for "decoder-based language models."
- **Why unresolved**: Gemma 2 utilizes specific architectural features like local sliding window attention and global attention in alternating layers. It is unclear if the observed gains from "switching" to bidirectional attention are consistent across standard decoder architectures (e.g., Llama, GPT) that use uniform global causal masking.
- **What evidence would resolve it**: Replicating the adaptation protocol (switching to bidirectional attention, 10% dropout, right padding) on non-Gemma decoder models and evaluating on GLUE/MS MARCO benchmarks.

## Limitations

- Implementation details missing: Fine-tuning hyperparameters (learning rates, batch sizes, weight decay, training duration) are not reported
- Architectural specification incomplete: Task-specific MLP projection layers' architecture details are not provided
- Scope limited: Experiments focus exclusively on Gemma family models without testing generalization to other decoder architectures

## Confidence

- **High Confidence**: The bidirectional attention mechanism is essential for encoder task performance. The ablation showing 2B model improvement from 84.5 to 89.4 on GLUE average provides direct, strong evidence that this change is the primary driver of success.
- **Medium Confidence**: Simple pooling strategies (mean, last-token) perform comparably to attention pooling on limited data. While the GLUE ablation results are clear, the claim that attention pooling fails due to parameter count alone may oversimplify—architecture details and initialization could also matter.
- **Medium Confidence**: 10% dropout on attention and FFN outputs improves generalization. The single-figure evidence is suggestive but limited in scope; the optimal rate may vary by task and dataset size.

## Next Checks

1. **Attention masking ablation with intermediate patterns**: Test if partial bidirectional attention (e.g., limited context windows) can achieve similar results with potentially lower computational cost than full bidirectional attention.

2. **Pooling strategy scalability study**: Evaluate the simple vs. attention pooling tradeoff as training data increases from GLUE scale (~tens of thousands) to SuperGLUE scale (~hundreds of thousands) to identify the crossover point where attention pooling becomes beneficial.

3. **Dropout location ablation**: Test whether applying dropout only to attention softmax versus only to FFN outputs yields different results, isolating which component benefits most from regularization in encoder adaptation scenarios.