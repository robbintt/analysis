---
ver: rpa2
title: Language Models reach higher Agreement than Humans in Historical Interpretation
arxiv_id: '2504.02572'
source_url: https://arxiv.org/abs/2504.02572
tags:
- historical
- llms
- phase
- humans
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper compares historical annotations by humans and LLMs on
  the same tasks: annotating historical decades with labels representing either cultural/social
  (Structural Demographic Theory) or economic (Big Cycle) cycles. Three LLMs (GPT-4,
  Claude 3.5, Gemini 1.5) and three human annotators labeled the same set of historical
  descriptions.'
---

# Language Models reach higher Agreement than Humans in Historical Interpretation
## Quick Facts
- arXiv ID: 2504.02572
- Source URL: https://arxiv.org/abs/2504.02572
- Reference count: 22
- Language models outperformed humans in historical annotation agreement

## Executive Summary
This study compares the agreement levels between large language models (LLMs) and human annotators when labeling historical events according to two theoretical frameworks: Structural Demographic Theory (cultural/social cycles) and Big Cycle Theory (economic cycles). The research found that LLMs demonstrated significantly higher inter-annotator agreement than humans on cultural/social annotation tasks (Fleiss' Kappa 0.33 vs. 0.09), while achieving comparable agreement on economic tasks (0.23 vs. 0.25). Notably, human annotators showed consistent Eurocentric bias across all participants, whereas only one LLM exhibited similar bias patterns.

## Method Summary
The study employed three LLMs (GPT-4, Claude 3.5, Gemini 1.5) and three human annotators to label the same set of historical descriptions using two annotation schemes: a 5-label cultural/social cycle framework and a 3-label economic cycle framework. Historical text snippets from Wikipedia were systematically processed and converted into structured annotation tasks. The researchers analyzed inter-annotator agreement using Fleiss' Kappa, examined bias patterns across different geographical regions, and investigated the sources of disagreement between annotators. All annotations were performed without external information access to ensure fair comparison between human and machine performance.

## Key Results
- LLMs achieved higher agreement than humans on 5-label cultural/social task (Fleiss' Kappa 0.33 vs. 0.09)
- LLMs and humans showed comparable agreement on 3-label economic task (0.23 vs. 0.25)
- All human annotators demonstrated consistent Eurocentric bias, while only one LLM showed similar bias

## Why This Works (Mechanism)
The mechanism underlying LLMs' superior agreement in historical interpretation appears to stem from their consistent application of learned patterns and reduced susceptibility to individual biases. Unlike humans who bring personal experiences and cultural perspectives that lead to divergent interpretations, LLMs apply uniform reasoning processes based on their training data. The study found that when LLMs disagreed, it typically resulted from technical constraints (such as phase skipping or constraint violations) rather than substantive interpretive differences. This suggests that LLM disagreements represent systematic errors rather than the kind of subjective divergence seen among human annotators.

## Foundational Learning
- **Fleiss' Kappa**: A statistical measure of inter-annotator agreement that accounts for chance agreement - needed to quantify and compare agreement levels between different annotator groups
- **Structural Demographic Theory**: A framework for analyzing historical cycles based on population dynamics, elite overproduction, and state fiscal health - needed as one of the annotation schemes
- **Big Cycle Theory**: An economic framework describing alternating periods of peace/prosperity and war/instability - needed as the second annotation scheme
- **Eurocentrism**: The tendency to interpret history from a European perspective - needed to analyze cultural bias patterns
- **Constraint violations**: Instances where annotations fail to follow logical rules (e.g., skipping phases) - needed to understand LLM disagreement patterns
- **Inter-annotator agreement**: The degree of consistency between different annotators - fundamental concept for comparing human and LLM performance

## Architecture Onboarding
Component Map: Historical texts -> Preprocessing -> Annotation Task Generation -> LLM/Human Annotation -> Agreement Analysis -> Bias Analysis
Critical Path: Historical text selection → Structured annotation task creation → Parallel annotation by humans and LLMs → Statistical agreement measurement → Bias pattern identification
Design Tradeoffs: Standardized prompts for LLMs versus natural human reasoning; controlled environment versus real-world annotation conditions; structured frameworks versus interpretive flexibility
Failure Signatures: High disagreement rates, systematic Eurocentric bias, constraint violations in phase transitions, inconsistent application of theoretical frameworks
First Experiments: 1) Test additional historical periods outside European context, 2) Compare agreement with and without specific prompting strategies, 3) Analyze individual LLM model differences in bias manifestation

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate Fleiss' Kappa values (0.23-0.33) indicate only fair to moderate agreement even for LLMs, suggesting inherent task difficulty
- Eurocentric bias in humans may reflect corpus composition rather than universal human tendencies
- Single biased LLM (Claude 3.5) complicates generalizations about systematic model bias patterns

## Confidence
- High confidence in LLM-human agreement differences for cultural/social task (0.33 vs 0.09)
- Medium confidence in economic task comparison (0.23 vs 0.25)
- Medium confidence in claims about LLM disagreement sources being technical rather than interpretive

## Next Checks
1. Replicate study with geographically diverse historical corpus to assess human Eurocentrism persistence and LLM pattern changes
2. Conduct detailed error analysis on LLM disagreements to determine if constraint violations represent systematic issues
3. Test additional LLM architectures and prompting strategies to isolate whether agreement patterns are model-specific or general capabilities