---
ver: rpa2
title: Enhancing Sentiment Classification with Machine Learning and Combinatorial
  Fusion
arxiv_id: '2510.27014'
source_url: https://arxiv.org/abs/2510.27014
tags:
- diversity
- roberta
- accuracy
- ensemble
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to sentiment classification
  using Combinatorial Fusion Analysis (CFA) to integrate an ensemble of diverse machine
  learning models, achieving state-of-the-art accuracy on the IMDB sentiment analysis
  dataset of 97.072%. CFA leverages cognitive diversity, utilizing rank-score characteristic
  functions to quantify the dissimilarity between models and strategically combine
  their predictions.
---

# Enhancing Sentiment Classification with Machine Learning and Combinatorial Fusion

## Quick Facts
- arXiv ID: 2510.27014
- Source URL: https://arxiv.org/abs/2510.27014
- Reference count: 30
- Primary result: Achieved 97.072% accuracy on IMDB sentiment analysis using CFA ensemble fusion

## Executive Summary
This paper introduces a novel approach to sentiment classification that combines an ensemble of diverse machine learning models using Combinatorial Fusion Analysis (CFA). The method integrates a RoBERTa transformer model with traditional machine learning models including Random Forest, SVM, and XGBoost. By leveraging cognitive diversity through rank-score characteristic functions, the approach achieves state-of-the-art accuracy of 97.072% on the IMDB sentiment analysis dataset. The key innovation is that CFA effectively computes and employs model diversity, outperforming traditional ensemble methods by integrating heterogeneous architectures. This demonstrates that combining cognitively diverse systems through CFA represents a robust and sustainable strategy for advancing machine learning performance.

## Method Summary
The method trains four base models independently on the IMDB dataset: RoBERTa (transformer-based, 94.67% standalone accuracy), SVM (linear kernel, 83.46%), XGBoost (85.36%), and Random Forest (25,000 trees, 86.15%). All models use bag-of-words features (20,000 features) except RoBERTa which uses subword tokenization. The ensemble applies CFA by first normalizing prediction scores using training ranges, computing rank-score characteristic functions for each model, calculating pairwise cognitive diversity and individual diversity strength, then applying weighted score combination (WCDS-SC) using the three models with highest diversity strength (RoBERTa, SVM, and Random Forest).

## Key Results
- Achieved 97.072% accuracy on IMDB test set, outperforming all individual models and traditional ensemble methods
- Diversity-weighted fusion (WCDS) achieved 97.072% accuracy versus 94.90% for performance-weighted fusion, demonstrating the value of cognitive diversity
- The best trio (RoBERTa, SVM, Random Forest) outperformed the full ensemble including XGBoost, suggesting not all models contribute equally to diversity
- Simple average combination achieved ~95% accuracy, establishing a strong baseline before CFA weighting

## Why This Works (Mechanism)

### Mechanism 1
Diversity-weighted fusion outperforms performance-weighted fusion for heterogeneous model ensembles. CFA computes Cognitive Diversity (CD) using rank-score characteristic (RSC) functions, which measure how differently two models distribute their confidence scores across ranked predictions. Models with higher Diversity Strength (DS) receive higher weights in fusion, amplifying signals from models that make different errors. Core assumption: Models with dissimilar RSC functions provide complementary information that can correct each other's errors when combined appropriately. Evidence anchors: [abstract] CFA leverages cognitive diversity... [section III-A] performance-weighted score fusion achieved 94.90% accuracy, which is notably less than the 97.072% with diversity weighting. Break condition: If all base models have near-identical RSC functions, diversity weighting collapses to near-uniform weighting with no advantage.

### Mechanism 2
Combining transformer models with bag-of-words models creates complementary error patterns exploitable by fusion. RoBERTa captures contextual semantics through self-attention; classical models operate on word frequency patterns. When RoBERTa fails on unusual linguistic constructions, classical models may succeed if telltale vocabulary is present, and vice versa. Core assumption: Architectural and feature-representation diversity translates to meaningfully different error distributions. Evidence anchors: [section II-B] This architectural diversity translates directly into diverse error patterns... [section III-B] The rank-score diversity analysis revealed significant differences in how the models rank the reviews. Break condition: If classical models provide no signal beyond what RoBERTa already captures, fusion adds noise without benefit.

### Mechanism 3
Rank combination can outperform score combination when score distributions differ significantly across models. CFA supports both score combination (SC) and rank combination (RC). Score combination requires normalized confidence values; rank combination uses ordinal positions. When models have incompatible score scales or calibration, rank combination may be more robust. Core assumption: Rank information preserves meaningful signal even when raw scores are poorly calibrated or incomparable. Evidence anchors: [section II-C] CFA offers various algorithms to combine the scores or ranks from multiple systems... [section III-A, Table II] Best results include both SC methods (97.072%) and RC methods (97.008%). Break condition: If ranks are highly correlated across models, rank combination offers no advantage over simpler voting.

## Foundational Learning

- **Rank-Score Characteristic (RSC) Function**: Links rank order to score distribution for each model. Why needed: RSC functions are the foundation of CFA's diversity measurement. Without understanding how RSC links rank order to score distribution, you cannot interpret why diversity weighting works. Quick check: Given two classifiers that both achieve 90% accuracy but one has steep RSC (high-confidence predictions) and one has flat RSC (uncertain predictions), which provides more information for fusion?

- **Cognitive Diversity vs. Statistical Diversity**: CFA's "cognitive diversity" differs from traditional ensemble diversity measures. Why needed: Understanding this distinction is critical for implementing CFA correctly. Quick check: If two models have uncorrelated prediction errors but identical RSC functions, would CFA consider them cognitively diverse?

- **Score Normalization for Heterogeneous Models**: Required for combining RoBERTa probabilities (0-1) with SVM decision function values (unbounded). Why needed: Incorrect normalization invalidates fusion. Quick check: Why might min-max normalization on test data (instead of training ranges) cause data leakage and inflated performance?

## Architecture Onboarding

- **Component map**: RoBERTa-base-imdb (HuggingFace pretrained) -> subword tokens -> probability scores; SVM (linear, C=1.0, probability=True) -> bag-of-words (20k) -> decision function values -> probability estimates; XGBoost (lr=0.3, max_depth=6, 100 rounds) -> bag-of-words (20k) -> raw scores; Random Forest (25k trees) -> bag-of-words (20k) -> class probabilities; CFA Fusion Layer -> normalizes all scores -> computes RSC functions -> calculates CD/DS -> applies weighted combination (SC or RC)

- **Critical path**: 1) Train/fine-tune all base models independently on same training split; 2) Generate predictions on training set to compute normalization ranges and diversity metrics; 3) Apply normalization and compute RSC functions for each model; 4) Calculate pairwise CD, then DS for each model; 5) Select combination method (AC/WCP/WCDS) and weight scheme; 6) Fuse predictions on test set using pre-computed weights

- **Design tradeoffs**: More estimators in Random Forest (25k) increases diversity contribution but adds memory/compute; including vs. excluding XGBoost - best trio (A,B,D) excludes XGBoost; score vs. rank combination - score combination achieved top result (97.072%), but rank combination was competitive (97.008%)

- **Failure signatures**: Diversity weighting underperforms performance weighting - check if RSC functions are too similar (low CD); fusion accuracy lower than best single model - indicates negative transfer, check normalization correctness and weight signs; large train-test performance gap - models may be overfitting individually

- **First 3 experiments**: 1) Reproduce single-model baselines - verify RoBERTa ~94.67%, SVM ~83.46%, XGBoost ~85.36%, RF ~86.15% before attempting fusion; 2) Implement simple average combination - establish baseline ensemble (should be ~95%+) before implementing CFA weighting; 3) Compare WCP vs. WCDS on held-out validation - directly test paper's central claim that diversity weighting outperforms performance weighting; expect ~2-3 percentage point gap

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the CFA framework maintain its performance advantage on short-form text or specialized domains compared to the long-form IMDb movie reviews? Basis: [explicit] The authors state that because experiments were limited to the IMDb dataset, "The generalizability of our findings to other domains, such as short-form text from social media... remains to be validated." Why unresolved: The current study relies on a benchmark known for long, structured reviews, whereas social media text involves significantly different linguistic characteristics. What evidence would resolve it: Application of the same RoBERTa-classical CFA ensemble to short-text benchmarks (e.g., Twitter sentiment) or domain-specific datasets (e.g., financial news), demonstrating consistent accuracy improvements over baselines.

- **Open Question 2**: Which specific linguistic phenomena (e.g., sarcasm, complex negation) are successfully resolved by the fusion of transformer and bag-of-words models? Basis: [explicit] The authors suggest "a deeper qualitative analysis of the errors corrected by the CFA fusion" is a "promising direction" to understand complementary strengths beyond aggregate metrics. Why unresolved: The current results rely on quantitative accuracy improvements rather than a fine-grained linguistic categorization of corrected instances. What evidence would resolve it: A manual or automated breakdown of the "corrected" test instances, classifying them by linguistic feature (sarcasm, double negation, etc.) to identify patterns in how diversity aids classification.

- **Open Question 3**: Can the inclusion of alternative architectures, such as XLNet or knowledge-based systems, further increase Cognitive Diversity and push accuracy beyond the current state-of-the-art? Basis: [explicit] The authors propose "broad[ening] the pool of base classifiers to include other diverse architectures, such as XLNet... to further probe the limits of cognitive diversity." Why unresolved: The study is restricted to RoBERTa, SVM, Random Forest, and XGBoost; it is unknown if other model paradigms offer sufficient rank-score dissimilarity to improve the fusion further. What evidence would resolve it: Experiments integrating additional heterogeneous models into the ensemble, measuring the resulting Cognitive Diversity (CD) scores and resulting accuracy changes.

## Limitations

- The central claim that diversity-weighted fusion outperforms performance-weighted fusion lacks direct validation in the broader literature, with no independent replication of the diversity-weighting advantage
- The study is limited to the IMDB dataset, and generalizability to other domains such as short-form text from social media remains to be validated
- The specific linguistic phenomena that are successfully resolved by the fusion of transformer and bag-of-words models are not identified or categorized

## Confidence

- **High confidence**: The standalone model accuracies and basic ensemble averaging results are straightforward to verify and align with typical IMDB benchmark performance
- **Medium confidence**: The CFA methodology implementation (normalization, diversity computation) follows established procedures, though details on exact parameter choices could affect reproducibility
- **Low confidence**: The claim that cognitive diversity weighting provides a 2-3 percentage point advantage over performance weighting requires careful validation, as this represents the key novel contribution and is not independently verified in the corpus

## Next Checks

1. **Direct comparison experiment**: Implement both WCP and WCDS weighting schemes on the same data split to verify the claimed 2-3 percentage point advantage of diversity weighting

2. **Ablation study**: Test ensemble performance with and without each classical model to quantify the contribution of architectural diversity beyond simply adding more models

3. **Cross-dataset validation**: Apply the same CFA ensemble to a different sentiment dataset (e.g., Amazon reviews) to test whether cognitive diversity weighting generalizes beyond IMDB