---
ver: rpa2
title: Teaching Language Models to Reason with Tools
arxiv_id: '2510.20342'
source_url: https://arxiv.org/abs/2510.20342
tags:
- code
- reasoning
- arxiv
- python
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of teaching large reasoning models
  to effectively use code interpreters (CIs) for mathematical problem-solving, focusing
  on the conflict between internal probabilistic reasoning and external deterministic
  code execution. The proposed CoRT framework introduces hint-engineering, a data
  synthesis strategy that strategically injects diverse hints at optimal points within
  reasoning paths to generate high-quality, code-integrated reasoning data.
---

# Teaching Language Models to Reason with Tools

## Quick Facts
- arXiv ID: 2510.20342
- Source URL: https://arxiv.org/abs/2510.20342
- Reference count: 40
- The paper addresses the challenge of teaching large reasoning models to effectively use code interpreters (CIs) for mathematical problem-solving, focusing on the conflict between internal probabilistic reasoning and external deterministic code execution.

## Executive Summary
The paper addresses the challenge of teaching large reasoning models to effectively use code interpreters (CIs) for mathematical problem-solving, focusing on the conflict between internal probabilistic reasoning and external deterministic code execution. The proposed CoRT framework introduces hint-engineering, a data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths to generate high-quality, code-integrated reasoning data. Through supervised fine-tuning and rejection fine-tuning on 30 high-quality samples, CoRT achieves absolute improvements of 4% and 8% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B models respectively across five mathematical reasoning datasets, while reducing token usage by approximately 30% for the 32B model and 50% for the 1.5B model compared to pure natural language reasoning baselines.

## Method Summary
The CoRT framework addresses the tension between LLMs' probabilistic reasoning and code interpreters' deterministic execution through a three-stage pipeline: supervised fine-tuning (SFT) with manually hint-engineered samples, rejection fine-tuning (RFT) to scale up with filtered trajectories, and reinforcement learning with code execution feedback. The key innovation is "hint-engineering" - strategically injecting textual prompts at specific points in reasoning traces to redirect the model from manual calculation toward immediate code execution. The framework creates a persistent execution environment and uses output masking to prevent context pollution from error traces during RL training.

## Key Results
- CoRT achieves 4% absolute improvement on DeepSeek-R1-Distill-Qwen-32B and 8% on DeepSeek-R1-Distill-Qwen-1.5B across five mathematical reasoning datasets
- Token usage reduced by approximately 30% for 32B model and 50% for 1.5B model compared to pure natural language reasoning baselines
- Only 30 high-quality samples needed for cold-start training, demonstrating data efficiency through rigorous filtering

## Why This Works (Mechanism)

### Mechanism 1: Hint-Engineering for Behavioral Steering
- **Claim:** Injecting targeted textual hints during synthesis redirects the model from inefficient behaviors (over-thinking or manual calculation) to immediate code execution.
- **Mechanism:** The framework identifies specific failure modes—"delayed code computation" and "code result distrust"—and inserts corrective instructions (e.g., "It looks tedious... use python") at precise token offsets. This conditions the model to treat the Code Interpreter (CI) as a primary reasoning tool rather than a verification fallback.
- **Core assumption:** The model has latent capability to use tools but lacks the policy to trigger them optimally; textual prompts can shifts this policy.
- **Evidence anchors:**
  - [section 2.2.2] Defines "Hint-engineering" to mitigate "delayed code computation" and "code result distrust."
  - [figure 3] Visualizes token reduction when hints redirect manual calculation to Python.
  - [corpus] Neighbor papers (e.g., *Search-o1*) highlight tool integration, but this specific "hint-injection" mechanism is unique to CoRT.
- **Break condition:** If hints are too frequent or generic, the model may learn to rely on code for trivial logic (e.g., "1+1"), increasing latency without accuracy gain.

### Mechanism 2: Data Quality Scaling via Rejection Sampling
- **Claim:** A very small dataset (30 samples) of high-quality, code-interleaved reasoning can outperform larger, noisier datasets through rigorous filtering.
- **Mechanism:** The authors use manual annotation to create a "cold start" dataset ($D_{Hint-engineering-SFT}$). They then scale this via Rejection Fine-Tuning (RFT), sampling trajectories and retaining only those with correct answers and efficient code usage. This filters out "unproductive deliberation" loops.
- **Core assumption:** Reasoning behavior is primarily limited by the quality of the policy's exploration space rather than the diversity of training tokens.
- **Evidence anchors:**
  - [abstract] States they "synthesized 30 high-quality samples" to achieve 4-8% gains.
  - [section 2.2.2] Describes the creation of $D_{Hint-engineering-RFT}$ by filtering 820 problems for correct and efficient trajectories.
  - [corpus] General consensus in reasoning literature (e.g., *LIMO*) supports "less is more" for reasoning data, though CoRT provides specific empirical thresholds.
- **Break condition:** If the 30 seed samples lack diversity (e.g., only algebra), the model will fail to generalize to geometry or combinatorics regardless of RFT volume.

### Mechanism 3: RL Policy Optimization with Execution Feedback
- **Claim:** Reinforcement Learning (GRPO) refines the interleaving of thought and code by directly penalizing failed execution and rewarding final correctness.
- **Mechanism:** The system creates a persistent execution environment. It implements a dual reward: $R = R_{accuracy} + \omega R_{code}$. Crucially, it applies "output masking" to prevent the model from collapsing due to copying long error logs.
- **Core assumption:** The gradient signal from a deterministic code executor is more stable for mathematical reasoning than pure text-based reward models.
- **Evidence anchors:**
  - [section 2.4] Details the GRPO implementation, persistent environment, and Equation (3) for rewards.
  - [figure 6] Ablation study showing the performance delta with vs. without the code execution reward.
  - [corpus] Neighbor *ToRL* also explores tool-integrated RL, validating this general direction, though CoRT’s "output masking" is a distinct stability feature.
- **Break condition:** If the penalty weight $\omega$ is too high (e.g., 0.5), the model becomes risk-averse and stops attempting complex code blocks, degrading performance.

## Foundational Learning

- **Concept:** **Tool-Integrated Reasoning (TIR)**
  - **Why needed here:** Unlike standard Chain-of-Thought (CoT), TIR requires the model to output a program (Python), execute it in an environment, and ingest the result (`stdout`) before continuing the thought process.
  - **Quick check question:** Can you distinguish between a model *verifying* a calculation with code versus *solving* it via code?

- **Concept:** **Rejection Sampling (RFT)**
  - **Why needed here:** The paper relies on generating many candidate solutions and keeping only the "perfect" ones (correct answer + efficient path) to fine-tune the model.
  - **Quick check question:** If a model solves a problem correctly but uses 10x the necessary tokens, should this trajectory be included in the RFT dataset? (CoRT says No).

- **Concept:** **Deterministic vs. Probabilistic Conflict**
  - **Why needed here:** The core tension identified is that LLMs (probabilistic) struggle to trust Code Interpreters (deterministic), leading to redundant verification loops.
  - **Quick check question:** Why does a model "distrust" a Python output like `125.0`?

## Architecture Onboarding

- **Component map:** Input: Math Problem $P$ -> Policy $\pi$: DeepSeek-R1 (LLM) generates text + Python code blocks -> Executor $E$: A persistent Jupyter-like environment executing code -> Data Synthesizer: Human-in-the-loop or automated hint insertion pipeline -> Trainer: SFT $\rightarrow$ RFT $\rightarrow$ RL (GRPO) pipeline with Output Masking.

- **Critical path:** The **Hint-Engineering** insertion point. If the hints (e.g., "We don't need to doubt...") are not injected at the exact token where the model usually starts manual verification, the token-efficiency gains are lost.

- **Design tradeoffs:**
  - **Manual vs. Automated Hints:** The paper found automated hint insertion by LLMs (DeepSeek-V3) "insufficiently precise," opting for manual annotation of 30 samples.
  - **Cold Start Scale:** Training on only 30 samples risks overfitting but ensures high behavioral quality; they mitigate this by upscaling via RFT on 830 samples later.

- **Failure signatures:**
  - **Infinite Loops:** The model writes code that errors, tries to fix it, errors again, etc. (Mitigated by Code Reward Penalty).
  - **Unproductive Deliberation:** The model outputs `We can calculate this...` and writes out arithmetic in text instead of code. (Mitigated by Hint-Engineering).
  - **Context Pollution:** The model copies massive error traceback into the context window, confusing subsequent steps. (Mitigated by Output Masking).

- **First 3 experiments:**
  1. **Prompt-Hint Ablation:** Run the base model on 100 problems with/without the generic "Okay, let's use python" hint to verify the jump in tool usage (50% to 90%).
  2. **SFT Scaling:** Train two 32B models—one on the 30-sample Hint-Engineering set, one on the 800-sample Prompt-Hint set. Compare token efficiency on AIME24.
  3. **RL Reward Ablation:** Train the 1.5B model with GRPO, toggling the code execution penalty ($\omega=0$ vs. $\omega=0.1$). Monitor for "code abandonment" behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Code-integrated Reinforcement Learning (RL) phase be successfully scaled to larger models (e.g., 32B parameters), and does it yield efficiency gains comparable to those observed in 1.5B models?
- Basis in paper: [explicit] The authors state in Section 2.3 that "computational constraints made it infeasible to perform RL on 32B-parameter models," confining the RL experiments to the 1.5B architecture.
- Why unresolved: The paper provides results for SFT/RFT on 32B models and RL on 1.5B models, but the combination of the full RL pipeline on high-parameter models remains untested due to infrastructure limits.
- What evidence would resolve it: Experimental results showing the performance and token efficiency of a Hint-Engineering-32B-RL model trained using the GRPO algorithm and code reward design described in the paper.

### Open Question 2
- Question: Can the identification of optimal hint insertion points be automated to maintain precision without relying on manual annotation?
- Basis in paper: [explicit] In Section 2.2.2, the authors note that "identifying suitable positions for hint insertion" was a critical challenge. They attempted automation using DeepSeek-V3 and R1 but "found the results to be insufficiently precise," forcing them to "opt for manual hint insertion."
- Why unresolved: The current "Hint-Engineering" method relies on human-in-the-loop annotation for the cold-start dataset (30 samples). The scalability of this approach is limited if high precision cannot be achieved automatically.
- What evidence would resolve it: A proposed algorithm or model capable of detecting "delayed code computation" or "code result distrust" with accuracy equal to or better than human annotators, validated by training a model on auto-annotated data that performs comparably to the manually annotated baseline.

### Open Question 3
- Question: Do the efficiency and reasoning improvements from code-integrated training transfer effectively to other external tools beyond code interpreters?
- Basis in paper: [explicit] In the Limitations section (K), the authors state that "there remain opportunities to investigate additional synergies between external tools and language models’ inherent reasoning capabilities," implying the current work is specific to code interpreters.
- Why unresolved: While the paper demonstrates strong results for Python code execution, it does not verify if the "Hint-Engineering" strategy (e.g., preventing verification of deterministic outputs) generalizes to non-computational tools like search engines or formal theorem provers.
- What evidence would resolve it: Evaluations of models trained with the CoRT framework on tasks requiring diverse tools (e.g., Retrieval-Augmented Generation or Lean theorem proving) to see if token efficiency and accuracy gains persist.

## Limitations
- The manual nature of the cold-start dataset creation creates a scalability bottleneck that the paper partially addresses through automated rejection fine-tuning, but the transition between these phases lacks detailed validation.
- The RL phase using only 1,000 hard samples may not provide sufficient exploration of the policy space to prevent catastrophic forgetting of non-mathematical reasoning capabilities.
- The effectiveness of the hint-engineering approach may not generalize beyond the specific mathematical domains tested.

## Confidence
- **High Confidence:** The token efficiency improvements (30-50% reduction) are well-documented through controlled ablations and are directly measurable. The framework's architecture and training pipeline are clearly specified.
- **Medium Confidence:** The absolute accuracy improvements (4-8%) are statistically significant within the tested datasets but may not generalize to broader mathematical reasoning tasks or different model families.
- **Low Confidence:** The long-term stability of the learned policy after RL training and the model's behavior on problems requiring mixed reasoning (mathematical and non-mathematical) are not adequately characterized.

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate the trained 32B model on non-mathematical reasoning tasks (e.g., commonsense reasoning, coding problems) to assess whether the tool-integrated reasoning policy transfers or degrades performance in other domains.

2. **Hint Robustness Analysis:** Systematically vary the timing, frequency, and specificity of injected hints across a validation set to identify the optimal hint-injection policy and test whether the current manual approach is actually optimal or merely sufficient.

3. **Scaling Law Investigation:** Conduct a controlled experiment varying the size of the cold-start dataset (e.g., 10, 30, 100, 300 samples) while keeping other factors constant to quantify the relationship between dataset size and performance gains, particularly for the RFT phase.