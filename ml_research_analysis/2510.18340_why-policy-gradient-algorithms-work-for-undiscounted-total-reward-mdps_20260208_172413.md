---
ver: rpa2
title: Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs
arxiv_id: '2510.18340'
source_url: https://arxiv.org/abs/2510.18340
tags:
- policy
- gradient
- proof
- convergence
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses convergence analysis of policy gradient methods
  in undiscounted total-reward infinite-horizon MDPs, a setting increasingly relevant
  for training large language models but lacking rigorous theoretical foundations.
  The key insight is that for policies assigning strictly positive probability to
  all actions (the typical case in deep RL with softmax output layers), the classification
  of states into recurrent and transient is invariant.
---

# Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs

## Quick Facts
- arXiv ID: 2510.18340
- Source URL: https://arxiv.org/abs/2510.18340
- Authors: Jongmin Lee; Ernest K. Ryu
- Reference count: 39
- Key outcome: Proves convergence of policy gradient methods in undiscounted total-reward MDPs using transient visitation measure, with applications to LLM training

## Executive Summary
This paper establishes theoretical foundations for policy gradient methods in undiscounted total-reward infinite-horizon Markov decision processes, a setting increasingly relevant for training large language models through reinforcement learning from human feedback. The key insight is that for policies assigning strictly positive probability to all actions (typical in deep RL with softmax output layers), the classification of states into recurrent and transient remains invariant during optimization. This enables defining a "transient visitation measure" that replaces the ill-defined classical visitation measure when the discount factor γ=1, allowing convergence analysis to proceed. The authors prove convergence results for projected policy gradient and natural policy gradient algorithms, with rates independent of state and action space sizes for the latter.

## Method Summary
The method centers on defining a "transient visitation measure" that replaces the standard discounted visitation measure when γ=1. This measure is computed using only the transient block of the transition matrix, which has spectral radius less than 1, ensuring convergence. Two algorithms are analyzed: Projected Policy Gradient (PPG) with direct parameterization requiring explicit projection onto the interior of policy space, and Natural Policy Gradient (NPG) with softmax parameterization that stays naturally in the interior. The PPG converges to ε-optimality with rate O(1/k) where the constant depends on state and action space sizes, while NPG achieves both sublinear and linear convergence rates independent of these sizes. Experiments validate convergence on Frozenlake, Cliffwalk, and a pathological MDP demonstrating the theoretical predictions.

## Key Results
- Policy gradient methods converge in undiscounted MDPs when policies assign strictly positive probability to all actions
- The transient visitation measure enables Policy Gradient Theorem to hold for γ=1 by replacing the ill-defined classical measure
- NPG with softmax parameterization achieves convergence rates independent of state and action space sizes
- PPG requires shrinkage parameter α that creates a gap between achieved and true optimal values
- Pathological MDP demonstrates fundamental limitation where NPG cannot reach boundary-optimal policies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Policy gradient methods can converge in undiscounted (γ=1) MDPs because the structure of the Markov chain remains stable during optimization, provided the policy assigns strictly positive probability to all actions.
- **Mechanism:** The paper proves that for any policy π ∈ Π⁺ (policies with π(a|s) > 0), the classification of states into "recurrent" (visited infinitely often) and "transient" (visited finitely often) is invariant (Proposition 1). This invariance prevents the value function from becoming discontinuous as the policy shifts, a pathology that usually breaks convergence in undiscounted settings.
- **Core assumption:** The optimization stays within the relative interior of the policy space Π⁺ (typical for softmax outputs).
- **Evidence anchors:**
  - [abstract] "the classification of the MDP states into recurrent and transient states is invariant over the set of policies that assign strictly positive probability to every action"
  - [section 4.1] Proposition 1 states the classification does not depend on the choice of π ∈ Π⁺.
- **Break condition:** If the policy assigns zero probability to any action (moving to the boundary of Π), the recurrent-transient classification may change, causing discontinuities in the value function and breaking the gradient signal.

### Mechanism 2
- **Claim:** The "transient visitation measure" replaces the standard discounted visitation measure, allowing the Policy Gradient Theorem to hold for γ=1.
- **Mechanism:** In standard discounted RL, the state visitation measure is a geometric series of the transition matrix P^π. When γ=1, this series often diverges. The authors define the **transient visitation measure** δ^π using only the transient block T^π of the transition matrix: δ^π ∝ (I - T^π)⁻¹. Since the spectral radius of T^π < 1 (Fact 1), this inverse exists and is finite, providing a well-defined gradient.
- **Core assumption:** The value function V^π is finite (Assumption 1), implying accumulated rewards in recurrent states must sum to zero (Lemma 1).
- **Evidence anchors:**
  - [abstract] "...the classical state visitation measure... can be replaced with a new object that we call the transient visitation measure."
  - [section 4.3] Defines δ^π_{s₀}(s) = e^⊤_{s₀}(I-T^π)⁻¹e_s.
- **Break condition:** If rewards are allowed to accumulate infinitely in recurrent states (violating Assumption 1), the value function is undefined, and the transient measure no longer captures the total reward objective.

### Mechanism 3
- **Claim:** Natural Policy Gradient (NPG) with softmax parameterization converges to the global optimum without requiring a discount factor, whereas Projected PG suffers from dependency on state-space size.
- **Mechanism:** NPG with softmax implicitly constrains the policy to Π⁺, avoiding the boundary pathologies mentioned in Mechanism 1. The update rule (multiplicative weights) ensures monotonic improvement. The paper proves NPG converges to V⁺_{*, μ} (Corollary 2), and if rewards are nonnegative, this equals the true global optimum V*.
- **Core assumption:** Rewards are nonnegative (for global optimality guarantees).
- **Evidence anchors:**
  - [abstract] "For natural policy gradient... rates independent of state and action space sizes."
  - [section 6] Theorem 3 and Corollary 2 show convergence rates of O(1/k) or linear depending on step size.
- **Break condition:** If rewards are negative and the policy approaches a boundary where V^π is discontinuous (Pathology 1), NPG might converge to a suboptimal interior point or fail to stabilize if the softmax collapses.

## Foundational Learning

- **Concept: Markov Chain Recurrence**
  - **Why needed here:** The entire theoretical framework relies on splitting states into "transient" and "recurrent." You must understand that recurrent states are visited infinitely often, implying that for the total reward to be finite, the reward in these states must effectively be zero (or controlled).
  - **Quick check question:** If a state is recurrent and yields a reward of +1 at every step, does the total reward V^π remain finite?

- **Concept: Spectral Radius and Matrix Invertibility**
  - **Why needed here:** The "transient visitation measure" is defined as (I - T^π)⁻¹. This Neumann series only converges (is finite) if the spectral radius of T^π is strictly less than 1. This is the mathematical reason why the transient measure works where the standard measure fails.
  - **Quick check question:** Why does the transient transition matrix T^π have a spectral radius < 1, while the full transition matrix P^π has a spectral radius of 1?

- **Concept: Interior vs. Boundary in Policy Space**
  - **Why needed here:** The paper highlights a pathology where the value function is discontinuous at the boundary of the policy space (where some actions have 0 probability). Understanding the difference between searching over Π (all policies) vs. Π⁺ (interior policies) is critical for choosing between Projected PG (needs projection) and NPG (stays in interior).
  - **Quick check question:** Why does the softmax parameterization guarantee the policy stays in the "interior" (Π⁺), and how does this protect against the discontinuity pathology?

## Architecture Onboarding

- **Component map:** Policy Engine (softmax parameterization θ → π_θ) -> Transition Analyzer (computes P^π, decomposes into T^π and R^π) -> Value Estimator (computes V^π = (I - T^π)⁻¹r^π) -> Optimizer (Natural Policy Gradient update using transient gradient)

- **Critical path:** The definition of the **Transient Visitation Measure** δ^π. The algorithm relies on the fact that the "transient" classification is invariant. If your implementation estimates visitation measures via sampling, you must ensure the trajectory length is sufficient to distinguish transient dynamics from recurrent ones, or the gradient will be biased.

- **Design tradeoffs:**
  - **Projected PG vs. NPG:** Projected PG is simpler but converges to an ε-optimal policy dependent on the "shrinkage" parameter α and state-space size |S|. NPG is independent of state-space size and converges to the true optimum (for non-negative rewards) but requires Fisher Information matrix inversion (or log-domain updates).
  - **Direct vs. Softmax:** Direct parameterization requires explicit projection to avoid boundary pathologies. Softmax handles this naturally but might suffer from saturation if logits grow too large.

- **Failure signatures:**
  - **Oscillation/Divergence:** Likely caused by violating Assumption 1 (infinite value function), perhaps due to non-zero rewards in recurrent states.
  - **Convergence to Sub-optimal:** Occurs in "Pathological MDPs" (Figure 1) where the optimal policy lies on the boundary (discontinuous). NPG will converge to the best *interior* policy, which might be strictly worse than the boundary optimal policy.
  - **Slow Convergence:** Using Projected PG on large state spaces (O(|S||A|/k)).

- **First 3 experiments:**
  1. **FrozenLake Validation:** Implement NPG with the transient gradient on a sparse reward gridworld. Verify convergence to V* matches the paper's theoretical rate.
  2. **Pathology Test:** Replicate the "Figure 1" MDP. Confirm that while standard value-iteration might struggle with the Q* pathology, the NPG converges to the interior optimum V⁺_μ but fails to reach the boundary optimum V*.
  3. **Step Size Ablation:** Compare constant step size vs. adaptive step size (Theorem 4) for NPG. Confirm that adaptive step sizes yield linear convergence while constant yields sublinear, as predicted.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the naive policy gradient algorithm (without Fisher information preconditioning) converge for undiscounted total-reward MDPs with softmax parameterization?
- **Basis in paper:** [explicit] "Another promising direction is to establish the convergence of the naive policy gradient with softmax parameterization, without preconditioning by the Fisher information matrix."
- **Why unresolved:** The paper analyzes projected policy gradient and natural policy gradient, but naive policy gradient convergence remains unproven in this setting.
- **What evidence would resolve it:** A convergence proof with rate bounds, or a counterexample showing divergence/non-convergence for naive policy gradient.

### Open Question 2
- **Question:** Can the transient policy gradient framework be extended to function approximation settings with finite samples?
- **Basis in paper:** [explicit] "One direction is to extend our results to function approximation in a sampling setting, where restricted parametric policies may not include the optimal policy, and the estimation and optimization errors from finite samples must be quantified."
- **Why unresolved:** Current analysis assumes tabular MDPs with exact gradient computation; practical deep RL uses function approximation and sampled estimates.
- **What evidence would resolve it:** Convergence guarantees with explicit sample complexity bounds for neural network policies under the transient policy gradient framework.

### Open Question 3
- **Question:** Under what conditions beyond nonnegative rewards does V*₊,μ = V*_μ, ensuring policy gradient methods converge to the true optimum?
- **Basis in paper:** [inferred] The paper shows V*₊,μ = V*_μ for nonnegative rewards, but the pathological MDP (Figure 1) demonstrates a gap exists for general rewards, suggesting boundary conditions matter.
- **Why unresolved:** The distinction between V*₊,μ and V*_μ is characterized only for nonnegative rewards; the general case remains unclear.
- **What evidence would resolve it:** A characterization theorem identifying necessary and sufficient conditions on reward structure or MDP topology for equality of these values.

### Open Question 4
- **Question:** Can the recurrent-transient classification technique be applied to analyze other RL algorithms (e.g., actor-critic, TRPO, PPO) in the undiscounted total-reward setting?
- **Basis in paper:** [explicit] "We expect that this technique can be used to analyze a wide range of RL algorithms in the undiscounted total-reward setting."
- **Why unresolved:** The technique was demonstrated only for projected and natural policy gradient methods.
- **What evidence would resolve it:** Convergence analyses for additional policy optimization algorithms using the transient visitation measure framework.

## Limitations

- **Boundary behavior limitation:** The framework requires policies to assign strictly positive probability to all actions, which may not hold in practice with large action spaces or deterministic policies
- **Model-based requirement:** The transient visitation measure computation requires full knowledge of transition dynamics, limiting applicability to model-based settings
- **Fundamental optimality gap:** Pathological MDP demonstrates that NPG cannot reach boundary-optimal policies even when they exist, showing theoretical guarantees may not translate to practical performance improvements

## Confidence

- **High Confidence:** The invariant recurrent-transient classification (Mechanism 1) and transient visitation measure derivation (Mechanism 2) are mathematically rigorous with formal proofs provided
- **Medium Confidence:** Convergence rates for NPG (Mechanism 3) are well-established for softmax parameterization, though practical performance may vary with problem structure
- **Medium Confidence:** PPG convergence guarantees depend heavily on the shrinkage parameter α and projection accuracy, which weren't extensively validated in experiments

## Next Checks

1. **Boundary Behavior Test:** Systematically test what happens when policies approach the boundary (softmax saturation) - measure how quickly recurrent-transient classification changes and whether value function discontinuities emerge
2. **Scalability Validation:** Evaluate the O(1/k) convergence rate empirically across different state-action space sizes to verify the theoretical dependence on |S| and |A|
3. **Reward Structure Impact:** Test convergence behavior with negative rewards and mixed reward structures to identify when Assumption 1 fails and transient measure becomes insufficient