---
ver: rpa2
title: Hear What Matters! Text-conditioned Selective Video-to-Audio Generation
arxiv_id: '2512.02650'
source_url: https://arxiv.org/abs/2512.02650
tags:
- video
- audio
- text
- sound
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating user-intended sound
  from multi-object videos, a key requirement for precise audio editing in multimedia
  production. The proposed SELVA model introduces a text-conditioned video encoder
  that uses text prompts as explicit selectors to extract prompt-relevant video features,
  achieved through a learnable supplementary token mechanism that refines cross-attention
  and suppresses irrelevant activations.
---

# Hear What Matters! Text-conditioned Selective Video-to-Audio Generation

## Quick Facts
- arXiv ID: 2512.02650
- Source URL: https://arxiv.org/abs/2512.02650
- Authors: Junwon Lee; Juhan Nam; Jiyoung Lee
- Reference count: 40
- Primary result: SELVA achieves state-of-the-art text-conditioned selective audio generation for multi-object videos, outperforming baselines in audio quality, semantic alignment, and temporal synchronization on VGG-MONOAUDIO benchmark.

## Executive Summary
This work addresses the challenge of generating user-intended sound from multi-object videos, a key requirement for precise audio editing in multimedia production. The proposed SELVA model introduces a text-conditioned video encoder that uses text prompts as explicit selectors to extract prompt-relevant video features, achieved through a learnable supplementary token mechanism that refines cross-attention and suppresses irrelevant activations. SELVA also employs a self-augmentation training scheme to learn selective generation without requiring mono audio supervision. Evaluated on the new VGG-MONOAUDIO benchmark, SELVA achieves state-of-the-art performance in audio quality (e.g., lower FAD and KAD, higher IS), semantic alignment (higher CLAP and IB scores), and temporal synchronization (lower DeSync), outperforming existing methods including VOS+MMAudio and text-conditioned baselines. Human perceptual studies confirm SELVA's superiority in controllability and temporal coherence.

## Method Summary
SELVA introduces a text-conditioned video encoder that leverages text prompts as explicit selectors to guide audio generation from multi-object videos. The key innovation is a learnable supplementary token mechanism that refines cross-attention to focus on prompt-relevant video features, effectively suppressing irrelevant audio sources. The model also employs a self-augmentation training scheme, enabling selective generation without the need for mono audio supervision. Evaluated on the new VGG-MONOAUDIO benchmark, SELVA demonstrates state-of-the-art performance in audio quality, semantic alignment, and temporal synchronization, with human studies confirming its controllability and coherence.

## Key Results
- SELVA achieves state-of-the-art audio quality (lower FAD/KAD, higher IS) on VGG-MONOAUDIO benchmark.
- Superior semantic alignment (higher CLAP/IB scores) and temporal synchronization (lower DeSync) compared to baselines.
- Human perceptual studies confirm SELVA's superiority in controllability and temporal coherence.

## Why This Works (Mechanism)
SELVA's effectiveness stems from its text-conditioned video encoder, which uses text prompts as explicit selectors to extract relevant video features. The learnable supplementary token mechanism refines cross-attention, suppressing irrelevant activations and focusing on the intended sound source. The self-augmentation training scheme enables selective generation without requiring mono audio supervision, making the model more flexible and practical.

## Foundational Learning
- **Text-Conditioned Audio Generation**: Needed to control which sound source is generated from multi-object videos. Quick check: Verify prompt relevance via CLAP/IB scores.
- **Supplementary Token Mechanism**: Required to refine cross-attention and suppress irrelevant features. Quick check: Compare performance with/without supplementary tokens.
- **Self-Augmentation Training**: Enables selective generation without mono audio supervision. Quick check: Test model performance on mono vs. augmented data.
- **Cross-Modal Alignment**: Ensures generated audio matches the semantics of the text prompt and video. Quick check: Evaluate via human perceptual studies.
- **Temporal Synchronization**: Critical for coherent audio-video alignment. Quick check: Measure DeSync scores.
- **Attention Refinement**: Suppresses irrelevant activations for focused sound generation. Quick check: Analyze attention maps for prompt relevance.

## Architecture Onboarding

### Component Map
Text Encoder -> Learnable Supplementary Token -> Video Encoder -> Cross-Attention Refinement -> Audio Decoder

### Critical Path
The critical path is: Text Encoder → Learnable Supplementary Token → Video Encoder → Cross-Attention Refinement → Audio Decoder. This path ensures that text prompts guide the extraction of relevant video features, which are then used to generate the intended audio.

### Design Tradeoffs
SELVA trades off model complexity (due to the supplementary token mechanism) for improved controllability and selectivity. The self-augmentation training scheme avoids the need for mono audio supervision, enhancing practicality but potentially limiting fine-grained control.

### Failure Signatures
SELVA may struggle with overlapping or ambiguous sound sources in complex acoustic scenes. The supplementary token mechanism might not fully isolate intended sounds if text prompts are vague or if video features are ambiguous.

### 3 First Experiments
1. Test SELVA on a simple single-object video to verify basic functionality.
2. Evaluate performance on a multi-object video with distinct sound sources to assess selectivity.
3. Compare SELVA's output with ground truth mono audio (if available) to measure precision.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but potential areas for future work include testing SELVA's generalizability to real-world multi-object videos with overlapping or ambiguous sound sources, and comparing its performance against audio separation baselines.

## Limitations
- Generalizability to complex, real-world multi-object scenarios remains uncertain.
- The supplementary token mechanism's contribution is not fully isolated from other architectural choices.
- SELVA's selective generation is a softer form of control than fine-grained object-level editing.

## Confidence
- Audio quality and alignment metrics: **High**
- Text-conditioned selective generation via supplementary token: **Medium**
- Generalizability to complex, real-world multi-object scenarios: **Low**
- Human perceptual superiority: **Medium**

## Next Checks
1. Test SELVA on a diverse set of real-world multi-object videos with overlapping or ambiguous sound sources to evaluate robustness beyond the benchmark.
2. Conduct an ablation isolating the effect of the learnable supplementary token from other architectural choices (e.g., text conditioning, attention tuning).
3. Compare SELVA's selective generation performance against audio separation baselines on shared tasks to benchmark precision and control.