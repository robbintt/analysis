---
ver: rpa2
title: 'Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali
  to Sylheti'
arxiv_id: '2510.18898'
source_url: https://arxiv.org/abs/2510.18898
tags:
- translation
- sylheti
- language
- machine
- bengali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a machine translation study on the Bengali\u2013\
  Sylheti language pair, a low-resource scenario due to Sylheti's underrepresentation\
  \ in NLP resources. The authors fine-tuned three transformer-based models\u2014\
  mBART-50, MarianMT, and NLLB-200\u2014on a curated 5,002-sentence parallel corpus,\
  \ and compared their performance with five zero-shot large language models (LLMs)\
  \ including GPT-4o, Claude Sonnet 4, Gemini 2.5 Flash, DeepThinkR1, and Sonar."
---

# Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti

## Quick Facts
- arXiv ID: 2510.18898
- Source URL: https://arxiv.org/abs/2510.18898
- Authors: Mangsura Kabir Oni; Tabia Tanzin Prama
- Reference count: 28
- Primary result: mBART-50 fine-tuned on 5,002 parallel sentences achieved BLEU 21.17, outperforming all zero-shot LLMs (BLEU ≤ 2.89)

## Executive Summary
This paper presents a machine translation study on the Bengali–Sylheti language pair, a low-resource scenario due to Sylheti's underrepresentation in NLP resources. The authors fine-tuned three transformer-based models—mBART-50, MarianMT, and NLLB-200—on a curated 5,002-sentence parallel corpus, and compared their performance with five zero-shot large language models (LLMs) including GPT-4o, Claude Sonnet 4, Gemini 2.5 Flash, DeepThink_R1, and Sonar. Results show that fine-tuned transformers significantly outperformed zero-shot LLMs, with mBART-50 achieving the highest BLEU score of 21.17 and MarianMT the highest chrF score of 36.41. The mBART-50 model showed strong semantic adequacy but unstable training, while MarianMT provided stable, morphologically accurate translations. In contrast, zero-shot LLMs performed poorly (BLEU ≤ 2.89, chrF ≤ 17.33), often producing literal or code-mixed outputs. The study concludes that fine-tuned transformer models are essential for low-resource language translation, and recommends mBART-50 with early stopping as the best system for this task.

## Method Summary
The authors constructed a 5,002-sentence parallel corpus from newspapers, social media, native speakers, and literature for the Bengali–Sylheti language pair. The corpus was cleaned, normalized, and split 80/20 for training and testing. Data augmentation techniques including back-translation, synonym replacement, and character-level perturbation were applied. Three transformer-based models (mBART-50, MarianMT, NLLB-200) were fine-tuned with sequence-to-sequence training using AdamW optimizer, FP16 precision, and weight decay. Early stopping was implemented for mBART-50 to prevent overfitting. Zero-shot LLMs (GPT-4o, Claude Sonnet 4, Gemini 2.5 Flash, DeepThink_R1, Sonar) were evaluated via standard prompting. Translation quality was assessed using BLEU and chrF metrics on the held-out test set.

## Key Results
- mBART-50 achieved the highest translation adequacy with BLEU score of 21.17
- MarianMT demonstrated the most stable performance with highest chrF score of 36.41
- Fine-tuned transformers outperformed zero-shot LLMs by an average of 15.57 BLEU points
- mBART-50 showed training instability, peaking at epoch 5 then dropping sharply before recovering

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned multilingual transformers outperform zero-shot LLMs for low-resource translation due to task-specific weight adaptation. Multilingual pretraining creates cross-lingual representations that can be efficiently adapted to new language pairs through gradient updates on parallel data. The encoder-decoder attention learns direct source-to-target mappings rather than relying on emergent translation capability from general pretraining.

Core assumption: The source language (Bengali) shares sufficient representation space with target (Sylheti) in pretrained models for effective transfer.

Evidence anchors:
- "fine-tuned models significantly outperform LLMs, with mBART-50 achieving the highest translation adequacy"
- "fine-tuned transformers outperformed zero-shot LLMs by an average of 15.57 BLEU (17.66 vs. 2.09)"
- Neighboring papers confirm RAG and prompting approaches for Sylheti underperform compared to fine-tuning

Break condition: If parallel corpus is too small (<1000 pairs) or too noisy, fine-tuning may overfit rather than generalize, collapsing this advantage.

### Mechanism 2
MarianMT's stable convergence and superior chrF scores stem from its lighter architecture being better matched to limited data regimes. Smaller parameter count (6-layer encoder/decoder vs. 12-layer in mBART-50) reduces overfitting risk on small datasets, while SentencePiece tokenization preserves morphological patterns that character-level metrics (chrF) reward.

Core assumption: Morphological fidelity correlates with downstream usability for Sylheti's orthographically variable writing system.

Evidence anchors:
- "MarianMT achieved the most stable and consistent performance...chrF attained the highest score in this study at 36.41"
- "Training loss decreased smoothly from 2.72 to 0.41, and validation loss stabilized"
- Limited direct evidence—neighbor papers don't compare MarianMT specifically

Break condition: If morphological correctness doesn't align with human-perceived quality, chrF optimization becomes a proxy failure.

### Mechanism 3
mBART-50's training instability despite strong peak performance reflects sensitivity to learning dynamics in low-resource settings. Large multilingual models have higher capacity that rapidly memorize limited training data, causing BLEU to spike then crash (epoch 5: 21.17 → epoch 7: 11.99). Early stopping captures generalization before memorization dominates.

Core assumption: Peak BLEU corresponds to genuine semantic adequacy rather than coincidental alignment with test set patterns.

Evidence anchors:
- "BLEU peaked at 21.17 (epoch 5), but dropped sharply to 11.99 (epoch 7) before recovering"
- "Early stopping based on BLEU scores prevented overfitting"
- No direct corpus evidence on early stopping for Sylheti specifically

Break condition: If validation set is too small to reliably detect overfitting onset, early stopping triggers become noisy.

## Foundational Learning

- **Encoder-Decoder Attention**
  - Why needed here: All three transformer models (mBART-50, MarianMT, NLLB-200) use this architecture to map Bengali source sequences to Sylheti target sequences autoregressively.
  - Quick check question: Can you explain why cross-attention allows the decoder to focus on different source positions at each generation step?

- **Subword Tokenization (SentencePiece/BPE)**
  - Why needed here: Sylheti's non-standardized orthography and morphological richness require subword units to handle out-of-vocabulary words and share cross-lingual morphology with Bengali.
  - Quick check question: Why does character-level chrF metric favor models that preserve morphological structure through subword tokenization?

- **Transfer Learning for Low-Resource NLP**
  - Why needed here: The core hypothesis is that multilingual pretraining provides sufficient inductive bias to adapt to Sylheti with only 5,002 parallel sentences.
  - Quick check question: What happens to transfer learning benefits when the target language is underrepresented in pretraining data?

## Architecture Onboarding

- **Component map**: Data Layer (Parallel corpus → preprocessing → 80/20 split → tokenization) → Model Layer (Transformer encoder-decoder with multilingual embeddings) → Training Layer (Sequence-to-sequence loss, AdamW, FP16, early stopping) → Evaluation Layer (BLEU + chrF on test set)

- **Critical path**: Corpus quality → augmentation robustness → tokenizer compatibility → learning rate/early stopping tuning → metric selection for deployment goals

- **Design tradeoffs**:
  - mBART-50: Highest semantic adequacy (BLEU 21.17) vs. training instability requiring careful early stopping
  - MarianMT: Stable convergence + best morphological fidelity (chrF 36.41) vs. slightly lower semantic scores
  - NLLB-200: Largest language coverage vs. adaptation difficulties on niche pairs
  - Zero-shot LLMs: No training cost vs. catastrophic quality degradation (BLEU ≤2.89)

- **Failure signatures**:
  - BLEU spiking then crashing = overfitting on limited data (mBART-50 pattern)
  - chrF high but BLEU low = morphologically plausible but semantically drifted outputs
  - Code-mixed or literal transfer outputs = zero-shot model lacks target language representation
  - Validation loss increasing while training loss decreases = memorization onset

- **First 3 experiments**:
  1. **Baseline establishment**: Run zero-shot translation with best available LLM (Gemini 2.5 Flash) to confirm expected poor performance (BLEU <3) and document failure modes (code-mixing, literal transfers).
  2. **MarianMT pilot**: Fine-tune MarianMT with default hyperparameters for 30 epochs, logging BLEU/chrF per epoch to establish stable convergence baseline before experimenting with larger models.
  3. **mBART-50 with early stopping**: Implement epoch-level checkpointing and validation BLEU monitoring, stopping when BLEU drops >15% from peak (based on paper's epoch 5→7 crash pattern).

## Open Questions the Paper Calls Out
None

## Limitations
- Parallel corpus not publicly available, restricting independent verification and broader research adoption
- 5,002-sentence parallel corpus represents a relatively small dataset for training large multilingual models, potentially limiting generalization
- Zero-shot LLM evaluation uses only standard prompting without specialized prompting strategies or retrieval augmentation
- Study focuses exclusively on automatic metrics (BLEU and chrF) without human evaluation to validate semantic adequacy and fluency claims

## Confidence
High: Fine-tuned transformer models significantly outperforming zero-shot LLMs (BLEU 17.66 vs 2.09) is a well-established finding in low-resource MT research.
Medium: Specific performance numbers (mBART-50 BLEU 21.17, MarianMT chrF 36.41) are reliable but may not generalize to other domains or test sets.
Low: Claims about training instability patterns and optimal stopping points require replication on the actual corpus, which is not publicly available.

## Next Checks
1. Verify mBART-50 training instability by logging BLEU/chrF per epoch and confirming the peak-then-crash pattern described
2. Test whether early stopping at epoch 5 consistently prevents overfitting for mBART-50 on this dataset
3. Compare MarianMT's stable convergence with mBART-50's instability across multiple random seeds to assess robustness of the findings