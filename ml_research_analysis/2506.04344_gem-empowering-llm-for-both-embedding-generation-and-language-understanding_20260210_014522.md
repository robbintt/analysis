---
ver: rpa2
title: 'GEM: Empowering LLM for both Embedding Generation and Language Understanding'
arxiv_id: '2506.04344'
source_url: https://arxiv.org/abs/2506.04344
tags:
- text
- special
- embedding
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEM, a method that enables large decoder-only
  language models (LLMs) to generate high-quality text embeddings while preserving
  their original generation and reasoning capabilities. The approach inserts special
  tokens into input text and manipulates attention masks to encourage the model to
  compress information into these tokens.
---

# GEM: Empowering LLM for both Embedding Generation and Language Understanding

## Quick Facts
- arXiv ID: 2506.04344
- Source URL: https://arxiv.org/abs/2506.04344
- Authors: Caojin Zhang, Qiang Zhang, Ke Li, Sai Vidyaranya Nuthalapati, Benyu Zhang, Jason Liu, Serena Li, Lizhu Zhang, Xiangjun Fan
- Reference count: 10
- Primary result: Method enables decoder-only LLMs to generate high-quality text embeddings while preserving language understanding capabilities

## Executive Summary
GEM introduces a novel approach that enables large decoder-only language models (LLMs) to generate high-quality text embeddings without sacrificing their original generation and reasoning capabilities. The method works by inserting special tokens into input text and manipulating attention masks to encourage the model to compress information into these tokens. Applied to Llama and Mistral models ranging from 1B to 8B parameters, GEM significantly improves performance on the Massive Text Embedding Benchmark (MTEB) while maintaining minimal impact on language understanding (MMLU). The approach requires as little as 32,000 training rows, making it computationally efficient compared to alternatives.

## Method Summary
GEM works by inserting special tokens into input text and manipulating attention masks to encourage the model to compress information into these tokens. The method uses mixed next-token prediction loss with optional special tokens and contrastive learning to improve embedding quality. The special tokens act as information bottlenecks, forcing the model to learn meaningful representations. During inference, the model generates special token embeddings that serve as the text representations. The approach is designed to be compatible with any existing LLM and can be integrated into post-training or fine-tuning stages. The method was validated on Llama and Mistral families of models with parameter sizes ranging from 1B to 8B.

## Key Results
- GEM significantly improves MTEB scores on Llama and Mistral models (1B-8B parameters) while maintaining language understanding (MMLU)
- The method requires only 32,000 training rows, demonstrating high data efficiency
- Llama 3-8B model showed performance degradation (MMLU dropped from 66.7 to 57.24), suggesting hyperparameter sensitivity at larger scales
- Special tokens enable information compression without requiring architectural modifications to the base models

## Why This Works (Mechanism)
GEM leverages the inherent capabilities of decoder-only LLMs by repurposing their attention mechanisms. By inserting special tokens and manipulating attention masks, the method creates an information bottleneck that forces the model to learn compressed representations. The mixed next-token prediction loss with contrastive learning ensures that these compressed representations capture meaningful semantic information while maintaining the model's ability to generate text. This approach exploits the model's existing capacity for information processing without requiring architectural changes.

## Foundational Learning
- **Attention mask manipulation**: Why needed - to control information flow and create compression bottlenecks; Quick check - verify attention weights are properly masked during training
- **Special token insertion**: Why needed - to provide explicit compression targets within the sequence; Quick check - ensure special tokens are correctly positioned and indexed
- **Mixed loss functions**: Why needed - to balance embedding quality with language understanding preservation; Quick check - monitor both reconstruction and contrastive loss during training
- **Contrastive learning**: Why needed - to improve embedding quality by learning semantic similarity; Quick check - validate embedding distances reflect semantic relationships
- **Information bottleneck concept**: Why needed - to force meaningful compression of input text; Quick check - test embedding recovery quality on known sequences
- **Tokenization compatibility**: Why needed - to ensure consistent processing across different model families; Quick check - verify tokenization doesn't break special token functionality

## Architecture Onboarding

**Component Map:** Input text -> Tokenizer -> Special token insertion -> Attention mask manipulation -> Mixed loss (next-token + contrastive) -> GEM-trained LLM -> Special token embeddings (output)

**Critical Path:** Special token insertion → Attention mask manipulation → Mixed loss computation → Parameter updates. This sequence ensures the model learns to compress information into special tokens while maintaining generation capabilities.

**Design Tradeoffs:** GEM trades computational overhead during training (special tokens and attention manipulation) for inference efficiency (simple embedding extraction). The approach preserves the base model architecture, avoiding the complexity of dual-model systems but potentially limiting maximum embedding quality compared to specialized embedding models.

**Failure Signatures:** 
- Poor MTEB performance despite successful training → insufficient contrastive learning signal
- Significant MMLU degradation → excessive information compression or incorrect loss balance
- Training instability → inappropriate learning rate or special token positioning
- Low embedding quality on long sequences → insufficient special token count for longer inputs

**First Experiments:**
1. Verify special token embeddings capture basic semantic relationships using nearest-neighbor tests on simple sentence pairs
2. Measure MMLU score retention after GEM training on a small validation set
3. Test embedding quality on a subset of MTEB tasks before full benchmark evaluation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What hyperparameters would enable GEM to achieve consistent scaling benefits on larger models (8B+) without the performance degradation observed on Llama 3-8B?
- Basis in paper: Section 5.4 notes "Llama 3-8B model didn't outperform 3B model significantly, which may suggest Llama 3-8B would need different hyper-parameters" and the conclusion states "we found the proposed method is less effective on Llama 3 8B models."
- Why unresolved: The authors used hyperparameters tuned on 1B models across all scales, which may not transfer. The 8B model showed a significant MMLU drop (66.7 to 57.24) unlike smaller models.
- What evidence would resolve it: Ablation studies on learning rate, mix ratio p, α scheduling, and number of special tokens specifically tuned for 8B+ models, comparing resulting MTEB and MMLU scores.

### Open Question 2
- Question: Can text summary and soft prompt objectives achieve comparable embedding quality to text reconstruction, and under what conditions?
- Basis in paper: Section 3.1 states the authors "consider regular next token prediction and text reconstruction due to the vast availability of the datasets" but "will study the feasibility of other approaches in the future study" (text summary, soft prompt).
- Why unresolved: These alternative objectives have different data requirements and compression characteristics that remain unexplored.
- What evidence would resolve it: Comparative experiments running GEM with each objective type on the same base models, reporting MTEB/MMLU scores and data efficiency.

### Open Question 3
- Question: Does the method transfer effectively to decoder-only architectures beyond Llama and Mistral (e.g., GPT, Qwen, DeepSeek)?
- Basis in paper: The paper claims the method "could be easily integrated into post-training or fine tuning stages of any existing LLMs" and "seamlessly compatible with all existing LLMs," but only validates on Llama and Mistral families.
- Why unresolved: Different architectures may have different attention patterns, tokenization schemes, or pre-training objectives that affect how special tokens compress information.
- What evidence would resolve it: Applying GEM with identical hyperparameters to GPT-style, Qwen, and DeepSeek models, reporting MTEB/MMLU results.

### Open Question 4
- Question: How does the number of special tokens affect embedding quality on long documents (>512 tokens) versus the 512-token limit tested?
- Basis in paper: Appendix B states "Adding more special tokens could handle longer inputs and this would be studied in our future works" after showing single-token compression struggles beyond ~30 tokens for exact recovery.
- Why unresolved: The experiments use max sequence length 512, but real-world RAG applications often require much longer contexts.
- What evidence would resolve it: Experiments varying special token count (1, 5, 10, 20) on long-document MTEB tasks (e.g., SummEval, long-context retrieval) with extended sequence lengths.

## Limitations
- The method shows performance degradation on Llama 3-8B models, suggesting hyperparameter sensitivity at larger scales
- Only validated on Llama and Mistral model families, limiting generalizability claims to other decoder-only architectures
- Limited testing on sequence lengths beyond 512 tokens, despite potential applications requiring longer contexts
- Optimal special token count and placement strategies remain unexplored across different text types and lengths

## Confidence
- Claim: GEM "outperforms baselines" on MTEB - High confidence based on reported benchmark results
- Claim: GEM "requires as little as 32,000 training rows" - Medium confidence, optimal training data requirements across domains remain unclear
- Claim: GEM "seamlessly compatible with all existing LLMs" - Low confidence due to limited validation on architectures beyond Llama and Mistral
- Claim: GEM maintains "minimal impact on language understanding" - Medium confidence, significant MMLU degradation observed on 8B models

## Next Checks
1. Evaluate GEM's performance on domain-specific benchmarks (e.g., biomedical, legal) to assess generalization beyond MTEB
2. Test scalability by applying GEM to 30B+ parameter models and measuring embedding quality degradation
3. Conduct ablation studies on special token placement strategies to determine optimal configurations for different text types