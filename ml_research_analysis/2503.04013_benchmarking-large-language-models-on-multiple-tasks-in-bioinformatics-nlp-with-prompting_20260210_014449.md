---
ver: rpa2
title: Benchmarking Large Language Models on Multiple Tasks in Bioinformatics NLP
  with Prompting
arxiv_id: '2503.04013'
source_url: https://arxiv.org/abs/2503.04013
tags:
- question
- answer
- your
- arxiv
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Bio-benchmark, a comprehensive prompting-based
  framework for evaluating large language models (LLMs) on 30 bioinformatics tasks
  across proteins, RNA, RNA-binding proteins, drugs, and biomedical text data. The
  authors assess six mainstream LLMs (GPT-4o, Llama-3.1-70b, Qwen-2.5-72b, Mistral-large-2,
  Yi-1.5-34b, and InternLM-2.5-20b) using zero-shot and few-shot Chain-of-Thought
  settings without fine-tuning.
---

# Benchmarking Large Language Models on Multiple Tasks in Bioinformatics NLP with Prompting

## Quick Facts
- arXiv ID: 2503.04013
- Source URL: https://arxiv.org/abs/2503.04013
- Reference count: 40
- Introduces Bio-benchmark framework evaluating LLMs on 30 bioinformatics tasks without fine-tuning

## Executive Summary
This paper presents Bio-benchmark, a comprehensive framework for evaluating large language models on 30 bioinformatics tasks across proteins, RNA, RNA-binding proteins, drugs, and biomedical text data. The authors assess six mainstream LLMs using zero-shot and few-shot Chain-of-Thought prompting without fine-tuning to reveal intrinsic capabilities. They develop BioFinder, an answer extraction model that improves extraction accuracy by over 40% compared to existing methods. Results show LLMs perform well on protein structure prediction and drug design with few-shot prompting, while RNA inverse folding and structure prediction remain challenging. The study provides architectural recommendations for future model development in bioinformatics applications.

## Method Summary
The Bio-benchmark framework evaluates six mainstream LLMs (GPT-4o, Llama-3.1-70b, Qwen-2.5-72b, Mistral-large-2, Yi-1.5-34b, and InternLM-2.5-20b) on 30 bioinformatics tasks using zero-shot and few-shot Chain-of-Thought prompting without fine-tuning. The authors construct domain-specific datasets with deduplication and length limits, format prompts with appropriate separators (newline for sequences), and run inference with temperature=0.2 and top-p=1.0. They develop BioFinder, a fine-tuned answer extraction model using QLoRA via the Xtuner framework, to improve answer extraction accuracy from LLM outputs. Task-specific metrics include accuracy, bit score, recovery rate, and BERTScore, with BioFinder achieving 93.5% extraction accuracy on bio-sequences.

## Key Results
- Few-shot prompting significantly improves performance on protein and drug design tasks, with Yi-1.5-34b showing at least sixfold increases in accuracy
- RNA inverse folding and structure prediction tasks show disappointing results with near-zero accuracy across all models
- Separating bio-sequences with newline characters achieves over three times the alignment accuracy compared to continuous inputs due to BPE tokenization effects
- BioFinder improves answer extraction accuracy by over 40% compared to RegEx methods, achieving 93.5% accuracy on bio-sequences

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Capability Revelation via Prompting
Prompting-based evaluation reveals intrinsic LLM capabilities on bioinformatics tasks without task-specific fine-tuning. Zero-shot and few-shot Chain-of-Thought prompting elicits reasoning from pre-trained knowledge, avoiding the confounds of gradient updates. The benchmark uses 30 tasks across proteins, RNA, drugs, and medical text, standardizing evaluation across diverse biological domains. Core assumption: LLMs possess transferable biological knowledge encoded during pre-training that can be accessed via prompt design alone. Break condition: If fine-tuning consistently outperforms prompting by large margins, the "intrinsic capability" claim weakens to "baseline capability."

### Mechanism 2: Few-shot Prompting Performance Gains
Few-shot prompting significantly improves performance on specific bio-sequence tasks compared to zero-shot, with task-dependent gains. Providing 5-10 examples in the prompt gives LLMs in-context learning signals that bias generation toward task-relevant patterns. The improvement varies: protein species prediction shows 6-20x gains, while RNA structure prediction shows minimal improvement (near-zero in both settings). Core assumption: In-context examples activate relevant latent knowledge and reduce output space ambiguity without updating weights. Break condition: If few-shot gains disappear with different prompt formats or model families, the mechanism may be prompt-specific rather than task-general.

### Mechanism 3: Tokenization Format Impact
Prompt format (tokenization strategy) substantially affects LLM alignment performance on biological sequences, with newline-separated sequences achieving ~3x higher alignment accuracy than continuous sequences. BPE tokenizers aggregate consecutive uppercase letters (common in bio-sequences) into single tokens, reducing granular sequence understanding. Inserting newline separators forces character-level or subword-level tokenization, improving the model's ability to process and generate sequences. Core assumption: Tokenization granularity directly impacts sequence comprehension and generation fidelity in BPE-based LLMs. Break condition: If models using character-level or non-BPE tokenization show no format sensitivity, the mechanism is tokenizer-specific.

## Foundational Learning

### Concept: Zero-shot vs. Few-shot CoT Prompting
Why needed here: The entire Bio-benchmark framework relies on this distinction to evaluate LLM intrinsic vs. exemplar-guided performance. Quick check question: Can you explain why few-shot might hurt performance on some tasks (e.g., CMB-Clinic) while helping others?

### Concept: Tokenization Bias in Sequence Tasks
Why needed here: Understanding how BPE tokenization affects biological sequence processing explains the 3x alignment difference between continuous and newline-separated inputs. Quick check question: Given a protein sequence "MVLSPADKTN," how might a BPE tokenizer represent it differently than a character-level tokenizer?

### Concept: Answer Extraction from Unconstrained LLM Outputs
Why needed here: LLMs embed answers in free-form text; BioFinder (94.7% extraction accuracy) vs. RegEx (72.1%) shows why traditional methods fail. Quick check question: Why might RegEx struggle with biological sequence extraction compared to a fine-tuned extraction model?

## Architecture Onboarding

### Component Map
Bio-benchmark (30 tasks across 7 domains) -> LLMs (GPT-4o, Llama-3.1-70b, Qwen-2.5-72b, Mistral-large-2, Yi-1.5-34b, InternLM-2.5-20b) -> BioFinder (fine-tuned answer extraction) -> Prompt templates (domain-specific CoT prompts) -> Evaluation metrics (task-specific)

### Critical Path
1. Construct domain-specific datasets with deduplication and length limits
2. Format prompts with appropriate separators (newline for sequences)
3. Run inference with temperature=0.2, top-p=1.0
4. Extract answers via BioFinder (not RegEx)
5. Score using domain-appropriate metrics

### Design Tradeoffs
- Tokenization: Newline separators improve alignment but increase token count and cost
- Few-shot vs. Zero-shot: Few-shot helps most tasks but can hurt high-performing zero-shot tasks (e.g., CMB-Clinic)
- Extraction model: BioFinder outperforms RegEx/GPT-4 but requires fine-tuning overhead

### Failure Signatures
- Near-zero accuracy on RNA structure prediction across all models (suggests fundamental limitation)
- Performance drop with few-shot on already high zero-shot tasks (interference from examples)
- RegEx extraction failures on complex bio-sequences

### First 3 Experiments
1. Ablate tokenization format: Compare continuous vs. space-separated vs. newline-separated on protein structure prediction (baseline: newline ~3x higher alignment)
2. Few-shot scaling: Test 0/1/3/5/10-shot on protein species prediction to identify saturation point (expect diminishing returns after 5-shot based on Table 2)
3. BioFinder vs. RegEx extraction: Measure extraction accuracy on 100 manually annotated samples across 3 sequence types (expect 30%+ improvement per Table 1)

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural or training modifications are required to bridge the performance gap between protein and RNA tasks, particularly given that current LLMs struggle significantly with RNA inverse folding and structure prediction? Basis: Section 5.2 notes RNA inverse folding and structure prediction results are "generally disappointing" compared to protein tasks. Unresolved because the paper does not determine if the cause is RNA secondary structure complexity, lack of RNA-specific pre-training data, or fundamental limitations in how LLMs process nucleotide sequences. What evidence would resolve it: Comparative analysis of LLMs pre-trained on RNA corpora versus general-purpose LLMs on the RNA subset of Bio-benchmark.

### Open Question 2
How can LLM tokenizers be redesigned to natively handle continuous biological sequences without relying on formatting "hacks" like newline separators? Basis: Section 5 demonstrates that BPE tokenizers treat continuous uppercase bio-sequences as single tokens, impairing understanding, while newline separation achieves over three times the alignment accuracy. Unresolved because the study validates newline separation as a temporary fix but leaves the underlying tokenization barrier as an unresolved structural limitation. What evidence would resolve it: Development and benchmarking of a bio-specific tokenizer that achieves comparable or superior performance to the newline-separated baseline without requiring artificial sequence fragmentation.

### Open Question 3
Under what specific conditions does few-shot prompting degrade LLM performance in bioinformatics tasks, as observed in Drug-Drug interaction and sgRNA efficiency prediction? Basis: Section 5.4 reports Drug-Drug interaction accuracy for GPT-4o dropped from 0.47 (0-shot) to 0.34 (5-shot), and InternLM-2.5-20b scored 36.67% on sgRNA efficiency in 0-shot but dropped to 0.00% in 5-shot. Unresolved because the authors attribute this to "interference" or "irrelevant information" but the exact mechanism remains unidentified. What evidence would resolve it: An ablation study varying the semantic distance between few-shot examples and the query to define the threshold where additional context transitions from helpful guidance to noise in biochemical tasks.

## Limitations
- Performance gaps in RNA inverse folding and structure prediction tasks suggest fundamental limitations of current LLMs for these domains that are not thoroughly investigated
- BioFinder extraction model trained on only 1,428 samples raises concerns about overfitting to benchmark-specific prompt styles
- The "intrinsic capability" claim assumes prompting-based evaluation provides pure measures of pre-trained knowledge without accounting for prompt-induced bias

## Confidence

- Intrinsic Capability Revelation via Prompting: Medium - Well-supported by methodology and results but lacks comparative validation against fine-tuned models
- Few-shot Performance Gains: Medium - Strong evidence for protein and drug design tasks, but claim weakens for RNA tasks where improvements are minimal
- Tokenization Impact: High - 3x alignment accuracy difference is directly measured and clearly explained through BPE mechanics
- BioFinder Extraction Accuracy: Medium - 40%+ improvement over RegEx is demonstrated, but small training dataset and lack of external validation reduce confidence

## Next Checks

1. Cross-validation of BioFinder: Test BioFinder's extraction accuracy on an independent dataset of LLM outputs from non-benchmark tasks to assess overfitting risk and generalization capability.

2. Fine-tuning vs. Prompting Comparison: Evaluate the same 30 tasks with task-specific fine-tuned models to determine whether prompting truly reveals intrinsic capabilities or simply provides a weaker baseline.

3. Tokenization Ablation Study: Test protein structure prediction accuracy using character-level tokenization (rather than BPE) with continuous and separated formats to isolate whether the 3x improvement stems from tokenization granularity or separator presence.