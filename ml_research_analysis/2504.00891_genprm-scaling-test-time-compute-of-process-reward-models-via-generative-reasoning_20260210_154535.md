---
ver: rpa2
title: 'GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative
  Reasoning'
arxiv_id: '2504.00891'
source_url: https://arxiv.org/abs/2504.00891
tags:
- genprm
- process
- reasoning
- reward
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenPRM introduces a generative process reward model that performs
  explicit Chain-of-Thought reasoning with code verification before providing judgment
  for each reasoning step. The model employs Relative Progress Estimation to obtain
  high-quality process supervision labels and rationale data.
---

# GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning

## Quick Facts
- **arXiv ID:** 2504.00891
- **Source URL:** https://arxiv.org/abs/2504.00891
- **Reference count:** 35
- **Primary result:** GenPRM achieves 75.2% F1 on ProcessBench using only 23K training samples, with 1.5B and 7B variants outperforming much larger models through test-time scaling.

## Executive Summary
GenPRM introduces a generative process reward model that performs explicit Chain-of-Thought reasoning with code verification before providing judgment for each reasoning step. The model employs Relative Progress Estimation to obtain high-quality process supervision labels and rationale data. Experimental results show that GenPRM significantly outperforms prior classification-based PRMs, achieving 75.2% F1 on ProcessBench using only 23K training data from the MATH dataset. Through test-time scaling, a 1.5B GenPRM surpasses GPT-4o and a 7B GenPRM exceeds Qwen2.5-Math-PRM-72B on ProcessBench. The model also demonstrates strong capabilities as a critic for policy model refinement, achieving 3.4× greater performance gains than baseline methods after three refinement iterations.

## Method Summary
GenPRM conditions its final Yes/No judgment on generated rationales and code execution feedback, computed as r̂_t = r_ψ(Yes | s_t, a_t, v_{1:t-1}, f_{1:t-1}, v_t, f_t). The model employs Relative Progress Estimation to obtain high-quality process supervision labels by comparing Monte Carlo scores between consecutive states. Training uses 23K filtered examples from MATH dataset synthesized through consensus filtering. Inference leverages multi-sample majority voting for test-time scaling, transforming PRM inference from single-shot to ensemble prediction.

## Key Results
- GenPRM-7B achieves 75.2% F1 on ProcessBench, outperforming prior PRMs trained on much larger datasets
- Test-time scaling enables 1.5B GenPRM to surpass GPT-4o and 7B GenPRM to exceed Qwen2.5-Math-PRM-72B
- As a critic, GenPRM achieves 3.4× greater performance gains than baseline methods after three refinement iterations

## Why This Works (Mechanism)

### Mechanism 1: Generative Reasoning Before Judgment
GenPRM conditions its final Yes/No judgment on generated rationales (v_t) and code execution feedback (f_t), computed as: r̂_t = r_ψ(Yes | s_t, a_t, v_{1:t-1}, f_{1:t-1}, v_t, f_t). This leverages the pre-trained generative capabilities of LLMs rather than forcing them into a classifier paradigm. Core assumption: explicit reasoning traces provide more reliable signal than direct scalar prediction; code verification catches errors that natural language reasoning misses. Evidence: Table 5 shows CoT + code verification (80.5 Maj@8) outperforms CoT alone (79.3) and direct generative (60.0).

### Mechanism 2: Relative Progress Estimation (RPE) for Label Quality
RPE computes P_t = MC(s_t, a_t) / MC(s_t), measuring whether adding step a_t improves solution probability. A step is labeled correct only if P_t ≥ ε (ε=0.8). This captures whether a step is "beneficial" rather than merely having non-zero completion success. Core assumption: a positive step should be both correct AND make reaching the final answer easier. Evidence: Table 3 shows RPE with ε=0.8 achieves 75.2 F1 vs. 73.2 for hard labels.

### Mechanism 3: Test-Time Scaling via Multi-Sample Aggregation
For each step, GenPRM samples N verification paths and averages: r̂_t = (1/N) Σ r_ψ(Yes | ...). This transforms PRM inference from single-shot to ensemble prediction, trading compute for accuracy. Core assumption: individual reasoning paths have independent error modes; aggregation cancels noise. Evidence: Table 1 shows GenPRM-7B Maj@8 achieves 80.5 F1 vs. 75.2 Pass@1.

## Foundational Learning

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - Why needed here: GenPRM operates at the step level, not solution level. Understanding this distinction is prerequisite to grasping why generative reasoning helps.
  - Quick check question: Given a 5-step solution with an error in step 3, would an ORM catch it? Would a PRM?

- **Concept: Monte Carlo Estimation for Process Labels**
  - Why needed here: RPE builds on MC scores. Without understanding how MC(s_t, a_t) approximates step correctness via K completion samples, the relative progress formula is opaque.
  - Quick check question: If K=32 completions from step t yield 8 correct answers, what is MC(s_t, a_t)?

- **Concept: Chain-of-Thought with Tool Use (Code Execution)**
  - Why needed here: GenPRM's architecture interleaves natural language reasoning with executable code verification. The feedback loop (generate code → execute → incorporate output) is the core innovation over text-only CoT.
  - Quick check question: In Figure 5, what happens when CoT analysis contradicts code output?

## Architecture Onboarding

- **Component map:** Input: (problem, solution_steps) → Step-by-step verification loop → CoT Generation → Code Generation → Code Execution → feedback f_t → Final Judgment (Yes/No) → Training: SFT on 23K filtered examples → Inference: N-sample majority voting for TTS

- **Critical path:** The rationale synthesis pipeline determines data quality. Consensus filtering discards ~51% of generated solutions where RPE labels disagree with LLM-as-a-judge labels. This filtering is the gatekeeper for training data quality.

- **Design tradeoffs:** ε threshold: Higher ε (0.8) improves precision but reduces training data; Model size: 7B offers best efficiency/effectiveness balance; Code execution latency: multi-round self-correction within code verification adds compute cost but catches CoT errors.

- **Failure signatures:** High disagreement rate between RPE and LLM-as-judge labels → insufficient data after consensus filtering; Code execution errors on valid verification code → environment/sandbox issues; Majority voting shows no improvement over Pass@1 → samples are correlated, not diverse.

- **First 3 experiments:** 1) Reproduce RPE ablation: Train GenPRM-7B with hard labels vs. RPE (ε=0.5, 0.8, 1.0) on a 25% data subset; 2) Test TTS scaling curve: Measure GenPRM-7B F1 on ProcessBench with N=1, 2, 4, 8, 16 samples; 3) Code verification contribution: Run inference with code generation disabled but CoT enabled; compare to full GenPRM.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the reasoning process be dynamically pruned to reduce inference latency without sacrificing verification accuracy? Basis: "Future work will investigate how to prune the reasoning process dynamically." Why unresolved: Current implementation introduces significant computational overhead during inference. Evidence needed: A study evaluating GenPRM variants that adaptively terminate reasoning chains based on confidence thresholds.

- **Open Question 2:** Does the GenPRM framework generalize effectively to coding tasks and general reasoning domains beyond mathematics? Basis: "Although GenPRM focuses mainly on mathematical reasoning tasks, it is worth to explore how to apply generative reasoning on coding and general reasoning tasks in the future." Why unresolved: Current experimental design is restricted to mathematical benchmarks. Evidence needed: Evaluation results of GenPRM on coding benchmarks (e.g., HumanEval, MBPP) or logical reasoning benchmarks (e.g., LogiQA).

- **Open Question 3:** Can reinforcement learning (RL) further enhance GenPRM's generative reasoning capabilities compared to supervised fine-tuning? Basis: "Additionally, it would be interesting to leverage RL to incentivize the generative reasoning abilities of GenPRM." Why unresolved: Current model relies solely on supervised fine-tuning. Evidence needed: A comparative analysis training GenPRM with RL versus SFT, measuring performance on out-of-distribution reasoning tasks.

## Limitations

- **Data quality dependence:** GenPRM's performance hinges on the quality of the consensus-filtered training set, which discards ~51% of generated solutions and may introduce bias toward certain problem types.
- **Generalization across domains:** Results on non-MATH domains (AMC23, AIME24) are not directly comparable due to different training/test distributions, weakening claims about generalization without domain-specific training.
- **Code execution reliability:** The paper assumes code verification is consistently accurate but doesn't report execution failure rates or how the model handles syntax/runtime errors in generated code.

## Confidence

- **High confidence:** Claims about test-time scaling effectiveness (1.5B surpassing GPT-4o, 7B exceeding Qwen2.5-72B) are well-supported by ProcessBench results with clear statistical comparisons.
- **Medium confidence:** RPE's superiority over hard labels is demonstrated but the mechanism could be sensitive to MC estimation quality. The ε=0.8 threshold choice is justified empirically but may not generalize.
- **Medium confidence:** The critic refinement capability is promising but evaluated only on a single policy model (DeepSeek-Math-Prover). More diverse policy model evaluations would strengthen this claim.

## Next Checks

1. **MC estimation sensitivity analysis:** Vary the number of Monte Carlo samples (K) and observe impact on RPE label quality and downstream GenPRM performance to quantify dependence on accurate MC estimation.

2. **Code verification ablation on non-MATH domains:** Evaluate GenPRM performance with and without code verification on AMC23 and AIME24 problems to isolate whether code verification's benefits transfer beyond MATH.

3. **Scaling law characterization:** Systematically measure GenPRM performance across 1B, 3B, 7B, 14B, and 32B model sizes with and without test-time scaling to quantify the optimal efficiency frontier.