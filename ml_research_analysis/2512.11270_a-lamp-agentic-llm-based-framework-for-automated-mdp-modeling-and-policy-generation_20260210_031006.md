---
ver: rpa2
title: 'A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy
  Generation'
arxiv_id: '2512.11270'
source_url: https://arxiv.org/abs/2512.11270
tags:
- policy
- a-lamp
- task
- agent
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A-LAMP is an agentic LLM-based framework that automates the generation
  of Markov decision processes and reinforcement learning policies from free-form
  natural language descriptions. It decomposes the modeling, coding, and training
  pipeline into specialized LLM agents that extract parameters, objectives, variables,
  and constraints, formulate them into a formal MDP, and generate executable RL environments
  and policies.
---

# A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation

## Quick Facts
- arXiv ID: 2512.11270
- Source URL: https://arxiv.org/abs/2512.11270
- Reference count: 40
- One-line primary result: Agentic decomposition improves policy generation success rates up to 2x over single-model baselines.

## Executive Summary
A-LAMP is an agentic framework that automates the generation of Markov decision processes and reinforcement learning policies from natural language task descriptions. It decomposes the modeling, coding, and training pipeline into specialized LLM agents that extract parameters, objectives, variables, and constraints, formulate them into formal MDPs, and generate executable RL environments and policies. Across five diverse tasks, A-LAMP consistently outperforms single-model baselines in policy generation success rates, achieving up to twice the success rate in custom environment tasks even when using smaller language models.

## Method Summary
A-LAMP uses a sequential pipeline of eight specialized LLM agents organized into three phases. The Abstract Idea phase extracts task components (Parameter, Objective, Variable, Constraint agents), the Formulation phase converts them into a formal MDP (Modeling, SAR agents), and the Coding phase generates executable environments and policies (Env, Coding agents with Code Executor feedback). The framework includes an Error Correction Module for agent self-validation. All agents operate on structured JSON schemas and formal mathematical representations to ensure semantic alignment before code generation.

## Key Results
- A-LAMP achieves up to 2x higher policy generation success rates compared to single-model baselines on custom environment tasks.
- Light A-LAMP using Gemma3-27B outperforms GPT-4o alone, demonstrating that performance gains derive from the framework architecture rather than model size.
- A-LAMP eliminates spurious coding successes (code runs but solves wrong problem) by 37.5% in drone-delivery tasks through semantic alignment and error correction mechanisms.

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition to Reduce Reasoning Burden
Breaking the complex NL-to-Policy pipeline into specialized agents reduces cognitive load on any single LLM invocation. By narrowing the scope of each prompt, the probability of correct extraction increases, particularly for smaller models. The error rate of a single complex prompt is lower than the cumulative error rate of sequential simpler prompts.

### Mechanism 2: Semantic Alignment via Explicit MDP Formalization
Generating an intermediate formal MDP before coding acts as a "semantic anchor," reducing logic errors in the final executable code. LLMs are better at translating a formal specification into code than translating vague natural language descriptions directly into code. This explicit representation preserves internal consistency before moving to the coding phase.

### Mechanism 3: Feedback Loops for Spurious Coding Elimination
The combination of an Error Correction Module and Code Executor feedback loop minimizes "spurious coding" successes - code that runs but solves the wrong problem. Runtime feedback provides a stronger learning signal for code correction than static prompting alone. Agents self-check extraction confidence and re-examine outputs when confidence is low.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Components**
  - Why needed: A-LAMP is essentially a factory for generating the 5-tuple $(S, A, P, R, \gamma)$. You cannot debug the Modeling Agent without knowing what constitutes a valid State vs. a Parameter.
  - Quick check: Can you distinguish between a "decision variable" (action) and a "system variable" (state) in a standard control loop?

- **Concept: Reinforcement Learning Training Loops (DQN)**
  - Why needed: The final output is a trained policy. Understanding the "Gym-style" interface (reset, step, render) is required to inspect the Code Agent's output and diagnose why a training curve might diverge.
  - Quick check: In a DQN loop, what is the consequence of defining a reward function that is always positive regardless of action?

- **Concept: Agentic Orchestration Patterns**
  - Why needed: A-LAMP uses a sequential pipeline. Understanding how context is passed between agents is vital for extending or debugging the framework.
  - Quick check: If the Parameter Agent extracts "User Distance" as a string but the Modeling Agent expects a float, where does the pipeline break?

## Architecture Onboarding

- **Component map:** Parameter -> Objective -> Variable -> Constraint -> Modeling -> SAR -> Env -> Coding <-> Code Executor
- **Critical path:** The chain is strictly sequential. The most fragile link appears to be the Variable Agent -> Constraint Agent transition for complex custom environments.
- **Design tradeoffs:** Decomposition vs. Context (long-range dependencies might be lost between non-adjacent agents) and Lightweight vs. Robust (Light A-LAMP using Gemma3-27B approaches but doesn't always match GPT-4o performance).
- **Failure signatures:** Semantic Drift (code runs but produces trivial results, signaling failure in Objective/Modeling phase) and Complete Failure (syntax errors or crashes, usually implying ambiguous natural language description).
- **First 3 experiments:** 1) Run A-LAMP on "Cart-pole" to check if it correctly identifies usage of existing Gym environment. 2) Input "Wireless" task with conflicting constraint to observe if Constraint Agent flags impossibility. 3) Disable Error Correction Module on "Drone-delivery" task to compare modeling success rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can finer-grained coding agents significantly reduce syntactic and structural errors in the A-LAMP coding phase?
- Basis: The conclusion identifies the coding stage as "fragile" and suggests extending the framework with finer-grained coding agents.
- Evidence needed: Comparative analysis of coding success rates between current single coding agent and proposed multi-agent coding architecture.

### Open Question 2
- Question: Does incorporating adaptive mechanisms for hyperparameter tuning and domain-informed priors enable A-LAMP to scale effectively to more complex domains?
- Basis: The conclusion states that incorporating adaptive mechanisms for hyperparameter tuning and validation, as well as domain-informed priors, is necessary to "further generalize A-LAMP to complex domains."
- Evidence needed: Performance benchmarks on complex, unstructured domains with and without proposed adaptive tuning and priors.

### Open Question 3
- Question: Does the agentic decomposition strategy retain its effectiveness when applied to continuous action spaces or alternative reinforcement learning algorithms beyond DQN?
- Basis: The methodology explicitly restricts experiments to discrete action spaces using DQN "to ensure a consistent backbone."
- Evidence needed: Evaluation on continuous control benchmarks using algorithms like PPO or SAC.

## Limitations
- Prompt Template Completeness: Full prompt templates and Error Correction Module implementation details are potentially underspecified, posing high risk for exact reproduction.
- Semantic Validation Gap: Framework relies on Code Executor for correctness validation, but cannot catch semantic errors like inverted reward functions.
- Error Propagation in Sequential Chain: Single mislabeling at Parameter/Variable phase can cascade through entire pipeline, though error distribution across agents is not fully characterized.

## Confidence

- **Claim Cluster 1: A-LAMP improves policy generation success rates (up to 2x)**: Medium Confidence
- **Claim Cluster 2: Decomposition reduces reasoning burden and error rates**: High Confidence
- **Claim Cluster 3: Generated policies preserve task optimality**: Low Confidence

## Next Checks

1. **Error Propagation Stress Test**: Systematically inject a single error at the Parameter Agent stage in the "Wireless" task and document how the error propagates through the pipeline to validate robustness.

2. **Semantic Validation Extension**: For "Drone-delivery" task, manually inspect reward function and state transitions, simulate policy for 100 episodes, and compare average reward to hand-coded baseline to quantify semantic accuracy.

3. **Agent Ablation Study**: Run "Inventory-management" task with SAR Agent disabled to compare policy generation success rate and spurious coding failures against full A-LAMP pipeline, isolating the marginal value of explicit MDP formulation.