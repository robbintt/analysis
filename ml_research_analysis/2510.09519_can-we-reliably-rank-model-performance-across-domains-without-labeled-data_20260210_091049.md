---
ver: rpa2
title: Can We Reliably Rank Model Performance across Domains without Labeled Data?
arxiv_id: '2510.09519'
source_url: https://arxiv.org/abs/2510.09519
tags:
- performance
- across
- error
- datasets
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether label-free methods can reliably
  rank model performance across diverse domains. Using a two-step framework, base
  classifiers are trained on source domains, and auxiliary error models predict where
  these classifiers will fail on unseen data.
---

# Can We Reliably Rank Model Performance across Domains without Labeled Data?

## Quick Facts
- arXiv ID: 2510.09519
- Source URL: https://arxiv.org/abs/2510.09519
- Reference count: 19
- One-line primary result: LLM-based error predictors can rank model performance across domains without labels with Spearman correlations up to 0.635.

## Executive Summary
This study investigates whether label-free methods can reliably rank model performance across diverse domains. Using a two-step framework, base classifiers are trained on source domains, and auxiliary error models predict where these classifiers will fail on unseen data. Experiments on GeoOLID and Amazon Reviews datasets show that large language model-based error predictors (e.g., LLaMa-3.1-8B, Gemma-3-12B-it) achieve Spearman correlations above 0.62 with true accuracy rankings, outperforming drift-based and zero-shot baselines. Ranking reliability depends on the magnitude of true performance differences and alignment between predicted and actual errors. When accuracy ranges are narrow, simpler error models like logistic regression perform competitively by capturing lexical and confidence-based patterns.

## Method Summary
The method employs a two-step framework to rank model performance across domains without labeled data. First, a base classifier is trained on a source domain for the target task. Second, an auxiliary error predictor is trained or prompted to predict instance-level errors of the base classifier. This error predictor is then applied to unlabeled target domains to estimate per-domain accuracy, which is used to generate performance rankings. Error predictors tested include logistic regression, RoBERTa classifiers, and LLM-based models (GPT-4o-mini, LLaMa-3.1-8B, Gemma-3-12B-it). Performance is evaluated using Spearman rank correlation between estimated and true accuracy rankings across multiple domains.

## Key Results
- LLM-based error predictors (e.g., LLaMa-3.1-8B, Gemma-3-12B-it) achieved Spearman correlations up to 0.635 on GeoOLID, outperforming drift-based and zero-shot baselines.
- Ranking reliability is higher when true accuracy differences between domains are larger (e.g., correlations rise as accuracy gaps increase).
- On datasets with narrow accuracy ranges (e.g., Amazon Reviews, SD=0.015), rankings become unreliable regardless of method, with correlations collapsing to near zero.

## Why This Works (Mechanism)

### Mechanism 1: Error Prediction Alignment
- Claim: Ranking reliability increases when an error model's predictions align with the base model's actual failure patterns.
- Mechanism: A base classifier produces predictions, and a secondary model (error predictor) is trained or prompted to identify instances where the base classifier is likely wrong. Aggregating these predicted errors across a domain provides an estimate of domain-level accuracy, which is then used for ranking.
- Core assumption: The failure patterns learned by the error predictor on a source domain generalize to unseen target domains.
- Evidence anchors:
  - [abstract] "Rankings are more reliable when... the error model's predictions align with the base model's actual failure patterns."
  - [Page 4] "Our objective is to estimate dataset-level performance p(k)... using the predicted probability of correctness from an auxiliary model g_φ."
- Break condition: The error model learns spurious correlations or fails to capture systematic errors that transfer across domains, leading to misaligned predictions and incorrect rankings.

### Mechanism 2: Magnitude of True Performance Differences
- Claim: Ranking reliability is higher when there are larger true accuracy differences across the domains being compared.
- Mechanism: When the actual performance gap between the best and worst domains is large, even imperfect estimates can preserve the correct ordinal ranking. Conversely, when accuracies are tightly clustered, small estimation errors can easily flip the rank order.
- Core assumption: The estimation method provides a monotonic signal that correlates with true accuracy.
- Evidence anchors:
  - [abstract] "Rankings are more reliable when true accuracy differences are larger..."
  - [Page 6, "Variation Study"] "...correlations rise gradually as the difference in true accuracy increases, showing that larger performance gaps make it easier to identify correct rankings."
- Break condition: Domains have very similar true accuracies (e.g., Amazon Reviews with SD = 0.015), causing ranking instability.

### Mechanism 3: LLM-Based Error Prediction Superiority
- Claim: LLM-based error predictors, when used as judges, produce more reliable cross-domain performance rankings compared to drift-based or simpler zero-shot baselines.
- Mechanism: Large Language Models, prompted with examples of correct and incorrect predictions, can leverage their broader linguistic knowledge to better identify model failures in new domains than metrics based on simple distributional drift or confidence scores.
- Core assumption: LLMs possess a "cross-model" understanding of correctness that generalizes better than self-confidence or simple text drift measures.
- Evidence anchors:
  - [abstract] "...LLM-based error predictors achieve Spearman correlations up to 0.635, outperforming drift-based and zero-shot baselines."
  - [Page 7, Table 1] Shows LLaMa-3.1-8B and Gemma-3-12B-it achieving average correlations of .625 and .635 on GeoOLID, substantially outperforming the Semantic Drift and Zero-Shot baselines.
- Break condition: The LLM judge is miscalibrated, biased towards certain linguistic styles, or lacks domain-specific knowledge, causing systematic errors in its error predictions.

## Foundational Learning

- Concept: Spearman Rank Correlation
  - Why needed here: This is the primary metric used to evaluate the reliability of the proposed ranking method. It measures the monotonic relationship between predicted and true performance rankings.
  - Quick check question: If a method produces estimated accuracies of [0.6, 0.8, 0.7] for domains A, B, C and true accuracies are [0.65, 0.85, 0.75], what would a high Spearman correlation indicate?

- Concept: Domain Shift (Covariate Shift)
  - Why needed here: The paper frames its problem as predicting performance across domains where the data distribution differs from the training domain. The error model must generalize across this shift.
  - Quick check question: A model is trained on movie reviews and tested on product reviews. What kind of shift is this, and why might accuracy degrade?

- Concept: LLM-as-a-Judge
  - Why needed here: The core of the paper's best-performing method involves using an LLM not for the primary task, but as a secondary evaluator (judge) to predict the correctness of another model's outputs.
  - Quick check question: Instead of training a classifier, you ask GPT-4, "Is this predicted sentiment label for this review correct or incorrect?" What is this paradigm called?

## Architecture Onboarding

- Component map:
  Base Classifier -> Error Predictor -> Performance Estimator -> Ranking Evaluator

- Critical path:
  1. Train or prompt Base Classifier on a source domain.
  2. On the same source domain, generate instance-level labels of correct/incorrect based on the Base Classifier's performance.
  3. Train or prompt the Error Predictor using these instance-level correctness labels.
  4. Apply the Base Classifier to new, unlabeled target domains to get task predictions.
  5. Apply the Error Predictor to the new target domains (using the input and the Base Classifier's prediction) to get correctness estimates.
  6. Aggregate Error Predictor outputs to estimate domain-level accuracy.
  7. Rank domains by estimated accuracy and evaluate ranking quality against true performance.

- Design tradeoffs:
  - **Error Predictor Choice:** An LLM (e.g., Gemma) may offer better ranking performance (up to 0.635 ρ) but at higher computational cost than a simpler linear model. The linear model, however, was competitive on the Amazon dataset, suggesting a tradeoff between cost and effectiveness depending on the dataset's accuracy spread.
  - **Base Model Selection:** A more powerful base model might have different, potentially more subtle, error patterns that could be harder or easier for the error predictor to learn.

- Failure signatures:
  - **Low Correlation on Clustered Accuracies:** If the true accuracies of target domains are very similar (e.g., standard deviation < 0.02), expect Spearman correlation to be low and unstable, as small prediction errors will change the ranking.
  - **Negative Correlation:** Indicates the error predictor's estimates are inversely related to true performance, suggesting a fundamental misalignment or overfitting in the error model.

- First 3 experiments:
  1. **Baseline Comparison:** Implement the framework using a RoBERTa base classifier and compare three error predictors: a linear model, a RoBERTa-based classifier, and an LLM (e.g., GPT-4o-mini). Compare their Spearman correlation on a held-out domain against drift-based and zero-shot baselines.
  2. **Synthetic Performance Gap Analysis:** Create a synthetic target domain by starting with perfect predictions and systematically injecting errors. Test if ranking correlation improves as the performance gap between this synthetic domain and others increases.
  3. **Error Alignment Analysis:** Train an error predictor and compare its predicted errors against the base classifier's actual errors. Assess if higher alignment (F1 score of the error predictor) directly correlates with higher ranking reliability (Spearman correlation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the two-step error-prediction framework be effectively extended to text generation and multimodal tasks where error alignment differs from classification?
- Basis in paper: [explicit] The conclusion states: "Future work will extend this analysis to generation and multimodal tasks, where the sources of variation and error alignment may differ..."
- Why unresolved: The current study strictly evaluates classification tasks (sentiment and offensive language detection). Generation tasks lack binary "correctness," and multimodal tasks introduce non-textual variation sources, making the current binary error-prediction setup potentially insufficient.
- What evidence would resolve it: Application of the framework to text generation benchmarks (e.g., summarization) using metrics like ROUGE as ground truth, and multimodal benchmarks, demonstrating significant rank correlation without labels.

### Open Question 2
- Question: To what extent can incorporating task-specific priors improve ranking stability across heterogeneous datasets?
- Basis in paper: [explicit] The conclusion proposes: "...investigate how task-specific priors can improve ranking stability across heterogeneous datasets."
- Why unresolved: The paper demonstrates that current methods fail when datasets are tightly clustered (Amazon) or structurally different, but it does not test if injecting prior knowledge about the task or domain can correct these instabilities.
- What evidence would resolve it: Experiments integrating domain-specific priors (e.g., product category metadata for reviews) into the error model, showing improved Spearman correlations on datasets where the baseline failed.

### Open Question 3
- Question: How does severe class imbalance in the target domain degrade the reliability of error-prediction models?
- Basis in paper: [inferred] The Limitations section notes that predictors may fail to capture variation "especially when data are highly imbalanced," yet the experiments utilize datasets (GeoOLID, Amazon) which are sampled to be balanced or manageable subsets.
- Why unresolved: The paper acknowledges imbalance as a limitation but provides no empirical analysis of how skew in the unlabeled target domain affects the error model's probability estimates ($\hat{p}(k)$) and subsequent rankings.
- What evidence would resolve it: A variation study simulating increasing class skew in the test domains, measuring the point at which Spearman correlation drops significantly compared to the balanced baseline.

### Open Question 4
- Question: Can ranking reliability be salvaged when true accuracy differences across domains are minimal (standard deviation < 0.02)?
- Basis in paper: [inferred] The results show that on the Amazon dataset, where accuracy SD is 0.015, correlations collapse to near zero. The paper concludes that "ranking becomes more reliable when underlying performance differences are broader," leaving the narrow-difference scenario as an open failure mode.
- Why unresolved: The paper identifies "tight clustering" of accuracy as a cause for failure but does not propose or test methods (e.g., amplifying uncertainty signals) to distinguish between models performing nearly identically.
- What evidence would resolve it: A method capable of statistically distinguishing rank orderings on datasets with narrow accuracy distributions (e.g., Amazon), achieving correlations significantly above the reported ~0.35 average.

## Limitations
- The framework fails when true accuracy differences across domains are minimal (standard deviation < 0.02), as seen on the Amazon dataset.
- Error predictors may fail to capture variation, especially when data are highly imbalanced in the target domain.
- The error alignment mechanism is theoretically grounded but not empirically quantified; no correlation between error predictor F1 and ranking Spearman is reported.

## Confidence
- **High**: The two-step error-prediction framework is clearly specified and reproducible with the given hyperparameters. The claim that performance rankings are more reliable when true accuracy differences are large is directly supported by the synthetic gap analysis (ρ rising with accuracy difference) and holds across both datasets.
- **Medium**: LLM-based error predictors outperform drift-based and zero-shot baselines in the reported experiments (e.g., Gemma-3-12B-it achieving ρ = 0.635 on GeoOLID). However, the exact prompting strategy and few-shot examples are unspecified, and no ablation on prompt design is provided. Corpus evidence on LLM-as-judge is related but not directly validating this specific use case.
- **Low**: The claim that ranking reliability fundamentally depends on alignment between predicted and actual base model errors is theoretically plausible but not empirically quantified in the paper; no correlation between error predictor F1 and ranking Spearman is reported.

## Next Checks
1. **Error Predictor Alignment Study**: Measure the F1 score of the error predictor against the base model's actual errors on a held-out domain. Test if higher F1 correlates with higher ranking Spearman across multiple base model/evaluator pairs.
2. **Prompt Ablation for LLM Evaluators**: Systematically vary the number and quality of few-shot examples in LLM prompts and measure the impact on ranking correlation to identify the minimum viable prompt strategy.
3. **Synthetic Gap Replicability**: Replicate the synthetic performance gap experiment on a third dataset (e.g., AG News) to confirm that ranking correlation scales monotonically with true accuracy differences across domains.