---
ver: rpa2
title: Table as a Modality for Large Language Models
arxiv_id: '2512.00947'
source_url: https://arxiv.org/abs/2512.00947
tags:
- table
- llms
- tamo
- data
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TAMO addresses the challenge of LLMs\u2019 limited ability to\
  \ understand table structures by treating tables as an independent modality, using\
  \ a hypergraph-based encoder to capture global structural information and aligning\
  \ it with LLM embeddings. This allows LLMs to perceive table structure globally\
  \ before processing text, improving robustness to row/column permutations and enhancing\
  \ table reasoning performance."
---

# Table as a Modality for Large Language Models

## Quick Facts
- arXiv ID: 2512.00947
- Source URL: https://arxiv.org/abs/2512.00947
- Authors: Liyao Li; Chao Ye; Wentao Ye; Yifei Sun; Zhe Jiang; Haobo Wang; Jiaming Tian; Yiming Zhang; Ningtao Wang; Xing Fu; Gang Chen; Junbo Zhao
- Reference count: 40
- Achieves up to 42.65% average relative gain over text-only baselines on five table reasoning benchmarks

## Executive Summary
TAMO introduces a novel approach to table understanding by treating tables as an independent modality, using hypergraph neural networks to capture global structural information and aligning it with LLM embeddings. This framework addresses the critical limitation of LLMs in perceiving table structures by providing a structural representation that is permutation invariant and hierarchically aware. The method achieves significant performance improvements across five benchmarks while maintaining robustness to row and column permutations, outperforming both text-only baselines and commercial LLMs on most tasks.

## Method Summary
TAMO encodes tables as hypergraphs where cells are nodes and rows/columns/hierarchical groupings are hyperedges, processed by a HyperTrans encoder to learn global structural embeddings. These embeddings are projected into the LLM's embedding space via an MLP and prepended to text embeddings, functioning as a learned global context. The model maintains parallel structural and textual input streams, with structural tokens providing global topology and text tokens providing fine-grained semantics, allowing frozen LLMs to utilize table structure as a "soft prompt" without weight modification.

## Key Results
- Achieves up to 42.65% average relative gain over text-only baselines on StructQA, HiTab, WikiTQ, WikiSQL, and FeTaQA
- Demonstrates strong generalization across datasets with robust performance against structural variations
- Outperforms GPT-3.5 and GPT-4 on most table reasoning tasks
- Shows permutation invariance with significant robustness gains on StructQA permuted tests

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Encoding tables as hypergraphs preserves complex hierarchical relationships and permutation invariance properties that are lost during text serialization.
- **Mechanism**: The framework constructs a hypergraph where cells are nodes and rows/columns/hierarchical groupings are hyperedges. A HyperTrans encoder (using multiset functions) aggregates information between nodes and hyperedges, learning a global structural representation ($X_{st}$) that is mathematically invariant to row/column swaps.
- **Core assumption**: Tables possess an inherent hierarchical structure where relationships are set-based (multiset) rather than pairwise, and preserving this topology is required for robust reasoning.
- **Evidence anchors**: [abstract] mentions "hypergraph neural network as the global table encoder"; [section 2.2] details hypergraph conversion to capture "high-order hierarchical structure and permutation invariance."
- **Break condition**: If input tables are flattened or hypergraph construction fails to map hierarchical headers to specific hyperedges, permutation invariance property is likely lost.

### Mechanism 2
- **Claim**: Aligning structural embeddings with the LLM's semantic space via a trainable projector allows frozen LLMs to utilize table structure as a "soft prompt."
- **Mechanism**: Structural embeddings ($X_{st}$) from the hypergraph encoder are projected into the LLM's dimension ($d_l$) using an MLP. These projected tokens are prepended to text embeddings ($X_{tt}$), acting as a learned global context that guides the LLM's attention without modifying its weights.
- **Core assumption**: The LLM's embedding space is sufficiently expressive to integrate external modality features when presented as prefix tokens, and the MLP can learn the necessary transformation.
- **Evidence anchors**: [abstract] states the model is "integrated with the LLM embeddings through learnable features"; [section 2.3] describes the "Alignment Projector" and how structure tokens are injected "in a manner similar to the soft prompt."
- **Break condition**: If projector MLP is too simple (e.g., linear only) or LLM embedding dimension is too small, structural features may not align effectively, resulting in negligible performance gain over text-only baselines.

### Mechanism 3
- **Claim**: Maintaining parallel structural and textual input streams provides complementary signals (global topology vs. local semantics) that jointly improve answer localization.
- **Mechanism**: TAMO feeds both projected structural tokens ($X_{st}$) and serialized text tokens ($X_{tt}$) into the LLM. The text stream provides content ("what"), while the structure stream provides positional/hierarchical context ("where"), optimizing attention over relevant cells.
- **Core assumption**: Text serialization destroys global relational context, while structure-only encoding lacks fine-grained semantic nuance; both are necessary for generation tasks.
- **Evidence anchors**: [section 2.3] explicitly states $X_{tt}$ provides "fine-grained semantic content" and $X_{st}$ provides "global relational context," validating their synergy; [figure 5] visualization shows TAMO attending more closely to correct cells compared to inference-only baselines.
- **Break condition**: If serialized text is truncated (exceeding context length) or structural tokens overpower text tokens, the model may hallucinate content or lose semantic details.

## Foundational Learning

- **Concept**: **Hypergraph Neural Networks (HyperGNNs)**
  - **Why needed here**: Standard graphs connect only two nodes (pairwise), but table cells relate to entire rows, columns, or nested headers (group relations). Hypergraphs use hyperedges to connect sets of nodes, modeling these high-order correlations directly.
  - **Quick check question**: Can you distinguish between a standard graph edge connecting two cells vs. a hyperedge representing an entire row?

- **Concept**: **Permutation Invariance**
  - **Why needed here**: This is the core diagnostic metric of the paper. A valid table representation should yield the same result regardless of row/column ordering. LLMs failing this (low robustness) indicate they rely on spurious sequential patterns rather than true structure.
  - **Quick check question**: If you shuffle the rows of a table, should the answer to "What is the sum of column A?" change?

- **Concept**: **Soft Prompts / Prefix Tuning**
  - **Why needed here**: TAMO injects the table modality into frozen LLMs by prepending learned continuous vectors (embeddings) rather than discrete text. Understanding this parameter-efficient technique is crucial to seeing how the "modality" is actually introduced.
  - **Quick check question**: How does adding learnable tensor prefixes to the input embedding layer differ from standard instruction tuning?

## Architecture Onboarding

- **Component map**: Input (Raw Table + Question) -> Hypergraph Builder (Nodes $V$ and Hyperedges $E$) -> Table Encoder (HyperTrans network) -> Alignment Projector (MLP + Mean Pooling) -> LLM Interface (Concatenation of $X_{st}$, $X_{tt}$, $X_{qt}$)

- **Critical path**: The definition of the hypergraph (Component 2). If hierarchical relationships (e.g., merged cells in HiTab) are not correctly mapped to hyperedges, the encoder receives a corrupted topology, and the downstream attention mechanism fails to locate correct answers.

- **Design tradeoffs**:
  - **Frozen vs. Tuned**: The paper shows TAMO works with frozen LLMs (low compute), but performance peaks with SFT. Use frozen for rapid integration; use SFT for maximum accuracy.
  - **Token Limit**: The number of structure tokens is a hyperparameter. The paper finds a minimum of 2 tokens is sufficient, balancing information density against context window usage.

- **Failure signatures**:
  - **Low Robustness Score**: Model accuracy drops significantly on StructQA permuted tests. Indicates the model is learning sequential heuristics (text-only behavior) rather than structural reasoning.
  - **Attention Drift**: Visualization shows high attention weights on irrelevant cells or the question token alone, ignoring the `[table_structure_token]`.

- **First 3 experiments**:
  1. **StructQA Permutation Test**: Run the baseline (text-only) vs. TAMO on randomly shuffled rows/columns to verify the robustness claim (>40% gain).
  2. **Ablation on Modality**: Train three variants (Graph-only, Text-only, Both) on WikiTQ to confirm that both streams are necessary (Graph-only should fail on generative tasks).
  3. **Frozen LLM Integration**: Plug the TAMO encoder and projector into a frozen Llama2-7B and measure the relative gain over prompt tuning to validate the "plug-and-play" capability.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework depends on task-specific data for fine-tuning (SFT), as frozen LLM integration shows lower performance gains
- Hypergraph construction assumes hierarchical headers and merged cells can be systematically mapped to hyperedges, which may not generalize to highly irregular table layouts
- Current evaluation focuses on English-language datasets, leaving questions about multilingual robustness and handling of right-to-left scripts

## Confidence

**High Confidence**: The core claim that encoding tables as hypergraphs provides permutation invariance is well-supported by mathematical formulation in Section 2.2 and StructQA robustness results showing 42.65% average relative gain.

**Medium Confidence**: The claim that TAMO outperforms GPT-3.5 and GPT-4 on most tasks is supported by experimental results, but comparisons use different prompting strategies and may not represent optimal usage of commercial LLMs.

**Low Confidence**: The assertion that TAMO can be seamlessly integrated as a "plug-and-play" module requires qualification. While demonstrated, the performance gap between frozen and SFT-tuned variants (up to 7.84% on HiTab) suggests integration is not truly plug-and-play without additional task-specific fine-tuning.

## Next Checks

1. **Cross-Domain Transferability Test**: Evaluate TAMO trained on StructQA and WikiTQ on an entirely different table reasoning dataset (e.g., TabFact for fact verification) without any task-specific fine-tuning to measure true modality transfer capability.

2. **Irregular Table Layout Robustness**: Create a benchmark with tables containing merged cells, nested headers spanning multiple columns, and irregular row heights. Measure TAMO's performance degradation compared to text-only baselines to assess real-world applicability.

3. **Multilingual and Script Directionality Test**: Test TAMO on a multilingual table dataset (e.g., WikiTables from non-English Wikipedias) and specifically on right-to-left scripts like Arabic or Hebrew tables to validate claims about general table structure understanding across languages.