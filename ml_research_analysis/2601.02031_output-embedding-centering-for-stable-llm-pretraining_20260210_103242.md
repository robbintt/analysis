---
ver: rpa2
title: Output Embedding Centering for Stable LLM Pretraining
arxiv_id: '2601.02031'
source_url: https://arxiv.org/abs/2601.02031
tags:
- loss
- z-loss
- output
- centering
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in large language models
  (LLMs), specifically the problem of output logit divergence that occurs during pretraining,
  particularly at high learning rates. The authors analyze the issue from the perspective
  of output embeddings' geometry and identify that anisotropic embeddings (non-uniform
  distribution in hidden space) are the root cause of this instability.
---

# Output Embedding Centering for Stable LLM Pretraining

## Quick Facts
- arXiv ID: 2601.02031
- Source URL: https://arxiv.org/abs/2601.02031
- Authors: Felix Stollenwerk; Anna Lokrantz; Niclas Hertzberg
- Reference count: 10
- Primary result: Output embedding centering methods (µ-centering and µ-loss) outperform z-loss for training stability in LLMs

## Executive Summary
This paper addresses training instability in large language models (LLMs), specifically the problem of output logit divergence that occurs during pretraining, particularly at high learning rates. The authors analyze the issue from the perspective of output embeddings' geometry and identify that anisotropic embeddings (non-uniform distribution in hidden space) are the root cause of this instability. They propose output embedding centering (OEC) as a new mitigation strategy, implemented in two ways: µ-centering (a deterministic operation that subtracts the mean output embedding) and µ-loss (a regularization method that penalizes the squared norm of the mean embedding).

## Method Summary
The authors propose output embedding centering (OEC) as a solution to training instability in LLMs. OEC is implemented in two variants: µ-centering, which deterministically subtracts the mean output embedding from each token's embedding, and µ-loss, which adds a regularization term to the loss function that penalizes the squared norm of the mean embedding. Both methods are designed to enforce isotropic output embeddings, thereby suppressing output logit divergence. The theoretical analysis proves that both approaches effectively mitigate training instability, and experiments demonstrate their superiority over the commonly used z-loss method across multiple model sizes.

## Key Results
- Both µ-centering and µ-loss outperform z-loss in training stability and learning rate sensitivity
- OEC methods ensure convergence even at high learning rates where z-loss fails
- µ-loss achieves optimal performance at λ=10⁻¹, showing significantly less sensitivity to hyperparameter tuning compared to z-loss
- Minimal computational overhead (0-6.4% additional training time)

## Why This Works (Mechanism)
The mechanism behind output embedding centering's effectiveness lies in addressing the geometric properties of output embeddings. During LLM training, particularly at high learning rates, output embeddings can develop anisotropic distributions in the hidden space - meaning they become non-uniformly distributed with varying density across different regions. This anisotropy leads to output logit divergence, where the logits (pre-softmax activations) grow uncontrollably large, causing training instability. By centering the output embeddings (either through deterministic subtraction or regularization), OEC enforces a more isotropic distribution, preventing the exponential growth of logits and maintaining stable training dynamics.

## Foundational Learning
- **Output Logit Divergence**: The phenomenon where pre-softmax activations grow exponentially during training, causing instability. Why needed: Understanding this problem is crucial as it's the core issue OEC addresses. Quick check: Verify that logits are growing beyond normal ranges during unstable training.
- **Anisotropic Embeddings**: Non-uniform distribution of embeddings in hidden space, where certain directions have higher variance than others. Why needed: This geometric property is identified as the root cause of instability. Quick check: Compute the variance-covariance matrix of embeddings to verify anisotropy.
- **Mean Embedding**: The average of all output embeddings across tokens in a batch. Why needed: Both OEC variants operate on this statistical property. Quick check: Monitor mean embedding norm during training to detect instability.

## Architecture Onboarding

**Component Map:** Input embeddings -> Transformer layers -> Output embeddings -> Mean computation -> Centering operation -> Loss computation

**Critical Path:** During training, tokens flow through the model to produce output embeddings. These embeddings are used to compute logits, which then generate the loss. The OEC methods intervene at the output embedding stage, either modifying the embeddings directly (µ-centering) or adding a regularization term based on their mean (µ-loss).

**Design Tradeoffs:** µ-centering provides deterministic stabilization but may slightly alter the model's representational capacity by enforcing strict centering. µ-loss offers more flexibility by allowing some anisotropy while still providing regularization, but requires tuning of the λ hyperparameter. Both methods add minimal computational overhead compared to the potential benefits of training stability.

**Failure Signatures:** Training instability manifests as exploding logits, NaN loss values, or training divergence. Without OEC, models may fail to converge at higher learning rates, limiting the optimization potential. The methods are particularly effective when training at or above learning rates where z-loss begins to fail.

**First Experiments:**
1. Implement µ-centering and compare training stability against baseline at various learning rates
2. Add µ-loss with different λ values and measure sensitivity to hyperparameter tuning
3. Benchmark computational overhead of both methods against z-loss

## Open Questions the Paper Calls Out
The paper acknowledges that the theoretical analysis assumes infinitely wide layers, which may not hold in practical scenarios, particularly for the smaller models tested. The extent to which the theory applies to these finite-width models remains uncertain. Additionally, the experimental validation was conducted exclusively on decoder-only Transformer architectures with a fixed hidden size of 512, leaving open questions about the method's effectiveness on encoder-decoder models, convolutional architectures, or models with different hidden dimensions.

## Limitations
- Theoretical analysis assumes infinitely wide layers, which may not apply to smaller models (16M-44M parameters)
- Experiments limited to decoder-only Transformer architectures with fixed hidden size of 512
- Evaluation focused primarily on pretraining stability without extensive downstream task testing
- Computational overhead measurements based on wall-clock time, which can vary by hardware

## Confidence

**Major Claims Confidence Assessment:**
- Output embedding centering effectively mitigates training instability: **High**
- µ-loss requires less hyperparameter tuning than z-loss: **Medium**
- Theoretical analysis holds for practical finite-width models: **Low**

## Next Checks

1. Evaluate OEC methods on encoder-decoder architectures and non-Transformer model families to assess generalizability
2. Test downstream task performance (fine-tuning, zero-shot inference) to verify that stability improvements translate to practical utility
3. Conduct ablation studies varying hidden dimension sizes and model scales to understand the relationship between layer width and OEC effectiveness