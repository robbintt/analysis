---
ver: rpa2
title: Is there Value in Reinforcement Learning?
arxiv_id: '2505.04822'
source_url: https://arxiv.org/abs/2505.04822
tags:
- value
- learning
- policy
- methods
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that policy-gradient (PG) methods, despite being
  favored as a simpler alternative to value-based (VB) methods, are not "value-free"
  since they require value representations for learning, not just acting. The authors
  explain that both PG and VB methods are instances of generalized policy iteration,
  differing only in how they implement the abstract idea of policy evaluation and
  improvement.
---

# Is there Value in Reinforcement Learning?

## Quick Facts
- arXiv ID: 2505.04822
- Source URL: https://arxiv.org/abs/2505.04822
- Reference count: 31
- Primary result: Policy-gradient methods require value representations for learning, not just acting, and both PG and value-based methods are instances of generalized policy iteration.

## Executive Summary
This paper challenges the notion that policy-gradient (PG) methods are "value-free" by demonstrating that they require value representations for learning, even when they don't require them for action selection. The authors argue that both PG and value-based (VB) methods are fundamentally instances of Generalized Policy Iteration (GPI), differing only in how they implement policy evaluation and improvement. The paper shifts focus from algorithm choice to questioning the underlying assumptions of the standard RL framework, including risk neutrality, full observability, and Markovian environments.

## Method Summary
This is a theoretical/position paper that makes conceptual arguments about the relationship between policy-gradient methods and value representations. The authors analyze REINFORCE algorithm and demonstrate mathematically that the gradient updates depend on action-values Q^π(s,a), with the empirical return R_t serving as a sampling-based estimate of Q^π. They show that replacing R_t with immediate reward r(s_t, a_t) yields sub-optimal policies in general sequential decision problems. The paper uses theoretical analysis rather than empirical experiments to support its claims.

## Key Results
- PG methods require value representations for learning, contradicting claims they are "value-free"
- Both PG and VB methods are instances of Generalized Policy Iteration, differing only in implementation details
- The existence of value functions depends on Bellman equations, which require non-trivial assumptions about problem structure
- Computational complexity (memory requirements) should be weighed alongside statistical complexity when evaluating model plausibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy-gradient methods require value representations for learning, even when they do not require them for action selection.
- Mechanism: The Policy Gradient Theorem establishes that gradient updates depend on action-values Q^π(s,a). In REINFORCE, the empirical return R_t = Σγ^τ r(s_{t+τ}, a_{t+τ}) serves as a sampling-based estimate of Q^π. Crucially, replacing R_t with the immediate reward r(s_t, a_t) yields sub-optimal policies in general sequential decision problems.
- Core assumption: The optimization objective is expected cumulative discounted reward over sequential trajectories.
- Evidence anchors:
  - [abstract] "PG methods are not, in fact, 'Value-free' – while they do not rely on an explicit representation of Value for acting (stimulus-response mapping), they do require it for learning."
  - [section 3] Equation 1 and Equation 2 demonstrate that R_t is "by definition, a sampling-based estimate of Q^π_θ(s_t, a_t)."
- Break condition: Multi-armed bandit problems (where expected reward and value coincide) may mask this requirement.

### Mechanism 2
- Claim: Both PG and VB methods instantiate Generalized Policy Iteration (GPI), differing only in which component is persistent versus volatile.
- Mechanism: GPI alternates between policy evaluation (measuring performance) and policy improvement (modifying policy). In VB methods, the critic (value function) is persistent and parametric; the actor (policy) is derived and volatile. In PG methods, the actor (policy) is persistent and parametric; the critic (value estimate) is non-parametric and volatile (trajectory-based).
- Core assumption: Learning can be decomposed into evaluation and improvement steps that iterate toward optimality.
- Evidence anchors:
  - [abstract] "The authors explain that both PG and VB methods are instances of generalized policy iteration, differing only in how they implement the abstract idea of policy evaluation and improvement."
- Break condition: Assumption fails if the problem structure prevents clean separation of evaluation and improvement.

### Mechanism 3
- Claim: The existence of value functions depends on Bellman equations, which require non-trivial assumptions about the problem structure.
- Mechanism: Bellman equations presuppose a Markovian environment, exponential discounting, risk neutrality, and full observability. If these assumptions are relaxed, the standard value function may not exist, and any algorithm (PG or VB) requires significant modification.
- Core assumption: The optimization problem admits a recursive decomposition via dynamic programming principles.
- Evidence anchors:
  - [abstract] "The existence of value functions relies on Bellman equations, which depend on a set of non-trivial assumptions about the problem."
- Break condition: Naturalistic settings likely violate one or more assumptions, requiring modified optimization formulations.

## Foundational Learning

- Concept: Action-value function Q^π(s,a)
  - Why needed here: Central to understanding why PG methods cannot be "value-free" – the gradient depends on Q^π.
  - Quick check question: Can you explain why Q^π differs from immediate reward r(s,a) in sequential settings?

- Concept: Generalized Policy Iteration (GPI)
  - Why needed here: Provides the unifying framework showing PG and VB as algorithmic variants rather than fundamentally different approaches.
  - Quick check question: In a VB method, which GPI component is persistent versus volatile? How does this invert in PG?

- Concept: Bellman equation assumptions (Markov property, exponential discounting, risk neutrality)
  - Why needed here: The paper argues that questioning value representation requires questioning these underlying assumptions.
  - Quick check question: If an environment is partially observable, does the standard Bellman equation still hold?

## Architecture Onboarding

- Component map:
  - Actor (policy) -> States/observations -> Action probabilities
  - Critic (value) -> States/transitions -> Value estimates
  - Memory buffer -> Trajectories/returns -> Stored for PG updates
  - Assumption layer -> Problem structure -> Enables value decomposition

- Critical path: Define optimization objective → Verify Bellman assumptions hold → Select GPI instantiation (PG vs VB) based on computational constraints → Implement evaluation and improvement modules consistently

- Design tradeoffs:
  - PG: Lower statistical complexity (fewer latent parameters), higher space complexity (trajectory storage), more direct behavior mapping
  - VB: Higher statistical complexity (value-to-action mapping), lower space complexity (online learning), stronger reliance on modeling assumptions
  - Assumption: Computational complexity should be weighed alongside parameter count when evaluating model plausibility for biological systems

- Failure signatures:
  - Using immediate reward r(s_t, a_t) instead of cumulative return R_t in PG: converges to sub-optimal policy in non-bandit MDPs
  - Applying standard value-based methods in partially observable or non-Markovian settings: value function may not exist or be meaningful
  - Concluding PG eliminates value representation: conflates acting-time requirements with learning-time requirements

- First 3 experiments:
  1. Implement REINFORCE on a multi-step MDP (not bandit); ablate by replacing R_t with r_t and measure performance gap
  2. Compare memory requirements: run equivalent PG and VB agents on same task, log trajectory storage vs single-transition updates
  3. Relax one Bellman assumption (e.g., introduce partial observability) and characterize how both PG and VB degrade or require modification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do violations of standard assumptions (e.g., risk neutrality, Markovian environments) alter the necessity or representation of Value in biological learning?
- Basis in paper: [explicit] The authors state that "the very notion of Value must be reconsidered when standard assumptions... are relaxed, as is likely in natural settings."
- Why unresolved: Current debates focus on algorithm choice (PG vs. VB) rather than validating the foundational constraints of the optimization problem itself.
- What evidence would resolve it: Behavioral and neural data from tasks explicitly designed to violate Markov properties or induce risk-sensitive behavior.

### Open Question 2
- Question: Do biological agents primarily utilize temporal-difference (bootstrapping) or Monte-Carlo (trajectory-based) methods for policy evaluation?
- Basis in paper: [explicit] The authors list "temporal-difference... versus monte-carlo methods" as a critical, yet under-researched, question obscured by the broader PG vs. VB debate.
- Why unresolved: Both PG and VB can be framed as Generalized Policy Iteration, making them functionally similar without specific inquiry into the update mechanism.
- What evidence would resolve it: Experiments separating learning signals based on immediate prediction errors versus accumulated trajectory rewards.

### Open Question 3
- Question: To what extent does computational complexity (e.g., memory requirements) outweigh statistical complexity (parameter count) in determining the biological plausibility of an RL algorithm?
- Basis in paper: [inferred] The paper argues that VB methods may be computationally simpler (online learning) than PG methods (requiring trajectory storage), a factor often ignored in standard model comparison.
- Why unresolved: Scientific modeling typically penalizes models based on free parameters rather than algorithmic resource demands like memory or processing time.
- What evidence would resolve it: Model comparison studies that incorporate computational constraints, such as working memory load, into the definition of model complexity.

## Limitations
- The paper is entirely theoretical with no empirical validation of its claims
- Claims about computational complexity trade-offs lack quantitative analysis
- The arguments about biological plausibility are speculative without experimental evidence
- The paper focuses on conceptual framework rather than practical implementation details

## Confidence
- High Confidence: The claim that PG methods require value representations for learning
- Medium Confidence: The GPI framework unifying PG and VB methods
- Medium Confidence: The argument about assumption relaxation

## Next Checks
1. Implement REINFORCE vs Q-learning on the same MDP and measure computational complexity (memory usage, convergence speed) to test the GPI framework claims
2. Systematically relax Bellman assumptions (partial observability, non-exponential discounting) and characterize how both PG and VB methods degrade
3. Conduct a computational complexity analysis comparing statistical versus computational complexity across PG and VB methods for cognitive modeling applications