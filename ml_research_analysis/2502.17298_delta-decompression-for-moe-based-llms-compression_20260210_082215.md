---
ver: rpa2
title: Delta Decompression for MoE-based LLMs Compression
arxiv_id: '2502.17298'
source_url: https://arxiv.org/abs/2502.17298
tags:
- compression
- weights
- delta
- experts
- d2-moe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces D2-MoE, a novel compression framework for
  Mixture-of-Experts (MoE) language models that addresses the challenges of prohibitive
  storage and memory requirements. The method decomposes expert weights into a shared
  base weight and unique delta weights, leveraging Fisher information matrix to capture
  shared components and Singular Value Decomposition (SVD) to compress delta weights
  by exploiting their low-rank properties.
---

# Delta Decompression for MoE-based LLMs Compression

## Quick Facts
- arXiv ID: 2502.17298
- Source URL: https://arxiv.org/abs/2502.17298
- Reference count: 16
- Key outcome: Achieves over 13% performance gains on Mixtral, Phi-3.5, DeepSeek, and Qwen2 MoE LLMs at 40-60% compression rates without additional training

## Executive Summary
This paper introduces D2-MoE, a novel compression framework for Mixture-of-Experts (MoE) language models that addresses the challenges of prohibitive storage and memory requirements. The method decomposes expert weights into a shared base weight and unique delta weights, leveraging Fisher information matrix to capture shared components and Singular Value Decomposition (SVD) to compress delta weights by exploiting their low-rank properties. The approach introduces a semi-dynamical structured pruning strategy for base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. D2-MoE achieves over 13% performance gains compared to other compressors on Mixtral, Phi-3.5, DeepSeek, and Qwen2 MoE LLMs at 40-60% compression rates, without requiring additional training.

## Method Summary
D2-MoE decomposes MoE expert weights into shared base weights and unique delta weights using Fisher information matrix analysis to identify shared components across experts. The delta weights are then compressed using Singular Value Decomposition (SVD) to exploit their low-rank properties, significantly reducing storage requirements. A semi-dynamical structured pruning strategy is applied to base weights, combining static analysis (identifying universally redundant parameters) with dynamic analysis (task-specific redundancy patterns) to achieve additional compression while preserving input adaptivity. The framework operates without requiring additional training, making it practical for deployment on resource-constrained systems.

## Key Results
- Achieves over 13% performance gains compared to other compression methods on multiple MoE models
- Maintains model performance at 40-60% compression rates across Mixtral, Phi-3.5, DeepSeek, and Qwen2 architectures
- Demonstrates effective compression without requiring additional training or fine-tuning

## Why This Works (Mechanism)
The approach works by exploiting the inherent redundancy in MoE architectures, where experts often share similar weight patterns for common patterns while maintaining unique components for specialized tasks. The Fisher information matrix identifies the shared base weights that capture universal patterns across experts, while SVD compression effectively reduces the dimensionality of delta weights that represent expert-specific variations. The semi-dynamical pruning strategy further optimizes base weights by removing both static redundancies (present across all inputs) and dynamic redundancies (specific to certain input patterns), achieving higher compression rates without sacrificing the model's ability to adapt to different inputs.

## Foundational Learning

**Fisher Information Matrix Analysis**
- Why needed: Identifies shared weight components across experts by measuring parameter sensitivity to data
- Quick check: Verify that computed Fisher scores show high values for parameters that are consistently important across different expert activations

**Singular Value Decomposition (SVD) Compression**
- Why needed: Exploits low-rank structure in delta weights to achieve significant dimensionality reduction
- Quick check: Confirm that top-k singular values capture >95% of the variance in delta weight matrices

**Semi-Dynamical Pruning Strategy**
- Why needed: Combines static (universal) and dynamic (input-specific) redundancy analysis for optimal base weight compression
- Quick check: Validate that pruned base weights maintain task-specific activation patterns while reducing overall parameter count

## Architecture Onboarding

**Component Map**
D2-MoE components: Input -> Fisher Analysis -> Base Weight Extraction -> SVD Compression (Delta Weights) -> Semi-Dynamical Pruning -> Compressed Model

**Critical Path**
1. Fisher information matrix computation across expert activations
2. Base weight decomposition and SVD compression of delta weights
3. Semi-dynamical pruning of base weights based on redundancy analysis

**Design Tradeoffs**
- Compression rate vs. performance retention: Higher compression yields more storage savings but may impact accuracy
- Static vs. dynamic pruning balance: More dynamic pruning preserves input adaptivity but reduces compression efficiency
- SVD rank selection: Higher ranks maintain fidelity but offer less compression benefit

**Failure Signatures**
- Significant performance drop indicates over-aggressive compression or incorrect Fisher threshold settings
- Input-specific failures suggest insufficient dynamic pruning, losing task-specific weight patterns
- Inconsistent results across different MoE models may indicate architecture-specific limitations

**First Experiments**
1. Benchmark D2-MoE against recent state-of-the-art MoE compression methods (DeRS, MoBE) on identical hardware and metrics
2. Measure and report inference-time computational overhead introduced by semi-dynamical pruning strategy
3. Conduct ablation studies on impact of different SVD rank thresholds and pruning parameters on final performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Computational overhead of semi-dynamical pruning strategy during inference is not quantified
- Scalability to larger MoE models with more experts remains unclear
- Method's sensitivity to hyperparameter choices (compression ratios, pruning thresholds) is not thoroughly explored

## Confidence

**Performance gains over baselines**: Medium
- Experimental validation covers multiple MoE models but lacks comparison to recent state-of-the-art methods

**No additional training requirement**: Medium
- The claim may be misleading as some fine-tuning or adaptation might be implicit in the compression process

**Generalization across MoE architectures**: Low
- Limited testing across different downstream tasks and model scales

## Next Checks

1. Benchmark D2-MoE against the most recent MoE compression methods (e.g., DeRS, MoBE) on identical hardware and metrics
2. Measure and report the inference-time computational overhead introduced by the semi-dynamical pruning strategy
3. Conduct ablation studies on the impact of different SVD rank thresholds and pruning parameters on final performance