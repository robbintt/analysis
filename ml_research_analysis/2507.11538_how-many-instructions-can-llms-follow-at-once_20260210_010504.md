---
ver: rpa2
title: How Many Instructions Can LLMs Follow at Once?
arxiv_id: '2507.11538'
source_url: https://arxiv.org/abs/2507.11538
tags:
- instruction
- instructions
- performance
- ratio
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IFScale is a benchmark measuring how instruction-following performance
  degrades as instruction density increases from 10 to 500 instructions. The benchmark
  uses keyword-inclusion instructions for business report generation, with performance
  measured by exact word matches.
---

# How Many Instructions Can LLMs Follow at Once?

## Quick Facts
- arXiv ID: 2507.11538
- Source URL: https://arxiv.org/abs/2507.11538
- Reference count: 23
- Primary result: Best models achieved only 68% accuracy at 500 instructions

## Executive Summary
This paper introduces IFScale, a benchmark designed to measure how instruction-following performance degrades as instruction density increases from 10 to 500 instructions. The benchmark uses keyword-inclusion instructions for business report generation, with performance measured by exact word matches. Testing 20 state-of-the-art models across seven providers revealed that even the best models achieved only 68% accuracy at maximum instruction density, highlighting significant limitations in LLM instruction-following capabilities.

The research uncovered three distinct degradation patterns (threshold, linear, exponential) across models, along with a universal primacy effect showing attention limitations. Notably, systematic error shifts from modification to omission were observed under cognitive load. These findings suggest fundamental constraints in how LLMs process and retain multiple instructions simultaneously, with implications for real-world applications requiring complex multi-step reasoning.

## Method Summary
IFScale measures instruction-following performance under increasing cognitive load by varying instruction density from 10 to 500 instructions using keyword-inclusion tasks for business report generation. Performance is quantified through exact word matching between generated outputs and expected results. The benchmark tests 20 state-of-the-art models across seven providers, analyzing how accuracy degrades as the number of concurrent instructions increases. The methodology systematically explores the relationship between instruction complexity and LLM performance, revealing how different models handle cognitive load differently.

## Key Results
- Best models achieved only 68% accuracy at 500 instructions
- Three distinct degradation patterns identified: threshold, linear, and exponential
- Universal primacy effect observed across all tested models, indicating attention limitations

## Why This Works (Mechanism)
The degradation in instruction-following performance as instruction density increases appears to stem from fundamental limitations in LLM attention mechanisms and working memory capacity. As the number of instructions grows, the model must maintain and process more information simultaneously, leading to competition for limited attentional resources. This manifests as the universal primacy effect, where earlier instructions are more likely to be followed correctly, suggesting that attention mechanisms prioritize information based on temporal order rather than instruction importance.

The systematic error shift from modification to omission under cognitive load indicates that when models become overwhelmed, they default to simpler strategies of simply skipping instructions rather than attempting (and potentially failing) to modify their output appropriately. This represents a fundamental trade-off between completeness and accuracy under resource constraints.

## Foundational Learning

Attention Mechanisms
- Why needed: Understanding how models allocate computational resources to different instructions
- Quick check: Examine attention weight distributions across instruction positions

Working Memory Capacity
- Why needed: Determines how many instructions can be actively processed simultaneously
- Quick check: Measure performance degradation rate as instruction count increases

Cognitive Load Theory
- Why needed: Provides framework for understanding performance degradation under complexity
- Quick check: Correlate instruction density with error type distribution

Prompt Engineering Strategies
- Why needed: Techniques for optimizing instruction delivery to models
- Quick check: Compare performance using different prompt structures

Evaluation Metrics Design
- Why needed: Ensures meaningful measurement of instruction-following capability
- Quick check: Test sensitivity of metrics to semantic vs. syntactic equivalence

## Architecture Onboarding

Component Map:
Instruction Generator -> LLM Engine -> Output Evaluator -> Performance Analyzer

Critical Path:
1. Instruction generation and formatting
2. Model inference with varying instruction counts
3. Output evaluation using exact matching
4. Performance analysis and pattern identification

Design Tradeoffs:
- Exact matching provides precision but may be overly strict
- Keyword-based instructions simplify evaluation but limit generalizability
- Fixed instruction counts enable systematic comparison but may not reflect real-world variability

Failure Signatures:
- Performance degradation follows predictable patterns (threshold, linear, exponential)
- Primacy effect indicates attention limitations
- Error type shifts from modification to omission under load

First 3 Experiments:
1. Test semantic equivalence matching vs. exact matching
2. Vary instruction types beyond keywords (e.g., logical constraints)
3. Compare open-source vs. closed models across parameter ranges

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on keyword-inclusion tasks which may not generalize to other domains
- Exact word matching metric may be overly strict, not accounting for semantic equivalence
- Small sample size of 20 models across seven providers limits generalizability

## Confidence

High confidence: Instruction-following performance degrades with increasing instruction density
Medium confidence: Three distinct degradation patterns are consistent across models
Medium confidence: Universal primacy effect indicates attention limitations
Low confidence: Systematic error shift from modification to omission needs replication

## Next Checks
1. Test IFScale with non-keyword-based instructions (e.g., logical reasoning, creative writing) to assess domain generalizability
2. Implement semantic equivalence matching rather than exact word matching to determine if performance gap is methodology-dependent
3. Validate degradation patterns and error shift hypothesis using models from additional providers and open-source models with varying parameter counts