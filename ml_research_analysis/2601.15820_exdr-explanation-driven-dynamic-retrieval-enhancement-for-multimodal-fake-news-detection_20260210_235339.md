---
ver: rpa2
title: 'ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake
  News Detection'
arxiv_id: '2601.15820'
source_url: https://arxiv.org/abs/2601.15820
tags:
- retrieval
- fake
- image
- label
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExDR, a framework that addresses multimodal
  fake news detection through explanation-driven dynamic retrieval. It leverages model-generated
  explanations to guide both retrieval triggering and evidence selection, assessing
  confidence via label uncertainty, token support, and sentence-level coherence.
---

# ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection

## Quick Facts
- arXiv ID: 2601.15820
- Source URL: https://arxiv.org/abs/2601.15820
- Authors: Guoxuan Ding; Yuqing Li; Ziyan Zhou; Zheng Lin; Daren Zha; Jiangnan Li
- Reference count: 40
- One-line primary result: ExDR achieves state-of-the-art accuracy on multimodal fake news detection using explanation-driven dynamic retrieval

## Executive Summary
ExDR introduces a novel framework for multimodal fake news detection that leverages model-generated explanations to guide dynamic retrieval of supporting evidence. The approach addresses the challenge of false news spread across multiple modalities by assessing confidence through label uncertainty, token support, and sentence-level coherence. By incorporating entity-enriched indexing and contrastive evidence based on fine-grained deception labels, ExDR significantly outperforms existing methods on AMG and MR2 datasets.

The framework demonstrates that explanation-driven retrieval can effectively enhance fake news detection accuracy while maintaining retrieval efficiency. With up to 86.7% accuracy achieved on evaluated datasets, ExDR establishes new state-of-the-art performance benchmarks while providing interpretable detection through its evidence-based approach.

## Method Summary
ExDR employs a two-stage retrieval enhancement mechanism guided by explanation quality. The framework first assesses retrieval triggering confidence using multiple metrics including label uncertainty, token-level support scores, and sentence coherence evaluation. When confidence thresholds are met, the system initiates evidence retrieval enhanced with entity-enriched indexing from knowledge graphs and contrastive evidence selection based on fine-grained deception labels. This dynamic approach ensures that evidence retrieval is both targeted and contextually relevant, reducing noise while improving detection accuracy.

## Key Results
- Achieves state-of-the-art accuracy on AMG and MR2 multimodal fake news detection datasets
- Demonstrates up to 86.7% accuracy with significant improvements in retrieval efficiency metrics
- Shows superior performance in both retrieval triggering and evidence retrieval components

## Why This Works (Mechanism)
The effectiveness of ExDR stems from its confidence-driven approach to evidence retrieval. By evaluating the model's certainty across multiple dimensions before initiating retrieval, the framework minimizes unnecessary computational overhead while ensuring that evidence collection occurs when it will be most beneficial. The use of entity-enriched indexing connects retrieved evidence to relevant knowledge graph entities, providing richer context for fake news detection. Additionally, the contrastive evidence mechanism, which considers fine-grained deception labels, enables more nuanced discrimination between authentic and deceptive content.

## Foundational Learning

1. **Confidence Metrics in Retrieval Systems** - Why needed: Determines when evidence retrieval should be triggered to optimize computational resources. Quick check: Evaluate retrieval triggering accuracy against fixed-interval baselines.

2. **Entity-enriched Indexing** - Why needed: Provides semantic context by connecting evidence to knowledge graph entities. Quick check: Measure retrieval precision improvement with and without entity enrichment.

3. **Contrastive Evidence Selection** - Why needed: Enables fine-grained discrimination between different types of deceptive content. Quick check: Assess detection accuracy improvements using contrastive vs. random evidence selection.

4. **Multimodal Fusion Techniques** - Why needed: Integrates evidence from multiple data modalities for comprehensive analysis. Quick check: Compare multimodal vs. single-modal detection performance.

## Architecture Onboarding

**Component Map**: Explanation Generator -> Confidence Assessor -> Retrieval Trigger -> Entity-enriched Indexer -> Contrastive Evidence Selector -> Detection Model

**Critical Path**: The most time-critical execution path is the Confidence Assessment -> Retrieval Trigger sequence, as this determines whether computationally expensive retrieval operations should proceed. This path must operate with minimal latency to maintain real-time detection capabilities.

**Design Tradeoffs**: The framework trades computational overhead for improved accuracy through selective evidence retrieval. While this increases processing time when retrieval is triggered, it reduces overall computational load by avoiding unnecessary evidence collection in low-confidence scenarios.

**Failure Signatures**: System failures typically manifest as either excessive false positives (indicating overly aggressive retrieval triggering) or missed detections (suggesting inadequate confidence assessment). Monitoring the false positive rate and detection recall provides early warning of these failure modes.

**Three First Experiments**:
1. Validate confidence assessment accuracy by comparing predicted confidence scores against human-annotated confidence levels
2. Test entity-enriched retrieval precision by measuring recall of relevant evidence with and without entity connections
3. Evaluate contrastive evidence effectiveness by comparing detection accuracy using contrastive vs. randomly selected evidence

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on model-generated explanations may introduce bias and error propagation
- Performance may degrade with nuanced deception patterns not captured by explanation generation
- Effectiveness depends on knowledge graph quality and coverage across different domains and languages

## Confidence
- High confidence in technical feasibility of explanation-driven retrieval mechanism
- Medium confidence in generalization across diverse news domains
- Medium confidence in scalability of entity-enriched retrieval to large knowledge graphs
- Low confidence in robustness to adversarial examples or misleading explanations

## Next Checks
1. Evaluate ExDR's performance on adversarial datasets containing intentionally misleading explanations to test robustness
2. Conduct cross-domain validation using news datasets from different languages and cultural contexts
3. Perform ablation studies to quantify individual contributions of explanation quality, entity enrichment, and contrastive evidence to overall performance