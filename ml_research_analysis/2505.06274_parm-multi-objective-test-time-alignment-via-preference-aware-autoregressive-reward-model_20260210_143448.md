---
ver: rpa2
title: 'PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive
  Reward Model'
arxiv_id: '2505.06274'
source_url: https://arxiv.org/abs/2505.06274
tags:
- parm
- preference
- alignment
- multi-objective
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-objective test-time
  alignment in large language models (LLMs), where the goal is to adapt frozen LLMs
  to diverse user preferences during inference. The authors propose PARM, a single
  unified autoregressive reward model trained across all preference dimensions, which
  uses a novel Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA) to condition
  the model on user preference vectors.
---

# PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model

## Quick Facts
- **arXiv ID**: 2505.06274
- **Source URL**: https://arxiv.org/abs/2505.06274
- **Reference count**: 18
- **Key outcome**: PARM is a single unified autoregressive reward model trained across all preference dimensions, using Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA) to condition the model on user preference vectors, enabling dynamic adjustment of output rewards to guide frozen LLMs toward aligned responses.

## Executive Summary
This paper addresses the challenge of multi-objective test-time alignment in large language models, where the goal is to adapt frozen LLMs to diverse user preferences during inference. The authors propose PARM, a single unified autoregressive reward model trained across all preference dimensions, which uses a novel Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA) to condition the model on user preference vectors. This allows PARM to dynamically adjust the output reward according to the given preference vector, guiding the frozen LLM to generate responses aligned with user needs. Experiments on safety alignment and helpful assistant tasks demonstrate that PARM significantly reduces inference costs and achieves better alignment with preference vectors compared to existing methods. Additionally, PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger frozen LLM without expensive training, making multi-objective alignment accessible with limited computing resources.

## Method Summary
PARM uses a unified autoregressive reward model trained across all preference dimensions via linear scalarization, where each training iteration samples a preference vector α and computes a weighted loss across datasets. The key innovation is PBLoRA, which decomposes adaptation into preference-agnostic and preference-aware terms using a bilinear form that spans an r²-dimensional subspace versus r-dimensional for standard LoRA. This enables more expressive preference conditioning. At inference, PARM outputs token-level rewards that modify the base model's logits, enabling test-time alignment without modifying the frozen LLM. The model can also perform weak-to-strong guidance, where a smaller PARM guides a larger frozen LLM.

## Key Results
- PARM significantly reduces inference costs compared to existing methods while achieving better alignment with preference vectors
- The bilinear PBLoRA mechanism provides higher expressivity than standard LoRA for preference-conditioned adaptation
- PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger frozen LLM without expensive training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training across all preference dimensions produces better preference alignment than combining independently-trained reward models.
- Mechanism: PARM optimizes a single unified ARM across all preference dimensions simultaneously via linear scalarization, forcing the model to learn trade-offs explicitly during training rather than post-hoc combination.
- Core assumption: Preference dimensions share learnable structure that can be captured jointly; linear scalarization recovers Pareto-optimal solutions.
- Evidence anchors: [abstract] "PARM is a single unified ARM trained across all preference dimensions... to explicitly optimize trade-offs between different preferences." [Section 4.3, Eq. 10] Shows the training objective with expectation over α sampled from simplex.

### Mechanism 2
- Claim: The bilinear form BWA in PBLoRA provides higher expressivity than standard LoRA for preference-conditioned adaptation.
- Mechanism: PBLoRA decomposes adaptation into preference-agnostic and preference-aware terms, enabling the model to represent more diverse preference trade-offs with comparable parameter count.
- Core assumption: The preference vector α can be mapped to W₂(α) via a simple linear layer; higher-dimensional subspace translates to better Pareto coverage.
- Evidence anchors: [Section 4.2, Theorem 4.1] Proves r² linearly independent matrices in bilinear form vs. r in standard LoRA. [Table 5] Ablation shows PBLoRA achieves HV=113.38 vs. SVD-LoRA at 101.81.

### Mechanism 3
- Claim: A smaller PARM can guide a larger frozen LLM (weak-to-strong guidance) without training the larger model.
- Mechanism: Since PARM outputs token-level rewards that modify the base model's logits, the reward model need not match the base model size. The guidance signal is decoupled from model capacity.
- Core assumption: Token-level reward predictions from a smaller model transfer to guide larger models; the β scaling factor compensates for model mismatch.
- Evidence anchors: [Section 5.1, Figure 2, Table 3] PARM-7B guiding Alpaca-65B achieves HV=121.73, outperforming GenARM and MOD-w2s.

## Foundational Learning

- **Concept**: Pareto Optimality in Multi-Objective Optimization
  - Why needed here: PARM approximates the Pareto set—a family of solutions where improving one objective requires sacrificing another. Understanding this is essential to interpret why preference vectors map to specific trade-offs.
  - Quick check question: Given two objectives (helpfulness, harmlessness), can a solution dominate another if it scores higher on both?

- **Concept**: Low-Rank Adaptation (LoRA)
  - Why needed here: PBLoRA extends LoRA's parameter-efficient fine-tuning by adding bilinear preference conditioning. Without understanding LoRA's BA decomposition, the BWA modification is opaque.
  - Quick check question: Why does LoRA use low-rank matrices B and A rather than fine-tuning the full weight matrix?

- **Concept**: Autoregressive Reward Models (ARM)
  - Why needed here: Unlike response-level reward models, ARM predicts token-level rewards enabling efficient test-time guidance without generating complete responses first.
  - Quick check question: How does token-level reward prediction enable test-time alignment without modifying the base LLM?

## Architecture Onboarding

- **Component map**: Base LLM (πbase) -> PARM backbone -> PBLoRA modules -> Preference encoder
- **Critical path**:
  1. Sample preference vector α from simplex during training
  2. Compute θ(α) = θ₀ + s[B₁W₁A₁ + B₂W₂(α)A₂]
  3. Forward pass through PARM to get token-level log probabilities
  4. Compute weighted loss across all preference dimension datasets
  5. At inference, α is user-specified; guide base LLM via Eq. 13
- **Design tradeoffs**:
  - r₁ vs. r₂ split: More r₂ increases preference sensitivity but reduces shared feature capacity
  - β parameter: Higher β weakens guidance signal; lower β may cause overfitting to reward model
  - PARM size vs. base LLM size: Smaller PARM is cheaper but may limit guidance quality on complex tasks
- **Failure signatures**:
  - Clustered Pareto front: Indicates preference misalignment—model cannot smoothly interpolate
  - Low MIP despite high HV: Model covers objective space but responses don't match specified α—suggests conditioning failure
  - Inference time blowup: If using multiple forward passes, PBLoRA integration is incorrect
- **First 3 experiments**:
  1. Validate PBLoRA vs. LoRA: Train PARM with PBLoRA vs. standard LoRA on PKU-SafeRLHF-10K. Compare HV and MIP.
  2. Weak-to-strong transfer test: Train PARM-7B, use it to guide Alpaca-7B and Alpaca-65B. Measure HV degradation.
  3. Preference interpolation smoothness: Sample α along simplex edges with 0.1 increments. Plot helpfulness vs. harmlessness scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PARM effectively identify and represent Pareto-optimal solutions located in non-convex regions of the objective space?
- Basis in paper: Section 4.3, Equation (10) employs a linear scalarization that is theoretically limited to finding solutions on the convex hull of the Pareto front and may miss optimal solutions in concave regions.
- Why unresolved: The paper does not analyze the convexity of the reward landscapes for LLM alignment or demonstrate that PARM captures the full diversity of the front if it is non-convex.
- What evidence would resolve it: Experiments on synthetic or real-world alignment tasks with known non-convex Pareto fronts, showing that PARM can recover solutions in the concave regions.

### Open Question 2
- Question: How does the fidelity of preference control scale as the number of distinct objectives (k) increases into higher dimensions (e.g., k > 10)?
- Basis in paper: The experiments are restricted to low-dimensional preference spaces (Safety: k=2; Helpful Assistant: k=3).
- Why unresolved: The PBLoRA mechanism uses a linear layer to generate the conditioning matrix W₂(α). As the dimensionality of α increases, the complexity of the interactions between objectives may exceed the capacity of a linear mapping to disentangle.
- What evidence would resolve it: Benchmarks on a multi-objective dataset with significantly more dimensions, analyzing the alignment accuracy degradation as k increases.

### Open Question 3
- Question: Does the weak-to-strong guidance capability transfer effectively across different model architectures or vocabularies?
- Basis in paper: Section 5.1 and 5.2 demonstrate weak-to-strong guidance, but strictly within the same model family.
- Why unresolved: The token-level rewards predicted by the ARM are heavily dependent on the vocabulary and log-probability distribution of the base model it was trained on.
- What evidence would resolve it: Experiments where a PARM trained on a small model from one family is used to guide the generation of a frozen large model from a different family.

## Limitations
- Parameterization of PBLoRA (initialization and tying) is unspecified, affecting reproducibility
- Preference vector sampling distribution is not detailed, which could bias the learned Pareto front
- Generalization across domains beyond safety and helpfulness remains untested

## Confidence
- **High confidence**: The core PBLoRA mechanism and its theoretical expressivity advantage over standard LoRA
- **Medium confidence**: The weak-to-strong guidance claim and preference alignment metrics
- **Medium confidence**: The ability to capture non-convex Pareto fronts and scale to higher-dimensional preference spaces

## Next Checks
1. **PBLoRA parameterization audit**: Re-run the PKU-SafeRLHF-10K experiment with different PBLoRA initialization schemes and measure HV sensitivity. Compare to baseline LoRA with matched parameter counts.
2. **Preference interpolation validation**: Generate responses for α values sampled at 0.05 intervals along the simplex edge from (1,0) to (0,1). Plot the trajectory of helpfulness vs. harmlessness scores. Check for discontinuities or clustering.
3. **Cross-domain generalization test**: Train PARM on safety and helpfulness, then evaluate on a third preference dimension (e.g., formality from the Formality Transfer dataset). Measure HV degradation and analyze whether PBLoRA parameters transfer or require retraining.