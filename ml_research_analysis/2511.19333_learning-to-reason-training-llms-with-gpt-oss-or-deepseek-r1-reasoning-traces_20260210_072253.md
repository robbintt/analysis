---
ver: rpa2
title: 'Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces'
arxiv_id: '2511.19333'
source_url: https://arxiv.org/abs/2511.19333
tags:
- reasoning
- traces
- gpt-oss
- deepseek-r1
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work compares the impact of training medium-sized language\
  \ models with two distinct reasoning styles generated by DeepSeek-R1 and gpt-oss\
  \ on math problem-solving tasks. Both reasoning styles achieve comparable accuracy\
  \ on math benchmarks, but gpt-oss traces are 4\xD7 more token-efficient, generating\
  \ significantly fewer tokens per response."
---

# Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces

## Quick Facts
- arXiv ID: 2511.19333
- Source URL: https://arxiv.org/abs/2511.19333
- Reference count: 9
- Medium-sized LLMs trained on two reasoning styles achieve comparable math performance, with gpt-oss being 4× more token-efficient

## Executive Summary
This work compares the impact of training medium-sized language models with two distinct reasoning styles generated by DeepSeek-R1 and gpt-oss on math problem-solving tasks. Both reasoning styles achieve comparable accuracy on math benchmarks, but gpt-oss traces are 4× more token-efficient, generating significantly fewer tokens per response. This demonstrates that verbose reasoning does not necessarily lead to better performance, and models can be effectively trained to adopt more efficient reasoning patterns. The reduction in token usage directly translates to lower latency and operational costs in real-world deployments.

## Method Summary
The study trains medium-sized language models on math problems using two different reasoning trace datasets: one generated by DeepSeek-R1 and another by gpt-oss. The models are evaluated on their problem-solving accuracy and token efficiency across various math benchmarks. The comparison focuses on how different reasoning verbosity levels affect both performance and computational costs, with particular attention to token usage per response.

## Key Results
- Both reasoning styles achieve comparable accuracy on math benchmarks
- gpt-oss reasoning traces are 4× more token-efficient than DeepSeek-R1 traces
- Verbose reasoning does not necessarily lead to better performance

## Why This Works (Mechanism)
The study demonstrates that reasoning trace verbosity and problem-solving accuracy are not directly correlated. Models trained on more concise reasoning traces (gpt-oss) can achieve the same accuracy as those trained on verbose traces (DeepSeek-R1) while using significantly fewer tokens. This suggests that the quality of reasoning steps matters more than their quantity, and that efficient reasoning patterns can be learned and transferred to smaller models.

## Foundational Learning
- **Token efficiency**: Understanding how token count relates to computational cost and latency - why needed for deployment optimization, quick check: compare FLOPs per token
- **Reasoning trace quality**: Distinguishing between useful and superfluous reasoning steps - why needed for model training, quick check: ablation studies on trace components
- **Math benchmark evaluation**: Standardized metrics for assessing problem-solving capabilities - why needed for fair comparison, quick check: cross-dataset validation
- **Medium-sized LLM training**: Techniques for effective training with limited parameters - why needed for practical deployment, quick check: parameter efficiency metrics

## Architecture Onboarding
- **Component map**: Math problems -> Reasoning traces (gpt-oss/DeepSeek-R1) -> Model training -> Benchmark evaluation
- **Critical path**: Problem input → Reasoning trace generation → Model response → Accuracy calculation
- **Design tradeoffs**: Token efficiency vs. reasoning completeness, model size vs. performance
- **Failure signatures**: Performance degradation on complex problems, increased token usage without accuracy gains
- **First experiments**: 1) Compare accuracy on simple vs. complex math problems, 2) Measure token usage across different problem types, 3) Test transferability to non-math domains

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on math benchmarks, limiting generalizability
- Does not investigate performance on more complex reasoning tasks requiring detailed explanations
- Token efficiency metric does not account for potential differences in reasoning quality

## Confidence
- Token efficiency comparison: High
- Performance-accuracy trade-off: Medium
- Cost/latency implications: Medium

## Next Checks
1. Test both reasoning styles on non-math domains (code generation, scientific reasoning, common sense reasoning) to assess generalizability
2. Conduct ablation studies varying reasoning trace verbosity levels to identify optimal efficiency-performance trade-offs
3. Measure actual inference latency and API costs using real deployment scenarios with production-grade models