---
ver: rpa2
title: Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify
arxiv_id: '2601.02306'
source_url: https://arxiv.org/abs/2601.02306
tags:
- learning
- promotions
- podcast
- multi-task
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the cold-start problem in podcast advertising\
  \ and promotions on Spotify by unifying separate targeting models through multi-task\
  \ learning (MTL). The core idea is to jointly optimize ad and promotion objectives\u2014\
  such as streams, clicks, and follows\u2014using a shared representation of user,\
  \ content, context, and creative features."
---

# Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify

## Quick Facts
- arXiv ID: 2601.02306
- Source URL: https://arxiv.org/abs/2601.02306
- Reference count: 33
- Primary result: Multi-task learning model improves promotion AP by 4.5% and ad AP by 50.2%, with up to 22% eCPS reduction in online tests.

## Executive Summary
This paper addresses the cold-start problem in podcast advertising and promotions on Spotify by unifying separate targeting models through multi-task learning (MTL). The core idea is to jointly optimize ad and promotion objectives—such as streams, clicks, and follows—using a shared representation of user, content, context, and creative features. A key innovation is directional transfer: promotion data updates both promotion and ad task heads, while ad data only updates ad heads, combined with source-balanced sampling to ensure balanced gradient contributions. Offline experiments show that the unified MTL model improves Average Precision for promotions by 4.5% and for ads by 50.2% compared to a promotions-only baseline, with ancillary engagement heads further boosting ad performance. In online A/B tests, the model achieves up to a 22% reduction in effective cost-per-stream (eCPS) and 18–24% higher stream rates, with the largest gains for less-streamed podcasts.

## Method Summary
The approach uses a shared encoder with task-specific towers for joint optimization of podcast ad and promotion targeting. The model processes four feature groups: user signals (history, follows, search), content signals (show/episode IDs, embeddings, genres, topics), context (time, surface, session state), and creative/metadata (slot, layout, campaign, ad format). Training uses source-balanced sampling (50% promotions, 50% ads) and directional loss masking that allows promotion impressions to update both promotion and ad towers while ad impressions update only ad towers. The model is trained with Adam optimizer and binary cross-entropy loss, evaluated using Average Precision offline and effective cost-per-stream, impression-to-stream rate, and CTR online.

## Key Results
- Offline: Promotions AP improved by 4.5% and Ads AP by 50.2% compared to promotions-only baseline
- Online A/B test: Up to 22% reduction in effective cost-per-stream (eCPS) and 18–24% higher stream rates
- Cold-start benefit: Largest improvements (up to +60% i2s) for less-streamed podcasts (Tier 3–5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing a unified encoder across ads and promotions enables positive transfer that particularly benefits cold-start (low-data) items.
- Mechanism: A shared encoder h_φ(x) maps user, content, context, and creative features into a joint representation z. Task-specific towers g_ψ_t(z) consume z to predict outcomes for each task. Because both ad and promotion tasks draw on overlapping user interests and content affinities, gradients from data-rich tasks regularize and improve representations for data-scarce tasks.
- Core assumption: Tasks share latent structure (e.g., user–podcast affinity) strong enough that joint training yields net positive transfer despite potential interference.
- Evidence anchors:
  - [abstract] "By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks."
  - [Section 4.4.1] For low-stream tiers (Tiers 3–5), i2s improves by +27%, +33%, and up to +60% with CPS reductions of ~20%, 24%, and 38% respectively—monotonically larger gains as popularity decreases.
  - [corpus] MTMD (Multi-Task Multi-Domain for Pinterest ads) reports similar findings: unified ranking across domains with task-specific heads improves performance where data is sparse, corroborating cross-task transfer benefits.
- Break condition: If tasks have conflicting optimal representations (e.g., an ad format that systematically rewards clickbait vs. a promotion optimizing long-term follows), negative transfer may degrade both; ablations should show AP declines on one or more heads.

### Mechanism 2
- Claim: Directional loss masking enables controlled transfer by allowing promotion data to inform ad towers, but not vice versa.
- Mechanism: A binary mask m_{s,t} zeroes loss for promotion tasks on ad impressions (s=A, t∈T_P) while keeping all tasks active for promotion impressions. Promotion examples thus update both ad and promotion towers, but ad examples update only ad towers. This helps ad tasks (often data-scarce for new objectives) benefit from the higher-volume promotion signal without letting ad-specific biases distort promotion towers.
- Core assumption: Promotion-to-ad transfer is net beneficial, while ad-to-promotion transfer would introduce noise or bias (e.g., ad engagement artifacts misaligned with organic promotion behavior).
- Evidence anchors:
  - [Section 3.3] "This implements directional transfer: promotion impressions update both promotion and ad towers, while ad impressions update only ad towers."
  - [Table 1] Ads-only configurations with ancillary heads still severely degrade Promotions AP (≈−65%), suggesting ad-only training lacks appropriate signal for promotion tasks.
  - [corpus] No direct corpus evidence for directional masking in neighbor papers; related work (PLE, PCGrad) focuses on gradient conflict mitigation rather than channel-aware masking.
- Break condition: If ad data contains high-signal patterns (e.g., strong creative–podcast affinity) that promotion data lacks, masking may underutilize ad signals; monitor whether unmasked bidirectional training improves promotion AP without harming ads.

### Mechanism 3
- Claim: Source-balanced sampling prevents the joint model from collapsing toward the higher-volume data source.
- Mechanism: Each mini-batch is constructed to contain ~50% promotion and ~50% ad impressions. Without this, higher-volume ad traffic would dominate gradient updates, implicitly reweighting the model toward ad-specific patterns and reducing promotion effectiveness.
- Core assumption: Volume imbalance between channels is large enough to cause mode collapse, and balanced batches yield stable, representative gradients without excessive variance.
- Evidence anchors:
  - [Section 3.3] "This keeps gradients from promotions and ads at comparable scales and avoids the joint model collapsing toward the higher-volume source."
  - [Section 4.2] The unified model improves Promotions AP by +4.5% and Ads AP by +50.2% relative to baseline, suggesting balanced training did not sacrifice one channel for the other.
  - [corpus] Audio-centric MTL for Spotify streaming ads (Verma et al. 2025) uses multi-task weighting but does not explicitly report source-balanced sampling; this design choice may be domain-specific.
- Break condition: If one channel has much noisier labels, equal sampling may amplify noise; consider adjusting the ratio or applying loss weighting if one task's gradient variance destabilizes training.

## Foundational Learning

- Concept: Multi-Task Learning (MTL) basics
  - Why needed here: The paper's core contribution is an MTL model; understanding shared vs. task-specific parameters, negative transfer, and gradient interactions is prerequisite.
  - Quick check question: Can you explain why adding a second task might hurt the first task's performance, and one technique to mitigate it?

- Concept: Cold-start in recommender systems
  - Why needed here: The stated motivation is addressing cold-start for new ad objectives and less-streamed creators; you need to recognize how data sparsity affects model estimation.
  - Quick check question: For a podcast with 10 lifetime streams vs. 10,000, what happens to the variance of a learned content embedding under the same learning rate?

- Concept: Binary cross-entropy with class imbalance
  - Why needed here: The paper uses AP (Average Precision) specifically because of heavy class imbalance; BCE loss behavior under imbalance is central to training stability.
  - Quick check question: Why might AUC-ROC be misleading compared to AP when positives are extremely rare?

## Architecture Onboarding

- Component map: Input features (user, content, context, creative) -> Shared encoder h_φ(x) with post-batch norm -> Task-specific MLP towers g_ψ_t(z) -> Sigmoid probabilities -> Masked BCE loss -> Adam optimizer

- Critical path:
  1. Pool ad and promotion impressions, tagging each with source label s ∈ {P, A}
  2. Apply source-balanced sampling to construct 50/50 mini-batches
  3. Forward pass through shared encoder → task towers → sigmoid probabilities
  4. Compute masked BCE loss: zero out promotion-task loss on ad impressions
  5. Backpropagate; only unmasked task gradients flow to shared encoder
  6. Evaluate offline via AP per task; online via i2s, eCPS, CTR

- Design tradeoffs:
  - Unified vs. siloed models: Unified reduces engineering overhead and enables transfer but requires careful interference management
  - Directional masking vs. bidirectional: Masking protects promotion heads from ad noise but may discard useful ad signal
  - Shared-bottom vs. MoE/PLE: Shared-bottom is simpler and worked here; MoE or PLE may help at larger scale or with noisier task overlap
  - Balanced sampling vs. natural distribution: Balanced stabilizes gradients but may distort calibration; may need post-hoc recalibration for serving

- Failure signatures:
  - Promotions AP drops significantly (>10%) after joint training: Likely negative transfer from ad gradients; increase masking strictness or add task-specific experts
  - Training loss diverges or oscillates: Check source balance; if one channel has extremely sparse labels, reduce its effective weight or increase batch size
  - Large gap between offline AP gains and online i2s/eCPS: Potential overfitting to proxy metrics; add more online-aware features or regularization
  - Cold-start tiers show no improvement: Verify feature coverage for low-stream items (e.g., episode embeddings, genre metadata); cold items may lack meaningful content features

- First 3 experiments:
  1. Ablation on directional masking: Train with and without the mask m_{s,t}; compare AP per task to quantify net transfer vs. interference
  2. Batch balance sweep: Test 25/75, 50/50, and 75/75 promo/ad splits; observe gradient norm ratios and AP stability to find robust sampling
  3. Tier-stratified evaluation: Split test set by stream tiers (0–5); report AP and online metrics per tier to confirm cold-start benefits and detect regression on head items

## Open Questions the Paper Calls Out

- Question: Does the unified modeling strategy transfer effectively to other content verticals beyond podcasts?
  - Basis in paper: [explicit] The conclusion states that "while our study focuses on podcasts, the approach naturally extends to other verticals (e.g., music, audiobooks, video) where ads and organic promotions share user and content representations."
  - Why unresolved: The current study validates the approach solely within the Spotify podcast ecosystem; generalization to media types with different interaction patterns (e.g., music listening vs. podcast consumption) remains untested.
  - What evidence would resolve it: Empirical results from applying the joint MTL framework to music or audiobook ad/promotion tasks, showing similar cold-start mitigation and eCPS reductions.

- Question: Would architectural advancements like Mixture-of-Experts (MoE) outperform the shared-bottom encoder used in this deployment?
  - Basis in paper: [inferred] The authors mention considering "Mixture-of-Experts (MoE) variants" in Section 3.1 but selected a shared-bottom model as the production baseline, leaving the potential performance gains of MoE specifically for this joint ads–promotions setup unquantified.
  - Why unresolved: The paper does not provide ablation results comparing the final shared-bottom architecture against MoE or PLE (Progressive Layered Extraction) architectures for the specific task of unifying ads and promotions.
  - What evidence would resolve it: Offline Average Precision comparisons and online A/B tests contrasting the current shared-bottom model against an MoE-based implementation for the unified task.

- Question: Does the "adaptive loss masking" technique provide superior performance compared to standard gradient manipulation methods for negative transfer?
  - Basis in paper: [inferred] Section 2 references "gradient surgery (PCGrad)" and "GradNorm" as industry-ready approaches to mitigate negative transfer, but Section 3.3 details a custom "adaptive loss masking" strategy instead.
  - Why unresolved: It is unclear if the directional transfer enabled by masking is strictly necessary or if general-purpose gradient balancing methods would achieve equivalent or better synergy without hard-coding source-target relationships.
  - What evidence would resolve it: A comparative ablation study replacing the binary loss mask with gradient normalization or conflict-resolving surgery techniques.

## Limitations

- The paper does not provide full architectural details (encoder/tower depths, hidden sizes, embedding dimensions) making exact replication difficult
- Only one A/B test configuration is reported without results across multiple runs or targeting scenarios
- The benefit of directional masking vs. bidirectional transfer is hypothesized but not directly tested through ablation
- Offline metrics are AP per task, but no calibration curves or bias-variance analysis are shown

## Confidence

- High for the claim that shared encoder + directional masking improves cold-start targeting for both ads and promotions (strong offline AP gains, supported online)
- Medium for the claim that source-balanced sampling is necessary to avoid mode collapse (only one batch ratio tested, but offline gains are consistent)
- Low for the generality of directional transfer benefits across all domains (no ablation or comparison to bidirectional training is provided)

## Next Checks

1. Ablate directional masking: Train with and without the promotion→ad-only gradient flow and compare AP per task
2. Sweep source balance ratios: Test 25/75, 50/50, and 75/25 promo/ad splits to confirm optimal balance
3. Tier-stratified evaluation: Report AP and online metrics per stream tier to verify cold-start gains and detect regression on high-volume items