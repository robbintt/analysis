---
ver: rpa2
title: 'Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning
  in Multilingual LLMs'
arxiv_id: '2510.23949'
source_url: https://arxiv.org/abs/2510.23949
tags:
- language
- unlearning
- multilingual
- name
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a critical flaw in English-only machine\
  \ unlearning for multilingual LLMs: when fine-tuned on parallel multilingual PII\
  \ and then unlearned using only English data, models exhibit severe language confusion\u2014\
  responding in a language different from the query language. This invalidates reference-based\
  \ evaluation metrics like EM and KM, which fail to detect retained knowledge."
---

# Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs

## Quick Facts
- arXiv ID: 2510.23949
- Source URL: https://arxiv.org/abs/2510.23949
- Reference count: 40
- This paper identifies a critical flaw in English-only machine unlearning for multilingual LLMs: when fine-tuned on parallel multilingual PII and then unlearned using only English data, models exhibit severe language confusion—responding in a language different from the query language.

## Executive Summary
This paper reveals a critical vulnerability in multilingual large language models (LLMs) during machine unlearning: English-only unlearning can induce severe language confusion, causing models to respond in unexpected languages while still retaining sensitive information. This phenomenon invalidates standard reference-based evaluation metrics like ROUGE-L and exact match, which fail to detect retained knowledge when responses are in the wrong language. The authors introduce N-Mix, an n-gram-based metric for detecting language confusion, and propose semantic-based evaluation using LLM judges. Experiments on Llama 2 and Qwen2 demonstrate that English-only unlearning can produce false negatives, with models appearing to forget when they actually retain information in other languages. Incorporating multilingual data during unlearning mitigates confusion but may not be feasible in real-world scenarios where the specific languages of memorization are unknown.

## Method Summary
The authors generate a synthetic parallel multilingual PII dataset (40 profiles × 7 attributes × 5 languages) using Faker and GPT-4o translation. They fine-tune Qwen2-7B-Instruct and Llama2-7B-chat-hf on this data until near-perfect memorization (EM/KM ≈ 1.0). Unlearning is performed using Gradient Ascent (GA) or Gradient Difference (GD) on English-only forget sets, with α=1.0. The study evaluates three metrics: N-Mix (n-gram language mismatch detection), reference-based KM/EM via ROUGE-L, and semantic-based evaluation using ChatGPT as an LLM judge. The authors compare English-only unlearning against multilingual unlearning to assess mitigation effectiveness.

## Key Results
- English-only unlearning induces severe language confusion (N-Mix scores up to 100) in multilingual LLMs
- Reference-based metrics (KM/EM) produce false negatives when language confusion is high, incorrectly judging models as unlearned
- Qwen2 confuses to Chinese (highest CKA to English), Llama 3.1 confuses to Spanish (highest CKA to English)
- Incorporating multilingual data during unlearning mitigates confusion but requires parallel data that may not be available
- Semantic evaluation via LLM judges provides robust assessment when language confusion is present

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English-only gradient ascent on multilingual LLMs induces cross-lingual token substitution rather than true forgetting.
- Mechanism: When gradient ascent suppresses probability of target English tokens, models with strong cross-lingual alignment can satisfy the unlearning objective by reallocating probability mass to semantically equivalent tokens in another language instead of genuinely erasing the knowledge. The optimization finds an "escape route" through the multilingual embedding space.
- Core assumption: The unlearning loss does not distinguish between semantic suppression and surface-language suppression.
- Evidence anchors:
  - [abstract] "Unlearning multilingual knowledge using only English data induces language confusion in multilingual LLMs, where models respond in a language different from the input query."
  - [section 6.2] "Since GA unlearning with English-only dataset suppresses the probability of target English tokens, the model can satisfy the objective by switching to semantically equivalent tokens in another language (e.g., outputting Chinese instead of English)."
  - [corpus] Weak direct evidence on mechanism; neighbor paper "Evaluating Cross-Lingual Unlearning in Multilingual Language Models" (arXiv:2601.06675) corroborates cross-lingual unlearning failures but does not address the confusion mechanism directly.

### Mechanism 2
- Claim: Reference-based metrics produce false negatives when language confusion is high because they match surface form, not semantic content.
- Mechanism: Metrics like KM (ROUGE-L) and EM compare generated text to language-matched ground truth. When the model outputs correct semantic content in an unexpected language, the surface mismatch yields near-zero scores even though the information remains accessible. This masks unlearning failure.
- Core assumption: Evaluation references are curated to match the query language; no cross-lingual comparison is attempted.
- Evidence anchors:
  - [abstract] "This confusion causes standard reference-based metrics like KM and EM to fail, producing false negatives by incorrectly judging the model as unlearned when it still retains the information in another language."
  - [section 5.2, Figure 3] Example shows an English query eliciting a Chinese answer with correct semantic content, but reference comparison yields zero.
  - [corpus] No direct corpus support for this evaluation-failure mechanism in neighbors.

### Mechanism 3
- Claim: The direction of language confusion correlates with cross-lingual representation similarity in the pretrained model.
- Mechanism: Models preferentially confuse to languages with higher CKA similarity to English in their representations. Qwen2 confuses to Chinese (highest CKA to English); Llama 3.1 confuses to Spanish (highest CKA to English in its supported set). The substitution path of least resistance follows the strongest cross-lingual coupling.
- Core assumption: CKA measured on early-layer embeddings reflects usable cross-lingual token-level substitution pathways during generation.
- Evidence anchors:
  - [section 6.2, Table 6, Table 7] Qwen2 CKA scores: ZH=0.90, DE=0.87, KO=0.84, RU=0.84; Llama 3.1: ES=0.94, DE=0.93, HI=0.84, TH=0.82.
  - [appendix B] "Qwen2 perceives Chinese as the closest language to English, while Llama 3.1 perceives Spanish as the closest."

## Foundational Learning

- Concept: Gradient Ascent Unlearning
  - Why needed here: The paper uses GA and Gradient Difference as unlearning objectives; understanding how ascent suppresses token probabilities is essential to grasp why cross-lingual substitution occurs.
  - Quick check question: Why would maximizing loss on target outputs sometimes fail to erase underlying knowledge?

- Concept: Reference-Based vs Semantic Evaluation
  - Why needed here: The core contribution is that reference-based metrics fail under confusion; semantic evaluation via LLM judges is proposed as a replacement.
  - Quick check question: If a model outputs "le chat noir" for an English query expecting "the black cat," would ROUGE-L capture semantic correctness?

- Concept: Cross-Lingual Representation Alignment (CKA)
  - Why needed here: The paper uses CKA to explain confusion direction; understanding representational similarity clarifies why multilingual LLMs are more vulnerable.
  - Quick check question: Does high CKA between two languages guarantee better translation, or merely similar internal representations?

## Architecture Onboarding

- Component map:
  Multilingual PII Dataset (1,400 QA pairs) -> Fine-Tuning (until EM/KM ≈ 1.0) -> Unlearning (GA/GD on English-only or multilingual sets) -> Evaluation (N-Mix, KM/EM, Semantic)

- Critical path:
  1. Generate parallel multilingual PII QA pairs (40 profiles × 7 attributes × 5 languages)
  2. Fine-tune model on D until EM/KM ≈ 1.0 (near-perfect memorization)
  3. Unlearn on D_f^en only (English-only) or multilingual D_f and D_r
  4. Measure N-Mix score (n-gram language detection vs query language)
  5. If N-Mix high, discard KM/EM; use semantic evaluation via LLM judges
  6. If confusion severe, re-run unlearning with multilingual D_f and D_r

- Design tradeoffs:
  - English-only unlearning: Simpler, cheaper, but risks false negatives and latent knowledge retention in multilingual models
  - Multilingual unlearning: Mitigates confusion, but requires parallel data and more compute; may not be feasible if target languages are unknown
  - LLM-judge evaluation: Robust to language switches, but introduces judge model dependency and potential prompt sensitivity

- Failure signatures:
  - Near-zero KM/EM on all non-English languages with near-zero retain scores → suspect language confusion rather than true forgetting
  - N-Mix > 80 on most query languages → reference metrics invalid; switch to semantic evaluation
  - Model consistently outputs in a single non-query language (e.g., always Chinese for Qwen2, Spanish for Llama 3.1) → cross-lingual substitution active

- First 3 experiments:
  1. Baseline sanity check: Fine-tune and unlearn both on D^en only (no multilingual data). Confirm N-Mix ≈ 0 and KM/EM behave as expected (Table 8).
  2. Confusion trigger: Fine-tune on full D, unlearn on D_f^en only. Measure N-Mix per language and compare KM/EM vs semantic evaluation on Qwen2 and Llama 2 (Tables 1, 2