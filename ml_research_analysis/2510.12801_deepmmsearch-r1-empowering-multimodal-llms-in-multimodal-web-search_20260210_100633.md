---
ver: rpa2
title: 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search'
arxiv_id: '2510.12801'
source_url: https://arxiv.org/abs/2510.12801
tags:
- search
- image
- information
- answer
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepMMSearch-R1 is a multimodal LLM that performs on-demand, multi-turn
  web searches for knowledge-intensive visual question answering. It dynamically crafts
  queries for both image and text search tools, using cropped image search to focus
  on relevant regions and iterative text search refinement for self-correction.
---

# DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search

## Quick Facts
- **arXiv ID**: 2510.12801
- **Source URL**: https://arxiv.org/abs/2510.12801
- **Reference count**: 29
- **Primary result**: Achieves state-of-the-art performance on knowledge-intensive visual question answering using on-demand multi-turn web search

## Executive Summary
DeepMMSearch-R1 is a multimodal LLM that performs on-demand, multi-turn web searches for knowledge-intensive visual question answering. It dynamically crafts queries for both image and text search tools, using cropped image search to focus on relevant regions and iterative text search refinement for self-correction. Trained on a novel DeepMMSearchVQA dataset with structured tool annotations, it achieves state-of-the-art performance, surpassing RAG workflows and prompt-based agents by +21.13% and +8.89% respectively, while maintaining general VQA capabilities.

## Method Summary
DeepMMSearch-R1 uses a two-stage training pipeline: supervised fine-tuning (SFT) on DeepMMSearchVQA data followed by online GRPO reinforcement learning. The model generates structured tool calls with tags like `<text_search>`, `<img_search>`, and `<reason>` to perform iterative web searches. Cropped image search focuses on relevant visual regions using Grounding DINO, while iterative text search refinement enables self-correction. The RL stage optimizes for correctness and format compliance while preserving general capabilities through a KL penalty.

## Key Results
- Achieves state-of-the-art performance on knowledge-intensive VQA tasks
- Surpasses RAG workflows by +21.13% and prompt-based agents by +8.89%
- Maintains general VQA capabilities on OCRBench, MMVet, and MM-Vet-Coherence benchmarks
- 50:50 ratio of search-required to search-free data found optimal during training

## Why This Works (Mechanism)

### Mechanism 1: Cropped Image Search
- Claim: Cropped image search improves retrieval relevance by reducing background noise when queries target specific visual entities rather than whole scenes.
- Core assumption: Background elements and unrelated entities introduce noise into image-search retrieval, and focusing on the question-relevant region mitigates this.
- Evidence: DeepMMSearch-R1 generates referring expressions, uses Grounding DINO to crop the corresponding region, and passes the crop to image search API instead of the full image.
- Break condition: If referring expressions are ambiguous or Grounding DINO produces inaccurate bounding boxes, cropped search may retrieve irrelevant or misleading results.

### Mechanism 2: Iterative Text Search Refinement
- Claim: Iterative text-search refinement with self-reflection enables better navigation of noisy web content than single-shot retrieval.
- Core assumption: Web information is noisy and often incomplete on first retrieval; multi-hop reasoning across multiple queries yields more complete answers.
- Evidence: Model reasons about remaining gaps after initial search and issues refined queries (e.g., from "speed of egret" to "highest recorded speed of egret").
- Break condition: If reward model penalizes multiple tool calls too strongly or SFT distribution lacks sufficient multi-turn examples, model may converge to overly conservative single-shot retrieval.

### Mechanism 3: Two-Stage Training (SFT → GRPO)
- Claim: Two-stage training separates capability acquisition from behavioral refinement, yielding efficient tool use while preserving general capabilities.
- Core assumption: SFT provides stable starting distribution that prevents RL from collapsing into low-reward regions; KL penalty preserves pre-trained capabilities.
- Evidence: RL checkpoint maintains performance on general VQA benchmarks relative to base Qwen2.5-VL-7B-Instruct.
- Break condition: If KL penalty is too weak, RL may cause catastrophic drift; if too strong, model may not sufficiently improve tool-use efficiency.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF) and GRPO**: Why needed - Second training stage uses GRPO, a variant of PPO, to optimize tool-use behavior. Quick check - How does GRPO's group-relative advantage computation differ from PPO's advantage estimate?

- **Vision-Language Grounding**: Why needed - Model must generate referring expressions and interface with grounding model (Grounding DINO) to crop image regions. Quick check - What types of referring expressions does Grounding DINO support, and how might ambiguous expressions degrade search performance?

- **Tool-Augmented LLMs and Structured Output Schemas**: Why needed - DeepMMSearch-R1 uses specific tag schema (<text_search>, <img_search>, <answer>, etc.). Quick check - Why are web-retrieved information tokens masked during SFT loss computation?

## Architecture Onboarding

- **Component map**: Base model (Qwen2.5-VL-7B-Instruct) -> Tools (text search API, image search API, Grounding DINO) -> Summarization module (gpt-5-chat-latest) -> Training pipeline (SFT → GRPO)

- **Critical path**: 1) Generate multi-turn VQA data with structured tool-call annotations (DeepMMSearchVQA) 2) Run SFT on LLM with LoRA (freeze vision encoder/projection) 3) Generate rollouts with SFT model interacting with three tools 4) Run online GRPO with composite reward (correctness + format adherence)

- **Design tradeoffs**: Search-required vs. search-free data balance (50:50 optimal), LoRA vs. full finetuning (preserves general capabilities but may limit adaptation), KL penalty strength (0.001 used), number of rollouts (8 per iteration used)

- **Failure signatures**: Excessive search (model invokes tools for non-knowledge-intensive questions), premature answering (model outputs <answer> before sufficient information), cropping errors (Grounding DINO fails to locate referred entity), reward hacking (model optimizes format score at expense of correctness), KL collapse (RL deviates too far from SFT policy)

- **First 3 experiments**: 1) Validate SFT model on held-out examples for proper tool calls and formatting 2) Ablate cropped vs. whole-image search for retrieval relevance and accuracy 3) Analyze RL tool-use behavior comparing tool-call statistics between SFT and RL checkpoints

## Open Questions the Paper Calls Out

- **Expanding tool diversity**: Future works may explore expanding tool diversity beyond text and image search tools to include code execution or navigation APIs, which could impact reasoning capabilities for tasks requiring calculations or spatial navigation.

- **Multilingual scaling**: The paper suggests future work includes scaling training to broader multilingual domains, as the current DeepMMSearchVQA dataset and FVQA data are primarily English-centric, leaving cross-lingual transfer capabilities unverified.

- **Long-context reasoning**: The current implementation caps context window at 8192 tokens, identifying long-context reasoning as an area for future exploration, particularly for multi-hop reasoning tasks requiring extensive evidence aggregation.

## Limitations

- Evaluation relies heavily on LLM-as-a-judge scoring, which may not fully capture human judgment of answer quality for subjective or nuanced responses
- Effectiveness of cropped image search depends critically on Grounding DINO's accuracy, with no error analysis provided for failed crops
- RL training requires expensive multiple tool calls per rollout (8 used), creating significant computational overhead that may limit practical deployment

## Confidence

- **High confidence**: Claims about SOTA performance relative to baselines (RAG and prompt-based agents) are well-supported by quantitative comparisons in Table 2
- **Medium confidence**: Mechanism of iterative text search refinement is demonstrated through examples but lacks systematic ablation studies comparing single vs. multi-turn retrieval effectiveness
- **Medium confidence**: Two-stage training pipeline's separation of capability acquisition from behavioral refinement is theoretically sound but could benefit from more detailed analysis of RL optimization dynamics

## Next Checks

1. **Ablate cropping accuracy**: Measure correlation between Grounding DINO's referring expression quality and final answer accuracy to quantify impact of cropping errors on retrieval performance

2. **Single vs. multi-turn search comparison**: Implement controlled experiment comparing DeepMMSearch-R1's iterative refinement against single-shot retrieval baseline using same base model and search tools

3. **Generalization stress test**: Evaluate model on out-of-domain visual questions requiring search to assess whether RL optimization overfits to FVQA training distribution