---
ver: rpa2
title: 'Probing then Editing: A Push-Pull Framework for Retain-Free Machine Unlearning
  in Industrial IoT'
arxiv_id: '2511.09414'
source_url: https://arxiv.org/abs/2511.09414
tags:
- unlearning
- data
- class
- knowledge
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a retain-free machine unlearning framework
  called Probing then Editing (PTE) for dynamic Industrial IoT environments where
  models need to selectively forget outdated or erroneous knowledge. Unlike existing
  methods that require access to retained data, PTE operates using only the original
  model and the data to be forgotten.
---

# Probing then Editing: A Push-Pull Framework for Retain-Free Machine Unlearning in Industrial IoT

## Quick Facts
- arXiv ID: 2511.09414
- Source URL: https://arxiv.org/abs/2511.09414
- Reference count: 10
- Primary result: Probing then Editing (PTE) achieves complete forgetting (Accft=0) while maintaining high retention performance (Accrt=94.11%) and harmonic mean scores approaching full retraining (96.81), outperforming existing baselines across multiple benchmarks.

## Executive Summary
This paper introduces a retain-free machine unlearning framework called Probing then Editing (PTE) designed for dynamic Industrial IoT environments where models must selectively forget outdated or erroneous knowledge without accessing retained data. The method operates through a two-phase "probe-edit" process: first probing decision boundary neighborhoods of the forget class via gradient ascent to generate self-consistent editing instructions, then performing push-pull collaborative optimization. The push branch dismantles the forget class using the generated instructions while the pull branch anchors retained class knowledge via masked knowledge distillation. Experimental results demonstrate complete forgetting while preserving utility across multiple benchmarks including CWRU and SCUT-FD, with harmonic mean scores approaching full retraining performance.

## Method Summary
PTE operates in two phases to achieve class-level unlearning without retain data. Phase 1 uses one-shot probing via projected gradient ascent to generate editing instructions - for each forget sample, the model computes perturbations that maximize loss, probes the decision boundary, and uses its own prediction at that point as a self-consistent relabeling target. Phase 2 implements alternating push-pull optimization: the push branch treats these edit instructions as synthetic training data and minimizes cross-entropy loss to actively collapse the forget class's decision region, while the pull branch applies masked knowledge distillation to anchor retained classes by zeroing forget-class probabilities and preserving relative retained-class ratios from the original model's knowledge.

## Key Results
- Achieves complete forgetting (Accft=0) while maintaining high retention accuracy (Accrt=94.11% on CIFAR-10)
- Harmonic mean scores approach full retraining performance (96.81 vs 96.86)
- Outperforms baselines across multiple benchmarks including CWRU and SCUT-FD
- Demonstrates robustness across single-class and multi-class unlearning scenarios (up to 10 classes)

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent Probing for Self-Supervised Edit Instruction Generation
The probing phase systematically discovers decision boundary neighborhoods through gradient-based perturbations, generating semantically coherent relabeling targets. Projected gradient ascent computes noise matrices that maximize loss on forget samples, pushing inputs toward decision boundaries. The model's own prediction at this probed point provides a self-consistent label, avoiding random supervision signals. Core assumption: the model's boundary prediction reflects meaningful alternative classes. Break condition: if probed predictions are unstable or highly sensitive to ε, edit instructions become inconsistent.

### Mechanism 2: Push Branch - Active Decision Region Collapse via Cross-Entropy Relabeling
The push branch forces confident misclassification of forget samples to systematically dismantle the target class's discriminative structure. By treating edit instructions as synthetic training data and minimizing cross-entropy loss, it actively reconfigures the decision boundary to absorb forget-class samples into alternative classes. Core assumption: relabeling forget samples to alternative classes doesn't severely corrupt shared representations needed for retained classes. Break condition: if learning rate is too high relative to pull branch, catastrophic interference with retained classes occurs.

### Mechanism 3: Pull Branch - Masked Knowledge Distillation for Utility Anchoring
The pull branch zeros forget-class probabilities while preserving relative retained-class ratios via masked distillation, anchoring utility without accessing retain data. For each forget sample, it computes original model logits, masks the forget-class probability to zero, renormalizes, then minimizes KL divergence. Core assumption: forget samples carry sufficient signal about the model's behavior on retained classes to enable effective distillation. Break condition: if forget-class probability dominates the softmax distribution, masking leaves near-uniform retained probabilities, providing weak anchoring.

## Foundational Learning

- **Gradient-based adversarial perturbations (FGSM/PGD)**
  - Why needed: The probing phase is essentially adversarial example generation - maximizing loss via bounded perturbations. Understanding this is critical for the edit instruction generation.
  - Quick check: Given a classifier f(x) and input x with true label y, how would you compute a perturbation δ that maximizes L(f(x+δ), y) subject to ||δ||∞ ≤ ε?

- **Knowledge distillation with temperature scaling**
  - Why needed: The pull branch uses teacher-student distillation with temperature T to soften probability distributions. The T² scaling factor maintains gradient magnitude - essential for understanding hyperparameter tuning.
  - Quick check: Why does increasing temperature T in softmax produce "softer" probability distributions, and why does the KL divergence gradient scale as T²?

- **Decision boundary geometry in deep networks**
  - Why needed: The entire framework conceptualizes unlearning as boundary manipulation - probing neighborhoods and anchoring non-target regions. The t-SNE visualizations show this explicitly: successful unlearning collapses the forget-class cluster while preserving others.
  - Quick check: In a K-class classifier, what happens to the decision boundary of class k when all samples labeled k are relabeled to class j during training?

## Architecture Onboarding

- **Component map:**
Phase 1 (One-shot Probing): Original Model f_w0 (FROZEN) → Forget Data D_f → Gradient Ascent → Probed Samples x_probe → Self-Relabeling → Edit Instructions D_E

Phase 2 (Push-Pull Optimization, alternating per epoch): Push Branch: D_E → Cross-Entropy Loss → η_push gradient step; Pull Branch: D_f → Original Model → Mask + Normalize → KL Divergence Loss → η_pull gradient step

- **Critical path:** The probing phase must generate valid edit instructions (y_edit ≠ y). If probing fails (e.g., y_edit = y for most samples), the push branch has no effective training signal. Check: after Phase 1, verify |D_E| > 0.5|D_f| as a sanity check.

- **Design tradeoffs:**
  - One-shot vs. iterative probing: Paper chooses one-shot (efficiency, scalability). Alternative: re-probe after each epoch - more accurate but O(T×E) computation.
  - Alternating vs. sequential push-pull: Ablation shows alternating achieves highest H-Mean (75.23). The interleaved optimization appears essential for stable trade-offs.
  - Learning rate asymmetry: Paper mentions η_push is "relatively high" vs. η_pull. Assumption: aggressive forgetting (push) must be counterbalanced by conservative anchoring (pull).

- **Failure signatures:**
  - Incomplete forgetting (Acc_f > 0, Acc_ft > 0): Push branch underpowered - increase η_push or probing radius ε.
  - Retention collapse (Acc_r drops sharply): Pull branch underpowered or push too aggressive - decrease η_push/η_pull ratio, or increase distillation temperature T.
  - Multi-class scaling failure: Table 3 shows baselines degrade at 5+ classes. PTE maintains Acc_ft=0 but requires monitoring H-Mean.
  - Instability across runs: Figure 2-3 show high variance in baselines. PTE shows low variance - alternating optimization provides implicit regularization.

- **First 3 experiments:**
  1. Single-class sanity check on CIFAR-10: Reproduce Table 1 results (PTE: Acc_f=0, Acc_rt=94.11). Vary ε ∈ {0.01, 0.05, 0.1} and T ∈ {1, 2, 4} to establish baseline hyperparameters.
  2. Ablation of push-pull scheduling: Implement "Push then Pull" vs. "Pull then Push" vs. alternating. Confirm alternating achieves highest H-Mean.
  3. Multi-class stress test: Run 5-class and 10-class unlearning on CIFAR-100. Monitor whether Acc_ft remains 0 and H-Mean stays competitive with retraining.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the PTE framework be effectively adapted for sample-level unlearning instances rather than just class-level removal?
- Basis in paper: The authors explicitly define the problem as "Class-Level Machine Unlearning" (Definition 1) and assume |C_f|=1 for simplicity.
- Why unresolved: The method relies on "masked knowledge distillation" which zeroes out the probability of a specific class. It is unclear how this mechanism would function when only specific samples (which still belong to valid classes) must be forgotten without erasing the entire class concept.
- What evidence would resolve it: A demonstration of PTE unlearning random subsets of data points within a class while maintaining high accuracy on the remaining points of that same class.

### Open Question 2
- Question: How can the optimal probing radius ε be determined in a retain-free environment where no validation data is available to detect performance collapse?
- Basis in paper: The probing phase (Eq. 3) requires a perturbation budget ε, and the experimental constraints forbid access to retained data.
- Why unresolved: Selecting ε involves a trade-off: a value too small results in incomplete boundary probing, while a value too large may disrupt shared features. Without a validation set, tuning this hyperparameter is currently an unsupported assumption.
- What evidence would resolve it: A sensitivity analysis showing consistent performance across a wide range of ε values, or an adaptive mechanism for ε that does not require validation feedback.

### Open Question 3
- Question: Does the "Pull" branch's reliance on the original model's predictions on the forget set effectively preserve utility when the forget set is unrepresentative of the retain set?
- Basis in paper: The Pull branch (Eq. 10) uses the original model's logits on the forget data D_f to anchor retained knowledge via distillation.
- Why unresolved: This assumes the model's behavior on D_f regarding non-target classes is representative of its behavior on retained data. If D_f is biased or distinct, anchoring to these predictions may fail to preserve the decision boundaries needed for the actual retained data.
- What evidence would resolve it: Experiments utilizing out-of-distribution or highly biased forget sets, verifying if the retention accuracy (Acc_rt) remains stable compared to the retrained model.

## Limitations
- Hyperparameter sensitivity: Key hyperparameters (ε, T, η_push/η_pull ratios) are not specified, making reproduction challenging without extensive tuning.
- Multi-class scalability: While maintaining complete forgetting at 5-10 classes, the mechanism assumes class-wise independence and may not fully characterize interference patterns in shared representations.
- Task specificity: The framework targets classification unlearning and would require fundamental architectural modifications for structured prediction or generative models.

## Confidence
- High confidence: Complete forgetting mechanism (Acc_f=0, Acc_ft=0) and MIA ≈ 0 across benchmarks - these are directly observable metrics with low variance.
- Medium confidence: Retention performance claims (Acc_rt ≈ 94% vs. retraining 96%) - valid but sensitive to hyperparameter tuning and data distribution.
- Low confidence: Generalization to real-world Industrial IoT scenarios - the benchmarks are controlled and may not capture sensor drift, adversarial data poisoning, or concept drift common in industrial deployments.

## Next Checks
1. **Hyperparameter ablation study:** Systematically vary ε ∈ {0.01, 0.05, 0.1} and T ∈ {1, 2, 4} on CIFAR-10 single-class unlearning. Plot H-Mean vs. hyperparameters to identify stable operating regions and quantify sensitivity.
2. **Cross-model architecture transfer:** Apply PTE to VGG-16 and EfficientNet-B0 on CIFAR-10. Compare whether the same ε and T values work across architectures or require model-specific tuning.
3. **Industrial scenario stress test:** Apply PTE to CWRU bearing fault diagnosis with corrupted forget data (10% label noise). Measure forgetting/retention trade-off degradation to validate robustness against real-world data quality issues.