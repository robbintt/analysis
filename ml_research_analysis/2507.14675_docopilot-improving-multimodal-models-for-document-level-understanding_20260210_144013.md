---
ver: rpa2
title: 'Docopilot: Improving Multimodal Models for Document-Level Understanding'
arxiv_id: '2507.14675'
source_url: https://arxiv.org/abs/2507.14675
tags:
- arxiv
- document
- multimodal
- data
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Doc-750K, a large-scale dataset of 758K multimodal
  document question-answer pairs designed to improve multi-page document understanding.
  The dataset includes diverse document structures, cross-page dependencies, and real
  QA pairs, with 31.6% derived directly from documents.
---

# Docopilot: Improving Multimodal Models for Document-Level Understanding

## Quick Facts
- **arXiv ID**: 2507.14675
- **Source URL**: https://arxiv.org/abs/2507.14675
- **Reference count**: 40
- **Primary result**: Retrieval-free Docopilot achieves +19.9% accuracy on MM-NIAH vs. InternVL2-8B + RAG with 81ms latency

## Executive Summary
This work introduces Doc-750K, a large-scale dataset of 758K multimodal document question-answer pairs designed to improve multi-page document understanding. The dataset includes diverse document structures, cross-page dependencies, and real QA pairs, with 31.6% derived directly from documents. Based on this data, the authors develop Docopilot, a retrieval-free multimodal model that processes long-context documents through engineering optimizations such as multimodal data packing, Ring Attention, and Liger Kernel. Docopilot achieves strong improvements over prior models, notably a +19.9% accuracy gain on MM-NIAH compared to InternVL2-8B, and matches or exceeds larger models with fewer parameters and lower latency. The approach sets a new baseline for document-level multimodal understanding.

## Method Summary
Docopilot builds on the InternVL2 architecture (InternViT-300M vision encoder + InternLM2.1/2.5 language model) and is trained on Doc-750K, a dataset of 758K QA pairs from academic documents (Arxiv, OpenReview, Sci-Hub). The model processes documents in two formats: interleaved text-image (via MinerU extractor) and multi-image (page-rendered). Training employs multimodal data packing to maximize GPU utilization, Ring Attention for distributed long-context processing, and Liger Kernel for memory-efficient operations. The model is fine-tuned for 1 epoch with specific hyperparameters (lr=1e-5, batch size 128, max 32K tokens, max 24 image tiles) and achieves retrieval-free multi-page understanding through native long-context attention.

## Key Results
- **MM-NIAH**: Docopilot-8B achieves 61.8 accuracy (+19.9% over InternVL2-8B + RAG)
- **Latency**: 81ms vs. 113ms for InternVL2-8B + RAG (130% faster)
- **MP-DocVQA**: 75.4 ANLS, surpassing InternVL2-8B + RAG by +3.3
- **Cross-page performance**: Maintains strong results across Short/Medium/Long categories on MM-NIAH

## Why This Works (Mechanism)

### Mechanism 1
High-quality, document-level training data with real QA pairs enables cross-page dependency learning that retrieval-based methods cannot achieve. Doc-750K contains 31.6% real QA pairs directly extracted from documents, providing grounded supervision signals for multi-page reasoning. The dataset includes 758K QA pairs averaging 11,245 text tokens and 6,178 image tokens per conversation, forcing the model to learn long-range attention patterns rather than relying on fragmented retrieval.

### Mechanism 2
Ring Attention combined with Liger Kernel enables training on 32K-token multimodal contexts within practical GPU memory limits. Ring Attention partitions long sequences across devices, distributing KV-block computation while overlapping communication. Liger Kernel reduces memory through kernel fusion, in-place operations, and input chunking. Together they allow the model to process full documents (up to 24 image tiles, 32K tokens) without truncation or chunking that would break cross-page dependencies.

### Mechanism 3
Retrieval-free native MLLMs avoid multi-stage error accumulation inherent in RAG pipelines while reducing inference latency. RAG systems introduce errors through fragmented retrieval contexts losing document structure, incorrect retrieval propagating to generation, and extra retrieval time. Docopilot processes full documents directly, maintaining semantic coherence. Experiments show Docopilot-8B achieves 61.8 MM-NIAH accuracy (+19.9 over InternVL2-8B) with 81ms latency versus 113ms for InternVL2-8B + RAG.

## Foundational Learning

- **Ring Attention for distributed long-context processing**
  - Why needed here: Core enabling technology for 32K-token training without OOM errors
  - Quick check question: Can you explain how Ring Attention differs from standard Flash Attention in terms of memory scaling across devices?

- **Vision-Language Model architecture (ViT-MLP-LLM)**
  - Why needed here: Docopilot builds on InternVL2's ViT-300M + InternLM2.5-7B backbone; understanding token flow is essential for debugging
  - Quick check question: How are image patches converted to soft prompts for the LLM, and what is the role of the MLP projector?

- **RAG limitations in document understanding**
  - Why needed here: Motivates the entire retrieval-free approach; understanding failure modes helps evaluate when Docopilot is appropriate
  - Quick check question: Name three specific failure modes of RAG systems on multi-page documents that Docopilot aims to address

## Architecture Onboarding

- **Component map**:
  InternViT-300M (448 tile resolution, max 24 tiles) -> 2-layer MLP projector -> InternLM2-1.8B/InternLM2.5-7B -> Docopilot model

- **Critical path**:
  1. Document -> MinerU extraction -> interleaved text-image OR multi-image format
  2. QA construction via GPT-4o or structural tasks (abstract/title/caption writing)
  3. Multimodal packing algorithm combines samples up to Ti=48 images, Tt=32K tokens
  4. Fine-tuning on packed data for 1 epoch (lr=1e-5, cosine schedule)
  5. Inference with direct document input, no retrieval

- **Design tradeoffs**:
  - Interleaved format preserves text accuracy but loses layout; multi-image preserves layout but relies on model OCR. Both formats used for robustness.
  - Academic document focus (Arxiv, Sci-Hub, OpenReview) limits domain diversity—may not generalize to financial, legal, or medical documents without additional data.
  - 32K token limit excludes very long documents; RAG may still be needed for >64K contexts.

- **Failure signatures**:
  - Model fails on documents with heavy tables/figures not in training distribution -> check image tile coverage
  - Cross-page reasoning breaks on documents >20 pages -> attention dilution, consider retrieval hybrid
  - Latency spikes on multi-turn conversations -> context accumulation exceeds 32K, need context management

- **First 3 experiments**:
  1. Baseline comparison: Evaluate Docopilot-8B vs InternVL2-8B + RAG on your domain-specific multi-page documents (not just academic benchmarks) to validate retrieval-free claim.
  2. Ablation on data sources: Train with/without each Doc-750K component (Sci-Hub, Arxiv, OpenReview) to measure contribution to your target task.
  3. Context length stress test: Test on documents at 16K, 32K, 48K tokens to identify attention degradation point and determine practical context limits for your use case.

## Open Questions the Paper Calls Out

- **Domain generalization**: The paper explicitly notes that Doc-750K's current domain restriction to academic documents limits generalizability, and plans to expand coverage to broader document types.

- **Scalability limits**: While the paper evaluates up to 64K tokens, it doesn't test beyond this limit or explore the trade-offs between native long-context processing and hybrid RAG approaches for extremely long documents.

- **LLM-generated data quality**: The paper notes that 4.8% of data is LLM-generated but doesn't analyze potential biases or hallucination patterns that synthetic data might introduce.

## Limitations

- **Domain specificity**: Doc-750K focuses exclusively on academic documents, potentially limiting generalization to legal, medical, or financial domains with different structures and terminology.

- **Context length constraints**: The 32K token limit may be insufficient for extremely long documents, potentially requiring retrieval methods for contexts exceeding 64K tokens.

- **Limited empirical validation**: Ring Attention + Liger Kernel integration lacks direct evidence through ablation studies or training curve analysis.

## Confidence

- **High confidence**: Retrieval-free performance gains on MM-NIAH (+19.9% over InternVL2-8B + RAG) are well-supported by benchmark results and latency analysis. The Doc-750K dataset construction methodology is clearly specified with verifiable statistics.
- **Medium confidence**: Claims about Ring Attention + Liger Kernel enabling 32K-token training are theoretically grounded but lack direct evidence—the paper doesn't show training curves or memory utilization data.
- **Medium confidence**: Cross-page dependency learning from Doc-750K is plausible given the dataset composition (31.6% real QA pairs), but generalization to non-academic domains remains unproven.

## Next Checks

1. **Domain generalization test**: Evaluate Docopilot on multi-page documents from legal, financial, or medical domains to verify cross-domain performance beyond academic papers.

2. **Ring Attention contribution isolation**: Run ablation studies comparing Docopilot with and without Ring Attention/Liger Kernel integration to quantify their individual contributions to training efficiency and final performance.

3. **Context length scalability analysis**: Systematically test performance on documents at 16K, 32K, 48K, and 64K tokens to identify the precise attention degradation point and determine when retrieval methods become necessary.