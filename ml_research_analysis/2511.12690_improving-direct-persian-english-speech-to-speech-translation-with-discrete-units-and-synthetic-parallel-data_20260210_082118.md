---
ver: rpa2
title: Improving Direct Persian-English Speech-to-Speech Translation with Discrete
  Units and Synthetic Parallel Data
arxiv_id: '2511.12690'
source_url: https://arxiv.org/abs/2511.12690
tags:
- speech
- english
- s2st
- persian
- direct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of direct speech-to-speech translation
  (S2ST) for low-resource language pairs, specifically Persian-to-English, where large
  amounts of parallel speech data are scarce. The authors propose a model that combines
  self-supervised pretraining, discrete speech units, and synthetic parallel data
  generation to improve translation quality.
---

# Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data

## Quick Facts
- arXiv ID: 2511.12690
- Source URL: https://arxiv.org/abs/2511.12690
- Reference count: 15
- Key outcome: 4.6 ASR BLEU improvement on CVSS corpus using synthetic parallel data

## Executive Summary
This paper addresses the challenge of direct speech-to-speech translation (S2ST) for low-resource language pairs, specifically Persian-to-English, where large amounts of parallel speech data are scarce. The authors propose a model that combines self-supervised pretraining, discrete speech units, and synthetic parallel data generation to improve translation quality. Their approach uses a conformer-based encoder pretrained on Persian speech, a causal transformer decoder with relative positional attention to predict discrete English speech units, and a unit-based neural vocoder for waveform synthesis. To address data scarcity, they construct a synthetic Persian-English parallel speech corpus by translating Persian speech transcriptions using a large language model and synthesizing corresponding English speech with a zero-shot TTS system, increasing available parallel speech data by roughly six times.

## Method Summary
The proposed approach employs a conformer-based encoder pretrained on Persian speech through self-supervised learning, which processes the input speech and generates contextualized representations. A causal transformer decoder with relative positional attention then predicts discrete speech units representing the target English speech. These discrete units are subsequently converted to waveforms using a unit-based neural vocoder. The synthetic data generation pipeline involves translating Persian speech transcriptions using a large language model to create English text, then synthesizing corresponding English speech using a zero-shot TTS system to produce parallel speech pairs. This approach effectively multiplies the available parallel speech data by approximately six times, addressing the fundamental data scarcity problem in low-resource S2ST scenarios.

## Key Results
- Achieved 4.6 ASR BLEU improvement over direct baselines when trained with synthetic data
- Successfully increased parallel speech data availability by approximately six times through synthetic data generation
- Demonstrated effectiveness of combining self-supervised pretraining, discrete units, and synthetic parallel data for low-resource S2ST

## Why This Works (Mechanism)
The approach works by addressing three critical challenges in low-resource direct S2ST: data scarcity, representation learning, and end-to-end speech generation. Self-supervised pretraining on unlabeled Persian speech allows the model to learn rich acoustic representations without requiring parallel speech data. Discrete speech units provide a compact, discrete representation that bridges the continuous speech domain with the translation task, enabling more effective cross-lingual transfer. The synthetic parallel data generation pipeline leverages text translation and TTS technologies to create additional training examples, effectively bootstrapping the learning process. The conformer encoder captures both local and global speech patterns through its attention mechanism, while the causal transformer decoder with relative positional attention ensures proper temporal modeling for speech generation. Together, these components create a robust pipeline that can learn effective Persian-to-English S2ST even with limited natural parallel data.

## Foundational Learning

**Self-supervised pretraining**: Learning representations from unlabeled data without human annotations. Why needed: Persian speech lacks large parallel corpora for supervised pretraining. Quick check: Pretrained encoder should perform well on downstream Persian speech tasks.

**Discrete speech units**: Quantized representations of continuous speech signals. Why needed: Bridge gap between continuous speech and discrete translation outputs. Quick check: Unit sequences should preserve phonetic and prosodic information.

**Causal transformer decoder**: Transformer architecture with causal attention for autoregressive generation. Why needed: Generate speech units sequentially while maintaining temporal dependencies. Quick check: Generated units should form coherent, natural-sounding speech when decoded.

**Relative positional attention**: Attention mechanism using relative rather than absolute positional encodings. Why needed: Better capture local temporal patterns in speech sequences. Quick check: Model should maintain proper word order and timing in generated speech.

**Unit-based neural vocoder**: Neural network that converts discrete speech units back to waveforms. Why needed: Synthesize natural-sounding speech from discrete unit representations. Quick check: Output waveforms should be perceptually similar to natural speech.

## Architecture Onboarding

Component map: Persian speech -> Conformer encoder -> Discrete unit sequence -> Transformer decoder -> English discrete units -> Neural vocoder -> English speech

Critical path: The conformer encoder processes Persian input speech and generates contextualized representations, which are then fed to the causal transformer decoder that predicts discrete English speech units autoregressively. These predicted units are finally converted to waveforms by the neural vocoder. Each component must function correctly for successful S2ST.

Design tradeoffs: The use of discrete units reduces model complexity and improves cross-lingual transfer but introduces quantization errors. Self-supervised pretraining requires substantial unlabeled data but eliminates dependency on parallel corpora. The synthetic data generation pipeline is cost-effective but may introduce translation and synthesis artifacts.

Failure signatures: Poor Persian speech encoding manifests as garbled or incoherent English output. Incorrect discrete unit prediction results in unnatural prosody or word order errors. Neural vocoder failures produce distorted or robotic-sounding speech. Translation errors in synthetic data propagate through the entire pipeline.

First experiments: 1) Test conformer encoder performance on Persian speech recognition to verify pretraining effectiveness. 2) Evaluate discrete unit quality by reconstructing speech from quantized representations. 3) Assess synthetic data quality by comparing generated English speech to natural speech in terms of naturalness and intelligibility.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on large language model for synthetic data generation raises concerns about translation quality and potential biases
- Six-fold increase in synthetic data assumes natural prosody and timing patterns, which may not fully capture authentic speech characteristics
- Discrete unit approach introduces quantization errors that could accumulate through the translation pipeline
- Model performance evaluated only on CVSS corpus, limiting generalization assessment

## Confidence
- High in core methodology combining self-supervised pretraining, discrete units, and synthetic data generation
- Medium in specific architectural choices (conformer encoder, causal transformer decoder with relative positional attention) and their configuration
- Medium in quantitative results, particularly the 4.6 ASR BLEU improvement, as evaluated only on a single corpus

## Next Checks
1. Conduct domain adaptation experiments using speech data from different domains (news, conversations, technical) to assess model generalization beyond the CVSS corpus
2. Perform ablation studies isolating the contributions of synthetic data, discrete units, and pretraining to quantify their individual impact on translation quality
3. Evaluate the translated speech output using human evaluation metrics beyond ASR BLEU, including naturalness, prosody preservation, and semantic accuracy, to assess the quality of the generated English speech