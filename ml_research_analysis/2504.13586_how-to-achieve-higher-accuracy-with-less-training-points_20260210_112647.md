---
ver: rpa2
title: How to Achieve Higher Accuracy with Less Training Points?
arxiv_id: '2504.13586'
source_url: https://arxiv.org/abs/2504.13586
tags:
- training
- data
- uence
- points
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of computational inefficiency in
  large-scale model training by identifying informative subsets of training data that
  can achieve comparable or superior model performance. The authors propose a method
  based on influence functions to determine which training samples should be included
  in the training set.
---

# How to Achieve Higher Accuracy with Less Training Points?
## Quick Facts
- arXiv ID: 2504.13586
- Source URL: https://arxiv.org/abs/2504.13586
- Reference count: 26
- One-line primary result: Influence-based subset selection achieves comparable or superior accuracy to full training using only 10-60% of data

## Executive Summary
This paper addresses computational inefficiency in large-scale model training by proposing a method to identify informative subsets of training data that achieve comparable or superior model performance. The authors use influence functions to estimate how adding training samples affects test predictions, then select optimal subsets based on these estimates. Their approach is particularly valuable in resource-constrained scenarios where training on full datasets is impractical.

The empirical evaluation on binary classification tasks using logistic regression models with the Stanford Sentiment Treebank dataset demonstrates that their method achieves performance comparable to training on the entire dataset while using only 10% of the data. Remarkably, the method achieved even higher accuracy when trained with just 60% of the dataset, suggesting that some training samples may have negative influence on model performance.

## Method Summary
The method trains an initial logistic regression model on a base training set, computes the Hessian matrix, and then uses influence functions to estimate how adding each candidate training sample would affect validation predictions. Based on these influence estimates, the algorithm scores and selects samples that positively contribute to model performance according to a sign-aligned contribution scoring mechanism. The selected subset is then used to retrain the final model. The approach leverages the convexity of logistic regression to enable efficient computation of influence functions through Hessian inversion.

## Key Results
- 10% selected data achieves test accuracy of 0.630, matching the performance of training on the full dataset (0.627)
- 60% selected data achieves highest test accuracy of 0.644, outperforming full dataset training
- Method 1 (simple contribution-based selection) achieves the highest test accuracy among all proposed variants
- The method demonstrates significant data efficiency while maintaining or improving model performance

## Why This Works (Mechanism)

### Mechanism 1: Influence Function Approximation Avoids Retraining
The paper claims influence functions can estimate the effect of adding training points on test predictions without actual retraining. When adding subset S to training, parameter change Δw ≈ -Ĥ_w^(-1) × (1/N × Σ∇_w L(z_i, ŵ)) where Ĥ_w is the Hessian of empirical risk. The prediction change Δf_t ≈ ∇_w f_ŵ(x_t)^T × Δw (termed "IP-adding"). This approximates how adding points shifts model parameters and predictions. The core assumption is that empirical risk R(w) is twice differentiable and strongly convex, which logistic regression with L2 regularization satisfies. Break condition: Non-convex loss landscapes (e.g., deep neural networks) violate the Hessian invertibility assumption.

### Mechanism 2: Sign-Aligned Contribution Scoring
Training samples whose influence aligns with correct prediction direction improve accuracy when added. Algorithm 1 computes for each candidate point z_i: score = Σ_j [sign(Δf[i,j]) == sign(τ - f[j])] ? +Δf[i,j] : -Δf[i,j]. If score > 0, the point is included. This selects points that push validation predictions toward correct classifications. The assumption is that validation set distribution approximates test set distribution and mislabeled samples in the additional set are rare. Break condition: If validation labels are noisy or distribution shift exists between validation and test, selection may optimize wrong targets.

### Mechanism 3: Diminishing Returns from Informative Subset Saturation
Adding more data beyond informative samples introduces noise or redundant information, reducing marginal gains. The paper empirically shows 10% selected data matches full-data performance, while 60% achieves better accuracy. Method 1 (60% added) yields 0.644 test accuracy, but Add Full (100%) drops to 0.627. The assumption is that the additional training set Z_tr' contains both helpful and harmful samples; harmful samples are filtered by the selection process. Break condition: If nearly all samples are informative, selection may over-prune useful examples.

## Foundational Learning

- **Influence Functions**
  - Why needed here: Core technique for estimating parameter/prediction changes without retraining. Requires understanding of robust statistics and perturbation analysis.
  - Quick check question: Can you explain why H_w^(-1) appears in the influence formula and what happens if the Hessian is singular?

- **Convex Optimization and Strong Convexity**
  - Why needed here: The method assumes strongly convex loss for Hessian invertibility and unique minima. Logistic regression with L2 regularization satisfies this; deep networks do not.
  - Quick check question: Why does L2 regularization with λ > 0 guarantee Ĥ_w is positive definite?

- **Gradient and Hessian Computations for Loss Functions**
  - Why needed here: Implementing IP-adding requires computing ∇_w L(z_i, w) for each sample and the full Hessian Ĥ_w. For logistic regression, these have closed forms.
  - Quick check question: For binary logistic regression with cross-entropy loss, what is ∇_w L(z_i, w) in terms of the sigmoid output?

## Architecture Onboarding

- Component map: [Initial Training Set Z_tr] → [Train Initial Model] → [ŵ, Ĥ_w] → [Additional Candidates Z_tr'] → [Compute ∇_w L per sample] → [Compute Δw via influence] → [Score each candidate via Algorithm 1] → [Select S where score > 0] → [Retrain on Z_tr ∪ S] → [Final Model]

- Critical path:
  1. Train initial model on Z_tr to get ŵ
  2. Compute and invert Hessian Ĥ_w (most expensive step—O(p³) for p parameters)
  3. Compute per-sample gradients for all candidates
  4. Run Algorithm 1 scoring on validation set
  5. Retrain with selected subset

- Design tradeoffs:
  - Method 1 (simple scoring) vs. Methods 2-6 (weighted/thresholded variants): Paper finds Method 1 achieves highest test accuracy despite being simplest
  - Hessian computation: Exact inversion is O(p³); for high-dimensional data (Bag-of-Words), this is prohibitive—consider LiSSA or diagonal approximations
  - Validation set size: Larger validation improves selection reliability but reduces available training data

- Failure signatures:
  - Test accuracy drops below "Original" baseline → Likely validation/test distribution mismatch or excessive filtering
  - Selected subset is empty or near-empty → Contribution threshold too strict; check sign alignment logic
  - Hessian inversion fails → Regularization λ too small or numerical instability; increase λ

- First 3 experiments:
  1. Reproduce Method 1 on SST binary: Use 4152 original + 2768 additional split as in paper. Verify 10% selection achieves ~0.63 test accuracy.
  2. Ablation on selection threshold: Vary the score > 0 condition (e.g., top-k instead of positive-only) to understand sensitivity of subset size.
  3. Scalability test with approximate Hessian: Replace exact H^-1 with diagonal or LiSSA approximation; measure accuracy degradation vs. speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the influence-based selection method be effectively adapted for non-convex loss functions in deep learning architectures?
- Basis in paper: [explicit] Section 7 explicitly calls for extending the methodology to complex neural architectures. Section 3.1 notes the current reliance on strong convexity for the Hessian calculation.
- Why unresolved: Standard influence functions require stable Hessian inversion, which is computationally prohibitive and theoretically unstable for the non-convex loss landscapes of deep neural networks.
- What evidence would resolve it: Empirical validation showing comparable or superior accuracy on standard deep learning benchmarks using approximated influence estimates.

### Open Question 2
- Question: How does the selection mechanism perform under distribution shifts or in the presence of high label noise?
- Basis in paper: [explicit] Section 7 identifies investigating robustness to distribution shifts and label noise as a specific direction for future research.
- Why unresolved: The method currently assumes the validation set distribution matches the test set (Section 4), and influence functions can be sensitive to outliers, potentially selecting harmful samples if the data is noisy.
- What evidence would resolve it: Experiments on datasets with injected label noise or domain shift, demonstrating the method's ability to filter noisy samples better than random selection.

### Open Question 3
- Question: What are the theoretical generalization bounds for models trained on influence-selected subsets compared to the full dataset?
- Basis in paper: [explicit] Section 7 lists "developing theoretical frameworks to understand how sample selection impacts model generalization" as a key open area.
- Why unresolved: The paper empirically observes that 60% of data outperforms the full dataset, but lacks a formal theoretical explanation for why removing "negative influence" samples improves generalization bounds.
- What evidence would resolve it: A formal derivation linking the cumulative influence score of the training subset to the model's generalization gap.

## Limitations

- The influence function approximation assumes strong convexity, which may not hold for non-convex models (e.g., deep networks), limiting generalizability.
- Distribution shift between validation and test sets could lead to suboptimal selection, as the method assumes validation set is representative.
- The method's effectiveness for extremely high-dimensional feature spaces (Bag-of-Words) is uncertain due to Hessian inversion computational cost.

## Confidence

- **High confidence**: The empirical results showing 10% subset achieving comparable performance to full training are reproducible and well-documented. The logistic regression framework with influence functions is theoretically sound.
- **Medium confidence**: The mechanism by which selecting 60% of data achieves higher accuracy than 100% (suggesting harmful samples in the full set) is plausible but requires additional validation. The specific scoring algorithm's sensitivity to validation set size is not fully explored.
- **Low confidence**: The generalizability to non-convex models and extremely high-dimensional features remains unverified. The computational scalability for real-world applications with millions of features is unclear.

## Next Checks

1. Verify distribution alignment between validation and test sets using statistical tests (e.g., KL divergence on feature distributions).
2. Implement Hessian approximation methods (diagonal or LiSSA) to test scalability to high-dimensional Bag-of-Words features.
3. Conduct ablation studies varying validation set size to quantify its impact on selection quality and final test accuracy.