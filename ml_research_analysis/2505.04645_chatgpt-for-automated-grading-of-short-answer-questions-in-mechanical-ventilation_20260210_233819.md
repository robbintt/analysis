---
ver: rpa2
title: ChatGPT for automated grading of short answer questions in mechanical ventilation
arxiv_id: '2505.04645'
source_url: https://arxiv.org/abs/2505.04645
tags:
- grading
- chatgpt
- language
- human
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models were tested for automated grading of medical\
  \ short answer questions using 557 student responses across three case-based scenarios.\
  \ ChatGPT 4o was prompted with standardized grading rubrics, and outputs were compared\
  \ to human grades using ICC, Cohen\u2019s kappa, Kendall\u2019s W, and Bland-Altman\
  \ statistics."
---

# ChatGPT for automated grading of short answer questions in mechanical ventilation

## Quick Facts
- arXiv ID: 2505.04645
- Source URL: https://arxiv.org/abs/2505.04645
- Authors: Tejas Jade; Alex Yartsev
- Reference count: 39
- Large language models were tested for automated grading of medical short answer questions using 557 student responses across three case-based scenarios.

## Executive Summary
This study evaluated ChatGPT 4o's performance in grading short answer questions on mechanical ventilation using standardized rubrics across 557 student responses. The LLM was systematically under-marking compared to human graders, with a bias of -1.34 points on a 10-point scale. Despite high internal consistency across grading sessions (G-value = 0.87), agreement with human graders was poor across all statistical measures, with over 60% of AI-assigned grades falling outside acceptable assessment boundaries.

The findings raise significant concerns about using consumer-grade LLMs for high-stakes medical assessment without substantial modifications. The study found that rubric type influenced agreement, with evaluative items showing particularly poor alignment. While the study provides important empirical evidence about LLM limitations in medical grading contexts, it also highlights the need for further research into prompt engineering and rubric redesign to potentially improve AI grading performance.

## Method Summary
The study employed a cross-sectional design using 557 student responses to three case-based mechanical ventilation questions. ChatGPT 4o was prompted with standardized grading rubrics and produced grades that were compared against human expert grades using multiple statistical measures including ICC, Cohen's kappa, Kendall's W, and Bland-Altman analysis. The grading process was repeated across multiple sessions to assess consistency, and different rubric types were tested to examine their influence on agreement levels.

## Key Results
- ChatGPT systematically under-marked responses with a bias of -1.34 points on a 10-point scale
- Poor agreement with human graders (ICC1 = 0.086, kappa = -0.0786) despite high internal consistency (G-value = 0.87)
- Over 60% of AI-assigned grades fell outside acceptable high-stakes assessment boundaries
- Rubric type significantly influenced agreement, with evaluative items showing poorest alignment

## Why This Works (Mechanism)
The study demonstrates that current consumer-grade LLMs like ChatGPT 4o struggle with the nuanced, context-dependent evaluation required for medical short answer grading. The systematic under-marking suggests the model may be applying overly conservative or literal interpretations of grading criteria, potentially due to training data biases or limitations in understanding medical context. The poor agreement despite high internal consistency indicates that while the model is consistent in its approach, that approach diverges significantly from human expert judgment.

## Foundational Learning

1. **Inter-rater reliability metrics** - Understanding ICC, Cohen's kappa, and Kendall's W is essential for evaluating grader agreement in educational assessment contexts. Quick check: Can you explain the difference between absolute agreement and consistency in ICC calculations?

2. **Medical short answer question grading** - Knowledge of how medical education assessments are structured and evaluated, particularly the distinction between recall and evaluative questions. Quick check: What distinguishes a recall question from an evaluative question in medical education?

3. **Rubric-based assessment** - Understanding how standardized grading rubrics are designed and applied in medical education to ensure consistent evaluation across graders. Quick check: How do rubrics typically handle partial credit allocation in medical short answer questions?

## Architecture Onboarding

**Component map:** Student responses → ChatGPT 4o → AI grades → Statistical comparison with human grades → Agreement analysis

**Critical path:** Student response input → Prompt engineering with rubric → LLM processing → Grade output → Statistical validation against human grades

**Design tradeoffs:** Using unmodified human grading rubrics vs. LLM-optimized rubrics; single domain testing vs. cross-domain validation; automated grading vs. human expert evaluation

**Failure signatures:** Systematic under-marking bias, poor agreement with human graders, high percentage of grades outside acceptable boundaries, inconsistent performance across rubric types

**First experiments:**
1. Test AI grading performance across multiple medical specialties to assess generalizability
2. Implement systematic prompt engineering variations to optimize rubric interpretation
3. Compare fine-tuned LLM performance against baseline model on held-out medical responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific prompt engineering strategies or rubric redesign improve the agreement between ChatGPT and human experts in grading postgraduate medical short answer questions?
- Basis in paper: [explicit] The conclusion states, "the question of whether agreement can be improved through prompt engineering and rubric design remains unanswered."
- Why unresolved: This study tested unmodified rubrics intended for human graders, which resulted in poor agreement (ICC1 = 0.086).
- What evidence would resolve it: A randomized comparison of grading accuracy using optimized, LLM-specific prompts versus standard rubrics on the same dataset.

### Open Question 2
- Question: Does fine-tuning LLMs on small datasets of expert-graded answers improve grading accuracy in specialized medical fields compared to consumer-level models?
- Basis in paper: [explicit] The discussion notes that while fine-tuning is a potential solution, "the effectiveness of this strategy in this particular use-case remains to be assessed."
- Why unresolved: The study relied on a generic model ("gpt-4-turbo") without specialized ICU training data, contributing to knowledge gaps and systematic under-marking.
- What evidence would resolve it: Comparison of performance metrics (ICC, Cohen's kappa) between a fine-tuned model and the baseline model on held-out medical responses.

### Open Question 3
- Question: Do the observed grading biases and poor agreement levels generalize to other postgraduate medical domains outside of mechanical ventilation?
- Basis in paper: [inferred] The authors note the study was "evaluated using a single domain... which may not generalize to other areas of medical education."
- Why unresolved: Mechanical ventilation is a narrow specialty; LLM performance may differ in fields with varying levels of representation in the model's training data.
- What evidence would resolve it: Replication of this methodology across diverse medical specialties (e.g., radiology, pediatrics) to compare agreement statistics.

## Limitations
- Study conducted within single postgraduate medical institution with specific grading rubrics
- Focus on only three case-based scenarios and mechanical ventilation domain limits generalizability
- Did not explore extensive prompt engineering or rubric redesign to potentially improve performance
- 557 student responses, while substantial, may not capture full diversity of medical short answer questions

## Confidence
- Confidence in the major claims about ChatGPT's unsuitability for high-stakes medical assessment is High
- Confidence in the generalizability of these findings to other contexts is Medium
- Confidence in the conclusion that prompt engineering alone cannot resolve the grading issues is Low

## Next Checks
1. Test ChatGPT's grading performance across multiple medical institutions with varied assessment formats and grading rubrics
2. Systematically evaluate the impact of extensive prompt engineering and rubric redesign on AI grading accuracy and agreement with human evaluators
3. Conduct a longitudinal study comparing AI and human grading consistency over multiple assessment cycles to assess the stability and reliability of AI-assisted grading in medical education