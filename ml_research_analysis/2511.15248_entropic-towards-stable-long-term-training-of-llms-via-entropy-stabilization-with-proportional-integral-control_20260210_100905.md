---
ver: rpa2
title: 'EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization
  with Proportional-Integral Control'
arxiv_id: '2511.15248'
source_url: https://arxiv.org/abs/2511.15248
tags:
- entropy
- training
- arxiv
- control
- entropic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining stable entropy
  during long-term training of large language models, which is crucial for preventing
  model collapse into sub-optimal behaviors. The proposed EntroPIC method uses Proportional-Integral
  control to dynamically adjust the influence of positive and negative samples by
  tuning their loss coefficients, stabilizing entropy throughout training.
---

# EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control

## Quick Facts
- arXiv ID: 2511.15248
- Source URL: https://arxiv.org/abs/2511.15248
- Reference count: 40
- Primary result: 3.5% improvement in average pass rate and 3.8% in pass rate over GRPO on mathematical benchmarks

## Executive Summary
EntroPIC addresses the critical challenge of entropy collapse during long-term LLM training by introducing a Proportional-Integral control mechanism that dynamically adjusts the influence of positive and negative samples. The method stabilizes entropy by modifying the policy gradient loss—positive samples (advantage > 0) are down-weighted while negative samples (advantage < 0) are up-weighted when entropy falls below target. This approach ensures that entropy remains at desired levels throughout training, preventing the model from collapsing into sub-optimal behaviors. Experimental results demonstrate significant improvements in mathematical reasoning tasks while maintaining strong generalization to non-mathematical domains.

## Method Summary
EntroPIC implements entropy stabilization by wrapping a base RL algorithm (GRPO or PPO) with a PI controller that computes a control coefficient α based on the difference between current entropy and target entropy. The coefficient is applied selectively to high-probability tokens (π>0.95) in the policy gradient loss, with positive advantages receiving weight (1+α) and negative advantages receiving weight (1−α). The PI controller uses proportional gain Kp and integral gain Ki to ensure convergence to target entropy for both on-policy and off-policy settings. The method requires task-specific target entropy values (0.1 for non-thinking, 0.4 for reasoning) and has been validated on mathematical reasoning benchmarks with strong generalization results on other domains.

## Key Results
- Maintains desired entropy levels throughout training, preventing model collapse
- Achieves 3.5% improvement in average pass rate and 3.8% in pass rate compared to GRPO on mathematical benchmarks
- Shows strong generalization to non-mathematical tasks (MMLU-Pro, LiveCodeBench, GPQA)
- Robust to temperature settings while maintaining entropy stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjusting loss coefficients for positive versus negative samples based on entropy deviation controls policy entropy.
- Mechanism: A control signal α modulates sample weights—positive samples (A>0) receive weight (1+α), negative samples (A<0) receive weight (1−α). When entropy falls below target, α>0 amplifies negative sample influence to raise entropy; when above target, α<0 amplifies positive samples to reduce entropy.
- Core assumption: The advantage distribution approximates binary reward structure where expected advantage equals zero.
- Evidence anchors:
  - [abstract] "...adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients."
  - [section 4.1, Corollary 4.1] "training exclusively with positive/negative samples... will result in a decrease/increase of the policy's entropy."
  - [corpus] BAPO (arXiv:2510.18927) observes similar entropy decline in off-policy LLM RL but uses adaptive clipping rather than PI control.
- Break condition: If advantage distribution has non-zero mean or multi-modal structure, the binary assumption fails and α may overshoot corrections.

### Mechanism 2
- Claim: Focusing weight adjustments on high-probability tokens (πθ>τ) achieves stable entropy control with minimal disruption to training.
- Mechanism: The modified loss applies α only to tokens exceeding probability threshold τ (default 0.95). High-probability tokens dominate entropy changes; low-probability tokens are left unmodified, preserving gradient flow for rare but valuable tokens.
- Core assumption: Entropy change correlates positively with token probability—high-probability tokens drive most entropy dynamics.
- Evidence anchors:
  - [section 4.3, Corollary 4.4] "entropy changes are positively correlated with token probabilities. Therefore, controlling the weights of high-probability tokens should yield similar results with minimal disruption."
  - [section 4.3, Figure 3] Ablation shows masking high-probability tokens produces larger entropy changes than low-probability tokens.
  - [corpus] Weak/no direct corpus support for high-probability thresholding specifically.
- Break condition: If task requires learning novel tokens that rarely achieve high probability, selective adjustment may slow acquisition of rare patterns.

### Mechanism 3
- Claim: Proportional-Integral control ensures convergence to target entropy for both on-policy and off-policy settings.
- Mechanism: P-term (Kp·e(t)) provides immediate error correction; I-term (Ki·∑e(k)) eliminates steady-state bias from distribution shift. On-policy needs only P-control; off-policy requires full PI to compensate for importance sampling bias.
- Core assumption: Learning rate η is small enough that linearized error dynamics approximate true entropy trajectory.
- Evidence anchors:
  - [section 4.2, Theorem 4.2] "using P-control or PI-control... the error et between the policy's entropy and the target entropy converges to zero."
  - [section 4.2, Theorem 4.3] "Only PI-control... ensures that the error et... converges to zero" for off-policy.
  - [corpus] Tracking Drift (arXiv:2601.19624) addresses non-stationary RL with entropy scheduling but does not prove PI convergence.
- Break condition: If gains Kp, Ki are too large relative to η, the discrete system may exhibit oscillatory or divergent error dynamics.

## Foundational Learning

- Concept: Policy gradient with advantage estimation
  - Why needed here: EntroPIC modifies the policy gradient loss by reweighting terms based on advantage sign; understanding how advantage drives gradient updates is essential to see why reweighting affects entropy.
  - Quick check question: Given loss L(θ) = −E[A(s,a)·logπ(a|s)], what happens to the gradient when A(s,a) is positive vs. negative?

- Concept: Shannon entropy and its gradient
  - Why needed here: The method targets entropy as the controlled variable; you must understand how entropy H(π) = −∑π·logπ changes under parameter updates.
  - Quick check question: For a softmax policy, does increasing the probability of the highest-probability action increase or decrease entropy?

- Concept: Discrete-time PI control theory
  - Why needed here: The α signal is computed via u(n) = Kp·e(n) + Ki·∑e(k); understanding stability conditions (small gains, bounded error) helps diagnose divergent training.
  - Quick check question: In discrete PI control, what happens if Ki is set too large relative to the system's natural response time?

## Architecture Onboarding

- Component map:
  - EntropyMonitor -> PIController -> LossModifier -> BaseTrainer

- Critical path:
  1. Rollout → compute advantages and token probabilities.
  2. Compute current entropy Ht from policy.
  3. PIController computes αt from Ht − Htar and accumulated error.
  4. LossModifier constructs modified loss with (1±α) weights on high-probability tokens.
  5. Backward pass updates θ; entropy should move toward Htar.

- Design tradeoffs:
  - Higher Kp gives faster response but risks oscillation; lower Kp is smoother but slower to correct deviations.
  - Lower threshold τ (e.g., 0.5) modifies more tokens but may destabilize low-probability token learning; τ=0.95 is conservative.
  - Target entropy Htar must be tuned per task—too high preserves exploration but slows convergence; too low risks collapse.

- Failure signatures:
  - Entropy oscillates wildly around target → Kp too large; reduce by 10×.
  - Entropy converges but to wrong value (off-policy) → Ki=0; enable integral term.
  - Training diverges with NaN losses → α exceeds ±1 due to large error accumulation; clamp α ∈ [−0.9, 0.9] or cap Ik.

- First 3 experiments:
  1. Toy entropy control: Train a small policy on synthetic binary rewards with Htar=1.0; verify entropy converges to target for both P-only (on-policy) and PI (off-policy).
  2. Ablate τ: Compare τ∈{0.5, 0.8, 0.95} on a small math dataset (e.g., 10K samples); plot entropy curves and final accuracy.
  3. Compare baselines: Run EntroPIC vs. GRPO vs. AEC on same dataset; measure entropy stability, final pass@1, and training wall-clock time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal target entropy be automatically determined for different model architectures, tasks, and training stages without manual tuning?
- Basis in paper: [explicit] "Since the target entropy needs to be manually set, its value must be carefully adjusted to fit the specific requirements of the model and task, which can be a limitation in automated or highly dynamic settings."
- Why unresolved: The paper relies on fixed target entropy values (0.1 for non-thinking, 0.4 for reasoning models) but provides no principled method for selecting these values a priori.
- What evidence would resolve it: A systematic study correlating target entropy with model size, task complexity, and dataset characteristics, or an adaptive mechanism that infers appropriate targets during training.

### Open Question 2
- Question: What mechanisms explain why entropy stabilization via EntroPIC leads to substantially better generalization compared to other RL methods?
- Basis in paper: [explicit] Table 3 shows EntroPIC achieves +6.5% on MMLU-Pro and +12.6% on LiveCodeBench over the backbone, while competing methods suffer catastrophic forgetting.
- Why unresolved: The paper demonstrates improved generalization empirically but does not provide theoretical analysis explaining why maintaining target entropy specifically preserves general capabilities while enhancing task-specific performance.
- What evidence would resolve it: Probes into how entropy stabilization affects internal representations, attention patterns, or knowledge retention compared to entropy-decaying methods.

### Open Question 3
- Question: How do the control coefficients (Kp, Ki) and high-probability threshold (τ) interact, and what are optimal settings across different model scales?
- Basis in paper: [inferred] The paper uses default values Kp=1, Ki=0.01, τ=0.95 but ablates only τ (Section D.5) and target entropy (Section D.4), leaving the control coefficients largely unexplored.
- Why unresolved: Control theory suggests Kp and Ki affect convergence speed and overshoot, but their interaction with LLM training dynamics and sensitivity to model scale remains unstudied.
- What evidence would resolve it: A comprehensive ablation varying Kp, Ki, and τ across different model sizes (e.g., 1.5B, 7B, 70B) and analyzing convergence speed and stability.

### Open Question 4
- Question: Can EntroPIC be extended to multi-objective entropy control for domains requiring different exploration levels across sub-tasks or reasoning stages?
- Basis in paper: [inferred] The paper uses a single global target entropy, but mathematical reasoning involves diverse sub-tasks (calculation, planning, verification) that may benefit from context-dependent exploration.
- Why unresolved: Theoretical analysis assumes a unified entropy target, and the method adjusts all high-probability tokens uniformly without distinguishing between reasoning phases or token types.
- What evidence would resolve it: Experiments with per-layer, per-position, or semantically-conditioned entropy targets demonstrating whether granular control improves performance over uniform targets.

## Limitations

- Empirical scope limited to mathematical reasoning benchmarks (17K samples), with non-mathematical generalization shown but not systematically studied
- Theoretical convergence proofs rely on assumptions (zero-mean advantage distribution, high-probability token dominance) that may not hold in all domains
- Requires task-specific target entropy tuning with no principled method for automatic selection across different model architectures and domains

## Confidence

**Entropy control mechanism**: High - Well-supported by theoretical analysis and ablation studies
**Performance improvement**: Medium - Statistically significant within tested mathematical benchmarks, but contribution of entropy stabilization vs. implementation differences not isolated
**Generalization to non-mathematical tasks**: Low-Medium - Reasonable performance shown but systematic advantages outside mathematical reasoning not demonstrated

## Next Checks

1. **Cross-domain ablation study**: Run EntroPIC with varying Htar values (0.1, 0.3, 0.5, 0.7) on a diverse set of non-mathematical tasks (e.g., code generation, general QA, creative writing) to identify optimal entropy targets per domain and isolate entropy's contribution to performance.

2. **Advantage distribution analysis**: Characterize the advantage distribution across different task types (mathematical vs. non-mathematical) to validate the zero-mean assumption. Test EntroPIC variants that handle non-zero mean advantages or multi-modal advantage distributions.

3. **Real-time entropy stability monitoring**: Implement continuous entropy tracking during training across multiple seeds and datasets to quantify variance in entropy stability. Compare EntroPIC's entropy variance against GRPO and AEC baselines to establish statistical significance of stability improvements.