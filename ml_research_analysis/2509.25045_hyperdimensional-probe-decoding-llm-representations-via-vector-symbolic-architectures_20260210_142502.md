---
ver: rpa2
title: 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures'
arxiv_id: '2509.25045'
source_url: https://arxiv.org/abs/2509.25045
tags:
- section
- concepts
- probing
- probe
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Hyperdimensional Probe, a novel probing
  paradigm that combines Vector Symbolic Architectures (VSAs) and hypervector algebra
  to decode internal representations of Large Language Models (LLMs). The method addresses
  the limitations of existing interpretability approaches by integrating input-oriented
  feature extraction with output-oriented analysis.
---

# Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures

## Quick Facts
- arXiv ID: 2509.25045
- Source URL: https://arxiv.org/abs/2509.25045
- Authors: Marco Bronzini; Carlo Nicolini; Bruno Lepri; Jacopo Staiano; Andrea Passerini
- Reference count: 40
- Key outcome: 83% precision in extracting target concepts from LLM embeddings across multiple models

## Executive Summary
This paper introduces the Hyperdimensional Probe, a novel probing paradigm that combines Vector Symbolic Architectures (VSAs) and hypervector algebra to decode internal representations of Large Language Models (LLMs). The method addresses the limitations of existing interpretability approaches by integrating input-oriented feature extraction with output-oriented analysis. It uses VSA encodings to create a bounded, interpretable proxy space that avoids the unbounded feature spaces of Sparse Autoencoders (SAEs) and the token-level constraints of logit-based methods. The approach includes an embedding ingestion algorithm that compresses LLM representations through k-means clustering and sum pooling, enabling probing of the full residual stream without layer selection.

## Method Summary
The Hyperdimensional Probe framework uses Vector Symbolic Architectures to create a bounded proxy space for decoding LLM representations. It employs MAP-Bipolar encoding with 4096-dimensional hypervectors to represent concepts, enabling semantic-level probing beyond individual tokens. The method uses an embedding ingestion algorithm that applies k-means clustering (k=5) to layer embeddings and sum pooling to compress representations. A 3-layer MLP with residual connections, GELU activations, and dropout is trained to map compressed embeddings to VSA encodings. The probe is evaluated on analogy-style completion tasks using cosine similarity and precision metrics, achieving 83% precision in extracting target concepts.

## Key Results
- Achieves 83% precision@1 in extracting target concepts from LLM embeddings across multiple models
- Reports 0.89 cosine similarity and 0.94 binary accuracy between predicted and target VSA encodings
- Outperforms traditional logit attribution methods in concept extraction tasks
- Reveals varying conceptual richness across models and input types during text generation

## Why This Works (Mechanism)
The Hyperdimensional Probe works by leveraging the distributed representation properties of VSAs to create a bounded, interpretable proxy space for LLM representations. By encoding concepts as high-dimensional bipolar vectors and using binding/unbinding operations, the method can extract semantic information without being constrained by token-level representations. The combination of k-means clustering and sum pooling effectively compresses the unbounded residual stream into a manageable representation space while preserving semantic information. The MLP encoder learns to map these compressed representations to VSA encodings, enabling the probe to recover conceptual relationships through cosine similarity-based unbinding operations.

## Foundational Learning

**Vector Symbolic Architectures (VSAs)**: Mathematical frameworks for representing symbolic structures in high-dimensional vector spaces using binding (⊙) and bundling (+) operations. Needed for creating distributed representations that can capture compositional relationships between concepts. Quick check: Verify codebook orthogonality with average pairwise similarity ~0±0.02.

**Hypervector Algebra**: Operations on high-dimensional vectors including circular convolution for binding and element-wise addition for bundling. Essential for creating compound representations and extracting individual components through unbinding. Quick check: Test binding/unbinding operations on known concept pairs to verify recovery accuracy.

**MAP-Bipolar Encoding**: Specific VSA architecture using bipolar {-1, +1} vectors with Mean-Average-Precision decoding. Provides bounded, interpretable representations suitable for semantic probing. Quick check: Confirm dimensional consistency (nc × D) and proper bipolar conversion for BCE loss.

## Architecture Onboarding

**Component map**: Corpus -> VSA Codebook -> LLM Embeddings -> K-means + Sum Pooling -> MLP Encoder -> VSA Predictions -> Unbinding -> Extracted Concepts

**Critical path**: Embedding ingestion (k-means + pooling) → MLP encoding → VSA unbinding → Concept extraction. Each stage must preserve semantic information while reducing dimensionality.

**Design tradeoffs**: VSA vs SAE: Bounded vs unbounded feature spaces; interpretable vs potentially more granular representations. MLP depth: Deeper networks may capture complex mappings but risk overfitting. k-means k: Balance between compression and information retention.

**Failure signatures**: Low cosine similarity (<0.5) indicates encoding problems; high "NONE" rates suggest threshold too strict or concept misalignment; inconsistent predictions across similar inputs may indicate overfitting.

**First experiments**:
1. Test VSA binding/unbinding with synthetic concept pairs to verify basic operations
2. Evaluate k-means compression quality by comparing pooled embeddings to original representations
3. Train MLP on reduced dataset (10k examples) to verify architecture before full-scale training

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown data augmentation procedures for expanding analogy corpus from 114,099 to 395,944 examples
- Underspecified unbinding candidate selection algorithm and threshold sensitivity
- Limited generalizability beyond analogy completion and QA tasks
- Fixed k=5 for k-means clustering without justification or hyperparameter exploration

## Confidence

**High confidence**: VSA encoding implementation with MAP-Bipolar architecture, MLP encoder architecture, and overall probing methodology are well-specified and reproducible.

**Medium confidence**: Performance metrics depend on unknown augmentation procedures and candidate selection methods that affect evaluation fairness.

**Low confidence**: Claims about "conceptual richness" varying across models lack statistical validation and controls for dataset composition effects.

## Next Checks

1. **Augmentation pipeline reconstruction**: Implement systematic key-value swapping patterns to recreate the data augmentation procedure and measure impact on training set diversity and model performance.

2. **Threshold sensitivity analysis**: Conduct ablation studies varying the unbinding cosine threshold (0.05, 0.1, 0.15, 0.2) to evaluate precision-recall trade-offs and determine optimal threshold selection criteria.

3. **Cross-task generalization test**: Apply the Hyperdimensional Probe framework to a different task type (e.g., natural language inference) using the same VSA codebook to assess method generalizability beyond analogy completion.