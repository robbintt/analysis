---
ver: rpa2
title: Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning
arxiv_id: '2505.12811'
source_url: https://arxiv.org/abs/2505.12811
tags:
- qmix
- test
- steps
- sight
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the sight range dilemma in multi-agent reinforcement
  learning (MARL), where agents either receive insufficient or excessive information
  from their environment. To tackle this issue, the authors propose Dynamic Sight
  Range Selection (DSR), a novel method that dynamically adjusts the sight range during
  training using an Upper Confidence Bound (UCB) algorithm.
---

# Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.12811
- Source URL: https://arxiv.org/abs/2505.12811
- Reference count: 40
- Agents dynamically adjust sight range during training using UCB to balance information quantity and quality

## Executive Summary
This paper addresses the sight range dilemma in multi-agent reinforcement learning (MARL), where agents struggle to balance receiving sufficient environmental information without being overwhelmed by irrelevant details. The authors propose Dynamic Sight Range Selection (DSR), a method that uses an Upper Confidence Bound (UCB) algorithm to dynamically adjust sight ranges during training. DSR allows agents to converge on optimal sight ranges without requiring global information or communication mechanisms, effectively resolving the trade-off between information sufficiency and noise.

## Method Summary
The paper introduces Dynamic Sight Range Selection (DSR) to tackle the sight range dilemma in MARL. DSR employs an Upper Confidence Bound (UCB) algorithm that dynamically adjusts sight ranges during training, allowing agents to balance information quantity and quality. The method enables agents to explore different sight ranges and converge on the most suitable one based on performance, without requiring global information or communication mechanisms. DSR is designed to be compatible with various MARL algorithms and can be applied to different environment types.

## Key Results
- DSR achieves better performance across three MARL environments: Level-Based Foraging (LBF), Multi-Robot Warehouse (RWARE), and StarCraft Multi-Agent Challenge (SMAC)
- Consistent improvements observed across multiple MARL algorithms including QMIX and MAPPO
- DSR accelerates training process and provides interpretability by indicating optimal sight range used during training

## Why This Works (Mechanism)
DSR works by using UCB to balance exploration and exploitation when selecting sight ranges. During training, agents explore different sight ranges while the UCB algorithm tracks performance, gradually converging to sight ranges that maximize reward. This dynamic adjustment allows agents to avoid both under-information (too narrow sight range) and over-information (too broad sight range) scenarios. The method implicitly learns which environmental information is most relevant for decision-making without requiring explicit communication or centralized coordination.

## Foundational Learning
- Upper Confidence Bound (UCB) algorithm: A bandit algorithm balancing exploration and exploitation; needed for dynamically selecting sight ranges during training; quick check: verify UCB formula implementation matches theoretical definition
- Multi-Agent Reinforcement Learning (MARL): Extension of RL to multiple interacting agents; needed as the primary learning framework; quick check: confirm state and action spaces properly defined for multi-agent setting
- Sight range in MARL: The observable radius for each agent; needed to control information flow and complexity; quick check: validate sight range implementation affects observable state space as expected

## Architecture Onboarding

Component map:
DSR component -> UCB algorithm -> Sight range adjustment -> MARL training loop -> Performance evaluation

Critical path:
During each training step, DSR's UCB algorithm evaluates performance across different sight ranges, selects optimal range for current step, and feeds this range into the MARL algorithm's observation function.

Design tradeoffs:
- Fixed vs. dynamic sight ranges: DSR trades computational overhead for adaptive information selection
- Exploration vs. exploitation: UCB balances trying new sight ranges against exploiting known good ranges
- Local vs. global information: DSR operates without global state, maintaining scalability but potentially missing coordination opportunities

Failure signatures:
- If UCB parameters are poorly tuned, agents may converge to suboptimal sight ranges
- In highly dynamic environments, static UCB parameters may prevent adequate adaptation
- Over-exploration can slow convergence, while under-exploration may miss better sight ranges

First experiments:
1. Verify DSR works with simple gridworld MARL task with known optimal sight range
2. Test DSR with different UCB parameters (Î± values) to find sensitivity
3. Compare training curves with and without DSR on a small SMAC scenario

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Evaluation restricted to relatively small-scale MARL tasks, raising scalability questions
- Computational overhead of UCB-based sight range selection not quantified through ablation studies
- Limited analysis of how sight range dynamics affect emergent agent coordination patterns

## Confidence
- Performance improvements in tested environments: High
- Generalizability to larger-scale or more complex tasks: Medium
- Training efficiency gains (absolute claims): Low (relative improvements noted, but no runtime analysis)
- Communication-free operation: High

## Next Checks
1. Conduct scalability experiments on larger MARL environments or with a greater number of agents
2. Perform runtime and ablation studies to quantify the computational overhead and isolate the impact of the UCB-based sight range selection
3. Investigate the influence of sight range dynamics on emergent agent coordination and strategic behaviors, and test robustness to environmental changes