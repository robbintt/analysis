---
ver: rpa2
title: Where Do You Go? Pedestrian Trajectory Prediction using Scene Features
arxiv_id: '2501.13848'
source_url: https://arxiv.org/abs/2501.13848
tags:
- pedestrian
- scene
- trajectory
- prediction
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel pedestrian trajectory prediction model
  that integrates environmental context with pedestrian interaction modeling. The
  approach combines a Sparse Graph Convolutional Network (SGCN) for capturing pedestrian
  interactions with scene features extracted through image enhancement and semantic
  segmentation.
---

# Where Do You Go? Pedestrian Trajectory Prediction using Scene Features

## Quick Facts
- arXiv ID: 2501.13848
- Source URL: https://arxiv.org/abs/2501.13848
- Authors: Mohammad Ali Rezaei; Fardin Ayar; Ehsan Javanmardi; Manabu Tsukada; Mahdi Javanmardi
- Reference count: 30
- Primary result: Achieves ADE 0.252m and FDE 0.372m on ETH/UCY datasets

## Executive Summary
This paper presents a novel pedestrian trajectory prediction model that integrates environmental context with pedestrian interaction modeling. The approach combines a Sparse Graph Convolutional Network (SGCN) for capturing pedestrian interactions with scene features extracted through image enhancement and semantic segmentation. A cross-attention mechanism fuses these features to prioritize relevant environmental factors, and a Temporal Convolutional Network predicts future trajectories. The method significantly outperforms existing state-of-the-art approaches, demonstrating the importance of incorporating both social interactions and environmental context in trajectory prediction.

## Method Summary
The model processes historical pedestrian trajectories and scene video frames through two parallel streams. The scene stream uses Real-ESRGAN for image enhancement, OneFormer for semantic segmentation, and ResNet-18 for feature extraction, producing a scene representation. The social stream constructs sparse graphs from pedestrian trajectories using SGCN to capture local interactions. A cross-attention module fuses these representations by treating pedestrian graph features as queries and scene features as keys/values, with a residual connection. The fused features are processed by a Temporal Convolutional Network to predict future trajectories. The model is trained end-to-end using SGD, with pre-trained Real-ESRGAN and OneFormer modules kept fixed.

## Key Results
- Achieves state-of-the-art ADE of 0.252 meters and FDE of 0.372 meters on ETH/UCY benchmarks
- Outperforms existing methods like SceneAware, MTP, and TGCN across all test scenes
- Ablation study confirms semantic scene features improve performance by 7-15% compared to trajectory-only models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model likely reduces physically impossible predictions by constraining trajectory hypotheses with semantic environmental features.
- **Mechanism:** A Feature Extraction Module processes video frames through Real-ESRGAN (enhancement) and OneFormer (segmentation) to identify walkable areas. This scene representation ($H_{scene}$) is fused with pedestrian data, implicitly penalizing paths that intersect with non-walkable segments identified in the semantic maps.
- **Core assumption:** Assumes that high-fidelity semantic segmentation maps correlate strongly with pedestrian navigation decisions and that the pre-trained segmentation models generalize well to the target domain.
- **Evidence anchors:** [abstract] Mentions employing "image enhancement and semantic segmentation techniques to extract detailed scene features." [section 4.4] The ablation study shows that the configuration "w/ Maps" consistently outperforms "w/o Maps" in ADE/FDE, evidencing the utility of semantic constraints. [corpus] Related work "SceneAware" supports this by explicitly using scene constraints for walkability, suggesting this is a viable causal pathway.
- **Break condition:** If the environment is highly dynamic (e.g., temporary obstacles not captured in static semantics) or occlusion is severe, the segmentation map may provide misleading constraints.

### Mechanism 2
- **Claim:** Using cross-attention to fuse graph features (social) and scene features (physical) allows the model to prioritize environmental factors relevant to specific social interactions.
- **Mechanism:** The Cross-Attention Module treats the Sparse Graph Convolutional Network (SGCN) output (pedestrian interactions) as the Query ($Q$) and the scene features as the Key ($K$) and Value ($V$). This forces the model to "attend" to parts of the environment that are contextually relevant to the pedestrian's current social state, rather than treating all scene data equally.
- **Core assumption:** Assumes that pedestrian intent is a function of both social dynamics and physical affordances, and that these can be effectively correlated via dot-product attention.
- **Evidence anchors:** [section 3.5] Defines the cross-attention mechanism specifically with $Q=H_{graph}$ and $K,V=H_{scene}$. [abstract] Claims the cross-attention mechanism enables the model to "prioritize relevant environmental factors." [corpus] The corpus generally supports attention mechanisms for fusion, though specific evidence for this exact $Q/K/V$ configuration is restricted to the primary paper.
- **Break condition:** If the scene features are high-dimensional and noisy without proper regularization, the attention mechanism might attend to spurious correlations (overfitting).

### Mechanism 3
- **Claim:** Sparse graph modeling captures local collision-avoidance and group coherence behaviors more efficiently than dense connectivity.
- **Mechanism:** The Interaction Module constructs a "Spatial Sparse Graph" where edges represent significant interactions rather than all-to-all connections. This isolates the most influential neighbors, preventing gradient dilution from irrelevant distant pedestrians.
- **Core assumption:** Assumes pedestrian movement is primarily influenced by immediate neighbors (locality assumption) and that "significant interactions" can be learned or heuristically defined.
- **Evidence anchors:** [abstract] Highlights the use of a "Sparse Graph Convolutional Network (SGCN) for capturing pedestrian interactions." [section 3.4] Describes the construction of spatial and temporal sparse graphs to encode group interactions and motion tendencies. [corpus] "Unified Spatial-Temporal Edge-Enhanced Graph Networks" validates the general efficacy of graph-based spatial-temporal modeling in this domain.
- **Break condition:** In extremely dense crowds where global flow dynamics matter more than local avoidance, a strictly sparse graph might miss macro-level crowd movement patterns.

## Foundational Learning

- **Concept: Semantic Segmentation (OneFormer/Cityscapes)**
  - **Why needed here:** You must understand how the model perceives "walkable" vs. "non-walkable" regions. The model relies on OneFormer to classify pixels; if you don't understand the segmentation classes (e.g., road, sidewalk, building), you cannot debug why the model predicts a specific path.
  - **Quick check question:** Can you explain the difference between raw visual features (ResNet) and semantic features (OneFormer) in the context of this architecture?

- **Concept: Graph Convolutional Networks (GCN) & Sparsity**
  - **Why needed here:** The core social engine is an SGCN. You need to grasp how adjacency matrices represent social connections and why sparsity (removing edges) helps the model focus on strong local interactions rather than noisy global ones.
  - **Quick check question:** How does the adjacency matrix in a Sparse GCN differ from a standard fully-connected attention mechanism regarding computational cost and inductive bias?

- **Concept: Cross-Attention (Query-Key-Value)**
  - **Why needed here:** This is the fusion point. Understanding that the "Query" comes from the pedestrian (social) and the "Key/Value" from the scene (environment) is critical. It explains *how* the environment influences the pedestrian, rather than just concatenating features.
  - **Quick check question:** In the equation $Attention(Q, K, V)$, which modality provides the $Q$ in this paper, and what does that imply about how the model selects scene features?

## Architecture Onboarding

- **Component map:** Input (Trajectories + Frame) -> Feature Extraction (Real-ESRGAN -> OneFormer -> ResNet-18 -> MLP) -> $H_{scene}$; Trajectories -> Sparse Graph Construction -> SGCN -> $H_{graph}$; $H_{graph}$ (Query) + $H_{scene}$ (Key/Value) -> Cross-Attention -> Residual Connection -> $H_{fused}$ -> TCN -> Future Trajectory

- **Critical path:** The data flow relies heavily on the quality of the **Feature Extraction Module**. If Real-ESRGAN or OneFormer fails to produce high-quality semantic maps (e.g., blurring boundaries or misclassifying sidewalks), the cross-attention module will attend to noise, and the TCN will predict invalid paths.

- **Design tradeoffs:**
  - **Performance vs. Speed:** Using Real-ESRGAN and OneFormer adds significant computational overhead compared to models using raw downscaled images, trading inference speed for prediction accuracy (ADE/FDE).
  - **Fixed vs. Trainable Backbone:** The authors keep Real-ESRGAN and OneFormer weights fixed. This saves memory and training stability but prevents the feature extractor from fine-tuning to specific trajectory nuances of the dataset.

- **Failure signatures:**
  - **Segmentation Error Propagation:** If the semantic map mislabels a road as a sidewalk, the model may predict dangerous trajectories into traffic.
  - **Static Assumption:** The scene features are extracted per frame but largely treated as static context; the model may struggle with highly dynamic obstacles not captured by semantic segmentation (e.g., moving construction barriers).

- **First 3 experiments:**
  1. **Reproduce Ablation (Table 2):** Run the model "w/o Maps" vs. "w/ Maps" on a single scene (e.g., ETH) to verify the semantic feature contribution delta.
  2. **Visualize Attention Weights:** Extract the attention map from the Cross-Attention module during a prediction. Verify if high attention weights correspond to logical physical constraints (e.g., attending to a wall the pedestrian is avoiding).
  3. **Noise Robustness Test:** Add Gaussian noise to the input coordinates of the pedestrians to test if the SGCN and TCN can recover smooth trajectories, isolating the stability of the temporal decoder.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would end-to-end fine-tuning of the pre-trained Real-ESRGAN and OneFormer modules improve trajectory prediction accuracy compared to keeping them fixed?
- **Basis in paper:** [inferred] The authors explicitly state they "train the network end-to-end using stochastic gradient descent, updating all parameters except those of the pre-trained Real-ESRGAN and OneFormer modules, which remain fixed."
- **Why unresolved:** The paper does not report experiments with fine-tuned modules, leaving unclear whether scene feature quality could be improved through joint optimization.
- **What evidence would resolve it:** Ablation experiments comparing frozen versus fine-tuned pre-trained modules on the same benchmark datasets.

### Open Question 2
- **Question:** Can the deterministic output be extended to multi-modal trajectory prediction to capture diverse plausible future paths?
- **Basis in paper:** [inferred] The model predicts a single trajectory per pedestrian (Ã‚Y = TCN(Hfused)), whereas many real-world scenarios involve multiple equally plausible paths.
- **Why unresolved:** The paper reports only ADE/FDE for deterministic predictions and does not address the multi-modal nature of pedestrian decision-making.
- **What evidence would resolve it:** Experiments incorporating sampling-based approaches or mixture density outputs, evaluated using multi-modal metrics like Best-of-N ADE/FDE.

### Open Question 3
- **Question:** What is the computational overhead and inference latency introduced by the image enhancement and semantic segmentation pipeline?
- **Basis in paper:** [inferred] The paper evaluates accuracy but provides no analysis of runtime, model size, or computational cost despite employing multiple heavy pre-trained models.
- **Why unresolved:** Autonomous vehicle applications require real-time performance, which the current pipeline with Real-ESRGAN and OneFormer may not satisfy.
- **What evidence would resolve it:** Reporting frames-per-second, latency measurements, and parameter counts across different hardware configurations.

### Open Question 4
- **Question:** How does the model perform on the excluded UNIV scene once frame data becomes available, and does the leave-one-out evaluation hold across all five standard scenes?
- **Basis in paper:** [explicit] "Due to the absence of publicly available frame data for the UNIV scene (specifically the student003 file), we exclude it from frame-based evaluations."
- **Why unresolved:** The standard benchmark protocol includes five scenes, but the paper evaluates only four, leaving the UNIV results as an open gap.
- **What evidence would resolve it:** Complete evaluation on all five ETH/UCY scenes once the missing frame data is obtained or reconstructed.

## Limitations
- Assumes pedestrian movement is primarily governed by static semantic constraints, which may not hold in highly dynamic environments with temporary obstacles.
- Fixed weights for pre-trained feature extraction modules prevent adaptation to dataset-specific characteristics.
- The sparsity assumption in the SGCN may miss global crowd flow patterns in extremely dense scenarios.

## Confidence
- **High confidence:** The general architecture combining SGCN, cross-attention, and TCN is technically sound and aligns with established practices in trajectory prediction. The reported ADE/FDE improvements over baselines are likely real given the ablation study showing consistent gains from semantic features.
- **Medium confidence:** The specific mechanism by which cross-attention prioritizes relevant environmental factors is plausible but not directly visualized or quantified in the paper. The assumption that pre-trained Cityscapes segmentation generalizes to ETH/UCY scenes is reasonable but unverified.
- **Low confidence:** The exact impact of Real-ESRGAN enhancement on OneFormer performance is unclear due to lack of ablations without enhancement. The claim about sparsity capturing "most influential neighbors" lacks quantitative analysis of edge selection criteria.

## Next Checks
1. **Semantic segmentation robustness:** Test OneFormer performance on raw ETH/UCY frames without Real-ESRGAN enhancement to quantify the contribution of image enhancement to overall prediction accuracy.
2. **Attention interpretability:** Extract and visualize cross-attention weights across multiple scenes to verify that high attention values consistently correspond to physically relevant constraints (walls, obstacles, walkable paths).
3. **Computational overhead analysis:** Profile inference time with and without Real-ESRGAN/OneFormer components to quantify the speed-accuracy tradeoff and assess practical deployment viability.