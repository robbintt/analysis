---
ver: rpa2
title: 'LATTS: Locally Adaptive Test-Time Scaling'
arxiv_id: '2509.20368'
source_url: https://arxiv.org/abs/2509.20368
tags:
- latexit
- sha1
- base64
- latts
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LATTS, a method that adaptively allocates
  computation to different reasoning steps in language models using verifier feedback.
  Unlike prior work that treats problems uniformly, LATTS uses acceptance-rejection
  sampling to dynamically adjust effort based on step-level difficulty, allowing more
  trials for harder steps and fallback strategies (stop, max, backtrack, restart)
  when needed.
---

# LATTS: Locally Adaptive Test-Time Scaling

## Quick Facts
- arXiv ID: 2509.20368
- Source URL: https://arxiv.org/abs/2509.20368
- Reference count: 40
- Key outcome: Adaptive computation allocation using verifier feedback achieves superior accuracy-compute tradeoffs

## Executive Summary
LATTS introduces a novel approach to test-time scaling that dynamically allocates computation based on step-level difficulty in reasoning tasks. Unlike traditional methods that apply uniform effort across all problems, LATTS uses acceptance-rejection sampling to determine how many trials to allocate to each reasoning step. The method employs a general-purpose verifier to assess step quality and makes intelligent decisions about when to stop, continue with maximum effort, backtrack, or restart. This adaptive strategy enables LATTS to achieve comparable accuracy to state-of-the-art methods while using up to 50× fewer tokens, particularly excelling on challenging problems like AIME.

## Method Summary
LATTS operates by generating multiple candidate solutions for each reasoning step and using a verifier to evaluate their quality. Based on this feedback, the system decides whether to allocate more computation to a step or move forward. The method implements four key strategies: (1) stopping early when a step is sufficiently solved, (2) applying maximum effort to particularly difficult steps, (3) backtracking when a promising but flawed path is detected, and (4) restarting from scratch when the current path is deemed unpromising. This approach contrasts with traditional uniform scaling methods by recognizing that different steps within the same problem may require vastly different amounts of computation.

## Key Results
- Achieves comparable accuracy to beam search and BoN while using up to 50× fewer tokens on MATH500
- Excels on challenging AIME problems where traditional methods struggle
- Works effectively with general-purpose LLM verifiers, not requiring specialized reward models

## Why This Works (Mechanism)
LATTS succeeds by recognizing that reasoning problems contain heterogeneous steps with varying difficulty levels. Traditional test-time scaling methods waste computation by applying uniform effort across all steps, including those that are trivial. LATTS instead uses verifier feedback to identify which steps truly need additional computation. The acceptance-rejection sampling framework allows the system to allocate more trials to harder steps while quickly accepting solutions for easier ones. This targeted allocation, combined with intelligent fallback strategies (backtracking, restarting, etc.), enables LATTS to find high-quality solutions more efficiently than brute-force approaches.

## Foundational Learning

**Acceptance-rejection sampling**: A statistical technique for generating samples from a target distribution by proposing candidates from a simpler distribution and accepting/rejecting them based on a criterion. *Why needed*: LATTS uses this to decide how many trials to allocate to each step based on verifier feedback. *Quick check*: Verify the acceptance probability formula matches the target distribution's density.

**Step-level reasoning decomposition**: Breaking down complex problems into individual reasoning steps that can be evaluated independently. *Why needed*: LATTS operates at the granularity of individual steps rather than whole solutions. *Quick check*: Confirm that each step has a clear termination condition and verifiable output.

**Verifier-guided decision making**: Using an external model to assess the quality of intermediate reasoning outputs and inform allocation decisions. *Why needed*: LATTS relies on verifier feedback to determine when to stop, backtrack, or allocate more computation. *Quick check*: Test verifier consistency across multiple runs on identical inputs.

## Architecture Onboarding

**Component map**: Problem Decomposition -> Step Generation -> Verifier Evaluation -> Decision Module -> (Stop/Max/Backtrack/Restart) -> Solution Assembly

**Critical path**: Step Generation → Verifier Evaluation → Decision Module → Solution Assembly. The decision module is the bottleneck as it must process verifier feedback and choose among four fallback strategies.

**Design tradeoffs**: LATTS trades computational complexity (managing multiple fallback strategies) for efficiency gains. The method requires careful tuning of acceptance thresholds and decision boundaries. Using general-purpose verifiers sacrifices some precision compared to specialized reward models but gains flexibility.

**Failure signatures**: Poor verifier quality leads to misallocation of computation. Over-aggressive backtracking can cause infinite loops. Insufficient step granularity makes the method ineffective. Excessive restarts indicate fundamental issues with problem decomposition.

**First experiments**:
1. Test LATTS on a simple arithmetic problem with known step difficulty to verify adaptive allocation
2. Evaluate verifier sensitivity by degrading its performance and measuring LATTS decisions
3. Compare LATTS with uniform scaling on problems with clearly identifiable hard/easy steps

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on verifier quality, with limited analysis of failure modes
- Managing multiple fallback strategies introduces computational overhead
- Lack of ablation studies on the relative importance of each fallback mechanism

## Confidence
- High confidence: Adaptive computation allocation strategy and its theoretical grounding
- Medium confidence: Empirical results on MATH500 and AIME datasets
- Medium confidence: General applicability with different verifiers

## Next Checks
1. Test LATTS with intentionally degraded verifiers to quantify robustness boundaries
2. Compare wall-clock time and resource usage against simpler baselines, not just token counts
3. Evaluate LATTS on diverse reasoning tasks beyond mathematical problems to assess generalizability