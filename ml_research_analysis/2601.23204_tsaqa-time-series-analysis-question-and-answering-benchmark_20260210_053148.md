---
ver: rpa2
title: 'TSAQA: Time Series Analysis Question And Answering Benchmark'
arxiv_id: '2601.23204'
source_url: https://arxiv.org/abs/2601.23204
tags:
- series
- time
- question
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TSAQA is a large-scale time series question-answering benchmark
  with 210K samples across 13 domains, covering 6 tasks (anomaly detection, classification,
  characterization, comparison, data transformation, temporal relationship) and 3
  question types (true-or-false, multiple-choice, puzzling). The dataset spans diverse
  domains and employs hierarchical uniform sampling to ensure balanced representation.
---

# TSAQA: Time Series Analysis Question And Answering Benchmark

## Quick Facts
- arXiv ID: 2601.23204
- Source URL: https://arxiv.org/abs/2601.23204
- Reference count: 40
- Primary result: TSAQA is a large-scale time series question-answering benchmark with 210K samples across 13 domains, covering 6 tasks and 3 question types

## Executive Summary
TSAQA is a comprehensive benchmark designed to evaluate large language models' capabilities in time series analysis across diverse domains. The benchmark encompasses 210,000 question-answer pairs spanning 13 domains including finance, healthcare, meteorology, and transportation. It tests six distinct tasks: anomaly detection, classification, characterization, comparison, data transformation, and temporal relationship analysis. The benchmark employs three question formats - true-or-false, multiple-choice, and puzzling questions - to comprehensively assess model reasoning abilities.

The evaluation reveals significant challenges for current LLMs in time series reasoning, with the best commercial model (Gemini-2.5-Flash) achieving only 65.08% accuracy. Instruction-tuned open-source models show improved performance (up to 85.26% with LLaMA-3.1-8B), but puzzling questions and advanced analytical tasks remain particularly difficult. Human evaluation confirms the benchmark's reliability, with high agreement rates on characterization and comparison tasks, validating TSAQA as a valuable tool for advancing time series reasoning capabilities in AI systems.

## Method Summary
TSAQA was constructed using a hierarchical uniform sampling approach to ensure balanced representation across 13 diverse domains and six distinct task categories. The benchmark includes 210K samples with questions covering anomaly detection, classification, characterization, comparison, data transformation, and temporal relationship analysis. Three question types are employed: true-or-false, multiple-choice, and puzzling questions to comprehensively evaluate model reasoning capabilities. The dataset spans domains such as finance, healthcare, meteorology, and transportation, providing real-world context for time series analysis. Zero-shot evaluation was conducted across multiple commercial and open-source LLMs, followed by instruction tuning experiments to assess performance improvements. Human evaluation was performed to validate the reliability and consistency of the benchmark tasks.

## Key Results
- Current LLMs struggle significantly on TSAQA, with best commercial model (Gemini-2.5-Flash) achieving only 65.08% accuracy
- Instruction tuning substantially improves open-source model performance, up to 85.26% accuracy with LLaMA-3.1-8B
- Puzzling questions and advanced analytical tasks show the largest performance gaps, indicating areas needing improvement
- Human evaluation confirms benchmark reliability with 91.2% agreement on characterization and 87.4% on comparison tasks

## Why This Works (Mechanism)
TSAQA works by providing a structured, comprehensive evaluation framework that tests multiple dimensions of time series reasoning. The hierarchical uniform sampling ensures balanced coverage across domains and tasks, preventing models from overfitting to specific patterns. The three question types progressively challenge models from simple factual recognition to complex temporal reasoning. The diverse domain coverage forces models to develop generalizable time series analysis capabilities rather than domain-specific heuristics. The large scale (210K samples) provides sufficient statistical power to distinguish between model capabilities and identify specific weaknesses in temporal reasoning.

## Foundational Learning
**Time series analysis fundamentals** - Understanding temporal patterns, trends, and seasonality is crucial for answering TSAQA questions. Quick check: Can the model identify whether a time series shows increasing, decreasing, or cyclical patterns?

**Temporal reasoning** - The ability to understand cause-and-effect relationships and temporal dependencies in sequential data. Quick check: Can the model determine if an event at time t influences another event at time t+1?

**Statistical pattern recognition** - Identifying anomalies, outliers, and statistical properties in time series data. Quick check: Can the model detect unusual spikes or drops that deviate from established patterns?

**Domain knowledge integration** - Applying contextual understanding from specific domains (finance, healthcare, etc.) to interpret time series data correctly. Quick check: Does the model understand domain-specific terminology and its implications for time series behavior?

**Comparative analysis** - Evaluating differences and similarities between multiple time series simultaneously. Quick check: Can the model identify which time series shows stronger correlation or more pronounced trends?

## Architecture Onboarding

Component map: Data Generation -> Question Formulation -> Task Classification -> Model Evaluation -> Performance Analysis

Critical path: The evaluation pipeline follows: benchmark dataset → model inference → accuracy calculation → task-specific analysis → human validation

Design tradeoffs: TSAQA prioritizes comprehensive coverage over computational efficiency, using large-scale data generation that requires significant processing resources but ensures robust evaluation across diverse scenarios.

Failure signatures: Models typically fail on questions requiring multi-step temporal reasoning, struggle with domain-specific terminology, and show poor performance on puzzling questions that require non-obvious temporal connections.

First experiments:
1. Zero-shot evaluation on commercial LLMs to establish baseline performance across all six task categories
2. Instruction tuning of open-source models to measure improvement potential
3. Human evaluation on random samples to validate question clarity and task consistency

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The benchmark may be artificially difficult, potentially revealing more about question formulation challenges than genuine time series reasoning limitations
- Performance metrics could be influenced by specific model versions and evaluation protocols rather than fundamental capabilities
- Some inherent ambiguity remains in task definitions, as evidenced by the 8-9% disagreement rates in human evaluation

## Confidence
- Major claims about model performance: Medium
- Dataset construction methodology: High
- Human evaluation reliability: Medium
- Benchmark utility for model development: High

## Next Checks
1. Cross-validate performance results using alternative evaluation protocols and different model versions to ensure findings aren't artifacts of specific implementation choices

2. Conduct ablation studies comparing performance on TSAQA questions versus temporally-irrelevant control questions to isolate genuine time series reasoning capabilities from general question-answering proficiency

3. Perform expert review of puzzling question samples to verify they require authentic temporal reasoning rather than pattern matching or memorization