---
ver: rpa2
title: 'HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction'
arxiv_id: '2512.23175'
source_url: https://arxiv.org/abs/2512.23175
tags:
- helm-bert
- linear
- esm-2
- molformer
- peptide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HELM-BERT is the first encoder-based peptide language model trained
  on HELM notation, addressing the limitations of SMILES and amino-acid representations
  for therapeutic peptides. By adopting a hierarchical representation that explicitly
  encodes both monomer composition and connectivity, HELM-BERT captures the chemical
  and topological complexity of modified peptides more effectively than SMILES-based
  models.
---

# HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction

## Quick Facts
- arXiv ID: 2512.23175
- Source URL: https://arxiv.org/abs/2512.23175
- Authors: Seungeon Lee; Takuto Koyama; Itsuki Maeda; Shigeyuki Matsumoto; Yasushi Okuno
- Reference count: 40
- Key outcome: HELM-BERT is the first encoder-based peptide language model trained on HELM notation, addressing the limitations of SMILES and amino-acid representations for therapeutic peptides. By adopting a hierarchical representation that explicitly encodes both monomer composition and connectivity, HELM-BERT captures the chemical and topological complexity of modified peptides more effectively than SMILES-based models. Pre-trained on a corpus of 39,079 diverse peptides, HELM-BERT significantly outperforms state-of-the-art SMILES models in membrane permeability prediction (R² = 0.717) and peptide-protein interaction tasks, while achieving competitive performance with large protein language models. Ablation studies show that disentangled attention is critical for learning HELM representations, and embedding analysis reveals that HELM-BERT encodes macrocyclic topology more effectively than SMILES-based encoders. This work establishes HELM as a powerful representation for peptide language modeling and opens the door for more accurate and data-efficient prediction of peptide properties.

## Executive Summary
HELM-BERT introduces the first encoder-based peptide language model trained on HELM (Hierarchical Editing Language for Macromolecules) notation. HELM notation provides a hierarchical syntax that explicitly encodes both monomer composition and connectivity, addressing the limitations of SMILES and amino-acid representations for therapeutic peptides. By pre-training on a curated corpus of 39,079 diverse peptides, HELM-BERT significantly outperforms SMILES-based models on membrane permeability prediction and peptide-protein interaction tasks while achieving competitive performance with large protein language models. Ablation studies demonstrate that disentangled attention is critical for learning HELM representations, and embedding analysis shows that HELM-BERT encodes macrocyclic topology more effectively than SMILES-based encoders.

## Method Summary
HELM-BERT is a transformer-based encoder pre-trained on HELM notation for peptide property prediction. The model uses a DeBERTa-inspired architecture with 6 layers, hidden size 768, and 12 attention heads. Key innovations include a dictionary-based character tokenizer with semantic compression (78 tokens), a hybrid block combining disentangled self-attention with an n-gram induced encoder (nGiE), and an enhanced mask decoder (EMD) that injects absolute position embeddings at the final stage. Pre-training uses masked language modeling with span masking (15% tokens, geometric p=0.2, span [1,5], 80-10-10 rule) on 39,079 unique peptides from ChEMBL, CycPeptMPDB, and Propedia. Downstream fine-tuning employs 10-fold CV for permeability and 5-fold CV for PPI tasks with a 3-layer MLP head.

## Key Results
- HELM-BERT achieves R² = 0.717 for membrane permeability prediction, significantly outperforming state-of-the-art SMILES models
- The model achieves 99.96±0.02% linear probing accuracy for structure type classification (cyclic, lariat, linear peptides)
- HELM-BERT demonstrates competitive performance with large protein language models despite training on approximately 39,000 peptides versus millions for ESM-2

## Why This Works (Mechanism)

### Mechanism 1: Topology-Explicit Representation Learning
- **Claim:** HELM notation encodes macrocyclic topology more effectively than atom-level SMILES, leading to superior prediction of topology-dependent peptide properties.
- **Mechanism:** HELM uses a hierarchical syntax where the linear monomer sequence is separate from an explicit connection list that defines cyclization. This allows the model to learn topological features directly, rather than inferring them from implicit ring identifiers (numbers) that may be non-local in a SMILES string.
- **Core assumption:** Explicitly encoding connectivity in a separate structure is more learnable by a sequence-based transformer than inferring implicit, non-local numerical pairings in a single long string.
- **Evidence anchors:**
  - [abstract] "HELM-BERT captures the chemical and topological complexity of modified peptides more effectively than SMILES-based models."
  - [Section 3.3] "For Structure Type (cyclic, lariat, and linear peptides), HELM-BERT achieved 99.96±0.02 % linear probing accuracy... [and] t-SNE projections... show clearer separation between cyclic, lariat, and linear peptides."
  - [corpus] Related work on designing cyclic peptides (2505.21452) highlights challenges in modeling the geometric constraints of cyclic peptides.
- **Break condition:** The advantage would diminish if predicting a property not dependent on macrocyclic topology or if the dataset consisted almost entirely of linear, unmodified peptides.

### Mechanism 2: Disentangled Attention for Non-Local Dependencies
- **Claim:** Disentangled attention is a critical architectural component for learning from HELM, better handling non-local dependencies induced by cyclization.
- **Mechanism:** Standard self-attention computes content-to-content similarity. Disentangled attention (from DeBERTa) decomposes the attention score into content-to-content, content-to-position, and position-to-content terms. This is hypothesized to better model how distant monomers in the linear HELM sequence interact due to cyclization, as the relative position term can directly account for these non-local couplings.
- **Core assumption:** The non-local dependencies in HELM sequences (from cyclization) are better captured by explicit relative position terms in attention than by standard self-attention.
- **Evidence anchors:**
  - [abstract] "Ablation studies show that disentangled attention is critical for learning HELM representations."
  - [Section 3.2] "Removing disentangled attention produced the largest performance drop... representing 75% of the total gap [between Vanilla-BERT and HELM-BERT]."
  - [corpus] No direct corpus evidence found on disentangled attention specifically for HELM.
- **Break condition:** If a model used a graph-based architecture that natively handled cyclic connections without relying on sequence position, the need for disentangled attention might be reduced.

### Mechanism 3: Data Efficiency via Monomer-Level Representation
- **Claim:** HELM-BERT is more data-efficient than large protein language models (PLMs), achieving competitive performance with orders of magnitude less pre-training data.
- **Mechanism:** The model operates at the monomer level, which provides a concise representation. This, combined with the explicit representation of chemical modifications in the HELM vocabulary, allows the model to learn richer representations from a smaller, more relevant corpus of therapeutic peptides.
- **Core assumption:** The monomer-level abstraction and the curated, domain-specific pre-training corpus are the primary drivers of data efficiency.
- **Evidence anchors:**
  - [abstract] "...HELM-BERT achieves comparable performance to ESM-2 variants despite being pre-trained on approximately 39,000 peptides, whereas ESM-2 was trained on millions of protein sequences."
  - [Section 3.4] "Remarkably, HELM-BERT achieved comparable performance to ESM-2 variants despite being pre-trained on approximately 39,000 peptides, whereas ESM-2 was trained on millions of protein sequences."
  - [corpus] No direct corpus evidence found comparing data efficiency between HELM-based and PLM-based approaches.
- **Break condition:** Performance on tasks involving highly novel, non-peptidic chemical scaffolds not present in the training data would likely degrade.

## Foundational Learning

- **Concept: HELM (Hierarchical Editing Language for Macromolecules)**
  - **Why needed here:** This is the core input representation. Understanding its structure (monomer sequences plus a connection table) is essential to appreciate why it's superior to SMILES for this task.
  - **Quick check question:** How does HELM encode a macrocyclic connection differently from SMILES?

- **Concept: Disentangled Attention (from DeBERTa)**
  - **Why needed here:** Ablation studies identify this as the most critical architectural component. An engineer must understand how it decomposes attention to grasp its role in handling non-local dependencies.
  - **Quick check question:** What are the three components of the attention score in disentangled attention, and which one is hypothesized to be key for cyclic peptides?

- **Concept: Masked Language Modeling (MLM) with Span Masking**
  - **Why needed here:** This is the pre-training objective. Understanding span masking (masking contiguous tokens) is important, as it differs from standard token-level masking.
  - **Quick check question:** How does the span masking strategy used in HELM-BERT pre-training differ from standard token-level MLM?

## Architecture Onboarding

- **Component map:**
  - Raw HELM string -> Tokenizer (dictionary-based, 78 tokens with semantic compression) -> nGiE layer (Conv1D k=3, Tanh, Dropout) -> Hybrid block (Disentangled Self-Attention + nGiE) -> Transformer layers 2-5 (Disentangled Attention) -> Layer 6 (Transformer with Disentangled Attention) -> Enhanced Mask Decoder (EMD) with absolute position embeddings -> Output

- **Critical path:**
  1. Raw HELM string is tokenized
  2. Tokens are processed by the first hybrid block (nGiE + Disentangled Attention)
  3. The sequence passes through the remaining transformer layers
  4. The EMD takes the final encoder output, adds absolute position information, and refines the representation for masked token prediction or final embedding

- **Design tradeoffs:**
  - HELM vs. SMILES vs. Protein AA: HELM provides explicit topology and modification awareness but is less common. SMILES is universal but obscures topology. Protein AA sequences cannot handle non-canonical residues or explicit cyclization
  - Disentangled Attention: Adds complexity but is empirically shown to be critical for this task
  - Curated corpus vs. massive public data: The 39k peptide corpus is small but highly relevant, trading scale for domain specificity

- **Failure signatures:**
  - Instability during pre-training: Removing disentangled attention leads to higher terminal loss and more epochs to converge
  - Poor performance on linear peptides: The model's advantage is primarily from encoding topology; performance on linear peptides may not show the same margin
  - Inability to handle novel chemical modifications: Novel modifications not in the vocabulary will be mapped to [UNK], potentially degrading performance

- **First 3 experiments:**
  1. Replicate ablation study: Train Vanilla-BERT and the w/o Disentangled Attention variant. Compare their validation loss curves to confirm the paper's findings
  2. Probing task replication: Re-run linear probing for structure type classification. Visualize t-SNE embeddings to observe topology-based clustering
  3. Tokenizer sensitivity test: Experiment with a simpler tokenizer to test how much performance relies on the custom tokenizer's handling of multi-character motifs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectures that explicitly model HELM's compositional semantics, such as graph-based modules, outperform the current transformer encoder?
- Basis in paper: [explicit] The authors state future work should "explore architectures that more explicitly model HELM's compositional semantics—for example, graph-based modules that jointly encode monomer sequences and connectivity graphs."
- Why unresolved: The current DeBERTa-based architecture models HELM sequences but may not fully capture the explicit topological connectivity graphs that HELM notation provides.
- What evidence would resolve it: Comparative benchmarking of a graph-augmented HELM model against the baseline transformer on cyclic peptide permeability and PPI tasks.

### Open Question 2
- Question: How does HELM-BERT performance scale relative to large protein language models when pre-trained on a corpus significantly larger than 39,079 peptides?
- Basis in paper: [explicit] The authors note a "fundamental challenge" regarding the scarcity of HELM annotations and suggest "community efforts toward HELM standardization" could enable larger corpora.
- Why unresolved: HELM-BERT achieves competitive performance with ESM-2 despite training on orders of magnitude less data; its performance ceiling with abundant data remains unknown.
- What evidence would resolve it: Re-training the model on a standardized, expanded HELM dataset (millions of sequences) and measuring performance deltas against current baselines.

### Open Question 3
- Question: Does HELM-BERT maintain advantages over protein language models on peptide-protein interaction tasks involving chemically modified residues?
- Basis in paper: [inferred] The Propedia dataset used for PPI was filtered to include only peptides with natural amino acids, limiting the evaluation of HELM-BERT's specific ability to handle non-canonical modifications.
- Why unresolved: The model's primary architectural advantage is encoding non-canonical residues, but the PPI experiments only validated it on the natural amino acid subset where ESM-2 operates best.
- What evidence would resolve it: Evaluating PPI prediction performance on a dataset containing diverse D-amino acids, N-methylations, and macrocyclization.

## Limitations

- The HELM tokenizer's exact 78-token vocabulary and semantic compression rules are not fully specified, which may affect reproducibility of the pre-training setup.
- The Propedia filtering process mentions removing "non-standard amino acids that could not be automatically converted" but lacks detail on the conversion heuristic, potentially introducing bias in the pre-training corpus.
- While disentangled attention is identified as critical, the exact mechanism by which it handles non-local dependencies from cyclization is not fully elucidated, and the claim relies heavily on ablation studies rather than direct mechanistic analysis.

## Confidence

- **High confidence** in the HELM notation's ability to explicitly encode topology and the resulting performance gains for topology-dependent properties (supported by structure type probing and t-SNE visualization).
- **Medium confidence** in the disentangled attention's role as the critical architectural component (supported by ablation study but lacks direct mechanistic explanation).
- **Medium confidence** in the data efficiency claim (supported by comparison to ESM-2 but lacks direct corpus evidence comparing HELM-based and PLM-based approaches on the same tasks).

## Next Checks

1. Replicate the ablation study by training Vanilla-BERT and w/o Disentangled Attention variants, and compare their validation loss curves to confirm the reported 64% longer training and 70% higher terminal loss for the variant without disentangled attention.
2. Perform a probing task replication for structure type classification, and visualize t-SNE embeddings to observe topology-based clustering and verify the claimed 99.96±0.02% linear probing accuracy.
3. Conduct a tokenizer sensitivity test by experimenting with a simpler tokenizer to assess how much performance relies on the custom tokenizer's handling of multi-character motifs and the 78-token vocabulary.