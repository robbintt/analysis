---
ver: rpa2
title: Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential
  Mobility with Pretrained Transformers
arxiv_id: '2512.07865'
source_url: https://arxiv.org/abs/2512.07865
tags:
- data
- https
- register
- trajectories
- life
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a method for transforming Swedish register
  data into textual life trajectories to address challenges of high cardinality and
  coding inconsistencies. By converting categorical codes into natural language descriptions,
  the researchers created semantically rich texts representing individuals' demographic
  profiles and annual life events (2001-2013).
---

# Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers

## Quick Facts
- arXiv ID: 2512.07865
- Source URL: https://arxiv.org/abs/2512.07865
- Reference count: 40
- Primary result: Qwen3-0.6B achieved 0.732 balanced accuracy and 0.467 AUPRC in predicting residential mobility from Swedish register text trajectories

## Executive Summary
This study introduces a method for transforming Swedish register data into textual life trajectories to address challenges of high cardinality and coding inconsistencies. By converting categorical codes into natural language descriptions, the researchers created semantically rich texts representing individuals' demographic profiles and annual life events (2001-2013). These trajectories were used to predict residential mobility (2013-2017) using various NLP architectures. The approach successfully preserved meaningful information about individual pathways and outperformed static baseline profiles. Transformer-based models, particularly Qwen3, achieved the best performance with 0.732 balanced accuracy and 0.467 AUPRC, demonstrating that semantically rich register data combined with modern language models can substantially advance longitudinal analysis in social sciences.

## Method Summary
The method converts Swedish register data (LISA database) into textual life trajectories by mapping categorical codes to natural language descriptions following year-specific dictionaries. Each trajectory consists of a baseline profile (sex, age, municipality, family relation) plus annual event sentences describing education, employment, occupation, industry, workplace, labor-market region, income, and government support. The resulting texts are tokenized and fed into transformer models (BERT, Qwen3) with partial fine-tuning (only embeddings and classifier head trainable). A 1M sample was used for training with a held-out test set of approximately 350k samples. Models were compared against TF-IDF logistic regression and evaluated using balanced accuracy, AUPRC, F1-macro, and per-class precision/recall metrics.

## Key Results
- Transformer-based models outperformed static baseline profiles, with Qwen3-0.6B achieving the highest AUPRC of 0.467
- Trajectory-based models (Group B) consistently outperformed static profile-only models (Group C) across all metrics
- The textualization approach preserved meaningful sequential information about individual life pathways
- BERT and Qwen partial fine-tuning (trainable embeddings + classifier) achieved competitive results with full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Textualizing categorical codes creates semantically dense representations that mitigate high cardinality and coding inconsistencies. Mapping numeric codes to natural language descriptions leverages pretrained language models' learned semantic similarity. Instead of treating codes as orthogonal one-hot vectors, the model sees that "accounting and bookkeeping" is semantically closer to "financial activities" than to "manufacturing," allowing it to generalize across related categories even when coding schemes change. This works because pretrained models have learned meaningful semantic relationships from their training corpora that transfer to the domain of Swedish register data categories. If the semantic relationships in Swedish administrative categories do not overlap sufficiently with the pretraining corpus, the model's embeddings will not capture relevant similarities.

### Mechanism 2
Transformer-based models capture long-range temporal dependencies in event sequences better than static or frequency-based models. Models like BERT and Qwen use self-attention to relate any two positions in a sequence directly, allowing the model to learn that an event early in the trajectory (e.g., "In 2004, the person has children") may be relevant to predicting mobility years later, even with many intervening events. This works because the predictive signal for residential mobility is encoded in the relationships between events across time, not just in the co-occurrence of recent events or aggregate statistics. If the relevant signals for prediction are purely local (only the most recent year matters) or can be captured by simple aggregates, transformers will overcomplicate the model without performance gains.

### Mechanism 3
Partial fine-tuning (training only embeddings + classifier) provides a computationally efficient way to adapt large models to domain-specific sequences without full fine-tuning. By freezing the transformer backbone and only updating the embedding layer (which maps tokens to the model's input space) and the classification head, the model adapts the input and output interfaces while preserving the general-purpose feature extraction learned during pretraining. This works because the pretrained backbone's representations are sufficiently general to be useful for the new task with only minor adaptations at the input/output boundaries. If the domain shift is large (Swedish registry text is very different from pretraining web text), the frozen backbone may not provide useful features, and performance will lag significantly behind full fine-tuning.

## Foundational Learning

**High Cardinality in Categorical Variables**: The paper tackles variables like industry and occupation codes with hundreds or thousands of distinct values. One-hot encoding these would create massive sparse matrices, making models inefficient and prone to overfitting. Quick check: If you have a "job title" column with 5,000 unique values, why would one-hot encoding be problematic for a neural network?

**Sequence Modeling with Transformers**: The core task is predicting an outcome from a sequence of life events. Understanding how self-attention allows models to weigh the importance of different events across time is crucial. Quick check: How does the self-attention mechanism in a transformer differ from the recurrent processing in an LSTM when handling a long sequence of events?

**Area Under the Precision-Recall Curve (AUPRC)**: The outcome (residential mobility) is imbalanced (only 13.6% are movers). AUPRC focuses on performance for the positive (minority) class, unlike ROC-AUC which can be misleading for imbalanced data. Quick check: For a dataset where only 5% of samples are positive, why might a high ROC-AUC be less informative than a high AUPRC?

## Architecture Onboarding

**Component map**: Swedish register CSVs (codes) → Year-specific lookup tables → Text fragments → Concatenated life trajectory string → BERT/Qwen tokenizer → Token IDs (truncated to max length, e.g., 512 for BERT) → Transformer backbone (frozen) → Pooled output → Classification head (trainable linear layer) → Weighted cross-entropy loss → AdamW optimizer → Evaluation metrics

**Critical path**: 1) Verify correct mapping from codes to text fragments using year-appropriate dictionaries (coding schemes change over time) 2) Tokenize and check max sequence length (99% of trajectories < 496 tokens for BERT; longer sequences will be truncated) 3) Run partial fine-tuning with embeddings + classifier trainable, backbone frozen 4) Evaluate on held-out test set using AUPRC as the primary metric for this imbalanced task

**Design tradeoffs**: Textualization vs. One-hot/Embedding Layers: Textualization leverages pretrained semantics but introduces longer sequences and tokenization overhead. Direct embedding layers are faster but don't handle coding changes or semantic similarity as naturally. Full vs. Partial Fine-tuning: Full fine-tuning may yield better performance but risks catastrophic forgetting and requires more compute. Partial tuning is efficient and works well for BERT/Qwen here. Model Size (DistilBERT vs. BERT vs. Qwen): Larger models (Qwen) achieve higher AUPRC but require more GPU memory. Compact models (DistilBERT) are faster but may lose some precision on the minority class.

**Failure signatures**: Very low precision for minority class: Model over-predicts movers; may need better class weighting or more focused features. Static profile model performing as well as trajectory model: Temporal dynamics not adding information; check if trajectory construction is flawed (e.g., missing events). Tokenization truncation warnings: Trajectories exceeding max sequence length may lose early events; verify distribution of token lengths.

**First 3 experiments**: 1) Replicate the TF-IDF logistic regression baseline on the same 1M sample to establish a benchmark (target: ~0.409 AUPRC as per Table 5) 2) Implement partial fine-tuning of a BERT-base model (trainable embeddings + classifier) on trajectory data and evaluate AUPRC (target: ~0.461) 3) Compare against a static profile-only model (no temporal events) to quantify the contribution of sequential information (expect drop in performance to ~0.348 AUPRC)

## Open Questions the Paper Calls Out
1. Would incorporating relational and geographic dependencies (e.g., ties to partners, workplaces, local labor markets) substantially improve residential mobility predictions? The study deliberately simplified modeling by excluding network and spatial structure, leaving unknown how much predictive signal exists in these relational factors.

2. How does the textualization approach perform on register data from other countries with different coding schemes and coverage levels? The authors note Swedish registers are uniquely comprehensive and that "few countries maintain longitudinal microdata with comparable coverage and precision," implying generalizability is untested.

3. Can the textual life-trajectory approach effectively predict other longitudinal social outcomes beyond residential mobility? The authors state their framework "provides a foundation for interpretable and scalable life-course modeling," suggesting broader applicability, but only mobility was tested.

## Limitations
- Performance gains over simpler models are modest relative to computational overhead of transformer models
- Analysis is limited to Swedish registry data with specific coding schemes, generalizability to other countries untested
- Partial fine-tuning approach may not fully leverage semantic richness of textualized trajectories
- Sequential information contribution beyond aggregate features is not quantified

## Confidence
- **High Confidence**: Methodological framework for converting register codes to text and basic performance metrics are well-documented and reproducible. Finding that transformers outperform static baselines is robust across multiple model architectures.
- **Medium Confidence**: Claim that textualization specifically addresses high cardinality and coding inconsistencies is mechanistically sound but not directly tested against alternative encodings. Superiority of transformer architectures over simpler models is demonstrated but effect size is modest.
- **Low Confidence**: Assertion that partial fine-tuning is optimal for this task lacks comparison with full fine-tuning or other adaptation strategies. Paper does not address whether sequential information truly adds value beyond what could be captured through aggregate features.

## Next Checks
1. **Direct Encoding Comparison**: Implement an alternative pipeline using learned embeddings for categorical variables (rather than textualization) and compare performance directly with the textual approach. This would validate whether the semantic richness claim is unique to the text-based method or could be achieved through other means.

2. **Sequential Information Value Test**: Train a model using only static demographic profiles and another using aggregate statistics from the trajectory (e.g., number of moves, education changes, employment duration) without temporal ordering. Compare these against the full sequential model to quantify exactly how much the temporal sequence contributes beyond simple event counts.

3. **Cross-Dataset Generalizability**: Apply the exact methodology to a different longitudinal dataset (e.g., U.S. Census data or Danish registers) with different coding schemes and demographic patterns. This would test whether the approach generalizes beyond Swedish administrative categories or is specifically tuned to their semantic structure.