---
ver: rpa2
title: 'DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency
  Controlled Pre-training'
arxiv_id: '2509.14642'
source_url: https://arxiv.org/abs/2509.14642
tags:
- time
- series
- temporal
- modeling
- decop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeCoP introduces a dependency-controlled pre-training framework
  that improves time series representation learning by explicitly modeling dynamic
  and multi-scale temporal dependencies. It uses Instance-wise Patch Normalization
  (IPN) to stabilize input distributions by preserving local semantic features while
  mitigating distribution shifts, and a hierarchical Dependency Controlled Learning
  (DCL) strategy that adaptively captures both short- and long-term dependencies across
  temporal scales.
---

# DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training

## Quick Facts
- arXiv ID: 2509.14642
- Source URL: https://arxiv.org/abs/2509.14642
- Reference count: 38
- Primary result: Achieves 3% MSE improvement on ETTh1 over PatchTST using 37% of FLOPs

## Executive Summary
DeCoP introduces a dependency-controlled pre-training framework for time series representation learning that explicitly models dynamic and multi-scale temporal dependencies. The approach combines Instance-wise Patch Normalization (IPN) to stabilize input distributions, a hierarchical Dependency Controlled Learning (DCL) strategy for capturing temporal dependencies across scales, and an Instance-level Contrastive Module (ICM) for global generalization. DeCoP achieves state-of-the-art performance on ten datasets, demonstrating significant improvements in both forecasting and classification tasks while maintaining computational efficiency.

## Method Summary
DeCoP processes time series through patch-based tokenization followed by three core components: IPN that balances patch-level and instance-level statistics to preserve local semantics while mitigating distribution shifts, DCL that progressively expands temporal receptive fields across encoder blocks to capture both short and long-term dependencies, and ICM that generates denoised positive pairs via frequency-domain filtering for contrastive learning. The framework employs random masking for reconstruction tasks and frequency filtering for contrastive objectives, with the final representations used for downstream forecasting and classification tasks.

## Key Results
- Achieves 3% MSE improvement on ETTh1 over PatchTST while using only 37% of FLOPs
- Outperforms baselines on ten datasets including ETT, Exchange, Traffic, Weather, and Human Activity datasets
- Demonstrates strong transfer learning capabilities with 9.53% accuracy improvement on SleepEEG→FD-B classification task

## Why This Works (Mechanism)

### Mechanism 1: Instance-wise Patch Normalization (IPN)
- Balances patch-level and instance-level statistics through learnable parameter α
- Preserves local semantic features (peaks, transitions) while stabilizing input distributions against temporal shifts
- Core assumption: Local intra-patch variations contain meaningful semantic signals; temporal distribution shifts are primarily instance-level phenomena

### Mechanism 2: Hierarchical Dependency Controlled Learning (DCL)
- Progressive expansion of temporal receptive fields across encoder blocks
- Captures both short-term fluctuations and long-term trends without spurious correlations
- Core assumption: Temporal dependencies are non-uniform—short-range patterns require narrow context, long-range patterns need broad context

### Mechanism 3: Instance-level Contrastive Module (ICM)
- Frequency-domain filtering removes time-variant noise to generate stable positive pairs
- Enhances global semantic learning for classification tasks by learning instance-discriminative representations
- Core assumption: Time-invariant frequency components encode stable semantic patterns; high-amplitude time-variant components represent noise

## Foundational Learning

- **Concept**: Patch-based time series tokenization
  - Why needed here: DeCoP operates on patches (segments of P timesteps) rather than individual timesteps, requiring understanding of how patching converts temporal sequences into token-like representations
  - Quick check question: Given a sequence of length L=100, patch size P=12, stride S=12, how many patches are generated? (Answer: N=⌊(100-12)/12⌋+2=9)

- **Concept**: Distribution shift in non-stationary time series
  - Why needed here: The core motivation for IPN is that train and test distributions diverge over time; understanding this explains why instance normalization alone fails
  - Quick check question: Why does applying the same normalization statistics across all patches cause over-smoothing? (Answer: Patches at different temporal positions have different local distributions; global statistics suppress local peaks)

- **Concept**: Contrastive learning with positive/negative pairs
  - Why needed here: ICM generates positive pairs via frequency filtering; understanding contrastive objectives explains why minimizing L_cl encourages representation alignment
  - Quick check question: In ICM, what makes (X_p, X̃_p) a positive pair versus a negative pair? (Answer: They are augmented views of the same instance—X̃_p is the denoised version of X_p—so they should have similar representations)

## Architecture Onboarding

- **Component map**:
  Input [L] → Patching [N×P] → IPN [N×P normalized] → ICM FFT-filtering → (X_p, X̃_p)
       ↓
  Random Masking → (masked X_p, masked X̃_p)
       ↓
  Linear Projection + Positional Encoding [N×D]
       ↓
  DCL Block 1 (window W_1) → DCL Block 2 (window W_2) → ... → Z_k [N×D]
       ↓
  Reconstruction Head [N×P] → L_recon
  Contrastive Loss (Z_e · ÃZ_e) → L_cl

- **Critical path**:
  1. IPN's α parameter controls local/global balance—monitor its convergence during pre-training
  2. DCL window sizes must align with data periodicity—start with (2,5) for hourly data with patch_size=12
  3. ICM's β parameter controls filtering intensity—default 0.3; validate that filtered noise has zero mean

- **Design tradeoffs**:
  - IPN α: Low α (≈0) = more instance normalization (stable but over-smoothed); high α (≈1) = more patch normalization (preserves local semantics but less stable)
  - DCL window sizes: Larger windows capture longer dependencies but increase parameters (2,5→999K params vs 42,42→88.8M params)
  - ICM β: Higher β removes more frequencies (cleaner positive pairs but risk of information loss)

- **Failure signatures**:
  - Training loss converges but validation loss diverges: Check IPN—may be over-normalizing (α→0) or under-normalizing (α→1)
  - Classification performance poor but forecasting good: ICM may not be generating meaningful positive pairs—visualize frequency spectrum
  - High FLOPs with poor performance: Window sizes too large; reduce to (2,5) or (4,8)

- **First 3 experiments**:
  1. Ablation of IPN vs Instance Normalization: Train with IPN (α=0.01) vs pure IN (α=0) on ETTh1; measure MSE gap and visualize patch distributions
  2. DCL window size sensitivity: Sweep window configurations from Table 5 on ETTm1; plot MSE vs parameters to validate the (2,5) optimal point
  3. ICM frequency filtering visualization: Apply ICM to a single ETTh1 sample; plot original spectrum, filtered spectrum, and reconstructed signal to confirm time-variant component removal

## Open Questions the Paper Calls Out
None

## Limitations
- The dependency-controlled pre-training framework's effectiveness relies heavily on proper tuning of the learnable parameter α in IPN and window sizes in DCL, which may not generalize across all time series domains without task-specific calibration
- The ICM's frequency-domain filtering assumes that high-amplitude components represent noise, but this may not hold for all time series where high-amplitude patterns are meaningful (e.g., spike detection in medical signals)

## Confidence
- **High confidence** in IPN's stabilizing effect: The dual-statistic normalization mechanism is theoretically sound, and ablation studies provide strong empirical support for its contribution to reducing MSE
- **Medium confidence** in DCL's hierarchical dependency modeling: While multi-scale temporal modeling is well-motivated and supported by related work, the specific window size configurations may be overfit to the tested datasets' characteristics
- **Medium confidence** in ICM's frequency-based contrastive learning: The approach is novel and theoretically justified, but the assumption that frequency filtering reliably isolates time-invariant features needs broader validation

## Next Checks
1. Cross-domain generalization test: Apply DeCoP to datasets with fundamentally different characteristics (e.g., financial time series with sharp transitions vs. smooth physiological signals) to verify whether the (2,5) window configuration remains optimal or requires adaptation
2. Frequency filtering ablation: Systematically vary the filtering intensity β in ICM and visualize the preserved vs. removed frequency components to confirm that meaningful signal features aren't being discarded as noise
3. Dependency modeling scalability: Test DCL with varying numbers of encoder blocks and window size progressions (e.g., 3 blocks with windows 2→4→8) to determine if the hierarchical approach scales effectively to very long sequences (>10K timesteps)