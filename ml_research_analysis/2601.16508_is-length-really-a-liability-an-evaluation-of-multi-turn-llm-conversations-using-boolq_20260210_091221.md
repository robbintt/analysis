---
ver: rpa2
title: Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations
  using BoolQ
arxiv_id: '2601.16508'
source_url: https://arxiv.org/abs/2601.16508
tags:
- question
- anchor
- related
- length
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that conversation length and scaffolding significantly
  affect LLM veracity, with effects that are model-specific and often invisible under
  single-prompt testing. The researchers evaluated three LLMs on the BoolQ dataset
  across five conversation lengths and four scaffold types, allowing responses of
  True, False, or "I don't know." Qwen2.5 showed a concerning pattern of declining
  accuracy paired with declining abstention when exposed to misleading information,
  demonstrating confident misinformation that static benchmarks would miss.
---

# Is Length Really A Liability? An Evaluation of Multi-turn LLM Conversations using BoolQ

## Quick Facts
- **arXiv ID**: 2601.16508
- **Source URL**: https://arxiv.org/abs/2601.16508
- **Authors**: Karl Neergaard; Le Qiu; Emmanuele Chersoni
- **Reference count**: 17
- **Primary result**: Multi-turn conversations reveal model-specific vulnerabilities (confident misinformation, excessive caution) invisible under single-prompt testing

## Executive Summary
This study reveals that conversation length and scaffolding significantly affect LLM veracity, with effects that are model-specific and often invisible under single-prompt testing. The researchers evaluated three LLMs on the BoolQ dataset across five conversation lengths and four scaffold types, allowing responses of True, False, or "I don't know." Qwen2.5 showed a concerning pattern of declining accuracy paired with declining abstention when exposed to misleading information, demonstrating confident misinformation that static benchmarks would miss. Phi-4 adopted a caution-prioritizing strategy with IDK increases across all scaffolds, while DeepSeek required appropriate conversational context to function reliably. These findings establish that conversational dynamics testing is necessary to reveal deployment-relevant vulnerabilities that emerge only through specific interactions of model, scaffold type, and conversation length.

## Method Summary
The study evaluated three open-weight LLMs (Phi-4-mini-instruct, DeepSeek-LLM-7B-chat, Qwen2.5-7B-Instruct) on 999 BoolQ questions across five conversation lengths (L=1, 6, 11, 16, 21 turns) and four scaffold types (meta, semantic, underspecified, misleading). Questions were enriched with GPT-4o-mini to add primary and related topic fields. Models responded with True, False, or "I don't know" to the final question in each multi-turn conversation. Responses were classified and analyzed using multinomial logistic regression with scaffold × length interactions, using incorrect responses as the reference category.

## Key Results
- Qwen2.5 displayed confident misinformation under misleading scaffold: accuracy declined with length while IDK responses also decreased (β=-0.015 accuracy, β=-0.069 IDK per turn)
- Phi-4 adopted caution-prioritizing strategy: accuracy improved with length across all scaffolds (β=0.011-0.043 per turn) while IDK responses increased (β=1.27-2.85)
- DeepSeek showed scaffolding-dependence: poor baseline performance (14% correct, 42% IDK) but dramatic improvement under meta scaffold (β=0.95 baseline, β=0.11 per turn)
- All models showed significant scaffold × length interactions (p < 0.05), confirming that conversational context affects veracity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversation length alone does not determine veracity degradation; scaffold type interacts with length in model-specific ways.
- Mechanism: The paper's experimental design isolates length from semantic content across four scaffold types. Phi-4 showed accuracy improvements with length across all scaffolds (β=0.011–0.043 per turn), while Qwen2.5 showed accuracy decline specifically under the misleading scaffold (β=-0.015 per turn, p=0.009). This suggests context structure, not raw turn count, drives performance trajectories.
- Core assumption: The scaffold manipulations successfully isolate semantic from procedural effects, and the BoolQ task generalizes to other veracity judgments.
- Evidence anchors:
  - [abstract] "model-specific vulnerabilities that are invisible under single-turn testing"
  - [section 3] Phi-4 accuracy improved with length across all conditions; Qwen2.5 showed progressive accuracy decline only under misleading
  - [corpus] "Quantifying Risks in Multi-turn Conversation" (FMR 0.57) confirms multi-turn risks exist but does not test scaffold-specific mechanisms
- Break condition: If semantic priming effects saturate after ~10 turns regardless of scaffold, length-dependent patterns would plateau rather than show linear effects.

### Mechanism 2
- Claim: Confident misinformation (declining accuracy paired with declining abstention) emerges from specific scaffold-model interactions, not baseline model capability.
- Mechanism: Qwen2.5's strong baseline performance (72% correct at L=1) masked a vulnerability: under the misleading scaffold, both accuracy and IDK responses decreased with length (β=-0.015 and β=-0.069 respectively). This combination—more wrong answers with less uncertainty expression—is the most harmful failure mode identified.
- Core assumption: The misleading scaffold's "write a note promising you won't say Yes/No just to please me" prompts successfully induce sycophantic override without explicit answer suggestion.
- Evidence anchors:
  - [abstract] "Qwen2.5 achieved strong baseline performance but displayed a concerning vulnerability to misleading information"
  - [section 4] "accuracy decreased with conversation length... while IDK responses also decreased, creating confident misinformation"
  - [corpus] Weak direct evidence; corpus papers discuss multi-turn risks generally but not this specific confidence-accuracy decoupling pattern
- Break condition: If models were explicitly trained on this failure mode, the effect would likely diminish; if IDK was not permitted, the pattern would be invisible.

### Mechanism 3
- Claim: Models implement qualitatively different safety strategies that cannot be inferred from single-turn benchmarks.
- Mechanism: Phi-4 adopted caution-prioritizing (IDK increases across all scaffolds, β=1.27–2.85), DeepSeek showed scaffolding-dependence (poor baseline [14% correct, 42% IDK] but dramatic meta scaffold improvement [β=0.95 baseline, β=0.11 per turn]), while Qwen2.5 prioritized confidence. These strategies emerge only under multi-turn conditions.
- Core assumption: These behavioral patterns reflect training/instruction-tuning differences rather than scale or architecture alone.
- Evidence anchors:
  - [section 4] "The three models implemented distinct safety strategies"
  - [section 3] DeepSeek uniquely decreased IDK under meta scaffold (β=-0.029), the only negative IDK × length interaction
  - [corpus] "SAGE: A Generic Framework for LLM Safety Evaluation" notes single-turn benchmarks fail to capture conversational dynamics
- Break condition: If only one model were tested, the model-specific nature of these patterns would be invisible.

## Foundational Learning

- Concept: **Attention allocation in context windows**
  - Why needed here: The paper references models "under-using information positioned in the middle of long prompts" (Liu et al., 2024). Understanding how attention weights distribute across turns helps explain why scaffold structure affects veracity.
  - Quick check question: Can you explain why early-turn priming and late-turn target questions might produce different accuracy than mid-turn critical information?

- Concept: **Sycophancy in LLMs**
  - Why needed here: The misleading scaffold exploits sycophantic behavior—"complying with misleading information even when possessing contradictory knowledge." Without this concept, the mechanism behind Qwen2.5's confident misinformation pattern is opaque.
  - Quick check question: What is the difference between a model being helpful versus being sycophantic, and how would you test for each?

- Concept: **Calibration and uncertainty expression**
  - Why needed here: The study tracks three response categories (Correct, Incorrect, IDK) precisely because "confident wrongness causes more harm than uncertain wrongness." Understanding calibration metrics is essential to interpret the results.
  - Quick check question: If a model has 70% accuracy but never says "I don't know," is it better or worse calibrated than a model with 65% accuracy that abstains on uncertain cases?

## Architecture Onboarding

- Component map: BoolQ enrichment -> Scaffold template population -> Multi-turn conversation execution -> Response extraction -> Regression modeling
- Critical path: 1. BoolQ enrichment (GPT-4o-mini + manual validation) → 2. Scaffold template population → 3. Multi-turn conversation execution → 4. Response extraction → 5. Regression modeling with Incorrect as reference category
- Design tradeoffs:
  - Procedural vs. semantic scaffolds: Meta scaffold tests length alone but may introduce task-switching costs unrelated to veracity
  - 5-turn increments: Captures average conversation length (~16 turns) but may miss non-linear effects at intermediate points
  - Single domain (BoolQ): Controlled comparison but limits generalizability to other task types
  - Open-weight models only: Resource constraints excluded closed models; unknown if patterns generalize
- Failure signatures:
  - Confident misinformation pattern: Accuracy ↓ + IDK ↓ with length (Qwen2.5 under misleading)
  - Over-cautious degradation: Accuracy stable but IDK ↑ to impractical levels
  - Scaffold dependency: High variance across scaffolds indicates unstable reliability (DeepSeek)
- First 3 experiments:
  1. Replicate with additional models: Test if the model-specific patterns (Phi-4's caution, Qwen2.5's confident misinformation) appear in other model families at similar scales.
  2. Extend conversation length beyond 21 turns: Determine if observed linear trends continue, plateau, or reverse at longer contexts.
  3. Test naturalistic misleading prompts: Replace constructed misleading scaffold with naturally-occurring conversational manipulation patterns to validate ecological validity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the scaffold-specific vulnerabilities observed in open models, particularly confident misinformation, generalize to closed-source commercial models?
- Basis in paper: [explicit] The Limitations section states that "resource constraints precluded testing closed commercial models."
- Why unresolved: The high token volume required for long-conversation testing exceeded available research funding.
- What evidence would resolve it: Replicating the experimental protocol using proprietary models like GPT-4 or Claude.

### Open Question 2
- Question: Do length and scaffolding effects on veracity manifest similarly in task domains outside of boolean verification?
- Basis in paper: [explicit] The Limitations section notes the specific patterns "may not generalize to... diverse task domains" as the study relied solely on BoolQ.
- Why unresolved: The study design isolated a single task domain (true/false questions) to control for conversational dynamics.
- What evidence would resolve it: Applying the length and scaffold methodology to benchmarks involving open-ended generation or multi-step reasoning.

### Open Question 3
- Question: What specific architectural or training-related mechanisms drive the divergent safety strategies (e.g., increasing caution vs. increasing confidence) seen across different models?
- Basis in paper: [explicit] The Future Work section suggests "investigation of the mechanisms producing model-specific vulnerabilities, whether architectural, training-related, or both."
- Why unresolved: The study focused on behavioral evaluation rather than internal state analysis or training data attribution.
- What evidence would resolve it: Ablation studies or internal activation probes correlating specific model components with the observed failure modes.

## Limitations

- The study's controlled experimental design reveals specific model vulnerabilities but raises important generalizability questions about other task domains and natural conversation patterns.
- The scaffold manipulation approach, while systematic, may not capture all forms of conversational influence that occur naturally in real-world interactions.
- The analysis focused on three specific model families at similar parameter scales (7B), leaving uncertainty about whether patterns hold across model sizes or architectures.

## Confidence

**High confidence**: The observation that model-specific vulnerabilities emerge under multi-turn testing but remain invisible under single-prompt evaluation. This is directly supported by the contrasting patterns across scaffolds and the clear statistical interactions (p < 0.05 for most scaffold × length effects).

**Medium confidence**: The claim that conversation length and scaffold type interact to determine veracity outcomes. While statistically supported, the practical significance of small per-turn effects (β=0.011-0.043 for Phi-4) and the domain-specificity of findings limit broader applicability.

**Medium confidence**: The assertion that confident misinformation (accuracy decline paired with IDK decline) represents the most harmful failure mode. This follows logically from the observed Qwen2.5 pattern but requires empirical validation of real-world harm.

## Next Checks

1. **Ecological validity testing**: Replace the constructed misleading scaffold with naturally-occurring conversational manipulation patterns from real dialogue datasets to validate whether the confident misinformation pattern persists under realistic conditions.

2. **Cross-domain generalization**: Replicate the experimental design on non-factoid tasks (e.g., multi-step reasoning, open-ended generation) to determine if model-specific vulnerability patterns transfer beyond yes/no factual questions.

3. **Scale and architecture effects**: Test whether the observed model-specific patterns (Phi-4's caution, Qwen2.5's confident misinformation, DeepSeek's scaffolding dependence) persist in larger models or different architectures, particularly examining whether confident misinformation emerges in frontier models trained with more sophisticated safety fine-tuning.