---
ver: rpa2
title: Explainable Lane Change Prediction for Near-Crash Scenarios Using Knowledge
  Graph Embeddings and Retrieval Augmented Generation
arxiv_id: '2501.11560'
source_url: https://arxiv.org/abs/2501.11560
tags:
- lane
- vehicle
- risky
- changes
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting risky lane changes,
  a significant cause of road traffic accidents, which existing research often overlooks.
  The authors developed the CRASH dataset, a customized collection of risky lane-changing
  scenarios recreated in the CARLA simulator with detailed numerical data.
---

# Explainable Lane Change Prediction for Near-Crash Scenarios Using Knowledge Graph Embeddings and Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2501.11560
- Source URL: https://arxiv.org/abs/2501.11560
- Reference count: 19
- Primary result: 91.5% F1-score for risky lane changes, 90.0% F1-score for safe lane changes with up to 4-second anticipation.

## Executive Summary
This paper addresses the challenge of predicting risky lane changes, a significant cause of road traffic accidents, using an interpretable machine learning approach. The authors developed the CRASH dataset of risky lane-changing scenarios in the CARLA simulator and proposed a four-stage pipeline combining Knowledge Graph (KG) embeddings, Bayesian inference, and Retrieval Augmented Generation (RAG) for explainability. The model achieves strong performance metrics while providing transparent, natural language explanations for its predictions, enabling automated vehicles to anticipate and react to dangerous maneuvers.

## Method Summary
The approach converts numerical sensor data into linguistic categories, structures them as KG triples, and embeds these relationships using TransE. Bayesian inference on the learned embeddings provides probabilistic predictions with traceable reasoning, while RAG with GPT-4 generates natural language explanations grounded in verifiable KG triples. The system was validated in CARLA simulator, demonstrating ability to anticipate sudden lane changes and provide sufficient reaction time for automated vehicles.

## Key Results
- Achieved 91.5% F1-score for predicting risky lane changes
- Achieved 90.0% F1-score for predicting safe lane changes
- Both predictions made with up to 4-second anticipation time
- Successfully validated in CARLA simulator with automated vehicle reaction

## Why This Works (Mechanism)

### Mechanism 1
Converting numerical sensor data into linguistic categories and structuring them as KG triples creates semantic representation that improves generalization for rare risky maneuvers. The system discretizes continuous inputs into ontology-defined concepts, forming triples embedded with TransE to learn relational logic rather than statistical correlations.

### Mechanism 2
Bayesian inference on learned KG embeddings provides transparent probabilistic predictions where influence of specific input events can be explicitly traced. The system calculates P(h|e) using embeddings, factoring probability into P(e|h) and P(h), allowing inspection of which specific risk factor drove the prediction.

### Mechanism 3
Grounding LLM explanations in a private vector database of KG triples (RAG) mitigates hallucination and ensures explanations are verifiable against actual input data. The system chunks KG triples, embeds them in vector store, and retrieves specific chunks relevant to current query to constrain LLM reasoning.

## Foundational Learning

- **Knowledge Graph Embeddings (KGE)**: Understanding how TransE optimizes relationships between entities is crucial since the model processes relational links rather than raw time-series data. *Quick check*: How does the model handle triples that don't exist in training data?

- **Bayesian Inference vs Discriminative Classification**: The paper claims "transparency" because it can inspect priors and likelihoods, unlike standard neural network classifiers. *Quick check*: What mathematical step determines that "High Risk Preceding" contributed more than "Left Motion" to the final prediction?

- **Retrieval Augmented Generation (RAG)**: This is the explainability layer that queries a specific private dataset rather than relying on generic LLM knowledge. *Quick check*: Why is chunking KG triples necessary before feeding them to the LLM?

## Architecture Onboarding

- **Component map**: Input Layer (HighD/CRASH Datasets) -> Linguistic Translator (Ontology terms) -> Graph Engine (Ampligraph TransE) -> Inference Engine (Bayesian module) -> Explanation Layer (Chroma VectorDB + GPT-4)

- **Critical path**: The Linguistic Translator. If numerical-to-linguistic thresholds are set incorrectly, the KG is populated with false relationships, making subsequent Bayesian inference and RAG explanations mathematically sound but factually wrong.

- **Design tradeoffs**: 
  - Interpretability vs Precision: Trades high precision of raw numerical inputs for explainability of linguistic buckets
  - Latency: Prediction is fast, but adding RAG/LLM loop for explanations introduces significant latency (1-2s for GPT-4)

- **Failure signatures**:
  - "Safe" bias if risky samples are under-represented
  - Threshold drift causing sudden jumps in prediction probability
  - Sparse or contradictory RAG chunks leading to confabulated explanations

- **First 3 experiments**:
  1. Threshold Sensitivity Analysis: Perturb inputs around linguistic thresholds to check for erratic prediction flips
  2. Risky vs Safe Ablation: Train only on HighD and test on CRASH to quantify performance drop
  3. RAG Retrieval Validation: Verify retrieved chunks match scenario features and LLM explanations align with facts

## Open Questions the Paper Calls Out
- How can the prediction model be integrated into the ego vehicle's decision-making process to execute advanced reactive actions beyond simple deceleration?
- How does the model's performance scale and generalize when trained on a significantly larger dataset of risky maneuvers compared to the 50 samples currently available?
- What is the quantitative performance drop when applying the simulation-trained model to real-world noisy sensor data?

## Limitations
- Dataset Scale and Diversity: CRASH dataset contains only 50 risky lane-change samples, potentially limiting generalization
- Linguistic Threshold Ambiguity: Exact numerical boundaries for categories beyond TTC are not explicitly provided
- RAG Explanation Reliability: Similarity search may retrieve irrelevant or contradictory chunks, risking confabulation

## Confidence
- **High Confidence**: Bayesian inference mechanism for tracing predictions is well-detailed and mathematically sound
- **Medium Confidence**: RAG-based explanation generation is described but lacks specific implementation details
- **Low Confidence**: Generalization to real-world aggressive driving scenarios not covered in CRASH dataset

## Next Checks
1. Perform threshold sensitivity analysis by perturbing numerical inputs around linguistic boundaries
2. Conduct dataset ablation test by training only on HighD and testing on CRASH
3. Validate RAG retrieval quality by manually verifying retrieved chunks and LLM explanations for known scenarios