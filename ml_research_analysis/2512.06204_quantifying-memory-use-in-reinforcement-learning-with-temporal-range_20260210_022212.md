---
ver: rpa2
title: Quantifying Memory Use in Reinforcement Learning with Temporal Range
arxiv_id: '2512.06204'
source_url: https://arxiv.org/abs/2512.06204
tags:
- temporal
- range
- memory
- copyk
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Temporal Range, a model-agnostic metric that
  quantifies how much a trained reinforcement learning policy relies on past observations.
  The metric computes a magnitude-weighted average look-back by aggregating Jacobian
  norms of the policy output with respect to historical inputs, providing a scalar
  measure of memory dependence for each sequence.
---

# Quantifying Memory Use in Reinforcement Learning with Temporal Range

## Quick Facts
- arXiv ID: 2512.06204
- Source URL: https://arxiv.org/abs/2512.06204
- Reference count: 14
- This paper introduces Temporal Range, a model-agnostic metric that quantifies how much a trained reinforcement learning policy relies on past observations.

## Executive Summary
This paper introduces Temporal Range, a model-agnostic metric that quantifies how much a trained reinforcement learning policy relies on past observations. The metric computes a magnitude-weighted average look-back by aggregating Jacobian norms of the policy output with respect to historical inputs, providing a scalar measure of memory dependence for each sequence. The authors validate Temporal Range across four architectures (LEM, GRU, LSTM, LinOSS) on POPGym diagnostic and control tasks. Results show Temporal Range remains near zero in fully observed control, scales with ground-truth lag in Copy-k tasks, and aligns with minimum history windows required for near-optimal performance as confirmed by window ablations.

## Method Summary
The Temporal Range metric measures the average temporal dependency of a trained policy by computing a magnitude-weighted average look-back using Jacobian norms. For a sequence of observations, the method calculates the Jacobian of the policy output at each time step with respect to all past inputs, aggregates these gradients into a weight profile, and computes the scalar memory range. The metric is model-agnostic, computationally efficient (requiring only one reverse-mode automatic differentiation pass per sequence), and invariant to uniform input/output rescaling.

## Key Results
- Temporal Range remains near zero on fully observed control tasks like CartPole, indicating no unnecessary memory dependence
- The metric scales correctly with ground-truth lag in Copy-k tasks, matching the required memory window
- Different architectures show varying memory patterns: GRU uses shorter history (~2 steps) on near-Markov tasks while LEM and LSTM use longer history (~10 steps) despite similar performance

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Temporal Attribution
The magnitude of Jacobian blocks (||∂y_s/∂x_t||_mat) serves as a proxy for the influence of past observations on current policy outputs. By locally linearizing the policy network, the method treats the norm of the gradient of the output at step s with respect to input at step t as a "weight" of temporal dependence. It aggregates these weights into an influence profile w_t and computes a magnitude-weighted average lag ρ̂_T. Core assumption: Local sensitivity (first-order gradients) accurately reflects the functional dependence of the policy on history.

### Mechanism 2: Axiomatic Normalization for Invariance
The normalized form ρ̂_T allows for valid comparison across different architectures and environment scales because it is invariant to uniform input/output rescaling. The metric uses magnitude-weighted averaging, and since uniform rescaling multiplies all Jacobian norms by the same constant, the ratio defining the average lag remains unchanged. Core assumption: "Memory use" is best defined as the relative distribution of influence over time, rather than absolute gradient magnitude.

### Mechanism 3: Proxy Modeling for Black-Box Policies
If the primary policy is non-differentiable, a compact Long Expressive Memory (LEM) network trained on the same task can approximate the temporal range of the original agent. LEM is designed for stable long-horizon gradients. The authors train a small LEM as a "proxy" and compute the Temporal Range on this surrogate, assuming it captures the task's intrinsic memory requirements rather than just the idiosyncrasies of the original architecture. Core assumption: The proxy learns a similar input-output mapping and thus exhibits similar temporal dependencies to the target policy.

## Foundational Learning

- **Reverse-Mode Automatic Differentiation (Backpropagation)**
  - Why needed here: The metric requires computing ∂y_s/∂x_t for all t < s. Understanding how gradients flow backward through time (BPTT) is essential to implement the single-pass computation efficiently.
  - Quick check question: How does the computational cost of reverse-mode differentiation scale with the number of inputs vs. outputs?

- **POMDPs and History Dependence**
  - Why needed here: You must distinguish between Markov tasks (current observation suffices) and POMDPs (history is required) to interpret the metric. A low ρ̂_T on a POMDP suggests a failed policy.
  - Quick check question: In a POMDP, why does augmenting state with history (or recurrent hidden state) theoretically restore the Markov property?

- **Matrix Norms (Frobenius/Operator)**
  - Why needed here: The method collapses vector-output Jacobians into scalar weights using a matrix norm. The choice of norm (e.g., Frobenius vs. Spectral) affects which types of sensitivity are emphasized.
  - Quick check question: Does the Frobenius norm measure the maximum singular value or the sum of squared singular values?

## Architecture Onboarding

- **Component map:**
  Policy Network -> Jacobian Computer -> Aggregator -> Window Ablation Wrapper

- **Critical path:**
  1. Freeze Policy: Ensure the policy network is in eval mode (no gradient updates)
  2. Rollout: Execute the policy to collect a sequence X_1:T and outputs y_1:T
  3. Differentiate: Run one reverse-mode pass per sequence to get Jacobian blocks
  4. Compute Metric: Aggregate norms to get ρ̂_T

- **Design tradeoffs:**
  - Mean vs. Max Aggregation: The paper uses Mean aggregation (L=mean) because Max aggregation tends to cluster values (reducing discrimination) based on single large gradient spikes. Use Mean for general diagnostics.
  - Proxy vs. Direct: Computing directly on the policy is most accurate, but requires differentiability. The LEM proxy adds training overhead but enables analysis of black-box agents.

- **Failure signatures:**
  - Inflated Range on Markov Tasks: If ρ̂_T ≈ 10 on CartPole (a near-Markov task), the architecture (likely LSTM or LEM) has "slow modes" or inefficiencies—it is using history unnecessarily.
  - Zero Range on POMDPs: If ρ̂_T ≈ 0 on Stateless CartPole, the policy has failed to learn memory and is likely performing poorly.
  - Distribution Shift: If Window Ablations show performance drops even when m > ρ̂_T, the model may have overfit to the statistical properties of the full training sequence length.

- **First 3 experiments:**
  1. Unit Test (Copy-k): Train a small GRU on a Copy-k task (where output = input from k steps ago). Verify that ρ̂_T converges to exactly k. This validates the implementation.
  2. Architecture Audit: Train GRU and LSTM on a standard control task (e.g., CartPole). Compare their ρ̂_T. You will likely see GRU has ≈ 2 and LSTM ≈ 10. Use this to identify the LSTM's inefficient memory usage.
  3. Window Validation: Take the trained LSTM from step 2. Run a window ablation (truncating history to sizes 1, 2, …, 16). Plot return vs. window size. Confirm that performance saturates once the window size exceeds the ρ̂_T measured in step 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Temporal Range be integrated as a differentiable regularizer during training to explicitly penalize unnecessary memory dependence and guide policies toward more efficient, shorter-context solutions?
- Basis in paper: The conclusion explicitly lists "regularizers that penalize unnecessary range to bias training toward simpler, shorter-context solutions" as a direction for future work.
- Why unresolved: The paper currently uses the metric only as a post-hoc diagnostic and analysis tool; the dynamics of optimizing this metric during the reinforcement learning process remain unexplored.
- What evidence would resolve it: Demonstrating that adding a TR-based loss term successfully reduces the context window size on near-Markov tasks (like CartPole) without degrading the return, effectively "correcting" the architectural inefficiencies observed in LEM/LSTM.

### Open Question 2
- Question: Does a divergence between the Temporal Range of the policy head and the value head predict training instability or error propagation in advantage estimation?
- Basis in paper: Section 3.5 states that a "gap between policy and value ranges helps explain unstable advantage estimates," and the conclusion lists "per-output-component ranges (policy vs. value)" as future work.
- Why unresolved: While the theoretical link to advantage estimation is proposed, the paper does not provide empirical data quantifying the correlation between the policy-value TR gap and training stability metrics (e.g., variance of gradients or final performance variance).
- What evidence would resolve it: A correlation analysis across multiple training runs showing that seeds or architectures with larger policy-value TR gaps suffer from higher variance in returns or earlier training divergence.

### Open Question 3
- Question: How does the local gradient-based sensitivity measured by Temporal Range compare to causal, perturbation-based measures of memory in highly non-linear environments?
- Basis in paper: The conclusion identifies "causal perturbation checks" as a necessary avenue for future work to complement the current first-order sensitivity analysis.
- Why unresolved: Jacobian-based methods can fail if the relationship between input and output is highly non-linear or discontinuous; purely gradient-based sensitivity might miss dependencies that would be revealed by actual perturbations.
- What evidence would resolve it: An experiment comparing TR against an intervention-based metric (measuring performance drop when specific history steps are zeroed out) to validate if TR accurately captures causal dependence in complex tasks.

## Limitations
- The assumption that first-order gradients capture all relevant temporal dependencies may fail in policies using higher-order temporal logic or non-smooth activation patterns
- The method's invariance to uniform rescaling holds mathematically but may not fully capture differences in sparse vs. dense memory strategies
- Major uncertainties remain around the metric's behavior in non-stationary environments and its sensitivity to specific architectural choices beyond the tested recurrent units

## Confidence

- **High confidence:** The metric correctly identifies ground-truth memory requirements in Copy-k tasks; the computational efficiency claim is well-supported.
- **Medium confidence:** The interpretation of higher-than-necessary memory usage as architectural inefficiency is plausible but depends on the specific task structure.
- **Low confidence:** The proxy LEM approach reliably captures the temporal range of arbitrary black-box policies without extensive validation across diverse architectures.

## Next Checks

1. Test Temporal Range on a synthetic POMDP with known optimal memory requirements (e.g., a delayed XOR task) to verify it captures non-Markov dependencies.
2. Evaluate whether the metric detects differences between architectures with similar performance but different hidden state dynamics (e.g., GRU vs. minimal LSTM variants).
3. Assess sensitivity by applying uniform noise or dropout to inputs and measuring how much Temporal Range changes, validating robustness to input quality variations.