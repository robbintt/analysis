---
ver: rpa2
title: The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic
  Architectures
arxiv_id: '2510.05364'
source_url: https://arxiv.org/abs/2510.05364
tags:
- attention
- arxiv
- wang
- zhang
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys sub-quadratic sequence modeling alternatives
  to transformers, focusing on their computational complexity, benchmark performance,
  and fundamental limitations. The authors review four main categories: sub-quadratic
  attention variants (approximate and sparse methods), linear RNN-based models, state-space
  models, and hybrid architectures.'
---

# The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures

## Quick Facts
- **arXiv ID**: 2510.05364
- **Source URL**: https://arxiv.org/abs/2510.05364
- **Reference count**: 39
- **Primary result**: Sub-quadratic architectures offer efficiency gains but pure attention transformers remain dominant in frontier models due to superior expressivity.

## Executive Summary
This paper surveys sub-quadratic sequence modeling alternatives to transformers, focusing on their computational complexity, benchmark performance, and fundamental limitations. The authors review four main categories: sub-quadratic attention variants, linear RNN-based models, state-space models, and hybrid architectures. They compare time and memory complexities across training and inference, and present benchmark results on tasks like MMLU, GSM8K, and ARC. While alternative architectures offer efficiency gains, especially in edge scenarios, pure attention transformers remain dominant in frontier models due to superior expressivity. Theoretical analysis shows that both attention and sub-quadratic models are limited to the T C0 circuit complexity class, restricting their ability to simulate finite automata or perform tasks like associative recall without scaling dimensions.

## Method Summary
The paper systematically compares sub-quadratic architectures against transformers through theoretical complexity analysis and empirical benchmarking. The methodology involves analyzing time and memory complexities for training and inference, evaluating benchmark performance across multiple datasets, and examining theoretical limitations through circuit complexity analysis. Results are compiled from technical reports and standardized evaluation frameworks, focusing on the 0.7-1.5B parameter range to compare edge models against transformer baselines.

## Key Results
- Sub-quadratic models (RNNs, SSMs) achieve O(n) efficiency by compressing context into fixed-size recurrent states but suffer from state capacity bottlenecks
- Hybrid architectures (Jamba, Samba) balance efficiency and expressivity by combining efficient local processing with occasional global attention
- Both attention and sub-quadratic models are limited to T C0 circuit complexity, preventing them from simulating finite automata without scaling dimensions

## Why This Works (Mechanism)

### Mechanism 1: State Compression for Linear Complexity
- **Claim**: Sub-quadratic models achieve O(n) efficiency by compressing context into fixed-size recurrent states, but this creates a "state capacity" bottleneck for precise recall
- **Mechanism**: Unlike attention's O(n²) pairwise comparison, models like Mamba or RWKV update compressed hidden states at each timestep, allowing infinite context streaming with constant memory but forcing the model to discard fine-grained details
- **Core assumption**: Not all historical tokens are equally important, allowing selective retention via gating or decay mechanisms
- **Evidence anchors**: Abstract mentions inherent quadratic complexity as the bottleneck; section 4.2 notes RWKV suffers from "reduced recall"; section 7.2 states constant-memory models cannot perform associative recall as effectively as shallow transformers
- **Break condition**: Tasks requiring exact retrieval from arbitrary earlier points in long sequences may fail compared to full attention

### Mechanism 2: Hybridization of Global and Local Processing
- **Claim**: Hybrid architectures work by offloading local processing to efficient SSM/RNN layers while reserving expensive quadratic attention for global coordination
- **Mechanism**: These models use striped (serial) or fusion (parallel) arrangements where sliding window attention or SSMs handle dense local context efficiently, while sparse full-attention layers recover global reasoning capabilities
- **Core assumption**: Global dependency requirements are sparse enough that full attention is only needed occasionally
- **Evidence anchors**: Section 5 defines striped vs. fusion hybrids and notes Jamba achieves 3x higher throughput; section 6.1 shows hybrids competing with pure transformers in 14-70B parameter range
- **Break condition**: Efficiency gains may be negated if attention layers are too sparse to capture necessary long-range dependencies

### Mechanism 3: Hardware-Optimized IO-Awareness
- **Claim**: Modern sub-quadratic and quadratic models achieve practical speedups through GPU memory hierarchy optimizations rather than just algorithmic complexity reduction
- **Mechanism**: Techniques like FlashAttention and custom CUDA kernels minimize memory reads/writes between HBM and GPU SRAM, making theoretical complexity less predictive of wall-clock time than implementation efficiency
- **Core assumption**: Memory bandwidth, not just FLOPs, is the primary bottleneck during training and inference
- **Evidence anchors**: Section 3 attributes 2-4x speedups in FlashAttention to "IO-awareness"; section 4.2 mentions Lightning Attention uses similar optimizations
- **Break condition**: These optimization strategies may lose effectiveness if hardware architectures shift significantly from current GPU HBM/SRAM hierarchy

## Foundational Learning

- **Concept**: **Computational Complexity Classes (O(n²) vs O(n))**
  - **Why needed here**: The paper centers on the trade-off between quadratic cost of attention and linear/sub-quadratic alternatives
  - **Quick check question**: If sequence length doubles, how does compute cost change for a standard Transformer versus a linear RNN?

- **Concept**: **State Space Models (SSMs) and Recurrence**
  - **Why needed here**: The paper identifies SSMs and RNNs as primary alternatives to attention
  - **Quick check question**: How does a recurrent state allow a model to process an infinite sequence with fixed memory, and what is the downside?

- **Concept**: **Expressivity vs. Efficiency Trade-off**
  - **Why needed here**: The core thesis is that sub-quadratic models offer efficiency but lack expressivity/recall of Transformers
  - **Quick check question**: Why does compressing context into a fixed-size state limit a model's ability to perform exact "lookup" tasks?

## Architecture Onboarding

- **Component map**: Standard Transformer -> Sparse/Approx Attention -> Linear Recurrence -> Selective SSMs -> Hybrids
- **Critical path**:
  1. Start with baseline Transformer (using FlashAttention-2/3) to establish performance bounds
  2. Implement pure SSM (like Mamba) or Linear RNN (RWKV) to measure efficiency gains
  3. Introduce Hybrid configuration (e.g., 1 Attention layer for every N SSM layers) to recover recall capabilities
- **Design tradeoffs**:
  - Latency vs. Recall: Pure SSMs/RNNs offer lowest latency; Hybrids offer middle ground; Transformers offer highest recall but highest latency
  - Training Parallelism vs. Inference State: Transformers parallelize perfectly during training; RNNs/SSMs often require specialized parallel scans during training but excel at constant-time inference
- **Failure signatures**:
  - "Lost in the Middle": Pure sub-quadratic models failing to retrieve information from middle of long context due to state compression limits
  - Training Instability: Linear attention models sometimes suffering from rank collapse or gradient issues
  - Inference Throughput Ceiling: Pure Transformers hitting KV-cache memory limits on long contexts
- **First 3 experiments**:
  1. Run standard Transformer vs. pure Mamba on Multi-Query Associative Recall task to quantify "lookup" performance gap
  2. Measure inference latency and memory usage for Transformer vs. Hybrid model as context length increases from 4k to 128k tokens
  3. In Hybrid model, vary ratio of Attention layers to SSM layers (e.g., 1:4 vs 1:8) to find minimum density of attention required to maintain benchmark performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can transformer expressivity extend beyond the complexity class TC^0, or is this a fundamental upper bound?
- **Basis in paper**: The authors note in Appendix A.3 that "it remains an open question whether the expressivity of transformers goes beyond the complexity class TC^0"
- **Why unresolved**: Current theoretical frameworks only prove membership in TC^0 for standard transformer forward passes
- **What evidence would resolve it**: A formal proof establishing tighter complexity class membership, or a construction demonstrating transformers solving problems outside TC^0 under realistic constraints

### Open Question 2
- **Question**: How should Mixture of Architectures (MoA) paradigms be designed to optimally combine quadratic attention with sub-quadratic primitives?
- **Basis in paper**: The authors anticipate "model routing and Mixture of Architectures (MoA) paradigms to become more relevant"
- **Why unresolved**: Current hybrids (striped vs. fusion) show no clear superiority; optimal routing mechanisms remain poorly understood
- **What evidence would resolve it**: Systematic studies comparing routing strategies across diverse tasks, demonstrating consistent performance gains over monolithic architectures

### Open Question 3
- **Question**: Can fundamental limitations in sub-quadratic architectures (e.g., finite state capacity, recall bottlenecks) be overcome without sacrificing efficiency?
- **Basis in paper**: The paper documents that SSMs and linear RNNs cannot recall arbitrary amounts of previously seen information due to state compression
- **What evidence would resolve it**: Novel architectures achieving both linear complexity and strong associative recall on MQAR/hopk benchmarks at scale

## Limitations
- Theoretical expressivity gap remains unclear in practical applications despite mathematical proofs
- Performance comparisons heavily depend on specific GPU architectures and implementation details
- Benchmark representativeness may not capture real-world deployment requirements, particularly in edge computing contexts

## Confidence
- **High Confidence**: Fundamental complexity analysis showing O(n²) vs O(n) scaling is mathematically sound and well-established
- **Medium Confidence**: Claim that sub-quadratic models cannot perform associative recall as effectively as Transformers is demonstrated through specific benchmarks but may be task-dependent
- **Low Confidence**: Theoretical argument that both attention and sub-quadratic models share the same TC^0 complexity class is mathematically rigorous but its practical significance remains speculative

## Next Checks
1. Reproduce latency and memory usage comparisons between Transformers, sub-quadratic models, and hybrids across multiple GPU architectures (A100, H100, L4) to verify hardware-dependent claims
2. Evaluate these architectures on production-like workloads including code completion, document summarization, and multi-turn dialogue with varying context lengths
3. Investigate whether TC^0 complexity limitation can be circumvented through architectural modifications such as hierarchical recurrence or adaptive state compression