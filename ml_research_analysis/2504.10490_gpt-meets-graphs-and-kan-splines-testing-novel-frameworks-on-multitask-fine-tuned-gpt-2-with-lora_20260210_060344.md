---
ver: rpa2
title: 'GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned
  GPT-2 with LoRA'
arxiv_id: '2504.10490'
source_url: https://arxiv.org/abs/2504.10490
tags:
- lora
- graph
- attention
- figure
- learnable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates integrating Kolmogorov-Arnold Networks (KAN)
  and Graph Attention Networks (GAT) with Low-Rank Adaptation (LoRA) into a pre-trained
  GPT-2 model for multi-task learning. The goal was to assess whether these interpretable,
  structured architectures could improve performance on sentiment analysis, paraphrase
  detection, and sonnet generation tasks.
---

# GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA

## Quick Facts
- arXiv ID: 2504.10490
- Source URL: https://arxiv.org/abs/2504.10490
- Reference count: 0
- Primary result: LoRA fine-tuning outperformed KAN and Graph variants on SST, CFIMDB, paraphrase, and sonnet generation tasks

## Executive Summary
This paper investigates whether integrating Kolmogorov-Arnold Networks (KAN) and Graph Attention Networks (GAT) with Low-Rank Adaptation (LoRA) into GPT-2 can improve multi-task performance. The authors tested these architectures on sentiment analysis, paraphrase detection, and sonnet generation tasks. Despite theoretical benefits of interpretable architectures, the LoRA-only transformer achieved the highest scores: 55.249% accuracy on SST, 99.18% on CFIMDB dev, 89.9% on paraphrase test, and 42.097 CHRF on sonnet generation. These results demonstrate that efficient parameter adaptation via LoRA remains the most effective strategy for this multi-task setting.

## Method Summary
The study adapted GPT-2 Large (1280-dim) using LoRA with rank r=32 and scaling factor α=64. Three variants were compared: standard LoRA (baseline), Hybrid KAN-LoRA (replacing MLPs with learnable B-spline functions), and Graph-LoRA (applying LoRA to GAT layers on token graphs). Training used AdamW optimizer (lr=1e-5, weight_decay=0.2, dropout=0.5) for 10-15 epochs on sentiment/sonnet tasks and 3-5 epochs on paraphrase. Datasets included SST (11,855 sentences, 5-class), CFIMDB (2,434 binary movie reviews), Quora Question Pairs (400,000 pairs), and Shakespearean Sonnets (154 sonnets).

## Key Results
- LoRA achieved 55.249% accuracy on SST (5-class sentiment)
- LoRA reached 99.18% accuracy on CFIMDB development set
- LoRA scored 89.9% on paraphrase test set
- LoRA generated sonnets with 42.097 CHRF score

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Adaptation (LoRA)
LoRA freezes pre-trained GPT-2 weights and learns low-rank updates via matrices B and A, where the forward pass becomes $h = W_0x + BAx$. This assumes task adaptation requires only low-dimensional changes to the weight space, making training efficient while preserving pre-trained knowledge.

### Mechanism 2: Kolmogorov-Arnold Networks (KANs)
KANs replace fixed activation nodes with learnable B-spline functions on edges, decomposing functions into sums of univariate functions. This theoretically enables smoother function approximation compared to standard MLPs with fixed activations.

### Mechanism 3: Graph Attention Networks (GATs) with LoRA
GATs structure text as token graphs with learned attention coefficients between nodes. Graph-LoRA applies low-rank updates to GAT weight matrices, attempting to combine structural inductive biases with efficient fine-tuning.

## Foundational Learning

**Concept: Low-Rank Matrix Factorization**
- Why needed: This is the mathematical engine of LoRA (decomposing $\Delta W$ into $B$ and $A$)
- Quick check question: If a weight matrix is $1024 \times 1024$ and rank $r=8$, what are the dimensions of the $A$ and $B$ matrices, and how many parameters does this save compared to full fine-tuning?

**Concept: B-Splines**
- Why needed: KANs replace weights with splines, so understanding that splines are piecewise polynomial functions controlled by "knots" or coefficients explains the "learnable activation" concept
- Quick check question: How does parameterizing an activation function as a spline differ theoretically from learning a standard weight scalar in an MLP?

**Concept: Inductive Bias (Graph vs. Sequential)**
- Why needed: The Graph-LoRA experiment tests whether explicit graph structures provide relational inductive bias superior to the implicit relational learning in standard transformers
- Quick check question: What specific "relation" does a GAT assume exists between tokens that a standard causal transformer might miss?

## Architecture Onboarding

**Component map:** Pre-trained GPT-2 (Frozen weights) -> LoRA Modules (Trainable A, B matrices) -> KAN Linear (B-spline activations) OR Graph Attention Layers (GAT with LoRA on W_k)

**Critical path:** The only component strictly necessary to reproduce the best results is the standard LoRA implementation applied to GPT-2 Large (1280-dim) model.

**Design tradeoffs:**
- LoRA Rank (r) vs. Capacity: Higher r (e.g., 32) improves performance but increases memory; paper found r=32 optimal
- Complexity vs. Efficiency: KAN/Graph variants offered theoretical interpretability but at the cost of 18+ hours training time and lower accuracy

**Failure signatures:**
- Graph-LoRA: Extremely slow convergence (18 hours for paraphrase) due to graph construction overhead; typically underperforms on classification accuracy
- KAN-LoRA: "Weaker memorization" leading to higher loss in early epochs and lower final accuracy on classification tasks compared to the MLP baseline

**First 3 experiments:**
1. **Baseline Sanity Check:** Implement standard GPT-2 with LoRA (r=32, α=64) on SST/CFIMDB. Confirm that BA updates are actually being applied by checking parameter counts.
2. **Hyperparameter Sweep:** Tune the LoRA scaling factor α and rank r on the validation set. Specifically test if the paper's finding (r=32, α=64) holds for your specific hardware batch size.
3. **Ablation on Scaling:** Verify the impact of the scaling factor $\frac{\alpha}{r}$ by setting it to 1.0 vs. 2.0 (paper's implicit ratio) to observe how strongly the low-rank updates influence the frozen weights.

## Open Questions the Paper Calls Out

**Open Question 1:** Would KAN-LoRA or Graph-LoRA performance match the optimized baseline if trained on the larger GPT-2-Large model (1280 dim) rather than the smaller 768-dim version? The authors note that the novel architectures were "too computationally expensive to train on larger GPT-2 models," creating a parameter count disparity compared to the best LoRA baseline.

**Open Question 2:** Can Hybrid KAN-LoRA outperform standard transformers on NLP tasks specifically requiring symbolic or mathematical reasoning rather than sentiment analysis? The paper cites prior work suggesting KANs only outperform MLPs on "symbolic formula tasks," while noting KANs' benefits in "mathematics and physics-related tasks."

**Open Question 3:** Can the significant computational overhead of document-level graph construction be mitigated to make Graph-LoRA viable for large-scale training? The paper highlights that "building a graph data structure for every transformer pass incurs significant cost," taking 18 hours for paraphrase training.

## Limitations

**Dataset-specific concerns:** The CFIMDB dataset (2,434 samples) is notably small, raising questions about whether the high LoRA accuracy (99.18%) represents true generalization or overfitting. The sonnet generation task uses only 154 training examples, making performance claims particularly sensitive to data sampling.

**Architectural ambiguity:** The paper does not specify critical KAN hyperparameters including spline order, grid resolution, or initialization strategies. Without these details, reproducing the KAN-LoRA variant remains challenging.

**Graph construction methodology:** The Graph-LoRA approach requires building token graphs, but the paper lacks specification of edge definition criteria and GAT head configurations, making it difficult to determine whether poor performance reflects fundamental limitations or implementation choices.

## Confidence

**High confidence:** LoRA outperforms KAN-LoRA and Graph-LoRA variants on the tested benchmarks. The mechanism of low-rank adaptation is well-established and the paper provides clear implementation details for the LoRA baseline.

**Medium confidence:** The specific performance numbers (55.249% SST, 99.18% CFIMDB, 89.9% paraphrase, 42.097 CHRF) are reproducible given the specified hyperparameters, though results may be sensitive to random seeds and exact data splits.

**Low confidence:** The theoretical explanations for why KANs and GATs underperformed are speculative. The paper attributes KAN failure to "weaker memorization" and GAT failure to "graph construction overhead" without extensive ablation studies to confirm these hypotheses.

## Next Checks

**Validation Check 1:** Implement the LoRA baseline with specified hyperparameters (r=32, α=64) and verify that the low-rank decomposition is correctly applied by monitoring parameter counts before and after LoRA insertion. Train on SST for 10 epochs and confirm accuracy exceeds 50%.

**Validation Check 2:** Conduct a controlled ablation experiment comparing standard MLP layers against KAN layers with identical LoRA integration. Systematically vary spline order (linear, quadratic, cubic) and grid resolution to determine whether KAN architecture or specific implementation choices caused the performance degradation.

**Validation Check 3:** Profile Graph-LoRA training to quantify the exact computational overhead from graph construction. Compare training throughput (samples/second) between Graph-LoRA and standard LoRA to determine whether the 18-hour training time is primarily due to graph overhead or model convergence issues.