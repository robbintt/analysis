---
ver: rpa2
title: 'BTS: Harmonizing Specialized Experts into a Generalist LLM'
arxiv_id: '2502.00075'
source_url: https://arxiv.org/abs/2502.00075
tags:
- expert
- experts
- seed
- training
- stitch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Branch-Train-Stitch (BTS), a method for combining
  specialized large language models (LLMs) into a single, capable generalist model.
  The approach addresses the challenge of creating powerful, flexible models that
  can handle diverse tasks without the high costs of dense training.
---

# BTS: Harmonizing Specialized Experts into a Generalist LLM
## Quick Facts
- arXiv ID: 2502.00075
- Source URL: https://arxiv.org/abs/2502.00075
- Authors: Qizhen Zhang; Prajjwal Bhargava; Chloe Bi; Chris X. Cai; Jakob Foerster; Jeremy Fu; Punit Singh Koura; Ruan Silva; Sheng Shen; Emily Dinan; Suchin Gururangan; Mike Lewis
- Reference count: 13
- Combines specialized LLMs into generalist model using Branch-Train-Stitch method

## Executive Summary
This paper introduces Branch-Train-Stitch (BTS), a method for combining specialized large language models (LLMs) into a single, capable generalist model. The approach addresses the challenge of creating powerful, flexible models that can handle diverse tasks without the high costs of dense training. BTS operates through three phases: branching a pretrained seed model into domain-specific experts, inserting lightweight stitch layers, and training these stitches to dynamically integrate expert representations during inference.

Experiments with a 2.7B parameter seed model and three expert models (code, math, and multilingual) show that BTS outperforms competitive baselines in generalist performance. It achieves the best average performance across tasks while using minimal training parameters, making it a modular and flexible approach for building generalist LLMs that can even outperform individual experts in their specialized domains.

## Method Summary
BTS operates in three phases: first, a pretrained "seed" model is branched into copies, each specialized in a different domain (e.g., coding, math, or multilingual tasks) through continued pretraining. Second, lightweight stitch layers are inserted between these expert models and the seed model. These stitch layers, trained on a small mix of expert domain data, enable the seed model to dynamically integrate representations from the experts during the forward pass, allowing it to generalize to new domains while remaining frozen. The model uses a hub-and-spoke architecture, with the seed model as the hub and experts as spokes, enabling efficient and interpretable integration of specialized knowledge.

## Key Results
- BTS outperforms competitive baselines in generalist performance with a 2.7B parameter seed model
- Achieves best average performance across diverse tasks including code generation, multilingual translation, and math reasoning
- Can outperform individual experts in their specialized domains (e.g., better MATH benchmark results than math expert)
- Demonstrates strong cross-capability performance by combining expertise from different domains

## Why This Works (Mechanism)
BTS works by leveraging the complementary strengths of specialized models through a novel integration mechanism. The stitch layers act as learned adapters that enable the seed model to selectively route information from different experts based on the task requirements. This hub-and-spoke architecture allows for efficient knowledge transfer while maintaining the frozen weights of the expert models, reducing computational overhead. The method's success stems from its ability to dynamically combine specialized representations during inference, creating a more capable generalist model without requiring full retraining of all components.

## Foundational Learning
- **Continued Pretraining**: Specialized expert models are created by further training base models on domain-specific data. Why needed: To develop deep expertise in specific domains before integration. Quick check: Verify domain performance improvements after specialization.
- **Dynamic Integration**: Stitch layers enable real-time combination of expert representations during inference. Why needed: To create flexible, context-aware responses across domains. Quick check: Test performance on cross-domain tasks requiring multiple expertise.
- **Hub-and-Spoke Architecture**: Seed model acts as central hub connecting specialized expert spokes. Why needed: To maintain modularity while enabling efficient knowledge sharing. Quick check: Measure parameter efficiency compared to dense training approaches.

## Architecture Onboarding
- **Component Map**: Seed Model -> Stitch Layers -> Expert Models (Code, Math, Multilingual)
- **Critical Path**: Forward pass through seed model → stitch layer selection → expert model integration → final output generation
- **Design Tradeoffs**: Minimal parameter addition vs. integration complexity; frozen experts vs. adaptability; modular design vs. potential knowledge gaps
- **Failure Signatures**: Performance degradation on tasks requiring deep integration of multiple domains; inability to handle novel task combinations; expert model conflicts during dynamic routing
- **First Experiments**: 1) Test individual expert performance before integration, 2) Evaluate stitch layer training convergence, 3) Measure cross-domain task performance with different stitch configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger models (10B+ parameters) remains untested
- Performance on complex multi-domain tasks requiring simultaneous integration of three or more capabilities unclear
- Computational efficiency gains compared to dense training not quantified in terms of wall-clock time or memory usage

## Confidence
- "Best average performance across tasks" - High confidence based on reported experimental results
- "Outperforming individual experts in specialized domains" - Medium confidence due to limited task diversity in evaluation
- "Modular and flexible approach" - High confidence given architecture design
- "Minimal training parameters" - Low confidence without explicit comparisons to baseline training costs

## Next Checks
1. Evaluate BTS on larger-scale models (10B+ parameters) to assess scalability limits and performance trends
2. Test the model's ability to handle complex, multi-domain tasks requiring simultaneous integration of three or more specialized capabilities
3. Conduct ablation studies to quantify the contribution of stitch layers versus other architectural components and measure inference-time efficiency compared to dense training alternatives