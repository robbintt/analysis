---
ver: rpa2
title: Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing
arxiv_id: '2512.21626'
source_url: https://arxiv.org/abs/2512.21626
tags:
- regret
- plays
- action
- play
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a variant of multiple-play stochastic bandits
  tailored to resource allocation problems arising from LLM applications, edge intelligence,
  etc. The model is composed of $M$ arms and $K$ plays.
---

# Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing

## Quick Facts
- arXiv ID: 2512.21626
- Source URL: https://arxiv.org/abs/2512.21626
- Reference count: 40
- Primary result: Near-optimal regret bounds for multiple-play bandits with prioritized capacity sharing

## Executive Summary
This paper introduces a novel bandit model (MSB-PRS) for resource allocation problems where multiple plays compete for stochastic arm capacities with priority-based allocation. The model is motivated by applications in LLM serving and edge computing, where resource availability is limited and requests have varying priorities. The authors establish tight instance-dependent and instance-independent regret lower bounds, then design an algorithm (MSB-PRS-ApUCB) that achieves these bounds up to logarithmic factors.

## Method Summary
The MSB-PRS model consists of M arms with stochastic capacities and K plays with priority weights. When multiple plays request capacity from an arm, allocation follows a largest-priority-first rule. The utility function is nonlinear due to this prioritization mechanism. The authors first develop an offline oracle (MSB-PRS-OffOpt) using bipartite matching to find optimal allocations in O(M³K³) time. They then build an online algorithm (MSB-PRS-ApUCB) that uses this oracle as a subroutine, computing upper confidence bounds on rewards and capacities to select allocations. The algorithm updates empirical estimates of rewards and capacities after each round.

## Key Results
- Proved instance-independent regret lower bound of Ω(α₁σ√(KMT))
- Proved instance-dependent regret lower bound of Ω(α₁σ²(M/Δ)lnT)
- Designed MSB-PRS-ApUCB achieving regret upper bounds matching these lower bounds up to factors of √(K ln KT) and α₁K² respectively
- Computational complexity of offline oracle is O(M³K³)

## Why This Works (Mechanism)
The prioritized resource sharing mechanism creates a unique combinatorial structure where the marginal value of capacity depends on the number and priority of competing plays. By designing an oracle that explicitly accounts for this priority-based allocation through bipartite matching with appropriate constraints, the algorithm can identify optimal allocations. The UCB-based approach then balances exploration and exploitation while maintaining theoretical guarantees.

## Foundational Learning
- Prioritized capacity allocation: Understanding how priority weights determine resource distribution when demand exceeds supply. Why needed: Forms the core mechanism distinguishing this model from standard bandits. Quick check: Verify allocation satisfies priority ordering when multiple plays request same arm.
- Combinatorial utility optimization: The utility function depends nonlinearly on which plays are assigned to which arms. Why needed: Standard bandit techniques don't apply due to this structure. Quick check: Confirm utility calculation matches the priority-based allocation formula.
- Upper confidence bound techniques: Using optimistic estimates for unknown parameters. Why needed: Enables balancing exploration and exploitation. Quick check: Verify confidence bounds properly shrink with more observations.

## Architecture Onboarding

**Component Map:** Environment (arms, capacities, rewards) -> Oracle (MSB-PRS-OffOpt) -> Algorithm (MSB-PRS-ApUCB) -> Action selection -> Observation -> Update estimates

**Critical Path:** At each round t: compute UCB indices -> call oracle to find argmax allocation -> execute allocation -> observe rewards/capacities -> update empirical estimates

**Design Tradeoffs:** The authors chose to develop a specialized offline oracle rather than adapt existing combinatorial bandit algorithms, trading computational complexity (O(M³K³)) for tighter regret bounds that match the theoretical lower bounds.

**Failure Signatures:** 
- Suboptimal allocations: Check if Hungarian algorithm properly enforces priority-compatible constraints
- Linear regret: Verify confidence bounds are shrinking appropriately with observations
- Invalid allocations: Ensure Hungarian matching produces V-monotone and priority-compatible results

**3 First Experiments:**
1. Verify oracle finds optimal allocation on a small instance (M=3, K=4) with known ground truth
2. Test algorithm convergence on synthetic data with varying priority distributions
3. Compare empirical regret against theoretical bounds on benchmark instances

## Open Questions the Paper Calls Out
- Can the gap between instance-dependent regret upper and lower bounds be closed? The current upper bound exceeds the lower bound by a factor of α₁K².
- Can the computational complexity of the offline oracle be improved below O(M³K³)?
- How does the model perform under non-i.i.d. capacity and reward distributions?

## Limitations
- Computational complexity of O(M³K³) may be prohibitive for large-scale applications
- The gap between upper and lower instance-dependent regret bounds remains unresolved
- Assumes i.i.d. capacity and reward distributions, which may not hold in real-world scenarios

## Confidence
- Theoretical results: High - regret bounds are mathematically rigorous and well-proven
- Computational complexity claims: Medium - depends on efficient implementation of the matching subroutine
- Empirical validation: Not provided in the paper

## Next Checks
1. Implement the capacity distribution normalization constant α to verify it matches the theoretical setup described in the capacity allocation mechanism.
2. Run experiments comparing MSB-PRS-ApUCB against the stated baselines (OnlinActPrf and OnlinActPrf-v) to verify the claimed empirical performance improvements.
3. Verify the Hungarian algorithm implementation correctly enforces both V-monotone and priority-compatible constraints when finding optimal allocations.