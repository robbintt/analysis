---
ver: rpa2
title: Error-Driven Prompt Optimization for Arithmetic Reasoning
arxiv_id: '2512.13323'
source_url: https://arxiv.org/abs/2512.13323
tags:
- error
- prompt
- scale
- change
- percentage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an error-driven prompt optimization framework
  for arithmetic reasoning in tabular data workflows, specifically targeting privacy-constrained
  industrial environments. The method uses a Code Generation Agent (CGA) coupled with
  HDBSCAN clustering of prediction errors to iteratively derive and refine domain-specific
  prompt rules without costly fine-tuning.
---

# Error-Driven Prompt Optimization for Arithmetic Reasoning

## Quick Facts
- arXiv ID: 2512.13323
- Source URL: https://arxiv.org/abs/2512.13323
- Reference count: 15
- Primary result: Qwen3 4B accuracy increases from 59.96% to 70.82% via error-driven prompt optimization

## Executive Summary
This study introduces an error-driven prompt optimization framework for arithmetic reasoning in tabular data workflows, specifically targeting privacy-constrained industrial environments. The method uses a Code Generation Agent (CGA) coupled with HDBSCAN clustering of prediction errors to iteratively derive and refine domain-specific prompt rules without costly fine-tuning. Applied to the Qwen3 4B small language model, this approach increases accuracy from 59.96% to 70.82%, surpassing the larger GPT-3.5 Turbo (66.27%). The work demonstrates that systematic error analysis and targeted rule refinement can significantly enhance small model performance for arithmetic tasks, enabling privacy-compliant, on-premises AI agents in regulated sectors.

## Method Summary
The framework reframes arithmetic reasoning as a code generation task where the model produces executable Python functions to query restructured tabular data rather than performing calculations internally. The optimization loop runs predictions, clusters errors using HDBSCAN, formulates domain-specific prompt rules for the largest error clusters, and validates improvements using McNemar's test. Rules are iteratively added until performance degrades beyond an optimal set size (K_opt). The approach specifically targets small language models in privacy-constrained environments where fine-tuning is impractical, demonstrating that systematic error analysis can overcome fundamental arithmetic reasoning limitations without model modification.

## Key Results
- Accuracy improvement: Qwen3 4B increases from 59.96% to 70.82% on TAT-QA arithmetic questions
- Outperforms GPT-3.5 Turbo (66.27%) despite being a smaller model
- Demonstrates existence of optimal rule set size (K_opt) beyond which performance degrades
- Achieves robust numerical reasoning through targeted prompt engineering rather than fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition via Code Generation
Replacing direct arithmetic reasoning with deterministic code execution reduces calculation errors in small language models. The Code Generation Agent (CGA) decouples semantic parsing from arithmetic execution, generating Python functions to query restructured tables rather than calculating answers internally. This shifts calculation burden from the model's probabilistic weights to deterministic runtime environment. The core assumption is that the model's ability to write syntactically correct selection logic is significantly higher than its internal arithmetic operations.

### Mechanism 2: Error Clustering for Rule Induction
Unsupervised clustering of prediction errors systematically identifies specific competence gaps that are recoverable via prompt engineering. The framework extracts features from failed test cases and uses HDBSCAN to group them, allowing analysis of largest dense clusters to identify "root causes" like confusing "percentage change" with absolute change. These are translated into natural language prompt rules. The core assumption is that errors cluster around specific conceptual misunderstandings or missing domain knowledge rather than being random noise.

### Mechanism 3: Optimal Rule Set Saturation (K_opt)
Performance gains from prompt rules follow a parabolic curve where adding rules beyond optimal set size degrades performance due to cognitive load. As rules increase, probability of rule conflict or ambiguity rises, potentially confusing the SLM. The framework posits an optimal threshold where marginal gain from new rule becomes negative, suggesting SLMs have limited "instruction absorption" capacity. The core assumption is that small models have hard limit on following complex, multi-constraint instructions before suffering from attention dilution.

## Foundational Learning

- **Density-Based Clustering (HDBSCAN)**
  - Why needed: Core innovation relies on grouping errors by similarity without requiring pre-specified cluster counts, handling noise effectively
  - Quick check: How does HDBSCAN handle error instances that do not fit into any distinct error pattern?

- **Code Generation Agents (CGA)**
  - Why needed: Understanding that "reasoning" is offloaded is key; must distinguish between reasoning error (wrong logic) and retrieval error (wrong variable selection)
  - Quick check: In CGA architecture, does model accuracy depend more on Python syntax knowledge or domain logic required to structure query?

- **McNemar's Test**
  - Why needed: Algorithm uses this statistical test to validate if new prompt rule provides significant correction over baseline on error cluster
  - Quick check: Why is McNemar's test suitable for comparing "before" and "after" states of specific error cluster compared to t-test?

## Architecture Onboarding

- **Component map:** Qwen3 4B (SLM) -> Table Restructuring -> Feature Extractor -> HDBSCAN Optimizer -> Human-in-the-loop Rule Formulation

- **Critical path:** Run Baseline (execute CGA on dataset → generate Error Set E) → Cluster Errors (extract features from E → run HDBSCAN → identify largest cluster κ*) → Formulate Rule (expert analyzes κ* → writes natural language constraint ρ) → Validate (re-run on E with ρ → check McNemar p-value and ΔEM)

- **Design tradeoffs:** Automation vs Precision (human-in-the-loop for rule formulation limits scalability but ensures quality); Cluster Size vs Granularity (choosing min_cluster_size that is too large merges distinct error types; too small creates noise)

- **Failure signatures:** Runtime Errors (generated code fails to execute); Scale Mismatch (correct value, wrong magnitude); Sign Error (correct value, wrong sign)

- **First 3 experiments:** Baseline Profiling (run SLM on target dataset with no rules to establish initial Error Set and calculate baseline EM score); Feature Correlation (visualize cross-tab heatmap of calc_pattern vs error_type to verify errors are structured); K_opt Stress Test (iteratively add rules until EM score drops, confirming existence and approximate location of K_opt)

## Open Questions the Paper Calls Out

- **Generalization to new domains:** How can specialized small models, equipped with fine-tuned rule-set, generalize to new, unseen domains without requiring iterative error-analysis cycle? The current framework relies on clustering errors specific to dataset to derive rules and does not test whether these rules transfer effectively to different domains without re-training.

- **Automation of rule formulation:** Can the `FormulateRule` step, currently requiring human intervention, be effectively automated? While clustering of errors is automated, synthesis of those clusters into natural language prompt rules relies on human interpretation, limiting scalability.

- **Portability of K_opt across models:** Is the optimal rule set (K_opt) derived for Qwen3 4B portable to other small language model architectures? It is unclear if rules derived from Qwen's specific error clusters address universal arithmetic reasoning failures or merely patch this specific model's unique weaknesses.

## Limitations
- Core mechanism relies heavily on model's ability to generate syntactically correct code that maps to table semantics
- Error clustering approach assumes errors are structured and clusterable; may stall with high noise or idiosyncratic failure modes
- Claim about enabling privacy-compliant on-premises AI agents is aspirational; 4B model still requires significant compute and memory

## Confidence
- **High confidence:** Core result that CGA + error-driven prompt optimization improves SLM arithmetic reasoning accuracy from ~60% to ~71% is well-supported by ablation studies and McNemar validation
- **Medium confidence:** Existence and location of K_opt is supported by empirical data but lacks theoretical grounding; clustering-based rule induction works for this dataset but may not generalize
- **Low confidence:** Claim that this approach "enables privacy-compliant, on-premises AI agents in regulated sectors" is aspirational; while method avoids fine-tuning, deploying 4B model on-premises still requires significant resources

## Next Checks
1. **Cross-task robustness test:** Apply CGA + error-driven optimization pipeline to different tabular reasoning dataset (WikiTableQuestions or FinQA) to verify if same K_opt phenomenon and error clustering patterns hold
2. **Model size ablation:** Test framework on 1B or 7B SLM to determine if K_opt scales with model size or if degradation is model-specific
3. **Rule automation experiment:** Replace human-in-the-loop rule formulation with LLM-based rule generator to assess whether automation introduces conflicts or degrades performance, and quantify trade-off between iteration speed and rule quality