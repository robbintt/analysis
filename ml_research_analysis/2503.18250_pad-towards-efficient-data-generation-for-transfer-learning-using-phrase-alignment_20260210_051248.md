---
ver: rpa2
title: 'PAD: Towards Efficient Data Generation for Transfer Learning Using Phrase
  Alignment'
arxiv_id: '2503.18250'
source_url: https://arxiv.org/abs/2503.18250
tags:
- data
- korean
- english
- language
- mgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Phrase Aligned Data (PAD), a novel data processing
  method that leverages phrase alignment from Statistical Machine Translation (SMT)
  to enhance transfer learning efficiency for Korean language models. PAD transforms
  English data into Korean by replacing phrases with their Korean equivalents while
  maintaining the English word order, creating syntactically flexible training data
  that exploits Korean's case marker system.
---

# PAD: Towards Efficient Data Generation for Transfer Learning Using Phrase Alignment

## Quick Facts
- arXiv ID: 2503.18250
- Source URL: https://arxiv.org/abs/2503.18250
- Reference count: 22
- Key outcome: PAD transforms English data into Korean using phrase alignment from SMT, achieving performance close to high-quality Korean datasets while requiring minimal resources

## Executive Summary
This paper introduces Phrase Aligned Data (PAD), a novel method for generating Korean training data from English sources by leveraging phrase alignment from Statistical Machine Translation. PAD replaces English phrases with their Korean equivalents while maintaining English word order, exploiting Korean's case marker system to create syntactically flexible training data. The method demonstrates consistent performance improvements over English-only training across multiple benchmark tasks (SA, NLI, STS) and model architectures (mGPT, koGPT, mT5), achieving F1 scores of 0.770 in SA, 0.681 in NLI, and Pearson's r of 0.844 in STS.

## Method Summary
PAD operates by utilizing phrase alignment dictionaries from Statistical Machine Translation to transform English datasets into Korean training data. The process involves replacing English phrases with their Korean equivalents while preserving the original English word order, then leveraging Korean's agglutinative case marker system to create syntactically valid Korean sentences. This approach exploits the structural flexibility of Korean grammar to produce training data that maintains semantic relationships from the original English data while being syntactically appropriate for Korean language models.

## Key Results
- Models trained on PAD data achieved F1 score of 0.770 in SA compared to English baseline of 0.727
- PAD reached 0.681 F1 in NLI versus English baseline of 0.572
- PAD achieved Pearson's r of 0.844 in STS compared to English baseline of 0.819
- PAD performance approaches that of high-quality Korean datasets while requiring minimal computational resources

## Why This Works (Mechanism)
PAD works by exploiting the linguistic properties of Korean's case marker system to create flexible syntactic structures from English phrase alignments. By preserving English word order while replacing phrases with Korean equivalents, the method generates data that maintains semantic relationships while being processable by Korean language models. The approach leverages SMT phrase alignment quality to ensure semantic fidelity during the transformation process, creating training data that bridges the gap between English resources and Korean model requirements.

## Foundational Learning

**Phrase Alignment in SMT**: The process of mapping phrases between source and target languages in Statistical Machine Translation. Needed to understand how PAD derives Korean equivalents from English phrases. Quick check: Verify alignment accuracy affects downstream task performance.

**Korean Case Marker System**: The grammatical system where particles indicate grammatical relationships (subject, object, etc.) that attach to word stems. Needed to understand how PAD creates syntactically valid Korean despite English word order. Quick check: Test model performance with varying marker accuracy.

**Transfer Learning Efficiency**: The principle that pre-training on one language can be adapted to another with appropriate fine-tuning data. Needed to contextualize PAD's resource efficiency claims. Quick check: Compare training time and data requirements against full Korean dataset training.

## Architecture Onboarding

**Component Map**: English Data -> Phrase Alignment Dictionary -> Korean Phrase Replacement -> Case Marker Application -> PAD Dataset

**Critical Path**: The transformation pipeline from English data through phrase alignment to final PAD dataset represents the core workflow, with phrase alignment quality being the most critical determinant of downstream performance.

**Design Tradeoffs**: The method trades syntactic naturalness for resource efficiency, accepting mixed Korean-English outputs when training is insufficient rather than generating nonsensical data. This represents a controlled degradation rather than complete failure.

**Failure Signatures**: Poor phrase alignment quality leads to semantically incorrect transformations, while insufficient training data results in mixed Korean-English outputs. Both failures are identifiable through model output inspection.

**First Experiments**:
1. Evaluate PAD performance across a broader range of Korean NLP tasks beyond SA, NLI, and STS
2. Conduct ablation studies with varying quality of phrase alignment dictionaries
3. Perform human evaluation studies comparing PAD data quality against English baselines and native Korean datasets

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Method's performance depends heavily on quality of phrase alignment dictionaries from SMT
- Evaluation limited to three benchmark tasks and three model architectures, potentially limiting generalizability
- Controllability advantage of producing mixed Korean-English sentences may be problematic for tasks requiring pure Korean output

## Confidence

| Claim | Confidence |
|-------|------------|
| PAD consistently outperforms English-only training | High |
| PAD achieves performance close to high-quality Korean datasets | Medium |
| PAD requires minimal resources and time | Medium |

## Next Checks
1. Evaluate PAD-generated data on broader range of Korean NLP tasks including named entity recognition and generation tasks
2. Conduct ablation studies comparing PAD performance with varying quality of phrase alignment dictionaries
3. Perform human evaluation studies comparing PAD-generated data quality against both English baselines and native Korean datasets