---
ver: rpa2
title: 'Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions'
arxiv_id: '2411.10163'
source_url: https://arxiv.org/abs/2411.10163
tags:
- questions
- compound
- arxiv
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Compound-QA, a benchmark for evaluating large
  language models on compound questions that contain multiple interrelated sub-questions.
  The authors propose a data synthesis framework (CQ-Syn) that generates compound
  questions from existing QA datasets, categorizes them into five types (Factual-Statement,
  Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, Evaluation-and-Suggestion),
  and evaluates models across understanding, reasoning, and knowledge dimensions.
---

# Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions

## Quick Facts
- arXiv ID: 2411.10163
- Source URL: https://arxiv.org/abs/2411.10163
- Reference count: 0
- Key outcome: LLMs perform significantly worse on compound questions (containing multiple interrelated sub-questions) compared to non-compound questions, with Qwen3 achieving the best overall performance (up to 90.5% win rate on Evaluation-and-Suggestion type).

## Executive Summary
This paper introduces Compound-QA, a benchmark for evaluating large language models on compound questions that contain multiple interrelated sub-questions. The authors propose a data synthesis framework (CQ-Syn) that generates compound questions from existing QA datasets, categorizes them into five types (Factual-Statement, Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, Evaluation-and-Suggestion), and evaluates models across understanding, reasoning, and knowledge dimensions. Testing nine open-source LLMs shows their performance on compound questions is notably lower than on non-compound questions. The study finds that sub-questions perform best when placed at the beginning or end of sequences, and that supervised fine-tuning with instruction data augmented by compound questions substantially improves model performance while maintaining general capabilities on other benchmarks.

## Method Summary
The Compound-QA benchmark uses a three-step synthesis framework (CQ-Syn) to generate compound questions: (1) LLM-based generation with type-specific prompts, (2) keyword and LLM filtering, and (3) reference generation with human verification. The benchmark contains 1,500 samples across three dimensions (understanding, reasoning, knowledge) and five compound question types, with 100 samples per type per dimension. Evaluation uses GPT-4o-mini as a judge to compute win rates against reference answers, with position-swapped averaging to mitigate evaluator bias. The study also evaluates improvement strategies including LoRA fine-tuning, Chain-of-Thought, Decomposition instructions, and few-shot learning.

## Key Results
- LLMs show significantly lower performance on compound questions compared to non-compound questions
- Qwen3 achieves the highest overall performance with up to 90.5% win rate on Evaluation-and-Suggestion type
- Sub-questions perform best when placed at the beginning or end of sequences (U-shaped pattern)
- LoRA fine-tuning substantially improves compound question handling while maintaining general task capabilities
- Performance varies significantly across the five compound question types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LoRA fine-tuning on compound question data improves multi-question handling without degrading general task performance.
- **Mechanism:** Parameter-efficient adaptation creates specialized weights for query decomposition and sequential response synthesis, while freezing base weights preserves original capabilities.
- **Core assumption:** The improvements transfer across compound question types and are not limited to the training distribution.
- **Evidence anchors:** LLaMA-LoRA maintains MMLU (67.78→67.73), GSM8K (83.00→84.50), TruthfulQA (47.61→52.39); InternLM-LoRA similarly stable.

### Mechanism 2
- **Claim:** Sub-question performance follows a U-shaped pattern with position—first and last sub-questions are answered more accurately than middle positions.
- **Mechanism:** Attention mechanisms exhibit primacy and recency biases; middle positions receive diluted attention when sandwiched between competing sub-question contexts.
- **Core assumption:** The effect is structural to transformer attention patterns, not specific to the Factual-Statement type used in the position experiment.
- **Evidence anchors:** LLaMA: position 1XX=40.7, X1X=38.8, XX1=37.7 for sub-question 1; InternLM shows similar pattern.

### Mechanism 3
- **Claim:** Explicit decomposition instructions (Decom-S) improve compound question answering, but underperform compared to LoRA fine-tuning.
- **Mechanism:** Prompting-based decomposition forces explicit sub-question identification, reducing omission errors; however, without weight updates, models lack robust internal representations for synthesis.
- **Core assumption:** The gap between Decom-S and LoRA is due to learned vs. prompted behavior, not prompt quality.
- **Evidence anchors:** All four methods (CoT, Decom-S, Few-shot, LoRA) tested; LoRA achieves highest scores across dimensions.

## Foundational Learning

- **Concept: Multi-hop reasoning vs. compound questions**
  - **Why needed here:** Multi-hop benchmarks (HotpotQA) chain reasoning steps; compound questions require parallel sub-answer synthesis. Conflating these leads to wrong evaluation design.
  - **Quick check question:** Can a correct answer to sub-question A be computed without knowing sub-question B? If yes → compound; if no → multi-hop.

- **Concept: Win-rate evaluation with position swapping**
  - **Why needed here:** LLM evaluators exhibit position bias; swapping model response and reference order mitigates this when computing win rates.
  - **Quick check question:** If you evaluate Model A vs. Reference with A first, then swap to Reference vs. A, should the scores differ? If yes → position bias present.

- **Concept: Hierarchical vs. sequential sub-question dependencies**
  - **Why needed here:** The five compound types differ in dependency structure; FS is near-independent, CE is strictly sequential. Models may handle one better than the other.
  - **Quick check question:** In "Why did X happen? What are its effects?", can the second sub-question be answered without the first? If no → sequential dependency.

## Architecture Onboarding

- **Component map:** CQ-Syn Framework: LLM generator → Keyword filter → LLM filter → Human verification → Reference generation → Compound-QA Benchmark: 3 subsets × 5 types × 100 samples = 1,500 total → Evaluation: GPT-4o-mini judge → Win rate computation with position swap → Agreement validation

- **Critical path:** 1. Define compound question type taxonomy (FS/CE/HA/CS/ES) 2. Generate questions via CQ-Syn with type-specific prompts 3. Filter via keywords + LLM verification 4. Human review for accuracy (3 annotators, unanimous agreement required) 5. Generate references and evaluate models using win-rate protocol

- **Design tradeoffs:** Synthetic data (scalable, controllable) vs. natural data (more realistic but sparse for compound questions); GPT-4o evaluation (cost-efficient) vs. human evaluation (expensive, slow); LoRA fine-tuning (good performance, requires training) vs. prompting strategies (zero-cost inference, lower performance)

- **Failure signatures:** Omission error: Response shorter than expected; missing sub-question coverage; Confusion error: Blended responses that fail to distinguish related sub-questions; Off-topic error: Tangential content without logical progression to final answer

- **First 3 experiments:** 1. Baseline benchmarking: Run all 9 models on Compound-QA, compute win rates by type and dimension to establish performance gaps 2. Position ablation: Reorder Factual-Statement sub-questions (1XX/X1X/XX1) and measure per-position win rates to confirm U-shaped pattern 3. LoRA vs. prompting comparison: Fine-tune LLaMA and InternLM with LoRA on 70% split; compare against CoT, Decom-S, Few-shot on held-out 30% to validate improvement hierarchy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Compound-QA framework and evaluation metrics be effectively extended to multimodal contexts?
- **Basis in paper:** The Conclusion states, "We leave the exploration of evaluating compound questions in multimodal applications as future work."
- **Why unresolved:** The current study exclusively focuses on text-based question answering, leaving the interaction between compound questions and non-textual inputs (e.g., images, audio) unexplored.
- **What evidence would resolve it:** A study applying the CQ-Syn framework to generate multimodal compound questions and evaluating multimodal models (e.g., GPT-4o Vision) using the proposed dimensions.

### Open Question 2
- **Question:** To what extent does the reliance on proprietary models (GPT-4o) for data synthesis and evaluation introduce bias against open-source models?
- **Basis in paper:** Section 2.2 notes the use of GPT-4o for generation and Section 3.1 uses GPT-4o-mini for evaluation, while Section 3.2.1 shows open-source models performing significantly worse.
- **Why unresolved:** Using a specific model to generate ground truth and judge responses risks penalizing models whose reasoning styles or output distributions differ from the generator/judge, a known issue in LLM evaluation.
- **What evidence would resolve it:** A comparative analysis using human annotators to judge open-source model responses versus GPT-4o-mini judgments, specifically looking for systematic penalization of valid but stylistically different answers.

### Open Question 3
- **Question:** Does the "lost in the middle" phenomenon affect compound questions with strong logical dependencies (e.g., Cause-and-Effect) differently than independent ones?
- **Basis in paper:** Section 3.2.3 explicitly limits the position analysis to Factual-Statement questions because they are "relatively independent" and reordering others affects answers.
- **Why unresolved:** The authors demonstrated that sub-questions in the middle are harder for independent questions, but it is unclear if the cognitive load of maintaining logical dependencies in types like Cause-and-Effect exacerbates or masks this positional bias.
- **What evidence would resolve it:** An experiment measuring performance on the logical components of dependent questions (e.g., the "Effect" clause) when the context length or sub-question count is varied.

## Limitations

- The benchmark relies entirely on synthetic data generated by GPT-4o, which may not capture the full complexity of naturally occurring compound questions
- The evaluation methodology depends on GPT-4o-mini judgments that could introduce systematic biases
- Position-based performance findings are based only on Factual-Statement questions, limiting confidence in the claimed U-shaped pattern across all compound types

## Confidence

- **High confidence:** Overall performance gap between compound and non-compound questions, LoRA fine-tuning effectiveness, five-category taxonomy validity
- **Medium confidence:** Position-based performance patterns (limited to one question type), evaluator agreement robustness
- **Low confidence:** Synthetic data representativeness for real-world compound questions, generalizability of performance patterns across all compound types

## Next Checks

1. **Natural Data Validation:** Test the same models on a small sample of naturally occurring compound questions from conversational datasets to assess whether synthetic benchmark performance correlates with real-world behavior.

2. **Out-of-Distribution Testing:** Evaluate LoRA-finetuned models on compound questions from types not present in the training data to verify generalization claims beyond the five-category taxonomy.

3. **Cross-Evaluator Reliability:** Compare GPT-4o-mini evaluation results with human judgments on a stratified sample of responses across all five compound types to quantify evaluator bias and measurement error.