---
ver: rpa2
title: Building Open-Retrieval Conversational Question Answering Systems by Generating
  Synthetic Data and Decontextualizing User Questions
arxiv_id: '2507.04884'
source_url: https://arxiv.org/abs/2507.04884
tags:
- user
- question
- propositions
- dialogs
- dialog
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a pipeline to generate synthetic open-retrieval
  conversational question answering (OR-CONVQA) dialogs and annotations from domain-specific
  documents, addressing the lack of training data. The method converts documents into
  propositions, generates synthetic dialogs with contextualized and decontextualized
  user questions, and creates ground-truth proposition annotations.
---

# Building Open-Retrieval Conversational Question Answering Systems by Generating Synthetic Data and Decontextualizing User Questions

## Quick Facts
- arXiv ID: 2507.04884
- Source URL: https://arxiv.org/abs/2507.04884
- Reference count: 40
- Key outcome: Proposition-based synthetic dialogs improve retrieval performance (MAP 0.50→0.20 proprietary, 0.54→0.31 public) vs sentence-based chunking, with fine-tuned models matching LLM performance while reducing inference cost.

## Executive Summary
This paper addresses the data scarcity problem in open-retrieval conversational question answering (OR-CONVQA) by generating synthetic dialogs and annotations from domain documents. The authors propose converting documents into propositions (self-contained atomic sentences), then using a large language model to generate synthetic dialogs with both contextualized and decontextualized user questions. These synthetic dialogs are used to fine-tune lightweight query rewriters and retrievers. Results show that proposition-based dialogs significantly outperform sentence-based ones for retrieval, and fine-tuned models achieve comparable performance to larger LLMs while being more computationally efficient. The approach also introduces conditional rewriting to improve inference efficiency by avoiding unnecessary rewrites.

## Method Summary
The authors propose a pipeline to generate synthetic OR-CONVQA dialogs from domain documents. First, documents are converted into propositions using an LLM (Claude 3.5). Then, a Dialog-LLM generates synthetic dialogs with contextualized and decontextualized user questions, system responses, and ground-truth proposition annotations. This synthetic data is used to fine-tune lightweight query rewriters (T5-220M) and retrievers (MiniLM). The retrieval combines BM25 and dense embeddings using Reciprocal Rank Fusion. A conditional rewriting approach improves efficiency by determining whether rewriting is needed before generation. The method is evaluated on proprietary data and Doc2Dial, showing substantial improvements over sentence-based approaches and matching LLM performance.

## Key Results
- Proposition-based dialogs achieve MAP of 0.50 (proprietary) and 0.54 (public) vs 0.20 and 0.31 for sentence-based with decontextualized queries
- Fine-tuned T5 models match ground-truth decontextualized query performance (0.49 vs 0.50 proprietary, 0.52 vs 0.54 public)
- Conditional rewriting reduces average generation time from 0.19s to 0.09s (proprietary) and 0.24s to 0.10s (public) with no performance loss
- T5 fine-tuned on synthetic data improves MAP from 0.17 to 0.21 on real-world DOC2DIAL test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proposition-based document representation improves retrieval effectiveness over sentence-based chunking in conversational QA.
- Mechanism: Propositions are atomic, decontextualized, self-contained sentences expressing prominent document information. By storing propositions rather than longer document chunks, the retrieval pool contains less irrelevant information per unit, improving signal-to-noise ratio when matching against decontextualized queries.
- Core assumption: Users primarily ask questions about information important enough to be extracted as propositions; documents contain substantial non-query-worthy content.
- Evidence anchors:
  - [abstract]: "Results show that proposition-based dialogs lead to substantially better retrieval performance compared to sentence-based ones."
  - [section 4.2, Table 2]: RRF retrieval MAP for proposition-based dialogs (0.50 proprietary, 0.54 public) vs sentence-based (0.20, 0.31) with decontextualized queries.
  - [corpus]: Weak direct validation. Neighbor papers discuss RAG and QA systems but do not independently verify proposition-vs-sentence retrieval granularity claims.

### Mechanism 2
- Claim: Synthetic dialog generation from propositions produces training data sufficient to fine-tune lightweight retrievers and rewriters, reducing inference cost relative to prompting large LLMs.
- Mechanism: An LLM (Dialog-LLM) generates contextualized and decontextualized question pairs, system responses, and ground-truth proposition annotations. Fine-tuned models (T5-220M, MiniLM) learn to map conversational queries to retrieval-optimized forms without runtime LLM calls.
- Core assumption: Synthetic dialogs generated from propositions capture realistic discourse patterns (ellipsis, coreference) and information-seeking behavior similar to human dialogs.
- Evidence anchors:
  - [abstract]: "Fine-tuned models match the performance of larger LLMs while reducing computational cost."
  - [section 4.3, Table 3]: T5 fine-tuned on synthetic data achieves comparable MAP (0.49 proprietary, 0.52 public) to ground-truth decontextualized queries (0.50, 0.54), outperforming QRECC-trained T5.
  - [section 4.5, Table 5]: On real-world DOC2DIAL test sets, T5 fine-tuned on synthetic data improves MAP from 0.17 to 0.21; MiniLM retriever matches Claude-prompted rewriting (0.24 vs 0.25).
  - [corpus]: Neighbor paper "Leveraging Synthetic Data for Question Answering" supports synthetic data utility in domain-specific QA but does not replicate this pipeline.

### Mechanism 3
- Claim: Conditional question rewriting—classifying whether rewriting is needed before generating—reduces average latency with no performance loss.
- Mechanism: During training, decontextualized questions are prepended with "rewrite" or "no_rewrite" tokens based on whether they differ from contextualized versions. At inference, if "no_rewrite" is generated first, decoding stops and the original question is used.
- Core assumption: A substantial portion of user questions are already self-contained; detecting these avoids unnecessary computation.
- Evidence anchors:
  - [abstract]: "The proposed conditional rewriting approach also improves inference efficiency by avoiding unnecessary rewrites."
  - [section 4.7]: Average generation time reduced from 0.19s to 0.09s (proprietary) and 0.24s to 0.10s (public); 36% and 27% of questions required rewriting respectively.
  - [corpus]: No corpus validation for conditional rewriting specifically.

## Foundational Learning

- Concept: **Query decontextualization / rewriting**
  - Why needed here: Conversational queries contain ellipsis and coreference (e.g., "How do I apply for it?" where "it" refers to a prior topic). Off-the-shelf retrievers expect standalone queries; rewriting resolves context dependencies.
  - Quick check question: Given "User: What is the fee for expedited shipping? System: It's $15. User: And for standard?", can you write the decontextualized version of the last question?

- Concept: **Propositions vs. document chunks**
  - Why needed here: Standard RAG systems chunk documents by fixed token/character count or sentence boundaries. Propositions are semantic units—one atomic fact per unit—reducing irrelevant content in retrieved passages.
  - Quick check question: Why might retrieving a 10-sentence paragraph hurt precision compared to retrieving 2 relevant propositions embedded within it?

- Concept: **Reciprocal Rank Fusion (RRF)**
  - Why needed here: The paper combines sparse (BM25) and dense (embedding-based) retrieval scores. RRF merges ranked lists without needing score normalization.
  - Quick check question: If BM25 ranks document A at position 3 and dense retrieval ranks it at position 7 with k=60, what is the RRF score for A?

## Architecture Onboarding

- Component map:
  Proposition Extractor (LLM) -> Proposition Repository -> Query Rewriter (fine-tuned T5) -> Retriever (fine-tuned MiniLM) -> Response Generator (LLM)

- Critical path:
  1. Offline: Run Proposition Extractor over domain documents → store in Repository.
  2. Offline: Generate synthetic dialogs → fine-tune Rewriter and/or Retriever.
  3. Online: User query → Query Rewriter (conditional) → Retriever → top-k propositions → Response Generator.

- Design tradeoffs:
  - **Proposition granularity**: Smaller units improve precision but may fragment context; tune based on query complexity.
  - **Rewriter vs. dialog-aware retriever**: Rewriter is computationally cheaper; dialog-aware retriever may capture subtler context but requires more training data.
  - **LLM choice for synthetic generation**: Larger models (Claude, GPT-4) yield higher-quality propositions but increase offline cost; smaller models may hallucinate.

- Failure signatures:
  - **Retrieval drops on follow-up questions**: Rewriter failing to resolve coreferences; check decontextualized output manually.
  - **Hallucinated propositions**: Proposition Extractor inventing facts not in source; spot-check propositions against original documents.
  - **"No_rewrite" misclassification**: Conditional rewriter skipping necessary rewrites; monitor retrieval metrics by rewrite classification.

- First 3 experiments:
  1. **Baseline retrieval**: Index raw document chunks; measure MAP/Recall on a held-out set of decontextualized queries. Compare to proposition-based indexing.
  2. **Rewriter ablation**: Fine-tune T5 on synthetic dialogs; compare retrieval performance (a) no rewriting, (b) T5 rewriting, (c) LLM-prompted rewriting. Measure latency per query.
  3. **Conditional rewriting validation**: On a sample of 100 queries, manually label whether rewriting was needed; compare to model predictions to estimate false positive/negative rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can data augmentation techniques generate a sufficient volume of synthetic dialogs to successfully fine-tune lightweight models for response generation?
- Basis in paper: [explicit] The authors state in Section 5 that they plan to explore "means of generating more dialogs given the same set of documents (e.g., data augmentation), which will allow us to fine-tune the response-generator as well."
- Why unresolved: Preliminary experiments showed that lightweight response generators (e.g., T5) fine-tuned on the current synthetic volumes severely under-performed compared to prompted LLMs.
- What evidence would resolve it: Experiments demonstrating that a lightweight model fine-tuned on augmented synthetic data achieves response quality (e.g., METEOR, BERTScore) comparable to large LLMs on real-world test sets.

### Open Question 2
- Question: Can the proposed pipeline be effectively adapted to low-resource languages using multilingual LLMs or machine translation?
- Basis in paper: [explicit] Section 5 notes that "as dialogs in low resource languages are scarcer still, we plan to extend our pipeline to such languages."
- Why unresolved: The current study validates the pipeline only on English documents (proprietary and Doc2Dial); its efficacy in generating coherent, grounded dialogs in other languages remains untested.
- What evidence would resolve it: Evaluation results showing successful retrieval and response generation performance in a low-resource language using synthetic data generated by the adapted pipeline.

### Open Question 3
- Question: Is it feasible to substitute the large commercial LLM (Claude 3.5) with smaller, cost-effective models in the pipeline without significantly degrading proposition quality?
- Basis in paper: [inferred] Section 6 lists the dependence on "large costly LLMs" as a limitation, noting that applying the pipeline to datasets with millions of passages may be cost-prohibitive.
- Why unresolved: While the authors follow prior work using GPT-4/Claude for proposition generation, they do not test if the specific prompts used in this pipeline could work with smaller, cheaper models to reduce the barrier for large-scale adoption.
- What evidence would resolve it: A comparative study measuring the retrieval performance of systems built using synthetic data generated by a small open-source LLM versus the current large LLM baseline.

## Limitations
- Heavy reliance on a single proprietary LLM (Claude 3.5) for both proposition extraction and dialog generation creates a potential single-point-of-failure
- BM25 parameters are highly tuned (k1=0.05, b=5) and may not generalize to other domains without re-tuning
- Conditional rewriting performance gain is reported only on synthetic test sets, leaving real-world effectiveness unclear

## Confidence

- **High Confidence**: Proposition-based retrieval granularity outperforms sentence-based chunking (supported by clear MAP differences in Tables 2 and 3, with 0.50→0.20 and 0.54→0.31 improvements)
- **Medium Confidence**: Fine-tuned lightweight models match LLM performance (supported by strong numbers in Tables 3 and 5, but based on synthetic test sets and a single proprietary LLM)
- **Low Confidence**: Conditional rewriting reduces latency with no performance loss (supported only by synthetic test sets and without real-world validation or error analysis)

## Next Checks

1. **Proposition Quality Audit**: Manually inspect 100 randomly sampled propositions from the proprietary dataset for self-containment, accuracy, and hallucination. Measure the proportion containing unresolvable pronouns or invented facts.

2. **BM25 Parameter Sensitivity**: Re-run retrieval experiments with standard BM25 parameters (k1=1.2, b=0.75) and a range of values. Quantify performance degradation to establish tuning requirements for domain transfer.

3. **Conditional Rewriting Error Analysis**: On a held-out set of 200 real user queries (from Doc2Dial), manually label whether rewriting was actually needed. Compare to model predictions to compute false positive/negative rates and measure impact on retrieval metrics.