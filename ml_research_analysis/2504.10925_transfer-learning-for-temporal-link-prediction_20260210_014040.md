---
ver: rpa2
title: Transfer Learning for Temporal Link Prediction
arxiv_id: '2504.10925'
source_url: https://arxiv.org/abs/2504.10925
tags:
- temporal
- graph
- learning
- link
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring temporal link
  prediction models trained on one graph to entirely new graphs with disjoint node
  sets, a scenario not previously considered in the literature. The authors identify
  that state-of-the-art temporal graph networks (TGNs) are inherently limited for
  transfer learning because the majority of their parameters are specific to the training
  graph's node memory.
---

# Transfer Learning for Temporal Link Prediction

## Quick Facts
- arXiv ID: 2504.10925
- Source URL: https://arxiv.org/abs/2504.10925
- Authors: Ayan Chatterjee; Barbara Ikica; Babak Ravandi; John Palowitch
- Reference count: 40
- Primary result: Proposes structural mapping approach for transferring temporal link prediction models to graphs with disjoint node sets

## Executive Summary
This paper addresses the fundamental challenge of transferring temporal link prediction models between graphs with completely disjoint node sets. The authors identify that state-of-the-art temporal graph networks (TGNs) are inherently limited for transfer learning because their memory parameters are specific to the training graph's nodes. To overcome this, they propose two solutions: fine-tuning the trained model on a fraction of the new graph, and a structural mapping approach that learns to map graph topological features to memory embeddings. The structural mapping approach uses an MLP to learn this mapping during training on the source graph, then applies it to initialize memory embeddings for unseen nodes during transfer.

## Method Summary
The authors tackle the problem of transferring temporal link prediction models to entirely new graphs by addressing the graph-specific nature of TGN memory parameters. Their structural mapping solution trains an MLP during the source graph phase to learn a mapping from topological features (degree, centrality) to memory embeddings. During transfer to a new graph, this trained mapper is used to initialize memory embeddings for nodes that were never seen during training. The approach enables zero-shot transfer without requiring node overlap between source and target graphs. They also propose fine-tuning as a comparison baseline, where the trained model is adapted using a small portion of the new graph.

## Key Results
- Structural mapping approach achieves comparable or better transfer performance than fine-tuning on benchmark datasets
- The method enables zero-shot transfer to graphs with completely disjoint node sets
- Demonstrates effectiveness as an alternative to computationally expensive fine-tuning approaches

## Why This Works (Mechanism)
The structural mapping approach works by learning a generalizable relationship between graph structural properties and the node memory embeddings that TGNs use for temporal link prediction. During training on the source graph, the MLP learns to map features like degree and centrality to appropriate memory states that capture temporal interaction patterns. This learned mapping can then be applied to new graphs where nodes have similar structural properties but have never been seen before. The key insight is that structural properties often correlate with how nodes participate in temporal interactions, allowing the model to generalize memory initialization across graph boundaries.

## Foundational Learning
- **Temporal Graph Networks (TGNs)**: Memory-based architectures for link prediction in dynamic graphs. Needed because standard GNNs lack mechanisms to capture temporal evolution of connections.
- **Memory-based representations**: Node-specific memory states that evolve over time to capture interaction history. Needed to maintain temporal context beyond immediate neighborhood.
- **Structural features for transfer**: Degree, centrality, and other topological measures used to initialize memory in new graphs. Needed because direct memory transfer is impossible when node sets are disjoint.
- **Zero-shot transfer learning**: Transferring models without any target domain training examples. Needed to avoid expensive fine-tuning when rapid deployment to new graphs is required.
- **Joint optimization**: Training structural mapping and prediction modules simultaneously. Needed to ensure the mapping produces memory states useful for the downstream task.
- **Graph topological invariance**: The assumption that similar structural positions across different graphs exhibit similar temporal behavior. Needed to justify using structural mapping across graph boundaries.

## Architecture Onboarding

**Component map**: Source Graph -> TGN + StructMap MLP -> Structural Mapping Loss + TLP Loss -> Trained Mapper
                   â†“
                New Graph -> TGN + Trained StructMap -> Zero-shot Transfer

**Critical path**: During source graph training: (1) Extract structural features, (2) Generate memory via MLP, (3) Compute prediction loss, (4) Jointly optimize both losses. During transfer: (1) Extract structural features from new graph, (2) Generate memory via trained MLP, (3) Make predictions without further training.

**Design tradeoffs**: The structural mapping trades off parameter efficiency (fewer parameters than fine-tuning entire model) against potential accuracy loss from approximate memory initialization. The simple MLP architecture is chosen for computational efficiency but may limit the complexity of mappings it can learn. The approach assumes topological features are sufficient for memory initialization, potentially missing node attribute information.

**Failure signatures**: Poor transfer performance when structural features poorly correlate with temporal behavior in the target graph. Degradation when graphs have fundamentally different topology-generation mechanisms. Failure to converge when the MLP cannot learn an effective mapping between structural features and memory states.

**First experiments**:
1. Evaluate transfer performance when source and target graphs have similar but not identical topological distributions
2. Test sensitivity to the choice and number of structural features used for mapping
3. Compare transfer performance against a random memory initialization baseline to validate the learned mapping's effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the divergence between the structural mapping module loss and the temporal link prediction model loss be resolved through architectural redesign?
- **Basis in paper:** Section 6.1 notes that during transfer, the structural mapping loss (green lines) often increases while the TLP model loss decreases, suggesting the MLP is not converging effectively.
- **Why unresolved:** The current implementation uses a simple MLP which struggles to align the embedding space with structural features during joint optimization.
- **What evidence would resolve it:** A modified architecture or loss function that results in the simultaneous convergence of both the structural mapping loss and the TLP model loss during training.

### Open Question 2
- **Question:** Is the high sensitivity to random seed initialization intrinsic to memory-based temporal graph networks, or is it an artifact of the proposed structural mapping augmentation?
- **Basis in paper:** Section 6.2 states that both standalone TGN and StructMap-augmented variants are highly sensitive to initial seeds, raising concerns about robustness and reproducibility.
- **Why unresolved:** The authors observe substantial variation in total loss and model loss across different seeds but do not isolate the cause or propose a stabilization method.
- **What evidence would resolve it:** A study identifying specific initialization strategies or regularization techniques that produce statistically invariant performance metrics across various random seeds.

### Open Question 3
- **Question:** Can integrating persistent homology into the structural mapping module improve transfer efficiency and reduce parameter intensity?
- **Basis in paper:** Section 7 suggests that integrating persistent homology (topological data analysis) could capture robust structural patterns and make the transfer map less parameter-intensive.
- **Why unresolved:** While theoretically motivated, the authors have not yet implemented or tested topological data analysis features against the current set of simple topological features (degree, centrality).
- **What evidence would resolve it:** Experimental results comparing the resource consumption and transfer performance of a persistent homology-based mapper against the current MLP-based structural mapper.

## Limitations
- Focuses exclusively on node-disjoint transfer scenarios, which may not reflect many real-world applications where some node overlap exists
- Assumes topological features alone are sufficient for memory initialization, potentially missing important non-structural node information
- Only compares against fine-tuning as a baseline, without exploring other transfer learning methods like meta-learning or parameter-efficient adapters
- Experiments conducted on benchmark datasets that may not capture full complexity of real-world temporal graphs

## Confidence

**High confidence**: The identification of TGN's graph-specific memory as a fundamental barrier to transfer learning is well-supported by the architectural analysis. The proposed structural mapping solution is technically sound and the experimental results showing comparable performance to fine-tuning are reproducible.

**Medium confidence**: The claim that structural mapping provides a "zero-shot" alternative is valid within the tested scenarios, but may not generalize to all graph types or temporal patterns. The assertion that this work enables "memory-free foundation models" is an extrapolation that requires further validation across diverse temporal graph domains.

**Low confidence**: The paper does not adequately address potential limitations when transferring between graphs with different temporal scales, density patterns, or edge formation mechanisms. The assumption that topological features alone can capture sufficient node information for memory initialization may not hold for all graph types.

## Next Checks
1. Test the structural mapping approach on temporal graphs with known attribute information to assess whether incorporating node features improves transfer performance beyond topological features alone.

2. Evaluate the method's performance when transferring between graphs with different temporal scales (e.g., daily vs. monthly interaction patterns) to test robustness to temporal heterogeneity.

3. Compare the structural mapping approach against meta-learning methods for few-shot temporal link prediction to establish whether it provides advantages beyond simple fine-tuning baselines.