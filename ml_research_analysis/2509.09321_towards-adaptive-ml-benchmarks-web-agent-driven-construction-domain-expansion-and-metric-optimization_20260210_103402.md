---
ver: rpa2
title: 'Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion,
  and Metric Optimization'
arxiv_id: '2509.09321'
source_url: https://arxiv.org/abs/2509.09321
tags:
- medium
- hard
- easy
- task
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TAM-Bench addresses limitations in existing machine learning benchmarks
  by introducing an automated, web-agent-driven system that constructs a diverse,
  structured benchmark from real-world competition platforms like Kaggle, AIcrowd,
  and Biendata. The benchmark features automated task acquisition and standardization,
  leaderboard-based difficulty modeling, and multi-dimensional evaluation metrics
  that assess performance, constraint compliance, format validity, and task generalization.
---

# Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization

## Quick Facts
- arXiv ID: 2509.09321
- Source URL: https://arxiv.org/abs/2509.09321
- Authors: Hangyi Jia; Yuxi Qian; Hanwen Tong; Xinhui Wu; Lin Chen; Feng Wei
- Reference count: 22
- Key outcome: TAM-Bench introduces automated web-agent construction of diverse ML benchmarks from real-world competition platforms, featuring automated task acquisition, leaderboard-based difficulty modeling, and multi-dimensional evaluation metrics.

## Executive Summary
TAM-Bench addresses critical limitations in existing machine learning benchmarks by introducing an automated, web-agent-driven system that constructs diverse, structured benchmarks from real-world competition platforms like Kaggle, AIcrowd, and Biendata. The benchmark features automated task acquisition and standardization, leaderboard-based difficulty modeling, and multi-dimensional evaluation metrics that assess performance, constraint compliance, format validity, and task generalization. Experiments on 18 tasks from the Lite version using agents like AIDE and OpenHands reveal high variability in submission success and format compliance, with GPT-4.1-based agents showing superior reliability and DeepSeek-V3 demonstrating competitive performance on select tasks.

## Method Summary
TAM-Bench employs an automated web-agent pipeline using Browser-Use and LangChain's ReAct architecture to scrape competition tasks across platforms, extract markdown content, and unify schemas via GPT-4o. The system constructs a benchmark of 150 tasks spanning 6 modalities, with difficulty scores derived from normalized leaderboard metrics and participant counts. Experiments use AIDE and OpenHands agents with GPT-4.1 or DeepSeek-V3 models, evaluating submissions across weighted average rank, constraint pass rates, and format compliance metrics. The Lite version contains 18 tasks for rapid experimentation, while the Full version contains 150 tasks for comprehensive evaluation.

## Key Results
- AIDE + GPT-4.1 achieves highest submission rates (78%) and validity (78%) but ranks below OpenHands + DeepSeek-V3 on average
- DeepSeek-V3 achieves competitive average rank percentile (86%) despite lower submission and validity rates
- OpenHands + DeepSeek-V3 achieved first place on the challenging denoising-dirty-documents tabular task
- Agents show high variability in performance across tasks, with some achieving 100% rank percentile on graph and multimodal tasks due to format generation failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Web-agent automation reduces benchmark construction overhead while maintaining cross-platform task diversity.
- Mechanism: A four-layer pipeline (Agent → Controller → DOM → Browser) uses LangChain's ReAct architecture to parse task pages, extract markdown, and unify schemas via GPT-4o. Browser-Use enables platform-agnostic scraping without API dependencies.
- Core assumption: Competition platforms' HTML structures remain stable enough for DOM parsing; LLM-based schema extraction preserves essential task constraints.
- Evidence anchors: [section] "We implement an automated pipeline using the open-source WebAgent framework, Browser-Use... to fetch and process competition tasks across platforms." [section] "We enhance the controller with an extract markdown tool to extract clean Markdown content from task pages."

### Mechanism 2
- Claim: Leaderboard-derived difficulty scores correlate with expert annotations and enable automated task calibration.
- Mechanism: Normalize mean/best leaderboard scores by metric range, combine with log-scaled participant count (weights: 0.4, 0.1, 0.5), threshold into Easy/Medium/Hard. Captures community-level and ceiling difficulty.
- Core assumption: Participant count and score dispersion proxy task accessibility and intrinsic complexity; metric direction (higher/lower-is-better) is correctly identified.
- Evidence anchors: [section] "Tasks labeled as 'Hard' in MLEBench consistently fall within the Medium or Hard bins under our scheme. No manually difficult task is misclassified as Easy." [section] "We empirically set the weights to w1=0.4, w2=0.1, w3=0.5."

### Mechanism 3
- Claim: Multi-dimensional evaluation (rank + constraints + format) reveals reliability gaps hidden by single-metric benchmarks.
- Mechanism: Modality-weighted average rank reduces task-type bias. Constraint pass rate uses LLM-as-Judge to verify code against special instructions. Format compliance tracks submission generation and structural validity.
- Core assumption: GPT-4o can accurately judge constraint adherence from final code; modality weighting corrects representation imbalance.
- Evidence anchors: [abstract] "Experiments on 18 Lite tasks show agents vary widely in submission success (28-78%) and validity (56-78%), but average constraint pass rates remain high (~85-90%)." [section] "DeepSeek-V3 achieves an average rank percentile of 86%... however, the differences are small, and given the much lower submission and validity rates of DeepSeek-V3, this ranking advantage may be biased."

## Foundational Learning

- Concept: **ReAct Agent Architecture**
  - Why needed here: Powers the web-agent's task parsing and decision loop for cross-platform scraping.
  - Quick check question: Can you trace how an observation from DOM parsing triggers the next browser action?

- Concept: **Leaderboard Score Normalization**
  - Why needed here: Enables comparing difficulty across heterogeneous metrics (AUC vs. RMSE vs. accuracy).
  - Quick check question: Given a loss metric with Min=0.1, Max=1.0, Mean=0.4, what is NormMean?

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: Automates constraint compliance checking without manual code review.
  - Quick check question: What failure modes could arise if the judge only sees partial code fragments?

## Architecture Onboarding

- Component map: Agent Layer -> Controller Layer -> DOM Layer -> Schema Mapper -> Difficulty Scorer -> Evaluator
- Critical path: Platform scrape → Markdown extraction → Schema unification → Difficulty scoring → Benchmark subset selection → Agent evaluation → Multi-metric scoring
- Design tradeoffs: Lite (18 tasks) vs. Full (150 tasks): Speed vs. coverage; LLM judge vs. human review: Scalability vs. annotation fidelity; Modality weighting vs. uniform: Bias correction vs. simplicity
- Failure signatures: Low Made Submission rate: Agent fails to complete pipeline (likely code generation errors); High rank but low validity: Reward hacking (good scores, bad formats); 100% rank percentile on Graph/MultiModal: Agent cannot produce valid outputs
- First 3 experiments: 1) Run AIDE + GPT-4.1 on Lite benchmark, log Made Submission / Valid Submission / Constraint Pass per task. 2) Swap DeepSeek-V3 as base model, compare submission rates and difficulty-stratified ranks. 3) Manually inspect 3 failed constraints to validate LLM judge accuracy against ground truth.

## Open Questions the Paper Calls Out
None

## Limitations
- Automated web-agent pipeline reliability depends on competition platform HTML structure stability and could break with UI changes
- Difficulty modeling assumes leaderboard signals remain reliable despite potential gaming or insufficient participants
- Multi-dimensional evaluation introduces complexity in interpreting trade-offs between rank performance and submission reliability

## Confidence
- **High confidence**: Lite benchmark task quality and basic construction pipeline; leaderboard-based difficulty thresholds for easy/medium/hard classification (validated against MLEBench); observed submission rate variations across agents
- **Medium confidence**: Automated schema unification preserving all critical task constraints; LLM-as-Judge constraint compliance accuracy; modality-weighted rank metric's ability to correct representation bias
- **Low confidence**: Generalizability to Full benchmark (150 tasks); stability of web-agent pipeline across platform UI changes; real-world applicability of difficulty scores for task selection

## Next Checks
1. Run AIDE + GPT-4.1 on 3-5 new competition tasks not in TAM-Bench to test web-agent pipeline robustness and schema extraction accuracy.

2. Conduct human annotation of constraint compliance on 10 random submissions to validate LLM-as-Judge accuracy and identify systematic failure modes.

3. Stress-test difficulty modeling by injecting synthetic participants/score distributions and measuring sensitivity of Easy/Medium/Hard thresholds.