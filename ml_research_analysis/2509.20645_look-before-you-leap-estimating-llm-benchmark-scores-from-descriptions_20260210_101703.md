---
ver: rpa2
title: 'Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions'
arxiv_id: '2509.20645'
source_url: https://arxiv.org/abs/2509.20645
tags:
- evaluation
- arxiv
- language
- performance
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of predicting large language model
  (LLM) performance on new tasks without running any experiments. The authors propose
  a novel "text-only performance forecasting" method, where an LLM predicts performance
  scores based solely on a textual description of the task and experimental setup.
---

# Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions

## Quick Facts
- arXiv ID: 2509.20645
- Source URL: https://arxiv.org/abs/2509.20645
- Reference count: 40
- Key result: GPT-5 predicts LLM benchmark scores from text descriptions with MAE 14.6 overall, dropping to 9.9 for high-confidence predictions

## Executive Summary
This paper introduces a novel "text-only performance forecasting" approach where an LLM predicts benchmark scores from textual task descriptions without running experiments. The authors curate PRECOG, a dataset of 2,290 description-performance pairs from 1,519 papers, to systematically study this task. Their results show that reasoning-enabled models like GPT-5 can achieve moderate prediction accuracy (MAE 14.6), with error dropping to 9.9 for high-confidence predictions. The method generalizes to post-knowledge-cutoff and streaming predictions, suggesting practical utility for experiment planning and resource allocation.

## Method Summary
The method uses a reasoning-enabled LLM to predict normalized benchmark scores (0-100) from redacted task descriptions. Given a description following a fixed schema (task, data collection, evaluation, difficulty cues, etc.), the model outputs a score in \boxed{} along with an optional confidence category. The approach can use optional retrieval over arXiv to find related papers, excluding source papers by ID. The PRECOG dataset was constructed by scraping arXiv papers, extracting experimental records, retrieving dataset papers, and normalizing scores across different metrics to a 0-100 scale.

## Key Results
- GPT-5 achieves MAE of 14.6 on leakage-controlled test set, dropping to 9.9 for high-confidence predictions
- Performance generalizes to post-knowledge-cutoff predictions (2025 papers) with similar accuracy
- Reasoning mode consistently improves predictions compared to non-reasoning variants
- Retrieval helps GPT-5 (+0.04 correlation) but hurts Qwen3 (-0.09), suggesting model-dependent effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Feature extraction and compositional reasoning
Reasoning models can map task descriptions to performance estimates by identifying difficulty-relevant features and comparing against known benchmark patterns. The model extracts salient features from descriptions (e.g., "20 answer options," "greedy decoding," "college-level materials"), then reasons about how each feature affects difficulty relative to known baselines. Pre-training exposes the model to sufficient benchmark-result patterns that it can generalize to novel task descriptions through compositional reasoning.

### Mechanism 2: Confidence-based filtering
Self-reported confidence scores provide a reliable filter for prediction quality, enabling selective deployment. After generating a prediction, the model assesses its own reasoning and evidence quality, outputting a verbalized confidence category. Higher confidence correlates with lower absolute error because confident predictions occur when descriptions match familiar patterns in the model's knowledge.

### Mechanism 3: Generalization beyond memorization
Prediction accuracy generalizes to truly unseen benchmarks because the model learns task-structure patterns rather than memorizing specific benchmark-result pairs. The leakage-controlled test split (2025 papers after knowledge cutoff) and streaming evaluation prevent direct memorization. The model instead relies on compositional understanding of how task properties affect performance across the model class being evaluated.

## Foundational Learning

- **Concept: Regression from natural language descriptions**
  - Why needed here: The task maps unstructured text to a continuous score (0-100), requiring understanding of how textual features numerically influence outcomes.
  - Quick check question: Can you explain why the paper normalizes all metrics to a 0-100 scale rather than predicting raw metric values?

- **Concept: Knowledge cutoff and data contamination**
  - Why needed here: The paper's credibility hinges on demonstrating that predictions generalize beyond training data, requiring careful temporal splits.
  - Quick check question: Why does using papers published after GPT-5's knowledge cutoff strengthen the claim that prediction isn't just memorization?

- **Concept: Calibration and uncertainty quantification**
  - Why needed here: Practical deployment requires knowing when to trust predictions; confidence calibration enables selective use.
  - Quick check question: If a model's confidence scores were perfectly calibrated, what would you expect when filtering to only high-confidence predictions?

## Architecture Onboarding

- **Component map:**
  Description extractor (GPT-5-mini) -> Predictor (GPT-5/Qwen3) -> Retrieval module (arXiv API) -> Evaluator

- **Critical path:**
  1. Curate PRECOG: scrape arXiv → extract experimental records → retrieve dataset papers → generate anonymized descriptions → normalize scores
  2. Run prediction: feed description to reasoning model → model optionally searches → model outputs \boxed{score} and \confidence{level}
  3. Evaluate: compute MAE and Pearson correlation on test split; analyze calibration curves

- **Design tradeoffs:**
  - Retrieval vs. no retrieval: Retrieval helps GPT-5 (+0.04 correlation) but hurts Qwen3 (-0.09); most signal from internal knowledge
  - Reasoning vs. non-reasoning: Reasoning mode consistently improves predictions; critical for this task
  - Coverage vs. accuracy: Higher confidence thresholds reduce MAE but cover fewer examples

- **Failure signatures:**
  - High MAE (>25) on specific metrics suggests insufficient training examples for those metric types
  - Predictions on GPT-4o worse than GPT-4 suggests newer/less-exposed models are harder to forecast
  - Math and instruction-following benchmarks show higher error, likely due to diverse formulations

- **First 3 experiments:**
  1. Baseline replication: Run GPT-5 (or available reasoning model) on PRECOG-2025 descriptions without retrieval; verify MAE and correlation match paper's ~14-15 range.
  2. Confidence calibration analysis: Plot MAE vs. coverage across confidence thresholds; confirm that filtering to high-confidence predictions reduces error.
  3. Ablation on description completeness: Remove one schema field (e.g., "difficulty cues") from descriptions and measure MAE increase to quantify feature importance.

## Open Questions the Paper Calls Out

### Open Question 1
Can text-only forecasting methods successfully extrapolate to predict performance for model architectures or paradigms that differ significantly from the frontier proprietary models (GPT-4/4o) used to curate the PRECOG dataset? The authors explicitly limit the dataset to "non-finetunable, proprietary models" to reduce variance, leaving the generalizability to open-weights or structurally different models untested.

### Open Question 2
Why does external retrieval significantly improve prediction accuracy for GPT-5 but fail to benefit (or slightly hurt) the Qwen3-32B baseline? The paper observes the discrepancy and attributes it to GPT-5's superior ability to aggregate diverse evidence, but the specific failure mode of Qwen3 in utilizing retrieved context remains unidentified.

### Open Question 3
Is the high error rate in predicting "Recall" and "Precision" metrics solely a function of training data sparsity, or does it reflect a fundamental limitation in mapping descriptions to trade-off metrics? The paper hypothesizes it is due to fewer training records, but it's unclear if simply adding more data will resolve the error.

### Open Question 4
Does integrating description-based forecasting into the experimental design loop actually reduce the net resource consumption (time, compute, annotation) compared to traditional pilot studies? While the paper claims efficiency benefits, it does not quantify these gains in a real user study.

## Limitations
- Model access: GPT-5 was not publicly available at time of writing, limiting reproducibility
- Confidence calibration: No direct validation of whether verbalized confidence accurately reflects true uncertainty
- Metric coverage: High MAE on precision/recall and math benchmarks suggests the method is not universally reliable across all task types

## Confidence
- **High confidence:** PRECOG dataset construction and overall feasibility of text-only forecasting
- **Medium confidence:** Superiority of reasoning mode and effectiveness of confidence-based filtering
- **Low confidence:** Generalization to streaming/continual forecasting and robustness to novel task types

## Next Checks
1. Reproduce baseline MAE: Run the same zero-shot prediction pipeline on PRECOG-2025 descriptions (without retrieval) using an available reasoning model; confirm that MAE and correlation are in the reported range.
2. Validate confidence calibration: Plot MAE vs. coverage across confidence thresholds; check if high-confidence predictions consistently show lower error than the overall average.
3. Test description completeness: Systematically remove one schema field at a time from descriptions and measure the increase in MAE; quantify which features are most critical for accurate forecasting.