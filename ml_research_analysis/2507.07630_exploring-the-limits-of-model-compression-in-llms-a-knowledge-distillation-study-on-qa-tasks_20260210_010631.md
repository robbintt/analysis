---
ver: rpa2
title: 'Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation
  Study on QA Tasks'
arxiv_id: '2507.07630'
source_url: https://arxiv.org/abs/2507.07630
tags:
- performance
- prompting
- qwen2
- one-shot
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates knowledge distillation (KD) for compressing
  large language models (LLMs) on question answering (QA) tasks. Student models from
  Pythia and Qwen2.5 families were distilled from teachers of 7B and 2.8B parameters
  down to as small as 70M parameters.
---

# Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks

## Quick Facts
- arXiv ID: 2507.07630
- Source URL: https://arxiv.org/abs/2507.07630
- Reference count: 12
- Students retained >90% teacher performance while reducing parameter counts by up to 57.1%

## Executive Summary
This study investigates knowledge distillation (KD) for compressing large language models (LLMs) on question answering (QA) tasks. Student models from Pythia and Qwen2.5 families were distilled from teachers of 7B and 2.8B parameters down to as small as 70M parameters. These models were evaluated on SQuAD v2.0 and MLQA datasets under zero-shot and one-shot prompting. Results show that student models retained over 90% of teacher performance while reducing parameter counts by up to 57.1%. One-shot prompting provided consistent performance gains over zero-shot setups. Distilled models outperformed fine-tuned counterparts, particularly at moderate scales. However, smaller models struggled with in-context learning, and some showed inconsistent performance between validation and test sets. Overall, KD combined with few-shot prompting proves effective for building compact, high-performing QA systems suitable for resource-constrained environments.

## Method Summary
The study employs knowledge distillation to compress LLMs for extractive QA tasks using SQuAD v2.0 and MLQA (English/German) datasets. Teachers (Qwen2.5-7B, Pythia-2.8B) were fine-tuned on each dataset under zero-shot and one-shot prompting setups. Student models from Qwen2.5 (3B, 1.5B, 0.5B) and Pythia (1.4B, 1B, 410M, 160M, 70M) families were distilled using the DISTILLM framework. One-shot prompts included a held-out demonstration set excluded from train/test splits. Models were evaluated using Exact Match (EM) and ROUGE-L metrics across 5 random seeds, with results reported as mean ± std.

## Key Results
- Student models retained over 90% of teacher performance while reducing parameter counts by up to 57.1%
- One-shot prompting provided consistent performance gains over zero-shot setups
- Distilled models outperformed fine-tuned counterparts, particularly at moderate scales
- Smaller models (e.g., Pythia-70M) struggled with in-context learning and showed inconsistent validation-test performance

## Why This Works (Mechanism)
Knowledge distillation transfers knowledge from larger teacher models to smaller student models through soft label supervision, allowing students to learn nuanced decision boundaries that would be difficult to discover through standard training alone. The DISTILLM framework enables this transfer while preserving task-specific capabilities. Few-shot prompting provides demonstration context that helps smaller models better understand task format and expected outputs, effectively augmenting their limited parameter capacity with task-specific guidance.

## Foundational Learning
- **Knowledge Distillation**: Why needed: Compresses large models while preserving performance. Quick check: Compare student vs teacher metrics on held-out data.
- **In-Context Learning**: Why needed: Enables models to follow task format without parameter updates. Quick check: Test with/without demonstrations in prompts.
- **Extractive QA**: Why needed: Task requires precise span identification from text. Quick check: Verify EM and ROUGE-L alignment on validation set.

## Architecture Onboarding
**Component Map**: Teachers (fine-tuned) -> DISTILLM distillation -> Students (evaluated)
**Critical Path**: Fine-tune teachers → Create prompts → DISTILLM distillation → Evaluation
**Design Tradeoffs**: Model size vs performance vs inference cost; zero-shot vs one-shot prompting complexity
**Failure Signatures**: Validation-test discrepancy (overfitting to prompts); small models failing ICL; high variance across seeds
**First Experiments**:
1. Fine-tune teacher on SQuAD v2.0 zero-shot, verify baseline performance
2. Create one-shot prompt variant, test teacher performance gain
3. Run DISTILLM distillation on smallest student family, evaluate performance retention

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters for teacher fine-tuning and student distillation are not specified
- Validation-test performance inconsistency observed for smaller models under one-shot prompting
- Unclear capacity threshold where ICL benefits diminish
- Hardware specifications and training durations not reported

## Confidence
**High Confidence**: KD with few-shot prompting effectively compresses LLMs while retaining >90% performance
**Medium Confidence**: One-shot prompting consistently improves performance, though magnitude varies
**Medium Confidence**: Distilled models outperform fine-tuned counterparts at moderate scales
**Low Confidence**: Specific capacity threshold for ICL effectiveness is not precisely quantified

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Vary key distillation hyperparameters to identify robust configurations
2. **Cross-Validation of Validation-Test Consistency**: Investigate one-shot setup discrepancies across splits
3. **ICL Capacity Threshold Mapping**: Systematically test model sizes to map transition point for ICL benefits