---
ver: rpa2
title: Dynamic Mixture-of-Experts for Incremental Graph Learning
arxiv_id: '2508.09974'
source_url: https://arxiv.org/abs/2508.09974
tags:
- data
- learning
- graph
- experts
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in graph incremental
  learning by proposing a Dynamic Mixture-of-Experts (DyMoE) framework. The key idea
  is to dynamically add expert networks specialized for each incoming data block while
  maintaining existing experts, with a gating mechanism to route nodes to the most
  relevant experts.
---

# Dynamic Mixture-of-Experts for Incremental Graph Learning

## Quick Facts
- **arXiv ID:** 2508.09974
- **Source URL:** https://arxiv.org/abs/2508.09974
- **Reference count:** 40
- **Primary result:** DyMoE achieves 4.92% relative accuracy increase over best baselines on class incremental learning tasks

## Executive Summary
This paper addresses catastrophic forgetting in graph incremental learning by proposing a Dynamic Mixture-of-Experts (DyMoE) framework. The key idea is to dynamically add expert networks specialized for each incoming data block while maintaining existing experts, with a gating mechanism to route nodes to the most relevant experts. The method includes a block-guided loss to ensure proper expert assignment and a graph block-guided loss to handle topology changes in incremental graph learning. A sparse variant uses only the top-k experts for efficiency. Experiments show DyMoE achieves strong performance across multiple graph datasets while maintaining computational efficiency.

## Method Summary
DyMoE introduces a dynamic architecture where a new expert network is added for each incoming data block, while previous experts are frozen to preserve knowledge. The framework uses a gating mechanism to route nodes to appropriate experts, with block-guided loss ensuring new experts receive training signal. A graph block-guided loss masks future node influence on old experts to handle topology changes. The sparse variant activates only top-k experts per node for efficiency. Training occurs only on the new expert and gating vectors, with a small memory buffer maintaining exemplars from previous blocks for router calibration.

## Key Results
- DyMoE achieves 4.92% relative accuracy increase compared to best baselines on class incremental learning tasks
- Maintains similar computational efficiency through sparse top-k expert selection
- Demonstrates strong performance across CoraFull, Reddit, Arxiv, and DBLP datasets
- Shows effectiveness in both class incremental learning (CIL) and instance incremental learning (IIL) settings

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Expert Expansion for Parameter Isolation
- **Claim:** Allocating distinct parameters (experts) to distinct temporal data blocks minimizes interference between old and new knowledge.
- **Mechanism:** When data block $t$ arrives, a new expert network $f_t$ is initialized and trained while previous experts $\{f_1, ..., f_{t-1}\}$ are frozen, isolating gradient updates.
- **Core assumption:** Knowledge from different timestamps is sufficiently distinct that it benefits from separate parameter spaces rather than shared weights with regularization.
- **Evidence anchors:** [abstract] "DyMoE assigns specialized experts to process data from different timestamps." [section 3.1] "training scheme completely preserves the knowledge obtained from previous data blocks."
- **Break condition:** If data blocks are highly correlated, allocating a full new expert wastes capacity and may dilute the signal.

### Mechanism 2: Block-Guided Gating for Router Specialization
- **Claim:** Supervising the router with temporal block indices ensures inputs are processed by the correct expert, mitigating the "local minimum" where a randomly initialized new expert is ignored.
- **Mechanism:** Adds Block-Guided Loss ($L_{BL}$), a cross-entropy objective forcing the gating value of expert $t$ to be high for data from block $t$, acting as explicit supervision for the router.
- **Core assumption:** The system has access to the timestamp/block index $b(v)$ during training.
- **Evidence anchors:** [section 3.1] "...inject the information about the correct experts for our dynamically initialized new modules." [section 5.2] "When the block-guided loss is absent, the experts fail to specialize."
- **Break condition:** If the memory buffer size $p$ is too small to represent the distribution of older blocks, the router may learn a biased mapping.

### Mechanism 3: Topology-Aware Attention Masking
- **Claim:** Filtering the influence of "future" nodes during message passing stabilizes the representations of old experts when graph topology changes.
- **Mechanism:** Modifies attention in old experts to down-weight neighbors that appeared after the expert was trained, forcing the old expert to approximate the graph state as it existed during its original training.
- **Core assumption:** New edges connecting to old nodes represent distribution shifts that are detrimental to the old expert's learned parameters.
- **Evidence anchors:** [abstract] "...uses graph block-guided attention to suppress future node influence on old experts." [section 3.2] Equation 13 shows modified attention with $\log(\beta_t)$ added to attention scores.
- **Break condition:** If new edges provide critical context that improves classification of old nodes, this masking would artificially constrain performance.

## Foundational Learning

- **Concept:** **Catastrophic Forgetting (Stability-Plasticity Dilemma)**
  - **Why needed here:** DyMoE is fundamentally designed to solve this by trading off the stability of frozen experts against the plasticity of new ones.
  - **Quick check question:** If I fine-tune a standard GNN on a new graph snapshot, why does accuracy drop on the old snapshot?

- **Concept:** **Mixture-of-Experts (MoE) & Sparse Gating**
  - **Why needed here:** The architecture relies on sparse gating (activating only top-$k$ experts) to scale efficiency, preventing computational cost from growing linearly with time.
  - **Quick check question:** How does a "Top-K" gate differ from a standard dense layer in terms of computational activation during a forward pass?

- **Concept:** **Graph Message Passing (Receptive Fields)**
  - **Why needed here:** Unlike images, graph nodes have variable neighbors. Mechanism 3 specifically manipulates this message passing, so understanding that node $v$ aggregates info from $N(v)$ is required.
  - **Quick check question:** Why does adding a node $u$ at time $t$ change the representation of an existing node $v$ at time $t-1$ if they become connected?

## Architecture Onboarding

- **Component map:** Input data -> Gating vectors ($g_i$) -> Top-K expert selection -> Expert MLPs and Attention weights -> Weighted sum aggregation -> Output
- **Critical path:**
  1. Input: Batch of nodes $X$ with block indices
  2. Gating: Calculate similarity between node embeddings and gating vectors; apply Top-K mask
  3. Expert Processing: Route nodes to active experts; apply Block-Guided Attention (masking future neighbors)
  4. Aggregation: Weighted sum of expert outputs
  5. Loss Calculation: $L = L_{cls} + \gamma L_{BL} + \delta L_{GBL}$

- **Design tradeoffs:**
  - **Memory Size ($p$):** Larger memory improves router accuracy but increases training overhead. Paper suggests $p < 0.05$.
  - **Top-k ($k$):** Low $k$ improves speed but risks dropping relevant experts for complex nodes (Table 5 shows $k=3$ is a sweet spot).
  - **Interleaving:** Integrating MoE into every GNN layer is computationally heavier but handles graph topology better than a single MoE head at the end.

- **Failure signatures:**
  - **"Dead Experts":** A new expert never receives gradients. Fix: Check Block-Guided Loss weight ($\gamma$) or use noise injection to encourage exploration.
  - **Accuracy Collapse on Old Data:** If $L_{GBL}$ is too low ($\delta$ is small), old experts may be corrupted by new topological connections. Fix: Increase $\delta$ to enforce stricter neighbor masking.

- **First 3 experiments:**
  1. **Router Validation:** Train on 3 blocks. Visualize the gating matrix (rows=blocks, cols=experts). Verify if the matrix is approximately diagonal (Block $i$ routes to Expert $i$).
  2. **Ablation on Topology:** Run DyMoE with and without Graph Block-Guided Loss ($L_{GBL}$) on a dataset with heavy edge additions (like CoraFull). Quantify the forgetting gap.
  3. **Sparsity Scaling:** Benchmark inference latency (ms/batch) while increasing the number of active experts ($k=1$ to $5$) to verify the efficiency claim of the sparse MoE variant.

## Open Questions the Paper Calls Out
None

## Limitations
- Temporal expert relevance assumption may not hold across datasets with varying degrees of temporal correlation
- Memory buffer size (p) is presented as critical hyperparameter without fully characterizing impact of memory scarcity on long-term forgetting
- Paper doesn't quantify magnitude of forgetting specifically attributable to structural drift versus other factors like class distribution shifts

## Confidence
- **High Confidence:** Core architectural contribution and empirical performance advantage over baselines are well-supported by experimental results
- **Medium Confidence:** Theoretical justification for topology-aware masking is sound but practical necessity varies by dataset
- **Medium Confidence:** Computational efficiency claims are supported but real-world scaling behavior with very long sequences of blocks is not extensively explored

## Next Checks
1. **Router Specialization Validation:** Train DyMoE on 3-4 blocks, then visualize the gating matrix to verify diagonal structure (block t routes primarily to expert t).
2. **Topology Change Ablation:** Run DyMoE with and without Graph Block-Guided Loss on CoraFull, measuring the specific forgetting gap attributable to topology changes.
3. **Memory Scaling Study:** Systematically vary the memory buffer size (p) from 0.01 to 0.1 and measure the trade-off between Average Accuracy and Average Forgetting.