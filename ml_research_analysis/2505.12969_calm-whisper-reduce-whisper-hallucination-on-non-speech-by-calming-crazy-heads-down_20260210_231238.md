---
ver: rpa2
title: 'Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy
  Heads Down'
arxiv_id: '2505.12969'
source_url: https://arxiv.org/abs/2505.12969
tags:
- hallucination
- heads
- speech
- non-speech
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses hallucination issues in OpenAI's Whisper model
  when processing non-speech audio segments, which is particularly problematic in
  industrial settings. The authors introduce a novel approach called Calm-Whisper
  that identifies and fine-tunes specific hallucinatory attention heads in the Whisper
  decoder without using any pre- or post-processing techniques.
---

# Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down

## Quick Facts
- arXiv ID: 2505.12969
- Source URL: https://arxiv.org/abs/2505.12969
- Reference count: 0
- Primary result: 80% hallucination reduction (99.97% → 15.51%) on non-speech with <0.1% WER increase

## Executive Summary
The paper addresses hallucination issues in OpenAI's Whisper model when processing non-speech audio segments, which is particularly problematic in industrial settings. The authors introduce Calm-Whisper, a novel approach that identifies and fine-tunes specific hallucinatory attention heads in the Whisper decoder without using any pre- or post-processing techniques. Through systematic head-wise masking experiments, they discover that only 3 out of 20 decoder heads are responsible for over 75% of hallucinations, enabling targeted fine-tuning that dramatically reduces hallucination while preserving speech recognition accuracy.

## Method Summary
The method involves three key steps: First, systematically mask each of the 20 decoder heads and measure hallucination rate on UrbanSound8K to identify the three heads (#1, #6, #11) responsible for most hallucinations. Second, fine-tune only these three heads on non-speech audio paired with blank labels while freezing all other parameters. Third, determine the optimal fine-tuning duration (5-6 epochs) that balances hallucination reduction with minimal WER degradation. The approach uses no pre- or post-processing, relying solely on internal model modifications to address the hallucination problem.

## Key Results
- 80.46% reduction in non-speech hallucination on UrbanSound8K (99.97% → 15.51%)
- Minimal speech recognition degradation (0.07% WER increase on LibriSpeech test-clean)
- Only 3 out of 20 decoder heads account for over 75% of hallucinations
- Fine-tuning only 3 heads preserves speech recognition (vs 100% WER when fine-tuning entire decoder)

## Why This Works (Mechanism)

### Mechanism 1: Localized Hallucination Attribution in Attention Heads
Hallucination on non-speech inputs concentrates in a small subset of self-attention heads rather than being uniformly distributed. By systematically masking each decoder head and measuring hallucination rate, heads #1, #6, and #11 are identified as contributing over 75% of hallucinations. This suggests these heads are hyper-sensitive to acoustic noise patterns and erroneously activate language modeling pathways when processing non-speech audio.

### Mechanism 2: Selective Head Fine-Tuning Preserves Global Competence
Fine-tuning only the identified hallucinatory heads on non-speech audio with blank labels reduces hallucination while preserving speech recognition accuracy. Freezing all parameters except heads #1, #6, #11 creates a constrained optimization where the model learns to suppress spurious text generation for noise inputs without overwriting pretrained speech representations. The frozen heads retain their learned speech-language mappings, providing redundancy that maintains overall model competence.

### Mechanism 3: Training Depth Governs Hallucination-Accuracy Trade-off
The number of fine-tuning epochs controls a non-linear trade-off between hallucination reduction and recognition degradation. Early epochs sharply reduce hallucination as the target heads learn to output blank tokens for noise inputs; beyond an optimal point (5-6 epochs), WER increases accelerate as the heads begin to overfit to silence and suppress legitimate speech activations. This creates a sweet spot where hallucination is minimized while speech recognition remains intact.

## Foundational Learning

- **Self-Attention Head Specialization**: Different attention heads can learn distinct functions, with some being noise-sensitive while others are robust. This specialization is prerequisite to the head-masking methodology.
  - Quick check: Can you explain why masking one attention head might improve performance on a specific task while degrading another?

- **Blank Label Training for Non-Speech**: Using empty transcriptions as targets for non-speech inputs shapes the model to suppress output generation for noise. This requires understanding how supervised learning can train output suppression.
  - Quick check: What would happen if you trained on non-speech with random text labels instead of blank labels?

- **Encoder-Decoder vs. CTC Architectures**: The paper contrasts Whisper (encoder-decoder with 99.97% hallucination) against Conformer-CTC (13.52% hallucination), highlighting architectural susceptibility to hallucination. This distinction is important for understanding why autoregressive decoders are more prone to hallucination than CTC-based models on non-speech inputs.
  - Quick check: Why might autoregressive decoders be more prone to hallucination than CTC-based models on non-speech inputs?

## Architecture Onboarding

- **Component map**: Audio input → Encoder → Decoder self-attention (heads #1, #6, #11 critical) → Token output. For non-speech, the critical failure path is these heads erroneously attending to noise features and triggering text generation.
- **Critical path**: The decoder's self-attention mechanism, specifically the query/key/value projections of heads #1, #6, #11 across all decoder layers, forms the critical path where hallucination occurs on non-speech inputs.
- **Design tradeoffs**: (1) More epochs → lower hallucination but higher WER risk; (2) Masking heads at inference → simpler but degrades WER more (3.57% vs 2.19%); (3) Full decoder fine-tuning → eliminates hallucination but destroys speech recognition (100% WER).
- **Failure signatures**: (1) Non-speech audio transcribed as filler words (" so" appears in 55.2% of hallucinations); (2) Hallucinations >5 tokens indicate severe failure mode; (3) WER spike >1% suggests over-fine-tuning.
- **First 3 experiments**: 1) Replicate single-head masking on UrbanSound8K to verify heads #1, #6, #11 are hallucinatory for your deployment audio distribution; 2) Fine-tune only head #1 for 3 epochs as a minimal intervention; measure hallucination reduction vs. WER impact; 3) Evaluate fine-tuned model on domain-specific non-speech (e.g., call center hold music, HVAC noise) to verify generalization beyond UrbanSound8K.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the mechanistic explanation for the complex interplay between attention heads, where masking multiple "hallucinatory" heads sometimes proves less effective than masking a single head? The authors observed that masking heads #1 and #11 together increases hallucination compared to masking head #1 alone, indicating complex interactions that were not resolved in the current study.

- **Open Question 2**: Does the "calm-down" fine-tuning on non-speech audio generalize to mitigate hallucinations that occur during silent gaps within long-form spoken content? The current evaluation uses isolated UrbanSound8K clips and LibriSpeech utterances, but it's unclear if the approach transfers effectively to maintaining silence during internal pauses in continuous speech streams.

- **Open Question 3**: Does reducing non-speech hallucination via specific head fine-tuning inadvertently affect the model's behavior regarding speech-content hallucinations? The proposed method targets non-speech hallucinations, but it's not established if the fine-tuned heads also contribute to generative errors during speech, which would require qualitative and quantitative analysis of speech-only hallucinations.

## Limitations

- **Architectural Generalization**: The identification of specific hallucinatory heads (#1, #6, #11) is based on UrbanSound8K testing and may not generalize to all non-speech audio distributions or Whisper deployments.
- **Fine-tuning Duration Trade-off**: The optimal 5-6 epoch count is based on a specific 105-hour non-speech training set and may vary significantly with different training data characteristics or distributions.
- **Implementation Specificity**: Critical implementation details about head masking and fine-tuning are underspecified, including whether "masking" means zeroing attention weights or another method, and exact parameter tensor specifications.

## Confidence

- **High Confidence**: The experimental results showing >80% hallucination reduction with minimal WER impact on tested datasets (UrbanSound8K and LibriSpeech).
- **Medium Confidence**: The mechanism explaining why heads #1, #6, and #11 are specifically hallucinatory, though deeper analysis of learned attention patterns is lacking.
- **Low Confidence**: The claim that this approach generalizes to all Whisper deployments and non-speech scenarios, as the paper demonstrates effectiveness on specific datasets but doesn't test industrial deployment scenarios with diverse non-speech audio types.

## Next Checks

1. **Cross-dataset Head Attribution Validation**: Test the same head masking experiments on different non-speech datasets (e.g., industrial noise recordings, various silence types, environmental sounds) to verify that heads #1, #6, and #11 remain the primary hallucinatory heads across diverse non-speech audio distributions.

2. **Domain-specific Fine-tuning Evaluation**: Fine-tune the three heads using non-speech audio specific to the target deployment environment (e.g., call center hold music, HVAC system noise, factory sounds) and evaluate hallucination reduction on that specific domain rather than relying on UrbanSound8K generalization.

3. **Attention Pattern Analysis**: Visualize and analyze the attention patterns of heads #1, #6, and #11 during both hallucinatory (non-speech) and normal (speech) conditions to understand what acoustic features trigger hallucination and whether the fine-tuning changes these patterns in interpretable ways.