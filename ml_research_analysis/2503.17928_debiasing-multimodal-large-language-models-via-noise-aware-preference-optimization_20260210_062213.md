---
ver: rpa2
title: Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization
arxiv_id: '2503.17928'
source_url: https://arxiv.org/abs/2503.17928
tags:
- data
- bias
- arxiv
- language
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles modality bias in multimodal large language models,
  where models over-rely on a single input modality (e.g., language or vision), leading
  to incorrect responses or irrelevant details. To address this, the authors propose
  a method that uses preference optimization with a custom dataset (RLAIF-V-Bias)
  and a noise-aware loss (NaPO).
---

# Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization

## Quick Facts
- arXiv ID: 2503.17928
- Source URL: https://arxiv.org/abs/2503.17928
- Reference count: 40
- Primary result: Reduces modality bias by ~19.5% and improves hallucination benchmarks

## Executive Summary
This paper addresses modality bias in multimodal large language models (MLLMs), where models over-rely on a single input modality leading to incorrect responses. The authors propose a method combining preference optimization with a custom dataset and noise-aware loss function. By constructing biased responses through modality masking and using a dynamic loss that interpolates between binary cross-entropy and mean absolute error, the method reduces language priors and vision hallucinations while improving overall performance on multimodal benchmarks.

## Method Summary
The method constructs the RLAIF-V-Bias dataset by perturbing modalities in RLAIF-V: masking visual tokens to generate language-biased responses and masking text tokens to generate vision-biased responses. These perturbed responses serve as negative samples in preference optimization. The noise-aware loss (NaPO) combines binary cross-entropy with a noise-robust mean absolute error through a Box-Cox transformation, dynamically adjusting its noise sensitivity during training based on log-probability margins between preferred and rejected responses. The model is trained with a weighted combination of the original DPO loss and the new NaPO losses.

## Key Results
- Reduces modality bias by approximately 19.5% on VLind-Bench
- Improves performance on hallucination benchmarks (Object HalBench, AMBER)
- Outperforms standard preference optimization approaches
- Dynamic weighting between original data and bias data shows synergistic effects

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Negative Sample Construction via Modality Masking
Forcing the model to generate responses with masked modalities creates high-quality negative preference pairs that expose bias. By masking visual tokens or text tokens, the model over-relies on the remaining modality, producing biased responses that become rejection samples in preference optimization. The core assumption is that biased responses generated under modality perturbation are sufficiently representative of the model's failure modes during normal inference.

### Mechanism 2: Noise-Robust Loss via Negative Box-Cox Transformation
Interpolating between BCE and MAE through the Box-Cox parameter q ∈ (0, 1] balances fast convergence with robustness to label noise inherent in automated data construction. As q → 0, the loss approaches BCE (fast convergence, noise-sensitive); as q → 1, it approaches MAE (slow convergence, noise-robust). The core assumption is that automatically constructed preference data contains meaningful noise that must be downweighted during optimization.

### Mechanism 3: Dynamic Noise Assessment via Log-Probability Margins
The margin between preferred and rejected response log-probabilities correlates with data quality, enabling sample-specific noise robustness adjustment. Language-biased samples use average log-probability margins while vision-biased samples use sum log-probability margins, based on observations that these metrics differentially distinguish noise in each bias type. The core assumption is that higher log-probability margins indicate noise-free samples.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: NaPO modifies DPO's loss function; understanding the baseline is essential
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model during preference optimization?

- **Concept: Symmetric Loss Functions and Noise Robustness**
  - Why needed here: The paper leverages MAE's symmetry property to justify noise robustness
  - Quick check question: Why does a symmetric loss function inherently suppress the impact of mislabeled examples?

- **Concept: Modality Bias in Vision-Language Models**
  - Why needed here: Distinguishing language bias (over-reliance on priors) from vision bias (over-description of irrelevant details) is core to the method
  - Quick check question: Given an image of a blue banana and the question "What color is the banana?", what would a language-biased response versus a vision-biased response look like?

## Architecture Onboarding

- **Component map:** Data Construction Module -> Margin Calculator -> Dynamic Q Estimator -> NaPO Loss Computer -> Weight Balancer
- **Critical path:** Data construction → Margin computation → Q estimation → Weighted loss aggregation → Backprop
  - Warning: The paper notes that replacing DPO with NaPO for all data caused performance collapse when combined with dynamic weighting
- **Design tradeoffs:**
  - α selection: Higher α increases q fluctuations and faster decay; paper uses α=0.5 for language-bias and α=0.01 for vision-bias
  - Margin type selection: Using ψ_μ for vision-bias or ψ_Σ for language-bias causes degradation
  - Data mixing: Language-bias data better addresses language priors; vision-bias data better suppresses hallucinations
- **Failure signatures:**
  - Model collapse when dynamic weighting is applied to NaPO-only training
  - Degraded bias metrics if α is set too high
  - Hallucination increase if vision-bias data is excluded
- **First 3 experiments:**
  1. Validate bias data quality by manually annotating ~100 generated biased responses to confirm log-probability margins correlate with noise
  2. Ablate loss components by comparing DPO-only, NaPO-only, and full L_γ objective on VLind-Bench
  3. Hyperparameter sweep α with grid search on α ∈ [0.01, 0.1] for vision-bias and α ∈ [0.1, 1.0] for language-bias

## Open Questions the Paper Calls Out

- Can NaPO effectively handle noise in LLM-synthesized data across broader application contexts beyond modality debiasing?
- Is modality bias universally detrimental to model performance, or does it serve a functional role in certain reasoning tasks?
- Why does the integration of dynamic weight balancing with a fully NaPO-based loss cause model collapse?

## Limitations
- Margin-noise correlation established only on specific dataset and model, may not generalize
- Modality-specific margin choices based on empirical observation rather than theoretical justification
- Method assumes base model produces coherent biased responses when modalities are masked

## Confidence
- **Mechanism 1 (Modality masking):** Medium confidence - Conceptually sound but lacks external validation
- **Mechanism 2 (Noise-robust loss):** Medium confidence - Mathematical framework established but specific application lacks validation
- **Mechanism 3 (Dynamic noise assessment):** Low confidence - Most speculative component with no external validation
- **Overall performance improvements:** Medium confidence - Promising but benchmark-specific

## Next Checks
1. Validate margin-noise correlation externally by manually annotating diverse samples from different base models
2. Ablate loss components with different base models to determine if performance patterns generalize
3. Test robustness to margin type mis-specification across different bias types and datasets