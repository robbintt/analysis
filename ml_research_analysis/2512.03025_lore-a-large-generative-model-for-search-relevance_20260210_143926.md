---
ver: rpa2
title: 'LORE: A Large Generative Model for Search Relevance'
arxiv_id: '2512.03025'
source_url: https://arxiv.org/abs/2512.03025
tags:
- relevance
- query
- reasoning
- item
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LORE introduces a comprehensive framework for e-commerce search\
  \ relevance using large language models. The core insight is that relevance judgment\
  \ comprises three distinct capabilities\u2014knowledge and reasoning, multi-modal\
  \ matching, and rule adherence\u2014and that a qualitative decomposition of the\
  \ task is essential for breaking through performance bottlenecks."
---

# LORE: A Large Generative Model for Search Relevance

## Quick Facts
- **arXiv ID:** 2512.03025
- **Source URL:** https://arxiv.org/abs/2512.03025
- **Reference count:** 16
- **Primary result:** Achieves cumulative +27% improvement in online GoodRate metrics for e-commerce search relevance

## Executive Summary
LORE introduces a comprehensive framework for e-commerce search relevance using large language models. The core insight is that relevance judgment comprises three distinct capabilities—knowledge and reasoning, multi-modal matching, and rule adherence—and that a qualitative decomposition of the task is essential for breaking through performance bottlenecks. The method employs a two-stage training paradigm: progressive Chain-of-Thought synthesis via supervised fine-tuning, followed by reinforcement learning with verifiable rewards for human preference alignment. A query frequency-stratified deployment strategy is used to transfer offline LLM capabilities to the online system.

## Method Summary
LORE employs a two-stage training approach. First, it synthesizes Chain-of-Thought data in three progressive steps: knowledge injection via RAG with top-clicked items and merchant selling points, multimodal comprehension through relevance-guided VLM captions, and rule-aware reasoning chains. This synthesized data trains a smaller model (Qwen2.5-7B) via supervised fine-tuning. Second, reinforcement learning with verifiable rewards (KL-free GRPO) prunes erroneous reasoning paths, converting high pass@8 performance to improved pass@1 accuracy through curriculum learning and clip-higher entropy management. The framework uses a query frequency-stratified deployment strategy (cache/distillation/real-time) for online application.

## Key Results
- Achieves cumulative +27% improvement in online GoodRate metrics
- Establishes new state-of-the-art performance on comprehensive RAIR benchmark
- Progressive CoT synthesis outperforms naive teacher distillation by 4.2% in pass@1
- RLVR converts high pass@8 ceilings to improved pass@1 accuracy without degradation

## Why This Works (Mechanism)

### Mechanism 1: Task Deconstruction into Core Capabilities
Decomposing relevance judgment into distinct capabilities—knowledge/reasoning, multi-modal matching, and rule adherence—allows targeted training that overcomes performance ceilings seen in monolithic approaches. The framework separates relevance into two phases: (1) Path Construction—mapping query requirements to specific item attributes via semantic understanding; (2) Path Following—executing rule-based judgments along those paths with multimodal verification. This enables synthesizing CoT data that progressively addresses each capability rather than conflating them.

### Mechanism 2: Progressive CoT Synthesis with RAG-Enhanced Knowledge
Multi-stage CoT synthesis that first injects domain knowledge, then multimodal comprehension, then rule awareness produces higher-quality reasoning traces than direct teacher distillation. The pipeline synthesizes CoT in three sequential steps: (1) Knowledge Injection using RAG with top-clicked items and merchant selling points to ground query understanding; (2) Multimodal Comprehension via relevance-guided VLM captions; (3) Rule-Aware CoT that embeds industry-specific rules into reasoning chains.

### Mechanism 3: RLVR with Verifiable Rewards Prunes Erroneous Reasoning Paths
Reinforcement Learning with Verifiable Rewards (RLVR) converts the high pass@8 ceiling established by SFT into improved pass@1 accuracy by pruning flawed reasoning trajectories. After SFT establishes capability bounds (high pass@8), RL with outcome-based rewards (format + result rewards with progressive scaling) aligns the model with human preferences. Curriculum learning on difficulty-stratified samples, combined with clip-higher entropy optimization, maintains exploration while converging on correct paths.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The model must generate intermediate reasoning steps to decompose queries, match attributes across modalities, and apply business rules systematically rather than jumping directly to labels.
  - Quick check question: Can you explain why "morning C evening A" requires domain knowledge beyond literal interpretation, and how CoT would address this?

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR)**
  - Why needed here: SFT establishes diverse reasoning capabilities but produces inconsistent single-pass outputs; RL aligns these with verifiable outcomes (relevance labels) to improve first-attempt accuracy.
  - Quick check question: What's the difference between format reward and outcome reward in LORE's design, and why does progressive reward matter?

- **Concept: Knowledge-Augmented Generation (RAG)**
  - Why needed here: Long-tail queries and niche items require e-commerce domain knowledge (brand slang, product nicknames, industry jargon) not captured in pre-trained weights.
  - Quick check question: How does the framework use top-clicked items and merchant selling points to enhance query understanding?

## Architecture Onboarding

- **Component map**: Feature Construction (Title, CPV, SKU, Main Image) → Prompt Template → Base Model (Qwen2.5-7B) → Three-Stage CoT Synthesis (Teacher LLM + VLM + Rule Engine) → SFT → Hard Sample Mining → RLVR with Curriculum Learning → RAIR Benchmark → Query-Stratified Deployment (Cache / Distillation / Real-time)

- **Critical path**: The correctness of CoT synthesis depends on RAG quality (top-clicked items, selling points) and enhanced captions; RL effectiveness requires accurate pass@8 measurement and difficulty stratification; deployment hinges on correct query frequency classification.

- **Design tradeoffs**: (1) VLM vs. Two-Stage LLM: VLM preserves visual info but has weaker reasoning; LLM+caption is stronger for hard samples. (2) Clip-higher vs. On-policy: Clip-higher balances exploration/exploitation; On-policy stabilizes entropy but limits convergence. (3) Prompt length: Mid-length (~800 tokens) outperforms exhaustive rules.

- **Failure signatures**: (1) Pass@1 drops after SFT with CoT → distribution shift from teacher context; (2) RL reward plateaus early → entropy collapse or saturated samples dominating; (3) Visual subset underperforms → caption quality issues or VLM limitations; (4) Hard queries misclassified → insufficient knowledge injection or missing rules.

- **First 3 experiments**:
  1. Validate feature contribution: Compare Title-only vs. Title+CPV+SKU on pass@1 using mid-length prompt and Qwen2.5-7B base.
  2. Test CoT synthesis stages: Run ablation removing RAG, enhanced captions, or rule-aware step; measure impact on pass@8 and RAIR hard subset.
  3. Diagnose RL convergence: Track entropy and reward curves with/without clip-higher; verify pass@1 improves without pass@8 degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a unified model architecture balance the superior reasoning capabilities of text-only LLMs with the visual fidelity of native VLMs to eliminate the "text shortcut" phenomenon and information loss inherent in caption-mediated approaches?
- Basis in paper: Section 6.3 compares VLM-based and two-stage LLM-based methods, finding that LLMs excel in reasoning while VLMs excel in visual salience; the authors note VLMs suffer from "modality dependence bias" while captioning risks information loss.
- Why unresolved: The paper concludes that with equivalent CoT capability injection, LLMs possess stronger reasoning, while VLMs perform better on the visual salience subset, suggesting a fundamental trade-off that the current two-stage framework mitigates but does not fully resolve.
- What evidence would resolve it: The development of a single model that matches the macro-F1 of the LLM-based model on the Long-Tail Hard subset while simultaneously matching the VLM-based model's performance on the Visual Salience subset.

### Open Question 2
- Question: Does the negative correlation between model performance and reasoning length observed in relevance tasks generalize to other domains with explicit rule sets, or is it specific to the "intuitive" nature of attribute matching?
- Basis in paper: Section 6.2 notes that response length decreased during RL training, leading to the conclusion that "Long CoT is not necessary for better performance" for relevance, contrasting this with mathematical tasks where longer reasoning is beneficial.
- Why unresolved: The paper hypothesizes that relevance is an "intuitive and clear" chain of thought, but does not validate if this efficiency holds for "implicit demand" queries which require deep semantic reasoning, or if it is merely a byproduct of the specific reward design.
- What evidence would resolve it: A comparative study measuring the correlation between output token length and accuracy specifically on the "Reasoning-dependent" subset of the RAIR benchmark versus a standard mathematical reasoning benchmark.

### Open Question 3
- Question: Can the distribution shift between teacher-synthesized CoT (training) and student-autoregressive generation (inference) be mitigated during the SFT phase to preserve pass@1 performance without relying on subsequent RL correction?
- Basis in paper: Section 6.1 states that "Naive teacher CoT distillation results in negative effects" (degrading pass@1) specifically due to the discrepancy between conditioning on teacher contexts during training versus student outputs during inference.
- Why unresolved: The paper uses RL to "prune flawed reasoning paths" and recover performance, but this creates a heavy dependency on the RL stage; the SFT stage itself remains susceptible to this degradation, limiting the efficiency of the cold-start phase.
- What evidence would resolve it: An ablation study testing "on-policy" SFT (where the student model generates the reasoning chain which is then corrected/verified by the teacher) versus the current teacher-generation approach, measuring the resulting pass@1 scores.

## Limitations
- GRPO hyperparameter space is underspecified, making exact replication challenging
- Industry-specific rule sets referenced in methodology are not publicly documented
- RAIR benchmark construction details, particularly visual salience mining pipeline, remain opaque
- Reliance on single e-commerce platform (Taobao) raises generalizability concerns

## Confidence
**High Confidence**: The three-capability decomposition is well-supported by both theoretical argumentation and empirical ablation results showing naive CoT distillation's 4.2% pass@1 degradation.

**Medium Confidence**: The progressive CoT synthesis mechanism shows strong internal validation, but external reproducibility depends on obtaining comparable RAG contexts and VLMs. The clip-higher entropy strategy appears superior to alternatives in controlled experiments.

**Low Confidence**: The query frequency-stratified deployment strategy's online impact is asserted but not independently verified. The RLVR's long-term stability beyond initial convergence is not characterized.

## Next Checks
1. **Ablation on Capability Independence**: Design a controlled experiment isolating each of the three capabilities (knowledge, multimodal, rules) to verify they represent orthogonal dimensions rather than correlated features.

2. **Cross-Platform Generalization**: Test the trained model on a different e-commerce dataset with distinct product categories to assess whether the three-capability decomposition generalizes beyond Taobao's specific domain.

3. **Long-term RL Stability**: Implement continuous monitoring of entropy and reward curves post-convergence to detect potential degradation patterns that might emerge after extended deployment.