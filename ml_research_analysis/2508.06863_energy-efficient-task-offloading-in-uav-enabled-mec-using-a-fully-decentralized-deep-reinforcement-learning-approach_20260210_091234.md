---
ver: rpa2
title: Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized
  Deep Reinforcement Learning Approach
arxiv_id: '2508.06863'
source_url: https://arxiv.org/abs/2508.06863
tags:
- uavs
- energy
- user
- users
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of energy-efficient task offloading
  in UAV-enabled MEC systems with multiple UAVs and mobile users. The main problem
  is to jointly optimize UAV trajectories and user-UAV associations while accounting
  for limited UAV coverage and communication ranges, user mobility, and lack of a
  central entity.
---

# Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach

## Quick Facts
- **arXiv ID**: 2508.06863
- **Source URL**: https://arxiv.org/abs/2508.06863
- **Reference count**: 40
- **Primary result**: Proposed GAT-based EPS-PPO achieves 99% task processing vs 92-93% for MADDPG, converges faster, and scales well with UAV count

## Executive Summary
This paper addresses energy-efficient task offloading in UAV-enabled mobile edge computing (MEC) systems with multiple UAVs and mobile users. The key challenge is jointly optimizing UAV trajectories and user-UAV associations in a fully decentralized manner without a central coordinator. The proposed solution uses Graph Attention Networks (GAT) combined with Experience and Parameter Sharing Proximal Policy Optimization (EPS-PPO) to enable efficient local information diffusion and accelerated learning across the UAV network. The approach eliminates communication bottlenecks and single points of failure inherent in semi-centralized methods.

## Method Summary
The method implements a fully decentralized multi-agent reinforcement learning system where each UAV learns to optimize its trajectory and user associations. UAVs observe local users and communicate only with immediate neighbors. GAT layers enable efficient information diffusion across the UAV network, while EPS-PPO shares both experiences and actor-critic network weights among neighboring UAVs. The reward function incorporates energy consumption and task processing penalties, with large penalties for collision and boundary violations. The system trains for 900 episodes with 80 time slots per episode, using a square area of 250m × 250m with UAVs at 100m altitude.

## Key Results
- GAT-based EPS-PPO achieves approximately 99% task processing rate versus 92-93% for MADDPG
- Convergence speed is significantly faster than MADDPG, with sum discounted rewards stabilizing earlier
- Collision rates approach zero after training (approximately 200 episodes), demonstrating effective collision-avoidance learning
- The algorithm scales well with increasing UAV numbers, maintaining high performance while reducing energy consumption per processed task

## Why This Works (Mechanism)

### Mechanism 1: Graph Attention for Local Information Diffusion
- Claim: Multi-head GAT layers enable each UAV to aggregate observations from immediate neighbors, propagating global situational awareness without any central coordinator.
- Mechanism: Each UAV constructs a feature vector from its local observation (position, battery, nearby users, grid map). Two stacked GAT layers compute attention-weighted combinations of neighbor features using learnable query/key/value projections. Skip-connections concatenate the GAT output with the original feature vector, preserving self-information while incorporating neighbor context.
- Core assumption: The communication graph among UAVs remains sufficiently connected so that information can diffuse across the network within two hops; UAVs are homogeneous with shared action/observation spaces.
- Evidence anchors:
  - [abstract] "Two main components of our proposed solution are (i) Graph attention layers (GAT), and (ii) Experience and parameter sharing proximal policy optimization (EPS-PPO)."
  - [section 3.3] "Two GAT layers offer the best trade-off... output of the second GAT layer... is concatenated with the input to the first GAT layer, to emphasize the observation of UAV i itself."
  - [corpus] Neighbor paper "AirFed" similarly proposes federated graph-enhanced MARL for multi-UAV MEC, suggesting GAT-based coordination is an emerging pattern; however, direct comparative evidence is limited.

### Mechanism 2: Experience and Parameter Sharing for Sample Efficiency
- Claim: Sharing replay buffers and averaging actor-critic weights among neighbors accelerates convergence and stabilizes learning compared to fully independent learners.
- Mechanism: After each PPO update, UAVs exchange updated network parameters with neighbors and average them. Replay buffers are also shared, where stored states are GAT-processed feature vectors containing diffused neighbor information, not raw local observations. This provides each UAV with a richer training signal without central coordination.
- Core assumption: Neighboring UAVs face sufficiently similar decision contexts (e.g., user density, mobility patterns) that shared experiences transfer well; the policy gradient signal remains meaningful across agents.
- Evidence anchors:
  - [abstract] "EPS-PPO shares both experiences and actor-critic networks' weights among neighboring UAVs."
  - [section 3.3] "The state in experiences in the replay buffer are given by the GAT processed zt_m, rather than the raw local observations... some global information is also captured."
  - [corpus] No direct corpus papers implement EPS-PPO; related work on federated MARL (e.g., "AirFed") shares models but not local replay buffers. Transferability of shared experiences in heterogeneous environments remains unvalidated.

### Mechanism 3: Penalty-Shaped Reward for Constraint Satisfaction
- Claim: Incorporating collision and boundary penalties directly into the reward function drives UAVs to learn safe navigation without explicit hard constraints during policy optimization.
- Mechanism: The per-UAV reward includes a large penalty λm when boundary or collision constraints are violated. Since the underlying optimization is non-convex and mixed-integer, the penalty method provides a tractable surrogate. Over training, UAVs learn to avoid penalties, effectively internalizing safety constraints.
- Core assumption: The penalty magnitude λm is set large enough that the penalized objective approximates the constrained problem well; the policy can represent collision-avoidance behaviors within its capacity.
- Evidence anchors:
  - [abstract] "The algorithm effectively learns collision-avoidance strategies, with collision rates approaching zero after training."
  - [section 3.1] Equation (20) defines the penalized reward rm(t) with indicator functions for boundary and collision violations.
  - [section 4, Figure 5] "Collision rate drops significantly, eventually stabilizing near zero after approximately 200 episodes."

## Foundational Learning

- Concept: **Graph Attention Networks (GAT)**
  - Why needed here: GAT is the core communication mechanism enabling decentralized information aggregation. Understanding attention-weighted message passing is essential to debug inter-UAV coordination failures.
  - Quick check question: Given a UAV with 3 neighbors, can you sketch how the multi-head attention weights αij are computed and combined to form its updated feature vector?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: EPS-PPO extends standard PPO with neighbor sharing. Understanding the clipping objective, advantage estimation, and update dynamics is prerequisite to modifying the sharing schedule or hyperparameters.
  - Quick check question: What role does the clipping parameter ε play in preventing overly large policy updates, and how would sharing parameters mid-update affect this stability?

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Each UAV only observes local users and neighbors, making the problem a POMDP. Understanding belief states, observation functions, and the gap between local and global state is key to interpreting why GAT-enriched features help.
  - Quick check question: Why does partial observability make multi-agent learning non-stationary from each agent's perspective, and how does neighbor communication mitigate this?

## Architecture Onboarding

- Component map:
  - Input Layer: Local observation om(t) containing UAV position, battery, neighbor UAV statuses, visible user positions/tasks, binary coverage grid map
  - CNN Branch: Processes grid map to extract spatial coverage features
  - MLP Branch: Encodes non-spatial status vectors (battery, neighbor distances, user features)
  - Fusion: Concatenate CNN and MLP outputs into initial feature vector gi
  - GAT Stack: Two multi-head GAT layers diffuse neighbor information; skip-connection concatenates input and output features
  - EPS-PPO Actor-Critic: Takes enriched feature vector zt_m, outputs continuous movement actions (∆x, ∆y) and discrete user selection
  - Sharing Module: Periodically exchange and average replay buffers and network weights with neighbors within Rcom

- Critical path:
  1. Each UAV collects raw local observation at time t
  2. CNN/MLP encode observation into feature vector
  3. GAT layers aggregate neighbor features (requires communication round)
  4. Skip-connection forms enriched state zt_m
  5. Actor samples action; critic estimates value
  6. Execute action, observe reward and next observation
  7. Store transition in local replay buffer; share with neighbors
  8. Run PPO update on mini-batch; share and average updated weights

- Design tradeoffs:
  - **GAT layers**: More layers increase receptive field (multi-hop information) but add latency and risk over-smoothing. Paper uses 2 layers based on empirical trade-off.
  - **Communication range (Rcom)**: Larger Rcom increases neighbors, improving coordination but risking information overload and dropped connections (heuristic neighbor selection). Paper finds ~60m optimal; beyond this, performance degrades.
  - **Coverage range (Rcov)**: Larger coverage improves task access but may reduce incentive to move; diminishing returns beyond ~25m.
  - **Sharing frequency**: More frequent parameter sharing speeds convergence but increases communication overhead; optimal schedule not rigorously ablated.

- Failure signatures:
  - **High collision rate**: Indicates insufficient penalty weight λm, inadequate GAT information diffusion, or too-small Rcom.
  - **Low task processing percentage**: May signal poor coverage, under-exploration (weak map incentives), or misaligned reward weights (w1, w2).
  - **Slow or unstable convergence**: Could arise from oversized learning rates, insufficient PPO clipping, or heterogeneous neighbor conditions breaking EPS-PPO assumptions.
  - **Boundary violations**: Penalty λm too low relative to reward scale; consider increasing penalty or shaping reward differently.

- First 3 experiments:
  1. **Baseline reproduction**: Train with 10 UAVs, 50 users, Rcom=60m, Rcov=25m over 900 episodes. Monitor sum discounted reward, task processing percentage, and collision rate. Validate that collision rate approaches zero by episode 200 and task processing stabilizes near 99%.
  2. **Ablate GAT depth**: Compare 1 vs. 2 vs. 3 GAT layers on convergence speed and final performance. Hypothesis: 2 layers balance information diffusion and over-smoothing.
  3. **Scalability test**: Train with 7 UAVs (general training) and test with varying UAV counts (3, 7, 10, 15). Measure task processing percentage and energy per task. Expect specialized training to outperform for small fleets, gap narrowing for larger fleets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed EPS-PPO algorithm perform in environments with heterogeneous UAVs possessing varying battery capacities or processing capabilities?
- Basis in paper: [inferred] The paper states, "Given the homogeneity of the UAVs and their action and observation space, the experience sharing process helps all UAVs" (Page 10), implying the current method relies on agents being identical.
- Why unresolved: The parameter sharing mechanism averages network weights among neighbors, which assumes a shared optimal policy exists for all agents. Heterogeneous agents would require distinct policies, potentially breaking the convergence properties of the current weight averaging approach.
- What evidence would resolve it: Simulation results comparing the current EPS-PPO against a modified version (e.g., using separate network heads or conditional sharing) in a scenario with diverse UAV hardware specifications.

### Open Question 2
- Question: Can the GAT-based communication mechanism be adapted to prevent performance degradation caused by information overload in densely connected networks?
- Basis in paper: [inferred] The authors note a performance drop at large communication ranges: "This behavior is due to information overload, where UAVs begin to receive excessive or irrelevant data... some important connections may be dropped" (Page 15).
- Why unresolved: The current implementation uses a heuristic to select the nearest neighbors when the degree exceeds the GAT limit. This static selection may discard topologically critical connections in favor of spatially close ones, degrading coordination in dense swarms.
- What evidence would resolve it: Experiments evaluating dynamic or attention-based neighbor selection strategies in scenarios with high UAV density or large communication radii to see if the performance drop is mitigated.

### Open Question 3
- Question: Can the "general training" approach be modified to effectively transfer learned policies to swarms with significantly fewer UAVs than seen during training?
- Basis in paper: [inferred] Figure 10 analysis states, "For a smaller UAV fleet, the specialized training significantly outperforms the general training approach... the near optimal strategy differs significantly between these two setups" (Page 16).
- Why unresolved: The strategy learned for a dense swarm (trained on 7 UAVs) prioritizes energy conservation via redundancy, which is ineffective when resources are scarce (fewer UAVs), causing the general model to fail in sparse deployments.
- What evidence would resolve it: Results from training using domain randomization over swarm sizes or curriculum learning within the EPS-PPO framework, specifically testing performance on sparse deployments.

## Limitations

- The paper lacks detailed specification of neural network architecture (number of layers, hidden units, attention heads) and PPO hyperparameters, making exact reproduction difficult.
- No real-world validation or ablation studies on key design choices (GAT depth, communication range, sharing frequency) limits generalizability of the results.
- The EPS-PPO mechanism has no direct prior implementations for comparison, making it difficult to assess whether observed gains are due to GAT, parameter sharing, or their combination.

## Confidence

- **High confidence**: The core problem formulation and constraints (coverage, communication, collision avoidance) are well-defined and internally consistent. The reported performance metrics (task processing percentage, collision rate approaching zero) align with the stated reward structure.
- **Medium confidence**: The GAT-based information diffusion mechanism is plausible and supported by the neighbor literature (e.g., AirFed), but the exact impact of GAT depth and attention mechanisms on convergence is not rigorously ablated.
- **Low confidence**: Claims about EPS-PPO's superiority over MADDPG rely on single comparisons without hyperparameter tuning or ablation of the sharing mechanism. The sensitivity to penalty weight λm and communication range Rcom is stated but not thoroughly explored.

## Next Checks

1. **Architecture ablation**: Systematically vary the number of GAT layers (1, 2, 3) and communication range Rcom (40m, 60m, 80m) to quantify their impact on convergence speed and final performance.
2. **Transferability test**: Train the model with 7 UAVs and evaluate performance with 3, 7, 10, and 15 UAVs to assess scalability and robustness to fleet size changes.
3. **Hyperparameter sensitivity**: Conduct a grid search over PPO learning rates, batch sizes, and clipping parameters to ensure reported gains are not due to favorable default settings.