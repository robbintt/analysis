---
ver: rpa2
title: Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents
arxiv_id: '2510.20199'
source_url: https://arxiv.org/abs/2510.20199
tags:
- risk
- which
- learning
- problem
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a risk-aware constrained reinforcement learning
  framework using optimized certainty equivalents (OCEs). The method extends previous
  reward-based risk formulations to handle constraints and ensures exact equivalence
  to the original problem under certain conditions.
---

# Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents

## Quick Facts
- arXiv ID: 2510.20199
- Source URL: https://arxiv.org/abs/2510.20199
- Reference count: 40
- Primary result: A framework that extends reward-based risk formulations to handle constraints while ensuring exact equivalence to the original problem under certain conditions.

## Executive Summary
This paper presents a risk-aware constrained reinforcement learning framework using optimized certainty equivalents (OCEs) that handles both reward maximization and risk constraints. The method extends previous reward-based risk formulations to handle constraints through partial Lagrangian relaxation, converting the constrained problem into a stochastic minimax optimization solvable with existing RL solvers like PPO. Experiments on locomotion tasks from Safety-Gymnasium demonstrate that the approach effectively prevents constraint violations while maintaining stable reward performance, achieving zero violations compared to baselines.

## Method Summary
The method presents a risk-averse constrained reinforcement learning framework that uses optimized certainty equivalents (OCEs) like CVaR. The approach employs partial Lagrangian relaxation to convert the constrained problem into a stochastic minimax optimization. A wrapper around PPO alternates between policy updates and updates to dual variables (λ) and auxiliary variables (t) using stochastic gradient descent-ascent. The framework ensures exact equivalence to the original problem under certain conditions by applying risk measures to the occupancy measure rather than the return, preserving structural properties that allow for stronger duality.

## Key Results
- Achieves zero constraint violations on Safety-Gymnasium locomotion tasks compared to baselines
- Converges dual and CVaR variables to appropriate values that effectively manage risk
- Maintains stable reward performance while preventing unsafe behaviors
- Produces interpretable policies that balance task completion with constraint satisfaction

## Why This Works (Mechanism)

### Mechanism 1: Reward Transformation via Optimized Certainty Equivalents (OCEs)
Risk-averse objectives (specifically OCEs like CVaR) can be converted into standard reward-maximization problems by introducing an auxiliary variable t, provided the original rewards are bounded. The paper utilizes the property of OCEs where ρ(Z) = sup_t {t + E[g(Z-t)]}. By optimizing over t (a scalar quantile estimate) and fixing it temporarily, the risk measure collapses into a modified reward function r'(s,a,t) = t - (1/β)(t-r(s,a))^+. This allows the agent to use standard RL solvers on a "risk-shaped" reward signal rather than requiring a solver that natively understands risk measures.

### Mechanism 2: Exact Partial Lagrangian Relaxation
The constrained risk-averse problem can be solved exactly via a partial Lagrangian relaxation, avoiding the approximation errors typical in standard constrained RL, provided a specific constraint qualification holds. The framework constructs a Lagrangian L(π, t, λ) where dual variables λ penalize constraint violations. Unlike standard constrained RL which often has a non-zero duality gap due to non-convexity, this work proves the gap is "almost-zero" because the risk measure is applied to the occupancy measure rather than the return, preserving structural properties that allow for stronger duality.

### Mechanism 3: Stochastic Minimax Optimization (SGDA)
The coupled optimization of policy π, dual variables λ, and auxiliary variables t converges to a stationary point using a wrapper approach that alternates updates. The algorithm functions as a wrapper around a black-box RL solver. In the inner loop, PPO optimizes the policy using the modified reward. In the outer loop, Stochastic Gradient Descent-Ascent (SGDA) updates λ (to enforce constraints) and t (to calibrate the risk estimate). This decoupling isolates the non-convexity of the policy from the convexity of the dual/auxiliary optimization.

## Foundational Learning

- **Concept: Reward-Based vs. Return-Based Risk**
  - Why needed here: The paper relies on "reward-based" risk (applying the measure to the instantaneous reward distribution via the occupancy measure) rather than "return-based" risk (applying it to the total discounted sum). This distinction is the theoretical engine that allows for the exact duality mechanism.
  - Quick check question: If you apply CVaR to the total return, does the paper claim you can achieve exact equivalence? (Answer: No, that typically leads to a duality gap).

- **Concept: CVaR and the Auxiliary Variable t**
  - Why needed here: Understanding t as the Value-at-Risk (VaR) estimate is crucial for debugging. t represents the threshold separating "safe" outcomes from the "worst β-fraction" of outcomes that the agent optimizes over.
  - Quick check question: If β=0.1 and the agent learns t=10, is the agent concerned with rewards below 10 or above 10? (Answer: Assuming costs/rewards are structured such that lower is worse, it optimizes the tail below this threshold).

- **Concept: Primal-Dual Optimization**
  - Why needed here: The algorithm balances three objectives (Reward, Constraint, Risk). Understanding that λ acts as a price for violating constraints helps explain why λ increases when the agent is unsafe and why it stabilizes when the agent satisfies the risk threshold c.
  - Quick check question: If λ grows indefinitely, what does that imply about the constraint threshold c? (Answer: The constraint is likely infeasible for the current risk level β).

## Architecture Onboarding

- **Component map:** Environment (Safety-Gymnasium) -> Risk Wrapper (maintains λ, t) -> Reward Transform (modifies reward using Eq. 8) -> PPO Solver
- **Critical path:** 1) Collect trajectories with current π. 2) Compute sample gradients for λ and t using stored costs/rewards. 3) Update λ (SGD) and t (SGA) using step sizes η_λ, η_t. 4) Update π (PPO) using the modified reward objective.
- **Design tradeoffs:** The wrapper design allows using any PPO implementation but requires tuning two sets of learning rates (η_RL vs η_λ,t). The paper suggests η_λ,t must be significantly smaller (e.g., 5e-5) than typical PPO rates to ensure convergence. Step size sensitivity affects convergence of t.
- **Failure signatures:** Diverging λ indicates persistent constraint violations. Stagnant t suggests mismatched risk level β or too-small η_t. High variance gradients can be mitigated by using batch trajectories (n=8) rather than single trajectories.
- **First 3 experiments:** 1) Sanity Check: Run PPO on transformed reward with fixed t and λ. 2) Variable Convergence: Train on simple task and plot t against empirical β-quantile of reward distribution. 3) Constraint Satisfaction: Compare cumulative cost against vanilla PPO on Safety-Gymnasium to verify zero-violation claim.

## Open Questions the Paper Calls Out

- **Question 1:** Does full strong duality hold unconditionally for the proposed risk-averse constrained RL formulation?
  - Basis: The authors state in the Conclusion: "An open problem resulting from this work asks whether or not full strong duality holds unconditionally."
  - Why unresolved: Current theoretical results rely on Assumption 3.4 (Slater's condition) and don't prove if the duality gap is zero outside this parametrization.
  - Resolution evidence: A theoretical proof demonstrating sup_π,t inf_λ L(π,t,λ) = inf_λ sup_π,t L(π,t,λ) holds for general OCEs without requiring Assumption 3.4.

- **Question 2:** How can the algorithm verify or ensure the existence of the set I where Slater's condition holds during training?
  - Basis: Section 3.2 and Theorem 3.5 rely on Assumption 3.4, which posits the existence of a set I where Slater's condition holds. The authors admit they don't have access to this set during training.
  - Why unresolved: No mechanism exists to check if heuristic search over T successfully finds a point in I where partial Lagrangian relaxation is exact.
  - Resolution evidence: Development of an algorithmic check that verifies Slater's condition on the fly, or proof that heuristic search over T almost surely finds a point in I.

- **Question 3:** Can the computational intensity of the method be reduced relative to standard risk-neutral approaches?
  - Basis: The authors list as a limitation that the framework "is more computationally intensive compared to risk-neutral methods (as each update of λ and t variables requires solving for an approximately optimal policy)."
  - Why unresolved: The nested optimization structure inherently increases sample complexity and wall-clock time, and the current paper doesn't propose mitigation methods.
  - Resolution evidence: Experiments demonstrating competitive performance with reduced inner-loop optimization steps, or theoretical bounds showing favorable sample complexity scaling.

## Limitations
- Relies on strong assumptions about bounded reward distributions and constraint qualification (Slater's condition) holding
- Exact projection bounds for dual and auxiliary variables are unspecified, requiring manual tuning
- Wrapper design introduces sensitivity to step size ratios between PPO solver and dual/auxiliary optimization
- Performance may not generalize to tasks where risk threshold c is tight relative to environment dynamics

## Confidence
- **High confidence:** Reward transformation mechanism via OCEs - well-established theory with clear algorithmic implementation
- **Medium confidence:** Exact partial Lagrangian relaxation - theoretically sound but relies on strict assumptions that may not hold in practice
- **Medium confidence:** SGDA convergence guarantees - theoretical convergence exists but practical implementation requires careful hyperparameter tuning

## Next Checks
1. **Robustness to constraint thresholds:** Test the algorithm across multiple risk thresholds c and parameters β to identify breaking points where the constraint becomes infeasible
2. **Projection bound sensitivity:** Systematically vary the projection bounds for λ and t to determine their impact on convergence stability and constraint satisfaction
3. **Step size ratio analysis:** Experiment with different ratios between PPO learning rate and dual/auxiliary variable step sizes to quantify the sensitivity of the wrapper approach