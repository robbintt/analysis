---
ver: rpa2
title: Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency
  Simulation for Item Difficulty Prediction
arxiv_id: '2512.18880'
source_url: https://arxiv.org/abs/2512.18880
tags:
- difficulty
- student
- item
- prompt
- proficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) struggle to estimate item difficulty
  in educational assessments, a critical task for test design and adaptive learning.
  While LLMs excel at problem-solving, this study reveals a systematic misalignment
  between their difficulty predictions and human perception.
---

# Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction

## Quick Facts
- arXiv ID: 2512.18880
- Source URL: https://arxiv.org/abs/2512.18880
- Authors: Ming Li; Han Chen; Yunze Xiao; Jian Chen; Hong Jiao; Tianyi Zhou
- Reference count: 40
- One-line primary result: LLMs struggle to estimate item difficulty in educational assessments, showing systematic misalignment with human perception regardless of model scale.

## Executive Summary
This study reveals that Large Language Models (LLMs) cannot reliably estimate educational item difficulty despite their strong problem-solving capabilities. Evaluating over 20 models across domains including medical knowledge and math reasoning, the research finds that models converge on a shared "machine perception" of difficulty that systematically diverges from human cognitive struggle. Increasing model scale does not improve alignment; instead, models exhibit capability inertia that prevents authentic simulation of lower proficiency levels and show no metacognitive introspection about their own limitations.

## Method Summary
The study evaluates LLM difficulty prediction through two modalities: Observer View (predicting difficulty given item and correct answer) and Actor View (solving items without answer). Four datasets with human field-test difficulty labels are used: USMLE (667 items), Cambridge (793 questions), SAT Reading (1338 questions), and SAT Math (1385 questions). Twenty-one models are evaluated using zero-shot prompting with four proficiency personas. Item Response Theory (IRT) Rasch Model is fitted on binary correctness matrices to derive machine difficulty parameters, which are then compared to human difficulty using Spearman correlation and AUROC for metacognition.

## Key Results
- Models converge on a shared "machine consensus" of difficulty that systematically diverges from human perception
- Increasing model scale does not improve alignment; correlation between predicted and human difficulty remains weak (ρ < 0.5)
- Proficiency prompting has negligible effect (<1%) on actual problem-solving capability, confirming "capability inertia"
- Models fail to predict their own correctness, showing random introspection with AUROC ≈ 0.5

## Why This Works (Mechanism)

### Mechanism 1: Machine Consensus Formation
As models scale, their internal representations stabilize around common patterns in training data, forming a cohesive view of difficulty based on statistical associations rather than human cognitive load. This results in high inter-model correlation but low human alignment. Core assumption: This divergence is driven by models' optimization for linguistic probability, which ignores human constraints like working memory limits. Break condition: If models are trained on datasets containing human error patterns rather than just solution text.

### Mechanism 2: Capability Inertia (Curse of Knowledge)
High problem-solving capability creates "inertia" that prevents models from authentically simulating lower proficiency levels, regardless of explicit prompting. The model's generation process is dominated by the high-probability path to the correct answer learned during training. Core assumption: The instruction to simulate struggle is processed as a stylistic instruction rather than a constraint on the reasoning mechanism. Break condition: If "student simulation" is implemented via specific fine-tuning on student traces.

### Mechanism 3: Metacognitive Decoupling
A model's explicit difficulty prediction is statistically decoupled from its internal probability of correctness. The model treats difficulty prediction as a distinct text generation task separate from the latent uncertainty of solving the problem. Core assumption: Difficulty prediction is generated via heuristics rather than introspection of internal confidence signals. Break condition: If hidden states or logit probabilities are used as features for difficulty prediction.

## Foundational Learning

- **Concept: Item Response Theory (IRT)**
  - Why needed here: The paper uses IRT to derive "Empirical Machine Difficulty" from model correctness, contrasting it with human difficulty. You cannot interpret the "Savant Rate" without understanding how IRT separates item difficulty from subject ability.
  - Quick check question: If 90% of models answer a question correctly that 70% of humans fail, does the IRT β for the model group reflect the human struggle?

- **Concept: Variance Collapse**
  - Why needed here: The paper identifies "distribution shift" and "variance collapse" as key failure modes. Models cluster predictions while human difficulty spans a wide spectrum.
  - Quick check question: If a model predicts a difficulty of 0.5 for every item in a dataset where true difficulties are uniformly distributed from 0 to 1, what is the likely impact on Spearman's correlation?

- **Concept: Cognitive Load vs. Feature Complexity**
  - Why needed here: To understand the "Curse of Knowledge." A problem may be linguistically simple for an LLM but cognitively demanding for a human.
  - Quick check question: Why might a model find a multi-step arithmetic problem "easy" while a human finds it "hard"?

## Architecture Onboarding

- **Component map:** Observer Module (Item, Ground Truth) → LLM → Predicted Difficulty; Actor Module (Item) → LLM → Answer → Binary Correctness; IRT Calibration Engine (Correctness Matrix) → Rasch Model → Machine Difficulty β; Alignment Evaluator (Human Truth, Predicted Difficulty, β) → Spearman ρ, AUROC

- **Critical path:**
  1. Generate predictions/correctness for the 4 datasets (USMLE, Cambridge, SAT)
  2. Fit the Rasch Model on the correctness matrix to obtain β
  3. Calculate the "Savant Rate" and AUROC for metacognition

- **Design tradeoffs:**
  - **Explicit vs. Implicit Estimation:** Relying on the LLM to "tell" you difficulty (Observer) vs. inferring it from its performance (Actor). *Paper shows the Actor view is even more misaligned than the Observer view.*
  - **Proficiency Prompting:** Adding persona instructions. *Tradeoff:* Increases compute/prompt length for negligible (<1%) change in actual capability.

- **Failure signatures:**
  - **Savant Syndrome:** High accuracy on items humans fail (High Savant Rate)
  - **Random Introspection:** AUROC ≈ 0.5 when predicting own errors
  - **Distribution Clustering:** Narrow prediction bands compared to ground truth

- **First 3 experiments:**
  1. **Baseline Correlation:** Run Observer inference on SAT-Math with GPT-4o and Llama3-8B. Verify both show weak Spearman correlation (ρ < 0.5).
  2. **Proficiency Stress Test:** Apply "Low-Proficiency" prompt to Actor setup. Verify accuracy remains nearly identical to baseline.
  3. **Metacognitive Check:** Calculate AUROC for a single model. Confirm predicted difficulty fails to separate correct from incorrect items.

## Open Questions the Paper Calls Out

- Can few-shot prompting with real student error patterns or fine-tuning on student response logs significantly improve Human-AI Difficulty Alignment? The authors note this was not explored and could potentially improve correlation.
- What novel approaches beyond model scaling or proficiency prompting are required to bridge the gap between LLM capability and human difficulty estimation? The paper calls for new approaches to ground machine cognition.
- Can LLMs be engineered to authentically simulate specific student misconceptions rather than simply adjusting answer confidence? Current prompting fails to suppress intrinsic capabilities.

## Limitations

- Dataset access issues: Exact USMLE and SAT sources with ground-truth difficulty labels not specified; may require data agreements
- Missing generation hyperparameters: Temperature, max tokens, sampling strategy not specified
- Parsing function details: Exact extraction logic for continuous vs. discrete difficulty beyond `\boxed{...}` not fully specified

## Confidence

- Machine Consensus Formation: **Medium** - supported by correlation patterns but lacks causal mechanism validation
- Capability Inertia: **High** - directly demonstrated through proficiency prompting experiments
- Metacognitive Decoupling: **Low** - concept well-defined but lacks direct empirical verification

## Next Checks

1. **Hidden State Analysis**: Examine model hidden states during difficulty prediction vs. problem solving to verify if predicted difficulty is decoupled from correctness probability.
2. **Fine-tuning Experiment**: Train a model on synthetic student traces showing error patterns and re-evaluate difficulty prediction alignment.
3. **Cross-Domain Transfer**: Apply the same evaluation framework to non-educational domains (e.g., code debugging) to test generalizability of the misalignment phenomenon.