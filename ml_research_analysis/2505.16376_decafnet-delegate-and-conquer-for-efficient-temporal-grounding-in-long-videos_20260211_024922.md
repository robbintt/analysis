---
ver: rpa2
title: 'DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos'
arxiv_id: '2505.16376'
source_url: https://arxiv.org/abs/2505.16376
tags:
- temporal
- features
- clips
- video
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeCafNet addresses the computational bottleneck in long video temporal
  grounding (LVTG) by introducing a "delegate-and-conquer" approach. Instead of processing
  all video clips through a computationally expensive expert encoder, it employs a
  sidekick encoder to efficiently extract dense features for all clips and generate
  a saliency map identifying the most relevant clips.
---

# DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos

## Quick Facts
- arXiv ID: 2505.16376
- Source URL: https://arxiv.org/abs/2505.16376
- Reference count: 40
- Primary result: Achieves state-of-the-art performance while reducing computation by up to 47% on LVTG benchmarks

## Executive Summary
DeCafNet addresses the computational bottleneck in long video temporal grounding (LVTG) by introducing a "delegate-and-conquer" approach. Instead of processing all video clips through a computationally expensive expert encoder, it employs a sidekick encoder to efficiently extract dense features for all clips and generate a saliency map identifying the most relevant clips. Only these top-c% salient clips are then processed by the expert encoder. To effectively combine features from both encoders at different temporal resolutions, DeCafNet introduces DeCaf-Grounder, which unifies and refines them via query-aware temporal aggregation and multi-scale temporal refinement.

## Method Summary
DeCafNet employs a dual-encoder architecture for efficient long video temporal grounding. The Sidekick Encoder processes all video clips using Conv-Pooling and Temporal Interpolation (Ï„=2) to generate dense features and a saliency map identifying the most relevant clips. Only the top-c% salient clips are passed to the frozen Expert Encoder for detailed processing. The DeCaf-Grounder then unifies and refines features from both encoders using query-aware temporal aggregation and multi-scale temporal refinement (8 scales). The system is trained end-to-end with a combination of saliency loss (contrastive, weight=1.0), distillation loss (weight=0.75), and grounding losses (focal and DIoU).

## Key Results
- Achieves state-of-the-art performance on Ego4D-NLQ and MAD datasets
- Reduces computation by up to 47% compared to existing methods
- Maintains high recall@top-k metrics (R1@0.3, R1@0.5, etc.) while improving efficiency

## Why This Works (Mechanism)
DeCafNet works by strategically distributing computational load between two encoders. The Sidekick Encoder rapidly processes all clips to identify the most relevant segments, while the Expert Encoder focuses its computational resources only on the top-c% salient clips. This delegation allows the system to maintain high accuracy while significantly reducing the number of expensive expert encodings required. The DeCaf-Grounder effectively bridges the temporal resolution gap between the two encoders, ensuring that the final predictions benefit from both the efficiency of dense processing and the accuracy of expert analysis.

## Foundational Learning

**Long Video Temporal Grounding (LVTG):** The task of locating specific moments in long videos based on text queries. Why needed: This is the core problem DeCafNet addresses. Quick check: Understanding the difference between short and long video temporal grounding tasks.

**Dual-Encoder Architecture:** A system using two separate encoders with different computational costs and capabilities. Why needed: Enables efficient processing by delegating different tasks to appropriate encoders. Quick check: Identifying how each encoder contributes to the final prediction.

**Saliency-Based Clip Selection:** Using a saliency map to identify the most relevant video clips for further processing. Why needed: Reduces computational cost by focusing expert analysis only on relevant segments. Quick check: Understanding how saliency scores are computed and used.

## Architecture Onboarding

**Component Map:** Text Encoder -> Sidekick Encoder -> Saliency Map -> Expert Encoder -> DeCaf-Grounder -> Output

**Critical Path:** The most computationally intensive part is the Expert Encoder processing. By using the Sidekick Encoder to filter clips, DeCafNet ensures this expensive operation only runs on top-c% of clips, dramatically reducing overall computation.

**Design Tradeoffs:** The system balances computational efficiency (using Sidekick Encoder for all clips) with accuracy (using Expert Encoder for top-c% clips). The main tradeoff is between the aggressiveness of the saliency filter (c%) and the risk of missing relevant content.

**Failure Signatures:**
- Low "Recall of Saliency" indicates the sidekick encoder is failing to identify relevant clips
- Out of Memory errors suggest the batching strategy for long videos needs optimization
- Poor grounding performance may indicate insufficient refinement in the DeCaf-Grounder

**First Experiments:**
1. Verify the sidekick encoder can accurately identify salient clips by monitoring "Recall of Saliency" during training
2. Test the memory efficiency of the batched feature extraction approach on long videos
3. Validate the training stability by monitoring the balance between saliency loss and distillation loss

## Open Questions the Paper Calls Out
None

## Limitations
- Training hyperparameters (learning rate, batch size, optimizer, epochs) are not specified
- Exact architectural details for the text encoder and feature projection layers in DeCaf-Grounder are unclear
- Implementation details for video-text fusion and temporal convolution dilation rates are not provided

## Confidence
- **High confidence** in the core methodology (dual-encoder approach with saliency-based clip selection)
- **Medium confidence** in the performance claims due to missing training details
- **Low confidence** in the exact implementation details required for faithful reproduction

## Next Checks
1. Verify the "Recall of Saliency" metric during training to ensure the sidekick encoder is correctly identifying relevant clips
2. Test the memory efficiency of the batched feature extraction approach on long videos to confirm the claimed 47% computation reduction
3. Validate the training stability by monitoring the balance between saliency loss and distillation loss during optimization