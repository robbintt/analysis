---
ver: rpa2
title: Polynomial-Time Approximability of Constrained Reinforcement Learning
arxiv_id: '2502.07764'
source_url: https://arxiv.org/abs/2502.07764
tags:
- constraints
- learning
- algorithm
- definition
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational complexity of constrained
  reinforcement learning (CRL), focusing on finding optimal policies under various
  constraint types. The core challenge is that many CRL settings are NP-hard, particularly
  for deterministic policies and multiple constraints.
---

# Polynomial-Time Approximability of Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.07764
- Source URL: https://arxiv.org/abs/2502.07764
- Reference count: 40
- Primary result: Polynomial-time (0, ε)-additive bicriteria approximation for tabular CMDPs and (ε, ε)-additive bicriteria approximation for continuous-state CMDPs.

## Executive Summary
This paper addresses the fundamental computational complexity of constrained reinforcement learning (CRL) by showing that polynomial-time approximation algorithms exist for a broad class of constrained MDPs. The key insight is that constraints can be transformed into per-time-step constraints through state augmentation with artificial budgets, enabling the use of dynamic programming. The approach provides the first polynomial-time algorithms for solving CMDPs with chance constraints, multiple expectation constraints, and non-homogeneous constraints - resolving longstanding open questions in the field.

## Method Summary
The paper transforms constrained MDPs into standard MDPs by augmenting the state space with artificial budgets and converting global constraints into per-step action constraints. This "reduced MDP" has exponentially large action spaces, which the algorithm handles through dynamic rounding of the budget space and iterative dynamic programming. The method provides additive bicriteria approximations that are optimal in value while allowing small budget violations, circumventing the NP-hardness of finding exactly feasible deterministic policies.

## Key Results
- Proves polynomial-time (0, ε)-additive bicriteria approximation for tabular, SR-criterion CMDPs
- Establishes (ε, ε)-additive bicriteria approximation for continuous-state CMDPs with bounded ranges
- Resolves polynomial-time approximability for policies under chance constraints and deterministic policies under multiple expectation constraints
- Extends results to infinite-horizon discounted settings and Markov games

## Why This Works (Mechanism)

### Mechanism 1: Constraint-to-Step Reduction via Augmentation
The algorithm augments the state space with an "artificial budget" $b$ and transforms global constraints into per-step action constraints. When taking action $a$, the agent simultaneously chooses future budgets for all potential next states, ensuring immediate cost plus weighted future cost remains within $b$. This requires the cost criterion to satisfy the SR (shortly recursive) property, decomposing recursively via associative, non-decreasing functions $f$ and $g$.

### Mechanism 2: Dynamic Rounding for Bellman Tractability
Exact Bellman updates over the augmented action space are NP-hard due to exponential size. The paper introduces rounding parameter $\ell$ to discretize budget space, tracking budgets rounded to nearest multiples of $\ell$. To account for accumulating rounding errors, the algorithm uses a relaxed threshold function $\kappa(b) = b + \ell(S+1)$ during updates, bounding the total error.

### Mechanism 3: Bicriteria Relaxation
Since finding exactly feasible deterministic policies is NP-hard, the algorithm produces additive bicriteria approximations guaranteeing near-optimal value ($V^* - 0$ or $V^* - \epsilon$) while allowing small constraint violations ($B + \epsilon$). This relaxation navigates non-convex feasible regions induced by non-expectation constraints.

## Foundational Learning

- **Concept:** **SR-Criteria (Shortly Recursive)**
  - *Why needed here:* The unifying mathematical property required for the augmentation to work. Verify if your constraint fits the recursive $f, g$ formulation before applying this algorithm.
  - *Quick check question:* Can your constraint be written as $C_h = c + f(\sum g(P) C_{h+1})$? (e.g., Expectation uses $f=\text{sum}, g=\text{id}$; Almost-sure uses $f=\text{max}, g=\text{indicator}$).

- **Concept:** **State/Action Augmentation**
  - *Why needed here:* The core architectural shift. You are building an augmented MDP where "state" includes remaining budget and "action" includes budget allocations for successor states.
  - *Quick check question:* If horizon is $H$ and state space is $S$, does your memory allow for augmented state space $S \times \hat{B}$?

- **Concept:** **Knapsack-style Bellman Updates**
  - *Why needed here:* The update step is an optimization over budget allocations $\mathbf{b}_{s'}$, not a simple max over actions. Understanding this prevents naive Q-learning implementation which will fail.
  - *Quick check question:* Why does the "Reduced MDP" require solving a knapsack-like sub-problem at every step? (Answer: Because you must distribute current budget $b$ among future states $s'$).

## Architecture Onboarding

- **Component map:** Input: caMDP + Constraint + Budget $\rightarrow$ Preprocessor: Discretize budget space + Augment state space $\rightarrow$ Solver: Backward Induction using Approximate Dynamic Programming $\rightarrow$ Output: Policy $\pi(s, b)$

- **Critical path:** The **Approximate Bellman Update (AU)**. This function computes optimal value by iterating over rounded partial costs. Inefficient implementation loses polynomial time guarantee.

- **Design tradeoffs:**
  - **$\ell$ (Rounding Granularity):** Smaller $\ell$ decreases additive error $\epsilon$ but increases state space $|\hat{B}|$ exponentially ($1/\ell^m$).
  - **Deterministic vs. Stochastic:** For stochastic policies, "max" becomes Linear Program (LP), adding complexity but potentially improving policy quality.

- **Failure signatures:**
  - **Constraint Oscillation:** Large $\ell$ causes rounding error to exceed budget slack, reporting "Infeasible" for solvable problems.
  - **Curse of Dimensionality (Constraints):** Runtime scales as $O(1/\epsilon^{3m})$. With $m > 2$ constraints, polynomial time may be practically intractable for tight $\epsilon$.

- **First 3 experiments:**
  1. **Knapsack Verification:** Implement "Knapsack Example" from Section 6 to verify $(0, \epsilon)$ guarantee on deterministic problem.
  2. **Budget Scaling:** Test Algorithm 4 on small gridworld. Plot runtime vs. $\epsilon$ to confirm polynomial scaling from Theorem 4.
  3. **Chance Constraint Test:** Formulate simple chance-constrained problem and compare "Anytime" vs. "End-to-end" constraint performance.

## Open Questions the Paper Calls Out

- **Question:** Can the proposed framework of parameterizing artificial budgets with function approximation be theoretically proven to converge to an optimal constrained policy?
- **Basis in paper:** [explicit] Section 6 discusses combining approach with function approximation but provides only empirical comparisons without theoretical guarantees.
- **Why unresolved:** Proves results for tabular and discretized continuous settings but leaves convergence properties of function approximation extension formally unanalyzed.
- **What evidence would resolve it:** A formal proof showing convergence rate or sample complexity of proposed closed-loop system for learning budget function $b_\theta$.

## Limitations

- Polynomial-time guarantees depend critically on cost criterion satisfying SR property; verification required for new constraint types
- Method requires polynomially bounded costs to prevent exponential blowup in discretized budget space
- Bicriteria relaxation allows budget constraint violations of ε, which may be unacceptable in safety-critical applications

## Confidence

- **High Confidence:** Polynomial-time complexity bound (Theorem 4) and correctness of state augmentation for expectation constraints
- **Medium Confidence:** Extension to chance constraints and almost-sure constraints via Proposition 1
- **Medium Confidence:** Practical tractability for problems with multiple constraints due to cubic dependence on constraint count

## Next Checks

1. **SR-Property Verification:** For a new constraint type (e.g., CVaR), explicitly verify whether it satisfies shortly recursive property by identifying functions f and g that decompose global constraint into per-step constraints. Test augmentation method on small instance.

2. **Budget Discretization Sensitivity:** Implement Algorithm 4 on simple gridworld with expectation constraints. Systematically vary ε and plot runtime, constraint violation, and solution quality to empirically verify polynomial scaling and accuracy-computational cost trade-off.

3. **Multi-Constraint Stress Test:** Implement small CMDP with 3+ constraints (e.g., multiple resource budgets). Measure actual runtime and memory usage for varying ε values to assess practical limits of polynomial-time guarantee and identify infeasibility threshold.