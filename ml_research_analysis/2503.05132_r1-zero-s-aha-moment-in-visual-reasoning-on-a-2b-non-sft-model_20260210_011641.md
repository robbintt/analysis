---
ver: rpa2
title: R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model
arxiv_id: '2503.05132'
source_url: https://arxiv.org/abs/2503.05132
tags:
- reasoning
- training
- length
- response
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper successfully replicates the "aha moment" phenomenon
  from DeepSeek R1 for multimodal reasoning by applying reinforcement learning directly
  to a non-SFT 2B model (Qwen2-VL-2B). Starting from the base model and training on
  the SAT dataset, their VisualThinker R1 Zero achieved 59.47% accuracy on CVBench,
  outperforming the base model by ~30% and instruction-tuned SFT models by ~2%.
---

# R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model

## Quick Facts
- arXiv ID: 2503.05132
- Source URL: https://arxiv.org/abs/2503.05132
- Authors: Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh
- Reference count: 20
- Key result: RL on non-SFT models induces autonomous reasoning capabilities, while RL on instruction-tuned models leads to trivial patterns

## Executive Summary
This paper demonstrates that reinforcement learning can induce genuine reasoning capabilities in multimodal models without requiring supervised fine-tuning. The authors successfully replicate DeepSeek R1's "aha moment" phenomenon in visual reasoning by applying RL directly to the base Qwen2-VL-2B model. Starting from the non-SFT model and training on the SAT dataset, their VisualThinker R1 Zero achieved 59.47% accuracy on CVBench, outperforming both the base model (~30% improvement) and instruction-tuned SFT models (~2% improvement). The key innovation is showing that RL on non-SFT models induces autonomous reasoning capabilities, while applying RL to instruction-tuned models leads to trivial reasoning patterns rather than genuine problem-solving.

## Method Summary
The authors applied reinforcement learning directly to the base Qwen2-VL-2B model (2B parameters) without any supervised fine-tuning. They used the SAT dataset containing 1,000 high-quality samples for training. The RL process was designed to reward correct answers while avoiding naive length-based rewards that could encourage superficial reasoning. The training was monitored for signs of the "aha moment" - characterized by increased response length and self-reflection patterns during problem-solving. The approach contrasts with previous methods that applied RL only after instruction tuning, demonstrating that starting from the base model is crucial for inducing genuine reasoning capabilities.

## Key Results
- VisualThinker R1 Zero achieved 59.47% accuracy on CVBench, outperforming the base model by ~30% (30.27%) and instruction-tuned SFT models by ~2% (57.40%)
- The model exhibited increased response length and self-reflection during training, key characteristics of R1-like reasoning
- RL on instruction-tuned models failed to produce meaningful reasoning improvements, instead leading to trivial reasoning patterns

## Why This Works (Mechanism)
The paper demonstrates that reinforcement learning on non-SFT models enables the emergence of genuine reasoning capabilities through autonomous exploration of problem-solving strategies. The key mechanism appears to be that non-SFT models retain more flexible, exploratory behavior that RL can shape into sophisticated reasoning patterns. In contrast, instruction-tuned models have been constrained to follow specific patterns and cannot develop novel reasoning approaches through RL. The "aha moment" represents a phase transition where the model discovers more efficient problem-solving strategies, evidenced by increased response complexity and self-reflective behavior. This suggests that the initial model state (non-SFT vs SFT) critically determines whether RL can induce genuine reasoning or merely optimizes existing patterns.

## Foundational Learning
- **Visual reasoning benchmarks**: Understanding CVBench, MMMU, and other visual reasoning datasets is essential for evaluating multimodal reasoning performance and establishing baseline comparisons.
- **Reinforcement learning in language models**: Knowledge of RLHF, Proximal Policy Optimization, and reward shaping is needed to understand how the training process induces reasoning behaviors and avoids degenerate solutions.
- **Base vs instruction-tuned models**: Recognizing the fundamental differences between non-SFT and SFT models explains why model initialization critically affects the ability of RL to induce genuine reasoning capabilities.
- **Reasoning chain-of-thought**: Understanding how models generate intermediate reasoning steps is crucial for interpreting the "aha moment" and distinguishing genuine problem-solving from superficial pattern matching.

## Architecture Onboarding
**Component Map**: Base model (Qwen2-VL-2B) -> RL training loop -> SAT dataset -> Reward function (correctness-based) -> Output reasoning chains
**Critical Path**: The RL training loop is the critical component where reasoning capabilities emerge. The base model provides the foundation, but the RL process with appropriate reward functions determines whether genuine reasoning develops.
**Design Tradeoffs**: Using non-SFT models enables genuine reasoning emergence but requires careful reward design to avoid degenerate solutions. Instruction-tuned models are more stable but prevent the development of novel reasoning strategies.
**Failure Signatures**: RL on SFT models leads to trivial reasoning patterns and minimal performance gains. Naive length rewards cause superficial reasoning without improved accuracy. Poor reward design can cause mode collapse or performance degradation on other tasks.

**First Experiments**:
1. Compare CVBench performance of VisualThinker R1 Zero against other visual reasoning approaches on MMMU and ScienceQA to test generalization
2. Vary training sample size (50, 100, 500, 1000) to characterize the learning curve and identify the "aha moment" threshold
3. Apply RL to both non-SFT and SFT versions of the same base model to directly test the hypothesis about initialization importance

## Open Questions the Paper Calls Out
None

## Limitations
- The sample efficiency is unclear - training required only 1,000 high-quality samples from SAT, but full training dynamics and generalization beyond CVBench are not thoroughly explored
- The study focuses primarily on a single visual reasoning benchmark (CVBench) and one non-SFT base model (Qwen2-VL-2B), limiting broader claims about the phenomenon
- The "aha moment" characterization relies on observed increases in response length and self-reflection, but lacks rigorous causal analysis of what specifically triggers this transition

## Confidence
- High confidence: RL on non-SFT models improves visual reasoning performance (59.47% vs 30.27% base, 57.40% SFT)
- Medium confidence: The "aha moment" phenomenon is generalizable to multimodal reasoning and occurs specifically with non-SFT models
- Medium confidence: RL on instruction-tuned models leads to trivial reasoning rather than genuine problem-solving

## Next Checks
1. Cross-benchmark validation: Test VisualThinker R1 Zero on multiple visual reasoning benchmarks (MMMU, GQA, ScienceQA) to confirm generalization beyond CVBench and rule out benchmark-specific overfitting
2. Sample efficiency analysis: Systematically vary the training sample size (50, 100, 500, 1000) to determine the minimum effective dataset size and characterize the learning curve, particularly around the hypothesized "aha moment"
3. Mechanism dissection: Conduct ablation studies removing self-reflection components or response length variations to determine whether these are causal factors in the reasoning improvement or merely correlated symptoms of underlying changes