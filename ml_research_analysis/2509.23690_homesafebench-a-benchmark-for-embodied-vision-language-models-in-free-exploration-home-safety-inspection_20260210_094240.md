---
ver: rpa2
title: 'HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration
  Home Safety Inspection'
arxiv_id: '2509.23690'
source_url: https://arxiv.org/abs/2509.23690
tags:
- safety
- embodied
- inspection
- arxiv
- hazard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HomeSafeBench, a new benchmark for evaluating
  Vision-Language Models (VLMs) in free-exploration home safety inspection tasks.
  Existing benchmarks oversimplify safety inspection by using textual descriptions
  instead of direct visual information and rely on static viewpoints, which can cause
  safety hazards to be overlooked.
---

# HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection

## Quick Facts
- **arXiv ID:** 2509.23690
- **Source URL:** https://arxiv.org/abs/2509.23690
- **Reference count:** 18
- **Primary result:** Current VLMs achieve only 10.23% F1 score on free-exploration home safety inspection

## Executive Summary
HomeSafeBench addresses critical limitations in existing VLM benchmarks for home safety inspection by enabling free exploration in simulated 3D environments. Unlike prior work that relies on static viewpoints and textual descriptions, this benchmark provides first-person dynamic images and allows embodied agents to actively navigate rooms to identify hazards. The comprehensive evaluation reveals that even state-of-the-art VLMs struggle significantly with both hazard identification and effective exploration strategies, with the best model achieving only 10.23% F1 score.

## Method Summary
HomeSafeBench provides 12,900 data points across five home safety hazard categories (fire, electric shock, falling objects, trip hazards, child safety) using the VirtualHome simulation environment. The benchmark enables embodied agents to freely explore rooms from a first-person perspective, with actions including 3-step walk forward, 90-degree turns, and look-up movements. Evaluation uses a 10-turn inference loop with VLMs following prompts that define two tasks: identifying hazards and selecting the next action. Ground truth hazards are derived from 136 annotated locations and 367 objects, with rule-based name matching applied before scoring.

## Key Results
- Best-performing VLM achieves only 10.23% F1 score on hazard identification
- All models show poor navigation performance, especially in complex environments
- Significant effectiveness drop occurs during long-horizon multi-turn interactions
- Models struggle with effective exploration strategies, often getting stuck in repetitive actions

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic simulation of free exploration requirements in safety inspection. By forcing agents to actively navigate and observe from multiple viewpoints rather than relying on static camera positions, it exposes the limitations of current VLMs in spatial reasoning and long-term planning. The first-person perspective and dynamic environment create a more authentic inspection scenario that better reflects real-world requirements.

## Foundational Learning
- **Embodied AI navigation**: Why needed - agents must move through 3D space to observe hazards; Quick check - can the agent reach all predefined locations within step budget
- **Vision-language integration**: Why needed - combining visual perception with language understanding for hazard identification; Quick check - does the model correctly identify hazards from single images
- **Long-horizon planning**: Why needed - maintaining effective exploration strategy over multiple turns; Quick check - does performance degrade with increasing interaction steps

## Architecture Onboarding
**Component Map:** VirtualHome Environment -> First-person Camera -> VLM Processor -> Action Selector -> Navigation Controller

**Critical Path:** Camera capture → VLM inference → Action selection → Environment update → Repeat

**Design Tradeoffs:** The benchmark prioritizes realistic exploration over computational efficiency, using 10-turn loops that may be expensive for large-scale evaluation but provide more thorough assessment of navigation capabilities.

**Failure Signatures:** Models getting stuck in repetitive actions, failing to cover the room, or showing performance degradation over time indicate fundamental limitations in planning and exploration strategies.

**First Experiments:**
1. Run single-step evaluation to establish baseline VLM performance on static hazard identification
2. Test navigation capability in simple room layouts before introducing complex environments
3. Measure action diversity entropy to quantify repetitive behavior patterns

## Open Questions the Paper Calls Out
- How can VLM inspection effectiveness be maintained during long-horizon multi-turn interactions?
- What mechanisms are needed to improve VLM navigation in high object density environments like living rooms?
- Does high performance on HomeSafeBench simulation correlate with real-world inspection capabilities?

## Limitations
- Exact 12,900 scene configurations not publicly available, limiting exact reproduction
- Evaluation relies on rule-based name matching that may introduce bias
- Focuses on static hazard identification without considering temporal dynamics

## Confidence
- **High confidence**: VLMs perform poorly on free-exploration safety inspection (F1=10.23%)
- **Medium confidence**: Free exploration is crucial for safety inspection
- **Low confidence**: Specific model limitations like repetitive actions based on qualitative observations

## Next Checks
1. Verify the 12,900 samples cover all five hazard categories proportionally once dataset is released
2. Implement entropy-based action diversity metric to quantify repetitive behavior patterns
3. Test model transfer from HomeSafeBench to real-world home safety datasets or other embodied benchmarks