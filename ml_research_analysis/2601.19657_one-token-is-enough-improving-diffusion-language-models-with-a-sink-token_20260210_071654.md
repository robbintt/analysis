---
ver: rpa2
title: 'One Token Is Enough: Improving Diffusion Language Models with a Sink Token'
arxiv_id: '2601.19657'
source_url: https://arxiv.org/abs/2601.19657
tags:
- sink
- token
- attention
- tokens
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion Language Models (DLMs) suffer from unstable attention
  sinks, where the sink token shifts unpredictably across diffusion steps due to bidirectional
  attention and dynamic masking. This phenomenon introduces inference instability.
---

# One Token Is Enough: Improving Diffusion Language Models with a Sink Token

## Quick Facts
- **arXiv ID:** 2601.19657
- **Source URL:** https://arxiv.org/abs/2601.19657
- **Reference count:** 20
- **Primary result:** Adding a single static sink token improves DLM performance by 2.28-11.70 points across reasoning benchmarks

## Executive Summary
Diffusion Language Models (DLMs) suffer from unstable attention sinks that shift unpredictably across diffusion steps, undermining inference robustness. This paper identifies that sink tokens consistently exhibit low L2 norms in the Transformer value space, functioning as a protective mechanism to reduce information mixing. By introducing a dedicated sink token constrained to attend only to itself while remaining globally visible, the authors create a stable attention anchor that improves performance by 2.28-11.70 points across multiple scales and training paradigms.

## Method Summary
The method involves prepending a special learnable sink token to the input sequence and applying a modified attention mask where the sink token can only attend to itself but all other tokens can attend to it. The model is trained using standard DLM objectives with AdamW optimizer (β1=0.9, β2=0.95, weight decay=0.1) and cosine learning rate schedule. Experiments are conducted on 0.5B and 1.5B parameter models initialized from autoregressive LLMs, as well as from-scratch trained models, using FineWeb and SlimPajama datasets.

## Key Results
- 0.5B-scale DLMs improve by 10.77-11.70 points on reasoning benchmarks when initialized from AR LLMs
- 1.5B-scale models improve by 2.28-3.00 points
- From-scratch trained DLMs improve by 8.25-7.95 points
- Sink token effectiveness is position-independent and a single token suffices
- Performance gains persist even when sink token value vectors are set to zero

## Why This Works (Mechanism)

### Mechanism 1
The "moving sink" phenomenon in DLMs introduces inference instability because the token acting as the attention sink shifts unpredictably across diffusion steps. Unlike AR models where causal masking anchors attention to the first token, DLMs use bidirectional attention and dynamic masking, allowing the sink to jump between different masked tokens at each step.

### Mechanism 2
Sink tokens consistently exhibit low L2 norms in the Transformer value space, functioning as an implicit "no-op" to minimize information mixing. When a model attends to a token with a low-norm value vector, the resulting update to the residual stream is minimal, allowing the model to satisfy softmax attention constraints without aggressively mixing information from irrelevant tokens.

### Mechanism 3
Introducing a dedicated, static sink token stabilizes the attention mechanism by providing a fixed, low-norm anchor regardless of its semantic content. An extra token is added and constrained via attention mask to attend only to itself while remaining visible to all other tokens, forcing it to remain semantically neutral and low-norm.

## Foundational Learning

- **Bidirectional vs. Causal Attention**: Understanding the difference is prerequisite to grasping why the sink "moves" in DLMs but stays fixed in standard LLMs. Quick check: How does the visibility of future tokens in DLMs prevent a specific token from naturally accumulating stable attention mass?

- **Softmax Competition & Attention Sinks**: The paper relies on softmax forcing attention weights to sum to 1, requiring models to dump excess mass on sink tokens when no strong semantic match exists. Quick check: Why doesn't the model simply assign zero attention to irrelevant tokens?

- **Value Space Norms (L2)**: The paper's diagnostic relies on measuring the magnitude of value vectors, where low norm implies minimal contribution to the residual stream update. Quick check: If a token receives 50% of the attention mass but has a value vector V ≈ 0, what is the effective impact on the output?

## Architecture Onboarding

- **Component map**: Input sequence X -> Prepend sink token [SINK] -> Modified attention mask M -> Standard DLM training
- **Critical path**: The asymmetric attention mask is critical - the sink token must not attend to content tokens to prevent semantic aggregation
- **Design tradeoffs**: Position is independent (start vs end works equally), single token is sufficient, sink can be initialized as zero-vector
- **Failure signatures**: Leaky sink (incorrect mask allowing sink to attend to content), training instability if sink aggregates semantic information
- **First 3 experiments**: 1) Zero-vector ablation to confirm structural vs semantic benefit, 2) Norm analysis to verify sink maintains lowest L2 norm, 3) Position ablation to confirm position independence

## Open Questions the Paper Calls Out
- Can the sink token approach maintain consistent performance gains when scaling to larger DLMs (7B+ parameters)?
- Can the sink token approach be effectively combined with sparse attention methods for DLMs?
- Why does element-wise gated attention improve performance at 0.5B scale but degrade it at 1.5B scale when training DLMs from autoregressive LLMs?

## Limitations
- Experiments were only conducted on 0.5B and 1.5B parameter scales, with diminishing returns observed at larger scales
- Performance gains are primarily validated on reasoning and commonsense benchmarks, not extensively on other NLP tasks
- The exact causal chain from mask constraint to improved performance is not fully proven, with alternative explanations not rigorously excluded

## Confidence
- **High Confidence**: Empirical performance gains are well-documented across multiple scales and training paradigms
- **Medium Confidence**: Core mechanism of low-norm tokens as protective "no-ops" is plausible but exact causal chain not fully proven
- **Medium Confidence**: Claim that effectiveness is independent of semantic content is supported but not strictly proven necessary

## Next Checks
1. Test robustness to mask variations by relaxing the sink token's self-attention constraint and measuring performance degradation
2. Evaluate 1.5B sink-enhanced DLM on broader NLP tasks including summarization, translation, and open-ended generation
3. Track attention mass variance to the sink token across diffusion steps and layers to verify stability as a true anchor