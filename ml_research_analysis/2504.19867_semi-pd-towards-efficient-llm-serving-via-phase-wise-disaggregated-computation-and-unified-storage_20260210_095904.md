---
ver: rpa2
title: 'semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation
  and Unified Storage'
arxiv_id: '2504.19867'
source_url: https://arxiv.org/abs/2504.19867
tags:
- prefill
- decode
- semi-pd
- disaggregated
- tpot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semi-PD, an LLM serving system that achieves
  disaggregated computation while maintaining unified storage to overcome the storage
  inefficiencies of existing disaggregated systems. The system employs a computational
  resource controller to dynamically partition streaming multiprocessors (SMs) between
  prefill and decode phases, and a unified memory manager to handle asynchronous memory
  access for both model weights and KV cache.
---

# semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage

## Quick Facts
- **arXiv ID**: 2504.19867
- **Source URL**: https://arxiv.org/abs/2504.19867
- **Reference count**: 40
- **Primary result**: Reduces average end-to-end latency per request by 1.27-2.58x and serves 1.55-1.72x more requests while adhering to latency constraints compared to state-of-the-art systems

## Executive Summary
This paper introduces semi-PD, an LLM serving system that achieves disaggregated computation while maintaining unified storage to overcome the storage inefficiencies of existing disaggregated systems. The system employs a computational resource controller to dynamically partition streaming multiprocessors (SMs) between prefill and decode phases, and a unified memory manager to handle asynchronous memory access for both model weights and KV cache. Additionally, semi-PD implements a low-overhead resource adjustment mechanism and an SLO-aware dynamic partitioning algorithm to optimize latency and throughput. Evaluation results show that semi-PD significantly reduces latency and increases throughput on various model series while maintaining service level objectives.

## Method Summary
The semi-PD system is built on existing serving frameworks (DistServe/SGLang) with modifications for SM-level disaggregation using NVIDIA MPS, unified memory management with atomic KV cache allocation, and a dynamic partitioning algorithm based on M/M/1 queuing models. The implementation requires CUDA 12.1, PyTorch 2.3.0, and NCCL 2.18 on A100/H200 GPUs, with specific kernel modifications for FlashAttention and fused FFN GEMMs. The system uses IPC-based weight/KV cache sharing with a resident worker mechanism to minimize switching overhead during SM re-partitioning.

## Key Results
- Reduces average end-to-end latency per request by 1.27-2.58x on DeepSeek series models
- Serves 1.55-1.72x more requests while adhering to latency constraints on Llama series models
- Eliminates storage imbalance and transfer overhead compared to fully disaggregated systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Isolating prefill and decode phases via SM-level partitioning eliminates latency interference while avoiding the storage overhead of GPU-level disaggregation.
- **Mechanism**: The system uses NVIDIA Multi-Process Service (MPS) to statically or dynamically partition Streaming Multiprocessors (SMs) into two distinct groups (x% for prefill, y% for decode). This allows compute-heavy prefill and memory-heavy decode to run asynchronously on the same physical GPU without time-slicing contention.
- **Core assumption**: The interference between phases is primarily computational resource contention, and partitioning SMs is sufficient to decouple their execution schedules.
- **Evidence anchors**: [Abstract]: "...dynamically partition streaming multiprocessors (SMs) between prefill and decode phases..."; [Section 4.3]: "We implement the disaggregated computation based on the Multi-Process Service (MPS) CUDA application interface..."; [Corpus]: "RAPID-Serve" abstract discusses "P/D Intra-GPU Disaggregation."

### Mechanism 2
- **Claim**: Unified storage with atomic KV cache management prevents storage imbalance and reduces memory waste common in disaggregated systems.
- **Mechanism**: A unified memory manager allows both prefill and decode workers to access a single pool of model weights and KV cache. To handle asynchronous access without corruption, the system uses atomic operations (Query -> Get -> Update) for KV cache block allocation, preventing Write-After-Read (WAR) conflicts.
- **Core assumption**: The overhead of atomic locking operations for memory allocation is lower than the overhead of transferring KV cache across GPUs or replicating weights.
- **Evidence anchors**: [Abstract]: "...unified memory manager to handle asynchronous memory access for both model weights and KV cache."; [Section 4.4]: "The WAR conflict happens when one worker immediately updates the utilization... we introduce the atomic operation for KV cache block allocating."; [Corpus]: Indirect; neighbor papers focus on disaggregation benefits but do not specifically validate the atomic allocation overhead tradeoff.

### Mechanism 3
- **Claim**: SLO-aware dynamic partitioning adapts to workload fluctuations more efficiently than static disaggregation.
- **Mechanism**: A feedback loop monitors TTFT and TPOT against Service Level Objectives (SLOs). It uses an M/M/1 queuing model to estimate latency based on current SM ratios and triggers a "worker switch" to rebalance SMs (e.g., shifting from 60/40 to 50/50) if latency models predict an SLO violation.
- **Core assumption**: Latency scales predictably with SM ratio (inverse linear relationship) and request rate, allowing the controller to pre-emptively adjust resources.
- **Evidence anchors**: [Abstract]: "...SLO-aware dynamic partitioning algorithm to optimize the SLO attainment."; [Section 5]: "We model TTFT and TPOT separately... T T F Tx = a1/(x − λ) + b1... TPOTy = a2/y + b2."; [Corpus]: Weak support; neighbors discuss scheduling but do not validate the specific M/M/1 model accuracy for LLMs.

## Foundational Learning

- **Concept**: **Prefill vs. Decode Phases**
  - **Why needed here**: The entire architecture relies on the distinct compute profiles of these phases. Prefill is compute-bound (processing prompt), while Decode is memory-bound (autoregressive generation).
  - **Quick check question**: Does increasing batch size improve the throughput of the Decode phase significantly, or does it primarily stress memory bandwidth?

- **Concept**: **Streaming Multiprocessors (SMs) & MPS**
  - **Why needed here**: Understanding that a GPU is a collection of parallel processors (SMs) is vital. The paper partitions these specific units rather than time-slicing the whole GPU.
  - **Quick check question**: If a GPU has 100 SMs and the configuration is (50, 50), can the Prefill worker access the memory cached by the Decode worker?

- **Concept**: **KV Cache & PagedAttention**
  - **Why needed here**: The system optimizes storage of the Key-Value cache. Understanding that this cache grows dynamically is key to seeing why "unified storage" saves memory compared to replicating it for separate GPUs.
  - **Quick check question**: In a disaggregated system, why does transferring the KV cache from Prefill to Decode instance cause latency spikes?

## Architecture Onboarding

- **Component map**: Request -> Prefill Queue -> Prefill Worker (computes KV cache, atomically allocates blocks) -> Decode Queue -> Decode Worker (reads KV cache, generates token)

- **Critical path**: Request arrives -> routed to prefill worker (x% SMs) -> generates KV cache blocks (atomic allocation) -> passes to decode worker (y% SMs) -> generates tokens using KV cache

- **Design tradeoffs**: 
  - Parallelism constraint: Prefill and Decode must share the same parallelism strategy (TP/PP), unlike fully disaggregated systems which can differ
  - Unified vs. Disaggregated: You gain storage efficiency and remove transfer overhead, but you lose the isolation of distinct GPU pools (memory pressure on one GPU)

- **Failure signatures**:
  - OOM during high Decode load: If the partition algorithm favors Prefill too heavily, Decode queue builds up, KV cache accumulates, and unified memory exhausts
  - Stalling during Switch: If the "resident worker" mechanism fails or IPC is slow, requests pause visibly during SM re-partitioning

- **First 3 experiments**:
  1. **Interference Baseline**: Run prefill-only and decode-only workloads, then combine them on a default GPU scheduler vs. semi-PD's partitioned SMs to visualize the "latency interference" removal
  2. **Memory Efficiency Stress Test**: Compare max batch size achievable on semi-PD (unified storage) vs. DistServe (disaggregated storage) on a single GPU to measure the "storage imbalance" gap
  3. **Switch Overhead Measurement**: Trigger rapid SM partition changes (e.g., every 10 seconds) and measure the throughput dip to verify the "low-overhead" claim of the resident worker mechanism

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- SM-level partitioning may not resolve interference when both phases become memory-bound under certain workloads
- Atomic KV cache allocation could introduce lock contention under extremely high allocation frequency
- M/M/1 queuing model for SLO-aware partitioning may not accurately capture complex, non-stationary LLM workload behavior

## Confidence

- **High confidence**: The architectural design of SM-level partitioning to eliminate latency interference is well-supported by the MPS implementation details and validated through comparative latency metrics
- **Medium confidence**: The unified storage approach with atomic KV cache management is theoretically sound, but the tradeoff between atomic overhead and transfer overhead lacks empirical validation against alternative designs
- **Low confidence**: The M/M/1-based SLO-aware dynamic partitioning algorithm's predictive accuracy under realistic, bursty workloads is not demonstrated, and the feedback loop could suffer from lag-induced oscillations

## Next Checks

1. **Memory-bound workload test**: Run workloads where both prefill and decode are memory-bandwidth limited (e.g., very long sequences with small batch sizes) to verify that SM partitioning still provides benefits when compute contention is not the primary bottleneck

2. **Atomic contention stress test**: Create a synthetic workload with extremely high KV cache allocation frequency (tiny tokens, massive parallelism) to measure whether atomic lock overhead becomes prohibitive compared to alternative designs like separate memory pools

3. **Bursty workload SLO validation**: Implement a realistic request pattern with sudden spikes and measure whether the M/M/1-based feedback controller maintains SLOs without oscillation, or if the latency prediction model requires refinement for non-stationary traffic