---
ver: rpa2
title: Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning
arxiv_id: '2502.06781'
source_url: https://arxiv.org/abs/2502.06781
tags:
- arxiv
- reasoning
- reward
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the limit of outcome reward-based reinforcement
  learning for mathematical reasoning tasks. The authors propose a new framework called
  OREAL that uses only binary outcome rewards (correct/incorrect) to train mathematical
  reasoning models.
---

# Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning

## Quick Facts
- arXiv ID: 2502.06781
- Source URL: https://arxiv.org/abs/2502.06781
- Reference count: 40
- Primary result: OREAL achieves state-of-the-art performance on MATH-500, with 7B model reaching 94.0 pass@1 accuracy through RL

## Executive Summary
This paper introduces OREAL, a novel reinforcement learning framework that achieves state-of-the-art performance on mathematical reasoning tasks using only binary outcome rewards (correct/incorrect). The key innovation is demonstrating that behavior cloning on positive trajectories from best-of-N sampling is theoretically sufficient to learn the KL-regularized optimal policy in binary feedback environments. The framework addresses the challenge of sparse rewards in long reasoning chains by introducing a token-level reward model that samples important tokens in reasoning trajectories for learning. OREAL-32B surpasses all previous models with 95.0 pass@1 accuracy on MATH-500.

## Method Summary
OREAL operates on the principle that binary outcome rewards can effectively train mathematical reasoning models when combined with trajectory sampling and token-level reward modeling. The framework samples multiple reasoning trajectories using best-of-N sampling, identifies positive trajectories (correct solutions), and then learns from these trajectories through behavior cloning. A token-level reward model is trained to predict the importance of individual tokens within reasoning chains, enabling the system to focus learning on critical decision points. This approach theoretically guarantees convergence to the KL-regularized optimal policy under binary feedback conditions while addressing the practical challenge of sparse rewards in long reasoning sequences.

## Key Results
- OREAL-7B achieves 94.0 pass@1 accuracy on MATH-500 through RL, matching previous 32B models
- OREAL-32B surpasses all previous models with 95.0 pass@1 accuracy on MATH-500
- The framework demonstrates strong performance on other mathematical reasoning benchmarks including AIME and AMC

## Why This Works (Mechanism)
The framework works by combining theoretical guarantees with practical engineering solutions. The theoretical foundation proves that behavior cloning on positive trajectories from best-of-N sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This is possible because the binary outcome reward, when combined with trajectory sampling, provides enough signal to guide policy optimization. The token-level reward model addresses the practical challenge of sparse rewards in long reasoning chains by identifying and focusing on important tokens within successful trajectories. This creates a feedback loop where the model learns to make critical decisions at key points in the reasoning process, rather than treating all tokens equally.

## Foundational Learning
- **KL-regularized optimal policy**: Understanding how to optimize policies under KL divergence constraints, which is essential for ensuring stable policy updates in RL
  - Why needed: Provides theoretical foundation for convergence guarantees in binary feedback settings
  - Quick check: Verify that policy updates follow KL-constrained optimization objectives

- **Behavior cloning on positive trajectories**: The technique of learning from successful examples rather than all attempts
  - Why needed: Enables learning from sparse binary rewards by focusing only on correct solutions
  - Quick check: Confirm that the model can learn from limited positive examples

- **Token-level reward modeling**: Predicting the importance of individual tokens in reasoning chains
  - Why needed: Addresses sparse reward problem by providing denser feedback within trajectories
  - Quick check: Validate that the token reward model accurately identifies critical decision points

- **Best-of-N sampling**: Generating multiple candidate solutions and selecting the best ones
  - Why needed: Creates a diverse set of trajectories to learn from in binary feedback environments
  - Quick check: Measure diversity and quality of sampled trajectories

## Architecture Onboarding

**Component map**: Environment -> Binary Reward -> Best-of-N Sampler -> Trajectory Selector -> Token-Level Reward Model -> Policy Learner -> Updated Policy

**Critical path**: The most critical execution path is Environment → Binary Reward → Best-of-N Sampler → Trajectory Selector → Policy Learner, as this sequence directly determines which examples the model learns from and how quickly it improves.

**Design tradeoffs**: The framework trades computational cost (from multiple sampling iterations) for improved sample efficiency and theoretical guarantees. Using binary rewards instead of dense rewards simplifies the reward engineering but requires sophisticated trajectory sampling to work effectively.

**Failure signatures**: Common failure modes include: (1) the token-level reward model failing to identify important tokens, leading to inefficient learning; (2) best-of-N sampling not generating diverse enough trajectories; (3) the policy learner not effectively generalizing from positive trajectories; and (4) reward hacking where the model learns to exploit the sampling process rather than genuine reasoning.

**First experiments**: (1) Validate binary reward effectiveness by comparing learning curves with and without trajectory sampling; (2) Test token-level reward model accuracy on identifying critical reasoning steps; (3) Evaluate best-of-N sampling diversity by measuring trajectory coverage across different reasoning strategies.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on specific assumptions about reward structure that may not generalize to all reasoning tasks
- Token-level reward model introduces additional complexity and potential biases in trajectory sampling
- Experimental validation is primarily focused on mathematical reasoning datasets, leaving uncertainty about performance on other reasoning domains

## Confidence

High confidence: Improved performance on MATH-500 and other mathematical reasoning benchmarks, and the basic framework of using binary outcome rewards with trajectory sampling.

Medium confidence: Theoretical claims about KL-regularized optimal policy learning under binary feedback, and general applicability beyond mathematical reasoning tasks.

Low confidence: Robustness to different reward structures, long-term stability of models trained with this method, and scalability to significantly larger models or different problem domains.

## Next Checks

1. Test OREAL's performance on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical puzzles) to evaluate domain generalizability and identify potential limitations in the reward modeling approach.

2. Conduct ablation studies systematically varying the number of samples in best-of-N sampling and the complexity of the token-level reward model to determine optimal configurations and computational trade-offs.

3. Implement long-term stability tests by training models for extended iterations and monitoring for reward hacking, performance degradation, or other failure modes that might emerge with prolonged training.