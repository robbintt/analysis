---
ver: rpa2
title: Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language
  Models
arxiv_id: '2601.12269'
source_url: https://arxiv.org/abs/2601.12269
tags:
- language
- belief
- autoregressive
- sampling
- annealing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors propose using simulated annealing\u2014a temperature-controlled\
  \ MCMC sampling method\u2014to improve theory-of-mind (ToM) reasoning in autoregressive\
  \ language models. Unlike standard decoding or Chain-of-Thought prompting, this\
  \ approach samples from a power-sharpened sequence-level distribution, enabling\
  \ exploration of globally coherent completions that align with underlying latent\
  \ belief states."
---

# Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models

## Quick Facts
- **arXiv ID:** 2601.12269
- **Source URL:** https://arxiv.org/abs/2601.12269
- **Reference count:** 10
- **Primary result:** Simulated annealing improves ToM reasoning accuracy by up to 16 percentage points on false-belief tasks versus direct decoding.

## Executive Summary
The authors propose using simulated annealing—a temperature-controlled MCMC sampling method—to improve theory-of-mind (ToM) reasoning in autoregressive language models. Unlike standard decoding or Chain-of-Thought prompting, this approach samples from a power-sharpened sequence-level distribution, enabling exploration of globally coherent completions that align with underlying latent belief states. Tested on the BigToM benchmark using three small language models (Phi-3.5-Mini-Instruct, LLaMA-3.2-3B-Instruct, and Qwen3-1.7B), the method achieved accuracy improvements of up to 16 percentage points on false-belief tasks compared to direct decoding. The gains were most pronounced in the harder false-belief condition, and model-generated reasoning showed evidence of structured belief hypothesis testing rather than simple reality-belief conflation.

## Method Summary
The method uses simulated annealing with MCMC to sample from power-sharpened sequence-level probability distributions, gradually shifting from high to low temperature to optimize ToM reasoning. Starting with a greedy-initialized sequence, the algorithm iteratively resamples random suffix blocks using the autoregressive model, accepts or rejects proposals via Metropolis-Hastings, and decays temperature exponentially (τ: 0.90 → 0.25) over 10 steps. This sequence-level approach contrasts with token-level power sampling by accounting for future trajectory quality, enabling discovery of coherent belief-consistent completions that standard decoding misses.

## Key Results
- Simulated annealing improved ToM accuracy by up to 16 percentage points on false-belief tasks versus direct decoding
- Accuracy gains were most pronounced for the harder false-belief condition (TB: 89%, FB: 74% vs direct decoding TB: 89%, FB: 58%)
- Model-generated reasoning exhibited structured belief hypothesis testing rather than simple reality-belief conflation

## Why This Works (Mechanism)

### Mechanism 1: Sequence-Level Power Sampling via MCMC
Sampling from power-sharpened sequence-level distribution (rather than token-level conditionals) recovers globally coherent completions that autoregressive decoding misses. The MCMC with Metropolis-Hastings acceptance compares whole-sequence likelihoods under p(x)^α, correctly accounting for future trajectory quality without computing the normalizing constant. This works because the pretrained model's sequence-level distribution contains high-quality modes accessible via MCMC but suppressed under greedy/local sampling.

### Mechanism 2: Annealing Temperature Schedule Enables Mode Discovery
Gradually decreasing temperature (τ: 0.90 → 0.25) over MCMC iterations converts sampling into optimization, improving ToM accuracy over fixed-temperature power sampling. High initial temperature permits exploration across the sequence distribution; gradual cooling concentrates probability mass, allowing the chain to converge toward a high-quality mode. This violates detailed balance but is acceptable for optimization, promoting rapid exploration early followed by progressively stronger concentration of probability mass.

### Mechanism 3: Elicited Reasoning Exhibits Belief Hypothesis Testing
Simulated annealing elicits reasoning chains that explicitly test alternative belief hypotheses (counterfactual evaluation), rather than conflating reality with agent belief. By exploring and settling on globally consistent sequences, the sampler favors completions where belief attribution is logically derived from observed actions and perceptual access, mirroring inverse planning in human ToM. This reveals rather than constructs the model's internalized belief-graph structure from pretraining.

## Foundational Learning

- **Autoregressive Language Models & Next-Token Prediction**
  - **Why needed here:** The method's core insight is that token-level autoregressive sampling optimizes local coherence at the expense of global consistency.
  - **Quick check question:** Can you explain why sampling from p(x_t|x_<t)^α does not generally yield samples from the power-sharpened joint p(x)^α?

- **Metropolis-Hastings MCMC**
  - **Why needed here:** The algorithm uses MH acceptance to compare proposed vs. current sequences, enabling sampling without computing intractable normalization constants.
  - **Quick check question:** In MH, what determines whether a proposed sequence x' replaces the current sequence x?

- **Theory of Mind & False-Belief Tasks**
  - **Why needed here:** ToM tasks test whether models maintain coherent latent belief graphs; false-belief conditions (agent doesn't observe a change) are especially diagnostic.
  - **Quick check question:** Why is false-belief reasoning harder than true-belief reasoning for both humans and language models?

## Architecture Onboarding

**Component map:**
[Prompt + Story] -> [Autoregressive LM] -> [Current Sequence x] -> [Random Position t] -> [Resample Suffix from LM] -> [Proposed x'] -> [Compute p(x)^α, p(x')^α, proposal ratios] -> [MH Accept/Reject] -> [Update Temperature τ_k per Schedule] -> [Repeat K iterations] -> [Final Sequence] -> [Extract Answer]

**Critical path:**
1. Initialize sequence with greedy/standard decoding
2. For each iteration k = 1...K:
   - Sample random edit position t
   - Resample suffix from autoregressive model
   - Compute acceptance probability A(x, x') with temperature τ_k
   - Accept or reject
   - Decay temperature: τ_k = τ_start × (τ_end/τ_start)^(k/K)

**Design tradeoffs:**
- **Compute vs. quality:** Annealing requires K=10 MCMC steps, each involving full-sequence likelihood computation. This is substantially more expensive than greedy decoding or CoT.
- **Exploration vs. convergence:** Wider temperature ranges might improve mode discovery but risk instability; the paper's τ: 0.90→0.25 is empirically tuned.
- **Block size:** Resampling blocks (vs. single tokens) accelerates mixing but may reduce granularity; paper uses 16 blocks, 512 max tokens.

**Failure signatures:**
- **Representational collapse:** Model asserts mutually inconsistent world-state facts (e.g., "valve was already open before rainfall")—indicates sampling found a local mode but it's incoherent.
- **Reality-belief conflation:** Generated reasoning ignores agent's perceptual access and updates beliefs to match true world state—suggests annealing failed to reach belief-consistent mode.
- **Stuck chain:** Acceptance rate near 0 or 100% indicates temperature schedule or proposal distribution is poorly matched.

**First 3 experiments:**
1. **Baseline replication:** Run direct decoding, CoT, fixed-τ power sampling (τ=0.25, τ=0.9), and simulated annealing (τ: 0.90→0.25) on BigToM backward-inference tasks for a single small LM; compare TB vs. FB accuracy.
2. **Ablation on temperature schedule:** Test alternative schedules (linear decay, slower decay over K=20 steps, fixed τ=0.5) to isolate the contribution of annealing vs. fixed-temperature sampling.
3. **Qualitative analysis of reasoning chains:** Manually annotate sampled outputs for belief hypothesis testing vs. reality-belief conflation patterns across conditions to verify mechanism 3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the simulated annealing approach generalize to other reasoning domains beyond Theory-of-Mind, such as moral reasoning, strategic interaction, multi-agent coordination, or long-horizon planning?
- **Basis in paper:** [explicit] "Finally, it remains an open question whether the benefits of sequence-level distortion generalize to other reasoning domains, such as moral reasoning, strategic interaction, multi-agent coordination, or long-horizon planning."
- **Why unresolved:** The study only evaluated ToM reasoning on the BigToM benchmark; no experiments were conducted in other reasoning domains.
- **What evidence would resolve it:** Systematic evaluation of simulated annealing decoding on benchmarks from each target domain (e.g., moral reasoning scenarios, game-theoretic tasks, multi-agent coordination problems), comparing against direct decoding and CoT baselines.

### Open Question 2
- **Question:** Can more efficient proposal mechanisms, adaptive stopping criteria, or hybrid approaches reduce the computational cost of simulated annealing while preserving accuracy gains?
- **Basis in paper:** [explicit] "Future work should investigate more efficient proposal mechanisms, adaptive stopping criteria, or hybrid approaches that selectively apply sequence-level optimization."
- **Why unresolved:** The current method is significantly more expensive than standard decoding or single-pass CoT, raising scalability concerns for longer sequences or real-time applications.
- **What evidence would resolve it:** Ablation studies testing alternative proposal distributions, convergence-based early stopping rules, or selective application heuristics that achieve comparable ToM accuracy with fewer MCMC iterations or reduced compute.

### Open Question 3
- **Question:** Do the accuracy improvements from simulated annealing scale to larger autoregressive language models beyond 4B parameters?
- **Basis in paper:** [inferred] The study deliberately focused on small language models (under 4B parameters) and did not test whether similar gains would emerge in larger models that may already perform well on ToM tasks.
- **Why unresolved:** Larger models may have different distributions over reasoning paths, and it is unclear whether sequence-level optimization provides marginal benefits when baseline performance is already high.
- **What evidence would resolve it:** Applying the identical simulated annealing protocol to models across a range of scales (e.g., 7B, 13B, 70B parameters) on the same BigToM benchmark, reporting accuracy deltas relative to direct decoding.

### Open Question 4
- **Question:** What are the representational limits of pretrained models that sequence-level optimization cannot overcome, particularly for deeply nested belief reasoning?
- **Basis in paper:** [explicit] "Even under annealing, failures persist in cases requiring deeply nested belief reasoning or complex causal structures, indicating that sequence-level optimization can reveal, but not fully overcome, limitations in the fixed representations of pretrained language models."
- **Why unresolved:** The paper identifies a failure mode but does not systematically characterize the types of nested reasoning structures that remain unsolvable.
- **What evidence would resolve it:** Controlled experiments varying the depth of belief nesting (e.g., "A believes that B believes that C believes...") and causal graph complexity, identifying the point at which simulated annealing accuracy drops to chance.

## Limitations
- The computational overhead of 10 MCMC iterations per query may limit practical applicability despite the accuracy gains.
- The qualitative claims about "structured belief hypothesis testing" versus "reality-belief conflation" lack rigorous validation.
- The method's benefits may not scale to larger models or generalize beyond the specific narrative format of the BigToM benchmark.

## Confidence

**High confidence:** The experimental methodology and implementation details for the MCMC sampling procedure are well-specified and reproducible. The baseline comparisons against direct decoding and CoT are appropriately controlled.

**Medium confidence:** The quantitative improvements on BigToM (up to 16 percentage points on FB tasks) are reported clearly, though the exact prompt formats and block resampling implementation details remain partially unspecified.

**Low confidence:** The qualitative claims about "structured belief hypothesis testing" versus "reality-belief conflation" lack rigorous validation. The mechanism by which annealing supposedly reveals rather than constructs ToM capability is not empirically verified.

## Next Checks

1. **Systematic belief-tracking analysis:** Implement automated evaluation of generated reasoning chains for explicit belief attribution markers (perceptual access, knowledge state updates) using the methods from "Doing Things with Words" (arXiv:2410.05602) to quantify the frequency of belief hypothesis testing versus reality conflation across conditions.

2. **Cross-task generalization:** Test simulated annealing on ToM tasks beyond BigToM, particularly first-order false-belief tasks from developmental psychology literature and the Social-Chem-100 benchmark, to determine whether improvements generalize beyond the specific narrative format.

3. **Intervention ablation study:** Compare simulated annealing against alternative sampling methods that maintain detailed balance (e.g., Langevin dynamics, tempered Langevin) to isolate whether the annealing schedule itself or the sequence-level power sampling is primarily responsible for the ToM improvements.