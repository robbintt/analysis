---
ver: rpa2
title: Training neural networks faster with minimal tuning using pre-computed lists
  of hyperparameters for NAdamW
arxiv_id: '2503.03986'
source_url: https://arxiv.org/abs/2503.03986
tags:
- hyperparameter
- workloads
- list
- workload
- lists
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using pre-computed lists of hyperparameters
  for NAdamW optimization to enable faster neural network training with minimal tuning.
  The method involves sampling hyperparameter points from a broad search space, running
  extensive experiments on the AlgoPerf benchmark workloads, and using a greedy procedure
  to construct ordered lists based on a cost function that prioritizes training time
  to reach validation targets.
---

# Training neural networks faster with minimal tuning using pre-computed lists of hyperparameters for NAdamW

## Quick Facts
- **arXiv ID:** 2503.03986
- **Source URL:** https://arxiv.org/abs/2503.03986
- **Authors:** Sourabh Medapati; Priya Kasimbeg; Shankar Krishnan; Naman Agarwal; George Dahl
- **Reference count:** 9
- **Primary result:** A 5-point hyperparameter list for NAdamW achieves at least 3x tuning efficiency gain on most AlgoPerf workloads compared to random search, successfully trains on 7 of 8 AlgoPerf base workloads in leave-one-out cross-validation, and performs well on held-out workload variants.

## Executive Summary
This paper proposes using pre-computed lists of hyperparameters for NAdamW optimization to enable faster neural network training with minimal tuning. The method involves sampling hyperparameter points from a broad search space, running extensive experiments on the AlgoPerf benchmark workloads, and using a greedy procedure to construct ordered lists based on a cost function that prioritizes training time to reach validation targets. The resulting 5-point hyperparameter list achieves at least 3x tuning efficiency gain on most AlgoPerf workloads compared to random search, successfully trains on 7 of 8 AlgoPerf base workloads in leave-one-out cross-validation, and performs well on held-out workload variants designed to test robustness to architectural modifications.

## Method Summary
The method constructs ordered hyperparameter lists by first sampling 200 candidate configurations from a broad search space that includes optimization parameters (learning rate, beta coefficients, warmup schedule) and regularization parameters (weight decay, dropout, label smoothing) using quasi-random search. These candidates are evaluated across 8 AlgoPerf benchmark workloads through 1,600 training trials. A greedy selection algorithm then constructs a 5-point list by iteratively adding the hyperparameter configuration that minimizes a geometric mean cost function with a penalty factor for missing validation targets. The final list is validated through leave-one-out cross-validation and testing on architectural variants of the base workloads.

## Key Results
- A 5-point hyperparameter list achieves at least 3x tuning efficiency gain on most AlgoPerf workloads compared to random search with the same budget
- The list successfully trains on 7 of 8 AlgoPerf base workloads in leave-one-out cross-validation
- The method outperforms basic learning rate/weight decay sweeps and Bayesian optimization tools when restricted to 5 parallel trials
- The approach performs well on held-out workload variants designed to test robustness to architectural modifications

## Why This Works (Mechanism)

### Mechanism 1: Diversity via Regularization-Inclusive Search Spaces
Including regularization hyperparameters (weight decay, dropout, label smoothing) in the search space increases the likelihood of finding configurations that generalize to architectural variants. The method samples from a space combining optimization and regularization, allowing the greedy selector to find "robust" points that balance fitting and generalization, which is necessary when the target workload's architecture shifts the optimal regularization strength.

### Mechanism 2: Penalty-Driven Greedy Selection for Coverage
A greedy construction algorithm using a geometric mean cost function with a failure penalty ($\tau > 1$) produces lists that prioritize coverage (successfully training at all) over speed, which is critical for generalization. By setting a penalty factor ($\tau=2.0$) for missing targets, the algorithm is forced to select points that cover "difficult" workloads in the library, incentivizing the selection of conservative configurations that are less likely to diverge on unseen tasks.

### Mechanism 3: Cross-Workload Transfer via "Resilient" Configurations
Validating on a leave-one-out basis filters out configurations that are "brittle" (high variance across tasks), leaving only those that transfer effectively. The method evaluates candidate lists on their ability to train the held-out workload, forcing the selection of points that lie in "flat" regions of the hyperparameter space where performance is stable across different loss landscapes.

## Foundational Learning

- **Concept: NAdamW (Nesterov-accelerated Adam with Decoupled Weight Decay)**
  - **Why needed here:** The lists are explicitly tuned for NAdamW. The interaction between weight decay and the gradient update differs from standard Adam + L2 regularization.
  - **Quick check question:** Can you explain why decoupled weight decay is preferred for generalization over L2 regularization in adaptive gradient methods?

- **Concept: Time-to-Result Benchmarking**
  - **Why needed here:** The cost function optimizes for the step count to reach a validation target, not the final accuracy. This changes the definition of a "good" hyperparameter from "highest peak" to "fastest ascent."
  - **Quick check question:** Why does the paper use the geometric mean of normalized training times instead of the arithmetic mean to aggregate costs?

- **Concept: Quasi-Random Search**
  - **Why needed here:** The candidate points are sampled via quasi-random search (not grid or pure random). This ensures better coverage of the high-dimensional search space with fewer samples.
  - **Quick check question:** How does low-discrepancy sequencing improve the probability of finding a viable hyperparameter point compared to standard random search?

## Architecture Onboarding

- **Component map:** Broad Search Space (Table 1: LR, betas, warmup, decay, dropout) + Workload Library (AlgoPerf) -> Runs 200 trials per workload → Generates Time-to-Target matrix -> Greedy Selector + Cost Function ($C_\tau$) -> Ordered 5-point List (Table 10)

- **Critical path:** The definition of the penalty factor $\tau$ in the cost function. If $\tau$ is too low, the list overfits to fast-but-brittle configurations.

- **Design tradeoffs:** Greedy vs. Exhaustive (Greedy chosen because Exhaustive overfitted training workloads); Budget vs. Robustness (5-point list fails on 1/8 variants, 7-point list covers all)

- **Failure signatures:** Infinite Cost ($\infty$) - configuration failed to reach validation target; Slow Convergence - step fraction high ($>0.8$), indicating stable but sub-optimal configuration

- **First 3 experiments:**
  1. Run the 5 points in Table 10 on ImageNet ResNet-50 workload to verify step fractions in Table 9
  2. Perform leave-one-out run excluding OGBG workload to test if resulting list still trains successfully on OGBG
  3. Run final list on ImageNet ViT Post-LN variant to analyze why point #4 or #5 failed (likely instability/NaNs due to Post-LN dynamics)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can scaling curves be derived for each hyperparameter in the list to ensure performance generalizes across varying model sizes?
- **Basis in paper:** Section 5.1 (Future Work) states: "An interesting extension of our work is to propose scaling curves for each hyperparameter starting at hyperparameter values suggested in this work to provide a solution that scales well with model size."
- **Why unresolved:** The current work focuses on specific benchmark workloads and does not provide a mechanism to adjust hyperparameters as model dimensions change.
- **What evidence would resolve it:** Experiments demonstrating that the proposed hyperparameter lists, when adjusted via specific scaling rules, maintain efficiency and training success on larger variants of the base workloads.

### Open Question 2
- **Question:** Can the dependence on a fixed training horizon (step budget) be eliminated by integrating schedule-free optimization methods?
- **Basis in paper:** Section 5.1 identifies "the problem of knowing the training horizon in advance" and suggests combining with methods that eliminate the need for learning rate tuning as done in Defazio et al. (2024).
- **Why unresolved:** The current hyperparameter list is tightly coupled with a cosine decay schedule requiring definition of total training steps a priori.
- **What evidence would resolve it:** A modified hyperparameter list utilizing schedule-free optimizers that achieves competitive time-to-result without requiring the user to specify the total number of training steps beforehand.

### Open Question 3
- **Question:** Does the greedy list construction methodology generalize effectively to other optimizers beyond NAdamW?
- **Basis in paper:** Section 5.1 notes: "We also believe that deriving such precomputed hyperparameter lists using the methodology in our work for a new training algorithm would make it easier for the practitioner to adopt such methods."
- **Why unresolved:** The paper validates the method only for NAdamW; it is unconfirmed if the cost function sensitivity and search space requirements are similar for fundamentally different update rules.
- **What evidence would resolve it:** Application of the greedy construction procedure to algorithms like AdamW or RMSprop, resulting in lists that outperform random search baselines on the same held-out workloads.

### Open Question 4
- **Question:** Do the derived hyperparameter lists transfer effectively to large-scale LLM workloads with billions of parameters?
- **Basis in paper:** Section 5 explicitly states: "We note that we have not tested our hyperparameter points on large scale problems like many of the LLM workloads prevalent in the industry."
- **Why unresolved:** The AlgoPerf benchmark consists of medium-scale workloads; optimization dynamics and regularization requirements may differ significantly in the LLM regime.
- **What evidence would resolve it:** Evaluation of the final 5-point hyperparameter list on standard LLM pre-training benchmarks (e.g., The Pile) showing competitive convergence rates compared to industry-standard hand-tuned settings.

## Limitations

- **Limited Scope of Generalization:** The method's primary limitation is its reliance on the AlgoPerf benchmark as a training set, with only 8 architectural variants tested for validation.
- **Resource Requirements for Initial Construction:** The initial list construction requires substantial computational resources: 1,600 full training runs (200 candidates × 8 workloads).
- **Sub-Optimality on Specific Workloads:** The cost function explicitly trades speed for robustness by incorporating a penalty for missing targets, making it slower than targeted sweeps for well-defined workloads.

## Confidence

- **High Confidence:** The core claim that pre-computed hyperparameter lists improve efficiency over random search within the same budget (5 trials), supported by leave-one-out cross-validation results and variant testing.
- **Medium Confidence:** The generalizability of the method to workloads outside the AlgoPerf benchmark, with encouraging but limited variant testing and failure on WMT indicating potential brittleness.
- **Low Confidence:** The long-term stability and continued effectiveness of the specific hyperparameter list, as the paper presents a static list without addressing how it would perform as neural network architectures and training paradigms evolve over time.

## Next Checks

1. **Robustness to Architectural Extremes:** Test the final 5-point list on a workload with architectural features drastically different from AlgoPerf (e.g., very deep transformer, GAN, or different graph neural network) to reveal if robustness extends beyond tested variants.

2. **Sensitivity to the Penalty Factor:** Systematically vary the penalty factor $\tau$ (e.g., 1.0, 1.5, 2.0, 2.5) and evaluate resulting lists on a held-out validation set to quantify the tradeoff between robustness and speed.

3. **Comparison to Modern Hyperparameter Tools:** Benchmark the 5-point list against current state-of-the-art hyperparameter optimization tools (e.g., Optuna, Ray Tune) when all are constrained to the same computational budget of 5 trials to contextualize efficiency relative to the broader field.