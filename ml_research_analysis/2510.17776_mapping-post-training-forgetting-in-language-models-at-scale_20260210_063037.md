---
ver: rpa2
title: Mapping Post-Training Forgetting in Language Models at Scale
arxiv_id: '2510.17776'
source_url: https://arxiv.org/abs/2510.17776
tags:
- forgetting
- transfer
- knowledge
- backward
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a sample-wise metric for quantifying forgetting
  and backward transfer during LLM post-training. It measures the proportion of items
  that flip from correct to incorrect (forgetting) or incorrect to correct (backward
  transfer) between pre- and post-training evaluations, with adjustments for chance
  in multiple-choice settings.
---

# Mapping Post-Training Forgetting in Language Models at Scale

## Quick Facts
- arXiv ID: 2510.17776
- Source URL: https://arxiv.org/abs/2510.17776
- Reference count: 40
- This work introduces a sample-wise metric for quantifying forgetting and backward transfer during LLM post-training.

## Executive Summary
This work introduces a sample-wise metric for quantifying forgetting and backward transfer during LLM post-training. It measures the proportion of items that flip from correct to incorrect (forgetting) or incorrect to correct (backward transfer) between pre- and post-training evaluations, with adjustments for chance in multiple-choice settings. Applied across post-training stages, model sizes, and data scales, the analysis finds that forgetting is generally low to moderate, with spikes in specific knowledge areas such as culture and knowledge categories. Instruction tuning and SFT/RL from base models show moderate-to-high backward transfer gains in math and logic, while reasoning training from instruction-tuned models shows data-scale-dependent effects. Model merging does not reliably mitigate forgetting. Overall, the sample-wise framework provides a clearer map of pretraining knowledge changes during post-training.

## Method Summary
The paper introduces a sample-wise metric for measuring forgetting and backward transfer in LLMs during post-training. Using the LightEval framework, it tracks per-item correctness transitions (1→0 for forgetting, 0→1 for backward transfer) between pre- and post-training evaluations, with chance adjustment for multiple-choice benchmarks. The method applies zero-shot chain-of-thought prompting for instruct models and few-shot examples for base models, using a strict "Answer: [LETTER]" format requirement. The framework evaluates 12 benchmarks across 9 knowledge categories and analyzes forgetting patterns across different post-training regimes, model sizes, and data scales.

## Key Results
- Forgetting is generally low to moderate across post-training regimes, with spikes in culture and knowledge categories
- Instruction tuning and SFT/RL from base models show moderate-to-high backward transfer gains in math and logic
- Model merging does not reliably mitigate forgetting
- Larger models show reduced forgetting and increased backward transfer, possibly due to greater representational capacity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sample-wise transition counting reveals forgetting masked by aggregate accuracy metrics.
- **Mechanism:** Track per-item correctness flips (1→0 for forgetting, 0→1 for backward transfer) rather than comparing aggregate pre/post accuracy. This isolates which specific knowledge items degrade, since "knowing one U.S. president does not compensate for forgetting another."
- **Core assumption:** Knowledge items are non-fungible; losing item A is not offset by gaining item B.
- **Evidence anchors:**
  - [abstract] "Our metric counts 1→0 transitions (correct before post-training, incorrect after) to quantify forgetting and 0→1 transitions to quantify backward transfer. Traditional task averages conflate these effects and obscure large changes."
  - [section 1] "Pretrained knowledge violates this assumption... In short, knowledge samples are not fungible: Each carries unique value."
  - [corpus] Limited direct validation; related work (Toneva et al. 2019) uses similar sample-level forgetting metrics but in smaller models.
- **Break condition:** If knowledge items were fungible (e.g., classification with interchangeable class members), aggregate accuracy would suffice.

### Mechanism 2
- **Claim:** Chance adjustment prevents illusory forgetting/transfer in multiple-choice benchmarks.
- **Mechanism:** Subtract expected flips from random guessing: F_chance = (1-ā_pre)/(k-1) × (1-ā_post) where k is number of choices. Corrected metrics (F_true, BT_true) isolate knowledge change beyond noise.
- **Core assumption:** When models don't know the answer, they guess uniformly at random; pre/post guessing events are independent.
- **Evidence anchors:**
  - [section 2] "An apparent '1→0' may simply be a lucky guess that later becomes an incorrect answer, even when the underlying knowledge did not change."
  - [section 2] "if accuracy drops from 80% to 70% on a 4-option MCQ test, raw forgetting is 10%, but chance-adjusted forgetting is only about 6%."
  - [corpus] Not validated in neighbors; assumptions about uniform guessing may not hold for well-calibrated models.
- **Break condition:** If models exhibit non-uniform guessing patterns (e.g., always favor option B), the correction misestimates chance contributions.

### Mechanism 3
- **Claim:** Forgetting severity depends on post-training regime and model scale.
- **Mechanism:** Different post-training objectives (domain-CPT, instruction tuning, RL reasoning) induce different forgetting patterns. Larger models show reduced forgetting, possibly due to greater capacity retaining representations.
- **Core assumption:** Forgetting is a function of training objective interference with pretraining representations, not just data volume.
- **Evidence anchors:**
  - [section 3.1] "Domain-continual pretraining induces little to moderate amounts of forgetting... larger models forget less."
  - [section 3.2] "scaling model size reduces forgetting and increases backward transfer."
  - [corpus] Consistent with Ramasesh et al. (2022) finding pretrained networks are robust to forgetting at scale, but contradicts Luo et al. (2025) reporting increased forgetting with scale.
- **Break condition:** If a new training objective fundamentally overwrites weights (e.g., full fine-tuning on narrow domain), scale may not protect.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper directly challenges assumptions about how severe forgetting is in modern post-training pipelines.
  - **Quick check question:** Can you explain why forgetting one fact (e.g., a president's name) is not "compensated" by learning another?

- **Concept: Backward Transfer**
  - **Why needed here:** The paper distinguishes forgetting from positive backward transfer (learning A helps with B) at the sample level.
  - **Quick check question:** Why might instruction tuning cause backward transfer gains in math categories?

- **Concept: Multiple-Choice Evaluation Bias**
  - **Why needed here:** The chance-adjustment mechanism depends on understanding how random guessing inflates metrics.
  - **Quick check question:** On a 4-choice benchmark, what fraction of 1→0 flips would occur purely by chance between two random classifiers?

## Architecture Onboarding

- **Component map:** Pre/post evaluation harness (LightEval framework) -> Sample-level logger -> Transition counter -> Chance adjuster -> Category analyzer
- **Critical path:** Run baseline evaluation → Apply post-training → Re-evaluate on identical samples → Compute transitions → Adjust for chance → Analyze by category
- **Design tradeoffs:**
  - Using only accuracy (no logits) enables scale but prevents richer uncertainty estimates
  - Zero-shot CoT prompting for all models; base models get few-shot format examples
  - 32K token budget excludes very long reasoning traces
- **Failure signatures:**
  - Instruction-following degradation: models answer in code/numbers instead of MCQ format, artificially inflating forgetting
  - Output degeneration after merging: repeated phrases without final answers
  - Low forgetting may simply reflect low initial knowledge (check F_max ceiling)
- **First 3 experiments:**
  1. Replicate the chance-adjustment sanity check: confirm raw F differs from F_true on a held-out MCQ benchmark with known ground-truth transitions.
  2. Test a new post-training regime (e.g., DPO preference tuning) using the same sample-wise framework to compare forgetting rates.
  3. Validate uniform-guessing assumption by analyzing incorrect-answer distributions for patterns (e.g., position bias) that would violate correction assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors (e.g., initialization, data regime, scale) robustly explain the dynamics of forgetting and backward transfer in high-data reasoning post-training?
- Basis in paper: [explicit] The authors state that for high-data regimes, "effects are mixed and warrant further study with better controls" and "no single dominant factor... sufficiently explains forgetting and backward-transfer dynamics."
- Why unresolved: The interaction between large-scale data and model behavior in this specific pipeline appears complex and inconsistent compared to low-data or base-model scenarios.
- What evidence would resolve it: A controlled ablation study isolating data mixture, model size, and initialization seed to identify causal drivers of forgetting.

### Open Question 2
- Question: Does the number of models merged or the magnitude of weight drift determine the success of merging in mitigating forgetting?
- Basis in paper: [explicit] The paper hypothesizes that merging failure might be due to "weight drift" between two checkpoints being larger than in typical merging literature and suggests "future works may consider the effect of the number of models merged."
- Why unresolved: Current experiments only merged two checkpoints and resulted in performance degradation rather than retention.
- What evidence would resolve it: Experiments merging more than two checkpoints (e.g., intermediate checkpoints) and measuring the correlation between weight drift magnitude and forgetting mitigation.

### Open Question 3
- Question: Does sample-wise forgetting represent true erasure of knowledge or merely a failure to elicit existing knowledge due to weakened instruction-following?
- Basis in paper: [inferred] The paper notes the metric "could quantify failure to elicit previously accessible knowledge" and qualitative analysis showed "reduced instruction-following fidelity" (e.g., base models outputting code in chat mode) led to apparent forgetting.
- Why unresolved: The current metric relies on behavioral output (answers) rather than internal state inspection, making it difficult to distinguish between unlearning and poor elicitation.
- What evidence would resolve it: Probing classifiers or mechanistic interpretability methods to check if the "forgotten" information still exists in the model weights despite not being outputted.

## Limitations
- The chance-adjustment mechanism assumes uniform random guessing, which may not hold for well-calibrated models with position bias or plausibility heuristics
- The metric treats all knowledge items as independent and equally valuable, ignoring possible hierarchical or compositional relationships
- The study focuses on post-training forgetting from pretraining but does not examine forgetting during pretraining itself or from other pretraining stages

## Confidence
**High confidence:** The sample-wise counting methodology for tracking knowledge transitions is well-defined and reproducible. The overall finding that forgetting is generally low-to-moderate across post-training regimes is supported by multiple experiments and consistent with the literature on scale-induced forgetting robustness.

**Medium confidence:** The chance-adjustment formulas provide a principled correction for random guessing, but their validity depends on the uniform guessing assumption which is not directly validated. The categorization of knowledge into 9 groups and subsequent analysis of forgetting patterns across categories is reasonable but could be sensitive to how sub-benchmarks are grouped.

**Low confidence:** The comparative claim that model merging does not reliably mitigate forgetting is based on limited experiments and may not generalize across different merging strategies or model architectures.

## Next Checks
1. Validate uniform guessing assumption: Analyze the distribution of incorrect answers across all MCQ benchmarks to detect non-uniform patterns (position bias, option plausibility) that would invalidate the chance adjustment formulas.

2. Cross-validation with alternative metrics: Apply alternative forgetting measures (e.g., parameter distance metrics, weight importance measures) to the same post-training scenarios to verify whether sample-wise forgetting captures the full picture of knowledge change.

3. Generalization to new post-training regimes: Test the framework on a broader set of post-training methods (e.g., DPO preference tuning, supervised fine-tuning with different objectives) to confirm the observed forgetting patterns hold across diverse training paradigms.