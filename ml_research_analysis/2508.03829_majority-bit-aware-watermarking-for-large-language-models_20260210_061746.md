---
ver: rpa2
title: Majority Bit-Aware Watermarking For Large Language Models
arxiv_id: '2508.03829'
source_url: https://arxiv.org/abs/2508.03829
tags:
- majormark
- message
- text
- decoding
- green
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the text quality-degradation problem in existing\
  \ multi-bit watermarking methods for LLMs, where large green lists improve text\
  \ quality but hurt decoding accuracy. The authors propose MajorMark, a majority-bit-aware\
  \ encoding scheme that ensures a minimum green list ratio of 0.5 and an expected\
  \ ratio of 0.5 + 1/\u221A(2\u03C0b), eliminating the need for hyperparameter tuning."
---

# Majority Bit-Aware Watermarking For Large Language Models

## Quick Facts
- arXiv ID: 2508.03829
- Source URL: https://arxiv.org/abs/2508.03829
- Reference count: 40
- Primary result: Majority-bit-aware encoding improves LLM watermarking text quality while maintaining high decoding accuracy.

## Executive Summary
This paper tackles the text quality-degradation problem in existing multi-bit watermarking methods for LLMs, where large green lists improve text quality but hurt decoding accuracy. The authors propose MajorMark, a majority-bit-aware encoding scheme that ensures a minimum green list ratio of 0.5 and an expected ratio of 0.5 + 1/√(2πb), eliminating the need for hyperparameter tuning. Instead of relying on token frequency counting, MajorMark uses clustering-based decoding, which maintains high accuracy even with large green lists. They further introduce MajorMark+, which divides the message into blocks for independent encoding and deterministic decoding, further improving text quality and accuracy.

## Method Summary
The method uses majority-bit-aware encoding where the green list is constructed by unioning all shards corresponding to the majority bit positions in the message. This guarantees a green list ratio γ ≥ 0.5. The encoder uses a hash function to permute vocabulary and partition into b equal shards, then boosts logits for tokens in the green list. Decoding uses KMeans clustering on shard-wise token counts rather than frequency counting. MajorMark+ extends this by dividing messages into blocks and using deterministic decoding that enumerates all valid (majority bit, occurrence count) configurations per block, selecting the one with maximum standard deviation in shard counts.

## Key Results
- MajorMark+ consistently outperforms baselines in both decoding accuracy (+4.30% over RSBH at b=64) and text quality
- Maintains high decoding accuracy (>94% BA) even under copy-paste and paraphrase attacks
- Achieves lower perplexity and higher top-5 hit rates compared to MPAC and RSBH baselines

## Why This Works (Mechanism)

### Mechanism 1: Majority Bit-Aware Encoding for Guaranteed Green List Size
Selecting the green list based on the majority bit of the message improves text quality by guaranteeing a green list ratio γ ≥ 0.5. Given a b-bit message m, identify the majority bit λ (0 or 1, whichever appears more). Partition vocabulary into b equal shards, then union all shards corresponding to bit positions where mi = λ. Since λ appears at least ⌈b/2⌉ times, γ ≥ 0.5 always. Expected E_m[γ] = 0.5 + 1/√(2πb).

### Mechanism 2: Clustering-Based Decoding (Not Frequency Counting)
Decoding by clustering shard-wise token occurrences maintains accuracy even with large green lists, breaking the accuracy-quality trade-off. For each token, reconstruct which shard it belongs to using the same hash function. After processing all tokens, count occurrences per shard. Apply KMeans (K=2) to partition shards into two clusters; the higher-mean cluster corresponds to the majority bit. Unlike prior methods that count how often tokens fall into green lists (which weakens as γ grows), this approach detects which shards were systematically boosted.

### Mechanism 3: Block-Wise Encoding with Deterministic Decoding (MajorMark+)
Dividing the message into r blocks and deterministically enumerating (λ_i, h_{λ_i}) pairs during decoding improves both γ and decoding reliability. Split message into r blocks; each token encodes one block (selected via (x_{t-1} + x_{t-2}) mod r). Expected γ = 0.5 + 1/√(2π(b/r)), larger than original for r > 1. Decoding enumerates all valid (majority bit, occurrence count) combinations per block; the configuration producing maximum standard deviation in shard counts is selected.

## Foundational Learning

- **Concept: Green/Red List Partitioning in LLM Watermarking**
  - Why needed here: Core mechanism—green list tokens receive logit boost δ, increasing sampling probability. Prior methods restrict green list size to strengthen signal; MajorMark expands it via majority-bit selection.
  - Quick check question: If |V| = 50,000 and γ = 0.25, how many tokens are in the green list? (Answer: 12,500)

- **Concept: Perplexity as Text Quality Metric**
  - Why needed here: Paper uses PPL (computed by a larger reference model) to measure how much watermarking degrades generation quality. Lower is better.
  - Quick check question: If baseline PPL = 3.81 and watermarked PPL = 6.06, what does the +2.25 difference indicate? (Answer: Watermarking introduced measurable quality degradation)

- **Concept: Hash-Based Pseudo-Random Seed Generation**
  - Why needed here: Encoder and decoder must independently reconstruct identical token-to-shard mappings. Hash(k, x_{t-1}, x_{t-2}, λ) provides deterministic, context-dependent seeds.
  - Quick check question: Why include λ in the hash input? (Answer: So decoder can enumerate λ ∈ {0,1} and identify which produces skewed shard distributions)

## Architecture Onboarding

- **Component map**: Encoder: Hash → Permute(V) → Partition into b shards → Union majority-bit shards → Add δ to green logits → Sample
- **Decoder (MajorMark)**: Enumerate λ ∈ {0,1} → Count shard occurrences → Select λ with highest std → KMeans(K=2) → Assign bits by cluster membership
- **Decoder (MajorMark+)**: Per block, enumerate (λ, h_λ) → Count shard occurrences → Select config with max std → Assign top-h_λ shards to λ

- **Critical path**: Encoding: Extract λ → Hash → Partition → Build green list → Boost → Sample (repeat T times). Decoding: Reconstruct shard mappings → Count → Cluster/Enumerate → Recover λ and bit assignments

- **Design tradeoffs**: r (blocks): Higher r → larger γ (better PPL) but more decoding iterations. δ (bias): Higher δ → stronger signal but more distortion. b (message length): Longer b → more capacity but lower BA and PPL. Clustering vs. enumeration: MajorMark simpler; MajorMark+ more accurate but higher compute

- **Failure signatures**: Near-uniform shard counts (std ≈ 0) → wrong λ or weak δ. PPL >> baseline → green list too restrictive or δ too high. BA drops sharply as b increases → insufficient tokens per block. Copy-Paste/Paraphrase mixing → external tokens dilute signal

- **First 3 experiments**: 1) Baseline comparison on C4 with b ∈ {8, 32, 64}, δ ∈ {2, 4, 6}: Verify MajorMark+ achieves lowest PPL and highest BA vs. MPAC, RSBH. 2) Ablation on r ∈ {1, 2, 4, 8} with b = 32: Confirm r = 2 balances BA and PPL; higher r improves PPL but may hurt BA. 3) Attack robustness (Copy-Paste 10% mix, Paraphrase with LLaMA-2-13B) with b = 32: Validate MajorMark+ maintains >94% average BA across attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the watermarking bias $\delta$ and the green list ratio $\gamma$ be co-designed or jointly optimized to further enhance the trade-off between text quality and decoding accuracy?
- Basis: [explicit] The authors state in the Introduction that they "leave the investigation of the latter factor [watermarking bias] as future work" and explicitly mention in Appendix A.9 that "Co-designing in the green list ratio $\gamma$ and the watermarking bias $\delta$ is also a promising future direction."
- Why unresolved: The current work treats the bias $\delta$ as a static hyperparameter while focusing primarily on optimizing the green list ratio $\gamma$. The interaction between dynamic bias adjustment and the majority bit-aware encoding remains unexplored.
- What evidence would resolve it: A framework where $\delta$ is adjusted dynamically or trained in conjunction with $\gamma$, demonstrating improved bit accuracy or lower perplexity compared to the fixed-bias baselines.

### Open Question 2
- Question: How can the decoding efficiency of MajorMark+ be improved to reduce the computational overhead associated with enumerating message configurations?
- Basis: [explicit] The authors note in Appendix A.9 that "MajorMark+ requires additional computation time to decode messages... further improving its decoding efficiency remains a promising direction."
- Why unresolved: MajorMark+ improves accuracy by exhaustively enumerating combinations of the majority bit and its frequency, which is more efficient than prior brute-force methods but still introduces latency compared to simpler clustering approaches.
- What evidence would resolve it: A modified decoding algorithm that reduces the time complexity of the configuration search (e.g., using pruning heuristics or approximate inference) without significantly sacrificing bit accuracy.

### Open Question 3
- Question: Can the encoding strategy be modified to allow for a more flexible specification of the green list ratio $\gamma$ without losing the benefits of frequency-independent decoding?
- Basis: [explicit] Appendix A.9 states, "it would be desirable to develop methods that preserve the advantages of MajorMark... while allowing for a more flexible specification of $\gamma$."
- Why unresolved: The current majority bit-aware encoding automatically determines $\gamma$ based on the message content (constraining it to $\ge 0.5$), which limits the ability to manually tune or restrict the green list size for specific application requirements.
- What evidence would resolve it: A variant of the encoding scheme that decouples the message payload from the strict determination of the green list size, allowing $\gamma$ to be set as an independent parameter while maintaining high decoding robustness.

### Open Question 4
- Question: How robust is the expected green list ratio and text quality when the embedded message bits do not follow a uniform distribution?
- Basis: [inferred] The theoretical guarantee for the green list ratio (Theorem 1) relies on Assumption 1, which posits that message bits are sampled uniformly at random (Bernoulli 0.5). Real-world payloads (e.g., structured IDs or timestamps) may violate this.
- Why unresolved: If messages contain long sequences of identical bits (low entropy), the majority bit dominance could force $\gamma$ to approach 1.0, potentially degrading the watermark's distinctiveness or text utility differently than predicted.
- What evidence would resolve it: An empirical or theoretical analysis of MajorMark's performance using non-uniform, structured message datasets to verify if the text quality (PPL) and decoding accuracy remain stable.

## Limitations

- The paper's robustness claims against paraphrasing and copy-paste attacks are based on limited testing scenarios.
- Computational complexity of MajorMark+ grows with message length due to enumeration of configurations, though the paper asserts this is manageable.
- The security and collision resistance of the simple linear congruential hash function for watermarking purposes is not discussed.

## Confidence

**High confidence**: The mechanism of majority-bit-aware encoding to guarantee γ ≥ 0.5 is mathematically sound and well-demonstrated. The empirical results showing MajorMark+ outperforming baselines in both PPL and BA across multiple settings are robust and reproducible.

**Medium confidence**: The clustering-based decoding's superiority over frequency-based methods is theoretically justified but relies on the assumption that shard counts will form distinct clusters. The deterministic decoding in MajorMark+ is validated but the enumeration approach may face scalability challenges not fully explored.

**Low confidence**: The paper's robustness claims against paraphrasing and copy-paste attacks are based on limited testing. While results show BA remains >99% after paraphrasing, the methodology doesn't explore adaptive attackers who might specifically target watermarking patterns.

## Next Checks

1. **Robustness under adaptive attacks**: Test MajorMark+ against an attacker who knows the watermarking scheme and attempts to remove it by: (a) re-ranking top-k tokens to avoid green list items, (b) using nucleus sampling with p < 1.0, and (c) applying controlled paraphrasing that specifically preserves semantic meaning while altering token distributions. Measure BA degradation and compare against baseline methods.

2. **Scalability analysis**: Systematically evaluate MajorMark+ decoding time as b increases from 32 to 256 in increments of 32, measuring: (a) enumeration time for all (λ, h_λ) configurations, (b) KMeans clustering stability as token counts per shard decrease, and (c) the point at which BA drops below 95% due to insufficient statistical power per block.

3. **Cross-model transferability**: Evaluate whether watermarks embedded in LLaMA-2-7B text can be accurately decoded by: (a) LLaMA-2-13B (as in perplexity evaluation), (b) GPT-3.5-turbo, and (c) an encoder-only model like BERT. This tests the practical deployment scenario where watermarks must be detected across different model architectures and the robustness of the shard-based encoding to model-specific token distributions.