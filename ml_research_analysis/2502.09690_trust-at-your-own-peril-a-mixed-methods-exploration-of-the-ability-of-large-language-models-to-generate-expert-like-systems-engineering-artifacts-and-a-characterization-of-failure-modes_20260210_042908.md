---
ver: rpa2
title: 'Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large
  Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization
  of Failure Modes'
arxiv_id: '2502.09690'
source_url: https://arxiv.org/abs/2502.09690
tags:
- llms
- system
- generated
- engineering
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the ability of multi-purpose Large Language
  Models (LLMs) to generate expert-like Systems Engineering (SE) artifacts without
  fine-tuning. A human-expert generated artifact served as a benchmark, and various
  LLMs were prompted to generate SE artifact segments.
---

# Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes

## Quick Facts
- arXiv ID: 2502.09690
- Source URL: https://arxiv.org/abs/2502.09690
- Authors: Taylan G. Topcu; Mohammed Husain; Max Ofsa; Paul Wach
- Reference count: 40
- Key outcome: LLMs can generate SE artifacts with high semantic similarity to expert benchmarks when prompted carefully, but these artifacts often contain serious deficiencies including premature requirements definition, unsubstantiated numerical estimates, and overspecification.

## Executive Summary
This study investigates whether multi-purpose Large Language Models (LLMs) can generate expert-like Systems Engineering artifacts without fine-tuning. Using a human-expert generated artifact as a benchmark, various LLMs were prompted to generate SE artifact segments that were then evaluated using both quantitative (MAUVE algorithm for semantic similarity) and qualitative (human expert analysis) methods. The research reveals that while LLMs can produce semantically similar artifacts to expert benchmarks when carefully prompted, these artifacts contain significant deficiencies that could lead to misleading information and poor design decisions.

The findings highlight a critical paradox in AI-assisted systems engineering: LLMs can generate outputs that appear expert-like in structure and language, yet contain fundamental flaws that could be catastrophic in real-world engineering contexts. This underscores the need for caution when deploying LLMs in systems engineering workflows and points to specific failure modes that practitioners should watch for when evaluating AI-generated content.

## Method Summary
The study employed a mixed-methods approach to evaluate LLM performance in generating systems engineering artifacts. Researchers first created a human-expert benchmark artifact, then prompted various LLMs to generate similar artifact segments. The generated artifacts were evaluated using two complementary methods: the MAUVE algorithm measured semantic similarity between LLM outputs and the expert benchmark, while human experts conducted qualitative analysis to identify failure modes. This dual approach allowed researchers to assess both the surface-level similarity of outputs and their substantive quality from an engineering perspective.

## Key Results
- LLMs can generate SE artifacts with high semantic similarity to expert benchmarks when prompted carefully
- LLM-generated artifacts frequently contain serious deficiencies including premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify
- These failure modes can lead to misleading information and poor design decisions in systems engineering contexts

## Why This Works (Mechanism)
The ability of LLMs to generate expert-like SE artifacts stems from their training on vast corpora of technical documentation and their capacity to recognize and reproduce patterns in language and structure. When prompted appropriately, LLMs can leverage their understanding of technical terminology, document structure, and domain-specific conventions to produce outputs that superficially resemble expert work. However, this surface-level competence masks deeper deficiencies in reasoning, validation, and contextual understanding that are critical for authentic systems engineering.

## Foundational Learning
- **Semantic similarity metrics**: Understanding how algorithms like MAUVE quantify textual similarity is essential for evaluating LLM outputs objectively. Quick check: Can you explain how semantic similarity differs from simple keyword matching?
- **Systems engineering artifact structure**: Knowledge of typical SE artifact components and their relationships helps identify when LLM outputs deviate from best practices. Quick check: Can you list the key sections typically found in a systems engineering requirements document?
- **Prompt engineering principles**: Understanding how prompt design affects LLM output quality is crucial for practical applications. Quick check: Can you identify three prompt characteristics that might influence the quality of technical outputs?

## Architecture Onboarding
Component map: User prompt -> LLM processing -> Token generation -> Output artifact
Critical path: Prompt design → Model inference → Output evaluation → Expert review
Design tradeoffs: High semantic similarity vs. substantive accuracy; broad knowledge vs. domain specificity; ease of use vs. reliability
Failure signatures: Premature specification, unsubstantiated numbers, overspecification in generated artifacts
First experiments: 1) Test different prompt structures on the same SE artifact type, 2) Compare semantic similarity across multiple LLM models for identical prompts, 3) Evaluate expert identification of failure modes across different SE artifact categories

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize to other domains beyond systems engineering
- Semantic similarity measurements may not fully capture practical utility or correctness
- Human expert evaluation represents a single perspective and may introduce subjective bias

## Confidence
- Medium confidence in the observation that LLMs can generate SE artifacts with high semantic similarity to expert benchmarks when prompted carefully
- High confidence in the identification of specific failure modes (premature requirements definition, unsubstantiated numerical estimates, overspecification)
- Low confidence in the generalizability of failure mode severity across different types of SE artifacts and organizational contexts

## Next Checks
1. Replicate the study with multiple independent expert evaluators to assess inter-rater reliability in identifying failure modes and to reduce individual bias
2. Test the same prompting strategy across different types of SE artifacts (e.g., system architecture diagrams, test plans, risk assessments) to evaluate consistency of results
3. Conduct a longitudinal study tracking the downstream impact of LLM-generated artifacts on actual design decisions and project outcomes in real engineering teams