---
ver: rpa2
title: '(Token-Level) InfoRMIA: Stronger Membership Inference and Memorization Assessment
  for LLMs'
arxiv_id: '2510.05582'
source_url: https://arxiv.org/abs/2510.05582
tags:
- membership
- privacy
- inference
- rmia
- informia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces InfoRMIA, a new information-theoretic approach\
  \ to membership inference attacks that consistently outperforms the state-of-the-art\
  \ RMIA across tabular, image, and text datasets while requiring far fewer population\
  \ samples. InfoRMIA improves upon RMIA by using a continuous test statistic derived\
  \ from a principled composite hypothesis testing formulation, eliminating the need\
  \ for the hyperparameter \u03B3 and reducing sensitivity to population dataset size."
---

# (Token-Level) InfoRMIA: Stronger Membership Inference and Memorization Assessment for LLMs

## Quick Facts
- arXiv ID: 2510.05582
- Source URL: https://arxiv.org/abs/2510.05582
- Reference count: 40
- Primary result: InfoRMIA consistently outperforms RMIA across tabular, image, and text datasets while requiring fewer population samples, and token-level InfoRMIA localizes privacy leakage to individual tokens in LLMs

## Executive Summary
This paper introduces InfoRMIA, an information-theoretic improvement over RMIA that uses continuous test statistics derived from composite hypothesis testing, eliminating the need for the γ hyperparameter and reducing sensitivity to population dataset size. The authors also propose a token-level framework for privacy assessment in LLMs that localizes information leakage to individual tokens, revealing that sequence-level membership inference can overestimate privacy risks by averaging over non-private tokens. Token-level InfoRMIA achieves stronger sequence-level inference performance and enables targeted privacy analysis for applications like surgical unlearning.

## Method Summary
InfoRMIA reformulates membership inference as a composite hypothesis testing problem, using an expected log-likelihood ratio as a continuous test statistic that combines log-likelihood ratio and KL divergence terms. For LLMs, token-level InfoRMIA replaces the population dataset with the vocabulary, computing per-token scores that aggregate to sequence-level predictions. The method uses reference models to estimate base probabilities and eliminates the hyperparameter γ required by RMIA, making it less sensitive to population dataset size while maintaining or improving attack performance across diverse datasets.

## Key Results
- InfoRMIA achieves higher AUC and TPR@0.1%FPR than RMIA across all tested datasets (Purchase-100, CIFAR-10, AG News) with significantly less dependence on large population datasets
- Token-level InfoRMIA reveals that sequence-level membership inference can overestimate privacy risks by averaging over non-private tokens, with token-level scores showing weak correlation to sequence-level scores
- On pretrained LLMs (Pythia), token-level InfoRMIA outperforms existing reference-based methods while eliminating the need for separate population datasets
- InfoRMIA maintains strong performance even with population sizes as small as |Z|=100, compared to RMIA's requirement for |Z|=10000

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InfoRMIA achieves higher AUC and TPR@low-FPR than RMIA by replacing a discrete counting-based test statistic with a continuous, information-theoretic one
- Mechanism: The original RMIA test statistic counts the proportion of population samples z where the likelihood ratio exceeds a threshold γ, producing a discrete score with granularity bounded by |Z|. InfoRMIA reformulates this as the expected log-likelihood ratio E_z[log(p(θ|x)/p(θ|z))], which decomposes into log(p(x|θ)/p(x)) + D_KL(p(z)||p(z|θ)). This continuous formulation eliminates the γ hyperparameter and decouples score precision from population dataset size.
- Core assumption: The composite null hypothesis is well-approximated by expectation over the population distribution; Bayes Factor provides a principled solution to this composite hypothesis testing problem
- Evidence anchors: [abstract]: "InfoRMIA improves upon RMIA by using a continuous test statistic derived from a principled composite hypothesis testing formulation, eliminating the need for the hyperparameter γ and reducing sensitivity to population dataset size."

### Mechanism 2
- Claim: Token-level MIA framework provides more accurate privacy risk assessment by localizing information leakage to individual tokens, avoiding the dilution effect of sequence-level averaging
- Mechanism: LLMs optimize losses at each token step. Sequence-level MIAs aggregate these into a single score ("lossy compression"). Private information is usually concentrated in few tokens. Averaging over non-private tokens dilutes the signal, causing overestimation for sequences with memorized non-private tokens and underestimation for sequences with isolated private tokens.
- Core assumption: Private information in a sentence is usually contained in a few words/tokens; average memorization over entire sequences is semantically less meaningful for privacy assessment than analyzing individual tokens
- Evidence anchors: [abstract]: "token-level framework... reveals that sequence-level membership inference can overestimate privacy risks by averaging over non-private tokens."

### Mechanism 3
- Claim: Token-level InfoRMIA enables efficient and practical MIA on LLMs by eliminating the need for a separate, large population dataset Z
- Mechanism: Standard RMIA requires population data Z from the training distribution, often unavailable for LLMs. Token-level InfoRMIA replaces this with vocabulary V, treating all tokens z ∈ V other than ground truth as population, computing probabilities directly from softmax outputs. This data-dependent Z=V requires no external dataset curation.
- Core assumption: Using vocabulary V as token-level population is a valid proxy for "other possible tokens from the data distribution"; normalization Σp(z)=1 and Σp(z|θ)=1 holds
- Evidence anchors: [Section 4.2]: "we no longer curate a separate population dataset Z. Instead, we treat all possible tokens in the vocabulary other than the ground-truth x as z... removes the high cost of curating and computing on an independent population dataset."

## Foundational Learning

**Composite Hypothesis Testing & Bayes Factor**
- Why needed here: The paper frames MIA as composite hypothesis testing (H0: model trained on any z in Z). Understanding this framework is essential to see why RMIA's multiple pairwise tests are suboptimal and why InfoRMIA's expected log-likelihood ratio (akin to Bayes Factor) is principled
- Quick check question: Why does a composite hypothesis ("θ is trained on some data from Z") require different statistical approach than a simple hypothesis ("θ is trained on specific data x")?

**KL Divergence**
- Why needed here: The InfoRMIA test statistic decomposes into log-likelihood ratio plus KL divergence term. Understanding KL divergence as a measure of distributional difference is key to interpreting the second term as capturing generalization-related signal
- Quick check question: In log(p(x|θ)/p(x)) + D_KL(p(z)||p(z|θ)), what does the D_KL term measure and why might it relate to model generalization?

**MIA Evaluation Metrics (AUC vs. TPR@Low-FPR)**
- Why needed here: The paper emphasizes TPR@0.1%FPR over AUC, arguing it better reflects privacy risk. This is crucial for interpreting experimental results and understanding benchmark limitations
- Quick check question: Why might a high AUC still correspond to weak privacy attack if TPR at very low FPR (e.g., 0.1%) is near zero?

## Architecture Onboarding

**Component map**: Target LLM (θ) -> Reference Model (θ_ref) -> Population Data/Vocabulary (Z) -> InfoRMIA Score Calculator

**Critical path**:
1. Load Models: Load target LLM checkpoint and reference model checkpoint(s) (e.g., Pythia-160M step-1)
2. Prepare Queries: For standard InfoRMIA, prepare target sequences x and population Z. For token-level, prepare sequences x and use V as Z
3. Compute Probabilities: Get p(x|θ), p(z|θ) from target model; p(x), p(z) from reference model(s)
4. Calculate Test Statistic: Standard: Compute log(p(x|θ)/p(x)) + D_KL(p(z)||p(z|θ)) using normalized probabilities. Token-Level: For each token x_i, compute score using V as Z via Eq. 9
5. Aggregate (Token-Level): If sequence-level scores needed, aggregate token scores (e.g., averaging)

**Design tradeoffs**:
- Token-Level vs. Sequence-Level: Token-level offers finer granularity and better privacy localization but requires aggregation for sequence-level benchmarks. Sequence-level is current standard but misleading (dilution effect)
- Population Set Size vs. Cost: Standard InfoRMIA reduces sensitivity to |Z| compared to RMIA, but still needs some population data. Token-level removes this dependency for LLMs, at cost of per-token computation
- Reference Model Quality: Early checkpoint (e.g., step-1) is practical but may be less ideal than model trained on disjoint data. Paper shows this works well

**Failure signatures**:
- Very Low TPR@Low-FPR: Indicates weak attack power, possibly from poor reference models or very low memorization in target model
- High AUC, Low TPR@Low-FPR: Attack good at ranking but poor at high-confidence predictions. Common for many MIAs on large LLMs
- Token-Level Scores Uniformly Low/High: May indicate probability normalization issues or vocabulary usage problems (e.g., model always assigns near-1 probability to ground truth)

**First 3 experiments**:
1. Reproduce AUC/TPR comparison: Run both RMIA (from ML Privacy Meter) and InfoRMIA on CIFAR-10 or Purchase-100 with same reference models. Verify InfoRMIA achieves higher TPR@0.1%FPR, especially with smaller population sizes (|Z|=100 vs |Z|=10000)
2. Implement Token-Level InfoRMIA: Implement token-level score calculation (Eq. 9) for pre-trained LLM (e.g., Pythia-160M). Use its step-1 checkpoint as reference. Compute per-token scores for example sequences from member set (e.g., Pile/Wikipedia split in MIMIR)
3. Visualize Leakage: Create heatmap visualization (as in Figure 1b/2/7/8) for handful of sequences. Identify sequences with high sequence-level scores but low private-token scores and vice-versa. This validates core claim of sequence-level dilution

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can token-level membership scores effectively guide targeted machine unlearning or data reconstruction?
- Basis in paper: [explicit] The authors conclude by stating they "leave the systematic exploration of these applications [targeted unlearning and token-guided reconstruction] to future work"
- Why unresolved: The paper establishes the inference method and assessment interface but does not implement or test the downstream mitigation algorithms
- What evidence would resolve it: Experiments showing successful surgical unlearning of identified private tokens without degrading utility on non-private tokens, compared to sequence-level unlearning

**Open Question 2**
- Question: Can tailored aggregation methods outperform the generic averaging and min-k heuristics used for sequence-level inference?
- Basis in paper: [explicit] Page 7 states "A stronger aggregation should depend on the model or underlying data distribution," but the authors only evaluate generic methods for practicality
- Why unresolved: Optimized aggregation requires additional holdout data and computation, which the authors avoided to ensure accessibility
- What evidence would resolve it: Benchmarks showing that a learned or distribution-specific aggregation function yields higher TPR@LowFPR than the generic baselines reported in the paper

**Open Question 3**
- Question: Does the token-level localization assumption fail for semantic privacy where sensitive information is distributed across tokens rather than localized in specific PII?
- Basis in paper: [inferred] The paper posits on Page 2 that private information is "usually contained in a few words/tokens," focusing on PII (e.g., names) rather than inferable attributes
- Why unresolved: The evaluation relies on datasets with explicit privacy masks (ai4privacy) or entity labels, potentially missing subtle, distributed memorization
- What evidence would resolve it: Evaluation on semantic privacy datasets to determine if the "dilution" hypothesis still holds when privacy is a function of token combinations rather than individual tokens

## Limitations

- Population dataset dependence uncertainty: While InfoRMIA reduces sensitivity to population dataset size compared to RMIA, the paper does not fully explore the lower bound of population size where the KL divergence term becomes unreliable
- Vocabulary-as-population assumption validity: The token-level InfoRMIA's replacement of population dataset Z with vocabulary V is innovative but rests on the assumption that V represents the true training token distribution, which may not hold for domain-specific or low-resource languages
- Reference model quality dependency: Both standard and token-level InfoRMIA depend on reference models to estimate base probabilities, and the sensitivity to reference model quality and potential for reference model leakage is not systematically studied

## Confidence

**High confidence**: The mechanism explaining how continuous test statistics outperform discrete counting-based approaches (Mechanism 1) - this is well-supported by the mathematical formulation and the elimination of the γ hyperparameter is clearly demonstrated

**Medium confidence**: The token-level framework's ability to avoid sequence-level dilution effects (Mechanism 2) - while the paper provides compelling visualizations and examples, the extent to which this generalizes across different types of private information and different LLM architectures requires broader validation

**Medium confidence**: The claim that token-level InfoRMIA eliminates the need for population datasets (Mechanism 3) - this is practically demonstrated for LLMs using vocabulary as proxy, but the theoretical justification for when vocabulary serves as a valid population distribution is not fully explored

## Next Checks

**Check 1**: Systematic population size ablation study - Implement InfoRMIA and RMIA on CIFAR-10 with controlled population sizes ranging from |Z|=10 to |Z|=10000, measuring TPR@0.1%FPR and AUC at each level to validate the claim that InfoRMIA maintains performance with significantly smaller populations

**Check 2**: Vocabulary coverage impact analysis - For token-level InfoRMIA, analyze how vocabulary completeness affects privacy assessment by creating controlled experiments where certain token types are removed from vocabulary (e.g., removing digits, removing common function words) and measuring how these changes affect token-level scores and sequence-level aggregation

**Check 3**: Reference model quality sensitivity - Design experiments comparing InfoRMIA performance using different reference model strategies: (a) early checkpoint (step-1), (b) model trained on disjoint dataset, (c) ensemble of multiple disjoint models, measuring false positive rates and true positive rates to quantify how reference model quality affects privacy assessment accuracy