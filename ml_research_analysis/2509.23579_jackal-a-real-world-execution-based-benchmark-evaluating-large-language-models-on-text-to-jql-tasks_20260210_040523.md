---
ver: rpa2
title: 'Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models
  on Text-to-JQL Tasks'
arxiv_id: '2509.23579'
source_url: https://arxiv.org/abs/2509.23579
tags:
- language
- natural
- semantically
- execution
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jackal is the first open, real-world, execution-based benchmark
  for mapping natural language to Jira Query Language (JQL). It addresses the lack
  of evaluation resources for text-to-JQL by providing 100,000 validated natural language-JQL
  pairs and execution results on a live Jira instance with over 200,000 issues.
---

# Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks

## Quick Facts
- **arXiv ID**: 2509.23579
- **Source URL**: https://arxiv.org/abs/2509.23579
- **Reference count**: 37
- **Primary result**: First open, execution-based benchmark for mapping natural language to Jira Query Language (JQL), showing best model (Gemini 2.5 Pro) achieves 60.3% execution accuracy overall

## Executive Summary
Jackal introduces the first open, real-world, execution-based benchmark for evaluating Large Language Models on text-to-JQL tasks. The benchmark addresses the critical gap in evaluation resources for semantic parsing in enterprise software domains by providing 100,000 validated natural language-JQL pairs with execution results on a live Jira instance containing over 200,000 issues. Each query is paired with four user query variants to capture real-world usage patterns, and the benchmark includes both an execution-based scoring toolkit and a static snapshot for reproducibility. Evaluation on Jackal-5K with 23 models reveals significant performance variations across query types and confirms the necessity of execution-based evaluation over exact match metrics.

## Method Summary
Jackal constructs a benchmark of 100,000 natural language queries paired with valid JQL expressions, each associated with execution results from a live Jira instance with 200,000+ issues. The queries are stratified into four types: Long NL (detailed descriptions), Short NL (concise queries), Semantically Similar (paraphrased variants), and Semantically Exact (direct translations). The benchmark provides both an execution-based scoring toolkit that runs queries on the live Jira instance and a static snapshot with pre-computed execution results for reproducibility. A subset of 5,000 pairs (Jackal-5K) was used for model evaluation, testing 23 different models ranging from open-weight models to proprietary systems.

## Key Results
- Best model (Gemini 2.5 Pro) achieves 60.3% execution accuracy overall on Jackal-5K
- Performance varies dramatically by query type: 86.0% for Long NL, 35.7% for Short NL, 22.7% for Semantically Similar, and 99.3% for Semantically Exact
- Exact match and canonical exact match metrics remain near zero, confirming execution-based evaluation is necessary
- Smaller open-weight models (Mistral 7B, Llama 3 8B) are dominated by execution errors due to invalid syntax

## Why This Works (Mechanism)
The benchmark works by providing a realistic, domain-specific evaluation environment that captures the complexity of real-world semantic parsing tasks. By using execution-based scoring rather than syntactic matching, it reveals whether generated queries actually retrieve the intended issues rather than just producing valid syntax. The inclusion of query variants captures the natural variation in how users express similar intents, exposing model robustness to paraphrasing and under-specification.

## Foundational Learning
- **Jira Query Language (JQL)**: Domain-specific language for filtering and searching issues in Jira; needed to understand the target language for semantic parsing
- **Execution-based evaluation**: Scoring method that runs generated queries and compares actual results to expected results; needed because syntactic correctness doesn't guarantee semantic correctness
- **Query stratification**: Categorization of queries by user expression patterns (Long NL, Short NL, Semantically Similar, Semantically Exact); needed to analyze model robustness to different types of natural language variation
- **Semantic parsing**: Task of converting natural language to formal queries; needed to understand the core AI challenge being evaluated
- **Zero-shot evaluation**: Testing models without fine-tuning on the benchmark data; needed to establish baseline performance and fairness across models

## Architecture Onboarding

**Component Map**: User Query Variants -> JQL Generator -> Execution Engine -> Result Comparator -> Score

**Critical Path**: Natural language input → Model generates JQL → JQL executes on Jira instance → Results compared to ground truth → Execution accuracy score

**Design Tradeoffs**: Execution-based scoring provides realistic evaluation but requires live Jira instance access; static snapshots enable reproducibility but may become stale; query variants increase benchmark richness but add complexity to evaluation

**Failure Signatures**: 
- Execution Errors: Invalid JQL syntax preventing execution
- Empty Results: Valid syntax but returns no issues
- Different Results: Valid syntax returns incorrect issues
- Exact Match Failures: Syntactically correct but semantically wrong

**3 First Experiments**:
1. Run the same 5,000 test queries through multiple models to establish baseline performance
2. Evaluate each query type stratification separately to identify failure modes
3. Compare execution-based scores with exact match metrics to quantify the gap

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can grammar-constrained decoding effectively mitigate the high rate of execution errors observed in smaller open-weight models (e.g., Mistral 7B, Llama 3 8B) without degrading the semantic correctness of the generated queries?
- Basis in paper: Page 3 suggests "grammar-constrained decoding" as a future direction. Page 9 notes that smaller open-weight models are "dominated by execution errors" (invalid syntax), whereas proprietary models fail semantically.
- Why unresolved: While constrained decoding guarantees valid syntax, it is unclear if it would simply shift the failure mode to "Empty Results" (valid syntax but incorrect logic) for these weaker models.
- What evidence would resolve it: An evaluation of open-weight models on Jackal using a JQL-constrained decoder (similar to PICARD), measuring the shift in error distribution from Execution Errors to Empty/Different Results.

### Open Question 2
- Question: What specific architectural or prompting mechanisms are required to close the massive performance gap between "Semantically Exact" (99.3% accuracy) and "Semantically Similar" (22.7% accuracy) inputs?
- Basis in paper: Page 9 highlights that "robustness to natural variation remains unresolved." The paper shows models rely heavily on lexical cues; accuracy collapses when field names are paraphrased (Semantically Similar) rather than mirrored.
- Why unresolved: The paper evaluates current SOTA LLMs but does not test specific interventions (like schema-aware retrieval or intermediate reasoning steps) to solve the paraphrase resolution problem.
- What evidence would resolve it: An ablation study testing Retrieval-Augmented Generation (RAG) or chain-of-thought prompting specifically on the "Semantically Similar" subset to see if external context bridges the lexical gap.

### Open Question 3
- Question: Does fine-tuning on the full 100,000-pair Jackal dataset improve robustness on under-specified "Short NL" queries, or does the ambiguity inherent in short queries impose a ceiling on performance?
- Basis in paper: Page 7 states the full 100k dataset supports "large-scale training," but results are reported zero-shot. Page 9 identifies "Short NL" (35.7% accuracy) as a major failure mode due to under-specification.
- Why unresolved: It is undetermined if the poor performance on Short NL is a lack of domain-specific priors (which training fixes) or a fundamental ambiguity problem where the model cannot infer the user's intent without more context.
- What evidence would resolve it: A comparison of zero-shot vs. fine-tuned performance on the "Short NL" stratification, analyzing if errors shift from "Execution Errors" (misunderstanding syntax) to "Different Results" (misunderstanding intent).

## Limitations
- Single domain focus (Jira/JQL) may limit generalizability to other semantic parsing tasks
- Reliance on specific Jira instance configuration may not represent all organizational setups
- Execution-based approach introduces dependencies on live Jira environment affecting reproducibility
- Massive performance gap between semantically exact and semantically similar queries raises questions about benchmark coverage

## Confidence
- **High confidence**: Technical implementation and execution methodology are sound
- **Medium confidence**: Generalization of findings to other semantic parsing domains
- **Medium confidence**: Absolute performance numbers due to potential environmental dependencies

## Next Checks
1. Test model performance on a second, independently configured Jira instance to verify environmental dependency claims
2. Evaluate the same models on a comparable text-to-SQL benchmark to assess cross-domain performance patterns
3. Conduct ablation studies removing different query types (Long NL, Short NL, etc.) to isolate their individual impact on overall performance metrics