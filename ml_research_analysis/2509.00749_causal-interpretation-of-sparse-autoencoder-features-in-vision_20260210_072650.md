---
ver: rpa2
title: Causal Interpretation of Sparse Autoencoder Features in Vision
arxiv_id: '2509.00749'
source_url: https://arxiv.org/abs/2509.00749
tags:
- features
- feature
- layer
- activation
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Causal Feature Explanation (CaFE) addresses the problem that sparse
  autoencoder (SAE) features in vision transformers are often misinterpreted when
  relying on activation locations, as self-attention mixes information across the
  image and activated patches may co-occur with but not cause feature firing. CaFE
  leverages Effective Receptive Field (ERF) by applying input attribution methods
  to identify the image patches that causally drive SAE feature activations, rather
  than just where activations are highest.
---

# Causal Interpretation of Sparse Autoencoder Features in Vision

## Quick Facts
- arXiv ID: 2509.00749
- Source URL: https://arxiv.org/abs/2509.00749
- Reference count: 12
- Sparse autoencoder features in vision transformers are often misinterpreted when relying on activation locations due to self-attention mixing information across the image

## Executive Summary
Causal Feature Explanation (CaFE) addresses a fundamental misinterpretation problem in vision transformer sparse autoencoders (SAEs), where features are often incorrectly associated with image patches where activations are strongest. Since self-attention mechanisms mix information across the entire image, activated patches may simply co-occur with but not cause feature firing. CaFE leverages Effective Receptive Field (ERF) analysis by applying input attribution methods to identify which image patches causally drive SAE feature activations, rather than just where activations are highest.

Through patch insertion tests, CaFE with AttnLRP demonstrated significantly higher area under the insertion curve (AUC) for recovering feature activations compared to baseline activation-based methods. The analysis revealed that non-local SAE features, whose activations scatter across images, increase in frequency in higher layers (up to ~14% at layer 22) and encode complex, compositional concepts. CaFE provides more faithful and semantically precise explanations of vision-SAE features, highlighting the risk of misinterpretation when relying solely on activation location.

## Method Summary
CaFE leverages Effective Receptive Field (ERF) analysis to identify which image patches causally drive SAE feature activations. The method applies input attribution techniques (like AttnLRP) to determine the ERF of activated features, then uses patch insertion tests to validate whether identified patches actually contribute to feature activation. This approach addresses the problem that self-attention mixes information across images, making activation location unreliable for interpretation. By focusing on causal drivers rather than co-occurring activations, CaFE provides more accurate semantic explanations of vision-SAE features, particularly for non-local features whose activations scatter across images in higher layers.

## Key Results
- CaFE with AttnLRP achieved higher area under the insertion curve (AUC) for recovering feature activations compared to baseline activation-based methods
- Non-local SAE features, whose activations scatter across images, increase in frequency in higher layers (up to ~14% at layer 22)
- CaFE yields more faithful and semantically precise explanations of vision-SAE features compared to methods relying solely on activation location

## Why This Works (Mechanism)
CaFE works by addressing the fundamental mismatch between activation location and causal drivers in vision transformers. Self-attention mechanisms mix information across the entire image, so patches with high activations may simply co-occur with but not cause feature firing. By using input attribution methods to identify the Effective Receptive Field, CaFE traces back to the actual image patches that causally contribute to feature activation. The patch insertion validation confirms that identified patches genuinely drive activation rather than just correlating with it, providing more accurate semantic interpretations of SAE features.

## Foundational Learning
- **Effective Receptive Field (ERF)**: The region of the input image that actually influences a neural network's output. Needed because self-attention mixes information globally, making local activation misleading. Quick check: Compare attribution maps from different methods to see which patches truly drive activation.
- **Input Attribution Methods**: Techniques like LRP that assign importance scores to input features. Needed to trace back from activations to their causal drivers. Quick check: Verify attribution consistency across similar inputs.
- **Patch Insertion Tests**: A validation method where patches are systematically inserted to measure their causal impact on feature activation. Needed to confirm that identified patches actually cause rather than just correlate with activation. Quick check: Measure AUC improvement when using causal vs. correlational patches.
- **Sparse Autoencoder (SAE) Features**: Learned representations in vision transformers that activate sparsely across the feature space. Needed as the target of interpretation, particularly challenging due to attention mixing. Quick check: Analyze activation sparsity patterns across layers.
- **Vision Transformer Architecture**: Transformer models applied to vision tasks using patch-based tokenization. Needed context for understanding self-attention's global mixing effect. Quick check: Examine attention maps to see information flow patterns.
- **Non-local Features**: SAE features whose activations scatter across images rather than concentrating locally. Needed to understand the prevalence and nature of complex, compositional concepts in higher layers. Quick check: Measure spatial dispersion of activations across layers.

## Architecture Onboarding

**Component Map:**
Input Image -> Vision Transformer -> Sparse Autoencoder -> Feature Activations -> Input Attribution (AttnLRP) -> ERF Identification -> Patch Insertion Tests -> CaFE Interpretation

**Critical Path:**
Vision Transformer layers -> SAE feature extraction -> Input attribution computation -> ERF-based patch selection -> Insertion test validation

**Design Tradeoffs:**
CaFE trades computational complexity (running attribution methods and insertion tests) for interpretation accuracy. The method prioritizes causal understanding over simple activation mapping, accepting the cost of more sophisticated analysis to avoid misinterpretation of non-local features that become more prevalent in higher layers.

**Failure Signatures:**
- Attribution methods failing to capture true causal pathways, leading to incorrect ERF estimation
- Patch insertion tests showing no AUC improvement, suggesting attribution method inadequacy
- Over-reliance on local activation patterns missing non-local features that encode compositional concepts
- High frequency of non-local features (>14%) indicating widespread misinterpretation risk

**3 First Experiments:**
1. Compare CaFE with AttnLRP against activation-only baselines on a held-out test set using patch insertion AUC
2. Analyze the spatial distribution of identified ERF patches vs. activation peaks across different layers
3. Measure the frequency and semantic content of non-local features in layers 10, 15, 20, and 22

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- ERF estimation depends on the validity of input attribution methods, which may imperfectly capture true causal pathways, especially for features with diffuse receptive fields
- Results are based on a single SAE architecture (Vision Transformers) trained on general image datasets, limiting generalizability
- The frequency of non-local features (~14% at layer 22) and their semantic properties may vary across different model architectures and training regimes

## Confidence
High: CaFE method design, patch insertion validation approach, and quantitative AUC comparisons
Medium: Generalization across different SAE architectures and datasets beyond the studied vision transformers
Low: Long-term implications for SAE interpretability and the relationship between non-local feature frequency and model performance

## Next Checks
1. Validate CaFE on alternative vision architectures (e.g., ConvNets, different transformer variants) to assess generalizability
2. Conduct ablation studies testing different input attribution methods (not just AttnLRP) to identify the most reliable for ERF estimation
3. Analyze the relationship between non-local feature prevalence and downstream task performance to understand practical implications of misinterpretation risk