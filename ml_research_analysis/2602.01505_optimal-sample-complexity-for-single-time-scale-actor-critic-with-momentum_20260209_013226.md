---
ver: rpa2
title: Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum
arxiv_id: '2602.01505'
source_url: https://arxiv.org/abs/2602.01505
tags:
- sample
- critic
- complexity
- policy
- actor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes an optimal sample complexity of $O(\epsilon^{-2})$
  for achieving an $\epsilon$-close globally optimal policy using a single-timescale
  actor-critic algorithm, improving upon the prior best of $O(\epsilon^{-3})$. This
  is achieved by integrating STORM momentum with a replay buffer for variance reduction
  in critic updates.
---

# Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum

## Quick Facts
- arXiv ID: 2602.01505
- Source URL: https://arxiv.org/abs/2602.01505
- Reference count: 40
- One-line primary result: Achieves optimal $O(\epsilon^{-2})$ sample complexity for globally optimal policy in single-timescale actor-critic with STORM momentum.

## Executive Summary
This paper establishes the first optimal $O(\epsilon^{-2})$ sample complexity for achieving an $\epsilon$-close globally optimal policy using a single-timescale actor-critic algorithm. The key innovation is integrating STORM momentum with a replay buffer to reduce variance in critic updates while handling the nonstationary occupancy measure induced by the evolving policy. The analysis introduces a Lyapunov function approach that enables faster step-size decay ($O(k^{-1/2})$) than previous methods, making the algorithm more practical for deep learning architectures with only minor modifications.

## Method Summary
The method combines standard single-timescale actor-critic with STORM momentum and a replay buffer. The actor updates follow standard gradient ascent with step-size $\eta_k \propto k^{-1/2}$. The critic uses STORM momentum with step-size $\nu_k \propto k^{-1}$, storing previous Q-values to compute a correction term. A replay buffer maintains a fraction of recent samples to mitigate bias from the nonstationary policy distribution. The analysis uses a Lyapunov function $x_k = \eta_k a_k + \eta_k z_k^2 + w_k^2$ and proves convergence via an ODE domination lemma.

## Key Results
- Establishes optimal $O(\epsilon^{-2})$ sample complexity for global policy optimization
- Improves upon prior best $O(\epsilon^{-3})$ by integrating STORM with replay buffer
- Enables faster step-size decay ($O(k^{-1/2})$) compared to previous methods ($O(k^{-2/3})$)
- Maintains compatibility with deep learning architectures requiring only minor modifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** STORM momentum reduces variance in critic updates, enabling faster step-size decay.
- **Mechanism:** STORM adds recursive correction term ($\nabla f(x_k, \xi_k) - \nabla f(x_{k-1}, \xi_k)$) to reduce variance to $O(\sigma^{2/3}/T^{2/3})$.
- **Core assumption:** Smooth loss function with PL condition allowing $O(T^{-1/2})$ convergence rate.
- **Evidence anchors:** Abstract states STORM reduces variance; section 2.1 details STORM's variance control; related corpus supports variance reduction as key lever.
- **Break condition:** Mismatched step-size decay (critic $\nu_k \propto k^{-1}$ vs actor $\eta_k \propto k^{-1/2}$) breaks recursion analysis.

### Mechanism 2
- **Claim:** Replay buffer mitigates bias from evolving policy's occupancy measure.
- **Mechanism:** Uniform sampling from buffer $B_k$ averages distribution shift, ensuring sufficient exploration assumption holds.
- **Core assumption:** Buffer distribution $b_k$ provides sufficient state-action coverage.
- **Evidence anchors:** Abstract notes buffer mitigates nonstationary occupancy; section 3 describes buffer maintenance; related corpus lacks specific buffer implementation details.
- **Break condition:** Insufficient buffer size or coverage causes unbounded $c_B$ term in STORM recursion.

### Mechanism 3
- **Claim:** Specific Lyapunov function handles heterogeneous recursion from mismatched step-sizes.
- **Mechanism:** Lyapunov $x_k = \eta_k a_k + \eta_k z_k^2 + w_k^2$ balances sub-optimality, critic error, and momentum error weighted by step-sizes.
- **Core assumption:** Initial step size $\eta_0$ sufficiently small for stability.
- **Evidence anchors:** Section 4.2 identifies non-trivial Lyapunov term; section 4.1 describes coupled recursion complexity; related corpus lacks specific Lyapunov construction.
- **Break condition:** Actor/critic step-sizes decaying slower than $O(k^{-1/2})$ causes noise dominance.

## Foundational Learning

- **Concept: Occupancy Measure (Discounted Stationary Distribution)**
  - **Why needed here:** Sampling distribution $d^{\pi_k}$ changes every iteration, fundamental bottleneck for STORM without buffer.
  - **Quick check question:** Does the algorithm sample from a fixed dataset or a distribution that changes with the policy?

- **Concept: STORM (STOchastic Recursive Momentum)**
  - **Why needed here:** Not standard momentum; requires gradient at current point and previous point using current sample.
  - **Quick check question:** Why do we need to evaluate previous model parameters on the current sample?

- **Concept: Sample Complexity**
  - **Why needed here:** Paper claims optimal $O(\epsilon^{-2})$; must understand this measures total samples for $\epsilon$-optimality.
  - **Quick check question:** Does $O(\epsilon^{-2})$ refer to finding a stationary point or globally optimal policy?

## Architecture Onboarding

- **Component map:** Actor Network -> Replay Buffer -> STORM Wrapper -> Critic Network
- **Critical path:**
  1. Sample current transition for Actor
  2. Actor Update: Update $\theta$ using standard step size $\eta_k$
  3. Buffer Sample: Draw random transition from Buffer for Critic
  4. STORM Step: Compute current/previous TD errors on buffer sample, compute momentum $h_k$
  5. Critic Update: Update $\phi$ using $h_k$
  6. Sync: Update $\phi_{prev} \leftarrow \phi$

- **Design tradeoffs:**
  - Memory vs. Stability: STORM requires 2x Q-network memory; buffer requires linear memory (constant size often sufficient empirically)
  - Speed vs. Complexity: Standard AC simpler but requires $O(k^{-2/3})$ step-sizes; this method allows $O(k^{-1/2})$ with added complexity

- **Failure signatures:**
  - Divergence: Omitting buffer causes high variance from distribution shift bias
  - Stalling: Incorrect step-size tuning prevents theoretical guarantees

- **First 3 experiments:**
  1. Tabular Validation: Compare algorithm vs. standard single-timescale AC on simple MDP (S=10, A=5)
  2. Buffer Ablation: Test performance with Buffer vs. No Buffer to isolate occupancy measure bias correction
  3. Step Size Scaling: Verify $O(k^{-1/2})$ converges faster than $O(k^{-2/3})$ while maintaining stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical requirement for linearly growing replay buffer be relaxed to fixed-size buffer without compromising $O(\epsilon^{-2})$ sample complexity?
- Basis in paper: [explicit] Authors state "We leave this interesting memory versus sample complexity trade-off for future work"
- Why unresolved: Current analysis relies on buffer size proportional to $c_b k$ for uniform sampling guarantees
- What evidence would resolve it: Convergence proof with buffer size independent of $k$, or empirical study verifying necessity of linear growth

### Open Question 2
- Question: Can global convergence be guaranteed without Sufficient Exploration Assumption (SEA) or Gradient Domination Lemma (GDL)?
- Basis in paper: [explicit] Paper lists "relaxation of SEA and GDL" as "possible future direction"
- Why unresolved: These assumptions require full state space exploration, restrictive in practical large-scale environments
- What evidence would resolve it: Novel algorithm design (e.g., tree search) guaranteeing global optimality with partial state coverage

### Open Question 3
- Question: Do sampling algorithms exist yielding smaller bias constant ($c_B$) than standard buffer approach?
- Basis in paper: [explicit] Authors note "This suggests future work on sampling algorithms with smaller $c_B$ to reduce bias and improve performance"
- Why unresolved: Current buffer approach may not achieve tightest possible $c_B$ bound
- What evidence would resolve it: New sampling mechanism with theoretical guarantee of strictly smaller $c_B$

### Open Question 4
- Question: Does optimal sample complexity hold for continuous state-action spaces with function approximation?
- Basis in paper: [inferred] Extension to continuous spaces described as "possible... requiring minor modification"
- Why unresolved: Function approximation introduces approximation errors and policy smoothness constraints
- What evidence would resolve it: Theoretical extension proving $O(\epsilon^{-2})$ convergence for continuous MDPs with function approximator error bounds

## Limitations
- Analysis restricted to tabular MDPs; scalability to continuous spaces requires additional assumptions
- Replay buffer size grows linearly with time theoretically, though constant size often sufficient empirically
- Algorithm requires careful tuning of multiple step-size schedules for theoretical guarantees

## Confidence
- **High confidence** in theoretical $O(\epsilon^{-2})$ convergence rate under stated assumptions
- **Medium confidence** in practical applicability due to step-size and buffer tuning requirements
- **Low confidence** in scalability to large state-action spaces given tabular analysis

## Next Checks
1. Buffer ablation study: Compare performance with and without replay buffer across multiple MDPs
2. Step-size sensitivity analysis: Systematically vary $\eta_k, \beta_k, \nu_k$ decay rates to assess robustness
3. Scalability test: Evaluate on continuous control benchmark to assess extension beyond tabular settings