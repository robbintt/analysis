---
ver: rpa2
title: Benignity of loss landscape with weight decay requires both large overparametrization
  and initialization
arxiv_id: '2505.22578'
source_url: https://arxiv.org/abs/2505.22578
tags:
- loss
- global
- activation
- equation
- minimum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates the loss landscape of \u21132-regularized\
  \ training loss for two-layer ReLU networks, focusing on the existence of spurious\
  \ local minima under weight decay. The authors show that for the loss landscape\
  \ to become benign\u2014i.e., mostly free of spurious local minima\u2014a large\
  \ overparameterization is required, specifically when the network width m \u2273\
  \ min(nd, 2n), where n is the number of data points and d is the input dimension."
---

# Benignity of loss landscape with weight decay requires both large overparametrization and initialization

## Quick Facts
- arXiv ID: 2505.22578
- Source URL: https://arxiv.org/abs/2505.22578
- Reference count: 40
- Paper investigates loss landscape of ℓ2-regularized training loss for two-layer ReLU networks, showing large overparameterization is required for benignity.

## Executive Summary
This paper analyzes the loss landscape of ℓ2-regularized two-layer ReLU networks, focusing on when spurious local minima disappear. The authors prove that for the landscape to become benign (mostly free of spurious local minima), a network width of m ≳ min(nd, 2n) is required, where n is the number of data points and d is input dimension. This is a stronger requirement than in the unregularized case. The paper also shows this level of overparameterization is necessary for orthogonal data, and that landscape results are primarily relevant in the large initialization regime.

## Method Summary
The paper studies two-layer ReLU networks trained with ℓ2 regularization (weight decay) using convex reformulation within activation cones. Experiments involve generating random Gaussian data with labels from teacher networks, then training with varying width m and initialization scale α. The convex solver Clarabel via CVXPY analyzes activation cone structure. Theorem 2 experiments use specific unit vector centers with added Gaussian noise. Training uses SGD with learning rate 0.01 until convergence or 100M epochs.

## Key Results
- Large overparameterization (m ≳ min(nd, 2n)) required for loss landscape to be mostly free of spurious local minima
- This threshold is necessary for orthogonal data, not just sufficient
- Landscape results primarily hold relevance in large initialization regime (NTK)
- Small initialization can lead to convergence to spurious local minima despite global landscape benignity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large overparameterization ensures most activation cones contain global minima with no spurious local minima
- Mechanism: Parameter space partitions into activation cones; each cone contains a global minimum iff it has neurons matching optimal activation patterns, reducing to coupon collector problem
- Core assumption: Optimal network has at most n+1 non-zero neurons (Wang et al., 2022)
- Evidence: Theorem 1 proof maps to coupon collector with O(min(nd, 2n)) coupons
- Break condition: For m ≪ min(nd, 2n), fraction of cones with global minima decays exponentially

### Mechanism 2
- Claim: Large initialization keeps optimization in regime where landscape benignity translates to convergence guarantees
- Mechanism: Large initialization keeps inner weights near-random, so dynamics encounter activation cone with high probability of containing global minimum
- Core assumption: Assumption 2 on balanced initialization and deactivated neurons staying deactivated
- Evidence: Theorem 2 proves convergence to bad local minima for small α regardless of overparameterization
- Break condition: When initialization scale α → 0, early alignment phase dominates

### Mechanism 3
- Claim: Overparameterization threshold is necessary for orthogonal data
- Mechanism: For orthogonal inputs, each neuron's activation pattern is constant; global minimum requires specific patterns with probability ≤ m·2^(-n/2)
- Core assumption: Assumption 3 (orthogonal data) and rotation-invariant initialization
- Evidence: Theorem 3 gives explicit probability bounds on convergence to non-global minima
- Break condition: For non-orthogonal data, threshold may be loose

## Foundational Learning

- **Activation cones (piecewise-linear regions)**: The analysis partitions parameter space by ReLU activation patterns; the network is linear within each cone. Quick check: Given n data points and m neurons, can you sketch why the number of distinct activation patterns is O(min(2^n, n^d))?

- **Coupon collector problem**: The proof of Theorem 1 reduces to collecting n+1 specific activation patterns among O(min(nd, 2n)) possibilities. Quick check: If you need to collect k specific coupons from N total types, how many draws m do you need with high probability?

- **NTK regime vs. feature learning regime**: Landscape results only help in NTK regime (large init); feature learning (small init) has different dynamics. Quick check: What happens to inner layer weights during training in the NTK regime vs. small initialization regime?

## Architecture Onboarding

- **Component map**: Two-layer ReLU network: W ∈ R^(m×d) (inner), a ∈ R^m (output) -> Loss: L_λ = (1/n)Σ(f_θ(x_k) - y_k)² + λ||θ||² -> Activation cones: C_A = {(W,a) : activation pattern matrix = A} -> Convex reformulation: Each cone admits tractable optimization

- **Critical path**: 1. Choose width m based on min(nd, 2n) threshold 2. Initialize with large scale α (for NTK regime) or small α (for feature learning) 3. Train with weight decay λ > 0 4. Convergence depends on regime: large init → likely global min; small init → may hit spurious local min

- **Design tradeoffs**: Larger m improves landscape but increases computation; larger initialization better landscape utilization but weaker feature learning; stronger weight decay better regularization but requires more overparameterization

- **Failure signatures**: Converged network has rank-1 structure when optimal requires rank > 1; training loss near zero but ||θ||² much larger than theoretical minimum; for orthogonal data with m < 2n, gradient flow converges to non-global stationary points with probability → 1 as n grows

- **First 3 experiments**:
  1. Sample random activation cones, count fraction containing global minima vs. m, verify threshold at O(min(nd, 2n))
  2. Train identical architectures with varying α, measure final ||θ||² and distance to global minimum
  3. Generate orthogonal inputs, train with m below/above 2n threshold, measure convergence rate to non-global minima

## Open Questions the Paper Calls Out

- **Can the regularization parameter λ be bounded independently of initialization scale α?**: The paper concludes future work could seek to bound λ independently of α, as Theorem 2 currently requires λ ≤ μ_min α^ε, coupling regularization to initialization scale.

- **How suboptimal are the norms of interpolators obtained with m ≲ min(2n, nd) parameters?**: The paper notes it's unclear how far from "norm minimality" the final estimator would be with fewer parameters than the threshold.

- **What are the generalization properties of spurious local minima reached under small initialization with weight decay?**: The paper leaves open whether these suboptimal stationary points generalize well, despite Theorem 2 showing convergence to them.

## Limitations

- The overparameterization threshold may be loose for non-orthogonal data, with tightness remaining an open question
- The coupling between regularization parameter λ and initialization scale α is theoretically restrictive
- Generalization properties of spurious local minima reached under small initialization remain unexplored

## Confidence

- **High Confidence**: Overparameterization requirement m ≳ min(nd, 2n) for landscape benignity; necessity for orthogonal data; initialization scale effects on convergence
- **Medium Confidence**: Practical relevance of landscape benignity in large initialization regime; tightness of thresholds for non-orthogonal data
- **Low Confidence**: Exact behavior for highly structured/non-random data; robustness to different weight decay schedules

## Next Checks

1. **Parameter Space Coverage**: Systematically vary m across the predicted threshold range (min(nd, 2n)) and measure the proportion of activation cones containing global minima for multiple random data realizations to verify the phase transition is sharp.

2. **Initialization Scale Sweep**: For fixed m ≳ min(nd, 2n), train networks across a wide range of initialization scales α to empirically map the transition between NTK and feature learning regimes, measuring convergence to global vs. spurious local minima.

3. **Generalization to Non-Orthogonal Data**: Test the necessity claim on datasets with varying levels of correlation (from orthogonal to highly correlated) to characterize how the overparameterization threshold degrades as data structure departs from orthogonality.