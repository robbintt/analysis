---
ver: rpa2
title: 'PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile
  Scenarios'
arxiv_id: '2601.22575'
source_url: https://arxiv.org/abs/2601.22575
tags:
- streaming
- video
- phostream
- forward
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoStream, the first mobile-centric streaming
  benchmark for evaluating omnimodal assistants in real-world streaming scenarios.
  Unlike existing benchmarks limited to multiple-choice questions or short clips,
  PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios
  (YouTube Vlog, Phone Tutorial, Phone Record, EgoBlind) with an average duration
  of 13.3 minutes.
---

# PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios

## Quick Facts
- arXiv ID: 2601.22575
- Source URL: https://arxiv.org/abs/2601.22575
- Reference count: 23
- Introduces first mobile-centric streaming benchmark for omnimodal assistants with 5,572 QA pairs from 578 videos

## Executive Summary
PhoStream introduces the first mobile-centric streaming benchmark for evaluating omnimodal assistants in real-world scenarios. The benchmark contains 5,572 open-ended QA pairs from 578 videos across four scenarios (YouTube Vlog, Phone Tutorial, Phone Record, EgoBlind) with an average duration of 13.3 minutes. Unlike existing benchmarks limited to multiple-choice questions or short clips, PhoStream requires temporal reasoning across Backward, Instant, and Forward tasks, creating a comprehensive evaluation of streaming capabilities.

The key finding reveals a severe "Early Response" bias in current models - they struggle to decide when to speak rather than what to say. While proprietary models like Gemini 3 Pro achieve high scores (80+) on Instant and Backward tasks, their Forward scores drop sharply to 16.40 due to premature responses. Open-source models show even worse performance, with Qwen3-Omni answering prematurely in 97.89% of Forward cases. This highlights a fundamental limitation in current MLLMs' ability to exercise temporal patience in streaming scenarios where waiting for future evidence is crucial for correct responses.

## Method Summary
PhoStream employs an automated generative pipeline with human verification to create timestamped QA pairs requiring temporal reasoning. The benchmark covers four mobile-centric scenarios with videos averaging 13.3 minutes in duration. Tasks are categorized into three types: Backward (answering questions about past content), Instant (responding to current content), and Forward (requiring anticipation of future content). The automated pipeline generates QA pairs which are then human-verified, ensuring quality while maintaining scalability. The benchmark design specifically targets the temporal reasoning challenges inherent in streaming scenarios, where models must decide not just what to say but when to say it.

## Key Results
- Proprietary models like Gemini 3 Pro achieve scores above 80 on Instant and Backward tasks
- Forward task scores drop dramatically to 16.40 due to premature responses
- Open-source models like Qwen3-Omni answer prematurely in 97.89% of Forward cases
- Models struggle with temporal patience rather than content understanding

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on temporal reasoning in streaming scenarios, which exposes a critical weakness in current omnimodal models. The automated generative pipeline with human verification creates realistic, temporally complex QA pairs that require models to exercise judgment about response timing. The three-task structure (Backward, Instant, Forward) systematically evaluates different temporal reasoning capabilities, with Forward tasks specifically testing whether models can delay responses until sufficient future context becomes available.

## Foundational Learning
**Temporal reasoning in streaming**: Understanding how to process information over time and make decisions about response timing is crucial for real-world applications like live captioning, tutoring, and personal assistance. Quick check: Evaluate model responses at different timestamps to assess temporal understanding.

**Mobile-centric scenario adaptation**: Models must handle diverse real-world scenarios common in mobile usage, from video consumption to tutorial following. Quick check: Test model performance across different mobile usage patterns and environmental conditions.

**Streaming processing vs batch processing**: Unlike traditional benchmarks that provide complete context, streaming requires continuous processing with limited lookahead capability. Quick check: Compare model performance on streaming vs batch versions of the same tasks.

## Architecture Onboarding

**Component Map**: Video Input -> Temporal Processing Module -> Response Generation Module -> Timing Control Module -> Output

**Critical Path**: The timing control module is critical as it determines when the model should respond. This component must balance the need for timely responses against the risk of premature answers that lack sufficient context.

**Design Tradeoffs**: The benchmark trades comprehensive context access for realistic streaming constraints. While models could theoretically perform better with full video access, this would not reflect real-world streaming scenarios where future information is unavailable.

**Failure Signatures**: Premature responses in Forward tasks indicate poor temporal patience. Consistent early answering across multiple scenarios suggests architectural limitations in streaming processing rather than scenario-specific issues.

**First Experiments**:
1. Test model response timing patterns across different task types to quantify premature response rates
2. Evaluate model performance with varying lookahead window sizes to identify optimal temporal processing windows
3. Compare streaming vs batch processing performance to isolate streaming-specific challenges

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework may introduce artifacts that influence the observed "Early Response" bias
- The benchmark's focus on four mobile-centric scenarios may not generalize to other streaming contexts
- The average video length of 13.3 minutes may not capture the full spectrum of streaming challenges

## Confidence

**High Confidence**: Quantitative finding that proprietary models score 80+ on Instant and Backward tasks while Forward scores drop to 16.40 is well-supported by the benchmark design and evaluation methodology.

**Medium Confidence**: Claim that models struggle to decide when to speak rather than what to say is compelling but requires careful interpretation as the observed bias could stem from multiple factors including architectural limitations or evaluation criteria.

**Low Confidence**: Assertion that this represents a "fundamental limitation" in MLLMs' temporal reasoning capabilities overstates the case as the benchmark captures only one specific temporal challenge.

## Next Checks

1. **Cross-Scenario Generalization Test**: Evaluate the same models on PhoStream against diverse streaming scenarios not represented in the original benchmark (e.g., live sports, medical procedures, educational lectures) to assess whether the "Early Response" bias persists across different temporal reasoning demands.

2. **Human Baseline Comparison**: Conduct a controlled study where human annotators respond to the same streaming video segments without visual access to future frames, then compare their response timing and accuracy against model outputs to establish whether the observed bias reflects a fundamental temporal reasoning challenge or an artifact of the evaluation setup.

3. **Temporal Tolerance Analysis**: Systematically vary the evaluation criteria's temporal tolerance window (e.g., allowing responses within 2, 5, or 10 seconds of optimal timing) to determine whether the dramatic performance drop in Forward tasks reflects strict criteria enforcement rather than genuine inability to exercise temporal patience.