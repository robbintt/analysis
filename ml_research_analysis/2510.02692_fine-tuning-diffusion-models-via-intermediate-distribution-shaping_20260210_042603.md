---
ver: rpa2
title: Fine-Tuning Diffusion Models via Intermediate Distribution Shaping
arxiv_id: '2510.02692'
source_url: https://arxiv.org/abs/2510.02692
tags:
- diffusion
- p-graft
- fine-tuning
- arxiv
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for fine-tuning diffusion
  models using rejection sampling (GRAFT) and introduces Partial-GRAFT (P-GRAFT),
  which shapes intermediate distributions to improve fine-tuning. P-GRAFT is theoretically
  justified via a bias-variance tradeoff and outperforms policy gradient methods like
  DDPO on text-to-image generation benchmarks (up to 8.81% relative improvement in
  VQAScore).
---

# Fine-Tuning Diffusion Models via Intermediate Distribution Shaping

## Quick Facts
- **arXiv ID:** 2510.02692
- **Source URL:** https://arxiv.org/abs/2510.02692
- **Reference count:** 40
- **Primary result:** P-GRAFT shapes intermediate distributions via partial denoising, achieving up to 8.81% relative improvement in VQAScore over DDPO on text-to-image generation.

## Executive Summary
This paper introduces a unified framework (GRAFT) for fine-tuning diffusion models using rejection sampling, and extends it with Partial-GRAFT (P-GRAFT) that shapes intermediate distributions to improve fine-tuning efficiency. The authors theoretically justify P-GRAFT via a bias-variance tradeoff and demonstrate significant performance gains across text-to-image, layout, molecule, and unconditional image generation tasks. They also introduce Inverse Noise Correction, an adapter-based method that improves flow models without explicit rewards by learning the distributional shift of initial noise.

## Method Summary
The paper proposes GRAFT as a unified framework for rejection sampling-based fine-tuning, where samples from a reference model are accepted based on rewards and used to fine-tune the model. P-GRAFT extends this by assigning rewards from fully denoised samples to partially denoised intermediate states, operating only on a subset of timesteps (typically the first 25%). This creates a bias-variance tradeoff where lower intermediate timesteps reduce reward variance but increase score function complexity. For flow models, Inverse Noise Correction learns the distributional shift between the idealized initial noise distribution and the actual inverse noise distribution obtained from data, enabling improved generation without explicit rewards.

## Key Results
- P-GRAFT achieves up to 8.81% relative improvement in VQAScore over DDPO on text-to-image generation benchmarks
- Inverse Noise Correction improves FID from 11.93 to 8.02 on CelebA-HQ at lower FLOPs
- Molecule generation stability increases from 36.6% to 51.3% with de-duplication
- Layout alignment improves from 70.1 to 76.3 on PubLayNet with P-GRAFT

## Why This Works (Mechanism)

### Mechanism 1: Rejection Sampling as Implicit KL-Regularization
Rejection sampling methods (Top-K, Best-of-N, preference rewards) implicitly solve the KL-regularized reward maximization objective. GRAFT generates samples from reference model $\bar{p}$, applies acceptance function A (monotonically increasing in reward), and trains on accepted samples. The accepted samples follow distribution $p_{RL}(x) \propto \exp(\hat{r}(x)/\alpha)\bar{p}(x)$ where $\hat{r}(x) = \log \mathbb{E}[A(\cdot)|X^{(1)}=x]$. This achieves marginal KL regularization despite intractable marginal likelihoods in diffusion models.

### Mechanism 2: Intermediate Distribution Shaping via Bias-Variance Tradeoff
Fine-tuning at intermediate noise level $t$ improves sample efficiency and final quality. P-GRAFT assigns rewards of fully denoised samples $X_0$ to partially denoised states $X_t$. As $t$ decreases toward data: (1) variance $\mathbb{E}[\text{Var}(r(X_0)|X_t)]$ decreases, providing cleaner reward signal; (2) score function $\nabla \log q_t$ becomes more complex, harder to learn. Optimal $t$ balances reward variance (lower is better) against learning difficulty (higher $t$ is easier).

### Mechanism 3: Flow Model Noise Distribution Correction
Learning the distributional shift of initial noise in flow models improves generation quality without explicit rewards. In flow models, discretization and learning errors cause the "inverse noise" distribution $p_{rev}^1$ to differ from $N(0,I)$. By training a lightweight corrector to map $N(0,I) \to p_{rev}^1$, then generating from corrected noise using original flow, we sample from the true data distribution. This corrects the noise distribution shift that occurs during flow model training.

## Foundational Learning

- **KL-regularized reward maximization (Eq. 1: $\max_p \mathbb{E}_{X\sim p}[r(X)] - \alpha \text{KL}(p\|\bar{p})$)**
  - Why needed: Core objective that GRAFT/P-GRAFT implicitly solve; understanding the tilted distribution $p_{RL} \propto \exp(r/\alpha)\bar{p}$ is essential.
  - Quick check: If $\alpha \to 0$, what happens to the optimal distribution? (Concentrates on highest-reward samples, may collapse diversity.)

- **Score matching and Tweedie's formula ($\mathbb{E}[X_0|X_t] = X_t + (1-\bar{\alpha}_t)\nabla \log p(X_t)$)**
  - Why needed: P-GRAFT analysis relies on score function properties; Theorem 4.4 bounds score deviation from Gaussian.
  - Quick check: Why is the score of $N(0,I)$ simply $-x$? ($\nabla \log p(x) = \nabla(-\|x\|^2/2) = -x$)

- **Forward/backward Euler for ODEs and fixed-point iteration**
  - Why needed: Inverse Noise Correction uses backward Euler (implicit method) to invert flow ODE; convergence requires contractive mapping.
  - Quick check: Why is backward Euler more stable for inversion than forward Euler? (Implicit formulation allows larger step sizes without instability when mapping is contractive.)

## Architecture Onboarding

- **Component map:**
  ```
  GRAFT Pipeline: Reference Model → Sample M trajectories → Compute rewards → Apply acceptance function A → Collect accepted samples → Train model p_θ on accepted samples
  P-GRAFT Extension: Save intermediate latents X_{t_NI} → Assign rewards from X_0 to X_{t_NI} → Retrain noise schedule → Inference: fine-tuned model for t ∈ [T, t_NI], reference model for t ∈ [t_NI, 0]
  Inverse Noise Correction: Pre-trained flow v_θ → Backward Euler on real data → Inverse noise dataset D_rev → Train corrector v_{θ}^{rev} → Inference: sample noise → corrector → pre-trained flow
  ```

- **Critical path:**
  1. For P-GRAFT: Intermediate timestep $N_I$ selection (start with 0.25$N$ based on empirical results)
  2. For Inverse Noise Correction: Step size $\eta$ tuning to ensure $\eta L < 1$; backward Euler convergence
  3. For both: Acceptance function A design (Top-K of M with $K/M \approx 0.1-0.25$ works well)

- **Design tradeoffs:**
  - $N_I$ selection: Lower $N_I$ = cleaner reward signal but harder score learning; Table 2 shows 0.25$N$ best for T2I, but task-dependent
  - $K/M$ ratio: Smaller $K/M$ = stronger distributional tilt (higher $\alpha$ penalty), may reduce diversity
  - Noise corrector size: 16M params (0.25× main model) sufficient; larger not needed based on Table 5
  - Reverse stitching (RP-GRAFT): Using fine-tuned model for later steps performs worse (Table 12), validating bias-variance theory

- **Failure signatures:**
  - DDPO instability on large prompt sets: Training curves collapse after ~150 steps; ratio saturation at clipping boundaries
  - Mode collapse without de-duplication: Molecule uniqueness drops sharply when training on all stable molecules
  - P-GRAFT with wrong $N_I$: $N_I = 0.75N$ underperforms $N_I = 0.25N$; too high introduces excessive reward variance
  - Inverse correction numerical instability: LSUN-Church requires $\sigma=10^{-3}$ perturbation for backward Euler

- **First 3 experiments:**
  1. Validate GRAFT vs. PPO on small benchmark: Implement Top-10 of 100 sampling on GenAI-Bench subset (100 prompts); compare VQAScore against DDPO baseline with matched compute.
  2. Ablate intermediate timestep $N_I$: Test P-GRAFT with $N_I \in \{0.75N, 0.5N, 0.25N\}$ on held-out prompts; measure both VQAScore and conditional variance of reward.
  3. Implement minimal Inverse Noise Correction: Train 16M-param corrector on CelebA-HQ with $\eta=0.005$; compare FID at 100+100 steps vs. baseline 1000 steps.

## Open Questions the Paper Calls Out

### Open Question 1
Can Inverse Noise Correction be generalized to stochastic diffusion models (SDEs), or is it fundamentally restricted to deterministic flow models? The authors state they "restrict our attention to flow models... which use ODEs to sample," and utilize deterministic reversibility for the noise correction algorithm. This remains unresolved because stochastic diffusion models inject noise during sampling, making the reverse mapping from image to a single initial noise vector ill-defined or stochastic, complicating the "inverse noise" distribution learning.

### Open Question 2
Does the "stitching" of fine-tuned and reference models at the intermediate timestep $N_I$ in P-GRAFT introduce distributional artifacts or discontinuities in the generation trajectory? Algorithm 2 explicitly switches from the fine-tuned model $\hat{p}$ to the reference model $\bar{p}$ at step $N_I$, but the analysis focuses on global metrics rather than local trajectory smoothness, which could theoretically introduce bias. This remains unresolved because the paper validates final image quality (VQAScore/FID) but does not analyze the probability flow continuity at the handoff point.

### Open Question 3
Is there a theoretically optimal heuristic for selecting the intermediate timestep $N_I$ in P-GRAFT that minimizes the bias-variance tradeoff without exhaustive search? The authors demonstrate a tradeoff and select $N_I$ empirically (e.g., $0.25N$), but do not provide a closed-form solution for the optimal time $t$. This remains unresolved because the optimal $t$ likely depends on the specific reward landscape and model architecture, and the paper relies on grid search to balance score simplicity (bias) against reward signal variance.

## Limitations

- GRAFT's theoretical framework assumes i.i.d. samples from reference model, but fine-tuning often operates on correlated trajectories; practical validity depends on maintaining sample diversity.
- P-GRAFT's bias-variance tradeoff relies on Ornstein-Uhlenbeck score convergence; multimodal target distributions may violate exponential decay assumptions.
- Inverse Noise Correction assumes backward Euler invertibility; numerical instability in complex datasets suggests practical limitations not captured in theory.

## Confidence

- **High:** GRAFT's implicit KL-regularization (Theorem 3.2 proof is complete); P-GRAFT improves VQAScore empirically; Inverse Noise Correction improves FID on CelebA-HQ.
- **Medium:** Theoretical justification of intermediate timestep selection (empirical evidence strong but theory assumes ideal conditions); noise corrector's general applicability beyond flow models.
- **Low:** GRAFT's performance guarantee under non-i.i.d. sampling; P-GRAFT's optimal $N_I$ across all tasks (0.25$N$ works well empirically but theoretical optimality unclear).

## Next Checks

1. Validate GRAFT vs. PPO equivalence: Implement Top-10 of 100 sampling on GenAI-Bench subset (100 prompts); compare VQAScore against DDPO baseline with matched compute (Table 17 shows ~7× fewer gradient calls needed).
2. Ablate intermediate timestep $N_I$: Test P-GRAFT with $N_I \in \{0.75N, 0.5N, 0.25N\}$ on held-out prompts; measure both VQAScore and conditional variance of reward (replicate Table 10 pattern).
3. Implement minimal Inverse Noise Correction: Train 16M-param corrector on CelebA-HQ with $\eta=0.005$; compare FID at 100+100 steps vs. baseline 1000 steps (target: Table 5 improvement magnitude of ~3-4 FID points).