---
ver: rpa2
title: 'The Expressive Capacity of State Space Models: A Formal Language Perspective'
arxiv_id: '2405.17394'
source_url: https://arxiv.org/abs/2405.17394
tags:
- languages
- theorem
- ssms
- state
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a theoretical study of the expressive capacity
  of linear state space models (SSMs) for language modeling, comparing their capabilities
  to transformers and traditional RNNs. The authors provide a fine-grained analysis
  of what formal languages SSMs can and cannot model, revealing overlapping but distinct
  strengths with transformers.
---

# The Expressive Capacity of State Space Models: A Formal Language Perspective

## Quick Facts
- arXiv ID: 2405.17394
- Source URL: https://arxiv.org/abs/2405.17394
- Reference count: 40
- Key outcome: SSMs can implement length-generalizing solutions for Flip Flop state tracking and model bounded hierarchical structure optimally without stacks, but nonnegative gates limit them to star-free languages only.

## Executive Summary
This paper provides a formal language-theoretic analysis of linear state space models (SSMs) for language modeling, comparing their capabilities to transformers and traditional RNNs. The authors prove that SSMs can implement exact, length-generalizing solutions for Flip Flop state tracking (a known transformer failure mode) and optimally model bounded hierarchical structures without stack simulation. However, they also prove that current SSM design choices—specifically nonnegative gates—restrict their expressive power to exactly the star-free regular languages, excluding important non-star-free languages like PARITY that require modular counting.

## Method Summary
The study uses formal language theory to characterize what SSMs can and cannot model, then validates these theoretical predictions using Mamba, a recent SSM implementation. The theoretical analysis proves expressivity bounds under assumptions of finite precision and specific gate parameterizations. Empirical validation involves training Mamba on formal language benchmarks including Flip Flop datasets, Tomita grammars, and bounded Dyck languages. The reproduction approach involves cloning a public repository, installing dependencies, generating or downloading specific datasets, and running training experiments across different model configurations to verify the theoretical claims about star-free versus non-star-free language recognition and hierarchical structure modeling.

## Key Results
- SSMs implement exact, length-generalizing solutions for Flip Flop state tracking, achieving near-zero test error where transformers struggle
- Mamba achieves near-perfect accuracy on all star-free languages but fails on non-star-free languages like PARITY, confirming the nonnegative gate limitation
- 2-layer Mamba achieves ~100% accuracy on Dyck(8,10) test sequences, demonstrating bounded hierarchical structure modeling without explicit stack simulation

## Why This Works (Mechanism)

### Mechanism 1: Length-Generalizing Flip Flop State Tracking
SSMs achieve length generalization through input-dependent gating that enables selective memory updates. The recurrence ht = A(xt) ◦ ht−1 + B(xt) allows Layer 1 to record the last instruction token while Layer 2 maintains state overwritten only on write instructions. Setting A(e(w)) = 0 enables selective overwrite while preserving state for read/ignore operations.

### Mechanism 2: Star-Free Language Characterization via Cascade Products
By the Krohn-Rhodes theorem, star-free languages decompose into cascade products of set-reset automata. Each set-reset automaton maps directly to one SSM layer, with the entire cascade simulated by stacking layers. The nonnegative gate constraint ensures only aperiodic monoids are required, preventing the need for negative/complex gates that would enable non-star-free languages.

### Mechanism 3: Bounded Hierarchical Structure Without Stack Simulation
Layer 1 implements a counter tracking current depth while Layer 2 applies the Flip Flop mechanism at each depth level to track the last opening bracket type. This direct state tracking achieves the same O(h log K) memory bound as optimal stack-based RNN constructions but without explicit stack operations.

## Foundational Learning

- **Star-Free vs. Non-Star-Free Regular Languages**: SSMs with nonnegative gates can represent star-free languages (definable without Kleene star) but not non-star-free languages (requiring modular counting like PARITY). Quick check: Is L = {w ∈ {0,1}* | the number of 1s is even} star-free? (No—it requires counting mod 2.)

- **Elementwise State Recurrence (Diagonal SSMs)**: The recurrence ht = A(xt) ◦ ht−1 + B(xt) limits expressivity by preventing arbitrary matrix operations on state within the recurrence step. Channel mixing must happen in the transform φ. Quick check: How does elementwise recurrence differ from full matrix recurrence ht = A ht−1 + B xt? (Elementwise precludes mixing across state dimensions within the recurrence step.)

- **Finite Precision Constraints**: All positive results assume finite precision with unbounded integer bits but bounded fractional bits. This matters for distinguishing states in Flip Flop and prevents PARITY solutions requiring infinitely many distinct states. Quick check: Why can't a finite-precision SSM track unbounded modular counts? (Reading out modular value from unbounded counts requires periodic functions over arbitrarily large inputs, which standard nonlinearities cannot represent uniformly.)

## Architecture Onboarding

- **Component map**: Token embeddings -> Input-dependent gates A,B -> Elementwise recurrence ht = A(xt) ◦ ht−1 + B(xt) -> Transform φ (channel mixing + normalization) -> Output predictions

- **Critical path**: Identify target language class -> Map to required automaton type -> Construct minimal layer count -> Set gate parameterization (nonnegative for stability)

- **Design tradeoffs**: 
  - Nonnegative gates ensure training stability but block PARITY and all non-star-free languages
  - Input-dependent gates enable Flip Flop selective memory but add parameter overhead
  - Bounded state values enable length generalization but limit unbounded depth modeling

- **Failure signatures**: 
  - PARITY and modular counting: Near-random performance on (aa)*, (aaaa)*, (abab)*
  - Length generalization on counters: Perfect in-distribution accuracy but sharp degradation on out-of-distribution lengths
  - Insufficient layers for hierarchical structure: 1-layer Mamba on Dyck(8,10) achieves ~60-70% vs. ~100% for 2-layer

- **First 3 experiments**: 
  1. Star-free language battery: Train on Tomita-1,2,4,7 and D2,D3,D4 languages; expect >95% accuracy validating Theorem 4
  2. PARITY stress test: Train on PARITY language; expect failure to generalize beyond training lengths
  3. Layer ablation on Dyck(K,h): Train 1-layer and 2-layer Mamba on Dyck(8,10); expect 2-layer achieves near-perfect accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Do SSM-based LLMs learn non-stack shortcuts for bounded hierarchical structure? The paper proves SSMs can model Dyck languages without stacks using counting-based shortcuts, but does not verify if trained models actually utilize these specific constructions over stack-simulation strategies. Interpretability studies on trained SSMs would resolve this.

### Open Question 2
Can complex-valued gates improve performance on periodic state-tracking tasks? The paper suggests that complex gates might enable modular counting required for non-star-free languages like PARITY. Training SSM variants with complex-valued gates on modular counting tasks would provide evidence.

### Open Question 3
Are the theoretical constructions learnable via gradient descent with flat minima? The paper focuses on in-principle expressivity rather than optimization dynamics required to find those weights. Analysis of loss landscape and generalization of SSMs initialized with or trained towards theoretical constructions would address this.

## Limitations

- The star-free language characterization depends on nonnegative gate parameterization, but modern SSMs like Griffin use different gating mechanisms (e.g., swish) that may permit negative values
- The Flip Flop mechanism relies on input-dependent gating, but time-invariant gates (as used in S4/RetNet) cannot implement this functionality
- The bounded hierarchical structure construction assumes a fixed depth bound, making it unclear how well SSMs generalize to unbounded context-free languages

## Confidence

- **High confidence**: Star-free language characterization (Theorem 4) and Mamba's empirical failure on non-star-free languages
- **Medium confidence**: Flip Flop length generalization claim (limited empirical validation)
- **Low confidence**: Bounded hierarchical structure construction (Theorem 6) (optimality claim not independently verified)

## Next Checks

1. Gate parameterization ablation study: Compare Mamba variants with sigmoid/exponential gates versus swish/linear gates on star-free versus non-star-free language batteries

2. Unbounded depth stress test: Extend Dyck language experiments to higher depth bounds (h > 8) to evaluate scaling behavior

3. Time-invariant vs. input-dependent gate comparison: Implement hybrid SSM architecture allowing switching between time-invariant and input-dependent gates, then measure exact tradeoff in Flip Flop accuracy versus efficiency