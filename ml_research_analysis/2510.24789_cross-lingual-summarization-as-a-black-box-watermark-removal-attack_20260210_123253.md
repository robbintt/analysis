---
ver: rpa2
title: Cross-Lingual Summarization as a Black-Box Watermark Removal Attack
arxiv_id: '2510.24789'
source_url: https://arxiv.org/abs/2510.24789
tags:
- clsa
- cross-lingual
- summarization
- language
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Cross-lingual summarization attacks systematically destroy distributional\
  \ watermarks by forcing text through a semantic bottleneck\u2014first translating\
  \ to a pivot language, then compressing with summarization. Evaluated across four\
  \ watermarking schemes and five languages, this black-box attack drives detection\
  \ accuracy toward chance (AUROC ~0.5) while maintaining semantic fidelity."
---

# Cross-Lingual Summarization as a Black-Box Watermark Removal Attack

## Quick Facts
- arXiv ID: 2510.24789
- Source URL: https://arxiv.org/abs/2510.24789
- Reference count: 8
- Cross-lingual summarization systematically degrades watermark detection to chance levels (AUROC ~0.5) while maintaining semantic fidelity

## Executive Summary
This paper presents Cross-Lingual Summarization Attack (CLSA), a black-box watermark removal technique that combines machine translation and abstractive summarization to destroy distributional watermarks. The attack forces text through a semantic bottleneck—first translating to a pivot language, then compressing to 15-25% of original length. Evaluated across four watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages (Amharic, Chinese, Hindi, Spanish, Swahili), CLSA reduces detection accuracy from baseline AUROC ~0.98 to near-chance levels (~0.53 for XSIR). The combination of position elimination, vocabulary consolidation, and cross-lingual compression proves more effective than translation or summarization alone.

## Method Summary
CLSA operates as a black-box pipeline using commodity models: (1) M2M100 translates watermarked text from source language ℓs to pivot language ℓp (typically English), (2) mT5/XLSum summarizes the translated text to 15-25% of original token count, and (3) optional back-translation returns the output to ℓs. The attack targets distributional watermarking schemes by systematically removing seeded token positions through compression and perturbing vocabulary support through cross-lingual translation. Detection thresholds are tuned on validation splits to minimize EER. The method requires no watermark keys or detector internals, making it practical against deployed systems.

## Key Results
- CLSA reduces XSIR detection AUROC from 0.827 (paraphrasing) and 0.823 (CWRA) to 0.53
- Position elimination: Summarization removes 75-85% of original positions, directly eliminating seeded tokens
- Cross-lingual perturbation: Translation alters tokenization boundaries and moves text off detector vocabulary support
- Vocabulary consolidation: Abstractive summarization prunes rare seeded synonyms toward high-frequency pivots
- Low-resource languages (Amharic, Swahili) show more severe degradation (AUROC 0.41-0.43) than high-resource pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Length compression systematically eliminates seeded watermark positions
- **Mechanism:** Summarization reduces text to 15-25% of original length, directly removing 75-85% of token positions where watermark signals were embedded
- **Core assumption:** Detectors rely on accumulating token-level statistics across sufficient positions to distinguish watermarked from unwatermarked text
- **Evidence anchors:** [abstract] "forcing text through a semantic bottleneck—first translating to a pivot language, then compressing with summarization"; [Section 6] "Position elimination: Summarization removes 75-85% of original positions, directly eliminating seeded tokens"
- **Break condition:** If detectors used fixed-position rather than cumulative statistics, or if watermark signals were concentrated in semantically essential tokens that survive summarization

### Mechanism 2
- **Claim:** Cross-lingual translation perturbs tokenization boundaries and moves text off detector vocabulary support
- **Mechanism:** Translation alters subword segmentation patterns, breaking the deterministic hash-to-vocabulary mapping that detectors depend on
- **Core assumption:** Watermark detectors implicitly rely on vocabulary support and tokenization consistency between generation and detection
- **Evidence anchors:** [Section 3] "This perturbs tokenization boundaries and moves the sample off the source vocabulary support that detectors implicitly rely on"; [Section 6] "Cross-lingual summarization can map semantically similar source contexts to dissimilar target contexts"
- **Break condition:** If watermarking schemes used cross-lingually consistent token clustering (XSIR attempts this but still fails against CLSA per Section 5: AUROC 0.53)

### Mechanism 3
- **Claim:** Abstractive summarization consolidates vocabulary toward high-frequency tokens, pruning rare seeded synonyms
- **Mechanism:** Summarization models favor generic, high-probability vocabulary over diverse synonyms
- **Core assumption:** Watermarks embed signals through vocabulary choice patterns that survive paraphrasing but not aggressive compression
- **Evidence anchors:** [Section 3] "Summarization concentrates probability mass on high-frequency pivots; rare seeded synonyms are pruned"; [Section 6] "Vocabulary consolidation: Abstractive summarization favors high-frequency, generic vocabulary over diverse synonyms"
- **Break condition:** If watermarking biased only toward high-frequency tokens that summarization naturally preserves

## Foundational Learning

- **Concept: Distributional watermarking (KGW-family)**
  - Why needed here: Understanding how detectors use token-frequency statistics explains why CLSA's position elimination is effective
  - Quick check question: Can you explain why reducing token count from 500 to 100 would lower a detector's confidence even if the same proportion of "green" tokens remained?

- **Concept: Semantic-invariant watermarking (SIR/XSIR)**
  - Why needed here: XSIR was designed specifically for cross-lingual robustness yet CLSA still degrades it to AUROC 0.53—understanding why reveals the attack's deeper mechanism
  - Quick check question: Why would clustering tokens across languages still fail when summarization changes which tokens appear at all?

- **Concept: Black-box threat model**
  - Why needed here: CLSA requires no watermark keys or detector internals—understanding this constraint clarifies why the attack uses commodity models rather than optimization-based approaches
  - Quick check question: What information does CLSA assume the attacker has access to, and what must remain hidden?

## Architecture Onboarding

- **Component map:** Input (watermarked text ℓs) -> M2M100 translation (ℓs→ℓp) -> mT5/XLSum summarization (15-25% length) -> (optional) M2M100 back-translation (ℓp→ℓs) -> Output (degraded watermark)
- **Critical path:** The summarization stage is the core novelty—translation alone (CWRA) achieves AUROC ~0.82 on XSIR, but adding summarization drops it to 0.53. The length constraint (15-25%) is the key hyperparameter.
- **Design tradeoffs:**
  - Tighter compression → better watermark removal but risk of semantic drift
  - Back-translation → enables same-language output but adds processing noise; paper shows mixed results
  - Pivot language choice → high-resource pivots (English, Chinese) give better MT quality but paper doesn't extensively compare pivots
- **Failure signatures:**
  - If AUROC remains >0.7: summarization ratio likely too conservative; reduce length budget
  - If semantic quality degrades noticeably: length budget too aggressive or MT quality poor for language pair
  - Low-resource languages show more severe detection degradation—possibly due to noisier MT compounding the attack
- **First 3 experiments:**
  1. Replicate XSIR result: Generate watermarked text, apply CLSA with English pivot, verify AUROC drops from ~0.98 baseline to ~0.53
  2. Ablate summarization: Compare translation-only (CWRA) vs. translation+summarization to quantify compression's contribution
  3. Vary length budget: Test 10%, 25%, 50% compression ratios to find quality-removal tradeoff frontier

## Open Questions the Paper Calls Out

- **Question:** Can length-aware detection strategies effectively mitigate CLSA without producing excessive false positives on legitimate abstractive summarization?
- **Basis in paper:** [explicit] Section 6 identifies "length-aware detectors" as a potential defense but immediately notes they face "fundamental limitations" because "length restrictions conflict with legitimate use cases."
- **Why unresolved:** The paper proposes the defense but does not implement or evaluate it, leaving the trade-off between detecting compressed attacks and flagging benign summaries undefined.
- **What evidence would resolve it:** Evaluation of false positive rates for length-aware detectors on datasets of human-written summaries compared to CLSA outputs.

- **Question:** Do CLSA attack efficacy rates generalize to higher-capacity commercial translation and summarization models (e.g., GPT-4) beyond the specific M2M100 and mT5 architectures tested?
- **Basis in paper:** [inferred] Section 7 (Limitations) explicitly states results depend on specific models and "may not generalize to other architectures," leaving the performance of SOTA models unknown.
- **Why unresolved:** The study relies on specific open-weight models (M2M100, mT5); it is unclear if higher-quality commercial models preserve more watermark signal or destroy it more effectively due to better semantic fidelity.
- **What evidence would resolve it:** Replication of the CLSA pipeline using commercial APIs (e.g., Google Translate, GPT-4) on the same watermarked datasets.

- **Question:** How can cryptographic or model-attestation protocols be effectively integrated with distributional watermarks to preserve provenance through cross-lingual semantic bottlenecks?
- **Basis in paper:** [explicit] The Abstract and Conclusion explicitly argue that solutions "must move beyond distributional watermarking and incorporate cryptographic or model-attestation approaches."
- **Why unresolved:** The paper identifies the need for hybrid systems but provides no implementation details or architecture for how these distinct provenance methods would interact.
- **What evidence would resolve it:** A proposed architecture where cryptographic signatures survive the semantic bottleneck, or a proof-of-concept where attestation complements statistical detection.

## Limitations

- **Semantic fidelity uncertainty:** The paper reports no automated or human evaluation of content fidelity beyond implicit assumptions about summarization quality
- **Low-resource language analysis gap:** Extreme degradation in Amharic/Swahili lacks analysis of whether this reflects superior watermark removal or catastrophic MT quality issues
- **Defensive countermeasures unexplored:** No evaluation of multi-signal watermarking or other defenses against CLSA's semantic bottleneck approach

## Confidence

**High Confidence:** The core empirical claim that cross-lingual summarization degrades watermark detection accuracy across all four schemes to near-chance levels is well-supported by reported AUROC/AUPRC metrics and systematic comparison against baselines.

**Medium Confidence:** The mechanism explanations (position elimination, vocabulary consolidation, cross-lingual perturbation) are plausible given observed results, but lack ablation studies isolating each component's contribution.

**Low Confidence:** Claims about attack practicality in real-world scenarios lack supporting evidence. The paper assumes commodity models are always available and effective without testing scenarios where MT quality is poor or summarization budgets are constrained.

## Next Checks

1. **Semantic Fidelity Validation:** Conduct human evaluation (Likert scale 1-5) on attacked vs. original text pairs to establish the quality-removal tradeoff frontier. Measure at what compression ratio (10%, 15%, 25%, 35%) semantic degradation becomes unacceptable (average score <3), then compare the corresponding watermark detection AUROC.

2. **Component Ablation Study:** Isolate the contribution of each attack stage by measuring detection AUROC for: (a) translation-only (CWRA baseline), (b) summarization-only (no translation), (c) translation+summarization (CLSA), (d) summarization+back-translation. This quantifies whether MT noise or length compression provides the primary degradation mechanism.

3. **Defensive Countermeasure Testing:** Implement a multi-signal watermarking scheme that combines positional KGW signals with syntactic pattern watermarking (e.g., specific dependency tree structures). Evaluate whether CLSA's semantic bottleneck approach can simultaneously degrade both signal types, or if one type proves more resilient to compression.