---
ver: rpa2
title: Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute
  Human Evaluation
arxiv_id: '2601.18630'
source_url: https://arxiv.org/abs/2601.18630
tags:
- health
- mental
- these
- your
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates nine large language models (LLMs) across 500
  counseling conversations to assess their therapeutic reliability. Two psychiatric-trained
  experts independently rated 4,500 LLM responses on a 6-attribute rubric covering
  cognitive support (guidance, informativeness, safety) and affective resonance (empathy,
  helpfulness, interpretation).
---

# Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation

## Quick Facts
- arXiv ID: 2601.18630
- Source URL: https://arxiv.org/abs/2601.18630
- Reference count: 9
- Primary result: LLM responses excel in cognitive support (safety, interpretation) but show significant affective inconsistency (empathy, helpfulness)

## Executive Summary
This study evaluates nine large language models across 500 counseling conversations using a comprehensive 6-attribute rubric covering both cognitive support and affective resonance. Expert psychiatric raters found that while models achieve strong scores on safety (4.89/5) and interpretation (4.85/5), they struggle with emotional attunement, particularly empathy (4.03/5). The research reveals a persistent cognitive-affective gap, with top-performing models like GPT-4o and Gemini 2.0-Flash achieving balanced profiles, while open-source models exhibit greater variability. The findings highlight the need for failure-aware evaluation frameworks that focus on both aggregate performance and specific failure modes in mental health AI applications.

## Method Summary
The study evaluates nine LLMs using 500 counseling conversations sampled from MentalChat16K, EmoCare, and CounselChat datasets. Two psychiatric-trained experts independently rate 4,500 model responses on six attributes using a 5-point Likert scale: Guidance, Informativeness, Safety, Interpretation, Empathy, and Helpfulness. The evaluation focuses on single-turn responses generated with temperature=0.7 and max_tokens=512. Responses are anonymized and rated blindly, with scores aggregated to produce composite Cognitive Support and Affective Resonance metrics. The analysis examines both mean performance and failure rates (responses scoring ≤3 on any attribute).

## Key Results
- Top models (GPT-4o, Gemini 2.0-Flash) achieve overall scores above 4.5/5, while Qwen-3 ranks lowest at 3.60
- Cognitive attributes show high reliability: Safety (4.89 ± 0.41), Interpretation (4.85 ± 0.34)
- Affective attributes lag significantly: Empathy (4.03 ± 0.84), Helpfulness (4.06 ± 0.75)
- Open-source models exhibit greater variability than closed-source models
- 77.6% of Qwen-3 responses receive low Guidance scores, indicating structural but unactionable responses

## Why This Works (Mechanism)

### Mechanism 1: Cognitive-Affective Decoupling
- Claim: LLMs develop cognitive reliability (safety, factual accuracy) more readily than affective alignment (empathy, emotional attunement) under standard training regimes.
- Mechanism: Training objectives optimize for informational accuracy and harm avoidance, which produce strong scores on Safety (4.89/5) and Interpretation (4.85/5). Affective qualities like Empathy (4.03/5) require nuanced emotional calibration that standard next-token prediction does not explicitly target.
- Core assumption: The gap reflects training data composition and objective function design rather than architectural impossibility.
- Evidence anchors:
  - [abstract]: "persistent cognitive-affective gap, with models excelling in factual precision yet struggling with emotional attunement"
  - [section - Results]: "Safety (4.89 ± 0.41)... In contrast, affective dimensions such as Empathy (4.03 ± 0.84) and Helpfulness (4.06 ± 0.75) trailed behind"
  - [corpus]: Related work (Marrapese et al.) found "no models achieving statistically significant correlations in anxiety-related conversations," corroborating affective inconsistency

### Mechanism 2: Multi-Attribute Expert Rubric for Failure Detection
- Claim: Decomposing therapeutic quality into six orthogonal attributes exposes failure modes hidden by aggregate scores.
- Mechanism: The rubric separates Cognitive Support (Guidance, Informativeness, Safety) from Affective Resonance (Empathy, Helpfulness, Interpretation), enabling detection of models that score well overall but fail critically on specific attributes (e.g., Qwen-3: 4.65 safety but 3.10 helpfulness).
- Core assumption: Attributes are sufficiently independent to provide diagnostic value.
- Evidence anchors:
  - [abstract]: "comprehensive 6-attribute rubric covering cognitive support... and affective resonance"
  - [section - Methodology]: "capturing not only the factual clarity and problem-solving adequacy... but also their emotional sensitivity and interpersonal appropriateness"
  - [corpus]: Related papers employ EPIT-OME and MITI frameworks for similar multi-dimensional assessment

### Mechanism 3: Model Scale and Training Investment Hierarchy
- Claim: Closed-source models achieve more balanced profiles due to scale and alignment refinement; open-source models show greater variability.
- Mechanism: Larger models with extensive RLHF and safety training (GPT-4o: 4.70 overall) develop better cross-dimensional capability. Smaller open-source models (Qwen-3 4B: 3.60 overall; 77.6% low scores on Guidance) lack capacity for consistent emotional calibration.
- Core assumption: Performance differences stem from model capability and training, not evaluation artifacts.
- Evidence anchors:
  - [abstract]: "closed-source models (e.g., GPT-4o) offer balanced therapeutic responses, open-source models show greater variability"
  - [section - Results]: "GPT-4o achieved the highest overall mean of 4.70... Qwen-3 (3.60) ranked lowest"
  - [corpus]: Limited direct comparisons available; corpus evidence on open vs. closed therapeutic performance is sparse

## Foundational Learning

- **Cognitive Support vs. Affective Resonance**
  - Why needed here: The paper's central finding depends on understanding that therapeutic quality has two distinct dimensions—factual/structural (cognitive) and emotional/relational (affective)—which don't correlate perfectly.
  - Quick check question: Can a response be safe but unempathetic? (Yes—this is the cognitive-affective gap.)

- **Likert Scale Evaluation with Expert Annotators**
  - Why needed here: The methodology relies on psychiatric-trained experts rating 4,500 responses; understanding inter-rater reliability and subjectivity is critical for interpreting results.
  - Quick check question: Why use two independent experts rather than one? (To reduce individual bias and improve reliability.)

- **Failure-Aware vs. Aggregate Evaluation**
  - Why needed here: The paper advocates for focusing on failure modes (≤3 scores) rather than just mean performance, which changes how you interpret model safety.
  - Quick check question: A model with 4.5 average but 15% low scores on Empathy—is it deployment-ready? (Depends on risk tolerance; aggregate scores mask tail failures.)

## Architecture Onboarding

- **Component map:**
  - Data curation -> LLM generation -> Expert evaluation -> Analysis pipeline

- **Critical path:**
  1. Prompt design (consistent psychiatrist persona across all models)
  2. Response generation (identical configuration)
  3. Blinded expert rating (anonymized model outputs)
  4. Attribute-level aggregation and failure mode analysis

- **Design tradeoffs:**
  - Single-turn evaluation (faster) vs. multi-turn dialogue (more realistic but costlier)
  - 2 experts (feasible) vs. larger rater pools (more robust)
  - Temperature 0.7 balances diversity and coherence; higher values increase affective variability

- **Failure signatures:**
  - Qwen-3: 77.6% low Guidance scores, 64.4% low Helpfulness—coherent but unactionable responses
  - Qwen-2.5: 10% low Safety (failed generation or incoherent output in case study)
  - Claude-3.5-Haiku: 45.6% low Empathy despite closed-source status—unexpected affective flatness

- **First 3 experiments:**
  1. Extend to multi-turn dialogues to test if affective alignment degrades over longer conversations.
  2. Add cultural/linguistic diversity to test generalizability beyond English counseling contexts.
  3. Develop automated metrics correlated with expert human judgment to enable scalable evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or structural patterns characterize the failure modes driving affective inconsistency in LLM mental health responses?
- Basis in paper: [explicit] The authors state: "We also aim to identify specific failure modes driving affective inconsistency."
- Why unresolved: The study documents that empathy and helpfulness scores are highly variable (SD ≈ 0.7–0.8) compared to cognitive attributes (SD ≈ 0.2–0.3), but does not systematically analyze what conversational features cause these affective breakdowns.
- What evidence would resolve it: Qualitative error analysis of low-scoring responses, linguistic feature extraction, and correlation analysis linking specific response patterns to poor affective ratings.

### Open Question 2
- Question: Can automated evaluation metrics be developed that reliably predict expert human judgment of therapeutic quality across all six attributes?
- Basis in paper: [explicit] Future work aims "to develop automated metrics that align with expert human judgment, enabling scalable yet clinically valid assessment."
- Why unresolved: The paper notes that standard metrics like BLEU and ROUGE are ill-suited for therapeutic qualities, and no validated automated proxy exists for the six-attribute rubric.
- What evidence would resolve it: Development and validation of novel metrics showing high correlation (r > 0.8) with expert ratings across guidance, empathy, and helpfulness attributes on held-out conversations.

### Open Question 3
- Question: How does LLM therapeutic performance change across multi-turn counseling dialogues compared to single-turn responses?
- Basis in paper: [explicit] The authors explicitly state future work will "extend this evaluation to multi-turn dialogues."
- Why unresolved: The current study evaluates 500 single-turn conversations; therapeutic effectiveness may degrade or improve across extended interactions requiring context retention and relational continuity.
- What evidence would resolve it: A longitudinal evaluation study using multi-turn conversation datasets with the same six-attribute rubric, comparing per-turn and aggregate performance.

### Open Question 4
- Question: What training or fine-tuning approaches can effectively bridge the cognitive-affective gap to improve emotional attunement without sacrificing safety and informativeness?
- Basis in paper: [inferred] The paper identifies the "cognitive-affective gap" as the core limitation and calls for "multi-dimensional fine-tuning strategies that jointly optimize informational accuracy and interpersonal sensitivity," but does not test any interventions.
- Why unresolved: No experiments test whether reinforcement learning, empathetic fine-tuning, or multi-objective training can improve empathy/helpfulness while maintaining safety scores above 4.8/5.
- What evidence would resolve it: Intervention studies comparing baseline and fine-tuned models on the six-attribute rubric, demonstrating statistically significant improvements in affective scores without cognitive degradation.

## Limitations

- Single-turn evaluation may not capture conversational dynamics and context retention capabilities
- Expert ratings are subjective and limited to two annotators without reported inter-rater reliability
- Limited cultural and linguistic diversity in counseling conversations may constrain generalizability

## Confidence

- **High Confidence**: The finding that top models achieve strong scores on safety (4.89/5) and interpretation (4.85/5) while affective attributes like empathy (4.03/5) lag behind
- **Medium Confidence**: The interpretation that this gap reflects training objectives rather than architectural impossibility
- **Medium Confidence**: The claim that open-source models show greater variability than closed-source models

## Next Checks

1. **Multi-turn validation**: Extend the evaluation to full counseling dialogues (5-10 turns) to test whether affective alignment degrades or improves over longer interactions, and whether the cognitive-affective gap persists in conversational contexts.

2. **Automated metric development**: Develop and validate automated metrics (e.g., empathy detection models, safety classifiers) that correlate with human expert ratings to enable scalable evaluation beyond expert annotation.

3. **Cross-cultural replication**: Repeat the evaluation with counseling conversations from diverse cultural contexts and languages to test whether the cognitive-affective patterns hold across different therapeutic norms and communication styles.