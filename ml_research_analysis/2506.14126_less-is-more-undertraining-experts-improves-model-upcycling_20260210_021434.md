---
ver: rpa2
title: 'Less is More: Undertraining Experts Improves Model Upcycling'
arxiv_id: '2506.14126'
source_url: https://arxiv.org/abs/2506.14126
tags:
- merging
- training
- learning
- experts
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that overtraining experts can degrade performance
  in model upcycling. Across multiple settings including merging fully fine-tuned
  and LoRA-adapted models, as well as MoErging, the authors find that experts trained
  longer yield worse results when combined.
---

# Less is More: Undertraining Experts Improves Model Upcycling

## Quick Facts
- arXiv ID: 2506.14126
- Source URL: https://arxiv.org/abs/2506.14126
- Reference count: 40
- Primary result: Overtrained experts degrade upcycling performance; task-dependent early stopping recovers optimal accuracy while reducing training costs

## Executive Summary
This paper reveals a counterintuitive phenomenon in model upcycling: experts trained longer yield worse results when merged. Across multiple settings including fully fine-tuned models and LoRA adapters, the authors find that overtraining experts degrades merged model performance. They trace this to memorization of difficult examples during late training stages, which are then forgotten during merging. Task-dependent aggressive early stopping significantly improves upcycling performance, recovering optimal accuracy while reducing training costs.

## Method Summary
The authors evaluate the impact of expert fine-tuning duration on model upcycling performance using CLIP ViT-B-32 and T5-base models. They fine-tune separate experts on multiple tasks for varying steps (2 to 2048), using AdamW with task-specific hyperparameters. For LoRA, they use rank 8 with alpha scaling. Experts are merged using techniques including Average, Task Arithmetic, TIES, and DARE. The study also explores MoErging, wrapping LoRA adapters into mixture-of-experts layers with router optimization. A key contribution is the task-dependent early stopping strategy that monitors validation accuracy plateaus to identify optimal training termination points.

## Key Results
- Overtraining experts by 8x (2048 vs 256 steps) degrades merged model accuracy by ~3% for fully fine-tuned models
- LoRA adapters show more severe overtraining effects, with accuracy drops of 5-17% compared to fully fine-tuned models
- Task-dependent early stopping recovers optimal merged accuracy while reducing training costs
- Higher LoRA ranks mitigate overtraining penalties by providing more capacity

## Why This Works (Mechanism)

### Mechanism 1
Early training captures generalizable, transferable features while late training memorizes task-specific outliers. Merging aggregates parameters, and updates from memorization are task-specific and get averaged away, causing the model to "forget" those examples. This assumes merging operations preserve shared structure while canceling out task-specific idiosyncrasies.

### Mechanism 2
LoRA adapters are more sensitive to overtraining because low-rank constraints force knowledge into a compressed subspace where memorization competes more directly with generalizable features. Low-rank structure creates a capacity bottleneck where memorization and generalization compete for the same subspace dimensions.

### Mechanism 3
Task-dependent early stopping improves upcycling by stopping each expert at the point where generalizable knowledge is learned but before memorization dominates. Different datasets have different difficulty distributions, so adaptive early stopping finds task-specific optima by correlating validation accuracy plateau with the transition from generalization to memorization.

## Foundational Learning

**Model Merging / Task Arithmetic**: Understanding how task vectors (θ_t - θ_0) are combined is crucial since merging "forgets" memorized updates. Quick check: Can you explain why averaging task vectors from two fine-tuned models might hurt individual task performance?

**LoRA (Low-Rank Adaptation)**: The stronger overtraining effect for LoRA requires understanding that LoRA adds low-rank matrices A, B to frozen weights. Rank limits capacity. Quick check: If LoRA rank r=8 but the full weight matrix is 4096×4096, how many trainable parameters does LoRA add versus full fine-tuning?

**Data Difficulty & Memorization**: The paper uses EL2N scores to show hard examples dominate late loss and are forgotten during merging. Understanding this link is crucial. Quick check: Why might a model need to "memorize" a difficult example rather than learn a generalizable rule from it?

## Architecture Onboarding

**Component map**: Pre-trained foundation model -> Fine-tuning loop -> Merging module -> MoErging module -> Early stopping controller

**Critical path**: 1) Identify tasks and datasets, 2) Configure fine-tuning, 3) Train experts with checkpoint saving, 4) Apply merging or MoErging, 5) Evaluate on held-out test sets

**Design tradeoffs**: Training duration vs expert accuracy (longer training improves single-expert performance but degrades merge quality), LoRA rank vs merge stability (higher rank reduces overtraining penalty but increases parameters), Early stopping sensitivity (aggressive stopping saves compute but requires reliable validation data)

**Failure signatures**: Merged model accuracy drops well below average expert accuracy (likely overtrained experts), TIES merging crashes on LoRA with large accuracy gaps (check rank and training duration), MoErging fails to improve after multi-task training (experts may be over-specialized; try earlier checkpoints)

**First 3 experiments**: 1) Baseline sweep: Train FFT experts for steps s ∈ {128, 256, 512, 1024, 2048}; merge with Task Arithmetic; plot merged accuracy vs s, 2) LoRA rank ablation: Repeat with LoRA ranks r ∈ {8, 32, 128}; observe how higher rank shifts the optimal stopping point later, 3) Early stopping validation: Implement plateau-based early stopping on one task; compare merged accuracy using early-stopped expert vs 2048-step expert

## Open Questions the Paper Calls Out

Does the overtraining effect hold for larger foundation models (e.g., 7B+ parameter LLMs)? Experiments only cover ViT-B-32 and T5-base; scaling behavior to billion-parameter models is unknown.

Can optimal merging checkpoints be identified post-hoc for already-published adapters on repositories like HuggingFace? Intermediate checkpoints are typically not released; only final converged models are available.

How does dataset size interact with optimal stopping time across heterogeneous tasks? Early stopping strategy is task-dependent but systematic interaction with dataset scale remains uncharacterized.

Does the phenomenon generalize to PEFT methods beyond LoRA (e.g., prefix tuning, prompt tuning)? Only LoRA adapters were tested; other PEFT methods have different parameter structures.

## Limitations
The overtraining effect was demonstrated on CLIP ViT-B-32 and T5-base, but generalization to other architectures like BERT or ConvNets is unclear. Experiments used curated datasets with clear difficulty spectra, but real-world tasks may have different memorization profiles. The evidence for the memorization-forgetting mechanism is indirect, relying on EL2N correlation and loss trends rather than direct ablation studies.

## Confidence

**High confidence**: Overtraining degrades model merging accuracy, especially for LoRA; task-dependent early stopping recovers this gap

**Medium confidence**: The memorization-forgetting mechanism is the primary cause; higher LoRA rank mitigates overtraining by providing more capacity

**Low confidence**: The proposed early stopping threshold is universally optimal; the pattern holds outside CLIP/T5 model families

## Next Checks

Apply the same overtraining sweep to RoBERTa and ResNet-50 on standard vision/NLP tasks to confirm if accuracy drop during merging persists across architectures.

During late-stage training, freeze a subset of parameters and continue fine-tuning the rest; after merging, measure whether the frozen subset retains accuracy better than the non-frozen one to test if forgetting drives degradation.

Sweep LoRA rank from 1 to full rank on one task and plot merge accuracy vs rank to locate the point where overtraining penalty disappears, testing the capacity bottleneck hypothesis.