---
ver: rpa2
title: A Survey of Vibe Coding with Large Language Models
arxiv_id: '2510.12399'
source_url: https://arxiv.org/abs/2510.12399
tags:
- arxiv
- code
- language
- coding
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides the first comprehensive review of Vibe Coding,
  a novel software development paradigm enabled by large language models (LLMs) where
  developers validate AI-generated code through outcome observation rather than line-by-line
  comprehension. The authors formalize Vibe Coding as a Constrained Markov Decision
  Process capturing the dynamic relationship among human developers, software projects,
  and coding agents.
---

# A Survey of Vibe Coding with Large Language Models

## Quick Facts
- **arXiv ID:** 2510.12399
- **Source URL:** https://arxiv.org/abs/2510.12399
- **Reference count:** 40
- **Primary result:** First comprehensive review of "Vibe Coding" as a software development paradigm where developers validate AI-generated code through outcome observation rather than line-by-line comprehension

## Executive Summary
This survey provides the first comprehensive review of Vibe Coding, a novel software development paradigm enabled by large language models (LLMs) where developers validate AI-generated code through outcome observation rather than line-by-line comprehension. The authors formalize Vibe Coding as a Constrained Markov Decision Process capturing the dynamic relationship among human developers, software projects, and coding agents. Through systematic analysis of over 1000 research papers, they establish a theoretical foundation and practical framework for this emerging field.

The work synthesizes existing practices into five distinct development models - Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models - providing the first comprehensive taxonomy in this domain. Critically, the analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.

## Method Summary
The authors conducted a systematic review of over 1000 research papers covering LLMs, coding agents, and development environments. They synthesized existing literature to establish a theoretical foundation and practical framework for Vibe Coding. The method involved categorizing different approaches into five development models and formalizing the developer-project-agent interaction as a Constrained Markov Decision Process (CMDP). The survey also provides a curated resource list and GitHub repository containing practical implementations and tools mentioned in the paper.

## Key Results
- Formalizes Vibe Coding as a Constrained Markov Decision Process capturing developer-project-agent dynamics
- Establishes the first comprehensive taxonomy of five development models (Unconstrained Automation, Iterative Conversational, Planning-Driven, Test-Driven, Context-Enhanced)
- Identifies that successful Vibe Coding requires systematic context engineering and collaborative development models beyond just agent capabilities
- Provides practical framework for researchers and practitioners navigating AI-augmented software engineering

## Why This Works (Mechanism)
Vibe Coding works by shifting from traditional line-by-line code comprehension to outcome-based validation, leveraging LLMs' ability to generate functional code while humans focus on high-level validation and direction. The Constrained MDP formalization captures how developers, projects, and coding agents interact dynamically, with rewards based on successful outcomes rather than syntactic correctness. This paradigm shift enables faster development cycles while maintaining quality through observable results rather than detailed code inspection.

## Foundational Learning
1. **Constrained Markov Decision Process (CMDP)** - Why needed: Provides mathematical framework for modeling the triadic relationship between developers, projects, and agents; Quick check: Verify reward function R^H properly captures outcome-based validation success
2. **Context-Enhanced Model (CEM)** - Why needed: Addresses hallucination and dependency issues through RAG-based project context retrieval; Quick check: Test retrieval mechanism c_code/c_know is indexing correct repository state
3. **Development Model Taxonomy** - Why needed: Categorizes diverse approaches into five coherent paradigms for systematic comparison; Quick check: Classify existing tools (Cursor, Claude Code) into appropriate models
4. **Outcome Observation Workflow** - Why needed: Defines validation methodology that prioritizes functional results over code comprehension; Quick check: Implement task with and without outcome-based validation to compare efficiency

## Architecture Onboarding

**Component Map:** Human Developer <-> Development Environment <-> Coding Agent <-> Software Project

**Critical Path:** Human validation of outcomes → Context engineering → Agent generation → Project integration → Outcome observation

**Design Tradeoffs:** Unconstrained automation offers speed but risks cascading errors; Planning-driven provides structure but slower; Context-enhanced requires RAG infrastructure but improves accuracy

**Failure Signatures:** Code passes local tests but fails integration (cascading errors), hallucinated dependencies in complex projects, security vulnerabilities in unconstrained automation

**3 First Experiments:**
1. Validate taxonomy by categorizing Cursor as Iterative Conversational or Context-Enhanced tool
2. Test CMDP formulation by implementing simple coding task scenario and comparing predictions to observed behavior
3. Compare code generation quality with and without RAG-based context retrieval across multiple coding agents

## Open Questions the Paper Calls Out
None

## Limitations
- Literature selection methodology lacks transparency, making exact duplication of scope difficult
- Theoretical CMDP contribution lacks empirical validation and practical simulation code
- Quantitative claims about model performance and adoption rates lack supporting data
- Difficulty verifying "first comprehensive review" claim due to incomplete methodological documentation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Taxonomy of five development models is well-supported and useful | High |
| Identification of key challenges in infrastructure, security, and human-centered design | High |
| Theoretical foundation establishing Vibe Coding as distinct paradigm | Medium |
| Claim about systematic context engineering being critical for success | Medium |
| Specific quantitative performance claims lack substantiation | Low |

## Next Checks

1. **Literature Scope Validation:** Replicate literature review using Awesome-Vibe-Coding resource list to verify taxonomy comprehensiveness and identify gaps
2. **CMDP Applicability Testing:** Implement simple coding task scenario and apply CMDP formulation to predict interactions, comparing against actual observed behaviors
3. **Context-Enhanced Model Benchmarking:** Design experiment comparing code generation quality and development efficiency with and without RAG-based context retrieval across multiple coding agents, measuring technical outcomes and developer satisfaction