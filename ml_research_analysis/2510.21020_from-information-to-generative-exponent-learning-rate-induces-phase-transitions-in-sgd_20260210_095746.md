---
ver: rpa2
title: 'From Information to Generative Exponent: Learning Rate Induces Phase Transitions
  in SGD'
arxiv_id: '2510.21020'
source_url: https://arxiv.org/abs/2510.21020
tags:
- learning
- complexity
- sample
- update
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how learning rate choices affect sample complexity\
  \ in gradient-based algorithms for learning Gaussian single-index models. The authors\
  \ develop a general framework for analyzing online iterative algorithms where the\
  \ update oracle includes both correlational and non-correlational terms modulated\
  \ by a learning rate \u03B7."
---

# From Information to Generative Exponent: Learning Rate Induces Phase Transitions in SGD

## Quick Facts
- **arXiv ID**: 2510.21020
- **Source URL**: https://arxiv.org/abs/2510.21020
- **Reference count**: 40
- **Primary result**: Learning rate choices induce phase transitions in sample complexity for gradient-based learning of Gaussian single-index models, shifting from information exponent to generative exponent regimes.

## Executive Summary
This paper demonstrates that learning rate selection in gradient-based algorithms for Gaussian single-index models can induce phase transitions in sample complexity. The authors develop a general framework analyzing online iterative algorithms where update oracles contain both correlational and non-correlational terms modulated by learning rate η. They prove that algorithms with non-correlational updates exhibit sharp transitions: below critical η thresholds, sample complexity follows the information exponent p (n=Θ(d^(p-1)∨1)), while above thresholds it follows the generative exponent p* (n=Θ(d^(p*-1)∨1)).

The framework applies to both batch reuse SGD and a novel alternating SGD algorithm that uses different learning rates for first and second layers. Alternating SGD achieves near-linear sample complexity for certain targets without batch reuse. The authors extend this to deeper networks, showing depth D plays an analogous role to degree in batch reuse SGD, with sample complexity controlled by Dth powers of the link function. Experiments validate the predicted phase transitions in η for both algorithms.

## Method Summary
The paper studies learning Gaussian single-index models y = σ*(⟨x,θ*⟩) using gradient-based algorithms where the update oracle includes terms scaled by powers of learning rate η. The key insight is that non-correlational terms (y^k for k>1) introduce higher-order structure that can probe the generative exponent p* rather than just the information exponent p. The authors develop a unified framework analyzing algorithms with update function ψ_η(y,z) containing η-dependent terms, proving phase transitions occur when η crosses critical thresholds determined by Hermite coefficient alignments. They analyze three algorithms: online SGD (information exponent only), batch reuse SGD (two-step updates with η creating non-correlational structure), and alternating SGD (layer-wise training with different η values). Experiments on noiseless single-index models with He₃ targets validate the predicted phase transitions across different η values and dimensions.

## Key Results
- Sample complexity exhibits phase transitions as learning rate η crosses critical thresholds
- Non-correlational updates enable access to generative exponent rather than information exponent
- Layer-wise two-timescale training (alternating SGD) achieves generative exponent complexity without batch reuse
- Depth D in deep alternating SGD plays an analogous role to degree in batch reuse SGD

## Why This Works (Mechanism)

### Mechanism 1
Sample complexity exhibits phase transitions as learning rate η crosses critical thresholds. The update function ψ_η(y,z) contains terms scaled by powers of η. The coefficient μ_i(η) determines which term dominates alignment dynamics. When η is small, lower-order (correlational) terms dominate with complexity Θ(d^(p-1)). When η exceeds threshold, higher-order terms scale sufficiently to dominate, yielding Θ(d^(p*-1)) complexity where p* ≤ p is the generative exponent. The coefficient μ_i(η) are non-decreasing in η and polynomial in η.

### Mechanism 2
Non-correlational updates (terms with y^k, k>1) enable access to generative exponent rather than information exponent. Correlational queries only access E[y·h(x)] which probes first-order Hermite coefficients. Non-correlational terms introduce powers y^k that, when expanded via Hermite polynomials, probe higher-order structure through products He_i(a)He_{i-1}(b). This allows detecting structure at exponent p* < p when the target's lower powers have non-zero coefficients. The target σ* is polynomial (or has polynomial growth) and there exists some power I such that IE(σ^I_*) = p*.

### Mechanism 3
Layer-wise two-timescale training (alternating SGD) achieves generative exponent complexity without batch reuse. First update second layer weights with learning rate η (producing ã = a + ηyσ(⟨x,w⟩)), then use ã in first-layer update. This implicitly creates y² terms in the first-layer gradient. The effective oracle becomes ψ(y,z) = yaσ'(z) + ηy²σ(z)σ'(z), introducing non-correlational structure controlled by η. The squared target σ*² has lower information exponent than σ* (i.e., p₂ < p), and Hermite coefficient condition μ_{p₂} > 0 holds.

## Foundational Learning

- **Concept: Hermite Polynomials & Information Exponent**
  - Why needed here: The entire framework uses Hermite expansions to decompose update functions; information exponent p is defined as the first non-zero Hermite coefficient
  - Quick check question: Given a function σ(z) = z³ - 3z (Hermite polynomial He₃), what is its information exponent?

- **Concept: Single-Index Models**
  - Why needed here: The paper specifically analyzes y = σ*(⟨x,θ*⟩) + noise; weak recovery means finding θ* direction
  - Quick check question: Why is weak recovery (correlation ≳ 1/polylog d) the computational bottleneck rather than strong recovery?

- **Concept: Correlational Statistical Query (CSQ) vs. Statistical Query (SQ)**
  - Why needed here: The paper frames the improvement as moving from CSQ-limited (information exponent) to full SQ (generative exponent)
  - Quick check question: What type of query does vanilla online SGD implement, and why does batch reuse change this?

## Architecture Onboarding

- **Component map:**
  GenericOnlineAlgorithm -> BatchReuseSGD -> AlternatingSGD -> DeepAlternatingSGD

- **Critical path:**
  1. Compute μ_i(η) coefficients for your specific σ*, σ pair
  2. Identify phase transition thresholds where μ_i^{-1}(η)d^{(i-2)/2} terms cross
  3. Set η large enough to be in generative exponent regime but small enough for stability (η ≍ d^{-1} for batch reuse, η ≍ d^{-1/2} for alternating)
  4. Set γ ≍ max_i μ_i(η)d^{-(i/2∨1)} for convergence

- **Design tradeoffs:**
  - Larger η → better sample complexity but higher instability risk
  - Batch reuse: requires degree r ≥ I to fully leverage generative exponent, but more stable
  - Alternating SGD: simpler, but only reduces to p₂ (square of target) not arbitrary p*
  - Depth D (deep alternating): analogous to degree in batch reuse, but requires sparse connectivity and specific activation choices

- **Failure signatures:**
  - Sample complexity doesn't improve beyond Θ(d^{p-1}) → η too small (still in information exponent regime)
  - Training diverges → η or γ too large, violating η ≍ d^{-1} or γ constraints
  - No weak recovery despite many iterations → Assumption 3.2 violated (μ_{i*} ≤ 0); check Hermite coefficient alignment between σ and σ*

- **First 3 experiments:**
  1. Replicate Figure 1: Train alternating SGD on σ* = He₃ with d ∈ {25,50,75}, sweep η ∈ [10⁻³, 1], plot alignment achieved vs. (η, n). Verify phase transition at η ≍ d^{-1/2}.
  2. Ablation on target exponent: Compare alternating SGD on σ* = He₂ (p=2, p₂=1 or 2) vs. He₄ (p=4, p₂=2). Confirm different phase transition locations per Corollary 4.3.
  3. Boundary test: Set η just below phase transition threshold (e.g., η = 0.5d^{-1/2} for He₃). Verify sample complexity matches online SGD (Θ(d²)) not generative regime (Θ(d)).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the phase transition framework be extended to multi-index models with more than one latent projection direction?
- Basis in paper: From the conclusion: "Other natural directions for future work include an extension of our framework to multi-index models..."
- Why unresolved: The analysis fundamentally relies on tracking alignment with a single direction θ*, and multi-index models introduce subspace recovery challenges with non-trivial interactions between directions.
- What evidence would resolve it: A generalization of the µi coefficients to capture multiple directions and proof that similar learning rate-induced transitions exist in the subspace recovery setting.

### Open Question 2
- Question: How does the sample complexity change for non-polynomial activation functions (e.g., ReLU, sigmoid)?
- Basis in paper: From the conclusion: "...non-polynomial activation functions..." listed as a future direction. The current analysis (Assumption 3.1) requires ψη to be a polynomial of degree r = Θ(1).
- Why unresolved: Non-polynomial activations lead to infinite Hermite expansions, making the truncation arguments and concentration bounds in the current proof strategy inapplicable.
- What evidence would resolve it: Extension of Theorem 3.3 to handle activations with controlled Hermite decay, or proof that specific non-polynomial activations admit analogous phase transitions.

### Open Question 3
- Question: Can the framework be adapted to general spherically symmetric or structured input distributions beyond standard Gaussian?
- Basis in paper: Listed in conclusion as "...more general input distributions..." and referenced [JKMS25] which studies single-index models with spherically symmetric distributions.
- Why unresolved: The proofs rely heavily on Gaussian-specific tools: Stein's lemma, exact Hermite coefficient orthogonality, and specific norm concentration properties.
- What evidence would resolve it: Identification of analogous spectral quantities to information/generative exponents for alternative distributions, with provable phase transitions.

### Open Question 4
- Question: Is there a universally optimal or adaptive activation choice for deep alternating SGD that achieves near-linear sample complexity without target-specific tuning?
- Basis in paper: Appendix C.4 notes the activation choice depends on target structure: "we cannot assume to know the target a priori, and a more generally applicable choice of σ is preferable."
- Why unresolved: The current deep analysis requires assumptions on Hermite coefficients that hold only for specific (σ, σ*) pairs, suggesting target-dependent design.
- What evidence would resolve it: A randomized activation construction or universal parameterization that guarantees the required Hermite coefficient conditions for all polynomial targets.

## Limitations
- The phase transition phenomenon is rigorously proven only for single-index models with Gaussian covariates; extension to multi-layer networks relies on sparse connectivity assumptions
- The framework depends critically on Assumption 3.2 (existence of positive μ_i(η) coefficients), which may fail for arbitrary activation functions
- Experimental validation is limited to noiseless settings with specific dimensions and doesn't fully explore the parameter space or verify batch reuse SGD phase transitions

## Confidence

**High Confidence**: The core theoretical framework for analyzing learning-rate-induced phase transitions in single-index models is mathematically rigorous. The Hermite expansion approach and the characterization of μ_i(η) coefficients are well-defined. The distinction between correlational and non-correlational updates is clearly articulated.

**Medium Confidence**: The extension from single-index to deep networks via alternating updates is plausible but relies on assumptions about sparse connectivity and specific activation functions. The experimental validation, while showing phase transitions, is limited in scope and doesn't fully explore the parameter space.

**Low Confidence**: Claims about achieving "near-linear" sample complexity for arbitrary target functions through deep alternating updates lack comprehensive experimental support. The practical significance of these improvements for real-world architectures remains unclear.

## Next Checks
1. **Phase Transition Boundary Verification**: For alternating SGD on He₃ targets with d=50, systematically sweep η around the predicted threshold η ≍ d^(-1/2) ≈ 0.14. Plot alignment vs. η and n to precisely identify the phase transition boundary and verify it matches Corollary 4.3 predictions.

2. **Batch Reuse SGD Validation**: Implement Algorithm 1 for batch reuse SGD with σ*=He₃, σ=He₃. Verify the predicted phase transition at η ≍ d^(-1) by sweeping η ∈ [10⁻⁴, 10⁻¹] for d ∈ {25, 50, 75}. Compare sample complexity against online SGD baseline.

3. **Activation Function Robustness**: Test alternating SGD with non-polynomial activations (e.g., ReLU, tanh) on simple single-index models. Characterize when Assumption 3.2 fails by computing μ_i(η) coefficients numerically and determining if phase transitions persist.