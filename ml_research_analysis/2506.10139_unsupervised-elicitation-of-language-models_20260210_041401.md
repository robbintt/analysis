---
ver: rpa2
title: Unsupervised Elicitation of Language Models
arxiv_id: '2506.10139'
source_url: https://arxiv.org/abs/2506.10139
tags:
- labels
- arxiv
- human
- unsupervised
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Internal Coherence Maximization (ICM), a
  novel unsupervised method to elicit specific capabilities from pretrained language
  models without relying on external supervision. The core idea is to fine-tune models
  on their own generated labels, optimizing for mutual predictability (how well each
  label can be inferred from others) and logical consistency (avoiding degenerate
  solutions).
---

# Unsupervised Elicitation of Language Models

## Quick Facts
- arXiv ID: 2506.10139
- Source URL: https://arxiv.org/abs/2506.10139
- Authors: Jiaxin Wen, Zachary Ankner, Arushi Somani, Peter Hase, Samuel Marks, Jacob Goldman-Wetzler, Linda Petrini, Henry Sleight, Collin Burns, He He, Shi Feng, Ethan Perez, Jan Leike
- Reference count: 39
- Key outcome: ICM matches or exceeds models trained with golden labels on NLP tasks, significantly outperforming human-labeled models

## Executive Summary
This paper introduces Internal Coherence Maximization (ICM), a novel unsupervised method to elicit specific capabilities from pretrained language models without relying on external supervision. The core idea is to fine-tune models on their own generated labels, optimizing for mutual predictability and logical consistency. ICM uses a simulated annealing-inspired search algorithm to find high-quality label sets. On standard NLP tasks like GSM8K-verification, TruthfulQA, and Alpaca reward modeling, ICM matches or exceeds performance of models trained with golden labels and significantly outperforms those trained with crowdsourced human labels.

## Method Summary
ICM fine-tunes pretrained language models using labels the model generates itself through an iterative search process. The method maximizes a scoring function U(D) that combines mutual predictability (how well each label can be inferred from others) and logical consistency (avoiding degenerate solutions). The search uses simulated annealing with temperature-scheduled acceptance of label changes. The approach works by finding label sets that reflect coherent concepts already represented in the pretrained model, then fine-tuning on these self-generated labels.

## Key Results
- On GSM8K-verification, TruthfulQA, and Alpaca reward modeling, ICM matches or exceeds performance of models trained with golden labels
- ICM significantly outperforms models trained with crowdsourced human labels across all tested benchmarks
- Remarkably, ICM matches golden supervision on tasks where LMs are strongly superhuman, such as author gender prediction
- In production, ICM-trained reward models enabled training a Claude 4 Sonnet-based assistant that matches its human-supervised counterpart on average

## Why This Works (Mechanism)

### Mechanism 1: Mutual Predictability as Concept Elicitation
The method assigns labels that maximize each label's probability conditioned on all other labels, encouraging the label set to reflect a single coherent concept already represented in the pretrained model. For each example, ICM computes P_θ(y_i | x_i, D \ (x_i, y_i)) via in-context learning, summing log probabilities across all examples. High scores indicate labels that "hang together" under the model's existing concept representations. This assumes the pretrained model has already encoded task-relevant concepts in its representations, and these concepts are salient enough to be recovered without external signals.

### Mechanism 2: Logical Consistency as Regularization
Simple logical constraints prevent degenerate solutions (e.g., assigning identical labels to all data points) that would otherwise maximize mutual predictability trivially. A consistency function c(x_i, y_i, x_j, y_j) ∈ {0,1} flags inconsistent label pairs. The scoring function U(D) = α·P_θ(D) - I(D) penalizes inconsistencies, where I(D) counts violations. This assumes task-relevant concepts admit simple logical constraints (asymmetry: "A > B contradicts B > A"; mutual exclusivity: "two different answers cannot both be true").

### Mechanism 3: Simulated Annealing Search for Approximate Optimization
Finding globally optimal labels is computationally intractable; simulated annealing provides tractable approximation with temperature-scheduled acceptance of label changes. The algorithm initializes with K random labels (K≈8 works well), then iteratively: (1) sample an example, (2) propose a label, (3) fix inconsistencies via Algorithm 2, (4) accept if U(D') > U(D) or probabilistically accept if worse (with probability decreasing over iterations via temperature decay). This assumes the search landscape is sufficiently smooth that local improvements compound toward high-quality global solutions.

## Foundational Learning

- **Concept: In-Context Learning (ICL) for Conditional Probability Estimation**
  - Why needed: ICM uses ICL to compute P_θ(y_i | x_i, D \ {(x_i, y_i)})—the probability of each label given all other labeled examples in context.
  - Quick check: Can you explain why ICL allows measuring "how well the model can infer each label from others" without gradient updates?

- **Concept: Simulated Annealing**
  - Why needed: The search algorithm uses temperature-based probabilistic acceptance to escape local optima when searching for high-scoring label sets.
  - Quick check: Why does the algorithm accept label changes that *decrease* U(D) with some probability, and why does this probability decrease over time?

- **Concept: Weak-to-Strong Generalization (conceptual background)**
  - Why needed: ICM can be viewed as a limiting case where "weak supervision" is replaced entirely by self-consistency—relevant for understanding why this might scale to superhuman capabilities.
  - Quick check: How does ICM differ from typical weak-to-strong generalization where a weaker model provides supervision?

## Architecture Onboarding

- **Component map:**
  Unlabeled Data D_unlabel → [Initialization: K random labels] → [Iterative Search Loop] → [Label Proposal] → [Consistency Fix] → [Scoring: U(D) = α·P_θ(D) - I(D)] → [Accept/Reject via Annealing] → Final Labels → [Fine-tune model on self-generated labels]

- **Critical path:** The consistency-fix step (Algorithm 2) is critical—without it, the algorithm produces substantial inconsistencies even though U(D) penalizes them. Algorithm 2 actively resolves inconsistencies by selecting label combinations that maximize U(D).

- **Design tradeoffs:**
  - K (initialization size): Too large → noise hinders convergence; too small → insufficient context for ICL. Paper finds K=8 works well.
  - α (mutual predictability weight): Higher α improves label quality but may cause excessive rejection. Paper uses α=50, adjusting down (20-30) if search stalls.
  - Context window vs. scalable variant: Algorithm 1 uses ICL (limited by context); Algorithm 3 (Appendix G) uses fine-tuning with fold-based cross-validation for production scale.

- **Failure signatures:**
  - Concept not salient: Zero-shot and ICM both perform at random (Figure 5, poem task).
  - Degenerate solution: All labels identical → likely missing logical consistency enforcement or α too low.
  - Slow convergence: Temperature decay too aggressive or α too restrictive; adjust α downward.
  - High variance across runs: Likely initialization sensitivity; consider multiple restarts.

- **First 3 experiments:**
  1. Replicate Figure 3 on a single benchmark (e.g., TruthfulQA) with Llama 8B: Compare zero-shot, ICM, golden labels, human labels. Verify ICM approaches golden-label performance.
  2. Ablate logical consistency: Run ICM with I(D)=0 on Alpaca and TruthfulQA. Confirm increased variance and potential collapse (per Figure 10).
  3. Test concept saliency failure: Construct a synthetic task with arbitrary labeling logic (like the "sun poems" task) to verify ICM performs at random when the concept is not in the pretrained model's representations.

## Open Questions the Paper Calls Out

### Open Question 1
Can we develop methods to determine a priori whether a concept is "salient" enough in a pretrained model for ICM to successfully elicit it? The paper demonstrates ICM fails when concepts are not salient but provides no predictive framework for identifying salient vs. non-salient concepts before running ICM.

### Open Question 2
To what extent does data contamination in pretraining corpora contribute to ICM's strong performance on benchmark tasks? The authors acknowledge this concern but cannot directly verify it as Llama pre-training corpus is not accessible.

### Open Question 3
Why does the unsupervised assistant underperform on math and coding tasks despite matching human supervision on average? This is presented as a hypothesis; the paper does not investigate whether this is an inherent limitation of unsupervised methods on formal reasoning domains or a tractable problem.

### Open Question 4
What implicit consistency constraints does mutual predictability enforce beyond simple axiomatic logical checks? The paper demonstrates mutual predictability's importance empirically but does not characterize what implicit constraints it imposes or why it yields correct labels.

## Limitations
- The approach fundamentally depends on the pretrained model having already encoded the target concept in its representations, making it inapplicable to arbitrary preferences or concepts outside the model's training distribution.
- The simulated annealing search provides only approximate optimization without guarantees of global optimality, and the algorithm's performance could degrade with poor initialization or complex consistency structures.
- For production scale, the paper mentions a fold-based variant (Algorithm 3) but provides limited details on the number of folds and iterations required.

## Confidence
- **High confidence:** ICM's ability to match golden supervision on GSM8K-verification and TruthfulQA with Llama 3.1 models; the mechanism of mutual predictability + logical consistency as a coherent framework; the empirical finding that ICM fails when concepts are not salient to the pretrained model.
- **Medium confidence:** Claims about ICM matching human labels in production settings (assistant training), as these results depend on additional infrastructure not fully detailed; scalability claims for Algorithm 3 without specified hyperparameters.
- **Low confidence:** The claim that ICM "significantly outperforms" human labels on GSM8K-verification and Alpaca, as the comparison may depend on specific human annotation quality and task framing.

## Next Checks
1. Implement Algorithm 3 with explicit fold count and iteration parameters, then validate on a production-scale dataset to verify the claimed scalability improvements.
2. Systematically test ICM on tasks with varying degrees of concept saliency (including arbitrary concepts like "sun poems") to precisely characterize the boundary between successful and failed elicitation.
3. Compare ICM's search convergence across different initialization strategies (random, informed, worst-case) with quantitative analysis of final label quality distributions.