---
ver: rpa2
title: 'LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard'
arxiv_id: '2504.13125'
source_url: https://arxiv.org/abs/2504.13125
tags:
- financial
- data
- tasks
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper fine-tuned large language models for financial tasks
  using supervised fine-tuning, direct preference optimization, and reinforcement
  learning. The models were trained on 28 of 41 available datasets from the Open FinLLM
  Leaderboard, with synthetic data generation for tasks lacking training data.
---

# LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard

## Quick Facts
- arXiv ID: 2504.13125
- Source URL: https://arxiv.org/abs/2504.13125
- Authors: Varun Rao; Youran Sun; Mahendra Kumar; Tejas Mutneja; Agastya Mukherjee; Haizhao Yang
- Reference count: 20
- Key outcome: Fine-tuned LLMs achieved significant performance gains on financial tasks, with F1 scores improving from ~0.14 to ~0.72 on named entity recognition

## Executive Summary
This paper presents a comprehensive fine-tuning approach for large language models on financial tasks using the Open FinLLM Leaderboard framework. The authors employ a sequential pipeline of supervised fine-tuning, direct preference optimization, and reinforcement learning with synthetic data to improve model performance across 24 financial tasks. The work demonstrates substantial improvements over baseline models, particularly in named entity recognition and sentiment analysis, while also investigating the complex dynamics of task transfer learning in the financial domain.

## Method Summary
The authors fine-tuned Qwen2.5-1.5B-Instruct and DeepSeek-R1-1.5B models using a three-stage pipeline: supervised fine-tuning (SFT) on 28 available financial datasets, direct preference optimization (DPO) to reduce repetitive outputs, and reinforcement learning with LLM-synthesized data for tasks lacking labeled training data. SFT used LoRA with rank 128 and learning rate 5e-5, while DPO employed rank 16 parameters to control generation behavior. For tasks without training data, the authors implemented a self-distillation approach using chain-of-thought reasoning and regex-based answer extraction.

## Key Results
- SFT improved F1 scores up to 0.7231 on named entity recognition tasks
- DPO reduced repetitive outputs from ~55% to ~2% while maintaining or improving task accuracy
- Data scaling law exponent of 0.28 measured and found consistent with prior literature across domains
- DeepSeek-R1-1.5B model showed better generalization than Qwen2.5-1.5B-Instruct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning on domain-specific financial data substantially improves task performance, with observed F1 gains from ~0.14 to ~0.72 on named entity recognition.
- Mechanism: Cross-entropy loss optimization on financial query-answer pairs enables the model to shift token probability mass toward domain-specific patterns. Transfer learning occurs when task structures overlap.
- Core assumption: Base model possesses sufficient foundational language understanding; fine-tuning primarily re-weights existing representations.
- Evidence anchors: Abstract reports SFT improving F1 scores up to 0.7231; Table I shows DeepSeek-R1-1.5B NER F1 improvement from 0.1448 to 0.7231.

### Mechanism 2
- Claim: Direct preference optimization applied after SFT reduces repetitive/overlong outputs from ~55% to ~2% while preserving or improving task accuracy.
- Mechanism: DPO implicitly optimizes a reward function by comparing accepted labels against rejected labels. The loss pushes the model toward appropriate stopping behavior without requiring explicit reward modeling.
- Core assumption: SFT model has learned correct answer content but lacks appropriate stopping behavior; overlength outputs stem from uncertainty about generation termination.
- Evidence anchors: Section IV.b, Table II shows overlength ratio dropping from 54.7% to 1.7% across DPO iterations; CC F1 improved from 0.3962 to 0.5592.

### Mechanism 3
- Claim: Reinforcement learning with LLM-synthesized training data improves performance on tasks lacking labeled datasets, with gains ranging from +3.4% to +87.1% across tested tasks.
- Mechanism: Self-distillation loop where current model generates chain-of-thought annotations, regex extraction yields structured pairs, and SFT/DPO produces next iteration model.
- Core assumption: Base model's CoT reasoning produces extractable correct answers at sufficient rate for positive training signal.
- Evidence anchors: Section IV.c, Table III shows MultiFin +87.1%, FOMC +22.5%, TSA +3.4% improvement after one RL iteration.

## Foundational Learning

- Concept: **Scaling Laws in LLMs**
  - Why needed here: The paper reports a data scaling exponent of 0.28 for financial tasks, enabling practitioners to predict returns from additional data investment.
  - Quick check question: Given the formula (1 - F1) ∝ d^(-0.28), approximately how much would you need to increase dataset size to halve the error rate?

- Concept: **Transfer Learning and Negative Transfer**
  - Why needed here: Table I shows task interactions are unpredictable—FiQASA → FPB helps (both sentiment) but NER → CC hurts (entity focus vs. causal reasoning).
  - Quick check question: Before multi-task fine-tuning, how would you assess whether two financial tasks share compatible structural priors?

- Concept: **Preference Optimization Objective**
  - Why needed here: Understanding DPO as implicit reward learning explains why it fixes generation behavior without requiring a separate reward model or RLHF pipeline.
  - Quick check question: What is the role of the reference model π_ref in DPO, and what happens if β (temperature) is set too high?

## Architecture Onboarding

- Component map:
Base Model (Qwen2.5-1.5B / DeepSeek-R1-1.5B)
    ↓
SFT Stage (LoRA r=128, α=256, lr=5e-5, batch=64)
    → Domain adaptation, task learning
    ↓
DPO Stage (LoRA r=16, α=32, β=1)
    → Output quality control, repetition reduction
    ↓
[Optional] RL Iteration with Synthetic Data
    → Coverage for unlabeled tasks

- Critical path:
  1. Data preparation: 28/41 datasets have training data; remaining require synthesis via CoT prompting + regex extraction
  2. SFT first: Always initial stage for alignment and task adaptation
  3. DPO second: Applied only if overlength behavior observed
  4. RL optional: For tasks without ground-truth labels

- Design tradeoffs:
  - Sequential vs. joint multi-task SFT: Authors found sequential fine-tuning did not outperform direct joint training
  - LoRA rank selection: High rank (128) for SFT to capture domain knowledge; low rank (16) for DPO to prevent overfitting to preference data
  - Model scale: 1.5B models chosen for computational feasibility (3-5 hours on 4×2080Ti); larger models unexplored

- Failure signatures:
  - Overlength outputs (>50% of samples): Indicates SFT learned answer tokens without stopping behavior → apply DPO
  - Negative transfer on related tasks: Training on Task A degrades Task B → check structural overlap before multi-task training
  - Synthetic data quality issues: RL iteration shows diminishing returns or performance collapse → validate extraction regex, sample outputs manually

- First 3 experiments:
  1. Single-task SFT baseline: Fine-tune on one dataset (e.g., NER), evaluate on held-out test and at least one related task (e.g., CC) to measure transfer effects.
  2. DPO ablation: After SFT, measure overlength ratio; apply one DPO iteration with rejected labels = overlong outputs, confirm ratio drops and task F1 is preserved.
  3. Scaling law validation: Train with 25%, 50%, 100% of aggregated financial data; plot (1 - F1) vs. data fraction on log-log axes to confirm exponent ≈0.28.

## Open Questions the Paper Calls Out
None

## Limitations

- Task interaction complexity: Transfer effects between financial tasks are unpredictable, with some task pairs showing positive transfer while others show severe negative transfer despite apparent similarity.
- Synthetic data quality control: The RL iteration with LLM-synthesized data lacks ground truth validation, risking systematic error amplification through iterative self-training.
- Limited model scale exploration: All experiments use 1.5B parameter models due to computational constraints, leaving questions about whether findings generalize to larger models.

## Confidence

**High confidence**: The core mechanism of supervised fine-tuning improving task performance is well-supported by direct measurements showing F1 improvements from ~0.14 to ~0.72 on NER. The data scaling law exponent of 0.28 is measured with sufficient granularity.

**Medium confidence**: The DPO mechanism for reducing repetitive outputs is supported by before/after measurements, but the lack of corpus evidence on DPO for repetition in financial LLMs suggests this may be an understudied area.

**Low confidence**: The synthetic data generation approach shows promising improvements but lacks ground truth validation, making it difficult to quantify the noise introduced by extraction errors.

## Next Checks

1. **Transfer predictability experiment**: Design a controlled study testing task pairs with varying degrees of structural similarity while holding dataset size and model architecture constant. Measure transfer effects and attempt to correlate them with task feature overlap metrics to develop predictive criteria.

2. **Synthetic data validation pipeline**: Implement a human-in-the-loop validation step for a subset of synthetically generated labels. Compare model performance trained on validated vs. unvalidated synthetic data to quantify the noise introduced by extraction errors and establish quality thresholds for reliable self-training.

3. **Scaling law cross-validation**: Replicate the data scaling experiments using the same methodology but with larger base models (7B-70B range). Plot (1-F1) vs. data fraction on log-log scales to verify whether the exponent remains 0.28 or changes systematically with model scale.