---
ver: rpa2
title: 'DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions'
arxiv_id: '2505.05091'
source_url: https://arxiv.org/abs/2505.05091
tags:
- severity
- kitti2015
- flyingthings3d
- corruptions
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DISP BENCH, the first standardized benchmarking
  tool for evaluating the robustness of disparity estimation methods against synthetic
  image corruptions. The tool assesses performance on two datasets (FlyingThings3D
  and KITTI2015) under adversarial attacks (FGSM, BIM, PGD, APGD, CosPGD) and 15 common
  2D image corruptions across five severity levels.
---

# DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions

## Quick Facts
- arXiv ID: 2505.05091
- Source URL: https://arxiv.org/abs/2505.05091
- Reference count: 40
- Introduces the first standardized benchmarking tool for disparity estimation robustness against synthetic image corruptions

## Executive Summary
This paper introduces DISP BENCH, the first standardized benchmarking tool for evaluating the robustness of disparity estimation methods against synthetic image corruptions. The tool assesses performance on two datasets (FlyingThings3D and KITTI2015) under adversarial attacks (FGSM, BIM, PGD, APGD, CosPGD) and 15 common 2D image corruptions across five severity levels. Experiments with four disparity estimation architectures (CFNet, GWCNet-G, STTR, STTR-light) reveal that while newer transformer-based models (STTR, STTR-light) outperform older CNN-based models (CFNet, GWCNet-G) on clean data, they generalize significantly worse to common corruptions, particularly weather-related ones like fog and frost. The analysis further shows that synthetic corruptions on synthetic datasets (FlyingThings3D) do not reliably predict performance on real-world datasets (KITTI2015), indicating the necessity of testing on real-world data for safety-critical applications. DISP BENCH is open-sourced to enable broader research into robust and generalizable disparity estimation methods.

## Method Summary
DISP BENCH evaluates disparity estimation robustness using pre-trained models (CFNet, GWCNet-G, STTR, STTR-light) on FlyingThings3D (synthetic) and KITTI2015 (real-world) datasets. The framework tests five adversarial attack types (FGSM, BIM, PGD, APGD, CosPGD) with epsilon=8/255, alpha=0.01, 20 iterations under L-infinity norm, plus 15 common 2D image corruptions (noise, blur, weather, digital distortions) at five severity levels. Evaluation uses mean End-Point-Error (EPE) as the primary metric, with mC-EPE representing mean EPE across all corruptions at a given severity level.

## Key Results
- Transformer-based models (STTR, STTR-light) outperform CNN-based models (CFNet, GWCNet-G) on clean data but generalize significantly worse to common corruptions, particularly weather-related ones like fog and frost
- Synthetic corruptions on synthetic datasets (FlyingThings3D) do not reliably predict performance on real-world datasets (KITTI2015), necessitating real-world evaluation for safety-critical applications
- All four methods show similar robustness to digital corruptions but extreme sensitivity to additive noise, with transformers showing particularly poor weather corruption robustness

## Why This Works (Mechanism)

### Mechanism 1
Transformer-based disparity estimation architectures may achieve superior i.i.d. performance while exhibiting degraded robustness to common corruptions, particularly weather-related ones, compared to older CNN-based architectures. Transformers replace cost volume construction with dense pixel matching using attention mechanisms. While this improves clean-data correspondence matching, attention-based matching appears more sensitive to distribution shifts that alter local texture and appearance patterns (fog, frost, snow) compared to group-wise correlation or cascaded cost volume approaches. The performance gap reflects fundamental architectural sensitivity differences rather than training data artifacts or checkpoint quality variations. Experiments show STTR and STTR-light are significantly more non-robust under weather corruptions while outperforming on clean data.

### Mechanism 2
Corruption robustness evaluated on synthetic datasets does not reliably predict corruption robustness on real-world datasets for the same architecture. Synthetic datasets (e.g., FlyingThings3D) lack the complexity of real-world imaging pipelines (sensor characteristics, lighting variations, material properties). Models may learn synthetic-specific shortcuts that transfer poorly when the same corruption types are applied to real-world imagery with different underlying statistics. The correlation gap reflects dataset characteristics rather than evaluation methodology artifacts. Figure 6 shows no correlation between mC-EPE on FlyingThings3D vs. KITTI2015 across all severity levels, indicating synthetic corruptions on synthetic datasets do not represent synthetic corruptions on real-world datasets.

### Mechanism 3
Synthetic 2D Common Corruptions applied to real-world datasets can serve as a proxy for evaluating real-world domain shift robustness. Common corruptions (noise, blur, weather effects, digital distortions) simulate distribution shifts encountered in deployment. When applied to real-world imagery, they approximate the joint distribution of (real_image, real_corruption) better than (synthetic_image, synthetic_corruption) pairs. The correlation between synthetic corruption performance and real-world adverse condition performance, established for semantic segmentation, transfers to disparity estimation. Cites Agnihotri et al. [7]: "very strong positive correlation" (Pearson r=0.759) between mean mIoU on ACDC (real adverse conditions) and mean mIoU on 2D Common Corruptions over Cityscapes for semantic segmentation.

## Foundational Learning

- **Disparity Estimation (Stereo Matching)**: Why needed here: Core task being benchmarked; understanding that disparity measures pixel offset between stereo pairs for depth inference is essential for interpreting EPE metrics and cost volume architectures. Quick check question: Given a stereo pair, what does a disparity value of 50 pixels at location (x,y) in the left image indicate about the corresponding point in the right image?

- **Out-of-Distribution (OOD) Robustness**: Why needed here: The benchmark specifically targets OOD evaluation via common corruptions and adversarial attacks; distinguishes from standard i.i.d. benchmarking. Quick check question: If a model achieves 2.0 EPE on clean KITTI2015 but 15.0 EPE on KITTI2015 with fog corruption (severity 3), what does this gap indicate about the model's deployment readiness?

- **Adversarial Attacks (White-Box)**: Why needed here: DispBench evaluates reliability via five attack types (FGSM, BIM, PGD, APGD, CosPGD); understanding perturbation budgets (ε), iteration counts, and threat models is required to configure evaluations. Quick check question: What is the difference between FGSM (single-step) and PGD (multi-step with random initialization) in terms of attack strength and computational cost?

- **Cost Volume Architectures**: Why needed here: The benchmarked architectures differ fundamentally in cost volume construction (group-wise correlation vs. cascaded fusion vs. transformer attention); this affects both accuracy and robustness characteristics. Quick check question: How does GWCNet's group-wise correlation cost volume differ from STTR's attention-based dense matching approach?

## Architecture Onboarding

- **Component map:** dispbench.evals.load_model(model_name, dataset) -> dispbench.evals.evaluate(model_name, dataset, retrieve_existing, threat_config)

- **Critical path:**
  1. Load model checkpoint via `load_model()` with model_name and training dataset
  2. Create config.yml specifying threat_model and parameters
  3. Call `evaluate()` with `retrieve_existing=True` to use pre-computed results if available (saves compute)
  4. Results return mean EPE across validation set for specified corruption/attack

- **Design tradeoffs:**
  - Synthetic vs. real dataset for evaluation: Synthetic (FlyingThings3D) enables more pre-trained checkpoints but doesn't predict real-world robustness (per paper findings). Recommendation: Use KITTI2015 for safety-critical assessments despite limited checkpoint availability.
  - Adversarial vs. common corruption evaluation: Adversarial attacks (especially CosPGD) test worst-case reliability; common corruptions test realistic distribution shifts. Both are needed for comprehensive robustness profiling.
  - Retrieve existing vs. fresh evaluation: Pre-computed results save significant compute but may not cover novel model/threat combinations. Fresh evaluation required for new architectures.

- **Failure signatures:**
  - Weather corruption failure: EPE >10× clean performance on fog/frost/snow indicates poor real-world deployment readiness; transformer architectures show this pattern more severely
  - Noise sensitivity: EPE >50 on Gaussian/shot/impulse noise suggests sensor noise vulnerability
  - Adversarial collapse: EPE increasing monotonically with attack iterations without plateau indicates poor representation robustness
  - Synthetic-to-real gap: If model ranks differently on FlyingThings3D vs. KITTI2015 corruptions, synthetic-only evaluation is insufficient

- **First 3 experiments:**
  1. Baseline i.i.d. profiling: Evaluate all four architectures on clean KITTI2015 and FlyingThings3D to establish accuracy baseline and confirm checkpoint quality matches reported performance.
  2. Weather corruption stress test: Apply fog, frost, and snow corruptions at severity levels 1, 3, 5 on KITTI2015 to quantify weather-specific robustness gaps between CNN and transformer architectures.
  3. Adversarial robustness comparison: Run CosPGD attack (20 iterations, ε=8/255, α=0.01, L∞ bound) on GWCNet-G vs. STTR to compare worst-case reliability between architectural paradigms.

## Open Questions the Paper Calls Out

- What specific architectural components cause transformer-based disparity models to suffer lower robustness to weather corruptions compared to older CNN-based models? The authors quantify the performance drop but do not investigate the internal model mechanisms or feature representations responsible for this specific vulnerability to weather shifts.

- Can a modified training regime or corruption protocol be developed to ensure synthetic corruptions on synthetic datasets reliably predict real-world robustness? The paper demonstrates the lack of correlation between FlyingThings3D and KITTI2015 robustness scores but does not propose a method to bridge this domain gap for robustness evaluation.

- Do recent foundational stereo matching models (e.g., StereoAnything) exhibit improved generalization to common corruptions compared to the standard architectures currently benchmarked? The current DispBench analysis is limited to four specific architectures, leaving the robustness of newer foundational models unassessed.

## Limitations
- Cannot directly validate synthetic-to-real correlation gap due to lack of in-the-wild disparity estimation datasets with adverse conditions
- Robustness findings for transformer architectures versus CNN baselines are based on pre-trained checkpoints without examining training procedures or data augmentation strategies
- Exact mechanisms driving transformer vulnerability to distribution shifts are not fully elucidated

## Confidence
- **High Confidence**: Disparity estimation architectures show consistent performance patterns on clean data across both datasets; synthetic corruption methodology is well-established through cross-task validation with semantic segmentation
- **Medium Confidence**: Transformer architectures exhibit systematically worse robustness to weather-related corruptions than CNN architectures; synthetic-to-real correlation gap in corruption robustness reflects fundamental dataset characteristics
- **Low Confidence**: Exact mechanisms driving transformer vulnerability to distribution shifts are not fully elucidated; the specific severity thresholds that define "non-robust" behavior may be dataset-dependent

## Next Checks
1. Apply the DispBench corruption suite to an existing adverse-condition driving dataset (e.g., BDD100K with fog/rain annotations) to test synthetic-to-real correlation empirically
2. Train STTR and CFNet from scratch with identical corruption-aware data augmentation to determine if architectural differences or training strategies drive robustness gaps
3. Apply DispBench methodology to optical flow estimation and semantic segmentation to validate whether transformer vulnerability to weather corruptions is task-specific or represents a broader architectural pattern