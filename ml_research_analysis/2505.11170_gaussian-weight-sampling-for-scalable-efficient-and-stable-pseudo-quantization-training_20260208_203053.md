---
ver: rpa2
title: Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization
  Training
arxiv_id: '2505.11170'
source_url: https://arxiv.org/abs/2505.11170
tags:
- training
- gaussws
- arxiv
- bf16
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient and stable training
  of large language models (LLMs) under limited precision constraints. Current low-precision
  training methods face consistency issues and an exponential search space for stable
  configurations.
---

# Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training

## Quick Facts
- **arXiv ID:** 2505.11170
- **Source URL:** https://arxiv.org/abs/2505.11170
- **Reference count:** 40
- **Primary result:** Proposes GaussWS, achieving stable LLM training at FP6 precision with <2% overhead and 2 bytes/param memory, matching BF16 baselines.

## Executive Summary
This paper tackles the challenge of efficient and stable low-precision training of large language models (LLMs) under memory and compute constraints. Existing fully-quantized training methods suffer from inconsistency and an exponential search space for stable configurations. The authors introduce Gaussian Weight Sampling (GaussWS), a pseudo-quantization training (PQT) method that uses a rounded Gaussian noise distribution to simulate low-precision effects during BF16 training. This approach enables training with effective bitwidths down to FP6, significantly reducing memory footprint while maintaining or exceeding baseline performance.

GaussWS combines three key innovations: a floating-point-friendly noise distribution that enables higher effective precision than uniform noise, efficient bitwise noise generation via integer operations, and stochastic precision annealing that improves stability. Experimental results on GPT2 and Llama2 models (up to 1B parameters and 300B tokens) demonstrate that GaussWS closely matches or exceeds BF16 baseline performance while achieving computational overhead as low as 1.40% and 2 bytes per parameter in GPU memory. The method offers a practical solution for scalable, efficient, and stable low-precision LLM training.

## Method Summary
GaussWS implements pseudo-quantization training by adding rounded Gaussian noise to weights during forward passes, simulating low-precision effects while maintaining BF16 computation. The method uses a 4-bit sign-mantissa format for noise generation via bitwise operations on random integers, avoiding expensive transcendental functions. A learnable bitwidth parameter $b_t$ is scheduled from an initial value (typically 6) to a target (typically 4) during training. The noise is applied block-wise (32x32) with scaling based on maximum absolute weight values. In the backward pass, gradients are computed ignoring the gradient through the max-block operation, which has negligible effect. The method stores noisy weights in BF16 format for simplicity, requiring 2 bytes per parameter.

## Key Results
- Achieves stable training at FP6 precision with computational overhead as low as 1.40% and 2 bytes/param memory
- Matches or exceeds BF16 baseline performance on GPT2-124M (300B tokens) and Llama2-1B (300B tokens)
- Supports effective 9-bit precision on BF16 hardware, compared to 5-bit for uniform noise distributions
- Demonstrates scalability with throughput tests on 70B model using 4 layers

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Precision Annealing via Rounded Normal Noise
The rounded normal distribution $R = \lfloor N(0, 1)/2 \rceil$ enables stable training at low bitwidths by stochastically preserving the precision of small-magnitude weights. Unlike uniform noise, this distribution has a high probability of yielding zero ($Pr(R=0) \approx 0.717$). When applied as noise, it effectively "masks" small weights ($w_{ij} \approx 0$) from quantization-induced underflow with probability $p$, allowing the optimizer to update them in high precision. As training progresses and weights potentially grow or shrink, the noise naturally adapts the effective precision without hard thresholds. The training process benefits from occasionally skipping the perturbation of small weights to maintain gradient flow, rather than enforcing strict low-precision constraints on every parameter at every step.

### Mechanism 2: Floating-Point-Friendly Noise Injection
The specific formulation of the noise injection $\hat{w} = w + R \cdot \text{scale}$ is designed to minimize floating-point underflow during the addition and casting steps, supporting effective bitwidths up to 9-bit on BF16 operators. Standard PQT uses uniform noise, which can underflow when added to a large FP weight, losing information. The rounded normal distribution's minimum non-zero value ($2^{\tau}$) allows for a higher effective bitwidth ($b_t < m + 2 + \tau$) than uniform noise. This ensures the noise actually perturbs the weight value rather than vanishing due to FP precision limits. The underlying hardware operator is BF16 or similar, where mantissa bits ($m$) are the limiting factor for precision during addition.

### Mechanism 3: Efficient Bitwise Noise Synthesis
Generating the rounded normal approximation via bitwise operations significantly reduces computational overhead compared to traditional transcendental function calls. The method constructs the noise distribution using logical AND/OR operations on random integer streams rather than expensive floating-point math. This maps efficiently to GPU hardware, reducing the bottleneck on vector units (CUDA cores) which are typically scarcer than Tensor Cores on datacenter GPUs like the A100. The PRNG produces bits that are sufficiently independent and random to approximate the target distribution statistically.

## Foundational Learning

- **Concept: Pseudo-Quantization Training (PQT) vs. Fully Quantized Training (FQT)**
  - **Why needed here:** GaussWS is a PQT method. You must understand that it simulates quantization noise during training (using BF16 compute) to prepare the model for eventual low-precision deployment or simply to regularize the training, distinct from FQT which executes actual low-precision math.
  - **Quick check question:** Does GaussWS perform matrix multiplications in FP6 during training?

- **Concept: Floating-Point Underflow and Precision**
  - **Why needed here:** The paper's core theoretical contribution relies on how floating-point addition casts values. You need to grasp why adding a small number (noise) to a large number (weight) can result in zero change due to mantissa limits.
  - **Quick check question:** Why does uniform noise limit the effective bitwidth to 5-bit while rounded normal allows 9-bit on the same hardware?

- **Concept: Block-wise Quantization (Microscaling - MX)**
  - **Why needed here:** GaussWS operates on 32x32 blocks of parameters to determine scaling factors. Understanding block-wise scaling is necessary to implement the broadcast and max operations correctly.
  - **Quick check question:** Why does the method use square blocks (32x32) instead of vector-wise (1x32) blocks?

## Architecture Onboarding

- **Component map:** PRNG state -> Noise Generator (Triton Kernel) -> Scaler/Unpacker -> Caster -> Compute (cuBLAS) -> Store $\hat{w}$ in BF16
- **Critical path:** The noise generation and scaling must be fast enough to hide behind memory latency or compute, otherwise the 1.40% overhead target is impossible. The backward pass requires regenerating the exact same noise $R$ to ensure gradient correctness.
- **Design tradeoffs:**
  - Memory vs. Compute: The authors chose to store $\hat{w}$ (2 bytes/param) explicitly rather than regenerating it in the backward pass, simplifying implementation at the cost of memory.
  - Fusion: They deliberately did not fuse the noise addition with the GEMM to maintain modularity and utilize optimized vendor libraries (cuBLAS), accepting a slight potential performance hit for stability and ease of integration.
- **Failure signatures:**
  - **Seed Desync:** If the backward pass cannot reproduce the exact seed state from the forward pass, gradients will be incorrect and training will diverge.
  - **Overflow:** If $b_t$ is set too low for the weight dynamic range (violating Lemma 2), the effective noise will vanish (underflow), removing the regularization benefit and causing oscillations similar to standard FQT.
- **First 3 experiments:**
  1. Unit Test Noise Distribution: Generate 10M samples using the bitwise kernel. Plot histogram vs. theoretical $\lfloor N(0,1)/2 \rceil$ to validate the implementation of Eq. 10.
  2. Overhead Benchmark: Profile a single linear layer (e.g., 4096x4096) with GaussWS vs. BF16 baseline to verify < 2% overhead on target GPU.
  3. Ablation on R: Train a small GPT2-124M for 1B tokens comparing GaussWS vs. DiffQ (Uniform Noise) vs. No Noise to replicate the stability divergence shown in Figure 1b.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Gaussian Weight Sampling (GaussWS) method be effectively extended to quantize activations and gradients during training, rather than being restricted to weights?
- **Basis in paper:** The authors state in the Discussion section: "The proposed method is applied only on weight, leaving activation and gradient same as baseline BF16. In particular, it is impossible to conduct differentiable search on gradient. Extending the proposed method to activation is left as future work."
- **Why unresolved:** The current formulation and efficiency gains rely on the specific numerical behavior of adding noise to weights ($w$); applying similar pseudo-quantization noise (PQN) to activations or gradients introduces different consistency and throughput challenges that were not addressed.
- **What evidence would resolve it:** A modification of the GaussWS algorithm that successfully applies PQN to activations/gradients, demonstrating stable convergence and memory savings comparable to the weight-only implementation on standard LLM benchmarks.

### Open Question 2
- **Question:** Does GaussWS maintain its stability and efficiency advantages when scaling to models significantly larger than 1 billion parameters?
- **Basis in paper:** While the authors validate the method on GPT2 and Llama2 up to 1B parameters, the experimental data for the 70B model (Table 1) is marked with a dagger indicating it used only "4 layers out of total 80" for throughput testing, lacking a full convergence validation at that scale.
- **Why unresolved:** The paper notes that "optimal precision $b_{opt}$ of larger, over-trained models tend to be larger," and the 1B experiments already showed sensitivity to bitwidth settings; it is unclear if the method scales without extensive hyperparameter retuning or numerical instability at 70B+ scale.
- **What evidence would resolve it:** Full pre-training or fine-tuning results for a model with $\geq$7B parameters showing that GaussWS matches BF16 baseline performance without divergence or excessive manual tuning of bitwidth targets.

### Open Question 3
- **Question:** Can the selection of bitwidth hyperparameters ($b_{init}$ and $b_{target}$) be automated to adapt to different model architectures without manual intervention?
- **Basis in paper:** The authors note regarding the Llama2-1B results that "the increase in loss is attributed to the lower bitwidth $b_t$... results can be improved by tuning hyperparameters... as in Figure F.1," implying that the default settings were suboptimal for this specific scale.
- **Why unresolved:** The current method relies on manually set targets for the learnable bitwidth parameter. The paper lacks a theoretical or heuristic rule to predict the optimal precision schedule based on model size or dataset size, necessitating trial-and-error.
- **What evidence would resolve it:** The derivation of a scaling law or an adaptive controller that sets $b_{init}$ and $b_{target}$ dynamically, achieving optimal loss without requiring the manual grid search implied by the comparison between Figure 4 and Figure F.1.

## Limitations

- The method is currently restricted to weight quantization, leaving activation and gradient quantization as future work
- Hyperparameter tuning is required for optimal bitwidth settings, particularly for different model scales
- The specific bitwise logic for noise generation is probabilistically described but not fully implemented in the paper
- Full convergence validation is missing for the largest 70B model scale, with only throughput testing on 4 layers

## Confidence

- **High Confidence:** The stability benefits of the rounded normal noise over uniform noise, the floating-point underflow analysis, and the efficiency gains from bitwise noise generation are well-supported by the theoretical framework and empirical results.
- **Medium Confidence:** The claims about the specific effective bitwidth limits (9-bit for rounded normal vs. 5-bit for uniform) on BF16 hardware are theoretically derived but would benefit from more extensive hardware-specific validation.
- **Medium Confidence:** The assertion that the computational overhead is as low as 1.40% is supported by Table 1, but the overhead can vary (up to 5.30%) depending on the implementation and hardware.

## Next Checks

1. **Implement the Bitwise Noise Kernel:** Write a Triton kernel to generate the noise distribution $R$ using the specified bitwise operations and validate its statistical properties against the theoretical rounded normal distribution.
2. **Benchmark Overhead on Target Hardware:** Profile the GaussWS implementation on the intended GPU hardware (e.g., A100) to confirm the computational overhead is within the claimed range (< 5%).
3. **Stability Ablation Study:** Conduct a controlled experiment training a small model (e.g., GPT2-124M) with GaussWS, DiffQ (uniform noise), and no noise to empirically verify the stability advantages of the rounded normal distribution.