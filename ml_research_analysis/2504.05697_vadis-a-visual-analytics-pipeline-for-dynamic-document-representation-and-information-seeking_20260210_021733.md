---
ver: rpa2
title: 'VADIS: A Visual Analytics Pipeline for Dynamic Document Representation and
  Information-Seeking'
arxiv_id: '2504.05697'
source_url: https://arxiv.org/abs/2504.05697
tags:
- document
- relevance
- attention
- documents
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents VADIS, a visual analytics pipeline designed
  to address key challenges in biomedical information-seeking. The main challenges
  addressed are: (1) static document embeddings that cannot adapt to user interests,
  (2) inability to effectively display document relevance to user queries, and (3)
  lack of interpretability in embedding generation.'
---

# VADIS: A Visual Analytics Pipeline for Dynamic Document Representation and Information-Seeking

## Quick Facts
- arXiv ID: 2504.05697
- Source URL: https://arxiv.org/abs/2504.05697
- Reference count: 40
- Primary result: VADIS introduces PAM for dynamic embeddings and relevance-preserving mapping, achieving 70.1% (Top 10) and 79.6% (Top 20) accuracy on emrQA.

## Executive Summary
VADIS addresses critical limitations in biomedical information-seeking by introducing a visual analytics pipeline that generates dynamic document embeddings conditioned on user prompts. The system overcomes static embeddings, poor relevance visualization, and lack of interpretability through a Prompt-based Attention Model (PAM) that weights token embeddings using prompt-token attention scores. A relevance-preserving document map using a circular grid layout jointly visualizes both semantic similarity and query relevance, while corpus-level attention visualization reveals model focus through NMF-decomposed topics.

## Method Summary
VADIS uses a dual-encoder architecture with BioBERT for prompts (EP) and documents (ED), feeding into a custom attention layer that computes token-level attention scores. These scores weight token embeddings to create prompt-conditioned document representations and relevance scores. The relevance-preserving map extends SOM with per-cell relevance parameters, optimizing both semantic similarity and relevance in a circular layout. Corpus-level attention is aggregated and decomposed via NMF to produce interpretable topics, supporting iterative exploration through dynamic query refinement.

## Key Results
- PAM achieves 70.1% (Top 10) and 79.6% (Top 20) retrieval accuracy on emrQA dataset
- Relevance-preserving mapping achieves silhouette score of 0.33
- Clustering accuracy (ARI) improves from 0.26 to 0.71 for population-based prompts
- Expert case study demonstrates usability in identifying relevant studies and treatments in ADHD research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic document embeddings adapt to user prompts through token-level attention weighting
- Core assumption: Token attention scores capture prompt relevance and provide valid contrastive training signals
- Evidence: Equations 3-5 define attention computation; accuracy metrics validate effectiveness
- Break condition: Uniform attention scores cause embedding collapse to generic averages

### Mechanism 2
- Claim: Modified SOM preserves both relevance (radial distance) and semantic similarity (neighbor proximity)
- Core assumption: Joint optimization of relevance and similarity is possible without severe trade-offs
- Evidence: Silhouette score of 0.33; RPC metric shows alignment under high relevance weighting
- Break condition: Poor ωs/ωr balance creates irregular empty spaces or sub-optimal clustering

### Mechanism 3
- Claim: NMF decomposition of attention matrices produces interpretable corpus-level topics
- Core assumption: Attention patterns across documents form coherent structures analogous to topics
- Evidence: Case study shows expert-identified population and treatment topics matching document clusters
- Break condition: Diffuse attention produces incoherent or uninterpretable topics

## Foundational Learning

- **Transformer [CLS] token attention**: Needed because PAM substitutes user prompts for [CLS] to make embeddings query-dependent. Quick check: Can you explain how Equation 1 (BERT [CLS] attention) differs from PAM's Equation 3 (prompt-token attention)?

- **Self-Organizing Maps (SOM)**: Needed because the relevance-preserving map extends SOM's competitive learning with relevance parameters. Quick check: What role does the Gaussian kernel Tk,j(t) play in SOM weight updates, and how does VADIS modify this for relevance?

- **Contrastive learning**: Needed because PAM uses contrastive loss to increase attention to relevant documents while decreasing it for irrelevant ones. Quick check: How does the loss function in Equation 6 ensure that positive documents receive higher relevance scores than negatives?

## Architecture Onboarding

- **Component map**: User Prompt + Document Corpus → EP → ep → Attention Layer → attn, relevance, embedding → ED → etij → Relevance-preserving Map → Circular grid visualization → Corpus-level Attention + NMF → Topic bar charts

- **Critical path**: 
  1. Preprocess corpus: ED encodes all documents, stores token embeddings
  2. User enters prompt → EP encodes prompt → Attention layer computes scores
  3. Generate document embeddings and relevance scores via weighted sums
  4. Run modified SOM to produce circular map layout
  5. Aggregate attention matrix, apply NMF, visualize topics

- **Design tradeoffs**:
  - Dual encoders vs. shared encoder: Separate EP and ED capture distinct prompt/document characteristics but double model size
  - Grid layout vs. scatterplot: Grid eliminates overlap but constrains placement
  - ωs/ωr weighting: User-tuned; high ωr emphasizes relevance, high ωs emphasizes semantic clustering

- **Failure signatures**:
  - Diffuse attention: Unrelated prompts cause uniform attention scores → generic embeddings
  - Irregular map spacing: SOM competitive learning creates empty cells or misaligned documents
  - Topic incoherence: Poor k selection produces uninterpretable NMF topics

- **First 3 experiments**:
  1. Validate PAM retrieval accuracy: Compute Top-10/Top-20 accuracy on emrQA/SQuAD/TriviaQA/NaturalQA; compare against BM25, DSSM, QDR baselines
  2. Test embedding clustering quality: Generate prompt-specific embeddings on ADHD dataset; compute ARI vs. ground truth; compare with BioBERT baseline
  3. Verify map relevance-similarity tradeoff: Run mapping on MNIST with synthetic relevance; compute silhouette scores and RPC across varying ωs/ωr

## Open Questions the Paper Calls Out

- **Automating weight selection**: How can the selection of relevance and similarity weights (ωr, ωs) be automated to optimize document map layout without manual user input? The authors note this depends on manual input and aim to enhance the framework for automation.

- **Refining mapping algorithm**: How can the relevance-preserving mapping algorithm be refined to prevent irregular empty spaces and sub-optimal document alignment? The paper acknowledges the trade-off between relevance and similarity combined with competitive learning causes these visual artifacts.

- **Stabilizing PAM for low-relevance prompts**: How can PAM be stabilized to maintain focus when processing prompts that are minimally relevant to the corpus? The limitations section highlights that low-relevance prompts cause attention diffusion and generic embeddings.

## Limitations

- Training hyperparameters (learning rate, epochs, batch size) are not specified, creating reproducibility uncertainty
- The discrete grid optimization does not guarantee optimal placement, potentially causing irregular empty spaces or sub-optimal clustering
- Attention diffusion occurs when prompts are unrelated to corpus content, causing embeddings to collapse to generic averages

## Confidence

- **High confidence**: PAM architectural design and retrieval accuracy metrics (70.1% Top-10, 79.6% Top-20 on emrQA) are well-supported
- **Medium confidence**: Relevance-preserving mapping effectiveness is supported by silhouette scores and RPC metrics, but discrete grid optimization introduces inherent limitations
- **Medium confidence**: Corpus-level attention visualization via NMF is a reasonable application of established techniques, but interpretability depends heavily on attention score distribution quality

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary learning rate (1e-5, 2e-5, 5e-5), batch size (16, 32), and epochs (2, 3) to determine optimal training configuration for PAM; measure impact on retrieval accuracy and attention score distribution

2. **Grid optimization benchmarking**: Compare relevance-preserving map against alternative visualization approaches (t-SNE, UMAP, force-directed layouts) on same datasets; evaluate clustering quality (ARI, silhouette), user preference through A/B testing, and computational efficiency

3. **Attention robustness testing**: Design prompts spanning highly relevant to completely irrelevant corpus content; measure attention score entropy, embedding cosine similarity distributions, and retrieval performance degradation; develop and test strategies to handle attention diffusion scenarios