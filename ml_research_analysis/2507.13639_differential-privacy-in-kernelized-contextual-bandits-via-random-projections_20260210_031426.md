---
ver: rpa2
title: Differential Privacy in Kernelized Contextual Bandits via Random Projections
arxiv_id: '2507.13639'
source_url: https://arxiv.org/abs/2507.13639
tags:
- algorithm
- bandits
- private
- kernel
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies private contextual kernel bandits where the\
  \ agent needs to ensure differential privacy with respect to both contexts and rewards.\
  \ The authors propose CAPRI, a novel algorithm achieving state-of-the-art regret\
  \ bounds of \xD5(\u221A\u03B3T T + \u03B3T/\u03B5DP) under joint differential privacy\
  \ and \xD5(\u221A\u03B3T T + \u03B3T\u221AT/\u03B5DP) under local differential privacy,\
  \ where \u03B3T is the effective dimension of the kernel and \u03B5DP is the privacy\
  \ parameter."
---

# Differential Privacy in Kernelized Contextual Bandits via Random Projections

## Quick Facts
- **arXiv ID:** 2507.13639
- **Source URL:** https://arxiv.org/abs/2507.13639
- **Reference count:** 40
- **Primary result:** Novel CAPRI algorithm achieving state-of-the-art regret bounds for private kernelized contextual bandits under both Joint and Local Differential Privacy.

## Executive Summary
This paper tackles the challenge of maintaining differential privacy in kernelized contextual bandits while preserving learning efficiency. The authors propose CAPRI, a novel algorithm that combines private covariance estimation with random projections to achieve strong regret guarantees. The key insight is using independent copies of context-action data to decouple sensitive information from the estimation process, combined with projecting functions onto finite, data-independent subspaces. This approach reduces sensitivity compared to classical methods while maintaining prediction accuracy, yielding regret bounds that scale as Õ(√γT T + γT/εDP) for JDP and Õ(√γT T + γT√T/εDP) for LDP.

## Method Summary
CAPRI is an explore-then-eliminate algorithm operating in epochs with doubling lengths. It maintains active sets of potentially optimal actions and samples uniformly from these sets to ensure independence between collected data and observed rewards. The private kernel-ridge regression estimator uses independent "covariance" sets R_t and "projection" sets S_t to construct statistically identical but independent approximations of the required matrices. For JDP, noise is added at epoch boundaries using a tree-based mechanism; for LDP, noise is added per-round with per-round sensitivity scaling. The algorithm eliminates suboptimal actions based on private estimates of the reward function, iterating until all actions are eliminated or the horizon is reached.

## Key Results
- Achieves regret bounds of Õ(√γT T + γT/εDP) under JDP and Õ(√γT T + γT√T/εDP) under LDP
- First cumulative regret guarantees for the Matérn kernel family under differential privacy
- Recovers optimal non-private regret as ε→∞, matching known bounds for linear bandits
- Demonstrates that random projections can effectively reduce sensitivity in kernelized settings

## Why This Works (Mechanism)

### Mechanism 1: Independent Covariance Approximation
Replacing the data-dependent covariance matrix with an i.i.d. copy decouples the estimator from sensitive observations, reducing sensitivity. Since R_t is independent of current rewards and contexts being privatized, the sensitivity reduces to the variance of the projection. This relies on contexts being stochastic and i.i.d.

### Mechanism 2: Random Projection Subspacing
Projecting the infinite-dimensional RKHS function onto a finite, data-independent subspace allows finite-dimensional privatization via coefficient perturbation. The algorithm selects random points S_t to form a basis and projects the predictor onto this span. This requires the random subspace to be sufficiently representative of the function space.

### Mechanism 3: Non-Adaptive (Uniform) Exploration
Using reward-independent, uniform sampling within active sets ensures the data required for covariance approximation and projection is truly i.i.d. This satisfies the independence requirements of the other mechanisms. The algorithm tolerates short-term efficiency loss of uniform sampling for long-term privacy guarantees.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS) & Mercer's Theorem**
  - *Why needed:* Core problem involves optimizing in infinite-dimensional space; kernels decompose into eigenfunctions (Mercer) to understand why effective dimension γ_T dictates difficulty
  - *Quick check:* Can you explain why a kernel with polynomial eigendecay (like Matérn) is harder to privatize than a linear kernel?

- **Joint vs. Local Differential Privacy (JDP vs. LDP)**
  - *Why needed:* Paper provides guarantees for both; JDP protects output sequence given trusted data access, LDP requires privatizing data before algorithm sees it
  - *Quick check:* In LDP setting, why can't we use Tree-Based Mechanism to aggregate covariance matrix?

- **Explore-Then-Eliminate (EtE) Bandits**
  - *Why needed:* CAPRI uses EtE architecture rather than UCB; understanding that algorithm eliminates suboptimal arms in phases is critical to analyzing regret bounds
  - *Quick check:* How does doubling epoch length in EtE affect trade-off between simple regret and cumulative regret compared to incremental UCB?

## Architecture Onboarding

- **Component map:**
  Context Generator -> Sampler -> Private Estimator (Covariance_Approximator -> Random_Projector -> Noise_Injection) -> Eliminator

- **Critical path:**
  1. Initialization: Generate initial R_1, S_1 sets to define projection subspace
  2. Execution: Receive context -> Uniform Sample -> Observe Reward -> Update sufficient statistics (projected)
  3. Noise Addition: JDP adds noise at epoch end, LDP adds per round
  4. Update: Re-compute elimination thresholds and generate R_{r+1}, S_{r+1} for next epoch

- **Design tradeoffs:**
  - Sample Efficiency vs. Privacy: Sacrifices tight efficiency of adaptive UCB for independence properties of uniform sampling
  - Complexity vs. Dimension: Projection size |S_t| must scale with effective dimension γ_T; higher γ_T requires larger projection sets

- **Failure signatures:**
  - Regret Stagnation: Active sets empty too quickly, likely due to noise scaling σ_0 too high or projection dimension too low
  - Privacy Leak: R_t or S_t accidentally seeded from private dataset W_t rather than independent draws
  - Instability: Kernel matrix inversion (K_{S,R}K_{R,S} + τ K_{S,S})^{-1} fails, check regularization parameter τ

- **First 3 experiments:**
  1. Sanity Check (Non-Private): Run CAPRI with ε_DP → ∞ on synthetic contextual bandit problem, verify matches non-private kernel UCB baselines
  2. Privacy-Utility Curve: Sweep ε_DP ∈ [0.1, 10.0] under JDP and LDP, plot cumulative regret vs. privacy budget to validate scaling dependencies
  3. Ablation (Projection Size): Fix ε_DP and vary size of projection sets S_t (e.g., T/2 vs T/10), confirm too-small sets degrade utility due to approximation error

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the CAPRI algorithm achieve the optimal regret rate for contextual kernel bandits under both Joint and Local Differential Privacy?
- **Basis:** The conclusion states "We conjecture that our proposed algorithm offer optimal regret performance under both JDP and LDP settings."
- **Why unresolved:** While upper bounds match known lower bounds for linear bandits, formal optimality for general kernelized setting has not been established
- **What evidence would resolve it:** Formal proof demonstrating obtained regret bounds match problem-specific lower bounds for kernelized contextual bandits

### Open Question 2
- **Question:** What are the fundamental lower bounds on regret for private kernelized bandits in the JDP and LDP settings?
- **Basis:** Authors identify "Establishing the lower bounds on regret in private settings" as "an interesting future direction to explore"
- **Why unresolved:** Paper focuses on deriving upper bounds but does not derive theoretical limits of performance for this problem class
- **What evidence would resolve it:** Theoretical derivation of regret lower bound depending on effective dimension γ_T and privacy parameter ε_DP

### Open Question 3
- **Question:** Can the performance gap between JDP and LDP observed in adversarial contexts be closed using techniques developed for stochastic contexts?
- **Basis:** Paper notes discrepancy with prior work (Dubey 2021) where gap existed between JDP and LDP for adversarial contexts, whereas CAPRI shows no gap for stochastic contexts
- **Why unresolved:** CAPRI relies on stochastic context assumptions for covariance approximation and random projection techniques, which may not hold in adversarial settings
- **What evidence would resolve it:** Analysis of CAPRI or new algorithm that maintains derived regret ratios under adversarial context distributions

## Limitations
- Relies on i.i.d. contexts and actions, restricting applicability to adversarial or non-stationary environments
- Performance degradation in non-i.i.d. settings not empirically characterized
- Empirical validation limited to synthetic environments, leaving questions about real-world performance

## Confidence
- **High Confidence:** Privacy mechanisms and theoretical guarantees (JDP/LDP bounds) rigorously derived and internally consistent
- **Medium Confidence:** Regret analysis and scaling with effective dimension γ_T theoretically sound, but practical impact of choices requires validation
- **Low Confidence:** Empirical claims regarding performance relative to non-private baselines not substantiated with numerical results

## Next Checks
1. Implement algorithm on standard contextual bandit benchmark and measure cumulative regret across range of ε_DP values to verify theoretical scaling
2. Systematically vary size of projection sets S_t relative to effective dimension γ_T and measure impact on regret and privacy loss to validate approximation error claims
3. Test algorithm on context stream with injected adversarial corruption or non-stationary shifts to quantify degradation in privacy and regret guarantees