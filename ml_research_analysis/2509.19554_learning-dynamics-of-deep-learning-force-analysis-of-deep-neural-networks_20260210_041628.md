---
ver: rpa2
title: Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks
arxiv_id: '2509.19554'
source_url: https://arxiv.org/abs/2509.19554
tags:
- learning
- training
- more
- which
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis investigates deep learning systems through a physics-inspired
  lens, focusing on how one training example influences another during learning. The
  central framework, called learning dynamics or force analysis, uses an AKG decomposition
  to break down this influence into three interpretable components: similarity (K),
  normalization (A), and prediction gap (G).'
---

# Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks

## Quick Facts
- arXiv ID: 2509.19554
- Source URL: https://arxiv.org/abs/2509.19554
- Authors: Yi Ren
- Reference count: 0
- Primary result: Introduces a physics-inspired "learning dynamics" framework using AKG decomposition to explain how one training example influences another, applying this across supervised classification, LLM finetuning, and transfer learning.

## Executive Summary
This thesis investigates deep learning systems through a physics-inspired lens, focusing on how one training example influences another during learning. The central framework, called learning dynamics or force analysis, uses an AKG decomposition to break down this influence into three interpretable components: similarity (K), normalization (A), and prediction gap (G). This approach treats learning as a series of small, local interactions, analogous to how forces shape an object's motion.

The work applies this framework across several settings. In supervised classification, it explains non-trivial learning trajectories like the "zig-zag" path observed in hard examples and inspires a novel knowledge distillation method, Filter-KD, which improves supervision under noisy labels. For large language model (LLM) finetuning, it provides a unified explanation for behaviors across methods like SFT, DPO, and GRPO, highlighting the often-underestimated role of negative gradients and the "squeezing effect" that can degrade model performance. In transfer learning, it analyzes feature adaptation by tracking how hidden representations evolve, leading to strategies for better initialization and controlled adaptation. Finally, it revisits the simplicity bias—why structured representations are learned faster—by showing that simpler mappings exhibit faster learning speeds and more aligned mutual influences.

## Method Summary
The thesis introduces the AKG decomposition framework to analyze one-step influence during training. For any observing example $x_o$ and updating example $x_u$, the influence on $x_o$'s log-probability is decomposed as $\Delta_t(x_o) = -\eta A_t(x_o) K_t(x_o, x_u) G_t(x_u, y_u)$, where $G$ captures the force direction (from current prediction toward supervision), $K$ projects this force based on example similarity (related to the empirical Neural Tangent Kernel), and $A$ normalizes based on the observing example's current confidence. The framework is applied to supervised classification (explaining zig-zag learning paths and inspiring Filter-KD distillation), LLM finetuning (unifying SFT/DPO/GRPO behaviors and proposing NTHR to mitigate squeezing effects), transfer learning (analyzing feature adaptation and proposing better initialization strategies), and revisiting simplicity bias (showing simpler compositional mappings are learned faster due to more coherent mutual influences).

## Key Results
- The AKG decomposition successfully explains non-trivial learning trajectories like the "zig-zag" path in hard examples and inspires Filter-KD, a novel knowledge distillation method that improves supervision under noisy labels
- In LLM finetuning, the framework unifies explanations across SFT, DPO, and GRPO, highlighting the critical role of negative gradients and the "squeezing effect" that causes probability mass concentration on dominant dimensions
- Token-level analysis in GRPO can mitigate Lazy Likelihood Displacement through the proposed NTHR method, which masks harmful token gradients
- Simpler compositional mappings are learned faster due to more coherent mutual influences, providing a new perspective on the simplicity bias phenomenon

## Why This Works (Mechanism)

### Mechanism 1: AKG Decomposition for Influence Quantification
The influence of learning one example on another can be decomposed into three interpretable components: similarity (K), normalization (A), and prediction gap (G). Using a first-order Taylor expansion of the model's log-probability change after one SGD update, the one-step influence $\Delta_t(x_o) = -\eta A_t(x_o) K_t(x_o, x_u) G_t(x_u, y_u)$. Here G captures the force direction (from current prediction toward supervision), K projects this force based on example similarity (related to the empirical Neural Tangent Kernel), and A normalizes based on the observing example's current confidence. The core assumption is that first-order Taylor expansion dominates and gradients remain bounded during training.

### Mechanism 2: Relative Stability of K Enables Cumulative Analysis
The empirical NTK (K term) remains relatively stable in ranking/order during most of training, enabling the accumulation of pairing effects across updates. In MNIST experiments, $\|\mathbf{K}_t^{\mathbf{u}\mathbf{o}}\|_F$ rankings between class pairs (e.g., 4 and 9, 6 and 7) remain relatively stable across epochs, even as the norm values change. This allows similar examples to exert consistent mutual influence, creating observable "pairing effects" and supporting the zig-zag learning path analysis. The framework assumes relative stability of influence rankings during training.

### Mechanism 3: Negative Gradients Induce Squeezing Effect in LLM Finetuning
In methods using negative gradients (e.g., DPO, GRPO), applying gradient descent on low-confidence regions causes probability mass to concentrate on already-dominant dimensions, leading to unintended behaviors. When penalizing a low-probability response $y^-_u$ (negative gradient), the model decreases $\pi(y=y^-_u)$ as intended, but simultaneously increases the most confident alternative dimension due to softmax normalization. This "squeezing effect" causes global distribution shifts, explaining phenomena like simultaneous decay of both preferred and rejected response confidence in DPO, and the "lazy likelihood displacement" problem in GRPO.

## Foundational Learning

- **Gradient Descent and Stochastic Gradient Descent (SGD)**: The entire framework analyzes one-step influence via SGD updates; understanding $\theta_{t+1} = \theta_t - \eta\nabla_\theta L$ is essential. Quick check: Can you derive the update rule for a single-example SGD step on cross-entropy loss?

- **Neural Tangent Kernel (NTK)**: The K term in AKG decomposition is identified as the empirical NTK; understanding its role as a similarity measure is central. Quick check: What does the NTK represent in the context of neural network training dynamics?

- **Cross-Entropy Loss and Softmax**: The G term derivation relies on the cross-entropy loss gradient; understanding softmax normalization is critical for the squeezing effect analysis. Quick check: Why does decreasing $\pi(y=k)$ via negative gradient also affect other dimensions in the softmax output?

## Architecture Onboarding

- **Component map**: AKG Decomposition Engine -> Probing Dataset Generator -> Learning Path Tracker -> Force Analysis Visualizer

- **Critical path**:
  1. Define the task (classification, LLM finetuning, etc.) and identify the target to observe (output confidence, hidden features)
  2. Compute the AKG decomposition for relevant example pairs
  3. Track the evolution of G_t and the stability of K_t during training
  4. Analyze accumulated influences and identify emergent patterns
  5. Design interventions based on insights (e.g., extended SFT for DPO, NTHR for GRPO)

- **Design tradeoffs**:
  - **Probing depth vs. computational cost**: More response types (for LLMs) or more example pairs (for classification) provide finer-grained analysis but require more forward passes and storage
  - **Simplification vs. fidelity**: Using UFM or OPM for theoretical analysis sacrifices architectural details for tractability; practical applications may require adjustments
  - **Local vs. global analysis**: One-step influence analysis is precise but may miss long-term interactions; accumulated influence captures global patterns but relies on stability assumptions

- **Failure signatures**:
  - **K instability**: If $\|\mathbf{K}_t\|_F$ rankings change chaotically during training, cumulative analysis becomes unreliable
  - **Squeezing dominance**: In LLM finetuning, if negative gradients consistently dominate, the model may converge to degenerate, repetitive outputs
  - **Energy-direction mismatch**: In feature adaptation, if the task head initialization provides energy but with inconsistent direction, features may adapt in harmful directions

- **First 3 experiments**:
  1. **MNIST Classification Dynamics**: Train a CNN on MNIST, compute AKG decomposition for pairs of digits (e.g., 4 and 9), visualize the pairing effect and learning paths for easy/hard examples. Verify that K rankings remain stable and zig-zag paths emerge for noisy/mislabeled examples.
  2. **SFT Probing Analysis**: Finetune a small LLM (e.g., Pythia-410M) on a preference dataset (e.g., Anthropic-HH), create a probing dataset with 5-10 response types (chosen, rejected, rephrases, random), track confidence changes for each type. Verify that ungrammatical responses decay immediately while grammatical ones show the ↗ then ↘ pattern.
  3. **DPO Squeezing Effect Validation**: Compare standard DPO with the proposed "extended SFT" method on a 3B parameter model. Track the confidence of the argmax sequence y* and observe whether the squeezing effect is mitigated (i.e., y* confidence should not skyrocket). Measure win rate against baseline using an independent judge.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's applicability is limited in "feature learning regimes" where the neural tangent kernel changes substantially during training, particularly for deep architectures and late-stage training
- The squeezing effect analysis assumes the Unconstrained Feature Model provides sufficient approximation for LLM behavior, but real architectures may exhibit additional complexities
- Claims about simplicity bias and compositional representation learning are validated primarily on synthetic data, with limited empirical verification on natural language tasks

## Confidence
**High Confidence**: The AKG decomposition framework is mathematically sound and consistently applied across different learning scenarios. The mechanism by which similarity, normalization, and prediction gaps combine to influence learning is well-established through derivations and toy examples.

**Medium Confidence**: The practical utility of the framework in explaining real-world phenomena like zig-zag learning paths and LLM finetuning instability is supported by experiments, but some results rely on simplified models (UMF/OPM) that may not capture all architectural nuances.

**Low Confidence**: Claims about simplicity bias and the exact mechanisms of compositional representation learning are based primarily on synthetic experiments. The extension to feature adaptation regimes requires further validation on more complex architectures and tasks.

## Next Checks
1. **Kernel Stability Verification**: Systematically track the empirical NTK (K term) across training epochs for different architectures (CNNs, Transformers) on real datasets to quantify when and how often the relative stability assumption breaks down.

2. **Feature Learning Regime Extension**: Apply the AKG framework to intermediate network layers during training to analyze how representations evolve, particularly focusing on when the first-order approximation fails and what additional terms might be needed.

3. **Real-World LLM Behavior Validation**: Test the squeezing effect predictions on larger language models (Llama, Mistral) with different finetuning methods (SFT, DPO, GRPO) to verify if the probability mass concentration phenomena generalize beyond the simplified settings used in the thesis.