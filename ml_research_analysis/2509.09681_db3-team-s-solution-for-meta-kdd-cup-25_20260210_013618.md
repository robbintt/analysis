---
ver: rpa2
title: DB3 Team's Solution For Meta KDD Cup' 25
arxiv_id: '2509.09681'
source_url: https://arxiv.org/abs/2509.09681
tags:
- query
- uni00000008
- image
- entity
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The db3 team developed a multi-modal retrieval-augmented generation
  system for the Meta CRAG-MM Challenge 2025, achieving 2nd place in Task 1, 2nd in
  Task 2, and 1st in Task 3, winning the grand prize for ego-centric query performance.
  The solution addresses multi-modal, multi-turn question answering by integrating
  domain-specific retrieval pipelines (image-indexed knowledge graphs, web sources,
  multi-turn conversations) with hallucination control through supervised fine-tuning,
  direct preference optimization, and reinforcement learning.
---

# DB3 Team's Solution For Meta KDD Cup' 25

## Quick Facts
- arXiv ID: 2509.09681
- Source URL: https://arxiv.org/abs/2509.09681
- Reference count: 9
- Primary result: 2nd place in Task 1, 2nd in Task 2, and 1st in Task 3, winning the grand prize for ego-centric query performance

## Executive Summary
The DB3 team developed a multi-modal retrieval-augmented generation system for the Meta CRAG-MM Challenge 2025, achieving 2nd place in Task 1, 2nd in Task 2, and 1st in Task 3, winning the grand prize for ego-centric query performance. The solution addresses multi-modal, multi-turn question answering by integrating domain-specific retrieval pipelines (image-indexed knowledge graphs, web sources, multi-turn conversations) with hallucination control through supervised fine-tuning, direct preference optimization, and reinforcement learning. Key innovations include merged query rewriting for text-based retrieval, entity grounding in ego-centric images, and adaptive refusal strategies to maximize correct answers while minimizing incorrect responses. The system demonstrates superior handling of first-person perspective challenges in complex visual-linguistic reasoning tasks.

## Method Summary
The DB3 team's solution combines a Llama 3.2-VL base model with LoRA fine-tuning, domain-specific retrieval pipelines, and hallucination control through SFT, DPO, and GRPO training. The system uses entity extraction and grounding for ego-centric images, merged query rewriting to bridge visual and text-based retrieval, and reinforcement learning with a reward function (+1 correct, 0 refusal, -1 incorrect) to optimize refusal behavior. The architecture employs domain classification routing, multiple retrieval strategies per domain, and ensemble selection to produce final answers.

## Key Results
- Achieved 2nd place in Task 1 (single-turn with image-indexed knowledge graphs)
- Achieved 2nd place in Task 2 (single-turn with web search)
- Won 1st place in Task 3 (multi-turn RAG) and the grand prize for ego-centric query performance

## Why This Works (Mechanism)

### Mechanism 1: Visual-to-Text Query Unification
Converting multimodal queries (image + text) into a unified "merged text query" likely improves retrieval accuracy from text-indexed sources (like web pages) by resolving visual ambiguity. The system uses an SFT-tuned VLM to extract the entity name from the image (e.g., identifying a "Ram 2500" from a picture of a truck) and rewrites the user query to include this explicit entity. This bridges the modality gap between the visual query and the text-based search index. Core assumption: The base VLM possesses sufficient internal knowledge to recognize the specific entity (plant, car model) in the image before the retrieval process begins.

### Mechanism 2: Reinforcement Learning for Refusal (GRPO)
Using Group Relative Policy Optimization (GRPO) with a specific reward function (+1 correct, 0 refusal, -1 incorrect) effectively trains the model to distinguish between answerable and unanswerable queries better than SFT alone. Instead of just learning to generate answers, the model explores the action space of "refusing" (saying "I don't know"). The reward structure incentivizes the model to refuse only when the probability of a correct answer is low, directly optimizing the competition metric. Core assumption: The "LLM Judge" used for local evaluation accurately approximates the online evaluation criteria; otherwise, the policy learns to optimize the wrong objective.

### Mechanism 3: Domain-Adaptive Retrieval Grounding
For ego-centric images, using object grounding (Grounding DINO) before retrieval significantly reduces noise compared to using global image embeddings. Ego-centric images often contain cluttered backgrounds. By detecting and cropping the specific object of interest (e.g., "the plant in the middle") before feeding it into the retrieval index, the system aligns the visual input closer to the "Wikipedia-style" index images. Core assumption: The query text allows for the generation of a reliable prompt for the grounding model.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) vs. GRPO**
  - **Why needed here:** The solution relies on distinct behavioral differences between these algorithms (DPO is conservative, GRPO is proactive) to build a robust ensemble.
  - **Quick check question:** Can you explain why DPO might lead to a higher "missing rate" (refusing to answer) compared to a policy-gradient method like GRPO when penalized for wrong answers?

- **Concept: Ego-Centric Vision**
  - **Why needed here:** Understanding the specific failure modes of first-person images (occlusion, angle) is required to justify the complexity of the grounding/cropping pipeline.
  - **Quick check question:** Why does a standard CLIP embedding fail to match an ego-centric photo of a car to a stock photo of the same car model?

- **Concept: Retrieval-Augmented Generation (RAG) Evaluation**
  - **Why needed here:** The team built a custom "LLM Judge" because the prompt/evaluation logic significantly impacts optimization.
  - **Quick check question:** If the local judge is "stricter" than human evaluation, how might this bias the model's refusal training?

## Architecture Onboarding

- **Component map:** Input (Image + Text Query) -> Router (Domain classifier) -> Retrieval (Math: Python tool API; Plant: Entity extraction + Knowledge Graph; General: Query rewrite + Web Search + Reranker) -> Generation (VLM with GRPO/DPO fine-tuning) -> Output Ensemble (domain-specific checkpoint voting)
- **Critical path:** The Query Rewrite Module. If the entity extraction fails here (e.g., identifying a specific plant species), the subsequent retrieval and generation steps cannot recover the correct context.
- **Design tradeoffs:** Latency vs. Accuracy: The ensemble strategy requires multiple rounds of inference (prohibitively slow for production), but was necessary for the competition's static dataset evaluation. Generalization vs. Optimization: The team used "cheated" rewrites (including ground truth in the prompt) to distill knowledge into the smaller model, trading data purity for model capability.
- **Failure signatures:** High Refusal Rate: Indicates the reward model/grpo training is too conservative or the retrieval pipeline is failing to surface relevant context. Entity Hallucination: The query rewriter invents an entity name not present in the image, leading to confident retrieval of irrelevant facts.
- **First 3 experiments:** 1) Calibrate the Judge: Implement the "LLM Judge" prompt locally and validate its correlation with ground truth labels to ensure training signals are valid. 2) Ablate the Rewriter: Run the retrieval pipeline with raw queries vs. merged queries to quantify the recall lift from query rewriting. 3) Refusal Threshold Tuning: Test the GRPO model on a validation set to find the optimal balance between "I don't know" and attempting an answer.

## Open Questions the Paper Calls Out

### Open Question 1
Can a dedicated, fine-tuned recognition model effectively bridge the performance gap in fine-grained entity recognition (e.g., specific car models or botanical names) where current state-of-the-art VLMs like GPT-4o fail to identify the majority of entities? Basis in paper: Section 3.1 notes that even GPT-4o "can only identify 40% of the entity's desired names," and the Conclusion explicitly lists "limitations... in fine-grained entity recognition" as an area for future work. Why unresolved: The authors attempted to find open-source models for these specific domains but found none, concluding that a solution requires "highly sophisticated database building and model training" which they did not undertake. What evidence would resolve it: A proposed model or pipeline demonstrating significantly higher entity recall (>80%) on the CRAG-MM vehicle and plant subsets compared to the GPT-4o baseline.

### Open Question 2
What is the optimal architecture for integrating OCR capabilities into a constrained multi-modal RAG pipeline that balances the high token cost of VLM-based extraction with the lower accuracy of standard OCR tools on ego-centric images? Basis in paper: Section 3.4 states that VLM-based OCR requires generating too many tokens for contest constraints, while tools like PaddleOCR produced "not as satisfying" results on ego-centric data, leading the team to exclude OCR entirely. Why unresolved: The trade-off between computational efficiency (token generation) and accuracy on complex, noisy ego-centric images remains an open engineering challenge within the team's specific framework. What evidence would resolve it: A comparative analysis of latency vs. accuracy for a "tool-VLM" hybrid OCR approach versus standard OCR tools on the CRAG-MM dataset.

### Open Question 3
Does the utility of Reinforcement Learning (RL) for query rewriting collapse when the base VLM lacks sufficient internal knowledge, or can RL still provide gains over Supervised Fine-Tuning (SFT) via policy optimization? Basis in paper: Section 3.2 notes that RL training "fail[ed] to obtain a better rewrite checkpoint than the original SFT-tuned checkpoint," which the authors attribute to a "lack of sufficient internal knowledge" rather than a methodological flaw. Why unresolved: It is unclear if the failure of RL (GRPO/DPO) was intrinsic to the task or simply a result of the specific base model's limitations, leaving the potential of RL for query rewriting uncertain. What evidence would resolve it: Experiments applying the described RL tuning to a larger base VLM with greater internal knowledge to see if it surpasses the SFT baseline.

## Limitations
- Fine-grained entity recognition remains a bottleneck, with even GPT-4o identifying only 40% of desired entity names
- VLM-based OCR was excluded due to token constraints and unsatisfactory performance of standard OCR tools on ego-centric images
- RL training for query rewriting failed to improve upon SFT when the base model lacked sufficient internal knowledge

## Confidence
- **High Confidence:** The basic architecture (VLM + domain-specific retrieval + hallucination control via GRPO) is well-documented and technically sound
- **Medium Confidence:** The claimed performance improvements from query rewriting and ego-centric grounding are supported by described mechanisms but lack quantitative ablation evidence
- **Medium Confidence:** The effectiveness of GRPO over SFT/DPO for refusal training is demonstrated empirically but the exact implementation details (sampling strategy, reward computation) are not fully specified

## Next Checks
1. **Judge Calibration Validation:** Implement the LLM Judge prompt locally and validate its correlation with ground truth labels across 100 validation samples to ensure training signals are valid
2. **Query Rewrite Ablation:** Run the retrieval pipeline with raw queries vs. merged queries on the validation set to quantify the exact recall lift from query rewriting
3. **Refusal Threshold Analysis:** Test the GRPO model on a held-out validation set to find the optimal balance between refusal rate and correct answer rate, comparing against SFT-only baseline performance