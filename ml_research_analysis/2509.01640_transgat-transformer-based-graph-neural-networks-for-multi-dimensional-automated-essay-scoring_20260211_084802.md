---
ver: rpa2
title: 'TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated
  Essay Scoring'
arxiv_id: '2509.01640'
source_url: https://arxiv.org/abs/2509.01640
tags:
- scoring
- essay
- graph
- automated
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TransGAT addresses limitations in automated essay scoring by integrating
  fine-tuned Transformer models with Graph Attention Networks (GATs) to capture both
  contextual and syntactic relationships. It uses a two-stream prediction mechanism,
  combining essay-level predictions from Transformers with graph-based modeling of
  token embeddings via syntactic dependencies.
---

# TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring

## Quick Facts
- **arXiv ID**: 2509.01640
- **Source URL**: https://arxiv.org/abs/2509.01640
- **Reference count**: 20
- **Primary result**: TransGAT achieves mean QWK of 0.854 across 6 analytic traits on ELLIPSE dataset

## Executive Summary
TransGAT integrates fine-tuned Transformers with Graph Attention Networks (GATs) to address limitations in automated essay scoring by capturing both contextual and syntactic relationships. The two-stream architecture combines essay-level predictions from Transformers with graph-based modeling of token embeddings via syntactic dependencies. Experiments on the ELLIPSE dataset demonstrate that TransGAT outperforms baseline models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across six analytic scoring dimensions.

## Method Summary
TransGAT uses a two-stream architecture where Stream 1 employs a frozen fine-tuned Transformer (BERT/RoBERTa/DeBERTaV3-large) to generate essay-level predictions from [CLS] embeddings, while Stream 2 constructs a graph from token embeddings with edges derived from syntactic dependencies processed through a two-layer GAT with 4 attention heads. The final scores are obtained by summing the predictions from both streams. The model is trained with AdamW optimizer (learning rate 1e-5 for Transformer, 1e-3 for GAT) for 6 epochs on the ELLIPSE dataset, predicting six analytic traits: Cohesion, Syntax, Vocabulary, Phraseology, Grammar, and Conventions.

## Key Results
- TransGAT achieves mean QWK of 0.854 across six analytic scoring dimensions on ELLIPSE dataset
- RoBERTa-large-based TransGAT outperforms DeBERTaV3-large on 4/6 traits despite smaller pre-training corpus
- Model demonstrates effectiveness in cross-prompt evaluation setting with ELL student essays

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stream prediction combining essay-level Transformer outputs with syntactic-graph-based modeling improves multi-dimensional AES accuracy.
- Mechanism: Stream 1 uses frozen fine-tuned Transformer [CLS] embeddings passed through a dense layer for essay-level predictions. Stream 2 applies GAT to token-level Transformer embeddings with edges from dependency parses, then mean-pools to graph-level predictions. Final scores are summed (ŷ = s₁ + s₂).
- Core assumption: Contextual semantics and syntactic structure provide complementary, non-redundant signals for scoring; errors in each stream are partially independent.

### Mechanism 2
- Claim: Encoding syntactic dependencies as graph edges allows GAT to learn attention over grammatically salient word relationships.
- Mechanism: Stanza parser extracts dependency tree → adjacency matrix (A_ij = 1 if syntactic relation exists) → two-layer GAT with 4 attention heads computes weighted aggregations over syntactic neighbors → global mean pooling produces essay representation.
- Core assumption: Syntactic relationships (subject-verb, modifier-noun) correlate with writing quality traits like grammar, cohesion, and conventions; attention can learn which dependencies matter per trait.

### Mechanism 3
- Claim: Freezing fine-tuned Transformer weights while training GAT preserves contextual representations while learning syntactic-relational patterns.
- Mechanism: Pre-train Transformer on general corpora → fine-tune on ELLIPSE → freeze weights → extract token embeddings as GAT node features → train GAT layers independently with higher learning rate (1e-3 vs 1e-5 for Transformer).
- Core assumption: Contextual embeddings from fine-tuned Transformers are sufficiently domain-adapted; syntactic-relational learning benefits from stable embeddings.

## Foundational Learning

- **Concept: Graph Attention Networks (GAT)**
  - Why needed here: Core mechanism for propagating information across syntactic dependency edges with learned importance weights.
  - Quick check question: Given node embeddings h_i and neighbors N(i), can you compute attention coefficient α_ij using equations 3-4 and explain why softmax normalization matters?

- **Concept: Dependency Parsing and Syntactic Graphs**
  - Why needed here: Determines graph topology; errors in parsing propagate to GAT structure.
  - Quick check question: For the sentence "The student wrote an essay," what dependency edges exist, and how would these map to adjacency matrix entries?

- **Concept: Transformer Fine-Tuning and [CLS] Token**
  - Why needed here: Provides both essay-level representation (Stream 1) and token embeddings for GAT nodes (Stream 2).
  - Quick check question: Why does the [CLS] token serve as a summary representation, and what are its limitations for long essays (~430 words average in ELLIPSE)?

## Architecture Onboarding

- **Component map**: Input text → Transformer encoder → [CLS] embedding (Stream 1) + token embeddings → Stanza parser → adjacency matrix → GAT layers → mean pooling → predictions (Stream 2) → sum fusion → 6 trait scores

- **Critical path**: 1) Fine-tune Transformer on ELLIPSE → 2) Pre-compute dependency parses → 3) Extract token embeddings + construct graph → 4) GAT forward pass → 5) Fuse predictions → 6) Compute MSE loss across 6 traits

- **Design tradeoffs**:
  - RoBERTa-large vs. DeBERTaV3-large: RoBERTa (160GB pre-training) outperforms DeBERTaV3 (78GB) on 4/6 traits, but DeBERTaV3 excels at Cohesion/Syntax
  - Frozen vs. joint training: Freezing reduces memory/compute but prevents gradient flow from GAT to embeddings
  - Sum fusion vs. learned weights: Simple addition assumes equal contribution; learned weighting could adapt per-trait but risks overfitting on small dataset

- **Failure signatures**:
  - Low QWK on Conventions (e.g., DeBERTaV3-GAT: 0.744): May indicate graph structure underrepresents surface-level conventions
  - Vocabulary dimension underperforms baseline (RoBERTa-base: 0.84 vs. TransGAT: 0.825): GAT's focus on syntactic relations may dilute lexical diversity signals
  - Dependency parse failures on ungrammatical ELL writing: Check Stanza parser output quality on error-heavy essays

- **First 3 experiments**:
  1. Ablation study: Run Transformer-only vs. GAT-only vs. fused TransGAT to quantify each stream's contribution per trait
  2. Parser sensitivity: Compare Stanza vs. spaCy dependency parsing impact on graph quality and QWK
  3. Fusion variants: Test learned fusion weights (ŷ = w₁s₁ + w₂s₂) vs. simple sum to evaluate trait-specific weighting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating heterogeneous graph structures with both word-level and essay-level nodes improve TransGAT's ability to model cross-essay relationships compared to the current homogeneous approach?
- Basis in paper: The conclusion states future work will explore expanding TransGAT with heterogeneous graph structures that include both word-level and essay-level nodes, allowing the model to capture more complex interactions across and within essays.
- Why unresolved: The current TransGAT implementation constructs graphs based on syntactic dependencies within single essays, lacking a mechanism to model relationships or similarities across different essays explicitly.
- What evidence would resolve it: A comparative study showing QWK improvements when heterogeneous graphs (inter-essay edges) are introduced versus the current single-essay graph implementation.

### Open Question 2
- Question: Can the TransGAT architecture be effectively adapted for morphologically rich languages like Arabic, given the challenges of root-based morphology and diacritics?
- Basis in paper: The conclusion identifies adapting TransGAT for morphologically rich languages such as Arabic as a promising direction, noting that unique features like optional diacritics pose challenges for standard NLP pipelines.
- Why unresolved: The model was evaluated exclusively on the English ELLIPSE dataset, and its reliance on specific syntactic dependency parsers (Stanza) and tokenizers optimized for English makes performance on languages with vastly different morphology unknown.
- What evidence would resolve it: Successful implementation and evaluation of TransGAT on a standard Arabic essay dataset, utilizing Arabic-specific morphological analyzers and tokenizers, yielding competitive QWK scores.

### Open Question 3
- Question: Would a weighted or attention-based fusion mechanism outperform the simple summation (s₁ + s₂) currently used to combine the Transformer and GAT prediction streams?
- Basis in paper: Section 3.2.2 defines the final score ŷ = s₁ + s₂, a simple addition of the two streams. While effective, this assumes equal contribution from both streams, which may not be optimal for all writing dimensions.
- Why unresolved: The paper does not perform an ablation study on the fusion technique; it is possible that one stream dominates the other or that different dimensions require different fusion weights.
- What evidence would resolve it: An ablation study comparing the performance of simple summation against learnable weighted fusion or concatenation-based fusion across all six analytic scoring dimensions.

### Open Question 4
- Question: Does converting directed syntactic dependency trees into undirected graphs result in a loss of grammatical information critical for scoring analytic traits like Syntax?
- Basis in paper: Section 3.2.2 states the syntactic graph is treated as undirected to help capture structure, but dependency grammar is inherently directed (e.g., subject → verb).
- Why unresolved: The paper does not compare the performance of undirected graphs against directed graphs, leaving it unclear if the simplification aids or hinders the detection of specific syntactic errors.
- What evidence would resolve it: An experiment comparing model performance when the adjacency matrix preserves edge directionality versus the current undirected implementation.

## Limitations

- Frozen fine-tuning approach prevents end-to-end optimization, potentially leaving suboptimal embeddings for graph propagation
- Dependency parsing accuracy on ungrammatical ELL writing represents a critical vulnerability that wasn't independently verified
- Simple sum fusion mechanism assumes equal stream contributions without task-specific weighting optimization

## Confidence

- **High confidence**: The two-stream architecture design is clearly specified and reproducible; the improvement over baseline Transformer models (QWK from ~0.84 to ~0.854) is directly measurable from reported experiments
- **Medium confidence**: The mechanism by which syntactic dependencies improve scoring relies on parser accuracy that wasn't independently verified on the ELLIPSE corpus; the assumption of complementary signals between streams needs broader validation
- **Low confidence**: The frozen training approach's impact on final performance remains untested against joint optimization; the generalization of cross-prompt evaluation results to other AES datasets is unverified

## Next Checks

1. **Parser Sensitivity Analysis**: Compare Stanza vs. spaCy dependency parsing on 50 manually annotated ELL essays to measure parse accuracy degradation on ungrammatical input; correlate parsing quality with GAT performance drops across traits

2. **Stream Contribution Quantification**: Implement ablation study measuring QWK for Transformer-only (Stream 1), GAT-only (Stream 2), and fused TransGAT across all six traits; calculate correlation between stream predictions to test independence assumption

3. **Fusion Optimization Testing**: Replace simple sum fusion with learned weighted combination (ŷ = w₁s₁ + w₂s₂) using trait-specific parameters; evaluate whether adaptive fusion improves underperforming dimensions (Vocabulary, Conventions) without overfitting on small dataset