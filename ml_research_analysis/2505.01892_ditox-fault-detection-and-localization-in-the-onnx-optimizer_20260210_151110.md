---
ver: rpa2
title: 'DiTOX: Fault Detection and Localization in the ONNX Optimizer'
arxiv_id: '2505.01892'
source_url: https://arxiv.org/abs/2505.01892
tags:
- onnx
- optimizer
- ditox
- passes
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DiTOX, an automated framework for detecting
  and localizing faults in the ONNX Optimizer using differential testing and iterative
  pass-by-pass analysis. DiTOX applies optimization passes to ONNX models, executes
  both original and optimized versions, and identifies discrepancies in behavior or
  optimizer failures.
---

# DiTOX: Fault Detection and Localization in the ONNX Optimizer

## Quick Facts
- **arXiv ID:** 2505.01892
- **Source URL:** https://arxiv.org/abs/2505.01892
- **Reference count:** 40
- **Primary result:** DiTOX found that 9.2% of 130 ONNX models crashed or produced invalid models under default optimization; 14 previously unknown issues affecting 9 passes and optimizer infrastructure were reported to ONNX developers.

## Executive Summary
DiTOX is an automated framework that uses differential testing to detect and localize faults in the ONNX Optimizer. It applies optimization passes to models, executes both original and optimized versions, and identifies discrepancies in behavior or optimizer failures. When divergences are detected, DiTOX isolates the responsible optimization pass through fine-grained, iterative analysis. Evaluated on 130 models from the ONNX Model Hub spanning vision and language tasks, DiTOX uncovered 15 issues—14 previously unknown—affecting 9 of the 47 optimization passes as well as optimizer infrastructure. The results demonstrate that DiTOX provides an effective approach for validating AI model optimizers and is readily extensible beyond ONNX.

## Method Summary
DiTOX fetches ONNX models from the Model Hub, applies optimization passes using the ONNX Optimizer, and executes both original and optimized models via ONNX Runtime. It compares outputs using task-specific metrics (Kendall’s Tau for classification, IoU for object detection, BLEU for text) to detect semantic drift. On discrepancy, it iteratively applies individual passes to isolate the faulty transformation. The framework uses fixed validation datasets (ImageNet ILSVRC2017, SQuAD-v2, IMDB) and supports both default pass bundles and single-pass modes for root-causing.

## Key Results
- 9.2% of model instances crashed the optimizer or produced invalid models under default settings.
- Output discrepancies occurred in 30% of classification models and 16.6% of object detection models, while text-based models were largely robust.
- DiTOX uncovered 15 issues—14 previously unknown—affecting 9 of the 47 optimization passes as well as optimizer infrastructure.
- All identified issues were reported to ONNX Optimizer developers.

## Why This Works (Mechanism)

### Mechanism 1: Differential Semantic Verification
Optimizations that preserve numerical precision may still alter semantic outputs; executing both original and optimized models on identical inputs reveals discrepancies. The system executes the original model ($M_{orig}$) and the optimized model ($M_{opt}$) using the ONNX Runtime. It compares the outputs not just for tensor equality, but for semantic shifts using task-specific metrics (e.g., label ranking or bounding box overlap). Graph-level optimizations (fusing, eliminating) should be functionally invariant and produce identical or statistically indistinguishable outputs given the same inputs.

### Mechanism 2: Iterative Pass-by-Pass Fault Localization
Faults introduced by a bundle of optimization passes can be isolated to a specific transformation by decomposing the optimization sequence. When a discrepancy is detected using the default bundle (fuse/eliminate), the system iteratively applies individual passes (e.g., `fuse_bn_into_conv`, `eliminate_nop_transpose`) to generate separate model versions. It re-runs the comparison to identify the single pass responsible for the crash or accuracy drop. The fault is caused by a single pass, rather than the emergent interaction of multiple passes applied in sequence.

### Mechanism 3: Task-Specific Metric Thresholding
Generic tensor comparison is insufficient for detecting semantic regression; domain-specific metrics (Kendall’s Tau, IoU, BLEU) are required to quantify "correctness" preservation. DiTOX routes the output tensors to a specific comparator based on the model task. For classification, it calculates rank correlation; for object detection, it calculates Intersection over Union (IoU); for text, it uses BLEU scores. The selected metrics accurately reflect the functional intent of the model (e.g., that a lower BLEU score or IoU indicates an optimization fault).

## Foundational Learning

- **Concept: ONNX Intermediate Representation (IR)**
  - **Why needed here:** To understand what the optimizer is actually manipulating. You must distinguish between the **graph structure** (nodes/edges) and the **initializers** (weights).
  - **Quick check question:** Can you explain why fusing a BatchNormalization node into a Convolution node changes the weights (initializers) but not the graph topology logic?

- **Concept: Compiler Optimization Passes**
  - **Why needed here:** The paper tests 47 distinct passes. Understanding the difference between "elimination" (removing dead code), "fusion" (combining ops), and "rewrite" is essential for debugging.
  - **Quick check question:** If `eliminate_nop_transpose` causes a crash, does that imply the transpose was actually a "nop" (no-operation), or did the pass misidentify it?

- **Concept: Differential Testing**
  - **Why needed here:** This is the core verification strategy. It differs from unit testing because there is no pre-defined "expected value" oracle; the unoptimized model *is* the oracle.
  - **Quick check question:** Why is differential testing particularly effective for finding bugs in compilers or optimizers where the "correct" output is mathematically complex to define?

## Architecture Onboarding

- **Component map:** Model Orchestrator -> Optimizer Module -> Runner Module -> Metrics Comparison Module -> (Decision Point) -> Optimizer Module (isolated passes)
- **Critical path:** Load Model (Orchestrator) → Apply Default Optimization (Optimizer Module) → Run Inference (Runner Module) → Decision Point: If Metrics Comparison detects a discrepancy → Loop back to Optimizer Module for Pass-by-Pass isolation
- **Design tradeoffs:**
  - **Granularity vs. Compute Cost:** The isolation mechanism (iterative single-pass runs) is computationally expensive but necessary for root-causing, whereas full optimization is fast but opaque.
  - **Strictness vs. Noise:** The paper notes that graph optimizations *should not* change numerics. However, choosing strict vs. loose thresholds for metrics like IoU determines whether you catch subtle bugs or drown in false positives.
- **Failure signatures:**
  - **Crash:** Optimizer throws exception or resulting model fails to load.
  - **Malformed Graph:** Model loads but runtime throws "Bad spec for node" or "Missing required input."
  - **Semantic Drift:** Model runs but outputs differ (e.g., Top-1 label flips, bounding box IoU < 0.9).
- **First 3 experiments:**
  1. **Sanity Check:** Run DiTOX on a simple ResNet model using the default "fuse/eliminate" bundle to verify the baseline pipeline produces identical outputs.
  2. **Pass Isolation:** Intentionally select a model known to have issues (e.g., EfficientNet-Lite4 mentioned in Table 3) and observe how DiTOX isolates `fuse_bn_into_conv` as the culprit.
  3. **Opset Sensitivity:** Test models with older opsets (7-9) vs. newer ones to reproduce the "Implicit ONNX format version upgrade" bug reported in Section 6.2.

## Open Questions the Paper Calls Out
- **Question:** How does the ordering of optimization passes influence the introduction of functional correctness bugs in ONNX models?
  - **Basis in paper:** Section 11 identifies the examination of optimization pass order as a future work direction, noting the computational infeasibility of testing all permutations.
  - **Why unresolved:** The current evaluation relied on default and atomic pass configurations, leaving the combinatorial effects of specific pass sequences unexplored.
  - **Evidence:** A study measuring fault rates across various pass permutations on the existing model corpus would determine if specific orderings trigger unique discrepancies.

- **Question:** Can data flow analysis or delta debugging be integrated to pinpoint the specific root cause of faults within a single optimization pass?
  - **Basis in paper:** Section 3 suggests these techniques as future enhancements to improve fault localization beyond simply identifying the responsible pass.
  - **Why unresolved:** DiTOX currently relies on compiler error messages and isolates faults only at the pass level rather than analyzing the internal logic of the transformation.
  - **Evidence:** An extension of the DiTOX framework that successfully isolates the specific transformation step causing a crash would demonstrate this capability.

- **Question:** Does the ONNX Optimizer introduce execution time regressions in models where it preserves or alters functional correctness?
  - **Basis in paper:** Section 11 lists execution time measurements as a future direction to assess the broader impact of the optimizer beyond correctness.
  - **Why unresolved:** The study focused strictly on functional correctness (crashes and accuracy), excluding runtime performance metrics like CPU or GPU latency.
  - **Evidence:** Benchmarking the inference latency of the optimized models versus the original models would reveal any performance trade-offs associated with the detected faults.

## Limitations
- The evaluation relies on the assumption that the unoptimized model is the ground truth for "correct" behavior, which may not hold if the original model itself has subtle bugs or non-deterministic components.
- The effectiveness of differential testing depends on the availability of diverse, representative inputs; the results are based on fixed validation sets (ImageNet, SQuAD, IMDB) and may not generalize to all possible inputs.
- The claim that text-based models are "largely robust" is based on a very small sample size (7 models) and may not reflect the broader landscape of text transformer architectures.

## Confidence
- **High Confidence:** The core mechanism of differential testing and pass-by-pass isolation is sound and clearly demonstrated through the reported 15 confirmed issues (14 previously unknown).
- **Medium Confidence:** The task-specific metrics (Kendall's Tau, IoU, BLEU) are appropriate for detecting semantic drift, but the chosen thresholds for what constitutes a "discrepancy" are not fully justified.
- **Low Confidence:** The claim that text-based models are "largely robust" is based on a very small sample size (7 models) and may not reflect the broader landscape of text transformer architectures.

## Next Checks
1. **Cross-Validation with Diverse Inputs:** Run DiTOX on the same set of models using multiple, independently sampled subsets of the validation data to assess the consistency of the detected discrepancies.
2. **Oracle Verification:** Manually inspect a sample of the isolated faulty graphs (e.g., from the EfficientNet-Lite4 case) to verify that the reported semantic drift is a genuine regression and not a limitation of the chosen metrics.
3. **Performance Impact Analysis:** Measure the runtime and memory overhead introduced by DiTOX's iterative per-pass isolation mechanism and assess whether it is feasible for continuous integration pipelines.