---
ver: rpa2
title: A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules
arxiv_id: '2503.12811'
source_url: https://arxiv.org/abs/2503.12811
tags:
- loss
- learning
- schedules
- schedule
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Multi-Power Law to predict the pretraining
  loss curve of large language models under various learning rate schedules. The law
  combines a power-law term based on the cumulative sum of learning rates with additional
  terms that capture the loss reduction induced by learning rate decay.
---

# A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules

## Quick Facts
- **arXiv ID:** 2503.12811
- **Source URL:** https://arxiv.org/abs/2503.12811
- **Reference count:** 40
- **Primary result:** Introduces Multi-Power Law (MPL) to predict pretraining loss curves under various learning rate schedules, validated across different model sizes with >0.99 R² accuracy on held-out schedules.

## Executive Summary
This paper introduces a Multi-Power Law to predict the pretraining loss curve of large language models under various learning rate schedules. The law combines a power-law term based on the cumulative sum of learning rates with additional terms that capture the loss reduction induced by learning rate decay. The law is validated across different model sizes and architectures, accurately predicting loss curves for unseen schedules after fitting on just a few training runs. Moreover, the law enables the discovery of optimized learning rate schedules that outperform standard cosine schedules. Theoretically, the law is shown to emerge under certain spectral properties of the Hessian and noise covariance matrices in quadratic loss settings. The work provides both practical tools for efficient hyperparameter tuning and theoretical insights into the dynamics of model pretraining.

## Method Summary
The method fits a Multi-Power Law (MPL) to predict loss curves under arbitrary learning rate schedules. The MPL equation combines a power-law sum term (based on cumulative learning rate) with a decay correction term that captures loss reduction from learning rate drops. The model is fit using Adam optimization on 2-3 training runs with different schedules (typically constant, cosine, and two-stage). After fitting, MPL predicts loss for unseen schedules and enables optimization of learning rate schedules by minimizing the predicted final loss. The theoretical foundation connects MPL to spectral properties of the Hessian and noise covariance under quadratic loss assumptions.

## Key Results
- MPL achieves >0.99 R² accuracy in predicting loss curves for held-out schedules after fitting on just 2-3 training runs
- Optimized schedules discovered via MPL outperform standard cosine schedules by 0.02-0.03 loss reduction on 400M-scale models
- Theoretical derivation shows MPL emerges from spectral power-law structure in Hessian and noise covariance matrices
- The law enables accurate long-horizon extrapolation (~3× beyond training horizon)

## Why This Works (Mechanism)

### Mechanism 1: Learning Rate Sum as Normalized Training Progress
- **Claim**: Training progress can be meaningfully quantified by the cumulative sum of learning rates rather than step count alone.
- **Mechanism**: Under small learning rates, SGD approximates gradient flow where each step η_t corresponds to continuous-time evolution of length η_t. Two runs with equal cumulative LR sum reach comparable parameter regions, regardless of schedule shape. This linearizes the contribution of each step's learning rate.
- **Core assumption**: Learning rates are sufficiently small that the continuous approximation holds; the loss landscape is locally smooth.
- **Evidence anchors**:
  - [Section 3.1]: "Two training runs with the same LR sum should result in similar training losses... If the learning rates η_1, ..., η_T are small, then SGD can be seen as a first-order approximation of its continuous counterpart, gradient flow."
  - [Section 3.2.1]: Empirical validation showing constant LR processes with identical LR sums align closely, fitted by L_const(Z(t)) = L_0 + A·(S(t) + S_W)^{-α}
  - [corpus]: Weak direct support—neighbor papers discuss LR schedules but don't validate this specific sum-matching principle.
- **Break condition**: Large peak learning rates (η_max ≳ 6×10^-4 in experiments) degrade prediction accuracy; highly non-convex landscapes with sharp curvature violate the continuous approximation.

### Mechanism 2: Learning Rate Decay Induces Additional Loss Reduction via Power-Law Saturation
- **Claim**: Each LR reduction (η_{k-1} → η_k) contributes an incremental loss drop that follows a power law in the remaining cumulative LR sum, not an instantaneous jump.
- **Mechanism**: When LR drops at step k, the optimization enters a new regime operating at smaller step sizes. The loss reduction L_Dk(t) = B(η_{k-1} - η_k)·[1 - (Cη_k^{-γ}S_k(t) + 1)^{-β}] captures this: it scales linearly with the LR reduction magnitude, but the *temporal* saturation follows a power law in S_k(t) (the sum of LRs from step k onward). This reflects the geometry of traversing flatter regions at lower resolution.
- **Core assumption**: The intermediate loss reduction is weakly dependent on the LR prefix (history before step k); β and γ are LR-scale-independent constants.
- **Evidence anchors**:
  - [Section 3.3.1, Figure 4]: Two-stage experiments show loss reduction follows power form (error 5.16×10^-7) better than exponential (error 1.21×10^-5).
  - [Section 3.3.2, Figure 6]: Multi-stage validation confirms ˜B^(i) ∝ (η^{(i-1)} - η^{(i)}) and ˜C^(i) ∝ (η^{(i)})^{-γ} hold across stages, indicating weak prefix dependence.
  - [corpus]: "Functional Scaling Laws in Kernel Regression" discusses LR schedule shaping dynamics but doesn't derive this power-law decomposition.
- **Break condition**: Discontinuous schedules with very abrupt LR drops show prediction deviation near transition points; the model predicts instant loss change but actual decline is smoother.

### Mechanism 3: Multi-Power Law Emerges from Spectral Power-Law Structure
- **Claim**: The MPL form arises theoretically when Hessian eigenvalues and gradient noise covariance follow power-law spectral distributions.
- **Mechanism**: For quadratic loss with Hessian H and noise covariance Σ: if eigenvalues λ_i follow p(λ) ∝ λ^{-ν}, noise variance follows E[Σ|λ] ∝ λ^{-ρ}exp(-rλ), and initial distance follows E[Δ²|λ] = D²λ^{-κ}, then Theorem 1 shows E[L(θ_t)] ≈ L_0 + A·S_1(t)^{-α} - B·Σ_k(η_{k-1}-η_k)·Ĝ(S_k(t)) where α = 2-ν-κ and β = 1-ν-ρ. The power-law exponents directly encode spectral properties.
- **Core assumption**: Assumption 1's spectral conditions hold; η_max is small and S_1(t) is large; the loss is locally quadratic near convergence.
- **Evidence anchors**:
  - [Section 4.2, Theorem 1]: Full derivation with error bound |E[L(θ_t)] - L̂(t)| = O(η_max·S_1^{-min{α+1,β}} + η²_max).
  - [Section 4.2, discussion]: Notes discrepancy between theoretical Ĝ(x) and empirical G(η_k^{-γ}S_k(t))—the latter has explicit η_k dependence, suggesting landscape changes (Edge of Stability) not captured by quadratic analysis.
  - [corpus]: "Scaling Collapse Reveals Universal Dynamics" supports spectral structure arguments for universal scaling but doesn't connect to LR schedules.
- **Break condition**: Non-quadratic loss landscapes with significant curvature changes; regimes where Edge of Stability dynamics are prominent (large LR causing implicit regularization).

## Foundational Learning

- **Concept: Chinchilla Scaling Law (CDSL)**
  - **Why needed here**: MPL extends CDSL by replacing step count T with cumulative LR sum and adding the decay correction term. Understanding CDSL's L(T) = L_0 + Ã÷T^{-α} form is prerequisite to grasping why MPL generalizes it.
  - **Quick check question**: Can you explain why CDSL fails to predict loss at intermediate steps when the LR schedule hasn't fully decayed?

- **Concept: Gradient Flow Continuous Approximation of SGD**
  - **Why needed here**: The LR sum matching principle (Mechanism 1) relies on understanding SGD as discrete samples from continuous gradient flow dθ/dτ = -∇L(θ), where step η_t maps to time interval η_t.
  - **Quick check question**: Given two schedules with the same total LR sum but different shapes, what does gradient flow predict about their final parameters?

- **Concept: Warmup-Stable-Decay (WSD) Schedules**
  - **Why needed here**: The paper's optimized schedules converge to WSD-like patterns. Understanding WSD's structure (warmup → constant η_max → decay phase) helps interpret why MPL's optimization discovers this shape.
  - **Quick check question**: In WSD, why might a "sqrt-cube" decay (η_t = η_max·(1-τ)^{1.5}) outperform linear or exponential decay?

## Architecture Onboarding

- **Component map**: Fitting pipeline (Input = {schedule-loss curve pairs} -> Huber loss objective -> Adam optimizer -> parameters Θ) -> Prediction module (L̂(t) = L_0 + A·(S_1(t)+S_W)^{-α} - Σ_k B(η_{k-1}-η_k)·[1-(Cη_k^{-γ}S_k(t)+1)^{-β}]) -> Schedule optimizer (Minimize L̂_Θ(E) subject to monotonicity constraints via Adam on Δ-vector)

- **Critical path**:
  1. Collect 2-3 training runs (constant + cosine + one two-stage recommended per Table 6)
  2. Fit MPL using entire loss curves (not just final points—key difference from CDSL)
  3. Validate on held-out schedules (WSD variants recommended as stress test)
  4. If R² > 0.99 on validation, proceed to schedule optimization

- **Design tradeoffs**:
  - **Fitting data**: More schedules → better generalization but higher compute. Paper shows 2-3 curves (constant + cosine + two-stage) suffice for 25M-400M models.
  - **Peak LR**: Higher η_max (≥6×10^-4) degrades accuracy (Figure 16). Trade peak LR vs. prediction reliability.
  - **Horizon extrapolation**: MPL extrapolates ~3× beyond training horizon (24K → 72K steps validated). Beyond this, uncertainty grows.

- **Failure signatures**:
  - **Oscillating predictions on non-monotonic schedules**: MPL derived for decay schedules; cyclic/random-polyline tests show it still works but with larger error near LR increase points.
  - **Trivial schedule collapse when optimizing with linear L_D approximation**: Setting G(x)=1 yields degenerate solution (η=η_max until last step). Use full power-law form.
  - **High β with near-zero C in fitting**: Indicates numerical instability; constraint β∈(0,1) recommended.

- **First 3 experiments**:
  1. **Sanity check**: Fit MPL on constant + cosine schedules (24K steps each). Predict 72K-step cosine. Verify R² > 0.99 and visualize zoomed final 5K steps (Figure 15 pattern).
  2. **Stress test decay forms**: Train WSD and WSDLD schedules (not in training set). Compare MPL prediction vs. actual loss curves. Check that decay-phase curvature matches (Figure 14b).
  3. **Schedule optimization pilot**: Fit MPL on your model scale. Optimize schedule by minimizing L̂_Θ(E). Compare final loss against cosine baseline (target: >0.02 improvement for 400M-scale per Figure 1b). Validate downstream task gains (LAMBADA, HellaSwag per Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Multi-Power Law be extended to handle varying peak learning rates within a unified framework?
- Basis in paper: [explicit] The conclusion states: "investigate empirical laws for schedule-aware loss curve prediction with varying peak LRs and other hyperparameters."
- Why unresolved: The current law assumes a fixed peak LR ηmax, with parameters fitted for that specific setting. The ablation (Figure 16) shows accuracy degrades at higher peak LRs (4×10⁻⁴ and 6×10⁻⁴).
- What evidence would resolve it: A unified formula where parameters scale predictably with peak LR, validated across schedules with different peak LRs without requiring separate fitting.

### Open Question 2
- Question: What is the theoretical explanation for the discrepancy between the empirical G function (dependent on ηk) and the theoretical G̃ function (independent of ηk)?
- Basis in paper: [explicit] Section 4 notes: "the form of G̃(Sk(t)) is not the same as G(ηk^-γ Sk(t)) in (2)... We suspect that this is due to that in practice, changing the learning rate can also change the local loss landscape, such as the Edge of Stability phenomenon."
- Why unresolved: The theoretical analysis assumes a fixed quadratic loss landscape, while practical training involves dynamic landscapes affected by LR changes.
- What evidence would resolve it: Analysis incorporating LR-dependent Hessian spectral properties or empirical validation linking EoS phenomena to the ηk-dependence in G.

### Open Question 3
- Question: Can the Multi-Power Law be extended beyond the quadratic loss assumption to capture the full complexity of deep learning loss landscapes?
- Basis in paper: [explicit] Section 4 states: "Extending the analysis of our Multi-Power Law beyond quadratic cases is a key direction for future work" and mentions the "river valley landscape" conjecture as a potential framework.
- Why unresolved: The current theoretical derivation (Theorem 1) relies on quadratic loss assumptions with specific power-law structures in Hessian and noise covariance matrices.
- What evidence would resolve it: A theoretical derivation for non-quadratic losses, or empirical validation that MPL parameters correlate with measured spectral properties of the actual Hessian during training.

### Open Question 4
- Question: Why does the coefficient β remain approximately constant across different LR scales, and what are its precise theoretical determinants?
- Basis in paper: [inferred] The paper notes deviations in long-horizon and high peak LR cases "likely due to several simplifications: (1) the coefficient β remains constant across different LR scales." The theoretical analysis suggests β depends on Hessian eigenvalue decay and gradient noise variance decay rates.
- Why unresolved: β is empirically fixed at 0.4 for convenience, but the paper lacks systematic investigation of when and why this approximation holds.
- What evidence would resolve it: Systematic measurement of β across architectures, scales, and peak LRs, correlated with measured spectral properties of H and Σ.

## Limitations
- **Dataset specificity**: Primary experiments use unnamed "data stream" dataset, creating uncertainty about transferability to other pretraining corpora
- **Peak learning rate sensitivity**: Law degrades for η_max ≥ 6×10^-4, suggesting narrow confidence region for η_max
- **Quadratic approximation boundary**: Theoretical derivation assumes quadratic loss near convergence, but real LLMs operate in highly non-convex landscapes

## Confidence
- **High confidence**: Mechanism 1 (LR sum as normalized progress) and empirical fitting pipeline work well within tested parameter ranges (25M-400M models, η_max ≤ 6×10^-4). The R² > 0.99 validation on held-out schedules is robust.
- **Medium confidence**: Schedule optimization produces measurable improvements (0.02-0.03 loss reduction), but these gains need validation on more diverse downstream tasks beyond LAMBADA/HellaSwag. The theoretical connection between spectral properties and MPL form is suggestive but not definitive.
- **Low confidence**: Extrapolation beyond 3× training horizon and application to architectures outside the tested range (25M-1B params) lack empirical validation. The law's behavior on multimodal or encoder-decoder architectures is unknown.

## Next Checks
1. **Cross-dataset robustness test**: Train the same 25M model on a different pretraining corpus (e.g., C4 vs. SlimPajama) and verify MPL maintains R² > 0.98 when fitting on constant + cosine schedules from the new dataset.

2. **Architecture scaling boundary**: Apply MPL to a 3B parameter model and test whether the same fitting pipeline (2-3 curves) achieves similar prediction accuracy. If accuracy drops, investigate whether larger models require more fitting schedules or modified parameter constraints.

3. **Downstream task transfer validation**: After optimizing schedules using MPL on pretraining loss, measure the actual impact on few-shot learning performance across 5+ diverse tasks (e.g., SuperGLUE, BBH, GSM8K) to verify that pretraining loss reduction translates to practical capability gains.