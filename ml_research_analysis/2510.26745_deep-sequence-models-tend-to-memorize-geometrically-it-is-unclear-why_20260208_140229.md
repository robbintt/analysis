---
ver: rpa2
title: Deep sequence models tend to memorize geometrically; it is unclear why
arxiv_id: '2510.26745'
source_url: https://arxiv.org/abs/2510.26745
tags:
- memory
- path
- learning
- token
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how deep sequence models like Transformers
  and Mamba store and retrieve atomic facts. The authors show that instead of using
  associative memory (a lookup table of local associations), these models often employ
  geometric memory, where embeddings encode global relationships inferred from local
  co-occurrences in training data.
---

# Deep sequence models tend to memorize geometrically; it is unclear why

## Quick Facts
- arXiv ID: 2510.26745
- Source URL: https://arxiv.org/abs/2510.26745
- Reference count: 40
- Key outcome: Deep sequence models like Transformers and Mamba often store atomic facts geometrically rather than as associative lookups, enabling implicit reasoning through geometric navigation.

## Executive Summary
This paper investigates how deep sequence models memorize atomic facts. Contrary to conventional wisdom that models use associative lookup tables, the authors demonstrate that Transformers and Mamba architectures tend to organize memorized information geometrically in the embedding space. This geometric organization allows models to perform implicit reasoning by transforming complex compositional tasks into simple geometric navigation problems. The phenomenon emerges naturally from training dynamics, even without explicit architectural constraints, and challenges traditional understandings of memory, generalization, and optimization in neural networks.

## Method Summary
The paper uses synthetic graph tasks (path-star and grid graphs) to study how sequence models memorize and retrieve information. Models are trained on local edge-memorization tasks and path-finding tasks, with evaluation focusing on exact-match path accuracy for held-out leaves and first-token accuracy. The analysis spans both Transformer (GPT-mid) and Mamba architectures, examining weight-tied vs. weight-untied variants. Key methods include UMAP visualization of embeddings, spectral analysis of learned representations, and ablation studies varying graph structure, training procedures, and architectural choices.

## Key Results
- Sequence models organize memorized atomic facts geometrically rather than as associative lookup tables
- Geometric memory enables implicit reasoning by transforming ℓ-fold compositional tasks into simple 1-step geometric navigation
- The geometric bias arises naturally from spectral dynamics during training, even without explicit architectural or regularization pressures
- This phenomenon is observed across both attention-based Transformers and state-space Mamba models

## Why This Works (Mechanism)

### Mechanism 1: Spectral Bias Drives Geometric Formation
- **Claim:** Geometric memory arises because gradient descent on cross-entropy loss exhibits spectral bias, causing embeddings to align with top eigenvectors of the graph Laplacian
- **Mechanism:** Training dynamics on local edges implicitly perform low-rank factorization of the adjacency matrix, filtering out lower eigenvectors over time and leaving Fiedler-like vectors encoding global structure
- **Core assumption:** Cross-entropy loss on local co-occurrences with weight-tying
- **Evidence anchors:** Abstract states geometry stems from spectral bias; Section 4 describes gradual filtering of lower eigenvectors from embedding matrix
- **Break condition:** Removing weight-tying or freezing embeddings prevents global geometry formation

### Mechanism 2: Implicit Reasoning Through Geometric Navigation
- **Claim:** Implicit reasoning is achieved by transforming compositional queries into simple geometric navigation tasks
- **Mechanism:** Instead of composing ℓ lookup functions, models place embeddings so that distance between query and target becomes a simple 1-hop retrieval problem
- **Core assumption:** Training distribution contains sufficient local edges to construct global manifold structure
- **Evidence anchors:** Abstract describes transformation of hard reasoning tasks into easy navigation; Section 2.4 quantifies reduced learning complexity
- **Break condition:** Adversarial graph topologies or in-context training can block gradient flow needed for geometry formation

### Mechanism 3: The Memorization Puzzle
- **Claim:** Why geometric memory wins over associative lookup remains theoretically unresolved, though geometry appears to be the stable attractor
- **Mechanism:** Authors refute standard explanations (supervisory pressure, capacity bottlenecks, ease of discovery), noting associative memory is actually easier to find but model converges to geometric solution
- **Core assumption:** Phenomenon is intrinsic to optimization process, not a regularization trick
- **Evidence anchors:** Abstract argues rise of geometry cannot be attributed to typical pressures; Section 3.2 observes associative memory is easier to find yet model eventually chooses geometry
- **Break condition:** Heavily constrained embedding dimensions may force default to easier associative lookup

## Foundational Learning

- **Concept:** Associative vs. Geometric Memory
  - **Why needed here:** Paper redefines memorization from brute-force lookup to structural synthesis. Must distinguish $\ell$-fold composition of local lookups from geometric embedding products.
  - **Quick check question:** Does a model storing "A is B" associatively know anything about relationship between "A" and "C" if they never co-occurred? Does geometric model?

- **Concept:** Graph Laplacian & Fiedler Vector
  - **Why needed here:** Paper explains geometry as spectral phenomenon. Understanding Fiedler vector's role in graph partitioning is essential to why model learns global structure from local edges.
  - **Quick check question:** Why does paper claim embedding space aligns with eigenvectors of graph Laplacian?

- **Concept:** In-Weights vs. In-Context Reasoning
  - **Why needed here:** Paper contrasts reading graph from prompt vs. storing in parameters. In-Weights setting allows overcoming "Clever Hans" failure mode.
  - **Quick check question:** Why does "hardest token" (first token) fail in in-context path-star but succeed in in-weights task?

## Architecture Onboarding

- **Component map:** Input token sequence -> Embeddings ($V \in \mathbb{R}^{n \times m}$) -> Core Architecture (Transformer or Mamba) -> Output unembedding matrix

- **Critical path:**
  1. Training: Train on local edge-memorization examples $(u, v)$
  2. Formation: Monitor cosine distance between non-adjacent nodes to verify geometry formation
  3. Testing: Evaluate on held-out path-finding tasks (predicting path from leaf to root)

- **Design tradeoffs:**
  - Weight Tying: Essential for clean geometry; weight-untied models produce zig-zag embeddings obscuring global structure
  - Reverse Edges: For large graphs, training on both $(u, v)$ and $(v, u)$ necessary to overcome reversal curse

- **Failure signatures:**
  - The "Zig-Zag": UMAP projections look chaotic or bipartite instead of clustered; check if weight-tying is disabled
  - Random First Token: Model predicts path correctly only after first token; indicates "Clever Hans" cheat learned but geometry failed
  - Collapsed Geometry: Embeddings for all nodes nearly identical; happens if training continues too long without early stopping

- **First 3 experiments:**
  1. Tiny Graph Visualization: Train 1-layer model on tiny path-star/grid graph; visualize embeddings with UMAP to verify paths cluster linearly
  2. Spectral Alignment Check: Calculate graph Laplacian for small training graph; project learned embeddings onto Laplacian eigenvectors to confirm spectral bias
  3. The "Hardest Token" Test: Train on edge-memorization only; freeze model; finetune only on first-token prediction task for held-out leaf; should succeed per Observation 1

## Open Questions the Paper Calls Out

- **Open Question 1:** Why does gradient-descent-trained sequence model prefer geometric memory over associative memory, even when associative storage is expressible, easier-to-find, and equally succinct?
- **Open Question 2:** To what extent do models memorize data geometrically versus associatively, and what specific graph complexities or optimization parameters control this bias?
- **Open Question 3:** What specific reasoning benefits does "bipartite geometric memory" offer compared to "global" geometric memory found in weight-tied models?
- **Open Question 4:** Do geometric memory structures and resulting implicit reasoning capabilities extend to natural language tasks given unstructured nature of tokenization?

## Limitations
- The central puzzle of why geometric memory emerges as stable attractor over associative lookup remains theoretically open
- Analysis focuses on synthetic path-star and grid graphs, which may not represent natural language or multimodal data complexity
- The conditions under which models default to associative shortcuts versus forming geometric structure in practice remain unclear

## Confidence
- **High Confidence:** Empirical demonstration that Transformers and Mamba form geometric memory on synthetic graph tasks; spectral alignment observations; refutation of associative memory as default stable state
- **Medium Confidence:** Proposed mechanism linking cross-entropy training dynamics to spectral bias and Fiedler vector alignment
- **Medium Confidence:** Claim that geometric memory enables implicit reasoning by transforming compositional tasks into geometric navigation

## Next Checks
1. **Real-World Graph Validation:** Test geometric memory formation on real-world graph datasets (knowledge graphs, social networks) to verify spectral bias mechanism generalizes beyond synthetic structures
2. **Theoretical Mechanistic Analysis:** Develop formal theoretical framework connecting cross-entropy loss landscape, gradient dynamics, and spectral properties of learned embeddings
3. **Transfer to Downstream Tasks:** Evaluate whether models with geometric memory demonstrate improved performance on downstream reasoning tasks compared to associative lookup models to assess practical benefits