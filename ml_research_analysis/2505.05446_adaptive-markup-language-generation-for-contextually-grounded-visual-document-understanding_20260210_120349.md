---
ver: rpa2
title: Adaptive Markup Language Generation for Contextually-Grounded Visual Document
  Understanding
arxiv_id: '2505.05446'
source_url: https://arxiv.org/abs/2505.05446
tags:
- document
- markup
- language
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of improving visual document
  understanding in multimodal large language models (MLLMs) by incorporating structured
  markup language generation. The authors propose a two-stage framework: (1) a multi-task
  pretraining stage using DocMark-Pile, a dataset of 3.8M examples converting various
  document types into structured markup languages (Markdown, LaTeX, HTML, JSON, TikZ,
  plain text), and (2) a fine-tuning stage using DocMark-Instruct, a 624k dataset
  with chain-of-thought reasoning annotations.'
---

# Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding

## Quick Facts
- arXiv ID: 2505.05446
- Source URL: https://arxiv.org/abs/2505.05446
- Authors: Han Xiao; Yina Xie; Guanxin Tan; Yinghao Chen; Rui Hu; Ke Wang; Aojun Zhou; Hao Li; Hao Shao; Xudong Lu; Peng Gao; Yafei Wen; Xiaoxin Chen; Shuai Ren; Hongsheng Li
- Reference count: 40
- The paper proposes a two-stage framework using adaptive markup language generation (Markdown, LaTeX, HTML, JSON, TikZ, plain text) to improve multimodal large language model performance on visual document understanding tasks.

## Executive Summary
This paper addresses the challenge of improving visual document understanding in multimodal large language models (MLLMs) by incorporating structured markup language generation. The authors propose a two-stage framework: (1) a multi-task pretraining stage using DocMark-Pile, a dataset of 3.8M examples converting various document types into structured markup languages, and (2) a fine-tuning stage using DocMark-Instruct, a 624k dataset with chain-of-thought reasoning annotations. The model learns to adaptively generate relevant context in appropriate markup formats to answer questions, mimicking human-like reasoning. Extensive experiments show that DocMark-2B and DocMark-8B significantly outperform state-of-the-art MLLMs on multiple document understanding benchmarks.

## Method Summary
The framework uses InternViT encoder + InternLM-2B/8B with MLP projector and dynamic resolution (448×448 tiles, max 12). Stage 1: pretrain on DocMark-Pile + LLaVA-558K (1 epoch, lr=2e-5, batch=128) to learn markup generation. Stage 2: fine-tune on DocMark-Instruct + general instruction data (1 epoch, lr=1e-5, batch=128) using two-round CoT format: Q1 extracts context → A1 generates <type>context</type>, Q2 answers → A2 produces final answer. Loss is applied to both A1 and A2. Inference follows the two-round template.

## Key Results
- DocMark-2B and DocMark-8B outperform state-of-the-art MLLMs on multiple document understanding benchmarks
- Accuracy improvements: 2-3% on TextVQA, 4-6% on DocVQA, and 2-4% on ChartQA compared to InternVL2 models
- Specialized task performance: HME100K ExpRate +1.6%, ChartQA-SE AP@strict +2.0%, DaTikZ-test similarity +1.8%, KIE F1 +1.3%

## Why This Works (Mechanism)

### Mechanism 1: Structured Markup as Intermediate Representation
Converting visual documents into structured markup languages provides explicit contextual grounding that improves downstream QA accuracy. The model learns bidirectional mappings: image→markup during pretraining, then markup→answer during fine-tuning. This decomposes the perception-reasoning pipeline, allowing the LLM backbone to process structured representations it already "understands" rather than raw visual features.

### Mechanism 2: Adaptive Format Selection Reduces Hallucination
Allowing the model to autonomously select the appropriate markup type per document reduces irrelevant context and focuses reasoning on task-relevant information. During fine-tuning, the model produces context tokens `<type>...</type>` before answering, acting as implicit routing: HTML for webpages, JSON for charts, Markdown for tables.

### Mechanism 3: CoT Fine-Tuning Enforces Grounded Reasoning
Training on chain-of-thought annotations where intermediate context is extracted before answering improves reasoning accuracy compared to direct QA fine-tuning. The DocMark-Instruct dataset provides supervision for the two-step reasoning process, forcing the model to produce faithful intermediate representations and reducing hallucination.

## Foundational Learning

- **Structured Document Representation Formats** (Markdown, JSON, HTML, LaTeX, TikZ)
  - Why needed here: The entire framework depends on understanding how different document types map to their source representations
  - Quick check question: Given a table image, can you manually write its Markdown representation? For a bar chart, can you construct the equivalent JSON with title, axes, and values?

- **Chain-of-Thought Reasoning in LLMs**
  - Why needed here: The adaptive generation pipeline is fundamentally a CoT approach applied to multimodal inputs
  - Quick check question: Explain why prompting an LLM to "think step by step" before answering often improves accuracy on reasoning tasks

- **Vision-Language Model Architectures** (Vision Encoder → Projector → LLM)
  - Why needed here: DocMark builds on InternVL's architecture
  - Quick check question: Sketch the data flow from image input through InternViT, MLP projector, to InternLM. Where does markup token generation occur?

## Architecture Onboarding

- **Component map:**
Input Image → InternViT (dynamic resolution, max 12 tiles @ 448×448) → MLP Projector → Visual Tokens → Concatenate with Text Tokens (including markup special tokens) → InternLM-2B/8B → Autoregressive Generation → Stage 1: Image → Markup (<md>, <json>, <html>, <latex>, <tikz>, <txt>) → Stage 2: Question → Context (<type>...</type>) → Answer

- **Critical path:**
1. Pretraining on DocMark-Pile (1 epoch, LR 2e-5, batch 128) teaches markup generation
2. Fine-tuning on DocMark-Instruct (1 epoch, LR 1e-5, batch 128) teaches adaptive context extraction
3. Inference: Two-step dialogue where Q1 triggers context generation, Q2 triggers final answer

- **Design tradeoffs:**
- Dynamic resolution vs. computational cost: More tiles capture finer detail but increase token count
- Six markup types vs. specialization: Broad coverage but may underperform compared to single-format specialist models
- Automated CoT annotation vs. quality: Scalable (624k examples) but introduces potential noise from ChatGPT-3.5 errors

- **Failure signatures:**
- Low performance on documents requiring spatial reasoning not captured in linear markup
- Context token outputs like "unclear" for questions requiring visual-only reasoning
- Incorrect format selection leading to missing information
- Hallucinated markup content not present in the image

- **First 3 experiments:**
1. Reproduce markup generation quality: Run DocMark-2B on 50 samples each from HME100K, ChartQA-SE, and DaTikZ-test. Compare generated markup against ground truth using task-specific metrics
2. Ablate CoT fine-tuning: Train DocMark-2B on DocMark-Pile only, then evaluate on DocVQA and ChartQA with and without manually injected CoT prompts
3. Analyze format selection accuracy: On a held-out set of 200 images across types, log which markup format the model selects and correlate with downstream QA performance

## Open Questions the Paper Calls Out

### Open Question 1
How would integrating advanced resolution scaling strategies (beyond dynamic 448×448 partitioning) impact the performance of the adaptive markup generation framework on extremely high-resolution documents? [explicit] Section 4.1 states, "As our work focuses on incorporating adaptive markup language generation... we do not explore additional resolution scaling strategies in this paper."

### Open Question 2
Can the pretraining objective be modified to preserve visual style attributes (e.g., font sizes, colors) without degrading the model's ability to parse structural content? [explicit] Supplementary Material C.1 notes, "our method might not preserve the style information... This is because our pre-training mainly concentrates on parsing the structured information."

### Open Question 3
Is the adaptive generation framework effective across different MLLM architectures, or is it dependent on the specific visual features of the InternVL encoder? [inferred] The experiments (Section 4.1) exclusively utilize the InternVL architecture, without validating the pipeline on alternative backbones like LLaVA or Qwen-VL.

## Limitations
- The reliance on ChatGPT-3.5 for CoT annotation introduces potential quality variability that is not systematically evaluated
- The adaptive format selection mechanism lacks explicit supervision during fine-tuning, depending on the model to learn format routing implicitly
- The paper does not provide ablations isolating the individual contributions of structured markup generation versus CoT reasoning
- Computational requirements for dynamic resolution may limit accessibility for reproduction

## Confidence
- **High confidence:** The core architecture (InternViT → MLP Projector → InternLM with markup generation) is well-specified and reproducible
- **Medium confidence:** Performance improvements on benchmarks are demonstrated, but the exact contribution of each component is not isolated through ablations
- **Low confidence:** The quality and consistency of ChatGPT-3.5 generated CoT annotations, and their impact on model behavior, is not empirically validated

## Next Checks
1. Validate annotation quality: Manually review 100 randomly selected DocMark-Instruct examples to assess CoT annotation accuracy
2. Ablate markup formats: Train models using only Markdown or only JSON throughout both pretraining and fine-tuning stages, then compare performance across document types
3. Test visual reasoning limits: Design and evaluate on a set of 100 visual-only questions that cannot be answered from markup alone to determine when the framework fails