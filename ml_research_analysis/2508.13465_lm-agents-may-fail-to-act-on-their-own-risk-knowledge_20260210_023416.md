---
ver: rpa2
title: LM Agents May Fail to Act on Their Own Risk Knowledge
arxiv_id: '2508.13465'
source_url: https://arxiv.org/abs/2508.13465
tags:
- agent
- user
- tool
- risky
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical gap between LM agents' risk awareness
  and their safe execution behaviors. While agents demonstrate near-perfect knowledge
  of risks in abstract settings (98% pass rates), they fail to identify risks in concrete
  scenarios (performance drops 23%) and often execute risky actions directly (<26%
  pass rates).
---

# LM Agents May Fail to Act on Their Own Risk Knowledge

## Quick Facts
- arXiv ID: 2508.13465
- Source URL: https://arxiv.org/abs/2508.13465
- Reference count: 40
- LM agents show >98% risk knowledge in abstract settings but fail to identify risks in concrete scenarios (>23% drop) and execute risky actions directly (<26% pass rate)

## Executive Summary
This paper identifies a critical safety gap in LM agents: while they demonstrate near-perfect knowledge of risks in abstract scenarios, they fail to identify and avoid these same risks when executing concrete actions. The study reveals three progressive gaps - knowledge vs. identification vs. execution - that persist across model scales and even in specialized reasoning models. To address this, the authors develop a risk verifier system that independently critiques agent actions using trajectory abstraction, achieving a 55.3% reduction in risky execution while maintaining task performance.

## Method Summary
The study evaluates LM agent safety across three progressive tests using 144 curated risky trajectories spanning 36 toolkits and 9 risk categories. The risk verifier system adds an independent critic module that reviews proposed agent actions, augmented with an abstractor that converts specific execution traces into high-level scenario descriptions. The verifier provides critiques when risks are detected, prompting the agent to revise actions. This leverages the observation that models are better at identifying risks in completed actions than at generating safe actions from scratch.

## Key Results
- Agents achieve >98% pass rate on abstract risk knowledge but drop to ~78% on concrete identification and only ~13% on execution
- Performance gaps persist across model scales and reasoning models like DeepSeek-R1
- Risk verifier with abstractor reduces risky action execution by 55.3% compared to vanilla agents
- The system maintains comparable helpfulness scores while significantly improving safety

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating risk verification from action generation can reduce the identification-execution gap.
- Mechanism: An independent verifier module reviews proposed agent actions before execution. When risks are detected, the agent is prompted to revise its action. This leverages the observed phenomenon that models are better at identifying risks in completed actions ("validating") than at generating safe actions from scratch.
- Core assumption: The model's validation capabilities are stronger than its generation capabilities.
- Evidence anchors: [abstract] "Our evaluation reveals two critical performance gaps that resemble the generator-validator gaps observed in LLMs"
- Break condition: If the verifier model itself suffers from the same identification-execution gap, the system degrades to vanilla agent performance.

### Mechanism 2
- Claim: Abstracting concrete execution trajectories into high-level descriptions improves risk identification.
- Mechanism: An "abstractor" converts specific, detailed execution traces into scenario-level summaries. This aligns the input format with the format where models demonstrate near-perfect risk knowledge.
- Core assumption: Models fail to identify risks in concrete trajectories due to distracting details or format mismatch.
- Evidence anchors: [Section 4.2] "by first abstracting the trajectories, the verifier can better bridge the gap between abstract risk knowledge and concrete scenario assessment"
- Break condition: If abstraction fails to capture essential risk-relevant aspects or over-generalizes, the verifier may miss risks or flag safe actions.

### Mechanism 3
- Claim: The knowledge-identification and identification-execution gaps persist across model scale and reasoning capabilities.
- Mechanism: The gaps appear rooted in how alignment is typically performed (on conversational rather than agentic scenarios) and in inherent model inconsistencies, not in lack of model capability.
- Core assumption: The gaps are structural artifacts of current training and alignment paradigms.
- Evidence anchors: [Figure 2] Shows no clear negative correlation between model capability and the gaps; correlation is near zero (Spearman's ρ = -0.06).
- Break condition: If future alignment procedures specifically target agentic scenarios at scale, these gaps may diminish.

## Foundational Learning

- Concept: Generator-Validator Gap in LLMs
  - Why needed here: This is the foundational phenomenon the paper builds upon. Understanding that models can be inconsistent between "generating" content and "validating" the same content is essential to grasp why a separate verifier helps.
  - Quick check question: Can you explain why a model might correctly label an action as risky when asked to judge it, but generate the same risky action when asked to perform a task?

- Concept: Agentic vs. Conversational Alignment
  - Why needed here: The paper hypothesizes that alignment on conversational data may not transfer to agentic scenarios. This explains why "more capable" models don't automatically become safer agents.
  - Quick check question: What is the key difference in context between a chat model answering a safety question and an agent executing a task that might explain the performance gap?

- Concept: Trajectory Abstraction for Risk Identification
  - Why needed here: This is the core technical contribution—the abstractor. Understanding why converting specifics to abstractions helps (matches the format where models have high knowledge) is critical to implementing or extending this work.
  - Quick check question: Given the example in Figure 4, what key risk-relevant information did the abstraction capture that the raw trajectory details may have obscured?

## Architecture Onboarding

- Component map: Agent -> Abstractor (optional) -> Verifier -> Iteration Loop -> Safety/Helpfulness Evaluators

- Critical path: User provides instruction to Agent → Agent proposes action → [If enabled] Abstractor summarizes trajectory → Verifier reviews and outputs critique/approval → If risky, Agent revises action and loop repeats (max k times) → Final trajectory evaluated by Safety and Helpfulness Evaluators

- Design tradeoffs:
  1. **Verifier Model Choice**: Using larger/more capable model increases safety but adds cost/latency; same model is cost-effective but may inherit agent's blind spots.
  2. **Abstraction Granularity**: Too detailed loses the "abstract" benefit; too vague may omit critical risk cues.
  3. **Iteration Count (k)**: More iterations allow more revision but with diminishing returns and higher cost; paper finds k=1 provides most gain efficiently.

- Failure signatures:
  1. **Verifier Failure to Identify Risk**: Approves clearly risky action (e.g., sending sensitive data without consent).
  2. **Abstraction Loss**: Omits critical detail leading verifier to miss risk.
  3. **Over-Critique / Paralysis**: Overly conservative verifier blocking many actions and reducing helpfulness.

- First 3 experiments:
  1. **Baseline Reproduction**: Implement vanilla agent and safety-prompted agent on ToolEmu test cases to confirm gaps exist.
  2. **Verifier Ablation**: Implement risk verifier (without abstractor) on baseline agent to measure safety improvement.
  3. **Abstraction Impact**: Add abstractor to verifier system and compare performance on privacy/consent scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the fundamental underlying causes of the safety awareness-execution gaps in language model agents?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section: "the underlying causes of these gaps remain unclear."
- **Why unresolved:** The study demonstrates the existence of the gaps but only hypothesizes that current conversational alignment procedures are a potential cause without providing mechanistic proof.
- **What evidence would resolve it:** Ablation studies on alignment data composition or mechanistic interpretability analyses identifying distinct activation patterns.

### Open Question 2
- **Question:** Can specialized agentic alignment training effectively bridge the identified safety gaps?
- **Basis in paper:** [explicit] The authors propose: "A promising future direction is to conduct more extensive agentic alignment training with the ToolEmu framework."
- **Why unresolved:** The current study shows that scaling base model capabilities or inference-time compute does not resolve the gaps.
- **What evidence would resolve it:** Training models specifically on agentic trajectory datasets and re-evaluating to see if gaps disappear.

### Open Question 3
- **Question:** How can systems be designed to achieve the optimal safety-helpfulness frontier?
- **Basis in paper:** [explicit] The authors conclude: "Developing a system that effectively achieves the safety-helpfulness frontier is crucial for practical deployment."
- **Why unresolved:** While the risk verifier significantly improves safety, it does not inherently improve helpfulness.
- **What evidence would resolve it:** Novel architectures demonstrating simultaneous increases in helpfulness scores while maintaining high safety pass rates.

## Limitations
- The underlying causes of the safety gaps remain unclear and may be structural artifacts of current training paradigms
- The abstractor's effectiveness depends on capturing risk-relevant information without introducing bias, which may vary across domains
- The evaluation framework relies on LM-based safety judgments that could introduce their own biases or inconsistencies

## Confidence
- **High Confidence**: The existence of the three progressive gaps (knowledge vs. identification vs. execution) is well-established through extensive testing across multiple model scales and architectures.
- **Medium Confidence**: The claim that these gaps are primarily structural artifacts rather than capability limitations is supported but not definitively proven.
- **Medium Confidence**: The abstractor's mechanism for bridging the gap is well-supported for tested risk categories but generalizability requires further validation.

## Next Checks
1. **Cross-Domain Validation**: Test the risk verifier system on agent tasks from domains not represented in the original 144 test cases (e.g., healthcare, legal, or autonomous vehicle scenarios) to assess generalizability across risk categories.

2. **Multi-Turn Sequence Testing**: Extend the evaluation framework to include multi-step agent trajectories where risks accumulate or compound over time, measuring whether the verifier system maintains effectiveness in more complex scenarios.

3. **Alignment Method Ablation**: Compare the gap sizes and verifier effectiveness across models trained with different alignment approaches (RLHF, constitutional AI, DPO) to determine whether the gaps are structural artifacts or artifacts of specific alignment techniques.