---
ver: rpa2
title: 'Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy
  Filters for Synthetic Data Generation'
arxiv_id: '2510.01793'
source_url: https://arxiv.org/abs/2510.01793
tags:
- images
- privacy
- training
- filters
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study critically evaluates post-hoc privacy filters for synthetic
  data generation in medical imaging. While such filters aim to remove privacy-sensitive
  samples from synthetic datasets, the authors demonstrate that current implementations
  fail to reliably detect near-duplicates of training data and exhibit poor specificity.
---

# Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation

## Quick Facts
- arXiv ID: 2510.01793
- Source URL: https://arxiv.org/abs/2510.01793
- Reference count: 14
- Primary result: Current privacy filters for synthetic medical data generation show poor specificity and consistency, failing to reliably detect privacy-sensitive near-duplicates while generating excessive false positives.

## Executive Summary
This study evaluates post-hoc privacy filters designed to remove privacy-sensitive samples from synthetic medical imaging datasets. Using a tripartite evaluation framework measuring sensitivity (detecting true patient matches), specificity (avoiding false positives on unseen patients), and consistency (agreement across random seeds), the authors demonstrate that current implementations fail to reliably protect patient privacy. Applied to chest X-ray synthesis, filters achieved high sensitivity only for real images from training patients, with flagging rates dropping from 88.5% to 8.7% when comparing only same-patient matches. Critically, synthetic near-duplicates of training images were rarely detected, and specificity was poor with 84.3% of holdout images incorrectly flagged. The findings indicate that current privacy filters provide inadequate protection and may create a false sense of security.

## Method Summary
The study evaluates privacy filters using contrastive learning to cluster same-patient images in latent space, with similarity measured via Pearson correlation. The filtering process involves training a ResNet-style encoder to minimize distances between same-patient images while maximizing distances between different patients, then calibrating a threshold as the 95th percentile of training-validation pair correlations. The evaluation is conducted on MIMIC-CXR data using a RoentGen diffusion model, testing both pixel-space and latent-space variants. Three key test scenarios are evaluated: holdout images from training patients (sensitivity), holdout images from unseen patients (specificity), and synthetic near-duplicates generated by prompting the diffusion model with exact training prompts. Ten independently trained filters with different random seeds assess consistency.

## Key Results
- Privacy filters achieved high sensitivity (88.5%) for real images from training patients but dropped to 8.7% when comparing only same-patient matches
- Synthetic near-duplicates of training images were rarely detected, indicating filters fail to catch the primary privacy attack vector
- Specificity was poor, with 84.3% of holdout images from unseen patients incorrectly flagged as privacy risks
- Inter-filter consistency was virtually nonexistent, with unanimous consensus achieved on less than 1% of images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning can cluster same-patient images in latent space for privacy detection.
- Mechanism: A neural network encoder is trained with contrastive loss to minimize distance between images from the same patient while maximizing distance between different patients. Pairwise similarity is measured via Pearson correlation in the learned embedding space.
- Core assumption: Intra-patient feature variance is smaller than inter-patient feature variance, enabling a distance threshold to separate privacy-violating samples from safe ones.
- Evidence anchors:
  - [Section 2]: "trained using contrastive learning to 'pull-together' (in latent space) video frames from the same patient and 'push apart' frames from different patients"
  - [Section 3.1]: "we use images of the same patients as positive pairs and images of different patients as negative pairs"
  - [corpus]: Weak corpus support—no directly comparable contrastive privacy filtering papers found.
- Break condition: If inter-patient similarity exceeds intra-patient similarity (common in medical imaging), the learned representations cannot reliably separate patients.

### Mechanism 2
- Claim: A calibrated threshold on correlation scores determines privacy flagging decisions.
- Mechanism: The 95th percentile of training-validation correlation scores sets threshold τ. Synthetic images with correlation > τ to any training image are flagged as privacy-critical.
- Core assumption: The 95th percentile captures natural variation in same-patient similarity while excluding most cross-patient matches.
- Evidence anchors:
  - [Section 2]: "decision threshold τ is then calculated as the 95th percentile of these values"
  - [Section 4, Table 1]: Flagging rates drop from 88.5% (overall pool) to 8.7% (same-patient only), suggesting the threshold is miscalibrated
  - [corpus]: No corpus evidence on threshold calibration for privacy filtering.
- Break condition: If the reference set distribution doesn't reflect true same-patient similarity distribution, the threshold will systematically over- or under-flag.

### Mechanism 3
- Claim: Filters can operate in either pixel space or latent space for similarity detection.
- Mechanism: Pixel-space filters compare high-resolution synthetic images directly; latent-space filters compare VAE-encoded representations. Both use the same contrastive encoder and threshold mechanism.
- Core assumption: Both spaces preserve patient-identifying features with sufficient signal-to-noise ratio.
- Evidence anchors:
  - [Section 3.1]: "We extend this approach and additionally evaluate how effectively a privacy filter can operate in pixel space, directly on the high-resolution synthetic images"
  - [Section 4]: Pixel space showed 59.8% vs latent space 8.7% for same-patient maximum strategy
  - [corpus]: No corpus evidence on pixel vs. latent space comparison for privacy.
- Break condition: If latent compression discards patient-specific features OR pixel-space noise dominates signal, neither space provides reliable detection.

## Foundational Learning

- Concept: **Contrastive Learning**
  - Why needed here: The entire filtering architecture depends on understanding how contrastive loss shapes embedding spaces and what "same-class" means for patient identity.
  - Quick check question: Can you explain why maximizing inter-patient distance might fail when patients share similar pathologies?

- Concept: **Membership Inference Attacks (MIAs)**
  - Why needed here: Privacy filtering is a defensive response to MIAs; understanding attack vectors clarifies what the filter must detect.
  - Quick check question: How would you test whether a synthetic image leaks training data membership?

- Concept: **Diffusion Model Memorization**
  - Why needed here: The paper exploits diffusion models' tendency to generate near-duplicates when conditioned on training prompts—this is the attack vector filters must catch.
  - Quick check question: Why might a diffusion model reproduce training images, and how does text conditioning affect this?

## Architecture Onboarding

- Component map: Contrastive Encoder -> Reference Dataset -> Threshold Calibration Module -> Similarity Scorer -> Decision Layer

- Critical path: Reference dataset quality -> Contrastive training convergence -> Threshold calibration -> Similarity scoring accuracy -> Flagging decision. The paper shows failures at steps 2, 3, and 5.

- Design tradeoffs:
  - Latent space: Faster computation, but lower sensitivity (8.7% vs 59.8% for same-patient detection)
  - Pixel space: Higher sensitivity but higher false positive rate (84.3% holdout flagging)
  - Threshold percentile: Higher = fewer false positives but more privacy leaks; lower = better detection but more rejected samples
  - Random seed: Different initializations produce fundamentally different flagging decisions (unanimous consensus <1%)

- Failure signatures:
  - High overall flagging rate but low same-patient detection -> spurious cross-patient matches
  - Flagging unseen patients -> specificity failure, threshold too low or representation not discriminative
  - Cross-seed disagreement -> unstable training, representation collapse, or threshold sensitivity
  - Synthetic near-duplicates pass filter -> the attack vector filters were designed for (Figure 1 shows visual duplicates that passed)

- First 3 experiments:
  1. **Reproduce same-patient flagging gap**: Train filter on MIMIC-CXR subset, test on held-out images from same patients. Verify that "overall" flagging >> "same-patient-only" flagging.
  2. **Synthetic near-duplicate injection**: Generate images using exact training prompts (per Carlini et al.), measure detection rate. Expect ~8.7% based on paper.
  3. **Cross-seed consistency check**: Train N=5 filters with different seeds, compute Cohen's kappa on flagging decisions for 10K synthetic images. Expect near-zero agreement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the poor specificity and consistency observed in chest X-ray filters generalize to other medical imaging modalities?
- Basis in paper: [explicit] The authors state in the Discussion that "A more comprehensive assessment would involve testing multiple filters across a wider variety of medical image modalities and datasets."
- Why unresolved: The study was restricted to chest X-rays (MIMIC-CXR and CheXpert), leaving the performance characteristics in other domains unknown.
- What evidence would resolve it: Applying the same tripartite evaluation protocol (sensitivity, specificity, consistency) to modalities such as MRI, CT, or histopathology slides.

### Open Question 2
- Question: How does the size of the reference training pool influence the sensitivity and specificity trade-offs in privacy filtering?
- Basis in paper: [explicit] The authors note that "recent theoretical research has shown a strong influence of data pool size on filter performance" and explicitly call for "future investigations" to explore this relationship.
- Why unresolved: The current study fixed the reference dataset size; thus, the scaling dynamics of the threshold τ and flagging rates relative to pool size remain unmeasured.
- What evidence would resolve it: Empirical results showing flagging rates and threshold drift across systematically varied reference dataset sizes (e.g., 1k, 10k, 100k images).

### Open Question 3
- Question: What architectural or training modifications are required to ensure privacy filters produce consistent, deterministic decisions across different random seeds?
- Basis in paper: [inferred] The paper reveals that filtering decisions are "dominated by random initialization effects," with unanimous consensus achieved on less than 1% of images.
- Why unresolved: Current contrastive learning approaches appear unstable, producing contradictory patient attributions for the exact same synthetic image depending solely on the seed.
- What evidence would resolve it: A modified filter design that achieves high inter-rater agreement (e.g., >90% consensus) across multiple independently trained models on the same synthetic dataset.

### Open Question 4
- Question: How can privacy filters be adapted to reliably detect synthetic near-duplicates of training data, rather than just matching real patient images?
- Basis in paper: [inferred] The study highlights a critical failure where filters detect real same-patient images (88.5%) but fail to flag synthetic near-duplicates of those same patients (8.7%).
- Why unresolved: The latent-space representations of synthetic duplicates may diverge from real images in ways that current Pearson correlation thresholds cannot capture.
- What evidence would resolve it: A filter that maintains high sensitivity on the "synthetic ground truth samples" (near-duplicates) generated via prompt engineering.

## Limitations
- Evaluation restricted to chest X-ray data and a single generative model (RoentGen), limiting generalizability to other medical imaging modalities and architectures.
- Contrastive learning framework's effectiveness depends on the assumption that same-patient images are more similar than different-patient images, which may not hold for certain pathologies.
- Threshold calibration method (95th percentile) lacks theoretical grounding for optimal privacy protection and may be sensitive to reference dataset composition.
- Evaluation does not account for potential adversarial attacks that could deliberately evade detection.

## Confidence
- High confidence: The tripartite evaluation framework (sensitivity, specificity, consistency) is methodologically sound and the empirical findings about filter limitations are robust within the tested domain.
- Medium confidence: The generalizability of findings to other medical imaging contexts and generative models remains uncertain.
- Low confidence: The optimal threshold calibration strategy for privacy filters and the theoretical guarantees of contrastive learning for patient identity separation.

## Next Checks
1. Replicate the evaluation framework on a different medical imaging modality (e.g., retinal fundus photography or histopathology images) to assess generalizability of privacy filter failures.
2. Test alternative threshold calibration strategies (e.g., ROC curve optimization, cost-sensitive thresholds) to determine if specificity can be improved without sacrificing sensitivity.
3. Evaluate the impact of adversarial training or defensive distillation techniques on privacy filter robustness against evasion attempts.