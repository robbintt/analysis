---
ver: rpa2
title: Improved Algorithms for Nash Welfare in Linear Bandits
arxiv_id: '2601.22969'
source_url: https://arxiv.org/abs/2601.22969
tags:
- logt
- regret
- phase
- nash
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper resolves the open problem of achieving order-optimal
  Nash regret in linear bandits, a setting where prior work suffered from suboptimal
  dependence on the ambient dimension. The key insight is that by designing a data-adaptive
  stopping rule for an initial exploration phase, the authors enable the use of standard
  upper confidence bound (UCB) intervals in the second phase, instead of the estimate-dependent
  Nash confidence bounds (NCB) used previously.
---

# Improved Algorithms for Nash Welfare in Linear Bandits

## Quick Facts
- arXiv ID: 2601.22969
- Source URL: https://arxiv.org/abs/2601.22969
- Authors: Dhruv Sarkar; Nishant Pandey; Sayak Ray Chowdhury
- Reference count: 40
- Key result: Achieves order-optimal $O(d\sqrt{T})$ Nash regret in linear bandits by replacing multiplicative concentration inequalities with additive ones via a data-adaptive exploration phase

## Executive Summary
This paper resolves the open problem of achieving order-optimal Nash regret in linear bandits, a setting where prior work suffered from suboptimal dependence on the ambient dimension. The key insight is that by designing a data-adaptive stopping rule for an initial exploration phase, the authors enable the use of standard upper confidence bound (UCB) intervals in the second phase, instead of the estimate-dependent Nash confidence bounds (NCB) used previously. This shift from multiplicative to additive concentration inequalities eliminates the dimensional bottleneck.

The authors propose FairLinBandit, a meta-algorithm that interleaves D-optimal design (for parameter estimation) with a John ellipsoid distribution (to maintain welfare). The exploration phase terminates when a certain condition involving the current parameter estimate is met, ensuring tight confidence widths. In the exploitation phase, any optimistic linear bandit algorithm can be used; the paper instantiates this with LinUCB and LinPE, both achieving order-optimal $O(d\sqrt{T})$ Nash regret.

## Method Summary
FairLinBandit is a meta-algorithm that operates in two phases: an initial exploration phase using D-optimal design and John ellipsoid sampling to ensure well-conditioned data, followed by an exploitation phase using standard UCB algorithms. The critical innovation is a stopping condition for exploration that guarantees the parameter estimate is accurate enough for additive confidence intervals to hold. This eliminates the need for multiplicative Nash confidence bounds, which previously introduced a $d^2$ factor in regret. The algorithm is instantiated with both LinUCB and LinPE to achieve the same order-optimal regret bound.

## Key Results
- Achieves $O(d\sqrt{T})$ Nash regret, resolving the open problem of order-optimal fairness-aware regret in linear bandits
- Introduces p-means regret as a unifying framework interpolating between fairness and utility objectives
- Extends FairLinBandit to achieve sublinear p-means regret for all $p \in \mathbb{R}$
- Demonstrates consistent improvement over state-of-the-art on real-world datasets

## Why This Works (Mechanism)
The core mechanism is the replacement of multiplicative Nash confidence bounds with additive UCB intervals. This is made possible by a carefully designed exploration phase that terminates when the parameter estimate is accurate enough. The D-optimal design ensures good conditioning of the data matrix, while the John ellipsoid distribution maintains welfare during exploration. The stopping condition ensures that standard concentration inequalities can be applied, eliminating the $d^2$ dimensional bottleneck present in previous approaches.

## Foundational Learning
- **D-optimal design**: Maximizes the determinant of the Fisher information matrix; needed to ensure well-conditioned data for accurate parameter estimation; quick check: verify that the design matrix has full rank and good conditioning
- **John ellipsoid**: The minimal volume ellipsoid containing a convex set; used here to maintain welfare during exploration; quick check: confirm that the ellipsoid contains the feasible set and touches it at extreme points
- **Nash regret**: Measures the worst-case deviation from the optimal Nash social welfare; the fairness objective in multi-agent bandit settings; quick check: compute the Nash social welfare of the algorithm's sequence of plays
- **p-means regret**: A generalization of Nash regret that interpolates between fairness ($p=0$) and utilitarian social welfare ($p=1$); quick check: verify that the p-means objective is convex in the allocation vector
- **Multiplicative vs additive concentration**: Multiplicative bounds depend on the unknown mean; additive bounds do not; needed to eliminate dimensional dependence; quick check: compare the width of multiplicative and additive confidence intervals
- **Stopping condition for exploration**: Ensures the parameter estimate is accurate enough for additive bounds to hold; the key to achieving order-optimal regret; quick check: verify that the stopping condition is triggered with high probability

## Architecture Onboarding

**Component map**: D-optimal design -> John ellipsoid sampling -> Parameter estimation -> Stopping condition check -> UCB exploitation

**Critical path**: The exploration phase must complete successfully (parameter estimate accurate) before exploitation can begin; the stopping condition is the critical bottleneck

**Design tradeoffs**: The D-optimal exploration is computationally expensive but ensures good conditioning; the John ellipsoid is more complex than uniform sampling but maintains welfare; the stopping condition adds a layer of complexity but enables order-optimal regret

**Failure signatures**: If the stopping condition is triggered too early, confidence intervals will be too wide and regret will be suboptimal; if triggered too late, exploration will dominate and regret will be high; poor conditioning of the design matrix will lead to inaccurate parameter estimates

**First experiments**: (1) Test FairLinBandit on a synthetic linear bandit problem with known parameters to verify the stopping condition; (2) Compare the performance of FairLinBandit with and without the D-optimal design to isolate its impact; (3) Benchmark against existing Nash-regret algorithms on a real-world dataset to demonstrate improvement

## Open Questions the Paper Calls Out
None

## Limitations
- The D-optimal exploration design may not be computationally tractable for high-dimensional or constrained action sets, potentially limiting scalability
- While the paper demonstrates that additive confidence intervals suffice for the linear bandit setting, it is unclear whether this extends to more complex environments such as non-linear bandits or structured action spaces
- The extension to p-means regret is well-founded but has Low confidence in general applicability across all values of p, particularly for p<1

## Confidence
- **High**: The core theoretical claims regarding the reduction from multiplicative to additive concentration inequalities and the resulting dimensional improvement
- **Medium**: The empirical validation on real-world datasets, though the datasets and experimental setup are not fully specified
- **Low**: The extension to p-means regret in general applicability across all values of p, particularly for p<1

## Next Checks
1. Test FairLinBandit on non-linear reward functions to verify robustness beyond the linear assumption
2. Benchmark the algorithm on high-dimensional action spaces to assess computational feasibility of the D-optimal design
3. Conduct ablation studies isolating the impact of the stopping condition on overall regret, particularly in early rounds where exploration dominates