---
ver: rpa2
title: 'QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural
  Networks'
arxiv_id: '2510.03276'
source_url: https://arxiv.org/abs/2510.03276
tags:
- quadratic
- should
- neural
- parameters
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QuadEnhancer, a method that enhances deep
  neural networks by adding quadratic transformations to each layer. The approach
  uses low-rank matrices, weight sharing, and sparsification to keep parameter and
  computational overhead minimal.
---

# QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks

## Quick Facts
- arXiv ID: 2510.03276
- Source URL: https://arxiv.org/abs/2510.03276
- Authors: Qian Chen; Linxin Yang; Akang Wang; Xiaodong Luo; Yin Zhang
- Reference count: 40
- Primary result: Consistently improves accuracy across image classification, text classification, and LLM fine-tuning with minimal overhead

## Executive Summary
This paper introduces QuadEnhancer, a method that enhances deep neural networks by adding quadratic transformations to each layer. The approach uses low-rank matrices, weight sharing, and sparsification to keep parameter and computational overhead minimal. Experiments on image classification, text classification, and LLM fine-tuning show consistent accuracy improvements across diverse tasks and datasets. For example, in image classification, ViT models with QuadEnhancer achieve 1.60–6.94% higher accuracy than baselines. In text classification, perplexity drops and classification accuracy improves by 0.86–0.91%. For LLM fine-tuning, average accuracy increases by 2.64% even with fewer LoRA parameters. Overall, the method delivers significant performance gains with negligible added complexity.

## Method Summary
QuadEnhancer enhances standard linear layers by adding a quadratic term that captures pairwise feature interactions. The method computes z = (Λỹ)⊙ỹ + ỹ + b where ỹ = Wx, using a sparse band matrix Λ with k=1 (K={1}) that implements circular shifts. The approach shares the linear weight matrix W between linear and quadratic pathways, applies sparsification to maintain efficiency, and excludes self-interaction terms to improve numerical stability. Applied to every linear layer in ViTs, GPT-2, and LLaMA models, with pre-training and fine-tuning following standard procedures.

## Key Results
- Image classification: ViT models achieve 1.60–6.94% higher accuracy on CIFAR-100 and ImageNet-1k
- Text classification: Perplexity drops and accuracy improves by 0.86–0.91% on IMDB, SST-2, and AG-News
- LLM fine-tuning: Average accuracy increases by 2.64% on Commonsense reasoning tasks even with fewer LoRA parameters

## Why This Works (Mechanism)

### Mechanism 1
Augmenting linear layers with a constrained quadratic term increases model expressiveness by capturing pairwise feature interactions at negligible computational cost. The QuadEnhancer adds a term (ΛWx) ⊙ (Wx) to the standard linear output Wx + b. This term models interactions between features in the latent space defined by W. The complexity is managed by reusing W and enforcing a sparse, low-rank structure on Λ, avoiding the O(n²) parameter cost of a full quadratic layer. The core assumption is that relevant higher-order interactions can be effectively captured as pairwise products of the linearly projected features.

### Mechanism 2
Sharing the linear weight matrix W between the linear and quadratic pathways minimizes new parameters and FLOPs. The quadratic term is constructed by reusing the pre-activation output ỹ = Wx. It is then mixed by a lightweight Λ and multiplied element-wise with itself: (Λỹ) ⊙ ỹ. This avoids introducing separate, large weight matrices for the quadratic transformation. The core assumption is that the feature space induced by the primary linear transformation W is rich enough that useful quadratic interactions exist within it.

### Mechanism 3
Excluding self-interaction (square) terms via sparse shifts improves numerical stability. The method constructs Λỹ by summing rolled (shifted) versions of ỹ, weighted by learnable parameters. By using shifts like K={1} (neighbor interaction) and explicitly excluding r=0 (self-interaction), the model avoids computing ỹᵢ² terms. Square terms have higher variance and are more prone to overflow in FP16 precision. The core assumption is that cross-feature interactions are primary while square terms are numerically dangerous and non-essential.

## Foundational Learning

- **Hadamard Product**: Why needed here: This is the core operation for the quadratic term in QuadEnhancer, performing element-wise multiplication between the mixed activations (Λỹ) and the original activations ỹ. Quick check: Given vectors a = [1, 2] and b = [3, 4], what is a ⊙ b? (Answer: [3, 8])

- **Sparse & Band Matrices**: Why needed here: The efficiency of the entire method hinges on representing the mixing matrix Λ as a sparse band matrix, which reduces its parameter count from O(d²) to O(kd). Quick check: If a 4 × 4 band matrix has a bandwidth of k=1, how many non-zero diagonal lines does it contain, excluding the main diagonal if specified? (Answer: Two, one above and one below the main diagonal)

- **Feature Interaction / Cross-Terms**: Why needed here: The primary motivation of the paper is that standard linear layers cannot model interactions between input features (e.g., xᵢxⱼ). The quadratic term is introduced specifically to capture these interactions. Quick check: In a model predicting house prices, if x₁ is "school quality" and x₂ is "safety," why might a model need to capture their interaction x₁x₂ rather than just their sum? (Answer: A high-quality school may only add value if the neighborhood is also safe; the effect is multiplicative, not additive)

## Architecture Onboarding

- **Component map**: Input Vector (x ∈ ℝⁿ) -> Shared Linear Projector (W ∈ ℝ^(d×n) producing ỹ = Wx) -> Rolling/Shift Operator (generates shifted copies of ỹ based on sparse index set K) -> Sparse Mixing Vector (λ learns to weight shifted copies forming Λỹ) -> Hadamard Product (computes (Λỹ) ⊙ ỹ) -> Additive Fusion (combines linear term, quadratic term, and bias: z = (Λỹ) ⊙ ỹ + ỹ + b)

- **Critical path**: Ensure ỹ = Wx is computed and stored. Apply the sparse shift (e.g., Roll(ỹ, 1)) and weight it by the learnable parameter λ₁. Compute the final output: z = (shifted_weighted_y) ⊙ ỹ + ỹ + b.

- **Design tradeoffs**: Bandwidth (k): A larger k (more shifts) captures more complex interactions but increases parameters and computation linearly. The paper finds k=1 is often sufficient. Excluding Shift 0: Improves stability in FP16 but loses self-squaring information. Parameter Sharing: Maximizes efficiency but constrains the quadratic term to the linear term's feature space.

- **Failure signatures**: Instability/NaNs: Likely caused by including the square term (shift 0), especially in FP16. Remedy: Ensure K={1} or use FP32. No Performance Gain: May indicate the backbone network (W) is not learning useful features or the sparsity k is too low. Increased Compute Time: Check if the efficient implementation of the sparse/band matrix multiplication is being used, rather than a dense matrix multiply with a sparse matrix.

- **First 3 experiments**: Ablation on Shift Set K: Train a small model (e.g., ViT-Tiny) on CIFAR-10 with K=∅ (baseline), K={1}, and K={1,-1} to verify the performance gain and stability tradeoffs. Compute/Parameter Overhead Verification: Profile the forward pass of a standard linear layer vs. the QuadEnhancer layer. Measure FLOPs and parameter count to confirm the theoretical O(k/n) overhead is negligible. Integration Test: Replace the linear layers in a simple pre-trained model (e.g., a small Transformer) with QuadEnhancer. Fine-tune on a downstream task and compare validation accuracy to the frozen baseline and a fully fine-tuned baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Does QuadEnhancer maintain its efficiency and performance advantages when applied to large-scale models and datasets? Basis in paper: Section 4.4 states that "large-scale experiments were not feasible due to computational constraints," limiting the analysis to "small to moderate scales." Why unresolved: It is unclear if the relative parameter/FLOP overhead remains "negligible" or if the accuracy gains persist as model size increases into the billions of parameters. What evidence would resolve it: Benchmarking results on large-scale models (e.g., LLaMA-70B) and datasets (e.g., full Common Crawl) comparing accuracy per training hour.

### Open Question 2
Can the exclusion of diagonal quadratic terms (square terms) be reversed through better normalization techniques? Basis in paper: Section 3.4 and Example 3.1 note that the shift r=0 is excluded because square terms are "more prone to numerical instabilities" in FP16. Why unresolved: The current architecture (using only cross-terms) is a compromise for stability; the theoretical "quadratic" capacity is thus intentionally curtailed. What evidence would resolve it: A study introducing specialized normalization (e.g., gradient clipping or custom quantization) for the r=0 shift, demonstrating whether including self-interactions improves accuracy.

### Open Question 3
How does QuadEnhancer interact with Convolutional Neural Networks (CNNs) compared to the Transformer architectures tested? Basis in paper: While the Introduction discusses CNNs, the experimental results focus exclusively on ViTs, GPT-2, and LLaMA (Transformer-based). Why unresolved: The sliding window nature of the "Roll" operation maps well to sequence data, but it is unclear if this band-matrix approach provides significant benefits over standard quadratic convolution methods. What evidence would resolve it: Comparative benchmarks on standard CNN backbones (e.g., ResNet-50) against standard quadratic convolution baselines.

## Limitations

- Large-scale experiments were not feasible due to computational constraints, limiting validation to small to moderate scales
- The exact training configurations contain some unspecified details including data augmentation, optimizer settings, and weight initialization for sparse mixing parameters
- Theoretical justification for excluding self-interaction terms improving stability lacks comprehensive ablation studies

## Confidence

- **High**: The core architectural mechanism (element-wise product of shifted activations) is clearly specified and implementable. The parameter efficiency claims (k×d vs d²) are mathematically sound.
- **Medium**: The empirical results showing consistent accuracy improvements across diverse tasks appear robust, but the exact training configurations that led to these results contain some unspecified details.
- **Low**: The theoretical justification for excluding self-interaction terms improving stability is reasonable but lacks comprehensive ablation studies demonstrating the necessity of this design choice.

## Next Checks

1. **Ablation on Shift Set K**: Train a small model on CIFAR-10 with K=∅ (baseline), K={1}, and K={1,-1} to verify the performance gain and stability tradeoffs of including/excluding self-interaction terms.

2. **Compute/Parameter Overhead Verification**: Profile a standard linear layer vs. QuadEnhancer layer on a small model. Measure FLOPs and parameter count to confirm the theoretical O(k/n) overhead is negligible in practice.

3. **Integration Test with LoRA**: Fine-tune a pre-trained GPT-2 model on a classification task using both LoRA alone and QuadEnhancer + LoRA. Compare parameter efficiency and accuracy to validate the claimed synergistic benefits.