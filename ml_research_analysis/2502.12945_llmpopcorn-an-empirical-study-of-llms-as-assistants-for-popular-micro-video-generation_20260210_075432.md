---
ver: rpa2
title: 'LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video
  Generation'
arxiv_id: '2502.12945'
source_url: https://arxiv.org/abs/2502.12945
tags:
- video
- generation
- llms
- prompt
- popularity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  as assistants for generating popular micro-videos. The authors propose a pipeline
  (LLMPopcorn) that uses LLMs to generate prompts for video generation models, then
  evaluates the resulting videos' predicted popularity.
---

# LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation

## Quick Facts
- arXiv ID: 2502.12945
- Source URL: https://arxiv.org/abs/2502.12945
- Reference count: 0
- This paper investigates the use of large language models (LLMs) as assistants for generating popular micro-videos, proposing a pipeline (LLMPopcorn) that uses LLMs to generate prompts for video generation models, then evaluates the resulting videos' predicted popularity.

## Executive Summary
This paper investigates the use of large language models (LLMs) as assistants for generating popular micro-videos. The authors propose a pipeline (LLMPopcorn) that uses LLMs to generate prompts for video generation models, then evaluates the resulting videos' predicted popularity. They introduce a prompt enhancement technique combining retrieval-augmented generation and chain-of-thought prompting. Experiments comparing five LLMs (Llama-3.3-70B, Qwen-2.5-72B, ChatGPT-4o, DeepSeek-V3, and DeepSeek-R1) and three video generation models show that DeepSeek-V3 and DeepSeek-R1 achieve the highest performance in generating popular micro-videos, with DeepSeek-V3 outperforming human-created content in popularity scores (0.56 vs 0.44 for concrete prompts). The prompt enhancement technique significantly improves results, with DeepSeek-R1 achieving a peak win rate of 66% for enhanced prompts. The study demonstrates that advanced LLMs can effectively assist in creating engaging micro-video content.

## Method Summary
The LLMPopcorn pipeline uses LLMs to generate video titles and prompts from user inputs, then feeds these to video generation models to create micro-videos. The predicted popularity of each video is calculated using the MMRA model, which estimates comment counts. The authors introduce a prompt enhancement technique that combines retrieval-augmented generation (RAG) with chain-of-thought prompting. For RAG, they retrieve top-K similar videos from the Microlens dataset based on tag similarity, partition them into popular and unpopular examples, and incorporate this context into the LLM generation process. They compare five LLMs and three video generation models using both concrete and abstract user prompts, measuring performance through popularity scores and pairwise win rates.

## Key Results
- DeepSeek-V3 and DeepSeek-R1 achieve the highest performance in generating popular micro-videos
- DeepSeek-V3 outperforms human-created content in popularity scores (0.56 vs 0.44 for concrete prompts)
- Prompt enhancement technique significantly improves results, with DeepSeek-R1 achieving a peak win rate of 66% for enhanced prompts

## Why This Works (Mechanism)
The study demonstrates that advanced LLMs can effectively assist in creating engaging micro-video content by leveraging their understanding of popularity patterns and content generation capabilities. The prompt enhancement technique, which combines retrieval-augmented generation with chain-of-thought reasoning, helps LLMs generate more contextually appropriate and popularity-optimized video prompts by learning from existing popular and unpopular video examples.

## Foundational Learning
- **Prompt Engineering**: Why needed - To optimize LLM outputs for specific tasks like video generation; Quick check - Test different prompt formats and observe output quality variations
- **Retrieval-Augmented Generation (RAG)**: Why needed - To provide contextual information from existing data to improve generation quality; Quick check - Compare performance with and without RAG augmentation
- **Chain-of-Thought Prompting**: Why needed - To improve reasoning and step-by-step generation for complex tasks; Quick check - Measure performance differences between standard and CoT prompts
- **Video Generation Models**: Why needed - To transform text prompts into actual video content; Quick check - Evaluate output quality and consistency across different generators
- **Popularity Prediction Models**: Why needed - To evaluate and optimize for engagement metrics; Quick check - Validate MMRA predictions against actual engagement data

## Architecture Onboarding

Component Map:
User Prompt -> LLM (Title/Prompt Generation) -> Video Generator -> MMRA Popularity Predictor

Critical Path:
The critical path involves generating optimized prompts from user inputs using LLMs with prompt enhancement, then creating videos and predicting their popularity. The bottleneck is typically GPU memory during video generation, followed by API latency for LLM calls.

Design Tradeoffs:
The study balances between prompt quality (using RAG and CoT) and computational efficiency (4-bit quantization, batch processing). The choice of MMRA as an offline predictor trades real-world validation for scalability and cost-effectiveness.

Failure Signatures:
- Low popularity scores across all models suggest issues with MMRA model or video generation pipeline
- Inconsistent results between LLMs may indicate insufficient prompt engineering or data quality issues
- High variance in win rates suggests sensitivity to random seeds or implementation differences

First Experiments:
1. Test basic prompt generation with each LLM using the same seed to establish baseline performance
2. Enable prompt enhancement with default RAG size (50) to measure improvement magnitude
3. Run pairwise comparisons between top-performing LLM-video generator combinations

## Open Questions the Paper Calls Out
1. Can reinforcement learning or fine-tuning techniques improve LLM alignment for popularity-optimized micro-video generation beyond prompt engineering? The current work relies solely on prompting strategies without updating model weights, and the potential gains from direct optimization remain unquantified.

2. Do offline popularity predictors (like MMRA) accurately forecast real-world engagement metrics on live platforms such as TikTok or YouTube? The study relies entirely on MMRA, an offline predictor trained on historical data, without deployment validation to capture dynamic factors like trending topics or platform algorithm changes.

3. Do prompt enhancement strategies generalize across different video generators, or are they model-specific? The interaction between prompt enhancement and different video generator architectures was not systematically analyzed, as evidenced by inconsistent win rates across generators.

## Limitations
- The study's performance comparisons between LLMs are based on predicted popularity scores rather than actual engagement metrics, introducing a validation gap
- Key implementation details are missing, including prompt templates, embedding models, and RAG size selection
- No real-world deployment or A/B testing was conducted to validate the MMRA predictor's accuracy

## Confidence
- High confidence: DeepSeek-V3 and DeepSeek-R1 outperform other LLMs in generating popular micro-videos based on MMRA predictions
- Medium confidence: Prompt enhancement technique significantly improves results, as evidenced by the 66% win rate for enhanced prompts with DeepSeek-R1
- Medium confidence: Human-created content performs worse than DeepSeek-V3 in popularity scores (0.44 vs 0.56), though this relies on the MMRA predictor's accuracy

## Next Checks
1. Verify the MMRA popularity predictor's accuracy by comparing its predictions against actual user engagement data from real micro-video platforms
2. Conduct a controlled user study measuring actual watch time and engagement for videos generated by different LLM-prompt combinations, not just predicted popularity
3. Test the reproducibility of results across different video generation models and varying computational resource constraints (GPU memory limitations)