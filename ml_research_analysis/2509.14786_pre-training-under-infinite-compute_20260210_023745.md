---
ver: rpa2
title: Pre-training under infinite compute
arxiv_id: '2509.14786'
source_url: https://arxiv.org/abs/2509.14786
tags:
- data
- scaling
- count
- parameter
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies language model pre-training under fixed data\
  \ and unlimited compute budgets. The authors show that standard approaches like\
  \ increasing epochs or parameter count eventually overfit, but can be improved significantly\
  \ by tuning regularization (finding optimal weight decay is 30\xD7 larger than standard\
  \ practice)."
---

# Pre-training under infinite compute

## Quick Facts
- arXiv ID: 2509.14786
- Source URL: https://arxiv.org/abs/2509.14786
- Reference count: 40
- Key outcome: With proper regularization and ensembling, the same validation loss can be achieved using 5.17× less data than standard approaches

## Executive Summary
This paper explores language model pre-training under fixed data constraints with unlimited compute budgets. The authors demonstrate that standard approaches like increasing epochs or parameter counts eventually overfit, but can be significantly improved by tuning regularization (optimal weight decay is 30× larger than standard practice). With proper regularization, loss scales monotonically with parameter count. They show that ensembling independently trained models achieves better performance than parameter scaling, and that combining both techniques (joint scaling) further improves results. Their best recipe achieves the same validation loss as the baseline using 5.17× less data, with gains persisting at larger scales and transferring to downstream tasks.

## Method Summary
The authors study pre-training under fixed data constraints by scaling model size and compute while keeping the training dataset constant at 200M tokens. They use Llama-style auto-regressive Transformers ranging from 150M to 1.4B parameters, trained with AdamW optimizer and cosine learning rate decay. The key innovation is finding that standard regularization (weight decay of 0.1) is insufficient for extreme over-parameterization, requiring weight decay values 30× larger than standard practice. They systematically tune weight decay, learning rate, and epochs for each model size using coordinate descent. The method combines three scaling strategies: parameter scaling (increasing model size), ensemble scaling (averaging logits from independently trained models), and joint scaling (combining both approaches).

## Key Results
- Optimal weight decay is 30× larger than standard practice (0.1) for extreme over-parameterization
- Loss scales monotonically with parameter count when proper regularization is applied
- Ensembling achieves lower loss asymptotes than parameter scaling alone, with 5× data efficiency gains
- Joint scaling (combining parameter and ensemble scaling) further improves results
- Distillation can compress ensemble benefits into smaller models, retaining 83% of ensembling gains

## Why This Works (Mechanism)

### Mechanism 1: Over-parameterized Regularization
Heavier weight decay (approximately 30× standard practice) is conditionally necessary to prevent overfitting when scaling parameter counts under fixed data constraints. In the infinite compute regime, increasing parameter count without increasing data typically causes validation loss to eventually rise due to overfitting. Aggressive weight decay constrains model capacity, aligning training dynamics with the "multi-view" structure of data and allowing loss to scale monotonically as a power law in parameter count.

### Mechanism 2: Ensemble Asymptotic Scaling
Ensembling independently trained models achieves a lower loss asymptote (limit as compute → ∞) than scaling a single dense model's parameter count. Different model seeds bias models toward learning different subsets of predictive features ("multi-view" learning). Averaging their logits aggregates these diverse features more efficiently than forcing a single model to learn them all, pushing the theoretical performance floor lower.

### Mechanism 3: Mixed-Regime Distillation
The performance gains from large ensembles can be compressed into a single small student model via sequence-level knowledge distillation, provided the student is trained on a mixture of real and teacher-generated data. A standard teacher-student setup often leads to model collapse or degradation. Mixing real tokens with synthetic tokens grounds the student in the ground-truth distribution while allowing it to absorb the ensemble's "knowledge" via the distillation loss.

## Foundational Learning

### Scaling Law Asymptotes
**Why needed:** Standard scaling laws optimize for a fixed compute budget. This paper shifts to optimizing the asymptote of the loss curve as parameter count approaches infinity, representing maximum extractable information from the data. **Quick check:** If a power law has a high exponent but high asymptote, is it preferred in the infinite compute regime? (No, you optimize for the lowest asymptote).

### Coordinate Descent for Hyperparameters
**Why needed:** The paper relies on finding "locally optimal" hyperparameters for every parameter count. A naive grid search is infeasible; understanding iterative search (tuning one param at a time until neighbors are worse) is critical for reproduction. **Quick check:** Why can't we fix weight decay for a 150M model and use it for a 1.4B model? (Optimal WD scales with model size; doing so breaks monotonic scaling).

### Multi-View Learning
**Why needed:** This is the theoretical basis for why ensembling works better than single-model scaling. It posits that data has multiple "views" (features) that can solve a task, and single models are biased toward one, while ensembles capture all. **Quick check:** Why does this imply that 2×300M models outperform 1×600M model? (The two models likely capture disjoint views, whereas the single model might still be biased to one dominant view).

## Architecture Onboarding

### Component map:
Input: Fixed token pool (e.g., 200M tokens) -> Backbone: Llama-style auto-regressive Transformer (varied width/depth) -> Optimization: AdamW (requires aggressive Weight Decay tuning) -> Evaluation: Log-likelihood on held-out validation set (Loss) -> Inference: Logit averaging over K independently trained models (Ensemble)

### Critical path:
1. **Regularization Sweep:** For a target parameter count, tune WD, LR, and Epochs using coordinate descent to find the loss minimum
2. **Asymptote Fitting:** Repeat for 4+ values of parameter count. Fit L(N) ≈ A/N^α + E. Extract E
3. **Ensemble Verification:** Train K models at the best parameter count. Average logits. Fit loss vs K to verify if the asymptote is lower than parameter scaling

### Design tradeoffs:
- **Train Compute vs. Data Efficiency:** The method sacrifices train compute (spending 17.5× or more FLOPs) to maximize data efficiency
- **Inference Cost:** Raw ensembles multiply inference cost by K. You must commit to a final distillation step to recover practical inference speeds
- **Hyperparameter Sensitivity:** The "locally optimal" WD is highly sensitive to the N/D ratio. Transferring WD settings across scales without retuning will cause failure

### Failure signatures:
- **Loss Saturation/Rise:** If loss rises as N increases, check Weight Decay. Standard 0.1 is probably insufficient
- **Distillation Collapse:** If student loss is worse than teacher during self-distillation, check the data mix. Using 100% synthetic data causes collapse; you must mix in real data
- **Non-monotonic Scaling:** If power law fit is poor, ensure you are evaluating loss after learning rate has annealed to zero

### First 3 experiments:
1. **Baseline Overfit:** Train a 300M model on 200M tokens for 16 epochs with WD=0.1. Confirm loss starts rising after epoch ~8
2. **Regularization Fix:** Retrain with WD=1.6 and LR=3e-3. Confirm loss now decreases monotonically through epoch 16
3. **Ensemble Benefit:** Train 3 independent 300M models (seeds only). Average their logits and measure the drop in validation loss compared to the single model

## Open Questions the Paper Calls Out

### Open Question 1
**Can alternative algorithms (different objectives, architectures, data augmentation) achieve better asymptotes than the joint scaling recipe?**
The paper only explores parameter scaling, regularization, ensembling, and distillation. Alternatives like diffusion language models or synthetic data augmentation are mentioned but not tested under the asymptote framework.

### Open Question 2
**Do the data efficiency gains and power-law scaling persist at frontier model scales (100B+ parameters)?**
The paper only validates up to 1.4B parameters and 1.6B tokens, extrapolating to larger scales via power laws without empirical verification.

### Open Question 3
**Why does the ensemble hyperparameter heuristic fail at extreme over-parameterization, and is there a universal rule?**
Appendix C.2 notes "We find only one counter-example to this heuristic... for 1.4B models trained on 200M tokens" but does not investigate the cause or whether failure persists at higher parameter-to-token ratios.

## Limitations
- Core scaling claims rely on a fixed, finite dataset (200M tokens) and may not generalize to larger datasets or different domains
- The "multi-view" theoretical justification for ensemble superiority relies on implicit assumptions about feature diversity that are not rigorously proven
- The exact 30× scaling factor for weight decay is dataset-specific and may not transfer to other domains
- The optimal distillation mixing ratio (1:3 vs 1:9) is not derived from theory

## Confidence

- **High Confidence:** The regularization mechanism (heavier WD prevents overfitting in the infinite compute regime) is well-supported by monotonic loss curves when WD is tuned
- **Medium Confidence:** The ensemble asymptotic scaling claim is supported by empirical fits, but the "multi-view" theoretical explanation is an assumption
- **Low Confidence:** The exact 30× scaling factor for WD is dataset-specific, and the benefits at scales vastly larger than 1.4B are extrapolation

## Next Checks

1. **Dataset Size Sensitivity:** Validate the optimal weight decay scaling rule on a dataset 10× larger (2B tokens) to check if the 30× factor holds or scales differently
2. **Architecture Ablation:** Test if monotonic scaling and ensemble benefits persist with a different backbone (e.g., GPT-style vs. Llama-style) to isolate architecture effects
3. **Real-to-Synthetic Ratio Sweep:** Systematically vary the distillation mixing ratio (0%, 25%, 50%, 75%, 100% synthetic) to find the Pareto frontier of student quality vs. data efficiency