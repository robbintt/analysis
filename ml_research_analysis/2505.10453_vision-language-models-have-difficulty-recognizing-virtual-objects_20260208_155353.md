---
ver: rpa2
title: Vision language models have difficulty recognizing virtual objects
arxiv_id: '2505.10453'
source_url: https://arxiv.org/abs/2505.10453
tags:
- objects
- vlms
- object
- table
- virtual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vision language models (VLMs) can process multimodal input and\
  \ perform complex tasks such as captioning and object detection. However, their\
  \ ability to reason about virtual objects\u2014those mentioned in text but not visually\
  \ present\u2014remains untested."
---

# Vision language models have difficulty recognizing virtual objects

## Quick Facts
- **arXiv ID**: 2505.10453
- **Source URL**: https://arxiv.org/abs/2505.10453
- **Reference count**: 40
- **Primary result**: VLMs achieve only 22-63% accuracy on virtual object recognition tasks

## Executive Summary
Vision language models can process multimodal input and perform complex tasks such as captioning and object detection. However, their ability to reason about virtual objects—those mentioned in text but not visually present—remains untested. We introduce a systematic evaluation methodology to benchmark VLMs on virtual object recognition by pairing images with prompts that describe additional objects. We evaluated three state-of-the-art VLMs—Idefics2, Llama 3.2, and BLIP—using 112,896 queries. The results reveal that VLMs struggle with this task: Idefics2 achieved 63% accuracy, Llama 3.2 57%, and BLIP only 22%. Performance was also affected by prompt wording and tense, with past-tense and numerically cued prompts yielding higher accuracy. These findings indicate significant limitations in VLMs' ability to update scene representations based on textual information, suggesting that current models lack robust visuospatial reasoning and imagination.

## Method Summary
We introduced TABLE TEST, a dataset of 4,032 synthetic 2-object images (64 Objaverse objects on tables) paired with prompts describing a virtual third object. We evaluated three VLMs (Idefics2 8B, InstructBlip-Vicuna 7B, Llama 3.2 11B vision) using 112,896 queries across 7 prompt types (act/assume/consider/if/imagine/pretend/suppose), 2 tenses, and 2 numerical cue conditions. Accuracy was measured as the proportion of responses listing all three objects (2 depicted + 1 virtual) in any order, with temperature=0 and deterministic seeding.

## Key Results
- VLMs achieved only 22-63% accuracy on virtual object recognition tasks
- Performance varied significantly by prompt wording: "pretend" (51%) outperformed "if" (40%)
- Numerical cues improved accuracy from 32% to 62%
- Past-tense prompts yielded higher accuracy than present-tense (51% vs 44%)
- BLIP performed substantially worse than other models (22% vs 57-63%)

## Why This Works (Mechanism)

### Mechanism 1: Numerical Cuing Enables Explicit Object Slot Allocation
- Claim: Providing explicit numerical cues (e.g., "what three items") improves virtual object recognition by triggering the model to allocate representational slots for all mentioned objects.
- Mechanism: The numerical cue forces the model to generate a response that enumerates nouns from both visual and textual modalities, reducing the likelihood of discarding the virtual object during generation.
- Core assumption: VLMs maintain implicit object counters that can be activated by explicit numerical references in prompts.
- Evidence anchors:
  - [abstract]: "numerically cued prompts yielding higher accuracy"
  - [section 3, results]: "62% correct with a numerical cue vs. 32% correct without; Wilcoxon test, z = 6.96, p < .001"
  - [corpus]: No direct corpus support for this specific mechanism.
- Break condition: If numerical cues cease to improve performance on larger or more complex scenes, the mechanism may reflect shallow lexical matching rather than genuine scene updating.

### Mechanism 2: Past-Tense Prompting Aligns With Caption Corpus Distribution
- Claim: Past-tense prompts yield higher accuracy because training corpora (e.g., web captions, news) disproportionately use past-tense descriptions, creating better alignment between prompt formulation and learned representations.
- Mechanism: The model's token predictions are conditioned on patterns prevalent in training data; past-tense hypothetical constructions may more strongly activate relevant reasoning pathways.
- Core assumption: Image-caption training corpora contain systematic tense asymmetries that transfer to hypothetical reasoning tasks.
- Evidence anchors:
  - [section 3, results]: "prompts in the past tense were more accurate than those in the present tense (51% vs. 44%)"
  - [section 3, discussion]: "One tentative reason for this difference may be that the captions used to train VLMs – from websites and newspapers – may use past tense descriptions more often than present tense descriptions."
  - [corpus]: No corpus papers directly validate training data tense distribution; mechanism remains speculative.
- Break condition: If tense effects disappear with different prompt formulations or on models trained on curated balanced corpora, the mechanism is corpus-specific rather than architectural.

### Mechanism 3: Distributed Representations Impede Coherent Scene Updating
- Claim: VLMs struggle with virtual objects because their distributed, parallel representations lack the discrete, mutable structure needed to integrate textual updates into visual scene representations.
- Mechanism: Unlike humans who construct sparse mental models that can be serially updated, VLMs process text and image embeddings holistically, making targeted updates to scene representations difficult without re-processing the entire input.
- Core assumption: The integration bottleneck occurs at the cross-modal representation level rather than at token generation.
- Evidence anchors:
  - [section 4, discussion]: "VLMs, in contrast, leverage parallel pipelines for processing text and image embeddings holistically, but they may have difficulty integrating the resulting distributed representations in coherent ways that permit rapid updating and analysis."
  - [section 4]: "Systems capable of human-like imaginative processing may have to create a synthesis of these approaches, e.g., by reasoning over both distributed and discretized structures."
  - [corpus]: "Vision language models are unreliable at trivial spatial cognition" (arXiv:2504.16061) corroborates broader spatial reasoning limitations.
- Break condition: If future architectures with explicit object-centric representations still fail at virtual object tasks, the mechanism is insufficient and other factors (e.g., training objectives) may dominate.

## Foundational Learning

- **Multimodal Tokenization and Embedding Fusion**
  - Why needed here: Understanding how VLMs convert images and text into shared embedding spaces is prerequisite to diagnosing why textual updates don't reliably modify visual scene representations.
  - Quick check question: Can you explain how a vision encoder's output is aligned with a language model's embedding space?

- **Transformer Cross-Attention Mechanisms**
  - Why needed here: The paper assumes VLMs process modalities "in parallel"—understanding cross-attention is essential to evaluate where integration failures might occur.
  - Quick check question: How does cross-attention differ from self-attention in multimodal transformers?

- **Prompt Sensitivity and Distributional Anchoring**
  - Why needed here: The study's core finding is that prompt wording (tense, numerical cues) significantly affects performance; understanding why requires knowledge of how prompts activate learned patterns.
  - Quick check question: Why might a model respond differently to "imagine there is" vs. "imagine there was" if both describe the same hypothetical?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP-style) -> Language encoder/decoder -> Cross-modal projection layer -> Transformer backbone -> Output head

- **Critical path**:
  1. Image is encoded into visual tokens
  2. Prompt text is tokenized and embedded
  3. Cross-attention layers fuse visual and textual information
  4. The failure point: virtual object from text must be integrated into the scene representation maintained across layers
  5. Generation produces object list—virtual objects may be dropped here

- **Design tradeoffs**:
  - Holistic parallel processing vs. discrete serial updating (speed vs. flexible updating)
  - Training on web-scale captions vs. curated reasoning data (coverage vs. controlled reasoning patterns)
  - Single-shot inference vs. iterative refinement (efficiency vs. accuracy on hypothetical tasks)

- **Failure signatures**:
  - Tense sensitivity: Past-tense prompts outperform present-tense (51% vs 44%)—abnormal, task-irrelevant factor
  - Prompt keyword variance: "pretend" (51%) vs. "if" (40%)—semantically equivalent prompts yield different results
  - Numerical cue dependency: 62% vs 32% accuracy—model requires explicit scaffolding
  - Low ceiling on BLIP (22%)—architectural limitation in cross-modal integration

- **First 3 experiments**:
  1. **Controlled tense analysis**: Test virtual object recognition on a held-out dataset where past/present tense is systematically balanced, measuring if the effect persists across different prompt templates.
  2. **Attention visualization on virtual objects**: Extract attention weights from cross-attention layers when processing virtual object prompts vs. non-virtual prompts to identify where integration fails.
  3. **Fine-tuning with hypothetical reasoning data**: Train a VLM variant on synthetic data where virtual objects are explicitly part of scene descriptions, then re-evaluate on TABLE TEST to test whether the limitation is training-data driven.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How could AI architectures be designed to track virtual objects more reliably?
- Basis in paper: [explicit] The Discussion explicitly asks, "How could AI architectures learn to track virtual objects?" and contrasts human mental models with VLMs' parallel processing of distributed representations.
- Why unresolved: The paper demonstrates the problem exists but does not propose or test architectural solutions; it only speculates that a synthesis of distributed and discretized structures may be needed.
- What evidence would resolve it: A study comparing VLMs augmented with explicit symbolic or spatial memory modules against baseline models on the same virtual object recognition task.

### Open Question 2
- Question: Does the numerical cue benefit reflect genuine scene updating or superficial pattern matching?
- Basis in paper: [inferred] The paper notes numerical cues boosted accuracy (62% vs. 32%) but offers two possibilities: the cue helps VLMs consider virtual objects, or it reflects trivial reasoning where the model matches nouns to the mentioned number.
- Why unresolved: The current methodology only measures final list accuracy, not the mechanism by which numerical cues help.
- What evidence would resolve it: A controlled experiment probing whether models with numerical cues can correctly answer follow-up spatial questions about virtual objects, not just list them.

### Open Question 3
- Question: Why do past-tense prompts yield higher accuracy than present-tense prompts?
- Basis in paper: [inferred] The authors report a significant tense effect (51% past vs. 44% present) and speculate about training corpus distributions but state, "Research into the data used to train these systems is necessary to evaluate these claims."
- Why unresolved: No analysis of training data composition was conducted; the explanation remains conjectural.
- What evidence would resolve it: Corpus analysis of caption datasets (e.g., LAION, CommonPool) quantifying tense distributions, paired with controlled fine-tuning experiments manipulating tense exposure.

## Limitations
- Corpus-level claims about tense distributions in training data remain unverified
- Attention mechanism analysis was not performed to localize integration failures
- The TABLE TEST dataset availability is uncertain for independent validation

## Confidence
- **High confidence**: VLMs struggle with virtual object recognition (22-63% accuracy)
- **Medium confidence**: Numerical cues improve performance by triggering object slot allocation
- **Low confidence**: Past-tense advantage due to training corpus distribution patterns

## Next Checks
1. Conduct a controlled experiment varying prompt tense systematically while holding other factors constant to isolate the effect
2. Extract and visualize cross-attention patterns when processing virtual vs. non-virtual object prompts
3. Fine-tune a VLM on synthetic hypothetical reasoning data and re-evaluate on TABLE TEST to test training-data dependence