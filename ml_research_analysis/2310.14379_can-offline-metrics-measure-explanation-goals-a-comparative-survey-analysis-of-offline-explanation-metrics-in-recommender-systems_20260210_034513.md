---
ver: rpa2
title: Can Offline Metrics Measure Explanation Goals? A Comparative Survey Analysis
  of Offline Explanation Metrics in Recommender Systems
arxiv_id: '2310.14379'
source_url: https://arxiv.org/abs/2310.14379
tags:
- explanation
- explanations
- metrics
- offline
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Offline evaluation of explanations in recommender systems is difficult\
  \ because explanation goals are subjective. We surveyed 103 papers and found that\
  \ explanations are mostly evaluated with anecdotal examples or metrics that don\u2019\
  t correlate with user perception."
---

# Can Offline Metrics Measure Explanation Goals? A Comparative Survey Analysis of Offline Explanation Metrics in Recommender Systems

## Quick Facts
- arXiv ID: 2310.14379
- Source URL: https://arxiv.org/abs/2310.14379
- Authors: André Levi Zanon; Marcelo Garcia Manzato; Leonardo Rocha
- Reference count: 40
- Primary result: Offline metrics for explanations don't correlate with user perception

## Executive Summary
This paper investigates the disconnect between offline explanation metrics and user perception in recommender systems through a comprehensive survey of 103 papers. The authors find that current evaluation methods heavily rely on anecdotal examples or metrics that fail to capture what users actually value in explanations. To address this gap, they propose path-based offline metrics that measure attribute popularity, diversity, and interacted-item recency, demonstrating through experiments that these metrics can reflect user perception goals while revealing important trade-offs between diversity and popularity.

## Method Summary
The study combines a large-scale literature survey with empirical experimentation. The survey analyzed 103 papers to understand current practices in explanation evaluation, revealing overreliance on anecdotal examples and superficial metrics. The authors then proposed three offline path metrics: popularity (frequency of attributes in paths), diversity (variety of attributes used), and interacted-item recency (temporal relevance of items in paths). These were tested across six recommendation algorithms and two datasets, followed by an online user study with 55 participants to validate the metrics against user experience dimensions including engagement, transparency, trust, and persuasiveness.

## Key Results
- Survey reveals explanations mostly evaluated with anecdotal examples or irrelevant metrics
- Diversity and popularity metrics show inherent trade-offs in path-based explanations
- User engagement is sensitive to perceived diversity, while transparency, trust, and persuasiveness are influenced by both perceived popularity and diversity
- Current offline metrics need refinement to better align with user experience

## Why This Works (Mechanism)
The proposed metrics work by operationalizing abstract explanation goals into measurable path characteristics. Popularity captures the relevance of commonly appreciated attributes, diversity ensures explanations aren't repetitive and cover different aspects, while interacted-item recency maintains temporal relevance. By measuring these dimensions independently yet accounting for their trade-offs, the metrics can better approximate what users actually value in explanations compared to traditional accuracy-focused approaches.

## Foundational Learning
- Path-based explanation: Sequential attribute-item paths that justify recommendations; needed to provide transparent reasoning for why items are recommended together
- Attribute popularity: Frequency of attributes appearing in explanation paths; quick check: calculate term frequency across all paths
- Diversity in explanations: Variety of different attributes used across explanations; quick check: measure distinct attribute count per path or overall
- Recency in recommendation: Temporal relevance of items in explanations; quick check: verify timestamp-based filtering of recent interactions
- User perception metrics: Subjective evaluation of explanation quality; quick check: survey design with Likert scales for engagement, trust, etc.
- Offline vs online evaluation: Computational metrics vs user-centered testing; quick check: compare metric values with user study results

## Architecture Onboarding

Component map:
Survey analysis -> Metric proposal -> Experimental validation -> User study validation

Critical path:
Literature survey identifies gap in explanation evaluation → Proposed path metrics operationalize explanation goals → Experiments validate metric behavior → User study confirms correlation with perception

Design tradeoffs:
The path-based approach balances computational efficiency with explanatory richness, but may not generalize to non-path explanation types. The three metrics capture different user needs but introduce inherent trade-offs that must be managed.

Failure signatures:
Metrics that maximize popularity may reduce diversity, leading to repetitive explanations. Overemphasis on recency may sacrifice relevance for temporal proximity. Poor correlation between offline metrics and user perception indicates need for metric refinement.

First experiments:
1. Test proposed metrics across diverse recommendation algorithms to verify robustness
2. Compare metric values against user perception scores in controlled conditions
3. Analyze trade-off patterns between popularity and diversity across different datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Small user study sample size (55 participants) may limit generalizability
- Focus on path-based explanations may not extend to other explanation types
- Survey methodology may introduce publication bias by overrepresenting successful implementations

## Confidence
- Current offline metrics need refinement: High confidence (strong survey evidence)
- Diversity/popularity trade-off: Medium confidence (limited algorithm/dataset scope)
- Engagement sensitivity to diversity: Medium confidence (correlational findings)
- Trust/persuasiveness influenced by both metrics: Medium confidence (potential confounding factors)

## Next Checks
1. Conduct a larger-scale online user study (minimum 200 participants) with randomized explanation conditions to establish causal relationships between metric values and user experience dimensions
2. Extend the experimental framework to include diverse explanation types beyond path-based methods to assess the generalizability of the proposed offline metrics
3. Implement a longitudinal study tracking user behavior over multiple sessions to determine whether initial perceptions of diversity and popularity persist or evolve with continued system interaction