---
ver: rpa2
title: Integrating External Tools with Large Language Models to Improve Accuracy
arxiv_id: '2507.08034'
source_url: https://arxiv.org/abs/2507.08034
tags:
- tools
- language
- external
- llms
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Athena, a framework that integrates external\
  \ tools with large language models (LLMs) to enhance accuracy in educational settings.\
  \ The core idea is to enable LLMs to access external APIs\u2014such as calculators,\
  \ calendars, and search engines\u2014so they can handle tasks requiring up-to-date\
  \ data or computational reasoning."
---

# Integrating External Tools with Large Language Models to Improve Accuracy

## Quick Facts
- **arXiv ID:** 2507.08034
- **Source URL:** https://arxiv.org/abs/2507.08034
- **Authors:** Nripesh Niketan; Hadj Batatia
- **Reference count:** 22
- **Primary result:** Athena framework achieves 83% math and 88% science accuracy on MMLU, outperforming standalone models

## Executive Summary
This paper introduces Athena, a framework that integrates external tools with large language models to enhance accuracy in educational tasks. The system enables LLMs to access external APIs—such as calculators, calendars, and search engines—to handle tasks requiring up-to-date data or computational reasoning. By dynamically identifying when queries need external tools, extracting relevant parameters, and integrating results back into responses, Athena significantly improves performance on mathematical and scientific reasoning tasks compared to state-of-the-art models.

## Method Summary
The Athena framework uses LangChain to orchestrate interactions between LLMs and external APIs, hosted on the Unify platform. It employs a schema-guided approach using Pydantic models to define tool interfaces, allowing LLMs to reliably map natural language queries to specific API calls. The system operates iteratively: RunMonitoring detects when tools are needed, HandleRequiredAction executes API calls, and UpdateMessage integrates results back into the LLM context. Evaluation uses MMLU dataset subsets with 67 math questions and 100 science questions, comparing against baseline models like LLaMA-Large and GPT-4o.

## Key Results
- Athena achieves 83% accuracy in mathematical reasoning versus 67% for LLaMA-Large and 53% for GPT-4o
- Scientific reasoning accuracy reaches 88% compared to 79% for LLaMA-Large and 77% for GPT-4o
- Computational offloading via external calculators significantly improves arithmetic precision
- Iterative retrieval-augmented generation enables access to up-to-date facts beyond LLM's static knowledge

## Why This Works (Mechanism)

### Mechanism 1: Computational Offloading via Symbolic Tools
- **Claim:** Accuracy improves because LLMs delegate calculations to deterministic external engines rather than relying on probabilistic token prediction.
- **Mechanism:** The framework identifies mathematical intent via RunMonitoring, routes queries to computational APIs like Wolfram Alpha, and returns results to LLM context, bypassing LLM weaknesses in arithmetic precision.
- **Core assumption:** LLMs can correctly parse natural language problems into structured arguments required by calculator APIs.
- **Break condition:** If queries are ambiguous and LLM fails to extract sufficient parameters for API calls.

### Mechanism 2: Schema-Guided Parameter Extraction
- **Claim:** LLMs reliably utilize diverse tools when interfaces are strictly defined via schemas, reducing hallucination of API calls.
- **Mechanism:** ExternalServiceIntegrator registers tools using detailed schemas (names, descriptions, argument types) that LLM uses as "blueprints" to map queries to JSON structures.
- **Core assumption:** LLM's context window is large enough to hold all tool schemas without degrading selection ability.
- **Break condition:** If tools have overlapping descriptions or ambiguous argument names causing incorrect selection.

### Mechanism 3: Iterative Retrieval-Augmented Generation (RAG)
- **Claim:** Scientific reasoning accuracy increases through iterative retrieval of up-to-date facts, correcting static knowledge limitations.
- **Mechanism:** System operates in loop: MessageSubmission -> RunMonitoring -> Tool Execution -> UpdateMessage, allowing LLM to assess if retrieved data sufficiently answers query.
- **Core assumption:** Retrieval tools return clean, high-quality data that doesn't confuse LLM's subsequent reasoning.
- **Break condition:** If retrieval loop exceeds context limits or model enters cycle of repeated queries without convergence.

## Foundational Learning

- **Concept: Tool-Integrated Reasoning (TIR)**
  - **Why needed:** This paper implements TIR. Understand that LLM is "acting" (calling tools) to think, not "thinking" harder.
  - **Quick check:** Can you distinguish between a model generating answers from weights vs. from external tool output?

- **Concept: Schema Validation (Pydantic/JSON)**
  - **Why needed:** Framework relies on structured schemas to bridge natural language and code. Type hints and validation are critical for debugging HandleRequiredAction.
  - **Quick check:** If API expects ISO date string but LLM extracts "next Tuesday," which component fails?

- **Concept: Agentic Orchestration**
  - **Why needed:** Paper moves beyond simple chatbots to agentic workflows (planning -> tool use -> reflection).
  - **Quick check:** What prevents agent from getting stuck in infinite loop of tool calls?

## Architecture Onboarding

- **Component map:** User Query -> RunMonitoring -> HandleRequiredAction -> External API -> UpdateMessage -> LLM Context -> Final Answer
- **Critical path:**
  1. User sends query
  2. RunMonitoring scans LLM response for tool triggers
  3. If trigger found, HandleRequiredAction executes external API
  4. UpdateMessage injects API result back into LLM context
  5. LLM generates final natural language answer
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Iterative loop increases accuracy but introduces multiple network hops, significantly increasing response time
  - **Flexibility vs. Stability:** Supporting "any API" requires complex schema management; restricting tool types simplifies HandleRequiredAction logic
- **Failure signatures:**
  - Silent Tool Failure: API returns 200 OK with error message that LLM interprets as valid answer
  - Context overflow: Aggregating multiple PDFs or search results exceeds LLM token limit
  - Shadowing: LLM ignores tool output and answers from internal (potentially outdated) weights
- **First 3 experiments:**
  1. Reproduce Math Baseline: Run 33 math questions from MMLU with Wolfram Alpha disabled to establish baseline failure rate
  2. Schema Stress Test: Provide vague tool description ("A tool for data") and verify if RunMonitoring fails to trigger or triggers incorrectly
  3. Iteration Depth Test: Ask multi-hop question requiring search 1 -> extraction -> search 2 to validate iterative loop

## Open Questions the Paper Calls Out
- How can the framework's decision-making logic for tool selection be optimized to handle ambiguous or complex queries?
- What is the specific performance contribution of computational tools versus search/retrieval tools in scientific reasoning tasks?
- Does Athena outperform state-of-the-art models like GPT-4o when those models are also equipped with native tool-use or code-interpreter capabilities?

## Limitations
- Exact methodology for selecting 67 math and 100 science questions from MMLU is not specified, raising questions about sampling bias
- Baseline model identifiers (LLaMA-Large, GPT-4o) are ambiguous without specific version or checkpoint details
- Framework's robustness to diverse or poorly documented APIs is not tested despite claims of broad applicability

## Confidence
- **High Confidence:** Core architectural claim that LLMs can be enhanced by integrating external tools is well-supported and aligns with existing literature
- **Medium Confidence:** Specific accuracy numbers are plausible but lack of detail on question sampling and baseline model versions introduces uncertainty
- **Low Confidence:** Claims of broad applicability to "any API" are not validated through testing with diverse or poorly documented APIs

## Next Checks
1. Baseline Reproduction: Disable all external tools in Athena and run same MMLU subsets to establish true baseline for framework's core LLM performance
2. Tool Description Sensitivity: Systematically vary clarity and specificity of tool descriptions in schema and measure impact on tool selection accuracy
3. Context Window Stress Test: Design multi-hop query requiring 3+ tool calls and verify iterative RAG mechanism converges without exceeding token limits or entering infinite loops