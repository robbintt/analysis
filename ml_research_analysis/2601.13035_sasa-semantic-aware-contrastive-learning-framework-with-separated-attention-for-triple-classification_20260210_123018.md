---
ver: rpa2
title: 'SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention
  for Triple Classification'
arxiv_id: '2601.13035'
source_url: https://arxiv.org/abs/2601.13035
tags:
- semantic
- learning
- knowledge
- entity
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses triple classification in knowledge graphs
  by proposing SASA, a framework combining separated attention mechanism and semantic-aware
  contrastive learning. The method encodes triples using dual-tower BERT models with
  separated attention to enhance semantic interaction between decoupled representations.
---

# SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification

## Quick Facts
- **arXiv ID**: 2601.13035
- **Source URL**: https://arxiv.org/abs/2601.13035
- **Reference count**: 40
- **Primary result**: Achieves accuracy improvements of +5.9% on FB15k-237 and +3.4% on YAGO3-10 for triple classification in knowledge graphs

## Executive Summary
This paper addresses triple classification in knowledge graphs by proposing SASA, a framework combining separated attention mechanism and semantic-aware contrastive learning. The method encodes triples using dual-tower BERT models with separated attention to enhance semantic interaction between decoupled representations. It then applies hierarchical contrastive learning at both local (dropout-augmented positives) and global (hard negative retrieval via BGE) levels. Experimental results show SASA significantly outperforms state-of-the-art methods, demonstrating effectiveness in capturing fine-grained semantic distinctions in knowledge graphs.

## Method Summary
SASA employs a dual-tower BERT architecture where each tower processes different components of the triple (subject, relation, object) separately. The separated attention mechanism allows for enhanced semantic interaction between these decoupled representations. The framework incorporates hierarchical contrastive learning: local contrastive learning creates positive pairs through dropout augmentation, while global contrastive learning retrieves hard negatives using the BGE model. This dual-level approach enables the model to learn both fine-grained local patterns and broader semantic relationships.

## Key Results
- Achieves +5.9% accuracy improvement on FB15k-237 benchmark
- Achieves +3.4% accuracy improvement on YAGO3-10 benchmark
- Outperforms state-of-the-art methods in triple classification tasks

## Why This Works (Mechanism)
The separated attention mechanism allows the model to focus on different semantic aspects of triple components independently before combining them, reducing interference between subject, relation, and object representations. The hierarchical contrastive learning framework strengthens the model's ability to distinguish between semantically similar but factually different triples by exposing it to both locally augmented positive examples and globally retrieved hard negatives. This dual approach enables the model to capture both fine-grained semantic distinctions and broader relational patterns in knowledge graphs.

## Foundational Learning

**BERT-based encoding**: Why needed - To leverage pre-trained language understanding capabilities for semantic representation of knowledge graph elements. Quick check - Verify that BERT embeddings capture semantic relationships between entities and relations.

**Contrastive learning**: Why needed - To learn discriminative representations by pulling similar triples together and pushing dissimilar ones apart. Quick check - Measure embedding distances between positive and negative pairs.

**Separated attention mechanism**: Why needed - To enable independent processing of triple components while maintaining their semantic relationships. Quick check - Analyze attention weight distributions across different triple components.

**Hard negative mining**: Why needed - To improve model robustness by exposing it to challenging negative examples. Quick check - Evaluate model performance on increasingly difficult negative samples.

**Dual-tower architecture**: Why needed - To process triple components separately while enabling cross-component interaction. Quick check - Compare performance with single-tower baseline.

## Architecture Onboarding

**Component map**: Input triples -> Dual BERT towers with separated attention -> Semantic representation fusion -> Local contrastive learning (dropout augmentation) -> Global contrastive learning (BGE hard negative retrieval) -> Triple classification output

**Critical path**: Triple encoding (separated attention) -> Semantic fusion -> Local contrastive learning -> Global contrastive learning -> Classification decision

**Design tradeoffs**: Dual-tower architecture increases parameter count and computational cost but enables more expressive semantic interactions. Hierarchical contrastive learning adds training complexity but improves discriminative power. The choice of BERT as encoder limits scalability but provides strong semantic understanding.

**Failure signatures**: Poor performance on rare relations, inability to handle compositional triples, sensitivity to noise in negative sampling, computational bottlenecks with large knowledge graphs.

**First experiments**: 1) Compare separated attention versus standard attention on validation accuracy. 2) Evaluate impact of local contrastive learning alone versus combined with global contrastive learning. 3) Test different negative sampling strategies (random vs BGE-retrieved) on model robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of dual-tower architecture with separated attention may hinder scalability to large-scale knowledge graphs
- Reliance on BERT-based encoders constrains applicability to scenarios with limited computational resources
- Dependence on high-quality negative sampling through BGE retrieval could introduce brittleness if retrieval quality degrades in different domains or languages

## Confidence

**High Confidence**: The core methodology (separated attention mechanism and contrastive learning framework) is well-defined and technically sound. The reported accuracy improvements on FB15k-237 and YAGO3-10 are substantial and reproducible.

**Medium Confidence**: The claim about capturing "fine-grained semantic distinctions" relies heavily on benchmark performance rather than qualitative analysis of what specific semantic features the model learns.

**Medium Confidence**: The hierarchical contrastive learning approach is innovative, but the relative contribution of local versus global contrastive components is not clearly isolated through ablation studies.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of separated attention versus contrastive learning components to the overall performance gains.
2. Test the framework's robustness on knowledge graphs with different languages, domains, and noise levels to assess generalization beyond FB15k-237 and YAGO3-10.
3. Perform computational efficiency analysis comparing SASA's runtime and memory requirements against baseline methods on datasets of varying sizes to evaluate scalability constraints.