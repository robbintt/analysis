---
ver: rpa2
title: 'HERA: Improving Long Document Summarization using Large Language Models with
  Context Packaging and Reordering'
arxiv_id: '2502.00448'
source_url: https://arxiv.org/abs/2502.00448
tags:
- hera
- long
- llms
- document
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HERA, a long document summarization framework
  that addresses the challenge of LLMs' poor performance on long documents due to
  scattered information and messy narrative order. HERA works by first segmenting
  the document by semantic structure, retrieving segments about the same event, and
  reordering them to form the input context.
---

# HERA: Improving Long Document Summarization using Large Language Models with Context Packaging and Reordering

## Quick Facts
- arXiv ID: 2502.00448
- Source URL: https://arxiv.org/abs/2502.00448
- Reference count: 12
- Primary result: HERA improves long document summarization by filtering irrelevant content and reordering narrative flow without fine-tuning

## Executive Summary
HERA addresses the challenge of long document summarization where LLMs struggle with scattered information and messy narrative order. The framework segments documents, retrieves event-relevant paragraphs, reorders them for coherence, and generates hierarchical summaries. It outperforms foundation models on ROUGE, BERTScore, and faithfulness metrics across multiple LLMs including LLaMA 2, LLaMA 3, Gemini 1.5, and GPT-4 on arXiv and PubMed datasets.

## Method Summary
HERA processes long documents through a pipeline: document segmentation by paragraphs, local summarization using BRIO, event extraction and paragraph retrieval using LLMs, narrative reordering via NAON, event-level summarization, and final aggregation. The approach filters irrelevant content through context packaging and improves comprehension through reordering, then hierarchically generates summaries to reduce computational complexity. No additional fine-tuning is required beyond the base models.

## Key Results
- HERA outperforms foundation models on ROUGE-1/2/L, BERTScore, FactCC, and SummaC metrics
- Context packaging and reordering significantly improve summary quality over baseline models
- Optimal bag size of 5-6 paragraphs balances information completeness with noise reduction
- Performance consistent across LLaMA 2, LLaMA 3, Gemini 1.5, and GPT-4

## Why This Works (Mechanism)

### Mechanism 1: Context Packaging via Event-Centric Retrieval
HERA filters irrelevant segments by generating sentence-long representations per paragraph, then using LLM retrieval to select top-k relevant segments per event. This reduces distractor interference that causes factual inconsistencies in LLM outputs. The core assumption is that LLMs are easily distracted by irrelevant yet misleading content. Break condition: bag size <5 loses information; Table 4 shows k=5-6 optimal.

### Mechanism 2: Narrative Reordering via Semantic Coherence
NAON reorders retrieved segments based on summary-sentence representations to create coherent narrative sequences. This addresses position bias where LLMs favor beginning/end context, improving comprehension and summary fluency. The reordering maps back to original paragraphs before summarization. Break condition: temporally ambiguous events may introduce artifacts when reordered.

### Mechanism 3: Hierarchical Summary Aggregation
Local summaries are generated per event then aggregated into an overall summary using LLM-generated connectives. This decomposition reduces cognitive load and hallucination risk compared to direct end-to-end generation. The approach trades multiple LLM calls for reduced computational complexity per call. Break condition: highly interdependent events may lose cross-event relationships during aggregation.

## Foundational Learning

- Concept: **Lost in the Middle phenomenon**
  - Why needed here: HERA's reordering mechanism implicitly addresses position bias where LLMs forget middle-position context
  - Quick check question: Can you explain why retrieving and reordering segments might mitigate middle-position forgetting?

- Concept: **Event Coreference Resolution**
  - Why needed here: HERA's "segment bag" creation requires identifying paragraphs describing the same event across a document
  - Quick check question: How would you distinguish two paragraphs about similar vs. identical events?

- Concept: **ROUGE / BERTScore / Faithfulness Metrics (FactCC, SummaC)**
  - Why needed here: Paper evaluates on lexical overlap (ROUGE), semantic similarity (BERTScore), and factual consistency (FactCC, SummaC)
  - Quick check question: Why might a summary score high on ROUGE but low on FactCC?

## Architecture Onboarding

- Component map: Paragraph Segmenter -> BRIO Local Summarizer -> Event Extractor (LLM) -> Retrieval Ranker (LLM) -> NAON Reordering -> Event Summarizer (LLM) -> Aggregator (LLM)
- Critical path: Local summarization → Event extraction → Retrieval (Top-k) → Reordering → Event summarization → Aggregation
- Design tradeoffs:
  - Bag size (k): Paper shows k=5-6 optimal; smaller loses coverage, larger reintroduces noise
  - Summarization model choice: BRIO used for efficiency; larger models may improve representation quality
  - Reordering model: NAON chosen for SOTA performance; adds dependency but speeds sorting via summary-sentence proxies
- Failure signatures:
  - Low ROUGE with high FactCC: Over-conservative summarization missing key content
  - High ROUGE with low FactCC: Retrieval including irrelevant/distractor segments
  - Incoherent aggregated summary: Reordering model misordering semantically ambiguous events
  - Increased inference time >2x baseline: Bag size too large or redundant retrieval calls
- First 3 experiments:
  1. Bag size sweep (k=3 to k=8) on validation set to replicate Table 4 curves for your target domain
  2. Ablation: packaging-only vs. packaging+reordering to quantify relative contribution per Table 3
  3. Per-event manual inspection of retrieved segment bags on 10 examples to verify retrieval precision and identify systematic misses

## Open Questions the Paper Calls Out

- Can task-specific prompt optimization further enhance HERA's performance on subtasks like event extraction and summary aggregation? The authors note they don't optimize prompt templates for subtasks, using generic zero-shot prompts instead.
- Would replacing LLM-based retrieval with specialized dense retrieval models improve context packaging quality? The paper acknowledges it doesn't use more powerful retrieval methods beyond LLM ranking.
- How do HERA-generated summaries align with human judgment regarding coherence and faithfulness compared to baseline models? The evaluation relies solely on automatic metrics without human assessment.

## Limitations
- Reliance on multiple LLM calls introduces computational overhead and potential brittleness
- No human evaluation of coherence and overall summary quality
- Complex, overlapping, or ambiguous events may challenge event boundary detection
- Computational overhead from multiple LLM calls may limit scalability despite claimed efficiency

## Confidence

**High Confidence**: Context packaging mechanism is well-supported by ablation study and aligns with LLM position bias literature. Factual consistency improvements over baseline models are reasonably supported.

**Medium Confidence**: Narrative reordering contribution is demonstrated but edge cases with ambiguous events aren't sufficiently addressed. Hallucination reduction claim is plausible but not directly validated.

**Low Confidence**: Efficiency claims lack rigorous time/compute analysis. "No additional fine-tuning" statement glosses over practical integration challenges with multiple models.

## Next Checks

1. **Retrieval Precision Validation**: Manually inspect retrieved segment bags for 50 randomly selected examples to measure precision@k and identify systematic retrieval failures.

2. **Cross-Domain Robustness**: Apply HERA to different domains (news articles, technical documentation) to test generalization beyond scientific papers.

3. **Computational Overhead Analysis**: Measure wall-clock time and cost per document across all HERA pipeline steps versus baseline end-to-end models to empirically validate efficiency claims.