---
ver: rpa2
title: Learning to Reason in LLMs by Expectation Maximization
arxiv_id: '2512.20169'
source_url: https://arxiv.org/abs/2512.20169
tags:
- iteration
- reasoning
- data
- accuracy
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal connection between expectation-maximization
  (EM) and reward-based optimization for learning reasoning in large language models
  (LLMs). The authors model reasoning as a latent variable problem, where intermediate
  rationales are sampled and filtered based on correctness.
---

# Learning to Reason in LLMs by Expectation Maximization

## Quick Facts
- arXiv ID: 2512.20169
- Source URL: https://arxiv.org/abs/2512.20169
- Reference count: 40
- Primary result: PPS outperforms RS and STaR in LLM reasoning tasks, achieving higher accuracy with fewer training samples by conditioning rationale generation on correct answers.

## Executive Summary
This paper introduces a formal connection between expectation-maximization (EM) and reward-based optimization for learning reasoning in large language models (LLMs). The authors model reasoning as a latent variable problem, where intermediate rationales are sampled and filtered based on correctness. They propose three sampling schemes: rejection sampling, self-taught reasoning (STaR), and prompt posterior sampling (PPS), the latter being a novel approach that conditions rationale generation on the correct answer. Experiments on LLM-as-a-judge calibration and summarization from feedback tasks with Llama and Qwen models show that PPS consistently outperforms other methods, achieving higher accuracy with fewer training samples. This demonstrates that the quality of the sampling distribution, not just the quantity of data, is critical for effective self-improvement in LLM reasoning.

## Method Summary
The authors formalize reasoning as a latent variable model x → z → y*, where z represents intermediate rationales and y* is the correct answer. They derive a Filtered EM (FEM) objective that optimizes a lower bound on expected reward by alternating between E-steps (sampling rationales) and M-steps (gradient updates on accepted samples). Three sampling schemes are proposed: Rejection Sampling (RS) generates rationales until correct answers appear, STaR adds self-rationalization when rejection fails, and PPS directly conditions on the correct answer via prompting. The FEM loop runs for K=5 iterations with SFT fine-tuning on accepted rationale-answer pairs. Experiments use Llama and Qwen models on HelpSteer2 and Summarize from Feedback datasets.

## Key Results
- PPS achieves highest accuracy in 22/25 HelpSteer2 metrics and 15/20 Summarize from Feedback metrics
- PPS outperforms RS and STaR despite using fewer training samples (higher data efficiency)
- Data utilization shows PPS achieves better performance with less training data than STaR
- Statistical significance: p < 0.01 for HelpSteer2 and p ≈ 0.02 for Summarize from Feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning can be formalized as a latent variable model where intermediate rationales are learned via expectation-maximization.
- Mechanism: The model treats rationales z as latent variables in the generative chain x → z → y*. The E-step approximates the posterior distribution over rationales conditioned on correct answers, while the M-step performs gradient-based optimization on accepted samples. A binary reward r(ŷ, y*) = 1[ŷ = y*] filters out inconsistent samples, ensuring gradient updates operate only on rationales that lead to correct answers.
- Core assumption: The posterior distribution is consistent with the evidence it conditions on—samples from π(·|x, y*) will almost surely produce the correct answer, making answer-consistency filtering theoretically equivalent to posterior support.
- Evidence anchors:
  - [abstract] "We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason."
  - [Section 2.1] Derives the filtered gradient update: θ(k) ← θ(k) + η(k)r(ŷ_i, y*_i)∇log π(ẑ_i, ŷ_i|x_i; θ)
  - [corpus] Related work "Training Large Language Models to Reason via EM Policy Gradient" (arXiv:2504.18587) similarly connects EM to RL-based reasoning, with FMR=0.58, suggesting convergent validation of the EM-to-reasoning connection.
- Break condition: If the model cannot generate correct answers at non-trivial rates, rejection sampling yields insufficient training data and EM iterations fail to improve the model.

### Mechanism 2
- Claim: The quality of the rationale sampling distribution matters more than the quantity of training data for self-improvement.
- Mechanism: Different sampling schemes (RS, STaR, PPS) instantiate the same FEM framework but differ in how they approximate the E-step posterior. Rejection sampling samples from π(·|x) until correct answers appear; STaR adds rationalization when rejection fails; PPS directly conditions on the correct answer via prompting. The sampling distribution determines both the acceptance rate and the quality of rationales that the model learns from.
- Core assumption: Conditioning on the correct answer y* during rationale generation provides informative guidance for producing higher-quality rationales, not just higher acceptance rates.
- Evidence anchors:
  - [abstract] "Our experiments... show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes."
  - [Section 4.2] "Crucially, the data utilization shows that this performance gain is not driven by data quantity; PPS achieves a higher accuracy with less training data than STaR."
  - [corpus] HS-STaR (arXiv:2505.19866) addresses sampling quality via difficulty estimation and budget reallocation, but does not provide FMR or citation evidence for direct comparison.
- Break condition: If conditioning on y* provides no additional signal for rationale quality (e.g., in tasks where the answer is uninformative for reasoning), PPS will not outperform simpler sampling schemes.

### Mechanism 3
- Claim: Prompt Posterior Sampling (PPS) outperforms rejection sampling and STaR by directly approximating the posterior through conditioning.
- Mechanism: PPS prompts the model with both the question x and the correct answer y*, asking it to generate a rationale that justifies that answer. This directly targets the posterior distribution π(z|x, y*) without expensive rejection. The model leverages its instruction-following capabilities to generate plausible reasoning paths that connect the question to the given answer.
- Core assumption: Modern instruction-tuned LLMs can perform "posterior inference" via prompting—given x and y*, they can generate coherent rationales z that plausibly explain the connection.
- Evidence anchors:
  - [Section 3] "Modern LLMs are instruction fine-tuned to follow human feedback. Therefore, one natural way of implementing posterior sampling is by conditioning on the correct answer in the prompt."
  - [Section 4.2, Table 1] PPS achieves highest accuracy in 22/25 HelpSteer2 metrics and 15/20 Summarize from Feedback metrics, with statistical significance p < 0.01 and p ≈ 0.02 respectively.
  - [corpus] No direct corpus comparison; related work focuses on different sampling strategies but does not systematically evaluate PPS-style conditioning.
- Break condition: Train-test mismatch—if rationales generated by conditioning on y* during training do not transfer to test-time generation (where y* is unavailable), PPS may fail to improve test performance.

## Foundational Learning

- **Concept: Expectation-Maximization (EM) Algorithm**
  - Why needed here: The entire FEM framework is built on EM theory; understanding E-step (posterior computation) and M-step (parameter optimization) is essential to grasp why sampling matters.
  - Quick check question: Given a latent variable model p(x, z|θ), write the EM objective J(θ) for the E-step and explain why we can't compute it exactly for LLMs.

- **Concept: Latent Variable Models and Posterior Distributions**
  - Why needed here: Rationales are treated as latent variables; the core challenge is approximating the posterior π(z|x, y*) when exact inference is intractable.
  - Quick check question: In the model x → z → y*, why is the posterior π(z|x, y*) harder to compute than the prior π(z|x)?

- **Concept: Reward-Weighted Policy Gradient / RL as Inference**
  - Why needed here: The paper connects EM to reward-based optimization (Lemma 1), showing FEM optimizes a lower bound on expected reward.
  - Quick check question: Explain how filtering samples by correctness (r = 1[ŷ = y*]) relates to both EM and RL objectives.

## Architecture Onboarding

- **Component map:**
  ```
  FEM Loop (K iterations):
  ├── E-step: Sampling Module
  │   ├── Input: (x_i, y*_i), current θ^(k-1)
  │   ├── Sampling scheme choice: RS / STaR / PPS
  │   └── Output: (ẑ_i, ŷ_i) sample
  ├── Reward Filter
  │   └── Binary check: ŷ_i == y*_i
  └── M-step: SFT Module
      ├── Gradient update on accepted samples
      └── Learning rate schedule: η ∈ [10^-6, 10^-7]
  ```

- **Critical path:** The sampling scheme (E-step) determines rationale quality → accepted samples → gradient updates → model improvement. Poor sampling distributions yield low-quality or insufficient training data, breaking the virtuous cycle.

- **Design tradeoffs:**
  - **RS vs. PPS:** RS samples from the true prior (matches test distribution) but has low acceptance rates; PPS has high acceptance but introduces train-test mismatch.
  - **Budget M:** Higher M in RS increases data utilization linearly but also increases computation cost linearly; PPS achieves better performance with M=1.
  - **Iteration count K:** Paper uses K=5; more iterations can help but may plateau or overfit.

- **Failure signatures:**
  - Low data utilization (<20%) indicates the model cannot generate correct answers reliably—consider starting with a stronger base model or using PPS.
  - Test accuracy plateauing early while reasoning length increases suggests the model is learning to be verbose without improving reasoning.
  - PPS underperforming RS suggests the task may not benefit from answer-conditioning (e.g., complex math where answers are uninformative).

- **First 3 experiments:**
  1. **Baseline comparison on your task:** Implement RS (M=1), STaR, and PPS on a held-out subset of your target task. Compare test accuracy, data utilization, and reasoning length across 5 FEM iterations to validate PPS advantage in your domain.
  2. **Ablation on conditioning prompt:** Test multiple PPS prompt variants (as in Section C, PPS/PPS2/PPS3) to measure sensitivity to prompt formulation and ensure your conditioning instruction is precise.
  3. **Data scaling test:** Run FEM with N ∈ {1000, 2000, 4000} training samples to determine minimum data requirements for stable learning and whether PPS advantages hold at your dataset scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Prompt Posterior Sampling (PPS) remain effective in complex mathematical reasoning domains where the correct answer provides weak guidance for generating the intermediate rationale?
- Basis in paper: [explicit] The authors state they chose alignment tasks because "conditioning on the correct answer provides a strong guiding signal" and explicitly plan to "experiment with more complex tasks, such as math reasoning."
- Why unresolved: The efficacy of PPS depends on the answer acting as a strong hint for the reasoning path; in math, a numeric answer offers less structural guidance than the textual scores used in the paper.
- What evidence would resolve it: Empirical evaluation of FEM on mathematical datasets (e.g., GSM8K or MATH) comparing PPS performance against rejection sampling and STaR baselines.

### Open Question 2
- Question: Do the efficiency improvements of the proposed sampling schemes generalize to online reinforcement learning algorithms like GRPO?
- Basis in paper: [explicit] The authors list "investigate if our improvements in sampling generalize to online reinforcement learning, such as GRPO" as a specific direction for future work.
- Why unresolved: The current method uses an iterative supervised fine-tuning (batch) paradigm; it is unclear if the "hint-based" sampling distribution remains stable or beneficial when integrated into the dynamic policy updates of online RL.
- What evidence would resolve it: Implementation of PPS within an online RL framework (like GRPO) demonstrating improved sample efficiency or accuracy over standard online sampling baselines.

### Open Question 3
- Question: Can the convergence and statistical properties of the Filtered EM (FEM) objective be formally analyzed given its reliance on single Monte Carlo samples and gradient approximations?
- Basis in paper: [explicit] The authors explicitly state they "have not formally analyzed FEM" and leave this theoretical validation "for future work."
- Why unresolved: The algorithm combines multiple approximations (Monte Carlo E-step, filtered M-step) that theoretically lack guarantees of convergence or convergence speed without specific assumptions about the model capacity and data.
- What evidence would resolve it: A theoretical proof establishing convergence bounds or sample complexity guarantees for the FEM objective under the defined sampling approximations.

## Limitations
- The paper doesn't validate whether PPS-generated rationales are qualitatively better or merely answer-consistent beyond matching correct answers.
- Potential train-test mismatch: conditioning on y* during training may not generalize to test-time generation where answers are unknown.
- The paper doesn't investigate whether FEM converges to local optima or whether learned reasoning generalizes beyond the specific task distribution.

## Confidence

**High confidence:** The formal EM connection and theoretical framework are mathematically sound. The experimental results showing PPS outperforms other sampling schemes are clearly demonstrated with statistical significance.

**Medium confidence:** The mechanism claim that PPS directly approximates the posterior through conditioning is plausible but not definitively proven. The claim about quality over quantity depends on accepting that conditioning on y* provides informative guidance for rationale quality.

**Low confidence:** The paper doesn't address whether learned reasoning transfers to novel problems or whether FEM might be overfitting to the answer-conditional distribution during training.

## Next Checks

1. **Qualitative rationale analysis**: Sample and compare rationales from RS, STaR, and PPS on identical problems. Evaluate whether PPS rationales are genuinely more coherent, logically structured, or pedagogically sound beyond simply matching the correct answer.

2. **Train-test distribution alignment test**: After FEM training with PPS, generate rationales on test data without answer conditioning and evaluate whether reasoning quality degrades compared to models trained with RS or STaR.

3. **Novel problem generalization**: Test whether FEM-trained models can solve structurally similar problems they haven't seen during training, measuring whether learned reasoning transfers beyond answer-conditional patterns.