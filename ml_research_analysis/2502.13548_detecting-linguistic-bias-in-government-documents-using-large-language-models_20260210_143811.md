---
ver: rpa2
title: Detecting Linguistic Bias in Government Documents Using Large language Models
arxiv_id: '2502.13548'
source_url: https://arxiv.org/abs/2502.13548
tags:
- bias
- detection
- terms
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting linguistic bias
  in government documents, a domain that has received limited attention despite its
  significant implications for governance. To tackle this issue, the authors introduce
  the Dutch Government Data for Bias Detection (DGDB), a dataset of 3,632 sentences
  sourced from the Dutch House of Representatives and annotated for bias by experts.
---

# Detecting Linguistic Bias in Government Documents Using Large language Models

## Quick Facts
- arXiv ID: 2502.13548
- Source URL: https://arxiv.org/abs/2502.13548
- Reference count: 35
- Primary result: Fine-tuned BERT-based models (BERTje, RobBERT) achieve F1=0.812 on Dutch government bias detection, significantly outperforming generative LLMs.

## Executive Summary
This paper introduces the Dutch Government Data for Bias Detection (DGDB), a specialized dataset of 3,632 sentences from Dutch parliamentary documents annotated for linguistic bias. The authors fine-tune several BERT-based models on this dataset and compare their performance against zero-shot generative LLMs like GPT-3.5 and GPT-4o mini. The results demonstrate that fine-tuned encoder models significantly outperform generative approaches for this nuanced task, with F1 scores reaching 0.812 in-domain and 0.554 out-of-domain. The study highlights the importance of domain-specific labeled datasets for detecting subtle linguistic biases in government contexts.

## Method Summary
The authors created the DGDB dataset by scraping Dutch House of Representatives documents, normalizing the text, and having expert annotators label sentences for bias. They fine-tuned BERTje and RobBERT models on this dataset using Adam optimization with learning rate 2e-5, batch size 8, and 4 epochs. The best-performing model used undersampling to balance the dataset, achieving F1=0.812 on in-domain testing. Performance was compared against zero-shot GPT-3.5 and GPT-4o mini models, which performed significantly worse.

## Key Results
- Fine-tuned BERTje achieves F1=0.812 on in-domain bias detection, outperforming GPT-3.5 (F1≈0.55)
- Undersampling the majority class improves F1 from 0.793 to 0.812 by balancing the training data
- Out-of-domain performance drops to F1=0.554, indicating limited generalization to unseen bias terms
- LIME analysis reveals contextual terms influence bias probability more than isolated keywords

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specialized encoder fine-tuning outperforms generative zero-shot inference for nuanced bias detection in Dutch government texts.
- **Mechanism:** Pre-trained BERT-based models are fine-tuned on the DGDB dataset, adjusting weights to prioritize domain-specific linguistic patterns over general reasoning capabilities.
- **Core assumption:** Generic instruction-following capabilities in LLMs do not substitute for gradient updates on high-quality, domain-specific labeled data.
- **Evidence anchors:**
  - "fine-tuned models achieve strong performance... significantly outperforming generative models."
  - "The significant performance improvement... underscores the continued necessity of labeled datasets."
  - Corpus neighbors confirm active research in Dutch bias, but primarily for general language models rather than the government domain.

### Mechanism 2
- **Claim:** Undersampling the majority (neutral) class improves detection of the minority (biased) class.
- **Mechanism:** Reducing "not biased" instances from 74.7% to ~50% forces the model to learn features of rare biased instances rather than defaulting to neutral classification.
- **Core assumption:** Bias labels in the minority class contain sufficient signal for generalization without overfitting.
- **Evidence anchors:**
  - "undersampling consistently produces the best results... benefits of a more balanced dataset outweigh the potential drawbacks."
  - Shows F1 score improvement from 0.793 (no sampling) to 0.812 (undersampling).
  - Weak/missing specific corpus evidence regarding sampling strategies for Dutch bias.

### Mechanism 3
- **Claim:** Contextual embeddings enable disambiguation of "conditionally biased" terms based on surrounding semantics.
- **Mechanism:** Transformer architectures process full sentence context rather than relying on static keyword blacklists.
- **Core assumption:** The model learns to associate specific collocations with the concept of "exclusion" defined in annotation guidelines.
- **Evidence anchors:**
  - Distinction made between "Prohibited" terms and "Conditionally biased" terms which require context.
  - LIME analysis shows specific contextual terms influence the bias probability.
  - Related works like Dutch CrowS-Pairs also utilize context to measure stereotype.

## Foundational Learning

- **Concept: Linguistic Bias vs. Hate Speech**
  - **Why needed here:** To correctly interpret the dataset labels; the paper explicitly excludes purely intentional hate speech in favor of subtler, often unintentional linguistic biases.
  - **Quick check question:** Does the model classify a sentence containing a positive prejudice (e.g., "women are more mature") as biased? (Answer should be Yes, per guidelines).

- **Concept: Inter-Annotator Agreement (Fleiss' Kappa)**
  - **Why needed here:** The dataset has a "fair" agreement (κ = 0.35). Learners must understand that low scores are expected in highly subjective tasks and do not necessarily render the dataset useless, but they introduce label noise.
  - **Quick check question:** Why might two experts disagree on whether the term "parents" implies bias against "caregivers"?

- **Concept: BERT-based Architectures (BERTje/RobBERT)**
  - **Why needed here:** Understanding that these are bidirectional encoders, not generative decoders. They are chosen for their ability to produce dense vector representations of text for classification.
  - **Quick check question:** Why is a bidirectional context crucial for detecting the "conditionally biased" terms described in Section 3.1?

## Architecture Onboarding

- **Component map:** Dutch House of Representatives API → Scraper → Normalizer → Expert annotators (14 humans) → Conflict resolution → BERTje/RobBERT (Transformer Encoder) → Classification Head (Softmax) → LIME for error analysis.

- **Critical path:** The transition from raw government documents to the 3,632 expert-labeled sentences. The paper notes that while the API provides data, the annotation (specifically handling "unsure" labels via re-annotation) is the bottleneck for data quality.

- **Design tradeoffs:**
  - **In-domain vs. Out-of-domain:** Training on all terms yields high accuracy (0.812 F1) but poor generalization to new bias terms (0.554 F1).
  - **Precision vs. Recall:** The confusion matrix shows high accuracy on unbiased sentences (93%) but lower recall on biased sentences (63%), suggesting a conservative bias towards the "neutral" class.

- **Failure signatures:**
  - **Rare Term Failure:** Model accuracy drops to 0 for specific rare terms (e.g., "nieuwkomer") if they appear infrequently in training.
  - **Context Misinterpretation:** Misclassifying "condition sensitive" terms when the context is neutral but contains a trigger keyword.

- **First 3 experiments:**
  1. **Baseline Validation:** Re-run the BERTje zero-shot (untrained) vs. fine-tuned comparison to verify the 0.812 vs. ~0.55 gap on the provided test split.
  2. **Sampling Ablation:** Train three identical models using (a) No Sampling, (b) Oversampling, and (c) Undersampling to confirm that undersampling yields the highest F1 score for the "Biased" class.
  3. **Out-of-Domain Generalization:** Train a model excluding a specific category (e.g., "Religion") and evaluate specifically on that held-out category to quantify the generalization penalty.

## Open Questions the Paper Calls Out

1. **Question:** How does the bias detection performance vary when the DGDB methodology is adapted to government documents in other languages and distinct cultural contexts?
   - **Basis in paper:** The conclusion states that future work should focus on "expanding DGDB to encompass a broader range of languages and cultural contexts, thereby enhancing the generalizability of bias detection models."
   - **Why unresolved:** The current study validates the methodology and models solely on Dutch-language documents from the Dutch House of Representatives.
   - **What evidence would resolve it:** Results from replicating the dataset creation and model fine-tuning process on government corpora from non-Dutch speaking countries, comparing F1 scores against the Dutch baseline.

2. **Question:** Can integrating uncertainty quantification frameworks into the fine-tuned models improve reliability and user trust in real-world governmental applications?
   - **Basis in paper:** The conclusion suggests that "incorporating more labels and integrating uncertainty quantification frameworks can further improve model reliability and user trust."
   - **Why unresolved:** The current models output point predictions without confidence intervals, which limits accountability in high-stakes policy environments.
   - **What evidence would resolve it:** Experiments measuring calibration metrics (e.g., Expected Calibration Error) of models equipped with uncertainty layers, alongside user studies assessing operator confidence.

3. **Question:** To what extent does the reliance on a predefined seed list of keywords prevent the detection of novel or rapidly evolving forms of linguistic bias?
   - **Basis in paper:** The limitations section notes that "the seed list may not be exhaustive, potentially omitting some forms of bias from the resulting dataset" as perceptions of bias evolve.
   - **Why unresolved:** The data collection pipeline is inherently constrained by the initial vocabulary list, potentially creating a blind spot for biases not yet cataloged in the guidelines.
   - **What evidence would resolve it:** A comparative analysis where models trained on seed-list data are evaluated against a "wild-caught" dataset annotated without keyword constraints to identify missed false negatives.

## Limitations

- The DGDB dataset has a Fleiss' Kappa of 0.35, indicating "fair" inter-annotator agreement and introducing label noise.
- The model's strong in-domain performance (F1 0.812) drops significantly (F1 0.554) on out-of-domain terms, indicating over-reliance on specific vocabulary.
- The paper focuses exclusively on Dutch government documents, limiting generalizability to other languages and domains.

## Confidence

- **High Confidence:** The claim that fine-tuned BERT-based models outperform zero-shot generative LLMs for this specific task is well-supported by experimental results showing a clear performance gap.
- **Medium Confidence:** The effectiveness of undersampling to balance the dataset and improve minority class detection is supported by the ablation study, but the choice over other techniques is not deeply justified.
- **Low Confidence:** The assertion that findings "demonstrate the effectiveness of DGDB for this task" is limited by lack of comparison with other bias detection benchmarks and narrow evaluation scope.

## Next Checks

1. **Reproduce Sampling Impact:** Train three identical models using no sampling, oversampling, and undersampling on the training set. Verify that undersampling consistently yields the highest F1 score for the "Biased" class.

2. **Test Generalization Rigorously:** Train a model excluding a specific bias category (e.g., "Religion") from the training data, and evaluate specifically on that held-out category in the test set to provide a more stringent test of generalization.

3. **Benchmark Against Standard Datasets:** Evaluate the best-performing DGDB-trained model on a standard, multilingual bias detection dataset (e.g., CrowS-Pairs or StereoSet) to assess performance beyond the Dutch government domain.