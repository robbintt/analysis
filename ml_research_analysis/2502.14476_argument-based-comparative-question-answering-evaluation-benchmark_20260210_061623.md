---
ver: rpa2
title: Argument-Based Comparative Question Answering Evaluation Benchmark
arxiv_id: '2502.14476'
source_url: https://arxiv.org/abs/2502.14476
tags:
- comparison
- evaluation
- human
- summary
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the CompQA evaluation framework to address
  the challenge of benchmarking comparative question answering systems. The framework
  defines 15 criteria for assessing comparative summaries and implements an automatic
  evaluation pipeline using LLMs as judges.
---

# Argument-Based Comparative Question Answering Evaluation Benchmark

## Quick Facts
- arXiv ID: 2502.14476
- Source URL: https://arxiv.org/abs/2502.14476
- Authors: Irina Nikishina; Saba Anwar; Nikolay Dolgov; Maria Manina; Daria Ignatenko; Viktor Moskvoretskii; Artem Shelmanov; Tim Baldwin; Chris Biemann
- Reference count: 40
- Key outcome: CompQA framework evaluates comparative QA systems across 15 criteria, showing GPT-4 generates highest quality answers while Llama-3 70B best serves as automatic evaluator with strong correlation to human judgments.

## Executive Summary
This paper introduces CompQA, a framework for benchmarking comparative question answering systems that addresses the challenge of evaluating abstractive comparative summaries. The framework defines 15 granular criteria for assessing comparative summaries and implements an automatic evaluation pipeline using LLMs as judges. The study evaluates 6 contemporary LLMs on two datasets, along with human annotations, demonstrating that the framework achieves strong correlation with human expert evaluations while providing a scalable assessment method.

## Method Summary
The CompQA framework evaluates comparative answers using 15 criteria across three categories: structure (7 points), relevance (5 points), and quality (7 points), totaling 19 points. The evaluation uses LLM-as-a-judge approach with few-shot prompting (2 examples) to calibrate scoring. Four prompt scenarios test different generation conditions, from minimal prompts to structured responses with provided arguments. The framework employs Krippendorf's α for agreement measurement and Spearman correlation to validate alignment with human judgments. The study evaluates 6 LLMs (GPT-3.5, GPT-4, Llama3-8B, Llama3-70B, Perplexity, Mixtral) on two datasets with human annotations.

## Key Results
- GPT-4 generates the highest quality comparative answers across all evaluation scenarios
- Llama-3 70B performs best as an automatic evaluator with Spearman correlation of 0.76 with human judgments
- Krippendorf's α = 0.75 for human final score agreement indicates acceptable inter-annotator consistency
- Scenario 3 (structured prompt with optional arguments) produces highest quality summaries, while Scenario 4 (structured with required arguments) sometimes underperforms

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Criteria-Based Evaluation
Breaking evaluation into 15 granular criteria improves annotation consistency and enables reliable LLM-based assessment. The framework decomposes comparative answer quality into structure (7 points), relevance (5 points), and quality (7 points) categories, totaling 19 points. Each criterion has explicit scoring rules that reduce ambiguity and increase inter-annotator agreement. This approach assumes human annotators and LLMs can apply binary/multi-level scoring rules consistently when criteria are sufficiently concrete.

### Mechanism 2: Few-Shot Prompting Anchors LLM Evaluators to Human Standards
Providing LLM evaluators with 2-shot examples (one high-quality, one low-quality summary) calibrates their scoring to human judgment distributions. The evaluation prompt includes few-shot examples with explicit score dictionaries that ground the LLM's internal scoring rubric to human-labeled exemplars, reducing systematic bias. This mechanism assumes LLMs can generalize from 2 examples to diverse summary types without extensive calibration data.

### Mechanism 3: Scenario Differentiation Tests Instruction-Following vs. Intrinsic Knowledge
Varying prompt complexity (Scenarios 1-4) reveals whether LLMs rely on provided arguments versus internal knowledge, and their ability to follow structural constraints. Higher scores on Scenarios 3-4 indicate the framework successfully differentiates instruction-following ability. This mechanism assumes higher scenario scores reflect genuine quality improvement rather than just surface-level format compliance.

## Foundational Learning

- **Concept: LLM-as-a-Judge Evaluation Paradigm**
  - Why needed: The entire CompQA framework relies on using LLMs as automated evaluators, which requires understanding their capabilities and limitations in approximating human judgment.
  - Quick check: If an LLM evaluator consistently scores summaries 2 points higher than humans across all criteria, is this a calibration issue or a rubric ambiguity issue? How would you diagnose which?

- **Concept: Comparative Question Answering (CQA) as Abstractive Summarization**
  - Why needed: CQA differs from standard QA by requiring synthesis of arguments about multiple objects in relation to each other, often without a single correct answer.
  - Quick check: Why does providing extracted arguments (Scenario 4) sometimes produce worse scores than letting the LLM generate its own arguments (Scenario 3)? What does this suggest about argument quality vs. LLM knowledge?

- **Concept: Annotation Agreement Metrics (Krippendorf's α)**
  - Why needed: Understanding Krippendorf's α is essential for interpreting the framework's inter-annotator agreement results and assessing the reliability of the evaluation criteria.
  - Quick check: Krippendorf's α = 0.75 indicates acceptable agreement, but how does this compare to typical thresholds for "good" agreement in annotation studies (often 0.8+)?

## Architecture Onboarding

### Component Map
Human annotations -> CompQA framework (15 criteria) -> LLM evaluators (6 models) -> Score aggregation -> Correlation analysis with human judgments

### Critical Path
Data preparation (50 object pairs with arguments) -> Human annotation (367 samples) -> LLM evaluation (2-shot prompting) -> Score computation -> Agreement analysis (Krippendorf's α, Spearman correlation)

### Design Tradeoffs
Granular criteria vs. evaluation complexity: 15 criteria provide detailed assessment but increase annotation burden. Few-shot prompting reduces calibration data needs but may miss edge cases. Multiple scenarios enable task differentiation but complicate analysis.

### Failure Signatures
Systematic over-scoring by LLM evaluators (particularly LLaMA-3 70b), indicating calibration issues. Poor correlation with human judgments suggesting criteria ambiguity or inadequate few-shot examples. Scenario score patterns that don't align with human quality assessments, suggesting surface compliance over genuine reasoning.

### Exactly 3 First Experiments
1. Run Llama-3 70B as evaluator on 20 summaries using 2-shot prompting to verify Spearman correlation of 0.76 with human judgments.
2. Test zero-shot vs. 2-shot vs. 5-shot prompting to quantify calibration impact on agreement with human evaluations.
3. Apply framework to 10 summaries from a different CQA dataset to test generalizability of the 15 criteria.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework effectiveness depends on quality and coverage of 2-shot examples, which are not fully specified in the paper
- Krippendorf's α = 0.75 falls short of the 0.8 threshold often considered good in annotation studies
- LLM evaluators systematically over-score compared to human judgments, suggesting calibration issues
- Framework's generalizability to other CQA domains beyond Touché dataset remains untested

## Confidence

**High confidence**: The framework's structural design (15 criteria decomposition, 4 scenario differentiation) and basic correlation results (Spearman up to 0.76) are well-supported by presented evidence.

**Medium confidence**: The claim that LLaMA-3 70b is the best automatic evaluator is supported but could benefit from additional model comparisons and calibration testing.

**Medium confidence**: The assertion that higher scenario scores reflect genuine quality improvement rather than surface compliance is plausible but not definitively proven given potential template-matching effects.

## Next Checks

1. **Calibration test**: Run the evaluation pipeline with zero-shot prompting (no examples) vs. 2-shot vs. 5-shot examples to quantify the impact of few-shot calibration on agreement with human judgments.

2. **Cross-domain validation**: Apply the CompQA framework to a different CQA dataset (e.g., from a different domain like healthcare or technology) to test generalizability of the 15 criteria and scoring rubric.

3. **Edge case analysis**: Systematically evaluate summaries containing contradictory arguments or biased information (like the Google vs. Yahoo example) to test whether the framework and LLM evaluators can detect reasoning flaws beyond structural compliance.