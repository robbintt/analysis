---
ver: rpa2
title: Improving Fairness of Large Language Models in Multi-document Summarization
arxiv_id: '2506.07479'
source_url: https://arxiv.org/abs/2506.07479
tags:
- fairpo
- fairness
- summaries
- social
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairPO, a preference tuning method that improves
  both summary-level and corpus-level fairness in multi-document summarization. FairPO
  generates preference pairs by perturbing document sets to improve summary-level
  fairness, and performs fairness-aware preference tuning by dynamically adjusting
  preference pair weights to improve corpus-level fairness.
---

# Improving Fairness of Large Language Models in Multi-document Summarization

## Quick Facts
- arXiv ID: 2506.07479
- Source URL: https://arxiv.org/abs/2506.07479
- Reference count: 23
- Key outcome: FairPO improves both summary-level and corpus-level fairness in multi-document summarization while maintaining quality across three LLMs and datasets

## Executive Summary
This paper addresses fairness in multi-document summarization by introducing FairPO, a preference tuning method that improves both Equal Coverage and Coverage Parity metrics. FairPO generates preference pairs by perturbing document sets to enhance summary-level fairness, then performs fairness-aware preference tuning with dynamic weight adjustment for corpus-level fairness. The method is evaluated across three large language models (Llama3.1, Mistral, and Gemma2) on three datasets (Amazon, MITweet, and SemEval), demonstrating superior performance compared to strong baselines including DPO, OPTune, prompting, and policy gradients while maintaining summary quality.

## Method Summary
FairPO combines perturbation-based preference pair generation with fairness-aware DPO. For each document set, the method generates three summaries by removing 10% of documents from overrepresented and underrepresented social attribute groups. It then selects the two summaries with maximum fairness difference as preference pairs. During training, FairPO dynamically adjusts preference pair weights based on running statistics of social attribute coverage, using a discount factor to balance short-term and long-term fairness improvements. The approach is implemented with LoRA fine-tuning at rank 16, learning rate 5e-5, and 8-bit quantization.

## Key Results
- FairPO achieves best overall performance across all models and datasets, improving both Equal Coverage and Coverage Parity metrics
- Human evaluation confirms FairPO generates fairer summaries compared to DPO-tuned models
- FairPO maintains summary quality while improving fairness, unlike some baselines that trade off quality for fairness gains
- The method outperforms strong baselines including DPO, OPTune, prompting, and policy gradients

## Why This Works (Mechanism)
FairPO works by explicitly addressing both summary-level and corpus-level fairness through preference learning. At the summary level, perturbation creates contrasting summaries that highlight fairness differences. At the corpus level, dynamic weight adjustment ensures the model learns to balance coverage across social attribute groups over time. The discount factor prevents extreme adjustments and maintains stability. By framing fairness as a preference learning problem rather than a direct constraint, FairPO can leverage existing DPO infrastructure while incorporating fairness-specific modifications.

## Foundational Learning
- **Multi-document summarization fairness**: Measuring whether summaries fairly represent diverse perspectives from source documents. Why needed: Standard summarization metrics don't capture fairness across social attribute groups. Quick check: Verify Equal Coverage (EC) and Coverage Parity (CP) metrics decrease after FairPO training.
- **Preference tuning with DPO**: Learning from preference pairs rather than direct supervision. Why needed: Provides a flexible framework for incorporating fairness objectives. Quick check: Ensure preference pairs show meaningful fairness differences before training.
- **Coverage probability estimation**: Using entailment models to measure how well different social attribute groups are represented in summaries. Why needed: Quantifies fairness at both summary and corpus levels. Quick check: Validate entailment model accuracy on held-out examples.

## Architecture Onboarding
- **Component map**: Documents -> Perturbation -> Summary Generation -> Preference Pair Selection -> Fairness-Aware Training
- **Critical path**: Document set → Perturbation (remove 10% docs) → Summary generation (3 versions) → Preference pair selection (min/max EC) → Weighted DPO training
- **Design tradeoffs**: Binary preference pairs vs. using all three summaries; fixed perturbation rate vs. adaptive; separate model training vs. unified multi-domain approach
- **Failure signatures**: 
  - EC/CP metrics not improving: Check perturbation strategy and entailment model accuracy
  - Quality degradation: Verify temperature settings and learning rate
  - Training instability: Reduce learning rate or increase discount factor
- **First experiments**:
  1. Generate preference pairs and verify they show meaningful fairness differences
  2. Train with basic DPO first, then add fairness-aware weighting
  3. Compare FairPO against baseline on a small subset before full training

## Open Questions the Paper Calls Out
- Can FairPO be effectively extended to optimize fairness simultaneously across multiple domains with diverse social attributes, rather than training separately for a single domain?
- Does incorporating all three generated summaries (original, positive-perturbed, negative-perturbed) into the optimization objective yield better fairness metrics than the current binary preference pair approach?
- Is FairPO robust to noisy or implicit social attribute labels, or does it require gold-standard explicit labels to function correctly?

## Limitations
- The fairness evaluation relies heavily on the entailment model's accuracy for coverage estimation, which is not fully detailed
- The perturbation strategy (removing 10% of documents) may not scale well to datasets with highly imbalanced social attributes
- The temperature hyperparameter varies by model and dataset without clear justification for chosen values

## Confidence
- **High confidence**: Core FairPO methodology is clearly described and theoretically sound
- **High confidence**: Experimental results showing FairPO outperforms baselines are reproducible
- **Medium confidence**: Human evaluation methodology lacks detail on rater selection and reliability measures

## Next Checks
1. Validate the entailment model choice and chunking strategy for coverage probability estimation
2. Test FairPO's robustness to different perturbation rates beyond the fixed 10%
3. Conduct ablation studies on the temperature hyperparameter τ to understand its impact on fairness-quality trade-offs