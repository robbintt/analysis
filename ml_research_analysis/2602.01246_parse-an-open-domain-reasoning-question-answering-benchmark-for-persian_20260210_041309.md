---
ver: rpa2
title: 'PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian'
arxiv_id: '2602.01246'
source_url: https://arxiv.org/abs/2602.01246
tags:
- persian
- reasoning
- question
- questions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARSE introduces the first open-domain Persian reasoning QA benchmark,
  featuring 10,800 questions across Boolean, multiple-choice, and factoid formats
  with diverse reasoning types and difficulty levels. The benchmark is generated via
  a controlled LLM-based pipeline and validated through human evaluation for linguistic
  and factual quality.
---

# PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian

## Quick Facts
- arXiv ID: 2602.01246
- Source URL: https://arxiv.org/abs/2602.01246
- Reference count: 40
- Key outcome: First open-domain Persian reasoning QA benchmark with 10,800 questions across 18 configurations

## Executive Summary
PARSE introduces the first open-domain Persian reasoning QA benchmark, featuring 10,800 questions across Boolean, multiple-choice, and factoid formats with diverse reasoning types and difficulty levels. The benchmark is generated via a controlled LLM-based pipeline and validated through human evaluation for linguistic and factual quality. Experiments show that Persian prompts and structured prompting (Chain-of-Thought for Boolean/multiple-choice, few-shot for factoid) significantly improve LLM performance. Fine-tuning on PARSE further boosts results, especially for Persian-specialized models. These findings demonstrate PARSE's effectiveness as both an evaluation benchmark and training resource for developing reasoning-capable LLMs in low-resource settings.

## Method Summary
The benchmark is generated using GPT-4o with 18 configuration-specific prompts (combining task type, reasoning type, and subtype) to create 600 questions per configuration. Questions are stratified into easy, medium, and hard difficulty levels. Human validators filter the generated questions for correctness, readability, and difficulty calibration. The dataset is split 80/20 into training (8,640 questions) and test sets (2,160 questions). Evaluation uses zero-shot, few-shot, and Chain-of-Thought prompting strategies with Persian and English prompts across multiple open-source models.

## Key Results
- Persian prompts consistently outperform English prompts across all task types
- Chain-of-Thought prompting is most effective for Boolean and multiple-choice questions
- Few-shot prompting is optimal for factoid questions
- Fine-tuning on PARSE improves performance, particularly for Persian-specialized models
- Models struggle most with unanswerable multiple-choice questions (accuracy <30%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Native-language prompting (Persian) yields higher reasoning performance than English prompting for Persian QA tasks.
- **Mechanism:** Aligning the prompt language with the question language reduces cross-lingual reasoning overhead and semantic drift during inference.
- **Core assumption:** The model has sufficient internal representation of Persian to follow instructions better in-language than via translation or English alignment.
- **Evidence anchors:** Experiments show Persian prompts significantly improve LLM performance; Persian prompts consistently outperform English prompts across Boolean, Multiple-choice, and Factoid tasks.

### Mechanism 2
- **Claim:** Structured prompting effectiveness varies by question type: Chain-of-Thought (CoT) aids reasoning-intensive tasks, while Few-shot aids extraction/formatting tasks.
- **Mechanism:** CoT decomposes multi-hop logic required for Boolean/MCQ tasks; Few-shot provides format anchors for Factoid tasks where reasoning depth is lower.
- **Core assumption:** Reasoning tasks require intermediate steps, whereas factoid tasks rely more on pattern matching or retrieval heuristics.
- **Evidence anchors:** For Boolean and Multiple-choice questions, chain-of-thought prompting provides the strongest performance; for Factoid questions, few-shot prompting performs best.

### Mechanism 3
- **Claim:** A hybrid pipeline of LLM generation followed by rigorous human validation creates high-quality benchmarks for low-resource languages.
- **Mechanism:** LLMs provide scale and syntactic diversity; human filters remove hallucinations, cultural errors, and enforce difficulty calibration.
- **Core assumption:** Human annotators can reliably distinguish genuine reasoning difficulty from linguistic complexity.
- **Evidence anchors:** Human evaluation scores > 4.0/5.0 for Correctness/Readability; human accuracy drops as difficulty rises, validating calibration.

## Foundational Learning

**Chain-of-Thought (CoT) Reasoning**
- **Why needed here:** Essential for Boolean and Multiple-choice portions, specifically for handling "negation," "comparative," and "multi-hop" subtypes where direct answers fail.
- **Quick check question:** Does prompting the model to "explain step-by-step" improve accuracy on a "Negation" Boolean question compared to a direct "Yes/No" request?

**Prompt Language Alignment**
- **Why needed here:** Critical for low-resource settings; using Persian prompts for Persian questions outperforms English prompts, likely due to reduced semantic drift.
- **Quick check question:** When evaluating a Persian QA model, are you translating the prompt to English or keeping it native? (If translating, you may be suppressing performance).

**Fine-tuning on Synthetic Benchmarks**
- **Why needed here:** Significant gains when fine-tuning models on the PARSE dataset prove that synthetic benchmarks can serve as effective training data, not just evaluation.
- **Quick check question:** Is the base model pre-trained on Persian, or does it require instruction tuning on the PARSE train split to handle "unanswerable" or "multi-answer" formats correctly?

## Architecture Onboarding

**Component map:** GPT-4o (Generator) -> Human Validators (Filter) -> JSON Schema (Validator) -> Model Harness (Evaluator)

**Critical path:** The Taxonomy Definition (Table 2) is the control lever. If the taxonomy is flawed, the prompt pipeline generates mislabeled data, invalidating the difficulty calibration.

**Design tradeoffs:**
- Scale vs. Quality: Relied on GPT-4o for scale but required 4 groups of human annotators to ensure quality.
- Format vs. Reasoning: The pipeline enforces answer format strictly, which may simplify evaluation but limits open-ended reasoning expressions.

**Failure signatures:**
- Unanswerable Detection: Models consistently score lowest on "Non-Ans" (unanswerable) subtypes (e.g., 0.05-0.33 accuracy in Table 5), indicating a strong bias toward generating answers rather than abstaining.
- Difficulty Inversion: If a model performs better on "Hard" than "Easy" questions, check for data leakage or prompt contamination.

**First 3 experiments:**
1. Baseline Establishment: Run vanilla LLaMA-3-8B and Dorna-8B on test split using Persian prompts with Zero-shot settings.
2. Prompt Strategy Ablation: Apply CoT to Boolean/MCQ and Few-shot to Factoid for Qwen-2.5-72B to verify optimal strategy findings.
3. Overfitting Check: Fine-tune a small model on training split and evaluate on hardest configuration (Multihop + Reasoning + Non-Ans) to test generalization limits.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does Retrieval-Augmented Generation (RAG) impact the performance of LLMs on the PARSE benchmark compared to the closed-setting evaluations presented?
- Basis in paper: The authors state in the conclusion: "As future directions, we aim to explore retrieval-augmented generation (RAG)."
- Why unresolved: The current study evaluates models in a closed setting (parametric knowledge only), leaving the interaction between external retrieval and complex Persian reasoning untested.
- What evidence would resolve it: A comparative evaluation of LLMs on PARSE with and without access to external retrieval corpora.

**Open Question 2**
- Question: Do specialized reasoning-oriented architectures (e.g., DeepSeek) outperform general-purpose multilingual models on Persian reasoning tasks?
- Basis in paper: The conclusion lists "more advanced reasoning-oriented models such as DeepSeek" as a specific avenue for future work.
- Why unresolved: The current experiments focus on general-purpose LLMs and one Persian-specialized model, but do not test models specifically optimized for reasoning.
- What evidence would resolve it: Benchmarking DeepSeek or similar reasoning-focused models on the PARSE dataset against the reported baselines.

**Open Question 3**
- Question: Why do models perform significantly worse on unanswerable multiple-choice questions compared to other subtypes, and can targeted fine-tuning close this gap?
- Basis in paper: Table 5 shows that model accuracy on "Non-Ans" multiple-choice questions is notably low (often <30%), whereas factoid "Non-Ans" performance in Table 6 is high.
- Why unresolved: The paper reports the low scores but does not analyze the specific failure modes causing models to select distractors instead of abstaining in multiple-choice settings.
- What evidence would resolve it: An error analysis of false positives in the multiple-choice "Non-Ans" category and subsequent experiments with calibration or specialized abstention training.

## Limitations

- The benchmark relies heavily on GPT-4o as both generator and implicit gold standard without external validation of answer correctness.
- Difficulty calibration depends on subjective human judgment rather than objective reasoning complexity metrics.
- The 4:1 train-test split ratio may introduce overfitting concerns for the fine-tuning experiments.

## Confidence

- **High Confidence:** The effectiveness of native-language Persian prompting over English prompting is well-supported by direct experimental comparisons.
- **Medium Confidence:** The taxonomy-based question generation and difficulty stratification show promise but require external validation beyond the human evaluation panel.
- **Low Confidence:** The generalizability of the benchmark to real-world Persian QA applications and its resistance to model-specific prompting artifacts remain uncertain.

## Next Checks

1. Cross-validate a random sample of PARSE questions against Persian Wikipedia and other authoritative Persian sources to assess factual accuracy beyond human readability judgments.
2. Test the benchmark with models not included in the original evaluation (e.g., specialized Persian models like PersianGPT) to verify that performance patterns hold across different architectures.
3. Conduct a second round of human evaluation with a different annotator pool to verify that difficulty stratification remains consistent across evaluator perspectives.