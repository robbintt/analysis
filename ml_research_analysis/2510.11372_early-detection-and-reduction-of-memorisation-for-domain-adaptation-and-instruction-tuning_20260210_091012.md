---
ver: rpa2
title: Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction
  Tuning
arxiv_id: '2510.11372'
source_url: https://arxiv.org/abs/2510.11372
tags:
- memorisation
- epoch
- fine-tuning
- instruction
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates memorisation during domain adaptation and
  instruction tuning of large language models (LLMs), focusing on both Pythia, Llama3,
  and Mistral model families across 1.4B-70B parameters. The key finding is that a
  simple n-gram partial memorisation score reliably predicts verbatim memorisation,
  with high-scoring n-grams often memorised in early epochs before validation perplexity
  or evaluation performance is optimised.
---

# Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning

## Quick Facts
- **arXiv ID**: 2510.11372
- **Source URL**: https://arxiv.org/abs/2510.11372
- **Reference count**: 17
- **Primary result**: N-gram partial memorization scores predict verbatim memorization; n-gram regularization reduces memorization by up to 40% with minimal performance loss

## Executive Summary
This study investigates memorisation during domain adaptation and instruction tuning of large language models (LLMs), focusing on both Pythia, Llama3, and Mistral model families across 1.4B-70B parameters. The key finding is that a simple n-gram partial memorisation score reliably predicts verbatim memorisation, with high-scoring n-grams often memorised in early epochs before validation perplexity or evaluation performance is optimised. Using this n-gram score as an early stopping criterion significantly reduces memorisation with minimal performance loss. Additionally, a novel n-gram-aware loss regularisation technique reduces memorisation by up to 40% across all model families tested, outperforming existing mitigation strategies while minimising evaluation performance trade-offs. These results provide practical, scalable methods for detecting and mitigating memorisation risks during LLM fine-tuning.

## Method Summary
The study fine-tunes multiple LLMs (Pythia 1.4B-12B, Llama2 7B, Llama3 8B/70B, Mistral 7B) on domain adaptation and instruction tuning tasks using datasets from P3, FLAN, and Alpaca-52K pools (SST-5, QQP, RTE, WANLI, SQuAD v2, HellaSwag, PubMedQA, XSum, CNN/DailyMail, Alpaca-52K, FLAN v2), with maximum 5,000 samples per dataset. Models are fine-tuned for up to 8 epochs with Adam optimizer. Memorization is evaluated using k-extractable metrics (k ∈ {12, 16, 20} prefixes, 20-token suffix) and n-gram partial memorization (4/5/6-grams). Two mitigation strategies are tested: early stopping based on n-gram thresholds and n-gram-aware loss regularisation that penalizes n-gram confidence exceeding pretrained model levels.

## Key Results
- N-gram partial memorization scores reliably predict which samples will become verbatim memorized in subsequent epochs
- Memorization increases dramatically in early fine-tuning epochs, often before validation perplexity or task performance is optimized
- N-gram-aware loss regularization reduces memorization by up to 40% across all model families with 35% smaller performance degradation than simple early stopping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** N-gram partial memorization scores predict which samples will become verbatim memorized, enabling early detection.
- **Mechanism:** The n-gram memorization score measures the proportion of matching n-grams (sizes 4-6) between model output and target sequence. Samples that will become verbatim memorized show elevated partial scores in preceding epochs—often 5-10 percentage points higher than non-memorized samples. This captures incremental pattern acquisition before full sequence reproduction.
- **Core assumption:** Partial n-gram matching precedes and indicates trajectory toward complete verbatim memorization rather than representing a separate phenomenon.
- **Evidence anchors:**
  - [abstract] "a simple n-gram partial memorisation score reliably predicts verbatim memorisation"
  - [section 4.1] Figure 2 shows "clear distinction in partial memorisation between the memorised and non-memorised samples, with the majority of epochs scoring markedly higher than the baseline"
  - [corpus] Limited corpus support; related work focuses on domain adaptation rather than memorization detection mechanisms

### Mechanism 2
- **Claim:** Memorization accumulates rapidly in early fine-tuning epochs before standard optimization criteria (validation perplexity, task accuracy) are satisfied.
- **Mechanism:** During fine-tuning, LLMs prioritize fitting training data patterns—including exact sequences—before achieving generalization. The model capacity for rapid memorization outpaces the development of robust task performance, creating a window where sensitive data exposure risk is high while usable performance remains low.
- **Core assumption:** The observed early memorization reflects inherent training dynamics rather than artifact of the specific datasets (max 5,000 samples) or epoch counts (8) tested.
- **Evidence anchors:**
  - [abstract] "memorisation increases dramatically in the first few epochs, often significantly before either validation perplexity or evaluation performance is optimised"
  - [section 1, Figure 1] Shows memorization increase occurring before dashed vertical lines marking best perplexity and accuracy epochs
  - [corpus] Corpus papers discuss domain adaptation challenges but do not directly address memorization timing

### Mechanism 3
- **Claim:** Regularizing n-gram confidence relative to the pretrained model reduces memorization with minimal performance degradation.
- **Mechanism:** The n-gram regularization loss adds a penalty term when the fine-tuned model's n-gram probability exceeds the pretrained model's probability by more than threshold τ. This discourages the model from assigning excessively high confidence to specific n-grams—proxy for memorization risk—while preserving learned task capabilities. The penalty is squared and only activates above the margin, creating selective pressure.
- **Core assumption:** Elevated n-gram confidence relative to the pretrained model indicates memorization rather than legitimate domain adaptation; requires maintaining pretrained model access during fine-tuning.
- **Evidence anchors:**
  - [abstract] "n-gram-aware loss regularisation technique reduces memorisation by up to 40% across all model families tested"
  - [section 4.3, Table 2] N-gram regularization achieves best trade-off: "≈40% relative reduction in memorisation and a ≈35% smaller performance hit compared with the simple Best n-gram early-stopping rule"
  - [corpus] No corpus papers address this specific regularization approach

## Foundational Learning

- **Concept: K-extractable memorization**
  - Why needed here: This is the paper's core memorization definition—whether a model reproduces exact suffix when prompted with k-token prefix using greedy decoding. Understanding this operationalizes what "memorized" means.
  - Quick check question: Given a training sample with prefix "The patient presented with" and suffix "acute respiratory distress syndrome," is the sample k-extractable if the model generates the exact suffix with k=12 but not k=16?

- **Concept: Perplexity vs. task evaluation as stopping criteria**
  - Why needed here: The paper reveals these metrics diverge in their memorization implications. Validation perplexity optimization correlates with higher memorization; task evaluation correlates with lower memorization but worse perplexity.
  - Quick check question: If your use case prioritizes privacy over perplexity optimization, which stopping criterion should you prefer?

- **Concept: Partial vs. full-parameter fine-tuning**
  - Why needed here: The paper tests both approaches. Freezing lower layers and updating only top-n transformer layers affects memorization dynamics—more frozen layers delay but may amplify late-epoch memorization.
  - Quick check question: Would partial fine-tuning (final 4 layers only) likely increase or decrease memorization risk compared to full fine-tuning by epoch 8?

## Architecture Onboarding

- **Component map:**
  - Pretrained model -> Fine-tuning loop (compute LLM loss + L_reg if using regularization) -> Epoch-end evaluation (memorization %, n-gram %, perplexity, task metrics) -> Early stopping check -> Deploy if thresholds met

- **Critical path:** Pretrained model → Fine-tuning loop (compute LLM loss + L_reg if using regularization) → Epoch-end evaluation (memorization %, n-gram %, perplexity, task metrics) → Early stopping check → Deploy if thresholds met

- **Design tradeoffs:**
  - **Early stopping threshold (n-gram=20):** Lower = more memorization reduction but worse performance; higher = better performance but more memorization risk
  - **Regularization strength λ and margin τ:** Higher λ = more memorization suppression but potential underfitting; τ controls how much confidence increase is tolerated
  - **N-gram sizes {4,5,6}:** Smaller = more granular signal but more computation; larger = coarser signal, may miss short-phrase memorization
  - **Prefix lengths k for extraction:** Shorter (12) = easier extraction, more samples testable; longer (20) = stricter test, fewer samples qualify

- **Failure signatures:**
  - **N-gram score elevated but no verbatim memorization:** May indicate false positive precursor; adjust threshold or n-gram sizes
  - **Verbatim memorization without prior n-gram signal:** Consider smaller n-gram sizes or different k values; mechanism may not generalize to your data
  - **Task performance collapses with regularization:** τ too low or λ too high; pretrained model baseline may be inappropriate for domain
  - **Memorization continues increasing despite early stopping:** Check if stopping criterion is being properly enforced; may need lower threshold

- **First 3 experiments:**
  1. **Baseline memorization dynamics:** Fine-tune your model on representative data for 8 epochs; log memorization %, n-gram %, perplexity, and task metrics at each epoch. Identify when memorization spike occurs relative to performance optimization.
  2. **Early stopping threshold calibration:** Test n-gram thresholds of 15, 20, 25 on validation split. Plot memorization reduction vs. performance degradation curve to select threshold for your privacy-performance requirements.
  3. **Regularization ablation:** Compare three conditions on same data: (a) no regularization, (b) n-gram regularization with λ=0.1, τ=0.05, (c) Goldfish loss baseline. Measure memorization reduction and task performance delta for each.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do alternative decoding strategies, such as beam search, influence memorization leakage patterns compared to the greedy decoding used in this study?
  - Basis in paper: [explicit] The authors state in Section 5 (Limitations) and Section 6 (Future Work): "Future work will extend this analysis... alternative decoding strategies such as beam search may surface different leakage patterns and should be audited with the same metrics."
  - Why unresolved: The study restricted its methodology to greedy decoding to establish a baseline, leaving the behavior of more complex, probability-maximizing decoding methods unexplored.
  - What evidence would resolve it: An evaluation of k-extractable memorization rates on the same fine-tuned models (Pythia, Llama3) when inference is performed using beam search or nucleus sampling.

- **Open Question 2:** Can the n-gram regularisation technique effectively mitigate memorisation in specialized domains such as code generation, mathematical reasoning, and multimodal tasks?
  - Basis in paper: [explicit] Section 6 (Conclusion and Future Work) explicitly lists this as a next step: "we will test whether the n-gram regulariser curbs memorisation in code generation, mathematical reasoning and multimodal tasks."
  - Why unresolved: The current experiments were confined to text-based NLP tasks (classification, QA, summarization), and it is unknown if the n-gram patterns in code or math exhibit the same memorisation dynamics.
  - What evidence would resolve it: Empirical results showing the percentage reduction in verbatim memorisation when applying the n-gram regularizer to models fine-tuned on coding benchmarks (e.g., HumanEval) or multimodal datasets.

- **Open Question 3:** Are the proposed mitigation strategies robust across a larger pool of model scales and different fine-tuning protocols, specifically parameter-efficient methods?
  - Basis in paper: [explicit] The authors note in Section 5 (Limitations): "Our experiments were limited to a single high-parameter model (Llama3 70B)... as well as different fine-tuning protocols."
  - Why unresolved: Computational budget constraints limited the validation of the 70B scale to a single model, and the study primarily utilized full-parameter fine-tuning rather than efficient adaptations like LoRA.
  - What evidence would resolve it: A study replicating the n-gram early stopping and regularisation experiments across multiple >70B parameter models and parameter-efficient fine-tuning (PEFT) configurations.

## Limitations

- Data scope limitations: The study uses datasets limited to 5,000 samples per domain, which may not represent real-world fine-tuning scenarios where datasets often contain millions of tokens.
- Architecture scope constraints: While the paper tests across Pythia, Llama3, and Mistral families, the results may not generalize to other architectures like Transformers with different attention mechanisms.
- Statistical validation gaps: The paper reports 10 runs for most experiments but shows 5-run results in key figures, creating inconsistency in reported variance.

## Confidence

**High Confidence Claims:**
- N-gram partial memorization scores predict verbatim memorization trajectory
- Memorization increases rapidly in early fine-tuning epochs before optimization criteria are met
- The n-gram regularization technique reduces memorization by 40% with 35% smaller performance hit than baseline early stopping

**Medium Confidence Claims:**
- Early stopping on n-gram scores is universally applicable across model families
- The 20-token suffix evaluation captures meaningful memorization
- Partial fine-tuning dynamics mirror full fine-tuning for memorization

**Low Confidence Claims:**
- N-gram regularization outperforms all existing techniques
- The regularization mechanism works identically for domain adaptation and instruction tuning
- The findings generalize to multilingual or code-focused fine-tuning

## Next Checks

**Validation Check 1:** Fine-tune a 7B model on a dataset scaled to 100K samples (10× current size) from the same domain. Compare memorization dynamics and n-gram predictive accuracy to the 5K baseline. This tests whether the early detection mechanism scales to realistic fine-tuning workloads and whether the n-gram signal remains discriminative at larger scales.

**Validation Check 2:** Apply the n-gram regularization technique to a BERT-based encoder-decoder model (e.g., T5) using the same datasets. Compare memorization reduction and performance trade-offs to the Transformer decoder-only results. This validates whether the regularization mechanism depends on autoregressive architecture or transfers to other LLM designs.

**Validation Check 3:** Systematically vary the early stopping threshold (n-gram=15, 20, 25, 30) and regularization strength λ (0.01, 0.1, 1.0) across three diverse domains (medical, questions, summarization). Generate memorization vs. performance trade-off curves for each configuration and identify domain-specific optimal operating points. This provides practical guidance for threshold selection beyond the single empirical value reported.