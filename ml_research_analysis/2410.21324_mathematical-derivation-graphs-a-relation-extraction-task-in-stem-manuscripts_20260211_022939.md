---
ver: rpa2
title: 'Mathematical Derivation Graphs: A Relation Extraction Task in STEM Manuscripts'
arxiv_id: '2410.21324'
source_url: https://arxiv.org/abs/2410.21324
tags:
- derivation
- equations
- wang
- article
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the novel task of extracting mathematical derivation
  graphs from STEM manuscripts, representing inter-equation dependencies as directed
  acyclic graphs. The authors construct the Mathematical Derivation Graphs Dataset
  (MDGD) with 107 arXiv articles and over 2000 manually labeled derivation edges.
---

# Mathematical Derivation Graphs: A Relation Extraction Task in STEM Manuscripts

## Quick Facts
- arXiv ID: 2410.21324
- Source URL: https://arxiv.org/abs/2410.21324
- Authors: Vishesh Prasad; Brian Kim; Nickvash Kani
- Reference count: 40
- Key outcome: Extracting mathematical derivation graphs from STEM manuscripts as a relation extraction task

## Executive Summary
This paper introduces the novel task of extracting mathematical derivation graphs from scientific manuscripts, representing inter-equation dependencies as directed acyclic graphs. The authors construct the Mathematical Derivation Graphs Dataset (MDGD) with 107 arXiv articles and over 2000 manually labeled derivation edges. They evaluate analytic, machine learning, and LLM approaches, finding best F1 scores of ~45%-52% with baseline methods and GPT-5. Proposed fixes like combining brute-force pattern matching with LLMs, edge-limiting, and refined prompting yield modest improvements, but performance remains limited. The work highlights the difficulty of extracting mathematical relationships compared to standard NLP tasks and establishes a benchmark for future research in mathematical relation extraction.

## Method Summary
The authors approach mathematical derivation graph extraction as a relation extraction task between equations in STEM manuscripts. They construct the MDGD dataset through manual annotation of 107 arXiv articles, labeling derivation edges between equations. Multiple approaches are evaluated: analytic methods (pattern matching, NLP-based algorithms), machine learning models (neural networks, transformer architectures), and LLM-based approaches (GPT-4o and GPT-5). The models predict directed edges between equations, with performance measured using precision, recall, and F1 scores. Proposed improvements include combining brute-force pattern matching with LLMs, limiting the number of predicted edges, and refining prompting strategies.

## Key Results
- MDGD dataset constructed with 107 arXiv articles and over 2000 manually labeled derivation edges
- Best F1 scores of ~45%-52% achieved with GPT-4o and GPT-5 approaches
- Analytic and machine learning baselines perform significantly worse than LLM-based methods
- Proposed fixes (edge-limiting, refined prompting) yield only modest improvements in performance
- Mathematical relation extraction proves significantly more challenging than standard NLP tasks

## Why This Works (Mechanism)
Mathematical derivation graph extraction works by identifying explicit and implicit relationships between equations in scientific manuscripts. The task requires understanding mathematical notation, recognizing when one equation depends on another, and capturing the logical flow of mathematical arguments. The success of LLM-based approaches stems from their ability to process contextual information across multiple equations and recognize patterns in mathematical reasoning that traditional pattern matching or machine learning models miss. The directed acyclic graph structure naturally represents the dependency relationships between equations in mathematical derivations.

## Foundational Learning
- **Directed Acyclic Graphs (DAGs)**: Mathematical derivations form dependency networks where equations reference previous results - understanding DAG properties is crucial for validating extracted graphs
- **Relation Extraction**: Core NLP task of identifying relationships between entities - mathematical relation extraction extends this to formal mathematical expressions
- **Mathematical Notation Parsing**: Converting LaTeX and mathematical expressions into structured representations - essential for comparing and relating equations
- **STEM Manuscript Structure**: Understanding how mathematical content is organized in academic papers - equations, sections, and logical flow
- **LLM Prompt Engineering**: Crafting effective prompts for mathematical reasoning tasks - critical for extracting complex relationships from text

## Architecture Onboarding

**Component Map**: Text preprocessing -> Equation extraction -> Relation prediction -> Graph validation

**Critical Path**: The most important sequence is equation extraction → relation prediction → graph validation, as errors compound through this pipeline. Equation extraction accuracy directly impacts the quality of relation predictions.

**Design Tradeoffs**: The authors chose to focus on explicit and obvious derivation relationships rather than attempting to capture all possible mathematical connections, trading completeness for higher precision. They also prioritized LLM-based approaches over complex custom architectures, leveraging existing capabilities rather than building specialized mathematical reasoning systems.

**Failure Signatures**: Models frequently miss implicit derivations, confuse similar-looking equations, and struggle with cross-referencing notation. Performance degrades significantly when equations span multiple pages or when mathematical arguments are presented non-linearly.

**First Experiments**:
1. Evaluate equation extraction accuracy independently to establish baseline performance
2. Test relation prediction on simplified mathematical expressions before scaling to full manuscripts
3. Compare LLM performance across different STEM domains to identify domain-specific challenges

## Open Questions the Paper Calls Out
None

## Limitations
- MDGD dataset limited to 107 articles primarily from physics and mathematics domains, limiting generalizability to other STEM fields
- Manual labeling process for derivation edges is subjective with unreported inter-annotator agreement
- Best-performing models achieve only 45-52% F1 scores, indicating the task remains fundamentally challenging
- Proposed fixes yield only modest improvements, suggesting current methods may be approaching their performance ceiling

## Confidence

**High Confidence**: The dataset construction methodology and the fundamental observation that mathematical relation extraction is more difficult than standard NLP tasks

**Medium Confidence**: The reported performance metrics of baseline models, given the novel nature of the task and limited comparative studies

**Low Confidence**: The generalizability of results to other STEM domains and the long-term effectiveness of proposed fixes

## Next Checks
1. **Cross-domain validation**: Test the trained models on mathematical content from chemistry, engineering, and computational biology to assess domain transferability
2. **Error analysis**: Conduct detailed qualitative analysis of false positives and false negatives to identify systematic failure modes and potential architectural improvements
3. **Human evaluation study**: Compare model predictions against expert mathematicians' assessments to establish ground truth reliability and identify cases where models capture non-obvious derivations