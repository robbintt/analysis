---
ver: rpa2
title: 'GazeLLM: Multimodal LLMs incorporating Human Visual Attention'
arxiv_id: '2504.00221'
source_url: https://arxiv.org/abs/2504.00221
tags:
- video
- gaze
- human
- full
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes GazeLLM, a method that integrates human visual\
  \ attention with multimodal large language models (MLLMs) to efficiently process\
  \ first-person video. The approach leverages eye-tracking data to crop high-resolution\
  \ video around the user\u2019s gaze point, reducing pixel input to one-tenth while\
  \ maintaining task comprehension."
---

# GazeLLM: Multimodal LLMs incorporating Human Visual Attention

## Quick Facts
- arXiv ID: 2504.00221
- Source URL: https://arxiv.org/abs/2504.00221
- Reference count: 32
- Primary result: Gaze-cropped video reduces pixel input by 90% while maintaining or improving task description quality versus full-frame input

## Executive Summary
GazeLLM integrates human visual attention with multimodal large language models (MLLMs) to efficiently process first-person video. By leveraging eye-tracking data to crop high-resolution video around the user's gaze point, the approach reduces pixel input to one-tenth while maintaining task comprehension. Evaluations using BLEU, ROUGE, SBERT, and LLM-based metrics show that gaze-cropped videos outperform center-cropped videos in generating accurate task descriptions. Human evaluations also confirm that descriptions from gaze-focused videos are rated higher than those from full or center-cropped videos.

## Method Summary
GazeLLM processes first-person video by extracting 448×448 pixel regions centered on gaze coordinates, reducing input pixels by 90% compared to full 1440×1440 frames. The method uses synchronized gaze data from Ego-Exo4D dataset (137 videos across 6 task categories) and processes videos at 1 fps. Gemini 1.5 Pro generates task descriptions from the cropped frames, with results compared against full-frame and center-cropped baselines using automated metrics and human evaluation.

## Key Results
- Gaze-cropped videos (448×448) received equal or superior ratings to full-frame videos in human evaluations across 6 task categories
- Automated metrics (BLEU, ROUGE, SBERT, LLM-based scoring) showed gaze-cropped videos significantly outperforming center-cropped videos (p < 0.05)
- The approach maintains comprehension while reducing pixel input to one-tenth, achieving substantial computational savings

## Why This Works (Mechanism)

### Mechanism 1: Gaze-Driven Selective Attention for Input Reduction
Cropping first-person video around gaze points reduces pixel input by 90% while maintaining or improving task description quality. Humans naturally focus visual attention on task-relevant regions, so extracting only the 448×448 region centered on the gaze point concentrates task-relevant information while filtering peripheral noise. This mimics human foveated vision—high resolution at fixation, low resolution in periphery.

### Mechanism 2: Token Reduction Through Spatial Cropping
Cropping reduces the number of visual tokens fed to the Vision Transformer, lowering memory and compute demands. ViT-based MLLMs divide images into patch grids, each becoming a token. Fewer pixels = fewer patches = fewer tokens. Transformer attention scales quadratically with sequence length, so ~10% pixels yields disproportionate memory savings.

### Mechanism 3: Improved Signal-to-Noise Ratio in Task Descriptions
Gaze-focused cropping yields higher-quality task descriptions than full-frame or center-cropped video, as measured by both human and automated metrics. Full-frame video includes peripheral objects unrelated to the task, which may distract the model or dilute task-relevant content. Center-cropping is a naive baseline that often misses the action.

## Foundational Learning

- **Vision Transformer (ViT) Patch Tokenization**: Understanding how MLLMs convert video frames into tokens clarifies why reducing pixels reduces compute. *Quick check: If you halve the image dimensions, how many fewer tokens does a standard ViT produce?*

- **Quadratic Attention Complexity**: Explains why small input reductions yield large memory savings in transformers. *Quick check: If input length doubles, how much does self-attention memory increase?*

- **Foveated Vision & Visual Attention**: GazeLLM is biologically inspired; understanding human vision clarifies why gaze is a strong attention signal. *Quick check: What is the primary difference between foveal and peripheral vision in humans?*

## Architecture Onboarding

- **Component map**: First-person camera -> Gaze-cropping module -> Video encoder (ViT-based) -> MLLM backbone -> Output generator

- **Critical path**: 1) Sync gaze timestamps with video frames, 2) Crop each frame around (gaze_x, gaze_y) to fixed size, 3) Downsample frame rate (paper used 1 fps), 4) Pass cropped frames to MLLM with task prompt, 5) Evaluate output vs. ground truth or human ratings

- **Design tradeoffs**: Crop size (smaller = more compression but higher risk of missing context), Frame rate (lower = fewer tokens but may miss fast actions), Gaze smoothing (raw gaze is noisy; smoothing may help but introduces lag), Center vs. Gaze vs. Full (paper shows Gaze > Center ≈ Full)

- **Failure signatures**: Gaze cropping produces descriptions worse than center crop (likely gaze calibration error or task not gaze-aligned), Model still OOM (crop size or video length still too large; reduce further), Descriptions miss obvious steps (gaze may have shifted away at critical moments)

- **First 3 experiments**: 1) Replicate the paper's comparison (Full vs. Gaze vs. Center) on a subset of Ego-Exo4D; verify BLEU/ROUGE/SBERT trends, 2) Vary crop size (e.g., 320×320, 512×512) to find the minimal viable window for your task domain, 3) Test on your own first-person video with your eye-tracker; compare description quality and latency vs. full-frame baseline

## Open Questions the Paper Calls Out

1. **Does providing a downsampled full-view image alongside the gaze-focused crop improve MLLM context awareness?** The paper discusses an alternative encoding scheme that mimics human central and peripheral vision but doesn't test this hybrid approach.

2. **Does integrating hand-position data with gaze tracking significantly enhance the model's understanding of manual tasks?** The paper suggests hand position information could be prioritized but doesn't test the synergistic effect of combining hand joint data with visual attention.

3. **Can a variable-density Vision Transformer (ViT) grid, dense at gaze points and sparse in peripheries, outperform standard rectangular cropping?** The paper proposes a deformed grid for ViT patches but uses standard rectangular cropping in the implemented method.

4. **Does the reduction in input token count translate to reduced latency for real-time "stop-and-ask" applications?** While the paper claims the method offers an "efficient solution" and demonstrates a "stop-and-ask" application, it only evaluates description quality, not actual processing speed or latency.

## Limitations

- Generalizability concerns across diverse real-world scenarios where gaze doesn't reliably track task-relevant objects
- Reliance on perfect gaze tracking calibration and reliability, with no analysis of failure modes from tracking errors
- Incomplete computational savings analysis that doesn't account for gaze processing overhead and system latency

## Confidence

- **High Confidence (95%+):** Core mechanism of gaze-driven cropping reducing pixel input by ~90% and the quadratic scaling of transformer attention complexity
- **Medium Confidence (75-85%):** Claim that gaze-cropped videos produce superior task descriptions, specific to Ego-Exo4D dataset and six task categories
- **Low Confidence (60-70%):** Assertion that the approach offers a "promising framework for real-world assistance systems" extending beyond empirical evidence

## Next Checks

1. **Cross-Domain Robustness Test**: Apply GazeLLM to first-person video from domains outside Ego-Exo4D (e.g., cooking shows, DIY tutorials, sports broadcasts) to assess whether gaze-based cropping maintains its performance advantage when gaze patterns and task structures differ significantly.

2. **Gaze Tracking Error Simulation**: Systematically degrade gaze tracking quality through synthetic noise, latency, and calibration errors to determine the failure threshold where gaze-cropping becomes detrimental to task description quality, establishing operational boundaries for real-world deployment.

3. **Real-Time Processing Benchmark**: Implement the complete GazeLLM pipeline with actual gaze hardware (e.g., Pupil Labs glasses) and measure end-to-end latency from video capture through description generation, comparing this against both full-frame and center-cropped baselines to quantify practical computational benefits.