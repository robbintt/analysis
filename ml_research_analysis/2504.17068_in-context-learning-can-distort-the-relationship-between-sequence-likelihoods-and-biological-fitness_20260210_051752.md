---
ver: rpa2
title: In-Context Learning can distort the relationship between sequence likelihoods
  and biological fitness
arxiv_id: '2504.17068'
source_url: https://arxiv.org/abs/2504.17068
tags:
- sequences
- sequence
- figure
- language
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how in-context learning can distort the
  relationship between sequence likelihood scores and biological fitness. The authors
  show that transformer-based masked language models (like ESM2) exhibit anomalously
  high likelihood scores for sequences containing repeated motifs, due to a retrieval
  mechanism that uses repeated patterns as references.
---

# In-Context Learning can distort the relationship between sequence likelihoods and biological fitness

## Quick Facts
- arXiv ID: 2504.17068
- Source URL: https://arxiv.org/abs/2504.17068
- Authors: Pranav Kantroo; Günter P. Wagner; Benjamin B. Machta
- Reference count: 40
- Key outcome: Transformer models exhibit anomalously high likelihood scores for sequences containing repeated motifs due to in-context retrieval mechanisms, distorting the relationship between sequence likelihood and biological fitness.

## Executive Summary
This paper investigates how in-context learning (ICL) capabilities in transformer-based masked language models can distort the relationship between sequence likelihood scores and biological fitness. The authors demonstrate that transformer models (ESM2) exhibit anomalously high likelihood scores for sequences containing repeated motifs due to a retrieval mechanism that uses repeated patterns as references. This effect is most pronounced in transformer models, with convolutional models showing a transition behavior based on repeat size. The study reveals that larger models show stronger distortions and that ICL can even detect biologically meaningful patterns like hairpin structures in RNA. Importantly, repeated sequences degrade the quality of learned embeddings, affecting downstream applications.

## Method Summary
The study uses Swissprot protein sequences parsed into domains via RPS-BLAST (20-1000 residues), clustered with mmseqs2 to yield 76,361 domains. Experiments focus on 1000 domains <400 residues, excluding sequences with ESM2 OFS pseudo-perplexity <5 and first positions. The core method involves computing pseudo-perplexity (one-at-a-time and OFS variants) for natural and doubled sequences across multiple architectures (ESM2 8M/650M, CARP 640M, LC-PLM 1.4B, RiNALMo 33.5M/650M, Progen2-M). The OFS regression task with MLP ensemble (5 runs, 80-20 splits) assesses embedding quality degradation. Key experiments include doubling tests, double-masking retrieval verification, and needle-in-haystack tests.

## Key Results
- Transformer models (ESM2) exhibit uncertainty collapse with pseudo-perplexity ≈ 1.0 when presented with doubled sequences containing repeated motifs
- Convolutional models (CARP) show transition behavior based on motif size, with uncertainty collapse only for motifs within their receptive field (~70 residues)
- Larger models (650M vs 33.5M RiNALMo) exhibit stronger distortions and recognize more complex patterns like RNA hairpins
- Repeated sequences degrade embedding quality, increasing validation loss on downstream regression tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a masked language model (MLM) encounters a repeated motif within its context window, it may assign an anomalously high likelihood (low pseudo-perplexity) by performing an in-context retrieval rather than relying on learned biochemical priors.
- **Mechanism:** The model utilizes a "look-up operation" where the attention mechanism identifies a repeated copy of the current motif. It uses the unmasked residue in the copy as a direct reference to resolve the uncertainty of the masked position, effectively collapsing the prediction entropy to near zero.
- **Core assumption:** The model's attention layers can identify and align equivalent positions across distant subsequences, allowing the retrieval signal to override statistical priors learned during pre-training.
- **Evidence anchors:** [abstract] "This behavior is mediated by a look-up operation where the model seeks the identity of the masked position by using the other copy of the repeated motif as a reference"; [section 2.2] "Adding a second copy of the domain to this sequence induces a dramatic reduction in the uncertainty of the model... The model is confident that W is the only sensible option."
- **Break condition:** The mechanism likely breaks if the context window is exceeded, if the repeated motif is shorter than the model's minimum recognition length (e.g., <10 residues for ESM2), or if the architecture lacks global attention (e.g., specific CNN or SSM limitations).

### Mechanism 2
- **Claim:** The strength of in-context retrieval is architecture-dependent, with Transformers exhibiting a distinct "uncertainty collapse" while Convolutional models show a transition behavior based on motif size.
- **Mechanism:** Transformers (e.g., ESM2) use global self-attention allowing immediate comparison between any two positions. Convolutional models (e.g., CARP) rely on stacking layers to expand the receptive field; they collapse uncertainty only for short motifs (within receptive field) and show progressive decline for longer ones.
- **Core assumption:** The "operational memory" or receptive field of the model dictates the threshold at which repeated patterns can be recognized as a single reference unit.
- **Evidence anchors:** [abstract] "This effect is most pronounced in transformer models, with convolutional models showing a transition behavior based on repeat size"; [section 2.1] "CARP (640M) exhibits uncertainty collapse for protein sequences shorter than approximately 70 residues... transformer-based masked language models (ESM2) exhibit an uncertainty collapse when presented with doubled sequences."
- **Break condition:** This comparison breaks if the convolutional model uses dilated convolutions large enough to mimic global attention, or if the Transformer is replaced by a recurrence-based architecture (like BiMamba) which shows only progressive decline.

### Mechanism 3
- **Claim:** Models may recognize complex, biologically meaningful composite patterns (like RNA hairpins/reversed complements) but fail to decompose them into constituent operations (reversal or complementation alone).
- **Mechanism:** In-context learning capabilities are data-dependent. The model learns to recognize "reversed complement" motifs because they are enriched in biological data (hairpins), but it does not learn the independent geometric transformations of "reversal" or "complementation" as they lack biological prevalence.
- **Core assumption:** The model learns statistical associations of composite structures as single features rather than learning symbolic rules for geometric transformations.
- **Evidence anchors:** [section 2.3] "Reversed complements are biologically meaningful sequences... Sequences that have only been reversed or complemented however are bound to be far less prevalent... this observation points towards the need for a systematic inquiry into their ability to recover simpler sub-skills."
- **Break condition:** The mechanism breaks if the training data is augmented with synthetic examples of pure reversals or pure complements, forcing the model to learn the independent transformations.

## Foundational Learning

- **Concept:** **In-Context Learning (ICL) vs. Weight Learning**
  - **Why needed here:** The paper argues that the distortion is not due to memorization (weight updates) but an emergent inference-time capability where the model "learns" from the prompt (the repeated context).
  - **Quick check question:** Can the model change its prediction based on a pattern in the input sequence without updating its weights?

- **Concept:** **Pseudo-perplexity and Entropy**
  - **Why needed here:** The paper quantifies "distortion" and "uncertainty collapse" using pseudo-perplexity. Understanding that a score of 1.0 implies absolute certainty (zero entropy) is crucial for interpreting the results.
  - **Quick check question:** Does a low pseudo-perplexity score always indicate high biological fitness, or can it indicate high pattern certainty?

- **Concept:** **Induction Heads**
  - **Why needed here:** The paper draws a parallel to "induction heads" in NLP—circuits that copy