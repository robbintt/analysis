---
ver: rpa2
title: Active Task Disambiguation with LLMs
arxiv_id: '2502.04485'
source_url: https://arxiv.org/abs/2502.04485
tags:
- questions
- solutions
- question
- task
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework for task ambiguity and
  proposes an active disambiguation method using Bayesian Experimental Design. The
  key idea is to select clarifying questions that maximize expected information gain
  by sampling from the space of possible solutions rather than relying on implicit
  reasoning.
---

# Active Task Disambiguation with LLMs

## Quick Facts
- arXiv ID: 2502.04485
- Source URL: https://arxiv.org/abs/2502.04485
- Reference count: 40
- Key outcome: EIG-uniform strategy outperforms baselines in task disambiguation by explicitly sampling solution space for information gain

## Executive Summary
This paper introduces a formal framework for task ambiguity and proposes an active disambiguation method using Bayesian Experimental Design. The key idea is to select clarifying questions that maximize expected information gain by sampling from the space of possible solutions rather than relying on implicit reasoning. Experiments on a 20 questions game and code generation tasks show that the proposed EIG-uniform strategy significantly outperforms baseline zero-shot and implicit reasoning approaches, especially with less capable models. The results demonstrate that explicit reasoning about solution space leads to more effective task disambiguation and better alignment with user intent.

## Method Summary
The authors formalize task ambiguity as uncertainty in the solution space and propose an active disambiguation framework based on Bayesian Experimental Design. The core approach, EIG-uniform, computes the expected information gain (EIG) of potential clarifying questions by sampling solutions from the current belief distribution and selecting questions that maximize information gain. Unlike implicit reasoning approaches that rely on the model to implicitly disambiguate tasks through prompt engineering, EIG-uniform explicitly samples the solution space to identify clarifying questions. The method is evaluated on a 20 questions game and code generation tasks, demonstrating superior performance in reducing ambiguity and improving task alignment.

## Key Results
- EIG-uniform significantly outperforms zero-shot and implicit reasoning baselines in both 20 questions game and code generation tasks
- The performance advantage is particularly pronounced with less capable models
- Explicit sampling of solution space leads to more effective task disambiguation than relying on implicit model reasoning
- EIG-uniform achieves better alignment with user intent compared to baseline approaches

## Why This Works (Mechanism)
The method works by explicitly modeling the uncertainty in task interpretation and actively selecting questions that maximize expected information gain about the true solution. By sampling from the space of possible solutions, EIG-uniform identifies clarifying questions that are most informative about reducing ambiguity, rather than relying on the model's implicit reasoning capabilities. This approach treats task disambiguation as an active learning problem where the goal is to minimize uncertainty about the user's intent through strategically chosen clarifying questions.

## Foundational Learning
- **Bayesian Experimental Design**: A framework for selecting experiments (or questions) that maximize expected information gain; needed to formalize the active question selection process
- **Expected Information Gain (EIG)**: The expected reduction in uncertainty from a clarifying question; needed to quantify the value of different questions
- **Solution Space Sampling**: The process of drawing samples from the distribution of possible task solutions; needed to estimate information gain without exhaustive enumeration
- **Task Ambiguity Formalization**: Representing task ambiguity as uncertainty in the solution space; needed to create a measurable framework for disambiguation
- **Active Learning**: The paradigm of selecting informative examples or questions to improve model understanding; needed as the conceptual foundation for active disambiguation

## Architecture Onboarding

**Component Map:**
User Intent -> Task Ambiguity -> Solution Space Distribution -> EIG-uniform Sampler -> Clarifying Question Selection -> Reduced Ambiguity -> Aligned Task Execution

**Critical Path:**
1. User provides ambiguous task specification
2. Model estimates solution space distribution based on task description
3. EIG-uniform samples solutions and computes expected information gain for candidate questions
4. Most informative question is selected and posed to user
5. User response updates solution space distribution
6. Process repeats until ambiguity is sufficiently reduced

**Design Tradeoffs:**
- Computational cost of EIG computation vs. disambiguation quality
- Granularity of solution space sampling vs. scalability
- Number of clarifying questions vs. user experience
- Explicit vs. implicit disambiguation approaches

**Failure Signatures:**
- High computational cost preventing real-time interaction
- Ineffective question selection due to poor solution space modeling
- User fatigue from excessive clarifying questions
- Model inability to accurately estimate solution space distribution

**First 3 Experiments:**
1. Compare EIG-uniform with varying numbers of solution space samples (10, 50, 100) to assess computational tradeoffs
2. Evaluate performance on tasks with different levels of inherent ambiguity (low, medium, high)
3. Test across a broader range of model sizes (small, medium, large) to verify scalability claims

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes discrete solution space that can be enumerated and sampled, which may not hold for many real-world tasks
- Computational complexity of expected information gain calculation could be prohibitive for high-dimensional or large solution spaces
- Experiments focus on controlled settings, limiting generalizability to more complex real-world scenarios
- Performance differences may be partially attributable to prompt engineering differences rather than fundamental methodological advantages

## Confidence
- Theoretical framework soundness: High
- Experimental results validity: Medium
- Scalability to real-world applications: Low

## Next Checks
1. Test EIG-uniform on tasks with continuous or high-dimensional solution spaces to evaluate computational scalability and performance degradation
2. Conduct ablation studies isolating the effects of prompt engineering from the core methodological contribution by comparing EIG-uniform with differently prompted versions of implicit reasoning baselines
3. Evaluate the approach across a broader range of model capabilities, including both smaller and larger models than those tested, to verify the claimed performance advantages across the full spectrum of LLM capabilities