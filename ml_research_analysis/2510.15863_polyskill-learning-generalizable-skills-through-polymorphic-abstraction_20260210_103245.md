---
ver: rpa2
title: 'PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction'
arxiv_id: '2510.15863'
source_url: https://arxiv.org/abs/2510.15863
tags:
- skill
- skills
- agent
- agents
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building web agents that
  can learn and reuse generalizable skills across diverse websites. Current skill
  induction methods often create over-specialized skills that fail to transfer to
  unseen domains.
---

# PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction

## Quick Facts
- arXiv ID: 2510.15863
- Source URL: https://arxiv.org/abs/2510.15863
- Reference count: 40
- Primary result: Improves web agent task success by up to 9.4% on unseen websites through polymorphic skill abstraction

## Executive Summary
This paper addresses the challenge of building web agents that can learn and reuse generalizable skills across diverse websites. Current skill induction methods often create over-specialized skills that fail to transfer to unseen domains. The proposed PolySkill framework applies polymorphic abstraction—inspired by software engineering—to separate a skill's abstract goal from its concrete implementation. This hierarchical structure allows agents to learn reusable, compositional skills that adapt across different websites. Experiments on Mind2Web and WebArena benchmarks show PolySkill improves task success rates by up to 9.4% on unseen websites, reduces steps by over 20%, and increases skill reusability by 1.7x compared to prior methods. Notably, it also prevents catastrophic forgetting during continual learning and enables autonomous skill discovery without predefined tasks, demonstrating superior generalization and efficiency.

## Method Summary
PolySkill applies object-oriented polymorphism to web skill learning, organizing skills into abstract base classes that define interfaces and concrete subclasses that implement site-specific details. The system first defines or induces an abstract domain class (e.g., `AbstractShoppingSite`) with method interfaces, then executes tasks on specific sites to generate concrete implementations. Upon successful task completion, an LLM generates a concrete subclass implementing the abstract methods using site-specific DOM elements. These skills are verified by re-execution before being added to the skill library. When encountering new websites, the agent retrieves the relevant abstract class to guide skill creation, enabling transfer learning by fitting new observations into existing abstract schemas rather than generating skills from scratch.

## Key Results
- Improves task success rates by up to 9.4% on unseen websites compared to baselines
- Increases skill reusability by 1.7x, enabling better transfer across domains
- Reduces average steps by over 20% through more efficient execution
- Prevents catastrophic forgetting during continual learning
- Enables autonomous skill discovery without predefined tasks

## Why This Works (Mechanism)

### Mechanism 1: Polymorphic Abstraction for Cross-Domain Transfer
Decoupling a skill's semantic intent (abstract goal) from its execution (concrete implementation) allows agents to transfer capabilities to unseen websites by fitting new observations into existing abstract "schemas." The system organizes skills into class hierarchies (e.g., `AbstractShoppingSite`). When encountering a new domain (e.g., Target), the agent retrieves the abstract class which defines what to implement (e.g., `search_product`), guiding the LLM to fill in only the site-specific implementation details rather than generating a skill from scratch. Core assumption: Websites within a domain share structural patterns (isomorphism) that can be captured by a single abstract interface.

### Mechanism 2: Compositional Stability via Interface Inheritance
Defining complex workflows at the abstract level prevents the need to re-learn high-level logic for every new website, reducing the search space and preventing catastrophic forgetting. Complex skills (e.g., `purchase_item`) are defined once in the abstract parent class as a composition of primitive abstract methods (e.g., `search`, `add_to_cart`). Subclasses for specific sites only override the primitives. The high-level compositional logic remains stable and is never re-written, preserving the skill's utility across domains. Core assumption: High-level task logic is domain-invariant, while only low-level interactions vary.

### Mechanism 3: Schema-Guided Exploration
In task-free settings, pre-defined abstract classes act as a curriculum, enabling more efficient autonomous skill discovery than unstructured exploration. Instead of randomly exploring, the agent uses the abstract class (the "schema") to identify missing capabilities. It proposes goals to "fill in" the missing concrete implementations of the abstract methods it possesses, effectively treating skill induction as satisfying an interface requirement. Core assumption: The agent has access to or can generate a relevant abstract class before exploration begins.

## Foundational Learning

**Concept: Object-Oriented Polymorphism (OOP)**
Why needed: The framework relies entirely on the software engineering concept of abstract base classes defining an interface and subclasses providing specific implementations. Understanding inheritance is required to read the skill code.
Quick check: Can you explain why `AbstractShoppingSite` cannot be executed directly, but `AmazonSite` can?

**Concept: Skill Induction (Distillation)**
Why needed: The system must convert successful trajectories (history of actions) into reusable code functions. You must understand how an LLM distills a trace into a Python function.
Quick check: If a trajectory has 50 steps, how does the induction process decide which steps belong to the new skill?

**Concept: Partially Observable Markov Decision Process (POMDP)**
Why needed: The paper formally models the web agent environment as a POMDP. Understanding that the agent only sees an observation $o_t$ (the DOM tree) and not the full state $S$ is critical for understanding why skills must be robust to hidden state.
Quick check: Why does the policy $\pi_L$ depend on the observation $o_t$ and not the true state $S$?

## Architecture Onboarding

**Component map:**
1. Agent/Policy ($\pi_L$): The LLM executing actions
2. Skill Library ($K_t$): The codebase containing Abstract Classes and Concrete Implementations
3. Inducer: An LLM prompt that takes a trajectory + Abstract Class and outputs Concrete Class code
4. Verifier: An LLM judge (GPT-4.1) that runs the new skill to confirm it reproduces the success before adding to $K_t$

**Critical path:**
1. Initialization: Define or induce the `AbstractClass` for a domain (e.g., Shopping)
2. Execution: Agent attempts task on Site A (e.g., Amazon). Trajectory recorded
3. Induction: Inducer prompt receives trajectory + `AbstractClass`. It outputs `class AmazonSite(AbstractShoppingSite)`
4. Verification: The agent attempts the same task using the new skill. If successful, `AmazonSite` is saved
5. Transfer: On Site B (e.g., Target), the agent retrieves `AbstractClass`, guiding it to create `TargetSite`

**Design tradeoffs:**
- Structure vs. Flexibility: Strong abstract interfaces improve transfer but fail on hybrid/long-tail sites that don't fit the schema
- Verification Cost: Every induced skill requires a re-run of the task for verification, increasing compute cost

**Failure signatures:**
- Over-specialization (Baselines): Generated skills contain hardcoded element IDs or URLs specific to one site (ASI/SkillWeaver issue)
- Bad Abstraction: The abstract class is too specific (e.g., `AbstractAmazonSite`) or too vague, making implementation impossible
- Code Execution Error: The induced concrete skill uses selectors that are dynamic and have changed since induction

**First 3 experiments:**
1. Sanity Check (In-Domain): Train on Amazon, test on Amazon tasks. Verify that `PolySkill` matches or exceeds baseline ASI performance
2. Generalization Test (Cross-Domain): Train skill library on Amazon, then immediately test on Target without further training. Measure Skill Reusability (Paper reports 31% vs 18% for baselines)
3. Ablation on Abstraction: Run the induction pipeline without providing the Abstract Class context. Check if the resulting skills become "spaghetti code" that fails to transfer

## Open Questions the Paper Calls Out

### Open Question 1
How can agents autonomously repair skill implementations in highly dynamic web environments without requiring costly full re-induction cycles? The current PolySkill framework requires re-validation and re-induction when layouts change, which limits practical applicability in fast-moving environments. A mechanism that successfully identifies minimal modifications needed to restore functionality (e.g., updating a single element locator) without regenerating the entire skill code would resolve this.

### Open Question 2
Can reinforcement learning (RL) be effectively used to train smaller, open-source models to autonomously induce polymorphic skills? Rather than prompting proprietary models, future work should investigate training models via RL, citing challenges such as designing rewards for abstraction over specialization. An RL agent that demonstrates the ability to learn abstract interfaces and generalized skills through environmental interaction rewards rather than prompt-based imitation would resolve this.

### Open Question 3
How can skill transfer be achieved for complex "long-tail" websites that blend functionalities from multiple domains? The framework's utility diminishes for complex sites that do not fit existing categories, such as a site combining social networking, e-commerce, and content creation. The current polymorphic structure relies on aligning a site with a single abstract class, failing when a site spans multiple semantic categories. A method allowing skills to inherit or compose traits from multiple abstract parent classes (e.g., mixins) would resolve this.

## Limitations
- The core assumption that websites within a domain share enough structural patterns for a single abstract interface is unverified for all domains
- The mechanism for auto-generating abstract interfaces versus manual definition is not detailed
- Every induced skill requires a re-run of the task for verification, which could be computationally expensive

## Confidence
**High Confidence**: The mechanism of decoupling abstract goals from concrete implementations is sound and well-supported by evidence (Table 1, Section 3.2). The improvement in skill reusability (1.7x) and reduction in steps (over 20%) is empirically demonstrated on the Mind2Web and WebArena benchmarks.

**Medium Confidence**: The prevention of catastrophic forgetting during continual learning is demonstrated, but the paper does not provide a direct comparison to baseline methods on this specific metric. The claim of superior generalization in task-free settings is supported by the comparison to sequential curriculum learning, but the specific conditions and limitations of this setting are not fully explored.

**Low Confidence**: The system's ability to handle "long-tail" or hybrid websites that do not fit existing domain categories is not addressed. The specific prompts and logic for abstract class seeding and inference retrieval are not provided, which are critical for reproduction.

## Next Checks
1. Reproduce Cross-Domain Transfer: Train the skill library on Amazon tasks, then test on Target tasks without further training. Measure the skill reusability and task success rate to validate the 31% vs 18% improvement claimed in the paper.

2. Ablate the Abstract Class: Run the induction pipeline without providing the Abstract Class context. Check if the resulting skills become "spaghetti code" that fails to transfer, demonstrating the necessity of the polymorphic abstraction.

3. Test on Hybrid Websites: Introduce a hybrid website that combines elements of multiple domains (e.g., a site with both shopping and travel functionalities). Attempt to induce skills and observe if the system can handle the lack of a clear abstract class or if it fails.