---
ver: rpa2
title: Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings
arxiv_id: '2601.18694'
source_url: https://arxiv.org/abs/2601.18694
tags:
- speaker
- audio
- voice
- cloning
- nepali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a neural voice cloning system for the Nepali
  language, addressing the lack of speech synthesis research for low-resource languages.
  The system employs a speaker encoder trained with Generative End2End loss to generate
  speaker embeddings, which are fused with Tacotron2-based text embeddings to produce
  mel-spectrograms.
---

# Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings

## Quick Facts
- arXiv ID: 2601.18694
- Source URL: https://arxiv.org/abs/2601.18694
- Reference count: 0
- Primary result: Neural voice cloning system for Nepali achieves MOS 3.84-4.01 and cosine similarity >0.90 for speaker identity preservation

## Executive Summary
This study presents a neural voice cloning system for the Nepali language, addressing the lack of speech synthesis research for low-resource languages. The system employs a speaker encoder trained with Generative End2End loss to generate speaker embeddings, which are fused with Tacotron2-based text embeddings to produce mel-spectrograms. These are converted to audio using a WaveRNN vocoder. Datasets were curated from multiple sources, including self-recordings, and underwent thorough preprocessing. The system achieves high-quality voice cloning, with cosine similarities above 0.90 for most speakers, and Mean Opinion Scores (MOS) of 3.84 (male) and 4.01 (female) for audio quality.

## Method Summary
The system uses a three-component pipeline: (1) a speaker encoder with three LSTM layers producing 256-dimensional embeddings trained on 833 speakers using GE2E loss, (2) a Tacotron2-based synthesizer that concatenates speaker embeddings with text representations to generate mel-spectrograms, and (3) a pre-trained WaveRNN vocoder fine-tuned on Nepali data. The speaker encoder requires diverse training data (235 hours), while the synthesizer needs quality-aligned pairs (8.67 hours). Audio is processed at 16kHz for the encoder and 22.05kHz for the synthesizer, with mel-spectrograms generated using filter_length=800, hop_length=200, and n_mel=80.

## Key Results
- Achieved MOS scores of 3.84 (male) and 4.01 (female) for audio quality
- Cosine similarities between original and cloned speaker embeddings exceeded 0.90 for most speakers
- Equal Error Rate (EER) dropped below 0.04 after training, demonstrating improved speaker discrimination
- UMAP visualization showed effective clustering of speaker embeddings by identity and gender

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The speaker encoder learns to map variable-length audio to fixed 256-dimensional embeddings that cluster by speaker identity.
- Mechanism: Three stacked LSTM layers (256 units each) process mel-spectrogram frames, followed by a fully connected layer producing the embedding. GE2E loss trains the network to minimize within-speaker embedding distance while maximizing between-speaker distance.
- Core assumption: That speaker identity can be disentangled from linguistic content and captured in a fixed-dimensional vector.
- Evidence anchors:
  - [abstract] "The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through UMAP"
  - [section 6.1] "EER steadily decreased... starting near 0.10 and dropping below 0.04" demonstrating improved speaker discrimination
  - [corpus] Weak direct support; corpus neighbors focus on defense/detection rather than encoder training dynamics
- Break condition: If speakers in training data are too acoustically similar (same recording environment, similar vocal range), embeddings may fail to separate meaningfully.

### Mechanism 2
- Claim: Concatenating speaker embeddings with text embeddings conditions the synthesizer on speaker identity while preserving linguistic content.
- Mechanism: Tacotron2's text encoder produces a hidden representation; speaker embeddings are concatenated (not added) to this representation before the attention mechanism. This joint representation guides mel-spectrogram generation.
- Core assumption: That speaker characteristics are independent of phonetic content and can be applied uniformly across any input text.
- Evidence anchors:
  - [section 5.2] "Speaker embeddings extracted from the encoder are concatenated with the corresponding text embeddings to form a joint representation"
  - [section 6.1] "Most cloned voices achieved a cosine similarity above 0.90" indicating speaker identity preservation
  - [corpus] No direct mechanism validation in corpus
- Break condition: If concatenation strength is too weak, the model may ignore speaker embeddings; if too strong, it may overfit to seen speakers.

### Mechanism 3
- Claim: Few-shot cloning works because the speaker encoder generalizes to unseen speakers via its training on diverse voices, not by memorization.
- Mechanism: The encoder is trained on 833 speakers with varied acoustic conditions. Once trained, it can extract meaningful embeddings from just 5-10 seconds of new speaker audio. The synthesizer, having learned to interpret these embeddings, produces speech matching the new voice.
- Core assumption: That the embedding space is sufficiently smooth and well-structured to interpolate to unseen speakers.
- Evidence anchors:
  - [abstract] "The system effectively clones speaker characteristics even for unseen voices"
  - [section 6.1] UMAP visualization shows distinct clusters for different speakers with gender-based separation
  - [corpus] SV2TTS framework [6] cited in paper demonstrates this approach works; corpus papers on zero-shot cloning (VoiceMark) support the transfer principle
- Break condition: If the target speaker's voice characteristics fall outside the distribution of training speakers (e.g., unusual dialect, speech pathology), embeddings may be unreliable.

## Foundational Learning

- Concept: **Speaker Embeddings (d-vectors)**
  - Why needed here: Understanding that the encoder compresses all speaker information into a 256-dim vector is essential for debugging cloning failures.
  - Quick check question: Can you explain why the same speaker produces similar embeddings across different utterances?

- Concept: **Mel-spectrograms as Intermediate Representations**
  - Why needed here: The synthesizer outputs mel-spectrograms, not audio. Understanding time-frequency tradeoffs (filter_length=800, hop_length=200) helps diagnose temporal artifacts.
  - Quick check question: What happens to temporal resolution if you double the hop_length?

- Concept: **GE2E Loss for Speaker Verification**
  - Why needed here: This loss function directly shapes the embedding space. Understanding it helps interpret EER metrics and why embeddings cluster.
  - Quick check question: Why would GE2E loss be preferred over triplet loss for speaker encoder training?

## Architecture Onboarding

- Component map:
  - Audio → Speaker Encoder → 256-dim embedding → Tacotron2 (with text encoder) → Mel-spectrogram → WaveRNN → Waveform

- Critical path:
  1. Collect reference audio from target speaker
  2. Extract speaker embedding via encoder
  3. Encode input text (Devanagari) via Tacotron2 text encoder
  4. Concatenate text representation with speaker embedding
  5. Generate mel-spectrogram through attention-decoder
  6. Convert to audio via WaveRNN

- Design tradeoffs:
  - **Data separation**: Speaker encoder needs quantity/diversity (235 hrs, 833 speakers); synthesizer needs quality/alignment (8.67 hrs paired data)
  - **Pre-trained vs scratch vocoder**: Paper used pre-trained WaveRNN to save resources—trades some quality for feasibility
  - **Chunking strategy**: 1.6-second chunks with 50% overlap for encoder maximizes training data but may introduce boundary artifacts

- Failure signatures:
  - **Low cosine similarity (<0.85)**: Encoder fails to capture speaker identity; check input audio quality (SNR <25 dB often problematic)
  - **Distorted timbre/unnatural pitch**: Synthesizer not properly conditioned; verify embedding concatenation is working
  - **Mispronunciations**: Text preprocessing issue; check numeral conversion and segmentation

- First 3 experiments:
  1. **Validate encoder clustering**: Train encoder, visualize embeddings with UMAP on held-out speakers. Expect distinct clusters per speaker; if merged, increase training diversity.
  2. **Ablate embedding conditioning**: Synthesize with zero-vector embeddings vs. real embeddings. Difference in speaker similarity scores quantifies conditioning effectiveness.
  3. **SNR threshold test**: Clone voices across different recording qualities (15-35 dB SNR). Establish minimum quality threshold for acceptable cloning (paper suggests >25 dB).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does replacing the fine-tuned WaveRNN vocoder with a Speaker-Conditional WaveRNN (SC-WaveRNN) or HiFi-GAN affect the naturalness and generalization for unseen Nepali speakers?
- Basis in paper: [explicit] The authors identify SC-WaveRNN and HiFi-GAN as "promising directions" that offer "superior performance" but were not utilized due to "resource and compatibility constraints."
- Why unresolved: The study relied on a pre-trained WaveRNN due to hardware limitations, leaving the potential improvements of advanced vocoders untested.
- What evidence would resolve it: A comparative analysis of MOS scores and speaker similarity metrics using the new vocoder architectures on the same dataset.

### Open Question 2
- Question: To what extent does scaling the speaker encoder training data beyond 1,000 speakers improve the system's robustness against noisy environments and unique linguistic variations?
- Basis in paper: [explicit] The authors note their model was trained on "fewer than 1,000 speakers," whereas state-of-the-art systems utilize "over 9,000 speakers," and they explicitly link data limitations to struggles with "noisy environments" and "unique linguistic variations."
- Why unresolved: The current dataset was constrained by availability and collection challenges, limiting the model's ability to generalize across diverse vocal characteristics.
- What evidence would resolve it: Training the encoder with a larger, more diverse corpus and measuring the reduction in Equal Error Rate (EER) and cosine similarity under noisy conditions.

### Open Question 3
- Question: Does expanding the synthesizer training dataset beyond the current 8.67 hours significantly mitigate the pronunciation clarity issues observed in the generated speech?
- Basis in paper: [inferred] The paper notes the synthesizer dataset was "capped at around 10 hours" and that the system exhibits "inconsistent" pronunciation clarity, attributing shortcomings to "insufficiently diverse training data."
- Why unresolved: The alignment of text and audio is time-intensive, leaving the correlation between dataset size and the resolution of pronunciation errors unstated.
- What evidence would resolve it: Experiments incrementally increasing paired text-audio data volume and measuring the resulting accuracy of phoneme production.

## Limitations
- Limited testing scope for few-shot cloning across diverse speaker types
- Use of pre-trained components without full specification of source or training conditions
- Subjective MOS evaluation without detailed rater methodology or reliability metrics

## Confidence

**High Confidence Claims:**
- The speaker encoder successfully learns to produce embeddings that cluster by speaker identity
- The system achieves reasonable audio quality with MOS scores above 3.8
- The preprocessing pipeline effectively handles Devanagari text and audio normalization

**Medium Confidence Claims:**
- Few-shot cloning works for unseen speakers
- Gender-based clustering in embedding space
- The concatenation mechanism effectively conditions the synthesizer on speaker identity

**Low Confidence Claims:**
- Generalizability to speakers with significantly different vocal characteristics from training data
- Robustness across diverse recording conditions
- Long-term consistency of voice cloning across extended utterances

## Next Checks
1. **Ablation Study on Embedding Conditioning**: Systematically remove speaker embeddings from the synthesis pipeline and measure degradation in both speaker similarity (cosine similarity) and audio quality (MOS).

2. **Cross-Dataset Generalization Test**: Evaluate the trained system on speakers from completely different recording environments or with different accents/dialects not represented in the training data.

3. **Temporal Consistency Analysis**: Generate extended utterances (30+ seconds) from cloned voices and analyze temporal consistency in speaker characteristics.