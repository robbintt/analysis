---
ver: rpa2
title: 'Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates:
  A Theoretical Perspective'
arxiv_id: '2507.07852'
source_url: https://arxiv.org/abs/2507.07852
tags:
- function
- pre-trained
- have
- missing
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies sequential decision-making under missing covariates
  with a pre-trained AI model for imputation. The key challenge is that missing data
  and model inaccuracies can degrade decision quality.
---

# Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates: A Theoretical Perspective

## Quick Facts
- **arXiv ID**: 2507.07852
- **Source URL**: https://arxiv.org/abs/2507.07852
- **Reference count**: 40
- **Primary result**: Introduces "model elasticity" metric and shows calibration procedure can reduce regret from linear to sublinear under Missing At Random assumption

## Executive Summary
This paper addresses the challenge of online decision-making when covariates are missing, leveraging pre-trained AI models for imputation. The authors introduce "model elasticity" as a new metric to quantify how sensitive reward functions are to imputation errors. They establish that under Missing Not At Random (MNAR) conditions, regret inevitably includes a linear term proportional to model elasticity, representing an unavoidable cost of using imperfect imputations. However, under the more benign Missing At Random (MAR) assumption, they propose a calibration procedure using orthogonal statistical learning that can eliminate this linear term, reducing regret to near-optimal rates with only a small noise-induced penalty.

## Method Summary
The paper proposes two algorithms: PRIMO (the base algorithm) uses a plug-in estimator from a pre-trained model to fill missing covariates, while PRIMO-Cal adds a calibration step for the MAR setting. Both employ an Inverse Gap Weighting (IGW) policy that assigns action probabilities inversely proportional to the reward gap between an action and the estimated optimal action. The algorithms operate in epochs, with periodic updates to both the reward model and imputation model. Under MNAR, regret scales as √T plus a linear term in T proportional to model elasticity. Under MAR, the calibration procedure reduces this to sublinear regret plus a small noise term.

## Key Results
- Under MNAR, regret bounds scale as √T + O(T·model elasticity), where model elasticity measures sensitivity of rewards to imputation errors
- Under MAR, calibration via orthogonal statistical learning eliminates the linear T term, yielding regret bounds of √T plus a small noise-induced linear term
- The calibration procedure improves regret bounds polynomially in the accuracy of the pre-trained model
- Theoretical analysis relies on realizability assumptions for reward function classes and bounded noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A pre-trained model imputes missing covariates, but errors introduce regret linear in time horizon T, scaled by a sensitivity metric.
- Mechanism: A plug-in estimator from a pre-trained model (g̃) fills missing covariates (z*). Regret analysis decomposes into oracle estimation error plus a linear term from imputation bias, quantified by "model elasticity"—the reward function's sensitivity to covariate discrepancies.
- Core assumption: Assumption 3.1 (realizability of reward function class F) and bounded noise.
- Evidence anchors:
  - [abstract]: "...introduces a new metric called 'model elasticity' that measures how sensitive the reward function is to imputation errors... leads to a regret bound with an unavoidable linear term in T proportional to the model elasticity."
  - [section 4.3, Definition 4.4]: Defines model elasticity as `sup_{f,a} E[(f(x,z*,a)-f(x,g̃(x),a))^2]`.
  - [corpus]: "Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards" addresses surrogate rewards, which parallels this imputation idea but focuses on rewards directly, not covariates.
- Break condition: If the pre-trained model's predictions are perfectly accurate (g̃(x)=z* almost surely), the linear term collapses.

### Mechanism 2
- Claim: Under Missing At Random (MAR), a calibration procedure using orthogonal statistical learning reduces imputation bias, transforming linear regret into sublinear terms plus a small noise-induced linear term.
- Mechanism: The calibration (AlgCal) estimates the missingness mechanism (propensity score e*(x)) and performs a weighted regression on a centered function class Gδ₀ to update the pre-trained model (ĝ). This orthogonalization decouples nuisance parameter estimation errors from the primary parameter, improving the imputed covariates' quality.
- Core assumption: Assumption 5.1, specifically the MAR condition (P(b=1|x,z*)=P(b=1|x)), and Lipschitz continuity of the reward function class F with respect to covariates.
- Evidence anchors:
  - [abstract]: "under the more benign missing-at-random assumption, the paper proposes a calibration procedure... This calibration eliminates the linear T term in regret."
  - [section 5, Algorithm 2]: Details the AlgCal procedure using cross-fitting, propensity estimation, and weighted regression.
  - [corpus]: "Reliable Imputed-Sample Assisted Vertical Federated Learning" discusses imputation in a federated setting but does not address sequential calibration.
- Break condition: If the MAR assumption is violated (e.g., Missing Not At Random), the calibration's theoretical guarantees do not hold, and linear regret from Mechanism 1 persists.

### Mechanism 3
- Claim: The Inverse Gap Weighting (IGW) policy translates the estimation error bounds from the ERM oracle into a regret bound for the online decision-making process.
- Mechanism: The IGW stochastic policy assigns action probabilities inversely proportional to the reward gap between an action and the estimated optimal action. This ensures sufficient exploration while concentrating probability on high-reward actions. The epoch-based structure (doubling epoch lengths) allows periodic model updates using accumulated data, converting improved estimation error (from calibration) into improved regret.
- Core assumption: Access to an offline ERM oracle (AlgErm) that can solve the regression problem.
- Evidence anchors:
  - [abstract]: "...leads to a regret bound that scales with the statistical complexity of the function classes plus a much smaller noise-induced linear term."
  - [section 4.1]: Describes the IGW policy and its role in balancing exploration/exploitation.
  - [corpus]: "SPARKLE" also uses a nonparametric approach but focuses on high-dimensional covariates in a different algorithmic framework.
- Break condition: If the ERM oracle fails to provide an accurate reward estimate (e.g., due to severe model misspecification), the policy explores suboptimally, increasing regret.

## Foundational Learning

- Concept: **Contextual Bandits and Regret**
  - Why needed here: The paper's core problem is formalized as a contextual bandit where context has missing components. Regret is the primary metric for evaluating the online learning algorithm's performance against an oracle with full information.
  - Quick check question: Can you define the key difference between a standard multi-armed bandit and a contextual bandit? What does "regret" measure in this setting?

- Concept: **Missing Data Mechanisms (MAR vs. MNAR)**
  - Why needed here: The paper's theoretical results critically bifurcate based on the missingness assumption. Under MNAR, a linear regret term is unavoidable; under MAR, it can be mitigated.
  - Quick check question: Explain the "Missing At Random" (MAR) assumption. Why does it make the missingness mechanism easier to handle than "Missing Not At Random" (MNAR)?

- Concept: **Orthogonal Statistical Learning / Double Machine Learning**
  - Why needed here: This is the core technique enabling the calibration procedure in Mechanism 2. It provides the theoretical tools to achieve bias reduction when estimating nuisance parameters (the missingness probability) and the primary parameter (the calibrated covariate function).
  - Quick check question: What is the main benefit of using orthogonal statistical learning when you have to estimate a "nuisance parameter" (like the missingness probability) before estimating your main parameter of interest?

## Architecture Onboarding

- Component map: Pre-trained model (g̃) -> Imputation -> IGW Policy -> Action -> Reward Observation -> ERM Oracle -> Updated Reward Model (f̂) -> (Under MAR) Calibration (AlgCal) -> Updated Imputation Model (ĝ)

- Critical path: The **imputation model quality** (ĝ_s) is the critical path. A poor initial model or failed calibration propagates error into the reward estimate (f̂_s), leading the IGW policy to make suboptimal decisions, which accumulates as linear regret.

- Design tradeoffs: The primary tradeoff is between **algorithmic complexity** and **regret guarantees**. The base PRIMO algorithm is simple (plug-in imputation) but suffers from unavoidable linear regret. The PRIMO-Cal algorithm, while providing better theoretical regret under MAR, adds significant complexity by requiring estimation of the missingness mechanism (e*), a second regression step for calibration, and cross-fitting.

- Failure signatures:
  - **Persistent Linear Regret:** If the linear regret term does not decay over time, it indicates the calibration is failing. This could be due to a violation of the MAR assumption or poor estimation of the propensity score (ê).
  - **High Variance in Reward Estimates:** Fluctuating performance between epochs could signal that the ERM oracle is overfitting to sparse or noisy data within an epoch.

- First 3 experiments:
  1. **Baseline Under MNAR:** Run PRIMO (Algorithm 1) on a simulated dataset with non-random missingness. Verify that the regret curve exhibits a dominant linear component proportional to the model elasticity.
  2. **Calibration Under MAR:** Run PRIMO-Cal (Algorithm 3) on a dataset where the MAR assumption holds. Compare its regret curve to the PRIMO baseline. The goal is to observe the elimination of the dominant linear term and the emergence of sublinear scaling.
  3. **Sensitivity Analysis:** Systematically vary the initial accuracy of the pre-trained model (δ₀) and observe its impact on the cumulative regret of PRIMO-Cal. This validates Theorem 5.3, which predicts that regret from calibration error should decrease as the prior model improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the calibration procedure be extended to the Missing Not At Random (MNAR) setting to eliminate the linear regret term?
- Basis: [explicit] Section 6 states it would be important to "incorporate modeling and learning the missing not at random (MNAR) mechanism while still maintaining provable guarantees."
- Why unresolved: The proposed calibration (Algorithm 2) relies on the MAR assumption (Assumption 5.1) for identifiability. MNAR requires different structural assumptions (e.g., instrumental variables) to disentangle the missingness mechanism from the unobserved covariate values.
- What evidence would resolve it: A theoretical derivation showing that calibration can proceed under specific MNAR conditions (e.g., using proxy variables) to achieve sublinear regret without the $\sqrt{E_F^P(\tilde{g})}T$ penalty.

### Open Question 2
- Question: How can the framework be adapted to sequentially select or ensemble information from multiple imperfect pre-trained models?
- Basis: [explicit] Section 6 suggests it "would be valuable to extend the framework to settings involving multiple imperfect pre-trained models, and to develop principled algorithms that adaptively select or ensemble information."
- Why unresolved: The current PRIMO algorithm assumes a single pre-trained model $\tilde{g}$. Using multiple models introduces a model selection problem that complicates the calibration of the nuisance parameters and the regret analysis.
- What evidence would resolve it: An algorithm that dynamically weights multiple models $\tilde{g}_i$ and a regret bound demonstrating that performance scales with the quality of the best model or an optimal ensemble.

### Open Question 3
- Question: How does the algorithm perform under reward model misspecification ($f^* \notin \mathcal{F}$)?
- Basis: [inferred] Assumption 3.1 imposes a strict realizability condition ($f^* \in \mathcal{F}$). In practice, the true reward function may lie outside the chosen function class.
- Why unresolved: The oracle inequality (Theorem 4.5) and regret guarantees (Theorem 4.6) depend on the convergence of $\hat{f}$ to $f^*$ within $\mathcal{F}$. It is unclear if "model elasticity" provides robustness against approximation error in addition to imputation error.
- What evidence would resolve it: A regret analysis that includes an approximation error term (e.g., $\min_{f \in \mathcal{F}} \|f - f^*\|$) and characterizes the regret under this misspecification.

## Limitations

- Theoretical analysis relies heavily on realizability assumption for reward function class (Assumption 3.1), which may not hold in practice with complex reward structures
- Calibration procedure's effectiveness depends critically on the MAR assumption, which is difficult to verify in real-world applications
- Assumes access to an offline ERM oracle that can solve the regression problem perfectly, which may be computationally intractable for complex function classes
- Model elasticity metric requires knowledge of true missing covariates z* for estimation, which is generally unavailable in practice

## Confidence

- **High confidence** in Mechanism 1 (MNAR regret bounds): The linear regret term proportional to model elasticity is rigorously derived and represents a fundamental limitation rather than a modeling artifact
- **Medium confidence** in Mechanism 2 (MAR calibration): While the orthogonal statistical learning framework is well-established, practical performance depends heavily on accuracy of propensity score estimation
- **Medium confidence** in Mechanism 3 (IGW policy): The policy design is sound, but regret bounds depend on ERM oracle's performance which may not translate to practice

## Next Checks

1. **MAR Assumption Verification**: Design a simulation framework where the missingness mechanism can be controlled to systematically test the calibration procedure's performance when MAR is violated versus when it holds.

2. **ERMS Oracle Practicality**: Implement the ERM oracle using practical regression methods (e.g., neural networks with early stopping) and evaluate how oracle approximation error affects the overall regret bounds.

3. **Model Elasticity Estimation**: Develop a practical method to estimate or bound model elasticity using only observable data, and test this estimation method on real-world datasets with known missingness patterns.