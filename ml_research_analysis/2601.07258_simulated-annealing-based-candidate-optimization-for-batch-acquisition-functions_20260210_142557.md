---
ver: rpa2
title: Simulated Annealing-based Candidate Optimization for Batch Acquisition Functions
arxiv_id: '2601.07258'
source_url: https://arxiv.org/abs/2601.07258
tags:
- optimization
- annealing
- simulated
- batch
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a simulated annealing-based approach for optimizing
  batch acquisition functions in multi-objective Bayesian optimization. The method
  addresses limitations of gradient-based optimization methods like SLSQP, which often
  get trapped in local optima and struggle with discrete candidate sets.
---

# Simulated Annealing-based Candidate Optimization for Batch Acquisition Functions

## Quick Facts
- **arXiv ID**: 2601.07258
- **Source URL**: https://arxiv.org/abs/2601.07258
- **Reference count**: 15
- **One-line primary result**: Simulated annealing optimization of batch acquisition functions consistently outperforms gradient-based SLSQP across benchmark problems

## Executive Summary
This paper presents a simulated annealing-based approach for optimizing batch acquisition functions in multi-objective Bayesian optimization, addressing limitations of gradient-based methods that get trapped in local optima. The method operates directly on discrete candidate sets, eliminating the relaxation-and-round errors inherent to continuous optimization approaches. The proposed approach was evaluated across four benchmark problems, demonstrating superior hypervolume performance, particularly for complex multi-modal acquisition function landscapes.

## Method Summary
The method optimizes batch acquisition functions (specifically qEHVI) using simulated annealing on discrete candidate sets. The algorithm perturbs batches by swapping 1-3 points with probabilities [0.6, 0.3, 0.1], uses Metropolis acceptance criterion, and employs geometric cooling. A parallel implementation with GPU acceleration was developed for real-world materials optimization. The approach contrasts with SLSQP by operating directly on discrete candidates and using probabilistic acceptance to escape local optima.

## Key Results
- Simulated annealing consistently achieves superior hypervolume performance across most test cases
- Improvement is particularly pronounced for DTLZ2 and Latent-Aware problems (up to 575% improvement)
- Parallel implementation achieves 2% improvement in hypervolume while significantly reducing computation time
- Method demonstrates superior exploration of diverse regions in objective space

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Probabilistic acceptance in simulated annealing enables escape from local optima that trap gradient-based methods in multi-modal acquisition function landscapes.
- **Mechanism**: The Metropolis criterion accepts inferior solutions with probability exp(Δ/T), where Δ is the acquisition function change and T is temperature. Early in optimization (high T), this permits uphill moves; as T decreases, the algorithm converges toward greedy selection. This allows traversal between basins of attraction.
- **Core assumption**: The acquisition function landscape contains multiple local optima separated by barriers that deterministic gradient descent cannot cross but probabilistic moves can.
- **Evidence anchors**:
  - [abstract] "simulated annealing...can become trapped in local optima, particularly in complex or high-dimensional objective landscapes"
  - [section 2.3] "The temperature-based acceptance mechanism of simulated annealing proves especially effective for escaping local optima that trap gradient-based methods"
  - [corpus] Limited direct corpus support; related work on evolutionary-guided BO (Low et al.) suggests metaheuristics help in constrained settings, but no direct mechanism comparison.
- **Break condition**: If acquisition function is convex or smoothly unimodal, gradient-based methods converge faster and SA adds unnecessary stochasticity.

### Mechanism 2
- **Claim**: Operating directly on discrete candidate sets eliminates relaxation-and-round errors inherent to continuous optimization approaches.
- **Mechanism**: The algorithm samples from a predefined candidate set C and proposes perturbations by replacing batch elements with other members of C, guaranteeing all proposals are valid discrete candidates without post-hoc projection.
- **Core assumption**: Discrete candidates represent feasible configurations where interpolation is meaningless (e.g., alloy compositions with finite resolution).
- **Evidence anchors**:
  - [abstract] "The method addresses limitations of gradient-based optimization methods...which often get trapped in local optima and struggle with discrete candidate sets"
  - [section 1, page 2] "Traditional approaches like relaxation-and-round perform poorly for discrete candidate sets, because the acquisition function does not account for discretization"
  - [corpus] No corpus papers directly address discrete candidate handling in qEHVI; this remains a gap in related literature.
- **Break condition**: If candidate set is dense enough to approximate continuity, or if constraints can be relaxed without violating physical feasibility, continuous methods may be preferred.

### Mechanism 3
- **Claim**: Multi-point perturbation with categorical probability distribution balances local refinement and global exploration across batch compositions.
- **Mechanism**: At each iteration, the algorithm samples k∈{1,2,3} points to modify with probabilities [0.6, 0.3, 0.1]. Higher probability for single-point changes promotes local search; multi-point changes enable exploration of different batch compositions. The combinatorial batch selection space (C(n,q) for n candidates, batch size q) is navigated without enumeration.
- **Core assumption**: Batch quality improves through both incremental adjustments and occasional larger compositional changes.
- **Evidence anchors**:
  - [section 2.3.1, page 4] "This multi-point perturbation strategy enables the algorithm to explore various batch compositions while maintaining computational efficiency"
  - [section 1, page 2] "combinatorial explosion in selecting which batch to evaluate...C(n,q)=n!/(q!(n-q)!)...making exhaustive enumeration computationally prohibitive"
  - [corpus] Corpus papers (MOBO-OSD, Generative MOBO) focus on continuous or generative approaches; no direct comparison of perturbation strategies.
- **Break condition**: If batch elements are strongly coupled (joint acquisition value depends critically on all elements simultaneously), single-point changes may rarely improve, requiring higher multi-point probabilities or different neighborhoods.

## Foundational Learning

- **Concept: q-Expected Hypervolume Improvement (qEHVI)**
  - Why needed here: This is the acquisition function being optimized; understanding its batch formulation and O(2^q) complexity explains why efficient inner-loop optimization matters.
  - Quick check question: Given a current Pareto frontier and batch size q=4, what is the asymptotic complexity of computing qEHVI, and why does this motivate metaheuristic inner optimization?

- **Concept: Gaussian Process surrogate modeling**
  - Why needed here: GPs provide the predictive distributions (mean and uncertainty) that qEHVI integrates over; the acquisition function quality depends on GP fidelity.
  - Quick check question: What two quantities does a GP posterior provide for each candidate, and how does qEHVI use them differently than single-objective EI?

- **Concept: Simulated annealing cooling schedules**
  - Why needed here: The paper uses geometric cooling T_t = T_0 · α^t; understanding exploration-exploitation trade-offs helps diagnose convergence issues.
  - Quick check question: If T_0 is too low or α is too aggressive (e.g., 0.90), what behavior would you expect in hypervolume convergence curves?

## Architecture Onboarding

- **Component map**: Discrete candidate set → Perturbation module (samples k changes) → qEHVI evaluator (batch acquisition computation) → Metropolis accept/reject → Temperature update → Loop
- **Critical path**: qEHVI evaluation dominates runtime; GPU batched evaluation in parallel chains reduces wall-clock time. The 575% improvement on Latent-Aware suggests acquisition landscape complexity, not evaluation cost, limits SLSQP.
- **Design tradeoffs**:
  - Aggressive cooling (α=0.95, T_0=5.0) prioritizes speed over optimality; slower cooling (α=0.9999) used in real-world campaign
  - Perturbation distribution [0.6, 0.3, 0.1] favors local moves; increase p_2, p_3 for more exploration
  - Parallel chains (M=10) with fewer iterations (2×10^4) vs. sequential (10^5): trade parallel compute for wall-clock reduction
- **Failure signatures**:
  - Hypervolume plateaus early with no improvement: temperature may be too low; check initial T_0
  - High variance across runs: perturbation too aggressive or cooling too slow; reduce multi-point change probability
  - SLSQP outperforms on smooth low-D problems (e.g., Kursawe): expected; SA advantage emerges with complexity
- **First 3 experiments**:
  1. Replicate Kursawe and Latent-Aware comparison with matched iteration budgets; confirm SA advantage correlates with landscape multimodality.
  2. Ablate perturbation distribution: test [0.8, 0.15, 0.05] vs. [0.4, 0.4, 0.2] on DTLZ2 to measure exploration-exploitation trade-off.
  3. Profile GPU batched qEHVI evaluation: measure speedup of M×r proposals vs. sequential; identify bottleneck in linear-operator routines for batch sizes q>12.

## Open Questions the Paper Calls Out
- Can hybridizing simulated annealing with local search heuristics recover the performance gap observed in smooth, low-dimensional problems?
- To what extent do adaptive cooling schedules improve convergence speed and final solution quality compared to the geometric cooling used in this study?
- Does the robustness of simulated annealing extend to acquisition functions specifically designed for noisy multi-objective optimization, such as qNoisyEHVI?

## Limitations
- Conflicting hyperparameters between text (T₀=5.0, α=0.95) and algorithm specification (T₀=1.0, α=0.9999) create ambiguity about which configuration produced results
- Candidate set sizes for synthetic benchmarks are not specified, requiring assumptions about discretization resolution
- No statistical significance testing is provided for the performance improvements
- Computational complexity analysis focuses on qEHVI evaluation but doesn't fully account for total cost of metaheuristic approach

## Confidence
- **High confidence**: The fundamental claim that discrete candidate set optimization is superior to relaxation-and-round approaches for acquisition function maximization
- **Medium confidence**: The 575% improvement claim for Latent-Aware, as this represents an extreme outlier that may be problem-specific rather than demonstrating general superiority
- **Low confidence**: The parallel implementation's 2% improvement claim, given the lack of baseline comparison details and potential confounding factors

## Next Checks
1. Systematically vary T₀ and α across the range specified in the paper to identify which configuration produces the reported results and understand performance sensitivity
2. Perform multiple runs (n≥30) of both methods on DTLZ2 and Kursawe to establish confidence intervals and determine if SA's advantages are statistically significant
3. Test the parallel implementation on larger batch sizes (q>12) and higher-dimensional problems to assess whether the GPU acceleration maintains its effectiveness as problem complexity increases