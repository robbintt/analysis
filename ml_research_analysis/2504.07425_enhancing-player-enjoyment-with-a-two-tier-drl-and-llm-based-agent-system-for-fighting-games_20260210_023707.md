---
ver: rpa2
title: Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for
  Fighting Games
arxiv_id: '2504.07425'
source_url: https://arxiv.org/abs/2504.07425
tags:
- agent
- reward
- agents
- player
- opponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Two-Tier Agent (TTA) system to enhance player
  enjoyment in fighting games, specifically Street Fighter II. The system combines
  Deep Reinforcement Learning (DRL) agents with a Large Language Model (LLM) Hyper-Agent
  to dynamically select opponents based on player data and feedback.
---

# Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games

## Quick Facts
- arXiv ID: 2504.07425
- Source URL: https://arxiv.org/abs/2504.07425
- Reference count: 40
- Key outcome: TTA system enhances player enjoyment with 42.73% improvement and 156.36% better special move usage

## Executive Summary
This paper presents a Two-Tier Agent (TTA) system that combines Deep Reinforcement Learning (DRL) agents with a Large Language Model (LLM) Hyper-Agent to enhance player enjoyment in fighting games. The system dynamically selects opponents in Street Fighter II based on player data and feedback, while the DRL agents are trained with modularized reward functions and hybrid training to exhibit diverse play styles. Experiments demonstrate significant improvements in both agent performance and player satisfaction compared to baseline approaches.

## Method Summary
The TTA system consists of DRL agents trained through hybrid reinforcement learning to execute advanced fighting game skills and exhibit diverse play styles. These agents are paired with an LLM Hyper-Agent that analyzes player data and feedback to dynamically select appropriate opponents. The DRL agents use a modularized reward function to learn specific behaviors like special move execution and defensive strategies. The LLM Hyper-Agent processes player performance metrics, win/loss records, and qualitative feedback to make opponent selection decisions that balance challenge and enjoyment.

## Key Results
- Trained DRL agents improve special move usage by 156.36% over baseline methods
- TTA system achieves a 66.7% win rate advantage for selected opponents
- Small-scale user study shows 42.73% improvement in overall enjoyment and better difficulty suitability

## Why This Works (Mechanism)
The system works by combining specialized DRL agents that master game mechanics with an LLM that understands player psychology and preferences. The DRL agents focus on executing complex fighting game actions through reward-modularized training, while the LLM Hyper-Agent uses natural language understanding to interpret player feedback and match players with appropriately challenging opponents. This two-tier approach addresses both the technical challenge of creating skilled AI opponents and the human-centered challenge of maintaining player engagement through personalized difficulty adjustment.

## Foundational Learning
- **Reinforcement Learning**: Agents learn through trial and error to maximize rewards in the game environment. Needed to create competent fighting game AI that can execute complex move sequences. Quick check: Agent win rate against scripted opponents.
- **Reward Function Design**: Modular rewards for different behaviors (offensive, defensive, special moves) guide learning. Needed to encourage diverse and skilled gameplay rather than single-minded optimization. Quick check: Distribution of move types used by trained agents.
- **Large Language Models**: LLM processes natural language feedback and player data to make decisions. Needed to understand and act on qualitative player input beyond numerical metrics. Quick check: LLM accuracy on player feedback classification tasks.
- **Dynamic Difficulty Adjustment**: Real-time opponent selection based on player performance. Needed to maintain optimal challenge level for different skill tiers. Quick check: Player performance metrics before and after opponent selection changes.
- **Hybrid Training Approaches**: Combining different training methodologies for robust agent behavior. Needed to handle the complexity of fighting game mechanics and diverse player strategies. Quick check: Agent performance across different fighting styles and scenarios.

## Architecture Onboarding

**Component Map:**
Player -> LLM Hyper-Agent -> DRL Agent Pool -> Game Environment -> Player Feedback

**Critical Path:**
Player performance data → LLM analysis → Opponent selection → DRL agent gameplay → Player experience → Feedback loop

**Design Tradeoffs:**
- LLM decision-making vs. pure metric-based selection: LLM provides nuanced understanding but adds computational overhead
- Diverse agent training vs. specialized master agents: Diversity supports broader player engagement but may sacrifice peak performance
- Real-time selection vs. pre-defined difficulty tiers: Dynamic selection adapts to player state but requires continuous monitoring

**Failure Signatures:**
- Poor opponent selection leading to player frustration or boredom
- DRL agents exploiting game mechanics in unintended ways
- LLM misinterpreting player feedback due to ambiguous language
- System latency causing delays in opponent selection

**First Experiments:**
1. Test LLM opponent selection accuracy with simulated player profiles of known skill levels
2. Validate DRL agent special move execution against baseline scripted opponents
3. Measure player engagement metrics with random vs. LLM-selected opponents in controlled settings

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Small-scale user study (12 participants) limits generalizability of enjoyment improvements
- Evaluation focuses on a single game (Street Fighter II) without cross-game validation
- LLM decision-making process lacks transparency in how it weighs different player data inputs

## Confidence
- **High confidence**: DRL agents successfully execute special moves and exhibit diverse play styles
- **Medium confidence**: LLM-based opponent selection improves player enjoyment
- **Medium confidence**: The modularized reward function enhances agent performance

## Next Checks
1. Conduct a larger-scale user study (minimum 50 participants) across multiple skill levels and gaming backgrounds
2. Test the TTA system across different fighting game titles to evaluate cross-game applicability
3. Implement ablation studies to isolate the specific contributions of the LLM Hyper-Agent versus DRL agents to overall system performance