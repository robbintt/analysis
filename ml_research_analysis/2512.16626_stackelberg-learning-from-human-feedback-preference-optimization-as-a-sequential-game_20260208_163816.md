---
ver: rpa2
title: 'Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential
  Game'
arxiv_id: '2512.16626'
source_url: https://arxiv.org/abs/2512.16626
tags:
- preference
- leader
- learning
- follower
- stackelberggda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Stackelberg Learning from Human Feedback (SLHF) is a novel framework
  for preference optimization that models alignment as a sequential-move game between
  two policies: a Leader and a Follower. The Leader commits to an action, and the
  Follower responds conditionally, enabling stable learning and inference-time refinement.'
---

# Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game

## Quick Facts
- **arXiv ID**: 2512.16626
- **Source URL**: https://arxiv.org/abs/2512.16626
- **Reference count**: 40
- **Primary result**: SLHF achieves strong alignment across diverse preference datasets and enables inference-time refinement that transfers across model families without retraining.

## Executive Summary
Stackelberg Learning from Human Feedback (SLHF) reframes preference optimization as a sequential game between two policies: a Leader that commits to an action first, and a Follower that responds conditionally. This sequential structure enables stable learning by converting a non-stationary opponent problem into a stationary refinement task, while naturally supporting iterative sampling for inference-time improvements. SLHF outperforms existing preference optimization methods (RLHF, NLHF) on alignment benchmarks and enables Followers to improve outputs from independently trained models without additional fine-tuning.

## Method Summary
SLHF models preference optimization as a Stackelberg game where the Leader policy π commits to an action y, and the Follower policy ω responds conditionally with y′ based on observing y. Both policies share parameters in a single transformer model with different prompt templates (Leader: "Answer the question", Follower: "Improve the previous answer!"). Training uses two-timescale gradient descent-ascent (κ=η_F/η_L=5) with KL regularization to optimize a pairwise preference function p(y≻y′|x) directly, avoiding scalar reward models. The Follower learns to refine Leader outputs, enabling inference-time improvements that transfer across different model families.

## Key Results
- SLHF consistently outperforms RLHF and NLHF baselines on preference datasets (HELPSTEER2, TULU-3-8B mixture) across model scales (0.5B to 8B parameters).
- The Follower policy improves outputs from independently trained models by up to 60% without further fine-tuning.
- Inference-time refinement with the Follower achieves gains over base models while maintaining reasonable instruction-following accuracy.
- SLHF demonstrates strong alignment with human preferences while naturally supporting iterative sampling for quality improvements.

## Why This Works (Mechanism)

### Mechanism 1
Sequential play structure converts a non-stationary opponent problem into a stationary refinement task. The Follower observes the Leader's committed action before responding, solving a supervised refinement problem rather than optimizing against a moving target. This leads to more stable learning and quicker adaptation, with the Leader optimizing against the Follower's anticipated best-response.

### Mechanism 2
Two-timescale gradient descent-ascent (η_F > η_L) enables the Follower to adapt faster, yielding more stationary feedback to the Leader. The Follower's faster timescale (κ=5) means it converges toward best-responses more quickly than the Leader changes, reducing non-stationarity for the Leader who can then optimize against a quasi-fixed best-response policy.

### Mechanism 3
The Follower learns a conditional refinement policy that generalizes to improve outputs from independently trained models without additional fine-tuning. By conditioning on observed actions rather than policy identity, the Follower can be applied to any Leader model at inference time, transferring learned refinement behavior across model families.

## Foundational Learning

- **Stackelberg Games (Sequential Games with Commitment)**: Why needed: SLHF frames preference optimization as a sequential-move game where the Leader commits first and the Follower best-responds. Understanding that the Follower's informational advantage leads to deterministic equilibria (vs. mixed Nash) is critical. Quick check: In a simultaneous game where no Condorcet winner exists, what type of equilibrium does NLHF converge to? (Answer: stochastic/mixed Nash equilibrium.)

- **Preference Functions vs. Reward Models**: Why needed: SLHF avoids scalar reward models and instead uses pairwise preference functions directly. This allows handling intransitive preferences that reward models cannot represent. Quick check: Why can a scalar reward model not represent intransitive preferences? (Answer: Scalar rewards imply a total order; cycles require incomparability or multi-dimensional structure.)

- **Gradient Descent-Ascent for Minimax Optimization**: Why needed: STACKELBERGGDA solves max_π min_ω f(π,ω) via alternating gradient updates. Understanding the role of timescale separation (η_F > η_L) in stabilizing convergence is essential. Quick check: What happens if Leader and Follower learning rates are equal in a nonconvex-concave game? (Answer: May oscillate or fail to converge to equilibrium; two-timescale helps stabilize.)

## Architecture Onboarding

- **Component map**: Context x → Leader template → sample y ~ π_θ(·|x) → Follower template with "Improve the previous answer!" → sample y′ ~ π_θ(·|x,y) → Preference model p(y≻y′|x) → Compute loss L(θ) with KL penalties → Update shared parameters θ

- **Critical path**: 1) Sample batch of contexts x_b ~ ρ, 2) Generate Leader response y_b and Follower response y′_b using respective templates, 3) Query preference model for p_b = p(y_b ≻ y′_b | x_b), 4) Compute gradient estimates with KL penalty terms, 5) Update shared parameters θ via single backward pass on combined loss

- **Design tradeoffs**: Single-model vs. separate models (single reduces memory but may cause interference); κ selection (higher emphasizes Follower but risks destabilizing Leader, κ=5 recommended); Preference model quality (SLHF inherits limitations of p, poor specification leads to biased equilibria)

- **Failure signatures**: Mode collapse in Leader if preference model overfits to specific attributes; Follower degradation on out-of-distribution inputs; training instability if κ is too high or KL penalties too low; if preference model exhibits high intransitivity, Leader may select "least exploitable" actions rather than globally optimal ones

- **First 3 experiments**: 1) Sanity check on synthetic preference matrix: construct 3-action cyclic preference and verify Leader selects dominant type's preferred action when one exists while Follower traverses cycle deterministically, 2) Ablation on κ: train with κ∈{1,3,5,10} and measure Leader/Follower preference scores vs base model, confirm κ=5 yields balanced improvement, 3) Cross-model transfer test: train STACKELBERGGDA on model A, apply Follower to refine outputs from independently trained model B, measure preference gain vs baseline self-correction prompting

## Open Questions the Paper Calls Out

- **Open Question 1**: Can extragradient or mirror-prox methods achieve last-iterate convergence guarantees for SLHF, similar to their success in NLHF? The paper notes STACKELBERGGDA currently has ergodic but not last-iterate guarantees, and developing SLHF algorithms with last-iterate convergence is an open direction.

- **Open Question 2**: How can SLHF integrate both verifiable feedback (e.g., IFEval-style instruction following) and comparative preference feedback into a unified training objective? The paper observes Follower policy degrades IFEval accuracy, suggesting the current formulation trades off preference alignment against verifiable instruction-following.

- **Open Question 3**: How can SLHF incorporate active preference elicitation to adapt dynamically to individual user preferences at inference time? The paper notes current SLHF operates on offline preference datasets representing population-level preferences, lacking mechanisms for real-time user interaction or online preference updates during inference.

## Limitations
- Reliance on well-specified and representative pairwise preference functions, which can be challenging to obtain in open-ended or under-specified domains
- The Follower's generalization across model families remains under-characterized for extreme distribution shifts
- Single-model dual-template approach may cause interference between Leader and Follower objectives, though this works empirically

## Confidence
- **High confidence**: Theoretical foundation of Stackelberg equilibrium in preference optimization; Follower's ability to improve over Leader outputs when trained on same model; basic convergence properties under two-timescale updates
- **Medium confidence**: Follower's cross-model generalization capability; relative performance gains over RLHF/NLHF baselines; specific hyperparameter choices (κ=5, τ=0.001)
- **Low confidence**: Mechanism by which Follower improvements transfer to independently trained models; robustness to preference model misspecification; behavior under severe distribution shift

## Next Checks
1. **Distribution Shift Robustness**: Systematically evaluate Follower performance on Leader outputs from models trained on different datasets, with varying architectures, or fine-tuned with different methods. Measure degradation curves as input distribution diverges from training data.

2. **Preference Model Sensitivity**: Construct synthetic preference functions with known intransitivities and biases, then train SLHF and measure whether the Follower exploits these artifacts rather than learning genuine alignment. Compare final equilibria against ground-truth optimal policies.

3. **Model Separation Ablation**: Implement a version using separate Leader and Follower models with parameter sharing only through gradient updates. Compare convergence speed, final preference scores, and inference-time refinement quality against the single-model baseline to isolate interference effects.