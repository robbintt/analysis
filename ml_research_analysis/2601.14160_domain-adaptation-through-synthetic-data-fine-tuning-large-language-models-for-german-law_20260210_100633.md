---
ver: rpa2
title: 'Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models
  for German Law'
arxiv_id: '2601.14160'
source_url: https://arxiv.org/abs/2601.14160
tags:
- legal
- data
- generation
- german
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic data generation pipeline for
  fine-tuning large language models (LLMs) on German legal question answering. The
  method generates diverse, statute-grounded question-answer pairs directly from authoritative
  German legal texts, using difficulty-graded generation and LLM-based filtering to
  ensure factual consistency and reduce hallucinations.
---

# Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law

## Quick Facts
- arXiv ID: 2601.14160
- Source URL: https://arxiv.org/abs/2601.14160
- Reference count: 27
- Primary result: Up to 40 percentage points improvement in open-ended correctness on statute-based German legal QA benchmarks using synthetic data fine-tuning

## Executive Summary
This paper introduces a synthetic data generation pipeline for fine-tuning large language models (LLMs) on German legal question answering. The method generates diverse, statute-grounded question-answer pairs directly from authoritative German legal texts, using difficulty-graded generation and LLM-based filtering to ensure factual consistency and reduce hallucinations. The approach produces training data without manual annotation or reliance on web-crawled sources. Experiments with LLaMA 3.1 (8B) and Gemma 3 (12B) show that models fine-tuned with this synthetic data significantly outperform baselines on held-out German legal QA benchmarks, achieving up to 40 percentage points improvement in open-ended correctness on statute-based tests, while maintaining or slightly improving general language understanding. The results demonstrate that carefully designed synthetic supervision can effectively specialize LLMs for high-stakes, knowledge-intensive domains like German law.

## Method Summary
The approach generates synthetic question-answer pairs directly from authoritative German legal texts using LLMs. It employs difficulty-graded generation to create a range of complexity levels and applies LLM-based filtering to ensure factual consistency and minimize hallucinations. This process enables training data creation without manual annotation or web-crawled sources. The fine-tuned models (LLaMA 3.1 8B and Gemma 3 12B) are evaluated on held-out German legal QA benchmarks, demonstrating substantial improvements in domain-specific performance while preserving general language capabilities.

## Key Results
- Up to 40 percentage points improvement in open-ended correctness on statute-based German legal QA benchmarks
- Significant outperformance of baseline models on held-out German legal QA tests
- Maintained or slightly improved general language understanding alongside domain specialization

## Why This Works (Mechanism)
The synthetic data generation pipeline leverages authoritative legal texts as source material, ensuring domain-relevant content. Difficulty-graded generation creates diverse QA pairs that cover various complexity levels, improving model robustness. LLM-based filtering maintains factual consistency and reduces hallucinations by validating generated content against source statutes. This approach eliminates dependency on manual annotation or web-crawled data, providing scalable, high-quality supervision for domain adaptation. The method effectively transfers legal knowledge into LLMs through targeted synthetic supervision, enabling strong performance on specialized legal QA tasks.

## Foundational Learning
- **Synthetic data generation**: Creating training examples algorithmically rather than manually; needed for scalable domain adaptation without expensive annotation; quick check: verify generated QA pairs cover target domain concepts
- **Difficulty grading**: Categorizing generated content by complexity levels; needed to ensure model learns across the full difficulty spectrum; quick check: confirm distribution of easy/medium/hard examples in training set
- **Factual consistency filtering**: Using LLMs to validate generated content against source material; needed to reduce hallucinations and ensure reliability; quick check: measure hallucination rate in filtered vs unfiltered data
- **Statute-grounded QA**: Questions and answers derived directly from legal texts; needed for authoritative, domain-specific knowledge transfer; quick check: verify all QA pairs can be traced to source statutes
- **Domain adaptation**: Fine-tuning general LLMs for specialized knowledge domains; needed to achieve high performance on niche tasks; quick check: compare domain-specific vs general model performance
- **Open-ended correctness evaluation**: Measuring model accuracy on free-form legal questions; needed for realistic assessment of practical utility; quick check: ensure evaluation covers multiple answer formats

## Architecture Onboarding

**Component Map**
LLM -> Difficulty Grader -> Factual Filter -> Training Dataset -> Fine-tuning Engine -> Domain-Specific LLM

**Critical Path**
The core workflow flows from the base LLM through difficulty grading, factual filtering, synthetic dataset creation, and finally to fine-tuning the target model. Each stage builds upon the previous: the LLM generates candidate QA pairs, the grader categorizes them by complexity, the filter removes inconsistent or hallucinated content, and the cleaned dataset trains the specialized model.

**Design Tradeoffs**
- Synthetic vs manual annotation: Synthetic data enables scalability and eliminates annotation costs but requires robust validation to ensure quality
- Open-ended vs multiple-choice: Open-ended questions better reflect real-world usage but are harder to evaluate automatically
- Model size vs performance: Larger models may achieve better results but increase computational costs; this work uses 8B and 12B parameters as a practical compromise
- Source coverage vs specificity: Broader legal text coverage enables generalization but may dilute domain focus

**Failure Signatures**
- Hallucinations persisting despite filtering indicate inadequate validation criteria or insufficient source material
- Overfitting to synthetic patterns suggests lack of diversity in generated data or insufficient regularization
- Poor generalization to unseen statutes indicates training data coverage gaps or overly narrow generation scope
- Degradation in general language understanding suggests excessive domain specialization or imbalanced training

**First 3 Experiments**
1. Generate and evaluate a small batch of synthetic QA pairs for factual consistency and coverage against source statutes
2. Fine-tune a base LLM on the synthetic dataset and measure open-ended correctness on a held-out legal QA subset
3. Compare performance of difficulty-graded synthetic data vs uniform generation across multiple model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy dependence on quality of underlying legal texts and LLM's generation accuracy
- Absence of human-annotated gold standards for synthetic data means potential subtle errors or biases may persist
- Results specific to German law and tested model architectures (LLaMA 3.1 8B, Gemma 3 12B); generalization to other languages, legal systems, or model families untested
- Absolute performance levels and robustness under distribution shift (unseen statutes or adversarial queries) not fully characterized

## Confidence
- **High confidence** in the synthetic data generation pipeline and its technical implementation
- **Medium confidence** in the magnitude of performance gains, given controlled benchmarks and strong baselines, but tempered by unverified fidelity of synthetic data
- **Medium confidence** in practical applicability, since results limited to single jurisdiction and model scale

## Next Checks
1. Conduct human evaluation of a sample of synthetic QA pairs for factual accuracy and coverage, comparing them to manually curated legal QA datasets
2. Test model generalization by evaluating on out-of-distribution German legal texts or statutes not seen during synthetic data generation
3. Replicate experiments with additional model architectures (e.g., Mistral, Qwen) and legal domains (e.g., commercial law, administrative law) to assess scalability and robustness