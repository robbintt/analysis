---
ver: rpa2
title: Data-driven simulator of multi-animal behavior with unknown dynamics via offline
  and online reinforcement learning
arxiv_id: '2510.10451'
source_url: https://arxiv.org/abs/2510.10451
tags:
- learning
- agents
- reward
- condition
- silkmoth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of simulating multi-animal behaviors
  in biological systems where transition dynamics are unknown. The proposed AnimaRL
  framework combines deep reinforcement learning with a data-driven locomotion parameter
  estimation step, enabling agents to both imitate real trajectories and acquire rewards.
---

# Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning

## Quick Facts
- arXiv ID: 2510.10451
- Source URL: https://arxiv.org/abs/2510.10451
- Reference count: 40
- The paper addresses the challenge of simulating multi-animal behaviors in biological systems where transition dynamics are unknown.

## Executive Summary
The paper introduces AnimaRL, a data-driven framework for simulating multi-animal behaviors with unknown dynamics. The method combines deep reinforcement learning with locomotion parameter estimation to recover interpretable movement variables from trajectory data alone. It employs distance-based pseudo-rewards to align cyber-physical states and optionally includes a counterfactual prediction head for generating what-if scenarios. Evaluated across artificial agents, flies, newts, and silkmoths, the approach achieves higher trajectory fidelity and reward acquisition compared to standard imitation and RL baselines, advancing the simulation of complex real-world animal dynamics.

## Method Summary
AnimaRL combines offline and online reinforcement learning with data-driven locomotion parameter estimation. The framework estimates movement variables (damping coefficient and control amplitude) from trajectory data to define an incomplete transition model. A distance-based pseudo-reward using dynamic time warping aligns simulated trajectories with demonstrations while optimizing task rewards. The method optionally includes an adversarial counterfactual prediction head that generates plausible behavior under hypothetical experimental conditions. The approach is evaluated across multiple species with varying data characteristics, demonstrating superior performance compared to standard imitation and RL baselines.

## Key Results
- Locomotion parameters estimated from trajectories enabled accurate simulation across species, with velocity RMSE ranging from 0.008 (newts) to 0.105 (flies)
- Distance-based pseudo-rewards improved spatiotemporal alignment compared to baseline methods, particularly for larger datasets
- Counterfactual predictions successfully generated plausible behavior changes under hypothetical conditions, with 95% confidence intervals excluding zero for supported query directions
- DQDIL excelled with larger datasets while DQAAS performed better for small, high-precision datasets requiring exact action alignment

## Why This Works (Mechanism)

### Mechanism 1: Locomotion Parameter Estimation as Action Inference
The framework recovers interpretable locomotion parameters (damping coefficient `d` and control amplitude `u`) from trajectory data alone, enabling simulation without explicit dynamics models. The system identifies rest-to-motion transitions in velocity data to estimate `u` (characteristic acceleration) and `d` (ratio of onset velocity to sustained top-speed). These parameters define an incomplete transition model `v' = (1-d)v + ua·Δt` that governs simulated agent motion. The approach assumes animal locomotion can be approximated by a low-order damped velocity transition model with discrete directional thrust.

### Mechanism 2: Distance-Based Pseudo-Reward for Trajectory Alignment
DTW-based pseudo-rewards enable policies to match demonstrated spatiotemporal structure while optimizing task rewards. The shaped reward `R_t = R_touch - α·R_DTW` penalizes cumulative alignment distance between simulated and expert trajectories at each timestep. DTW handles variable-length sequences and preserves temporal ordering, which standard occupancy-matching methods ignore. The approach assumes demonstrated trajectories represent near-optimal behavior for both task completion and naturalistic movement patterns.

### Mechanism 3: Adversarial Disentanglement for Counterfactual Generation
Gradient-reversal training produces condition-invariant latent states that enable plausible behavior synthesis under hypothetical experimental conditions. A treatment-prediction head attempts to recover binary condition `c_t` from hidden representation `h_t`. During backpropagation, gradients from this loss are negated, forcing the encoder to remove condition-specific information while the head learns to recover it. At inference, flipping the condition cue generates counterfactual trajectories. The approach assumes conditions affect behavior through learnable, disentangleable latent factors rather than fundamentally altering dynamics.

## Foundational Learning

- **Deep Q-Learning from Demonstrations (DQfD):** Standard RL requires extensive exploration; biological datasets provide limited demonstrations. DQfD enables offline pre-training on expert data before online fine-tuning.
- **Dynamic Time Warping (DTW):** Trajectories have variable lengths and non-linear temporal distortions. DTW aligns sequences frame-by-frame, capturing temporal structure that fixed-metric distances miss.
- **Dueling Network Architecture:** Separates state-value V(s) from advantage A(s,a), improving policy evaluation when action-space is large relative to state-value differences.

## Architecture Onboarding

- **Component map:** Input coordinates/sensory observations -> 2 FC layers (32 units) -> GRU (32 hidden units) -> Dueling heads (V-stream + A-stream) -> Q(s,a) output
- **Critical path:** 1) Estimate locomotion parameters (`d`, `u`) from demonstration velocities (Eq. 5) 2) Pre-train Q-network offline using DTW-shaped reward + supervised loss (DQAAS) or pure pseudo-reward (DQDIL) 3) Fine-tune online in simulated environment with estimated transition model 4) (Optional) Train counterfactual head with adversarial gradient reversal
- **Design tradeoffs:** DQAAS excels with small datasets requiring precise action alignment; DQDIL better for matching global spatiotemporal structure with larger datasets; pre-training essential for sparse data/reward domains
- **Failure signatures:** High velocity RMSE (>0.07) with bimodal velocity distributions; counterfactual queries producing CIs overlapping zero; DTW improvement over BC marginal or negative
- **First 3 experiments:** 1) Parameter validation: Estimate `d` and `u` on held-out trajectories, compute velocity RMSE. Target: RMSE < 0.03 for unimodal species 2) Ablation comparison: Train DQAAS vs. DQDIL-PT vs. DQDIL on each species. Measure KDE distance to ground-truth distributions 3) Counterfactual sanity check: Train DQCIL with binary condition variable, verify directional path-length shifts matching empirical hypotheses

## Open Questions the Paper Calls Out
- Can incorporating mixture or state-switching dynamics improve locomotion parameter estimation for animals with bimodal velocity distributions?
- Does integrating partially known physics directly into the latent loss improve the accuracy of counterfactual behavior predictions?
- Can the AnimaRL framework maintain trajectory fidelity and generalizability when scaled to three-dimensional settings?

## Limitations
- Assumes unimodal velocity distributions and discrete directional thrust, limiting applicability to animals with complex gait switching
- Parameter estimation errors correlate with trajectory length and velocity distribution complexity
- Transition model treats locomotion as Markovian, ignoring biomechanical constraints like momentum or energy expenditure
- Counterfactual predictions are only valid within training distribution support; extrapolations often fail

## Confidence
- **High confidence:** Locomotion parameter recovery works for unimodal distributions (RMSE < 0.03 for newts/silkmoths)
- **Medium confidence:** DQDIL outperforms DQAAS for larger datasets with sufficient spatiotemporal coverage
- **Low confidence:** Counterfactual predictions in under-constrained directions and performance on animals with complex, multimodal locomotion patterns

## Next Checks
1. Test mixture transition models (e.g., state-switching between rest/ballistic modes) on bimodal velocity distributions to reduce RMSE below 0.07
2. Evaluate counterfactual performance on held-out experimental conditions not used during training to assess generalization
3. Compare against biomechanics-grounded simulators (e.g., musculoskeletal models) for a subset of species to quantify trade-offs between anatomical accuracy and data-driven flexibility