---
ver: rpa2
title: 'Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning
  and Phonetic-Semantic Embeddings'
arxiv_id: '2507.06506'
source_url: https://arxiv.org/abs/2507.06506
tags:
- puns
- translation
- word
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research introduces a novel three-stage approach for translating
  English puns into French, combining large language models (LLMs) with wordplay generation
  techniques. The method employs contrastive learning, phonetic-semantic embeddings,
  and a multi-agent evaluation framework to preserve humor and linguistic creativity
  rather than literal translation.
---

# Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings

## Quick Facts
- arXiv ID: 2507.06506
- Source URL: https://arxiv.org/abs/2507.06506
- Authors: Russell Taylor; Benjamin Herbert; Michael Sana
- Reference count: 30
- Primary result: First and second place in CLEF JOKER 2025 Task 2 human evaluation for pun translation, outperforming all other submissions

## Executive Summary
This research introduces a novel three-stage approach for translating English puns into French, combining large language models (LLMs) with wordplay generation techniques. The method employs contrastive learning, phonetic-semantic embeddings, and a multi-agent evaluation framework to preserve humor and linguistic creativity rather than literal translation. Evaluated through the CLEF JOKER 2025 Task 2, the approach achieved first and second place in human evaluation, with human scores of 9.27% and 7.85% for pun preservation, respectively—outperforming all other submissions. Automated metrics like BLEU and BERTScore remained low, reflecting the method's emphasis on non-literal, humor-preserving translation. Key innovations include a guided chain-of-thought pipeline and a multi-agent refinement loop, demonstrating that humor translation benefits from linguistic and cultural nuance modeling over strict word-for-word accuracy.

## Method Summary
The approach consists of three stages: (1) baseline generation with contrastive learning using LLMs to create initial translations while avoiding literal translation, (2) guided chain-of-thought generation with phonetic-semantic embeddings to find cross-lingual homonyms that preserve both meaning and sound patterns, and (3) multi-agent evaluation with iterative refinement where specialized LLM evaluators assess meaning preservation, mistranslation, emotion, and authenticity. The method uses contrastive learning to train a discriminator on synthetic pun/non-pun pairs, phonetic-semantic embeddings combining IPA-based phonetic features with FastText semantic embeddings, and a feedback loop of up to 5 iterations for refinement.

## Key Results
- First and second place in CLEF JOKER 2025 Task 2 human evaluation (9.27% and 7.85% pun preservation scores)
- BLEU scores: 3.94% and 2.45% (41st and 49th place), BERTScore: 49.36% and 48.76% (39th and 42nd place)
- Multi-agent evaluation scores: 3.71 and 3.58 (best among 15 submissions)
- Iteration 2 most frequently achieved maximum scores in refinement loop

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Learning Discriminator for Pun Validation
A discriminator trained on pun/non-pun pairs can reliably filter outputs to ensure wordplay preservation. Generate synthetic non-puns by prompting an LLM to replace pun words with contextually appropriate alternatives, creating a binary classification dataset. Several-shot prompting with 25 positive and 25 negative examples trains the discriminator to distinguish wordplay from literal text. Break condition: If the generator produces puns that are semantically valid but use wordplay mechanisms the discriminator wasn't trained on, filtering may reject valid outputs.

### Mechanism 2: Phonetic-Semantic Embedding Space for Cross-Lingual Homonym Discovery
Combining phonetic and semantic embeddings enables finding target-language words that capture both meanings of a source pun. Train phonetic embeddings using IPA representations and PanPhon articulatory features with BiLSTM encoder. Concatenate with FastText semantic embeddings (600-dim total). Search for candidate words where cosine similarity > 0.75 to both semantic input (one meaning) and phonetic input (sound pattern from second meaning). Break condition: When no cross-lingual homonym exists that satisfies both constraints, the pipeline returns no candidates—requiring fallback to literal translation or creative rewriting.

### Mechanism 3: Multi-Agent Evaluation with Iterative Refinement
Specialized evaluator agents with distinct criteria can iteratively improve pun translations through feedback loops. Four agents assess equivalence (meaning preservation), mistranslation detection, emotion matching, and authenticity. Generator receives aggregated feedback and regenerates. Process iterates up to 5 times, selecting highest-scoring output. Break condition: If evaluator agents have conflicting criteria or systematic biases, the refinement loop may converge to mediocre outputs that satisfy all agents minimally rather than optimizing for humor.

## Foundational Learning

- Concept: **Homographic vs. Homophonic Puns**
  - Why needed here: The pipeline branches based on pun type—homographic puns use identical spellings with different meanings, while homophonic puns rely on sound similarity.
  - Quick check question: Given "The bicycle couldn't stand on its own because it was two-tired"—is this homographic or homophonic, and which embedding dimension matters more for translation?

- Concept: **Script Opposition (GTVH)**
  - Why needed here: Puns work through juxtaposing incompatible frames of reference. The translation goal is preserving script opposition, not vocabulary.
  - Quick check question: If translating a pun where "bank" means both financial institution and river edge, what two scripts must the French translation preserve?

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: Guides generation through explicit reasoning steps (identify pun word → translate meanings → search embeddings → generate), preventing LLMs from defaulting to literal translation.
  - Quick check question: What happens if you prompt an LLM to "translate" versus "create a pun in French that captures the same humor"?

## Architecture Onboarding

- Component map: Input English Pun → [Stage 1] Pun Identification (LLM) → [Stage 1] Translation of Meanings (LLM) → [Stage 2] Phonetic-Semantic Search (Embeddings) → [Stage 2] Guided Generation (LLM with constraints) → [Stage 3] Multi-Agent Evaluation (4 evaluators) → Final French Pun Output
- Critical path: Stage 2's phonetic-semantic search is the bottleneck. In the majority of cases no candidate was found that met our thresholds—meaning fallback logic is exercised frequently.
- Design tradeoffs: Literal accuracy (BLEU/BERTScore) vs. functional equivalence (human evaluation): Paper explicitly prioritized the latter. Generation freedom vs. semantic anchoring: Baseline models (unconstrained) scored 1.07-3.57% on human eval; constrained approaches scored 7.85-9.27%.
- Failure signatures: Low BLEU with high human scores: Expected behavior, not failure. "Pun word not translated with appropriate homonymic gloss": Indicates over-creative baseline—pun departed too far from source meaning. Empty candidate set from embedding search: Cross-lingual homonym gap—requires polygonal expansion.
- First 3 experiments:
  1. Reproduce Stage 1 baseline: Run o4-mini with translation-avoiding prompt on 50 English puns, evaluate with contrastive learning discriminator. Expect ~99% "is_pun" classification but low semantic overlap.
  2. Validate embedding search: For 20 puns with known French homonyms, verify phonetic-semantic search retrieves correct candidates (cosine > 0.75). Measure retrieval recall.
  3. Ablate multi-agent feedback: Run Stage 3 with single evaluator vs. four-agent panel on same inputs. Compare human ratings to isolate contribution of specialized roles.

## Open Questions the Paper Calls Out

### Open Question 1
Does integrating the guided chain-of-thought pipeline (Part 2) as the input for the multi-agent evaluation framework (Part 3) significantly outperform the current state-of-the-art results? The authors note they "used the baseline o4-mini runs as inputs to Part 3" and state in Future Work: "We believe that combining our approaches from Parts 2 and 3 would produce results that would outperform both of them." The components were evaluated separately or sequentially using baseline inputs; the specific synergy between the phonetic-semantic guided generation and the multi-agent refinement loop was not tested in this study.

### Open Question 2
To what extent does implementing "hexagon" or higher-level iterations of Low's polygonal algorithm increase the yield of viable phonetic-semantic homonym candidates? The authors state their implementation "extended to the pentagon level," but acknowledge that "in the majority of cases no candidate was found that met our thresholds." They conclude: "Our approach would benefit from pursuing further levels of Low's polygonal algorithm."

### Open Question 3
How does the accuracy of pun translation change when moving from a binary classification (homographic/homophonic) to a more nuanced taxonomy of wordplay? The authors found that their "binary classification of pun types was a limiting factor" because of "ambiguous cases" and "many varied linguistic forms." They propose: "Implementing more nuanced categories... should produce improved results."

## Limitations
- Phonetic-semantic embedding approach relies on cross-lingual homonym discovery that succeeds in only a minority of cases, necessitating fallback strategies
- Multi-agent evaluation depends heavily on LLM reliability for subjective quality assessment, with no independent validation of evaluator consistency
- Contrastive learning discriminator's synthetic non-pun generation process may not adequately represent all failure modes in actual translation scenarios

## Confidence
- **High confidence**: Human evaluation results showing first and second place rankings; three-stage pipeline architecture is clearly specified
- **Medium confidence**: Phonetic-semantic embedding methodology (training details incomplete); multi-agent evaluation framework (evaluator consistency not validated)
- **Low confidence**: Exact prompt templates and discriminator training procedure (critical for reproducibility but only referenced via external links)

## Next Checks
1. **Evaluator consistency test**: Run the four evaluator agents on the same 100 pun translations multiple times to measure inter-rater reliability and detect systematic biases
2. **Phonetic-semantic coverage analysis**: Systematically catalog cases where candidate search returns empty sets versus successful retrievals, then analyze linguistic patterns in the failures
3. **Ablation study on contrastive learning**: Compare discriminator performance using the paper's synthetic non-pun dataset versus human-annotated negative examples to assess synthetic data quality