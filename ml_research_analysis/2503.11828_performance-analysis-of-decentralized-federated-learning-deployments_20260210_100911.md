---
ver: rpa2
title: Performance Analysis of Decentralized Federated Learning Deployments
arxiv_id: '2503.11828'
source_url: https://arxiv.org/abs/2503.11828
tags:
- data
- device
- linear
- non-iid
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive performance analysis of decentralized
  federated learning (DFL) under various network topologies, non-IID data distributions,
  and training strategies. The authors conduct both theoretical convergence analysis
  and extensive experimental evaluations across six DFL deployment configurations
  (continuous/aggregate linear, ring, star, and mesh) using five different models
  including traditional ML (SVM, logistic regression), deep learning (ResNet, DistilBERT),
  and a large language model (MiniGPT-4).
---

# Performance Analysis of Decentralized Federated Learning Deployments

## Quick Facts
- arXiv ID: 2503.11828
- Source URL: https://arxiv.org/abs/2503.11828
- Reference count: 20
- Primary result: DFL converges under IID conditions but degrades with non-IID data, with star/mesh topologies most robust

## Executive Summary
This paper presents a comprehensive performance analysis of decentralized federated learning (DFL) across six network topologies and five different models. The authors conduct theoretical convergence analysis for strongly convex functions and extensive experiments validating DFL behavior under both IID and non-IID data distributions. The study reveals that DFL performance is inversely proportional to non-IID data distribution severity, with deep learning models being particularly sensitive to highly imbalanced label distributions.

The experimental results demonstrate that while all models achieve high F1 scores matching baseline performance under IID conditions across all topologies, convergence becomes topology-dependent under non-IID conditions. Star and mesh topologies show superior robustness due to their concurrent training nature, while linear and ring topologies exhibit convergence only on devices with better label balance. The authors conclude that devices with incomplete label sets should be excluded in practical applications to maintain global model convergence quality.

## Method Summary
The authors conduct theoretical convergence analysis establishing that DFL models converge to global optimum under IID data conditions for strongly convex functions, but convergence rates degrade with increasing non-IID distribution. Experimental evaluations are performed across six DFL deployment configurations (continuous/aggregate linear, ring, star, and mesh) using five different models including traditional ML (SVM, logistic regression), deep learning (ResNet, DistilBERT), and a large language model (MiniGPT-4). The study systematically varies network topologies, data distribution patterns, and training strategies to assess DFL performance under different conditions.

## Key Results
- All models achieve F1 scores of 0.95-0.99 matching baseline performance under IID conditions across all topologies
- Under non-IID data, convergence becomes topology-dependent with star and mesh topologies demonstrating superior robustness
- Linear and ring topologies show convergence only on devices with better label balance under non-IID conditions
- DFL performance is inversely proportional to degree of non-IID data distribution
- Deep learning models are particularly sensitive to highly imbalanced label distributions

## Why This Works (Mechanism)
The convergence behavior in DFL systems is fundamentally governed by the interplay between network topology and data heterogeneity. In IID scenarios, the decentralized aggregation mechanisms can effectively approximate global gradient descent regardless of topology structure. However, under non-IID conditions, the topology's ability to facilitate concurrent information exchange becomes critical. Star and mesh topologies enable faster information diffusion across the network, allowing devices to compensate for local data imbalances through multi-hop communication. Linear and ring topologies, with their sequential communication patterns, create bottlenecks that prevent effective global consensus when local data distributions are highly skewed.

## Foundational Learning
- **Strongly convex functions**: Why needed - theoretical convergence guarantees; Quick check - verify function satisfies μ-strong convexity condition
- **Non-IID data distributions**: Why needed - realistic federated learning scenario; Quick check - measure label distribution skew across devices
- **Network topology effects**: Why needed - impacts convergence and robustness; Quick check - analyze diameter and connectivity metrics
- **Concurrent vs sequential training**: Why needed - affects information propagation speed; Quick check - measure communication rounds to convergence
- **Model sensitivity to data imbalance**: Why needed - determines practical deployment viability; Quick check - evaluate performance degradation with increasing skew

## Architecture Onboarding
Component map: Devices -> Local Training -> Peer Communication -> Model Aggregation -> Global Model Update

Critical path: Device training → Inter-device communication → Local model aggregation → Model update → Next training round

Design tradeoffs: Topology choice balances convergence speed vs. communication overhead; IID tolerance vs. model complexity; Theoretical guarantees vs. practical deep learning applicability

Failure signatures: Convergence stalls in linear/ring topologies under high non-IID; Star/mesh maintain progress but with reduced accuracy; Complete failure when devices have disjoint label sets

First experiments:
1. Measure convergence speed across topologies under controlled IID conditions
2. Quantify accuracy degradation as non-IID level increases
3. Test impact of device churn on convergence stability

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis limited to strongly convex functions, excluding non-convex deep learning models
- Experimental focus on F1 scores without examining model calibration, fairness, or adversarial robustness
- No evaluation of DFL performance under dynamic network conditions with client churn

## Confidence
- Theoretical convergence guarantees (strongly convex cases): High
- Experimental convergence patterns (deep learning models): Medium
- Topology-specific performance differences: High
- Non-IID sensitivity claims: Medium
- Recommendation to exclude devices with incomplete label sets: Medium

## Next Checks
1. Conduct experiments with non-convex loss functions and establish empirical convergence bounds for deep learning models in DFL settings
2. Evaluate model fairness metrics across different device groups under non-IID conditions to quantify equity impacts
3. Test DFL performance under dynamic network conditions with client churn and varying communication delays to assess real-world robustness