---
ver: rpa2
title: 'BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting'
arxiv_id: '2502.05729'
source_url: https://arxiv.org/abs/2502.05729
tags:
- speech
- audio
- speaker
- data
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BnTTS, the first Bangla speaker adaptation-based
  TTS system built on XTTS architecture. The model was pre-trained on 3.85k hours
  of Bangla speech data and fine-tuned for few-shot speaker adaptation using 4.22
  hours of studio recordings from four speakers.
---

# BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting

## Quick Facts
- arXiv ID: 2502.05729
- Source URL: https://arxiv.org/abs/2502.05729
- Reference count: 36
- First Bangla speaker adaptation-based TTS system built on XTTS architecture

## Executive Summary
BnTTS presents the first Bangla speaker adaptation-based TTS system built on the XTTS architecture. The model was pre-trained on 3.85k hours of Bangla speech data and fine-tuned for few-shot speaker adaptation using 4.22 hours of studio recordings from four speakers. Evaluations show BnTTS-n outperforms existing Bangla TTS systems, achieving SMOS scores of 4.624 and 4.601 on reference-aware and reference-independent datasets respectively, with competitive naturalness and clarity. The system demonstrates strong speaker fidelity with SECS scores of 0.548 (reference) and 0.586 (prompt), though higher CER compared to commercial systems indicates a trade-off between naturalness and transcription accuracy.

## Method Summary
BnTTS adapts the XTTS architecture for Bangla speaker adaptation through a two-stage training process. The system begins with continual pre-training on 3.85k hours of Bangla speech data, then fine-tunes using 20 minutes of studio recordings per speaker across four speakers. The architecture uses VQ-VAE discrete tokenization, a Perceiver Resampler for variable-length conditioning, and HiFi-GAN vocoder with adversarial training. The few-shot adaptation achieves speaker cloning capabilities while maintaining naturalness, though the model shows limitations with short speech generation and handling diverse dialects.

## Key Results
- BnTTS-n achieves SMOS scores of 4.624 (reference) and 4.601 (prompt), outperforming existing Bangla TTS systems
- Speaker fidelity reaches SECS scores of 0.548 (reference) and 0.586 (prompt) for few-shot adaptation
- Naturalness and clarity scores of 4.552 (reference) and 4.555 (prompt) demonstrate competitive performance
- CER of 0.034 indicates a trade-off between expressiveness and transcription accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot speaker adaptation via fine-tuning on minimal data improves speaker fidelity over zero-shot baselines.
- Mechanism: The pre-trained XTTS checkpoint is continually pre-trained on 3.85k hours of Bangla speech, then fine-tuned with 20 minutes per speaker (4 speakers total) in a multi-speaker setting for 10 epochs. This aligns the model's latent speaker representations with target voice characteristics.
- Core assumption: The pre-trained multilingual XTTS provides transferable acoustic representations that can be specialized to Bangla with limited target-domain data.
- Evidence anchors:
  - [section] "BnTTS-n outperforms BnTTS-0 in both SECS (reference) (0.548 vs 0.529) and SECS (prompt) (0.586 vs 0.576)"
  - [section] "For few-shot speaker adaptation, we fine-tuned the BnTTS model using our In-House HQ dataset... randomly selected 20 minutes of audio for each speaker"
  - [corpus] Related work A2TTS and FMSD-TTS similarly use speaker encoders with short reference samples, suggesting cross-architectural validity of few-shot conditioning approaches.
- Break condition: If target speakers have vocal traits far outside the pre-training distribution, few-shot adaptation may fail—the paper explicitly notes this limitation.

### Mechanism 2
- Claim: Two-stage training (partial → complete audio prompting) stabilizes learning of speaker-conditioned generation.
- Mechanism: Stage (a) uses random segments of ground truth audio as speaker prompts for 5 epochs; Stage (b) uses full audio for 1 epoch. This curriculum aligns inference behavior with training, particularly for duration matching.
- Core assumption: Gradual exposure to longer prompts reduces distribution shift between training and inference.
- Evidence anchors:
  - [section] "The pretraining process consists of two stages: a) Partial Audio Prompting... b) Complete Audio Prompting"
  - [section] Table 3 shows DurationEquality improves from 0.699 to 0.827 when using short prompts aligned with training strategy
  - [corpus] No direct corpus comparison available for this specific two-stage curriculum; validation remains limited to this paper.
- Break condition: If inference-time prompt duration differs substantially from training distribution, duration and intelligibility degrade.

### Mechanism 3
- Claim: Trade-off exists between naturalness and CER due to conversational prosody.
- Mechanism: BnTTS generates more expressive prosody, which improves SMOS but creates phonetic boundaries that ASR systems struggle to transcribe accurately.
- Core assumption: CER reflects ASR robustness to prosodic variation rather than pure synthesis quality.
- Evidence anchors:
  - [section] "BnTTS generates speech with more conversational prosody and expressiveness, which, while improving perceived quality, may negatively impact CER"
  - [section] CER of 0.034 (BnTTS-n) vs 0.021 (AzureTTS) vs 0.020 (GTTS)
  - [corpus] Weak direct evidence; related TTS papers do not systematically examine this prosody-CER trade-off.
- Break condition: If downstream applications require ASR-optimized speech rather than natural prosody, this approach may be unsuitable.

## Foundational Learning

- Concept: **VQ-VAE discrete tokenization**
  - Why needed here: The Audio Encoder discretizes mel-spectrograms into codebook tokens that the LLM predicts autoregressively.
  - Quick check question: Can you explain why discrete tokens enable LLM-based speech generation rather than continuous spectrogram regression?

- Concept: **Perceiver Resampler for variable-length conditioning**
  - Why needed here: Converts variable-length speaker spectrograms (L frames) into fixed-dimensional representations (P × d) for consistent LLM input.
  - Quick check question: Why is fixed-output dimensionality necessary when concatenating speaker embeddings with text tokens?

- Concept: **HiFi-GAN vocoder with adversarial training**
  - Why needed here: Converts LLM spectrogram outputs to waveforms while preserving speaker characteristics through adversarial and feature-matching losses.
  - Quick check question: What role does the Multi-Period Discriminator play in capturing speech signal characteristics?

## Architecture Onboarding

- Component map:
  - Audio Encoder (VQ-VAE): mel-spectrogram → discrete tokens → d-dim embeddings (Ye)
  - Conditioning Encoder + Perceiver Resampler: speaker mel-spectrogram → 32 fixed-length tokens (Sp)
  - Text Encoder: text tokens → d-dim embeddings (Te)
  - LLM (GPT-2 decoder, 1024 hidden): autoregressively generates spectrogram embeddings given Sp ⊕ Te ⊕ Ye
  - HiFi-GAN Decoder: spectrogram embeddings → waveform

- Critical path:
  1. Initialize from pre-trained XTTS checkpoint
  2. Continual pre-training on 3.85k hours (partial → complete audio prompting)
  3. Fine-tune HiFi-GAN separately on GPT-2 embeddings
  4. Few-shot adaptation on 20 min/speaker for 10 epochs
  5. Inference: concatenate speaker embedding + text embedding only (no ground truth Ye)

- Design tradeoffs:
  - SMOS vs. CER: optimizing for naturalness increases expressive prosody at cost of ASR accuracy
  - Prompt duration: short prompts improve short-text generation but may reduce speaker capture fidelity
  - Continual pre-training vs. scratch: resource constraints led to continual pre-training; training from scratch may yield better results (acknowledged limitation)

- Failure signatures:
  - Short text (<30 characters): generates overly long speech with reduced intelligibility under default T=0.85, TopK=50
  - Out-of-distribution voices: "struggles to adapt to speakers with unique vocal traits"
  - Dialect mismatch: "challenges in handling diverse dialects"

- First 3 experiments:
  1. **Baseline reproduction**: Load XTTS checkpoint, run zero-shot inference on BnTTSTextEval subsets, measure SMOS and CER against reported BnTTS-0 scores (4.456 SMOS, 0.081 CER overall).
  2. **Short-text ablation**: On ShortText-200 subset, compare (T=0.85, TopK=50, long prompt) vs. (T=1.0, TopK=2, short prompt). Target: replicate DurationEquality improvement from ~0.70 to ~0.83.
  3. **Few-shot adaptation sanity check**: Fine-tune on single held-out speaker with 10/20/30 minutes. Plot SECS vs. training data amount to validate 20-minute design choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training the BnTTS architecture from scratch yield superior performance in speaker fidelity and naturalness compared to the current continual pre-training approach?
- Basis in paper: [explicit] Section 7 states the authors "did not investigate the performance of the XTTS model by training from scratch" due to resource constraints, noting it "may have yielded better results."
- Why unresolved: The current methodology relies on transfer learning from a multilingual checkpoint (XTTS), which may retain biases or feature representations suboptimal for Bangla's specific phonetic characteristics compared to a native-only initialization.
- What evidence would resolve it: A controlled ablation study training the model solely on the 3.85k hours of Bangla data versus the continual pre-training method, evaluated on SMOS and Speaker Encoder Cosine Similarity (SECS).

### Open Question 2
- Question: Can knowledge distillation successfully compress BnTTS into smaller variants for real-time applications without significant degradation in few-shot adaptation capabilities?
- Basis in paper: [explicit] Section 6 explicitly lists "developing medium and small model variants, and exploring knowledge distillation to optimize inference speed for real-time applications" as future work.
- Why unresolved: The current system relies on a large GPT-2 backbone (1024 hidden size), and it is untested whether the delicate speaker adaptation mechanism survives aggressive model compression or pruning.
- What evidence would resolve it: Benchmarking distilled "small" and "medium" model variants on latency (Real-Time Factor) against the baseline SECS and naturalness scores.

### Open Question 3
- Question: Is the observed high Character Error Rate (CER) in BnTTS caused by actual intelligibility failures or an evaluation bias in ASR systems against expressive, conversational prosody?
- Basis in paper: [inferred] Section 4 notes BnTTS generates "conversational prosody... which, while improving perceived quality, may negatively impact CER" because ASR systems prefer "standardized speech patterns."
- Why unresolved: The paper relies on ASR-based CER for objective intelligibility but hypothesizes the metric penalizes the very expressiveness the model optimizes for, creating a conflict between subjective (SMOS) and objective metrics.
- What evidence would resolve it: A study comparing human phonetic transcription of BnTTS outputs against ASR transcriptions to determine if the "errors" are perceptible to humans or artifacts of ASR rigidity.

## Limitations

- Limited speaker diversity: Study conducted on only four speakers, constraining generalizability claims about speaker adaptation robustness
- Dialect handling challenges: System explicitly notes difficulties with diverse dialects despite claims of low-resource setting effectiveness
- Unclear data composition: Pseudo-labeled Phase B data composition and filtering thresholds not fully specified, affecting reproducibility

## Confidence

**High Confidence Claims:**
- BnTTS outperforms existing Bangla TTS systems on subjective metrics (SMOS, Naturalness, Clarity)
- Few-shot adaptation improves speaker fidelity over zero-shot baselines (verified through SECS comparisons)
- Two-stage training strategy improves duration alignment for short texts

**Medium Confidence Claims:**
- The trade-off between naturalness and CER is inherent to expressive prosody
- Continual pre-training on 3.85k hours Bangla data effectively adapts multilingual XTTS to Bangla
- 20 minutes per speaker is sufficient for few-shot adaptation

**Low Confidence Claims:**
- Claims about handling diverse dialects (limited to four speakers)
- Generalizability to speakers with "unique vocal traits" (explicitly noted as limitation)
- Comparison with commercial systems given their proprietary nature

## Next Checks

1. **Dataset Composition Validation**: Reconstruct the exact composition of the 3.85k hours Bangla training data by contacting authors or reverse-engineering from available open-source datasets. Verify the pseudo-labeling pipeline and filtering thresholds.

2. **Short-Text Generation Replication**: On a held-out subset of ShortText-200, systematically compare short-prompt vs. long-prompt configurations with different T and TopK settings. Validate the claimed improvement in DurationEquality from ~0.70 to ~0.83.

3. **CER Trade-off Investigation**: Design an experiment comparing BnTTS-n against a prosody-constrained version optimized for ASR accuracy. Measure both SMOS and CER to quantify the naturalness-CER trade-off and determine if ASR-specific fine-tuning could bridge this gap.