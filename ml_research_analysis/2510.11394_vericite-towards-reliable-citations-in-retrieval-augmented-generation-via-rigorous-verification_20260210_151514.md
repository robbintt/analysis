---
ver: rpa2
title: 'VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via
  Rigorous Verification'
arxiv_id: '2510.11394'
source_url: https://arxiv.org/abs/2510.11394
tags:
- answer
- citation
- generation
- citations
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of hallucinations in Retrieval-Augmented
  Generation (RAG) systems by proposing VeriCite, a framework that rigorously validates
  supporting evidence and enhances answer attribution through citations. The core
  idea is to break down the generation process into three stages: initial answer generation
  with NLI-based verification, supporting evidence selection through fine-grained
  extraction and validation, and final answer refinement that integrates verified
  statements while preserving citations.'
---

# VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification

## Quick Facts
- arXiv ID: 2510.11394
- Source URL: https://arxiv.org/abs/2510.11394
- Reference count: 40
- Primary result: VeriCite significantly improves citation quality (Citation F1 scores) while maintaining answer correctness comparable to strong baselines in RAG systems.

## Executive Summary
VeriCite addresses the challenge of hallucinations in Retrieval-Augmented Generation (RAG) systems by proposing a rigorous verification framework that enhances answer attribution through citations. The core innovation lies in decomposing the generation process into three stages: initial answer generation with NLI-based verification, supporting evidence selection through fine-grained extraction and validation, and final answer refinement that integrates verified statements while preserving citations. Experiments across five open-source LLMs and four datasets demonstrate substantial improvements in citation quality while maintaining answer correctness. Ablation studies confirm the necessity of each component, with the NLI verification module being particularly critical for citation reliability.

## Method Summary
VeriCite implements a three-stage pipeline to improve citation reliability in RAG systems. First, an initial answer is generated from retrieved passages and verified using a Natural Language Inference (NLI) model to filter unsupported claims. Second, supporting evidence is selected by assessing passage utility and extracting fine-grained evidence statements, each verified against its source passage. Third, a final refinement stage synthesizes verified content while preserving pre-assigned citations. The framework uses TRUE NLI model for verification, tested on Llama3-8B-Instruct, Qwen2.5-7B-Instruct, DeepSeek-Coder-V2, Yi-6B-Chat, and Baichuan2.5-7B-Chat models across ASQA, ELI5, HotpotQA, and MuSiQue datasets.

## Key Results
- Citation F1 scores improve significantly across all datasets compared to baselines
- Answer correctness (EM/Claim Recall) remains comparable to strong baselines
- NLI verification module is critical: removing it drops Citation F1 by ~9 points on ASQA
- Evidence selection stage provides complementary content: removing it reduces EM by ~3 points
- Performance degrades on multi-hop reasoning tasks (HotpotQA, MuSiQue) suggesting architectural limitations

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Verification with NLI-Based Filtering
- **Claim:** Decomposing generated answers into individual statements and filtering each through NLI verification removes hallucinated content while preserving supported claims.
- **Mechanism:** The framework generates an initial answer, decomposes it into statements, and applies an NLI model to verify whether each statement is entailed by its cited passages. Unsupported statements are discarded before refinement, preventing hallucinated content from reaching the final output.
- **Core assumption:** The NLI model accurately identifies entailment relationships between retrieved passages and generated statements.
- **Evidence anchors:**
  - [abstract]: "The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model"
  - [Section 3.2]: "The verification outcome sup_i is binary-valued... when the model determines that the answer statement s_i is entailed by (a combination of) the retrieved passages, sup_i is assigned True; otherwise, it returns False, indicating an unsupported statement likely containing hallucinated content"
  - [Section 5.1, Table 3]: Ablation shows removing NLI verification drops Citation F1 from 77.73 to 68.91 on ASQA

### Mechanism 2: Pre-Attributed Evidence Extraction
- **Claim:** Extracting and pre-annotating evidence from each passage before final synthesis reduces the LLM's attribution burden and improves citation accuracy.
- **Mechanism:** Rather than requiring the generator to simultaneously answer and attribute, VeriCite first extracts evidence statements from each passage, verifies them against their source, and pre-assigns citation markers. The final synthesis step only reorganizes pre-verified content with existing citations.
- **Core assumption:** LLMs are better at synthesis and reorganization than simultaneous generation and attribution.
- **Evidence anchors:**
  - [abstract]: "supporting evidence selection assesses the utility of each document and extracts useful supporting evidences"
  - [Section 3.3]: "This design fundamentally decouples attribution from generation during the summarization phase... significantly reducing demands on the LLM's citation capacity"

### Mechanism 3: Three-Stage Pipeline with Complementary Contribution
- **Claim:** Initial answer generation and supporting evidence selection provide complementary contributions that jointly improve both correctness and citation quality.
- **Mechanism:** Initial answers capture the model's holistic reasoning across all passages, while per-passage evidence extraction ensures no single relevant passage is overlooked. Both sources feed into final refinement, combining breadth (initial answer) with depth (evidence extraction).
- **Core assumption:** The two stages capture different types of useful content that neither alone would provide.
- **Evidence anchors:**
  - [Section 5.1]: "removal of either the initial answer generation stage or the supporting evidence selection stage induces a substantial decline in answer correctness... statements originating from both stages possess a complementary nature"
  - [Table 3]: Removing initial answer drops EM from 41.63 to 39.24; removing evidence selection drops EM to 38.57

## Foundational Learning

- **Natural Language Inference (NLI) for Entailment Detection**
  - **Why needed here:** The core verification module uses NLI to determine if generated statements are supported by retrieved passages. Understanding entailment vs. contradiction vs. neutrality is essential for interpreting verification outputs.
  - **Quick check question:** Given passage "The company was founded in 2010" and statement "The company has existed for over a decade (as of 2024)," what is the entailment relationship?

- **Retrieval-Augmented Generation (RAG) Pipeline Components**
  - **Why needed here:** VeriCite modifies the standard RAG pipeline by inserting verification and pre-attribution stages. Understanding the baseline (retriever → generator) clarifies where and why modifications occur.
  - **Quick check question:** In a standard RAG pipeline, what two components interact and what potential failure mode does VeriCite address?

- **Citation Evaluation Metrics (Recall, Precision, F1)**
  - **Why needed here:** The paper reports Citation F1 improvements. Understanding what citation recall (are all needed citations present?) and precision (are all cited passages actually relevant?) measure is necessary to interpret results.
  - **Quick check question:** If an answer cites 5 passages but only 3 are actually relevant, and 2 relevant passages are uncited, what are the precision and recall?

## Architecture Onboarding

- **Component map:** Retriever → Initial Answer Generator → NLI Verifier → Evidence Selection (Check → Extract → Verify → Pre-attribute) → Final Refiner
- **Critical path:** Retrieval → Initial Answer Generation → NLI Verification → Evidence Selection → Final Refinement. The NLI verification after both generation stages is critical—ablation shows Citation F1 drops ~9 points without it.
- **Design tradeoffs:**
  - NLI vs. LLM Verifier: Table 4 shows LLM-as-verifier underperforms NLI (+7 Citation F1 advantage for NLI)
  - Multi-stage vs. Single-stage: Three stages add latency but provide complementary content
  - Pre-attribution vs. Generation-time Attribution: Pre-attribution decouples citation from generation, reducing LLM burden
- **Failure signatures:**
  1. Low citation quality despite verification: NLI model may not capture domain-specific entailment patterns
  2. Degraded answer correctness on multi-hop QA: Evidence selection may fragment cross-passage reasoning
  3. Over-aggressive filtering: If NLI threshold is too strict, supported content may be discarded
- **First 3 experiments:**
  1. Reproduce ASQA results with single model: Run VeriCite with Llama3-8B-Instruct on ASQA subset (200 samples). Verify Citation F1 ~77 and EM ~41 match paper.
  2. Ablate NLI verification: Remove NLI verification from both stages (assume all statements supported). Confirm Citation F1 drops to ~69 per Table 3.
  3. Compare verifier options: Test NLI verifier vs. same-LLM verifier vs. stronger LLM verifier (if available) on 200-sample ASQA subset. Compare cost/performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the supporting evidence selection stage be refined to better handle multi-hop reasoning that requires cross-passage information integration?
- **Basis in paper:** [explicit] Page 5 notes that on multi-hop datasets (HotpotQA, MuSiQue), performance suggests the evidence selection stage "may be suboptimal for multi-hop scenarios requiring cross-passage information integration, highlighting a potential area for architectural refinement."
- **Why unresolved:** The current method evaluates passage utility and extracts evidence independently, likely failing to capture logical dependencies that span across different retrieved documents.
- **What evidence would resolve it:** A modified framework that maintains high Citation F1 while achieving superior EM Recall on multi-hop benchmarks compared to current baselines.

### Open Question 2
- **Question:** Can a single LLM be optimized to perform both generation and verification roles effectively to reduce framework complexity?
- **Basis in paper:** [inferred] Page 6 explores using the same LLM for verification but finds it detrimental compared to a dedicated NLI model. However, the authors initially frame this integration as a "promising avenue" for reducing complexity.
- **Why unresolved:** The paper demonstrates that naive self-verification (Llama3 verifier) degrades performance, but leaves open whether specific training or prompting could make a unified model viable.
- **What evidence would resolve it:** An LLM-based verifier that matches the cost and efficiency of the NLI baseline while maintaining or improving the Citation F1 scores.

### Open Question 3
- **Question:** How can the framework be adapted to prevent the degradation of answer correctness for non-factoid, long-form questions (e.g., ELI5) while maintaining high citation quality?
- **Basis in paper:** [explicit] Page 5 states that on the ELI5 dataset, VeriCite "underperforms relative to the more robust baselines in answer correctness... indicating potential limitations in its answer generation mechanism for non-factoid questions."
- **Why unresolved:** The rigorous verification process, while reducing hallucinations, may be too restrictive for complex "how/why" questions that require synthesis rather than strict entailment from a single context.
- **What evidence would resolve it:** An adaptation of VeriCite that achieves state-of-the-art Claim Recall on ELI5 without sacrificing the gains in Citation F1.

## Limitations
- Performance degrades on multi-hop reasoning tasks (HotpotQA, MuSiQue), suggesting the three-stage pipeline may fragment cross-passage reasoning needed for complex questions
- Computational overhead of running multiple NLI verifications and evidence extractions per query remains unclear without detailed latency measurements
- NLI model generalization across diverse domains is uncertain; the TRUE NLI model's performance on specialized domains isn't validated

## Confidence

- **High Confidence:** Ablation studies demonstrating the necessity of each pipeline component are well-supported by experimental data across multiple datasets
- **Medium Confidence:** The claim that pre-attribution reduces LLM attribution burden is plausible but lacks direct comparison with generation-time attribution approaches
- **Low Confidence:** The assertion that VeriCite maintains "answer correctness comparable to strong baselines" is questionable given inconsistent performance on multi-hop reasoning tasks

## Next Checks
1. **Cross-domain NLI Generalization:** Test VeriCite on a specialized domain dataset (e.g., medical or legal questions) to validate whether the TRUE NLI model maintains high accuracy for entailment verification outside general text domains.
2. **Multi-hop Reasoning Analysis:** Conduct qualitative analysis of HotpotQA/MuSiQue outputs to identify specific failure modes where evidence selection fragments reasoning chains, and test whether relaxing per-passage extraction for multi-hop queries improves correctness.
3. **Computational Cost Profiling:** Measure end-to-end latency of VeriCite pipeline versus baseline RAG on representative query sets, including detailed breakdown of time spent in each stage (NLI verification, evidence extraction, final refinement) to quantify practical deployment tradeoffs.