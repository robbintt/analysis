---
ver: rpa2
title: 'A Comedy of Estimators: On KL Regularization in RL Training of LLMs'
arxiv_id: '2512.21852'
source_url: https://arxiv.org/abs/2512.21852
tags:
- gradient
- training
- reward
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically analyzes how different implementations
  of KL regularization affect RL fine-tuning of LLMs. It identifies that widely-used
  practices, such as adding the K3 estimator to the loss or reward, introduce biased
  gradient estimates that can destabilize training or reduce performance.
---

# A Comedy of Estimators: On KL Regularization in RL Training of LLMs

## Quick Facts
- arXiv ID: 2512.21852
- Source URL: https://arxiv.org/abs/2512.21852
- Reference count: 31
- This paper identifies that widely-used KL regularization practices in RL fine-tuning introduce biased gradient estimates, and shows that using the K1 estimator in the reward leads to unbiased gradients, more stable training, and better generalization.

## Executive Summary
This work systematically analyzes how different implementations of KL regularization affect RL fine-tuning of large language models. The authors identify that adding the K3 estimator to the loss or reward introduces biased gradient estimates that can destabilize training or reduce performance. In contrast, configurations producing unbiased gradients—specifically using the K1 estimator in the reward—lead to more stable training and better generalization on both in-domain and out-of-domain tasks. The findings highlight the importance of correct gradient estimation in KL-regularized objectives, especially in on-policy settings, and show that KL regularization can also stabilize asynchronous (off-policy) RL training.

## Method Summary
The authors conduct a comprehensive analysis of KL regularization in LLM RL fine-tuning, comparing four configurations: K1/K3 estimators placed in either the reward or loss. They use RLOO with normalization for RL optimization, training on MATH problems and evaluating on in-domain (MATH500, MATH2) and out-of-domain (MMLU physics/chemistry/biology) tasks. The key implementation detail is using stop-gradient on K1 when added to the reward, which produces unbiased gradients for the reverse KL-regularized objective. They validate findings across two model sizes (Qwen2.5-7B, Llama-3.1-8B-Instruct) and multiple β values (0.05-1.0).

## Key Results
- K1-in-reward provides unbiased gradients and consistently outperforms all other configurations on both in-domain and out-of-domain tasks
- K1-in-loss introduces harmful variance through zero-mean path-wise gradients, causing training instability
- K3-in-loss provides stable training but produces biased gradients equivalent to forward KL distillation, resulting in 3-19% worse out-of-domain performance
- KL regularization stabilizes asynchronous RL training, with K1-in-reward maintaining its advantage over K3-in-loss

## Why This Works (Mechanism)

### Mechanism 1: Unbiased gradient estimation via K1-in-reward
- Claim: Adding the K1 (naïve) estimator to the reward with stop-gradient yields unbiased gradients for the sequence-level reverse KL-regularized objective.
- Mechanism: The gradient of the KL-regularized objective decomposes into a path-wise derivative (∇_θ cKL_t) and a score-function derivative (cKL_t · ∇_θ log π_θ). For K1, the path-wise derivative equals zero in expectation (Equation 13), while the score-function derivative exactly matches the true gradient (Equation 12). Stop-gradient on K1 in reward isolates the score-function term, producing unbiased updates.
- Core assumption: On-policy sampling (importance ratio ω_t = 1); the decomposition assumes autodiff cannot backprop through sampling.
- Evidence anchors:
  - [section 3.2.1]: "The expected gradient (under π_θ) of the K1 estimator when used in the reward is unbiased with respect to the reverse KL gradient."
  - [abstract]: "adding K1 to the reward provides unbiased gradients"
  - [corpus]: Weak direct support; neighbor papers discuss gradient estimation in other contexts (e.g., SAPPHIRE for variance reduction) but not KL gradient bias in LLM RL.
- Break condition: Off-policy sampling (ω_t ≠ 1) or removing stop-gradient introduces bias or additional variance.

### Mechanism 2: K1-in-loss yields zero-mean path-wise gradients that add variance
- Claim: Using K1 directly in the loss produces a path-wise gradient that is zero in expectation but adds optimization variance, causing instability.
- Mechanism: Automatic differentiation computes only the path-wise term Σ_t ∇_θ K1_t. Since ∇_θ log(π_θ/π_ref) has zero expectation under π_θ (Equation 13), this contributes no signal but adds noise. In practice, this variance destabilizes training, especially with off-policy updates or larger β values.
- Core assumption: The path-wise gradient's zero-mean property holds under on-policy sampling; off-policy settings amplify instability.
- Evidence anchors:
  - [section 4.1, Observation 1]: "A potential explanation is that the term Σ_t ∇_θ log π_θ(y_t|x, y_{<t}), despite having an expectation of 0, adds variance to the optimization, leading to instabilities."
  - [Figure 2]: Training curves show instabilities for Qwen2.5-7B with β=0.3,1 and Llama-3.1-8B-Instruct for most β values.
  - [corpus]: Weak; no direct corpus papers address this specific KL gradient variance issue.
- Break condition: Very small β or fully on-policy with few update steps may mask instability; increasing off-policyness (multiple minibatch updates) exposes it.

### Mechanism 3: K3-in-loss approximates forward KL distillation, enabling stable but suboptimal training
- Claim: K3-in-loss, though biased for reverse KL, produces stable training by implicitly optimizing a token-level forward KL distillation objective.
- Mechanism: The path-wise gradient of K3 equals Σ_t E[∇_θ KL(π_ref(·|x,y_{<t}) || π_θ(·|x,y_{<t}))] (Equation 41), which is the gradient of forward KL at each token. This matches on-policy distillation with the reference model as teacher, providing stable gradients. However, it does not optimize the intended reverse KL-regularized objective, leading to lower performance.
- Core assumption: Stability arises because forward KL gradients are well-behaved (mode-covering) rather than mode-seeking; empirical generalization gap reflects objective mismatch.
- Evidence anchors:
  - [section 4.1, Observation 3]: "This may be explained by the observation that the gradient estimate (16) in this case is a sum of unbiased gradient estimates of the forward KL divergences computed at the token level, making this configuration equivalent to a forward KL divergence based stable on-policy distillation objective."
  - [Figure 5-6]: K1-in-reward consistently outperforms K3-in-loss across in-domain and out-of-domain tasks.
  - [corpus]: Weak; corpus papers do not address forward/reverse KL gradient equivalence in this setting.
- Break condition: If the true objective were forward KL regularization, K3-in-loss would be appropriate; for reverse KL, it remains biased.

## Foundational Learning

- Concept: **Reverse vs. Forward KL Divergence**
  - Why needed here: The paper regularizes with reverse KL (mode-seeking) to keep the policy concentrated on high-reward sequences within the reference support. Forward KL (mode-covering) would preserve mass over the full reference support, hurting performance.
  - Quick check question: Given two Gaussians (reference wide, policy narrow), which KL penalizes the policy for assigning low probability where the reference has mass?

- Concept: **Path-wise vs. Score-function (REINFORCE) Gradient Estimators**
  - Why needed here: The gradient of an expectation over a parameterized distribution decomposes into these two terms. Understanding which term each KL configuration computes explains bias/variance differences.
  - Quick check question: For E_{y∼π_θ}[f(y)], write the two gradient terms and identify which requires reparameterization vs. log-derivative trick.

- Concept: **Stop-gradient Operation**
  - Why needed here: Adding KL to the reward uses stop-gradient on the KL term so autodiff does not compute the path-wise derivative; only the score-function term contributes. Without stop-gradient, both terms would contribute (potentially incorrectly).
  - Quick check question: In PyTorch, how would you implement `sg[cKL_t]` to prevent gradient flow through cKL_t while allowing flow through the advantage?

## Architecture Onboarding

- Component map:
  - Policy π_θ -> KL Estimator Module -> Advantage Estimator -> Objective Builder -> Backprop Update

- Critical path:
  1. Sample sequences y_{1:T} ∼ π_θ for each prompt x.
  2. Compute token-level log-probabilities under π_θ and π_ref (no grad through π_ref).
  3. Compute KL estimator (K1_t or K3_t) per token.
  4. Compute task reward R (verifiable: extract answer, compare to ground truth).
  5. Compute advantage A using baseline (e.g., RLOO: leave-one-out within group).
  6. If KL-in-reward: form r_t = s_t - β·sg[cKL_t]; compute policy gradient with A.
  7. If KL-in-loss: add -β·Σ cKL_t to loss; autodiff computes path-wise gradient.
  8. Backpropagate and update π_θ.

- Design tradeoffs:
  - **K1-in-reward (recommended)**: Unbiased gradients, best performance, but may have higher variance than K3-in-loss.
  - **K3-in-loss**: Stable training, lower variance, but biased gradients and ~3-19% worse out-of-domain performance.
  - **β selection**: Lower β (0.05-0.1) improves performance; higher β (0.3-1.0) hurts both configurations.
  - **Off-policy/asynchronous settings**: KL regularization stabilizes training; K1-in-reward still outperforms K3-in-loss.

- Failure signatures:
  - Training collapse or sudden accuracy drop: Likely using K3-in-reward (high bias/variance) or K1-in-loss with off-policy updates.
  - Training stable but underperforming baseline: Likely using K3-in-loss (biased) or β too high.
  - In-domain OK, out-of-domain poor: Suggests biased gradient configuration; check if using K3-in-loss.

- First 3 experiments:
  1. **Validate gradient bias in toy setting**: Replicate Section 3.3 with a parametric autoregressive model (binary sequences). Compare squared bias and variance of gradients for K1-in-reward, K1-in-loss, K3-in-reward, K3-in-loss. Confirm K1-in-reward has near-zero bias.
  2. **Ablate KL configurations on small LLM**: Fine-tune a 1B-parameter model on a verifiable task (e.g., GSM8K subset) with K1-in-reward vs. K3-in-loss at β=0.05, 0.1. Measure in-domain accuracy and out-of-domain transfer. Expect K1-in-reward to win.
  3. **Stress-test off-policy stability**: Train with async level 10 (high off-policyness) using K1-in-reward, K3-in-loss, and no KL. Confirm KL-regularized configs stabilize training; verify K1-in-reward outperforms K3-in-loss on held-out tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an unbiased sequence-level reverse KL gradient estimate be effectively implemented and analyzed in off-policy settings?
- Basis in paper: [explicit] Section 3 and Table 1 state the authors leave the "implementation and analysis of an unbiased sequence-level reverse KL gradient estimate in off-policy settings for future work."
- Why unresolved: The paper establishes that current unbiased configurations (K1-in-reward) rely on on-policy sampling, while off-policy (asynchronous) setups are preferred for efficiency.
- Evidence: A derivation of an importance sampling correction or a new estimator that maintains unbiased gradients during asynchronous RL training.

### Open Question 2
- Question: What specific theoretical or mechanistic factors explain the consistent performance gap between K1-in-reward and K3-in-loss?
- Basis in paper: [explicit] Appendix D.3 states that observed metrics (entropy, forward KL) "do not directly explain the performance differences... and further analysis needs to be carried out."
- Why unresolved: K3-in-loss provides stable training but suboptimal generalization compared to the unbiased K1-in-reward, and standard metrics failed to account for why.
- Evidence: A mechanistic analysis of the gradient directions or a theoretical study on how gradient bias affects generalization in high-dimensional policy spaces.

### Open Question 3
- Question: Do the stability and generalization benefits of unbiased gradient configurations persist when scaling to significantly larger parameter counts (e.g., >70B)?
- Basis in paper: [inferred] The paper validates its findings only on models in the 4B–8B range (Qwen2.5-7B, Llama-3.1-8B-Instruct).
- Why unresolved: Optimization dynamics, specifically training collapse and sensitivity to variance, often change non-linearly with model scale.
- Evidence: Replication of the KL estimator comparisons on 70B+ parameter models to verify if K1-in-reward remains superior or if biased estimators become necessary for stability.

## Limitations

- The theoretical analysis relies heavily on on-policy assumptions (importance ratio ω_t = 1), which may not hold in practical asynchronous or off-policy RL settings.
- Empirical validation is limited to a single task (MATH) and two model sizes, raising questions about generalizability to other domains, model scales, or reward structures.
- The claim that K1-in-reward produces unbiased gradients depends on the specific decomposition of the reverse KL gradient into path-wise and score-function terms, which may behave differently with alternative advantage estimators or reward normalization schemes.

## Confidence

**High confidence** in the claim that K1-in-reward produces unbiased gradients under on-policy sampling, supported by both theoretical derivation (Section 3.2.1) and empirical validation showing superior performance.

**Medium confidence** in the assertion that K1-in-loss introduces harmful variance through zero-mean path-wise gradients, as this relies on the assumption that autodiff will compute and propagate this variance, and the empirical evidence shows instability but doesn't definitively isolate the mechanism.

**Medium confidence** in the characterization of K3-in-loss as producing stable but biased training, as the equivalence to forward KL distillation is mathematically demonstrated but the performance gap (3-19%) may depend on specific task characteristics and reward structures.

## Next Checks

1. **Gradient bias verification in controlled setting**: Implement a synthetic autoregressive model (e.g., binary sequences with Bernoulli policies) and measure empirical bias and variance of gradients for all four KL configurations (K1-in-reward, K1-in-loss, K3-in-reward, K3-in-loss). Compare against theoretical predictions to confirm that K1-in-reward produces near-zero bias while K3-in-loss shows systematic bias toward the reference model.

2. **Cross-domain generalization study**: Fine-tune the same models on a different verifiable task (e.g., coding problems from APPS dataset) using K1-in-reward vs K3-in-loss at multiple β values. Measure both in-domain performance and transfer to unrelated domains (e.g., commonsense reasoning benchmarks) to quantify the out-of-domain generalization gap and test whether the pattern observed in MATH holds across task types.

3. **Off-policy stability stress test**: Implement a high-off-policyness setting (e.g., 10-20 update steps per sample, level 10-20 asynchronous updates) and compare training stability and final performance across K1-in-reward, K3-in-loss, and no-KL configurations. Measure gradient norms, KL divergence between π_θ and π_ref over training, and final task performance to determine whether KL regularization remains beneficial and whether K1-in-reward maintains its advantage in highly off-policy regimes.