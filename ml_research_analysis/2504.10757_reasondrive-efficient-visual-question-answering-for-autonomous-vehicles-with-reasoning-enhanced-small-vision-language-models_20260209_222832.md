---
ver: rpa2
title: 'ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with
  Reasoning-Enhanced Small Vision-Language Models'
arxiv_id: '2504.10757'
source_url: https://arxiv.org/abs/2504.10757
tags:
- driving
- reasoning
- autonomous
- fine-tuning
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether explicitly modeling reasoning during
  fine-tuning enhances vision-language models for autonomous driving tasks. The authors
  use GPT-4o to generate structured reasoning chains for driving scenarios from the
  DriveLM benchmark with category-specific prompting strategies.
---

# ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models

## Quick Facts
- arXiv ID: 2504.10757
- Source URL: https://arxiv.org/abs/2504.10757
- Reference count: 32
- Primary result: Reasoning-based fine-tuning consistently outperforms answer-only and instruction-tuned baselines, with Llama3.2-11B-reason achieving the highest performance (final score of 0.55)

## Executive Summary
This paper investigates whether explicitly modeling reasoning during fine-tuning enhances vision-language models for autonomous driving tasks. The authors use GPT-4o to generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. They compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance (final score of 0.55). Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, with accuracy scores ranging from 0.47 to 0.68 compared to 0.00 for instruction-tuned models.

## Method Summary
The paper fine-tunes small vision-language models (Llama-3.2-Vision, Llava-1.5, Qwen-2.5-VL) using LoRA with rank-8 on language layers while freezing vision and attention modules. The training dataset consists of 5,280 QA pairs from DriveLM-nuScenes with 800 frames, each containing 6 camera views. GPT-4o generates reasoning chains using category-specific prompts for four driving task categories: Perception, Prediction, Planning, and Behavior. Models are fine-tuned for 3 epochs with early stopping, using 8-bit AdamW, learning rate of 2e-4, and batch size of 1 with 12 gradient accumulation steps. Three training variants are compared: reason (with reasoning chains), simple (answers only), and instruct (baseline).

## Key Results
- Reasoning-based fine-tuning consistently outperforms alternatives across all model families and metrics
- Llama3.2-11B-reason achieves highest accuracy (0.68) and final score (0.55)
- Models fine-tuned with reasoning show substantial improvements in accuracy (0.47-0.68) compared to 0.00 for instruction-tuned models
- ChatGPT evaluation scores inversely correlate with accuracy, suggesting trade-off between fluency and correctness

## Why This Works (Mechanism)

### Mechanism 1
Explicit reasoning chains during fine-tuning improve internal representations for driving decisions. Training models to generate structured reasoning before answers creates intermediate supervision signals that guide gradient updates toward causally-ordered representations, rather than directly mapping inputs to outputs through opaque transformations. Core assumption: The reasoning chains generated by GPT-4o accurately capture valid decision logic transferable to smaller models. Evidence: Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions.

### Mechanism 2
Category-specific prompting aligns reasoning structure with task demands. Different driving tasks (Perception, Prediction, Planning, Behavior) require different reasoning depths and foci; tailored prompts constrain reasoning generation to task-relevant patterns, reducing noise in training signals. Core assumption: The four categories adequately partition driving reasoning with distinct cognitive requirements. Evidence: Our prompting strategy was tailored to each driving task category as shown in Table 1, with Perception using 1 concise sentence and Planning using 2-3 sentences with trade-off evaluation.

### Mechanism 3
Domain-specific fine-tuning is prerequisite for driving scenario understanding. Pre-trained VLMs lack grounded representations for driving-specific concepts (ego vehicle, traffic interactions, safety constraints); fine-tuning on driving QA pairs aligns model distributions with target domain vocabulary and reasoning patterns. Core assumption: The 5,280 QA pairs provide sufficient coverage of driving scenarios. Evidence: Accuracy scores ranging from 0.47 to 0.68 compared to 0.00 for instruction-tuned models demonstrate the necessity of domain adaptation.

## Foundational Learning

- **Chain-of-Thought Distillation**: Understanding how reasoning traces from teacher models can transfer to student models without requiring identical architecture. Quick check: Can you explain why training on reasoning traces differs from training on answers alone in terms of gradient signal?

- **LoRA Fine-Tuning**: Paper uses rank-8 LoRA with selective layer targeting; understanding parameter-efficient tuning is essential for reproducing results. Quick check: What layers were frozen vs. fine-tuned in the paper's configuration?

- **Multi-View Visual Fusion**: Input consists of 6 camera views (front/back/left/right variants); understanding how VLMs process multiple images is prerequisite. Quick check: How does the paper structure multi-camera inputs during training?

## Architecture Onboarding

- **Component map**: DriveLM subset → GPT-4o reasoning generation → Category-specific formatting → Training examples with 6 camera views + question + reasoning + answer → LoRA fine-tuning → Evaluation on DriveLM benchmark

- **Critical path**: 1) Generate reasoning chains with GPT-4o using category-specific prompts 2) Format training data with system prompt + question + `<think/>` reasoning + `<answer/>` tags 3) Apply LoRA fine-tuning for 3 epochs with early stopping 4) Inference requires models to output reasoning before answers

- **Design tradeoffs**: Reason vs. Simple vs. Instruct: Reason trades ChatGPT-evaluated fluency (0.56-0.62) for accuracy (0.47-0.68). Model size vs. efficiency: Qwen-3B-reason underperforms 7B variants but may suit tighter constraints. BLEU vs. semantic correctness: Llava-1.5-simple achieves high BLEU (0.55) but lower accuracy than reason variants.

- **Failure signatures**: Accuracy = 0.00 indicates no domain fine-tuning applied. ChatGPT score high but accuracy low suggests plausible-but-incorrect responses. Missing or malformed `<think/>` tags during inference indicate training configuration issues.

- **First 3 experiments**: 1) Baseline reproduction: Train Llama-3.2-11B with reason/simple/instruct configurations; verify accuracy matches reported 0.68/0.26/0.00 2) Ablation on reasoning depth: Test whether truncated reasoning (1 sentence across all categories) degrades performance vs. category-specific lengths 3) Cross-category generalization: Evaluate whether Perception-trained models can answer Planning questions, probing reasoning transfer within domain

## Open Questions the Paper Calls Out

- How does reasoning-based fine-tuning affect model robustness to out-of-distribution driving scenarios, adversarial inputs, and edge cases? All experiments used the DriveLM-nuScenes benchmark under standard conditions. No evaluation was conducted on unseen weather conditions, unusual traffic situations, corrupted inputs, or adversarially perturbed images.

- What are the appropriate safety-oriented evaluation metrics for reasoning-enhanced driving VLMs, and how do current text-based metrics correlate with actual driving safety outcomes? Current metrics (accuracy, BLEU, ROUGE, ChatGPT score) measure linguistic quality and answer matching but do not capture safety-critical distinctions.

- Does reasoning-based fine-tuning yield models that genuinely reason, or do they simply learn to generate plausible post-hoc rationalizations? The reasoning chains are generated by GPT-4o using ground truth answers as input, meaning the reasoning is constructed to justify correct answers rather than derived independently.

## Limitations

- Limited dataset scale: The 5,280 QA pairs represent a relatively small subset of DriveLM, raising concerns about potential overfitting and limited generalization to novel driving scenarios.

- Unknown interpretability of generated reasoning chains: The quality and correctness of GPT-4o-generated reasoning chains remain unverified, with no validation that learned reasoning reflects genuine causal understanding.

- Single prompt template limitation: Category-specific prompting assumes clear boundaries between Perception, Prediction, Planning, and Behavior tasks, but driving scenarios often involve overlapping cognitive demands.

## Confidence

- **High confidence**: Experimental results showing reasoning-based fine-tuning consistently outperforms answer-only and instruction-tuned baselines across multiple model families with clear quantitative differences.

- **Medium confidence**: Claim that explicit reasoning enhances internal representations is supported by performance improvements but relies on indirect evidence.

- **Low confidence**: Assumption that GPT-4o-generated reasoning chains accurately capture valid decision logic transferable to smaller models lacks independent verification.

## Next Checks

1. **Reasoning chain quality audit**: Sample 100 reasoning chains from GPT-4o and have domain experts evaluate their correctness and completeness to quantify potential negative transfer.

2. **Cross-category transfer experiment**: Train Perception-only models and test them on Planning and Prediction questions to measure whether category-specific reasoning is transferable or represents genuinely distinct reasoning patterns.

3. **Dataset scale sensitivity analysis**: Systematically reduce training set size (25%, 50%, 75%) and measure performance degradation rates for reason vs. simple vs. instruct configurations to reveal whether improvements are robust to dataset size constraints.