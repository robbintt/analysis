---
ver: rpa2
title: 'LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow
  Automation'
arxiv_id: '2510.04851'
source_url: https://arxiv.org/abs/2510.04851
tags:
- memory
- task
- agent
- legomem
- subtask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEGOMem introduces a modular procedural memory framework for multi-agent
  LLM systems, decomposing successful task trajectories into reusable memory units
  (full-task and subtask memories) that are allocated to orchestrators and task agents
  respectively. Evaluated on the OfficeBench benchmark, LEGOMem variants consistently
  improve task success rates over memory-less and baseline methods by 12.61-13.38
  percentage points, with orchestrator memory critical for planning and fine-grained
  subtask retrieval benefiting smaller agents.
---

# LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation

## Quick Facts
- arXiv ID: 2510.04851
- Source URL: https://arxiv.org/abs/2510.04851
- Reference count: 40
- Improves multi-agent workflow task success rates by 12.61-13.38 percentage points over memory-less baselines

## Executive Summary
LEGOMem introduces a modular procedural memory framework for multi-agent LLM systems that decomposes successful task trajectories into reusable memory units. The framework allocates full-task memories to orchestrators and fine-grained subtask memories to specialized task agents, enabling more effective planning and execution in workflow automation. Evaluated on the OfficeBench benchmark, LEGOMem variants consistently outperform memory-less and baseline methods, with orchestrator memory proving critical for effective task decomposition and delegation.

## Method Summary
LEGOMem operates through an offline memory curation phase followed by online inference. The system runs training tasks without memory, extracts successful trajectories, and uses an LLM to distill these into structured full-task and subtask memories. These memories are stored in FAISS vector databases indexed by task and subtask descriptions. During inference, the orchestrator retrieves top-5 full-task memories for planning while each task agent retrieves top-3 subtask memories for execution guidance. The framework includes three variants: vanilla (extract subtasks from full-task memories), Dynamic (per-agent subtask banks with just-in-time retrieval), and QueryRewrite (pre-retrieve subtasks using LLM-drafted plans).

## Key Results
- LEGOMem variants improve task success rates by 12.61-13.38 percentage points over memory-less baselines
- Orchestrator memory is critical, with orchestrator-only memory outperforming task-agent-only by 3.5-12.3 percentage points
- Fine-grained subtask retrieval benefits smaller task agents, with Dynamic and QueryRewrite variants outperforming vanilla by 4-5% on hybrid teams

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Memory Decomposition with Role-Specific Allocation
- Decompose successful trajectories into full-task and subtask memories, allocating them by agent role to reduce context noise and provide appropriate abstraction levels
- Core assumption: Agents perform better when memory granularity matches their decision-making scope
- Evidence: Orchestrator+agent memory yields strongest results; orchestrator-only memory outperforms task-agent-only by 3.5-12.3 percentage points

### Mechanism 2: Orchestrator Memory as Planning Scaffold
- Full-task memories provide orchestrators with exemplar plans, agent selection patterns, and error recovery strategies
- Core assumption: High-level planning quality constrains execution success more than execution-level guidance
- Evidence: Orchestrator-only memory achieves 47.59-53.29% success vs. 35.31-49.78% for task-agent-only memory

### Mechanism 3: Fine-Grained Subtask Retrieval Benefits Weaker Agents
- Dynamic and query-rewrite retrieval strategies help smaller task agents more by providing more relevant execution guidance
- Core assumption: Smaller models benefit more from precise, contextually relevant examples than from broader but noisier context
- Evidence: LEGOMem-Dynamic and QueryRewrite outperform vanilla by 4-5% on Hybrid teams with smaller task agents

## Foundational Learning

- Concept: Multi-agent orchestration patterns (orchestrator + specialized task agents)
  - Why needed: LEGOMem assumes a specific architecture where a central planner delegates to tool-using agents
  - Quick check: Can you explain how an orchestrator differs from a task agent in terms of decision scope and tool access?

- Concept: Retrieval-augmented generation (RAG) with vector databases
  - Why needed: Memory storage uses FAISS with dense embeddings; retrieval uses semantic similarity over task/subtask descriptions
  - Quick check: How would you index a procedural memory containing both high-level plans and step-by-step actions for efficient retrieval?

- Concept: Procedural vs. episodic memory in AI agents
  - Why needed: LEGOMem specifically targets procedural memory (how to execute workflows) not episodic memory (what happened in past conversations)
  - Quick check: What type of memory would store "always check calendar availability before creating meetings" vs. "the user mentioned they prefer morning meetings"?

## Architecture Onboarding

- Component map: Memory curation pipeline -> Structured memory storage -> FAISS vector database -> Retrieval layer (orchestrator + task agents) -> Inference loop (retrieve -> allocate -> orchestrate -> execute -> observe -> re-plan)
- Critical path: Memory curation quality -> retrieval relevance -> orchestrator planning accuracy -> task agent execution precision
- Design tradeoffs: Vanilla is simpler but may miss relevant subtask memories; Dynamic is more precise but adds latency; QueryRewrite shifts cost to planning phase
- Failure signatures: Subtask retrieval mismatch, orchestrator memory absence causing 5-10 percentage point drops, stalled progress requiring re-planning
- First 3 experiments:
  1. Replicate memory curation on 10 sample tasks: Run baseline multi-agent system, extract successful trajectories, verify LLM summarization produces parseable JSON
  2. Ablate memory placement: Test orchestrator-only, task-agent-only, and full allocation on 20 held-out tasks
  3. Compare retrieval variants on Hybrid team: Implement vanilla and one fine-grained variant, measure both success rate and execution steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-agent systems learn from failed task trajectories, not just successful ones?
- Basis: Conclusion states "Future work may explore continual learning also from failed past trajectories"
- Why unresolved: Current LEGOMem only curates memories from 93 successful trajectories out of 148 training tasks
- What evidence would resolve it: Framework that extracts actionable lessons from failed executions and demonstrates improved success rates with both positive and negative exemplars

### Open Question 2
- Question: Can LEGOMem scale to open-ended environments with dynamic tool ecosystems?
- Basis: Conclusion states "Future work may explore... scaling LEGOMem to open-ended environments and tool ecosystems"
- Why unresolved: Evaluation limited to OfficeBench with fixed APIs; tool set is static and bounded
- What evidence would resolve it: Results on benchmarks with evolving or unbounded tool spaces showing sustained performance as environment changes

### Open Question 3
- Question: Does online, continual memory updating during inference provide benefits over offline memory curation?
- Basis: Memory construction is entirely offline; the framework treats memory curation and inference as separate phases
- Why unresolved: No mechanism for agents to learn from new experiences at runtime
- What evidence would resolve it: Experiments comparing static vs. dynamically updated memory banks measuring compounding performance gains

## Limitations
- Dependency on OfficeBench's proprietary environment limits reproducibility and generalizability
- Memory curation relies on LLM-generated summaries that may introduce biases or omissions
- Relatively low memory yield (93/148 training tasks) may limit effectiveness on other datasets

## Confidence

- High Confidence: Hierarchical memory decomposition mechanism is well-supported by ablation results showing 3.5-12.3 percentage point differences
- Medium Confidence: Fine-grained subtask retrieval benefiting smaller agents is supported but context-dependent
- Low Confidence: Claim that small LLM teams with LEGOMem match or outperform larger LLM-only teams lacks strong comparative evidence

## Next Checks

1. **Domain Transferability Test**: Implement LEGOMem on a different multi-agent workflow benchmark to verify effectiveness beyond OfficeBench's simulated environment
2. **Memory Yield Analysis**: Systematically measure memory curation yield across different task types and agent configurations to identify degradation conditions
3. **Agent Size Scaling Study**: Conduct controlled experiments varying task agent sizes with and without LEGOMem to precisely quantify benefits for weaker agents