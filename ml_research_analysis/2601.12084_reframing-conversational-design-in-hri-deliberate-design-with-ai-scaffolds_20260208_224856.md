---
ver: rpa2
title: 'Reframing Conversational Design in HRI: Deliberate Design with AI Scaffolds'
arxiv_id: '2601.12084'
source_url: https://arxiv.org/abs/2601.12084
tags:
- design
- prompt
- robot
- participants
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing effective human-robot
  conversations in the era of large language models (LLMs), where generic LLM outputs
  often require context-specific adaptation that is non-intuitive and tedious to engineer.
  To overcome this, the authors present the AI-Aided Conversation Engine (ACE), a
  system that scaffolds deliberate design of robot behavior prompts.
---

# Reframing Conversational Design in HRI: Deliberate Design with AI Scaffolds

## Quick Facts
- arXiv ID: 2601.12084
- Source URL: https://arxiv.org/abs/2601.12084
- Reference count: 40
- AI-Aided Conversation Engine (ACE) enables designers to create robot behavior prompts with significantly higher clarity and specificity, leading to higher-quality human-robot conversational interactions.

## Executive Summary
Designing effective human-robot conversations with large language models is challenging because generic LLM outputs require context-specific adaptation that is non-intuitive and tedious to engineer. The authors present the AI-Aided Conversation Engine (ACE), a system that scaffolds deliberate design of robot behavior prompts through three innovations: an LLM-powered voice agent for initial prompt creation, an annotation interface for granular feedback on conversation transcripts, and LLMs to translate user feedback into prompt refinements. Evaluation through two user studies demonstrated that ACE enables designers to create robot behavior prompts with significantly higher clarity and specificity, and that prompts generated with ACE lead to higher-quality human-robot conversational interactions.

## Method Summary
ACE is a three-component system that scaffolds deliberate design of robot behavior prompts. First, an LLM-powered voice agent (gpt-4.1-mini) conducts a dialogue with users to elicit task goals, role definitions, and stylistic preferences, overcoming the "blank page problem" of open-ended prompt creation. Second, users interact with a robot using the generated prompt and annotate the conversation transcript using structured tags (liked, disliked, clear, ambiguous, etc.) and free-form comments to provide granular, grounded feedback. Third, an LLM (gpt-4o-mini) processes the annotations to generate categorized refinement suggestions and produce an updated prompt. The system was evaluated through two studies: Study 1 compared ACE to a baseline interface for prompt creation, while Study 2 evaluated the quality of interactions using ACE-generated versus manually-crafted prompts.

## Key Results
- ACE-generated prompts had significantly more descriptive words, concrete constraints, and positive examples compared to baseline prompts
- Interactions using ACE-generated prompts received significantly higher goodness-of-interaction ratings (13-item questionnaire, 0-5 scale)
- The voice-based LLM agent successfully scaffolded initial prompt creation, with participants reporting it helped them articulate design intent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A voice-based LLM agent scaffolds initial prompt creation by eliciting task goals, role definitions, and stylistic preferences through guided dialogue.
- Mechanism: The agent asks clarifying questions rather than presenting a blank text field, externalizing design reasoning and structuring user intent into concrete prompt components.
- Core assumption: Users can articulate goals verbally when prompted but struggle with open-ended text input.
- Evidence anchors: Participants with no prior prompt engineering experience reported difficulty getting started with drafting initial prompts, while the assistant co-created initial robot behavior prompts by engaging users in dialogue.

### Mechanism 2
- Claim: Transcript annotation with structured tags enables granular, grounded feedback that supports reflection on specific conversational moments.
- Mechanism: Users highlight transcript segments and apply tags plus free-form comments, anchoring abstract impressions to concrete interaction evidence.
- Core assumption: Access to interaction logs with annotation tools reduces reliance on impression-based recall.
- Evidence anchors: During the activity, participants began to provide many concrete feedback, and transcript annotation enabled participants to move from abstract impressions to granular, concrete feedback.

### Mechanism 3
- Claim: LLMs translate annotated feedback into categorized prompt refinements, reducing the skill barrier of prompt engineering.
- Mechanism: The system processes annotations, infers patterns, generates bulleted suggestions, and produces an editable refined prompt, offloading the translation from natural language feedback to technical prompt syntax.
- Core assumption: LLMs can reliably infer behavioral intent from tagged annotations and map them to prompt modifications.
- Evidence anchors: Final prompts crafted using ACE had significantly more descriptive words, concrete constraints, and positive examples, demonstrating successful translation of feedback into technical prompt improvements.

## Foundational Learning

- Concept: Prompt engineering heuristics
  - Why needed here: ACE encodes guidelines (be specific, use positive examples, add constraints) into its prompt generation; understanding these helps interpret why ACE prompts score higher on clarity and specificity.
  - Quick check question: Can you explain why "avoid vague prohibitions" and "provide positive exemplars" improve prompt effectiveness?

- Concept: Iterative design cycles with grounded evaluation
  - Why needed here: ACE replaces impression-based trial-and-error with structured reflection loops; understanding this shift clarifies the system's value proposition.
  - Quick check question: How does annotating a transcript differ from recalling "what felt wrong" after an interaction?

- Concept: Multi-modal HRI behavior generation
  - Why needed here: The robot combines LLM speech with facial expressions and head movements; prompts only control text, not timing or non-verbal channels.
  - Quick check question: If a user wants the robot to "speak slower when the child seems confused," can a text-only prompt achieve this?

## Architecture Onboarding

- Component map:
  Voice Agent (gpt-4.1-mini + Google TTS) -> Robot Backend (ROS2 + gpt-4.1-mini + Google TTS + expression bank) -> Annotation Interface (React) -> Translation Pipeline (gpt-4o-mini) -> History Store

- Critical path:
  1. User converses with voice agent → initial prompt generated
  2. Prompt sent to robot → test conversation occurs → transcript logged
  3. User annotates transcript → annotations fed to translation LLM
  4. Suggestions displayed → user reviews/edits → refined prompt produced
  5. Return to step 2 until satisfied

- Design tradeoffs:
  - Voice agent uses distinct voice from robot to avoid confusion, but adds complexity
  - System prompt appended to user prompt ensures robot compatibility but may constrain user intent
  - Suggestions are editable for user control, but increases cognitive load vs. fully automated refinement
  - Tags fixed to specific categories; extensible but may not cover all feedback types

- Failure signatures:
  - Initial prompt too vague: Voice agent failed to probe adequately; check dialogue logs for missed clarification
  - Annotations not reflected in refined prompt: Translation LLM misinterpreted tags; review suggestion-generation step
  - Robot behavior still inappropriate after refinement: Prompt changes not specific enough; check if constraints/examples were added
  - User frustrated with suggestions: Mismatch between inferred intent and actual goals; verify user edited suggestions before accepting

- First 3 experiments:
  1. Run a design session with ACE on a novel task; log time per stage and count of prompt iterations. Compare prompt clarity score to manual baseline.
  2. Intentionally provide ambiguous annotations (e.g., tag "disliked" without comments); observe how translation LLM handles missing rationale and whether suggestions remain useful.
  3. Deploy two prompts (ACE-generated vs. manually crafted) on the same robot for the same task; run a within-subjects comparison of interaction quality ratings.

## Open Questions the Paper Calls Out

- Question: Does the increased specificity achieved through deliberate design tools like ACE result in a trade-off between task appropriateness and output diversity or creativity?
  - Basis in paper: Participants observed the LLM output "felt less diverse and creative" as prompts became more detailed, with the authors noting "Future work is needed to explore this potential trade-off and prevent 'over-engineering' prompts."
  - Why unresolved: The current evaluation focused on "goodness of interaction" and task success in single sessions, rather than measuring variance, novelty, or "hallucination" rates across prolonged use.
  - What evidence would resolve it: A longitudinal study comparing ACE-generated prompts against baseline prompts using metrics for lexical diversity and creative deviation alongside adherence scores.

- Question: Can ACE sustain effective conversational design in longer-term, multi-session interactions involving domain experts as designers?
  - Basis in paper: The authors explicitly state "Future work should assess whether ACE prompts can sustain engagement in longer-term, multi-session interactions in the wild engaging domain experts as designers."
  - Why unresolved: The reported studies were limited to single, in-lab interactions with novice participants (students/community) rather than domain experts or long-term deployment.
  - What evidence would resolve it: A field study where experts (e.g., museum curators or healthcare providers) use ACE to refine robot behavior over multiple distinct sessions with end-users.

- Question: How can design tools extend scaffolding to holistic robot behaviors, such as non-verbal cues, speaking rate, and user mental state estimation?
  - Basis in paper: Participants attempted to control timing and mental state triggers via text prompts but failed because the system only scaffolded text generation. The authors conclude "Future work should investigate design tools for a more holistic design... considering multi-modal inputs and outputs."
  - Why unresolved: The current iteration of ACE isolates speech content, while effective HRI relies heavily on synchronized non-verbal behavior and state estimation which were not supported by the scaffold.
  - What evidence would resolve it: Developing an ACE variant that scaffolds multimodal rules (e.g., "if user looks confused, slow down") and testing if this improves user satisfaction compared to text-only scaffolding.

## Limitations

- The evidence linking prompt improvements to actual interaction quality gains is correlational rather than causal, as the study does not isolate whether improvements stem from better prompt engineering or from the iterative design process itself.
- The system relies on specific LLM configurations (gpt-4.1-mini, gpt-4o-mini) and Google TTS voices that may not generalize to other model families or voice synthesis approaches.
- The evaluation focuses on prompt clarity metrics and subjective interaction ratings but does not measure learning transfer - whether designers improve their prompt engineering skills through ACE use or remain dependent on the scaffolding system.

## Confidence

- High confidence: ACE demonstrably improves prompt clarity and specificity scores compared to baseline methods
- Medium confidence: The correlation between ACE-generated prompts and higher interaction quality ratings
- Medium confidence: The mechanism by which voice-based scaffolding helps users articulate design intent
- Low confidence: Generalizability of results to different robot platforms, languages, or interaction domains beyond educational contexts

## Next Checks

1. Conduct an ablation study removing the voice agent component while keeping annotation and translation features to isolate which ACE component drives the largest improvements in prompt quality
2. Test ACE with non-English prompts and evaluate whether the annotation-to-suggestion translation maintains accuracy across languages
3. Implement a longitudinal study tracking designer skill development over multiple ACE sessions to determine if users internalize prompt engineering heuristics or remain dependent on scaffolding