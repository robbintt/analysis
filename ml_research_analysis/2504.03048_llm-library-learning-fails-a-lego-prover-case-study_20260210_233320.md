---
ver: rpa2
title: 'LLM Library Learning Fails: A LEGO-Prover Case Study'
arxiv_id: '2504.03048'
source_url: https://arxiv.org/abs/2504.03048
tags:
- lego-prover
- lemmas
- proof
- learning
- library
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates whether library learning\u2014an approach\
  \ where large language models (LLMs) automatically create, store, and reuse composable\
  \ tools and knowledge\u2014actually works as claimed. The authors conduct a detailed\
  \ case study on LEGO-Prover, a system designed to generate reusable lemmas for mathematical\
  \ theorem proving."
---

# LLM Library Learning Fails: A LEGO-Prover Case Study

## Quick Facts
- arXiv ID: 2504.03048
- Source URL: https://arxiv.org/abs/2504.03048
- Reference count: 35
- Primary result: LEGO-Prover shows no evidence of lemma reuse; performance gains disappear when compute is properly normalized

## Executive Summary
This study investigates whether large language models (LLMs) can successfully learn and reuse composable tools and knowledge (library learning) by analyzing LEGO-Prover, a system designed to generate reusable lemmas for mathematical theorem proving. Through systematic behavioral analysis, the authors find no evidence that LEGO-Prover reuses learned lemmas across different problems. The apparent performance improvements over baselines stem from unaccounted inference-time compute scaling rather than actual library learning behavior. These findings challenge the effectiveness of current LLM-based library learning systems and suggest that improvements often come from increased inference-time compute rather than from actual tool reuse.

## Method Summary
The study evaluates LEGO-Prover against a Draft-Sketch-Prove (DSP) baseline on the miniF2F test set, using GPT-4o-mini, GPT-4o, and o3-mini models. LEGO-Prover employs parallel EVOLVER processes (8) to create and evolve lemmas while PROVER processes (3) decompose, formalize, and verify proofs using retrieved lemmas. The baseline uses 11 parallel processes with temperature 0.0. Both systems use Isabelle verifier with 120s timeout. The evaluation measures accuracy, direct lemma reuse (verbatim/name appearance), soft reuse (modified Levenshtein similarity), and computes compute-normalized comparisons using token counts as GPU proxy. Three trials per configuration were run with 100 attempts (full) or 50 (subsets).

## Key Results
- LEGO-Prover demonstrates zero direct lemma reuse across 189 successful proofs
- Soft-reuse survival curves show no significant difference between retrieved and non-retrieved lemmas
- Performance advantages vanish when computational costs are properly normalized (5.84x-14.23x more compute)
- EVOLVER-generated lemmas are task-specific and fail to generalize across problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEGO-Prover's apparent performance gains over baselines stem from unaccounted inference-time compute scaling, not from library learning behavior.
- Mechanism: The system runs parallel EVOLVER processes (8 by default vs. 3 PROVER processes) and performs multi-stage operations (decomposition + formalization), consuming approximately 6-14x more compute per attempt than the Draft-Sketch-Prove baseline. When equalized, the baseline matches or outperforms.
- Core assumption: Cost/token usage serves as a valid proxy for GPU compute comparison.
- Evidence anchors:
  - [abstract] "the improvements in task accuracy vanish once computational cost is accounted for"
  - [section 5, Table 4] GPU budget ratios: 5.84x (4o-mini), 5.94x (4o), 14.23x (o3-mini)
  - [corpus] Related agentic AI work (Kapoor et al.) finds planning/reflection offers no improvement over trivial baselines on HumanEval

### Mechanism 2
- Claim: LLMs generate task-specific lemmas that fail to generalize across problems, preventing cumulative library value.
- Mechanism: The EVOLVER creates lemmas tailored to originating proof structures but doesn't produce abstractions transferable to different problem contexts. Retrieval returns semantically similar but non-reapplicable lemmas.
- Core assumption: Transfer learning requires lemmas with cross-task applicability, not merely single-task relevance.
- Evidence anchors:
  - [section 4.3, Table 3] "only one instance of name reuse in 189 successful proofs generated by 4o-mini"; manual inspection revealed this was in erroneous code later corrected by heuristics
  - [section 4.3] Soft reuse curves "within one standard deviation of 0% of tasks by the 70% soft-reuse threshold"
  - [corpus] Weak—neighbor papers (Lemmanaid, MathlibLemma) focus on lemma generation but don't report reuse failure rates

### Mechanism 3
- Claim: Retrieved lemmas assist single-task solutions but don't accumulate into reusable knowledge—demonstrating use without reuse.
- Mechanism: Soft-use analysis shows retrieved lemmas have higher similarity to solutions than non-retrieved lemmas (evidence of use), but survival curves for reuse (matching ≥2 solutions) show no significant difference from baselines.
- Core assumption: Soft-use score (modified Levenshtein distance with insertion weight=0) captures meaningful lemma-solution relationships.
- Evidence anchors:
  - [section 4.2] Retrieved lemmas "maintain a larger population than the non-retrieved lemmas" at high similarity thresholds
  - [figure 1, section 4.3] Bottom-row reuse curves show "no significant difference between retrieved and non-retrieved lemmas"
  - [corpus] Berlot-Attwell et al. (NeurIPS'24 workshop) found similar single-use patterns in LEGO-Prover and TroVE

## Foundational Learning

- Concept: **Inference-Time Compute Scaling**
  - Why needed here: Apparent algorithmic improvements can mask hidden compute increases; evaluating systems requires total token accounting across all sub-processes.
  - Quick check question: When comparing two AI systems, have you summed tokens from ALL parallel processes, not just the main loop?

- Concept: **Behavioral Analysis vs. Black-Box Evaluation**
  - Why needed here: Task accuracy alone cannot verify whether claimed mechanisms (e.g., tool reuse) actually occur—internal behavior must be inspected.
  - Quick check question: Beyond measuring accuracy, can you point to evidence that your system's hypothesized mechanism is actually occurring?

- Concept: **Library Learning = Creation + Storage + Retrieval + Reuse**
  - Why needed here: This paper shows creation and retrieval work, but reuse fails—all four components are necessary for true library learning.
  - Quick check question: Does your evaluation measure whether created tools appear in ≥2 distinct tasks, or only whether they're retrieved?

## Architecture Onboarding

- Component map:
  - Task enters pending queue -> PROVER retrieves relevant lemmas -> PROVER decomposes → formalizes → attempts verification -> Simultaneously: EVOLVER proves/evolves lemmas, populating database -> Cycle repeats until success or max attempts

- Critical path:
  1. Task enters pending queue
  2. PROVER retrieves relevant lemmas from database
  3. PROVER decomposes → formalizes → attempts verification
  4. Simultaneously: EVOLVER proves/evolves lemmas, populating database
  5. Cycle repeats until success or max attempts

- Design tradeoffs:
  - More EVOLVER processes → more lemma creation but 6-14x compute overhead
  - Larger library → better retrieval candidates (in theory) but no evidence it improves reuse
  - Multi-stage decomposition → may help single-task accuracy but adds latency

- Failure signatures:
  - Lemmas never appear in ≥2 successful proofs (direct reuse ≈ 0)
  - Soft-reuse survival curve reaches zero while soft-use remains 30-40%
  - Performance advantage vanishes with compute-normalized baseline

- First 3 experiments:
  1. **Compute-normalized baseline**: Run Draft-Sketch-Prove with 6x more attempts; if DSP ≥ LP, gains are compute-driven
  2. **Reuse audit**: Log all lemma retrievals; check if any lemma appears in ≥2 successful proofs; expected: near-zero
  3. **EVOLVER ablation**: Disable lemma creation entirely; if performance doesn't significantly drop, library provides no cumulative benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do performance gains in other LLM-based library learning systems persist when compared against baselines that are strictly controlled for inference-time compute budget?
- Basis in paper: [explicit] The authors state, "We are unaware of any LLM-based library learning work that controls for compute in its evaluation... It is possible that the observed benefits of library learning are simply due to a hidden form of inference-time scaling."
- Why unresolved: This study only analyzes LEGO-Prover; other systems like TroVE or DynaSaur have not yet been evaluated under equal computational constraints.
- What evidence would resolve it: Re-evaluating existing library learning systems using compute-matched baselines (e.g., repeated sampling) to see if accuracy improvements vanish.

### Open Question 2
- Question: Can library learning be achieved effectively using fine-tuning or reinforcement learning rather than in-context learning?
- Basis in paper: [explicit] The paper concludes, "If we instead wish to use LLMs directly for library learning then, as context learning is not enough, it may be worthwhile to explore finetuning or RL (e.g., via GRPO...)."
- Why unresolved: The study demonstrates that the specific mechanism of in-context learning used by LEGO-Prover fails to yield reusable tools, but alternative training paradigms remain untested.
- What evidence would resolve it: Implementation of a library learning system using RL or fine-tuning that demonstrates successful lemma reuse and superior performance over compute-matched baselines.

### Open Question 3
- Question: Is the failure of current systems due to an inability to generate general theorems, and would providing human-curated libraries improve performance?
- Basis in paper: [explicit] In Related Work, the authors hypothesize that "these systems may be held back by an inability to reliably produce general theorems and would improve in performance if given human-created libraries."
- Why unresolved: The paper confirms that LLMs fail to *create* reusable libraries, but does not isolate whether the bottleneck is the generation of general lemmas or the retrieval/usage of them.
- What evidence would resolve it: An experiment where the system is provided with a ground-truth library of human-written lemmas to determine if the model can successfully utilize them.

### Open Question 4
- Question: Does "soft reuse" occur via structural imitation or semantic paraphrasing that is invisible to token-based similarity metrics?
- Basis in paper: [inferred] The limitations section notes the soft use score relies on token similarity and states, "It remains possible that there are other forms of soft reuse (e.g., ones that involves substantial paraphrasing or merely imitate the general structure of the lemma) that our soft use score would not detect."
- Why unresolved: The paper uses a Levenshtein-based metric to rule out soft reuse, but this method may miss abstract forms of structural transfer.
- What evidence would resolve it: A manual analysis of generated proofs or the development of semantic/structural similarity metrics to detect high-level transfer of reasoning patterns.

## Limitations

- **Compute proxy validity**: The study uses token counts as a proxy for GPU compute, which may not perfectly capture actual computational costs across different model configurations and parallel processes.
- **Evaluation scope**: Results are based on a single benchmark (miniF2F) and specific theorem-proving setup, potentially limiting generalizability to other domains.
- **Retrieval mechanism limitations**: The text-embedding-ada-002 retrieval method may not optimally capture mathematical semantic similarity, potentially affecting reuse patterns.

## Confidence

- **High confidence**: The core finding that LEGO-Prover shows no direct lemma reuse (0% in 189 successful proofs) and that performance advantages disappear when compute is normalized. These results are directly measurable and statistically clear.
- **Medium confidence**: The claim that EVOLVER-generated lemmas are task-specific rather than generalizable. While the soft-reuse curves support this, alternative explanations (retrieval ranking, embedding quality) cannot be fully ruled out.
- **Low confidence**: That all library learning approaches are fundamentally flawed. This study examines one specific implementation; other designs with different retrieval mechanisms, lemma representations, or training procedures might succeed where LEGO-Prover fails.

## Next Checks

1. **Cross-benchmark validation**: Test LEGO-Prover on additional formal mathematics benchmarks (e.g., PISA, Lean Forward) to determine if reuse failure is specific to miniF2F or represents a general limitation.
2. **Alternative retrieval analysis**: Replace the embedding-based retrieval with formal structure matching or symbolic similarity measures to test whether better retrieval could enable lemma reuse.
3. **Multi-task training evaluation**: Train EVOLVER on diverse lemma collections before deployment to test whether curriculum-style preparation enables creation of more generalizable lemmas.