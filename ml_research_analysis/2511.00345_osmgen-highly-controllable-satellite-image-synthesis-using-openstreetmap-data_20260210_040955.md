---
ver: rpa2
title: 'OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap
  Data'
arxiv_id: '2511.00345'
source_url: https://arxiv.org/abs/2511.00345
tags:
- image
- diffusion
- data
- generation
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OSMGen, a novel generative framework that
  synthesizes high-fidelity satellite imagery directly from raw OpenStreetMap (OSM)
  JSON data. By conditioning on vector geometries, semantic tags, location, and time,
  OSMGen achieves fine-grained control over scene generation, surpassing prior approaches
  that rely on raster tiles or bounding boxes.
---

# OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data

## Quick Facts
- arXiv ID: 2511.00345
- Source URL: https://arxiv.org/abs/2511.00345
- Reference count: 40
- Authors: Amir Ziashahabi; Narges Ghasemi; Sajjad Shahabi; John Krumm; Salman Avestimehr; Cyrus Shahabi
- One-line primary result: Novel generative framework that synthesizes high-fidelity satellite imagery directly from raw OpenStreetMap (OSM) JSON data with fine-grained control over scene generation.

## Executive Summary
This paper introduces OSMGen, a novel generative framework that synthesizes high-fidelity satellite imagery directly from raw OpenStreetMap (OSM) JSON data. By conditioning on vector geometries, semantic tags, location, and time, OSMGen achieves fine-grained control over scene generation, surpassing prior approaches that rely on raster tiles or bounding boxes. The framework leverages a diffusion model augmented with ControlNet to enforce geometric fidelity and can generate consistent before-after image pairs by editing OSM inputs, preserving scene coherence outside the modified regions.

## Method Summary
OSMGen takes raw OSM JSON as input and generates 256×256 satellite tiles conditioned on two segmentation masks (general and specific), location, time, and text prompts. The model uses a frozen Stable Diffusion U-Net with a trainable ControlNet branch that enforces geometric fidelity from the fused masks. Spatial and temporal embeddings are injected into the timestep embedding, while text embeddings are injected via cross-attention. Training uses diffusion loss over ~20,000 FMoW points for 500 epochs. DDIM inversion enables consistent before/after pair generation by editing OSM inputs and re-denoising with modified conditions.

## Key Results
- Synthesizes high-fidelity satellite imagery directly from raw OSM JSON with fine-grained control
- Achieves geometric fidelity through ControlNet conditioning on vector-derived masks
- Generates consistent before-after image pairs via DDIM inversion while preserving scene coherence outside modified regions

## Why This Works (Mechanism)

### Mechanism 1: Vector-to-Mask Conditioning via ControlNet
Converting OSM JSON vector geometries directly into segmentation masks preserves spatial precision that raster tile approaches lose. The pipeline extracts two complementary mask types from OSM JSON—general masks (broad categories like roads, water, buildings) and specific masks (fine-grained POI subtypes like storage tanks, stadiums)—fuses them via a learned convolutional layer, and feeds the result to a trainable ControlNet branch. ControlNet adds control features to the frozen Stable Diffusion U-Net at each block, enforcing geometric fidelity without retraining the base model.

### Mechanism 2: Multi-Modal Conditioning Fusion at the Timestep Embedding
Injecting location, time, and text embeddings into the diffusion timestep embedding provides complementary control signals that influence global appearance without altering spatial layout. SatCLIP encodes latitude/longitude into a location embedding; Date2Vec encodes capture timestamps; a frozen CLIP encoder produces text embeddings from generated prompts. Each passes through a learned linear projection and is summed into the timestep embedding at every denoising iteration. This allows the model to modulate texture, seasonality, and semantic emphasis while the mask-driven ControlNet governs geometry.

### Mechanism 3: DDIM Inversion for Consistent Before/After Pair Generation
The deterministic DDIM sampling process enables inversion to recover the latent corresponding to a generated image, allowing targeted edits while preserving unmodified regions. After generating a "before" image, DDIM inversion runs the deterministic forward process to obtain a latent code. To edit, the latent is re-denoised using modified conditions (e.g., an edited mask) but the same noise path. Strong mask conditioning permits lower classifier-free guidance (CFG), which mitigates DDIM inversion's known instability at high CFG scales.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPM)**: OSMGen builds on latent diffusion; understanding the forward/reverse process, noise schedules, and the role of the denoising network is prerequisite to comprehending both generation and inversion.
  - Quick check question: Can you explain why the reverse process in DDPM is learned rather than analytically derived?

- **Denoising Diffusion Implicit Models (DDIM)**: DDIM's deterministic sampling is the mechanism enabling inversion; without this property, the paper's before/after consistency would not be achievable training-free.
  - Quick check question: What is the key difference between DDPM and DDIM sampling that makes DDIM invertible?

- **ControlNet Architecture**: ControlNet is the conditioning backbone; understanding zero convolution, trainable/frozen branch separation, and feature map addition explains how geometric control is achieved without modifying the pretrained U-Net.
  - Quick check question: Why does ControlNet use zero-initialized convolution layers to connect to the base model?

## Architecture Onboarding

- **Component map**: Raw OSM JSON -> Preprocessing pipeline (extracts: general mask, specific mask, lat/lon, timestamp, text prompt) -> Mask fusion MLP -> ControlNet -> Frozen Stable Diffusion U-Net + trainable ControlNet branch -> Spatial/temporal embeddings injected at timestep embedding, text embeddings via cross-attention -> Generated 256×256 satellite tile -> DDIM inversion (forward to latent) -> Modify conditions -> DDIM denoising (reverse to edited image)

- **Critical path**: The mask -> ControlNet -> U-Net pathway is the primary determinant of geometric fidelity. If this path is misconfigured (wrong mask resolution, improper fusion, or ControlNet not training), spatial accuracy collapses.

- **Design tradeoffs**:
  - Two-mask system (general vs. specific) trades simplicity against granularity; more specific masks improve rare POI rendering but increase memory and complexity.
  - Lower CFG stabilizes DDIM inversion but may reduce prompt adherence; the paper relies on strong mask conditioning to compensate.
  - Inversion depth t* controls edit strength vs. fidelity; higher t* enables larger changes but risks inconsistency in preserved regions.

- **Failure signatures**:
  - **Geometric drift**: Generated roads/buildings do not match mask contours -> check mask preprocessing and ControlNet training convergence.
  - **Inconsistent before/after**: Unedited regions change noticeably -> reduce CFG or inversion depth t*; verify identical seeds/noise paths.
  - **Seasonal/location hallucination**: Wrong vegetation or building style for given lat/lon -> inspect SatCLIP/Date2Vec projection training; may need broader geographic coverage in training data.

- **First 3 experiments**:
  1. **Mask ablation**: Generate images using only the general mask vs. only the specific mask vs. both fused. Compare geometric accuracy and POI specificity on held-out locations.
  2. **Inversion depth sweep**: For a fixed edit (e.g., add a building), vary t* from low to high and measure consistency (SSIM/PSIM) in unedited regions vs. edit magnitude.
  3. **Temporal conditioning isolation**: Fix location and masks; generate images across 4 seasonal dates. Qualitatively assess whether only color/texture changes (not geometry).

## Open Questions the Paper Calls Out

### Open Question 1
Can models trained on OSMGen synthetic before/after pairs accurately propose structured OSM JSON updates from real satellite imagery? While the paper demonstrates generating consistent before/after pairs from JSON edits, the inverse direction—training a model to detect real satellite changes and output structured OSM updates—remains unimplemented and unevaluated.

### Open Question 2
How effective are synthetic OSMGen images for training downstream segmentation and change detection models compared to real data? The paper claims OSMGen "makes it possible to generate training data that addresses scarcity and class imbalance" and lists "building footprint segmentation" and "land-use classification" as target downstream tasks, but provides no quantitative evaluation demonstrating that models trained on synthetic OSMGen data achieve comparable performance to those trained on real labeled data.

### Open Question 3
What quantitative metrics capture the consistency between original and unmodified regions in generated before/after pairs? The paper claims DDIM inversion produces "consistent before/after image pairs" where "regions outside the edited area remain consistent," but provides only visual examples without quantitative fidelity measurements.

### Open Question 4
How well does OSMGen generalize to geographic regions, urban layouts, or land cover types underrepresented in the FMoW training sample? The model is trained on approximately 20,000 FMoW points, which may have geographic biases. SatCLIP location embeddings are used but their effectiveness for generalization is not evaluated.

## Limitations

- The effectiveness of ControlNet conditioning on fused segmentation masks lacks rigorous quantification against baselines.
- SatCLIP and Date2Vec embeddings' generalization to unseen geographies or temporal contexts is untested.
- DDIM inversion mechanism for controlled edits is theoretically sound but unproven in practice, particularly at higher inversion depths.
- The model's ability to handle incomplete or noisy OSM geometries—a common real-world scenario—is not discussed or tested.

## Confidence

- **High confidence**: The feasibility of using OSM JSON as input to a diffusion model via mask conditioning is supported by the described architecture and preprocessing pipeline.
- **Medium confidence**: The claim that fused general and specific masks improve both geometric fidelity and POI specificity is plausible but lacks ablation studies to isolate the contribution of each mask type.
- **Low confidence**: The assertion that DDIM inversion enables reliable, training-free before/after pair generation is theoretically grounded but unproven in practice, particularly at higher inversion depths or with significant edits.

## Next Checks

1. **Mask ablation study**: Systematically compare generated imagery using only the general mask, only the specific mask, and the fused mask across held-out locations to quantify the trade-off between geometric accuracy and POI specificity.

2. **DDIM inversion stability sweep**: For a fixed edit (e.g., adding a building), vary the inversion depth t* and measure consistency in unedited regions (using metrics like SSIM or PSIM) while tracking edit magnitude. Identify the depth threshold beyond which consistency degrades.

3. **Temporal conditioning robustness test**: Generate imagery for the same location and mask set across a range of dates spanning multiple seasons. Quantitatively assess whether only texture and color change (not geometry), and test model behavior on out-of-distribution temporal inputs (e.g., southern hemisphere dates with northern hemisphere training data).