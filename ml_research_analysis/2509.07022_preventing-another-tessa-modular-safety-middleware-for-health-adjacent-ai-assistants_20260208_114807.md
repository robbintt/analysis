---
ver: rpa2
title: 'Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI
  Assistants'
arxiv_id: '2509.07022'
source_url: https://arxiv.org/abs/2509.07022
tags:
- tessa
- safety
- prompts
- chatbot
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Tessa chatbot failure showed that absence of explicit safety
  checks can lead to harmful AI outputs in health-adjacent contexts. We propose a
  modular safety middleware combining deterministic lexical gates with an in-line
  LLM policy filter that emits a strict JSON verdict.
---

# Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants

## Quick Facts
- arXiv ID: 2509.07022
- Source URL: https://arxiv.org/abs/2509.07022
- Reference count: 3
- One-line primary result: Perfect blocking of unsafe health-adjacent content using modular safety middleware with minimal latency overhead

## Executive Summary
The Tessa chatbot failure highlighted the risks of AI assistants lacking explicit safety checks in health-adjacent domains. This paper proposes a modular safety middleware architecture combining deterministic lexical gates with an in-line LLM policy filter that emits a strict JSON verdict. On a synthetic dataset of 50 malicious and 50 safe prompts, the approach achieved perfect blocking of unsafe content while maintaining near-baseline latency and token costs. The design demonstrates that explicit, testable safety mechanisms at the last mile are sufficient to prevent harmful outputs without requiring heavyweight infrastructure.

## Method Summary
The middleware architecture uses a three-layer approach: deterministic lexical gates intercept obvious policy violations with near-zero latency before model access; an in-line LLM policy filter generates both the answer and a JSON verdict in a single call, failing closed if the verdict indicates unsafe content; and post-generation deterministic scans provide final backstop verification. The system was evaluated on a synthetic dataset of 100 prompts (50 malicious, 50 safe) using Google Gemma 2 (2B) for both generation and judging with different prompting profiles. The Single-Call JSON Mode method achieved perfect malicious-blocking recall with only 1.2× time overhead and 1.5× token overhead versus baseline.

## Key Results
- Perfect blocking of all 50 malicious prompts in synthetic evaluation
- Maintained near-baseline latency (1.2× time overhead) and token costs (1.5×)
- Deterministic lexical gates provided fast pre-filtering with identified precision-recall trade-offs
- Post-generation deterministic scans added robust final verification layer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic lexical gates intercept obvious policy violations with near-zero latency before reaching the model.
- Mechanism: A denylist of domain-specific patterns (e.g., `\b(deficit|restrict|weigh[- ]?in|BMI)\b`, calorie numeric patterns) blocks or routes high-risk requests upstream. Matching triggers immediate refusal or escalation, bypassing generation entirely.
- Core assumption: Harmful queries in this domain frequently contain surface-level lexical markers that are enumerable and stable across phrasings.
- Evidence anchors:
  - [abstract]: "deterministic lexical gates" cited as one of the two core components of the hybrid middleware.
  - [Proposed Safety Module Design → Input Filtering]: Reports "low latency and high explainability" but notes a "precision–recall trade-off" with potential for false positives/negatives.
  - [corpus]: Weak direct corpus support; neighbor papers focus on contextual and multi-turn safety, not lexical fast paths. Explicit note: corpus does not validate the efficacy of lexical gates for health-adjacent assistants.
- Break condition: Adversarial or obfuscated phrasing (e.g., leetspeak, indirect frames) evades regex; over-strict patterns harm legitimate queries.

### Mechanism 2
- Claim: An in-line LLM policy filter that emits a strict JSON verdict can enforce safety without a separate model call, reducing latency and cost.
- Mechanism: The model is instructed to generate the answer first, then append a JSON verdict (`is_safe`, `violations`). At the API boundary, only the trailing JSON is parsed. If `is_safe=false` or parsing fails, the buffered answer is discarded (fail-closed).
- Core assumption: The same model can both generate and reliably self-adjudicate under a clear policy rubric; verdict ordering after the answer provides full autoregressive context.
- Evidence anchors:
  - [abstract]: "in-line LLM policy filter that emits a strict JSON verdict" and "perfect blocking of unsafe content while maintaining near-baseline latency and token costs."
  - [Results → Table 3]: Method F (Single-Call JSON Verdict) blocks 50/50 malicious prompts with 1.2× time overhead and 1.5× token overhead vs. 1.0× baseline.
  - [corpus]: Neighbor papers (RedDebate, Qwen3Guard) discuss multi-agent debates and guardrails but do not specifically validate single-call JSON self-adjudication; corpus support is weak.
- Break condition: Correlated failures when generator and judge share the same model family; ambiguous or malformed JSON; policy drift across sessions.

### Mechanism 3
- Claim: Post-generation deterministic scans provide a final safety backstop independent of model judgments.
- Mechanism: After generation and LLM adjudication, a lexical/numeric scanner searches for prohibited content markers (calorie counts, weigh-ins, BMI targets, dieting frames). Content is rendered only if all checks return SAFE; any UNSAFE/UNCERTAIN triggers refusal or escalation.
- Core assumption: Some unsafe content will evade LLM adjudication but remain detectable via surface patterns; deterministic checks are robust to model uncertainty.
- Evidence anchors:
  - [Proposed Safety Module Design → Output Moderation]: "Determinism at delivery mandates a final numeric or lexical re-scan of the buffered output, even in cases where `is_safe=true`."
  - [Methodology → Deployment Patterns]: Output Judge (E) and combined D+E both achieve 50/50 blocking, suggesting post-hoc checks close residual gaps.
  - [corpus]: Neighbor paper "Do You Feel Comfortable?" discusses hidden escalation and affective drift, but not deterministic post-generation scans. Explicit note: no direct corpus validation.
- Break condition: Novel euphemisms or context-dependent harms without surface markers; scans may also produce false positives.

## Foundational Learning

- Concept: **Fail-closed systems**
  - Why needed here: The middleware must default to refusal when verdicts are ambiguous or unparsable to prevent unreviewed content from reaching users in a high-risk domain.
  - Quick check question: If the JSON verdict is malformed, should the system display the buffered answer or trigger refusal?

- Concept: **LLM-as-a-judge pattern**
  - Why needed here: The paper relies on an LLM policy filter to adjudicate both inputs and outputs; understanding prompt rubrics, confidence thresholds, and correlated failures is essential.
  - Quick check question: Why does the paper recommend using a different model family for adjudication than for generation?

- Concept: **Precision–recall trade-off in safety filters**
  - Why needed here: Deterministic lexical gates and classifier thresholds balance over-blocking (user friction) against under-blocking (harm); threshold calibration is context-dependent.
  - Quick check question: If a lexical gate blocks 52% of malicious prompts but also flags 15% of safe prompts, what adjustment would reduce false positives?

## Architecture Onboarding

- Component map: User query → lexical gate (block/review/allow) → model generates answer + JSON verdict → parallel LLM judge + deterministic scanner → if all checks SAFE → render answer; else → refusal/escalation
- Critical path:
  1. User query → lexical gate (block/review/allow)
  2. If allowed → model generates answer + JSON verdict in single call
  3. Buffered answer → parallel LLM judge + deterministic scanner
  4. If all checks SAFE → render answer; else → refusal/escalation
- Design tradeoffs:
  - Single-call JSON (F) vs. two-stage judge (D+E): F offers lower overhead but risks correlated failures; D+E is more robust but at 1.6–1.7× time and 4.8× token cost
  - Lexical gate strictness: Tighter patterns improve recall but increase false positives; threshold tuning per domain is required
  - Judge model choice: Same-family judges reduce infrastructure complexity but may share failure modes; cross-family judges improve robustness
- Failure signatures:
  - Malformed JSON → system falls back to fail-closed refusal; investigate prompt structure or API parsing
  - High false-positive rate on safe prompts → loosen regex or adjust classifier thresholds
  - Persistent evasion via obfuscation → expand denylist with leetspeak/homoglyph variants; consider classifier upgrade
  - Latency spikes → check verdict caching, parallel scan efficiency, or excessive review-state routing
- First 3 experiments:
  1. Replicate Table 3 on a held-out split of the synthetic dataset to validate blocking rates and overhead for methods C, E, and F
  2. Introduce adversarial paraphrases (e.g., indirect dieting frames, numeric obfuscation) to measure recall degradation and identify threshold adjustment needs
  3. Swap the adjudication model to a different family and compare correlated failure rates vs. latency/token impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the single-call JSON verdict approach under adversarial conditions such as prompt injection, jailbreaks, and obfuscated inputs?
- Basis in paper: [explicit] Authors state the approach "must be extended to adversarial threats like prompt injection and indirect attacks to be production-ready" and deliberately excluded adversarial vectors from scope.
- Why unresolved: The evaluation used only non-adversarial malicious prompts, not sophisticated attacks like obfuscated language or indirect prompt injection.
- What evidence would resolve it: Performance metrics on adversarial benchmarks with jailbreak attempts, obfuscation, and indirect attacks showing blocking rates and specific failure modes.

### Open Question 2
- Question: What is the false positive rate on legitimate safe prompts in real-world deployment contexts?
- Basis in paper: [explicit] Authors acknowledge they "do not estimate precision on safe prompts or quantify false positives/negatives in the wild" using only a synthetic 100-prompt dataset.
- Why unresolved: Synthetic data emphasized clear-cut cases and did not capture ambiguous, context-dependent queries common in actual use.
- What evidence would resolve it: Real-world deployment with human-annotated ground truth showing precision metrics and false positive impact on user experience.

### Open Question 3
- Question: Does using the same model family for both generation and adjudication introduce correlated failures?
- Basis in paper: [explicit] Authors state they "reused the same model family for adjudication (for reproducibility), which risks correlated failures and limits external validity" despite best practice favoring distinct model families.
- Why unresolved: Resource constraints led to single-model-family evaluation, leaving the correlation effect unquantified.
- What evidence would resolve it: Ablation study comparing same-family versus cross-family judge configurations measuring divergence in verdict accuracy and failure correlation rates.

### Open Question 4
- Question: Can session-level risk accumulation improve detection of harmful patterns that emerge gradually across multiple conversation turns?
- Basis in paper: [explicit] Authors identify "stateless moderation" as a Tessa failure pattern and note the prototype omits "session-level risk memory."
- Why unresolved: Current implementation evaluates each prompt independently without maintaining risk state across conversation history.
- What evidence would resolve it: Comparative evaluation on multi-turn datasets showing whether session-level risk scoring improves detection of escalating harmful patterns versus turn-by-turn evaluation.

## Limitations

- The evaluation uses a small synthetic dataset (100 prompts) that may not capture real-world adversarial diversity or context-dependent harms
- Key implementation details such as exact denylist patterns and system prompts are not fully specified, limiting faithful reproduction
- The paper does not report false-positive rates on safe prompts, and robustness under distribution shift or obfuscation is untested
- Correlated failures between generator and judge are acknowledged but not quantitatively characterized

## Confidence

- **High confidence**: The core architectural claim that a fail-closed, modular middleware can block harmful health-adjacent outputs is well-supported by the synthetic evaluation results, particularly the perfect recall of Method F.
- **Medium confidence**: The mechanism of single-call JSON self-adjudication achieving low overhead while maintaining safety is plausible but under-validated; the small dataset and lack of adversarial robustness testing limit generalizability.
- **Low confidence**: The efficacy of deterministic lexical gates for this domain is asserted but not directly validated; the paper explicitly notes the corpus does not support this claim, and the precision–recall trade-off is only qualitatively discussed.

## Next Checks

1. Replicate the blocking performance and overhead metrics (Table 3) on a held-out split of the synthetic dataset to confirm reproducibility and assess variance.
2. Introduce adversarial paraphrases (e.g., indirect dieting frames, numeric obfuscation, leetspeak) to measure recall degradation and identify gaps in lexical gate and adjudication robustness.
3. Swap the adjudication model to a different family (e.g., a non-Gemma model) and compare correlated failure rates, false-positive/negative patterns, and latency/token impacts versus the original same-family setup.