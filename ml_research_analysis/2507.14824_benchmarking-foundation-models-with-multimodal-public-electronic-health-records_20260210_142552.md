---
ver: rpa2
title: Benchmarking Foundation Models with Multimodal Public Electronic Health Records
arxiv_id: '2507.14824'
source_url: https://arxiv.org/abs/2507.14824
tags:
- data
- multimodal
- foundation
- clinical
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study developed a comprehensive benchmark for evaluating
  foundation models using multimodal electronic health records from the MIMIC-IV database.
  A standardized data processing pipeline was created to harmonize heterogeneous clinical
  data into a consistent format, enabling reproducible analysis across four core modalities:
  demographics, time-series vital signs, chest X-ray images, and clinical notes.'
---

# Benchmarking Foundation Models with Multimodal Public Electronic Health Records

## Quick Facts
- arXiv ID: 2507.14824
- Source URL: https://arxiv.org/abs/2507.14824
- Reference count: 40
- Primary result: Multimodal foundation models improve clinical predictions over structured data alone

## Executive Summary
This study develops a comprehensive benchmark for evaluating foundation models using multimodal electronic health records from the MIMIC-IV database. The benchmark standardizes data processing across four core modalities—demographics, time-series vital signs, chest X-ray images, and clinical notes—enabling reproducible analysis. Eight foundation models, both unimodal and multimodal, were evaluated across in-hospital mortality and length-of-stay prediction tasks while assessing fairness and interpretability. Results demonstrate that incorporating multiple data modalities consistently improves predictive performance compared to using structured data alone, without introducing additional bias. The benchmark provides a holistic framework to assess foundation models for real-world clinical applications.

## Method Summary
The benchmark employs a two-stage modular framework for unimodal models: first extracting embeddings independently from each modality using specialized encoders (GRU for time-series, domain-specific models for text and images), then concatenating these embeddings and training a logistic regression classifier. For multimodal large language models, data is converted to text format and processed through prompting. The pipeline harmonizes heterogeneous clinical data into a consistent format, processing the first 24 hours of ICU stays from MIMIC-IV. Evaluation metrics include AUROC, AUPRC, and fairness measures, with interpretability assessed using SHAP values.

## Key Results
- Multimodal integration consistently improved predictive performance compared to models using structured data alone
- Domain-specific fine-tuning was effective for unimodal models but did not translate to multimodal scenarios
- Current large vision-language models showed limited generalizability across clinical tasks
- Incorporating multiple modalities improved performance without introducing additional bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal data integration enriches the representation space for clinical prediction, provided that the modalities offer complementary signal.
- Mechanism: The system concatenates embeddings from unimodal encoders into a unified vector, allowing the downstream classifier to access feature correlations invisible to single-modality models, such as linking textual descriptions with specific anomalies in chest X-rays.
- Core assumption: The feature spaces of different modalities are roughly aligned or the concatenation operation preserves sufficient information for the linear classifier to find cross-modal correlations.
- Evidence anchors:
  - [abstract] "incorporating multiple data modalities leads to consistent improvements in predictive performance without introducing additional bias."
  - [section] (Results) "Integrating multimodal data consistently improved predictive performance compared to models trained exclusively on structured data..."
- Break condition: If modalities are highly correlated (redundant) or if missing data is not handled correctly, the concatenation may introduce noise that degrades performance.

### Mechanism 2
- Claim: Domain-specific pre-training aligns model weights to clinical ontologies more efficiently than general-purpose scaling.
- Mechanism: Models pre-trained on medical data learn priors specific to clinical language and imaging textures, requiring less fine-tuning data to achieve high performance because their latent spaces are already structured to differentiate medical concepts.
- Core assumption: The statistical distribution of the pre-training data shares fundamental features with the target task data.
- Evidence anchors:
  - [abstract] "domain-specific fine-tuning offers a cost-effective solution for unimodal foundation models..."
  - [section] (Discussion) "domain-specific models, even when trained on a smaller scale, can achieve performance comparable to their larger counterparts..."
- Break condition: If the specific clinical task requires reasoning far outside the scope of the pre-training corpus, domain-specific models may overfit to the pre-training distribution, losing generalizability.

### Mechanism 3
- Claim: Late fusion via embedding concatenation enables modular benchmarking but limits cross-modal interaction depth.
- Mechanism: The architecture decouples feature extraction from prediction, freezing encoders and only training the classifier to isolate the quality of the embeddings.
- Core assumption: A simple linear classifier is sufficient to approximate the decision boundary required for complex clinical predictions if the input embeddings are high-quality.
- Evidence anchors:
  - [section] (Methods) "...implemented a two-stage modular framework... In the first stage, embeddings... were extracted independently... In the second stage, these embeddings were concatenated..."
  - [section] (Discussion) "...fusion strategy employed relied on simple concatenation, which may not fully capture the complex interactions among different data modalities."
- Break condition: If the optimal prediction requires modeling complex non-linear interactions between modalities, a simple concatenation + LR will fail to capture it.

## Foundational Learning

- Concept: **Multimodal Embedding Alignment**
  - Why needed here: The benchmark concatenates vectors from diverse architectures. Understanding that these vectors must exist in a comparable geometric space is essential for debugging why multimodal performance might degrade.
  - Quick check question: Can you explain why concatenating a time-series embedding from a GRU with a text embedding from BERT might pose a challenge for a simple linear layer?

- Concept: **Missingness and Data Completeness**
  - Why needed here: The study explicitly notes that missing modalities influence feature importance and may cause performance drops.
  - Quick check question: How does the "missingness" of a chest X-ray affect the gradient updates for the text encoder in a late-fusion architecture?

- Concept: **Zero-shot vs. Fine-tuned Evaluation**
  - Why needed here: The study contrasts general-purpose models against domain-specific ones.
  - Quick check question: What is the risk of evaluating a foundation model on MIMIC-IV mortality without any domain-specific fine-tuning or prompting engineering?

## Architecture Onboarding

- Component map:
  - **Data Layer:** MIMIC-IV Database (Demographics, Vitals, CXR, Notes)
  - **Processing Layer:** Pipeline (Harmonization, Cleaning, 24-hour windowing)
  - **Encoder Layer:** Unimodal Encoders (GRU/Moment for Time-series, Swin/CXR-Foundation for Images, RadBERT/Text-embedding for Notes)
  - **Fusion Layer:** Concatenation (Joins fixed-dimensional vectors)
  - **Head:** Logistic Regression (Predictive output)
  - **Alternative Path:** LVLMs (GPT-4o mini, LLaVA-Med) process multimodal text/image inputs directly via prompting

- Critical path: The Data Processing Pipeline is the most critical bottleneck. If the 24-hour window extraction or the table-to-text conversion is flawed, neither encoders nor LLMs can recover the signal.

- Design tradeoffs:
  - **Modularity vs. Interaction:** The chosen architecture is highly modular (easy to swap encoders) but sacrifices the deep cross-modal attention mechanisms found in end-to-end multimodal transformers.
  - **LVLM vs. Modular Framework:** LVLMs offer ease of use (prompting) but showed limited generalizability and are harder to interpret locally compared to the modular framework with SHAP.

- Failure signatures:
  - **Performance Degradation on Full Modality:** Adding images to text+structured data caused a modest decline, suggesting the simple fusion strategy struggles with the noise-to-signal ratio of the image embeddings.
  - **Inconsistent LVLM Refusal:** LVLMs may refuse to answer or hallucinate when confronting complex numerical tables converted to text.

- First 3 experiments:
  1. **Baseline Validation:** Run the GRU + Best Text Encoder on the structured data subset to reproduce the "Structured + Notes" AUROC (~0.89). This validates the pipeline is working before adding images.
  2. **Ablation on Modality:** Systematically remove one modality at a time to verify the reported feature importance rankings and identify which modality is causing the "modest decline" in your specific setup.
  3. **Encoder Substitution:** Replace the domain-specific RadBERT with a general-purpose BERT to quantify the exact performance gap of domain pre-training on the specific MIMIC-IV cohort.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced multimodal fusion techniques significantly outperform the simple concatenation strategy currently used for integrating heterogeneous clinical data?
- Basis in paper: [explicit] The authors state their fusion strategy relied on simple concatenation, which "may not fully capture the complex interactions among different data modalities," and suggest future work investigate more sophisticated techniques.
- Why unresolved: Simple concatenation acts as a baseline that may fail to model complex inter-modality relationships necessary for optimal clinical reasoning.
- What evidence would resolve it: Comparative benchmarking results showing superior AUROC or AUPRC when using attention-based or tensor-fusion methods compared to the concatenation baseline.

### Open Question 2
- Question: What evaluation protocols are required to reliably assess the interpretability and faithfulness of Multimodal Large Language Models (MLLMs) in clinical settings?
- Basis in paper: [explicit] The paper notes that interpretability evaluation focused on modular frameworks and did not extend to MLLMs due to the difficulty of assessing language-based explanations which may be "plausible but ungrounded."
- Why unresolved: Traditional post-hoc feature attribution methods do not apply to the generative outputs of MLLMs, creating a gap in trustworthiness assessment.
- What evidence would resolve it: The development of a standardized, quantitative metric that validates MLLM textual explanations against known clinical ground truths.

### Open Question 3
- Question: Why do current domain-specific Large Vision-Language Models (LVLMs) underperform compared to general-purpose models on clinical prediction tasks?
- Basis in paper: [explicit] The results showed that medical LVLMs (e.g., LLaVA-Med) exhibited "inferior performance compared to general-purpose models," indicating a failure in domain specialization to translate to task improvement.
- Why unresolved: It is unclear if this deficit arises from insufficient medical training data, architectural limitations, or the specific prompting strategies used in the benchmark.
- What evidence would resolve it: An ablation study comparing general and medical LVLMs with controlled training datasets and optimized prompting for clinical tasks.

### Open Question 4
- Question: To what extent does the absence of specific data modalities (missingness) contribute to the performance decline observed when combining image and text data?
- Basis in paper: [inferred] The authors observed a modest performance decline when combining text and images compared to text alone, speculating it may stem from "missing modalities or ineffective multimodal fusion strategies."
- Why unresolved: The paper identifies missing modalities as a major challenge but does not disentangle the effect of missing data from the effect of the simple fusion strategy on the observed performance drop.
- What evidence would resolve it: Analysis of model performance on strictly complete-case datasets versus datasets with imputed or masked missing modalities.

## Limitations

- Domain-specific models effective for unimodal tasks show reduced performance in multimodal scenarios, suggesting their learned representations may not generalize across modalities
- Simple concatenation fusion strategy may not capture complex cross-modal interactions essential for clinical reasoning
- Missing data handling remains a significant challenge, with performance degradation observed when integrating multiple modalities

## Confidence

- **High Confidence:** Claims about performance improvements from multimodal integration and the superiority of domain-specific models for unimodal tasks
- **Medium Confidence:** Conclusions regarding fusion strategy limitations and missing data impacts
- **Low Confidence:** LVLM generalizability assessments due to potential confounding factors from text-prompting approach

## Next Checks

1. **Fusion Strategy Investigation:** Implement alternative fusion approaches (attention-based, cross-modal transformers) to quantify the performance gap created by simple concatenation
2. **Missing Data Impact Analysis:** Systematically vary missingness patterns across modalities to identify which data gaps most severely impact multimodal performance
3. **LVLM Prompt Optimization:** Develop structured input formats that preserve table and image information more effectively when generating prompts for large vision-language models