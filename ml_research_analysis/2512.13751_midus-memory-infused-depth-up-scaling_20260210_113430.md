---
ver: rpa2
title: 'MIDUS: Memory-Infused Depth Up-Scaling'
arxiv_id: '2512.13751'
source_url: https://arxiv.org/abs/2512.13751
tags:
- memory
- arxiv
- midus
- layer
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIDUS, a memory-based alternative to FFN
  replication for depth up-scaling in large language models. Instead of duplicating
  Transformer blocks with dense FFNs, MIDUS replaces them with memory blocks containing
  head-wise memory layers.
---

# MIDUS: Memory-Infused Depth Up-Scaling

## Quick Facts
- arXiv ID: 2512.13751
- Source URL: https://arxiv.org/abs/2512.13751
- Reference count: 40
- Primary result: Replaces FFN replication with head-wise memory layers for depth up-scaling

## Executive Summary
This paper introduces MIDUS (Memory-Infused Depth Up-Scaling), a novel approach to scaling transformer depth by replacing dense feed-forward network (FFN) replication with head-wise memory layers. Unlike traditional methods that duplicate blocks with larger FFNs, MIDUS assigns each attention head its own memory bank and generates values through an efficient factorization scheme. Experiments on Llama-3.2-1B and Llama-3.1-8B demonstrate improved language and reasoning performance while using fewer parameters and less memory than standard FFN replication methods.

## Method Summary
MIDUS addresses depth up-scaling by replacing replicated FFNs with memory blocks that contain head-wise memory layers. Each attention head has its own dedicated memory bank, and value generation is optimized through a factorization approach that reduces computational overhead. The method amplifies specialized attention heads identified through importance analysis, creating a more efficient architecture for increased depth. This memory-based approach aims to maintain or improve performance while reducing the parameter count and memory footprint compared to traditional FFN replication strategies.

## Key Results
- MIDUS achieves 53.50 average zero-shot accuracy versus ~52.6-53.4 for baseline FFN replication methods
- Uses fewer parameters and less memory than standard FFN replication approaches
- Demonstrates lower inference latency while maintaining or improving language and reasoning performance
- Head importance analysis shows MIDUS amplifies the role of specialized attention heads

## Why This Works (Mechanism)
MIDUS works by leveraging memory layers that are specific to each attention head, rather than using dense FFNs that treat all positions uniformly. This head-wise specialization allows the model to maintain more granular control over information processing as depth increases. The factorization scheme for value generation reduces computational redundancy that typically comes with depth scaling. By amplifying specialized heads through the memory mechanism, MIDUS can achieve better performance without the parameter explosion associated with traditional FFN replication.

## Foundational Learning

**Attention Head Specialization**: Different attention heads learn distinct functions (some focus on syntax, others on semantics). Why needed: Understanding this allows MIDUS to allocate memory resources more efficiently. Quick check: Verify through head importance analysis that certain heads consistently contribute more to task performance.

**Factorization in Neural Networks**: Decomposing complex operations into simpler, more efficient components. Why needed: Enables MIDUS to scale depth without proportional increases in computation. Quick check: Compare FLOPs between MIDUS and FFN replication for equivalent depth increases.

**Memory-Based Neural Architectures**: Using external or internal memory structures to augment standard neural operations. Why needed: Provides a mechanism for scaling capacity without simply adding more parameters. Quick check: Measure parameter efficiency (performance per parameter) across different architectures.

## Architecture Onboarding

**Component Map**: Input -> Multi-Head Attention -> MIDUS Memory Blocks -> Output
**Critical Path**: Token embedding → Attention computation → Head-wise memory lookup/generation → Residual addition → Next layer
**Design Tradeoffs**: Memory-based scaling vs. parameter-based scaling; specialized head treatment vs. uniform processing; computational efficiency vs. architectural complexity
**Failure Signatures**: Degraded performance if memory banks are too small; increased latency if factorization is inefficient; loss of general capabilities if specialization is too extreme
**First Experiments**: 1) Compare parameter counts between MIDUS and FFN replication for same depth; 2) Measure inference latency on representative hardware; 3) Analyze head importance distributions before and after MIDUS application

## Open Questions the Paper Calls Out

None

## Limitations

- Tested only on 1B and 8B parameter models, limiting generalizability to larger scales
- Primary comparisons against standard FFN replication rather than most recent advanced up-scaling methods
- Evaluation focused on moderate benchmark sets without extensive cross-domain robustness testing
- Memory factorization efficiency needs validation across different hardware configurations
- Ablation studies don't fully isolate memory layer contributions from other architectural changes

## Confidence

- Performance improvements over FFN replication: High
- Memory efficiency claims: Medium
- Head specialization findings: Medium
- Generalizability to larger models: Low

## Next Checks

1. Test MIDUS on larger models (e.g., 70B scale) to assess scalability and diminishing returns
2. Benchmark against the most recent depth up-scaling methods beyond standard FFN replication
3. Conduct cross-domain evaluation (code, multilingual, long-context) to verify robustness