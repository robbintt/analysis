---
ver: rpa2
title: 'CURing Large Models: Compression via CUR Decomposition'
arxiv_id: '2501.04211'
source_url: https://arxiv.org/abs/2501.04211
tags:
- curing
- decomposition
- matrix
- compression
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CURing Large Models: Compression via CUR Decomposition CURing
  addresses the challenge of compressing large neural network models, particularly
  large language models, to reduce memory usage while maintaining performance. The
  core idea is to apply CUR matrix decomposition to weight matrices, approximating
  them as the product of selected columns (C), rows (R), and a small linking matrix
  (U).'
---

# CURing Large Models: Compression via CUR Decomposition

## Quick Facts
- **arXiv ID**: 2501.04211
- **Source URL**: https://arxiv.org/abs/2501.04211
- **Reference count**: 40
- **Primary result**: Reduces Llama3.1-8B parameters from 8.03B to 7.32B (-9%) in 129 seconds, over 20× faster than prior methods

## Executive Summary
CURing addresses the challenge of compressing large neural network models, particularly large language models, to reduce memory usage while maintaining performance. The core idea is to apply CUR matrix decomposition to weight matrices, approximating them as the product of selected columns (C), rows (R), and a small linking matrix (U). This decomposition is applied to weights chosen based on their magnitudes and activations, using a combined approach called WANDA alongside DEIM-CUR decomposition.

The primary result shows that CURing can significantly reduce model size with minimal performance loss. For example, it reduces Llama3.1-8B's parameters from 8.03B to 7.32B (-9%) in just 129 seconds, over 20 times faster than prior compression methods. The approach inherently heals compression damage without retraining, and can further improve performance through layer-wise knowledge distillation, often achieving better results than the original model.

## Method Summary
CURing compresses LLMs by applying CUR matrix decomposition to select weight matrices. The method first calibrates on 128 samples to compute angular distances between consecutive layer outputs, then ranks layers by these distances (excluding first/last). For target layers, it computes a WANDA importance matrix from weight magnitudes and activations, applies DEIM to select r indices from SVD components, and extracts C and R from original weights. U is computed as C†WR†. Optional healing uses layer-wise knowledge distillation training only ∆U to recover performance.

## Key Results
- Reduces Llama3.1-8B parameters from 8.03B to 7.32B (-9%) in 129 seconds
- Achieves C4 perplexity of 17.56 after healing, surpassing original 23.79
- Outperforms prior methods by over 20× in compression speed
- Maintains performance across multiple benchmarks including BoolQ and MMLU

## Why This Works (Mechanism)

### Mechanism 1: CUR Decomposition Preserves Weight Matrix Structure
- Claim: Approximating weight matrices as W ≈ CUR reduces parameters while maintaining functional fidelity.
- Mechanism: Selected columns C and rows R retain actual weight values from the original matrix (not learned projections), while U = C†WR† optimally links them via pseudoinverse. This preserves interpretability and enables parameter reduction when mn > mr + r² + rn.
- Core assumption: Weight matrices in transformers have exploitable low-rank structure, particularly in layers that produce similar outputs to their predecessors.
- Evidence anchors:
  - [abstract] "approximates weight matrices as the product of selected columns (C) and rows (R), and a small linking matrix (U)"
  - [section 3] "U = C†WR†... is optimal with respect to the Frobenius norm"
  - [corpus] Related work on CUR decomposition confirms bounded approximation error relative to optimal low-rank solutions
- Break condition: If weight matrices have rapidly decaying singular values that don't satisfy σr+1 ≤ δ/(L(ηp + ηq)(‖W2‖₂‖K‖₂)⁻¹), reconstruction error becomes unbounded.

### Mechanism 2: WANDA+DEIM Selection Identifies Informative Subspaces
- Claim: Combining weight magnitudes with activation patterns identifies the most semantically important rows/columns.
- Mechanism: WANDA computes importance matrix S = |W| · ‖X‖₂, then DEIM iteratively selects indices from SVD-derived singular vectors to minimize redundancy. This jointly captures structural (weights) and behavioral (activations) importance.
- Core assumption: Calibration data (128 C4 samples) provides representative activation statistics.
- Evidence anchors:
  - [section 4.2] "the information matrix S is computed by multiplying the absolute values of the weights with the input activations"
  - [appendix D.2] Table 5 shows CURing (WANDA+DEIM) has smallest Frobenius norm difference from original
  - [corpus] Neighbor papers confirm CUR decomposition efficacy for LLM compression tasks
- Break condition: If activation patterns on calibration data diverge significantly from inference distribution, selected columns/rows may not capture true importance.

### Mechanism 3: Subspace-Constrained Healing Mitigates Forgetting
- Claim: Updating only ∆U during knowledge distillation recovers performance while constraining gradient directions.
- Mechanism: With fixed C and R, gradients ∇U L(U) ∈ {C⊤MR⊤} are restricted to a subspace, preventing arbitrary weight modifications. Layer-wise MSE distillation from the original model acts as implicit regularization.
- Core assumption: The subspace spanned by C and R contains sufficient capacity to represent performance-critical corrections.
- Evidence anchors:
  - [section 4.5] Theorem 4.3: "∇U L(U) ∈ {C⊤MR⊤}" proves gradient restriction
  - [section 5.2] C4 perplexity improves from 77.33 to 17.56 after healing, surpassing original 23.79
  - [corpus] Limited direct corpus evidence on subspace-constrained healing specifically
- Break condition: If compression is too aggressive (many layers, low rank), the subspace may lack expressiveness to recover critical functionality.

## Foundational Learning

- Concept: **CUR Matrix Decomposition**
  - Why needed here: Core technique enabling parameter reduction by selecting actual matrix elements rather than learning new representations.
  - Quick check question: Given a 4096×4096 weight matrix, explain why selecting 256 columns and 256 rows plus a 256×256 matrix reduces parameters.

- Concept: **DEIM (Discrete Empirical Interpolation Method)**
  - Why needed here: Determines which specific indices to select for C and R; understanding it is essential for modifying selection criteria.
  - Quick check question: How does DEIM differ from selecting the top-k rows by norm?

- Concept: **Knowledge Distillation for Compression Recovery**
  - Why needed here: Healing phase relies on matching teacher-student activations; misuse can cause overfitting or insufficient recovery.
  - Quick check question: Why use MSE on layer outputs rather than cross-entropy on final predictions?

## Architecture Onboarding

- Component map:
  Input → Calibration Pass: compute WANDA scores + angular distances
        → Layer Selection: rank layers by angular distance, exclude first/last
        → Per-Layer CUR: S=|W|·‖X‖₂ → SVD → DEIM indices → extract C,R → compute U
        → Optional Healing: layer-wise KD, train ∆U only
        → Compressed Model

- Critical path:
  1. Calibration (forward pass on 128 samples, collect activations)
  2. Layer ranking by angular distance
  3. For each target layer: WANDA importance → DEIM selection → CUR decomposition
  4. If healing: initialize ∆U=0, run KD with teacher=original model

- Design tradeoffs:
  - More layers compressed → smaller model but higher perplexity before healing
  - Higher rmax (128→256→512) → better performance, less compression, slower
  - Target weights {Q,K,Gate} vs subsets: all three gives maximum compression; {Q,K} preserves more performance

- Failure signatures:
  - Perplexity explodes (>1000 on C4): rank too low or too many layers compressed
  - Healing doesn't converge: learning rate too high or subspace insufficient
  - Task-specific performance crashes while C4 improves: overfitting to distillation corpus

- First 3 experiments:
  1. **Baseline compression**: Compress 10 layers of Llama3.1-8B at rmax=256, measure C4/WikiText perplexity and BoolQ/MMLU accuracy without healing. Target: perplexity <100, accuracy >0.7×baseline.
  2. **Healing ablation**: Same setup, apply 500-step KD healing. Compare perplexity recovery with LoRA/MoRA baselines using equal trainable parameters.
  3. **Layer selection validation**: Compare angular-distance selection vs last-N-layers vs random on 10-layer compression. Expect angular distance to outperform on WikiText/MMLU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced decomposition techniques, such as Compact Matrix Decomposition (CMD) or approaches designed for sparse matrices, yield more efficient factorization than the current dense CUR implementation?
- Basis in paper: [explicit] The Conclusion states, "Future research directions include exploring advanced decomposition techniques to further enhance efficiency and compactness... methods like Compact Matrix Decomposition (CMD)... could yield more efficient low-rank factorization."
- Why unresolved: The current work focuses on standard CUR decomposition, which may not be optimal for all matrix structures found in LLMs.
- What evidence would resolve it: Comparative experiments applying CMD or sparse CUR variants to the Llama or Mistral models, measuring the trade-off between compression ratio and perplexity relative to the standard CURing method.

### Open Question 2
- Question: Does diversifying the calibration dataset beyond the C4 corpus enhance the robustness of the angular distance layer selection and the resulting compression performance?
- Basis in paper: [explicit] Appendix D.1 notes that "diversifying the calibration dataset could potentially further enhance the benefits of angular distance selection across various tasks."
- Why unresolved: The current experiments rely solely on 128 examples from the C4 dataset for calibration; it is unclear if this data sufficiently represents the model's full functional domain.
- What evidence would resolve it: Experiments using varied calibration domains (e.g., code, math, general prose) and evaluating the resulting compressed models on out-of-distribution tasks.

### Open Question 3
- Question: Is the current heuristic for rank selection (powers of 2 constrained by matrix dimensions) optimal for preserving model capabilities, or does it introduce unnecessary approximation error?
- Basis in paper: [inferred] Section 3.2 defines rank selection via a specific power-of-2 formula "to ensure compatibility with hardware acceleration," suggesting this constraint is practical rather than theoretically optimal for accuracy.
- Why unresolved: The paper does not investigate if a non-power-of-2 rank or a layer-specific adaptive rank could achieve better performance at similar parameter counts.
- What evidence would resolve it: Ablation studies comparing the fixed power-of-2 rank against fine-grained or singular-value-based adaptive ranks on the same compression budget.

## Limitations

- Implementation complexity: DEIM index selection algorithm is referenced but not fully specified in the paper, requiring careful implementation from Sorensen & Embree 2016 source.
- Generalization boundaries: Results demonstrated primarily on Llama3.1-8B; performance on other architectures or modalities remains untested.
- Theoretical guarantees: While Theorem 4.3 proves gradient subspace restriction, the paper doesn't establish bounds on approximation error relative to optimal CUR decomposition.

## Confidence

- **High confidence**: Core CUR decomposition mechanism (W ≈ CUR) and basic parameter reduction claims. The mathematical framework is well-established and the 8.03B→7.32B reduction is verifiable.
- **Medium confidence**: WANDA+DEIM selection effectiveness and healing phase performance recovery. These rely on empirical calibration data and knowledge distillation dynamics that may vary with implementation details.
- **Low confidence**: Claims about superior performance compared to all prior methods (over 20× faster compression), as comparative timing data and ablation studies are limited.

## Next Checks

1. **DEIM implementation validation**: Implement the full DEIM selection algorithm following Sorensen & Embree 2016, then verify that selected indices minimize the residual norm ∥X - PX∥ where P projects onto selected columns. Compare Frobenius norm preservation against random selection baselines.

2. **Layer selection sensitivity analysis**: Systematically vary the number of compressed layers (1, 5, 10, 15) and rank values (128, 256, 512) while measuring C4 perplexity and WikiText2 perplexity. Document the trade-off curve to establish optimal compression points.

3. **Cross-architecture generalization test**: Apply the exact CURing procedure to a different architecture (e.g., OPT-6.7B or BERT-base) using the same calibration methodology. Compare parameter reduction, perplexity, and task performance to establish whether results transfer beyond Llama3.1-8B.