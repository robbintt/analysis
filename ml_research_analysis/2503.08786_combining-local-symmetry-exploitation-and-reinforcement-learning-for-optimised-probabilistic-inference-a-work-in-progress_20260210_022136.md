---
ver: rpa2
title: Combining Local Symmetry Exploitation and Reinforcement Learning for Optimised
  Probabilistic Inference -- A Work In Progress
arxiv_id: '2503.08786'
source_url: https://arxiv.org/abs/2503.08786
tags:
- factor
- inference
- symmetries
- order
- compact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper adapts a recent reinforcement learning (RL) approach\
  \ for tensor network contraction optimization to probabilistic inference in graphical\
  \ models, leveraging the duality between these domains. The key innovation is incorporating\
  \ structure exploitation\u2014specifically local symmetries within factors\u2014\
  into the RL agent\u2019s cost function."
---

# Combining Local Symmetry Exploitation and Reinforcement Learning for Optimised Probabilistic Inference -- A Work In Progress

## Quick Facts
- **arXiv ID:** 2503.08786
- **Source URL:** https://arxiv.org/abs/2503.08786
- **Reference count:** 5
- **Primary result:** Adaptation of RL-based tensor network contraction optimization to probabilistic inference, incorporating local symmetry exploitation to reduce intermediate factor sizes during variable elimination.

## Executive Summary
This paper presents a novel approach that combines reinforcement learning (RL) with structure exploitation for optimized probabilistic inference in graphical models. The key innovation lies in adapting an RL framework for tensor network contraction optimization to variable elimination in factor graphs, while incorporating local symmetries within factors through compact encodings. By reformulating the cost function to account for symmetric random variables (SRVs), the agent can discover more efficient elimination orders. Experimental results demonstrate that exploiting local symmetries significantly reduces cumulative intermediate result sizes, offering a promising direction for scalable probabilistic reasoning.

## Method Summary
The method adapts RL-based tensor network contraction optimization to probabilistic inference by formulating variable elimination as a Markov Decision Process. The state space consists of factor graphs, actions represent selecting random variables to sum out, and costs correspond to intermediate factor sizes. The key innovation is modifying the cost function to use compact encodings via Symmetric Random Variables (SRVs) for factors containing interchangeable random variables. The RL agent employs a Graph Neural Network to select the next variable to eliminate, with the compact encoding changing the size calculation from exponential to polynomial complexity. The approach bridges RL-based optimization with structure-aware inference by leveraging the duality between tensor networks and probabilistic graphical models.

## Key Results
- Reformulating the RL cost function to use compact SRV encodings significantly reduces cumulative intermediate result sizes during variable elimination.
- Local symmetries in factors can be preserved through standard VE operations (sum-out and multiplication), allowing compact representations to persist throughout inference.
- The GNN-based RL agent can generalize learned optimization strategies to unseen graphical models when properly trained.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exploiting local symmetries via compact encodings reduces memory and computational cost of intermediate results during Variable Elimination.
- **Mechanism:** Local symmetries exist where factor potential values are invariant to permutations of variable subsets (interchangeable RVs). These can be encoded using Symmetric Random Variables (SRVs), changing factor size from exponential O(d^n) to polynomial O((n+d-1 choose d-1)). Theorems 5.1 and 5.2 establish that standard operations preserve these symmetries.
- **Core assumption:** Graphical model factors contain exploitable symmetries a priori.
- **Evidence anchors:** Abstract states compact encodings significantly reduce intermediate sizes; Section 5 proves symmetry preservation.
- **Break condition:** Overhead of detecting symmetries and managing encodings exceeds computational savings.

### Mechanism 2
- **Claim:** Modified RL cost function allows discovery of fundamentally more efficient elimination orders.
- **Mechanism:** RL agent perceives "structure-aware" costs when compact encodings are used. Actions creating large but highly symmetric intermediate factors appear "cheap" under the new cost function, enabling exploration of structure-exploiting paths.
- **Core assumption:** Elimination orders minimizing compact encoding sizes correspond to faster overall inference times.
- **Evidence anchors:** Abstract mentions enabling exploration of efficient contraction orders; Section 4 discusses redefining costs to scale with structures.
- **Break condition:** Weak correlation between compact encoding size and actual wall-clock inference time due to implementation overhead.

### Mechanism 3
- **Claim:** GNN-based RL agent can generalize learned optimization strategies to unseen graphical models.
- **Mechanism:** GNN embeds factor graph state (variables and factors as nodes) and learns policy mapping from graph state to elimination action. Operating on graph topology rather than fixed-size vectors enables application to different graph sizes and structures.
- **Core assumption:** Structural features of factor graphs are predictive of optimal elimination steps regardless of specific model instance.
- **Evidence anchors:** Paper adapts approach from Meirom et al. (2022) which demonstrated RL for tensor network contraction.
- **Break condition:** Agent overfits to training graph topologies and fails on structurally different test graphs.

## Foundational Learning

- **Concept:** Variable Elimination (VE) & Intermediate Factors
  - **Why needed here:** Target process being optimized; VE works by sequentially multiplying factors and summing out variables, creating intermediate factors whose efficiency depends on size.
  - **Quick check question:** In Variable Elimination, why does the order of eliminating variables affect computational complexity?

- **Concept:** Markov Decision Process (MDP) in RL
  - **Why needed here:** Optimization strategy relies on formulating elimination order search as MDP; understanding tuple (State, Action, Transition, Reward) is crucial for how agent "learns" to eliminate variables.
  - **Quick check question:** In this specific MDP, what represents an "action" and what represents the "cost" (negative reward)?

- **Concept:** Symmetric Random Variables (SRVs) / Counting RVs
  - **Why needed here:** Core data structure for innovation; grasp how factor over n variables can be compressed if variables are interchangeable, mapping joint assignments to histograms.
  - **Quick check question:** If factor is defined over 3 symmetric Boolean variables, what is the size of its standard potential table versus SRV domain size?

## Architecture Onboarding

- **Component map:** PGM Loader -> Symmetry Detector -> State Manager -> Cost Function Module -> RL Agent (GNN)
- **Critical path:** Interaction between Symmetry Detector, State Manager, and Cost Function. System fails if symmetries aren't correctly identified or if propagation logic is buggy, as Cost Function provides incorrect feedback to agent.
- **Design tradeoffs:**
  - Detection Cost vs. Inference Gain: Aggressive symmetry detection finds more savings but adds overhead
  - Encoding Complexity: Implementing factor multiplication and sum-out on SRVs is more complex than on standard tables
- **Failure signatures:**
  - Cost Misalignment: Agent minimizes "compact cost" but runs slowly in practice, indicating encoding overhead is too high
  - State Drift: Intermediate factors lose symmetry tags during graph updates, causing agent to revert to standard cost estimation
- **First 3 experiments:**
  1. Symmetry Propagation Validation: Verify Theorems 5.1 and 5.2 by taking small graph with known symmetries, performing multiplication and sum-out, checking if resulting factor correctly preserves symmetry sets
  2. Cost Function A/B Test: Generate random graphs with planted symmetries, compare cumulative cost of random elimination order using Standard Cost vs. Compact Cost function
  3. Overfit Test: Train agent on specific topology (e.g., chains) with symmetries, test on different topology (e.g., grids) to ensure GNN learns generalizable policy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can trained RL agent using symmetry-aware cost function consistently outperform standard greedy heuristics (e.g., min-fill) in finding optimal elimination orders for diverse graphical models?
- **Basis in paper:** Paper describes MDP formulation but states on Page 3 that "detailed description of RL procedure is left for future work." Experimental results rely on greedy heuristic rather than trained agent.
- **Why unresolved:** Authors have defined environment and reward signal but haven't implemented or validated learning algorithm's ability to generalize or optimize better than traditional methods.
- **What evidence would resolve it:** Empirical data comparing inference costs of elimination orders generated by trained RL agent against standard heuristics on benchmark set of factor graphs.

### Open Question 2
- **Question:** Can MDP formulation and cost function be extended to account for other structural properties, such as low-rank tensor decompositions or Fourier representations?
- **Basis in paper:** Conclusion states "There exist many other interesting compact encodings such as low-rank tensor decompositions that can be taken into account for the cost function."
- **Why unresolved:** Current implementation focuses exclusively on local symmetries; integrating distinct structural properties like low-rank approximations may require fundamentally different methods.
- **What evidence would resolve it:** Reformulated cost function estimating rank or spectrum complexity, combined with experiments showing agent learning to exploit these alternative structures.

### Open Question 3
- **Question:** How can RL framework be modified to optimize for conditional queries (marginals) and evidence incorporation rather than just partition function?
- **Basis in paper:** Conclusion explicitly notes "one can reformulate the RL setting to consider evidence as well as other types of queries."
- **Why unresolved:** Current MDP defines goal state as elimination of all variables to compute partition function Z(G). Handling evidence requires absorbing specific variable assignments during elimination, altering state transitions and potentially optimal policy structure.
- **What evidence would resolve it:** Modified transition function F(G, X, e) handling evidence e, along with demonstration of agent optimizing elimination orders for marginal probability queries.

### Open Question 4
- **Question:** Does computational overhead of detecting local symmetries and managing histogram encodings during inference negate complexity reduction gained from smaller intermediate factors?
- **Basis in paper:** Paper proves compact encodings reduce size of intermediate results (Theorems 5.1/5.2) and Fig. 3 shows reduced cumulative sizes, but assumes symmetries are identifiable without analyzing time complexity of detection or histogram management operations.
- **Why unresolved:** While space complexity is reduced, constant factors for checking symmetries and managing histogram domains during factor multiplication could impose significant time penalty.
- **What evidence would resolve it:** Wall-clock time comparison between standard VE and proposed symmetry-aware elimination on identical hardware, measuring actual runtime rather than cumulative variable size.

## Limitations
- Work explicitly labeled "in progress" with detailed GNN architecture and RL training procedures deferred to future work
- Experimental results focus only on synthetic factor graphs with planted symmetries, leaving real-world model performance unvalidated
- Critical assumption that compact encoding size correlates with wall-clock time is asserted but not empirically demonstrated
- Symmetry detection described abstractly without algorithmic details or complexity analysis

## Confidence
- **High confidence:** Mathematical framework for local symmetry detection and compact encoding via SRVs (Theorems 5.1/5.2, Equation 3) is well-defined and theoretically sound
- **Medium confidence:** MDP formulation mapping variable elimination to RL is straightforward, but effectiveness of symmetry-aware cost function remains theoretical without full agent training results
- **Low confidence:** Claims about GNN-based generalization across graph topologies are entirely speculative pending actual architecture and training details

## Next Checks
1. Implement and benchmark complete pipeline on real graphical models (e.g., from UAI inference competitions) to test practical viability beyond synthetic data
2. Conduct ablation studies comparing wall-clock inference times between standard VE, RL-based VE, and symmetry-aware RL-based VE to validate cost function correlation assumption
3. Test symmetry propagation robustness by systematically varying factor potential distributions and measuring detection accuracy across noise levels