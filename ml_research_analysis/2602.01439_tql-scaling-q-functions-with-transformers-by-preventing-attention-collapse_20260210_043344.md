---
ver: rpa2
title: 'TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse'
arxiv_id: '2602.01439'
source_url: https://arxiv.org/abs/2602.01439
tags:
- step
- attention
- value
- entropy
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformer Q-Learning (TQL) addresses the problem of scaling value
  functions in reinforcement learning, where naively increasing model size leads to
  training instability and performance degradation. The core issue identified is attention
  collapse - as transformer capacity increases, attention distributions become increasingly
  concentrated on a few tokens rather than utilizing the full input space.
---

# TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse
## Quick Facts
- arXiv ID: 2602.01439
- Source URL: https://arxiv.org/abs/2602.01439
- Reference count: 40
- Primary result: Achieves up to 43% improvement in scaling performance when increasing model size, achieving state-of-the-art results on OGBench across 25 tasks

## Executive Summary
Transformer Q-Learning (TQL) addresses a fundamental scaling problem in transformer-based reinforcement learning: as model capacity increases, attention distributions collapse to focus on only a few tokens, preventing the expected performance gains from larger models. The authors identify this "attention collapse" phenomenon and propose a solution that directly controls attention entropy through layer-wise temperature parameters. This ensures attention remains distributed across all input tokens rather than concentrating on dominant ones. TQL achieves state-of-the-art performance on the challenging OGBench benchmark, outperforming existing offline RL methods including those using flow-matching and transformer-based value functions.

## Method Summary
TQL scales transformer Q-functions by preventing attention collapse through entropy control. The method adds layer-wise learnable temperature parameters that maintain a target entropy level for attention distributions, ensuring attention is distributed across all input tokens rather than collapsing to a few dominant ones. State and action inputs are treated as scalar tokens, embedded with modality-specific embeddings and positional encodings, then processed through a transformer decoder with [VALUE] token for Q-value prediction. The training objective includes standard TD loss plus entropy control losses. The method also incorporates learnable modality embeddings to help larger models better attend to task-relevant patterns. Policy extraction uses a flow-based approach (FQL-style) with behavior cloning distillation.

## Key Results
- Achieves up to 43% improvement in performance when scaling from 0.4M to 26M parameters, while prior methods suffer degradation
- Achieves state-of-the-art performance on OGBench across 25 challenging tasks, outperforming existing offline RL methods
- Demonstrates that entropy control is essential for scaling transformers in RL - without it, performance degrades as model size increases

## Why This Works (Mechanism)
The core insight is that naively increasing transformer capacity in RL leads to attention collapse, where attention distributions become increasingly concentrated on a few tokens. This prevents the model from leveraging the full input space and degrades performance. TQL prevents this by directly controlling the entropy of attention scores through layer-wise temperature parameters that maintain a target entropy level. By ensuring attention remains distributed across all input tokens, the model can effectively utilize its increased capacity. The entropy control acts as a regularizer that forces the attention mechanism to explore the full input space rather than collapsing to local patterns.

## Foundational Learning
- **Attention entropy**: Measures the uniformity of attention distributions across tokens. Why needed: Quantifies the extent of attention collapse; the target for entropy control. Quick check: Compute H = -Σ_j A_ij log A_ij for attention matrix A; values near zero indicate collapse.
- **Layer-wise temperature scaling**: Per-layer learnable parameters that control the sharpness of attention distributions. Why needed: Allows fine-grained control over attention entropy at different depths of the transformer. Quick check: Monitor temperature values during training; they should stabilize rather than diverge.
- **Flow-based policy extraction**: Method to derive stochastic policies from deterministic Q-functions using normalizing flows. Why needed: Enables sample-efficient policy improvement from Q-learning in continuous action spaces. Quick check: Verify policy density estimates match expected behavior from Q-values.

## Architecture Onboarding
**Component map**: Input scalars -> Linear projections -> Modality embeddings + Positional embeddings -> [VALUE] token prepend -> L=2 Pre-LN decoder blocks (MHA + MLP) -> Q-value output
**Critical path**: The attention entropy control loop: Compute entropy H^ℓ per layer → Compare to target H̄ → Update temperatures α_ℓ via gradient descent → Affect attention distribution → Influence Q-value smoothness
**Design tradeoffs**: Layer-wise vs global temperature (layer-wise provides finer control but more hyperparameters), fixed vs adaptive target entropy (adaptive could reduce tuning but adds complexity)
**Failure signatures**: Attention entropy dropping to near-zero as model scales (attention collapse), Q-value landscape becoming non-smooth, performance degrading with increased model size
**First experiments**:
1. Train smallest model (0.4M) with and without entropy control; verify attention distributions and performance difference
2. Scale to 26M parameters with entropy control; measure attention entropy per layer and compare to target H̄
3. Train with single global temperature instead of layer-wise; compare attention maps and performance to TQL

## Open Questions the Paper Calls Out
**Open Question 1**: Will TQL continue to enable positive scaling at parameter counts significantly beyond 26M (e.g., 100M+ parameters), or do new failure modes emerge at larger scales?
- Basis: The authors note they "scale up to 26M parameters" but do not explore larger scales
- Why unresolved: Entropy control mechanism may not suffice as model capacity increases further
- What evidence would resolve it: Scaling experiments with models in 100M-1B parameter range

**Open Question 2**: Can TQL be effectively combined with online RL methods that use network resets or other plasticity-preserving techniques?
- Basis: TQL is evaluated only in offline RL; online methods like network resets "risk catastrophic forgetting"
- Why unresolved: Interaction with plasticity preservation methods in high update-to-data ratio settings is untested
- What evidence would resolve it: Online RL experiments combining TQL with network resets

**Open Question 3**: Can the target entropy hyperparameter H̄ be determined automatically without per-environment tuning?
- Basis: Authors acknowledge H̄ introduces "an additional hyperparameter" requiring environment-specific tuning
- Why unresolved: While heuristics exist, no principled or adaptive method for setting H̄ is provided
- What evidence would resolve it: Meta-learning or adaptive scheme adjusting H̄ during training across all 25 OGBench tasks

## Limitations
- Requires careful tuning of target entropy H̄ hyperparameter, which is initialized based on input dimensionality but then manually tuned
- Flow-based policy extraction adds complexity and may limit applicability to domains where such extraction is feasible
- Analysis focuses on continuous control tasks; effectiveness on other RL domains (games, language) remains untested

## Confidence
- **High confidence**: Core mechanism of preventing attention collapse through entropy control is well-supported by theoretical reasoning and empirical results
- **Medium confidence**: Claim of state-of-the-art performance on OGBench is credible given comprehensive evaluation
- **Medium confidence**: Scalability analysis showing 43% improvement is convincing for tested range, but longer-term scaling behavior beyond 26M parameters is unexplored

## Next Checks
1. Test sensitivity to target entropy H̄ by systematically varying it across tasks and measuring impact on performance and attention distributions
2. Evaluate the method on non-continuous-control domains (Atari, text-based games) to assess generalizability beyond OGBench
3. Perform scaling analysis beyond 26M parameters to identify potential new failure modes or optimal scaling regimes