---
ver: rpa2
title: Multi-round, Chain-of-thought Post-editing for Unfaithful Summaries
arxiv_id: '2501.11273'
source_url: https://arxiv.org/abs/2501.11273
tags:
- summary
- error
- editorspan
- editing
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using LLMs for post-editing unfaithful
  summaries, finding that LLM-based faithfulness critics achieve strong correlation
  with human judgments and can guide multiple rounds of post-editing. Chain-of-thought
  prompting, especially error span identification, significantly improves editing
  performance compared to prior work.
---

# Multi-round, Chain-of-thought Post-editing for Unfaithful Summaries
## Quick Facts
- arXiv ID: 2501.11273
- Source URL: https://arxiv.org/abs/2501.11273
- Reference count: 28
- Key outcome: LLM-based post-editing with chain-of-thought error span identification improves faithfulness of unfaithful summaries, achieving up to 50% improvement across multi-round editing with 98%+ editing success on FRANK and DeFacto datasets.

## Executive Summary
This paper presents a multi-round post-editing framework for correcting unfaithful summaries using LLMs. The approach employs a critic LLM to score summary faithfulness and an editor LLM to make corrections, with chain-of-thought prompting that includes error span identification and type classification. The framework achieves strong correlation between critic judgments and human ratings (PCC up to 0.907), with multi-round editing gradually improving faithfulness by up to 50% between first and final rounds. Chain-of-thought prompting, particularly error span identification, significantly outperforms prior approaches on factuality metrics.

## Method Summary
The framework iteratively applies a critic LLM to score summary faithfulness (1-5 Likert scale) and an editor LLM to correct identified errors. The critic uses 2-shot in-context examples from FRANK for calibration. The editor employs one of four chain-of-thought prompts: basic editing, error span identification, error type classification, or both span and type. The loop continues until the critic scores faithfulness as 5 or reaches 5 rounds. Error spans are predicted via zero-shot CoT reasoning rather than gold annotations, and error types are classified into Predicate, Entity, Circumstance, Out-of-Article, Grammar, Coreference, and Discourse Link categories.

## Key Results
- Chain-of-thought error span identification outperforms both basic editing and gold span provision, with QAFE scores of 2.707 vs 2.754 (gold span) despite poor span prediction accuracy (ROUGE-L 0.314)
- Multi-round editing achieves up to 50% improvement in faithfulness between first and final rounds, with most summaries requiring 4 rounds
- The approach achieves 98.57% editing success on FRANK and 98.75% on DeFacto datasets, outperforming previous methods on factuality metrics
- Error type classification specifically helps correct Predicate, Entity, and Out-of-Article errors, which are the most common and impactful error types

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Chain-of-thought error span identification improves post-editing by priming the model's internal state, even when predicted spans are imperfect.
- Mechanism: CoT-predicted spans had low ROUGE-L (0.314) against gold spans, yet produced better edits than using gold spans directly. This suggests the act of reasoning about errors—rather than the correctness of the prediction—is what drives improvement.
- Core assumption: The reasoning process itself conditions the model's generation, regardless of prediction accuracy.
- Evidence anchors:
  - Table 5 shows CoT span prediction outperforms gold span on QAFE (2.707 vs 2.754) despite poor span accuracy
  - Section 5.3: "Chain-of-thought reasoning is more important than correctness for error spans"
  - Related work on CoT faithfulness suggests CoT can be unfaithful to actual reasoning

### Mechanism 2
- Claim: Multi-round editing catches residual errors and newly-introduced inconsistencies through iterative critic evaluation.
- Mechanism: Each round applies the critic to the edited summary, identifying both uncorrected original errors and new errors introduced during editing.
- Core assumption: Assumes the critic's judgments are sufficiently reliable to guide termination, and that errors don't compound indefinitely.
- Evidence anchors:
  - "about a 50% improvement between the first and final editing rounds"
  - Figure 4 shows QAFactEval scores improving across rounds
  - Related work showed mixed results on other tasks

### Mechanism 3
- Claim: Error type classification specifically helps correct Predicate, Entity, and Out-of-Article errors.
- Mechanism: The paper identified three error types where knowing the error type (via EditorSpan+Type) improved results over span-only prompting.
- Core assumption: Assumes error type taxonomy maps cleanly to correction strategies the model can execute.
- Evidence anchors:
  - Table 6 shows EditorSpan+Type outperforms EditorSpan on all factuality metrics for Predicate, Entity, and Out-of-Article errors
  - Section 5.3: "Predicate, Entity, and Out-of-Article error types are the most important to identify"
  - No direct corpus support for this specific finding

## Foundational Learning
- Concept: Chain-of-thought prompting
  - Why needed here: Understanding that CoT improves performance through reasoning-state conditioning, not just output quality. Critical for interpreting why imperfect span predictions still help.
  - Quick check question: Can you explain why gold error spans might underperform CoT-predicted spans in this architecture?

- Concept: Faithfulness vs. factuality in summarization
  - Why needed here: The paper distinguishes faithfulness (consistency with source) from general factuality. Errors include hallucinations, contradictions, and unverifiable statements.
  - Quick check question: What are the three main categories of faithfulness metrics mentioned (entailment, QA-based, LLM-based)?

- Concept: In-context learning with demonstrations
  - Why needed here: The critic uses 2-shot demonstrations from FRANK. Understanding how few-shot examples affect calibration is essential for reproducibility.
  - Quick check question: Why did the authors avoid using XSum examples (which had extreme scores of 1 or 5) as demonstrations?

## Architecture Onboarding
- Component map: Input summary → Critic scores → (if <5) Editor edits → Critic re-scores → repeat → output
- Critical path: The critic's calibration directly determines both edit triggers and termination
- Design tradeoffs:
  - Separate sessions for critic/editor improved editing success rates but increased inference cost
  - Simpler prompts (Editor, EditorSpan) have higher edit rates (~95%) but lower factuality improvement
  - Complex prompts (EditorSpan+Type) have lower edit rates (~84%) but better corrections when they work
  - Binary vs. 5-point critic: Binary produces more edits but lower quality; 5-point is more selective
- Failure signatures:
  - Critic predicts "faithful" on still-unfaithful summary
  - Editor introduces new errors during correction
  - Editor refuses to edit or changes subject entirely
  - Low edit success rate indicates prompt is too complex
- First 3 experiments:
  1. Replicate the critic correlation analysis on FRANK using your target LLM
  2. Single-round comparison of all four editor prompts on a held-out sample
  3. Multi-round ablation: Run the full loop with max_rounds=1 vs. max_rounds=5

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why does prompting the LLM to reason about error spans result in better post-editing performance than providing the gold-standard error spans?
- Basis in paper: Section 5.3 notes that while CoT-predicted spans have low ROUGE scores compared to gold spans, they yield better edits, suggesting "updating the LLM's state via CoT reasoning is more important than correctly identifying the exact error span."
- Why unresolved: The paper identifies the counter-intuitive phenomenon but does not isolate the specific mechanism by which the act of reasoning improves generation quality over factual correctness.
- What evidence would resolve it: An ablation study varying the faithfulness of the reasoning chain or an analysis of attention heads during the editing phase.

### Open Question 2
- Question: Why does running the critic and editor in a single LLM session lead to dramatically lower editing success rates compared to separate sessions?
- Basis in paper: Footnote 3 states that the authors "experimented with running both critic and editor in a single session, but this setup resulted in dramatically lower editing success rates."
- Why unresolved: The paper reports the failure mode but offers no analysis on whether this is due to context contamination or instruction following failure.
- What evidence would resolve it: A qualitative and quantitative error analysis of single-session outputs.

### Open Question 3
- Question: Can this multi-round critic-editor framework maintain high success rates when applied to summaries generated by LLMs rather than fine-tuned models?
- Basis in paper: The Limitations section states, "we do not use any LLM-generated summaries," restricting experiments to system-generated summaries.
- Why unresolved: LLMs may produce hallucinations that are stylistically different or more subtle than the errors found in fine-tuned model outputs.
- What evidence would resolve it: Applying the framework to a dataset of LLM-generated summaries annotated with human faithfulness judgments.

## Limitations
- Temporal generalization: Performance may shift with newer LLM versions or different prompting strategies
- Dataset specificity: Effectiveness on non-news domains (scientific, legal) remains untested
- Statistical significance: Some comparisons lack statistical tests, making it difficult to assess whether improvements are robust

## Confidence
**High confidence** in: The general effectiveness of multi-round editing for improving faithfulness, the superiority of chain-of-thought prompting over baseline approaches, and the strong correlation between LLM critics and human judgments on FRANK.

**Medium confidence** in: The specific mechanisms by which CoT span identification improves editing, the optimal stopping criterion based on critic scores, and the relative importance of different error types for correction.

**Low confidence** in: Whether the identified mechanisms would generalize to smaller models or different prompting strategies, the long-term stability of critic-editor feedback loops, and the performance on datasets outside the news domain.

## Next Checks
1. **Model drift validation**: Replicate the critic correlation analysis with a different LLM version on the same FRANK samples to test robustness to model changes.

2. **Cross-dataset generalization**: Apply the complete pipeline to a non-news summarization dataset and compare improvements against FRANK baseline to identify domain-specific limitations.

3. **Prompt sensitivity analysis**: Systematically vary the CoT prompt complexity while holding model and dataset constant to identify the minimal prompt complexity that maintains >90% of maximum performance.