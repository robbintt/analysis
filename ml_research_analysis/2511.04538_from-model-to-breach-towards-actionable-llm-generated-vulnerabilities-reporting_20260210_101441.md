---
ver: rpa2
title: 'From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting'
arxiv_id: '2511.04538'
source_url: https://arxiv.org/abs/2511.04538
tags:
- code
- instruct
- security
- score
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates the security of code generated by popular open-weight
  large language models (LLMs), finding that even the latest models produce vulnerable
  code in well-documented scenarios, with vulnerability rates ranging from 10% to
  40%. This suggests a lack of effective vulnerability reporting and patching pipelines
  in LLM-generated code.
---

# From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting

## Quick Facts
- arXiv ID: 2511.04538
- Source URL: https://arxiv.org/abs/2511.04538
- Reference count: 40
- Key outcome: LLM-generated code shows 10-40% vulnerability rates, prompting new severity metrics (Prompt Exposure, Model Exposure) that weight risk by severity, generation probability, and prompt usage likelihood

## Executive Summary
This study systematically evaluates the security of code generated by popular open-weight large language models (LLMs), revealing that even the latest models produce vulnerable code in well-documented scenarios with vulnerability rates ranging from 10% to 40%. The authors introduce two new severity metrics—Prompt Exposure (PE) and Model Exposure (ME)—that better reflect real-world risk than raw vulnerability rates by exponentially weighting high-severity vulnerabilities based on their CVSS scores, generation likelihood, and prompt usage probability. Using the Asleep at the Keyboard benchmark with 17 prompts covering 8 CWE classes, the research demonstrates that minor prompt variations can drastically change vulnerability probabilities, highlighting model sensitivity to input formulation. While CVSS-based severity estimates have limitations, PE and ME improve upon existing failure-rate metrics by incorporating severity-aware aggregation that mirrors how attackers prioritize high-impact exploits.

## Method Summary
The study evaluates 7 target LLMs (including Llama 3.1, CodeLlama, and DeepSeek Coder) using the Asleep at the Keyboard benchmark, which contains 20 scenarios targeting 8 CWEs. For each scenario, the authors generate N=10 semantically equivalent prompt reformulations via ChatGPT-3.5, then produce M=25 code completions per reformulation using T=0.2 temperature, top-p=0.95, and max_tokens=1024. Valid code snippets (verified via py_compile) are scanned with CodeQL for the target CWE only. Vulnerability probability P_y is calculated as vulnerable/valid ratio per reformulation. Prompt Exposure (PE) aggregates CVSS severity, P_y, and prompt likelihood R_y (derived from perplexity via sigmoid calibration) using exponential-log averaging (base 2). Model Exposure (ME) aggregates PE scores across all benchmark scenarios.

## Key Results
- Vulnerability rates range from 10% to 40% across models and CWEs, with CWE-502 (deserialization) showing the highest severity at CVSS 8.8
- Minor prompt variations cause dramatic swings in vulnerability probability—CWE-79-1 shows P_y=0 in one reformulation vs P_y=1 in another with nearly identical wording
- PE and ME metrics produce different model rankings than raw vulnerability rates, with CodeLlama 70B-Instruct ranking worst by vulnerability rate but best by ME due to lower severity CWE targeting
- DeepSeek Coder-V2-33B shows highest overall vulnerability rate (40.5%) but CodeLlama 70B-Instruct has highest Model Exposure due to concentrating vulnerabilities in high-severity CWEs
- The exponential-log severity weighting (base 2) ensures high-severity vulnerabilities dominate risk assessment, mirroring attacker prioritization

## Why This Works (Mechanism)

### Mechanism 1: Severity-Weighted Exposure Aggregation
The Prompt Exposure (PE) and Model Exposure (ME) metrics more accurately reflect real-world risk than raw vulnerability rates by exponentially weighting high-severity vulnerabilities. PE aggregates three factors via exponential-log averaging (base b=2): (1) CVSS severity of the vulnerability class, (2) probability P_y of generating vulnerable code across prompt reformulations, and (3) likelihood R_y of the prompt being used (estimated via perplexity). High-severity vulnerabilities (CVSS 8-10) receive disproportionately higher weight, mirroring how attackers prioritize high-impact exploits. ME then aggregates PE scores across all benchmark scenarios.

### Mechanism 2: Multi-Prompt Sampling for Robustness Testing
Testing N+1 semantically similar prompts per scenario reveals prompt sensitivity that single-prompt evaluation misses entirely. For each original prompt x, the study generates N=10 reformulations via paraphrasing. Sampling M=25 completions per reformulation at T=0.2, top-p=0.95 computes P_y (vulnerable/valid ratio) for each reformulation y∈Φ_x. The variance in P_y across reformulations quantifies model fragility—some prompts swing from P_y=0 to P_y=1 with minimal wording changes.

### Mechanism 3: CWE-Targeted Static Analysis Pattern Matching
CodeQL automatically detects specific vulnerability patterns in LLM-generated code when configured per CWE. Each AATK scenario targets one CWE (e.g., CWE-89 for SQL injection). CodeQL rules scan only for that target CWE—not all patterns. This focused detection reduces false positives but may miss variant implementations or related vulnerability classes. Invalid (non-compilable) code is excluded from P_y calculation.

## Foundational Learning

- Concept: Common Vulnerability Scoring System (CVSS)
  - Why needed here: The paper uses CVSS-B (base) scores as severity proxies. Understanding the 0-10 scale, how base/temporal/environmental metrics differ, and why exponential weighting is appropriate for risk aggregation is essential to interpret PE/ME results.
  - Quick check question: Under exponential-log averaging with base 2, how much more weight does CVSS 9.0 receive vs. CVSS 5.0? (Answer: 2^(9-5) = 16×)

- Concept: Common Weakness Enumeration (CWE)
  - Why needed here: The benchmark covers 8 CWEs (20, 22, 78, 79, 89, 502, 732, 798). You need to know what each represents to interpret which vulnerability types models struggle with and why severity differs (CWE-502 deserialization = 8.8 CVSS; CWE-79 XSS = 6.4).
  - Quick check question: Which tested CWE has the highest representative CVSS-B, and why might it be particularly dangerous in LLM-generated code? (Answer: CWE-502 at 8.8; deserialization of untrusted data is common in ML pipelines)

- Concept: Taint Analysis
  - Why needed here: CodeQL detects vulnerabilities by tracking data flow from "sources" (untrusted inputs) to "sinks" (sensitive operations) without sanitization. Understanding this model helps diagnose why certain generated code patterns trigger or evade detection.
  - Quick check question: For CWE-89 (SQL injection), what are the source and sink in taint analysis? (Answer: Source = user input parameter; Sink = database query execution function)

## Architecture Onboarding

- Component map: AATK scenarios -> Prompt reformulation module -> Code generation engine -> Validity filter -> Static analysis scanner (CodeQL) -> Severity database -> Perplexity estimator -> PE calculator -> ME aggregator

- Critical path:
  1. Load AATK scenarios → convert infilling to auto-regressive format
  2. For each prompt x: generate N=10 reformulations
  3. For each y∈Φ_x: generate M=25 completions → filter valid → scan with CodeQL → count vulnerable
  4. Compute P_y per reformulation; compute R_y via perplexity
  5. Aggregate to PE_x via Equation 2
  6. Aggregate PE_x to ME via Equation 4

- Design tradeoffs:
  1. Target CWE only vs. broad scanning: Paper scans only the designed CWE per scenario (reduces noise, may miss variants). Trade-off: specificity vs. coverage.
  2. Exponential base 2 vs. 10: Base 2 is conservative; base 10 drastically overweights high severity. Paper uses base 2 for main results (Table 2), base 10 in appendix (Table D5).
  3. Manual vs. automated reformulation: Manual curation via ChatGPT ensures quality but doesn't scale. Automated paraphrasing risks semantic drift.
  4. Temperature T=0.2: Low temperature for reproducibility. Higher T explores more of vulnerability space but generates less realistic code.

- Failure signatures:
  1. High P_y variance across reformulations: Model learned spurious prompt correlations (e.g., Table 4: CWE-79-1 swings P_y=0↔1)
  2. Low valid code rate (<95%): P_y estimates unreliable—too many exclusions
  3. CodeQL false negatives: Generated code uses non-standard libraries or novel vulnerability patterns not in rules
  4. R_y saturation: If perplexity-derived R_y clusters near 0 or 1, sigmoid parameters (μ=20, k=10) need recalibration

- First 3 experiments:
  1. Baseline vulnerability rate: Run 7 target models on AATK with M=25, N=0 (no reformulations). Compute raw vulnerable/valid ratio. Compare to Table 1. Validates pipeline matches prior work before adding PE/ME complexity.
  2. Prompt sensitivity validation: Select 3 high-variance scenarios (CWE-79-1, CWE-798-0, CWE-20-1). Generate N=20 reformulations each. Plot P_y distributions. Identify wording patterns that maximize/minimize vulnerability rates. Quantifies reformulation impact.
  3. ME vs. naive rate comparison: Compute both ME (base 2) and raw vulnerability rate for all models. Check ranking inversions (Section 5.3: CodeLlama 70B-Instruct worst by naive, best by ME). Identify which CWEs drive inversions—demonstrates severity-weighting value.

## Open Questions the Paper Calls Out

### Open Question 1
How can the Prompt Exposure metric be refined to better account for the heterogeneity of severity scores within a single CWE class? The current methodology relies on a single "representative" CVSS-B score per CWE using exponential-log averaging, which assumes a magnitude of impact that may overestimate the severity due to reporting biases in CVE databases. A comparative analysis where PE scores are calculated using granular, instance-specific CVSS estimates rather than class-wide aggregates, validated against expert security assessments, would resolve this.

### Open Question 2
To what degree does the reliance on static analysis (CodeQL) underestimate the true vulnerability exposure compared to dynamic analysis? The Discussion acknowledges the limitation that CodeQL "only scans specific patterns," and the authors suggest that "a more precise evaluation would likely reveal an even more serious problem." Static analysis tools often fail to detect runtime vulnerabilities or logic flaws that do not match predefined taint patterns, potentially resulting in a lower Prompt Exposure score than is warranted. A follow-up study applying dynamic analysis (e.g., fuzzing or runtime monitoring) to the generated code snippets to identify vulnerabilities missed by the static scanner would resolve this.

### Open Question 3
Do the observed prompt sensitivities and high vulnerability rates in Python generalize to systems programming languages like C or Rust? The Methodology section states the authors "only keep the Python scenarios," effectively narrowing the scope despite the original benchmark containing C scenarios. The "safety-functionality trade-off" and the nature of CWEs (e.g., buffer overflows vs. script injection) differ significantly by language, so it is unclear if ME scores would remain consistent across language paradigms. Applying the PE and ME scoring pipeline to the discarded C scenarios from the AATK benchmark or new Rust datasets would resolve this.

## Limitations

The vulnerability detection relies heavily on CodeQL's ability to recognize standard CWE patterns, which may systematically undercount vulnerabilities in LLM-generated code that employ non-standard libraries or novel implementation approaches. The manual prompt reformulation process via ChatGPT-3.5 doesn't scale to larger benchmarks and may introduce selection bias toward reformulations that researchers found interesting. The perplexity-based prompt likelihood estimates assume that reference-model perplexity correlates with real-world usage probability, an assumption requiring empirical validation.

## Confidence

**High Confidence**: The observed vulnerability rates (10-40%) across multiple models and CWEs are reproducible findings. The prompt sensitivity phenomenon is well-demonstrated with clear examples. The basic methodology of using static analysis on LLM-generated code is sound and implementable.

**Medium Confidence**: The Prompt Exposure and Model Exposure metrics provide meaningful risk differentiation, as evidenced by ranking inversions compared to raw vulnerability rates. However, the exponential-log aggregation's sensitivity to CVSS assumptions and the perplexity-sigmoid calibration introduce uncertainty in absolute PE/ME values.

**Low Confidence**: The claim that CVSS-B scores on 2025 CVEs provide valid severity proxies for LLM-generated code is speculative. The sigmoid parameters (μ=20, k=10) for converting perplexity to usage likelihood are heuristic choices without validation. The manual reformulation process lacks reproducibility and may not represent the full space of semantically equivalent prompts.

## Next Checks

1. **Cross-Scanner Validation**: Run the same AATK scenarios through multiple static analysis tools (CodeQL, Semgrep, Bandit) and compare vulnerability detection rates. Calculate precision and recall by manually auditing a sample of flagged vs. unflagged code. This quantifies CodeQL-specific limitations and establishes detection consistency across tools.

2. **Automated Reformulation Stress Test**: Replace manual ChatGPT reformulations with automated paraphrasing tools (e.g., text-davinci-003) to generate N=50 reformulations per prompt. Measure variance in P_y across reformulations and compare to manual results. If variance increases significantly, manual curation is masking semantic drift issues that would appear in production.

3. **CVSS-to-Impact Calibration**: For each CWE, collect empirical exploit data (CVEs with known exploitation metrics) and correlate CVSS-B scores against actual exploit frequency, ease of exploitation, and real-world impact. Use this to refine the exponential base or develop an alternative severity weighting that better reflects LLM-specific threat models.