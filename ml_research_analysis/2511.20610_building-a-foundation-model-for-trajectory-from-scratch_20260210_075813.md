---
ver: rpa2
title: Building a Foundation Model for Trajectory from Scratch
arxiv_id: '2511.20610'
source_url: https://arxiv.org/abs/2511.20610
tags:
- foundation
- trajectory
- mobility
- gpt-2
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This tutorial demonstrates how to build a minimal trajectory foundation
  model from scratch, adapting GPT-2 for spatiotemporal data. It walks through the
  complete process: from reworking GPT-2''s embedding, positional encoding, and attention
  mechanisms to handle GPS trajectories, to implementing a delta-encoding training
  scheme and masked prediction tasks.'
---

# Building a Foundation Model for Trajectory from Scratch

## Quick Facts
- arXiv ID: 2511.20610
- Source URL: https://arxiv.org/abs/2511.20610
- Authors: Gaspard Merten; Mahmoud Sakr; Gilles Dejaegere
- Reference count: 16
- Key outcome: Tutorial demonstrates building a minimal trajectory foundation model from scratch by adapting GPT-2 for spatiotemporal data

## Executive Summary
This tutorial provides a comprehensive walkthrough for building a trajectory foundation model adapted from GPT-2, demonstrating how to process GPS trajectories as sequential data. The implementation covers reworking GPT-2's embedding, positional encoding, and attention mechanisms to handle continuous spatiotemporal inputs through delta encoding and learnable projections. Drawing from state-of-the-art models like TrajFM and TrajGPT, the tutorial shows how to integrate techniques such as Rotary Positional Embeddings, patching, and masked prediction tasks into a working prototype. The resulting educational model enables trajectory completion, next-step prediction, and transfer learning, with modular PyTorch code provided for reproducibility.

## Method Summary
The method adapts GPT-2 for trajectory data by replacing the tokenizer with custom parsers for (latitude, longitude, timestamp) triplets, implementing a learnable projection layer instead of token embeddings, and applying delta-encoding to predict relative changes between consecutive steps. The architecture uses sinusoidal positional encoding from "Attention is All You Need" and reduces the transformer to 2 blocks for educational purposes. A streaming data loader processes continuous trajectories, and masked variants enable filling missing segments. The tutorial walks through each component modification systematically, providing code implementations and validation steps including an autoencoder pretext task to verify embedding quality.

## Key Results
- Successfully adapts GPT-2 architecture for continuous spatiotemporal trajectory data
- Implements delta-encoding scheme that stabilizes training by predicting relative changes rather than absolute coordinates
- Creates modular PyTorch implementation supporting trajectory completion, next-step prediction, and transfer learning
- Provides educational framework bridging general-purpose language models and mobility-specific architectures

## Why This Works (Mechanism)

### Mechanism 1: Delta Encoding Stabilizes Continuous Coordinate Prediction
Predicting relative changes between consecutive trajectory points rather than absolute coordinates improves training stability for GPS data. The model learns to predict Δ(lat, lon, t) at each step, constraining the output range and reducing scale variance across geographic regions. This aligns with autoregressive modeling where each prediction conditions on prior steps. The core assumption is that trajectory increments follow a more bounded, consistent distribution than raw coordinates. Evidence includes the explicit implementation of delta encoding and its theoretical advantages for scale stability. The break condition occurs when trajectories span highly heterogeneous speed regimes (e.g., walking vs. highway driving), where delta magnitudes may still vary drastically and potentially destabilize learning.

### Mechanism 2: Learnable Spatiotemporal Projection Replaces Discrete Token Embeddings
Mapping continuous (lat, lon, t) inputs through a learnable projection layer into a shared latent space enables the transformer to process trajectory data without discretization. Each point is decomposed into normalized coordinates plus temporal components (day, hour, minute, second), then projected via a neural network. This preserves continuous structure while creating fixed-dimension inputs compatible with transformer blocks. The core assumption is that the projection layer can learn representations that preserve sufficient spatial and temporal discriminability. Evidence includes the architectural description of replacing GPT-2's embedding layer with a learnable projection and references to similar approaches in TrajFM. The break condition occurs when projection dimensionality is too low, causing spatial resolution degradation, or when temporal features are inadequately encoded, leading to loss of periodicity patterns.

### Mechanism 3: Masked Segment Prediction Enables Multi-Task Generalization
Training with both point-level and segment-level masking allows a single model to support trajectory completion, next-step prediction, and interpolation tasks. Dimension-specific masking (spatial vs. temporal) and segment masking (continuous trajectory gaps) force the model to learn bidirectional context, extending beyond purely causal next-token prediction. The core assumption is that masked positions can be reconstructed from surrounding context with sufficient regularity in mobility patterns. Evidence includes the explicit implementation of masked prediction tasks and references to dimension-specific masking strategies inspired by BERT's Masked Language Modeling. The break condition occurs when trajectories are highly irregular or sparse, creating unrecoverable information gaps that lead to hallucinated interpolations.

## Foundational Learning

- Concept: **Latent Space Representations**
  - Why needed here: The tutorial explicitly introduces latent space as the foundation for understanding how trajectory points are encoded into vectors the model manipulates internally.
  - Quick check question: Can you explain why two nearby GPS points might have similar latent vectors but very different coordinate values?

- Concept: **Positional Encoding in Transformers**
  - Why needed here: Trajectory data is inherently sequential; positional encoding injects order information that embeddings alone lack. The paper discusses both GPT-2's approach and RoPE alternatives.
  - Quick check question: What would happen to trajectory prediction if you removed positional encoding entirely?

- Concept: **Autoregressive vs. Masked Prediction**
  - Why needed here: The tutorial contrasts next-step (causal) prediction with masked segment completion, both used in trajectory foundation models.
  - Quick check question: Given a trajectory with a 10-minute gap, which approach would better fill it—pure autoregressive or masked segment prediction?

## Architecture Onboarding

- Component map:
  Raw (lat, lon, t) triplets → Normalization → Feature decomposition → Learnable projection → Positional encoding → Transformer blocks (2) → Output heads (delta prediction/region classification) → Training objectives (next-step delta prediction, masked segment completion)

- Critical path:
  1. Build data loader that streams (lat, lon, t) triplets and applies delta transformation
  2. Implement projection layer; validate with autoencoder pretext task (can you reconstruct inputs?)
  3. Add positional encoding; re-validate reconstruction
  4. Integrate transformer blocks; train on masked prediction
  5. Evaluate on held-out trajectories using completion and next-step metrics

- Design tradeoffs:
  - Fewer transformer blocks (2 vs. 12+): Faster iteration, lower capacity—suitable for educational prototyping but likely insufficient for production transfer learning
  - Delta vs. absolute prediction: Stability vs. direct interpretability
  - Continuous vs. discrete outputs (TrajGPT region-level): Coordinate precision vs. training simplicity with cross-entropy loss

- Failure signatures:
  - Predicted deltas cluster near zero → model learned to output mean; check learning rate or loss scaling
  - Reconstructions from autoencoder pretext task fail → projection layer under-capacity or input normalization issue
  - Large geographic transfer performance drop → positional encoding not generalizing (consider RoPE or geography-agnostic normalization)

- First 3 experiments:
  1. **Embedding validation**: Train a small autoencoder to reconstruct trajectory points from projected embeddings. If reconstruction error is high, increase projection dimensionality or improve normalization.
  2. **Delta vs. absolute comparison**: Train identical architectures on delta-encoded vs. raw coordinate targets. Compare training stability (loss variance) and final MSE on a held-out test set.
  3. **Masking ablation**: Train three variants—(a) pure autoregressive, (b) point-level masking only, (c) segment masking. Evaluate each on next-step prediction and gap-filling tasks to quantify multi-task benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Between continuous coordinate prediction (as in TrajFM) versus discrete region/POI prediction (as in TrajGPT), which approach provides better transferability and downstream task performance for trajectory foundation models?
- Basis in paper: [explicit] The paper contrasts these approaches: "While TrajFM predicts continuous spatial coordinates... TrajGPT specifically reframes trajectory prediction as a sequence prediction task of regions" and notes this "enables us to discuss the trade-offs between these methodologies" without resolving the trade-off.
- Why unresolved: The paper presents both approaches but implements only continuous prediction; no empirical comparison is made despite the architectural differences being highlighted.
- What evidence would resolve it: Controlled experiments comparing identical model architectures with continuous vs discrete prediction heads on standardized trajectory benchmarks across multiple geographic regions.

### Open Question 2
- Question: How does delta encoding (predicting relative changes) compare to absolute coordinate prediction in terms of cross-regional transferability and different downstream tasks?
- Basis in paper: [explicit] "Instead of predicting absolute coordinates and timestamps, which can vary widely in scale, we use delta encoding... This creates a more stable and learnable target." The paper adopts this approach without empirical validation against alternatives.
- Why unresolved: Delta encoding is implemented for pedagogical purposes; no comparative evaluation against absolute coordinate prediction is provided.
- What evidence would resolve it: Ablation studies with matched architectures comparing encoding schemes on trajectory completion, next-step prediction, and cross-city transfer tasks.

### Open Question 3
- Question: What is the minimum data scale and diversity required for effective pre-training of trajectory foundation models to achieve meaningful transfer learning?
- Basis in paper: [inferred] The paper mentions using synthetic data generators "when real-world datasets are noisy or cumbersome to use" and streaming data loaders for scalability, but provides no guidance on data requirements for effective pre-training.
- Why unresolved: The tutorial focuses on implementation mechanics rather than scaling properties; transfer learning is listed as an outcome but not systematically evaluated.
- What evidence would resolve it: Scaling experiments varying pre-training dataset size and geographic diversity, measuring downstream performance on held-out regions.

### Open Question 4
- Question: What architectural depth (number of transformer blocks) is optimal for trajectory foundation models when balancing computational efficiency with modeling capacity?
- Basis in paper: [explicit] The authors "reduce the GPT-2 architecture to just two transformer blocks" to "simplify training, prevent overfitting on smaller datasets," but do not evaluate whether this educational choice affects model capability.
- Why unresolved: The 2-block design is a pedagogical simplification; the relationship between depth and trajectory modeling performance remains unexplored.
- What evidence would resolve it: Systematic comparison of models with varying depths (2, 4, 6, 12 blocks) on trajectory benchmarks with varying dataset sizes.

## Limitations
- Lack of specific quantitative evaluation metrics and benchmark comparisons against established trajectory datasets
- Reduced architecture (2 transformer blocks) may limit real-world applicability despite being appropriate for educational purposes
- Delta-encoding assumption may not hold for heterogeneous mobility patterns spanning different speed regimes
- Learnable projection layer's effectiveness not empirically validated against discretization alternatives

## Confidence
- High confidence in the pedagogical value and reproducibility of the implementation approach, given detailed walkthrough and open-source code availability
- Medium confidence in the delta-encoding mechanism's general effectiveness for training stability, supported by logical argument but lacking direct comparative evidence
- Medium confidence in the masked prediction framework's ability to support multi-task generalization, based on architectural reasoning and parallels to BERT but without specific trajectory dataset validation
- Low confidence in quantitative performance claims, as no specific metrics, benchmarks, or comparative results are provided in the paper

## Next Checks
1. **Autoencoder pretext task validation**: Before full model training, verify that the learnable projection layer preserves spatial information by training an autoencoder to reconstruct trajectory points from projected embeddings. Compare reconstruction error against varying projection dimensionalities to identify optimal capacity.

2. **Delta vs. absolute prediction ablation**: Train identical architectures on delta-encoded versus raw coordinate targets using the same dataset. Measure training stability (loss variance over first 100 steps) and final MSE on a held-out test set to quantify the practical benefit of delta encoding.

3. **Masking strategy ablation**: Implement and train three variants—(a) pure autoregressive (next-step only), (b) point-level masking only, and (c) segment masking with both spatial and temporal gaps. Evaluate each on a held-out trajectory completion task with artificially inserted gaps of varying lengths to measure the incremental benefit of segment-level masking.