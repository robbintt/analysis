---
ver: rpa2
title: 'CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense
  Differentiation'
arxiv_id: '2508.04494'
source_url: https://arxiv.org/abs/2508.04494
tags:
- cale
- word
- lexical
- semantic
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating contextualized embeddings
  that can accurately represent semantic concepts across both same-lemma and cross-lemma
  contexts. To achieve this, the authors introduce Concept Differentiation, a binary
  classification task that extends Word-in-Context by comparing semantic concepts
  across any word pair, not just within the same lemma.
---

# CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation

## Quick Facts
- arXiv ID: 2508.04494
- Source URL: https://arxiv.org/abs/2508.04494
- Reference count: 40
- Primary result: CALE fine-tuned embeddings reach 79.3 balanced accuracy on Concept Differentiation, outperforming baselines including XL-LEXEME and XLM-R.

## Executive Summary
This paper introduces Concept-Aligned Embeddings (CALE), a framework that uses contrastive learning to create contextualized embeddings that accurately represent semantic concepts across both same-lemma and cross-lemma contexts. The authors construct SPCD, a dataset derived from SemCor with pairs of word occurrences labeled by their WordNet synset concepts, and fine-tune representation models to align embeddings of occurrences referring to the same concept. CALE models significantly outperform strong baselines on Concept Differentiation, Lexical Semantic Change Detection, and In-context Lexical Similarity tasks, with analysis showing embeddings shift from lemma-centric to concept-centric representations.

## Method Summary
The CALE framework constructs the SPCD dataset from SemCor by filtering sentences (10-100 words) and lemmas (Nouns/Verb/Adjective, ≥3 letters, ≥10 occurrences, non-proper nouns) and partitioning unique concepts and lemmas into train/val/test splits. Target word occurrences are formatted with `<t> ... </t>` delimiters and paired to create Same Concept/Same Lemma, Same Concept/Diff Lemma, Diff Concept/Same Lemma, and Diff Concept/Diff Lemma pairs. A Siamese network architecture with weight-shared encoders (XLM-R or ModernBERT) is fine-tuned using margin-based contrastive loss (m=0.7) with Adam optimizer (LR=6.02e-6, warmup ratio=0.24, weight decay=0.05) for one epoch. Classification uses cosine distance with a threshold optimized on validation data.

## Key Results
- CALE achieves 79.3 balanced accuracy on Concept Differentiation, significantly outperforming baselines (XL-LEXEME, XLM-R)
- On Lexical Semantic Change Detection, CALE reaches Spearman correlations of .78 (German), .65 (Spanish), .75 (Italian), .83 (Swedish) using English-only training
- CALE shows improved alignment with WordNet structure, with Silhouette scores rising from -0.041 to 0.042 and Spearman correlation with Wu-Palmer similarity exceeding 0.50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning on both intra-lemma and inter-lemma pairs shifts embedding organization from lemma-centric to concept-centric.
- Mechanism: The margin-based contrastive loss pulls same-concept occurrences together regardless of word form while pushing different-concept pairs apart, even when they share the same lemma, forcing attention to conceptual meaning rather than surface lexical identity.
- Core assumption: WordNet synsets provide valid ground-truth for "same concept" labels and binary same/different distinction captures meaningful semantic structure.
- Evidence anchors:
  - [Section 6] Figure 1 shows DC/SL distances increase from avg .130 to .343 while SC/DL distances decrease from .157 to .269 after CALE fine-tuning
  - [Section 6] Spearman correlation with Wu-Palmer similarity in WordNet rises to >0.50 for CALE models vs. 0.31-0.49 for baselines
  - [corpus] Related work on multi-sense embeddings (arxiv 2504.06036) finds explicit sense separation improves downstream tasks

### Mechanism 2
- Claim: Target word delimitation with special tokens concentrates representation learning on the marked word's contextual meaning.
- Mechanism: `<t></t>` tags act as attention focal points, ensuring the pooled sequence embedding reflects the target word's sense in context rather than aggregate sentence semantics, critical for same-sentence multi-target disambiguation.
- Core assumption: The pooling layer preserves target-specific information when delimiters are present.
- Evidence anchors:
  - [Section 4] Explicitly states delimiters ensure "the representation is specific to that target" and "distinct from the embedding of another target word occurring in the same sentence"
  - [Section 5.3] For CoSimLex, two separate input sequences are created with delimiters around each target
  - [corpus] LANE (arxiv 2511.11234) uses adversarial training to shift focus to local semantic details

### Mechanism 3
- Claim: English-only contrastive fine-tuning on multilingual pre-trained models yields cross-lingual concept transfer.
- Mechanism: XLM-R's multilingual pre-training creates shared cross-lingual space; fine-tuning on English CALE data reorganizes this space along conceptual lines that generalize because semantic relations are broadly consistent across languages.
- Core assumption: Conceptual structure is sufficiently parallel across languages for this transfer to hold.
- Evidence anchors:
  - [Section 5.2] CALE (XLM-R) matches or outperforms XL-LEXEME on LSCD in 5/6 languages despite English-only training
  - [Section 5.3] CoSimLex results show CALE achieving .63-.74 correlation across Finnish, Croatian, Slovenian without target-language training
  - [corpus] No direct corpus evidence on this specific transfer mechanism

## Foundational Learning

- Concept: **Contrastive learning objectives**
  - Why needed here: CALE's core training signal is a margin-based contrastive loss; understanding how positive/negative pairs shape embedding geometry is essential.
  - Quick check question: Given a margin m=0.7, what happens to loss when a negative pair has cosine similarity 0.5 vs. 0.8?

- Concept: **WordNet synsets as concept labels**
  - Why needed here: The SPCD dataset derives all labels from SemCor's WordNet annotations; "same concept" means shared synset ID.
  - Quick check question: If two occurrences of "bank" map to synsets financial_institution_01 and river_bank_01, what label would this pair receive?

- Concept: **Siamese network architectures**
  - Why needed here: CALE uses weight-shared twin encoders to process paired occurrences into a common embedding space for comparison.
  - Quick check question: During inference, can you encode each occurrence independently, or must pairs always be processed together?

## Architecture Onboarding

- Component map:
```
Input: sentence with <t>target_word</t> delimiters
   ↓
Tokenizer (subword split, delimiters preserved as separate tokens)
   ↓
Pre-trained encoder (XLM-R / ModernBERT / XL-LEXEME)
   ↓
Pooling layer (mean over final-layer token embeddings)
   ↓
Output: 1024-dim embedding for the target word in context
```

- Critical path:
  1. Delimiter placement during data preprocessing (must wrap exact target span)
  2. Margin parameter in contrastive loss (m=0.7 from hyperparameter search)
  3. Threshold calibration on validation split for binary classification at inference

- Design tradeoffs:
  - **XLM-R base vs. ModernBERT**: XLM-R enables cross-lingual transfer; ModernBERT shows stronger WordNet structure alignment (Silh=0.042 vs. -0.041) but is English-only
  - **Binary vs. graded labels**: Current formulation uses hard same/different; this may miss nuanced similarity gradations
  - **Mid vs. late layers**: Pre-fine-tuning, mid layers (14-17) perform best; post-fine-tuning, the pooling layer renders this choice less relevant

- Failure signatures:
  - Low Silhouette scores (<0.1) even for CALE: indicates substantial overlap between synset clusters—likely due to WordNet's fine granularity
  - Negative Silhouette on Unique Beginners (broad categories like "entity"): CALE does not organize by coarse semantic groups
  - Latin LSCD correlation near zero: all models struggle, suggesting dataset/resource issues

- First 3 experiments:
  1. **Ablate inter-lemma pairs**: Train on same-lemma pairs only; expect DL accuracy to drop toward baseline (50%), confirming that cross-lemma supervision enables synonym detection
  2. **Probe layer sensitivity post-fine-tuning**: Extract embeddings from different encoder layers; verify that fine-tuning homogenizes layer utility for semantic tasks
  3. **Test on out-of-domain concepts**: Evaluate on held-out concept classes (e.g., technical jargon) to assess whether the model generalizes beyond WordNet's conceptual coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CALE framework be effectively applied to capture the meaning of Multi-Word Expressions (MWEs)?
- Basis in paper: [explicit] The authors state in Section 4 and the Conclusion that while the study limited scope to single-word targets, "our framework could be applied to capture the meaning of Multi-Word Expressions in future work."
- Why unresolved: The current implementation and dataset (SPCD) focus exclusively on single-word targets and filter out compounds or non-single-word lemmas.
- What evidence would resolve it: An evaluation of CALE models fine-tuned on a dataset of MWE occurrences, measuring performance on tasks like MWE disambiguation or in-context similarity for compound terms.

### Open Question 2
- Question: Does incorporating structured ontological knowledge (e.g., hypernymy, graded similarity) improve CALE's ability to model semantic concepts?
- Basis in paper: [explicit] The Introduction and Limitations sections note that the work focuses only on synonymy and polysemy, leaving "the integration of other relations such as hyponymy" to future work, and that the binary task may oversimplify graded similarity.
- Why unresolved: The model currently receives no direct supervision regarding the ontological hierarchy or fine-grained relations between concepts, treating concept differentiation as a binary classification problem.
- What evidence would resolve it: A modified training objective that includes relational constraints, evaluated on benchmarks requiring hierarchical reasoning (e.g., hypernym discovery) or graded similarity scores.

### Open Question 3
- Question: Would training on specifically-curated, human-annotated datasets improve CALE's performance over the current WordNet-derived supervision?
- Basis in paper: [explicit] The Conclusion suggests that "Future work could explore specifically-curated datasets for Concept Differentiation, with human annotators and a different annotation scheme than WordNet's synsets."
- Why unresolved: The current SPCD dataset is derived automatically from SemCor using WordNet synsets, which may inherit the resource's specific sense distinctions or noise.
- What evidence would resolve it: A comparative study where CALE is fine-tuned on a human-annotated concept differentiation dataset and evaluated against the current WordNet-supervised baseline.

### Open Question 4
- Question: Does explicit multilingual fine-tuning enhance CALE's robustness in lower-resource or typologically distant languages?
- Basis in paper: [explicit] The Limitations section states that "A more rigorous multilingual training and evaluation setup is needed to assess CALE's robustness across languages," as fine-tuning was performed exclusively on English data.
- Why unresolved: While the model shows cross-lingual transferability via XLM-R, it is unclear if English-only fine-tuning is sufficient for all language types or if multilingual supervision is required for robust performance.
- What evidence would resolve it: Evaluation of a multilingually fine-tuned CALE model on non-English LSCD and CoSimLex benchmarks, specifically analyzing performance in typologically distant or low-resource languages.

## Limitations
- Reliance on WordNet synsets as ground-truth concepts may introduce misalignment with true semantic boundaries, particularly for graded or context-dependent meanings
- Pair generation algorithm for SPCD is underspecified, particularly for selecting "unrelated" negative examples, which could affect training signal quality
- Cross-lingual transfer mechanism lacks direct empirical validation—results are positive but mechanism is not directly analyzed

## Confidence
- **High confidence**: Contrastive learning mechanism for within-lemma differentiation (supported by clear distance metrics and Silhouette score improvements)
- **Medium confidence**: Cross-lemma synonym detection capability (results are strong but depend on WordNet's coverage and granularity)
- **Medium confidence**: Cross-lingual transfer from English-only training (empirical results support transfer but mechanism is not directly validated)
- **Medium confidence**: Target word delimitation effectiveness (supported by task-specific design but no ablation studies provided)

## Next Checks
1. **Ablation of inter-lemma pairs**: Retrain CALE using only same-lemma pairs and measure DL accuracy degradation. If accuracy drops toward 50%, this would confirm that cross-lemma supervision is essential for synonym detection.
2. **Cross-lingual embedding space analysis**: For languages with strong LSCD performance (German, Italian, Swedish), compute and compare intra-concept vs. inter-concept distances across languages to verify conceptual alignment persists in the shared space.
3. **Threshold calibration sensitivity**: Systematically vary the classification threshold around the optimal value and measure accuracy changes to determine robustness and identify potential overfitting to validation data.