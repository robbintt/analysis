---
ver: rpa2
title: 'From Recall to Reasoning: Automated Question Generation for Deeper Math Learning
  through Large Language Models'
arxiv_id: '2505.11899'
source_url: https://arxiv.org/abs/2505.11899
tags:
- generation
- question
- level
- genai
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored using generative AI for creating high-quality
  math practice problems across varying cognitive depths. Initial testing showed GenAI
  could produce relevant questions with basic support, but quality and correctness
  improved when more context was provided.
---

# From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models

## Quick Facts
- arXiv ID: 2505.11899
- Source URL: https://arxiv.org/abs/2505.11899
- Reference count: 34
- Primary result: QG-DOK framework combining RAG with DOK taxonomy significantly improves question quality and depth alignment, especially for higher-order thinking skills.

## Executive Summary
This study addresses the challenge of generating high-quality math practice problems across varying cognitive depths using generative AI. Initial testing revealed that while GenAI could produce relevant questions with basic support, quality and correctness improved with more context—though relevance sometimes declined due to hallucinations. To overcome these limitations, the researchers developed QG-DOK, a framework that integrates retrieval-augmented generation with Webb's Depth of Knowledge taxonomy. The system retrieves relevant mathematical content and generates questions at four cognitive levels, showing significant improvements in appropriateness and lexical diversity, particularly for higher-order thinking skills.

## Method Summary
The QG-DOK framework combines retrieval-augmented generation with Webb's Depth of Knowledge taxonomy to generate math questions at four cognitive levels. The system preprocesses mathematical content, creates vector embeddings using text-embedding-ada-002, stores them in Pinecone, and retrieves relevant chunks based on queries. It then generates questions using structured DOK prompt templates with retrieved context. The framework was evaluated using three LLMs (GPT-4o, DeepSeek-V3, Gemini-1.5-Pro) with metrics including relevance, DOK alignment, appropriateness (via G-Eval), and PINC scores for lexical diversity.

## Key Results
- DOK+RAG consistently outperformed DOK-only across all models, particularly for higher-order thinking skills (DOK Levels 3 & 4)
- GPT-4o showed the most significant gains in appropriateness, with DOK alignment improvements of 0.19-0.23
- High PINC scores (average 0.92) indicated strong lexical diversity in generated questions
- Challenges remained with mid-level cognitive complexity (DOK Level 2) and mathematical notation handling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation grounds outputs in source materials, reducing hallucination-driven relevance loss.
- Mechanism: Vector database retrieval provides semantically relevant content chunks that constrain the LLM's generation space, forcing it to source from provided materials rather than extrapolating from parametric knowledge.
- Core assumption: The embedding model captures mathematical semantic relationships adequately, and retrieved chunks contain the necessary information for the target concept.
- Evidence anchors: Abstract shows quality improved with context but relevance declined due to hallucinations; section discusses RAG approaches to ground outputs in provided materials.

### Mechanism 2
- Claim: Structured DOK prompt templates with level-specific descriptions and examples guide LLMs toward target cognitive complexity.
- Mechanism: The prompt template encodes DOK level definitions, example contexts, math-specific examples, and reasoning patterns, providing the LLM with concrete patterns to emulate for each cognitive level.
- Core assumption: LLMs can reliably interpret and apply abstract cognitive complexity descriptors when given sufficient structured examples.
- Evidence anchors: Abstract shows DOK+RAG outperformed DOK-only for higher-order skills; Figure 2 shows prompt template structure with description, example, math-example, and reasoning fields.

### Mechanism 3
- Claim: Context quantity creates a quality tradeoff: correctness improves with more context while relevance can degrade through expanded hallucination pathways.
- Mechanism: Additional context broadens the generation space, introducing more opportunities for the model to synthesize plausible-but-ungrounded connections that fall outside the actual course scope.
- Core assumption: LLMs lack inherent constraints to confine generation strictly to provided context boundaries.
- Evidence anchors: Abstract notes relevance declined with additional context due to hallucinations; Table 1 shows relevance dropping from 1.00 to 0.80 while correctness improves from 0.60 to 1.00.

## Foundational Learning

- Concept: Webb's Depth of Knowledge (DOK) Taxonomy
  - Why needed here: DOK provides the cognitive complexity framework that structures all question generation; understanding the four levels is essential for configuring prompts, evaluating outputs, and diagnosing alignment failures.
  - Quick check question: Given a calculus problem asking students to "prove that the Intermediate Value Theorem guarantees a root exists in [0,2] for f(x) = x³ - 4x + 1," can you classify this as DOK Level 1, 2, 3, or 4—and explain why?

- Concept: Retrieval-Augmented Generation (RAG) Architecture
  - Why needed here: QG-DOK's core improvement comes from RAG integration; understanding embedding, chunking, vector databases, and retrieval mechanics is prerequisite to implementing or modifying the framework.
  - Quick check question: If your retrieval system returns chunks with high semantic similarity but incorrect mathematical context (e.g., "compactness theorem" chunks when querying "continuity"), where in the pipeline should you investigate first—embedding quality, chunk boundaries, or query formulation?

- Concept: Evaluation Metrics for Generative Quality (G-Eval, PINC)
  - Why needed here: Interpreting results and designing your own experiments requires understanding what these metrics measure—and their limitations.
  - Quick check question: A model achieves PINC = 0.94 and G-Eval relevance = 0.85, but human evaluation reveals mathematical errors in 30% of answers. What does this reveal about the gaps between these metrics and actual educational utility?

## Architecture Onboarding

- Component map: Data Preprocessing -> Vectorization Layer -> Retrieval Engine -> Generation Layer
- Critical path: Data preprocessing quality determines embedding quality → Embedding quality determines retrieval relevance → Retrieval relevance + prompt structure determine output alignment → All downstream metrics depend on this chain
- Design tradeoffs:
  - Fixed-size vs. semantic chunking: Fixed chunks are simpler but may split mathematical expressions mid-formula
  - Temperature 0 vs. default: Zero temperature improves reproducibility for evaluation but may reduce question diversity
  - DOK-only vs. DOK+RAG: RAG adds infrastructure complexity but significantly improves higher-order question quality (Levels 3-4)
- Failure signatures:
  - Level 2 alignment inconsistency across models (mid-level cognitive complexity is ambiguously defined)
  - LaTeX notation corruption in retrieved chunks or generated output
  - High PINC scores co-occurring with low DOK alignment (model generates diverse but cognitively shallow questions)
  - Relevance degradation when source materials span multiple related but distinct topics
- First 3 experiments:
  1. Reproduce the DOK-only vs. DOK+RAG comparison on your own mathematical content corpus to validate whether the 0.19-0.23 DOK alignment improvements generalize beyond the study's source materials.
  2. Test chunking strategies by comparing fixed-size (current implementation) against math-aware semantic chunking that preserves LaTeX blocks, measuring impact on notation integrity and retrieval precision.
  3. Isolate the DOK Level 2 alignment problem by creating a targeted evaluation set of mid-complexity questions, then iteratively refine the Level 2 prompt template with additional math-specific examples to measure alignment improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a step-by-step framework derived from human expert problem-design processes improve LLM alignment with cognitive depth levels compared to description-based prompting?
- Basis in paper: The authors note that describing DOK levels may be insufficient and suggest future work should "explore how educators design problems to derive a step-by-step framework for AI-driven question generation."
- Why unresolved: The current implementation relied on simple descriptions of Webb's Depth of Knowledge, which resulted in inconsistent depth alignment, suggesting the model lacks the procedural knowledge to construct complex questions.
- What evidence would resolve it: A comparative evaluation showing that a framework derived from educator workflows yields statistically higher DOK alignment scores than the description-based method used in Study 2.

### Open Question 2
- Question: How can LLMs be optimized to consistently generate questions at DOK Level 2 (Skills/Concepts), where current models show the most significant inconsistency?
- Basis in paper: The results indicated that "depth alignment at Level 2 was inconsistent across the models, suggesting that LLMs struggle with categorizing mid-level cognitive complexity."
- Why unresolved: While the DOK+RAG framework improved performance for higher-order thinking (Levels 3 & 4), the semantic boundary between basic recall and routine problem solving (Level 2) remains difficult for models to categorize and generate.
- What evidence would resolve it: A targeted ablation study resulting in consistently high DOK alignment scores for Level 2 specifically, potentially requiring a specialized taxonomy or few-shot examples for that cognitive tier.

### Open Question 3
- Question: To what extent does adopting LaTeX-based representation in the data preprocessing pipeline resolve the persistent errors in mathematical notation handling?
- Basis in paper: The discussion notes that "challenges remained... in mathematical notation handling that would benefit from LaTeX-based representation in future implementations."
- Why unresolved: The current text-based embedding and generation process introduced notation errors, but the proposed solution (LaTeX integration) was suggested but not implemented or tested.
- What evidence would resolve it: An evaluation of the QG-DOK framework comparing standard text output against LaTeX-integrated output, showing a measurable reduction in syntax errors and an increase in mathematical correctness scores.

## Limitations
- Evaluation relied entirely on automated metrics (G-Eval, PINC) without human expert validation of mathematical correctness or pedagogical appropriateness
- Mathematical corpus was limited to logic and proof-based content, leaving open questions about performance across diverse mathematical domains
- Level 2 DOK alignment inconsistency across all models suggests the taxonomy may be inadequately specified for mid-complexity mathematical tasks

## Confidence
- **High confidence**: RAG consistently improves higher-order question quality (DOK Levels 3-4) across multiple LLMs; GPT-4o shows largest appropriateness gains; PINC scores confirm lexical diversity
- **Medium confidence**: DOK+RAG outperforms DOK-only for overall DOK alignment (0.19-0.23 improvement); context quality tradeoffs (correctness up, relevance down) are reproducible
- **Low confidence**: Level 2 alignment improvements are inconsistent; generalizability to non-logic mathematical domains; automated metrics capture actual educational utility

## Next Checks
1. **Human expert validation study**: Have mathematics educators independently evaluate a stratified sample of generated questions for mathematical correctness, pedagogical appropriateness, and DOK alignment fidelity
2. **Cross-domain corpus test**: Replicate the framework using different mathematical domains (calculus, linear algebra, statistics) to assess whether RAG benefits persist across mathematical content types
3. **Fine-grained DOK refinement**: Create a calibration set of Level 2 mathematical tasks with expert annotations, then iteratively refine prompt templates to resolve alignment inconsistencies observed across all tested models