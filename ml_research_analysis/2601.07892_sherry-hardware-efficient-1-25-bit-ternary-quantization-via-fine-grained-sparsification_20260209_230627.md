---
ver: rpa2
title: 'Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained
  Sparsification'
arxiv_id: '2601.07892'
source_url: https://arxiv.org/abs/2601.07892
tags:
- ternary
- quantization
- arxiv
- sherry
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sherry introduces a hardware-efficient 1.25-bit ternary quantization
  framework for LLMs by combining a 3:4 structured sparsity pattern with an annealing
  residual synapse module (Arenas). This design achieves power-of-two alignment with
  SIMD lanes, enabling 25% bit savings and 10% inference speed-up on commodity CPUs
  while matching state-of-the-art ternary performance.
---

# Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification

## Quick Facts
- **arXiv ID**: 2601.07892
- **Source URL**: https://arxiv.org/abs/2601.07892
- **Reference count**: 40
- **Primary result**: Achieves zero accuracy loss vs SOTA ternary methods while providing 10% inference speedup on commodity CPUs

## Executive Summary
Sherry introduces a hardware-efficient 1.25-bit ternary quantization framework for LLMs by combining a 3:4 structured sparsity pattern with an annealing residual synapse module (Arenas). This design achieves power-of-two alignment with SIMD lanes, enabling 25% bit savings and 10% inference speed-up on commodity CPUs while matching state-of-the-art ternary performance. Sherry addresses weight trapping in sparse training through gradient diversity injection, ensuring representational capacity is preserved. Evaluated on LLaMA-3.2 across five benchmarks, it achieves zero accuracy loss compared to existing SOTA ternary methods.

## Method Summary
Sherry combines 3:4 structured sparsity with an annealing residual synapse mechanism to achieve 1.25-bit ternary quantization. The approach uses Sparse-AbsMean to zero the smallest magnitude weight in each 4-weight block, ternarize the remaining three, and compute a scaling factor α. The Arenas module adds a decaying full-precision residual path during training to prevent weight trapping. Offline packing maps the sparse ternary weights into 5-bit indices (1 sign + 4 index bits) aligned with SIMD word boundaries. Inference uses LUT-based engines that replace multiplications with memory lookups and integer additions.

## Key Results
- Achieves zero accuracy loss vs SOTA ternary methods on LLaMA-3.2-1B/3B across five benchmarks
- Provides 10% inference speedup on Intel i7-14700HX CPU compared to 1.67-bit ternary baseline
- Eliminates weight trapping through gradient diversity injection during training

## Why This Works (Mechanism)

### Mechanism 1: 3:4 Structured Sparsity for SIMD-Aligned Packing
Enforcing exactly 3 non-zero weights per 4-weight block achieves 1.25-bit effective width while maintaining power-of-two alignment for SIMD efficiency. The constraint yields C(4,3) × 2³ = 32 unique permutations, saturating a 5-bit index space. This packs 4 weights into 5 bits (1.25 bits/weight) with zero bit wastage, unlike 1.67-bit strategies that create 3-way patterns incompatible with SIMD lane widths.

### Mechanism 2: Arenas Residual Synapse for Gradient Diversity Injection
An annealing full-precision residual path during training prevents weight trapping (collapse to binary-like distributions) caused by gradient homogenization. Forward pass becomes Y = XTα + λ_t·XW, where λ_t decays from 1→0. The residual term injects heterogeneous gradients, breaking the rank collapse observed in 3:4 sparse training. By training end, λ_t→0 removes all overhead.

### Mechanism 3: LUT-Based Inference Eliminating Multiplications
Lookup table engines transform ternary dot products into memory fetches and integer additions, realizing the 10% speedup on commodity CPUs. Offline packing creates 5-bit indices (1 sign + 4 index bits). Online inference pre-computes activation LUTs per segment; weight indices retrieve pre-summed values. Final accumulation uses integer addition with channel-wise scaling.

## Foundational Learning

- **Ternary Quantization Basics**
  - Why needed: Understanding that {-1, 0, +1} weights enable addition-only inference is prerequisite to grasping why Sherry's packing matters.
  - Quick check: Explain why a weight constrained to {-1, 0, +1} eliminates multiplication during inference.

- **N:M Structured Sparsity**
  - Why needed: 3:4 sparsity is a specific N:M pattern; knowing how NVIDIA's 2:4 sparsity works helps contrast hardware alignment differences.
  - Quick check: Why does 2:4 sparsity on GPUs not directly apply to 1.25-bit ternary packing?

- **Straight-Through Estimator (STE) in QAT**
  - Why needed: Arenas operates during quantization-aware training; STE is the baseline gradient approximation method being augmented.
  - Quick check: How does STE handle non-differentiable quantization functions during backpropagation?

## Architecture Onboarding

- **Component map**:
  [Pre-trained LLM] → [3:4 Sparse-AbsMean Quantization] → [QAT with Arenas Module] → [Offline 5-bit Packing] → [LUT Engine (BitNet.cpp compatible)]

- **Critical path**:
  1. Sparse-AbsMean assignment: For each 4-weight block, zero the smallest |W|, ternarize remaining 3 via sign(W), compute α as mean of non-zero |W|
  2. Arenas forward augmentation: Y = XTα + λ_t·XW; implement cosine-decay with warmup for λ_t schedule
  3. 5-bit packing: Split into 4-bit index (32 patterns via mirror-symmetry) + 1-bit sign; align to SIMD word boundaries

- **Design tradeoffs**:
  - Bit savings vs. sparsity threshold: 3:4 (25% sparse) is optimal; 2:4 (50% sparse) degrades accuracy per prior work
  - Training overhead vs. inference efficiency: Arenas adds ~1 forward pass equivalent per step (residual XW); amortized over one-time QAT
  - LUT granularity vs. scaling factor storage: Per-group (128) balances accuracy (0.519 avg) vs. memory overhead; per-tensor drops to 0.502

- **Failure signatures**:
  - Weight trapping: Weight histogram shows binary-like polarization (±1 dominant, near-zero intermediate values) → check Effective Rank of gradients; add warmup to λ_t schedule
  - SIMD misalignment: Inference slower than 2-bit baseline → verify 4-way blocking is preserved through packing
  - Accuracy cliff at 50%+ sparsity: If experimenting with N:M variants, 2:4 is the hard lower bound

- **First 3 experiments**:
  1. Validate packing efficiency: Implement 5-bit packing/unpacking on a toy 4×4 weight matrix; verify 32 unique patterns map bijectively
  2. Ablate Arenas: Train LLaMA-1B with 3:4 sparsity, comparing: (a) no Arenas, (b) Arenas with linear decay, (c) Arenas with cosine+warmup. Plot weight histograms and ER per layer
  3. Benchmark on target CPU: Deploy Sherry 1B on Intel i7 (or equivalent), measure tokens/sec vs. BitNet.cpp 1.67-bit (TL2) baseline; confirm ≥10% speedup holds for your SIMD width

## Open Questions the Paper Calls Out

- **Scalability to larger models**: The behavior of the Arenas mechanism and the 3:4 sparsity pattern on larger, server-grade models (70B+) remains to be validated. The evaluation was limited to 1B and 3B models, which are edge-deployment candidates; scaling dynamics of sparse ternary training may change significantly at larger scales.

- **Activation quantization integration**: Future integration with activation quantization could further alleviate memory bottlenecks during long-context inference. This work focuses on weight-only quantization; activations and KV-cache remain in BF16, limiting memory savings during autoregressive generation.

- **Data-center GPU performance**: Future benchmarks on data-center GPUs could further explore its potential for high-throughput server applications. Experiments focused on commodity CPUs (Intel i7-14700HX) for edge deployment; server-specific optimizations were not explored.

- **Generalization to other architectures**: Whether Sherry transfers effectively to non-LLaMA architectures (e.g., Mistral, Gemma, decoder-free models) remains unclear. All experiments use LLaMA-3.2-1B/3B; architectural assumptions about weight distribution and gradient flow may not generalize.

## Limitations
- Zero accuracy loss claim validated only on LLaMA-3.2-1B/3B models without testing on diverse architectures
- 10% speedup claim depends on specific CPU architecture features that may not generalize across all commodity CPUs
- Weight trapping solution shows promising results but lacks ablation studies on alternative sparsity ratios or gradient diversity mechanisms

## Confidence
- **High confidence**: 3:4 structured sparsity packing mechanism achieving 1.25-bit width through 32 permutations in 5-bit space; Arenas residual synapse adding gradient diversity during training
- **Medium confidence**: Zero accuracy loss claim vs SOTA ternary methods; 10% inference speedup on Intel i7-14700HX; weight trapping prevention mechanism effectiveness
- **Low confidence**: Generalization to other LLM architectures beyond Llama-3.2; performance portability across different CPU architectures; long-context inference behavior with KV-cache

## Next Checks
1. Replicate Sherry 1B training on alternative LLM architectures (e.g., Mistral, Qwen) to test zero accuracy loss generalization claims across different model families
2. Benchmark inference performance on multiple CPU architectures (AMD Zen, ARM-based systems) to verify 10% speedup claim is not architecture-specific
3. Conduct ablation studies comparing Arenas residual synapse against alternative gradient diversity mechanisms (dropout-based, stochastic weight averaging) to isolate the specific contribution to weight trapping prevention