---
ver: rpa2
title: 'Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset
  Size'
arxiv_id: '2509.01541'
source_url: https://arxiv.org/abs/2509.01541
tags:
- graph
- learning
- dataset
- untrained
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates whether Graph Contrastive Learning (GCL)\
  \ outperforms simple untrained baselines, finding that GCL\u2019s advantage depends\
  \ strongly on dataset size and task difficulty. On standard TU datasets, untrained\
  \ Graph Neural Networks (GNNs), simple MLPs, and handcrafted statistics often match\
  \ or exceed GCL performance."
---

# Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size

## Quick Facts
- **arXiv ID**: 2509.01541
- **Source URL**: https://arxiv.org/abs/2509.01541
- **Reference count**: 40
- **Primary result**: GCL's advantage depends strongly on dataset size; untrained GNNs and simple baselines often match or exceed GCL on standard datasets, with crossover at ~4k graphs on OGBG-MOLHIV.

## Executive Summary
This study investigates the empirical performance of Graph Contrastive Learning (GCL) against simple untrained baselines, revealing that GCL's advantage is strongly contingent on dataset size and task difficulty. On standard TU graph classification benchmarks, untrained Graph Neural Networks, simple MLPs, and handcrafted statistics frequently match or surpass GCL performance. However, on the large molecular dataset OGBG-MOLHIV, GCL only outperforms untrained baselines beyond ~4k graphs, with gains plateauing at larger scales. Synthetic datasets with controlled size and difficulty confirm that GCL accuracy scales approximately logarithmically with dataset size, and its performance gap versus untrained GNNs varies with task complexity. The findings underscore the importance of dataset size as a critical factor in GCL evaluation and highlight the diagnostic value of simple baselines.

## Method Summary
The authors conduct a systematic empirical comparison of GCL against untrained baselines across multiple graph datasets. They evaluate standard TU graph classification datasets, the large-scale molecular dataset OGBG-MOLHIV, and synthetic datasets with adjustable size and difficulty. Performance is measured using standard classification accuracy metrics. The study examines the relationship between dataset size and GCL effectiveness, identifying crossover points where untrained baselines are outperformed. The authors also analyze how task difficulty modulates the performance gap between GCL and untrained methods, and propose logarithmic scaling as a model for GCL accuracy growth with dataset size.

## Key Results
- On standard TU datasets, untrained GNNs, simple MLPs, and handcrafted statistics often match or exceed GCL performance.
- On OGBG-MOLHIV, GCL lags untrained baselines at small scales but pulls ahead beyond ~4k graphs, with gains plateauing at larger sizes.
- On synthetic datasets, GCL accuracy scales approximately logarithmically with the number of graphs, and its performance gap versus untrained GNNs varies with task difficulty.

## Why This Works (Mechanism)
GCL's effectiveness is contingent on the availability of sufficient data to learn useful representations. At small dataset sizes, the complexity of GCL objectives and the need for positive and negative pairs may not yield better representations than simple untrained models or handcrafted features. As dataset size increases, the contrastive loss can better capture graph-level invariances, but this advantage saturates due to architectural or sampling limitations.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Used as untrained baselines; needed to establish performance without contrastive pretraining. Quick check: verify GNN architecture matches reported baselines.
- **Contrastive Learning**: Core mechanism for GCL; needed to generate positive/negative pairs and learn invariant representations. Quick check: confirm contrastive loss implementation.
- **Dataset Size and Task Difficulty**: Modulators of GCL performance; needed to contextualize results across benchmarks. Quick check: ensure dataset size statistics are reported.
- **Synthetic Data Generation**: Allows controlled variation of size and difficulty; needed to isolate dataset-size effects. Quick check: verify synthetic data parameters.
- **Handcrafted Statistics**: Simple baselines for comparison; needed to establish lower performance bounds. Quick check: confirm feature extraction methods.
- **Cross-validation and Benchmarking**: Standard evaluation protocols; needed for fair comparison across methods. Quick check: verify train/test splits.

## Architecture Onboarding
**Component Map**: Data -> GNN/MLP/Handcrafted -> Classification Task
**Critical Path**: Dataset size → Model complexity → Representation quality → Task performance
**Design Tradeoffs**: GCL adds complexity and data requirements versus untrained baselines; simplicity may suffice for small or easy tasks.
**Failure Signatures**: GCL underperforms untrained baselines at small dataset sizes; performance plateaus despite further data increases.
**First Experiments**:
1. Replicate baseline comparisons on a small TU dataset to verify untrained GNN/MLP performance.
2. Measure GCL vs. untrained performance on OGBG-MOLHIV at varying dataset sizes (e.g., 1k, 4k, 8k graphs).
3. Generate and evaluate synthetic datasets with controlled size and difficulty to test logarithmic scaling hypothesis.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Crossover points are dataset-specific and may not generalize to other domains or graph sizes.
- Logarithmic scaling is only validated on synthetic data; applicability to real-world graphs is uncertain.
- The analysis does not explore the impact of graph size or density on GCL performance.
- The role of negative sampling strategies in GCL's performance plateau is not examined.

## Confidence
- **High confidence**: GCL's dependence on dataset size and its initial lag versus untrained baselines on OGBG-MOLHIV.
- **Medium confidence**: The logarithmic scaling law on synthetic data, given its narrow scope and controlled conditions.
- **Medium confidence**: The diagnostic value of simple baselines for GCL evaluation, as this depends on benchmark diversity and practitioner adoption.

## Next Checks
1. Test whether the ~4k graph crossover point holds across diverse molecular and non-molecular datasets with varying graph sizes and densities.
2. Validate the logarithmic scaling hypothesis on real-world graph benchmarks beyond synthetic data, controlling for graph complexity.
3. Investigate whether augmenting negative sampling strategies or modifying GCL objectives can delay or eliminate performance plateaus at large dataset sizes.