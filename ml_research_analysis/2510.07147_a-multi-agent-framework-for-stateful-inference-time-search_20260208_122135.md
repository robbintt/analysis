---
ver: rpa2
title: A Multi-Agent Framework for Stateful Inference-Time Search
arxiv_id: '2510.07147'
source_url: https://arxiv.org/abs/2510.07147
tags:
- edge
- cases
- test
- coverage
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a training-free, multi-agent framework for stateful
  inference-time search to address the limitations of stateless inference in multi-step
  reasoning tasks. The core method combines persistent inference-time state, adversarial
  mutation, and evolutionary preservation through specialized agents (Actor, Adversary,
  Critic, Executor) coordinated by a Controller.
---

# A Multi-Agent Framework for Stateful Inference-Time Search

## Quick Facts
- arXiv ID: 2510.07147
- Source URL: https://arxiv.org/abs/2510.07147
- Reference count: 40
- Multi-agent framework achieves superior code coverage (line, branch, function) compared to stateless baselines on HumanEval and TestGenEvalMini benchmarks

## Executive Summary
This paper introduces a training-free, multi-agent framework for stateful inference-time search that addresses limitations of stateless inference in multi-step reasoning tasks. The system employs specialized agents (Actor, Adversary, Critic, Executor) coordinated by a Controller to maintain persistent state across iterations, enabling progressive refinement of unit test generation. Experiments demonstrate substantial improvements in code coverage across three LLM families (Llama, Gemma, GPT) compared to stateless baselines, with the stateful approach achieving higher coverage particularly for complex, unseen codebases.

## Method Summary
The framework implements a stateful, multi-agent evolutionary loop for automated unit test generation. The Actor proposes edge cases using rule-based heuristics for initialization and LLM-guided generation for subsequent iterations. The Adversary generates program mutants and computes mutation scores to ground the search in semantic correctness. The Critic calculates scalar rewards combining coverage metrics and mutation robustness. The Executor runs tests in a sandboxed environment and returns coverage data. The Controller maintains persistent state across all iterations, orchestrating the agent sequence and applying stopping criteria based on reward thresholds or plateau detection.

## Key Results
- Substantial improvements in line, branch, and function coverage compared to stateless baselines across HumanEval and TestGenEvalMini benchmarks
- Stateful approach outperforms single-step inference methods, particularly for complex, unseen codebases
- Framework achieves higher coverage across three diverse LLM families (Llama, Gemma, GPT)
- Cold-start rule-based heuristics effective for initial coverage (62% of HumanEval problems resolved at initialization)

## Why This Works (Mechanism)

### Mechanism 1: Persistent State Accumulation
Maintaining structured non-Markovian state across iterations enables progressive refinement that stateless inference cannot achieve. The Controller accumulates edge cases, mutation scores, coverage scores, exceptions, and rewards across all stages, conditioning the Actor's next proposal and allowing later stages to inherit and refine earlier judgments.

### Mechanism 2: Adversarial Grounding Through Mutation Testing
Multiplying reward by mutation score prevents the Actor from optimizing toward shallow coverage gains that don't detect actual faults. The Adversary generates mutants of source code, and only edge cases that "kill" mutants (produce different outputs on mutants than on original code) receive high rewards.

### Mechanism 3: Evolutionary Elite Preservation
Retaining top-K edge cases across generations maintains diversity and prevents premature convergence to local optima. After each stage, the system retains the K highest-reward edge cases from all historical candidates, employing a population-based approach rather than single-trajectory search.

## Foundational Learning

- **Concept: Actor-Critic Methods (Inference-Time Variant)**
  - Why needed here: The framework uses Actor-Critic terminology but clarifies this is inference-time reward shaping, not gradient-based RL.
  - Quick check question: In this framework, does the Critic update model parameters? (Answer: No—it only computes scalar rewards to guide the next Actor inference call.)

- **Concept: Mutation Testing and Mutation Score**
  - Why needed here: The Adversary's core contribution depends on mutation testing concepts—generating program variants and assessing whether tests detect differences.
  - Quick check question: What does it mean for an edge case to "kill" a mutant? (Answer: The edge case produces different output on the mutant than on the original program.)

- **Concept: Coverage Metrics (Line, Branch, Function)**
  - Why needed here: The Critic's reward function explicitly incorporates coverage scores, and experimental results report all three metrics.
  - Quick check question: Why might branch coverage lag behind line coverage (as observed with GPT-o4-mini and Gemma)? (Answer: Exception-heavy tests may exercise one path per branch without exploring complements.)

## Architecture Onboarding

- **Component map:**
  Controller maintains state S_n and orchestrates: Actor -> Executor -> Adversary -> Critic -> Controller

- **Critical path:**
  1. Controller initializes S_0 ← ∅, n ← 1
  2. Actor generates ζ_n (rule-based if n=1, LLM-conditioned if n>1)
  3. Executor runs ζ_n on f → obtains κ_n, c_n
  4. Adversary generates mutants, executes ζ_n on mutants → obtains μ_n
  5. Critic computes R_n
  6. Update archive: retain top-K edge cases by R_n
  7. Controller checks ShouldStop; if false, n ← n+1, repeat from step 2
  8. Upon termination, synthesize final unit test file via single LLM call

- **Design tradeoffs:**
  - Computational cost vs. coverage quality: TestGenEvalMini requires ~3584 TFLOPs per iteration
  - Branch coverage vs. exception discovery: Reward structure may bias toward exception-heavy tests
  - Threshold tuning: τ, δ, and window p must balance thoroughness against compute budget

- **Failure signatures:**
  - Executor limitations: Single-file only; no repository indexing; fails on complex dependencies
  - Cold-start insufficiency: If rule-based heuristics don't cover meaningful edge cases
  - Context window overflow: Persistent state grows unboundedly; large N may exceed LLM context limits

- **First 3 experiments:**
  1. Reproduce HumanEval baseline comparison to validate cold-start effectiveness
  2. Ablate mutation score (set γ=0) to isolate the contribution of adversarial grounding
  3. Profile TestGenEvalMini iteration-by-iteration to identify where coverage gains plateau

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating explicit branch-aware objectives into the reward function correct the framework's tendency toward exception-heavy tests and improve branch coverage for non-Llama models?
- Basis in paper: Section 5.2 notes that GPT-o4-mini and Gemma-2-27B lag in branch coverage, likely due to a bias toward "exception-heavy or assert-focused tests," and suggests "incorporating branch-aware objectives could balance thorough path exploration."
- Why unresolved: The current reward function integrates coverage and mutation robustness but lacks a specific mechanism to penalize unexplored control-flow complements.
- What evidence would resolve it: An ablation study adding a branch-coverage term to the reward equation, resulting in statistically significant improvements in branch coverage for the underperforming models.

### Open Question 2
- Question: How can the Executor architecture be extended to support multi-file repositories and complex cross-functional dependencies without triggering the instability observed with modules like Django's autoreload?
- Basis in paper: Appendix A.3.2 lists the lack of "comprehensive repository indexing" and the inability to handle "dependencies spanning multiple files" as a primary limitation.
- Why unresolved: The current implementation isolates execution in a sandboxed Docker environment but fails to resolve relative package imports or manage rapid successive requests.
- What evidence would resolve it: Successful execution and coverage measurement on a benchmark of multi-file Python projects where the system correctly resolves imports without server disconnections.

### Open Question 3
- Question: To what extent can learned reward models replace the current heuristic reward function to stabilize scoring and reduce the need for manual hyperparameter tuning?
- Basis in paper: The Conclusion lists "incorporating learned reward models to stabilize scoring" as a specific direction for future work.
- Why unresolved: The current Critic relies on a manually tuned formula with hyperparameters that may be brittle or require re-tuning across different codebases or model families.
- What evidence would resolve it: A comparative analysis showing that a learned critic (trained on coverage/mutation data) converges faster or achieves higher final coverage scores than the heuristic baseline across diverse repositories.

## Limitations

- Computational cost per iteration (3584 TFLOPs for TestGenEvalMini) raises scalability concerns for production deployment
- The system is limited to single-file source code and cannot handle complex multi-file dependencies or repository indexing
- Experimental validation focuses on coverage metrics rather than direct measurement of fault detection capability in real-world scenarios

## Confidence

- **High confidence**: The stateful architecture design and multi-agent coordination mechanism are well-specified and technically sound
- **Medium confidence**: Coverage improvements over stateless baselines are demonstrated, but mutation score's actual contribution to semantic correctness remains theoretically grounded
- **Low confidence**: Claims about edge-case robustness and ability to handle complex real-world codebases are based on synthetic benchmarks rather than production validation

## Next Checks

1. **Fault Detection Validation**: Apply the generated tests to actual buggy versions of target codebases and measure real fault detection rates versus mutation score improvements
2. **Context Window Stress Test**: Systematically evaluate how accumulated state affects LLM performance as iteration count increases, identifying practical limits of the persistent state approach
3. **Computational Cost Analysis**: Profile the full system across different code complexity levels to establish precise resource requirements and identify optimization opportunities for practical deployment