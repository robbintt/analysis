---
ver: rpa2
title: 'Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization
  for Deep Neural Network Training'
arxiv_id: '2510.14009'
source_url: https://arxiv.org/abs/2510.14009
tags:
- training
- learning
- lanton
- layers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of geometry-aware optimizers
  like Muon when applied to deep neural networks with heterogeneous layer-wise curvature
  and gradient noise. These optimizers typically use fixed learning rates within layers
  of the same group, which can be suboptimal.
---

# Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training

## Quick Facts
- arXiv ID: 2510.14009
- Source URL: https://arxiv.org/abs/2510.14009
- Reference count: 40
- Primary result: LANTON achieves faster convergence and improved sample efficiency across transformer architectures (LLaMA, GPT2) with minimal computational overhead (~4% additional training time).

## Executive Summary
This paper addresses the inefficiency of geometry-aware optimizers like Muon when applied to deep neural networks with heterogeneous layer-wise curvature and gradient noise. The authors propose LANTON, a novel method that estimates gradient variance in the dual norm induced by the chosen linear minimization oracle and dynamically assigns layerwise, noise-adaptive learning rates. The method is compatible with geometry-aware optimizers and theoretically achieves a convergence rate of \(\tilde{O}(1/\sqrt{T} + P_\ell \bar{\sigma}_\ell / T^{1/4})\), improving upon prior work by accounting for layer-wise noise heterogeneity. Empirically, LANTON demonstrates faster convergence and improved sample efficiency across multiple transformer architectures and datasets.

## Method Summary
LANTON extends geometry-aware optimizers by adding per-layer variance tracking and adaptive learning rate scaling. For each layer ℓ, it maintains a momentum-based variance tracker H^ℓ_t that estimates gradient noise in the dual norm. The learning rate for each layer is then scaled by α^ℓ_t = α/√(α² + H^ℓ_t), with the scaling normalized by the maximum within each parameter group. This creates noise-adaptive learning rates while preserving the geometry-aware update directions computed via layer-specific linear minimization oracles (LMOs). The method introduces minimal overhead by estimating noise every 10 iterations and using efficient randomized SVD for nuclear norm approximations.

## Key Results
- LANTON achieves lower training and validation losses compared to state-of-the-art optimizers (AdamW, Muon, D-Muon) across multiple transformer architectures.
- The method demonstrates improved sample efficiency, requiring fewer tokens to reach target loss values.
- Computational overhead is minimal at approximately 4% additional training time.
- Theoretical convergence rate improves upon prior work by incorporating layer-wise noise heterogeneity.

## Why This Works (Mechanism)

### Mechanism 1: Noise-Adaptive Learning Rate Scaling
- Claim: Assigning time-varying learning rates inversely proportional to estimated gradient noise per layer accelerates convergence compared to fixed learning rates within layer groups.
- Mechanism: A momentum-based variance tracker H^ℓ_t = β₂H^ℓ_{t-1} + (1-β₂)||G^ℓ_t - Ḡ^ℓ_t||²_* per layer ℓ estimates gradient variance in the dual norm. The scaling factor α^ℓ_t = α/√(α² + H^ℓ_t) ensures layers with higher noise (larger H^ℓ_t) receive smaller effective learning rates η^ℓ_t = η_t√(α^ℓ_t/α^m_t), where α^m_t = max_{j∈G_ℓ} α^j_t.
- Core assumption: Assumption 5.2(ii)—stochastic gradient noise is almost surely bounded both above and below in the dual space: σ̄_ℓ ≤ ||∇_ℓF(X,ξ) - ∇_ℓf(X)||_{(ℓ)*} ≤ σ̄_ℓ.
- Evidence anchors: [abstract], [Algorithm 1, lines 7-9], [corpus]
- Break condition: If gradient noise becomes uniform across layers (σ̄_ℓ ≈ σ_max for all ℓ), α^ℓ_t/α^m_t → 1 and noise-adaptivity provides no benefit.

### Mechanism 2: Geometry-Aware Norm Constraints via Linear Minimization Oracles (LMOs)
- Claim: Using operator-norm constraints with layer-specific LMOs provides better optimization geometry than treating all parameters uniformly under Euclidean norms.
- Mechanism: Different parameter groups use different norms: (1) hidden layers—RMS→RMS operator norm with dual nuclear norm; (2) embedding/LM head—ℓ₁→ℓ_∞ operator norm; (3) normalization vectors—RMS norm. LMO(B^ℓ_t) returns the optimal direction under these constraints (e.g., -√(d_out/d_in)UV^⊤ via Newton-Schulz for hidden layers).
- Core assumption: Assumption 5.1—layer-wise L-smoothness holds under non-Euclidean norms: ||∇_ℓf(X) - ∇_ℓf(Y)||_{(ℓ)*} ≤ L_ℓ||X^ℓ - Y^ℓ||_{(ℓ)}.
- Evidence anchors: [abstract], [Table 1], [corpus]
- Break condition: If the loss landscape does not exhibit assumed layer-wise smoothness structure under operator norms, convergence guarantees may not hold.

### Mechanism 3: Variance Estimation in Dual Norm Space
- Claim: Estimating gradient variance using the dual norm induced by the LMO (not Euclidean norm) provides theoretically consistent noise bounds for convergence analysis.
- Mechanism: The variance tracker uses ||G^ℓ_t - Ḡ^ℓ_t||_* with the dual norm matching the parameter type: nuclear norm for hidden layers, ℓ₁→ℓ₁ for embeddings, ℓ₂ for vectors. This aligns with convergence bounds expressed in dual norm spaces.
- Core assumption: The noise lower bound σ̄_ℓ > 0 is crucial for establishing tight bounds on α^ℓ_t/α^m_t (Lemma 5.5).
- Evidence anchors: [Section 4], [Lemma 5.4], [corpus]
- Break condition: If σ̄_ℓ → 0 (very low noise), the ratio α^ℓ_t/α^m_t may become unbounded below, destabilizing learning rate bounds.

## Foundational Learning

- **Concept: Linear Minimization Oracle (LMO)**
  - Why needed here: LANTON updates via LMO(B) = argmin_{||x||≤1}⟨B, x⟩ rather than gradient descent.
  - Quick check question: Given momentum buffer B and norm constraint ||x|| ≤ 1, can you derive LMO(B)?

- **Concept: Operator Norms and Dual Norms**
  - Why needed here: Different layers use different operator norms; dual norms are used for variance estimation.
  - Quick check question: What is the dual norm of the nuclear norm? How does it relate to singular values?

- **Concept: Stochastic Gradient Variance Estimation**
  - Why needed here: Core contribution is per-layer variance estimation using momentum-averaged squared gradient differences.
  - Quick check question: Why use ||G^ℓ_t - Ḡ^ℓ_t||²_* (difference) rather than ||G^ℓ_t||²_* (magnitude)?

## Architecture Onboarding

- **Component map:**
  1. Gradient computation per layer
  2. Momentum buffer: B^ℓ_t = β₁B^ℓ_{t-1} + (1-β₁)G^ℓ_t
  3. LMO direction: Newton-Schulz (hidden), signum (embeddings), RMS norm (vectors)
  4. Variance tracker: H^ℓ_t = β₂H^ℓ_{t-1} + (1-β₂)||G^ℓ_t - G^ℓ_{t-1}||²_*
  5. Adaptive scaling: α^ℓ_t = α/√(α² + H^ℓ_t), η^ℓ_t = η_t√(α^ℓ_t/α^m_t)
  6. Parameter update: X^ℓ_{t+1} = X^ℓ_t + η^ℓ_t O^ℓ_t

- **Critical path:** Variance tracker → Scaling ratio → Layer-wise learning rate → LMO direction → Update. H^ℓ_t must remain O(1) (Lemma 5.4).

- **Design tradeoffs:**
  - Option I (G_t - G_{t-1}) vs Option II (independent gradients): Option I avoids 2× gradient computation but uses correlated estimates. Paper uses Option I in practice.
  - Estimate noise every 10 iterations (~4% overhead vs D-Muon).
  - Randomized SVD for nuclear norm approximation (not full SVD).

- **Failure signatures:**
  1. Learning rate collapse: Check if α^ℓ_t/α^m_t → 0 (verify σ̄_ℓ ≫ 0).
  2. No speedup: If noise is uniform, α^ℓ_t ≈ α^m_t. Plot H^ℓ_t across layers to confirm heterogeneity.
  3. Newton-Schulz divergence: Monitor ||O^ℓ_t|| during early training.

- **First 3 experiments:**
  1. GPT-2 small on OpenWebText-100k: LANTON vs AdamW, Muon, D-Muon. Track loss curves and wall-clock time.
  2. Ablation on noise estimation: Option I vs Option II on GPT-2 small. Measure convergence speed and final loss.
  3. Layer-wise variance logging: For LLaMA-1.1B on C4, log H^ℓ_t for all layers. Visualize heat map to verify Figure 1's heterogeneity claim.

## Open Questions the Paper Calls Out
- **Question:** Does LANTON maintain its acceleration benefits when scaling to models with 7B+ parameters or frontier model scales?
  - Basis in paper: [explicit] "Another limitation is that our experiments are conducted on moderately sized models; extending and validating the approach at larger scales is an important direction for future work."
  - Why unresolved: Current validation only covers models up to 2B parameters; gradient noise heterogeneity patterns may differ at larger scales.
  - What evidence would resolve it: Empirical results on LLaMA-7B/70B showing comparable training acceleration relative to D-Muon and other baselines.

- **Question:** Can the convergence guarantees be reformulated to remove dependence on dimension-dependent constants (C₁, C₂)?
  - Basis in paper: [explicit] "One limitation of our work is that the theoretical results may depend on the parameter dimension."
  - Why unresolved: The analysis relies on norm equivalence constants scaling with matrix dimensions, potentially weakening guarantees for wide layers.
  - What evidence would resolve it: A refined convergence analysis with dimension-free constants, or empirical demonstration that performance is independent of layer width.

- **Question:** Is the almost-sure lower bound on gradient noise (Assumption 5.2(ii)) necessary for convergence, or can it be relaxed?
  - Basis in paper: [inferred] The assumption enables tight bounds on learning rate ratios but may not hold universally, especially near convergence or for sparse gradient layers.
  - Why unresolved: The paper notes this assumption is "crucial" technically but does not analyze whether it can be weakened.
  - What evidence would resolve it: Convergence proof without the lower-bound assumption, or empirical robustness analysis when noise estimates approach zero.

- **Question:** What is the approximation gap between the practical noise estimator (Option I) and the theoretical estimator (Option II)?
  - Basis in paper: [inferred] The paper uses consecutive gradients for experiments but independent gradients for theory, without quantifying approximation quality.
  - Why unresolved: Consecutive gradients may be correlated via momentum, potentially biasing noise estimates.
  - What evidence would resolve it: Theoretical bound on Option I's bias/variance, or matched-compute empirical comparison.

## Limitations
- The convergence theory relies on bounded noise with a non-zero lower bound (σ̄_ℓ > 0), which may not hold in all training scenarios.
- Implementation of layerwise LMOs involves non-trivial engineering choices not fully specified, particularly randomized SVD for nuclear norm approximation and Newton-Schulz iterations.
- The hyperparameter α in the adaptive scaling formula is critical but not explicitly provided in the paper.
- Current validation only covers models up to 2B parameters, leaving scalability to larger models as an open question.

## Confidence
- **High Confidence**: The empirical observation that noise heterogeneity exists across layers (Figure 1) and the core mechanism of using variance-based scaling to adjust learning rates within groups.
- **Medium Confidence**: The theoretical convergence rate improvement (tilde-O(1/√T + P_ℓ σ̄_ℓ/T^(1/4))) assuming the stated conditions hold, particularly the noise lower bound assumption.
- **Low Confidence**: The practical impact of dual-norm variance estimation versus simpler alternatives, and the robustness of the method when layer-wise noise patterns change during training.

## Next Checks
1. **Layer-wise variance heterogeneity verification**: Run LLaMA-1.1B on C4 and log H^ℓ_t for all layers across training epochs. Plot heat maps to confirm that gradient noise truly varies significantly across layers as claimed in Figure 1.

2. **Ablation study on noise estimation method**: Compare LANTON using Option I (G_t - G_{t-1}) versus Option II (independent gradient estimates) on GPT-2 small. Measure convergence speed, final loss, and training overhead to quantify the benefit of the correlated estimate approach.

3. **Noise lower bound sensitivity**: Systematically vary the noise level in synthetic experiments (e.g., by adding controlled Gaussian noise to gradients) and measure how the performance gap between LANTON and D-Muon changes. This would test the importance of the σ̄_ℓ > 0 assumption for practical effectiveness.