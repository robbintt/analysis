---
ver: rpa2
title: 'Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained
  Sentence-Level Self-Evolution'
arxiv_id: '2503.01695'
source_url: https://arxiv.org/abs/2503.01695
tags:
- training
- faithfulness
- gendie
- answer
- asqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving context faithfulness
  in large language models, particularly for long-form question answering tasks where
  hallucinations can occur. The authors propose GenDiE (Generate, Discriminate, Evolve),
  a novel self-evolving framework that enhances context faithfulness through fine-grained
  sentence-level optimization.
---

# Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution

## Quick Facts
- arXiv ID: 2503.01695
- Source URL: https://arxiv.org/abs/2503.01695
- Authors: Kun Li; Tianhua Zhang; Yunxiang Li; Hongyin Luo; Abdalla Moustafa; Xixin Wu; James Glass; Helen Meng
- Reference count: 18
- One-line primary result: GenDiE surpasses various baselines in both faithfulness and correctness, achieving up to 84.9% AlignScore and 45.75% EM Recall on ASQA.

## Executive Summary
This paper addresses the problem of improving context faithfulness in large language models, particularly for long-form question answering tasks where hallucinations can occur. The authors propose GenDiE (Generate, Discriminate, Evolve), a novel self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. The core method idea involves combining generative and discriminative training to equip LLMs with self-generation and self-scoring capabilities, enabling iterative self-evolution. Experiments on ASQA (in-domain LFQA) and ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE surpasses various baselines in both faithfulness and correctness, achieving up to 84.9% AlignScore and 45.75% EM Recall on ASQA.

## Method Summary
GenDiE is a self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. The method combines generative and discriminative training in a multi-task loop, where the model is trained to both generate text and discriminate between faithful and unfaithful sentences. The training procedure involves three iterations: pre-stage training using gold answers, followed by self-evolving iterations where the model generates candidates, scores them, and constructs contrastive pairs for the next round. The approach uses Llama-3.1-8B with QLoRA fine-tuning, tree-structured sampling, and hierarchical inference with self-scoring to guide answer selection.

## Key Results
- GenDiE achieves up to 84.9% AlignScore and 45.75% EM Recall on ASQA dataset
- The approach shows robust performance for domain adaptation, outperforming GenDiEgold-answer consistently across iterations
- Faithfulness scores increase through self-evolving iterations, demonstrating the effectiveness of the iterative self-improvement process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained sentence-level optimization provides more precise supervision for faithfulness than holistic answer-level training.
- Mechanism: By treating each sentence as an independent optimization unit, the model receives a higher-fidelity training signal. This isolates unfaithful sentences within a long-form answer that would otherwise be "diluted" if the entire answer were treated as a single positive/negative example. This targeted signal directly adjusts the model's probability distribution for generating individual context-grounded sentences.
- Core assumption: Faithfulness is not uniform across all sentences in a long-form answer.
- Evidence anchors:
  - [abstract] "...treating each sentence in a response as an independent optimization unit, GenDiE effectively addresses the limitations of previous approaches that optimize at the holistic answer level, which may miss unfaithful details."
  - [Section 2.1.1] "...these approaches fail to capture the nuanced differences in faithfulness across individual sentences."
  - [corpus] This is a common thread in related work, as seen in "Enhancing Long Document Long Form Summarisation with Self-Planning" (Corpus Neighbor 5), which leverages "sentence-level information as a content plan to improve the traceability and faithfulness."
- Break condition: The benefit is negated if the majority of errors are not sentence-localized (e.g., a systemic logic error across the whole paragraph) or if the sentence tokenizer is poor, splitting related facts and destroying context.

### Mechanism 2
- Claim: Integrating generative and discriminative training in a single multi-task loop equips the model with a reliable internal scoring capability for self-improvement.
- Mechanism: The model is trained not only to generate text (language modeling objective) but also to discriminate between faithful and unfaithful sentences (discrimination objective). This is achieved via a unified loss function (Eq. 1) that combines a language modeling term with an ORPO-inspired preference optimization term. This forces the model to develop an internal representation of faithfulness that can be used as a scalar score (Sa), enabling it to self-generate and self-score its own outputs for iterative refinement.
- Core assumption: A model can be trained to be both a competent generator and a reliable discriminator for the same property (faithfulness).
- Evidence anchors:
  - [abstract] "GenDiE combines both generative and discriminative training, equipping LLMs with self-generation and self-scoring capabilities..."
  - [Section 2.1.2] "...we train our model by maximizing [a unified loss function combining language modeling and a discrimination objective]."
  - [corpus] Related work like "Self-Rewarding Language Models" (Yuan et al., 2024, cited in Related Work) and "From Faithfulness to Correctness: Generative Reward Models that Think Critically" (Corpus Neighbor 8) explore similar themes of models generating their own rewards or critiques.
- Break condition: The mechanism fails if the self-scoring capability is inaccurate or suffers from reward hacking, where the model learns to generate output that maximizes its own score without being genuinely faithful.

### Mechanism 3
- Claim: Iterative self-evolution via tree-structured sampling and contrastive pair construction drives continuous improvement in faithfulness.
- Mechanism: The model generates multiple candidate sentences, scores them using its learned discriminative capability, and constructs contrastive pairs (positive, negative) from the highest and lowest-scoring siblings in a search tree. These pairs are then used for the next round of training. This iterative process allows the model to "bootstrap" its own performance, learning from its own generated examples and progressively refining its understanding of faithfulness.
- Core assumption: The self-generated data in subsequent iterations is of higher quality (or at least offers useful contrastive signals) than the previous iteration's data.
- Evidence anchors:
  - [abstract] "...equipping LLMs with self-generation and self-scoring capabilities to facilitate iterative self-evolution."
  - [Section 4 Main Results] "Remarkably, our approach enables the model to consistently improve with each successive training iteration."
  - [corpus] The concept is supported by work in self-evolution, such as "SSFO: Self-Supervised Faithfulness Optimization..." (Corpus Neighbor 4) which also aims to reduce supervision reliance.
- Break condition: Performance gains will plateau or degrade if the self-generated negative examples become too similar to positive ones, or if the positive examples suffer from distributional drift away from genuine faithfulness.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO) and its variants (e.g., ORPO).
  - Why needed here: GenDiE's training objective is inspired by ORPO. Understanding how DPO works—optimizing a policy directly from preference data without an explicit reward model—is crucial for grasping how GenDiE's discrimination loss functions.
  - Quick check question: How does ORPO differ from standard DPO in its handling of the reference model and the optimization objective?

- Concept: Retrieval-Augmented Generation (RAG).
  - Why needed here: The entire problem GenDiE solves—improving context faithfulness—is central to building trustworthy RAG systems. The model is conditioned on a question and retrieved passages to generate an answer.
  - Quick check question: In a standard RAG setup, where does the model's context come from and what is the primary failure mode GenDiE aims to fix?

- Concept: Contrastive Learning and Preference Modeling.
  - Why needed here: The core of GenDiE's data construction involves creating contrastive pairs of (faithful, unfaithful) sentences. Understanding how these pairs are used to shift the model's preference is fundamental.
  - Quick check question: If you have a model-generated sentence that is faithful and one that is not, how would you structure a training example to teach the model to prefer the former?

## Architecture Onboarding

- Component map: Base Model (Llama-3.1-8b with QLoRA) -> Tokenizer & Sentence Splitter -> Tree-Structured Sampler -> Self-Scoring Module -> Contrastive Pair Constructor -> Hierarchical Decoder (Inference)

- Critical path: The path from Pre-stage training (using gold answers) -> Self-evolving training (using model-generated data) -> Hierarchical Inference. Errors or poor data quality in the pre-stage will propagate and degrade all subsequent self-evolving iterations.

- Design tradeoffs:
  - Sentence-level vs. Answer-level: Finer control vs. potentially losing inter-sentence context.
  - Self-scoring vs. External Scorer: Eliminates need for an external model (like a large NLI model) during training but risks the model being a poor judge of its own faithfulness.
  - Hierarchical Inference vs. Greedy Decoding: Significant performance boost vs. substantially higher computational cost and latency at inference time.

- Failure signatures:
  - Reward Hacking: The model generates outputs that sound plausible and get a high self-score but are factually incorrect or unfaithful.
  - Mode Collapse: The tree-structured sampler fails to produce diverse candidates, making contrastive pair selection ineffective.
  - Performance Plateau: Faithfulness scores stop improving after the second or third iteration, indicating the model's capacity or the data construction method has hit a ceiling.

- First 3 experiments:
  1. Sanity Check - Pre-stage Only: Train the model for one iteration (Pre-stage) using the gold answers and filtered negatives. Evaluate on a small validation set to ensure the base model and data pipeline are working and that the multi-task objective is learning.
  2. Ablation - Self-Scoring Validity: Replace the model's self-scoring component with a ground-truth or strong external faithfulness metric during data construction for one iteration. Compare the resulting model's performance to the standard GenDiE to quantify the accuracy and impact of the self-scoring function.
  3. Convergence Analysis - Iteration Limit: Run the full self-evolving training for 5-7 iterations and plot the faithfulness and correctness scores on a held-out test set. Determine the optimal number of iterations before performance plateaus or degrades, and observe if the quality of self-generated data changes.

## Open Questions the Paper Calls Out

- Question: Can GenDiE generalize effectively to QA tasks requiring fast-changing world knowledge beyond the tested ASQA and ConFiQA benchmarks?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section: "we need to show GenDiE can generalize to other QA tasks that require fast-changing world knowledge."
  - Why unresolved: Current experiments only cover ASQA (static factoid questions) and ConFiQA (synthetic counterfactual scenarios), neither representing real-time knowledge evolution.
  - What evidence would resolve it: Evaluation on temporal QA datasets or news-based QA where correct answers shift over time, demonstrating sustained faithfulness with dynamically updated contexts.

- Question: How do alternative multi-task training objectives compare to the ORPO-based approach for achieving self-evolution?
  - Basis in paper: [explicit] The authors note: "due to computational constraints, we did not conduct investigation of how other training objective for multi-task training... might affect the performance outcomes."
  - Why unresolved: Only one specific training formulation combining language modeling and discrimination objectives was tested, leaving the optimal training approach unclear.
  - What evidence would resolve it: Systematic comparison with alternative multi-task objectives (e.g., separate reward models, different preference optimization algorithms) on the same benchmarks with controlled experiments.

- Question: Can the computational overhead of hierarchical inference be reduced while preserving the faithfulness gains?
  - Basis in paper: [explicit] The authors acknowledge: "hierarchical inference... introduces additional computational overhead compared to standard inference methods... Further efforts to reduce this overhead can be pursued."
  - Why unresolved: The test-time scaling approach requires generating and scoring multiple candidate sentences per step, which may be prohibitive for deployment.
  - What evidence would resolve it: Ablation studies exploring adaptive beam widths, early stopping criteria, or distilled scoring models that maintain performance while reducing generation and evaluation costs.

## Limitations

- The approach relies on the model's own self-scoring capability for iterative improvement, which could lead to reward hacking or distributional drift.
- Sentence-level optimization may lose important inter-sentence context crucial for maintaining logical coherence in long-form answers.
- The hierarchical inference process introduces substantial computational overhead that could limit practical deployment.

## Confidence

- High Confidence: The experimental results showing GenDiE outperforming baselines on both ASQA and ConFiQA datasets, particularly the consistent improvement across self-evolving iterations.
- Medium Confidence: The mechanism explanations, particularly the claim that sentence-level optimization provides superior supervision compared to answer-level training, as this relies on assumptions about error localization that aren't fully validated.
- Medium Confidence: The architectural choices and their tradeoffs (sentence-level vs answer-level, self-scoring vs external scorer, hierarchical vs greedy inference), as these are logically sound but their relative importance isn't quantitatively isolated through ablation studies.

## Next Checks

1. Self-Scoring Reliability Test: Replace the model's self-scoring component with a strong external faithfulness metric (e.g., T5NLI or a fine-tuned faithfulness classifier) during data construction for one self-evolving iteration. Compare the resulting model's performance and the quality of generated contrastive pairs to quantify how accurate and impactful the self-scoring function is.

2. Error Localization Analysis: Manually analyze a sample of unfaithful sentences in model outputs to determine whether errors are truly localized to individual sentences or whether they stem from broader contextual or logical issues that sentence-level optimization might miss. This would validate the core assumption underlying the fine-grained approach.

3. Inter-Sentence Context Ablation: Implement a variant of GenDiE that optimizes at the answer level (grouping all sentences together) and compare its performance to the sentence-level approach on both faithfulness and correctness metrics. This would quantify the tradeoff between fine-grained control and contextual coherence.