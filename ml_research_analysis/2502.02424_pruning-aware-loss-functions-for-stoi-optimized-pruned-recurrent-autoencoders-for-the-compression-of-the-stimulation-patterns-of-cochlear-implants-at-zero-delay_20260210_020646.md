---
ver: rpa2
title: Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent Autoencoders
  for the Compression of the Stimulation Patterns of Cochlear Implants at Zero Delay
arxiv_id: '2502.02424'
source_url: https://arxiv.org/abs/2502.02424
tags:
- pruning
- vstoi
- loss
- proposed
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of model size in deep neural networks
  for cochlear implant stimulation pattern compression, where limited computational
  resources in hearing aids restrict the application of large models. The proposed
  method introduces a pruning-aware loss function that incorporates weight perturbations
  during training, allowing the network to become more robust to subsequent pruning.
---

# Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent Autoencoders for the Compression of the Stimulation Patterns of Cochlear Implants at Zero Delay

## Quick Facts
- **arXiv ID**: 2502.02424
- **Source URL**: https://arxiv.org/abs/2502.02424
- **Reference count**: 22
- **Key outcome**: Pruning-aware loss significantly improves post-pruning VSTOI scores compared to conventional magnitude-informed pruning, especially at high pruning rates (>45%).

## Executive Summary
This work addresses the problem of model size in deep neural networks for cochlear implant stimulation pattern compression, where limited computational resources in hearing aids restrict the application of large models. The proposed method introduces a pruning-aware loss function that incorporates weight perturbations during training, allowing the network to become more robust to subsequent pruning. This approach captures the global impact of pruning, unlike existing methods based on local derivatives. Experiments on feedback recurrent autoencoders show that the proposed pruning-aware loss significantly improves post-pruning objective speech intelligibility (VSTOI scores) compared to conventional magnitude-informed pruning, especially at higher pruning rates. For example, little degradation in VSTOI scores is observed up to 55% pruning rate after fine-tuning, with substantial gains for pruning rates above 45%.

## Method Summary
The method involves training a Feedback Recurrent Autoencoder (FRAE) with a pruning-aware loss function that incorporates weight perturbations mimicking future pruning. The loss function is L_PA = L_ω + λ|L_ω - L_(ω+Δω)|, where Δω is scaled by a perturbation function g(t) that ramps from 0 to full pruning magnitude over training iterations. After training, the network is pruned using magnitude-informed pruning, then fine-tuned. The approach is compared to conventional magnitude-informed pruning on whole-model and decoder-only scenarios at pruning rates from 5% to 95%. Evaluation uses VSTOI scores on TIMIT speech corpus mixed with noise, processed through a cochlear implant vocoder.

## Key Results
- Pruning-aware loss significantly improves post-pruning VSTOI scores compared to conventional magnitude-informed pruning, especially at high pruning rates (>45%).
- Linear perturbation function g(t)=t outperforms aggressive ramps (t², t³) at pruning rates between 60% and 80%.
- Decoder-only pruning preserves VSTOI scores longer than whole-model pruning across all tested pruning rates.

## Why This Works (Mechanism)

### Mechanism 1: Anticipatory Weight Reconfiguration via Perturbation
Networks trained with weight perturbations mimicking future pruning reconfigure their weights to minimize the impact of those perturbations. The pruning-aware loss term |L_ω - L_(ω+Δω)| penalizes sensitivity to weight changes in the pruning direction. Gradients from this term push surviving weights to compensate for weights that will be removed, redistributing functional importance before pruning occurs.

### Mechanism 2: Global Loss Sensitivity Estimation
Explicitly computing the loss difference between perturbed and unperturbed weights captures global pruning impact better than local gradient approximations. Unlike gradient-informed pruning which uses ∂L/∂ω at a single point, the term |L_ω - L_(ω+Δω)| measures the actual loss change from a finite perturbation, incorporating higher-order effects and weight interactions across layers.

### Mechanism 3: Progressive Perturbation Ramp
Gradually increasing perturbation strength during training prevents early instability while allowing late-stage specialization to pruning. The perturbation function g(n/n_max) scales Δω from 0 toward full pruning magnitude. Early training focuses on task learning; later training specializes weights toward the specific pruning mask.

## Foundational Learning

- **Magnitude-informed pruning**: Weights with smallest absolute values are set to zero; assumes small weights contribute least to output. Why needed here: Baseline comparison; perturbation direction is derived from this criterion. Quick check question: Why might small weights still be functionally important in recurrent networks?

- **STOI/VSTOI metrics**: Short-Time Objective Intelligibility measures correlation between temporal envelopes of clean and degraded speech; VSTOI applies vocoder simulation for cochlear implant evaluation. Why needed here: Primary evaluation metric; ~0.006 VSTOI change ≈ 5% word recognition difference per cited literature. Quick check question: What are the limitations of using objective intelligibility metrics for hearing aid deployment?

- **Feedback Recurrent Autoencoder (FRAE)**: Autoencoder with recurrent connections and feedback pathways for zero-delay sequential compression. Why needed here: Target architecture; encoder-decoder split matters for whole-model vs decoder-only pruning scenarios. Quick check question: Why does decoder-only pruning preserve performance longer than whole-model pruning?

## Architecture Onboarding

- **Component map**: Encoder (M=22 subbands → encoded representation) -> Vector quantization (6-bit codebook) -> Decoder (reconstructs stimulation patterns) -> Pruning mask (applied globally/unstructured) -> Pruning-aware loss (wraps VSTOI loss)

- **Critical path**: 1) Start from pretrained FRAE checkpoint; 2) Run 1000 iterations with pruning-aware loss, ramping perturbation via g(t); 3) Apply hard pruning at target rate; 4) Fine-tune pruned model for 7000 iterations without perturbation term; 5) Evaluate VSTOI on test set

- **Design tradeoffs**: Perturbation function aggressiveness (faster ramps may work for lower pruning rates; linear safer for high rates); Training overhead (requires additional forward pass per iteration for perturbed loss computation); Pruning granularity (unstructured pruning maximizes compression but limited hardware acceleration benefits vs structured)

- **Failure signatures**: VSTOI dropping below ~0.45 (model outputs noise-like signals, compression scheme has collapsed); Pre-pruning VSTOI degradation without post-pruning recovery (perturbation ramp too aggressive); Fine-tuning fails to recover baseline (pruning rate exceeds model capacity for task)

- **First 3 experiments**: 1) Reproduce baseline magnitude-informed pruning curve on FRAE to validate evaluation pipeline; 2) Ablate perturbation function: Compare linear, quadratic, cubic at fixed 50% pruning rate to verify linear superiority; 3) Test generalization: Apply pruning-aware training to a different architecture (e.g., larger GRU-based autoencoder) to assess architecture-agnostic claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would more aggressive or learned perturbation functions (e.g., monotonic interpolation, root-square) yield better pruning robustness than the linear, quadratic, and cubic functions tested?
- Basis in paper: [explicit] "Accordingly, it is likely that even more aggressive perturbations could allow to achieve even better results. This was partially – but not for all pruning rates – observed for a root-square function. Parametrizing the perturbation function though, e.g. monotonic interpolation, could allow to yield optimal pruning-aware loss function."
- Why unresolved: Only three hand-designed perturbation functions were evaluated; no systematic optimization of the perturbation schedule was performed.
- What evidence would resolve it: Experiments comparing learned/parametrized perturbation functions across pruning rates on the same task.

### Open Question 2
- Question: Does the proposed pruning-aware loss generalize effectively to larger neural networks (e.g., ResNets, transformers) beyond the tiny FRAE model with ~3300 weights?
- Basis in paper: [explicit] "Therefore, the proposed method should be investigated using larger networks to fully evaluate its potential."
- Why unresolved: The study only evaluated a small recurrent autoencoder; the claim of being model-agnostic remains unverified on larger architectures.
- What evidence would resolve it: Application of the method to standard benchmark networks (e.g., ResNet on ImageNet) with comparisons to baseline pruning.

### Open Question 3
- Question: Would alternative pruning directions (e.g., gradient-informed, movement-based) provide different or better results when combined with the pruning-aware loss?
- Basis in paper: [explicit] "While it is conceivable to use other pruning-directions, like gradient-informed pruning directions, because the network reconfigures itself to be robust to the chosen direction, there seems to be no obvious reason, why a certain direction should be better than another."
- Why unresolved: Only magnitude-informed pruning direction was used; no comparison to other pruning criteria was conducted.
- What evidence would resolve it: Ablation study comparing gradient-informed and movement pruning directions under the same pruning-aware training framework.

### Open Question 4
- Question: Do improvements in VSTOI scores from pruning-aware training translate to perceptible benefits in speech intelligibility for actual cochlear implant users?
- Basis in paper: [inferred] The paper relies exclusively on VSTOI as an objective proxy for intelligibility; no subjective listening tests with CI users are reported.
- Why unresolved: Objective metrics may not fully capture real-world perceptual outcomes, especially for hearing-impaired populations.
- What evidence would resolve it: Subjective word recognition scores from CI users comparing stimuli processed by baseline vs. pruning-aware models.

## Limitations

- Architecture details of the Feedback Recurrent Autoencoder are only referenced from prior work [9], requiring reverse engineering or assumption of layer configurations
- The perturbation function's optimal schedule (linear vs higher-order polynomials) may be task-specific and not generalize to other architectures or pruning strategies
- Limited ablation studies on λ (weighting of pruning-aware term) and perturbation magnitude scale prevent full understanding of hyperparameter sensitivity

## Confidence

- **High**: VSTOI improvement at high pruning rates (≥45%) for decoder-only pruning scenario; fundamental mechanism of perturbation-based robustness training
- **Medium**: Superiority of linear perturbation function over polynomial alternatives; architecture-agnostic claims for the pruning-aware loss approach
- **Low**: Exact VSTOI values at specific pruning rates (likely dependent on pretrained checkpoint quality and training seed)

## Next Checks

1. Test pruning-aware loss on a different neural architecture (e.g., Transformer-based encoder-decoder) to validate architecture-agnostic claims
2. Compare against structured pruning methods that might offer better hardware acceleration despite lower compression ratios
3. Conduct ablation study on perturbation function parameters (λ, magnitude scale, ramp schedule) to identify sensitivity to hyperparameter choices