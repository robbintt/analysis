---
ver: rpa2
title: Data Augmentation to Improve Large Language Models in Food Hazard and Product
  Detection
arxiv_id: '2502.08687'
source_url: https://arxiv.org/abs/2502.08687
tags:
- augmentation
- roberta
- available
- data
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the impact of ChatGPT-4o-mini data augmentation
  on large language models (LLMs) for food hazard and product detection. The original
  dataset with 5,082 samples was augmented to 6,513 samples, focusing on underrepresented
  classes.
---

# Data Augmentation to Improve Large Language Models in Food Hazard and Product Detection

## Quick Facts
- **arXiv ID**: 2502.08687
- **Source URL**: https://arxiv.org/abs/2502.08687
- **Reference count**: 25
- **Primary result**: Data augmentation improved F1-scores by ~4 points for both RoBERTa-base and Flan-T5-base models in food hazard and product detection tasks

## Executive Summary
This study investigates the effectiveness of ChatGPT-4o-mini data augmentation for improving large language model performance in food hazard and product detection tasks. The researchers augmented an original dataset of 5,082 samples to 6,513 samples, with particular focus on underrepresented classes. Two transformer-based models were fine-tuned and evaluated: RoBERTa-base and Flan-T5-base. The results demonstrate that data augmentation significantly improves model performance across multiple evaluation metrics, with Flan-T5-base consistently outperforming RoBERTa-base. The study highlights the potential of using LLM-generated augmentation to address class imbalance issues in specialized domains like food safety.

## Method Summary
The researchers employed ChatGPT-4o-mini to generate synthetic data samples for underrepresented classes in their food hazard and product detection dataset. The original dataset of 5,082 samples was expanded to 6,513 samples through this augmentation process. Two transformer-based models were fine-tuned on the dataset: RoBERTa-base and Flan-T5-base. Both models were evaluated using standard classification metrics including F1-score, precision, and recall. The augmentation strategy specifically targeted classes with limited representation to address class imbalance issues. Training times were recorded, with RoBERTa-base showing shorter training times due to its smaller model size compared to Flan-T5-base.

## Key Results
- Flan-T5-base achieved superior F1-scores: 77.40 (hazard) and 78.10 (product) with augmentation
- RoBERTa-base achieved F1-scores: 77.40 (hazard) and 76.26 (product) with augmentation
- Data augmentation improved F1-scores by approximately 4 points for both models
- Training times were shorter for RoBERTa-base due to its smaller model size

## Why This Works (Mechanism)
Data augmentation works by expanding the training dataset with synthetic examples that help models generalize better, particularly for underrepresented classes. In this study, ChatGPT-4o-mini generated additional samples that exposed the models to more diverse linguistic patterns and edge cases in food hazard descriptions and product categories. The augmentation helped the models learn more robust feature representations by providing additional context and variations in how food hazards and products are described. This is particularly valuable in domains with class imbalance, where certain categories have limited training examples, as it prevents overfitting to the majority classes and improves the model's ability to correctly identify rare or underrepresented categories.

## Foundational Learning

**Transformer architecture** - why needed: Provides the foundation for understanding how attention mechanisms process sequential data in both RoBERTa and Flan-T5 models
quick check: Can explain self-attention and multi-head attention mechanisms

**Fine-tuning vs. pre-training** - why needed: Critical for understanding how the models were adapted from their pre-trained states to the specific food detection task
quick check: Can distinguish between parameter updates in fine-tuning versus pre-training

**Class imbalance techniques** - why needed: Essential for understanding why augmentation was focused on underrepresented classes and how it addresses this common ML problem
quick check: Can explain different strategies for handling imbalanced datasets beyond augmentation

**Evaluation metrics** - why needed: Understanding F1-score, precision, and recall is crucial for interpreting the reported performance improvements
quick check: Can calculate and interpret these metrics from a confusion matrix

## Architecture Onboarding

**Component map**: ChatGPT-4o-mini (augmentation) -> Pre-trained model (RoBERTa-base or Flan-T5-base) -> Fine-tuning pipeline -> Evaluation metrics

**Critical path**: Data collection → ChatGPT-4o-mini augmentation → Tokenization → Model fine-tuning → Evaluation

**Design tradeoffs**: The study chose between two established transformer architectures rather than exploring newer or larger models, prioritizing computational efficiency over potentially higher performance from larger models. The augmentation approach prioritized quantity over quality control of generated samples.

**Failure signatures**: Potential overfitting to augmented data, quality issues with ChatGPT-generated samples introducing noise, and class imbalance persisting if augmentation didn't adequately represent all categories.

**First experiments**:
1. Baseline evaluation without augmentation to establish performance metrics
2. Cross-validation to assess model stability across different data splits
3. Error analysis on misclassified samples to identify systematic weaknesses

## Open Questions the Paper Calls Out
None

## Limitations
- The relatively small sample size (6,513 samples) may limit generalizability of results
- Only two specific model architectures were compared, leaving uncertainty about performance with other transformer models
- Quality control measures for augmented data were not thoroughly characterized
- The study doesn't examine potential bias or artifacts introduced through ChatGPT-4o-mini augmentation

## Confidence
**High**: Flan-T5-base consistently outperforms RoBERTa-base across all metrics
**Medium**: Data augmentation improves F1-scores by approximately 4 points across both models
**Medium**: Data augmentation effectively addresses class imbalance

## Next Checks
1. Conduct ablation studies comparing augmentation strategies with different prompt engineering approaches for ChatGPT-4o-mini to identify optimal generation parameters and assess consistency of augmented data quality.

2. Expand model comparison to include larger language models (e.g., GPT-4, Claude) and alternative architectures (e.g., DeBERTa, Longformer) to determine if observed performance patterns generalize beyond the two tested models.

3. Implement cross-validation with stratified sampling across multiple food hazard categories to assess model robustness and identify whether augmentation benefits persist across different domain-specific subsets of the data.