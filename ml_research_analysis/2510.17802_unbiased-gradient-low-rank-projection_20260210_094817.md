---
ver: rpa2
title: Unbiased Gradient Low-Rank Projection
arxiv_id: '2510.17802'
source_url: https://arxiv.org/abs/2510.17802
tags:
- arxiv
- galore
- low-rank
- training
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GaLore Unbiased with Muon (GUM), a memory-efficient
  optimization algorithm for large language model training that addresses the bias
  issue in gradient low-rank projection methods. GUM combines GaLore's gradient low-rank
  projection mechanism with layerwise sampling debiasing and the Muon optimizer to
  achieve unbiased gradient updates while preserving memory efficiency.
---

# Unbiased Gradient Low-Rank Projection

## Quick Facts
- **arXiv ID:** 2510.17802
- **Source URL:** https://arxiv.org/abs/2510.17802
- **Reference count:** 40
- **Primary result:** Proposes GUM, an unbiased low-rank optimization method that outperforms GaLore in LLM fine-tuning and even surpasses full-parameter AdamW in LLM pretraining by 0.3%-1.1% accuracy.

## Executive Summary
This paper addresses the bias problem in gradient low-rank projection methods for efficient LLM training. The authors propose GUM (GaLore Unbiased with Muon), which combines GaLore's gradient low-rank projection with layerwise sampling debiasing and the Muon optimizer to achieve unbiased gradient updates while preserving memory efficiency. Theoretically, GUM matches the convergence guarantees of full-parameter Muon training. Empirically, GUM consistently outperforms GaLore in LLM fine-tuning across multiple tasks and notably outperforms full-parameter AdamW in LLM pretraining, achieving better performance with significantly reduced memory usage.

## Method Summary
GUM extends GaLore by introducing layerwise sampling to debias gradient estimation during low-rank projection. The algorithm performs SVD on gradients every K steps, then samples γ layers with probability q=γ/NL for full-rank updates while using low-rank updates for other layers. The unbiased property is maintained through compensation terms (1/q for sampled layers, 1/(1-q) for others). The method is built on the Muon optimizer, which uses spectral normalization and Newton-Schulz iteration. This design allows GUM to match the theoretical convergence guarantees of full-parameter Muon while achieving significant memory savings compared to both GaLore and standard optimizers.

## Key Results
- GUM consistently outperforms GaLore in LLM fine-tuning across instruction-following, mathematical reasoning, and commonsense reasoning tasks
- In LLM pretraining, GUM outperforms full-parameter AdamW by 0.3%-1.1% overall accuracy while matching or exceeding performance in 6 out of 7 tasks
- GUM achieves significant memory savings compared to full-parameter training while maintaining or improving performance
- The improvement stems from GUM's ability to create more uniform singular value distributions in model weights, leading to better memorization

## Why This Works (Mechanism)

### Mechanism 1: Unbiased Gradient Estimation via Layerwise Sampling
GUM compensates for the bias introduced by low-rank projection through periodic full-rank updates. By sampling γ layers with probability q and applying compensation terms (1/q or 1/(1-q)), the biased error is canceled out in expectation, yielding an unbiased gradient estimate E[G̃]=G. This requires orthonormal projection matrices and commutative update rules.

### Mechanism 2: Preservation of Theoretical Convergence Guarantees
The unbiased property allows GUM to match the non-convex convergence rate of the base Muon optimizer. The theoretical proof leverages unbiasedness to establish O(T^(-1/4)) convergence in stochastic cases and O(T^(-1/2)) in deterministic cases, contrasting with biased methods that can diverge in noisy settings.

### Mechanism 3: Empirical Performance Gain from High-Rank Updates
GUM-trained models exhibit higher stable rank and more uniformly distributed singular values in their weights. This more efficient utilization of parameter space and more long-tailed activation distribution is hypothesized to improve memorization, though the causal link remains correlational rather than proven.

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD) for Low-Rank Approximation
  - **Why needed here:** GaLore projects gradients using top-r singular vectors from SVD. Understanding SVD is essential to grasp what information is kept versus discarded.
  - **Quick check question:** How does projecting a matrix onto its top-r singular vectors affect its rank and information content?

- **Concept:** Stochastic Optimization and Convergence Guarantees
  - **Why needed here:** The core contribution is a convergence proof. Understanding what a convergence rate signifies and why unbiasedness is critical for establishing it is essential.
  - **Quick check question:** Why is the unbiasedness of a gradient estimator a key requirement for proving an optimizer will converge to a stationary point?

- **Concept:** The Muon Optimizer and Matrix-Normalized Updates
  - **Why needed here:** GUM is built on Muon, which uses spectral normalization. This matrix-level operation is crucial for the commutativity property that GUM's unbiased proof relies on.
  - **Quick check question:** How does Muon's update rule differ from Adam's, and why is its matrix-level operation important for GUM's design?

## Architecture Onboarding

- **Component map:** Model parameters, optimizer hyperparameters (η, β, r, γ, K) -> SVD computation -> Layer sampling (probability q) -> Full-rank or low-rank update with compensation terms -> Updated model parameters

- **Critical path:** The scaling factors 1/(1-q) and 1/q in the update equations. These constants mathematically ensure the unbiased property; a bug here breaks the theoretical guarantee.

- **Design tradeoffs:**
  - Memory vs. Convergence: Higher sampling probability q increases memory usage but improves convergence stability
  - Projection rank r: Lower r saves memory but may require higher q
  - Period length K: Longer periods reduce SVD frequency but may allow the projector to become stale

- **Failure signatures:**
  - Divergence in noisy settings: If q is too low, variance from sparse full-rank updates may cause instability
  - Stale projector: If K is too long, the projection matrix may misalign with true gradient subspace
  - Implementation error: Incorrect scaling factors would break unbiasedness, potentially causing divergence

- **First 3 experiments:**
  1. Synthetic Validation: Replicate noisy linear regression experiment to verify GUM converges where GaLore fails
  2. Fine-tuning Ablation: Sweep over q and r on single LLM/task to find memory-performance Pareto frontier
  3. Stable Rank Analysis: Train small model with GUM and GaLore, plot stable rank of weights over time to verify correlation with performance

## Open Questions the Paper Calls Out

- **Open Question 1:** Can standard variance reduction techniques effectively mitigate the high variance and instability introduced by GUM's sampled high-rank updates?
  - **Basis:** Authors state the technique "introduces high variance... which leads to instability" and leave variance reduction tools "for future work"

- **Open Question 2:** How does GUM perform when applied to non-LLM architectures such as diffusion models or BERT?
  - **Basis:** Paper lists "other types of models" and different applications as "interesting questions" regarding empirical performance

- **Open Question 3:** What are the theoretical convergence properties when combining GUM with acceleration and generalization techniques?
  - **Basis:** Limitations section states combining analysis with "other acceleration... and generalization techniques" results in properties "worth investigating as open problems"

## Limitations

- Theoretical guarantees rely on idealized conditions (exact Newton-Schulz iteration, smooth spectral-norm objectives) that may not hold in practical large-scale training
- Pre-training experiments limited to small models (60M-350M parameters), raising uncertainty about performance at true LLM scale
- Proposed performance mechanism (uniform singular value distributions) is supported by correlation but lacks direct causal evidence

## Confidence

- **High Confidence:** Unbiasedness proof and theoretical convergence guarantees are mathematically rigorous given stated assumptions
- **Medium Confidence:** Empirical performance improvements in fine-tuning are well-documented across multiple tasks and models
- **Low Confidence:** Claim that GUM outperforms full-parameter AdamW in LLM pretraining is based on small models and may not generalize

## Next Checks

1. **Large-Scale Pre-training Validation:** Replicate pre-training experiments using 1B-3B parameter model on C4 to verify 0.3%-1.1% accuracy improvements over AdamW at larger scales

2. **Mechanism Isolation Experiment:** Train identical models with GUM and GaLore while monitoring singular value distributions and stable ranks; artificially enforce uniform distributions to test direct causal effects

3. **Noisy Setting Robustness:** Systematically vary noise levels in synthetic linear regression experiment and test GUM across wider range of sampling probabilities and projection ranks to map stability frontier