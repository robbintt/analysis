---
ver: rpa2
title: Continual Learning With Quasi-Newton Methods
arxiv_id: '2503.19939'
source_url: https://arxiv.org/abs/2503.19939
tags:
- csqn
- learning
- methods
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in neural networks
  during continual learning, where models lose previously learned knowledge when adapting
  to new tasks. The authors propose Continual Learning with Sampled Quasi-Newton (CSQN),
  an extension of Elastic Weight Consolidation (EWC) that improves Hessian approximation
  by leveraging Quasi-Newton methods.
---

# Continual Learning With Quasi-Newton Methods

## Quick Facts
- arXiv ID: 2503.19939
- Source URL: https://arxiv.org/abs/2503.19939
- Reference count: 40
- Key outcome: CSQN reduces EWC's catastrophic forgetting by 50% and improves performance by 8% on average

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by extending Elastic Weight Consolidation (EWC) with Sampled Quasi-Newton (SQN) methods. The authors propose CSQN, which captures parameter interactions beyond the diagonal Fisher approximation used in EWC, without requiring architecture-specific modifications. By sampling points in parameter space and applying BFGS or SR1 updates, CSQN constructs a more accurate Hessian approximation that better preserves learned knowledge when adapting to new tasks. The method demonstrates consistent improvements over EWC and other baselines across four benchmarks, achieving superior results in three out of four scenarios while providing strategies to manage memory requirements.

## Method Summary
CSQN extends EWC by using Sampled Quasi-Newton methods to construct a more accurate Hessian approximation for regularization. After training each task, it computes the diagonal Fisher matrix (EWC baseline), then samples M points around the current parameters to build S and Y matrices through gradient differences. These matrices update the Hessian approximation via BFGS or SR1 equations, creating a low-rank correction to the diagonal baseline. The regularization loss constrains parameter updates to preserve performance on previous tasks. The method includes memory management strategies (BTREE and MRT) to handle the storage cost of accumulating Hessian information across multiple tasks.

## Key Results
- CSQN reduces EWC's forgetting by 50% and improves average accuracy by 8%
- Outperforms EWC and other state-of-the-art baselines on three out of four benchmarks
- Achieves superior results on Split CIFAR-10/100, Split TinyImageNet, and Vision Datasets
- Demonstrates effectiveness across both CNN and MLP architectures, though with architecture-dependent performance patterns

## Why This Works (Mechanism)

### Mechanism 1: Off-Diagonal Curvature Capture
EWC relies on the diagonal of the Fisher Information Matrix, assuming parameters are independent. CSQN uses Sampled Quasi-Newton methods to construct a Hessian approximation that includes these correlations. By sampling points in parameter space and computing gradient differences, it captures parameter interactions beyond the diagonal, providing a more accurate estimate of the loss landscape's curvature. This allows for better protection of critical parameter combinations during new task learning.

### Mechanism 2: Sampling-Based Decoupling
Unlike standard Quasi-Newton methods that accumulate curvature information along the optimization path, CSQN decouples Hessian estimation from the optimizer's trajectory. It samples points around converged parameters at task end, allowing the use of standard first-order optimizers during training while still obtaining second-order information for regularization. This approach provides a representative estimate of local curvature independent of the path taken to reach the optimum.

### Mechanism 3: Hybrid Regularization Initialization
CSQN initializes the Quasi-Newton estimate with EWC's diagonal FIM, creating a hybrid regularization approach. This initialization stabilizes the approximation and acts as a performance floor, ensuring CSQN never performs worse than EWC alone. The regularization loss can be split into EWC's diagonal protection plus an interaction term from the SQN component, combining the strengths of both approaches.

## Foundational Learning

- **Concept: Laplace Approximation & Fisher Information**
  - Why needed: CSQN refines the Laplace approximation used in EWC. Understanding EWC's Gaussian posterior approximation using the diagonal Fisher is essential to grasp what CSQN improves.
  - Quick check: Can you explain why assuming the Hessian is diagonal implies assuming parameter independence?

- **Concept: Quasi-Newton Methods (BFGS/SR1)**
  - Why needed: CSQN leverages SR1 and BFGS update equations to improve the Hessian estimate. You need to understand how these methods use gradient differences and parameter steps to approximate second-order derivatives.
  - Quick check: What is the difference in storage cost between a rank-1 (SR1) and rank-2 (BFGS) update?

- **Concept: Continual Learning Regularization**
  - Why needed: The core objective is to constrain optimization to preserve old task performance. CSQN modifies the quadratic constraint to be anisotropic and correlated rather than axis-aligned.
  - Quick check: How does the regularization loss mathematically penalize moving away from previous task parameters?

## Architecture Onboarding

- **Component map:** Trainer -> FIM Calculator -> SQN Sampler -> Hessian Builder -> Memory Manager
- **Critical path:** 1) Train task to completion, 2) Compute diagonal Fisher, 3) Sample perturbations and build S,Y matrices, 4) Compute Hessian via BFGS/SR1, 5) Accumulate and compress if necessary
- **Design tradeoffs:**
  - Rank M: Higher values capture more curvature but cost more memory; M=10 often sufficient
  - SR1 vs BFGS: SR1 is more memory efficient but requires eigenvalue decomposition; BFGS guarantees positive definiteness
  - Memory Strategy: BTREE offers best balance; MRT is most memory-efficient constant strategy
- **Failure signatures:**
  - Over-regularization: Too high M on small datasets can restrict learning new features
  - Memory Explosion: Linear growth without compression leads to OOM on long sequences
  - Numerical Instability: SR1 requires checking positive definiteness; negative eigenvalues cause issues
- **First 3 experiments:**
  1. Validate rank importance: Run Split CIFAR-10/100 with M ∈ {1, 5, 10, 20} to confirm M=1 already outperforms EWC
  2. Memory strategy stress test: Run Split TinyImageNet using MRT and BTREE, compare against non-reduced CSQN
  3. Architecture comparison: Run Rotated MNIST (MLP) vs. Vision Datasets (LeNet) to verify architecture-dependent performance patterns

## Open Questions the Paper Calls Out

- Can efficient sampling strategies or hybrid approaches significantly reduce CSQN's memory overhead without compromising regularization performance? The authors note that future work should explore efficient sampling or hybrid approaches to address substantial memory requirements, though strategies like BTREE and MRT were tested.

- How does CSQN's Hessian-based regularization perform in non-continual learning paradigms such as transfer learning, semi-supervised learning, or model merging? The paper identifies these as potential extensions where Hessian estimation could be beneficial but states this is left for future research.

- Why does the Kronecker-factored (KF) approximation outperform CSQN on MLP architectures while CSQN outperforms KF on CNNs? The authors observe this discrepancy and speculate it relates to low-rank versus block-diagonal approximations, but haven't isolated the architectural factor.

## Limitations
- Memory complexity grows linearly with tasks, requiring additional compression strategies for long task sequences
- Comparisons against the most recent state-of-the-art methods (like DIAMOND) are limited
- The assumption that local sampling accurately captures Hessian geometry may not hold in highly non-quadratic regions

## Confidence

- CSQN outperforms EWC mechanism: **High** (supported by multiple experiments and theoretical justification)
- Memory reduction strategies work without major degradation: **Medium** (BTREE shows promise, but MRT requires task-specific tuning)
- CSQN consistently beats all baselines: **Medium** (mixed results; excels on CNNs but may lag on MLPs)

## Next Checks

1. Test CSQN on a 50-task sequence to evaluate practical limits of memory reduction strategies and identify when performance degradation becomes unacceptable.

2. Compare CSQN against the most recent rehearsal-based methods on the same benchmarks to establish its position in the current state-of-the-art.

3. Perform ablation studies on the sampling covariance matrix Σ to determine if alternative sampling distributions yield better Hessian approximations for specific architectures.