---
ver: rpa2
title: 'DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry
  Understanding'
arxiv_id: '2508.20416'
source_url: https://arxiv.org/abs/2508.20416
tags:
- uni00000011
- uni00000015
- uni00000014
- uni00000016
- uni0000001b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DentalBench is the first bilingual benchmark for evaluating LLMs
  in dentistry, featuring DentalQA (36,597 questions across 4 tasks and 16 subfields)
  and DentalCorpus (337.35 million tokens for domain adaptation). Evaluations of 14
  LLMs revealed significant performance gaps across task types and languages.
---

# DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding

## Quick Facts
- arXiv ID: 2508.20416
- Source URL: https://arxiv.org/abs/2508.20416
- Reference count: 23
- Introduces first bilingual benchmark for evaluating LLMs in dentistry

## Executive Summary
DentalBench introduces the first comprehensive bilingual benchmark for evaluating Large Language Models (LLMs) in dentistry, addressing the critical need for trustworthy AI in healthcare applications. The benchmark features DentalQA with 36,597 questions across 4 task types (multiple-choice, multi-answer, open-ended, and definition questions) and 16 subfields, plus DentalCorpus containing 337.35 million tokens for domain adaptation. Evaluations of 14 LLMs revealed significant performance gaps across task types and languages, with domain adaptation experiments demonstrating that supervised fine-tuning and retrieval-augmented generation substantially improve performance, especially on knowledge-intensive tasks.

## Method Summary
The study constructs a bilingual benchmark using keyword filtering to collect 440 English and 235 Chinese dental keywords from online sources, then employs GPT-4o for initial filtering followed by human validation to ensure quality. DentalQA contains 36,597 questions spanning four task types and 16 subfields, split 4:1 for training and testing. DentalCorpus provides 337.35 million tokens (319.08M English, 18.27M Chinese) for domain adaptation. The evaluation uses zero-shot prompting with task-specific templates, measuring MCQ/MAQ accuracy and OEQ/DEF using BERTScore F1. Domain adaptation experiments employ Qwen-2.5-3B with supervised fine-tuning (4 epochs, lr=1e-6) and retrieval-augmented generation using FAISS with bge-m3 embeddings.

## Key Results
- SFT improved MCQ-ZH accuracy from 48.63% to 54.58% (+5.95%)
- RAG increased OEQ-ZH BERTScore from 20.89 to 30.18 (+9.29)
- SFT+RAG combination achieved additive gains of +11.43 on MCQ-ZH
- Medical LLMs (BioMistral-7B) underperformed general models (Llama-3.1-8B) on factual tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning on domain-specific QA data improves factual recall in specialized medical tasks.
- Mechanism: SFT exposes the model to in-domain question-answer patterns, updating model weights to better encode dental terminology, diagnostic knowledge, and clinical reasoning patterns.
- Core assumption: The training distribution in DentalQA sufficiently represents the target dental knowledge space to improve generalization on held-out test questions.
- Evidence anchors: SFT improved MCQ-ZH from 48.63% to 54.58% (+5.95%); related work confirms LLMs struggle with dental reasoning without domain adaptation.

### Mechanism 2
- Claim: Retrieval-augmented generation provides stronger gains on open-ended generative tasks than on structured multiple-choice tasks.
- Mechanism: RAG prepends the top-5 most relevant passages from DentalCorpus to the prompt, providing explicit context for the model to condition on.
- Core assumption: The retrieval system successfully surfaces contextually relevant passages.
- Evidence anchors: RAG improved OEQ-ZH BERTScore by 44% (20.89 to 30.18); RAG impact was larger on open-ended tasks than structured tasks.

### Mechanism 3
- Claim: Combining SFT and RAG yields additive improvements on knowledge-intensive tasks but shows diminishing returns on generative tasks compared to RAG alone.
- Mechanism: SFT internalizes domain knowledge while RAG provides external context at inference time.
- Core assumption: The combination preserves the benefits of each method without interference.
- Evidence anchors: SFT+RAG achieved +11.43 on MCQ-ZH vs +5.95 (SFT alone) and +4.47 (RAG alone).

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Understanding how external knowledge retrieval improves model performance on knowledge-intensive tasks without modifying model weights.
  - Quick check question: Can you explain why RAG might be preferred over SFT when rapid knowledge updates are required?

- **BERTScore for Text Generation Evaluation**
  - Why needed here: Understanding how semantic similarity (rather than exact match) is used to evaluate open-ended medical responses.
  - Quick check question: Why might BERTScore be more appropriate than BLEU for evaluating clinical explanations?

- **Domain Adaptation vs. General Medical Training**
  - Why needed here: Understanding why medical-specific LLMs may underperform general models on specialized domains like dentistry.
  - Quick check question: Why might BioMistral-7B underperform Llama-3.1-8B on dental multiple-choice questions despite medical pre-training?

## Architecture Onboarding

- **Component map:** DentalQA (benchmark) -> DentalCorpus (knowledge base) -> FAISS + bge-m3 embeddings -> SFT/RAG adaptation -> LLM evaluation
- **Critical path:** Corpus construction → Benchmark construction → Baseline evaluation → Domain adaptation experiments
- **Design tradeoffs:** Bilingual coverage vs. balanced dataset; keyword-based filtering vs. false negatives; SFT efficiency vs. RAG latency
- **Failure signatures:** Medical LLMs underperforming general models on factual tasks; low OEQ/DEF scores across all models; RAG effectiveness varies by language
- **First 3 experiments:** 1) Replicate SFT experiment on Qwen-2.5-3B; 2) Implement RAG pipeline with FAISS + bge-m3; 3) Evaluate SFT+RAG combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can DentalBench be expanded to include balanced bilingual resources and full task coverage, specifically by adding English Multi-Answer Questions (MAQ)?
- Basis in paper: The authors state in the Limitations section: "In future work, we aim to construct balanced bilingual resources and expand task coverage across languages," explicitly noting that the MAQ format is currently only available in Chinese.

### Open Question 2
- Question: Why does increasing model scale significantly improve performance on selection-based tasks (MCQ/MAQ) while yielding limited gains on generative tasks (OEQ/DEF)?
- Basis in paper: The paper observes that within the Qwen-2.5 family, "scaling improves MCQ and MAQ notably but yields limited gains on OEQ and DEF," suggesting factual recall benefits more from model size than generative reasoning.

### Open Question 3
- Question: What causes the inconsistent benefit of Retrieval-Augmented Generation (RAG) on open-ended tasks between Chinese and English contexts?
- Basis in paper: The results show that combining SFT with RAG offers a "clear benefit over SFT alone in Chinese, while in English the effect is less consistent," indicating a language sensitivity in retrieval effectiveness.

## Limitations

- Severe language imbalance (319M EN vs 18M ZH tokens) potentially biasing RAG performance
- MAQ task exists only in Chinese, preventing direct cross-lingual comparison of multi-answer reasoning
- Heavy reliance on automated metrics (BERTScore) without extensive human validation of open-ended responses

## Confidence

- **High Confidence:** SFT improves factual recall on MCQ/MAQ tasks (supported by 5-11% accuracy gains)
- **Medium Confidence:** RAG provides consistent gains on knowledge-intensive tasks (though English effectiveness is less reliable)
- **Medium Confidence:** Domain-specific adaptation outperforms general medical LLMs on dental tasks

## Next Checks

1. Test retrieval quality on underrepresented subfields to verify whether SFT+RAG combinations can compensate for corpus coverage gaps
2. Conduct human evaluation of open-ended responses to validate BERTScore as a proxy for clinical reasoning quality
3. Replicate experiments with balanced English-Chinese corpus to isolate language effects from domain adaptation benefits