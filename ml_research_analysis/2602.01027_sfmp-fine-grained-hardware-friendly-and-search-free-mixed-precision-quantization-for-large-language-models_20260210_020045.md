---
ver: rpa2
title: 'SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization
  for Large Language Models'
arxiv_id: '2602.01027'
source_url: https://arxiv.org/abs/2602.01027
tags:
- sfmp
- quantization
- mixed-precision
- weight
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SFMP introduces a search-free and hardware-friendly mixed-precision
  quantization framework for large language models. It extends integer bit-width to
  fractional values, enabling direct precision allocation based on global weight salience
  without solving discrete optimization problems.
---

# SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2602.01027
- Source URL: https://arxiv.org/abs/2602.01027
- Reference count: 40
- Key outcome: Search-free mixed-precision quantization achieving up to 98.90% zero-shot accuracy at 3.5 bits per weight for LLaMA3.1-70B, with 30-50% inference speedup over group-wise methods

## Executive Summary
SFMP introduces a search-free and hardware-friendly mixed-precision quantization framework for large language models. It extends integer bit-width to fractional values, enabling direct precision allocation based on global weight salience without solving discrete optimization problems. The method combines block-wise mixed-precision with row-column weight reordering to aggregate salient weights and align them with hardware-friendly memory layouts, supported by a unified GEMM kernel that eliminates weight dequantization overhead.

## Method Summary
SFMP operates through three key stages: (1) Fisher Information-based salience estimation from calibration data, (2) row-column reordering to cluster high-salience weights, and (3) block-wise mixed-precision allocation using fractional bit-widths. The method restricts each weight to either floor or ceiling of target BPW based on salience threshold, then applies AWQ quantization per block. A unified GEMM kernel using one-bit LUT-based computation avoids dequantization overhead, enabling inference speed to scale linearly with BPW while maintaining accuracy.

## Key Results
- Achieves 98.90% of full-precision zero-shot accuracy at 3.5 bits per weight for LLaMA3.1-70B
- Inference speed improved by 30-50% compared to group-wise mixed-precision (SliM-LLM)
- Compression time reduced from 44 hours (AMQ) to 0.05-0.15 hours
- Outperforms uniform quantization (GPTQ) at low bit-widths due to one-bit LUT-based GEMM design

## Why This Works (Mechanism)

### Mechanism 1: Search-Free Fractional Bit Allocation via Continuous Relaxation
Transforming mixed-precision allocation from a discrete combinatorial problem into a continuous thresholding problem eliminates expensive search costs. The method restricts candidates to floor and ceiling of target BPW, allocating higher precision to top-k percentile of weights based on Fisher salience. Core assumption: Fisher diagonal accurately ranks global weight importance without cross-term interactions.

### Mechanism 2: Hardware-Friendly Salience Aggregation via Row-Column Reordering
Structured reordering of weight matrices aligns scattered salient weights into contiguous blocks, enabling efficient block-wise mixed-precision without sparse memory access. Instead of element-wise allocation, the method permutes rows and columns based on aggregated salience sums to cluster high-salience weights into specific regions.

### Mechanism 3: Dequantization-Free Compute via Bit-Serial LUT
A unified GEMM kernel using lookup tables avoids runtime overhead of weight unpacking and dequantization, allowing inference speed to scale linearly with BPW. Quantized weights are decomposed into binary matrices, with pre-computed dot products stored in LUTs for all possible binary combinations of activation groups.

## Foundational Learning

- **Concept**: Fisher Information Matrix (FIM)
  - Why needed: Used to estimate "salience" or sensitivity of model weights to perturbation, approximating Hessian for precision decisions
  - Quick check: Why does the paper approximate full Hessian with diagonal FIM, and what cross-term information is lost?

- **Concept**: Integer Linear Programming (ILP) & NP-Hardness
  - Why needed: Standard mixed-precision quantization is framed as NP-hard ILP problem to motivate "search-free" approach
  - Quick check: How does restricting bit-width candidates to {floor(b), ceiling(b)} simplify optimization versus selecting from {2, 3, 4, 8}?

- **Concept**: Bit-Serial Computation
  - Why needed: Inference kernel processes weights one bit at a time rather than parallel word-level arithmetic
  - Quick check: In LUT-based GEMM, how does computational cost scale with bit-width of weights (W)?

## Architecture Onboarding

- **Component map**: Calibration Data -> Fisher Estimation (Salience Matrix) -> Row/Col Sorting -> Weight Matrix Permutation -> Block Partitioning -> Bit Allocation -> Bit-Plane Decomposition & Packing -> Runtime Activation Reordering -> LUT Construction -> Bit-Serial Accumulation

- **Critical path**: LUT Construction and Lookup during runtime kernel. Efficiency relies on pre-computed table fitting in fast memory and being accessed without bank conflicts.

- **Design tradeoffs**: Block Size (mb=512, nb=groupsize) affects precision granularity vs overhead. Group Size impacts LUT size (2^group_size). Reordering overhead occurs at runtime but enables faster GEMM.

- **Failure signatures**: Accuracy collapse at low BPW if reordering fails to cluster salient weights. Kernel latency plateau if LUT exceeds shared memory. Search cost anomaly if Fisher estimation takes longer than expected.

- **First 3 experiments**: 1) Implement bit-allocation logic (Eq. 7) on small MLP comparing random vs actual Fisher salience. 2) Profile LUT-based GEMM kernel varying Group Size to measure shared memory break point. 3) Run SFMP with reordering disabled vs enabled on LLaMA-8B at 3.0 BPW to quantify clustering benefit.

## Open Questions the Paper Calls Out

### Open Question 1
Can group size parameter be integrated into mixed-precision optimization process to become adaptive rather than fixed heuristic? The paper notes this as a promising future direction since Table 6 shows group size significantly impacts accuracy non-monotonically.

### Open Question 2
How can unified GEMM kernel and block-major memory layout be efficiently adapted for non-GPU hardware platforms? Current implementation leverages GPU-specific features like warp-level parallelism that may not map to CPUs or specialized accelerators.

### Open Question 3
Does efficiency of unified GEMM kernel degrade with larger batch sizes, and how can it be optimized for server-side inference? Current implementation targets memory-constrained edge scenarios and optimizes primarily for batch size = 1.

## Limitations
- Reliance on Fisher Information as global salience proxy may fail to capture local weight interactions critical for preserving model fidelity at extreme bit-widths
- Hardware-centric LUT design assumes specific GPU memory hierarchies that may not generalize across architectures
- Block-wise constraint (512x128) may not optimally handle non-rectangular attention heads or residual connections common in modern LLM architectures

## Confidence

- **High Confidence**: Search-free fractional bit allocation mechanism and computational benefits are well-supported by continuous relaxation formulation and comparative runtime results
- **Medium Confidence**: Hardware efficiency claims depend heavily on specific CUDA kernel implementation details not fully disclosed
- **Medium Confidence**: Empirical superiority over AMQ and BitStack demonstrated, but comparisons assume identical calibration datasets and training procedures

## Next Checks

1. **Fisher Estimation Robustness**: Validate diagonal Fisher approximation by comparing accuracy when using full Hessian estimation versus diagonal approximation across varying calibration dataset sizes

2. **Memory Hierarchy Dependence**: Benchmark LUT-based GEMM kernel on different GPU architectures with varying shared memory capacities to quantify break condition where performance degrades due to memory spilling

3. **Reordering Effectiveness Analysis**: Systematically measure correlation between Fisher salience distribution entropy and accuracy degradation at low BPW to quantify when structured clustering assumption breaks down