---
ver: rpa2
title: 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?'
arxiv_id: '2502.00674'
source_url: https://arxiv.org/abs/2502.00674
tags:
- self-moa
- performance
- arxiv
- diversity
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether ensembling different Large Language
  Models (LLMs) is beneficial compared to repeatedly sampling from a single top-performing
  model. The authors introduce Self-MoA, which aggregates multiple outputs from the
  same high-performing LLM, and compare it to standard Mixed-MoA, which mixes outputs
  from different models.
---

# Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?

## Quick Facts
- arXiv ID: 2502.00674
- Source URL: https://arxiv.org/abs/2502.00674
- Reference count: 21
- Key outcome: Self-MoA achieves 6.6% improvement over Mixed-MoA on AlpacaEval 2.0 by aggregating multiple outputs from a single top-performing model.

## Executive Summary
This paper investigates whether ensembling different Large Language Models (LLMs) is beneficial compared to repeatedly sampling from a single top-performing model. The authors introduce Self-MoA, which aggregates multiple outputs from the same high-performing LLM, and compare it to standard Mixed-MoA, which mixes outputs from different models. Experiments on AlpacaEval 2.0 show that Self-MoA achieves a 6.6% improvement over Mixed-MoA. Further analysis reveals that MoA performance is highly sensitive to quality, and mixing different LLMs often lowers average quality. The authors also propose Self-MoA-Seq, a sequential version that scales inference compute without requiring full context length. Overall, Self-MoA outperforms Mixed-MoA in many scenarios, especially when quality differences among models are significant.

## Method Summary
The method introduces Self-MoA (aggregating multiple samples from a single strong model) and Self-MoA-Seq (scalable sequential aggregation) as alternatives to Mixed-MoA (aggregating different models). For Self-MoA, a single proposer model generates multiple outputs at temperature 0.7, which an aggregator LLM synthesizes at temperature 0. Self-MoA-Seq processes samples in sliding windows to bypass context length limits. The paper evaluates on AlpacaEval 2.0, MMLU-redux, CRUX, and MATH benchmarks using various models including Qwen, WizardLM, LLaMA-3, Mixtral, and others. Quality-diversity trade-offs are analyzed through linear regression, with quality measured by average accuracy and diversity by Vendi Score.

## Key Results
- Self-MoA outperforms Mixed-MoA by 6.6% on AlpacaEval 2.0
- MoA performance is highly sensitive to quality rather than diversity
- Self-MoA-Seq achieves comparable performance to full aggregation while bypassing context limits
- Mixed-MoA only shows marginal gains (0.17-0.35%) even in constructed multi-task scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregator performance is more sensitive to proposer quality than proposer diversity.
- Mechanism: Linear regression analysis across 200+ configurations shows quality coefficient (α) exceeds diversity coefficient (β) on all benchmarks (α=2.56–4.72 vs β=1.42–2.84, p<0.001). When average proposer quality drops due to weak models in the mixture, aggregation cannot recover the loss.
- Core assumption: The aggregator reliably synthesizes correct components when present, rather than being misled by confident but wrong outputs.
- Evidence anchors:
  - [abstract] "MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models."
  - [section 4.1] "α is greater than β across all three datasets... MoA's performance is particularly sensitive to variations in quality."
  - [corpus] Related work on efficient MoA routing (RouteMoA, arXiv 2601.18130) supports that model selection matters, but does not directly validate the quality-diversity coefficient comparison.
- Break condition: If aggregator has weak reasoning ability relative to task difficulty, it may fail to identify correct components even from high-quality proposers.

### Mechanism 2
- Claim: Repeated sampling from a single strong model (in-model diversity) provides sufficient diversity for effective aggregation.
- Mechanism: Sampling with temperature >0 from a capable model yields varied reasoning paths. The aggregator synthesizes these paths, selecting correct elements more reliably than majority vote would.
- Core assumption: The model's error patterns are not systematically correlated across samples—diverse samples produce uncorrelated errors that aggregation can filter.
- Evidence anchors:
  - [abstract] "Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios."
  - [section 3.1] "Self-MoA demonstrates remarkable effectiveness... outperforming the Mixed-MoA baseline by 6.6 point."
  - [corpus] No direct corpus validation for in-model diversity sufficiency claim.
- Break condition: If the model has systematic blind spots (e.g., consistent reasoning failures on certain problem types), repeated sampling will not help.

### Mechanism 3
- Claim: Sequential aggregation with sliding window maintains performance while bypassing context length limits.
- Mechanism: Self-MoA-Seq processes k samples at a time, carrying forward the current best synthesis. Repeating this synthesis in the context biases subsequent rounds toward it, reducing noise accumulation.
- Core assumption: Early synthesis quality is sufficiently high that carrying it forward does not entrench early errors.
- Evidence anchors:
  - [abstract] "Sequential version... capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once."
  - [section 5] "Self-MoA-Seq delivers performance that is comparable to, or slightly better than, Self-MoA."
  - [corpus] Attention-MoA (arXiv 2601.16596) explores residual synthesis connections but does not validate the sliding-window claim.
- Break condition: If early rounds produce incorrect synthesis and the model over-commits to prior context, errors compound.

## Foundational Learning

- Concept: **Quality-Diversity Trade-off in Ensembles**
  - Why needed here: The central claim hinges on understanding that higher diversity from weaker models harms aggregation more than lower diversity from a single strong model helps.
  - Quick check question: Given proposers A (90% accuracy) and B (60% accuracy), does mixing both necessarily improve ensemble output over A alone?

- Concept: **Temperature Sampling for Diversity**
  - Why needed here: Self-MoA relies on stochastic sampling (temperature 0.7) to generate diverse outputs from a single model.
  - Quick check question: What happens to output diversity if temperature is set to 0?

- Concept: **Context Window Constraints in Aggregation**
  - Why needed here: Self-MoA-Seq exists because aggregators cannot ingest unlimited proposer outputs; understanding token limits is critical for scaling.
  - Quick check question: If each proposer output is ~500 tokens and the aggregator has 8K context, how many outputs can be aggregated in one pass (accounting for prompt overhead)?

## Architecture Onboarding

- Component map:
  - Proposer -> Aggregator -> Final Output
  - Self-MoA-Seq Controller manages sliding window processing

- Critical path: Proposer sampling → Aggregator synthesis → (if Seq) window advancement → Final output. Latency dominated by sequential proposer calls and aggregator forward passes.

- Design tradeoffs:
  - More samples → better coverage but higher latency and context pressure
  - Higher temperature → more diversity but risk of incoherent samples
  - Mixed-MoA vs Self-MoA: Mixed may help when models have comparable quality but orthogonal strengths (Table 6 shows narrow 0.17–0.35% gains in constructed multi-task setting)

- Failure signatures:
  - Aggregator output regresses to proposer average: suggests aggregator is underpowered or prompt is weak
  - Self-MoA-Seq performance degrades with more rounds: early synthesis errors are compounding
  - No improvement over base model: sampling temperature too low, or task does not benefit from aggregation

- First 3 experiments:
  1. **Baseline validation**: On MMLU-redux (3K samples), compare base model vs Self-MoA (6 samples) vs Mixed-MoA (6 models). Confirm Self-MoA wins when quality variance is high.
  2. **Temperature sweep**: Fix n=6, vary temperature {0.5, 0.7, 1.0, 1.2}. Plot quality vs diversity (Vendi Score) vs final accuracy to locate Pareto front.
  3. **Scaling test**: With n={6, 12, 20, 30} samples, compare Self-MoA (single-pass) vs Self-MoA-Seq (window=6). Measure accuracy and latency; identify where context overflow occurs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Self-MoA performance scale effectively with the number of repeated samples, or does it suffer from diminishing returns due to diversity plateau?
- Basis in paper: [explicit] Section 5 explicitly poses the question: "Given a strong model, does Self-MoA’s performance scale with the number of repeated samples?" The authors identify three limitations to indefinite scaling, including diversity plateau and aggregation difficulty.
- Why unresolved: The experiments (Figure 3) show that adding samples can have "positive and negative effects," leading the authors to conclude there is "no universal compute-optimal solution."
- What evidence would resolve it: A theoretical analysis of the diversity-quality curve as sample count $N \to \infty$, or empirical results demonstrating a consistent saturation point across different model architectures and task types.

### Open Question 2
- Question: In what specific, non-constructed scenarios does Mixed-MoA provide a statistically significant advantage over Self-MoA?
- Basis in paper: [explicit] The abstract and Section 4.2 aim to "identify the scenarios where mixing different LLMs could be helpful." While the authors simulated a "mixture task" with specialized models, Mixed-MoA only outperformed Self-MoA by negligible margins (0.17%–0.35%).
- Why unresolved: The paper concludes that Self-MoA is dominant even in tasks designed to favor the diversity of specialized models, leaving the existence of a robust use-case for Mixed-MoA uncertain.
- What evidence would resolve it: Identification of a naturalistic, complex task (e.g., long-horizon coding or multi-modal reasoning) where cross-model diversity yields consistent gains exceeding the noise margin.

### Open Question 3
- Question: To what extent does the relative capability of the aggregator limit the performance of Self-MoA when aggregating outputs from a superior proposer?
- Basis in paper: [inferred] In Section 4.1, the authors note a breakdown in their statistical analysis on the MATH dataset, conjecturing that the aggregator (Qwen2-7B-Instruct) was "relatively weak" compared to the proposer (Qwen2-Math-7B-Instruct).
- Why unresolved: The paper does not isolate the "aggregator bottleneck" as an independent variable; it remains unclear if a weaker aggregator synthesizes a "lowest common denominator" response, wasting the potential of a stronger proposer.
- What evidence would resolve it: An ablation study systematically varying the "strength gap" (e.g., model size or benchmark score) between the proposer and aggregator to measure the resultant performance degradation.

## Limitations
- Empirical findings heavily rely on AlpacaEval 2.0 instruction-following benchmark
- Quality-diversity regression analysis not independently validated across external datasets
- Assumption that aggregator synthesis is reliable untested under edge cases where proposers are systematically wrong
- Sequential variant evaluated on fewer tasks with sparse ablation of window size

## Confidence
- **High confidence**: Quality sensitivity of MoA performance, mechanism that mixing lowers average quality when models differ in capability
- **Medium confidence**: Claim that in-model diversity suffices for aggregation, based on AlpacaEval win but limited scope
- **Low confidence**: That Self-MoA-Seq is always "as effective" as full aggregation—evidence is thin and task coverage sparse

## Next Checks
1. **Cross-benchmark consistency test**: Run Self-MoA and Mixed-MoA on MMLU and MATH with the same quality-diversity splits used in AlpacaEval to verify whether the 6.6% gap persists under different task demands
2. **Aggregator robustness ablation**: For each task, measure final accuracy when aggregator receives a mixture of high-quality and low-quality proposers, and compare to theoretical upper bound from quality regression coefficients
3. **Systematic error analysis**: Construct a synthetic benchmark where proposer models have known correlated blind spots; measure whether Self-MoA reduces or amplifies error clustering compared to Mixed-MoA