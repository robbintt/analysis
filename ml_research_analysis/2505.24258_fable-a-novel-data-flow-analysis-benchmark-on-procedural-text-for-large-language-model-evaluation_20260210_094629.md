---
ver: rpa2
title: 'FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language
  Model Evaluation'
arxiv_id: '2505.24258'
source_url: https://arxiv.org/abs/2505.24258
tags:
- data-flow
- procedural
- recipes
- travel
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FABLE introduces a novel benchmark for evaluating large language\
  \ models' data-flow reasoning abilities over procedural texts. The benchmark adapts\
  \ eight classical software engineering analyses\u2014such as reaching definitions,\
  \ live variable, and taint analysis\u2014into natural language tasks across three\
  \ real-world domains: cooking recipes, travel routes, and automated plans."
---

# FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation

## Quick Facts
- arXiv ID: 2505.24258
- Source URL: https://arxiv.org/abs/2505.24258
- Reference count: 40
- 3 models evaluated: deepseek-r1:8b (80-84% accuracy, 20-37× slower), llama3.1:8b, granite-code:8b (both near random chance)

## Executive Summary
FABLE introduces a novel benchmark for evaluating large language models' data-flow reasoning abilities over procedural texts. The benchmark adapts eight classical software engineering analyses—such as reaching definitions, live variable, and taint analysis—into natural language tasks across three real-world domains: cooking recipes, travel routes, and automated plans. It comprises 2,400 question-answer pairs with 100 examples per domain-analysis combination. Evaluation of three 8-billion-parameter models shows that the reasoning-focused deepseek-r1:8b achieves 80–84% accuracy but is over 20 times slower than the other models. In contrast, general-purpose (llama3.1:8b) and code-specific (granite-code:8b) models perform near random chance. These results highlight both the challenges of data-flow reasoning and the computational trade-offs in current LLMs, establishing FABLE as a diagnostic tool for developing models with stronger procedural understanding.

## Method Summary
FABLE adapts eight classical data-flow analyses from software engineering to evaluate procedural reasoning in natural language. The benchmark uses three real-world domains—cooking recipes, travel routes, and automated plans—to generate 2,400 QA pairs through template-based question generation from formally constructed graphs. For each procedural text, dual graphs are constructed: a Step-Dependency Graph (GS) encoding ordering constraints and an Entity-Flow Graph (GE) capturing entity creation, modification, and consumption. The methodology employs a pipeline of domain-specific parsers, procedural text generation, template-based question generation, and evaluation using majority voting over multiple completions. Three 8-billion-parameter models are evaluated: deepseek-r1:8b (reasoning-focused), llama3.1:8b (general-purpose), and granite-code:8b (code-specific), with inference latency and accuracy measured across all combinations.

## Key Results
- DeepSeek-R1 achieves 80-84% accuracy on FABLE but is 20-37× slower than general-purpose models
- LLaMA3.1 and granite-code perform near random chance (~50%) on binary questions
- Inference times: DeepSeek-R1 (600-1100s/100 prompts) vs LLaMA3.1 (38.3s/100 prompts) on Plans
- Standard deviation across analyses shows DeepSeek-R1 has uneven performance (SD ~25%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Software engineering data-flow analyses can be systematically adapted to evaluate procedural reasoning in natural language.
- Mechanism: Eight classical analyses (reaching definitions, live variables, taint analysis, etc.) map to tracking entity state changes, causal dependencies, and temporal constraints across procedural steps. The benchmark constructs Step-Dependency Graphs (GS) encoding ordering constraints and Entity-Flow Graphs (GE) capturing entity creation, modification, and consumption.
- Core assumption: Procedural text exhibits formal structure analogous to program control flow, enabling transfer of verification techniques.
- Evidence anchors:
  - [abstract] "FABLE adapts eight classical data-flow analyses from software engineering: reaching definitions, very busy expressions, available expressions, live variable analysis, interval analysis, type-state analysis, taint analysis, and concurrency analysis."
  - [section 3.2] "For each input, the parser produces two outputs: a Step-Dependency Graph GS = (S, ES)... and an Entity-Flow Graph GE = (V, EE)."
  - [corpus] Weak direct corpus support—neighbors focus on multimodal and game-based reasoning rather than SE-to-NL transfer.
- Break condition: If procedural texts lack sufficient formal structure (e.g., highly ambiguous or incomplete instructions), graph construction becomes unreliable.

### Mechanism 2
- Claim: Template-based question generation from formal graphs yields diagnostic, ground-truth QA pairs without manual annotation.
- Mechanism: Given GS and GE, canonical templates map graph properties to natural language questions with formally determined answers. For binary analyses, questions are yes/no queries; interval analysis requires numeric range extraction.
- Core assumption: Template-based generation preserves semantic validity and diagnostic value across domains.
- Evidence anchors:
  - [section 3.4] "We employ a template-based generation approach to create question instances. For each data-flow analysis, we define canonical templates that map properties of GS and GE to corresponding natural language questions and formally determined answers."
  - [section 3.4] "This allows for scalable and consistent generation across domains and procedures without requiring manual annotation."
  - [corpus] No corpus support—neighbor papers use human annotation or synthetic generation differently.
- Break condition: Templates may fail when domain-specific language deviates significantly from canonical patterns (e.g., recipes with implicit steps).

### Mechanism 3
- Claim: Reasoning-focused LLM architectures trade inference latency for accuracy on compositional procedural tasks.
- Mechanism: DeepSeek-R1 achieves 80-84% accuracy but incurs 20-37× slowdown. General-purpose and code-specific models perform near random chance (~50%), suggesting they rely on surface pattern matching rather than explicit state tracking.
- Core assumption: The reasoning model's extended computation implements explicit chain-of-thought or simulation over procedural states.
- Evidence anchors:
  - [abstract] "Results show that the reasoning model achieves higher accuracy, but at the cost of over 20 times slower inference compared to the other models."
  - [section 4, Table 5] DeepSeek-R1 averages 1136.3s per 100 prompts vs. 38.3s for LLaMA3.1 on Plans (29.6× slowdown).
  - [corpus] Neighbor "The Illusion of Procedural Reasoning" (arxiv:2511.14777) corroborates LLM struggles with long-horizon procedural execution, suggesting pattern matching limitations.
- Break condition: If latency constraints preclude extended reasoning (real-time systems), the accuracy advantage becomes infeasible.

## Foundational Learning

- Concept: **Data-flow analysis fundamentals**
  - Why needed here: The benchmark maps 8 classical SE analyses to NL; understanding forward/backward analyses (reaching definitions, live variables) is prerequisite to interpreting results.
  - Quick check question: Given a 3-step procedure where Step 1 defines variable X, Step 2 redefines X, and Step 3 uses X—what is the reaching definition at Step 3?

- Concept: **Procedural text representation**
  - Why needed here: FABLE uses dual graphs (step-dependency + entity-flow); understanding how to extract these from unstructured text is core to the pipeline.
  - Quick check question: For "Heat butter. Add onions. Sauté until golden," identify entities, actions, and implicit temporal dependencies.

- Concept: **Evaluation metrics for reasoning benchmarks**
  - Why needed here: The paper uses majority voting over 5 completions, accuracy vs. random baseline, and latency trade-offs.
  - Quick check question: Why might majority voting over completions underestimate model capability compared to single-pass evaluation?

## Architecture Onboarding

- Component map: Domain-specific parsers -> Graph construction (GS + GE) -> Procedural text generation -> Template-based question generation -> Model inference -> Aggregated evaluation
- Critical path: Parser → Graph construction → Template instantiation → QA generation → Model inference → Aggregated evaluation. Parser accuracy directly determines graph validity; template design controls question diagnosticity.
- Design tradeoffs:
  - Binary vs. open-ended QA: Binary questions (7/8 analyses) enable scalable evaluation but may miss partial reasoning. Interval analysis requires numeric extraction, adding complexity.
  - Domain coverage vs. scalability: Three domains with 800 QA pairs each; expanding domains requires custom parsers and entity vocabularies.
  - Model size constraints: All tested models are 8B parameters for fair comparison; larger models unexplored.
- Failure signatures:
  - Near-random performance (general/code models): Indicates reliance on surface patterns; suggest models lack explicit state simulation.
  - High variance across analyses (DeepSeek-R1 SD ~25%): Suggests uneven capability—strong on state tracking, weak on numeric/temporal inference (Interval Analysis: 31-39%).
  - Domain-specific biases: Travel routes yield deterministic answers for certain analyses (e.g., Available Expressions always "Yes") due to sequential structure.
- First 3 experiments:
  1. Replicate baseline evaluation on FABLE HuggingFace dataset with same 3 models and majority-voting protocol. Verify accuracy and latency claims match Table 6.
  2. Ablation by analysis type: Separate accuracy by analysis category (state vs. causal vs. temporal) to isolate which reasoning types drive DeepSeek-R1 advantage.
  3. Prompt variation study: Test chain-of-thought prompting on LLaMA3.1 to determine if explicit reasoning scaffolds close the performance gap with DeepSeek-R1, or if architectural differences persist.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompting strategies (e.g., chain-of-thought, few-shot) significantly improve general-purpose and code-specific model performance on FABLE, or is the data-flow reasoning failure more fundamental?
- Basis in paper: [explicit] Appendix A.1 states: "model evaluation is limited to a fixed prompting style using template-based instructions. Variations in prompt phrasing, such as chain-of-thought or few-shot prompting, could significantly impact model performance and reveal hidden capabilities or weaknesses."
- Why unresolved: The study deliberately used a single prompting approach to establish baselines; no prompting ablations were conducted.
- What evidence would resolve it: Systematic evaluation of llama3.1:8b and granite-code:8b using chain-of-thought, few-shot, and structured reasoning prompts on FABLE.

### Open Question 2
- Question: Can the accuracy-efficiency tradeoff observed with reasoning-focused models (80–84% accuracy but 20× slower) be bridged through model architecture innovations or distillation?
- Basis in paper: [explicit] The paper states: "This pronounced disparity highlights a critical trade-off between model performance and deployment efficiency, particularly relevant for real-time or resource-constrained applications."
- Why unresolved: No architectural solutions or distillation experiments were explored; the tradeoff is documented but not addressed.
- What evidence would resolve it: Demonstrating a model that achieves >75% accuracy on FABLE with inference latency comparable to llama3.1:8b.

### Open Question 3
- Question: Why do code-specific models like granite-code:8b fail to transfer their data-flow reasoning capabilities from code to procedural text?
- Basis in paper: [inferred] Despite granite-code:8b being pre-trained on code where data-flow analysis is fundamental, it performs near random chance on FABLE—only marginally better than general-purpose models.
- Why unresolved: The paper documents the failure but does not investigate whether the issue is the natural language framing, domain shift, or lack of procedural text in pre-training.
- What evidence would resolve it: Analysis probing whether granite-code:8b succeeds when FABLE questions are reformulated in pseudo-code, or fine-tuning experiments on procedural text.

### Open Question 4
- Question: Will data-flow reasoning capabilities generalize to domains beyond recipes, travel routes, and automated plans—such as medical protocols or scientific procedures?
- Basis in paper: [explicit] Appendix A.1 states: "we plan to incorporate additional domains such as conversational dialogs, design diagrams, and scientific protocols, which would allow for a more comprehensive assessment of procedural reasoning."
- Why unresolved: Current benchmark covers three domains; cross-domain generalization remains untested.
- What evidence would resolve it: Extending FABLE to new domains and evaluating whether model rankings and failure patterns persist.

## Limitations
- Domain Generalization: Benchmark relies on three specific procedural domains, limiting generalizability to other real-world procedural texts.
- Human-Annotated Ground Truth: Ground truth answers are template-generated rather than human-verified, introducing potential systematic errors.
- Evaluation Protocol: Majority voting may underestimate model capability by smoothing out occasional correct reasoning paths.

## Confidence
- High Confidence: The adaptation of classical software engineering data-flow analyses to natural language procedural text is technically sound and well-documented.
- Medium Confidence: The claim that reasoning-focused architectures trade latency for accuracy is supported by experimental results but underlying mechanism remains unverified.
- Low Confidence: The assertion that general-purpose and code-specific models perform "near random chance" may be overstated given the binary question format.

## Next Checks
1. **Parser Accuracy Validation**: Conduct human evaluation of parser outputs across all three domains to measure accuracy of GS and GE graph construction, including inter-annotator agreement scores and error analysis.
2. **Prompt Engineering Impact**: Test whether chain-of-thought prompting or structured reasoning scaffolds can close the performance gap between general-purpose models and deepseek-r1 on FABLE tasks.
3. **Scaling Analysis**: Evaluate larger model variants (e.g., 70B parameters) on FABLE to determine if the latency-accuracy trade-off scales predictably and establish whether reasoning advantage is architecture-specific or parameter-dependent.