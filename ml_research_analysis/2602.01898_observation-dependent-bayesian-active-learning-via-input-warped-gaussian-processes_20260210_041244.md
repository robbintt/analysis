---
ver: rpa2
title: Observation-dependent Bayesian active learning via input-warped Gaussian processes
arxiv_id: '2602.01898'
source_url: https://arxiv.org/abs/2602.01898
tags:
- learning
- input
- gaussian
- warp
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses a limitation in standard Gaussian process-based\
  \ Bayesian active learning: the posterior variance\u2014and thus variance-based\
  \ acquisition functions\u2014depends only on hyperparameters and input locations,\
  \ not on observed function values. This decouples exploration from the actual complexity\
  \ of the target function."
---

# Observation-dependent Bayesian active learning via input-warped Gaussian processes

## Quick Facts
- arXiv ID: 2602.01898
- Source URL: https://arxiv.org/abs/2602.01898
- Authors: Sanna Jarl; Maria Bånkestad; Jonathan J. S. Scragg; Jens Sjölund
- Reference count: 14
- Primary result: Learning input warps for the acquisition function (not the GP) improves sample efficiency by making variance-based exploration sensitive to observed function complexity.

## Executive Summary
Standard Gaussian process active learning decouples exploration from function complexity because posterior variance depends only on hyperparameters and inputs, not observed values. This work proposes learning an input reparameterization—a monotone warp—used exclusively within the acquisition function to restore observation-dependent feedback. The warp reshapes input space to redistribute uncertainty based on observed function behavior, improving exploration in non-stationary domains while keeping the underlying GP unchanged.

The approach uses conditional rational quadratic splines to parameterize the warp, trained via a self-supervised geometric objective that aligns the warped input distribution with the GP posterior. Experiments on synthetic and photoluminescence benchmarks show improved sample efficiency over both standard GPs and likelihood-trained warps, especially when function complexity varies across the domain.

## Method Summary
The method learns a monotone input warp used only in the acquisition function to make variance-based exploration sensitive to observed function values. The warp is parameterized using conditional rational quadratic splines and trained with a self-supervised geometric objective that redistributes uncertainty. This contrasts with standard GPs, where acquisition variance is input-location-dependent only. By separating the warp (for exploration) from the GP (for prediction), the method improves sample efficiency without requiring more expressive surrogate models.

## Key Results
- Input warping applied only in the acquisition function improves sample efficiency over standard GPs and likelihood-trained warps.
- The geometric objective successfully redistributes uncertainty in response to observed function complexity.
- Gains are most pronounced on non-stationary benchmarks (Gramacy-Lee08, Peaks, Box, Hartmann) and photoluminescence datasets.

## Why This Works (Mechanism)
By learning an input warp for the acquisition function alone, the method decouples the geometry of exploration from the GP's posterior. This allows variance-based acquisition to be sensitive to observed function complexity, restoring the coupling between exploration and the true function structure that standard GP-based active learning loses.

## Foundational Learning
- Gaussian processes: probabilistic surrogate models for Bayesian optimization; needed to understand posterior variance and active learning frameworks.
  - Quick check: GP mean and variance formulas depend only on hyperparameters and inputs, not observed function values.
- Active learning / Bayesian optimization: sequential design of experiments to minimize function evaluations; needed to contextualize the acquisition function's role.
  - Quick check: Acquisition functions balance exploration (high variance) and exploitation (high mean).
- Monotone input warping: reparameterization of inputs to reshape uncertainty; needed to enable observation-dependent exploration.
  - Quick check: Warps must be strictly increasing to preserve function ordering.
- Rational quadratic splines: flexible, invertible mappings for warping; needed for smooth, learnable input transformations.
  - Quick check: Splines provide piecewise polynomial flexibility while maintaining monotonicity.

## Architecture Onboarding

**Component map**: Input space -> Rational Quadratic Spline Warp -> Warped acquisition function -> Acquisition function optimization -> Next query point

**Critical path**: The warp is applied only to inputs in the acquisition function; the GP posterior remains unchanged. This ensures exploration geometry adapts to observations while preserving prediction fidelity.

**Design tradeoffs**: Separating warp learning from GP inference simplifies training but may introduce a mismatch if the learned geometry poorly reflects the true function. Using splines offers smoothness and monotonicity but may limit expressiveness in highly non-stationary or discontinuous domains.

**Failure signatures**: If the warp overfits to early observations, exploration may become too localized. If the warp is too smooth, it may fail to capture sharp changes in function complexity.

**Three first experiments**: (1) Compare acquisition performance on Gramacy-Lee08 with and without input warping; (2) Visualize the learned warp as a function of observation count; (3) Test sensitivity of the geometric objective to warp initialization.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation is limited to low-dimensional benchmarks; performance in high-dimensional or noisy settings is untested.
- No direct comparisons to non-GP alternatives like Bayesian neural networks or Tree-structured Parzen estimators.
- The impact of the geometric objective on long-term exploration-exploitation trade-offs is not quantified.

## Confidence
- Problem framing and method description: High
- Ablation study conclusions: Medium
- Robustness in high-dimensional or noisy settings: Low

## Next Checks
1. Extend experiments to higher-dimensional problems (d ≥ 10) with realistic noise levels.
2. Compare against acquisition functions using non-stationary kernels or input-space neural networks.
3. Analyze the sensitivity of the geometric objective to initialization and its effect on exploration in multi-modal landscapes.