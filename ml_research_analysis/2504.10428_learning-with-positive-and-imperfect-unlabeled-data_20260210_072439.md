---
ver: rpa2
title: Learning with Positive and Imperfect Unlabeled Data
arxiv_id: '2504.10428'
source_url: https://arxiv.org/abs/2504.10428
tags:
- learning
- samples
- algorithm
- theorem
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Positive and Imperfect Unlabeled (PIU)
  learning model, a generalization of Positive and Unlabeled (PU) learning that allows
  for distribution shifts between positive and unlabeled data. The key insight is
  that PIU learning provides a unified framework that underlies several fundamental
  problems in statistics and learning theory, including learning from smooth distributions,
  truncated statistics, and truncation detection.
---

# Learning with Positive and Imperfect Unlabeled Data

## Quick Facts
- arXiv ID: 2504.10428
- Source URL: https://arxiv.org/abs/2504.10428
- Reference count: 40
- Primary result: Introduces PIU learning model with sample complexity bounded by VC dimension and smoothness parameters

## Executive Summary
This paper introduces Positive and Imperfect Unlabeled (PIU) learning, a generalization of Positive and Unlabeled (PU) learning that handles distribution shifts between positive and unlabeled data. The key insight is that PIU learning provides a unified framework that characterizes several fundamental problems in statistics and learning theory, including learning from smooth distributions, truncated statistics, and truncation detection. The authors establish sample complexity bounds that scale with VC dimension and smoothness parameters, and develop computationally efficient algorithms that require only L1-approximation of hypotheses with respect to the unlabeled distribution.

## Method Summary
The paper presents an iterative Pessimistic-ERM algorithm that solves T=1/ε constrained learning problems to find hypotheses that minimize unlabeled mass while covering all positive samples. The final classifier is the intersection of these T hypotheses. For computational efficiency, they reduce the problem to constrained L1-regression by approximating hypotheses with low-degree polynomials, solving a linear program that minimizes L1-norm on unlabeled samples subject to fitting positive samples within tolerance ρ. This approach improves upon previous work requiring L2-approximation and provides the first efficient algorithms for truncated estimation with unknown survival sets.

## Key Results
- Characterizes sample complexity of PIU learning as Õ((VC(H)/ε²)·σ^(-2q)) under smoothness assumption
- Develops first computationally efficient PIU learning algorithm using constrained L1-regression instead of L2-approximation
- Provides new algorithms for truncated estimation and truncation detection problems
- Shows proper learning is impossible for PIU, requiring improper learners (intersections of hypotheses)
- Establishes equivalence between PIU learning and several fundamental statistical problems

## Why This Works (Mechanism)

### Mechanism 1: Bounding Shift via Generalized Smoothness
The framework assumes D*(S) ≤ (1/σ)·D(S)^(1/q) for any measurable set S, allowing error under true distribution D* to be upper-bounded using only samples from imperfect distribution D.

### Mechanism 2: Iterative Pessimistic-ERM
Standard ERM fails; instead iterative constrained optimization finds hypotheses minimizing unlabeled mass while covering positive samples, with final hypothesis as intersection of T=1/ε solutions.

### Mechanism 3: Constrained L1-Regression
Reduces PIU to constrained L1-regression solvable via LP by approximating hypotheses with low-degree polynomials, requiring only L1-approximability rather than stronger L2 conditions.

## Foundational Learning

- **VC Dimension & Sample Complexity**: Needed to understand how sample complexity scales with VC dimension and smoothness parameter q. Quick check: How does sample complexity in Theorem 1.1 scale with VC dimension and q?

- **Proper vs. Improper Learning**: Key result shows proper learning is impossible for PIU; algorithms output intersections of hypotheses (improper learners). Quick check: Why does algorithm output H = H₁ ∩ ... ∩ H_T instead of single H ∈ H?

- **Polynomial Threshold Functions**: Computational algorithm relies on approximating concepts with low-degree polynomials. Understanding degree vs. approximation error trade-off is crucial for runtime analysis. Quick check: What is computational cost if polynomial degree k scales exponentially with 1/ε?

## Architecture Onboarding

- **Component map**: Sampler -> Constraint Generator -> Solver (Boosted-Constrained-Regression) -> Aggregator
- **Critical path**: 1) Verify Assumption 1 holds (or estimate σ, q), 2) Initialize survival set S = ℝᵈ, 3) Iterate T=1/ε times: sample P,U, filter U by S, solve constrained LP, update S ← S ∩ Hᵢ, 4) Output final intersection
- **Design tradeoffs**: L1 vs. L2 approximation trades generality for efficiency; improper vs. proper outputs complex intersections but necessary for correctness
- **Failure signatures**: Empty intersection (rejects everything), LP infeasibility, slow convergence with high T or exploding LP size
- **First 3 experiments**: 1) Toy validation on 1D data with Uniform D and truncated Gaussian D*, 2) Vary polynomial degree k on halfspaces under non-Gaussian D, 3) Stress test smoothness by violating Assumption 1

## Open Questions the Paper Calls Out

### Open Question 1
Is there a black-box reduction from efficient agnostic learning to efficient PIU learning? Unresolved because current algorithm requires new analysis rather than using existing agnostic learners as black box.

### Open Question 2
Is poly(d/ε) time achievable for PIU learning with intersections of O(1) halfspaces or single halfspace under Gaussian distributions? Current SQ algorithm has exponential dependence on 1/ε.

### Open Question 3
What is optimal sample complexity of PIU learning under smoothness with q=1? Gap exists between upper bound Õ(VC(H)/ε²) and lower bound Ω(VC(H)/ε).

### Open Question 4
What is characterization of universal rates for binary classification in PU and PIU learning models? Standard uniform rates model allows distributions to vary with ε, but fixed distributions remain unexplored.

## Limitations
- Relies critically on smoothness assumption which may not hold when density ratio has heavy tails
- Sample complexity scales poorly with q, making approach impractical for highly non-smooth distributions
- Computational efficiency hinges on polynomial L1-approximation which may require high degrees for complex hypothesis classes

## Confidence

- Sample complexity bounds and theoretical guarantees: **High** - Follow from standard VC dimension arguments combined with smoothness condition
- Computational efficiency claims: **Medium** - Relies on polynomial approximation which may not be practical for high-dimensional problems
- Generalization to list-decoding model: **Low** - Extension mentioned but not developed in detail

## Next Checks

1. **Smoothness Violation Test**: Systematically evaluate algorithm performance as smoothness condition is violated by increasing density ratio in specific regions

2. **Approximation Degree Experiment**: Measure actual polynomial degree required to achieve desired L1-approximation error for common hypothesis classes (halfspaces, rectangles) under different distributions

3. **Intersection Stability Analysis**: Track how mass of intersection hypothesis evolves across iterations and identify when algorithm becomes overly pessimistic