---
ver: rpa2
title: Ranking-based Fusion Algorithms for Extreme Multi-label Text Classification
  (XMTC)
arxiv_id: '2507.03761'
source_url: https://arxiv.org/abs/2507.03761
tags:
- norm
- rank
- zmuv
- borda
- min-max
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of Extreme Multi-label Text Classification
  (XMTC) where the long-tail distribution of labels makes it challenging to balance
  effectiveness across both frequent head labels and infrequent tail labels. The core
  idea is to combine predictions from sparse retrievers (e.g., BM25) and dense retrievers
  (e.g., fine-tuned BERT) using rank-based fusion algorithms.
---

# Ranking-based Fusion Algorithms for Extreme Multi-label Text Classification (XMTC)

## Quick Facts
- arXiv ID: 2507.03761
- Source URL: https://arxiv.org/abs/2507.03761
- Reference count: 23
- Primary result: CombMNZ fusion with ZMUV normalization achieves highest nDCG@k and Precision@k across four XMTC benchmark datasets for both head and tail labels

## Executive Summary
This paper addresses the challenge of Extreme Multi-label Text Classification (XMTC) where label distributions follow a long-tail pattern. The authors propose a rank-based fusion approach that combines predictions from sparse retrievers (BM25) and dense retrievers (fine-tuned BERT) to improve effectiveness across both frequent head labels and infrequent tail labels. By systematically evaluating six normalization strategies with ten fusion algorithms across four benchmark datasets, the study identifies CombMNZ with ZMUV normalization as the optimal combination, achieving significant improvements over individual retriever baselines.

## Method Summary
The approach combines sparse retrieval using BM25 (with b=0.75, k=1.5) and dense retrieval using BERT fine-tuned with Normalized Temperature-Scaled Cross Entropy Loss. For each test instance, the system retrieves 64 head and 64 tail candidate labels from each retriever (128 total). Six normalization strategies (none, MinMax, Max, Sum, ZMUV, Borda Count) are applied to the rankings before fusion. Ten fusion algorithms (Borda Count, CombSUM, CombMNZ, Reciprocal Rank, Reciprocal Rank Fusion, Interpolated KNRM, Interpolated BERT, Interpolated MaxP, Sum Normalized, Product Normalized) combine the normalized scores. The best configuration (CombMNZ + ZMUV) achieved the highest effectiveness across all datasets.

## Key Results
- CombMNZ with ZMUV normalization consistently outperformed all other fusion configurations across all datasets
- The fusion approach significantly improved nDCG@k and Precision@k for both head and tail labels compared to individual retrievers
- ZMUV normalization showed particular effectiveness in stabilizing fusion performance across different datasets
- Borda Count normalization produced near-zero scores when used with rank-based or voting fusion algorithms

## Why This Works (Mechanism)
The paper exploits the complementary strengths of sparse and dense retrievers: BM25 captures exact lexical matches while fine-tuned BERT captures semantic relationships. By fusing their rankings, the system leverages both lexical precision and semantic coverage. The normalization strategies address score scale differences between retrievers, while fusion algorithms aggregate evidence from multiple sources to improve overall ranking quality.

## Foundational Learning
- **Pareto principle for label splitting**: Needed to define head vs tail labels in long-tail distributions. Quick check: Verify 80% of labels are in tail by frequency count.
- **Normalized Temperature-Scaled Cross Entropy Loss (NT-Xent)**: Needed for contrastive learning in BERT fine-tuning. Quick check: Ensure temperature parameter is properly tuned for dataset size.
- **Z-score normalization (ZMUV)**: Needed to standardize score distributions across different retrievers. Quick check: Confirm mean ≈ 0 and std ≈ 1 after normalization.

## Architecture Onboarding

**Component Map**: Text -> BM25 Index -> Sparse Scores; Text -> BERT Embeddings -> Dense Scores -> ANN Index -> Dense Scores; Both Scores -> Normalization -> Fusion -> Final Ranking

**Critical Path**: Input text → Retrieval from both indices → Normalization → Fusion → Output ranking

**Design Tradeoffs**: The dual-retrieval approach trades increased storage and inference time for improved effectiveness across head and tail labels. Alternative single-model approaches might be more efficient but sacrifice coverage.

**Failure Signatures**: Score scale mismatch between retrievers leads to dominance of one source in final ranking. Borda Count normalization incompatible with certain fusion algorithms. Tail labels underrepresented if retrieval lists are truncated too aggressively.

**First Experiments**:
1. Verify ZMUV normalization produces comparable distributions (mean≈0, std≈1) per ranking list before fusion
2. Test Borda Count normalization with voting-based fusion algorithms to confirm incompatibility
3. Experiment with different temperature values in NT-Xent loss to assess impact on dense retrieval quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the CombMNZ fusion strategy change when substituting BM25 or fine-tuned BERT with more advanced learned sparse retrievers (e.g., SPLADE) or late-interaction dense models (e.g., ColBERT)?
- Basis in paper: The study restricts the "sparse" retriever to BM25 and the "dense" retriever to BERT, acknowledging they utilize "bag-of-words" and "approximate nearest neighbor" approaches respectively, without exploring newer neural retrieval architectures.
- Why unresolved: The observed benefits rely on the specific complementary nature of BM25 (lexical) and BERT (semantic). Advanced models might exhibit different overlap behaviors or score distributions, potentially rendering the ZMUV normalization less effective.
- What evidence would resolve it: Comparative experiments measuring nDCG@k and Precision@k on the same datasets using SPLADE or ColBERT in the fusion pipeline.

### Open Question 2
- Question: Is the arbitrary candidate cutoff of 64 head and 64 tail labels optimal for fusion performance, or does varying the candidate list size ($k$) alter the ranking of fusion algorithms?
- Basis in paper: Section 4 explicitly states, "we set the number of candidates to be 64 tail labels and 64 head labels," but provides no justification or ablation study regarding this specific limit.
- Why unresolved: Fusion algorithms like CombMNZ rely on the overlap of results; truncating the list at 128 might artificially inflate consensus scores or discard relevant labels that appear lower in the individual rankings.
- What evidence would resolve it: An ablation study evaluating the fusion performance across varying candidate list sizes (e.g., 10, 50, 100, 500) to determine sensitivity.

### Open Question 3
- Question: What are the computational latency and memory overheads of maintaining dual retrieval indices (sparse and dense) compared to single-model XMTC approaches?
- Basis in paper: The paper focuses entirely on effectiveness metrics (nDCG, Precision) while addressing the "Extreme" nature of the task, which typically implies a requirement for high efficiency and scalability that is not discussed.
- Why unresolved: Deploying two distinct retrieval systems plus a fusion layer introduces significant inference latency and storage costs (indexing for BM25 and embeddings for BERT), which may be prohibitive for real-world applications.
- What evidence would resolve it: Reporting inference time per query and storage requirements for the Eurlex-4k and Amazon-670k datasets alongside the effectiveness scores.

### Open Question 4
- Question: Does the ZMUV normalization strategy remain the most robust choice when the definition of "head" versus "tail" labels is shifted away from the Pareto principle (80/20)?
- Basis in paper: The paper defines head and tail labels using a fixed threshold based on the Pareto principle, but performance on "tail" labels is the primary motivation for the work.
- Why unresolved: The distribution of scores in the tail may change if the threshold is adjusted (e.g., 90/10 or 70/30), potentially affecting the mean/variance calculations in ZMUV normalization and favoring a different fusion method.
- What evidence would resolve it: A sensitivity analysis measuring algorithm performance on tail labels using different frequency thresholds for categorization.

## Limitations
- BERT model variant and architecture details not specified
- Label text preprocessing and representation method unclear
- Temperature parameter for NT-Xent loss not reported
- ANN algorithm choice for dense retrieval not documented

## Confidence
- High confidence in the general methodology and experimental design
- Medium confidence in specific implementation details (BERT architecture, ANN parameters)
- High confidence in the conclusion that fusion improves both head and tail label performance

## Next Checks
1. Verify ZMUV normalization produces comparable score distributions between BM25 and BERT rankings before fusion
2. Test Borda Count normalization with voting-based fusion algorithms to confirm incompatibility
3. Experiment with different temperature values in NT-Xent loss to assess impact on dense retrieval quality