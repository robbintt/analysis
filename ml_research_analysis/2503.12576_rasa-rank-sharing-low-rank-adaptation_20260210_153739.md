---
ver: rpa2
title: 'RaSA: Rank-Sharing Low-Rank Adaptation'
arxiv_id: '2503.12576'
source_url: https://arxiv.org/abs/2503.12576
tags:
- rasa
- lora
- training
- rank
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rank-Sharing Low-Rank Adaptation (RaSA) is proposed to overcome
  the limited expressive capacity of LoRA in large language model fine-tuning by introducing
  partial rank sharing across layers. Instead of learning independent low-rank updates
  per layer, RaSA extracts a subset of ranks from each layer to form a shared rank
  pool, with layer-specific weighting.
---

# RaSA: Rank-Sharing Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2503.12576
- Source URL: https://arxiv.org/abs/2503.12576
- Reference count: 17
- RaSA outperforms LoRA on code generation and mathematical reasoning while maintaining parameter efficiency

## Executive Summary
Rank-Sharing Low-Rank Adaptation (RaSA) addresses the limited expressive capacity of traditional LoRA by introducing partial rank sharing across transformer layers. Rather than learning independent low-rank updates for each layer, RaSA extracts a subset of ranks from each layer to form a shared rank pool, with layer-specific weighting. This approach theoretically guarantees reconstruction error bounded above by LoRA while achieving superior empirical performance. The method maintains LoRA's parameter efficiency and can be easily merged back into the base model, demonstrating effectiveness across model scales including 70B and Mixtral-8×7B.

## Method Summary
RaSA modifies the standard LoRA approach by creating a shared rank pool from which each layer draws a subset of ranks. Instead of each layer having its own independent low-rank update matrices, RaSA computes layer-specific weighting coefficients for the shared ranks. This allows the model to leverage cross-layer information while maintaining the computational efficiency of low-rank adaptation. The theoretical foundation shows that RaSA's minimum reconstruction error is bounded above by LoRA's for the same parameter count. During fine-tuning, the shared ranks are learned collectively while each layer applies its own weighting, enabling more expressive capacity without proportional parameter increase.

## Key Results
- On Llama-3.1-8B with rank 32, RaSA achieves 59.5% Pass@1 on code generation (HumanEval+) versus LoRA's lower performance
- For mathematical reasoning on MATH benchmark, RaSA reaches 31.7% accuracy while showing less catastrophic forgetting
- RaSA demonstrates faster convergence and improved data efficiency compared to baseline PEFT methods across multiple model scales

## Why This Works (Mechanism)
RaSA works by enabling information sharing across transformer layers through a common rank pool. Traditional LoRA treats each layer independently, which limits the model's ability to capture cross-layer dependencies and global patterns. By sharing ranks across layers with layer-specific weighting, RaSA allows the model to learn more expressive transformations while maintaining the parameter efficiency of low-rank methods. The layer-specific weighting ensures that each layer can still specialize its adaptation while benefiting from the shared representational capacity. This design effectively increases the model's expressive power without the computational overhead of full fine-tuning or the parameter explosion of higher-rank LoRA variants.

## Foundational Learning

**Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that approximates weight updates using low-rank matrices. Why needed: Enables efficient LLM adaptation without full parameter updates. Quick check: Verify that the rank decomposition reduces parameters by roughly rank²/(d_in × d_out).

**Transformer Layer Adaptation**: Modifying self-attention and feed-forward network weights during fine-tuning. Why needed: Core mechanism for adapting LLMs to downstream tasks. Quick check: Confirm that only attention Q/K/V and MLP weights are adapted in LoRA-style methods.

**Singular Value Decomposition (SVD)**: Matrix factorization technique used to analyze rank properties. Why needed: Provides theoretical foundation for low-rank approximation guarantees. Quick check: Ensure SVD decomposition correctly identifies principal components for rank selection.

**Catastrophic Forgetting**: Degradation of base model capabilities during fine-tuning. Why needed: Critical evaluation metric for adaptation methods. Quick check: Measure performance drop on base model tasks after adaptation.

## Architecture Onboarding

**Component Map**: Input → Shared Rank Pool ← Layer-Specific Weighting → Each Transformer Layer → Output

**Critical Path**: The forward pass computes layer outputs using base weights plus weighted shared ranks, with gradients flowing through both the shared ranks and weighting coefficients during backpropagation.

**Design Tradeoffs**: RaSA trades increased inter-layer dependency and slightly more complex computation for improved expressive capacity and reduced forgetting. The shared rank pool increases memory access patterns but maintains parameter efficiency.

**Failure Signatures**: Poor performance may manifest as degraded base model capabilities (forgetting) or inability to specialize to target tasks if rank sharing is too aggressive or weighting is poorly initialized.

**First 3 Experiments**: 1) Verify rank sharing implementation by checking parameter count matches theoretical expectations, 2) Compare reconstruction error against LoRA baseline with identical parameters, 3) Test convergence speed on a small-scale task before full evaluation.

## Open Questions the Paper Calls Out

None specified in the source material.

## Limitations

- Theoretical guarantees rely on assumptions about matrix structure that may not hold in all practical settings
- Performance sensitivity to rank configuration requires careful hyperparameter tuning
- Limited evaluation scope focused primarily on code generation and mathematical reasoning tasks
- Computational overhead during training not thoroughly analyzed compared to baseline methods

## Confidence

- High Confidence: Empirical superiority over LoRA, MoRA, and VeRA on tested benchmarks (HumanEval+, MATH)
- Medium Confidence: Theoretical bound on reconstruction error is mathematically sound but practical implications need further validation
- Medium Confidence: Claims about reduced forgetting and faster convergence are supported but would benefit from more extensive ablation studies

## Next Checks

1. Conduct systematic ablation studies varying the proportion of shared vs. layer-specific ranks across different model scales and tasks
2. Evaluate RaSA on additional diverse benchmarks including reasoning tasks (BBH, AGIEval), multilingual tasks, and domain-specific datasets
3. Perform detailed profiling of training time, memory usage, and computational overhead compared to LoRA baseline