---
ver: rpa2
title: 'COMMA: A Communicative Multimodal Multi-Agent Benchmark'
arxiv_id: '2410.07553'
source_url: https://arxiv.org/abs/2410.07553
tags:
- solver
- agents
- press
- expert
- button
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces COMMA, a benchmark designed to evaluate\
  \ collaborative multimodal agent performance through language communication. The\
  \ benchmark simulates a scenario where two agents\u2014Solver and Expert\u2014must\
  \ work together to solve vision-language puzzles, with the Solver having access\
  \ to visual information and the Expert possessing manuals."
---

# COMMA: A Communicative Multimodal Multi-Agent Benchmark

## Quick Facts
- arXiv ID: 2410.07553
- Source URL: https://arxiv.org/abs/2410.07553
- Reference count: 36
- Primary result: o4-mini achieves 53.98% success rate vs human 69.01% on collaborative multimodal puzzles

## Executive Summary
This paper introduces COMMA, a benchmark for evaluating collaborative multimodal agent performance through language communication. The benchmark simulates asymmetric information scenarios where two agents—Solver (with visual access) and Expert (with manuals)—must work together to solve vision-language puzzles. The study evaluates state-of-the-art models across 10 puzzle types, revealing significant performance gaps between AI and human agents, particularly in maintaining role constraints and protecting private information during collaboration.

## Method Summary
COMMA is an evaluation-only benchmark with 10 puzzle types (100 initializations each = 1000 total instances). Solver agents receive task prompts plus images, while Expert agents receive manuals only. Agents alternate turns (max 10 turns, max 3 mistakes) using conversation history as episodic memory. Success is measured via success rate, partial success rate, efficiency score (harmonic mean of PSR and conciseness), average mistakes, and conversation length. The benchmark tests various cognitive capabilities including memory recall, multimodal grounding, and multi-step reasoning.

## Key Results
- Even best AI models significantly underperform humans (o4-mini 53.98% vs human 69.01% success rate)
- Chain-of-thought models like LLaVA-CoT and R1-OneVision perform worse than random baseline due to roleplay errors
- GPT-4o and Gemini 2.0 frequently reveal private information (PINs, diagnoses) despite explicit privacy instructions
- Episodic memory accumulation improves multi-step puzzle performance, with maze performance dropping from 33% to 3% when memory is removed

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Information Partitioning
- Claim: Deliberately splitting visual access from procedural knowledge creates task dependency that forces language-based coordination
- Mechanism: Solver cannot complete tasks without expert guidance (lacks rules), expert cannot act without solver descriptions (lacks visual context)
- Core assumption: Agents are motivated to complete tasks and will communicate when necessary information is unavailable
- Evidence: Section 3.1 describes Expert relying solely on communication with Solver, while abstract emphasizes scenarios requiring collaboration beyond individual capabilities

### Mechanism 2: Role-Constrained Dialogue Protocol
- Claim: Explicit role assignment with structured prompts shapes agent behavior toward complementary communication patterns
- Mechanism: Distinct system prompts specify responsibilities (solver: describe + act; expert: advise + clarify), reducing role confusion
- Core assumption: Agents follow prompt instructions and do not override role constraints during execution
- Evidence: Section 4.1 specifies exact prompts for both agents; Section 5.2 identifies roleplay errors accounting for 6-36% of errors depending on model

### Mechanism 3: Episodic Memory Accumulation
- Claim: Accumulating conversation history as context enables agents to learn from past mistakes within an episode
- Mechanism: Each turn includes full dialogue history, allowing agents to recognize repeated states and adjust behavior
- Core assumption: Models can attend to and retrieve relevant information from growing context windows
- Evidence: Section 5.3 shows removing memory drops GPT-4o overall performance from 41.74% to 36.20%, with largest declines on multi-step puzzles

## Foundational Learning

- Concept: **Asymmetric multi-agent coordination**
  - Why needed here: Benchmark explicitly tests collaboration under information asymmetry—standard single-agent evaluation does not transfer
  - Quick check question: Can you explain why a solver with perfect vision capabilities might fail without expert communication?

- Concept: **Episodic vs. working memory distinction**
  - Why needed here: Architecture separates transient visual input from accumulated dialogue history, with different ablation effects
  - Quick check question: Which memory type would be most affected by removing conversation history from agent inputs?

- Concept: **Privacy-preserving communication constraints**
  - Why needed here: Two puzzles explicitly test whether agents leak sensitive information despite instructions—not standard in most benchmarks
  - Quick check question: What specific mechanism would you add to detect or prevent private information disclosure in agent dialogue?

## Architecture Onboarding

- Component map: Task prompt + current image + conversation history → Solver Agent → Text description OR action command → Environment → Feedback → Expert Agent + manual + conversation history → Guidance text → Memory Buffer

- Critical path: 1) Initialize puzzle state → solver receives image 2) Solver describes or acts → message logged to memory 3) Expert receives description + manual → provides guidance 4) Loop until success, max turns (10), or max mistakes (3)

- Design tradeoffs:
  - Token usage vs. memory depth: Full history enables learning but increases cost (o4-mini has high performance but low efficiency score 0.15 due to long CoT)
  - Exact string matching vs. flexible parsing: Current design uses rigid action parsing—robust but fragile to model output variation
  - Shared vs. separate model instances: Paper uses same model for both roles; different model pairings unexplored

- Failure signatures:
  - Roleplay error: Expert describes visual elements it cannot see (expert acts as solver)
  - Miscommunication: Solver ignores expert instructions, acts independently
  - Repetition loop: Identical state-action pairs repeated despite negative feedback
  - Privacy leak: Sensitive data (PIN, diagnosis) transmitted in dialogue

- First 3 experiments:
  1. Reproduce GPT-4o baseline on 3 puzzles (Wire, Keypad, Memory) to validate environment integration before testing new models
  2. Ablate episodic memory by limiting context to last N turns; measure performance drop on multi-step puzzles
  3. Test cross-model pairings (e.g., GPT-4o solver + open-source expert) to isolate solver vs. expert capability contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning on collaborative dialogue tasks effectively adapt chain-of-thought reasoning models to utilize inter-agent communication rather than attempting independent problem-solving?
- Basis: Section 5.2 notes reasoning models "struggle to outperform even a random baseline" because they ignore the expert agent
- Why unresolved: Current reasoning models optimized for single-agent instruction following, causing domain shift that makes them disregard expert advice
- What evidence would resolve it: Demonstrating reasoning models fine-tuned on communicative datasets significantly outperform base versions by reducing "miscommunication" errors

### Open Question 2
- Question: To what extent does performance gap between AI and human agents change when evaluating against diverse, crowdsourced human baseline rather than authors' self-reported performance?
- Basis: Section 7 states authors' human baseline "creates possibility of unconscious bias," suggesting future work should involve "crowdsourcing diverse pool of participants"
- Why unresolved: Current human upper bound (69.01% success) may be over- or under-estimated due to authors' familiarity with task design
- What evidence would resolve it: Re-evaluating human baseline with statistically significant sample of non-author participants to validate efficiency and success rate metrics

### Open Question 3
- Question: Is it possible to enforce strict private information retention in high-performing proprietary models without significantly degrading task success rates?
- Basis: Section 5.3 highlights trade-off where high-performing models like GPT-4o frequently disclose private info while o4-mini maintains privacy but achieves lower success rates
- Why unresolved: Current SOTA models struggle to balance explicit privacy constraints with effective collaborative reasoning
- What evidence would resolve it: Identifying training method or architectural constraint that allows model to achieve high success rates on ATM/Telehealth puzzles with 0% information leakage

## Limitations
- Exact string matching for action parsing creates brittleness in reproduction
- Human baseline collected from authors rather than diverse crowdsourced participants
- Model configuration details (exact versions, temperature settings) are incompletely specified

## Confidence

**High Confidence**:
- Asymmetric information partitioning creates meaningful collaboration challenges
- Episodic memory accumulation improves multi-step puzzle performance
- Role-constrained dialogue protocols reduce roleplay errors when properly followed
- Privacy leakage occurs despite explicit instructions in 2/10 puzzles

**Medium Confidence**:
- AI models significantly underperform humans on collaborative tasks
- Chain-of-thought models perform worse than non-CoT models on this benchmark
- Communication quality deteriorates after 4-5 turns for AI agents

**Low Confidence**:
- Specific model rankings may reflect configuration differences rather than inherent capability
- Efficiency score comparisons difficult to validate without exact token usage data

## Next Checks
1. Run 10 identical puzzle initializations across different model instances to verify puzzle states are fully reproducible from seeds
2. Test exact string matching parser with minor variations in model output formatting to quantify evaluation pipeline brittleness
3. Evaluate mixed-model agent teams (e.g., GPT-4o solver + open-source expert) to isolate solver vs. expert capability contributions