---
ver: rpa2
title: Causal Structure and Representation Learning with Biomedical Applications
arxiv_id: '2511.04790'
source_url: https://arxiv.org/abs/2511.04790
tags:
- causal
- data
- learning
- variables
- interventions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of integrating causal structure
  learning with representation learning to enable prediction and control of complex
  biological systems using multi-modal data. The authors propose a statistical and
  computational framework that combines observational and interventional data across
  different modalities (imaging, sequencing, etc.) to learn causal variables and their
  relationships.
---

# Causal Structure and Representation Learning with Biomedical Applications

## Quick Facts
- arXiv ID: 2511.04790
- Source URL: https://arxiv.org/abs/2511.04790
- Authors: Caroline Uhler; Jiaqi Zhang
- Reference count: 40
- Integrates causal structure learning with representation learning for multi-modal biomedical data

## Executive Summary
This work addresses the fundamental challenge of learning causal structure and representations from complex biomedical data where causal variables are not directly observed but mixed through unknown transformations. The authors propose a unified framework that combines observational and interventional data across multiple modalities (imaging, sequencing, etc.) to identify causal variables and their relationships. Their approach bridges causal inference and representation learning, providing theoretical guarantees for identifiability under various assumptions and practical algorithms for biomedical applications including gene regulatory network inference from single-cell CRISPR screens.

## Method Summary
The framework consists of three main components: (1) causal discovery algorithms (GAS, GSP) that minimize conditional independence tests through targeted ancestral reasoning and greedy permutation search; (2) identifiability results showing when causal representations can be recovered under linear/nonlinear mixing, interventional faithfulness, and multi-modal overlap assumptions; and (3) methods for optimal experimental design to select informative interventions. The approach handles single-modality observational data, interventional data with unknown targets, and multi-modal datasets with shared latent variables. Algorithms include prefix-set-based adjacency search, intervention clustering by marginal distribution changes, and noise distribution matching across modalities.

## Key Results
- GAS algorithm provably reduces CI tests from PC's p^Ω(d) to p^O(s) where s ≤ d-1 is the maximum undirected clique size
- Interventional data enables identification of latent causal variables and their ancestral relationships under linear interventional faithfulness
- Multi-modal overlap enables identification of shared latent causal variables and causal graph structure under rank and noise asymmetry conditions
- Framework enables prediction of intervention effects and translation between data modalities

## Why This Works (Mechanism)

### Mechanism 1
Constraint-based causal discovery can achieve provably fewer conditional independence tests than PC algorithm while correctly identifying the Markov equivalence class. GAS integrates adjacency search and edge orientation by targeting ancestral relationships directly using two specific CI test patterns: v-structure detection via `Xi ⊥⊥ Xj |XS` and `Xi ⋆⊥ Xj |XS∪{k}`, and Meek Rule 1 detection via prefix sets. By maintaining expanding prefix sets, GAS avoids exhaustive conditioning set enumeration. Core assumption: λ-strong faithfulness in finite samples. Evidence: Theorem 7 states GAS outputs E(G) using at most p^O(s) CI tests where s ≤ d-1 is the maximum undirected clique size.

### Mechanism 2
Interventional data enables identification of latent causal variables and their ancestral relationships even when causal variables are not directly observed. Under polynomial mixing O = f(X) and linear interventional faithfulness, interventions with unknown targets can be clustered by detecting marginal distribution changes. Source nodes are identified first (only their own intervention changes their marginal), then iteratively removed to identify next layer, yielding transitive closure TS(G). Core assumption: Linear interventional faithfulness ensures intervention effects propagate without cancellation. Evidence: Theorem 21 guarantees TS(G) and intervention targets are identifiable up to permutation under Assumptions 19-20.

### Mechanism 3
Partially overlapping multi-modal data enables identification of shared latent causal variables and their causal graph plus modality-specific variables. Under linear mixing with shared variables XL and disjoint modality-specific variables, asymmetry in noise distributions across modalities enables matching. Assumption 26 (each shared node has ≥2 observed variables depending only on it) plus rank conditions enable disentangling shared from modality-specific structure. Core assumption: No causal edges between shared variables XL and modality-specific variables Lm; exogenous noise distributions are non-symmetric and pairwise different. Evidence: Theorem 28 shows XL and shared graph GL are identifiable up to permutation and scaling.

## Foundational Learning

- **d-separation and Markov property**: All causal discovery algorithms assume the joint distribution factorizes as P(X1,...,Xp) = ∏P(Xi|X_{Pa(i)}). Understanding d-separation is prerequisite for interpreting CI tests as graph structure constraints. Quick check: Given DAG X→Z←Y with no edge between X and Y, what CI relation holds?

- **Markov equivalence class (MEC) and essential graphs**: Observational data alone cannot distinguish DAGs in the same MEC. GAS/GSP outputs essential graph E(G), not G itself. Interventional data is required to orient remaining edges. Quick check: Can you distinguish X→Y→Z from X←Y→Z using only observational CI tests?

- **Identifiability up to equivalence classes**: CRL never recovers X exactly—only up to permutation and scaling (single-modality), or transitive closure (interventional), or shared variables (multi-modal). Understanding these limits prevents overclaiming. Quick check: In CRL, why can't we distinguish X from AX where A is an invertible diagonal matrix?

## Architecture Onboarding

- **Component map**: CI Testing Module -> Ancestral Reasoning Module -> Intervention Clustering Module -> Multi-modal Alignment Module
- **Critical path**: 1) Estimate intrinsic dimensionality p from observed data O (rank detection); 2) Run GAS for observational structure learning → essential graph E(G); 3) If interventional data available: cluster interventions → TS(G); 4) If multi-modal: align modalities → shared variables XL and graph GL
- **Design tradeoffs**: CI test threshold λ (higher reduces false positives but misses weak edges); intervention batch size (more improves coverage but increases cost); polynomial mixing degree s (higher captures complex mixing but requires more data)
- **Failure signatures**: GAS outputs fully connected graph → λ-strong faithfulness likely violated; intervention clustering merges distinct targets → interventional faithfulness violated; multi-modal alignment collapses to single modality → noise distributions insufficiently different
- **First 3 experiments**: 1) Validate GAS on synthetic Gaussian data with known DAG; measure CI test count vs. p and s parameters; compare to PC baseline; 2) Test interventional CRL on Perturb-seq subset with K=50 interventions; verify intervention clustering accuracy against known gene targets; 3) Apply multi-modal CRL to paired scRNA-seq/scATAC-seq data; check that shared latent dimension |L| is stable across random seeds

## Open Questions the Paper Calls Out

### Open Question 1
How can causal information be effectively incorporated into experimental design to guide data collection and accelerate the discovery of underlying causal mechanisms? Basis: Section 4 states this is a "nascent area" where it's not well understood how to incorporate causal information so experimental design guides data collection. Why unresolved: Current experimental design literature lacks integration with causal structure learning, preventing feedback loops where causal models actively inform subsequent experiment selection. Evidence would resolve it: Algorithms that actively select perturbations to reduce uncertainty in the causal graph, demonstrating faster convergence to ground-truth structure compared to non-causal adaptive sampling methods.

### Open Question 2
How can principled approaches be developed to prioritize data modalities given a specific downstream task and associated costs? Basis: Section 4 notes that developing principled approaches to decide which modality to prioritize given a particular downstream task is critical for future work. Why unresolved: While methods exist to disentangle shared and modality-specific latent variables, there's no formal framework to quantify cost-to-information trade-off of acquiring different modalities for a specific causal query. Evidence would resolve it: A theoretical framework and corresponding algorithm that outputs an optimal sequence of modalities to query, maximizing task-relevant information gain while minimizing acquisition costs.

### Open Question 3
What is the precise trade-off between the strength of the required faithfulness assumptions and computational efficiency in causal discovery algorithms? Basis: Page 8 discusses GRaSP algorithm, noting that higher tiers require weaker faithfulness assumptions, and states this suggests an important trade-off between correctness condition and computational efficiency that remains to be better understood. Why unresolved: Theoretically optimal algorithms like Sparsest Permutation (SP) are computationally intractable, while greedy variants (GSP) are faster but rely on stronger assumptions; the boundary between these extremes is unmapped. Evidence would resolve it: A theoretical characterization of the sample complexity and runtime required to guarantee correctness under progressively weaker λ-strong faithfulness conditions.

## Limitations

- Theoretical framework assumes ideal conditions (λ-strong faithfulness, linear interventional faithfulness, sufficient overlap across modalities) that may not hold in real biomedical data
- p^O(s) CI test complexity for GAS assumes small undirected clique sizes s, which may not be realistic for dense regulatory networks
- Multi-modal CRL requires strong assumptions about mixing functions and noise distributions that are difficult to verify empirically
- Performance claims on real biomedical datasets lack quantitative results in the abstract

## Confidence

**High Confidence**: Mechanism 1 (GAS complexity and adjacency search) - supported by concrete theorems and direct algorithmic description; Mechanism 2 (intervention clustering for source node identification) - well-defined under linear faithfulness assumptions.

**Medium Confidence**: Mechanism 3 (multi-modal alignment) - theoretical identifiability results exist but practical implementation details are sparse; intervention design framework - conceptual soundness established but optimization procedures need specification.

**Low Confidence**: Performance claims on real biomedical datasets - no quantitative results provided in abstract; scalability to genome-wide regulatory networks - theoretical bounds suggest limitations for dense graphs.

## Next Checks

1. **Test GAS on synthetic regulatory networks**: Generate DAGs with varying maximum clique sizes s and undirected degrees d; measure actual CI test counts against theoretical p^O(s) bound; compare adjacency recovery accuracy to PC algorithm.

2. **Validate intervention clustering empirically**: Use Perturb-seq data with known intervention targets; quantify clustering accuracy (precision/recall of intervention grouping); test sensitivity to number of interventions K and faithfulness violations.

3. **Benchmark multi-modal CRL on controlled datasets**: Create paired multi-modal data with known shared variables and causal structure; measure identification accuracy of XL and GL under varying overlap ratios and noise asymmetries; test robustness to Assumption 26 violations.