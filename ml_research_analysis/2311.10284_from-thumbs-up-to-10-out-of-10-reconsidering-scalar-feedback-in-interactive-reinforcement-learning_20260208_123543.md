---
ver: rpa2
title: 'From "Thumbs Up" to "10 out of 10": Reconsidering Scalar Feedback in Interactive
  Reinforcement Learning'
arxiv_id: '2311.10284'
source_url: https://arxiv.org/abs/2311.10284
tags:
- feedback
- scalar
- learning
- binary
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the effectiveness of scalar feedback versus
  binary feedback in interactive reinforcement learning. The authors conducted a study
  with 90 online participants evaluating a robot button-pressing task, comparing feedback
  consistency and correlation with RL targets.
---

# From "Thumbs Up" to "10 out of 10": Reconsidering Scalar Feedback in Interactive Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.10284
- Source URL: https://arxiv.org/abs/2311.10284
- Reference count: 37
- Scalar feedback, when properly handled with STEADY, significantly outperforms binary feedback in interactive reinforcement learning

## Executive Summary
This paper challenges the assumption that binary feedback is superior to scalar feedback in interactive reinforcement learning. Through a user study and algorithmic contribution, the authors demonstrate that scalar feedback—despite being less consistent at exact matches—performs comparably to binary feedback when small tolerance thresholds are allowed. The paper introduces STEADY, an algorithm that reconstructs scalar feedback distributions and rescales feedback based on statistical confidence, enabling binary-feedback-based methods to learn effectively from richer scalar signals. Models trained with scalar feedback plus STEADY achieved 4.22 higher average return rewards than binary feedback models.

## Method Summary
The authors conducted a study with 90 online participants evaluating a robot button-pressing task, comparing binary (good/bad) and scalar (0-10 scale) feedback consistency and correlation with RL targets. They implemented STEADY, an algorithm that treats scalar feedback as multi-distributional rather than noisy, reconstructing positive and negative distributions by maximizing Wasserstein distance between them. For each feedback value, STEADY assigns it to the distribution that maximizes inter-distribution separation and computes a confidence-weighted binary label based on how far the value deviates from its assigned distribution's mean. This approach enables binary-feedback-based methods to learn effectively from scalar feedback.

## Key Results
- Scalar feedback showed 25.2% exact-match self-agreement vs. 76.3% for binary, but with ±2 tolerance, scalar reached 77.7%—no significant difference from binary
- Both feedback types showed no significant correlation differences with RL targets
- Models trained with scalar feedback plus STEADY significantly outperformed binary feedback and raw scalar feedback baselines, achieving 4.22 higher average return rewards than binary feedback models

## Why This Works (Mechanism)

### Mechanism 1
Treating scalar feedback as multi-distributional rather than a single noisy distribution enables binary-feedback-based algorithms to learn effectively from richer signals. STEADY separates incoming scalar feedback into two underlying distributions (positive/preferable and negative/non-preferable) by maximizing Wasserstein distance between distributions. Each new feedback value is assigned to whichever distribution maximizes inter-distribution separation, then converted to a confidence-weighted binary label. This works because human scalar feedback naturally clusters into at least two latent intent categories (approval vs. disapproval) that can be statistically recovered without explicit thresholds.

### Mechanism 2
Scalar feedback inconsistency is largely tolerable noise rather than signal degradation, enabling learning gains when small mismatches are accommodated. The user study showed scalar feedback had 25.2% exact-match self-agreement vs. 76.3% for binary, but with ±2 tolerance, scalar reached 77.7%—no significant difference from binary. This suggests the "instability" concern is overstated if algorithms incorporate tolerance. Small variations in scalar ratings (e.g., 6 vs. 7 on a 10-point scale) represent equivalent evaluative intent rather than meaningful signal differences.

### Mechanism 3
Confidence-weighted relabeling preserves magnitude information while enabling compatibility with binary-feedback algorithms. STEADY computes a "confidence degree" based on how far feedback deviates from its assigned distribution's mean toward/away from the opposing distribution. This scalar confidence (range >1 for "increase weight" cases, <1 for "decrease weight") multiplies the binary label, effectively injecting magnitude information without requiring algorithm modification. The distance from distribution center toward the opposing distribution correlates with evaluative intensity (i.e., far from center = more confident judgment).

## Foundational Learning

- **Concept: Interactive Reinforcement Learning with Evaluative Feedback (TAMER/COACH frameworks)**
  - Why needed here: STEADY is evaluated using TAMER as the base learner; understanding how human feedback replaces or augments environmental rewards is essential to interpret why the confidence-weighted relabeling matters.
  - Quick check question: Can you explain why TAMER treats human feedback as a reward signal versus COACH treating it as an advantage signal?

- **Concept: Wasserstein Distance (Earth Mover's Distance)**
  - Why needed here: STEADY uses Wasserstein distance to measure separation between reconstructed positive/negative distributions; understanding this metric explains why the algorithm iteratively assigns feedback to maximize inter-distribution distance.
  - Quick check question: Why might Wasserstein distance be preferred over KL-divergence for separating overlapping scalar feedback distributions?

- **Concept: Human Feedback Dynamics in HRI**
  - Why needed here: The paper explicitly studies feedback changes within participants (session 1 vs. session 2) and between participants; understanding that feedback is "dynamic, not static" informs why online distribution updating is necessary.
  - Quick check question: What factors (per the paper's Background section) cause human feedback to change even when the robot behavior is identical?

## Architecture Onboarding

- **Component map:**
  Initialization Module -> Distribution Assignment -> Confidence Calculator -> Overlap Reducer -> Output Interface

- **Critical path:**
  1. Collect first 20 feedback points → initialize distributions via midpoint split
  2. For each subsequent feedback: compute Wasserstein distances → assign to distribution → compute confidence → check overlap condition → return (confidence, label)
  3. Feed (confidence × label) to downstream learner (e.g., TAMER) as reward signal
  4. Monitor distribution statistics as more feedback arrives; distributions evolve online

- **Design tradeoffs:**
  - Initialization strategy: Midpoint heuristic is simple but assumes roughly balanced initial feedback; alternatives (sliding window, fixed threshold) performed worse in experiments (4.99-6.07 lower returns than STEADY)
  - Two-distribution vs. multi-distribution: Paper implements 2-class but notes extension to m-distributions is possible; more distributions may capture finer intent distinctions but requires more data and risks overfitting
  - Online vs. batch: STEADY is designed for online learning; if batch preprocessing is acceptable, more sophisticated clustering (GMM, KDE) could improve distribution estimates

- **Failure signatures:**
  - Collapsed distributions: If all feedback falls in narrow range, positive/negative distributions heavily overlap; confidence scores become near-uniform, reverting to binary-like behavior
  - Distribution oscillation: If user feedback pattern shifts dramatically mid-session, historical distribution statistics may lag, causing misassignment
  - 3σ overlap persistent: If overlap reducer can't resolve ambiguity, label assignments become unstable

- **First 3 experiments:**
  1. Sanity check—reproduction on button-press task: Implement STEADY + TAMER on simulated version of the button-press task with recorded human feedback data; target: achieve >4.0 return improvement over binary baseline
  2. Ablation—confidence degree impact: Run STEADY with confidence degree fixed at 1.0 (pure binary relabeling) vs. full confidence weighting; quantify how much of the 4.22 gain comes from magnitude recovery vs. distribution-based relabeling alone
  3. Stress test—distribution collapse scenario: Synthesize feedback data where 80%+ of ratings fall within a 2-point range; measure STEADY performance degradation and compare to midpoint classifier baseline

## Open Questions the Paper Calls Out
- How do feedback dynamics evolve in long-term interactions, and can stabilization methods adapt to these temporal changes? The authors explicitly state that the experiments were "relatively short-term" and "did not address long-term feedback dynamics."
- Does the STEADY algorithm generalize to more complex tasks with high-dimensional state spaces? The authors acknowledge the button-pressing task is "relatively simple" and used a specific, structured representation that may not reflect complex environments.
- How can agents effectively learn from multiple teachers simultaneously given the significant variance in individual feedback patterns? The results show feedback patterns differ significantly between participants, and the authors note this makes "learning from multiple participants... difficult."

## Limitations
- The controlled experimental setup—a button-pressing task with predetermined trajectories—may not generalize to open-ended tasks where feedback patterns are less predictable
- STEADY assumes at least partial separation between positive and negative feedback distributions, which may not hold in all domains
- The human study used a specific scalar range (0-10 without negatives), and results might differ with alternative scales or inclusion of negative values

## Confidence
- **High confidence**: The user study methodology (self-agreement rates, correlation analysis) is sound and the results are statistically robust
- **Medium confidence**: STEADY's performance advantage (+4.22 return) is well-demonstrated, but the algorithm's assumptions about distributional separability need further validation across diverse tasks
- **Low confidence**: Generalization claims to real-world HRI scenarios exceed the evidence provided by a single task type

## Next Checks
1. Test STEADY on tasks with multiple optimal behaviors (e.g., maze navigation with several valid paths) to assess distribution separation under ambiguous feedback conditions
2. Evaluate STEADY with negative scalar values included to determine if performance differs from the non-negative range used in the study
3. Implement an ablation study removing the confidence degree calculation to isolate how much of the performance gain comes from magnitude recovery versus distribution-based relabeling