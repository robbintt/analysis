---
ver: rpa2
title: 'Explanation User Interfaces: A Systematic Literature Review'
arxiv_id: '2505.20085'
source_url: https://arxiv.org/abs/2505.20085
tags:
- https
- user
- explanations
- design
- international
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review identifies and analyzes Explanation
  User Interfaces (XUIs) for explainable AI systems. The study examines 146 publications
  to understand how XUIs are designed, evaluated, and what influences their development
  across different domains and user types.
---

# Explanation User Interfaces: A Systematic Literature Review

## Quick Facts
- arXiv ID: 2505.20085
- Source URL: https://arxiv.org/abs/2505.20085
- Reference count: 40
- Primary result: Systematic review of 146 XUI publications identifies visual explanations dominate, with HERMES platform providing design guidelines for practitioners

## Executive Summary
This systematic literature review examines 146 publications to understand how Explanation User Interfaces (XUIs) are designed, evaluated, and developed across different domains and user types. The study reveals that visual explanations, particularly heatmaps, dominate over text-based approaches for conveying AI decision rationale. Neural networks are the most frequently explained AI models, with feature importance, counterfactual explanations, and SHAP values being the most common explanation techniques. The review identifies trust as the most frequently evaluated metric and reveals a significant gap between algorithmic transparency and practical usability. To address these findings, the authors developed HERMES, a web platform providing design guidelines and frameworks for practitioners to create effective, human-centered XUIs.

## Method Summary
The study employed Kitchenham's Systematic Literature Review methodology, conducting database searches across ACM Digital Library, IEEE Xplore, and Scopus from June to December 2024 using the search string `("xai" OR "expla*") AND ("user" OR "human") AND ("interface" OR "interact*")`. After applying inclusion criteria (published â‰¥2013, English, A*/A/B conferences or Q1/Q2 journals, must include user study + explanation algorithm + AI model), 146 papers were included. Each paper was classified across 13 dimensions including application domain, user type, AI model, XAI technique, and explanation modality. Backward and forward snowballing was performed to ensure comprehensive coverage.

## Key Results
- Visual explanations dominate over text, with heatmaps being the most common visualization technique
- Neural networks are the most frequently explained AI models, with feature importance, counterfactual explanations, and SHAP values as top techniques
- Trust is the most frequently evaluated metric across XUI studies
- 21 of 146 papers failed to specify user types, indicating gaps in methodology
- Only 8 studies explicitly assessed transparency despite its prominence in XAI discourse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual explanations (particularly heatmaps) reduce cognitive load compared to text-based explanations when conveying AI decision rationale
- Mechanism: Visual representations translate complex AI reasoning into interpretable patterns without requiring sequential processing. Heatmaps, bar charts, and trend lines enable rapid pattern recognition for both experts and non-experts.
- Core assumption: Users process visual information more efficiently than equivalent text for understanding feature attributions and model behavior
- Evidence anchors: Visual explanations dominate, likely due to their intuitive and accessible nature... Heatmaps stand out as the most frequently used visualization

### Mechanism 2
- Claim: Interactivity in XUIs enhances comprehension and trust by enabling user-driven exploration of explanations
- Mechanism: Interactive elements (filtering, what-if exploration, parameter adjustment) allow users to tailor explanations to their mental models
- Core assumption: Users have sufficient domain knowledge and motivation to engage with interactive features meaningfully
- Evidence anchors: Interactivity plays a central role in the explanation process... Interactive XUIs allow users to manipulate explanation parameters, explore different perspectives, or request additional details

### Mechanism 3
- Claim: Matching explanation technique to user type (domain experts vs. non-experts vs. AI experts) improves perceived helpfulness and trust
- Mechanism: Domain experts prioritize helpfulness for decision-making over transparency; non-experts benefit from intuitive visualizations like SHAP feature contributions
- Core assumption: User type can be reliably identified and explanation needs are relatively stable within user categories
- Evidence anchors: The choice of explanation technique often depends on the audience... SHAP explanations tend to be preferred when working with non-expert users

## Foundational Learning

- Concept: **Global vs. Local Explanations**
  - Why needed here: HERMES framework requires understanding whether explanations describe overall model behavior (global) or individual predictions (local) to select appropriate XAI techniques
  - Quick check question: Can you distinguish when a user needs to understand "how the model generally works" vs. "why this specific prediction occurred"?

- Concept: **XAI Technique Taxonomy** (Feature Importance, Counterfactuals, Saliency Masks, SHAP, Decision Rules)
  - Why needed here: Selecting appropriate explanation techniques depends on model type (neural network vs. ensemble) and explanation modality constraints
  - Quick check question: Given a neural network image classifier, which explanation technique would highlight spatial regions influencing predictions?

- Concept: **Human-Centered Design (HCD) for AI Systems**
  - Why needed here: The paper explicitly frames effective XUIs as requiring user involvement from planning through evaluation, not just algorithmic transparency
  - Quick check question: What is the minimum user study methodology needed before deploying an XUI to domain experts?

## Architecture Onboarding

- Component map: HERMES Platform -> Guideline Cards -> Evaluation Output
- Critical path:
  1. Define project constraints along HERMES dimensions (at minimum: domain and user type)
  2. Receive filtered XAI technique and modality recommendations
  3. Review specific guideline cards (e.g., "Provide Contextual Information")
  4. Plan user study methodology aligned with target user group

- Design tradeoffs:
  - Visual vs. text modality: Visuals better for pattern recognition; text required for semantic nuance
  - Interactivity vs. simplicity: Interactivity enables exploration but increases cognitive load and development cost
  - Trust vs. transparency metrics: Domain experts prioritize helpfulness; transparency rarely explicitly assessed in studies

- Failure signatures:
  - XUIs designed without user studies show gaps between algorithmic output and user needs
  - "Not specified" user types in 21 of 146 reviewed papers indicate missing persona definition
  - Transparency metric assessed in only 8 studies despite its importance in XAI discourse

- First 3 experiments:
  1. **Persona Definition Test**: Before selecting XAI techniques, conduct at least 3 interviews with target users to document mental models and explanation expectations
  2. **Modality Comparison**: A/B test visual-only vs. hybrid (visual + text) explanations for same XAI technique, measuring trust and understandability with minimum 20 participants per condition
  3. **Interactivity Value Validation**: Implement a minimal interactive feature, measure task performance and workload changes using controlled experiment methodology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Large Language Models (LLMs) be integrated into XUIs to facilitate conversational explanations while mitigating risks associated with their inherent opacity and tendency for hallucinations?
- Basis in paper: Section 9 identifies LLMs as a "novel challenge" because they enable conversational interactions but "remain opaque, prone to hallucinations, and can undermine user trust"
- Why unresolved: While LLMs offer new modalities for explanation, their lack of transparency conflicts with the core goals of XAI, requiring new methods to ensure reliability
- Evidence: Comparative user studies measuring trust and decision-making accuracy between standard XUIs and LLM-enhanced conversational XUIs

### Open Question 2
- Question: What is the impact of using co-design and focus group methodologies on the reliability and practical applicability of Human-Centered XAI systems compared to current evaluation standards?
- Basis in paper: Section 9 highlights the "limited adoption of co-design practices and focus group methodologies" and states these are essential to strengthen "real co-creation dynamic[s]"
- Why unresolved: The literature currently relies heavily on controlled experiments, potentially neglecting the iterative, qualitative insights necessary for robust human-centered design
- Evidence: Longitudinal studies comparing the usability and trust of XUIs developed through standard controlled experiments versus those developed via iterative co-design

### Open Question 3
- Question: How can crowdsourcing mechanisms be effectively utilized to validate and enrich the context-aware design guidelines proposed in the HERMES framework?
- Basis in paper: Section 9 proposes that integrating crowd-sourcing could "enrich the guideline repository" and validate guidelines through "collective evaluation"
- Why unresolved: Current guidelines are derived from academic literature and may lack the granularity needed for complex or specialized use cases involving multiple intersecting dimensions
- Evidence: Platform analytics and qualitative feedback from a diverse set of practitioners using a crowdsourced version of HERMES to solve domain-specific design problems

## Limitations

- 146-paper corpus may not capture full landscape of rapidly evolving XUI development
- Classification of user types shows substantial gaps, with 21 papers failing to specify target audience
- Notable disconnect between algorithmic transparency metrics and practical usability evaluation

## Confidence

- **High Confidence**: Visual explanations dominate over text-based approaches
- **Medium Confidence**: Interactivity enhances comprehension and trust
- **Low Confidence**: Matching explanation technique to user type reliably improves outcomes

## Next Checks

1. **User Type Validation**: Conduct a replication study with at least 50 participants spanning different expertise levels to test whether explanation effectiveness varies significantly by user type
2. **Modality Effectiveness Test**: Perform an A/B test comparing visual-only, text-only, and hybrid explanations for the same XAI technique across three distinct domains
3. **HERMES Framework Validation**: Implement the HERMES recommendation system for three real-world projects and measure whether adherence to filtered guidelines improves user trust and task performance compared to ad hoc design approaches