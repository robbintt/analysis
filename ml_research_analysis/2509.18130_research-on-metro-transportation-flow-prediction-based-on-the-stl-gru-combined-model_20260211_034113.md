---
ver: rpa2
title: Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined
  Model
arxiv_id: '2509.18130'
source_url: https://arxiv.org/abs/2509.18130
tags:
- flow
- prediction
- data
- passenger
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately predicting metro
  transfer passenger flow, which is crucial for optimizing metro operation plans and
  improving transportation efficiency. It proposes an innovative STL-GRU combined
  model that integrates Seasonal and Trend decomposition using Loess (STL) for data
  preprocessing and Gated Recurrent Unit (GRU) for prediction.
---

# Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model

## Quick Facts
- arXiv ID: 2509.18130
- Source URL: https://arxiv.org/abs/2509.18130
- Authors: Zijie Zhou; Huichen Ma
- Reference count: 0
- The STL-GRU model achieves lowest MAPE across all scenarios (weekdays excluding Fridays, Fridays, rest days) with reductions of at least 2.3, 1.36, and 6.42 percentage points respectively.

## Executive Summary
This paper addresses the challenge of accurately predicting metro transfer passenger flow by proposing an innovative STL-GRU combined model. The method integrates Seasonal and Trend decomposition using Loess (STL) for data preprocessing with Gated Recurrent Units (GRU) for prediction. Experiments on a multi-line transfer hub demonstrate the model outperforms traditional approaches like LSTM and GRU, particularly excelling on rest days with over 50% MAPE reduction.

## Method Summary
The approach extracts transfer passenger flow data from raw AFC data using a graph-based depth-first search algorithm, then applies STL decomposition to separate the time series into trend, seasonal, and residual components. Each component is predicted separately using GRU models, with residuals cleaned using 3σ outlier treatment. The final prediction is obtained by summing the component predictions. The model uses 2 hidden GRU layers (128 and 256 neurons) with ReLU activation and dropout regularization.

## Key Results
- STL-GRU achieves lowest MAPE across all tested scenarios: weekdays (excl. Fridays), Fridays, and rest days
- The model shows particularly strong performance on rest days with over 50% MAPE reduction compared to baselines
- While MAPE is lowest, weekday RMSE (136) is higher than some baselines (98-102), indicating trade-off between overall accuracy and peak prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing passenger flow time series before prediction improves accuracy by reducing noise interference with the learning process.
- Mechanism: STL separates raw time series into trend, seasonal, and residual components, each predicted separately by GRU models, then summed for final output. This reduces burden on any single model to learn mixed patterns.
- Core assumption: Additive decomposition accurately reflects underlying data-generating process and residual noise is largely random.
- Evidence anchors: Abstract mentions STL decomposition into trend, periodic, and residual components; page 3 states STL effectively separates components; corpus shows alternative spatial-temporal approaches exist.

### Mechanism 2
- Claim: 3σ principle for residual outlier treatment stabilizes GRU training by limiting extreme value influence.
- Mechanism: Residual values exceeding 3 standard deviations are replaced with average of same time period on adjacent days, preventing gradient spikes during training.
- Core assumption: Outliers in residuals represent measurement errors or random noise rather than genuine rare events.
- Evidence anchors: Abstract mentions 3σ principle for outlier elimination; page 4 details outlier removal and filling; corpus shows alternative uncertainty estimation approaches exist.

### Mechanism 3
- Claim: GRU's simplified gating structure provides comparable accuracy to LSTM with faster training, making it suitable for operational short-term prediction.
- Mechanism: GRU uses only two gates versus LSTM's three, reducing parameter count and training time while preserving temporal dependency capture.
- Core assumption: Temporal dependencies in metro passenger flow are sufficiently captured by GRU's simplified architecture.
- Evidence anchors: Abstract mentions Keras implementation; page 4 compares GRU's simpler structure to LSTM; corpus shows more complex architectures for other prediction tasks.

## Foundational Learning

- Concept: **Time Series Decomposition (Additive Models)**
  - Why needed here: Understanding STL's additive assumption (Y_t = T_t + S_t + R_t) is foundational for debugging decomposition quality and interpreting residual behavior.
  - Quick check question: If you plot your decomposed residual component and see a clear weekly pattern, what does this indicate about your STL configuration?

- Concept: **Gated Recurrent Units vs. LSTM**
  - Why needed here: Selecting between GRU and LSTM requires understanding the trade-off between computational efficiency and capacity to model long-term dependencies.
  - Quick check question: Which gate in GRU combines the functions of LSTM's forget and input gates, and what is the implication for gradient flow?

- Concept: **MAPE vs. RMSE Trade-offs**
  - Why needed here: The paper shows STL-GRU achieving lowest MAPE but sometimes higher RMSE than baselines; understanding when to prioritize each metric matters for operational decisions.
  - Quick check question: If your model underpredicts extreme peaks by 50%, which metric (MAPE or RMSE) will penalize this more heavily, and why?

## Architecture Onboarding

- Component map: AFC raw data -> Cleaning (5 invalid types removed) -> Transfer flow extraction (graph-based DFS) -> STL decomposition (period=272, inner=15, outer=3) -> 3σ residual treatment -> Three parallel GRU models (128,256 neurons, dropout=0.1) -> Sum component predictions -> Final forecast

- Critical path: AFC raw data quality -> accurate transfer flow extraction -> proper STL period parameter alignment -> GRU hyperparameter tuning. The STL period parameter of 272 is particularly sensitive; misalignment propagates errors through all downstream components.

- Design tradeoffs:
  - Accuracy vs. Speed: STL-GRU takes ~39s vs. 0.68s for vanilla GRU. Acceptable for 5-minute operational windows, not real-time second-by-second decisions.
  - Peak Prediction vs. Overall Accuracy: Lower MAPE but higher RMSE on weekdays suggests model trades peak accuracy for stability.
  - Outlier Treatment vs. Event Sensitivity: 3σ filtering improves regular-day performance but may blunt response to planned special events.

- Failure signatures:
  - Consistently high RMSE with low MAPE: Model captures overall patterns but misses extreme peaks; consider peak-aware loss function or increased model capacity.
  - MAPE spike on transition days (Fridays): Mixed commuting/leisure patterns not captured; consider separate models for different day types.
  - Decomposition artifacts: If seasonal component shows irregular patterns, check period parameter alignment with actual data collection intervals.

- First 3 experiments:
  1. Baseline replication: Reproduce STL decomposition on sample data with period=272; verify trend/seasonal/residual components visually separate and residuals approximate white noise.
  2. Ablation study: Compare vanilla GRU on raw data, STL-GRU without 3σ treatment, and full STL-GRU on held-out week to quantify contribution of each processing step.
  3. Period sensitivity test: Vary STL period parameter (68, 272, 1904) and measure impact on decomposition quality and downstream prediction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the STL-GRU model be enhanced to better capture sudden passenger flow fluctuations during weekday peak hours?
- Basis in paper: The authors state that during weekday scenarios (excluding Fridays), the model exhibits higher RMSE (136) compared to other models (98–102) because it has a "large prediction deviation for extreme peaks," and explicitly conclude it is "necessary to further optimize the model's ability to respond to sudden passenger flow."
- Why unresolved: While the STL component effectively filters noise and captures general trends, the current configuration smooths out extreme values too aggressively, leading to lag or underestimation during rush hours.
- What evidence would resolve it: A modification to the loss function or residual handling that results in the STL-GRU model achieving a lower weekday RMSE than the baseline GRU and LSTM models.

### Open Question 2
- Question: Can parallel computing architectures reduce the STL-GRU model's prediction time to near real-time levels without compromising accuracy?
- Basis in paper: The paper notes that while the prediction time of 39.21s meets the "practical needs" (within 5 minutes), it is significantly slower than single models (0.68s). The authors suggest these shortcomings are "mitigable via... parallel computing architecture."
- Why unresolved: The current implementation processes the decomposition and GRU components sequentially, creating a computational bottleneck that has not yet been optimized in the study.
- What evidence would resolve it: A demonstration of the model running on a parallelized framework (e.g., multi-threaded or GPU-accelerated decomposition) that reduces inference time to under 5–10 seconds.

### Open Question 3
- Question: Does the STL-GRU model's high performance on rest days generalize to transfer hubs with different operational characteristics?
- Basis in paper: The study validates the model specifically on a "three-line transfer hub" (Xizhimen/People's Square) which has "complex spatio-temporal distribution." It is inferred that stations with simpler topologies or less distinct transfer patterns might not yield the same significant error reductions (e.g., >50% MAPE reduction on rest days).
- Why unresolved: The specific "rest day" success relies on the model's ability to filter noise in volatile data; it is unclear if this advantage holds when the "noise" is actually structural variance in smaller stations.
- What evidence would resolve it: Comparative results showing the STL-GRU model maintaining the lowest MAPE across different station archetypes (e.g., residential vs. commercial) and transfer complexities.

## Limitations
- The 3σ outlier removal mechanism may systematically underestimate genuine extreme events like concerts or emergencies by assuming residual noise follows Gaussian distribution
- Transfer flow extraction accuracy depends on accurate metro network topology data, which was not specified and may introduce route inference errors
- The optimal STL period parameter for 5-minute intervals (204, 272, or 288) remains unclear, with incorrect configuration potentially degrading decomposition quality

## Confidence
- **High confidence**: STL decomposition reduces noise interference and enables parallel GRU training (well-established time series technique)
- **Medium confidence**: 3σ outlier treatment improves stability but may miss genuine extreme events; MAPE improvements are significant but RMSE trade-offs suggest peak prediction limitations
- **Low confidence**: Transfer flow extraction accuracy and optimal STL period selection for 5-minute intervals cannot be verified without additional metro network data

## Next Checks
1. **Decomposition quality verification**: Plot STL output components (trend, seasonal, residual) to confirm residual approximates white noise distribution and seasonal pattern matches expected daily/weekly cycles
2. **Outlier handling sensitivity**: Compare STL-GRU performance with and without 3σ outlier treatment on data containing known special events to quantify peak prediction degradation
3. **Parameter sensitivity analysis**: Systematically vary the STL period parameter (204, 272, 288) and measure impact on decomposition quality metrics and final MAPE/RMSE to identify optimal configuration for 5-minute interval data