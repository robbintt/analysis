---
ver: rpa2
title: Why Are You Wrong? Counterfactual Explanations for Language Grounding with
  3D Objects
arxiv_id: '2505.06030'
source_url: https://arxiv.org/abs/2505.06030
tags:
- counterfactual
- utterances
- object
- original
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of explaining misclassifications\
  \ in object referent identification tasks, where neural networks must select a 3D\
  \ object based on a natural language description. The authors propose a method to\
  \ generate counterfactual explanations\u2014alternative yet semantically similar\
  \ utterances that would have led to correct predictions."
---

# Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects

## Quick Facts
- arXiv ID: 2505.06030
- Source URL: https://arxiv.org/abs/2505.06030
- Reference count: 34
- This paper proposes a method to generate counterfactual explanations for misclassifications in object referent identification tasks using genetic algorithm optimization and LLM-based sampling strategies.

## Executive Summary
This paper addresses the challenge of explaining misclassifications in object referent identification tasks, where neural networks must select a 3D object based on a natural language description. The authors propose a method to generate counterfactual explanations—alternative yet semantically similar utterances that would have led to correct predictions. Their approach combines sampling strategies (random, word-type aware, synonym-based, and LLM-based) with a genetic algorithm optimization to generate valid counterfactuals that maintain semantic similarity to the original utterance while achieving the desired classification. Experiments using the ShapeTalk dataset and three different neural network models demonstrate that LLM-based sampling strategies outperform other approaches, achieving high success rates (97-100%) in generating valid counterfactuals with strong semantic similarity (average cosine similarity 0.808-0.855).

## Method Summary
The method generates counterfactual explanations for misclassified samples in object referent identification tasks through a two-stage approach. First, sampling strategies generate initial candidate counterfactuals by replacing words in the original utterance—strategies include random replacement, word-type aware sampling, synonym-based replacement, and LLM-based generation. Second, a genetic algorithm optimizes these candidates over 30 generations with a population of 20, using tournament selection, single-point crossover, and 10% mutation rate. The fitness function balances class-flip objective (rewarding prediction reversal) with semantic similarity measured by Universal Sentence Encoder embeddings. The approach is evaluated on the ShapeTalk dataset using three neural listener models (PC-AE, ResNet, ViT) across 1,000 misclassified samples.

## Key Results
- LLM-based sampling strategies achieve 97-100% success rates in generating valid counterfactuals, outperforming other approaches
- Generated counterfactuals show strong semantic similarity (average cosine similarity 0.808-0.855) to original utterances
- The method reveals model biases and weaknesses in original descriptions, providing actionable insights for improving human-AI interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Genetic algorithm optimization effectively discovers valid counterfactual utterances by iteratively refining a population toward both class-flip and semantic-similarity objectives.
- Mechanism: A population of N=20 candidate utterances undergoes M=30 generations of tournament selection, single-point crossover, and mutation (resampling via the chosen strategy). Fitness is computed as the unweighted sum of (1) a class-flip term that rewards prediction reversal and (2) USE-based cosine similarity to the original utterance. A −1 penalty ensures invalid counterfactuals score lower than any valid one.
- Core assumption: The search space of utterance modifications is structured such that evolutionary operations can discover valid counterfactuals without exhaustive enumeration, and that similarity can be meaningfully captured by sentence embeddings.
- Evidence anchors:
  - [abstract]: "genetic algorithm optimization to generate valid counterfactuals that maintain semantic similarity to the original utterance while achieving the desired classification"
  - [section III.C]: "The GA optimizes the population over M generations by performing crossover and mutation... The final fitness function is thus the unweighted sum of these two objectives"
  - [corpus]: Weak corpus support for GA-based counterfactual generation in text domains; corpus neighbor "Ranking Counterfactual Explanations" discusses ordering but not GA search
- Break condition: If success rate fails to improve across generations, or if similarity continuously degrades without recovery after initial crossover-induced drop.

### Mechanism 2
- Claim: Context-aware LLM-based sampling produces more semantically coherent and grammatically correct counterfactuals than synonym-based or random replacement strategies.
- Mechanism: When a content word (NVAA: noun, verb, adjective, adverb) is selected for replacement, the LLM receives the masked utterance and generates five contextually appropriate alternatives. The GA then searches among these constrained candidates, ensuring replacements respect sentence context rather than just lexical similarity.
- Core assumption: LLMs trained on natural language can capture subtle semantic relationships required for geometric object descriptions, and their suggestions remain within the distribution of valid utterances.
- Evidence anchors:
  - [abstract]: "LLM-based sampling strategies outperform other approaches, achieving high success rates (97-100%) in generating valid counterfactuals with strong semantic similarity (average cosine similarity 0.808-0.855)"
  - [section IV.C, Table I]: Qwen achieves 38.7% grammatical correctness vs 19.0% for word-aware on PC-AE; Falcon/Qwen show higher "equivalent"/"very similar" ratings
  - [corpus]: Limited corpus evidence; corpus neighbor "Temporal Counterfactual Explanations" addresses robot decisions but not LLM-based text generation
- Break condition: If LLM suggestions become overly creative and semantically divergent (e.g., "smooth knees" replacing "curved legs"), or if context-unrelated insertions bypass USE similarity penalties.

### Mechanism 3
- Claim: Restricting mutable words to content words (NVAA) preserves sentence structure while enabling meaningful modifications that reveal model biases and description weaknesses.
- Mechanism: The method identifies content words as mutable targets while leaving function words (determiners, prepositions, etc.) unchanged. This constraint ensures counterfactuals maintain grammatical skeleton and structural plausibility, focusing modifications on information-carrying elements that models rely upon for object differentiation.
- Core assumption: Object referent identification models primarily depend on content words to distinguish targets from distractors, as established in prior work on ShapeGlot.
- Evidence anchors:
  - [section III.B]: "we restrict the mutable word types to content words, such as nouns, verbs, adjectives, and adverbs (NVAA), to maintain the core sentence structure while still allowing modifications to information-rich elements"
  - [section IV.C, Table II]: Content word replacements reveal patterns—"small" vs "short" distinctions expose model's dimensional interpretation biases
  - [corpus]: No direct corpus evidence for NVAA constraint; corpus neighbor on contrastive explanations focuses on different domains
- Break condition: If critical distinguishing information is encoded in function words (unlikely for geometric descriptions), or if NVAA restriction prevents discovery of valid counterfactuals in edge cases.

## Foundational Learning

- **Object Referent Identification**
  - Why needed here: This is the core task being explained—understanding that the model must select a target 3D object from distractors based solely on geometric/structural descriptions (no color/texture) is essential for interpreting counterfactual validity.
  - Quick check question: Given utterance "it has curved legs" and two chair objects differing only in leg curvature, which object should receive higher target probability?

- **Counterfactual Explanations**
  - Why needed here: The paper builds on the formal counterfactual framework—explanations showing minimal input changes that would achieve desired output—rather than causal attribution or feature-importance methods.
  - Quick check question: If a model misclassifies an object given utterance "the tall one," what does a counterfactual explanation provide versus a feature-attribution explanation?

- **Genetic Algorithm Fundamentals**
  - Why needed here: GA provides the search mechanism; understanding population initialization, fitness-proportionate selection, crossover recombination, and mutation operators is necessary for debugging optimization failures.
  - Quick check question: In a GA with tournament selection and single-point crossover, how does the mutation operator maintain exploration pressure?

## Architecture Onboarding

- **Component map**: Sampler Module -> Fitness Evaluator -> GA Optimizer -> Quality Assessor

- **Critical path**: Misclassified sample identification → Initial population sampling (N=20) → Fitness evaluation → Generation loop (selection → crossover → mutation → population update) × M=30 → Return highest-similarity valid counterfactual

- **Design tradeoffs**:
  - Population size (N=20) vs compute: 620 total evaluations per sample; larger N increases diversity but linearly increases cost
  - Mutation rate (p=0.10): Low enough to preserve good candidates, high enough to escape local optima
  - LLM sampler choice: Falcon3-1B/LLaMA-3.2-1B/Qwen2.5-1.5B trade off suggestion quality against inference latency
  - Unweighted fitness sum: Assumes equal importance of validity and similarity; alternative weightings unexplored

- **Failure signatures**:
  - Success rate plateau: GA trapped in local optimum → increase mutation rate or add diversity preservation
  - Low grammatical correctness (<30%): Non-LLM samplers producing incoherent replacements → switch to context-aware LLM sampling
  - High similarity + low validity: Class-flip objective underweighted → verify −1 penalty applied correctly
  - USE similarity inflated despite semantic drift: Embedding metric under-penalizes contextually unrelated insertions (paper's stated limitation)

- **First 3 experiments**:
  1. **Sampler baseline**: Run all seven strategies (six samplers + random search baseline) on 100 misclassified samples across three backbone models (PC-AE, ResNet, ViT); report success rate, cosine similarity, and normalized Levenshtein distance
  2. **Ablation by generation**: Track success rate and similarity at generations 0, 10, 20, 30 for Qwen sampler; expect initial similarity drop (crossover introduces multi-word changes) followed by recovery
  3. **Word-pair analysis**: Extract most frequent replacement pairs per strategy; verify LLM-based samplers produce semantically meaningful pairs (e.g., "short→tiny") vs word-aware's uninformative pairs (e.g., "has→have")

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative similarity metrics or scoring functions more effectively penalize contextually unrelated word insertions than the Universal Sentence Encoder (USE) used in the optimization process?
- Basis in paper: [explicit] The Conclusion states that a limitation of the current method is that the embedding-based similarity "under-penalize[s] the insertion of contextually unrelated words as long as semantical key parts of the utterance are retained."
- Why unresolved: While the current metric optimizes for general semantic similarity, it lacks the sensitivity to detect when inserted words, while semantically proximate, are contextually nonsensical within the specific utterance.
- What evidence would resolve it: Experiments implementing context-aware metrics (e.g., learned reward models or stricter linguistic constraints) that demonstrate a higher correlation with human judgments of contextual fit compared to the USE baseline.

### Open Question 2
- Question: To what extent can the generated counterfactual utterances be utilized for data augmentation to enhance the performance and robustness of object referent identification models?
- Basis in paper: [explicit] In the Conclusion, the authors propose that "our work on generating counterfactual utterances could be used for data augmentation, potentially enhancing the performance of state-of-the-art models."
- Why unresolved: The current study focuses on the generation success rate and similarity of the counterfactuals, but it does not measure the downstream impact of using these synthetic examples for model training.
- What evidence would resolve it: Benchmarks comparing the accuracy of neural listeners trained on the original ShapeTalk dataset versus those trained on a dataset augmented with the generated counterfactuals.

### Open Question 3
- Question: Do the generated counterfactuals provide actionable insights that align with human intuition and correction strategies in real-world human-robot interaction scenarios?
- Basis in paper: [inferred] The paper relies entirely on GPT-4o-mini as a "human surrogate" to evaluate semantic similarity and grammatical correctness, leaving the actual utility of these explanations for human practitioners unverified.
- Why unresolved: Automated metrics and LLM evaluations do not guarantee that a human user would find the suggested word replacements natural or helpful for correcting their descriptions during an interaction.
- What evidence would resolve it: User studies where human participants attempt to correct misclassifications using the generated counterfactuals, measuring the speed and success rate of the interaction compared to unassisted correction.

## Limitations

- The USE similarity metric may under-penenalize contextually unrelated word insertions, potentially allowing nonsensical modifications that maintain high similarity scores
- The method relies heavily on LLM quality for context-aware sampling, making performance dependent on the specific LLM chosen and its geometric object description capabilities
- The approach is computationally intensive, requiring 620 evaluations per sample (N=20 population × M=30 generations + initial sampling)

## Confidence

- **High confidence**: GA optimization effectively discovers valid counterfactuals (supported by high success rates and established GA literature)
- **Medium confidence**: LLM-based samplers produce more semantically coherent counterfactuals (results show improvement but similarity metric limitations acknowledged)
- **Medium confidence**: Counterfactuals reveal model biases and weaknesses (qualitative analysis provided but requires deeper causal investigation)

## Next Checks

1. **Similarity metric validation**: Manually evaluate a subset of LLM-generated counterfactuals using human annotators to verify whether USE similarity scores align with human semantic similarity judgments for geometric object descriptions.

2. **Local optimum robustness**: Run ablation studies varying population size (N=10, 20, 30) and mutation rate (5%, 10%, 15%) to determine sensitivity to GA parameters and whether the algorithm consistently finds valid counterfactuals across different hyperparameter settings.

3. **Function word ablation**: Generate counterfactuals for a subset of samples allowing function word modifications alongside NVAA words to test whether this expands the counterfactual space and reveals additional model weaknesses.