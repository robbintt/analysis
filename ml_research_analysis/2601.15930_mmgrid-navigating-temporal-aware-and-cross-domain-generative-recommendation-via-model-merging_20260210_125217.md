---
ver: rpa2
title: 'MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation
  via Model Merging'
arxiv_id: '2601.15930'
source_url: https://arxiv.org/abs/2601.15930
tags:
- merging
- task
- domains
- domain
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents MMGRid, the first systematic study of model
  merging (MM) in generative recommendation (GR) through a contextual lens. The authors
  construct a unified experimental framework with GR checkpoints specialized to different
  real-world contexts (temporal evolution and domain diversity) built from a shared
  base LLM.
---

# MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging

## Quick Facts
- arXiv ID: 2601.15930
- Source URL: https://arxiv.org/abs/2601.15930
- Reference count: 40
- Authors: Tianjun Wei; Enneng Yang; Yingpeng Du; Huizhong Guo; Jie Zhang; Zhu Sun
- Primary result: First systematic study of model merging in generative recommendation, demonstrating base model replacement and weighted contextual merging effectively reduce parameter conflicts and balance recency bias

## Executive Summary
MMGRid presents the first systematic investigation of model merging (MM) in generative recommendation (GR) through a contextual lens. The authors construct a unified experimental framework using three GR paradigms (text-grounded, semantic ID, semantic embedding) and two mainstream merging algorithms (weight-based, subspace-based) across six Amazon domains with temporal splits. They identify that parameter conflicts arise from token distribution shifts and objective disparities during cross-domain merging, and that incremental training induces recency bias. Their key contributions include disentangling task-aware and context-specific parameter changes via base model replacement, balancing temporal preferences through weighted contextual merging, and open-sourcing their complete framework for future research.

## Method Summary
The method constructs a unified experimental framework with GR checkpoints specialized to temporal evolution and domain diversity, built from a shared base LLM (Qwen3-0.6B). Three GR paradigms are implemented: BIGRec (text-grounded), LC-Rec (semantic indexing with RQ-VAE), and HLLM (semantic embedding). Models are trained on Amazon Reviews 2023 data with six domains split into pretrain (t0), incremental phases (P1, P2), and test periods. Task vectors are computed as parameter differences between checkpoints to represent domain and temporal adaptations. Two merging approaches are evaluated: weight-based linear interpolation with domain-specific weights, and subspace-based merging using TIES (top 20%) and DARE (p=0.1) for trimming. The framework systematically analyzes conflicts and proposes solutions including base model replacement and neutral base construction.

## Key Results
- Cross-domain merging of GR models induces severe parameter conflicts (98-99% performance degradation in SemEmb), which can be alleviated by replacing the base model with a task-adapted checkpoint
- Incremental training across temporal contexts induces recency bias that can be effectively balanced through weighted contextual merging, with optimal weights correlating with domain item recency characteristics
- The neutral base strategy (averaging same-phase checkpoints from multiple domains) further reduces domain-specific contamination in task vectors, though gains are limited for high-magnitude asymmetries
- Text-grounded paradigms show positive transfer during merging, while semantic-based paradigms exhibit higher conflict severity due to token distribution shifts and objective disparities

## Why This Works (Mechanism)

### Mechanism 1: Task-Aware Parameter Disentanglement via Base Model Replacement
- Claim: Replacing the pretrained LLM base with a task-adapted checkpoint reduces cross-domain merging conflicts.
- Mechanism: Task vectors computed from the original LLM base conflate general recommendation adaptation and domain-specific knowledge. Using a historical checkpoint already fine-tuned on recommendation tasks as the new base yields task vectors that isolate domain-incremental knowledge, reducing destructive interference during merging.
- Core assumption: Task-aware parameter shifts stabilize early in pretraining and remain relatively consistent across domains for the same GR paradigm.
- Evidence anchors: Parameter conflicts due to token distribution shifts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement; task-aware parameter shifts can potentially be disentangled from task vectors, enabling computed task vectors to more specifically reflect domain-incremental knowledge.

### Mechanism 2: Weighted Contextual Merging for Temporal Preference Balance
- Claim: Interpolating between temporal checkpoints with domain-tuned weights balances recency bias against historical preference retention.
- Mechanism: Define a temporal preference shift vector encoding emerging behavioral patterns. Scaling via λ allows controlled trade-offs between historical and latest preferences, with optimal λ correlating with item recency characteristics (shorter average interaction time gap → smaller λ for non-active users).
- Core assumption: Active and non-active users share preference shifts when item turnover is rapid, making domain-level statistics predictive of optimal weights.
- Evidence anchors: Incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging; domains where users interact with newer items show smaller optimal λ values for non-active users, suggesting greater benefit from recent model updates.

### Mechanism 3: Semantic-Aligned Neutral Base Construction
- Claim: Averaging same-phase checkpoints from multiple domains creates a neutral base that further reduces domain-specific contamination in task vectors.
- Mechanism: Compute averaged checkpoints across domains and derive task vectors relative to this merged base. This removes domain-biased parameters that would otherwise be double-counted during merging.
- Core assumption: Simple averaging preserves shared task-aware knowledge while diluting domain-specific signals proportionally.
- Evidence anchors: Results demonstrate further performance improvements with the neutral base strategy, suggesting this approach more effectively separates domain-specific knowledge; however, simple averaged neutral base model brought limited performance gain for domains with large parameter shifts, suggesting potential for more sophisticated strategies.

## Foundational Learning

- **Task Vectors in Model Merging**
  - Why needed here: MMGRid builds on task arithmetic to represent domain and temporal adaptations; without this, you cannot reason about disentanglement or weighted interpolation.
  - Quick check question: If Θ⁽ᴬ⁾ and Θ⁽ᴮ⁾ are two fine-tuned models from base Θ⁽⁰⁾, what does τᴬ = Θ⁽ᴬ⁾ − Θ⁽⁰⁾ represent, and why might τᴬ + τᴮ cause interference?

- **Generative Recommendation Paradigms (Text-Grounded / SemID / SemEmb)**
  - Why needed here: The paper shows paradigm-dependent conflict severity; understanding item representation differences explains why token distribution shifts and objective disparities arise.
  - Quick check question: Why does SemEmb exhibit larger task vector magnitudes and domain variance than Text-Grounded, and how does this affect cross-domain merging?

- **Temporal Dynamics in Sequential Recommendation**
  - Why needed here: The temporal merging mechanism relies on the intuition that incremental training data encodes preference shifts; distinguishing active vs. non-active users and item recency is central to weight selection.
  - Quick check question: In a domain with rapid product turnover (e.g., smartphones), would you expect optimal λₜₑₘₚ for non-active users to be closer to 0 or 1? Why?

## Architecture Onboarding

- **Component map**: Qwen3-0.6B base → GR paradigm training (Text-Grounded/SemID/SemEmb) → 6 domain checkpoints (t0, t1, t2) → Task vector computation → Merging (Weight-based/Subspace-based) → Evaluation (Recall@10/20, NDCG@10/20)

- **Critical path**: 1) Pretrain all domains on t0 data → save Θ⁽ᴰ,ᵗ⁰⁾ 2) Incremental training for P1 and P2 → save Θ⁽ᴰ,ᵗ¹⁾, Θ⁽ᴰ,ᵗ²⁾ 3) For cross-domain merging: choose base model, compute task vectors, apply merging algorithm, evaluate on t2 test set 4) For temporal merging: define τₜₑₘₚ, sweep λₜₑₘₚ, analyze per-domain and per-user-group performance

- **Design tradeoffs**: Historical vs. Neutral Base (simple but domain-biased vs. requires extra storage, may fail under large asymmetries); Weight-based vs. Subspace-based (interpretable λ vs. reduces redundancy but introduces hyperparameters); Paradigm Selection (Text-Grounded merge-friendly but underperforms vs. SemEmb best standalone but highest conflict risk)

- **Failure signatures**: Severe performance degradation (>50%) on one domain after cross-domain merge (likely magnitude imbalance, check L1 norms); Optimal λₜₑₘₚ >> 1 for non-active users (historical knowledge harmful, may need domain-specific fine-tuning); SemID vocabulary mismatch errors (domain-specific IDs scale poorly beyond 2-3 domains)

- **First 3 experiments**: 1) Reproduce baseline conflict patterns: Merge two SemEmb models using equal-weight task arithmetic from original base; confirm degradation matches Figure 3 (≈98-99% drop) 2) Test base model replacement: Repeat merge using t1 checkpoint from one domain as base; sweep α ∈ [0, 1] and plot joint NDCG@20 curves as in Figure 4 3) Temporal weight prediction: For held-out domain, compute average interaction time gap; use leave-one-out linear regression to predict λ̂*ₜₑₘₚ; compare performance against λₜₑₘₚ = 1 baseline using Table 3 metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-linear models incorporating user engagement patterns, item diversity, and seasonal trends predict optimal temporal merging weights more accurately than the proposed linear regression on item recency?
- Basis in paper: Page 9 states the linear regression model may not capture complex nonlinear relationships and "other factors such as user engagement patterns, item diversity, and seasonal trends may also play important roles."
- Why unresolved: The authors restricted their predictive model to a simple linear regression based on average interaction time gaps.
- What evidence would resolve it: A study demonstrating that a multi-feature, non-linear model significantly outperforms the linear baseline in predicting the optimal $\lambda_{temp}$ for non-active users across diverse domains.

### Open Question 2
- Question: How can we develop more sophisticated strategies to disentangle task-aware parameter shifts from domain-specific knowledge beyond simple averaging of historical checkpoints?
- Basis in paper: Page 7 notes that the "simple averaged neutral base model brought limited performance gain" for domains with large parameter shifts, suggesting potential for "more sophisticated strategies to better disentangle."
- Why unresolved: The proposed solution of averaging checkpoints to create a neutral base model failed to fully resolve conflicts in SemEmb paradigms with high-magnitude task vectors.
- What evidence would resolve it: A new algorithm capable of mathematically separating task-adaptive weights from domain-specific knowledge, resulting in merged models that match or exceed single-domain baseline performance in high-conflict paradigms.

### Open Question 3
- Question: Do the observed parameter conflicts and recency bias dynamics persist or change when scaling to larger backbone models (beyond 0.6B parameters) or merging more than two domains simultaneously?
- Basis in paper: The framework relies on Qwen3-0.6B and cross-domain experiments are limited to pairwise merging, leaving scalability to larger LLMs or multi-domain fusion untested.
- Why unresolved: The study focuses on a controlled, smaller-scale decoder-only architecture to ensure consistency across paradigms, but real-world unified recommenders often utilize larger scales.
- What evidence would resolve it: Reproducing the MMGRid experiments on 7B+ parameter models or merging all six domains simultaneously to verify if the magnitude of task vector conflicts scales with model size or complexity.

## Limitations

- Domain Neutrality Assumption: The effectiveness of base model replacement and neutral base averaging relies on the assumption that task-aware parameter shifts are consistent across domains, which may break down for domains with fundamentally different interaction patterns.
- Generalizability of Temporal Weighting: The proposed temporal weighting mechanism assumes domain-level item recency statistics can predict optimal interpolation weights, which may not hold when non-active user preferences diverge significantly from active users within the same domain.
- Scalability of Merging Strategies: While the paper demonstrates effectiveness for merging 2-3 domains, scalability to larger domain collections remains untested, with SemID facing vocabulary explosion and subspace-based methods becoming computationally prohibitive.

## Confidence

- **High Confidence**: The core experimental framework construction and baseline performance measurements are reproducible given the specified dataset splits, model architectures, and training procedures. The identification of cross-domain merging conflicts in GR models and the demonstration of base model replacement benefits are well-supported by the presented results.
- **Medium Confidence**: The temporal-aware merging mechanism and optimal weight prediction based on domain statistics are theoretically sound but rely on assumptions about user preference consistency that require further validation across diverse domain combinations. The neutral base averaging strategy shows promise but lacks systematic evaluation across different magnitude imbalance scenarios.
- **Low Confidence**: The generalizability of findings to domains beyond Amazon Reviews 2023, to other GR paradigms not tested, and to real-world production environments with continuous data streams and dynamic domain relationships remains speculative without additional validation.

## Next Checks

1. **Cross-Paradigm Generalization Test**: Apply the MMGRid framework to a different GR paradigm (e.g., retrieval-based methods like CL4SRec) and evaluate whether the same base model replacement and neutral base strategies provide comparable conflict reduction benefits, or if paradigm-specific adaptations are needed.

2. **Multi-Domain Scaling Experiment**: Systematically evaluate the neutral base averaging strategy when merging 4+ domains, measuring performance degradation as domain count increases and comparing against alternative disentanglement methods like magnitude normalization or domain-weighted averaging.

3. **User-Level Temporal Weighting Validation**: Implement a user-level variant of the temporal weighting mechanism that incorporates individual user activity patterns and preference stability scores, then compare its performance against the domain-level statistics approach across multiple temporal intervals and user segments.