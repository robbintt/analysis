---
ver: rpa2
title: 'AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with
  Unsupervised Coefficient Optimization'
arxiv_id: '2503.23733'
source_url: https://arxiv.org/abs/2503.23733
tags:
- merging
- performance
- adamms
- mllms
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AdaMMS addresses the challenge of merging heterogeneous multimodal
  large language models (MLLMs) with different architectures by introducing a novel
  three-step method: mapping, merging, and searching. The approach first maps parameters
  between models with different architectures, then applies linear interpolation to
  adaptively optimize performance across tasks, and finally uses an unsupervised hyper-parameter
  selection method based on generation consistency to find optimal merging coefficients
  without labeled data.'
---

# AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization

## Quick Facts
- arXiv ID: 2503.23733
- Source URL: https://arxiv.org/abs/2503.23733
- Reference count: 40
- Key outcome: AdaMMS achieves +3.36 average gain over Qwen2-VL and +3.90 over CogVLM across 8 vision-language tasks through unsupervised coefficient optimization without labeled data

## Executive Summary
AdaMMS introduces a novel approach for merging heterogeneous multimodal large language models (MLLMs) with different architectures. The method addresses a critical challenge in model merging by introducing a three-step framework: mapping parameters between architecturally different models, applying adaptive linear interpolation, and using unsupervised hyper-parameter selection based on generation consistency. The approach successfully combines capabilities from distinct MLLMs without requiring labeled data or additional training, achieving significant performance improvements on vision-language benchmarks.

## Method Summary
AdaMMS employs a three-step process to merge heterogeneous MLLMs. First, it maps parameters between models with different architectures using a novel parameter mapping technique. Second, it applies linear interpolation to merge the mapped parameters while optimizing performance across tasks. Third, it uses an unsupervised hyper-parameter selection method based on generation consistency to find optimal merging coefficients without labeled data. The method specifically addresses the challenge of merging models with different vision encoders (CNN vs ViT) and vision-language connectors, demonstrating effectiveness across various model pairs including Qwen2-VL and CogVLM combinations.

## Key Results
- Achieves average gain of +3.36 over Qwen2-VL and +3.90 over CogVLM across 8 vision-language tasks
- Successfully merges models with different vision backbones (CNN vs ViT) and connectors
- Demonstrates effectiveness of unsupervised coefficient optimization without requiring labeled data

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of parameter misalignment between heterogeneous MLLMs. By first mapping parameters between different architectural spaces, AdaMMS creates a common ground for merging. The adaptive linear interpolation then combines these mapped parameters while optimizing for task performance. The unsupervised coefficient optimization using generation consistency as a proxy metric allows the system to find optimal merging weights without expensive labeled data, making the approach practical for real-world applications where labeled data may be scarce or expensive to obtain.

## Foundational Learning

**Parameter Mapping**
- Why needed: Heterogenous MLLMs have different architectural structures requiring alignment before merging
- Quick check: Verify mapped parameters preserve functional relationships between source models

**Generation Consistency**
- Why needed: Provides unsupervised metric for coefficient optimization without labeled data
- Quick check: Correlate generation consistency scores with actual downstream task performance

**Linear Interpolation in Parameter Space**
- Why needed: Combines capabilities from multiple models while preserving learned representations
- Quick check: Validate that interpolated parameters produce coherent intermediate representations

## Architecture Onboarding

**Component Map**
Parameter Mapping -> Linear Interpolation -> Unsupervised Coefficient Optimization

**Critical Path**
The critical path flows through parameter mapping (ensuring architectural alignment), followed by the merging process, and culminating in the unsupervised search for optimal coefficients. Each step depends on successful completion of the previous one.

**Design Tradeoffs**
The method trades computational overhead in the unsupervised search phase for the benefit of not requiring labeled data. This makes it more practical for scenarios where labeled data is expensive or unavailable, but increases inference time during the coefficient optimization phase.

**Failure Signatures**
- Poor mapping quality leading to degraded performance after merging
- Generation consistency metric not correlating with actual task performance
- Suboptimal coefficients leading to catastrophic forgetting of source model capabilities

**First Experiments**
1. Test mapping quality by comparing feature representations before and after mapping
2. Validate generation consistency metric by correlating with labeled benchmark performance
3. Stress-test mapping step with significantly different architectural paradigms (CNN vs transformer-based vision encoders)

## Open Questions the Paper Calls Out
The paper raises several open questions regarding the generalization of the unsupervised coefficient optimization method across different model pairs and task types. It also questions how well the generation consistency metric correlates with actual downstream performance across diverse multimodal applications. The effectiveness of the mapping step across significantly different architectural paradigms is not thoroughly explored, particularly for models with fundamentally different vision backbones.

## Limitations
- Reliance on generation consistency metric may not always correlate with downstream task performance
- Evaluation focused primarily on vision-language benchmarks with limited analysis of other multimodal tasks
- Mapping step effectiveness not thoroughly tested across significantly different architectural paradigms

## Confidence

**High confidence**: The three-step framework structure (mapping, merging, searching) is logically sound and well-motivated

**Medium confidence**: The unsupervised coefficient optimization method shows promise, but its general effectiveness across different model pairs and task types needs more validation

**Medium confidence**: The reported performance gains are significant, but the comparison with baseline methods could be more comprehensive

## Next Checks

1. Test the generation consistency metric's correlation with actual downstream performance across a broader range of multimodal tasks beyond vision-language benchmarks

2. Evaluate AdaMMS on significantly more architecturally divergent model pairs (e.g., models with different vision backbones like CNN vs ViT) to stress-test the mapping step

3. Conduct ablation studies removing the unsupervised search component to quantify its contribution versus simple uniform merging across different model combinations