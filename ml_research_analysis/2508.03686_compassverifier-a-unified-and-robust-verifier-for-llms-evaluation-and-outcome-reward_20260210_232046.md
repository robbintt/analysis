---
ver: rpa2
title: 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome
  Reward'
arxiv_id: '2508.03686'
source_url: https://arxiv.org/abs/2508.03686
tags:
- answer
- arxiv
- verification
- preprint
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CompassVerifier, a unified and robust verifier
  model for evaluating and optimizing large language models (LLMs). The key contributions
  are: (1) VerifierBench, a comprehensive benchmark for answer verification that aggregates
  over 1 million model responses across 15 datasets, and (2) CompassVerifier, a lightweight
  model trained on VerifierBench with three key augmentation techniques: Complex Formula
  Augmentation, Error-Driven Adversarial Augmentation, and Generalizability Augmentation.'
---

# CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward

## Quick Facts
- arXiv ID: 2508.03686
- Source URL: https://arxiv.org/abs/2508.03686
- Authors: Shudong Liu; Hongwei Liu; Junnan Liu; Linchen Xiao; Songyang Gao; Chengqi Lyu; Yuzhe Gu; Wenwei Zhang; Derek F. Wong; Songyang Zhang; Kai Chen
- Reference count: 40
- Primary result: State-of-the-art answer verification model achieving 80.8%-94.8% F1 scores across domains, also serving as effective RL reward model

## Executive Summary
This paper introduces CompassVerifier, a unified verifier model for evaluating and optimizing large language models (LLMs) through answer verification. The key innovation is VerifierBench, a comprehensive benchmark aggregating over 1 million model responses across 15 datasets, and CompassVerifier itself, a lightweight model trained with three augmentation techniques: Complex Formula Augmentation, Error-Driven Adversarial Augmentation, and Generalizability Augmentation. The resulting model achieves state-of-the-art performance across diverse domains and serves dual purposes as both an evaluation tool and reinforcement learning reward model, improving base LLM reasoning capabilities.

## Method Summary
CompassVerifier employs a multi-stage training pipeline starting with VerifierBench data filtered through consensus voting and human annotation. The model uses Qwen2.5 base architectures (3B/7B/32B) trained with three parallel augmentation techniques: generating formula variants using DeepSeek-V3 for mathematical equivalence, creating error-driven adversarial samples from clustered failure modes identified by human experts, and applying generalizability augmentation for cross-prompt robustness. The final training set combines 54,420 base samples with 42,412 augmented samples. During inference, the model performs direct ternary classification (Correct/Incorrect/Invalid) without chain-of-thought reasoning for efficiency, though training utilizes CoT reasoning.

## Key Results
- CompassVerifier-7B achieves state-of-the-art F1 scores ranging from 80.8% (math) to 94.8% (knowledge) across four domains
- Three-augmentation combination yields optimal performance with additive gains: +2.7% accuracy from formula augmentation, +2.4% from error-driven augmentation
- As RL reward model, CompassVerifier-7B improves Qwen3-4B reasoning by 12-18% on AIME24/25 and MATH500 benchmarks
- Generalizability augmentation enables cross-prompt robustness, maintaining >86 F1 with unified prompts versus 70-84 F1 for baselines

## Why This Works (Mechanism)

### Mechanism 1
Error-driven adversarial augmentation systematically improves robustness by encoding failure modes into training data. Domain experts manually verify ~5,000 samples to identify failure rationales, which are density-based clustered into 20+ error categories. Meta-Judge templates guide synthetic data generation targeting decision boundaries. Core assumption: Human-identified error patterns generalize to unseen cases. Evidence: Table 9 shows +2.4% accuracy from this augmentation alone. Break condition: If error patterns are dataset-specific rather than fundamental.

### Mechanism 2
Formula equivalence learning through variant generation reduces false negatives on mathematically complex answers. Reference normalization is followed by DeepSeek-V3 generating 1-3 equivalent variants (symbolic rearrangements, precision-preserving expansions, alternative representations) verified by symbolic algebra tools. Core assumption: LLM can reliably generate mathematically equivalent expressions. Evidence: Table 9 shows +2.7% accuracy from this augmentation. Break condition: If LLM generates pseudo-equivalent variants that pass symbolic checks but fail semantic equivalence.

### Mechanism 3
Multi-stage filtering with progressive difficulty creates training distribution matching deployment challenges. Stage 1 removes trivial consensus cases via multi-expert voting. Stage 2 uses multi-prompt voting with CoT to generate training pool. Stage 3 applies human annotation for disputed cases. Core assumption: Trivial samples add noise while hard samples at decision boundaries provide highest learning signal. Evidence: Figure 2 shows progressive filtering from 1M+ raw responses to 96,832 training samples.

## Foundational Learning

- **Concept:** Outcome vs. Process Verification
  - Why needed: CompassVerifier explicitly targets outcome verification; understanding the distinction clarifies design scope and limitations
  - Quick check: Can you explain why process verifiers remain "less frequently adopted in evaluations" despite recent advances?

- **Concept:** Reinforcement Learning from Verifiable Rewards (RLVR)
  - Why needed: CompassVerifier serves dual purpose as evaluation tool and reward model; Section 5.3 shows GRPO integration
  - Quick check: How does a reward model differ from a process reward model (PRM) in RL training loops?

- **Concept:** LLM-as-a-Judge (Pointwise vs. Pairwise, Subjective vs. Objective)
  - Why needed: Paper positions CompassVerifier within this paradigm; objective judgment with ground truth is the target
  - Quick check: Why might subjective judgment approaches be inappropriate for mathematical answer verification?

## Architecture Onboarding

- **Component map:** VerifierBench Pipeline (OpenCompass collection → Multi-expert voting → Multi-prompt voting → Human annotation → Error pattern catalog) → CompassVerifier Training (Base train set + Error-driven augmentation + Formula augmentation = 96,832 total) → Three augmentation modules feeding unified training pool → Direct judgment inference

- **Critical path:** Data quality determines verifier ceiling—multi-stage filtering is non-negotiable; error pattern analysis drives augmentation effectiveness; formula normalization quality gates mathematical domain performance; invalid response detection prevents reward hacking in RL

- **Design tradeoffs:** Scale vs. specialization (3B model outperforms GPT-4.1 +10.6% F1 but requires domain-specific training); CoT vs. direct judgment (training uses CoT reasoning; inference uses direct judgment for speed); Generalizability vs. prompt-dependence (augmentation enables cross-prompt robustness but increases training complexity)

- **Failure signatures:** Hallucination in formula equivalence (LLM accepts semantically different expressions); Over-strict rejection (false negatives on valid but unconventional answer formats); Invalid response insensitivity (Category C detection remains weakest); Domain-specific degradation (Math F1 80.8% significantly lower than Knowledge 94.8%)

- **First 3 experiments:**
  1. Reproduce ablation (Table 9): Train CompassVerifier-7B-Base → add formula augmentation → add error-driven augmentation → verify additive gains (+2.7%, +2.4%, +3.5% accuracy)
  2. Test prompt robustness (Table 2 protocol): Evaluate on VerifyBench hard subset using both model-specific and unified prompts; CompassVerifier should maintain >86 F1 with unified prompt
  3. Validate RL reward signal (Figure 6 protocol): Train Qwen3-4B-Base with GRPO using CompassVerifier-7B vs. Math-Verify on Open-S1 dataset; target +12-18% avg@32 on AIME24/25

## Open Questions the Paper Calls Out

**Open Question 1:** How can robust answer verification be extended to non-binary judgment cases, such as proof-based questions and open-ended problems with ambiguous acceptability thresholds? The current model architecture and training data rely on discrete labels (Correct, Incorrect, Invalid) derived from clear ground truths, lacking a mechanism to handle the spectrum of validity or logical rigor required for proofs.

**Open Question 2:** Can the efficiency and stability of outcome verification be combined with the granularity of process verification without sacrificing performance? The paper focuses exclusively on outcome verification to ensure scalability, leaving the integration of intermediate step checking within this lightweight framework unexplored.

**Open Question 3:** Does CompassVerifier's effectiveness as a reward model generalize to improving reasoning capabilities in non-mathematical domains during reinforcement learning? While the abstract claims multi-domain competency, validation occurs exclusively using mathematical reasoning benchmarks, leaving uncertainty about reward signal precision for "knowledge" or "general reasoning" domains.

## Limitations
- Data quality dependency: Performance hinges on VerifierBench quality, with filtering criteria underspecified and potential for over-aggressive filtering removing distribution diversity
- Formula equivalence reliability: Complex Formula Augmentation assumes DeepSeek-V3 can generate mathematically equivalent variants that symbolic verification tools will accept, but mathematical verification remains challenging (80.8% F1)
- Error pattern generalizability: Error-driven augmentation clusters ~5,000 human-verified samples into 20+ error categories, assuming these represent fundamental failure modes rather than dataset-specific artifacts

## Confidence

**High Confidence (Evidence > 3 anchors):**
- CompassVerifier achieves state-of-the-art performance on VerifierBench
- Formula augmentation provides measurable improvement (+2.7% accuracy)
- Error-driven augmentation adds consistent gains (+2.4% accuracy)
- Verifier serves as effective RL reward model

**Medium Confidence (Evidence 1-2 anchors):**
- Generalizability augmentation enables cross-prompt robustness
- Error pattern analysis reveals systematic failure modes
- Prompt-dependent vs. general verifiers show meaningful performance gaps

**Low Confidence (Evidence < 1 anchor):**
- Three-augmentation combination achieves optimal performance without interference effects
- Formula complexity is the sole bottleneck for mathematical domain performance
- VerifierBench fully represents real-world verification challenges

## Next Checks

1. **Formula Equivalence Stress Test:** Generate 1,000 math problems with human-verified answers, create DeepSeek-V3 variants, and run CompassVerifier-7B evaluation. Compare against ground truth to quantify false acceptance rate for pseudo-equivalent expressions. Target: <5% false acceptance rate.

2. **Error Pattern Transferability Analysis:** Take 200 samples from datasets outside VerifierBench and run error-driven augmentation pipeline. Measure how many error categories from VerifierBench apply to these new samples versus novel error types discovered. Target: >80% overlap in error categories.

3. **RL Reward Model Robustness Test:** Train Qwen3-4B-Base with GRPO using CompassVerifier-7B reward on three datasets: VerifierBench math subset, MATH500, and AIME24/25. Measure reward consistency across datasets and compare reasoning performance improvements. Target: consistent reward signal with <15% variance across domains.