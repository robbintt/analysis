---
ver: rpa2
title: 'KnowThyself: An Agentic Assistant for LLM Interpretability'
arxiv_id: '2511.03878'
source_url: https://arxiv.org/abs/2511.03878
tags:
- language
- knowthyself
- interpretability
- user
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KnowThyself is an agentic assistant for LLM interpretability that
  consolidates fragmented tools into a conversational interface, enabling users to
  upload models and query them using natural language. It uses an orchestrator LLM
  to reformulate queries, an embedding-based router to dispatch them to specialized
  agents, and produces interactive visualizations with natural language explanations.
---

# KnowThyself: An Agentic Assistant for LLM Interpretability

## Quick Facts
- arXiv ID: 2511.03878
- Source URL: https://arxiv.org/abs/2511.03878
- Authors: Suraj Prasai; Mengnan Du; Ying Zhang; Fan Yang
- Reference count: 25
- Primary result: Agentic conversational interface for LLM interpretability that routes natural language queries to specialized visualization and analysis tools

## Executive Summary
KnowThyself is an agentic assistant that consolidates fragmented LLM interpretability tools into a conversational interface. It uses an orchestrator LLM to reformulate natural language queries, an embedding-based router to dispatch them to specialized agents, and produces interactive visualizations with natural language explanations. The system lowers technical barriers by abstracting complex tool interactions behind a chat-based frontend, supporting extensibility through modular agent encapsulation.

## Method Summary
The system implements a LangGraph-based multi-agent orchestration pipeline where Gemma3-27B serves as the orchestrator LLM for query reformulation and output contextualization. An embedding-based router using nomic-embed-text via Ollama dispatches queries to specialized agents including BertViz (attention visualization), TransformerLens (activation analysis), RAG-based literature grounding, and bias evaluation tools. The architecture supports modular agent integration through isolated dependencies and shared state coordination, enabling users to upload custom models and query them using natural language.

## Key Results
- Consolidates multiple interpretability tools (BertViz, TransformerLens, bias evaluation) into unified conversational interface
- Uses embedding-based routing to automatically select appropriate specialized agents from natural language queries
- Supports interactive visualizations with natural language explanations and extensible modular design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding-based routing enables appropriate tool selection from natural language queries.
- Mechanism: User queries are encoded and compared against agent descriptions via similarity search; the closest match is selected for dispatch.
- Core assumption: User intent can be captured adequately by embedding similarity to static agent descriptions.
- Evidence anchors:
  - [abstract] "an embedding-based router to dispatch them to specialized agents"
  - [section 2] "The router dispatches queries to specialized agents using embedding-based similarity search to match user intent with agent descriptions."
  - [corpus] No direct corpus validation; neighboring papers focus on agentic systems broadly, not routing mechanisms.
- Break condition: Ambiguous or multi-intent queries that map poorly to single agent descriptions may route incorrectly.

### Mechanism 2
- Claim: Orchestrator reformulation bridges natural language inputs and specialized tool interfaces.
- Mechanism: The orchestrator LLM parses user queries, identifies missing parameters, and synthesizes required inputs (e.g., example sentences) before dispatching to agents; it then contextualizes outputs into coherent explanations.
- Core assumption: The orchestrator model possesses sufficient reasoning capability to infer missing inputs and summarize technical outputs accurately.
- Evidence anchors:
  - [abstract] "an orchestrator LLM first reformulates user queries... outputs are finally contextualized into coherent explanations"
  - [section 4] "The Orchestrator supplies required inputs by synthesizing a sentence... when no input is provided."
  - [corpus] "Because we have LLMs, we Can and Should Pursue Agentic Interpretability" supports LLM-driven interpretability but does not validate this specific orchestration pattern.
- Break condition: Complex queries requiring domain knowledge outside the orchestrator's training may produce incomplete reformulations.

### Mechanism 3
- Claim: Modular agent encapsulation with isolated dependencies enables extensibility without disrupting core components.
- Mechanism: Each interpretation tool (e.g., TransformerLens, BertViz) is wrapped as an independent agent with its own dependencies; the system uses LangGraph's shared state model for coordination.
- Core assumption: Tools can be abstracted into consistent agent interfaces without losing critical functionality.
- Evidence anchors:
  - [abstract] "modular design supports extensibility"
  - [section 3] "By isolating these dependencies, new tools can be integrated without disrupting the system."
  - [corpus] Weak direct validation; corpus papers discuss agentic architectures but not this specific modular isolation strategy.
- Break condition: Tools with conflicting dependencies or non-standard interfaces require additional engineering to adapt.

## Foundational Learning

- Concept: Transformer attention mechanisms and head-level analysis
  - Why needed here: BertViz and TransformerLens agents visualize attention patterns; understanding attention is prerequisite to interpreting their outputs.
  - Quick check question: Can you explain what an attention head computes and how to read an attention map?

- Concept: Embedding-based similarity search and vector retrieval
  - Why needed here: The agent router and RAG agent both rely on embedding similarity; understanding FAISS and dense retrieval is essential.
  - Quick check question: Given two text embeddings, how would you compute their semantic similarity?

- Concept: LLM bias evaluation metrics (toxicity, regard, HONEST)
  - Why needed here: BiasEval agent reports these scores; users must understand what they measure and their limitations.
  - Quick check question: What does the "regard" metric capture, and what are its known failure modes?

## Architecture Onboarding

- Component map:
  - **Orchestrator LLM** (Gemma3-27B) -> **Agent Router** (embedding-based) -> **Specialized Agents** (BertViz, TransformerLens, RAG, BiasEval) -> **Conversational Interface** (chat frontend with visualization rendering)

- Critical path:
  1. User submits query → 2. Orchestrator reformulates and identifies subtasks → 3. Router computes embedding similarity against agent descriptions → 4. Selected agent executes analysis → 5. Orchestrator contextualizes output → 6. Interface renders visualization + explanation

- Design tradeoffs:
  - **Embedding vs. LLM routing**: Current embedding-based approach is faster but less adaptive; LLM-based routing is planned for complex cases.
  - **Local vs. API serving**: Local execution via Ollama ensures privacy but requires sufficient GPU resources; large models may not fit.
  - **Pre-included models vs. custom uploads**: Demo models (GPT-2, BERT, LLaMA2-13B) are pre-configured; custom models require compatible architectures.

- Failure signatures:
  - Router selects wrong agent for ambiguous queries → check embedding similarity scores, consider fallback to LLM-based routing.
  - Orchestrator fails to synthesize required inputs → verify query contains sufficient context; may need explicit user clarification.
  - Visualization rendering errors → confirm frontend dependencies; TransformerLens outputs may require specific rendering libraries.

- First 3 experiments:
  1. Run the two use cases from Figure 1 (attention visualization and bias evaluation) on a pre-included model to validate end-to-end pipeline.
  2. Upload a custom HuggingFace model checkpoint and query attention patterns; verify model compatibility with TransformerLens/BertViz.
  3. Add a new specialized agent (e.g., a feature attribution method) by wrapping it in the agent interface and registering its description for routing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can routing precision be improved when user queries overlap multiple specialized agents?
- Basis in paper: [explicit] The authors state future work will "improve routing precision for overlapping tasks."
- Why unresolved: The current embedding-based similarity router may struggle with ambiguous queries that could match multiple agents (e.g., a question about bias in attention patterns), and no evaluation of routing accuracy is provided.
- What evidence would resolve it: Benchmarking routing accuracy on ambiguous queries; comparing embedding-based vs. LLM-based routing on a test set of overlapping tasks.

### Open Question 2
- Question: What architectural modifications are required to extend the system to multimodal models?
- Basis in paper: [explicit] The authors identify that the system "supports text inputs exclusively" and plan to "extend support to multimodal models."
- Why unresolved: The current agent design encapsulates text-based tools (BertViz, TransformerLens); supporting vision/audio would require new visualization paradigms and possibly different interpretability methods.
- What evidence would resolve it: A prototype multimodal agent implementation; user evaluation showing non-text inputs can be interpreted through the same conversational workflow.

### Open Question 3
- Question: How does the quality of orchestrator-generated explanations affect user understanding compared to raw tool outputs?
- Basis in paper: [inferred] The paper claims the orchestrator "contextualizes intermediate results" into "coherent natural language explanations," but no user study validates that these explanations improve comprehension over direct visualization.
- Why unresolved: Without empirical evaluation, it remains unclear whether the abstraction layer helps or potentially introduces hallucinations or oversimplifications.
- What evidence would resolve it: A controlled user study measuring task completion accuracy and subjective understanding when using KnowThyself versus direct tool access.

### Open Question 4
- Question: How does system performance scale as more specialized agents are integrated?
- Basis in paper: [inferred] The modular architecture claims to support "seamless integration of new tools," but only four agents are currently implemented; routing latency and state management with dozens of agents is untested.
- Why unresolved: Embedding-based routing involves similarity search across agent descriptions; as the agent pool grows, retrieval efficiency and disambiguation complexity may degrade.
- What evidence would resolve it: Performance benchmarks measuring routing time and accuracy as the number of agents increases from 4 to 20+.

## Limitations

- The paper lacks quantitative evaluation metrics to assess routing accuracy, response quality, or usability improvements over existing tools.
- Claims about lowering technical barriers are qualitative assertions without user studies or comparative evaluations.
- System performance and scalability under load with large models are not characterized.

## Confidence

- **High Confidence:** The core architectural design (orchestrator-router-agent pattern) is clearly specified and technically sound. The integration of established interpretability tools (BertViz, TransformerLens) with LLM-driven query processing follows standard patterns in agentic systems.
- **Medium Confidence:** The embedding-based routing mechanism is plausible given current vector similarity techniques, but its effectiveness across diverse query types remains unverified without quantitative benchmarks.
- **Low Confidence:** Claims about lowering technical barriers and improving interpretability workflows are qualitative assertions without user studies or comparative evaluations against baseline tools.

## Next Checks

1. **Routing Accuracy Benchmark:** Design a test suite of 100 diverse interpretability queries spanning attention, bias, and activation analysis. Measure the embedding-based router's accuracy against human-annotated correct agent assignments, and compare against a baseline LLM-based router.
2. **Performance Under Load:** Measure end-to-end query latency and memory usage when processing concurrent requests with large models (LLaMA2-13B) through Ollama, identifying bottlenecks in the orchestration pipeline.
3. **Tool Integration Stress Test:** Attempt to integrate three additional interpretability tools with conflicting dependencies (e.g., different PyTorch versions or incompatible visualization libraries) to validate the modular isolation claims and identify edge cases requiring architectural modifications.