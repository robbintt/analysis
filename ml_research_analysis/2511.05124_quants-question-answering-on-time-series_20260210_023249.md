---
ver: rpa2
title: 'QuAnTS: Question Answering on Time Series'
arxiv_id: '2511.05124'
source_url: https://arxiv.org/abs/2511.05124
tags:
- time
- series
- question
- answer
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QuAnTS, a novel dataset for question answering
  on time series data, specifically focusing on human motion trajectories. The dataset
  comprises 150,000 samples, each consisting of a numerical time series of synthetic
  human motion tracking data, a textual question, and an answer.
---

# QuAnTS: Question Answering on Time Series

## Quick Facts
- arXiv ID: 2511.05124
- Source URL: https://arxiv.org/abs/2511.05124
- Authors: Felix Divo; Maurice Kraus; Anh Q. Nguyen; Hao Xue; Imran Razzak; Flora D. Salim; Kristian Kersting; Devendra Singh Dhami
- Reference count: 40
- Primary result: Current TS-LLMs struggle with TSQA tasks; neuro-symbolic pipelines with reliable action recognition can achieve near-human performance

## Executive Summary
QuAnTS introduces a novel dataset for question answering on time series data, specifically focusing on human motion trajectories. The dataset comprises 150,000 samples of synthetic 3D skeleton motion data paired with textual questions and answers across descriptive, temporal, and comparison categories. The authors evaluate baseline models including naive LLMs, ChatTS, and a neuro-symbolic pipeline (xQA), demonstrating that current models struggle with the challenging TSQA tasks. Human performance provides a reference for gauging practical usability. The dataset aims to foster research in the emerging field of time series question answering.

## Method Summary
The QuAnTS dataset is constructed using synthetic human motion data generated by STMC (Spatio-Temporal Motion Collage), a diffusion-based motion generator. The pipeline samples from 46 predefined question templates across three categories (descriptive, temporal, comparison), applies LLM-assisted paraphrasing, and undergoes manual review. Questions are paired with 320×24×3 numerical time series representing 3D joint positions over time. Three answer types are supported: binary, multiple-choice, and open-ended. Evaluation uses accuracy/F1 for binary/multi answers and ROUGE/METEOR/LLMJudge for open answers. The LLMJudge employs Qwen3 8B-AWQ with Reasoning, validated against human judgment with Pearson correlation of 0.912.

## Key Results
- Naive LLM baseline shows 36.78% unparsable responses and near-random accuracy (48.49% binary, 9.69% multi)
- ChatTS performs similarly poorly despite time-series tokenization, suggesting current TS-LLMs lack robust perception capabilities
- xQA with ground-truth labels achieves F1 scores of 78-88%, approaching human performance
- Ablation studies confirm questions alone are insufficient for accurate answers (~55% binary accuracy)
- Human performance provides reference for practical usability of TSQA models

## Why This Works (Mechanism)

### Mechanism 1
Neuro-symbolic separation of perception and reasoning can achieve near-human performance on time series QA when action recognition is reliable. The xQA pipeline factorizes the problem: an xLSTM-Mixer encoder segments and classifies 4-action sequences into discrete labels; a separate instruction-tuned LLM then performs reasoning over this symbolic sequence using few-shot prompting and constrained JSON generation. Core assumption: the supervised action encoder generalizes to unseen sequences with comparable accuracy. Evidence: xQA with ground-truth labels achieves F1 ~78-88% on binary/multi, approaching human performance (~77-86%). Break condition: If action sequences become longer, overlapping, or more fine-grained beyond the 19-class vocabulary, encoder accuracy may degrade, causing cascading reasoning failures.

### Mechanism 2
Template-based question generation with paraphrasing ensures coverage of core temporal reasoning skills while maintaining surface diversity. The construction pipeline procedurally samples from 46 predefined question templates across descriptive, temporal, and comparison categories, with LLM-assisted paraphrasing and manual review to increase linguistic diversity without altering underlying reasoning requirements. Core assumption: The identified question categories are sufficient to evaluate fundamental time series reasoning. Evidence: The questions are diverse, covering descriptive, temporal, and comparison aspects. Break condition: Models may exploit template regularities rather than learning robust temporal reasoning; performance may not generalize to natural, unconstrained questions.

### Mechanism 3
LLM-as-judge with constrained ordinal output provides a scalable semantic evaluation metric that correlates with human judgment for open-ended TSQA. Qwen3 8B-AWQ with Reasoning is prompted to score answers on a 3-level scale (1=terrible, 2=bad, 3=good) with brief rationale, using SGLang for constrained JSON output. Validation on 200 human-annotated samples shows Pearson correlation of 0.912 with mean human ratings. Core assumption: The validation subset is representative of the full dataset distribution. Evidence: We finally chose Qwen3 (8B-AWQ with Reasoning) due to its sufficient inference speed at excellent alignment with human judgment. Break condition: If models learn to game the judge, LLMJudge scores may inflate without genuine quality gains; periodic human re-validation is required.

## Foundational Learning

- **Time Series Question Answering (TSQA) as multimodal reasoning**
  - Why needed: TSQA requires jointly processing numerical multivariate time series (320×72 dimensions) and natural language questions, demanding cross-modal alignment beyond traditional forecasting or classification
  - Quick check: Given a time series of joint positions and a question "What happened after the person bowed?", what two modalities must be aligned and what intermediate representation might bridge them?

- **Neuro-symbolic architecture design**
  - Why needed: xQA's success shows that explicitly separating neural perception (action recognition) from symbolic reasoning (LLM-based QA) can outperform end-to-end multimodal models when structured intermediate representations are available
  - Quick check: For a neuro-symbolic TSQA system, what are the inputs and outputs of the perception module versus the reasoning module?

- **Diffusion-based synthetic data generation for controlled evaluation**
  - Why needed: QuAnTS uses STMC to generate synthetic human motions conditioned on action sequences, enabling precise control over ground-truth labels and question-answer pairs
  - Quick check: What are two advantages and one limitation of using diffusion-generated synthetic data versus collecting real motion capture data for TSQA?

## Architecture Onboarding

- **Component map**: Time Series Generator (STMC) -> QA Template Engine -> Action Encoder (xLSTM-Mixer) -> Reasoning LLM (Llama 3.1 / Qwen3) -> Evaluator

- **Critical path**: Action Encoder Accuracy → Symbolic Sequence Quality → LLM Reasoning Accuracy. xQA performance on ground-truth labels (F1 78-88%) versus action encoder predictions (comparable) confirms encoder reliability is the bottleneck; naive LLM and ChatTS fail at perception, not reasoning.

- **Design tradeoffs**:
  - Synthetic vs. real data: Synthetic enables 150K samples with perfect labels and balanced question types but may not capture real-world motion variability or sensor noise
  - Template vs. free-form questions: Templates guarantee coverage of 46 reasoning types but constrain linguistic diversity; paraphrasing partially mitigates this
  - Neuro-symbolic vs. end-to-end: Neuro-symbolic enables interpretability and modular debugging but requires supervised action labels unavailable at inference; end-to-end (ChatTS) avoids this dependency but currently underperforms significantly

- **Failure signatures**:
  - Naive LLM baseline: 36.78% unparsable responses; near-random accuracy (48.49% binary, 9.69% multi)
  - ChatTS: Similar near-random performance despite time-series tokenization; suggests current TS-LLMs lack robust perception capabilities
  - Ablation "Only Question": ~55% binary accuracy, ~64% multi—above random but far below full-context performance, confirming questions alone are insufficient
  - Human confusion pairs: "catching a ball" ↔ "throwing a ball" suggests perceptual ambiguity in motion data

- **First 3 experiments**:
  1. Reproduce ablation baselines: Fine-tune Llama 3.1 8B on "Only Question" and "Only TS" conditions to verify no spurious correlations; confirm F1 < 65% for all variants
  2. Perturbation study on xQA: Systematically corrupt action encoder predictions (e.g., 10%, 20%, 30% label noise) to quantify the perception-reasoning coupling strength; plot F1 degradation curves
  3. Cross-generalization test: Evaluate ChatTS and xQA on a held-out subset with longer sequences (6 actions instead of 4) or additional action classes to assess scalability beyond training distribution

## Open Questions the Paper Calls Out

- How can we design architectures that enable pre-trained models to perform zero-shot reasoning on time series without specific retraining? The conclusion states, "substantially more work is needed to equip pre-trained models with the capabilities to reason about time series in zero-shot settings." Current baselines fail to generalize effectively, performing barely above random guessing on the dataset.

- Can conversational abilities in TSQA be fully retained from instruction-tuned models, or do they require dedicated multi-turn datasets? The authors ask, "It remains unclear to what degree this requires dedicated datasets or whether conversational abilities can be fully retained from instruction-tuned models." The current work focuses on single-turn QA, leaving the challenge of maintaining context over longer interactions unresolved.

- How can models bridge the performance gap between naive LLMs and neuro-symbolic pipelines without relying on ground-truth action labels? The xQA baseline succeeded only by using ground-truth action labels for segmentation, which the authors note are "generally not available when solving QuAnTS." Current end-to-end models struggle to identify actions ("perception"), whereas the successful neuro-symbolic approach relies on external supervision unavailable in the standard setting.

## Limitations

- Dataset relies on synthetic data that may not capture real-world motion variability and sensor noise characteristics
- Template-based question generation may introduce systematic biases that models can exploit rather than demonstrating genuine temporal reasoning
- Neuro-symbolic approach's performance is fundamentally bounded by action recognition accuracy, which may not be achievable in real-world deployment

## Confidence

- **High Confidence (8-10/10)**: Dataset construction methodology and evaluation metrics are well-specified and reproducible; baseline performance results for naive LLM and ChatTS are reliable indicators of current TS-LLMs' limitations
- **Medium Confidence (5-7/10)**: Neuro-symbolic pipeline's near-human performance with ground-truth labels suggests the dataset is solvable with reliable action recognition, but this upper bound may not be achievable in practice; template-based question generation provides sufficient coverage of temporal reasoning skills
- **Low Confidence (1-4/10)**: LLM-as-judge's scalability and resistance to gaming strategies over time remains unproven; synthetic data's ability to capture real-world motion variability is uncertain

## Next Checks

1. **Generalization Stress Test**: Evaluate xQA and ChatTS on sequences with 6+ actions (vs. 4 in training) and additional action classes beyond the 19-class vocabulary to assess scalability and robustness to distribution shifts

2. **Human-in-the-Loop Judge Validation**: Periodically resample 200 new samples for human annotation to verify that LLMJudge correlations remain stable over time and don't degrade as models potentially learn to game the evaluation criteria

3. **Real-World Data Transfer**: Test the best-performing models on a small real human motion capture dataset (e.g., CMU Graphics Lab Motion Capture Database) to assess synthetic-to-real generalization and identify domain gaps