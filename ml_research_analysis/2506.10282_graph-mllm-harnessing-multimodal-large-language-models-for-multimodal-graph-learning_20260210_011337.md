---
ver: rpa2
title: 'Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph
  Learning'
arxiv_id: '2506.10282'
source_url: https://arxiv.org/abs/2506.10282
tags:
- image
- information
- text
- multimodal
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Graph-MLLM, a comprehensive benchmark designed\
  \ to evaluate multimodal graph learning (MMGL) using multimodal large language models\
  \ (MLLMs). The benchmark categorizes existing MMGL approaches into three paradigms\u2014\
  MLLM-as-Encoder, MLLM-as-Aligner, and MLLM-as-Predictor\u2014and systematically\
  \ evaluates them across six datasets spanning e-commerce and social network domains."
---

# Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning

## Quick Facts
- **arXiv ID:** 2506.10282
- **Source URL:** https://arxiv.org/abs/2506.10282
- **Reference count:** 40
- **Primary result:** Fine-tuning MLLMs as standalone predictors achieves state-of-the-art performance on multimodal graph learning, even without explicit graph structure information.

## Executive Summary
Graph-MLLM introduces a comprehensive benchmark for evaluating multimodal graph learning (MMGL) using multimodal large language models (MLLMs). The benchmark categorizes MMGL approaches into three paradigms—MLLM-as-Encoder, MLLM-as-Aligner, and MLLM-as-Predictor—and systematically evaluates them across six datasets spanning e-commerce and social network domains. Extensive experiments reveal that fine-tuning MLLMs as standalone predictors delivers the most significant performance gains, even without explicit graph structure, highlighting their potential as powerful backbones for MMGL. The study also shows that combining visual and textual attributes consistently benefits graph learning, and that structure-aware augmentation methods improve performance, especially in denser graphs. However, effectiveness depends heavily on dataset properties such as graph density and multimodal quality. The benchmark library is open-sourced to support equitable evaluation and inspire future research in this emerging field.

## Method Summary
Graph-MLLM evaluates three MLLM paradigms for node classification on multimodal graphs. The MLLM-as-Encoder approach uses pre-trained models like CLIP to generate multimodal embeddings fed into GNNs. The MLLM-as-Aligner paradigm employs MLLMs to convert visual attributes to text descriptions, which are then processed by GraphLLMs like GraphPrompter. The MLLM-as-Predictor method fine-tunes MLLMs directly with LoRA for multimodal node classification. The benchmark includes six datasets (Movies, Toys, Grocery, Arts, CDs, Reddit) with text and image node features. Performance is measured using accuracy across various model configurations and structure-aware or non-structure-aware settings.

## Key Results
- Fine-tuning MLLMs as standalone predictors achieves state-of-the-art results in most scenarios, even without explicit graph structure information
- Jointly considering visual and textual attributes benefits graph learning, even when using pre-trained text-to-image alignment models (e.g., CLIP) as encoders
- Effectiveness of structure-aware augmentation methods varies significantly with graph density—improving performance in dense graphs but degrading it in sparse ones

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Feature Fusion
- **Claim:** Multimodal feature fusion consistently improves graph learning performance over unimodal inputs.
- **Mechanism:** Pre-trained multimodal alignment models (e.g., CLIP) generate complementary text and image embeddings that capture distinct semantic aspects of nodes. When concatenated and fed into GNNs, these richer representations enable better neighbor aggregation and message passing, improving downstream task accuracy.
- **Core assumption:** Visual and textual modalities encode non-redundant, task-relevant information about nodes.
- **Evidence anchors:** [abstract] "jointly considering the visual and textual attributes of the nodes benefits graph learning, even when using pre-trained text-to-image alignment models (e.g., CLIP) as encoders"; [section] Table 1a shows Text+Image inputs achieve highest accuracy for MLP in 5/6 datasets, with +17.39% gain on Reddit and +26.28% on Grocery over image-only; [corpus] Limited direct support; MLaGA paper confirms multimodal graph integration benefits but focuses on LLM-assisted approaches.
- **Break condition:** When modalities contain contradictory signals or when image quality is poor (e.g., Movies dataset with lowest image quality), fusion may introduce noise that degrades performance.

### Mechanism 2: Fine-tuning MLLMs as Standalone Predictors
- **Claim:** Fine-tuning MLLMs as standalone predictors achieves state-of-the-art performance even without explicit graph structure information.
- **Mechanism:** Fine-tuning adapts the pre-trained multimodal understanding capabilities to specific graph domains through instruction tuning. The model learns to leverage implicit patterns in node attributes and labels, compensating for lack of explicit structural information through strong semantic reasoning.
- **Core assumption:** MLLMs have sufficient representational capacity to infer domain-specific patterns from multimodal node attributes alone.
- **Evidence anchors:** [abstract] "fine-tuning MLLMs on specific MMGs can achieve state-of-the-art results in most scenarios, even without explicit graph structure information"; [section] Table 3 shows Qwen-VL-7B achieves +41.62% gain on Movies (12.56%→54.18%), +44.61% on Grocery, +38.44% on Reddit after fine-tuning; [corpus] Weak corpus evidence; limited prior work directly evaluates fine-tuned MLLMs for graph prediction tasks.
- **Break condition:** When graph topology is essential for prediction (e.g., dense graphs like CDs with avg. degree 47 benefit from structure awareness) and cannot be inferred from attributes alone.

### Mechanism 3: Converting Visual Attributes to Textual Descriptions
- **Claim:** Converting visual attributes into textual descriptions improves GraphLLM performance compared to direct visual input.
- **Mechanism:** Image-to-text transformation (via MLLM summarization) aligns visual information with the LLM's pre-training distribution, reducing the modality gap. This enables GraphLLMs—designed primarily for text—to process visual semantics through their stronger language understanding capabilities.
- **Core assumption:** The textual description generation preserves task-relevant visual information without excessive information loss.
- **Evidence anchors:** [abstract] "converting visual attributes into textual descriptions further improves performance compared to directly using visual inputs"; [section] Figure 2 and Table 2 show GraphPrompter achieves gains in 4/6 datasets with aligner augmentation; Finding 4 notes CDs dataset gains of 5.79% with GraphPrompter; [corpus] Vision Enhancing LLMs paper supports mechanism: "map visual information into language representation space, leveraging the vast knowledge and powerful text generation abilities of LLMs".
- **Break condition:** When visual details are critical and cannot be adequately captured in concise text summaries, or when summarization introduces noise.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) and Message Passing**
  - **Why needed here:** Understanding how GNNs aggregate neighbor information (Eq. 1: h_v^(l+1) = ψ(ϕ(h_v'^l : v'∈N(v)), h_v^l)) is essential for interpreting why multimodal embeddings improve GNN-based methods.
  - **Quick check question:** Can you explain why richer node embeddings (from multimodal fusion) would improve message passing quality?

- **Concept: Contrastive Learning for Multimodal Alignment**
  - **Why needed here:** The paper uses contrastive objectives (Eq. 5) for structure-aware encoder training to align text and image embeddings while incorporating graph neighborhood signals.
  - **Quick check question:** How does maximizing cosine similarity between image and text embeddings enable cross-modal retrieval?

- **Concept: Instruction Tuning for LLMs/MLLMs**
  - **Why needed here:** Fine-tuning MLLMs with task-specific prompts (Tables 8-9) is the mechanism behind the Predictor paradigm's success.
  - **Quick check question:** What is the difference between zero-shot inference and instruction fine-tuning in the context of MLLMs?

## Architecture Onboarding

- **Component map:**
  - **Encoder track:** CLIP/CLIP-F/CLIP-F-S → GNN models (GCN, GraphSAGE, MMGCN, MGAT, UniGraph2) → classification head
  - **Aligner track:** MLLM (Qwen-VL) image summarization → text synthesis (with/without neighbor info) → GraphLLM (GraphPrompter, LLaGA, GraphGPT, MLaGA) → prediction
  - **Predictor track:** MLLM (Qwen-VL, LLaVA) with multimodal prompts (with/without structure) → LoRA fine-tuning → direct prediction

- **Critical path:** For fastest results, start with Predictor paradigm (Qwen-VL fine-tuning) as it shows highest performance gains with simplest pipeline—no GNN training or complex alignment required.

- **Design tradeoffs:**
  - **Encoder:** Lower compute (no LLM inference during training), but requires GNN expertise and structure-aware encoders show inconsistent gains
  - **Aligner:** Enables use of text-only GraphLLMs, but augmentation effectiveness is model-dependent (LLaGA degrades in 4/6 datasets) and dataset-specific
  - **Predictor:** Highest performance potential, but requires GPU memory for 7B parameter MLLM and LoRA fine-tuning infrastructure

- **Failure signatures:**
  - Sparse graphs (Toys: avg. degree 12): Structure-aware methods degrade performance
  - Poor image quality (Movies dataset): Visual inputs undermine all methods
  - Dense graphs (CDs: avg. degree 47): Non-structure methods underperform—explicit structure needed
  - Model-architecture mismatch: LLaGA relies on embeddings, degrades with prompt-level augmentation

- **First 3 experiments:**
  1. **Establish baseline:** Run MLP with text-only, image-only, and text+image (CLIP concatenated) on 2-3 datasets to confirm multimodal fusion benefit (expect +10-25% gain from fusion)
  2. **Validate Predictor promise:** Fine-tune Qwen-VL-7B (non-structure-aware) on single dataset using LoRA, compare zero-shot vs. fine-tuned accuracy (expect 3-4× improvement)
  3. **Diagnose structure sensitivity:** Test structure-aware vs. non-structure-aware fine-tuning on CDs (dense) vs. Toys (sparse) to confirm Finding 9—that structure awareness effectiveness depends on graph density

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can graph structural information be seamlessly integrated into MLLM reasoning beyond simple textual augmentation?
- **Basis in paper:** [explicit] Finding 7 states that injecting structure into MLLM prompts is "not sufficient" and emphasizes the "need for more advanced methods to seamlessly integrate graph structure."
- **Why unresolved:** Current structure-aware fine-tuning yields only modest gains (less than 1%) and can introduce noise, failing to leverage topological cues effectively compared to non-structure baselines.
- **What evidence would resolve it:** Development of specialized graph attention adapters or architectural modifications within the MLLM that significantly outperform text-only neighbor prompting.

### Open Question 2
- **Question:** How can MMGL methods be adapted to maintain effectiveness on sparse graphs where current structural awareness degrades performance?
- **Basis in paper:** [explicit] Finding 9 highlights that in sparse graphs like the Toys dataset, "structure awareness not only fails to provide benefits but may even degrade performance."
- **Why unresolved:** Standard aggregation methods likely amplify noise or lack sufficient signal in low-degree environments, making modality-only approaches superior.
- **What evidence would resolve it:** A method specifically designed for sparsity that improves performance over the text-only baseline on datasets with low average node degrees.

### Open Question 3
- **Question:** How does the omission of efficient structural encodings in baseline models affect the comparative potential of the MLLM-as-Encoder paradigm?
- **Basis in paper:** [inferred] Appendix A.2 notes that the shortest path distance (SPD) module was excluded from UniGraph2 due to O(n^3) complexity, potentially handicapping the GNN-based baseline against MLLM predictors.
- **Why unresolved:** It is unclear if the MLLM-as-Predictor's dominance over the Encoder paradigm holds when the GNN baseline is equipped with efficient, approximate structural embeddings.
- **What evidence would resolve it:** A comparative study evaluating UniGraph2 with efficient structural approximations (e.g., random walk probabilities) to verify if the performance gap with fine-tuned MLLMs narrows.

## Limitations
- The effectiveness of MLLM-based methods depends heavily on dataset-specific properties (graph density, multimodal quality) with no universal best approach identified
- Limited corpus evidence exists for direct evaluation of fine-tuned MLLMs on graph prediction tasks
- Performance degradation occurs when visual quality is poor or when modalities contain contradictory information

## Confidence
- **High Confidence:** Multimodal feature fusion consistently improves performance; fine-tuning MLLMs achieves state-of-the-art results in most scenarios
- **Medium Confidence:** Converting visual attributes to text improves GraphLLM performance; structure-aware augmentation effectiveness varies by graph density
- **Low Confidence:** LLaGA's performance degradation with augmentation (model-architecture mismatch hypothesis requires further validation)

## Next Checks
1. **Reproduce key finding:** Run MLP with text+image fusion on 2-3 datasets to confirm +10-25% performance gain over unimodal inputs
2. **Validate fine-tuning impact:** Compare zero-shot vs. fine-tuned Qwen-VL-7B accuracy on single dataset (expect 3-4× improvement)
3. **Test structure sensitivity:** Compare structure-aware vs. non-structure-aware fine-tuning on dense (CDs) vs. sparse (Toys) graphs to verify density-dependent effectiveness