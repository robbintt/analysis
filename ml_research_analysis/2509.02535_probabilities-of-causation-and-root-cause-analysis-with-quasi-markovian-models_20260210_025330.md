---
ver: rpa2
title: Probabilities of Causation and Root Cause Analysis with Quasi-Markovian Models
arxiv_id: '2509.02535'
source_url: https://arxiv.org/abs/2509.02535
tags:
- causal
- variables
- probabilities
- root
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses computational challenges in calculating probabilities
  of causation under partial identifiability and latent confounding, proposing algorithmic
  simplifications that significantly reduce complexity. The authors extend quasi-Markovian
  models to streamline counterfactual graph reductions, lowering the degree of multilinear
  programs used for tight bounds on PN, PS, and PNS.
---

# Probabilities of Causation and Root Cause Analysis with Quasi-Markovian Models

## Quick Facts
- arXiv ID: 2509.02535
- Source URL: https://arxiv.org/abs/2509.02535
- Reference count: 0
- Key outcome: The paper proposes algorithmic simplifications for calculating probabilities of causation under partial identifiability and latent confounding, demonstrating that necessity-based metrics consistently identify correct root causes in microservice failure models.

## Executive Summary
This paper addresses computational challenges in calculating probabilities of causation under partial identifiability and latent confounding. The authors propose algorithmic simplifications that significantly reduce complexity by extending counterfactual graph reductions to streamline multilinear programs. A novel Root Cause Analysis methodology leverages these probabilities to rank causal paths in DAGs rather than isolated variables. Experiments on synthetic microservice models demonstrate that necessity-based metrics (PN and weak-PN) consistently identify correct root causes under limited observability, while sufficiency-based metrics underperform.

## Method Summary
The method extends quasi-Markovian models to streamline counterfactual graph reductions, lowering the degree of multilinear programs used for tight bounds on PN, PS, and PNS. The authors construct a "counterfactual graph" and identify d-separating sets of variables to remove non-essential exogenous parents from the objective function, reducing polynomial degree. A Root Cause Analysis methodology traverses upstream from incident nodes, scoring variables using PN and pruning paths where scores drop significantly relative to median change. The approach uses heuristics to collapse interval bounds into scalar values for practical ranking when exact probabilities are unidentifiable due to latent confounders.

## Key Results
- Necessity-based metrics (PN, weak-PN) consistently identify correct root causes in microservice failure models while sufficiency-based metrics underperform
- Algorithmic simplifications reduce multilinear program degree through counterfactual graph reductions, making probability of causation calculations computationally tractable
- The method successfully identifies causal narratives in cascading failure paths where traditional approaches fail to distinguish root causes from spurious correlations

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Graph Reduction
The method reduces multilinear program degree by constructing counterfactual graphs and identifying d-separating sets of variables to remove non-essential exogenous parents. This simplification exploits the quasi-Markovian property where each endogenous variable has at most one exogenous parent, lowering polynomial degree and making optimization tractable.

### Mechanism 2: Necessity-Based Path Ranking
The RCA algorithm traverses upstream from incident nodes, scoring variables using PN (probability failure would not have occurred without the cause). Paths are pruned when PN scores drop significantly relative to median change, isolating causal narratives where removing the root cause is necessary to prevent failure.

### Mechanism 3: Interval Scalarization
Since latent confounders prevent point-identification, the method computes tight lower and upper bounds for causation probabilities. Heuristics (Minimum, Maximum, Mean, Midpoint) collapse these intervals into scalar values for practical ranking, with "Minimum" prioritizing lower bounds to filter unimportant variables.

## Foundational Learning

- **Quasi-Markovian Structural Causal Models (SCMs)**: Models where each endogenous variable has at most one exogenous parent. Why needed: The entire computational simplification relies on this constraint to reduce optimization complexity. Quick check: Can a variable have two different unobserved common causes?

- **Probabilities of Causation (PN, PS, PNS)**: Scoring metrics where PN measures necessity ("Would it not have happened without X?") and PS measures sufficiency ("Would it always happen if X?"). Why needed: These metrics determine root cause ranking, with necessity proving more reliable than sufficiency. Quick check: Are you measuring probability service would not have crashed without upgrade (PN) or that upgrade always leads to crash (PS)?

- **C-Components and Canonicalization**: Process of defining exogenous domains through canonical form of SCMs. Why needed: Determines cardinality of optimization variables for computing bounds. Quick check: How does exogenous variable cardinality relate to c-component in quasi-Markovian models?

## Architecture Onboarding

- **Component map**: DAG structure $G$ and observational dataset $\hat{Pr}$ -> Canonicalization module -> Multilinear optimization engine (Gurobi) -> DFS traversal (Algorithm 1) -> Ranked causal paths

- **Critical path**: The multilinear optimization step, which remains computationally intensive despite degree reduction from Theorem 1

- **Design tradeoffs**: PN provides better discrimination but requires expensive counterfactual optimization vs w-PN (cheaper interventional); precision vs tractability by using interval bounds vs point estimates; quasi-Markovian constraint vs general graph applicability

- **Failure signatures**: False positives with PS identifying intermediate variables instead of true root causes; optimization overhead when graph reduction fails; interval overlap causing scalarization heuristics to fail

- **First 3 experiments**:
  1. Implement Algorithm 1 on "Small-scale observability" model (Fig 3), inject "MemLeak" narrative, verify PN ranks correct path higher than spurious paths
  2. Measure solver time for PN bounds on standard SCM vs reduced counterfactual graph, confirm drop in optimization variable degree
  3. Run RCA on Model 2 (Fig 4) comparing PN vs PS, confirm PS fails to identify "DB_Latency" when "HeavyTraffic" confounders present while PN succeeds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the derived simplifications for counterfactual graphs and RCA ranking methodology be extended to discrete and continuous variables without relying on data binarization?
- Basis in paper: [explicit] The conclusion suggests extending methods to recently formalized definitions of probabilities of causation for discrete and continuous variables to enhance accuracy.
- Why unresolved: Current theoretical results and experiments focus on binary variables, and binarization can lead to information loss.
- What evidence would resolve it: Proofs of simplification theorems applicable to non-binary structural causal models and empirical validation showing improved RCA precision on continuous datasets.

### Open Question 2
- Question: Is it computationally feasible to obtain tight bounds for probabilities of causation in non-quasi-Markovian models where endogenous variables have multiple exogenous parents?
- Basis in paper: [inferred] Section 2 states relaxing quasi-Markovian assumption makes optimization problems often practically intractable, yet real-world systems often violate this.
- Why unresolved: Proposed algorithmic simplifications specifically exploit quasi-Markovian structure to reduce multilinear program complexity.
- What evidence would resolve it: Algorithmic adaptations or approximation theorems maintaining computational efficiency when single-exogenous-parent constraint is removed.

### Open Question 3
- Question: What is the optimal decision-theoretic strategy for collapsing interval estimates of causation probabilities into single scalar scores required for path ranking?
- Basis in paper: [inferred] Section 4.3 notes estimation yields intervals, requiring "heuristic strategies" without theoretical grounding for best approach.
- Why unresolved: Paper empirically tests different heuristics but does not derive normative rule linking specific interval-aggregation methods to error rates in root cause identification.
- What evidence would resolve it: Theoretical analysis linking specific interval-aggregation methods to error rates in root cause identification across different noise models.

## Limitations

- Method restricted to Quasi-Markovian models, limiting applicability to systems where each endogenous variable has at most one exogenous parent
- Scalarization heuristics for interval bounds may introduce information loss when true and spurious causes have overlapping intervals
- Computational complexity, while reduced, still scales with number of variables and confounders, challenging large-scale application

## Confidence

**High confidence** in computational efficiency gains from counterfactual graph reduction (Mechanism 1), supported by explicit algorithmic descriptions and alignment with established optimization theory.

**Medium confidence** in superiority of necessity-based metrics (Mechanism 2), based on synthetic experiments showing consistent performance differences, though real-world validation is absent.

**Medium confidence** in interval scalarization approach (Mechanism 3), as specific heuristics are proposed but robustness to interval overlap is not thoroughly demonstrated.

## Next Checks

1. **Real-world dataset validation**: Apply RCA method to production system failure log (e.g., cloud infrastructure incidents) to verify PN consistently identifies root causes better than PS in practical scenarios.

2. **Interval overlap stress test**: Construct synthetic SCMs where true and spurious causes have overlapping PN bounds. Measure failure rate of each scalarization heuristic (Minimum, Maximum, Mean, Midpoint) in correctly ranking true cause.

3. **Scalability benchmark**: Implement method on graphs with 50+ variables and varying exogenous parent degrees. Measure solver time and memory usage to identify practical limits of reduction technique.