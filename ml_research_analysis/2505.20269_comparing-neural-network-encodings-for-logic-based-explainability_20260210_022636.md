---
ver: rpa2
title: Comparing Neural Network Encodings for Logic-based Explainability
arxiv_id: '2505.20269'
source_url: https://arxiv.org/abs/2505.20269
tags:
- explanations
- constraints
- anns
- encodings
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two logical encodings of artificial neural
  networks (ANNs) for computing minimal explanations of model predictions. The first
  encoding uses logical implications to represent ReLU activation functions, while
  the second, adapted from prior work, achieves the same behavior without implications
  and uses fewer variables and constraints.
---

# Comparing Neural Network Encodings for Logic-based Explainability

## Quick Facts
- arXiv ID: 2505.20269
- Source URL: https://arxiv.org/abs/2505.20269
- Reference count: 19
- Primary result: Adapted MILP encoding achieves up to 18% faster build times and 16% better overall performance for computing minimal explanations of ANN predictions.

## Executive Summary
This paper compares two logical encodings of artificial neural networks (ANNs) for computing minimal explanations of model predictions. The first encoding uses logical implications to represent ReLU activation functions, while the second, adapted from prior work, achieves the same behavior without implications and uses fewer variables and constraints. Experiments across 12 datasets and various ANN architectures show that both encodings have similar running times for computing explanations, but the adapted encoding performs up to 18% better in building logical constraints and up to 16% better in overall time. The adapted method is particularly more efficient for ANNs with a single hidden layer. These results suggest the adapted encoding offers a more scalable approach for logic-based explainability in ANNs.

## Method Summary
The paper investigates two MILP encodings for computing minimal explanations of ANN predictions. The standard encoding uses logical implications to model ReLU activations, requiring auxiliary slack variables and indicator constraints. The adapted encoding, based on Tjeng et al., replaces implications with "Big-M" linear constraints, reducing variable count by eliminating slack variables. Both encodings translate ANN weights and biases into linear constraints, with binary variables controlling ReLU activation. Minimal explanations are found by iteratively testing subsets of features for logical entailment via unsatisfiability checking.

## Key Results
- Both encodings have similar running times for computing explanations (Algorithm 1).
- Adapted encoding achieves up to 18% better performance in building logical constraints.
- Adapted encoding shows up to 16% better overall running time, especially for single-layer ANNs.
- Both methods successfully computed explanations for 12 benchmark datasets with various ANN architectures.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Eliminating logical implications in favor of "Big-M" linear constraints reduces the variable count and model build time.
- **Mechanism:** The adapted encoding (from Tjeng et al.) avoids indicator constraints ($z=1 \rightarrow y \le 0$) used in the standard approach. Instead, it uses large constant bounds ($M$) to activate/deactivate linear inequalities (e.g., $y \le M(1-z)$). This formulation removes the need for auxiliary slack variables ($s_i^l$) required by the implication-based method to track negative pre-activation values.
- **Core assumption:** The MILP solver processes standard linear inequalities more efficiently during model construction than it processes native indicator constraints, and the reduction in variable count ($\sum n_l$) outweighs the potential complexity of handling large bound constants.
- **Evidence anchors:**
  - [abstract] "The second encoding... uses fewer variables and constraints, thus, potentially enhancing efficiency."
  - [section 4] "This encoding has $\sum_{l=1}^{L-1} n_l$ fewer variables... and $\sum_{l=1}^{L-1} n_l$ fewer constraints."
  - [corpus] Corpus evidence for this specific MILP encoding comparison is weak; related papers focus on logical rules or anomaly detection rather than constraint optimization.
- **Break condition:** If the bounds ($M$) are excessively large or loose, numerical instability may arise in the solver, or the "Big-M" constraints may become weak relaxations, failing to yield speed benefits.

### Mechanism 2
- **Claim:** Minimal explanations are generated by iteratively testing for logical entailment via unsatisfiability.
- **Mechanism:** The system operates by testing if a subset of features $C_m$ is sufficient to guarantee a classification $E$. It formulates the query $C_m \land F \land \neg E$ (where $F$ is the network logic). If the solver finds this combination **unsatisfiable**, it proves that no counter-example exists; thus, the features in $C_m$ logically entail the prediction $E$.
- **Core assumption:** The NP-complete nature of the satisfiability check is manageable for shallow networks (2 layers) within reasonable time limits using modern MILP solvers like CPLEX.
- **Evidence anchors:**
  - [section 3] "Since to check entailments $C \land F \models E$ is equivalent to test whether $C \land F \land \neg E$ is unsatisfiable... such a entailment can be addressed by a MILP solver."
  - [corpus] "Space Explanations of Neural Network Classification" similarly leverages logic for provable guarantees.
- **Break condition:** If the network depth or neuron count increases significantly, the search space for the unsatisfiability check explodes, causing timeouts.

### Mechanism 3
- **Claim:** Pre-computing tight bounds for neuron activations is required to make the "no-implication" encoding valid and efficient.
- **Mechanism:** The linear constraints in the adapted encoding depend on lower ($lb$) and upper ($ub$) bounds of neuron outputs (e.g., $x_i^l \le ub_i^l z_i^l$). The system runs optimization routines (minimization/maximization) to find these bounds before building the main explanation model.
- **Core assumption:** The computational cost of finding these bounds is included in the "build time" and is low enough relative to the total process to justify the tighter formulation it enables.
- **Evidence anchors:**
  - [section 4] "Constants $lb_i^l$ and $ub_i^l$ are... found via a MILP solver... necessary to maintain the integrity of the set of constraints."
  - [abstract] Results show better running times specifically in "building logical constraints," which includes this bound computation.
- **Break condition:** If the input feature space is unbounded or extremely wide, calculating tight neuron bounds becomes impossible or computationally expensive, degrading the encoding's performance.

## Foundational Learning

- **Concept: ReLU Activation as a Linear Piece**
  - **Why needed here:** The entire encoding strategy relies on the piecewise-linear nature of the Rectified Linear Unit (ReLU). You cannot map the network to MILP constraints if the activation functions are non-linear (e.g., Sigmoid) without complex approximation.
  - **Quick check question:** Can you formulate $y = \max(0, x)$ using only linear inequalities and a binary switch?

- **Concept: Mixed Integer Linear Programming (MILP)**
  - **Why needed here:** This is the solver engine. Understanding the difference between continuous variables (neuron outputs) and binary variables (activation switches) is essential to debug why the solver might be stalling.
  - **Quick check question:** Why does adding a binary variable make a linear program significantly harder to solve?

- **Concept: Logical Entailment ($\models$)**
  - **Why needed here:** The paper defines an "explanation" not as a probability, but as a logical guarantee. Understanding $A \models B$ (A entails B) is necessary to grasp why checking unsatisfiability ($A \land \neg B$) proves the explanation is correct.
  - **Quick check question:** If $A \models B$ is false, what does that imply about the satisfiability of $A \land \neg B$?

## Architecture Onboarding

- **Component map:** TensorFlow (ANN Trainer) -> CPLEX (Bound Calculator) -> Python (Encoder) -> CPLEX (Explainer)
- **Critical path:** The translation of the ANN layer math into the solver's constraint format. Specifically, ensuring the binary variable $z_i^l$ correctly switches the ReLU output $x_i^l$ between 0 and the linear combination.
- **Design tradeoffs:**
  - **Implication Encoding:** Standard Fischetti & Jo method. Uses more variables (includes slack $s$). Often native to solvers but creates a larger search space.
  - **Big-M Encoding:** Adapted Tjeng et al. method. Uses fewer variables (no slack). Requires pre-computed bounds. Paper suggests this is ~18% faster for building the model.
- **Failure signatures:**
  - **Timeouts:** Occur if layers > 2 or neurons > 40 per layer (as hinted by experimental limits).
  - **Inconsistent Explanations:** Occurs if feature normalization transforms integer spaces into continuous ones improperly (mentioned in Section 5.1).
  - **Trivial Explanations:** If bounds are too loose, the solver may find "trivial" solutions that don't reflect actual network behavior.
- **First 3 experiments:**
  1. **Unit Test Encoding:** Create a 1-neuron network. Encode it using both Implication and Big-M methods. Verify the solver outputs identical explanations for 10 random inputs.
  2. **Scaling Limit Check:** Train a network on the `voting` dataset. Increase neurons from 10 to 50 and plot the "Build Time" vs. "Explain Time" curve to verify the 18% build-time reduction claimed in the paper.
  3. **Ablation on Bounds:** Modify the bound calculator to return "infinity" instead of tight bounds for the Big-M encoding. Run the explainer and observe numerical instability or failure to converge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the efficiency of the implication-free encoding persist when applied to neural networks with significantly more layers and neurons?
- Basis in paper: [explicit] The authors state, "More experiments are necessary, especially with additional layers and neurons, to further validate our findings," noting that current experiments were limited to 2 layers.
- Why unresolved: Experiments were restricted to small architectures due to the computational complexity of the NP-complete solver calls required for larger networks.
- What evidence would resolve it: Benchmark results comparing runtimes on deeper networks (e.g., >2 hidden layers) and wider architectures (e.g., >100 neurons per layer).

### Open Question 2
- Question: How does the adapted encoding compare to other logical encodings, such as those used in the Marabou or Reluplex frameworks, for computing minimal explanations?
- Basis in paper: [explicit] The paper notes that "others encodings [9, 10] can be evaluated for computing logic-based minimal explanations for ANNs."
- Why unresolved: The study restricted its comparative analysis to only two methods: the encoding by Fischetti and Jo and the adaptation of Tjeng et al.
- What evidence would resolve it: A performance comparison (build time and explanation time) including the encodings utilized by Reluplex or Marabou on the same datasets.

### Open Question 3
- Question: Can the scalability of computing explanations be enhanced by integrating network simplification techniques, such as pruning or slicing, prior to encoding?
- Basis in paper: [explicit] The authors propose that "ANNs can be simplified... via pruning or slicing... to improve the scalability... This results in equivalent ANNs with smaller sizes."
- Why unresolved: This is suggested as future work; the current implementation and experiments do not incorporate or evaluate these simplification methods.
- What evidence would resolve it: Experiments measuring the reduction in overall running time when the encoding pipeline includes a pre-processing step of network pruning.

## Limitations

- Experimental scope is limited to small networks (1-2 hidden layers), leaving scalability to deeper networks unverified.
- Results are based on a specific solver (CPLEX), and performance advantages may not generalize to other solvers or hardware.
- Method's handling of continuous or unbounded input features is not addressed, limiting real-world applicability.
- Aggregate statistics (up to 18% and 16% improvements) may mask dataset-specific variations or outliers.

## Confidence

- **High:** The core mechanism of using MILP to compute minimal explanations is well-established and correctly described.
- **Medium:** The observed efficiency gains of the adapted encoding are likely valid within the tested parameter ranges but may not scale.
- **Low:** The paper's claims about handling general, high-dimensional, or unbounded input spaces are not substantiated by the experiments.

## Next Checks

1. **Deeper Networks:** Evaluate both encodings on ANNs with 3-5 hidden layers to test scalability.
2. **Solver Independence:** Replicate key experiments using Gurobi or SCIP to assess solver-specific performance differences.
3. **Unbounded Inputs:** Test the method on datasets with continuous or unbounded features to identify potential failure modes.