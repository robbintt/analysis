---
ver: rpa2
title: 'TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition'
arxiv_id: '2510.11944'
source_url: https://arxiv.org/abs/2510.11944
tags:
- code
- data
- formal
- mathematical
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TopoAlign, a framework that unlocks widely
  available code repositories as training resources for Math Large Language Models
  (LLMs). The framework addresses the scarcity of large-scale corpora containing pairs
  of informal and formal mathematical statements, a key bottleneck in autoformalisation
  tasks.
---

# TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition

## Quick Facts
- arXiv ID: 2510.11944
- Source URL: https://arxiv.org/abs/2510.11944
- Reference count: 40
- Primary result: Unlocks code repositories for training Math LLMs by aligning code structure to formal math, achieving up to 68.82% improvement in typecheck@10

## Executive Summary
TopoAlign is a framework that addresses the scarcity of large-scale corpora containing pairs of informal and formal mathematical statements by aligning code repositories to formal math structure. The framework decomposes code into docstrings, main functions, and dependency functions, reassembling these components into analogues that structurally mirror formal statements. This enables training Math Large Language Models (LLMs) on widely available code data without requiring additional human annotation. The method is evaluated by training DeepSeek-Math and Herald models on the aligned code data and formal mathematical data, showing substantial gains on MiniF2F, Putnam, and ProofNet benchmarks.

## Method Summary
TopoAlign works by first extracting Python repositories from The Stack v2 dataset and parsing them into function-level dependency trees using AST-based BFS extraction. Repositories are filtered for depth 3-6 and max siblings 3-10. Qwen3-14B generates docstrings for root functions when missing. The aligned data is structured as (docstring/informal statement + dependencies/lemmata) → (main function/formal statement). This aligned code data is then mixed with Herald Statements corpus at a 50:50 ratio (α=0.5) for multi-task training using next-token prediction. The approach is evaluated on DeepSeek-Math and Herald models using Typecheck and Bidirectional Equivalence metrics at pass@k.

## Key Results
- DeepSeek-Math trained with TopoAlign shows 17.77% improvement in BEq@10 and 68.82% in typecheck@10
- Herald model achieves 0.12% and 1.09% gains in BEq@10 and typecheck@10 respectively
- Pure code training causes models to generate syntactically invalid outputs (collapses BEq to 0 for Herald)
- Optimal mixing ratio is α=0.5 (50% aligned code, 50% math), with α=0.25 performing worst

## Why This Works (Mechanism)

### Mechanism 1: Structural Transfer via Topological Decomposition
- Claim: Decomposing code into components that mirror formal math structure enables transfer learning from code to autoformalisation.
- Mechanism: Code is decomposed into docstrings (↦ informal statements), main functions (↦ formal statements), and dependency functions (↦ supporting lemmata). The model learns compositional patterns from this aligned structure even when semantic content differs.
- Core assumption: Models can acquire structural knowledge from syntactically aligned data without semantic overlap.
- Evidence anchors: [abstract] "TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements."
- Break condition: If code and formal math lack compositional similarity, or if dependency structures diverge significantly (e.g., flat scripts with depth <3), alignment yields minimal benefit.

### Mechanism 2: Function-Level Dependency Extraction
- Claim: Function-level (not file-level) dependency graphs preserve the hierarchical reasoning patterns required for formal mathematics.
- Mechanism: A topological dependency parser performs breadth-first search over abstract syntax trees, tracing intra-file and inter-file function calls to build tree-structured representations. Repositories are filtered for depth 3–6 and max siblings 3–10.
- Core assumption: Hierarchical code organization approximates the structure of formal proofs and theorem dependencies.
- Evidence anchors: [section 3.1] "This contrasts with file-level dependency extraction (i.e., DeepSeek), which captures inter-file execution order but omits intra-file functional relationships."
- Break condition: If target formal language lacks comparable dependency granularity (e.g., very flat lemma structures), or if codebases are too shallow/complex, alignment weakens.

### Mechanism 3: Balanced Code–Math Multi-Task Training (CAF Objective)
- Claim: A 50:50 mixture of structurally aligned code and formal math data optimizes autoformalisation by combining problem-solving transfer with syntactic grounding.
- Mechanism: The training objective is L = αL_math + (1−α)L_CAF. The model is conditioned on input (docstring/informal statement) and dependencies, then trained to generate the target (main function/formal statement). α=0.5 yields highest or near-highest performance.
- Core assumption: Code data primarily transfers problem-solving heuristics; math data ensures syntactic correctness. Neither alone is sufficient.
- Evidence anchors: [section 5.3, Table 2] α=0.5 consistently yields best BEq@1 and TC@1; α=0.25 weakest; α=0.75 improves TC but not BEq consistently.
- Break condition: If the model overfits to code syntax (α too low) or loses structural transfer (α too high), performance degrades.

## Foundational Learning

- Concept: **Autoformalisation**
  - Why needed here: The target task—translating informal natural language math into formal Lean 4 statements. Understanding this task frames why structural alignment matters.
  - Quick check question: Given "For all odd n, show that 8|n²−1", can you identify the key components that must appear in a formal Lean statement?

- Concept: **Proof Assistants (Lean 4, Isabelle)**
  - Why needed here: These are the target formal systems. Their libraries (e.g., Mathlib) define the dependency structures TopoAlign mimics.
  - Quick check question: What is the difference between a theorem statement and a proof tactic in Lean 4?

- Concept: **Dependency Graphs / Call Trees**
  - Why needed here: The core data structure TopoAlign extracts from code. Understanding BFS-based tree construction is essential for implementing the parser.
  - Quick check question: Given a function A that calls B and C, where B calls D, draw the dependency tree. What is the depth of node D?

## Architecture Onboarding

- Component map: Stack v2 → Dependency Parser → Repository Filter → Docstring Generator → Structural Aligner → CAF Trainer → Trained Model

- Critical path:
  1. Extract Python repos from Stack v2 → parse ASTs → build dependency trees → filter repos.
  2. Generate/augment docstrings for main functions.
  3. Assemble aligned samples: (docstring, dependencies, main function).
  4. Mix with Herald Statements (formal math) at α=0.5.
  5. Train DeepSeek-Math or Herald with next-token prediction loss.

- Design tradeoffs:
  - Python vs typed languages: Python lacks explicit type enforcement → hypothesized cause of type errors (e.g., Z vs N). Future work could use Java/C++.
  - Function-level vs file-level: Finer granularity better matches formal math but increases parsing complexity.
  - LLM-generated docstrings: Augments missing docs but may introduce noise.

- Failure signatures:
  - **Type mismatches**: Model generates semantically correct but type-incorrect statements (e.g., `n : Z` instead of `n : N`).
  - **Code-only training degradation**: HERALD trained only on unaligned code drops BEq to 0, outputs syntactically invalid Lean.
  - **Over-complex repos**: Repos with depth >6 or siblings >10 excluded to avoid noise.

- First 3 experiments:
  1. Reproduce α ablation (0.25, 0.5, 0.75) on MiniF2F-valid to confirm optimal mixing ratio.
  2. Run error analysis on ProofNet samples to quantify type-mismatch frequency and identify systematic patterns.
  3. Pilot test with Java/C++ repos to assess whether stricter typing reduces type errors in generated formal statements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can applying TopoAlign to strictly typed programming languages (e.g., Java, C++) mitigate the variable type mismatch errors observed when using Python?
- Basis in paper: [explicit] In Section 5.1, the authors hypothesize that type errors (e.g., assigning $\mathbb{Z}$ instead of $\mathbb{N}$) arise because Python code lacks explicit variable type enforcement, and they explicitly suggest investigating stricter typing systems in future work.
- Why unresolved: The current study exclusively utilizes Python repositories from the Stack v2 dataset, leaving the potential benefits of static typing unexplored.
- What evidence would resolve it: An evaluation of models trained on TopoAlign-curated datasets from Java or C++ repositories, specifically measuring the frequency of type errors on the same benchmarks.

### Open Question 2
- Question: Does the TopoAlign framework transfer effectively to other formal proof assistants like Isabelle or Coq, or is it optimized specifically for Lean 4 syntax?
- Basis in paper: [inferred] The introduction frames the problem around proof assistants generally (Isabelle, Lean 4), yet the methodology and evaluation are restricted entirely to Lean 4 (MiniF2F, ProofNet, Putnam).
- Why unresolved: The dependency extraction relies on Python-specific ASTs and Lean 4's specific library structure (Mathlib), creating uncertainty regarding generalizability to other formal languages.
- What evidence would resolve it: Applying the same topological decomposition to code and evaluating the resulting model's autoformalisation performance on Isabelle or Coq benchmarks.

### Open Question 3
- Question: Why does training exclusively on aligned code data (the Code-only setting) cause the model to generate syntactically invalid outputs, and can this failure be prevented?
- Basis in paper: [inferred] Section 5.2 shows that the Herald model trained only on code experienced a performance collapse, generating Python syntax instead of Lean 4.
- Why unresolved: The paper demonstrates the failure but does not fully isolate the mechanism behind this "catastrophic forgetting" of formal syntax when problem-solving skills are emphasized without paired formal examples.
- What evidence would resolve it: A detailed ablation study analyzing the token distributions of Code-only models to determine if the syntax collapse is due to token probability shifts or a loss of structural attention.

### Open Question 4
- Question: Does the optimal mixing ratio ($\alpha=0.5$) between aligned code and formal math data remain stable as the total volume of training tokens increases?
- Basis in paper: [inferred] Section 5.3 establishes a 50/50 balance as optimal for the current sample size, but notes that lower math ratios bias the model toward code generation.
- Why unresolved: The ablation study is performed on a fixed, small dataset; it is unclear if "math saturation" occurs faster or slower when training on billions of tokens.
- What evidence would resolve it: A scaling law analysis plotting autoformalisation performance against varying $\alpha$ values at increasing dataset sizes.

## Limitations
- Structural alignment depends on assumed similarity between Python code and formal math, which is not empirically validated across diverse codebases
- Generated docstrings from Qwen3-14B introduce potential noise or bias into the training data
- Type errors in generated formal statements indicate incomplete transfer of formal type discipline from untyped Python
- Evaluation relies on synthetic benchmarks and a strong LLM for BEq scoring, which may not reflect real-world deployment scenarios

## Confidence

- **High Confidence:** The structural alignment methodology and its impact on typecheck@10 performance, particularly for DeepSeek-Math.
- **Medium Confidence:** The general transfer learning mechanism from code to formal math, given observed gains across multiple benchmarks.
- **Medium Confidence:** The optimal mixing ratio (α=0.5) for balanced training, though results show some sensitivity to α.
- **Low Confidence:** Generalization to typed programming languages and whether stricter typing would reduce type errors in generated formal statements.

## Next Checks

1. **Cross-language validation:** Test TopoAlign with Java/C++ repositories to assess whether stronger type systems reduce the frequency of type errors in generated formal statements.
2. **Structural similarity quantification:** Measure and compare dependency graph properties between Python code and formal math statements to validate the core alignment assumption.
3. **Human evaluation pilot:** Conduct a small-scale human review of generated formal statements to validate the BEq metric against expert judgment and identify systematic error patterns.