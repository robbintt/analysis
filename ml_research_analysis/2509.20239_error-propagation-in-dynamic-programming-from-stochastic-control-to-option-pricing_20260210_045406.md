---
ver: rpa2
title: 'Error Propagation in Dynamic Programming: From Stochastic Control to Option
  Pricing'
arxiv_id: '2509.20239'
source_url: https://arxiv.org/abs/2509.20239
tags:
- control
- error
- optimal
- stochastic
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical foundations for stochastic optimal
  control in discrete time, focusing on error propagation in dynamic programming.
  The authors propose a kernel-based regression framework that combines Monte Carlo
  sampling with regularized empirical risk minimization (specifically KRR) to approximate
  value functions in reproducing kernel Hilbert spaces.
---

# Error Propagation in Dynamic Programming: From Stochastic Control to Option Pricing

## Quick Facts
- arXiv ID: 2509.20239
- Source URL: https://arxiv.org/abs/2509.20239
- Reference count: 36
- This paper provides theoretical foundations for stochastic optimal control in discrete time, focusing on error propagation in dynamic programming

## Executive Summary
This paper develops a kernel-based regression framework for approximating value functions in discrete-time stochastic optimal control, with applications to American option pricing. The authors establish rigorous error bounds that decompose total approximation error into regression, Monte Carlo sampling, and propagation components. By proving that the Bellman operator is Lipschitz continuous, they show errors propagate linearly rather than exponentially through the backward induction process. The framework provides theoretical error guarantees that existing approaches lack, while demonstrating competitive performance against benchmark methods.

## Method Summary
The method employs Kernel Ridge Regression (KRR) within a dynamic programming framework to approximate value functions. At each time step, the algorithm generates training data through Monte Carlo sampling to estimate continuation values, then fits a KRR model to minimize regularized empirical risk in a Reproducing Kernel Hilbert Space. The backward induction proceeds from maturity to present, with the total error decomposed into three components: regression error (function approximation quality), Monte Carlo sampling error (estimation of conditional expectations), and propagation error (accumulation of errors through time steps). The authors use FALKON algorithm for efficient KRR computation and apply the method to American option pricing problems.

## Key Results
- The Bellman operator's Lipschitz continuity ensures errors propagate linearly rather than exponentially through time steps
- Convergence rates of $n^{-\frac{\beta}{\beta+1}}$ are achieved for KRR under source conditions, degrading gracefully with function smoothness
- Monte Carlo sampling error decays at rate $1/\sqrt{M_t}$ for finite control spaces
- The framework demonstrates competitive performance against benchmarks while providing theoretical error guarantees
- Error bounds show the approximation error at time $t$ depends on both sample size and propagated error from later stages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The total error in estimating the value function remains bounded across time steps because the Bellman operator is Lipschitz continuous.
- **Mechanism:** The authors decompose the total error $E_t$ into three additive components and show that the Bellman operator $\mathcal{T}_t$ has a Lipschitz constant $c_P$, proving errors propagate linearly rather than exponentially.
- **Core assumption:** The Bellman operator is a contraction mapping ($c_P < 1$) and the target function satisfies a source condition.
- **Evidence anchors:** [Theorem 1, Eq. 27] establishes the recursive error bound $E_t \lesssim n^{-\frac{\beta}{\beta+1}} + c_P E_{t+1}$.
- **Break condition:** If $c_P \geq 1$ (e.g., in an explosive stochastic process without discounting), errors may accumulate rather than dampen.

### Mechanism 2
- **Claim:** Kernel Ridge Regression approximates the continuation value effectively even under model misspecification.
- **Mechanism:** KRR minimizes regularized empirical risk in an RKHS, using a source condition with parameter $\beta$ to quantify target smoothness relative to the kernel.
- **Core assumption:** The target function $W^*_t$ lies in the range of the integral operator $L_k^{\beta/2}$ and the kernel is bounded.
- **Evidence anchors:** [Section 4.1, Eq. 25] provides the convergence rate dependent on the smoothness parameter $\beta$.
- **Break condition:** If the kernel is poorly chosen such that $\beta$ is very small, the convergence rate approaches $n^{-0}$, requiring prohibitive sample sizes.

### Mechanism 3
- **Claim:** Monte Carlo sampling provides consistent approximation of the conditional expectation operator.
- **Mechanism:** The true expectation is replaced by an empirical average using $M_t$ i.i.d. samples, with error controlled via Rademacher complexity bounds.
- **Core assumption:** The function class has bounded complexity (finite Rademacher complexity).
- **Evidence anchors:** [Section 4, Term II] bounds the Monte Carlo error using Massart's Lemma for finite control classes.
- **Break condition:** If the control space is continuous and large, or the transition function is highly irregular, function class complexity may explode.

## Foundational Learning

- **Concept:** Dynamic Programming & The Bellman Equation
  - **Why needed here:** This is the structural core of the paper. The entire algorithm is built on solving the problem backward from time $T$ to $0$ using the relationship $V_t = \mathcal{T}_t V_{t+1}$.
  - **Quick check question:** Can you explain why we solve for the value function backward in time, but the control policy is applied forward in time?

- **Concept:** Reproducing Kernel Hilbert Spaces (RKHS) & Kernel Ridge Regression
  - **Why needed here:** This is the function approximator. Understanding the "kernel trick" and why regularization is necessary to prevent overfitting to Monte Carlo noise is vital.
  - **Quick check question:** Why does adding a norm penalty $\lambda ||f||^2_{H_k}$ help when our training labels $y_i$ are noisy estimates of the true value?

- **Concept:** Monte Carlo Integration
  - **Why needed here:** The method relies on estimating $\mathbb{E}[V_{t+1} | X_t]$ by sampling paths.
  - **Quick check question:** If I double the number of Monte Carlo samples $M_t$, how much does the variance of my estimator decrease?

## Architecture Onboarding

- **Component map:** Data Generator -> Empirical Bellman Operator -> KRR Solver -> Backward Loop
- **Critical path:** The *Data Generation* function (Algorithm 1, Line 7-13) couples simulation with the previous step's value function. Ensuring this step is vectorized is key to performance.
- **Design tradeoffs:**
  - **$n_t$ vs. $M_t$:** Theorem 1 suggests $M_t \sim n_t^{\frac{\beta}{\beta+1}}$. Increasing $n_t$ improves function approximation but requires increasing $M_t$ to reduce noise in training labels.
  - **Computational Cost:** Standard KRR scales as $O(n^3)$. FALKON (Nyström approximation) is suggested to reduce this.
- **Failure signatures:**
  - **Exploding Variance:** If the clipping threshold $B$ is too low or the kernel bandwidth is too narrow, predicted values may spike, causing instability.
  - **Stagnation:** If the regularization $\lambda$ is too high, the function approximator becomes constant (underfitting), causing option value to collapse.
- **First 3 experiments:**
  1. **Sanity Check (1D Black-Scholes):** Implement KRR-DP for a single asset and compare against analytical Black-Scholes formula.
  2. **Scaling Test (Geometric Basket Put):** Replicate $d=2, 5, 10$ experiments from Table 1 and monitor relative error as dimensionality increases.
  3. **Ablation on $M_t$:** For fixed $n_t=500$, vary $M_t$ (e.g., 10, 50, 100) and plot error to confirm theoretical bound $\lesssim 1/\sqrt{M_t}$.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can random projection techniques be integrated into the KRR-DP framework to improve computational efficiency without violating theoretical error guarantees?
- **Open Question 2:** Does the smoothness of the target function (parameter $\beta_t$) improve or degrade throughout the backward recursion when the terminal condition is non-smooth?
- **Open Question 3:** Can sophisticated quadrature schemes replace standard Monte Carlo sampling to reduce the sample complexity $M$ required for estimating continuation values?

## Limitations
- The source condition requiring the value function to be smooth relative to the kernel is a strong assumption that may not hold for all option payoffs
- The Lipschitz continuity assumption for the Bellman operator may fail in explosive or highly path-dependent environments
- Monte Carlo error bounds assume finite control spaces, limiting applicability to continuous control problems

## Confidence
- **High confidence:** The decomposition of total error into three additive components and the recursive error bound structure appear mathematically sound
- **Medium confidence:** The convergence rates for KRR under source conditions are well-established, though their applicability to option pricing requires validation
- **Medium confidence:** The practical implementation details are sufficiently specified for reproduction, though exact state sampling remains ambiguous

## Next Checks
1. Test the algorithm on option payoffs with varying degrees of smoothness to empirically validate the theoretical dependence on the source condition parameter β
2. Implement a sensitivity analysis varying the Lipschitz constant c_P (through discounting parameters) to verify theoretical error propagation bounds under different market conditions
3. Compare the KRR-DP approach against state-of-the-art deep learning methods on the same American option pricing tasks to assess the practical tradeoff between theoretical guarantees and computational efficiency