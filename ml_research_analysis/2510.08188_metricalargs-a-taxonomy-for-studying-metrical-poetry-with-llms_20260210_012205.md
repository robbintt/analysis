---
ver: rpa2
title: 'MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs'
arxiv_id: '2510.08188'
source_url: https://arxiv.org/abs/2510.08188
tags:
- poem
- tasks
- poetry
- generation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces METRICALARGS, a taxonomy of tasks for studying
  metrical poetry with Large Language Models (LLMs) across four dimensions: Analysis,
  Retrieval, Generation, and Support. The taxonomy connects metrical poetry tasks
  to standard NLP tasks and provides guidance on datasets and evaluation metrics.'
---

# MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs

## Quick Facts
- arXiv ID: 2510.08188
- Source URL: https://arxiv.org/abs/2510.08188
- Authors: Chalamalasetti Kranti; Sowmya Vajjala
- Reference count: 22
- One-line primary result: LLM evaluation on Telugu metrical poetry reveals systematic failures in retrieval and style transfer, with LLM-as-a-judge overestimating performance by prioritizing semantic over structural correctness.

## Executive Summary
This paper introduces METRICALARGS, a taxonomy of 10 tasks for studying metrical poetry with Large Language Models across four dimensions: Analysis, Retrieval, Generation, and Support. The framework connects poetry-specific tasks to standard NLP paradigms while adding metrical constraints. Using Telugu as a case study with 170 samples, the authors evaluate GPT-5 and Gemini-2.5-Pro on zero-shot tasks. Results show mixed performance: high accuracy on summarization (0.70-0.85) but complete failure on retrieval (0) and style transfer (0). Human evaluation revealed that LLM-as-a-judge systematically overestimates model performance by focusing on semantic similarity while overlooking structural and stylistic deficiencies.

## Method Summary
The study uses 170 Telugu metrical poetry samples from Grade 7-10 textbooks, covering all 10 tasks in the METRICALARGS taxonomy. Models are evaluated zero-shot using GPT-5 and Gemini-2.5-Pro via the Inspect framework with temperature=0. An LLM-as-a-judge (Gemini-2.5-Pro) evaluates outputs using the same prompt templates as the tasks. Human evaluation by two native speakers validates judge accuracy on a subset of samples. Tasks include syllabification, meter identification, glossing, sentiment analysis, retrieval from fragments, poem generation, summarization, style transfer, error correction, and vocabulary suggestion.

## Key Results
- GPT-5 achieved 0.70 accuracy for poem summarization while Gemini-2.5-Pro scored 0.85
- Both models scored 0 accuracy on all retrieval subtasks (from verse fragments, descriptions, and by meaning/meter)
- Style transfer tasks failed completely (0 accuracy) due to metrical constraint violations
- LLM-as-a-judge overestimated performance by 15-30% compared to human evaluation
- Error correction and vocabulary suggestion showed moderate success (0.45-0.60 accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metrical poetry tasks expose gaps in LLMs' structural language processing that are invisible in standard semantic-only evaluations.
- Mechanism: Metrical verse requires models to coordinate phonology (syllable counting), morphology (pattern adherence), and semantics simultaneously. When models fail structural constraints while preserving meaning, this reveals that apparent fluency masks underlying linguistic incompetence.
- Core assumption: Structural errors in constrained generation indicate deeper limitations in linguistic representation, not just task-specific failures.
- Evidence anchors:
  - [abstract] "Human evaluation revealed that LLM-as-a-judge often overestimated model performance by focusing on semantic similarity while overlooking structural and stylistic deficiencies."
  - [Section 5.1] "While the LLM-as-a-judge marked the GPT-5 output as correct, human inspection revealed a mismatch: the verse was rhythmically consistent but contained lexical errors and ungrammatical phrasing in Telugu."
  - [corpus] Related work on Arabic poetry benchmarks (Alghallabi et al.) similarly finds LLMs struggle with form understanding.
- Break condition: If structural failures correlate only with low-resource language coverage rather than architectural limitations, the mechanism weakens.

### Mechanism 2
- Claim: Mapping poetry-specific tasks to standard NLP paradigms enables transfer of evaluation infrastructure while preserving domain complexity.
- Mechanism: The taxonomy translates poetry tasks into equivalents like sequence labeling (syllabification), classification (meter detection), and style transfer (meter-to-meter conversion). This allows researchers to leverage existing datasets, metrics, and training approaches while testing higher-order constraints.
- Core assumption: The complexity added by metrical constraints does not fundamentally change the task type, only the difficulty level.
- Evidence anchors:
  - [Section 3] "The first task can be compared to a standard sequence labeling task such as part-of-speech tagging... Metrical verse identification is a classification problem."
  - [Figure 2] Visual mapping of 10 poetry tasks to 8 standard NLP task categories.
  - [corpus] Russian poetry evaluation work (Koziev and Fenogenova) uses similar NLP task framing for meter evaluation.
- Break condition: If evaluation metrics from standard NLP tasks systematically misalign with human judgments in the poetry domain (as suggested by LLM-judge failures), the transfer is unreliable.

### Mechanism 3
- Claim: Retrieval tasks requiring exact verbatim recall fail in current LLMs due to training dynamics that prioritize paraphrase over memorization.
- Mechanism: When prompted with partial verses, models treat the input as conditional generation cues rather than retrieval queries. They generate plausible continuations rather than recalling memorized texts, possibly due to safety guardrails preventing verbatim reproduction and tokenization that fragments verse structure.
- Core assumption: Retrieval failure reflects deliberate training choices rather than fundamental architectural limitations.
- Evidence anchors:
  - [Section 5] "Neither model was able to successfully retrieve the complete poem given only the first, last, or a random line as input, resulting in an accuracy of 0 for this task."
  - [Section 5.1] "Rather than recalling the original poem, the model interprets the task as one of conditional generation."
  - [corpus] Limited corpus evidence on exact retrieval in low-resource languages; mechanism remains under-tested.
- Break condition: If retrieval succeeds in high-resource languages with identical prompts, the mechanism shifts from architectural to data-coverage issues.

## Foundational Learning

- Concept: Syllable-based metrical systems (guru/laghu in Telugu and Sanskrit)
  - Why needed here: Understanding that meter is governed by quantitative syllable patterns (heavy vs. light), not stress patterns as in English, is essential for interpreting scansion results and model errors.
  - Quick check question: Can you explain why a Telugu syllable like "కా" (kā) counts as guru (heavy) while "క" (ka) counts as laghu (light)?

- Concept: Taxonomy-to-NLP task mapping
  - Why needed here: The entire METRICALARGS framework depends on recognizing that novel-sounding poetry tasks (scansion, style transfer between meters) map to familiar NLP problems with added constraints.
  - Quick check question: What standard NLP task category does "identifying the meter of a given poem" fall under, and what evaluation metric would you use?

- Concept: LLM-as-a-judge limitations in structural evaluation
  - Why needed here: The paper demonstrates that automated evaluation systematically overestimates quality when judges focus on semantics; understanding this gap is critical for designing valid evaluation pipelines.
  - Quick check question: If an LLM judge evaluates a poem as semantically accurate but a human evaluator marks it as metrically incorrect, which dimension is the judge likely ignoring?

## Architecture Onboarding

- Component map:
  - Analysis dimension: Structural (syllabification, meter detection), Morphological (glossing), Other (sentiment, authorship) → sequence labeling, classification
  - Retrieval dimension: From verse fragments, from descriptions, by meaning/meter → information retrieval
  - Generation dimension: Summarization, poem from description, style transfer → text generation and style transfer
  - Support dimension: Error correction, vocabulary suggestion, descriptive feedback → grammar error correction

- Critical path:
  1. Define task category and sub-task → 2. Map to standard NLP task → 3. Identify or construct dataset → 4. Select evaluation metric (accuracy for classification, precision/recall for retrieval, human + rule-based for generation) → 5. Run zero-shot or few-shot evaluation → 6. Compare LLM-judge vs. human evaluation

- Design tradeoffs:
  - LLM-judge vs. human evaluation: LLM judges are scalable but overestimate semantic correctness; human evaluation is accurate but expensive. Paper recommends hybrid approach.
  - Zero-shot vs. fine-tuned evaluation: Zero-shot reveals base capabilities but may underrepresent potential; fine-tuning requires dataset construction that may not exist.
  - Rule-based vs. learned evaluation: Meter adherence can be checked algorithmically (tools like Chandam exist), but creative quality requires human judgment.

- Failure signatures:
  - Retrieval returning paraphrases: Model generates plausible continuation instead of exact verse → indicates retrieval task misinterpreted as generation.
  - Style transfer producing wrong meter: Output achieves 70% metrical similarity but fails full constraint satisfaction → indicates partial pattern learning.
  - Glosing producing multi-word definitions: Model provides semantic equivalents instead of morphological glosses → indicates task misunderstood.

- First 3 experiments:
  1. Baseline taxonomy replication: Take 20 samples per task from Telugu textbooks, run zero-shot evaluation with GPT-4/Gemini, compare LLM-judge scores to human annotation on 10% sample to quantify overestimation bias.
  2. Cross-language transfer test: Apply same taxonomy to related language (e.g., Kannada, Sanskrit) using identical prompts to test whether failure patterns are language-specific or architecture-general.
  3. Rule-augmented evaluation: For generation tasks, pipe model outputs through existing scansion tools (Chandam for Telugu/Sanskrit) to get objective meter adherence scores, then correlate with LLM-judge and human scores to calibrate evaluation pipelines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuned or specialized judge models be developed to evaluate metrical poetry tasks more accurately than general-purpose LLM-as-a-judge approaches?
- Basis in paper: [explicit] Section 5.2 states: "More experiments with judge prompts, and potentially fine-tuned judge models may be needed for better automated evaluation of such tasks."
- Why unresolved: The study found that LLM-as-a-judge consistently overestimated performance by focusing on semantic similarity while missing structural and stylistic deficiencies, with human annotators assigning lower scores across most tasks.
- What evidence would resolve it: A comparative study showing that fine-tuned judge models achieve higher correlation with human evaluators on structural/stylistic criteria than current LLM judges.

### Open Question 2
- Question: Is the near-zero retrieval accuracy for metrical poetry caused by limited training data coverage in low-resource languages or by safety guardrails preventing verbatim reproduction?
- Basis in paper: [explicit] Section 5.1 states: "Possible reasons include safety guardrails that prevent verbatim reproduction and limited Telugu training coverage, warranting further tests in other languages."
- Why unresolved: Both models scored 0 on all retrieval subtasks, but the root cause remains unclear—whether models lack memorization of Telugu poems or actively avoid verbatim output.
- What evidence would resolve it: Replicating the retrieval experiments in higher-resource languages (e.g., English or Chinese poetry) where training data is abundant, comparing retrieval success rates.

### Open Question 3
- Question: How does the METRICALARGS taxonomy need to be adapted for languages with different metrical traditions (e.g., tonal meters in Chinese, quantitative meters in Arabic)?
- Basis in paper: [explicit] Section 6 states: "Extending to other world languages with similar poetic traditions, and adapting the taxonomy accordingly should be considered too."
- Why unresolved: The taxonomy was built around syllable-based meters (Telugu/Sanskrit) and may not directly transfer to languages where meter depends on tone, stress, or other phonological features.
- What evidence would resolve it: Case studies applying METRICALARGS to at least two non-syllabic metrical traditions, documenting required modifications to task definitions and evaluation protocols.

### Open Question 4
- Question: Can LLMs be improved on style transfer tasks for metrical poetry through task-specific fine-tuning or prompt engineering?
- Basis in paper: [explicit] Tables 2-3 show both GPT-5 and Gemini-2.5-Pro achieved 0 accuracy on style transfer tasks, representing a complete failure on this generation subtask.
- Why unresolved: Style transfer requires maintaining semantic content while restructuring to a new meter—a multi-constraint problem current LLMs fail at entirely.
- What evidence would resolve it: Demonstrating non-zero accuracy on style transfer after fine-tuning on aligned poem pairs in different meters, or through chain-of-thought prompting that explicitly handles constraints sequentially.

## Limitations
- The METRICALARGS framework's generalizability beyond Telugu remains uncertain, as failure patterns may stem from language-specific factors rather than architectural limitations.
- The study reveals LLM-as-a-judge overestimation bias but does not provide clear guidelines for calibrating this bias or developing hybrid evaluation frameworks.
- Core mechanisms are under-tested across multiple linguistic systems, particularly the claim that structural errors reveal fundamental linguistic incompetence.

## Confidence
- **High confidence**: The taxonomy framework (METRICALARGS) and its mapping to standard NLP tasks are well-grounded and provide useful structure for future research. The demonstration of LLM-judge overestimation bias is supported by direct human evaluation comparisons.
- **Medium confidence**: The core claim that retrieval failures indicate training dynamics prioritizing paraphrase over memorization is plausible but under-tested, with limited corpus evidence beyond the Telugu case study.
- **Low confidence**: The assertion that structural errors in constrained generation reveal fundamental linguistic incompetence (Mechanism 1) requires broader cross-linguistic validation to distinguish between architectural limitations and language-specific coverage gaps.

## Next Checks
1. Cross-linguistic replication: Apply the METRICALARGS taxonomy and evaluation protocol to at least two additional languages with different metrical systems (e.g., Sanskrit, Arabic, or Russian) to determine whether failure patterns are architecture-general or language-specific.

2. Calibration study: Design a systematic experiment to quantify the systematic bias in LLM-as-a-judge evaluations by creating test cases with controlled structural vs. semantic differences, then measuring judge accuracy on each dimension to develop correction factors.

3. Hybrid evaluation framework: Implement and validate a hybrid evaluation approach that combines rule-based structural checking (for meter adherence, syllabification) with LLM-based semantic evaluation to achieve both accuracy and scalability.