---
ver: rpa2
title: Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs
arxiv_id: '2509.18113'
source_url: https://arxiv.org/abs/2509.18113
tags:
- prompt
- task
- scheduling
- multi-task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic prompt scheduling framework to
  address the generalization challenges of large language models in multi-task and
  cross-domain settings. The proposed method dynamically combines and aligns task-specific
  prompts using a task-aware scheduler, gated fusion with task embeddings, and an
  automatic learning strategy for scheduling weights.
---

# Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs

## Quick Facts
- arXiv ID: 2509.18113
- Source URL: https://arxiv.org/abs/2509.18113
- Authors: Xin Hu; Yue Kang; Guanzi Yao; Tianze Kang; Mengjie Wang; Heyao Liu
- Reference count: 24
- Primary result: Achieves SuperGLUE accuracy of 82.6 and MMLU accuracy of 71.3 with 1.38 Prompt Transfer Gain on CrossFit benchmark

## Executive Summary
This paper introduces a dynamic prompt scheduling framework to address the generalization challenges of large language models in multi-task and cross-domain settings. The proposed method dynamically combines and aligns task-specific prompts using a task-aware scheduler, gated fusion with task embeddings, and an automatic learning strategy for scheduling weights. Experiments on the CrossFit benchmark show that the method outperforms competitive baselines, achieving SuperGLUE accuracy of 82.6 and MMLU accuracy of 71.3, with a 1.38 Prompt Transfer Gain—a 38% improvement over the baseline. Sensitivity analyses confirm that the model maintains strong robustness and adaptability to task distribution changes, especially under moderate task scales. The results demonstrate that dynamic prompt scheduling significantly enhances cross-task and cross-domain generalization in LLMs.

## Method Summary
The method uses a prompt pool P = {p_1, ..., p_K} of K learnable soft prompts and a task scheduling weight matrix W ∈ R^(T×K) to dynamically combine prompts for T tasks. Each task t gets a weight vector w_t normalized via softmax, then combined as t̃p = Σ(w_k,t · p_k). Task embeddings e_t are fused with the combined prompt using a gated mechanism: p_final = σ(W_g · e_t) ⊙ e_t + (1 - σ(W_g · e_t)) ⊙ p̃_t. The total loss L_total = Σ λ_t · L_t is optimized with gradient normalization to balance task contributions and mitigate negative transfer. Optimal temperature is 0.9, with peak performance at 12 tasks.

## Key Results
- Achieves SuperGLUE accuracy of 82.6 and MMLU accuracy of 71.3 on CrossFit benchmark
- Demonstrates 1.38 Prompt Transfer Gain—a 38% improvement over baseline
- Shows robustness to task distribution changes, especially under moderate task scales (12 tasks optimal)
- Performance declines when task count exceeds 12-16 due to prompt interference and diluted attention

## Why This Works (Mechanism)

### Mechanism 1: Task-Aware Dynamic Prompt Scheduling
Dynamically combining prompts from a shared pool via learned task-specific weights improves cross-task generalization over fixed prompt templates. A task scheduling weight matrix W maps T tasks to K prompts, with weights normalized via softmax to produce probabilistic prompt selection, then combined as t̃p = Σ(w_k,t · p_k). This allows the model to draw from multiple prompt templates rather than relying on a single fixed prompt per task.

### Mechanism 2: Gated Task Embedding Fusion
Injecting task-specific embeddings through a gated mechanism preserves task structural information during prompt fusion, reducing task confusion. Task embeddings e_t are fused with combined prompts via: p_final = σ(W_g · e_t) ⊙ e_t + (1 - σ(W_g · e_t)) ⊙ p̃_t, where σ is sigmoid. The gate dynamically balances between task identity (via e_t) and prompt content (via p̃_t).

### Mechanism 3: Gradient-Normalized Multi-Task Loss Balancing
Automatically adjusting task loss weights via gradient normalization mitigates negative transfer and ensures balanced optimization across heterogeneous tasks. Total loss L_total = Σ λ_t · L_t where λ_t reflects task importance and is adaptively updated. Gradient normalization ensures loss gradients of different tasks have balanced contributions during training, reducing dominance of high-gradient tasks.

## Foundational Learning

- **Soft Prompt Tuning (Prefix/Continuous Prompts)**
  - Why needed here: The entire framework builds on the premise that prompts are learnable continuous vectors, not discrete text. Without understanding that p_k ∈ R^d is a trainable embedding, the scheduling mechanism makes no sense.
  - Quick check question: Can you explain why soft prompts are different from few-shot discrete prompts, and where they attach in the transformer architecture?

- **Multi-Task Learning Objective Conflicts**
  - Why needed here: The paper explicitly addresses negative transfer and task interference. Understanding why tasks conflict (gradient interference, representation corruption, catastrophic forgetting) is prerequisite to appreciating why scheduling + gating helps.
  - Quick check question: What is negative transfer, and can you name two scenarios where adding more tasks hurts rather than helps?

- **Temperature Scaling in Softmax Distributions**
  - Why needed here: Figure 2 shows temperature significantly impacts performance (optimal at 0.9). Understanding how temperature controls distribution sharpness vs. uniformity is essential for tuning the scheduler.
  - Quick check question: What happens to a softmax distribution as temperature approaches 0 vs. infinity? How would this affect prompt selection?

## Architecture Onboarding

- **Component map:**
  Input x → Concatenate with prompt → [Prompt Pool P] → [Task Scheduler W] → [Softmax over prompts] → [Combined prompt t̃p] → [Task embedding e_t] ⊕ [t̃p] → [Gated Fusion Gate] → [Final prompt p_final] → [LLM forward pass] → [Task-specific heads/outputs] → [Weighted loss aggregation] → [Gradient normalization update]

- **Critical path:**
  1. Prompt pool initialization (K prompts, typically K=8–32 based on task count)
  2. Task embedding initialization (random or from task name embeddings)
  3. Scheduler weight matrix W initialization
  4. Gating matrix W_g initialization
  5. Temperature hyperparameter selection (start at 0.9 based on paper findings)
  6. Multi-task training with gradient normalization

- **Design tradeoffs:**
  - Prompt pool size K: Larger K offers more expressive combinations but increases scheduling complexity and memory. Paper uses K proportional to task diversity (moderate scales optimal per Figure 3).
  - Temperature: Low (0.5) = concentrated selection, high (1.3+) = uniform mixing. Paper shows 0.9 is optimal—too concentrated misses beneficial cross-prompt signals; too uniform dilutes task-specific focus.
  - Task count: Performance peaks at ~12 tasks, declines at 16–20. Trade-off between semantic sharing benefits and interference costs.

- **Failure signatures:**
  - Prompt collapse: Scheduler weights converge to near-identical distributions across tasks → task embeddings not differentiating. Check w_t variance across tasks.
  - Gate saturation: σ(W_g · e_t) stuck near 0 or 1 → either task embedding or prompt dominates entirely. Monitor gate value distribution.
  - Temperature mismatch: If performance degrades sharply at task scale changes, temperature may need re-tuning for new task distributions.
  - Negative transfer resurfaces: If adding tasks hurts specific task performance despite scheduling, task objectives may be fundamentally conflicting—consider task grouping.

- **First 3 experiments:**
  1. Ablate the scheduler: Compare (a) fixed single prompt per task vs. (b) dynamic scheduling. Measure on 4–8 task subset of CrossFit. Expect 5–15% gap in transfer gain if mechanism works.
  2. Temperature sweep: Replicate Figure 2 on your task set. Validate that 0.7–1.0 range is optimal; if your optimal differs significantly, investigate task heterogeneity differences.
  3. Gate removal test: Remove gating (use simple concatenation or averaging of e_t and p_t) and measure SuperGLUE/MMLU degradation. Isolates contribution of gated fusion vs. scheduling alone.

## Open Questions the Paper Calls Out

- Can cross-modal prompt fusion extend dynamic prompt scheduling to vision-language or audio-language multi-task settings? Current framework only evaluates text-only tasks; cross-modal integration requires new mechanisms for aligning heterogeneous prompt spaces across modalities.

- How can prompt scheduling be optimized for edge and low-resource deployment scenarios with limited compute? Current experiments assume sufficient compute for prompt pool queries, gating mechanisms, and gradient normalization; real-time edge constraints may require lightweight alternatives.

- What mechanisms can enable adaptive latency-aware scheduling under real-time constraints? Current scheduler optimizes for accuracy, not inference speed; dynamic prompt fusion adds computational overhead that may violate latency budgets in production systems.

- Why does performance decline when task count exceeds 12, and can the scheduler scale to 50+ heterogeneous tasks? The paper demonstrates robustness only under "moderate task scales"; the interference mechanism at larger scales remains uncharacterized.

## Limitations

- The paper lacks critical implementation details including base LLM architecture, prompt pool size K, prompt length, and task embedding dimensions, limiting reproducibility.
- Learning rate, batch size, optimizer configuration, and exact training duration are absent from the experimental setup.
- The specific CrossFit task subset used is unspecified, making it difficult to replicate the exact evaluation conditions.
- Performance declines at task scales above 12-16 tasks, though the paper doesn't propose solutions for this interference problem.

## Confidence

- **High confidence:** The mechanism of dynamic prompt scheduling with task-aware weights is well-specified and theoretically sound. The temperature sensitivity analysis (optimal at 0.9) is clearly demonstrated and reproducible.
- **Medium confidence:** The gated task embedding fusion mechanism is described with sufficient mathematical detail for implementation, though the specific architectural choices (embedding dimensions, gate parameterization) are underspecified.
- **Medium confidence:** The gradient-normalized multi-task loss balancing addresses a real problem (negative transfer), but the paper doesn't provide sufficient detail on how gradient normalization is implemented or how task weights λ_t are adaptively updated.

## Next Checks

1. **Scheduler ablation study:** Implement and compare fixed single prompts per task versus dynamic scheduling on a 6-8 task subset of CrossFit. Measure SuperGLUE accuracy and Prompt Transfer Gain to isolate the scheduling contribution.

2. **Temperature robustness validation:** Replicate the temperature sweep experiment on your chosen task set. Verify that performance peaks in the 0.7-1.0 range and that extreme temperatures (>1.3 or <0.5) cause significant degradation.

3. **Gate contribution isolation:** Remove the gating mechanism and use simple concatenation or averaging of task embeddings with prompts. Compare SuperGLUE/MMLU performance to quantify the marginal benefit of gated fusion versus scheduling alone.