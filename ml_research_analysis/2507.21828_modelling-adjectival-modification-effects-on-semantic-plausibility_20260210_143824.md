---
ver: rpa2
title: Modelling Adjectival Modification Effects on Semantic Plausibility
arxiv_id: '2507.21828'
source_url: https://arxiv.org/abs/2507.21828
tags:
- plausibility
- https
- evaluation
- sentence
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the task of predicting changes in semantic
  plausibility induced by adjectival modifiers, using the ADEPT benchmark dataset.
  We introduce a novel modeling approach based on sentence transformers, which process
  sentence pairs in parallel to predict similarity scores.
---

# Modelling Adjectival Modification Effects on Semantic Plausibility

## Quick Facts
- arXiv ID: 2507.21828
- Source URL: https://arxiv.org/abs/2507.21828
- Reference count: 25
- Primary result: Sentence transformers underperform standard transformers on adjectival plausibility classification despite conceptual alignment

## Executive Summary
This study investigates how adjectival modifiers affect semantic plausibility using the ADEPT benchmark dataset. The authors introduce a novel sentence transformer approach for processing sentence pairs to predict plausibility changes, finding that while transformer models struggle with this task, sentence transformers surprisingly under-perform compared to models like RoBERTa and MPNet. The research highlights the critical importance of balanced training and testing datasets, demonstrating that class imbalances can significantly distort evaluation metrics and weaken result trustworthiness.

## Method Summary
The study employs sentence transformers that process sentence pairs in parallel to predict similarity scores for 3-class plausibility classification (less likely, equally likely, more likely). Models are fine-tuned on the ADEPT dataset with classes `impossible` and `necessarily true` removed, and the majority class `equally likely` downsampled to ~1,500 instances for balanced training. Evaluation uses a cross-balanced sliding window approach to address test set imbalance, with F1-macro as the primary metric alongside accuracy and ROC-AUC.

## Key Results
- Both transformer models and sentence transformers struggle with adjectival plausibility classification
- Sentence transformers underperform RoBERTa and MPNet despite conceptual alignment with sentence-pair tasks
- Balanced training improves cross-balanced evaluation metrics but reduces standard evaluation accuracy
- Class imbalance in test sets inflates accuracy by rewarding majority-class prediction

## Why This Works (Mechanism)

### Mechanism 1: Cross-Balanced Evaluation Corrects Metric Inflation
Class imbalance in test sets inflates accuracy by rewarding majority-class prediction; balanced evaluation reveals true model capabilities. The cross-balanced method extracts equal-sized samples per class via sliding windows, averaging metrics across iterations.

### Mechanism 2: Standard Transformers Outperform Sentence Transformers
Transformer models with classification heads achieve higher F1-macro than sentence transformers despite ST's conceptual fit. Standard transformers process concatenated pairs and learn task-specific attention patterns; sentence transformers encode sentences independently then compare via cosine similarity.

### Mechanism 3: Training Data Balance Improves Cross-Balanced Performance
Balanced training prevents models from learning majority-class bias, improving per-class performance but reducing accuracy on imbalanced test sets. Down-sampling the majority class during training yields comparable results across models in cross-balanced evaluation.

## Foundational Learning

- **Class imbalance effects on evaluation metrics**: Why needed here - the paper's central finding is that prior work's accuracy claims were inflated by imbalanced evaluation. Quick check: If a test set is 80% class A and 20% class B, what accuracy does a majority-class baseline achieve?

- **Sentence transformer architecture (Siamese networks)**: Why needed here - the paper introduces ST as a novel approach for this task. Quick check: What information might be lost when encoding two sentences separately versus processing them jointly with cross-attention?

- **Semantic plausibility vs. typicality**: Why needed here - the ADEPT task distinguishes plausibility changes from mere atypicality. Quick check: Why might "a dead horse goes away" be semantically similar to "a horse goes away" but differ drastically in plausibility?

## Architecture Onboarding

- **Component map**: Input sentence pair → Standard Transformer: tokenize as [CLS] s1 [SEP] s2 [SEP] → classification head OR Sentence Transformer: encode s1 and s2 independently → cosine similarity → threshold into classes → Evaluation: cross-balanced or standard

- **Critical path**: Filter ADEPT dataset (remove impossible and necessarily true classes), down-sample equally likely class to ~1,500 instances, fine-tune RoBERTa/MPNet for 3 epochs (lr=2e-5), evaluate using cross-balanced method, report F1-macro

- **Design tradeoffs**: ST vs. Standard Transformer (ST conceptually aligned but empirically underperforms), balanced vs. imbalanced training (balanced improves per-class metrics but may hurt accuracy), 3-class vs. 5-class setup (simpler but loses edge cases)

- **Failure signatures**: Model predicts "equally likely" for most inputs (majority-class bias), high standard-evaluation accuracy but low cross-balanced F1 (exploiting test imbalance), ST outputs clustered near similarity threshold boundaries (cosine similarity may not capture plausibility directionality)

- **First 3 experiments**: 1) Replicate RoBERTa-balanced with cross-balanced evaluation to establish baseline, 2) Ablate training balance: train on imbalanced data, evaluate cross-balanced, 3) Error analysis on confusion matrix: inspect which adjectives most commonly cause misclassification

## Open Questions the Paper Calls Out

- Do specific linguistic patterns cause sentence transformers to underperform compared to standard transformers in predicting plausibility changes? The authors recommend investigating model error analysis in greater depth.

- Do language models struggle equally with adjectival plausibility changes in languages other than English? The authors explicitly recommend replicating experiments for other languages.

- Why do sentence transformers underperform standard transformers like RoBERTa on this task despite their conceptual alignment? The paper demonstrates the performance gap but doesn't investigate the architectural reasons.

## Limitations

- The sentence transformer architecture's underperformance is surprising but the paper doesn't definitively explain why the siamese network structure fails to capture adjectival shifts better than standard classifiers

- Reproduction barriers exist due to unspecified batch size, loss function details for ST, and max sequence length

- The cross-balanced evaluation method assumes that windowed performance generalizes to real-world skewed distributions where majority-class prediction might be operationally correct

## Confidence

- **High Confidence**: Class imbalance inflates accuracy metrics and cross-balanced evaluation provides more reliable performance estimates (directly demonstrated through contrast between standard and cross-balanced results)

- **Medium Confidence**: Sentence transformers underperforms standard transformers on this task (empirical results clear but architectural explanation plausible but not definitively proven)

- **Low Confidence**: Balanced training universally improves model performance (improves cross-balanced metrics but reduces standard accuracy, suggesting relationship depends on evaluation methodology)

## Next Checks

1. **Error Analysis Deep Dive**: Examine model confusion matrices to identify which specific adjectives and semantic domains cause the most classification errors, and whether ST's independent encoding particularly fails on certain modifier types

2. **Hyperparameter Ablation for ST**: Systematically vary batch size, learning rate, and loss function parameters for the sentence transformer to determine if underperformance is architectural or due to insufficient tuning

3. **Real-world Deployment Simulation**: Create test sets with varying class distributions to evaluate whether cross-balanced F1 scores actually predict performance in deployment scenarios where majority-class prediction might be correct