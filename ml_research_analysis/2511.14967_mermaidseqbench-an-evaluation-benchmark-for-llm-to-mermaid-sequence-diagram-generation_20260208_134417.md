---
ver: rpa2
title: 'MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram
  Generation'
arxiv_id: '2511.14967'
source_url: https://arxiv.org/abs/2511.14967
tags:
- sequence
- mermaid
- diagrams
- diagram
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MermaidSeqBench is a human-verified benchmark for evaluating LLM-generated
  Mermaid sequence diagrams, consisting of 132 structured natural language-prompt
  diagram pairs. The benchmark uses LLM-as-a-judge evaluation across six fine-grained
  metrics: syntax, Mermaid-specific correctness, logic, completeness, activation handling,
  and error/status tracking.'
---

# MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation

## Quick Facts
- **arXiv ID:** 2511.14967
- **Source URL:** https://arxiv.org/abs/2511.14967
- **Reference count:** 40
- **Primary result:** Human-verified benchmark (132 samples) using LLM-as-a-judge evaluation across six metrics reveals clear scaling effects and capability gaps in LLM-generated Mermaid sequence diagrams.

## Executive Summary
MermaidSeqBench addresses the challenge of evaluating LLM-generated Mermaid sequence diagrams by providing a human-verified benchmark with 132 structured natural language-prompt diagram pairs. The benchmark employs an LLM-as-a-judge approach across six fine-grained metrics: syntax, Mermaid-specific correctness, logic, completeness, activation handling, and error/status tracking. Experiments with six model families (Qwen 2.5, Llama 3.1/3.2, Granite 3.3) reveal consistent scaling effects, with larger models (7B/8B) outperforming smaller ones (0.5B-2B), and expose meaningful capability gaps particularly in error handling and completeness.

## Method Summary
The benchmark uses a three-stage dataset construction pipeline: 10 human-crafted seed diagrams, 30 synthetic samples generated via an SDG pipeline with Mistral Large, and rule-based augmentation that quadruples coverage through syntactic variations like alt/else reordering and participant renaming. Evaluation employs two large LLMs (DeepSeek-V3 671B, GPT-OSS 120B) as judges, scoring each generated diagram across six dimensions on a 0-1 scale. The methodology combines human verification for quality with synthetic expansion for coverage, enabling scalable evaluation of structured diagram generation where traditional metrics fail.

## Key Results
- Larger models (7B/8B) consistently outperform smaller ones (0.5B-2B) across all six metrics
- Qwen 2.5-7B and Llama 3.1-8B achieve highest scores, particularly on syntax and logical flow
- Granite 3.3-8B lags slightly on error handling and completeness despite competitive syntax performance
- Clear scaling effects emerge in structured diagram generation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-as-a-judge enables scalable, multi-dimensional evaluation of structured diagram generation where traditional metrics fail.
- **Mechanism:** A large LLM receives the NL prompt, reference diagram, and candidate diagram, then scores across six fine-grained dimensions (syntax, Mermaid-specific correctness, logic, completeness, activation handling, error/status tracking). This bypasses the need for deterministic parsers that cannot assess semantic correctness.
- **Core assumption:** The judge LLM possesses sufficient understanding of Mermaid syntax and diagram semantics to reliably discriminate quality differences.
- **Evidence anchors:** Abstract states use of LLM-as-a-judge model to assess generation across fine-grained metrics; Section 4.1 describes six-dimensional scoring; MCeT paper uses similar approach for behavioral model evaluation.

### Mechanism 2
- **Claim:** Hybrid dataset construction (human-verified seed + synthetic expansion + rule-based augmentation) yields benchmark diversity while maintaining quality.
- **Mechanism:** Manual seed (10 diagrams) ensures correctness; SDG pipeline with Mistral Large expands coverage; rule-based transformations (alt/else reordering, participant renaming) create syntactic variations without semantic drift. Total: 132 NL-diagram pairs.
- **Core assumption:** Rule-based variations preserve semantic equivalence while surface-level changes expose model brittleness to naming/ordering differences.
- **Evidence anchors:** Section 3.3 describes deterministic augmentation rules producing four-fold increase in coverage while preserving logical meaning; full pipeline described in Sections 3.1-3.3.

### Mechanism 3
- **Claim:** Clear scaling effects emerge in structured diagram generation—larger models (7B/8B) consistently outperform smaller ones (0.5B-2B) across all six metrics.
- **Mechanism:** Parameter scale correlates with improved syntax adherence, logical flow tracking, and complex construct handling (activations, error paths). Smaller models struggle particularly with completeness and error/status tracking.
- **Core assumption:** Observed performance gaps reflect genuine capability differences, not just judge model preference for certain output styles.
- **Evidence anchors:** Section 4.2 shows larger models consistently outperform smaller counterparts; Table 1 demonstrates Qwen 2.5-7B scoring 91.29 syntax vs. 58.90 for 0.5B (DeepSeek-V3 judge); scaling effects consistent with general LLM scaling literature.

## Foundational Learning

- **Concept: Mermaid sequence diagram syntax**
  - **Why needed here:** The benchmark evaluates correctness of Mermaid syntax including `participant`, `activate`/`deactivate`, `alt`/`else`/`end` blocks, and arrow types (`->>`, `-->>`).
  - **Quick check question:** Can you write a minimal Mermaid sequence diagram with two participants, one synchronous call, one return arrow, and an activation bar?

- **Concept: LLM-as-a-judge evaluation paradigm**
  - **Why needed here:** Understanding how one LLM scores another's output—prompting strategy, scoring scales, potential biases—is essential to interpreting results.
  - **Quick check question:** Why might DeepSeek-V3 and GPT-OSS assign different scores to the same candidate diagram?

- **Concept: Benchmark validity and coverage**
  - **Why needed here:** The 132-sample benchmark must cover enough structural patterns (nested conditionals, multiple actors, error paths) to expose model weaknesses.
  - **Quick check question:** If all 132 samples used only 2-3 participants with no error handling, what capability gaps would remain undetected?

## Architecture Onboarding

- **Component map:** NL Prompt → Candidate LLM → Mermaid Output → Judge LLM → 6-dimensional scores
- **Critical path:** Load 132 NL-diagram pairs → prompt candidate LLM to generate Mermaid from NL description → pass (NL, reference, candidate) to judge LLM with scoring rubric → aggregate scores across 6 dimensions; compare across models/sizes
- **Design tradeoffs:** Single vs. multiple judges (uses two to reduce bias but doubles compute); human vs. synthetic data (10 seeds ensure quality, 122 synthetic enable scale); fine-grained (6 metrics) vs. coarse (single score) (more diagnostic but requires careful prompt engineering)
- **Failure signatures:** Small models (0.5B-2B) show low completeness scores, missing activation bars, truncated alt blocks; Granite 3.3-8B relative weakness in error/status tracking scores; judge disagreement with GPT-OSS systematically harsher than DeepSeek-V3
- **First 3 experiments:** 1) Reproduce baseline scores for one model family using one judge to validate pipeline integration; 2) Ablate rule-based augmentations—evaluate on seed+SDG only vs. full 132 samples to measure augmentation value; 3) Test prompt sensitivity—vary NL description format (structured vs. free-form) to assess robustness of top models

## Open Questions the Paper Calls Out

- **Question:** How well do LLM-as-a-judge evaluations correlate with human expert evaluations for Mermaid sequence diagram quality?
- **Basis in paper:** The paper relies entirely on LLM judges (DeepSeek-V3 and GPT-OSS) for scoring, with no human ground-truth comparison reported for the evaluation phase.
- **Why unresolved:** The two judges already show scoring discrepancies, but no human baseline establishes which judge is more accurate.
- **What evidence would resolve it:** A human expert annotation study on a subset of generated diagrams, correlating human scores with LLM judge scores.

- **Question:** Can the benchmark methodology and evaluation metrics transfer effectively to other Mermaid diagram types (class, flowchart, state) or other textual representations like PlantUML?
- **Basis in paper:** The paper explicitly states "our methodology can be applied to other structural diagrams or even other textual representations such as PlantUML."
- **Why unresolved:** The paper only validates the approach on sequence diagrams; transferability is claimed but not demonstrated.
- **What evidence would resolve it:** Applying the same benchmark construction pipeline to another diagram type and evaluating whether similar capability gaps emerge across models.

- **Question:** What architectural or training differences explain why Qwen 2.5-7B and Llama 3.1-8B outperform Granite 3.3-8B on error handling and completeness despite similar parameter scales?
- **Basis in paper:** Results show clear performance gaps at the 7B-8B scale across families, with Granite lagging on specific metrics, but no analysis explains the root cause.
- **Why unresolved:** The paper reports scaling effects but does not investigate whether differences stem from training data, architecture, or tokenization.
- **What evidence would resolve it:** Controlled experiments varying training data composition or ablation studies across model families on the specific subtasks.

## Limitations

- Dataset size of 132 samples may not cover full spectrum of complex Mermaid constructs including nested conditionals and advanced timing constraints
- LLM-as-a-judge evaluation relies on two large models whose judgment consistency and potential biases remain unquantified
- Benchmark focuses exclusively on Qwen, Llama, and Granite families; results may not generalize to other architectures
- Rule-based augmentation pipeline may not capture all valid Mermaid variations, potentially underestimating model robustness

## Confidence

**High confidence:** Scaling effects (larger models outperform smaller ones) and syntax correctness measurements—align with established LLM scaling literature and have deterministic components (Mermaid parser validation).

**Medium confidence:** Completeness and error/status tracking metrics—require deeper semantic understanding and are more susceptible to judge model interpretation differences.

**Low confidence:** Cross-judge consistency and benchmark coverage completeness—paper does not report inter-judge correlation coefficients or systematic coverage analysis of Mermaid's full feature set.

## Next Checks

1. **Inter-judge reliability assessment:** Run all samples through 3-5 different judge models and compute inter-rater reliability (e.g., Cohen's kappa) for each metric to quantify scoring consistency.

2. **Coverage gap analysis:** Manually inspect the 132 samples to identify which Mermaid features (e.g., loop constructs, parallel sequences, complex timing) are absent, then expand the benchmark to address these gaps.

3. **Prompt sensitivity study:** Systematically vary the NL prompt format (structured vs. free-form, different levels of detail) and measure performance degradation across all models to establish robustness bounds.