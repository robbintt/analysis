---
ver: rpa2
title: 'CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal
  LLMs'
arxiv_id: '2501.16629'
source_url: https://arxiv.org/abs/2501.16629
tags:
- preference
- optimization
- chip
- llav
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of hallucinations in Multimodal
  Large Language Models (MLLMs) by proposing a novel approach called Cross-modal Hierarchical
  Direct Preference Optimization (CHiP). The method introduces two key components:
  visual preference optimization and hierarchical textual preference optimization.'
---

# CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs

## Quick Facts
- arXiv ID: 2501.16629
- Source URL: https://arxiv.org/abs/2501.16629
- Reference count: 25
- Primary result: CHiP achieves 52.7% and 55.5% relative improvements in hallucination reduction compared to DPO for Muffin and LLaVA models respectively on Object HalBench

## Executive Summary
This paper addresses the persistent challenge of hallucinations in Multimodal Large Language Models (MLLMs) through a novel approach called Cross-modal Hierarchical Direct Preference Optimization (CHiP). The method introduces two key innovations: visual preference optimization and hierarchical textual preference optimization. These components work together to enable MLLMs to learn from both textual and visual preferences simultaneously while capturing preferences at multiple granular levels including response, segment, and token levels.

## Method Summary
CHiP proposes a dual optimization framework that tackles hallucination reduction from two complementary angles. The visual preference optimization module allows the model to align with both textual and visual feedback simultaneously, addressing the multimodal nature of the hallucination problem. The hierarchical textual preference optimization module captures preferences at three distinct levels - response, segment, and token - enabling fine-grained control over the generation process. This hierarchical approach allows the model to optimize for preferences at different granularities, from overall response quality to specific word choices.

## Key Results
- Achieves 52.7% relative improvement in hallucination reduction compared to DPO for Muffin model on Object HalBench
- Achieves 55.5% relative improvement in hallucination reduction compared to DPO for LLaVA model on Object HalBench
- Demonstrates effectiveness of combining visual and hierarchical textual preference optimization

## Why This Works (Mechanism)
The method works by addressing hallucinations through two complementary mechanisms. Visual preference optimization allows the model to ground its responses in visual evidence, reducing the likelihood of fabricating information not supported by the input images. Hierarchical textual preference optimization captures preferences at multiple levels of granularity, enabling the model to learn fine-grained patterns of hallucination-free responses while maintaining overall response coherence. The combination of these approaches provides both broad coverage and precise control over the generation process.

## Foundational Learning

1. **Multimodal Alignment**
   - Why needed: Ensures model responses are consistent with both textual and visual inputs
   - Quick check: Measure alignment accuracy between generated text and corresponding images

2. **Preference Learning**
   - Why needed: Enables model to distinguish between desirable and undesirable outputs
   - Quick check: Evaluate preference ranking accuracy on held-out samples

3. **Hierarchical Optimization**
   - Why needed: Allows optimization at different granularities for better control
   - Quick check: Compare performance at each hierarchy level

4. **Direct Preference Optimization (DPO)**
   - Why needed: Provides foundation for preference-based fine-tuning
   - Quick check: Verify baseline DPO implementation performance

5. **Cross-modal Learning**
   - Why needed: Enables integration of information from different modalities
   - Quick check: Test cross-modal consistency on multimodal inputs

6. **Hallucination Detection**
   - Why needed: Essential for measuring and improving hallucination reduction
   - Quick check: Validate hallucination detection accuracy on benchmark datasets

## Architecture Onboarding

**Component Map:**
Visual Pref Optimization -> Hierarchical Text Pref Optimization -> Final Response Generation

**Critical Path:**
1. Input processing (text + image)
2. Visual preference computation
3. Hierarchical preference computation (response → segment → token)
4. Combined preference signal generation
5. Model fine-tuning with preference signals

**Design Tradeoffs:**
- Granularity vs. computational efficiency in hierarchical optimization
- Visual vs. textual preference weighting
- Model complexity vs. training stability

**Failure Signatures:**
- Overfitting to training preferences
- Loss of general knowledge in pursuit of hallucination reduction
- Degradation in non-hallucination related capabilities

**First Experiments:**
1. Ablation study: Remove visual preference component
2. Ablation study: Remove hierarchical structure
3. Scalability test: Measure training time and memory usage

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on hallucination-specific benchmarks, limiting generalizability assessment
- Performance metrics lack absolute values and statistical significance reporting
- Computational efficiency and scalability concerns with dual optimization approach

## Confidence
- Effectiveness of CHiP in reducing hallucinations: High (supported by benchmark results, though limited in scope)
- Superiority over DPO baseline: Medium (relative improvements are significant but absolute performance and statistical significance are unclear)
- Generalizability across different MLLM architectures: Low (only tested on Muffin and LLaVA models)

## Next Checks
1. Conduct ablation studies to isolate the contributions of visual preference optimization and hierarchical textual preference optimization components.
2. Evaluate CHiP's performance on diverse, real-world multimodal datasets beyond hallucination-specific benchmarks to assess generalization.
3. Perform long-term stability analysis to measure performance degradation over extended use and across different domains.