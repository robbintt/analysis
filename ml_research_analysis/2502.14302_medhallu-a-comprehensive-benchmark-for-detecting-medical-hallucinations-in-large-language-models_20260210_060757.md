---
ver: rpa2
title: 'MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in
  Large Language Models'
arxiv_id: '2502.14302'
source_url: https://arxiv.org/abs/2502.14302
tags:
- hallucination
- medical
- llms
- detection
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedHallu, the first benchmark specifically
  designed for medical hallucination detection in large language models (LLMs). MedHallu
  contains 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated
  answers systematically generated through a controlled pipeline.
---

# MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2502.14302
- Source URL: https://arxiv.org/abs/2502.14302
- Authors: Shrey Pandit; Jiawei Xu; Junyuan Hong; Zhangyang Wang; Tianlong Chen; Kaidi Xu; Ying Ding
- Reference count: 40
- State-of-the-art LLMs struggle with medical hallucination detection, with best model achieving F1 as low as 0.625 for "hard" hallucinations

## Executive Summary
This paper introduces MedHallu, the first benchmark specifically designed for medical hallucination detection in large language models (LLMs). The authors find that state-of-the-art LLMs struggle with medical hallucination detection, with the best model achieving an F1 score as low as 0.625 for detecting "hard" category hallucinations. The study reveals that hallucinated answers semantically closer to ground truth are harder to detect. Incorporating domain-specific knowledge and introducing a "not sure" category as one of the answer options improves precision and F1 scores by up to 38% relative to baselines.

## Method Summary
MedHallu contains 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. The generation process involves quality checking via ensemble voting across three LLMs, bidirectional entailment checking for semantic distinctness from ground truth, TextGrad refinement for failed samples, and fallback selection based on semantic similarity. The benchmark evaluates 13 discriminator models on their ability to detect hallucinations under different conditions: zero-shot, with knowledge context, and with a "not sure" option.

## Key Results
- State-of-the-art LLMs achieve F1 scores as low as 0.625 for detecting "hard" category hallucinations
- Incorporating domain-specific knowledge improves precision and F1 scores by up to 38% relative to baselines
- Introducing a "not sure" option allows models to abstain from uncertain predictions, improving performance
- General-purpose LLMs outperform fine-tuned medical LLMs in medical hallucination detection tasks when knowledge is provided

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hallucinated answers that are semantically closer to ground truth are significantly harder for LLMs to detect.
- **Mechanism:** Detection difficulty correlates with semantic proximity—when hallucinations share conceptual overlap with correct answers, models cannot rely on surface-level divergence signals. Bidirectional entailment clustering shows uniform detection patterns within clusters, indicating semantic content drives discrimination rather than lexical features.
- **Core assumption:** The bidirectional entailment score accurately captures meaningful semantic relationships between hallucinated and ground truth text.
- **Evidence anchors:** [abstract] "Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth." [section 5.3] "Clusters containing samples that reliably fool detection LLMs are notably closer to the ground truth answer in semantic vector space... with p-value 0.004 for cosine similarity."

### Mechanism 2
- **Claim:** Providing domain-specific knowledge substantially improves hallucination detection, with larger general-purpose LLMs benefiting more than fine-tuned medical models.
- **Mechanism:** External knowledge provides a grounding reference for verification. General LLMs (+0.251 F1 average) improve more than medical fine-tuned models (+0.138 F1) because fine-tuned models already encode domain knowledge but may lack the reasoning flexibility to apply it to novel verification tasks.
- **Core assumption:** The provided knowledge context is accurate and sufficient for verification; models can effectively integrate this context into their reasoning.
- **Evidence anchors:** [abstract] "Incorporating domain-specific knowledge... improves precision and F1 scores by up to 38% relative to baselines." [table 2] Gemma-2-9B shows largest improvement (+0.323 F1); general LLMs average +0.251 vs. medical models +0.138.

### Mechanism 3
- **Claim:** Introducing a "not sure" option allows models to abstain from uncertain predictions, improving precision and F1 scores.
- **Mechanism:** Abstention filters low-confidence predictions. General LLMs selectively skip uncertain questions (Qwen2.5-14B responds to only 27.9%), while medical fine-tuned models rarely abstain (OpenBioLLM: 99.7% response rate), often to their detriment.
- **Core assumption:** Models can accurately calibrate their uncertainty and will appropriately use the abstention option rather than over-using it.
- **Evidence anchors:** [abstract] "Introducing a 'not sure' category as one of the answer categories improves the precision and F1 scores by up to 38%." [section 5.4] GPT-4o achieves 79.5% F1 with "not sure" option; general LLMs show 10-15% improvement while smaller models gain 3-5%.

## Foundational Learning

- **Concept: Bidirectional Entailment (NLI)**
  - Why needed here: The paper uses NLI scores to verify hallucinations are semantically distinct from ground truth and to cluster responses by meaning. Understanding entailment directionality is essential for the correctness checking pipeline.
  - Quick check question: Given statements A and B, if A→B has entailment score 0.9 and B→A has 0.3, are they semantically equivalent?

- **Concept: Semantic Similarity via Embeddings**
  - Why needed here: Cosine similarity between embeddings determines fallback selection when quality checks fail, and measures proximity between hallucinations and ground truth for difficulty analysis.
  - Quick check question: Two medical statements with 0.85 cosine similarity—would you expect them to be easy or hard to distinguish as hallucinated vs. factual?

- **Concept: Multi-Model Ensemble Voting**
  - Why needed here: The quality filtering phase uses majority voting across three LLMs (GPT-4o-mini, Gemma2-9B, Qwen2.5-7B) to classify difficulty levels and reduce individual model biases.
  - Quick check question: If 2/3 discriminator models are fooled by a hallucinated answer, what difficulty level should it receive?

## Architecture Onboarding

- **Component map:** PubMedQA Source → Candidate Generator (Qwen2.5-14B) → Quality Check (3 LLM ensemble voting) → Correctness Check (DeBERTa NLI, τ=0.75) → [Pass] → Difficulty Label (Easy/Medium/Hard) → [Fail] → TextGrad Refinement → Regenerate (max 5 attempts) → [Still Fail] → Fallback (max similarity selection)

- **Critical path:** The correctness check (bidirectional entailment) is the gatekeeper—if this passes but quality fails, TextGrad refines. If correctness fails, answers are rejected regardless of quality scores.

- **Design tradeoffs:**
  - Temperature range (0.3-0.7): Lower = more coherent but less diverse; higher = more creative hallucinations but more rejections
  - NLI threshold (0.75): Higher = stricter semantic separation but more fallbacks needed
  - Max regeneration attempts (5): More attempts = higher quality but increased compute (26.5 hours on 4x A6000 for 10K samples)

- **Failure signatures:**
  - High fallback rate (>20%): Indicates generator model struggles with certain hallucination categories—review category definitions
  - Cluster contamination (ground truth appearing in hallucination clusters): NLI threshold too low
  - Medical models outperforming general models unexpectedly: Check knowledge context is actually being provided in prompts

- **First 3 experiments:**
  1. **Baseline replication:** Run the 13 discriminator models (table 2) on the 1,000 pqa_labeled split with and without knowledge context; verify F1 ranges match (general LLMs: 0.515-0.737 without knowledge, 0.676-0.877 with knowledge)
  2. **Difficulty stratification validation:** Sample 50 examples each from Easy/Medium/Hard; manually verify that "hard" examples have higher semantic similarity to ground truth (target: >0.70 cosine similarity for hard vs. <0.65 for easy)
  3. **Abstention calibration test:** Run discriminators with "not sure" option enabled; verify general LLMs show appropriate response rates (target: 30-60% for well-calibrated models) and medical models show higher rates (70-95%)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do advanced reasoning models (e.g., OpenAI o1, DeepSeek-R1) significantly outperform current state-of-the-art baselines in detecting "hard" category hallucinations?
- **Basis in paper:** [explicit] Section 7 (Limitations) states that due to resource constraints, the authors "could not employ the most advanced reasoning models (e.g., OpenAI o1, Gemini 2.0, DeepSeek-R1) for benchmark generation."
- **Why unresolved:** It is unknown if these specific architectures, designed for complex reasoning, can bridge the performance gap on "hard" hallucinations which currently stymie models like GPT-4o (F1 0.625).
- **What evidence would resolve it:** Benchmarking the specific reasoning models listed in the limitation against the MedHallu "hard" subset.

### Open Question 2
- **Question:** Can chain-of-thought (CoT) or self-consistency prompting strategies improve hallucination detection F1 scores beyond the input-output baselines reported?
- **Basis in paper:** [explicit] Section 7 notes that the evaluation was "restricted to input-output prompting (zero-shot, with/without knowledge provision)" and "resource limitations precluded exploration of advanced techniques like chain-of-thought."
- **Why unresolved:** The paper establishes a zero-shot baseline but leaves open the possibility that eliciting intermediate reasoning steps could help models distinguish semantically close hallucinations.
- **What evidence would resolve it:** Re-evaluating the MedHallu dataset using CoT prompting and comparing the resulting precision and F1 scores against Table 2.

### Open Question 3
- **Question:** How does the pipeline's effectiveness and model performance vary when generating hallucinations from diverse medical corpora outside of PubMedQA?
- **Basis in paper:** [explicit] Section 7 identifies the reliance on the PubMedQA corpus as a constraint, suggesting "future work should incorporate diverse high-quality corpora to improve scalability and domain coverage."
- **Why unresolved:** The current results are tied to the specific linguistic patterns and knowledge density of PubMedQA abstracts; it is unclear if the "hard" hallucination patterns generalize to other medical text types.
- **What evidence would resolve it:** Applying the described generation pipeline (Figure 2) to a different dataset (e.g., clinical notes or medical textbooks) and analyzing the detection difficulty distribution.

## Limitations
- Benchmark generation pipeline relies heavily on LLM-based filtering, creating potential systematic biases
- Study focuses exclusively on PubMedQA's question-answering format, limiting generalizability to other medical text types
- TextGrad refinement process uses GPT-4o-mini as a black box without transparency into feedback incorporation

## Confidence

- **High Confidence:** General-purpose LLMs outperform medical fine-tuned models in hallucination detection when knowledge is provided
- **Medium Confidence:** Semantic proximity between hallucinated and ground truth answers determines detection difficulty
- **Medium Confidence:** The "not sure" option improves precision and F1 scores by up to 38%

## Next Checks
1. **External Domain Validation:** Test MedHallu-trained detection models on medical QA data from other sources (e.g., MedQA, MedMCQA) to assess generalization beyond PubMedQA's question format and answer style.

2. **Clinical Relevance Assessment:** Have medical experts evaluate whether "hard" hallucinations from MedHallu represent clinically significant errors that could impact patient care, or whether they remain academic constructs that wouldn't appear in real clinical settings.

3. **Dynamic Knowledge Integration Test:** Evaluate whether providing context dynamically (retrieving from knowledge bases during inference) performs comparably to static context provision, addressing the practical limitation that knowledge context may not always be available at detection time.