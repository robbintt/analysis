---
ver: rpa2
title: A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual
  Decoding
arxiv_id: '2511.02565'
source_url: https://arxiv.org/abs/2511.02565
tags:
- semantic
- visual
- fmri
- subject-agnostic
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VCFLOW, the first subject-agnostic framework\
  \ for fMRI-to-video reconstruction. The method models the human visual system\u2019\
  s dual-stream architecture, using hierarchical feature extraction from early visual,\
  \ ventral, and dorsal streams."
---

# A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding

## Quick Facts
- arXiv ID: 2511.02565
- Source URL: https://arxiv.org/abs/2511.02565
- Authors: Jingyu Lu; Haonan Wang; Qixiang Zhang; Xiaomeng Li
- Reference count: 19
- Primary result: VCFLOW achieves subject-agnostic fMRI-to-video reconstruction with 14.0% 50-way classification accuracy and 7% performance drop compared to subject-specific models

## Executive Summary
VCFLOW introduces the first subject-agnostic framework for decoding videos from fMRI data of unseen subjects without requiring retraining. The method is inspired by the human visual system's dual-stream architecture, extracting hierarchical features from early visual, ventral, and dorsal streams. It employs CLIP-based alignment and contrastive learning to extract subject-invariant semantic representations, achieving competitive performance while offering a scalable solution for clinical applications. The model can reconstruct videos in approximately 10 seconds, making it a promising approach for real-world deployment.

## Method Summary
VCFLOW implements a three-stage architecture for subject-agnostic fMRI-to-video reconstruction. First, Hierarchical Cognitive Feature Extraction (HCAM) extracts features from early visual, ventral, and dorsal streams using a ViT backbone and aligns them to CLIP embeddings through BiMixCo loss and diffusion prior. Second, Subject-Agnostic Representation Alignment (SARA) disentangles subject-invariant and subject-specific tokens using a redistribution layer with InfoNCE inter-subject loss and subject classifier. Finally, Hierarchical Explicit Decoder (HED) generates videos using Stable Diffusion conditioned on fused embeddings with progressive loss weight scheduling. The method requires no subject-specific training and can decode videos from unseen subjects in approximately 10 seconds.

## Key Results
- Achieves 14.0% accuracy on 50-way classification task
- SSIM score of 0.396 and PSNR of 10.478
- Only 7% performance drop compared to subject-specific models
- Reconstruction time of ~10 seconds per video

## Why This Works (Mechanism)
VCFLOW's success stems from its cognitive-inspired architecture that mimics the human visual system's hierarchical processing. By extracting features from early visual, ventral, and dorsal streams separately, the model captures both semantic content and spatial information. The SARA component effectively separates subject-invariant semantic information from subject-specific neural patterns through contrastive learning and redistribution mechanisms. The HED decoder, conditioned on CLIP-aligned embeddings, ensures semantic coherence in the reconstructed videos while maintaining reasonable temporal dynamics.

## Foundational Learning
- fMRI preprocessing and surface reconstruction: Converts volumetric fMRI to 32k fs_LR surface format with 8,405 visual ROI vertices (needed for standardized cross-subject comparison; check by verifying vertex count and ROI alignment)
- Hierarchical visual stream extraction: Separates early visual, ventral, and dorsal features to capture different aspects of visual processing (needed for comprehensive feature representation; check by comparing feature distributions across streams)
- Contrastive learning for subject invariance: Uses InfoNCE loss to learn representations that generalize across subjects (needed for true subject-agnostic decoding; check by testing on held-out subjects)
- CLIP alignment: Maps neural features to semantic embeddings for coherent reconstruction (needed for meaningful video generation; check by measuring CLIP-pcc scores)

## Architecture Onboarding
Component map: fMRI input -> HCAM (ViT backbone) -> SARA (redistribution layer) -> HED (Stable Diffusion) -> video output

Critical path: The SARA component is critical as it determines the quality of subject-invariant representations. Without effective disentanglement, the model cannot generalize to unseen subjects.

Design tradeoffs: The framework prioritizes subject-agnostic performance over fine-grained reconstruction quality, accepting a 7% performance drop for scalability. The use of CLIP embeddings ensures semantic coherence but may limit detailed visual fidelity.

Failure signatures: Poor cross-subject transfer manifests as high subject classifier accuracy with degraded semantic alignment. Motion incoherence appears as temporally inconsistent reconstructions with low CLIP-pcc scores. Semantic drift affects rare categories due to limited training data.

Three first experiments:
1. Validate preprocessing pipeline by reconstructing a known stimulus and comparing to ground truth
2. Test subject-invariant feature extraction by training subject classifier on SARA outputs
3. Evaluate semantic alignment by measuring CLIP-pcc between reconstructed and original videos

## Open Questions the Paper Calls Out
None

## Limitations
- Missing exact loss weight hyperparameters (Î» values) for combined losses
- Unspecified ViT architecture details and training hyperparameters
- Reliance on extensive pretraining datasets (DIR+GOD) that may not be accessible to all researchers

## Confidence
Subject-agnostic decoding capability (Medium): Supported by architecture and metrics, but implementation details are missing
Performance metrics (High): SSIM, PSNR, and classification accuracy values are clearly reported
Clinical scalability claim (Medium): 10-second inference time supports claim, but practical deployment considerations are not discussed

## Next Checks
1. Verify cross-subject generalization by testing on a held-out subject from the cc2017 dataset not used in pretraining or validation
2. Conduct ablation studies to quantify each component's contribution, particularly focusing on subject-invariant token extraction impact
3. Test model robustness to fMRI preprocessing variations by comparing performance across different surface reconstructions and ROI definitions