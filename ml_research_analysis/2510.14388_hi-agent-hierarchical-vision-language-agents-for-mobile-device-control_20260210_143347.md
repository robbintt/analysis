---
ver: rpa2
title: 'Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control'
arxiv_id: '2510.14388'
source_url: https://arxiv.org/abs/2510.14388
tags:
- action
- reasoning
- instruction
- point
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hi-Agent introduces a hierarchical vision-language agent for mobile
  device control, featuring a jointly optimized high-level reasoning model and low-level
  action model. The design reformulates long-horizon tasks as sequences of single-step
  subgoals, using a foresight advantage function to guide high-level optimization
  with low-level feedback.
---

# Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control

## Quick Facts
- **arXiv ID**: 2510.14388
- **Source URL**: https://arxiv.org/abs/2510.14388
- **Reference count**: 40
- **Primary result**: Hi-Agent achieves 87.9% task success rate on AitW benchmark, setting new SOTA

## Executive Summary
Hi-Agent introduces a hierarchical vision-language agent architecture for mobile device control, featuring a jointly optimized high-level reasoning model and low-level action model. The system reformulates long-horizon tasks as sequences of single-step subgoals, using a foresight advantage function to guide high-level optimization with low-level feedback. This approach overcomes path explosion issues in Group Relative Policy Optimization (GRPO) and enables stable, critic-free joint training. Hi-Agent achieves state-of-the-art performance across multiple benchmarks, demonstrating superior generalization to UI layout perturbations and scaling effectively to larger models.

## Method Summary
Hi-Agent employs a hierarchical architecture with a high-level vision-language model (πh) generating semantic subgoals and a low-level model (πℓ) executing atomic UI actions. The system uses GRPO with a foresight advantage function combining format, environment, and feasibility rewards. Training alternates between updating πh and πℓ using oracle-generated trajectories. The high-level model maps screenshots and instructions to subgoals like "Open Chrome," while the low-level model grounds these to specific coordinates and actions. The foresight advantage function enables critic-free joint training by leveraging execution feedback and a frozen judge model's feasibility assessment.

## Key Results
- Achieves 87.9% task success rate on AitW-General benchmark, setting new state-of-the-art
- Demonstrates 68.8% success on AitW-WebShopping and 56.5% on AndroidWorld with 72B+7B models
- Outperforms prior methods across prompt-based, supervised, and reinforcement learning paradigms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating multi-step trajectories into single-step subgoals reduces sampling complexity of policy optimization.
- Mechanism: The architecture decouples planning and execution, reducing search space complexity from exponential (G^n) to linear (n·G).
- Core assumption: Optimal policy can be decomposed such that single-step subgoals are sufficient for low-level executor.
- Evidence: Abstract states reformulation as single-step subgoals; section 4.2 shows reduction from G^n to n·G; ODYSSEY highlights semantic reasoning bottlenecks in long-horizon tasks.

### Mechanism 2
- Claim: Foresight advantage function enables stable, critic-free joint training of hierarchical components.
- Mechanism: High-level model updated on composite signal including execution success and feasibility from frozen judge, propagating low-level feedback without separate value-critic.
- Core assumption: Frozen external VLM and immediate execution feedback are sufficient proxies for long-term value estimation.
- Evidence: Abstract mentions foresight advantage leveraging execution feedback; section 4.2 discusses alleviating path explosion and enabling critic-free training; LoHoVLA supports unified models for planning and control.

### Mechanism 3
- Claim: Semantic abstraction in high-level planner improves robustness to UI layout perturbations.
- Mechanism: Mapping raw pixels to semantic subgoals creates abstraction layer that remains valid across layout changes.
- Core assumption: Low-level action model has sufficient visual grounding to adapt to new layouts for fixed semantic instructions.
- Evidence: Section 5.1 shows Hi-Agent effective under layout shift while DigiRL drops sharply; section 1 discusses direct state-to-action mappings generalizing poorly; Mobile-Agent-RAG supports need for dynamic semantic reasoning.

## Foundational Learning

- **Markov Decision Processes (MDPs) & Hierarchical Decomposition**
  - Why needed: Mobile control defined as MDP with decomposition into subtasks to manage long time-horizons
  - Quick check: Explain why flat policy struggles with path explosion in 10-step mobile task vs. hierarchical policy

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: Hi-Agent builds on GRPO (critic-free RL method) for advantage estimation
  - Quick check: How does GRPO estimate advantage function differently from standard Actor-Critic methods like PPO?

- **Visual Grounding & Generalization**
  - Why needed: Separation of reasoning and action relies on low-level model's ability to ground instructions
  - Quick check: In UI perturbation experiment, why did flat agent (DigiRL) fail while hierarchical agent (Hi-Agent) succeed?

## Architecture Onboarding

- **Component map**: Screenshot + Instruction → πh (Reason Model) → Subgoal → πℓ (Action Model) → Atomic Action
- **Critical path**: 1) User Instruction + Screenshot input, 2) πh generates subgoal "Open Chrome", 3) πℓ grounds to coordinates, 4) Environment returns success/failure, 5) Foresight Reward computed, 6) GRPO updates πh and πℓ alternately
- **Design tradeoffs**: Inference latency vs. robustness (two models sequential vs. single flat model), data efficiency (fewer steps but requires distinct reward shaping)
- **Failure signatures**: External dependency (timeout without wait action), complex UI (occluded elements causing redundant swipes), reward misalignment (valid-looking but incorrect subgoals)
- **First 3 experiments**: 1) Baseline comparison replicating AitW General/WebShopping evaluation, 2) UI perturbation stress test (home→all-apps layout shift), 3) Ablation on hierarchy (flat vs. hierarchical training)

## Open Questions the Paper Calls Out

- **Inference efficiency**: How to reduce hierarchical generation latency for real-time edge deployment (explicitly stated in conclusion)
- **Asynchronous environment robustness**: Improving agent for environments with network latency or external dependencies (identified in Appendix E.1 as primary failure mode)
- **Judge model sensitivity**: How high-level policy performance is affected by errors/biases in frozen VLM judge (inferred from reliance on frozen judge for feasibility rewards)

## Limitations

- Oracle generation pipeline fidelity uncertainty: 93.2% oracle success requires exact Qwen2.5-VL-72B reasoning + Qwen2.5-VL-7B action coordination not fully specified
- Alternating update schedule lacks precise iteration counts per switch, affecting reported performance
- Foresight advantage function effectiveness depends heavily on frozen judge's consistency across different UI states and task contexts

## Confidence

- **High Confidence**: Hierarchical architecture reducing sampling complexity from G^n to n·G; semantic abstraction mechanism for UI layout generalization
- **Medium Confidence**: Critic-free joint training stability claims; foresight advantage function's reliance on frozen external judge introduces potential variance
- **Low Confidence**: Exact oracle pipeline reproducibility; precise alternation schedule between model updates

## Next Checks

1. **Oracle Pipeline Validation**: Reproduce oracle generation using Qwen2.5-VL-72B and Qwen2.5-VL-7B to verify 93.2% success rate on AitW-WebShopping
2. **Foresight Reward Stability**: Conduct ablation studies varying λ weights in foresight advantage function to quantify sensitivity
3. **Generalization Across Model Scales**: Train Hi-Agent with different model sizes to verify architectural claims beyond 3B+3B configuration