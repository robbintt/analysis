---
ver: rpa2
title: An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring
arxiv_id: '2505.24239'
source_url: https://arxiv.org/abs/2505.24239
tags:
- agents
- agent
- adversarial
- credibility
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a credibility scoring framework to make multi-agent
  LLM systems robust against adversarial agents. The core idea is to dynamically assign
  credibility scores to agents based on their past contributions and use these scores
  to weigh agent outputs during aggregation.
---

# An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring

## Quick Facts
- **arXiv ID**: 2505.24239
- **Source URL**: https://arxiv.org/abs/2505.24239
- **Reference count**: 23
- **Primary result**: Credibility scoring improves accuracy by 6-30 percentage points even when adversaries form a majority.

## Executive Summary
This paper introduces a credibility scoring framework to make multi-agent LLM systems robust against adversarial agents. The core idea is to dynamically assign credibility scores to agents based on their past contributions and use these scores to weigh agent outputs during aggregation. Contributions are measured via Shapley values or an LLM judge, and credibility is updated iteratively using rewards. Experiments across five benchmarks show that this method consistently improves accuracy—by 6-30 percentage points—compared to baselines, even when a majority of agents are adversarial. The approach is topology-agnostic and effective across different agent roles and communication structures.

## Method Summary
The method initializes all agent credibility scores (CrS) to 0.5 and iteratively updates them based on past contributions measured via Shapley values (for no-communication settings) or an LLM-as-Judge (for communicating agents). After each query, the system computes contribution scores, aggregates outputs weighted by CrS, evaluates final answers using an external LLM judge, and updates CrS multiplicatively. The framework supports three topologies: standalone agents (SAA), stochastic interaction graphs (SIA), and chain-ordered communication. The key innovation is using learned credibility to suppress low-quality agents' influence during aggregation, enabling the system to maintain accuracy even when most agents are adversarial.

## Key Results
- Accuracy improves by 6-30 percentage points compared to baselines across five benchmarks
- System maintains 31% accuracy with 4 adversarial agents vs 1 faithful agent
- Topology-agnostic design works across SAA, SIA, and chain-ordered configurations
- Shapley values and LLM-as-Judge both effective for contribution scoring

## Why This Works (Mechanism)

### Mechanism 1: Credibility-Weighted Aggregation
- Claim: Weighting agent outputs by learned credibility scores improves system accuracy by 6-30 percentage points even when adversaries form a majority.
- Mechanism: Each agent receives a CrS ∈ [0,1] reflecting historical reliability. During aggregation, outputs are weighted proportionally to CrS rather than treated equally. For centroid-based aggregation: x⃗⁺ = (1/N)∑ CrS(i)·v⃗(O(aᵢ,q)). This suppresses low-credibility agents' influence on final outputs.
- Core assumption: Agents exhibit consistent behavior patterns over time, allowing past performance to predict future reliability.
- Evidence anchors:
  - [abstract] "Our system associates a credibility score that is used when aggregating the team outputs... learned gradually based on the past contributions."
  - [Section 4] Formal definition of CrS-aware centroid and LLM-assisted aggregation with CrS.
  - [corpus] Related work "SentinelNet" uses credit-based threat detection, suggesting credit-based approaches are an emerging defense pattern, though no direct comparison data available.
- Break condition: If adversaries strategically alternate between helpful and harmful behavior to maintain moderate CrS, the weighting becomes less discriminative.

### Mechanism 2: Contribution-Based Credibility Updates
- Claim: Distributing rewards proportional to contribution scores enables CrS to converge toward true agent reliability.
- Mechanism: After each query, compute CSc(i) via Shapley values (for no-communication settings) or LLM-as-Judge (for communicating agents). Update rule: CrSₜ(i) = CrSₜ₋₁(i)·(1 + η·CSc(i)·rₜ), where rₜ ∈ [-1,1] is the team reward. High contributors to correct answers gain CrS; adversarial agents lose CrS over iterations.
- Core assumption: The judge accurately evaluates both final answer quality and individual contributions. Assumption: Shapley values meaningfully capture contribution in non-communicating settings.
- Evidence anchors:
  - [Section 5.1] "The contribution score of the agent aᵢ is then computed using... Shapley values" and LLM-as-Judge alternative.
  - [Section 5.2] Equation 2 shows the multiplicative update rule.
  - [corpus] Weak direct evidence; neighboring papers don't validate Shapley-based contribution scoring for LLM agents.
- Break condition: If the judge is weaker than task requires (e.g., LLaMA-3.2 judging GSM8K), CSc becomes noisy and CrS updates corrupt credibility signals (Section 6.3.5 shows 54% accuracy drop with weak judge).

### Mechanism 3: Topology-Agnostic Design with CrS-Ordered Communication
- Claim: Ordering chain topology by CrS limits adversarial influence propagation.
- Mechanism: In CrS-ordered chains, agents communicate only with adjacent neighbors, sorted by descending CrS. High-credibility agents speak first, establishing correct anchors before lower-credibility agents can inject noise. The system also works with stochastic topologies (SIA) and standalone architectures (SAA).
- Core assumption: Early-position agents have outsized influence on downstream agents' final outputs.
- Evidence anchors:
  - [Section 6.2] "Positioning the most reliable agents earlier in the chain limits the influence of adversaries further down the chain."
  - [Table 1] CrS-ordered Chain shows +10-20% gains over baselines across models.
  - [corpus] "Agents Under Siege" demonstrates topology affects attack success rates, supporting topology as a relevant design axis.
- Break condition: If adversaries gain high CrS early (cold-start problem), they secure influential positions and amplify their impact.

## Foundational Learning

- Concept: Shapley Values
  - Why needed here: Used to fairly attribute team reward to individual agents by measuring marginal contribution across all possible agent subsets.
  - Quick check question: Can you explain why Shapley values require O(2^N) evaluations and why the paper uses LLM-as-Judge as an alternative for communicating settings?

- Concept: Multi-Agent Communication Topologies
  - Why needed here: The system operates across chain, ring, hierarchical, and stochastic graphs; understanding how topology affects information flow and adversarial influence is essential.
  - Quick check question: Why does a fully-connected topology facilitate rapid consensus but increase vulnerability to adversarial persuasion?

- Concept: LLM-as-Judge Evaluation
  - Why needed here: The system relies on an external LLM to score final answers and compute contribution scores; judge quality directly impacts CrS learning.
  - Quick check question: What happens to CrS convergence when the judge cannot accurately evaluate the task domain (e.g., code correctness on HumanEval)?

## Architecture Onboarding

- Component map:
  Query → Team Formation → Local Inference → Peer Interaction → CrS-Weighted Aggregation → Judge Evaluation → CrS Update

- Critical path: Query → Team Formation → Local Inference → Peer Interaction → CrS-Weighted Aggregation → Judge Evaluation → CrS Update. The judge is the bottleneck; two API calls per query (reward + CSc) drive cost and latency.

- Design tradeoffs:
  - Shapley vs. LLM-as-Judge for CSc: Shapley is theoretically grounded but #P-hard; LLM-as-Judge scales but introduces judge-dependent noise.
  - Link density in SIA: 6 links optimal; more links increase information overload and token compression artifacts (Section 6.3.6).
  - Judge capacity: Stronger judges improve CrS quality but raise cost; weaker judges corrupt CSc and degrade accuracy.

- Failure signatures:
  - CrS fails to separate faithful from adversarial agents → check judge accuracy on task domain.
  - Accuracy degrades with more communication links → reduce link density or check token compression losses.
  - Majority voting outperforms CrS on math reasoning → CSc computation may be error-prone for complex reasoning chains (Section 6.3.2).

- First 3 experiments:
  1. **Cold-start validation**: Initialize all CrS = 0.5 with 2 faithful + 3 adversarial agents on GSM8K. Plot CrS convergence over 100 queries. Verify faithful agents' CrS diverges upward from adversaries.
  2. **Judge ablation**: Run identical configuration with GPT-4o mini vs. LLaMA-3.2 as judge. Measure accuracy delta and CrS stability (expect noise with weaker judge per Section 6.3.5).
  3. **Topology stress test**: Compare SIA (6 random links) vs. CrS-ordered Chain vs. SAA on MMLU-MS with 4 adversarial / 1 faithful agent. Verify CrS maintains ~31% accuracy regardless of adversary count (per Figure 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the credibility scoring framework maintain effectiveness when using self-calibrating judges or judge ensembles instead of relying on a single strong external judge?
- Basis in paper: [explicit] Section 8 states: "Future research could mitigate this sensitivity by developing self-calibrating judges or employing ensembles of judges."
- Why unresolved: Current experiments show significant performance degradation (e.g., 54% accuracy drop on GSM8K) when switching from GPT-4o mini to LLaMA-3.2 as judge. No alternative judge mechanisms were tested.
- What evidence would resolve it: Experiments comparing CrS performance with (a) ensemble judges combining multiple weaker models, or (b) self-calibrating judges that adjust evaluation criteria based on consistency signals.

### Open Question 2
- Question: How robust is the credibility scoring mechanism against sophisticated adversaries specifically fine-tuned for deceptive behaviors or adversarial agents that adaptively game the scoring system?
- Basis in paper: [explicit] Section 8 states: "Real-world adversaries, whether human actors or LLMs specifically fine-tuned for deceptive behaviors, may exhibit more sophisticated and unpredictable patterns. Such advanced adversaries could potentially evade detection or mitigation through CrS."
- Why unresolved: All experiments used synthetic adversaries with explicit prompts to introduce subtle errors; these agents "typically became easier to influence after multiple rounds of interaction."
- What evidence would resolve it: Experiments with (a) adversarial agents trained to maintain consistent high credibility while occasionally injecting targeted misinformation, or (b) adaptive adversaries that observe and exploit the scoring update dynamics.

### Open Question 3
- Question: What cost-effective approximations can replace exact Shapley value computation for contribution scores while preserving adversarial robustness?
- Basis in paper: [explicit] Section 8 states: "Exploring cost-effective approximations or more efficient evaluation techniques represents valuable avenues for future research," noting that "Shapley-like CrS scales quadratically with the number of agents."
- Why unresolved: The paper uses LLM-as-Judge as an alternative for some settings but does not explore efficient approximations for the Shapley-based approach in no-communication settings.
- What evidence would resolve it: Comparative experiments testing approximation methods (e.g., Monte Carlo Shapley, Truncated Monte Carlo, or grouping-based approximations) measuring both computational cost and accuracy retention under adversarial conditions.

## Limitations
- Judge dependency creates fundamental vulnerability; weak judges corrupt CrS updates and degrade accuracy significantly
- Adversarial threat model realism uncertain due to unspecified attack patterns beyond "subtle errors"
- Computational overhead from multiple API calls per query limits scalability
- Long-term stability of CrS beyond 100 queries not demonstrated

## Confidence

**High confidence**: The core mechanism of credibility-weighted aggregation is theoretically sound and the experimental methodology (using controlled faithful/adversarial ratios across five benchmarks) appears rigorous. The topology-agnostic design and multi-baseline comparisons provide strong empirical support.

**Medium confidence**: The Shapley value implementation for contribution scoring is well-grounded, but the LLM-as-Judge alternative introduces judge-dependent variability that wasn't fully characterized across different judge strengths. The 6-7 link density optimization for SIA appears empirically validated but lacks theoretical justification.

**Low confidence**: The adversarial threat model realism is uncertain due to unspecified attack patterns. The cold-start convergence dynamics and long-term stability of CrS beyond 100 queries are not demonstrated.

## Next Checks

1. **Judge strength ablation**: Run identical experiments across three judge configurations (strong: GPT-4o, medium: GPT-4o mini, weak: LLaMA-3.2) on GSM8K and HumanEval to quantify the accuracy-cost tradeoff and establish minimum judge requirements for reliable CrS learning.

2. **Adversarial strategy robustness**: Implement and test against three distinct adversarial strategies - consistent harmful outputs, strategic helpfulness (maintaining moderate CrS), and early-position high-credibility attacks - to identify which attack patterns break the system most effectively.

3. **Long-term stability test**: Extend the GSM8K experiment to 1000 queries with continuous adversarial injection, monitoring CrS convergence stability, accuracy trends, and computational cost per query to assess whether performance gains persist over extended deployment periods.