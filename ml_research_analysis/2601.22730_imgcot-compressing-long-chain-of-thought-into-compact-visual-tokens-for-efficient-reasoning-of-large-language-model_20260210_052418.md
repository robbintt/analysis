---
ver: rpa2
title: 'ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient
  Reasoning of Large Language Model'
arxiv_id: '2601.22730'
source_url: https://arxiv.org/abs/2601.22730
tags:
- reasoning
- latent
- tokens
- visual
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImgCoT, a method for compressing long chains
  of thought into compact visual tokens to enable efficient reasoning in large language
  models. The key idea is to render textual CoT into images and use a 1D visual tokenizer
  to encode them into discrete latent tokens, which shifts the inductive bias from
  linguistic form to spatial layout and better captures global reasoning structure.
---

# ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model

## Quick Facts
- arXiv ID: 2601.22730
- Source URL: https://arxiv.org/abs/2601.22730
- Reference count: 31
- Authors: Xiaoshu Chen; Sihang Zhou; Ke Liang; Taichun Zhou; Xinwang Liu
- Primary result: ImgCoT compresses CoT into compact visual tokens, achieving strong reasoning performance with fewer tokens than full CoT.

## Executive Summary
This paper introduces ImgCoT, a method for compressing long chains of thought into compact visual tokens to enable efficient reasoning in large language models. The key idea is to render textual CoT into images and use a 1D visual tokenizer to encode them into discrete latent tokens, which shifts the inductive bias from linguistic form to spatial layout and better captures global reasoning structure. To preserve fine-grained details, a variant called Loose ImgCoT selectively retains a few critical textual reasoning steps based on token log-likelihood. Experiments across four datasets and three LLM backbones show that ImgCoT achieves strong reasoning performance with fewer tokens than full CoT, while Loose ImgCoT further improves performance by retaining domain-specific reasoning skills. Visual inductive bias is shown to be more effective than linguistic bias for CoT compression, leading to better generalization and reasoning quality.

## Method Summary
ImgCoT compresses long chains of thought (CoT) by rendering textual reasoning steps into images and encoding them into compact visual tokens using a 1D visual tokenizer. This approach shifts the inductive bias from linguistic form to spatial layout, capturing global reasoning structure more effectively. A variant, Loose ImgCoT, selectively retains critical textual reasoning steps based on token log-likelihood to preserve fine-grained details. The method is evaluated across four datasets and three LLM backbones, demonstrating strong reasoning performance with fewer tokens than full CoT.

## Key Results
- ImgCoT achieves strong reasoning performance with fewer tokens than full CoT.
- Loose ImgCoT further improves performance by retaining domain-specific reasoning skills.
- Visual inductive bias is more effective than linguistic bias for CoT compression, leading to better generalization and reasoning quality.

## Why This Works (Mechanism)
ImgCoT leverages the spatial layout of visual tokens to capture global reasoning structure, which is more effective than the sequential linguistic form of traditional CoT. By rendering text as images, the method reduces redundancy and focuses on the most critical reasoning steps, improving efficiency. The Loose ImgCoT variant enhances this by selectively retaining domain-specific reasoning steps, ensuring that fine-grained details are preserved where necessary.

## Foundational Learning
- **Chain of Thought (CoT) Reasoning**: A step-by-step reasoning approach used by LLMs to solve complex tasks.
  - *Why needed*: Provides a structured way to break down reasoning into manageable steps.
  - *Quick check*: Verify that the CoT steps are logically connected and lead to the correct answer.

- **Visual Tokenization**: The process of converting images into discrete latent tokens for model input.
  - *Why needed*: Enables the model to process visual representations of reasoning steps efficiently.
  - *Quick check*: Ensure the visual tokens retain the essential information from the original text.

- **Inductive Bias**: The assumptions a model makes about the data to generalize better.
  - *Why needed*: Shifting from linguistic to spatial bias helps capture global reasoning structure.
  - *Quick check*: Compare the performance of models with different inductive biases on reasoning tasks.

- **Token Log-Likelihood**: A measure of how likely a token is to appear in the context of the reasoning steps.
  - *Why needed*: Used in Loose ImgCoT to identify and retain critical reasoning steps.
  - *Quick check*: Validate that high log-likelihood tokens correspond to essential reasoning steps.

## Architecture Onboarding
- **Component Map**: Textual CoT -> Image Rendering -> 1D Visual Tokenizer -> Compact Visual Tokens -> LLM Reasoning
- **Critical Path**: Image rendering and visual tokenization are the core steps that enable efficient reasoning.
- **Design Tradeoffs**: Balances compression efficiency with the preservation of critical reasoning details.
- **Failure Signatures**: Loss of semantic nuance or context due to image rendering; potential bias in token selection.
- **First Experiments**:
  1. Test ImgCoT on a simple reasoning task to validate the visual tokenization process.
  2. Compare the performance of ImgCoT with full CoT on a standard benchmark.
  3. Evaluate the impact of token log-likelihood on the selection of reasoning steps in Loose ImgCoT.

## Open Questions the Paper Calls Out
None

## Limitations
- The shift from linguistic to spatial inductive bias may not generalize well to domains where fine-grained textual reasoning is critical.
- The criteria for selecting reasoning steps in Loose ImgCoT (token log-likelihood) may not capture all critical patterns, especially in complex domains.
- The evaluation is limited to four datasets and three LLM backbones, which may not represent the diversity of real-world reasoning tasks.

## Confidence
- **High**: The experimental results demonstrate improved efficiency and performance compared to full CoT in the evaluated settings.
- **Medium**: The claim that visual inductive bias is more effective than linguistic bias for CoT compression is supported by the results but may not generalize to all domains.
- **Low**: The scalability and robustness of the approach to diverse and complex reasoning tasks remain uncertain.

## Next Checks
1. Test ImgCoT on a broader range of datasets, including those with longer and more complex chains of thought, to evaluate scalability and robustness.
2. Compare the performance of ImgCoT with other compression methods (e.g., dynamic pruning, multi-task learning) to establish its relative effectiveness.
3. Conduct ablation studies to quantify the impact of the visual tokenization process on reasoning quality and identify potential sources of bias or information loss.