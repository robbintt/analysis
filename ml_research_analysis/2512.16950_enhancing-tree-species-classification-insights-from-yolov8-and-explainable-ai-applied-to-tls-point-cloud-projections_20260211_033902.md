---
ver: rpa2
title: 'Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable
  AI Applied to TLS Point Cloud Projections'
arxiv_id: '2512.16950'
source_url: https://arxiv.org/abs/2512.16950
tags:
- tree
- species
- data
- crown
- pixels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a novel explainable AI framework linking Finer-CAM
  saliency maps to segmented structural tree features in TLS point cloud projections
  to understand how YOLOv8 models classify tree species. Using 2,445 trees across
  seven European species, five YOLOv8 models achieved 96% mean accuracy (SD=0.24%)
  through cross-validation.
---

# Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections

## Quick Facts
- arXiv ID: 2512.16950
- Source URL: https://arxiv.org/abs/2512.16950
- Reference count: 40
- Primary result: Five YOLOv8 models achieved 96% mean accuracy (SD=0.24%) on 2,445 trees across seven European species using TLS point cloud projections

## Executive Summary
This study presents a novel explainable AI framework that links Finer-CAM saliency maps to segmented structural tree features in TLS point cloud projections, enabling interpretation of how YOLOv8 models classify tree species. Using 2,445 trees across seven European species, the research demonstrates that models achieve high accuracy by leveraging species-specific branching patterns and structural features rather than dataset artifacts. Analysis of 630 saliency maps reveals that crown features, particularly finer branches, drive classification for birch, beech, oak, and spruce, while stem features are more important for ash, pine, and Douglas-fir. The work highlights the importance of interpretable AI for understanding model limitations and building confidence in predictions.

## Method Summary
The study combines TLS point cloud data with deep learning and explainable AI to classify tree species. Point clouds are orthographically projected to 2D side-views with density-aware pixel sizing, creating 640×640 grayscale images. Five YOLOv8 models are trained using cross-validation with weighted cross-entropy loss. Finer-CAM generates saliency maps by comparing target classes against their most similar contrastive classes, highlighting discriminative features. Heuristic segmentation divides trees into structural regions (crown base/middle/top, crown edge, stem), and salient pixels are attributed to these segments to quantify feature importance. This framework enables both high-accuracy classification and interpretable feature attribution.

## Key Results
- Five YOLOv8 models achieved 96% mean accuracy (SD=0.24%) through cross-validation on 2,445 trees
- Crown features, particularly finer branches, drove classification decisions for birch, beech, oak, and spruce
- Stem features were more important for ash, pine, and Douglas-fir classification
- Models consistently used similar features across species that human experts would also recognize as distinguishing traits
- The framework demonstrates that YOLOv8 leverages species-specific branching patterns rather than dataset artifacts

## Why This Works (Mechanism)

### Mechanism 1: 2D Orthographic Projection Preserves Discriminative Structural Features
- Claim: If 3D TLS point clouds are orthographically projected to 2D with density-aware pixel sizing, branching patterns and crown morphology remain distinguishable for CNN-based classification.
- Mechanism: Point density–adjusted pixel sizing (Δpx = √(w × h) ÷ n) ensures each tree fills the canvas proportionally. Log-transformation of point counts prevents extreme pixel values from dominating, preserving fine branch representations across varying tree sizes.
- Core assumption: Structural features distinguishing species (branching patterns, crown shape) are captured in side-view projections and not destroyed by projection to 2D.
- Evidence anchors:
  - [abstract] "five YOLOv8 models achieved 96% mean accuracy (SD=0.24%) through cross-validation"
  - [section 3] "The point density adjusted pixel size in metres (Δpx) as the theoretical space occupied by each point... We then aggregated the points along the x-axis... and stored the point count as pixel values... we log-transformed the pixel values"
  - [corpus] SilvaScenes paper notes challenges with "heavy occlusion, variable lighting" in under-canopy images—suggesting projection-based methods may avoid some in-situ perception issues
- Break condition: If point density is too low (coarse scans), finer branches become indistinguishable; if too high without normalization, pixel saturation masks structural variation.

### Mechanism 2: Finer-CAM Contrastive Attribution Isolates Species-Specific Features
- Claim: If Finer-CAM compares a target class against its most similar contrastive classes, the resulting saliency maps highlight discriminative features while suppressing features common to similar species.
- Mechanism: Finer-CAM computes gradients on the difference between target and contrastive class logits (∂(f(x)c − γ · f(x)c′) / ∂A(l)), suppressing features that contribute to both. Aggregating the top 3 contrastive classes produces class-discriminative saliency.
- Core assumption: The most confused classes (ranked 2nd–4th by logits) represent perceptually similar species whose shared features should be suppressed.
- Evidence anchors:
  - [abstract] "Analysis of 630 saliency maps revealed that crown features, particularly finer branches, drove classification decisions for birch, beech, oak, and spruce"
  - [section 7] "Finer-CAM... only highlight features of the target class by suppressing features of the class visually most similar to the target class"
  - [corpus] BarkXAI paper proposes "quantifiable concepts" for bark-based XAI—suggests contrastive attribution is an active research direction, but comparative validation across CAM variants remains limited
- Break condition: If the contrastive class selection is arbitrary (not based on model confusion), saliency may highlight irrelevant features; if γ is poorly tuned, maps become too coarse or too noisy.

### Mechanism 3: Saliency-to-Segment Mapping Enables Quantitative Feature Attribution
- Claim: If saliency maps are thresholded (Otsu's method) and overlaid on heuristically defined structural segments (crown base/middle/top, crown edge, stem), the distribution of salient pixels quantifies which features drive per-species decisions.
- Mechanism: Otsu's method binarizes saliency maps; salient pixels are attributed to segments via spatial overlap. Ratios (e.g., rNsc = Nsc/Nst) quantify crown vs. stem contribution. This reduces reliance on visual interpretation and cognitive bias.
- Core assumption: The heuristic segmentation (crown base manually identified, edge buffer at 52px Euclidean distance) accurately separates structural features; localization errors from 32x upsampling are mitigated by the buffer.
- Evidence anchors:
  - [abstract] "stem features were more important for ash, pine, and Douglas-fir"
  - [section 10] "the edges of the crown, which capture small branches, species-specific branching patterns, and terminal branch segments, were delineated by including all crown pixels within a Euclidean distance of 52 pixels from the edges"
  - [corpus] No direct corpus evidence for this specific heuristic; related XAI work (BarkXAI, SilvaScenes) focuses on different modalities—suggests this segmentation-attribution linkage is novel but lacks external validation
- Break condition: If segmentation heuristics misclassify crown/stem boundaries (e.g., drooping branches), attribution is biased; if buffer size (32px) mismatches upsampling error scale, localization drifts.

## Foundational Learning

- Concept: **Class Activation Mapping (CAM) and Gradient-Based Attribution**
  - Why needed here: Finer-CAM builds on Grad-CAM; understanding how gradients flow from output logits to intermediate feature maps is essential to interpret saliency results and select appropriate target layers.
  - Quick check question: If you change the target layer from the final c2f block to an earlier convolutional block, would you expect saliency maps to become more localized or more diffuse? Why?

- Concept: **Orthographic Projection of 3D Point Clouds to 2D**
  - Why needed here: The paper's entire pipeline depends on projecting TLS point clouds to side-view images; understanding how projection angle, point density, and pixel aggregation affect feature preservation is critical.
  - Quick check question: A tree scanned from one side only will have occluded branches. How would this affect the 2D projection, and what artifact might the model learn?

- Concept: **Cross-Validation for Model Stability Assessment**
  - Why needed here: Five independently trained models were evaluated to ensure saliency patterns are not artifacts of a single training run. Understanding cross-validation helps assess result reliability.
  - Quick check question: If all five models show high accuracy but produce inconsistent saliency maps for the same input, what would this suggest about the learned features?

## Architecture Onboarding

- Component map:
  1. Data Preprocessing: TLS point clouds → random point jitter (±0.5cm) → orthographic projection at 4 angles (0°, 45°, 90°, 135°) → density-adjusted rasterization → log-transform → 640×640 grayscale images
  2. Classification Model: YOLOv8s-cls backbone (conv + c2f blocks) + classification head (adaptive avg pool + dropout + linear); pretrained COCO weights; SGD optimizer with 1cycle LR scheduling (lr_max=1e-2, lr_min=1e-4); weighted cross-entropy loss
  3. Explainability Layer: Finer-CAM targeting last c2f block; top-3 contrastive classes aggregated; γ=1; 32x bilinear upsampling; Otsu thresholding for binary saliency
  4. Segmentation & Attribution: Gaussian blur + binarization → contour detection → manual crown base annotation → pathfinding for stem axis → crown subdivision (base/middle/top) + 52px edge buffer → pixel-to-segment attribution

- Critical path:
  1. Ensure TLS point clouds have sufficient density for branch-level detail (paper suggests minimum visibility of "finer branches" in projections)
  2. Generate all 4 projection angles per tree; validate that point density pixel sizing produces consistent spatial resolution across tree sizes
  3. Train 5-fold cross-validated models; select checkpoints at peak validation accuracy (not final epoch)
  4. For each correctly classified test sample, generate Finer-CAM saliency using top-3 contrastive classes
  5. Apply segmentation heuristics; verify crown base annotations via independent verification

- Design tradeoffs:
  - **Projection angle count**: 4 angles increase data augmentation but multiply inference cost; paper uses all 4 and averages logits per tree
  - **Image size (640×640)**: Larger sizes improve branch detail but increase memory/compute; paper selected 640px as maximum supported by YOLOv8s-cls
  - **Contrastive class count (3)**: More contrastive classes may suppress more shared features but risk over-suppression if classes are not truly similar
  - **Edge buffer size (52px)**: Tuned for this dataset; may need adjustment for different image sizes or tree morphologies
  - **Manual crown base annotation**: Accurate but not scalable; automated crown base detection would be needed for operational use

- Failure signatures:
  - **High accuracy but saliency on background/artifacts**: Model may have learned dataset artifacts (e.g., scanner patterns, spatial autocorrelation); paper addresses this via random point jitter and multi-source data requirement (≥3 sources per species)
  - **Inconsistent saliency across cross-validation folds**: Suggests features are not stable; investigate training instability or insufficient data diversity
  - **Saliency concentrated on stem for species where crown is expected**: May indicate dataset bias (e.g., ash stem bends) or insufficient crown detail in projections
  - **Crown top segments systematically under-attended**: Likely due to TLS occlusion effects (lower point density at height); paper notes this limitation

- First 3 experiments:
  1. **Projection angle sensitivity**: Train models using only 1 projection angle vs. all 4; measure accuracy drop and analyze whether saliency shifts to more view-stable features.
  2. **Contrastive class ablation**: Generate Finer-CAM using 1 vs. 3 vs. all other classes as contrastive; assess whether discriminative power and saliency interpretability change.
  3. **Cross-dataset generalization test**: Apply trained models to fully independent TLS data (different scanner, region, or season); if accuracy drops sharply, inspect saliency for new artifacts or missing features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the models learn dataset artifacts (specifically bent stems for Ash) rather than true species-specific structural features, leading to poor generalization on unseen data?
- Basis in paper: [explicit] The authors state "This could be tested in future research by applying the trained models to instances of other tree species in which stem bends similar to the ones observed for Ash are present."
- Why unresolved: The 64.4% focus on stem bends for Ash cannot be validated as species-specific since this feature predominates only for Ash in the current dataset.
- What evidence would resolve it: Applying trained models to external data containing bent stems across multiple species to test whether classification fails when non-Ash trees exhibit similar stem bends.

### Open Question 2
- Question: How well do CNN-based tree species classification models trained on public benchmark datasets generalize to fully independent test datasets?
- Basis in paper: [explicit] Authors highlight the need for future research that "analyses the generalizability of CNN-based tree species classification models trained on public benchmark data sets by application on fully independent test data sets."
- Why unresolved: Classification accuracies may be optimistic due to spatial autocorrelation within data sources, limiting real-world applicability.
- What evidence would resolve it: Performance evaluation on completely independent TLS datasets from different geographic regions, scanners, and data collectors.

### Open Question 3
- Question: How does agreement between model-identified salient regions and features used by human experts for tree species classification compare quantitatively?
- Basis in paper: [explicit] "The agreement between salient regions and regions in the side-projections used by human experts for tree species classification is not further assessed here but would be a suitable topic for future research."
- Why unresolved: Results suggest models use features humans recognize, but no quantitative validation of this alignment exists.
- What evidence would resolve it: Systematic comparison where forestry experts identify discriminating features on identical images, correlated with saliency map outputs.

### Open Question 4
- Question: What dataset artifacts exist in publicly available TLS datasets, and how do they affect CNN decision-making for tree species classification?
- Basis in paper: [explicit] Authors call for research that "investigates the occurrence of data set artifacts of publicly available TLS data sets and their effects on the decision of CNN for tree species classification."
- Why unresolved: Shortcut learning remains a risk; models may exploit scanner-specific point patterns or source-specific traits rather than biological features.
- What evidence would resolve it: Cross-dataset analysis examining whether models trained on specific data sources fail when applied to trees scanned with different equipment or sampling protocols.

## Limitations

- Orthographic projections may lose 3D structural information critical for species with subtle branching differences
- Heuristic segmentation relies on manual crown base annotation and assumes consistent crown morphology within species
- The framework's generalizability to different tree ages, growth conditions, or geographic regions remains untested

## Confidence

- **High Confidence**: Cross-validation accuracy results and basic Finer-CAM saliency patterns showing crown vs. stem feature importance
- **Medium Confidence**: Attribution of specific feature importance to species-level classification decisions, as these depend on segmentation heuristics that were not independently validated
- **Low Confidence**: Claims about the mechanism by which models distinguish between visually similar species, as contrastive class selection and saliency interpretation involve subjective judgments

## Next Checks

1. Test model performance on independently collected TLS data from different geographic regions and seasons to verify feature generalization beyond the original dataset
2. Compare Finer-CAM saliency patterns with alternative XAI methods (e.g., SHAP, LIME) to ensure results are not method-specific artifacts
3. Conduct ablation studies varying projection angles and image resolutions to quantify the sensitivity of both accuracy and feature attribution to these preprocessing choices