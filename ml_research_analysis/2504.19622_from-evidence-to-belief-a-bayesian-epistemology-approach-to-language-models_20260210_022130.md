---
ver: rpa2
title: 'From Evidence to Belief: A Bayesian Epistemology Approach to Language Models'
arxiv_id: '2504.19622'
source_url: https://arxiv.org/abs/2504.19622
tags:
- evidence
- confidence
- accuracy
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) adjust
  their confidence and responses when presented with evidence of varying informativeness
  and reliability, from a Bayesian epistemology perspective. A dataset with diverse
  evidence types was created, and LLM responses were analyzed using verbalized confidence,
  token probability, and sampling methods.
---

# From Evidence to Belief: A Bayesian Epistemology Approach to Language Models

## Quick Facts
- arXiv ID: 2504.19622
- Source URL: https://arxiv.org/abs/2504.19622
- Reference count: 16
- Key outcome: LLMs follow Bayesian confirmation with true evidence but fail disconfirmation and irrelevance assumptions; high confidence doesn't guarantee accuracy; bias toward golden evidence observed.

## Executive Summary
This study investigates how large language models adjust confidence and responses when presented with evidence of varying informativeness and reliability from a Bayesian epistemology perspective. Researchers created a dataset with diverse evidence types and analyzed LLM responses using verbalized confidence, token probability, and sampling methods. Results show LLMs align with Bayesian confirmation when evidence supports correct answers but fail to properly handle disconfirmation and irrelevance, revealing limitations in how models represent and update beliefs based on evidence.

## Method Summary
The study used zero-shot inference with GPT-3.5-turbo and GPT-4o on three datasets (SciQ, TriviaQA, GSM8K) after generating perturbed evidence types using GPT-4-0613. Researchers created evidence variations including conflicting, coincidental, contradictory, and irrelevant types through structured prompts. Confidence was elicited via three methods: verbalized probability, token probability, and sampling-based consistency measures. Performance was evaluated using accuracy metrics (Rouge-L ≥ 0.3 for open-ended tasks, exact match for GSM8K) and Expected Calibration Error (ECE) to measure confidence calibration.

## Key Results
- LLMs significantly increase confidence and accuracy with golden evidence, satisfying Bayesian confirmation (P(H|E,θ) > P(H|θ))
- Models fail to lower confidence appropriately with coincidental evidence, maintaining high accuracy despite irrational reasoning
- Performance degrades with irrelevant evidence, though less severely when irrelevant content comes from different domains
- Contradictory evidence (50% golden + 50% conflicting) is handled similarly to pure golden evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs align with Bayesian confirmation when evidence is supportive and reliable
- Mechanism: When provided with golden evidence, models increase confidence and accuracy significantly, satisfying P(H|E,θ) > P(H|θ). This suggests the models can integrate confirmatory signals from context into their parametric knowledge.
- Core assumption: Models have relevant parametric knowledge that can be activated and reinforced by aligned context.
- Evidence anchors:
  - [abstract] "language models follow the Bayesian confirmation assumption well with true evidence"
  - [section 4.1] "when golden evidence is provided, the p-value for confidence showed at least a marginal increase, with a significant difference particularly observed in verbal"
  - [corpus] Corpus papers on Bayesian rationality metrics (e.g., Martingale Score) suggest Bayesian alignment is measurable but partial in current LLMs
- Break condition: When evidence contradicts parametric knowledge or contains noise, confidence calibration degrades (ECE increases).

### Mechanism 2
- Claim: LLMs exhibit a bias toward correct information even when contradictory signals are present
- Mechanism: When contradictory evidence (golden evidence + 50% conflicting sentences) is provided, models treat it similarly to pure golden evidence. The paper's ablation study shows performance only degrades significantly when golden sentences are entirely removed, not when their proportion gradually decreases.
- Core assumption: Models weight context aligned with parametric knowledge more heavily than context contradicting it.
- Evidence anchors:
  - [section 5] "LLMs tend to focus more on correct than incorrect information"
  - [section 4.1] "LLMs handle contradictory evidence as golden evidence"
  - [corpus] BASIL paper suggests sycophantic behavior complicates disentangling genuine belief shifts from agreeableness
- Break condition: When conflicting evidence exceeds a threshold (near 100% replacement), accuracy and calibration collapse dramatically (e.g., GSM8K accuracy drops from 95.1% to 2.3%).

### Mechanism 3
- Claim: Irrelevant evidence disrupts LLM reasoning in proportion to its domain relatedness
- Mechanism: Models show decreased confidence and accuracy with irrelevant evidence, but this effect is weaker when irrelevant content comes from a completely different domain (e.g., math evidence for science questions) versus same-domain-but-wrong-content. This suggests partial semantic matching occurs even with irrelevant inputs.
- Core assumption: Models cannot fully distinguish relevance from surface-level semantic similarity.
- Evidence anchors:
  - [section 4.1] "LLMs are confused by irrelevant evidence"
  - [section 5] "as the irrelevance increases, the LLMs become less distracted by the context"
  - [corpus] Corpus lacks direct replications of irrelevance calibration; related work on adversarial context supports distraction hypothesis
- Break condition: Completely out-of-domain irrelevant evidence approaches baseline (no-evidence) behavior, partially restoring Bayesian irrelevance alignment.

## Foundational Learning

- Concept: Bayesian confirmation theory fundamentals
  - Why needed here: The paper frames LLM belief evaluation through confirmation (P(H|E,θ) > P(H|θ)), disconfirmation, and irrelevance assumptions. Understanding these is prerequisite to interpreting results.
  - Quick check question: If P(H) = 0.5 and P(H|E) = 0.7, does E confirm, disconfirm, or is irrelevant to H?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: The paper uses ECE to measure whether model confidence matches actual accuracy—a key metric for assessing whether models "believe" their outputs.
  - Quick check question: If a model outputs 80% confidence on 100 predictions but is correct only 60% of the time, is it overconfident or underconfident?

- Concept: Verbalized vs. token probability confidence
  - Why needed here: The paper compares three confidence elicitation methods (verbalized, token probability, sampling) with different results—understanding these distinctions is critical for replication.
  - Quick check question: Why might a model's self-reported confidence (verbalized) differ from its output token probabilities?

## Architecture Onboarding

- Component map: Evidence Generation Pipeline -> Confidence Elicitation Layer -> Evaluation Metrics -> Dataset Sources
- Critical path:
  1. Start with golden (Q, A, E) tuples from source datasets
  2. Generate perturbed evidence E′ using GPT-4-0613 with structured prompts (see Appendix I.2)
  3. Filter generated evidence for quality (correct answer preservation where applicable, no template errors)
  4. Run inference with GPT-3.5-turbo or GPT-4o using temperature=1.0, top_p=1.0
  5. Extract confidence via chosen method and compute accuracy/ECE
  6. Perform paired t-tests across conditions for statistical significance

- Design tradeoffs:
  - Verbalized confidence provides interpretable scores but depends on model's self-assessment capability; smaller open-source models fail format compliance
  - Token probability is architecture-dependent but doesn't capture reliability distinctions well (see Figure 4)
  - Sampling is computationally expensive but better reflects uncertainty through response variance
  - Dataset size limited to ~1000 samples per dataset due to API costs; GPT-4o runs only 200 samples

- Failure signatures:
  - Models treat contradictory evidence (50% golden + 50% conflicting) as equivalent to pure golden evidence—ECE remains low despite noise
  - Conflicting evidence causes confusion without proportional confidence decrease (only GPT-4o verbalized shows significant drop)
  - Coincidental evidence (correct answer via irrational reasoning) increases accuracy but models fail to lower confidence appropriately
  - Specificity manipulation has inconsistent effects—more detail sometimes hurts accuracy

- First 3 experiments:
  1. Replicate confirmation task with a single dataset (e.g., SciQ subset of 200 samples) using verbalized confidence to verify golden evidence increases and irrelevant evidence decreases confidence significantly
  2. Ablate the proportion of golden sentences in contradictory evidence (30%, 50%, 80%, 100% conflicting) to identify the threshold where performance collapses
  3. Test irrelevance with cross-domain evidence (e.g., GSM8K context for SciQ questions) to confirm the paper's finding that domain distance reduces distraction effects

## Open Questions the Paper Calls Out

- Question: What specific properties of training data and algorithms cause LLMs to deviate from Bayesian disconfirmation and irrelevance assumptions?
- Basis in paper: [explicit] "In this paper, we did not theoretically investigate the causes behind the observed phenomena, such as the training algorithm and model architecture, leaving such investigations and their potential implications for future research."
- Why unresolved: The paper speculates that pretraining data containing mostly correct explanations and supervised training methods may explain deviations, but provides no empirical or theoretical validation.
- What evidence would resolve it: Ablation studies varying training data composition (e.g., including more conflicting/unreliable information) or controlled experiments with different training objectives measuring Bayesian alignment.

- Question: How does human belief updating under varying evidence types compare to LLM behavior on the same tasks?
- Basis in paper: [explicit] "We did not conduct a human evaluation as a baseline in this study as our focus was on aligning language models with ideal Bayesian epistemology. Future research could incorporate human evaluation to further assess how models' belief updating and confidence calibration compare to human cognitive processes."
- Why unresolved: Without human baselines, it remains unclear whether LLM deviations represent failures relative to ideal rationality or patterns that mirror human cognitive biases.
- What evidence would resolve it: Human subject experiments using the same evidence types and confidence elicitation protocols, enabling direct comparison of belief revision patterns.

- Question: How do finer-grained classifications of conflicting evidence (entirely incompatible vs. obstructive) affect LLM disconfirmation behavior?
- Basis in paper: [explicit] "One limitation of our dataset is the varying nature of conflicting evidence, some evidence is entirely incompatible, while other types obstruct correct answers. Hence, as we varied the extent of irrelevance in the ablation study, a finer classification of conflicting evidence could benefit future research."
- Why unresolved: The current conflates distinct types of conflicting evidence, potentially masking different failure modes.
- What evidence would resolve it: Systematic categorization of conflicting evidence types followed by targeted experiments measuring confidence and accuracy responses to each category.

## Limitations

- Dataset construction relies on GPT-4-generated perturbations, which may introduce biases or inconsistencies in how evidence types are represented
- Sampling-based confidence metric uses GPT-4o for semantic clustering without fully specified implementation details, creating potential reproducibility issues
- Analysis focuses on a limited set of evidence types and may not generalize to more complex real-world scenarios where evidence credibility is ambiguous or multimodal

## Confidence

- **High Confidence:** The observation that LLMs follow Bayesian confirmation assumptions with true evidence (increased confidence and accuracy with golden evidence) is well-supported by consistent results across datasets and confidence metrics.
- **Medium Confidence:** The finding that LLMs handle contradictory evidence similarly to golden evidence has strong statistical support but may reflect prompt engineering artifacts rather than genuine reasoning limitations.
- **Low Confidence:** The claim about domain-specific irrelevance effects is based on limited evidence and requires further validation, as the mechanism distinguishing same-domain versus cross-domain irrelevant evidence is not fully explained.

## Next Checks

1. Replicate the contradictory evidence experiment with varying golden-to-conflicting ratios (10%, 30%, 50%, 80%, 100%) to identify the precise threshold where confidence calibration breaks down.
2. Test the irrelevance hypothesis by creating cross-domain irrelevant evidence (e.g., medical context for math questions) and measuring whether ECE approaches baseline levels as predicted.
3. Implement the sampling confidence metric using a fixed semantic clustering method with documented prompts to verify that the observed confidence patterns are not artifacts of the clustering process.