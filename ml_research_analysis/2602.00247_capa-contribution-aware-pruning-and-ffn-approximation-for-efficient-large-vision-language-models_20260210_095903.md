---
ver: rpa2
title: 'CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large
  Vision-Language Models'
arxiv_id: '2602.00247'
source_url: https://arxiv.org/abs/2602.00247
tags:
- visual
- tokens
- attention
- layers
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency of large vision-language
  models (LVLMs) by tackling two key sources of redundancy: (1) visual tokens with
  high attention scores but low actual contribution to the residual stream, and (2)
  redundant Feed-Forward Network (FFN) computations for visual tokens in intermediate
  layers. The authors propose CAPA, a dual-strategy framework that prunes visual tokens
  based on attention contribution (weighted by value vector magnitude) rather than
  raw attention scores, and approximates redundant FFNs for visual tokens using lightweight
  element-wise Hadamard products.'
---

# CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2602.00247
- **Source URL:** https://arxiv.org/abs/2602.00247
- **Reference count:** 19
- **Primary result:** Achieves 78% FLOPs reduction while maintaining near-Vanilla performance even at 75% visual token pruning across three LVLM backbones

## Executive Summary
This paper addresses the inefficiency of large vision-language models (LVLMs) by tackling two key sources of redundancy: (1) visual tokens with high attention scores but low actual contribution to the residual stream, and (2) redundant Feed-Forward Network (FFN) computations for visual tokens in intermediate layers. The authors propose CAPA, a dual-strategy framework that prunes visual tokens based on attention contribution (weighted by value vector magnitude) rather than raw attention scores, and approximates redundant FFNs for visual tokens using lightweight element-wise Hadamard products. Experiments across three LVLM backbones (LLaVA-1.5, Qwen2.5-VL, and InternVL3) on six benchmarks show CAPA achieves strong efficiency-performance trade-offs, maintaining near-Vanilla performance even at 75% pruning while reducing FLOPs by 78%.

## Method Summary
CAPA implements a dual optimization strategy: (1) Contribution-aware pruning that computes attention contribution scores Ci = ||Σh Ai,h(xiWV,h)WO,h||2 for each visual token, distinguishing between Probability Dumps (low contribution, prunable) and Structural Anchors (high contribution, essential) based on sink thresholds; (2) FFN approximation that replaces dense SwiGLU FFNs with learned Hadamard products y ≈ x ⊙ α for visual tokens in intermediate layers where input-output cosine similarity exceeds 0.96. The framework prunes at transition layers (LLaVA: 5,12,16; QwenVL: 3,11,21; InternVL: 3,11,21) and applies FFN approximation to layers with visual token linearity (LLaVA: 2–5 & 22–29; Qwen: 2–5 & 13–19; InternVL: 2–4 & 14–20). α is calibrated via closed-form OLS on 500 COCO samples.

## Key Results
- Maintains near-Vanilla performance even at 75% pruning across all three LVLM backbones
- Achieves 78% FLOPs reduction while outperforming baselines like FastV and Feather
- Particularly effective at transition layers where other methods show significant degradation
- Strong performance on tasks requiring fine-grained visual perception (TextVQA, OCR)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention contribution (attention probability weighted by value vector magnitude) is a more faithful proxy for visual token importance than raw attention scores alone.
- **Mechanism:** Standard attention scores capture routing probability but not information transfer magnitude. A token with high attention but near-zero value vectors contributes minimally to the residual stream. By computing Ci = ||Σh Ai,h(xiWV,h)WO,h||2, CAPA captures the actual contribution each token makes to downstream representations.
- **Core assumption:** The L2 norm of the projected value vector correlates with semantic importance for generation tasks.
- **Evidence anchors:** [abstract] "We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection."
- **Break condition:** If value vector magnitudes do not correlate with downstream task performance, this proxy becomes unreliable.

### Mechanism 2
- **Claim:** Visual attention sinks are functionally heterogeneous, comprising Probability Dumps (prunable) and Structural Anchors (essential), distinguishable via attention contribution.
- **Mechanism:** Tokens exceeding sink threshold τ=20 are not uniformly important. Those with low Ci serve as passive probability mass receptacles from Softmax normalization; those with high Ci anchor the residual stream. CAPA selectively prunes only the former.
- **Core assumption:** The functional role of a token can be inferred from the combination of its activation magnitude and contribution score.
- **Evidence anchors:** [abstract] "Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance."
- **Break condition:** If model architecture changes cause all sinks to cluster at one extreme, the binary classification becomes moot.

### Mechanism 3
- **Claim:** FFN transformations for visual tokens in intermediate layers exhibit near-linear behavior, enabling replacement with O(d) Hadamard products.
- **Mechanism:** Visual tokens achieve high input-output cosine similarity (>0.96) in intermediate layers, indicating the FFN contributes minimal non-linearity. CAPA replaces dense FFN with y ≈ x ⊙ α, where α is learned via closed-form OLS on 500 calibration samples.
- **Core assumption:** Linearity in intermediate layers is a stable property of visual token processing, not an artifact of specific inputs.
- **Evidence anchors:** [abstract] "We identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior."
- **Break condition:** If downstream tasks require non-linear feature disentanglement from visual tokens in intermediate layers, the Hadamard approximation will underperform.

## Foundational Learning

- **Concept: Residual Stream and Value Vectors**
  - Why needed here: CAPA's core insight is that contribution to the residual stream—not attention probability—determines token importance. Without understanding how attention writes to the residual via value projections, the mechanism is opaque.
  - Quick check question: Given attention weights A and value matrix WV, what operation determines the update written to the residual stream?

- **Concept: Attention Sinks and Softmax Saturation**
  - Why needed here: The paper reframes attention sinks as functionally heterogeneous. Understanding why Softmax creates "sink" tokens that absorb probability mass is essential to distinguishing Probability Dumps from Structural Anchors.
  - Quick check question: Why does the Softmax operation tend to concentrate probability mass on a small subset of tokens regardless of semantic relevance?

- **Concept: SwiGLU FFN Architecture**
  - Why needed here: CAPA targets SwiGLU-based FFNs for approximation. Understanding the gate/up/down projection structure clarifies why replacing them with Hadamard products yields ~6d_ff × FLOP reduction.
  - Quick check question: How many matrix multiplications does a standard SwiGLU FFN require per token, and what are their dimensions?

## Architecture Onboarding

- **Component map:** Input tokens → Attention Layer → Contribution Score Computation (Ci) → Top-k Selection → Pruned KV Cache → Redundant Layer? (cos sim > η=0.96) → Hadamard: x ⊙ α or Standard FFN → Next Layer / Output

- **Critical path:**
  1. Compute Ci per visual token at each generation step using current query token
  2. Retain top-k tokens (k = 25% of visual tokens per paper settings)
  3. At layers with E[cos(x, x+FFN(x))] > 0.96, bypass FFN and apply learned α
  4. Calibration: Compute α* = Σ(xn ⊙ yn) / Σ(xn ⊙ xn) on 500 samples

- **Design tradeoffs:**
  - **Pruning at transition layers vs. late layers:** Transition layers are most sensitive; CAPA preserves performance better than baselines here but still shows some degradation vs. late-layer pruning.
  - **Hadamard approximation vs. full FFN skip:** Skipping FFN entirely causes ~5-10% accuracy drop; Hadamard recovers most of this by capturing linear scaling.
  - **Static vs. dynamic α:** Current approach uses calibration-set statistics; adaptive α per input could improve but adds complexity.

- **Failure signatures:**
  - Sudden accuracy drop on TextVQA or OCR tasks → likely pruning Structural Anchors; increase k or check Ci distribution
  - High Hellinger distance in early generation steps → contribution metric may not align with query context; verify Ci is computed against current query
  - Performance collapse at specific layers → layer selection threshold η may be too aggressive; raise to 0.97+

- **First 3 experiments:**
  1. **Reproduce Figure 3 for your target model:** Measure cos(x, x+FFN(x)) for visual vs. text tokens across all layers on 100 samples. Confirm intermediate-layer linearity before enabling FFN approximation.
  2. **Ablate pruning at different layer stages:** Compare Early (layers 3-5), Transition (layers 12-16 for LLaVA), and Late (layers 20+) pruning on VQAv2 and TextVQA. Quantify transition-layer sensitivity.
  3. **Validate Hadamard recovery:** For 3 redundant layers, compare (a) full FFN, (b) skip FFN, (c) Hadamard approximation. Measure accuracy gap and FLOP reduction on a held-out benchmark subset.

## Open Questions the Paper Calls Out
None

## Limitations
- Functional heterogeneity of attention sinks may not generalize across different model architectures or training regimes
- FFN linearity assumption may not hold for all visual tokens, especially those encoding fine-grained details
- Dynamic contribution scoring under context shifts is not addressed; proxy stability across diverse query types is uncertain

## Confidence
- **High confidence:** FLOPs reduction claims (78% at 75% pruning) and accuracy retention metrics on six benchmark datasets
- **Medium confidence:** Attention contribution as a more faithful proxy than raw attention scores; effectiveness of distinguishing Probability Dumps from Structural Anchors
- **Low confidence:** Generalizability of FFN linearity assumption to other LVLM architectures and visual token types

## Next Checks
1. **Cross-architecture sink heterogeneity validation:** Test Probability Dump vs Structural Anchor classification on two additional LVLM architectures and verify bimodal Ci distribution stability.
2. **Token-specific FFN approximation analysis:** Compute token-level cosine similarity distributions rather than layer averages to identify tokens requiring full FFN processing.
3. **Context-shift contribution stability test:** Track Ci scores evolution for same visual tokens across generation steps with gradually changing visual context to assess proxy stability.