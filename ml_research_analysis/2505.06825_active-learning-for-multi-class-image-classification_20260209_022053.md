---
ver: rpa2
title: Active Learning for Multi-class Image Classification
arxiv_id: '2505.06825'
source_url: https://arxiv.org/abs/2505.06825
tags:
- learning
- active
- classification
- training
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates active learning for multi-class image classification,
  demonstrating its effectiveness in reducing training data requirements. The core
  method uses uncertainty metrics to strategically select informative examples from
  unlabeled data pools, improving model performance with fewer labeled samples.
---

# Active Learning for Multi-class Image Classification

## Quick Facts
- **arXiv ID:** 2505.06825
- **Source URL:** https://arxiv.org/abs/2505.06825
- **Reference count:** 13
- **Primary result:** Active learning reduces training data requirements to 4-5% while maintaining high accuracy on MNIST and Fruits360 datasets

## Executive Summary
This paper investigates active learning for multi-class image classification, demonstrating its effectiveness in reducing training data requirements. The core method uses uncertainty metrics to strategically select informative examples from unlabeled data pools, improving model performance with fewer labeled samples. Experiments on MNIST and Fruits360 datasets compare four uncertainty measures: largest margin, smallest margin, least confidence, and entropy. Results show that least confidence and entropy consistently outperform other metrics, with active learning achieving high accuracy using only 4-5% of the original training data. The study also finds that active learning's advantage over random sampling is more pronounced in complex multi-class tasks compared to simpler binary classification problems.

## Method Summary
The paper implements pool-based active learning using uncertainty sampling with four different metrics: largest margin, smallest margin, least confidence, and entropy. The method iteratively trains a CNN on a small labeled set, computes uncertainty scores for all unlabeled samples, and selects the most uncertain k samples to add to the training set. The CNN architectures use 2-3 convolutional layers followed by pooling and fully connected layers with batch normalization. Training runs on MNIST (60K training samples) and a modified Fruits360 dataset (10 classes from original 120) with batch sizes selected to sample approximately 4-5% of the dataset per epoch.

## Key Results
- Active learning achieves high accuracy using only 4-5% of training data
- Least Confidence and Entropy metrics consistently outperform Largest Margin and Smallest Margin
- Active learning's advantage over random sampling is more pronounced for multi-class tasks than binary classification
- Smallest Margin can cause catastrophic failure, with some classes dropping to 0% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting samples the model is most uncertain about accelerates learning more than random sampling.
- **Mechanism:** Uncertainty metrics identify examples near class boundaries where the classifier has low confidence. By training on these difficult cases, the model refines its decision boundaries more efficiently than processing easy, redundant examples. The paper shows active learning achieves high accuracy using only 4-5% of training data (Section 7.1, Figure 7.1).
- **Core assumption:** Uncertain predictions correlate with informative samples that improve the decision boundary. This assumes model uncertainty reflects true ambiguity rather than noise or mislabeled data.
- **Evidence anchors:**
  - [abstract] "Assigning values to image examples using different uncertainty metrics allows the model to identify and select high-value examples in a smaller training set size."
  - [Section 4] "The intuition behind selecting the most uncertain examples is that by obtaining the label for those particular examples, the examples with which the model has the least certainty are the most difficult examples, the most likely the ones near the class boundaries."
  - [corpus] Neighbor paper "Evidential Deep Active Learning" explicitly addresses uncertainty estimation reliability during learning—suggesting this is an active research concern.
- **Break condition:** If uncertainty primarily reflects label noise or outliers rather than boundary ambiguity, selected samples may degrade rather than improve performance.

### Mechanism 2
- **Claim:** Least Confidence and Entropy metrics outperform margin-based metrics for multi-class image classification.
- **Mechanism:** Least Confidence (1 - P(most likely class)) directly targets examples where the model's top prediction is weak. Entropy captures uncertainty across all classes simultaneously. Both focus on genuine ambiguity. In contrast, Largest Margin compares best-to-worst predictions, which is uninformative when many intermediate classes exist. Smallest Margin can fail when confused pairs (e.g., 6s and 9s) have low second-best probabilities, causing systematic exclusion (Section 7.1, Figure 7.3 shows Class 9 dropping to 0% accuracy).
- **Core assumption:** The optimal metric depends on task structure—metrics that consider the full probability distribution or top-class uncertainty scale better to multi-class settings.
- **Evidence anchors:**
  - [Section 7.1] "Least Confident active learning method does the best, followed by the Entropy active learning method...random sampling baseline outperforms largest margin and smallest margin active learning method after approximately epoch 70."
  - [Section 8] "For the MNIST data set we found that least confident followed by entropy performed the best."
  - [corpus] Limited direct comparison in neighbors—"Evidential Deep Active Learning" addresses uncertainty estimation but doesn't compare these specific metrics. Weak corpus support for this claim.
- **Break condition:** On datasets where confusion is concentrated between specific class pairs (rather than diffuse uncertainty), Smallest Margin may recover and outperform.

### Mechanism 3
- **Claim:** Active learning's advantage over random sampling is more pronounced on complex multi-class tasks than simple binary tasks.
- **Mechanism:** In binary classification, most samples are informative early, and models converge rapidly (within ~1 epoch for Fruits360 binary, ~25 epochs for MNIST binary). Sample selection matters less when the task is easy. In multi-class settings, class boundaries are more complex, and strategic sample selection identifies boundary cases random sampling would miss.
- **Core assumption:** Task difficulty modulates the value of intelligent sample selection—harder problems have sparser informative regions.
- **Evidence anchors:**
  - [Section 7.2] "With binary classification, we found that all models could learn the binary classification task with high accuracy within one epoch, so all four active learning uncertainty sampling measures performed similarly."
  - [Section 8] "Active learning is highly effective on simpler tasks, but marked improvement over random sampling is evident in more difficult tasks."
  - [corpus] Neighbor "Investigating Active Sampling for Hardness Classification" evaluates active sampling strategies—relevant but different domain (tactile sensors).
- **Break condition:** If binary tasks have extreme class imbalance or highly overlapping distributions, active learning may still provide substantial benefit.

## Foundational Learning

- **Concept: Uncertainty Sampling**
  - **Why needed here:** This is the core selection mechanism. Without understanding how uncertainty is quantified (probabilities, entropy), you cannot reason about why certain metrics fail or succeed.
  - **Quick check question:** Given predicted class probabilities [0.45, 0.40, 0.15], which has higher entropy: this or [0.80, 0.10, 0.10]?

- **Concept: Decision Boundaries in Classification**
  - **Why needed here:** The paper's intuition is that uncertain samples are near class boundaries. Understanding how classifiers partition feature space explains why boundary samples are informative.
  - **Quick check question:** In a 2D feature space with two classes, where would you expect the most uncertain predictions to lie?

- **Concept: Iterative Training and Data Acquisition**
  - **Why needed here:** Active learning is inherently iterative—model uncertainty changes as training progresses. Understanding this feedback loop is critical for implementation.
  - **Quick check question:** After adding high-uncertainty samples to training data, should you expect uncertainty on remaining unlabeled samples to increase, decrease, or vary unpredictably?

## Architecture Onboarding

- **Component map:**
  Unlabeled Pool (P) → Uncertainty Scorer → Top-k Selector → Oracle → Training Set (T) → CNN Model
                                    ↑                                              ↓
                                    └────────── Trained Model ──────────────────────┘

- **Critical path:**
  1. Initialize training set T with k random samples from pool P
  2. Train CNN on T for one epoch (or to some criterion)
  3. For each unlabeled sample in P, compute uncertainty score using trained model
  4. Select k samples with highest uncertainty (lowest for margin metrics)
  5. Query oracle for labels, move samples from P to T
  6. Repeat until accuracy target or epoch limit reached

- **Design tradeoffs:**
  - **Batch size (k):** Larger batches reduce oracle queries but may select redundant samples. Paper uses k=128 for MNIST (5.3% of data/epoch) and k=1280 for Fruits360 (4.2% of data/epoch).
  - **Metric choice:** Least Confidence and Entropy are more robust for multi-class; Smallest Margin can catastrophically fail on specific classes (see Class 9 in Figure 7.3).
  - **Resampling frequency:** Paper retrains and rescores uncertainty every epoch. Less frequent updates reduce computation but may select stale uncertain samples.

- **Failure signatures:**
  - Class accuracy drops to 0% during training → likely Smallest Margin excluding that class's samples (Figure 7.3)
  - Random sampling catches up to active learning → metric is uninformative (Largest Margin after epoch 70)
  - No convergence improvement across epochs → uncertainty not correlating with informative samples; check data quality

- **First 3 experiments:**
  1. **Baseline comparison:** Implement all four metrics (LMU, SMU, LCU, Entropy) on a small multi-class subset (e.g., MNIST digits 0-4). Track accuracy per class to catch exclusion failures early.
  2. **Ablation on batch size:** Test k={32, 128, 512} with Least Confidence on MNIST. Plot accuracy vs. epochs and total labeled samples. Identify saturation point where larger batches add diminishing returns.
  3. **Binary vs. multi-class validation:** Reproduce the paper's finding by running Least Confidence on binary (digits 0 vs 1) and multi-class (all 10 digits) MNIST. Confirm that the gap between active and random sampling widens with task complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific mechanisms cause the Smallest Margin uncertainty metric to result in catastrophic failure (0% accuracy) for specific classes like the digit '9' in MNIST?
- **Basis in paper:** [explicit] The results section notes that for Smallest Margin, "class 9... has zero percent accuracy after about epoch 50." The authors state, "We believe this phenomenon occurs because... all 9’s could’ve been misclassified," but they do not verify this hypothesis.
- **Why unresolved:** The paper identifies the failure mode and offers a speculative explanation regarding the "best versus second best" probability difference, but provides no experimental validation or gradient analysis to confirm why this metric specifically fails on this class while others succeed.
- **What evidence would resolve it:** A t-SNE visualization or confusion matrix analysis of the latent space specifically for Class 9 during the failure epochs to confirm if samples are permanently misclassified as '6' or excluded from the training set.

### Open Question 2
- **Question:** Why do Largest Margin and Smallest Margin active learning strategies underperform compared to random sampling in later training epochs (post-epoch 70) on MNIST?
- **Basis in paper:** [explicit] The results section observes that "the random sampling baseline... actually manages to do better than the largest margin and smallest margin active learning methods after approximately epoch 70."
- **Why unresolved:** The authors suggest these metrics might be "ill informative" for multi-class tasks, but they do not explain why the performance degrades relative to random selection specifically in later stages of convergence.
- **What evidence would resolve it:** A comparative analysis of sample diversity over time to determine if these margin-based metrics suffer from a lack of diversity (selecting redundant outliers) compared to the stochastic nature of random sampling.

### Open Question 3
- **Question:** Do the comparative advantages of Least Confidence and Entropy metrics persist in datasets with significantly higher intra-class variability or noise than MNIST and Fruits360?
- **Basis in paper:** [inferred] The paper concludes "definitive support" for active learning based on "two vastly different data sets" (standardized digits and isolated fruit images).
- **Why unresolved:** Both datasets used are relatively clean and feature distinct classes. It is unclear if the finding that "Least Confident... does the best" holds when class boundaries are more ambiguous or when image quality is inconsistent (real-world noise).
- **What evidence would resolve it:** Replicating the study on a dataset with high visual clutter (e.g., CIFAR-100) or fine-grained classification (e.g., subtle bird species differences) to test the robustness of the uncertainty metrics.

## Limitations

- Key hyperparameters (optimizer, learning rate, exact CNN architecture details, random seeds) are not specified
- Fruits360 dataset preprocessing is simplified (10 classes from 120), which may not reflect real-world complexity
- The study doesn't address potential label noise issues—uncertainty might reflect annotation errors rather than genuine ambiguity
- Computational overhead of uncertainty scoring across large pools is not explored, which becomes prohibitive in real applications

## Confidence

- **High confidence:** Active learning significantly reduces labeled data requirements (4-5% achieves high accuracy); Least Confidence and Entropy consistently outperform margin-based metrics; task complexity affects active learning's relative advantage
- **Medium confidence:** The claim that Smallest Margin systematically excludes certain classes is well-supported but may be dataset-specific; binary vs. multi-class performance differences are observed but the underlying reasons could vary across domains
- **Low confidence:** The superiority of Least Confidence/Entropy over other metrics is demonstrated on two datasets but lacks broader validation across different data distributions and model architectures

## Next Checks

1. Implement per-class accuracy monitoring during active learning to detect class exclusion failures (especially with Smallest Margin), validating the paper's observation about Class 9 dropping to 0%
2. Test the same uncertainty metrics on a multi-class dataset with known label noise to verify the assumption that uncertainty correlates with informative samples rather than annotation errors
3. Measure computational overhead of uncertainty scoring relative to model training time, particularly for large unlabeled pools, to assess practical scalability beyond the demonstrated MNIST/Fruits360 scenarios