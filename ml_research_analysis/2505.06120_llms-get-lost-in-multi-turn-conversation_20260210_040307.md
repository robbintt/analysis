---
ver: rpa2
title: LLMs Get Lost In Multi-Turn Conversation
arxiv_id: '2505.06120'
source_url: https://arxiv.org/abs/2505.06120
tags:
- conversation
- instruction
- multi-turn
- sharded
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the performance of large language models\
  \ (LLMs) in multi-turn, underspecified conversations. It finds that all tested models\u2014\
  from small open-weight to state-of-the-art closed-weight LLMs\u2014exhibit a significant\
  \ drop in performance in multi-turn settings compared to single-turn, with an average\
  \ degradation of 39%."
---

# LLMs Get Lost In Multi-Turn Conversation

## Quick Facts
- arXiv ID: 2505.06120
- Source URL: https://arxiv.org/abs/2505.06120
- Authors: Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville
- Reference count: 40
- Key outcome: LLMs exhibit a 39% average performance degradation in multi-turn conversations compared to single-turn settings, primarily due to unreliability rather than loss of capability.

## Executive Summary
This paper investigates how large language models perform in multi-turn, underspecified conversations compared to single-turn, fully-specified tasks. Through extensive experimentation with 15 models across 6 analytical tasks and over 200,000 simulated conversations, the authors find that LLMs get significantly worse at conversationally specified tasks. The degradation stems primarily from unreliability—models make incorrect assumptions early and fail to recover when provided with new information. The study introduces a sharded instruction methodology to systematically compare single- and multi-turn performance while isolating the conversation effect from task difficulty.

## Method Summary
The paper introduces "sharded instructions" where fully-specified tasks are automatically split into 3-8 discrete information shards. A User Simulator (GPT-4o-mini) reveals one shard per turn in a conversation with an Assistant (tested LLM). The conversation state determines which shard to reveal next. After each response, a Strategy Classifier determines if a final answer was attempted, and if so, an Answer Extractor retrieves it for task-specific evaluation. This process is repeated N=10 times per instruction with Temperature=1.0 to measure both aptitude (P90) and unreliability (P90-P10). The methodology enables controlled comparison between single-turn (FULL), concatenated (CONCAT), and multi-turn (SHARDED) settings on identical underlying tasks.

## Key Results
- All tested LLMs show significant performance degradation in multi-turn settings compared to single-turn baselines
- Average degradation across tasks and models is 39%, primarily driven by increased unreliability rather than loss of aptitude
- Performance drop is strongly correlated with premature answer attempts occurring early in conversations
- Simple prompting interventions (SNOWBALL - recapitulating previous shards) recover 15-20% of lost performance

## Why This Works (Mechanism)

### Mechanism 1: Premature Answer Attempts Anchor Incorrect Assumptions
When information is underspecified, models fill gaps with assumptions that become "sticky"—the model treats them as given facts rather than hypotheses. When new information contradicts them, the model fails to invalidate and rebuild, instead grafting new requirements onto the corrupted foundation. Table 6 shows conversations with first answer attempts in the first 20% of turns achieve 30.9% performance versus 64.4% when attempts occur in the last 20%.

### Mechanism 2: Cascade Amplification Through Turn-Level Stochasticity
Multi-turn unreliability persists even at temperature=0 because small token-level variations compound across conversation turns. In single-turn, one token deviation has bounded impact, but in multi-turn, an early deviation changes the context for all subsequent turns, creating divergent trajectories. Table 3 shows GPT-4o unreliability drops only 22% from 41.0→31.8 in SHARDED when lowering temperature from 1.0→0.0.

### Mechanism 3: Attention U-Shaped Distribution Across Turns
LLMs overweight information from first and last turns while underweighting middle turns, causing integration failures. This extends the "lost-in-the-middle" phenomenon to multi-turn conversations. Figure 10 shows in 8-turn summary conversations, 20% of citations reference documents from turn 8 versus only 8% from turns 2-3.

## Foundational Learning

- **Aptitude vs. Reliability Decomposition**
  - Why needed: The paper's core insight is that multi-turn degradation comes primarily from unreliability (variance), not aptitude loss (capability ceiling). Understanding this distinction is essential for diagnosing whether failures stem from model limitations or instability.
  - Quick check: If a model scores 90% aptitude but 50% unreliability in multi-turn, what percentage of conversations would you expect to fail in the worst case?

- **Sharding as Experimental Control**
  - Why needed: The "sharded instruction" methodology enables controlled comparison between single- and multi-turn settings on identical underlying tasks. Without this, you cannot isolate the multi-turn effect from task difficulty differences.
  - Quick check: Why must shards be decontextualized and order-insensitive (property P3) for valid experiments?

- **Simulation Fidelity vs. Scalability Tradeoff**
  - Why needed: The paper uses LLM-based user simulation to scale experiments to 200,000+ conversations. Understanding what's lost versus gained is critical for interpreting results.
  - Quick check: Why does the paper argue that observed degradation is likely an underestimate of real-world performance?

## Architecture Onboarding

### Component map
Original Instruction → Sharding Process → Sharded Instruction → User Simulator (GPT-4o-mini) ← Conversation State → Assistant Response → Strategy Classifier → [7 categories] → Answer Extractor → Task-Specific Evaluator → Score → Repeated N times → Aptitude (P90) & Unreliability (U)

### Critical path
The simulation loop where the user simulator decides which shard to reveal based on conversation state. Errors here propagate. Manual inspection found 98% of conversations were valid, but the 2% error rate is non-trivial at 200K scale.

### Design tradeoffs
- **User simulator temperature:** High UT increases conversation diversity but also simulation noise. Paper uses UT=1.0 by default.
- **Shard granularity:** More shards → more turns → more degradation. Gradual sharding experiment shows degradation starts at just 2 shards.
- **Agent-style interventions:** SNOWBALL (repeating all prior shards each turn) recovers 15-20% performance but increases token costs linearly with turns.

### Failure signatures to monitor
- **Answer bloat:** Multi-turn final answers are 14-27% longer than single-turn equivalents, indicating failure to prune incorrect assumptions.
- **Hallucinated citations:** In summary task, 4% of citations reference documents never introduced.
- **Verbosity-performance correlation:** Longer responses correlate with 10-50% lower performance across 5/6 tasks.

### First 3 experiments
1. **Establish baseline on your domain:** Apply sharding to 20-50 of your own instructions. Compare FULL vs CONCAT vs SHARDED. Confirm degradation exists and measure aptitude/reliability split.
2. **Test SNOWBALL intervention:** Implement turn-level recapitulation on your highest-degradation task. If recovery is <15%, the failure mode differs from the paper's.
3. **Ablate premature answering:** Prompt models to ask clarification questions before attempting solutions. Measure whether this shifts the answer-attempt timing distribution and improves performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs exhibit similar "lost in conversation" performance degradation in open-ended creative tasks compared to the analytical tasks tested?
- Basis: Section 9 states determining degradation on creative tasks is an important future direction.
- Why unresolved: Study restricted to analytical tasks with established evaluation metrics to ensure scalability and objective scoring.
- What evidence would resolve it: Application of sharded simulation framework to creative writing benchmarks using human or LLM-based evaluators.

### Open Question 2
- Question: Does the multi-turn reliability degradation persist across different languages and multimodal inputs?
- Basis: Section 9 notes establishing whether models get lost in conversation in other languages or multiple modalities could help establish the scope of degradation.
- Why unresolved: Experimental scope was limited to text-only tasks in English.
- What evidence would resolve it: Replicating sharded simulation experiments using multimodal benchmarks and non-English datasets.

### Open Question 3
- Question: Can native model training or architectural modifications effectively mitigate the "premature answer attempt" behavior?
- Basis: While Section 7.1 tests prompting interventions, Section 7.2 calls for LLM builders to jointly optimize model aptitude and reliability at T=1.0.
- Why unresolved: Paper focuses on evaluating existing models via inference-time strategies rather than training interventions.
- What evidence would resolve it: Training specific model variants with reward functions that penalize incorrect early assumptions in multi-turn contexts.

## Limitations

- **Simulation vs. Reality Gap:** The study uses LLM-based simulation rather than human users, which may underestimate real-world degradation since human conversations involve frustration, clarification loops, and derailing behaviors not captured in simulation.
- **Task Scope Restriction:** Experiments were limited to analytical generation tasks (code, math, database queries, etc.) with established evaluation metrics, leaving open whether similar degradation occurs in other task types.
- **Temperature Setting:** The temperature=1.0 setting maximizes multi-turn degradation but may not represent all deployment scenarios where models operate at lower temperatures.

## Confidence

- **High Confidence:** The 39% average performance degradation across tasks and models is strongly supported by extensive experimental data (200,000+ conversations, 6 tasks, 15 models).
- **Medium Confidence:** The three mechanistic explanations are plausible and supported by targeted experiments, but correlation doesn't prove causation.
- **Low Confidence:** The claim that observed degradation is likely an underestimate of real-world performance rests on assumptions about human behavior that weren't directly tested.

## Next Checks

1. **Human-in-the-Loop Validation:** Replicate the sharded conversation experiment with 50-100 human users across 2-3 tasks (Code, Math, Summary). Compare performance degradation against the LLM-simulated baseline to quantify the gap between simulation and reality.

2. **Mechanistic Intervention Testing:** For each proposed mechanism, design targeted interventions: (a) Force clarification questions before answer attempts and measure timing distribution changes; (b) Implement belief state tracking to test cascade containment; (c) Apply turn-level attention weighting to test U-shaped bias mitigation.

3. **Cross-Domain Generalization:** Apply the sharded conversation methodology to non-generation tasks like classification (sentiment analysis with incomplete features) or reasoning (common sense with partial premises). Test whether the 39% degradation benchmark holds across fundamentally different task types.