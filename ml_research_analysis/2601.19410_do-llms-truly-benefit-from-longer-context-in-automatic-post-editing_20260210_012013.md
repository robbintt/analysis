---
ver: rpa2
title: Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?
arxiv_id: '2601.19410'
source_url: https://arxiv.org/abs/2601.19410
tags:
- translation
- context
- llms
- linguistics
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  improve automatic post-editing (APE) by leveraging document-level context. It compares
  proprietary and open-weight LLMs in a systematic evaluation of APE quality, contextual
  behavior, robustness, and efficiency.
---

# Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?

## Quick Facts
- arXiv ID: 2601.19410
- Source URL: https://arxiv.org/abs/2601.19410
- Authors: Ahrii Kim; Seong-heum Kim
- Reference count: 27
- Primary result: Document-level context does not consistently improve APE quality over segment-level approaches; proprietary models are robust but context-insensitive, while open-weight models suffer from hallucinations in long context

## Executive Summary
This study systematically evaluates whether document-level context improves automatic post-editing (APE) quality for machine translation. Using both proprietary and open-weight LLMs on the WMT24++ English-Korean dataset, the authors compare segment-level APE (APEseg) against document-level APE (APEdoc) with naive context injection. Results show that while proprietary models achieve near-human APE quality even with simple prompting, they largely fail to exploit document context for contextual error correction. Open-weight models are more sensitive to long and noisy contexts, suffering from data poisoning attacks, while proprietary models are more robust but less context-aware. Human evaluation reveals that APE edits are often paraphrastic and poorly captured by automatic metrics. Despite strong performance, the high cost and latency of proprietary LLMs make them impractical for real-world APE deployment.

## Method Summary
The study compares segment-level (APEseg) and document-level (APEdoc) APE using the WMT24++ English-Korean dataset (7,974 segments across 59 documents). APEdoc uses one-shot in-context learning (ICL) with document context appended to the prompt, while APEseg processes each segment independently. Four models are evaluated: GPT-4o, GPT-4o-mini, Qwen2.5-32B, and LLaMA3-8B. Automatic metrics include TER, COMET, BLEU, and chrF++; human evaluation ranks outputs via relative ranking with 3 annotators on 886 segments. Statistical significance is assessed using Friedman test with Nemenyi post-hoc analysis. Document context is placed at the end of prompts to mitigate verbatim copying.

## Key Results
- Document-level APE (APEdoc) does not significantly outperform segment-level APE (APEseg) despite higher token overhead
- Proprietary models (GPT-4o, GPT-4o-mini) are robust to noisy context but make fewer edits as document length increases
- Open-weight models (LLaMA3-8B) suffer from hallucinations and data poisoning when processing extended context, with TER scores exceeding 100 in 50%+ of cases
- Human evaluation shows APE edits are often paraphrastic rather than corrective, poorly captured by automatic metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level context injection via naive prompting does not yield consistent APE quality improvements over segment-level APE.
- Mechanism: The prompt appends full source and target documents after the current segment, instructing the model to use context only as background reference while post-editing the current segment.
- Core assumption: Models can selectively attend to contextually relevant information while filtering irrelevant document content.
- Evidence anchors:
  - [abstract] "they largely fail to exploit document-level context for contextual error correction"
  - [section 5.1] "Although APEdoc achieves the highest performance among the compared settings, the improvement over APEseg is not statistically significant"
  - [corpus] Limited corpus support; neighbor papers focus on sentence-level APE without document context analysis
- Break condition: When document length increases, proprietary models make fewer edits (conservative behavior) while open-weight models produce excessive, often hallucinated outputs.

### Mechanism 2
- Claim: Open-weight models are highly vulnerable to contextual drift and data poisoning when processing extended context.
- Mechanism: Long documents contain segments irrelevant to the current editing task; smaller or less robust models attend to this noise, leading to hallucinated content borrowed from elsewhere in the document.
- Core assumption: Attention mechanisms distribute weight across all context tokens without effective relevance filtering.
- Evidence anchors:
  - [abstract] "Open-weight models are more sensitive to long and noisy contexts, suffering from data poisoning attacks"
  - [section 5.3] "small open-weight models are particularly vulnerable to contextual drift or data poisoning effects when handling extended context"
  - [section 6.2] "hallucinated fragments are borrowed from elsewhere in the document but are absent in the corresponding source text"
  - [corpus] Zhao et al. (2025) and Shi et al. (2023) cited for data poisoning and distraction effects in LLMs
- Break condition: TER scores exceeding 100 indicate complete rewrites or irrelevant content generation; this occurs in 50%+ cases for LLaMA3-8B with document context.

### Mechanism 3
- Claim: Proprietary model robustness to noisy context comes at the cost of limited context utilization for genuine error correction.
- Mechanism: Alignment training and safety mechanisms make proprietary models resistant to distractor content, but this same conservatism prevents them from leveraging genuinely useful contextual signals.
- Core assumption: Robustness and contextual sensitivity are inversely related under current training paradigms.
- Evidence anchors:
  - [abstract] "they largely fail to exploit document-level context for contextual error correction"
  - [section 5.2] "GPTs perform stable and conservative edits despite the inherent data poisoning attack and the extended context"
  - [corpus] Assumption: corpus does not directly address robustness-sensitivity tradeoff in translation context
- Break condition: GPT-4o-mini reduces edit magnitude as document length increases (TERΔ: 2.35→1.24 from short to long documents).

## Foundational Learning

- Concept: Translation Edit Rate (TER) as edit distance metric
  - Why needed here: TER measures how much the APE output differs from input translation; scores exceeding 100 indicate complete rewrites or hallucinated content
  - Quick check question: Given a draft translation of 10 words and an APE output of 15 words with 5 substitutions, what would the TER indicate?

- Concept: Data poisoning attacks on long-context models
  - Why needed here: Explains why naive document injection degrades open-weight model performance through attention to irrelevant context
  - Quick check question: If a document contains a news article about sports followed by a technical manual, which segments might act as "poison" when post-editing the technical content?

- Concept: Paraphrastic vs. corrective editing
  - Why needed here: LLM APE produces semantically equivalent rephrasings rather than error corrections; automatic metrics like COMET cannot distinguish these
  - Quick check question: If COMET scores remain high (0.90+) but TER increases substantially, what type of edits is the model likely making?

## Architecture Onboarding

- Component map: Source segment -> Draft translation -> Source document context -> Target document context -> Model -> Post-edited segment
- Critical path:
  1. Prepare source segment and draft translation pair
  2. Retrieve full source and target documents
  3. Construct prompt with ICL example + current segment + document context
  4. Model generates post-edited segment only
  5. Extract output from `<pe>` tags
  6. Evaluate against human PE reference using TER and COMET
- Design tradeoffs:
  - Token overhead: APEdoc requires 1500-6000% more input tokens than APEseg
  - Latency: Document-level inference adds 100-1000% latency overhead
  - Robustness vs. utility: Proprietary models stable but context-insensitive; open-weight models context-sensitive but unstable
  - Metric alignment: TER captures edit magnitude; COMET captures semantic preservation; neither captures contextual appropriateness
- Failure signatures:
  - TER > 100: Indicates hallucination or complete rewrite
  - High COMET with high TER: Paraphrastic editing rather than error correction
  - Output contains document segments verbatim: Model attending to context instead of task
  - Empty or malformed `<pe>` tags: Instruction following failure under long context
- First 3 experiments:
  1. Compare APEseg vs. APEdoc on a held-out test set using identical model and decoding parameters; measure TER, COMET, and latency overhead to quantify the quality-efficiency tradeoff.
  2. Bucket results by document length (short/medium/long) to identify context length thresholds where performance degrades or hallucination rates increase.
  3. Sample 100 segments with TER > 100 and manually annotate error types (hallucination, document copying, excessive paraphrase) to characterize failure modes specific to each model family.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval-based document injection or adaptive context selection effectively mitigate the "data poisoning" and latency overheads observed in naive full-document APE?
- Basis in paper: [explicit] The authors state in "Limitations & Future Work" that "Exploring adaptive context selection, memory compression, and retrieval-based document injection would be promising directions" to address the inefficiency of the naive method.
- Why unresolved: This study only evaluated a naive strategy of prepending the full document, which led to high latency and distraction for open-weight models.
- What evidence would resolve it: An experiment comparing naive prompting against RAG-based or compression-based context injection on the WMT24++ dataset, measuring TER changes and inference latency.

### Open Question 2
- Question: Does the failure to exploit document context persist across low-resource language pairs or languages with different typological features?
- Basis in paper: [explicit] The authors note the study is "limited to a single, high-resource language pair (English–Korean)" and suggest that "Extending this analysis to low-resource and unseen language pairs would be an important next step."
- Why unresolved: Current results are specific to Korean (a high-resource, agglutinative language); it is unknown if LLMs rely more heavily on document context when training data is scarce.
- What evidence would resolve it: Replicating the APEdoc protocol on low-resource language pairs (e.g., English to low-resource languages) included in the WMT24++ dataset.

### Open Question 3
- Question: Do proprietary models like Claude and Gemini exhibit the same behavioral trade-off—high robustness to noise but failure to leverage context—as observed in GPT-4o?
- Basis in paper: [explicit] The "Limitations" section mentions leaving "other proprietary systems—such as Claude and Gemini—unexplored," necessitating a "systematic comparison" to understand model-specific biases.
- Why unresolved: The conclusion that proprietary models are robust but "context-blind" is based solely on the GPT-4o architecture.
- What evidence would resolve it: Running the same APEdoc and data poisoning experiments using Claude and Gemini APIs to compare their APEseg vs. APEdoc performance gaps.

## Limitations

- The study is limited to a single high-resource language pair (English-Korean), potentially limiting generalizability to other language pairs or domains
- The evaluation cannot distinguish between paraphrastic and corrective edits, as TER captures edit magnitude while COMET captures semantic preservation
- The human evaluation sample size (886 segments) represents only ~11% of the total test set, potentially introducing sampling bias

## Confidence

**High Confidence**: Document-level context does not consistently improve APE quality over segment-level approaches, supported by statistically non-significant improvements in multiple metrics and consistent TER trends across models.

**Medium Confidence**: Proprietary models are characterized as context-insensitive but robust, though the underlying mechanism (alignment training vs. architectural differences) remains unclear without access to model internals.

**Low Confidence**: The claim that open-weight models suffer from "data poisoning" specifically, as observed hallucinations could result from various factors including attention limitations, prompt engineering issues, or insufficient training data for long-context processing.

## Next Checks

1. **Cross-domain validation**: Test the same APEseg vs APEdoc comparison on specialized domains (legal, medical, technical) to assess whether contextual benefits emerge in more formulaic text types where document coherence is more critical.

2. **Controlled hallucination study**: Create synthetic documents with clearly marked irrelevant content (e.g., lorem ipsum sections) and measure whether models can successfully filter out this noise while maintaining contextual awareness for genuinely related segments.

3. **Human edit type annotation**: Have annotators classify a stratified sample of APE outputs into categories (corrective edit, paraphrastic rewrite, hallucination, verbatim copy) to disentangle what the TER metric is actually capturing and validate the paraphrastic vs. corrective distinction.