---
ver: rpa2
title: Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation
arxiv_id: '2512.07650'
source_url: https://arxiv.org/abs/2512.07650
tags:
- scaling
- test-time
- prediction
- merging
- rankmixer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces test-time scaling as an orthogonal approach
  to parameter scaling for large-scale recommendation systems. The key challenge is
  generating diverse yet meaningful predictions during inference, which is addressed
  by two solutions: leveraging heterogeneity among different model architectures and
  exploiting randomness within homogeneous architectures.'
---

# Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation

## Quick Facts
- **arXiv ID:** 2512.07650
- **Source URL:** https://arxiv.org/abs/2512.07650
- **Reference count:** 40
- **Primary result:** Test-time scaling via prediction merging consistently outperforms parameter scaling, with improvements up to 16× across three datasets and eight recommendation models.

## Executive Summary
This paper introduces test-time scaling as an orthogonal approach to parameter scaling for large-scale recommendation systems. The key challenge is generating diverse yet meaningful predictions during inference, which is addressed by two solutions: leveraging heterogeneity among different model architectures and exploiting randomness within homogeneous architectures. A prediction merging technique directly averages logits from multiple models to obtain the final prediction. Evaluations across three datasets (Criteo, Avazu, KDD12) and eight recommendation models show that test-time scaling consistently outperforms parameter scaling, with improvements up to 16×. The method demonstrates superior stability, efficiency, and deployment advantages, particularly in parallel server environments. Empirical results confirm that prediction diversity drives performance gains, with the approach showing clear orthogonality to existing parameter scaling methods.

## Method Summary
The paper proposes test-time scaling for recommendation systems through prediction merging, where multiple trained models' logits are averaged during inference. Two sources of diversity enable this: heterogeneous architectures (different model types) and homogeneous architectures with different random seeds. Models are trained independently or in groups, and their predictions are merged via simple averaging (Equation 10). The approach is evaluated against parameter scaling (depth/width) and shows consistent gains across three datasets. Code is available at https://github.com/aTitye/TTS4CTR with preprocessing including log discretization for numerics and OOV handling for infrequent features.

## Key Results
- Test-time scaling consistently outperforms parameter scaling, with improvements up to 16× across all tested architectures
- Prediction diversity (measured via JS-divergence) correlates with performance gains
- The method is orthogonal to parameter scaling, showing additive improvements when combined
- Superior stability compared to parameter scaling, which shows diminishing returns on some datasets

## Why This Works (Mechanism)

### Mechanism 1: Diversity via Heterogeneous Architectures
- Claim: Combining predictions from models with different inductive biases yields consistent gains over single models.
- Mechanism: Different architectures (e.g., Wukong, RankMixer, DCNv2) capture distinct feature interaction patterns. Averaging their logits reduces individual model errors and exploits complementary representations.
- Core assumption: Architectural heterogeneity translates to prediction diversity, and simple averaging suffices to combine diverse predictions without learned weights.
- Evidence anchors:
  - [abstract] "leveraging heterogeneity among different model architectures"
  - [Section 3.2.2, Table 2] Heterogeneous combinations (Wukong+RankMixer) show scalable improvements across Avazu, Criteo, KDD12.
  - [corpus] Related work on MoE and test-time compute (e.g., "Local Mixtures of Experts") suggests model diversity improves performance, but direct evidence for averaging in recommendation is limited in the corpus.
- Break condition: If models produce highly correlated predictions (low JS-divergence), merging yields diminishing returns; corpus notes test-time scaling can plateau (cf. "Scaling over Scaling").

### Mechanism 2: Diversity via Random Initialization in Homogeneous Architectures
- Claim: Even within the same architecture, different random seeds produce meaningfully diverse predictions, enabling effective merging.
- Mechanism: Random initialization leads to different local minima and feature representations. Averaging predictions from multiple seeds acts as an implicit regularizer, reducing variance without increasing model capacity.
- Core assumption: The recommendation loss landscape is sufficiently multimodal that different seeds capture different but useful patterns.
- Evidence anchors:
  - [abstract] "exploiting randomness within homogeneous architectures"
  - [Section 3.2.1, Table 1] Homogeneous scaling (up to 16×) shows consistent AUC/Logloss improvements across all tested architectures.
  - [corpus] No direct corpus evidence for this specific mechanism; it is an empirical finding of this paper.
- Break condition: If the architecture is too simple or the dataset too small, different seeds may converge to nearly identical solutions, limiting diversity.

### Mechanism 3: Orthogonality to Parameter Scaling
- Claim: Test-time scaling provides gains on top of parameter scaling (depth/width), and under fixed compute budgets can outperform parameter scaling.
- Mechanism: Parameter scaling increases model capacity and training cost, while test-time scaling increases inference compute but reuses trained models. The two approaches operate on different resources and can be combined.
- Core assumption: Inference compute is more readily parallelizable or cheaper than training larger models, and diminishing returns in parameter scaling are not shared by test-time scaling.
- Evidence anchors:
  - [abstract] "test-time scaling can outperform parameter scaling" and "orthogonal improvements"
  - [Section 3.5, Table 5] Shows additive gains when applying test-time scaling to parameter-scaled Wukong and RankMixer.
  - [corpus] "Scaling Test-time Compute for LLM Agents" and other LLM work support orthogonality, but direct evidence in recommendation is limited outside this paper.
- Break condition: If inference latency constraints are extremely tight or parallel serving infrastructure is unavailable, test-time scaling may be impractical despite theoretical gains.

## Foundational Learning

- Concept: Embedding Tables for Categorical Features
  - Why needed here: All recommendation models in the paper map sparse categorical features (e.g., user ID, item ID) to dense embeddings before interaction layers. Understanding this is essential to grasp how models differ and how they can be combined.
  - Quick check question: Given a feature field with 1 million unique IDs and embedding dimension 16, what is the shape of the embedding table?

- Concept: Depth Scaling vs. Width Scaling in Recommendation Models
  - Why needed here: The paper positions test-time scaling against these two parameter scaling paradigms. Depth scaling stacks interaction modules; width scaling uses multiple parallel experts with separate embedding tables.
  - Quick check question: In width scaling, why might each interaction module need its own embedding table (per the paper)?

- Concept: Ensemble Averaging and Bias-Variance Trade-off
  - Why needed here: Prediction merging is a form of ensemble averaging. The paper relies on the intuition that averaging reduces variance, especially when predictions are diverse.
  - Quick check question: If two models have predictions perfectly correlated (r=1), does averaging their predictions improve accuracy? Why or why not?

## Architecture Onboarding

- Component map: Input → Embedding → Interaction → Prediction → (Training: single model loss) / (Inference: average logits from multiple models)

- Critical path:
  Input → Embedding → Interaction → Prediction → (Training: single model loss) / (Inference: average logits from multiple models).

- Design tradeoffs:
  - Homogeneous vs. heterogeneous models: Homogeneous is simpler (same architecture, different seeds) but may yield less diversity; heterogeneous offers higher diversity but requires maintaining multiple codebases.
  - Number of models (M): More models improve performance but linearly increase inference compute; the paper shows diminishing returns after ~16×.
  - Group training: Some models can be trained jointly (Algorithm 1) with shared loss, trading off independence for potential diversity regularization.

- Failure signatures:
  1. Performance plateaus early (e.g., <4× scaling): Likely insufficient diversity; try heterogeneous architectures or more random seeds.
  2. Merging hurts performance: Predictions may be anti-correlated or poorly calibrated; check JS-divergence heatmap.
  3. Inference latency too high: Even with parallel serving, the merging step or I/O overhead may dominate; profile GPU/CPU utilization.

- First 3 experiments:
  1. Homogeneous scaling baseline: Pick one architecture (e.g., DCNv2), train 4 models with different seeds, merge predictions. Verify AUC improves over single model.
  2. Heterogeneous scaling comparison: Train Wukong and RankMixer separately, merge predictions. Compare against homogeneous merge of each architecture alone.
  3. Orthogonality check: Take a parameter-scaled Wukong (e.g., 4× depth), apply 4× test-time scaling via seeds. Measure if gains are additive over parameter scaling alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance plateau observed beyond 16× scaling indicate a theoretical limit of test-time scaling for recommendation, or is it merely a constraint of the fixed training data distribution?
- Basis in paper: [explicit] Appendix B.1 notes, "We can witness a performance plateau after 16×" in Table 12.
- Why unresolved: The paper identifies the saturation point but does not investigate whether the limit is caused by exhausting the information diversity available in the dataset or by the merging methodology itself.
- What evidence would resolve it: Experiments varying the training dataset size alongside the scaling factor to see if the plateau shifts, or analysis of the marginal information gain per added model.

### Open Question 2
- Question: Can model weight merging techniques be adapted for large-scale recommendation models that lack a shared pre-trained ancestor?
- Basis in paper: [explicit] Section 3.6 demonstrates that direct model merging fails (dropping to random guessing) and hypothesizes it is due to independent training with random initialization.
- Why unresolved: The paper establishes the failure of naive averaging in this domain but leaves open the possibility that advanced alignment techniques could resolve the parameter mismatch.
- What evidence would resolve it: Applying techniques like permutation symmetry or model alignment to independently trained recommendation models before averaging their weights.

### Open Question 3
- Question: Is simple averaging of logits sufficient, or can performance be further improved by weighting models based on their individual confidence or accuracy?
- Basis in paper: [inferred] Equation 10 uses a simple average ($\frac{1}{M}\sum$), treating all models equally despite potential variations in their individual predictive power.
- Why unresolved: While simple averaging effectively proves the test-time scaling concept, the paper does not explore whether a learned fusion layer could better exploit the diverse outputs.
- What evidence would resolve it: A comparison between the proposed averaging method and learned weighted averaging or attention-based fusion mechanisms.

## Limitations
- The paper does not establish whether test-time scaling generalizes to other recommendation tasks (e.g., ranking, sequential recommendation) or domains beyond CTR prediction.
- While prediction diversity correlates with performance gains, the paper does not prove diversity is the causal driver versus other ensembling benefits like noise averaging.
- The computational overhead of maintaining and serving multiple models is not quantified in absolute terms or compared against realistic serving conditions.

## Confidence
- **High confidence:** The empirical finding that test-time scaling provides consistent improvements across multiple datasets and architectures, with rigorous experimental methodology.
- **Medium confidence:** The mechanism that architectural and random-seed diversity drives gains, though alternative explanations are not ruled out.
- **Medium confidence:** The orthogonality claim, with demonstrated additive gains but limited exploration of compute budget constraints.

## Next Checks
1. **Generalization test:** Apply test-time scaling to a non-CTR recommendation task (e.g., sequential recommendation) using models like SASRec or GRU4Rec. Measure whether prediction merging still yields gains.
2. **Diversity manipulation:** Train models with constrained initialization or regularization to control prediction diversity. Test whether gains scale with diversity beyond what is observed from random seeds.
3. **Deployment benchmark:** Implement a parallel serving setup for test-time scaling and measure end-to-end latency and throughput versus a parameter-scaled single model under realistic load conditions.