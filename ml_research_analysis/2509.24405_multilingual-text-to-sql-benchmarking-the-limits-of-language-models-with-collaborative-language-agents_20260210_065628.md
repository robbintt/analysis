---
ver: rpa2
title: 'Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with
  Collaborative Language Agents'
arxiv_id: '2509.24405'
source_url: https://arxiv.org/abs/2509.24405
tags:
- language
- multispider
- text-to-sql
- multilingual
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiSpider 2.0, a multilingual Text-to-SQL
  benchmark extending Spider 2.0 to eight languages (English, German, French, Spanish,
  Portuguese, Japanese, Chinese, Vietnamese). It features enterprise-scale, cross-domain
  schemas and compositional SQL complexity, adding linguistic and dialectal variation
  for realistic multilingual evaluation.
---

# Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents

## Quick Facts
- arXiv ID: 2509.24405
- Source URL: https://arxiv.org/abs/2509.24405
- Reference count: 40
- Primary result: COLA improves multilingual Text-to-SQL accuracy from 4% to 15% without fine-tuning

## Executive Summary
This paper introduces MultiSpider 2.0, a multilingual Text-to-SQL benchmark extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It features enterprise-scale, cross-domain schemas and compositional SQL complexity, adding linguistic and dialectal variation for realistic multilingual evaluation. The benchmark reveals a significant multilingual gap: state-of-the-art LLMs like DeepSeek-R1 and OpenAI o1 achieve only 4% execution accuracy on MultiSpider 2.0 versus 60% on MultiSpider 1.0. To address this, the authors propose Collaborative Language Agents (COLA), a modular framework that iteratively refines SQL queries through specialized agents (Classifier, Analyzer, Corrector), improving accuracy to 15% without task-specific fine-tuning. COLA demonstrates consistent gains across languages and models, highlighting the need for robust, multilingual Text-to-SQL methods in real-world enterprise environments.

## Method Summary
The authors propose COLA, a modular multi-agent framework for multilingual Text-to-SQL generation. COLA consists of three specialized agents: a Classifier that partitions large databases into relevant sub-databases, an Analyzer that decomposes queries and generates SQL using the reduced schema, and a Corrector that iteratively refines queries using execution feedback. The framework operates without task-specific fine-tuning, instead leveraging inference-time refinement. The method is evaluated on MultiSpider 2.0, a new benchmark featuring enterprise-scale schemas across eight languages with three database dialects (BigQuery, Snowflake, SQLite). The paper demonstrates that COLA achieves 15% execution accuracy compared to 4% for reasoning-only approaches.

## Key Results
- MultiSpider 2.0 reveals a 15x multilingual gap: state-of-the-art LLMs achieve only 4% execution accuracy versus 60% on MultiSpider 1.0
- COLA improves execution accuracy to 15% through iterative refinement without fine-tuning
- Schema linking errors account for 33% of failures, particularly problematic for non-English languages
- East Asian languages (ja, zh, vi) underperform by 6.1% average due to non-Latin script challenges
- Pass@20 reaches only 14.88%, indicating sampling alone is insufficient for complex queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured multi-agent decomposition reduces schema-linking errors by partitioning large databases before query generation
- Mechanism: The Classifier agent reduces search space from 200+ enterprise databases to relevant sub-databases, decreasing the probability of selecting incorrect tables/columns during schema linking (the dominant error at 33% per Figure 6)
- Core assumption: Database relevance can be determined independently from query decomposition
- Evidence anchors:
  - [abstract] COLA "iteratively refines queries, improving accuracy to 15%"
  - [section 4.3, Figure 8] Ablation shows Classifier provides +2.4% absolute gain (5.6% → 8.0%)
  - [corpus] AgentSM paper confirms semantic memory helps with "large, complex schemas" but doesn't quantify partitioning gains
- Break condition: If Classifier prunes tables required for multi-hop joins, subsequent agents cannot recover (no rollback mechanism described)

### Mechanism 2
- Claim: Execution-grounded iterative correction outperforms reasoning-only self-refinement
- Mechanism: The Corrector agent uses execution feedback (not just chain-of-thought) to identify logic errors, providing +3.8% gain—the largest single-component improvement per ablation
- Core assumption: Execution errors map reliably to correctable SQL modifications
- Evidence anchors:
  - [abstract] Reasoning-only LLMs achieve 4% vs. COLA's 15%
  - [section 4.3, Figure 8] Corrector provides largest gain (11.6% → 15.4%)
  - [corpus] ExCoT paper shows execution feedback improves reasoning but doesn't compare to multi-agent setups
- Break condition: Semantic errors that execute without syntax errors (wrong aggregation logic) may not trigger correction

### Mechanism 3
- Claim: Sequential pipeline design creates compounding error recovery opportunities
- Mechanism: Three-stage flow (classify → analyze → correct) allows later stages to catch earlier failures; cumulative ablation gain (5.6% → 15.4%) exceeds any single component
- Core assumption: Errors are detectable at each stage boundary
- Evidence anchors:
  - [section 3.3, Figure 2] Shows three-agent pipeline explicitly
  - [section 4.3] Pass@20 reaches only 14.88%, suggesting sampling alone insufficient
  - [corpus] No direct corpus evidence for sequential vs. parallel agent comparison
- Break condition: If early-stage errors propagate without detection (e.g., Classifier misses required schema), Corrector has no mechanism to request re-classification

## Foundational Learning

- Concept: Schema linking in cross-lingual contexts
  - Why needed here: 33% of errors stem from mapping non-English queries to database columns (Figure 6); models struggle with code-switching between localized queries and English schema terms
  - Quick check question: Given a Vietnamese query and English schema, can you identify which columns correspond to "doanh thu quý" (quarterly revenue)?

- Concept: Execution-grounded verification vs. syntactic matching
  - Why needed here: EX >> EM (Figure 5 shows points above y=x line), meaning functionally correct queries differ syntactically from gold standard; evaluation must use execution results
  - Quick check question: Two SQL queries return identical result sets but use different JOIN orders—which metric (EM or EX) correctly marks both as successful?

- Concept: Test-time compute tradeoffs in multi-agent systems
  - Why needed here: COLA requires 3 LLM calls per query minimum; Section 3.2 mentions "extended query generation times" as motivation for the baseline
  - Quick check question: If latency budget allows only 2 LLM calls, which COLA component should be disabled first (Classifier, Analyzer, or Corrector)?

## Architecture Onboarding

- Component map: Input: (Question Q, Schema D, Docs E) → [Classifier] → Partitioned sub-schema D' → [Analyzer] → Decomposed sub-questions + draft SQL → [Corrector] → Execute SQL → Error feedback → Refined SQL → Output: Final executable SQL

- Critical path: Analyzer → Corrector (provides 7.4% combined gain; Classifier optional for smaller schemas)

- Design tradeoffs:
  - Classifier adds latency but essential for 200+ column schemas (22.15% of tasks have multiple schemas per Table 2)
  - Corrector requires database execution access (not all environments allow this)
  - No rollback: if Classifier prunes needed tables, must restart entire pipeline

- Failure signatures:
  - EX < 8%: Likely Classifier failing on nested schemas (18.51% of tasks per Table 2)
  - High EM/EX gap: Corrector not triggered (check execution feedback pipeline)
  - East Asian languages (ja, zh, vi) underperforming by 6.1% average: Schema linking fails on non-Latin scripts; consider transliteration preprocessing

- First 3 experiments:
  1. Run ablation with only Analyzer+Corrector on 50-sample subset to establish upper bound when Classifier is perfect
  2. Stratify by schema size (<100 vs >200 columns) to identify Classifier threshold where partitioning becomes necessary
  3. Compare error distributions between English and Vietnamese to confirm if "wrong schema linking" (33%) increases proportionally for non-English languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can stepwise "schema-grounded planning" significantly reduce the "wrong schema linking" errors that account for 33% of failures?
- Basis in paper: [explicit] The authors identify "Schema-grounded planning" as a future priority to bind entities early and select join paths, directly addressing the primary error mode identified in the analysis
- Why unresolved: Current models fail to perform long-horizon reasoning on large, heterogeneous enterprise schemas without explicit planning stages
- What evidence would resolve it: An ablation study comparing baseline linking accuracy against a stepwise planning agent on the MultiSpider 2.0 development set

### Open Question 2
- Question: Does execution-grounded learning (e.g., RLHF or verifier-guided search) improve the sample efficiency ceiling beyond the observed 14.88% Pass@20?
- Basis in paper: [inferred] The paper notes that simply generating more answers is insufficient (low Pass@20) and proposes "Execution-grounded learning" as a necessary methodological advance
- Why unresolved: The COLA framework currently relies on inference-time decomposition rather than trained feedback loops for optimization
- What evidence would resolve it: Comparing the Pass@N curve of the current COLA baseline against an RLHF-optimized variant on complex compositional queries

### Open Question 3
- Question: To what extent does "dialect-aware normalization" (handling code-switching and regional synonyms) bridge the performance gap for East Asian languages?
- Basis in paper: [explicit] The authors propose developing "dialect-aware normalization" to handle code-switching and ambiguity, noting specific difficulties with Chinese, Japanese, and Vietnamese due to linguistic structure
- Why unresolved: Current models suffer from brittleness when mixing English technical terms (schema elements) with non-English queries
- What evidence would resolve it: Evaluating a model equipped with learned normalizers or lightweight lexicons specifically on the East Asian language subsets of the benchmark

## Limitations
- Agent Implementation Gaps: The paper lacks specific details on the Classifier agent's schema partitioning algorithm, making exact reproduction difficult without engineering decisions about database relevance scoring
- Dialect Coverage: While the dataset includes BigQuery, Snowflake, and SQLite, the paper doesn't specify how COLA handles dialect-specific SQL syntax variations during correction
- Execution Environment Requirements: The Corrector agent requires database access for execution feedback, which may not be feasible in all deployment scenarios

## Confidence
- High Confidence: The benchmark construction methodology and the documented multilingual gap (4% vs 60%) are well-supported by dataset statistics
- Medium Confidence: The 15% accuracy improvement claim is credible given the ablation results, though dependent on implementation details not fully specified
- Low Confidence: Claims about COLA's superiority in enterprise settings are theoretical, as no real-world enterprise deployment results are presented

## Next Checks
1. Implement a simplified version of the Classifier agent using basic token overlap heuristics and measure the impact on schema linking accuracy
2. Test COLA's performance on synthetic schemas with controlled complexity to isolate the effect of database partitioning
3. Evaluate whether execution feedback from Corrector can be replaced with simulated feedback to assess the importance of actual database access