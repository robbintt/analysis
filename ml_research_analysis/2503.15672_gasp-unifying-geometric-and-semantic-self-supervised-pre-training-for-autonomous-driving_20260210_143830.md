---
ver: rpa2
title: 'GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous
  Driving'
arxiv_id: '2503.15672'
source_url: https://arxiv.org/abs/2503.15672
tags:
- occupancy
- gasp
- semantic
- vision
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GASP addresses the challenge of learning rich representations for
  autonomous driving from large-scale unlabeled spatiotemporal data. The method integrates
  geometric, semantic, and temporal supervision by predicting future occupancy, vision
  foundation model features, and ego vehicle path in a unified 4D representation.
---

# GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving

## Quick Facts
- **arXiv ID:** 2503.15672
- **Source URL:** https://arxiv.org/abs/2503.15672
- **Reference count:** 40
- **Primary result:** GASP achieves state-of-the-art self-supervised pretraining for autonomous driving by predicting geometric, semantic, and ego-path occupancy in continuous 4D spacetime

## Executive Summary
GASP addresses the challenge of learning rich representations for autonomous driving from large-scale unlabeled spatiotemporal data. The method integrates geometric, semantic, and temporal supervision by predicting future occupancy, vision foundation model features, and ego vehicle path in a unified 4D representation. The model is trained to forecast these elements at any queried point in spacetime, conditioned on past lidar inputs. This approach enables the model to capture both the structure of the 3D environment and its semantic evolution over time. Evaluations on multiple autonomous driving benchmarks demonstrate that GASP significantly outperforms state-of-the-art pretraining methods, particularly on semantic tasks like map segmentation and semantic occupancy forecasting, while also improving geometric occupancy prediction and ego trajectory forecasting.

## Method Summary
GASP uses a 4D occupancy field prediction framework that combines geometric, semantic, and ego-motion supervision. The method takes K=3 past lidar scans (0.5s intervals) and ego poses as input, processes them through a ResNet-style encoder with deformable attention and FPN to produce BEV features Z. Three implicit decoders query Z at continuous coordinates to predict: (1) binary occupancy probability, (2) 16-dim DINOv2 feature regression, and (3) ego-path probability. Training uses multi-task loss with λ_occ=1.0, λ_dino=0.5, λ_ego=0.1, optimized via Adam with cosine LR schedule. The model predicts at arbitrary spacetime queries without requiring voxelization, enabling continuous supervision from sparse lidar.

## Key Results
- GASP significantly outperforms state-of-the-art pretraining methods on Argoverse 2 and ZOD benchmarks
- Shows particular strength on semantic tasks like map segmentation (mIoU) and semantic occupancy forecasting
- Achieves superior geometric occupancy prediction (R@P70) and ego trajectory forecasting
- Demonstrates strong scaling properties with logarithmic performance improvement as training data increases

## Why This Works (Mechanism)

### Mechanism 1: Continuous 4D Occupancy Field Prediction
Predicting occupancy at arbitrary spatiotemporal queries forces the model to learn a continuous, structured representation rather than memorizing discrete sensor patterns. The model encodes past lidar into BEV feature map Z, which implicit decoders query at continuous coordinates q=(x,y,z,t) to predict occupancy probability. Positive samples come from buffer zones behind lidar returns; negative samples from along ray paths (unoccupied space). This creates dense supervision from sparse lidar without labels.

### Mechanism 2: Vision Foundation Model Feature Distillation for Semantic Understanding
Distilling pretrained 2D vision features into 3D spatiotemporal predictions injects semantic knowledge without manual annotation. Future lidar points project to temporally-aligned camera images, where DINOv2 features (reduced to 16 PCA components) become regression targets. The model learns to predict how semantic features evolve in 3D over time, transferring 2D visual knowledge to 3D reasoning.

### Mechanism 3: Ego-Path Prediction as Motion Prior
Predicting ego vehicle path (time-independent positions) teaches drivable area understanding and multi-modal motion reasoning. Future ego poses define positive path queries within 1m radius; negatives fill remaining ROI. This path, not trajectory, avoids ambiguity when stationary. The decoder learns a probability field over possible ego positions.

## Foundational Learning

- **Implicit Neural Representations**: GASP queries a neural network to predict values at continuous coordinates rather than discrete grid outputs. Understanding how MLPs can represent continuous fields is essential.
  - Quick check: Can you explain why predicting at continuous queries differs from predicting on a fixed voxel grid?

- **Self-Supervised Learning from Sensor Structure**: All supervision comes from exploiting sensor physics (lidar ray geometry, camera-lidar correspondence, ego odometry logs).
  - Quick check: What makes a lidar "missing ray" a valid supervisory signal for unoccupied space?

- **BEV (Bird's Eye View) Feature Maps**: The encoder compresses 3D lidar history into a 2D top-down representation Z ∈ R^(H×W×C), which all decoders query.
  - Quick check: What information is lost when projecting 3D to BEV, and how does GASP mitigate this?

## Architecture Onboarding

- **Component map**: Past LiDAR (K=3 scans) → Voxelization → ResNet+Deformable Attn+FPN → BEV Feature Map Z → Occupancy Decoder (binary classification), DINOv2 Feature Decoder (16-dim regression), Ego-Path Decoder (binary classification)

- **Critical path**: Query generation → BEV feature interpolation → Decoder prediction → Loss computation. Queries must align with supervision targets spatially and temporally.

- **Design tradeoffs**:
  - DINOv2 dimensions: 8→16→32 PCA components tested; 16 balances expressiveness and training stability
  - Rotation augmentation: ±20° optimal; larger angles (±90°) hurt performance due to unrealistic driving orientations
  - Missing ray supervision: Improves qualitative artifacts (halos above vehicles) but shows no quantitative gain

- **Failure signatures**:
  - Occupancy halos: Ghost predictions above vehicles if missing ray supervision disabled
  - Directional bias: Over-prediction along cardinal directions without rotation augmentation
  - Semantic leakage: Incorrect DINOv2 features if depth filtering fails

- **First 3 experiments**:
  1. Ablate each loss term: Train with only occupancy, add DINOv2, add ego-path. Measure 4D occupancy recall and downstream task transfer
  2. Scaling test: Pre-train on 1k→1M samples, plot log-scale performance curve to verify no saturation
  3. Frozen vs. unfrozen encoder: Fine-tune downstream tasks with encoder frozen vs. unfrozen to test representation quality vs. initialization quality

## Open Questions the Paper Calls Out

- **Camera-centric or multi-modal BEV encoders**: Can GASP be effectively adapted for camera-centric or multi-modal Bird's Eye View (BEV) encoders?
- **Alternative foundation models**: Does distilling features from alternative foundation models (e.g., CLIP, SAM) yield superior representations compared to DINOv2?
- **Missing ray supervision paradox**: Why does "missing ray" supervision fail to improve quantitative occupancy metrics despite reducing visual artifacts?
- **Scaling efficiency limits**: What algorithmic improvements are necessary to bridge the gap between current scaling trends and the data volume required for "near-perfect" prediction?

## Limitations

- Architecture reproducibility issues due to unspecified encoder dimensions and deformable attention configuration
- Semantic feature extraction depends on closed-source "denoised DINOv2" procedure with unclear implementation details
- Missing ray supervision algorithm details are unspecified, affecting occupancy prediction quality
- Performance evaluation limited to LiDAR-based models without testing camera or multi-modal variants

## Confidence

- **High confidence** in geometric occupancy prediction claims: Well-grounded in query-based implicit representations with consistent improvements across datasets
- **Medium confidence** in semantic understanding claims: Strong downstream transfer but dependent on specific vision foundation model features and PCA compression
- **Medium confidence** in ego-path prediction claims: Novel approach with clear distinction from trajectory prediction, but lacks direct corpus validation

## Next Checks

1. **Architecture ablation study**: Systematically vary encoder depth, attention head count, and FPN configuration to establish sensitivity and provide clearer reproduction guidelines
2. **VFM feature sensitivity analysis**: Test DINOv2 vs. other foundation models (CLIP, MAE) and vary PCA dimensionality (8→32) to determine if performance gains depend on specific VFM or compression method
3. **Cross-dataset generalization**: Evaluate pre-trained models on entirely disjoint driving datasets (e.g., nuScenes pre-trained on Argoverse 2) to test true representation quality beyond in-domain transfer