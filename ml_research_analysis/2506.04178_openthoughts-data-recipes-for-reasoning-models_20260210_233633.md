---
ver: rpa2
title: 'OpenThoughts: Data Recipes for Reasoning Models'
arxiv_id: '2506.04178'
source_url: https://arxiv.org/abs/2506.04178
tags:
- questions
- question
- code
- reasoning
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The OpenThoughts project creates open-source datasets to train
  reasoning models by systematically investigating data generation pipelines. Through
  over 1,000 controlled experiments across math, code, and science domains, the project
  develops a scalable pipeline that generates 1.2 million question-answer pairs using
  QwQ-32B as the teacher model.
---

# OpenThoughts: Data Recipes for Reasoning Models

## Quick Facts
- **arXiv ID**: 2506.04178
- **Source URL**: https://arxiv.org/abs/2506.04178
- **Reference count**: 40
- **Key outcome**: OpenThinker3-7B achieves 53% on AIME 2025, 51% on LiveCodeBench, and 54% on GPQA Diamond—15.3, 17.2, and 20.5 percentage point gains over DeepSeek-R1-Distill-Qwen-7B

## Executive Summary
OpenThoughts presents a systematic investigation of data generation pipelines for training reasoning models, producing OpenThinker3-7B with state-of-the-art performance on reasoning benchmarks. Through over 1,000 controlled experiments across math, code, and science domains, the project develops a scalable pipeline that generates 1.2 million question-answer pairs using QwQ-32B as the teacher model. Key findings include that concentrating on 1-2 high-quality data sources outperforms diverse mixing, QwQ-32B teaches better than stronger models like DeepSeek-R1, and sampling multiple answers per question provides effective scale augmentation. The project releases all models and datasets at openthoughts.ai.

## Method Summary
The OpenThoughts pipeline starts with Qwen2.5-7B-Instruct as the base model and uses QwQ-32B as the teacher to generate reasoning traces. Data is sourced from 27 open repositories, filtered by difficulty (code) or response length (math/science) using GPT-4o-mini, then processed with exact deduplication for math/science and no deduplication for code. The pipeline generates 16 responses per question, creating a 1.2M sample dataset (850K math, 250K code, 100K science) that's decontaminated using N-gram and Indel similarity filtering. Training uses LlamaFactory with DeepSpeed v3, learning rate 8e-5, global batch size 512, 5 epochs, and cosine decay.

## Key Results
- OpenThinker3-7B achieves 53% on AIME 2025, 51% on LiveCodeBench, and 54% on GPQA Diamond
- 15.3, 17.2, and 20.5 percentage point improvements over DeepSeek-R1-Distill-Qwen-7B
- QwQ-32B outperforms DeepSeek-R1 as a teacher despite lower raw benchmark scores
- Source concentration (top 2 sources) outperforms mixing 16 diverse sources by ~5% average accuracy
- 16× answer sampling per question significantly improves performance over more unique questions

## Why This Works (Mechanism)

### Mechanism 1: Source Concentration over Diversity
Restricting training data to top 1-2 highest-quality sources yields better reasoning performance than mixing diverse sources. Lower-quality sources introduce noise that dilutes the signal from high-quality sources, and strict selection allows the model to learn consistently high-caliber reasoning patterns.

### Mechanism 2: "Weak" Teacher Distillation Efficiency
A teacher model with lower raw benchmark scores (QwQ-32B) can produce superior student models via SFT compared to a teacher with higher raw scores (DeepSeek-R1). The "teachability" or pedagogical alignment of the teacher's reasoning traces matters more than the teacher's final accuracy, as QwQ-32B may generate more learnable reasoning steps for a 7B student.

### Mechanism 3: Redundancy-as-Scale via Multiple Sampling
Increasing dataset scale by sampling multiple answers per question (up to 16×) is an effective data augmentation strategy that outperforms collecting more unique questions. Sampling multiple reasoning paths for the same question exposes the model to diverse problem-solving heuristics and self-correction behaviors for a fixed context.

## Foundational Learning

**Concept: Supervised Fine-Tuning (SFT) Data Curation**
- Why needed: The paper focuses entirely on creating the dataset (OpenThoughts3) rather than novel model architectures. Understanding how to filter, mix, and select data is the core competency required.
- Quick check: Why does the paper argue that filtering questions by LLM-labeled difficulty outperforms embedding-based filtering?

**Concept: Knowledge Distillation (Teacher-Student)**
- Why needed: The pipeline relies on using "teacher" models (QwQ-32B, DeepSeek-R1) to generate reasoning traces for the "student" (7B model). The counter-intuitive finding about "weak" teachers requires a grasp of this relationship.
- Quick check: What specific metric did the paper use to determine QwQ-32B was a "better" teacher than DeepSeek-R1?

**Concept: Decontamination**
- Why needed: To ensure benchmark results (AIME, GPQA) are valid, the training data must be rigorously decontaminated. The paper details a specific N-gram and Indel similarity approach to this.
- Quick check: What two methods does the paper use to identify and remove contaminated samples from the training set?

## Architecture Onboarding

**Component map**: Sourcing -> Mixing -> Filtering -> Deduplication -> Answer Generation -> Teacher Model
**Critical path**: The paper highlights **Question Source Selection** (Section 4.1) and **Teacher Model Selection** (Section 4.6) as the stages providing the largest performance gains
**Design tradeoffs**: A key tradeoff is between **Question Diversity vs. Answer Diversity**. The pipeline explicitly sacrifices question diversity (by selecting few sources and using deduplication) to maximize answer diversity (16× sampling)
**Failure signatures**:
- Over-mixing: Mixing >4 sources caused consistent performance degradation
- Inefficient Filtering: Classical filtering (FastText, Embeddings) failed to improve over random selection
- Strong Teacher Trap: Using DeepSeek-R1 (a stronger model) as a teacher resulted in a weaker student compared to using QwQ-32B
**First 3 experiments**:
1. Replicate Source Mixing: Take a small slice (e.g., 31.6k samples) from the top Code source vs. a mix of top 16 Code sources and verify the performance delta on CodeElo
2. Verify Weak Teacher Hypothesis: Fine-tune two small models (7B), one on QwQ-32B traces and one on DeepSeek-R1 traces, and compare results on a held-out math benchmark
3. Sampling Ratio Test: For a fixed compute budget (e.g., generating 50k tokens), compare training on 1k questions x 50 answers vs. 50k questions x 1 answer to validate the "redundancy-as-scale" mechanism

## Open Questions the Paper Calls Out

### Open Question 1
Does the cross-domain transfer effect (e.g., math training improving science performance) persist when all domains are mixed together in a single dataset? The authors note it's unclear whether transfer persists once domains are mixed, as their ablations measured transfer with single domains but the final dataset mixes all domains.

### Open Question 2
Does model performance plateau or continue to improve as the student model approaches teacher-level performance, and can students ever surpass teachers? The authors ask how this changes as the student's performance approaches the teacher's and whether models eventually plateau or surpass the teacher.

### Open Question 3
How does the interaction between question diversity and answer diversity affect performance across different dataset sizes and model scales? The authors note that limited question diversity has relatively little impact if answer diversity is high, but how this interaction behaves across domains, dataset sizes, and model scales remains open.

### Open Question 4
Would incorporating explicit safety-tuning data preserve reasoning capabilities while mitigating the safety degradation observed in OpenThinker models? The paper identifies the safety-utility tradeoff but doesn't experiment with safety-aware data mixing strategies.

## Limitations
- The 1.2M dataset size represents a specific experimental configuration that may not generalize to other reasoning domains or model sizes
- The paper doesn't extensively explore upper bounds of mixing strategies or systematically analyze why QwQ-32B produces more "learnable" traces than DeepSeek-R1
- Safety degradation is mentioned but not deeply analyzed, suggesting potential deployment risks requiring further investigation

## Confidence

**High Confidence**: The effectiveness of QwQ-32B as a teacher over DeepSeek-R1 (Section 4.6 provides clear comparative evidence across all domains)

**Medium Confidence**: The source concentration hypothesis (Top 2 sources outperforming mixed sources is well-demonstrated, but the theoretical mechanism remains somewhat speculative)

**Medium Confidence**: The 16× sampling strategy's superiority (experimentally validated but the exact mechanism of why redundancy beats diversity isn't fully explained)

## Next Checks

1. **Teacher Model Generalizability Test**: Repeat the teacher comparison experiment using different student model sizes (e.g., 3B and 13B) to determine if QwQ-32B's advantage holds across model scales

2. **Source Quality Boundary Analysis**: Systematically test mixing boundaries by creating datasets that blend top-tier sources with progressively lower-quality sources to identify the precise inflection point where mixing quality degrades performance

3. **Safety Alignment Validation**: Given the reported safety degradation, conduct a controlled experiment fine-tuning OpenThinker3-7B with safety data and measure the impact on both reasoning performance and harmfulness rates to establish whether safety alignment is feasible without significant performance loss