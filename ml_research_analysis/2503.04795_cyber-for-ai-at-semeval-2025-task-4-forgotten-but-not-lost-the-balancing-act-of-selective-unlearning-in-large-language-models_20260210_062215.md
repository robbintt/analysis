---
ver: rpa2
title: 'Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing
  Act of Selective Unlearning in Large Language Models'
arxiv_id: '2503.04795'
source_url: https://arxiv.org/abs/2503.04795
tags:
- unlearning
- gradient
- forget
- language
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents experiments on selective unlearning in LLMs
  for SemEval 2025 Task 4, addressing the challenge of removing sensitive data from
  models without full retraining. The core method involves gradient-based weight modifications,
  including gradient ascent, descent, and controlled variants, to balance unlearning
  effectiveness with knowledge retention and model utility.
---

# Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2503.04795
- Source URL: https://arxiv.org/abs/2503.04795
- Reference count: 25
- Primary result: Gradient-based selective unlearning achieved aggregate scores of 0.409 (7B) and 0.389 (1B) on SemEval 2025 Task 4 test set

## Executive Summary
This paper presents a gradient-based approach to selective unlearning in large language models for SemEval 2025 Task 4, where the goal is to remove sensitive data from models without full retraining. The method employs gradient ascent to unlearn forget-set data and gradient descent to preserve retain-set knowledge, balancing privacy with model utility. Experiments on OLMo-7B and OLMo-1B models demonstrate that selective unlearning is feasible but highly dependent on model scale and data characteristics, with the 7B model requiring more complex multi-stage training to achieve optimal results.

## Method Summary
The method uses gradient-based weight modifications to selectively unlearn sensitive data from LLMs. For the 7B model, a sequential approach was employed: gradient difference on forget set followed by gradient ascent, then gradient descent on retain set. The 1B model used gradient descent on retain set only. Both models used inverted loss functions for unlearning (maximizing loss on forget samples) and standard cross-entropy for retention. The 7B model utilized 4-bit quantization with LoRA adapters, while the 1B model used full fine-tuning. Training was carefully tuned to balance unlearning effectiveness with knowledge retention and utility preservation.

## Key Results
- 7B model achieved highest aggregate score of 0.409 using gradient difference → gradient ascent approach
- 1B model achieved aggregate score of 0.389 using gradient descent on retain set
- Gradient ascent degraded MMLU accuracy from 0.512 to 0.284, recoverable to 0.511 with single epoch of gradient descent (LR=2e-6)
- MIA scores approached 0.5 target (indistinguishable from unseen data) with optimal parameter settings

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent for Selective Forgetting
- Claim: Inverting the loss function on the forget set forces the model to diverge from learned representations of targeted data.
- Mechanism: Instead of minimizing loss on forget samples (standard training), the method maximizes loss by negating the gradient direction. This pushes model weights away from configurations that predict forget-set content, degrading the model's ability to reproduce or reason about that information.
- Core assumption: The forget set's learned representations are sufficiently isolated in parameter space that gradient ascent degrades them without systematically destroying general language capabilities.
- Evidence anchors:
  - [section] "In this method, the target model was trained on the forget set with an inverted loss, thereby, making it to unlearning the training set rather than learning it."
  - [section] "By maximizing the loss on the forget set, this method forces the model to unlearn."
  - [corpus] Related work (QUAIL) confirms gradient-based unlearning can catastrophically restore forgotten information under quantization, indicating the mechanism is fragile to precision changes.
- Break condition: If learning rate or weight decay is too aggressive, model utility collapses—MMLU accuracy dropped from 0.512 to 0.284 in the 7B model after 10 epochs of gradient ascent.

### Mechanism 2: Catastrophic Forgetting via Retain-Set Reinforcement
- Claim: Aggressively training on the retain set can induce natural forgetting of the forget set through representational competition.
- Mechanism: By optimizing the model exclusively on retain-set samples, the optimization trajectory shifts weights toward configurations that predict retain content well. Since model capacity is finite, this repurposes parameters previously encoding forget-set knowledge, achieving unlearning as a side effect of retention-focused training.
- Core assumption: Forget and retain sets occupy overlapping or competing regions of parameter space, so strengthening retain representations weakens forget representations.
- Evidence anchors:
  - [section] "The intuition is that, the strong adaption of the model only to the retain set can naturally make it tend to forget the other information (forget set) it was previously trained on, like catastrophic forgetting."
  - [section] For the 1B model trained for 6 epochs on retain set: "it has reached the optimal level of unlearning required (∼ 0.5), and also got a slight boost in the MMLUAA."
  - [corpus] Weak corpus signal—related SemEval-2025 Task 4 submissions used different primary approaches; corpus does not strongly validate this mechanism's generality.
- Break condition: On the 7B model, gradient descent alone achieved near-zero task aggregate (0.170) with no effective unlearning (MIA score 0.005), suggesting scale-dependent failure.

### Mechanism 3: Sequential Gradient Operations for Forgetting-Retention Balance
- Claim: Composing gradient ascent (unlearning) followed by gradient descent (recovery) enables finer control over the forgetting-utility tradeoff than either operation alone.
- Mechanism: Gradient ascent first degrades forget-set representations but also damages general capabilities. A subsequent, carefully-tuned gradient descent phase on the retain set selectively restores utility-critical weights while leaving forget-set degradation intact. The sequence leverages asymmetric recovery dynamics.
- Core assumption: The retain set provides sufficient signal to restore general capabilities without simultaneously recovering forget-set knowledge.
- Evidence anchors:
  - [section] "It is important to note that the 10 epochs of gradient ascent has brought down the MMLUAA to 0.284 from 0.512, and a single subsequent gradient descent epoch with LR as low as 2e-6 brought it back to 0.511."
  - [section] "Gradient difference followed by Gradient ascent... with optimal parameter settings, this method excelled on the 7B model, achieving the highest aggregate score."
  - [corpus] Related SemEval submissions (Lacuna Inc., Atyaephyra) explored similar multi-stage approaches combining gradient methods with low-rank adaptations.
- Break condition: Learning rate sensitivity is extreme; gradient descent with LR=2e-6 successfully recovered utility, but LR=2e-8 after strong unlearning (MIA 0.982) "reinstated it alike the original," effectively reversing the unlearning.

## Foundational Learning

- Concept: **Gradient Ascent vs. Descent in Loss Landscapes**
  - Why needed here: The entire method inverts standard training intuition; you must understand that gradient direction determines whether knowledge is acquired or suppressed.
  - Quick check question: If a model has loss L = 2.3 on a forget sample, what happens to its prediction probability for that sample after one gradient ascent step?

- Concept: **Catastrophic Forgetting in Sequential Training**
  - Why needed here: The retain-only training approach relies on this phenomenon to achieve unlearning as a side effect; understanding it helps predict when the method will work.
  - Quick check question: Why does training a model on dataset B after dataset A often degrade performance on A, even without explicitly targeting A?

- Concept: **Membership Inference Attacks (MIA) as Unlearning Verification**
  - Why needed here: The evaluation framework uses MIA AUC to detect whether forget samples remain in the model's "memory"; you need to interpret 0.5 as the target (indistinguishable from unseen data).
  - Quick check question: If an unlearned model has MIA AUC of 0.9 on forget-set members, what does this indicate about the unlearning's effectiveness?

## Architecture Onboarding

- Component map:
  - Forget set: 1,112 training samples to be unlearned (synthetic documents, PII biographies, real training data)
  - Retain set: 1,136 training samples to preserve
  - Target models: OLMo-7B-Instruct (trained 4-bit PEFT) and OLMo-1B (full fine-tuning)
  - Evaluation triad: Task aggregate (forget/retain performance), MIA score (privacy verification), MMLU accuracy (utility preservation)

- Critical path:
  1. Load target model with appropriate quantization (4-bit for 7B, none for 1B)
  2. Run gradient ascent on forget set (inverted loss)
  3. Run gradient descent on retain set (standard loss, low LR)
  4. Evaluate on all three metrics before submission

- Design tradeoffs:
  - Aggressive unlearning (high LR, many epochs) → strong MIA score but collapsed MMLU
  - Conservative unlearning → preserved utility but incomplete forgetting
  - 4-bit quantization for 7B → feasible memory but introduces numerical instability that may enhance or destabilize unlearning unpredictably

- Failure signatures:
  - MIA score near 0 → under-unlearning; forget data still recoverable
  - MIA score near 1 → over-unlearning; model may be corrupted
  - MMLU < 0.371 threshold → utility collapse, disqualifying per task rules
  - Task aggregate = 0 with non-zero MIA → retain set catastrophically forgotten

- First 3 experiments:
  1. Establish baseline: Run gradient ascent on 1B model with LR=2e-7, WD=2e-6, 6 epochs; record all three metrics to understand unlearning intensity vs. utility tradeoff.
  2. Test retain-only approach: Train 1B model on retain set only with LR=2e-5, 20 epochs; compare MIA score to baseline to validate catastrophic forgetting hypothesis for this data.
  3. Tune the recovery phase: Take the 7B model after gradient ascent (10 epochs), then run gradient descent on retain set with LR=2e-6 for 1-3 epochs; plot MMLU recovery vs. MIA degradation to find the Pareto frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quantization (e.g., 4-bit PEFT) affect unlearning effectiveness compared to full-precision training in larger models?
- Basis in paper: [inferred] The authors note that "Quantization in the 7B model may have also enhanced unlearning, potentially due to increased numerical instability aiding divergence from the learned state," but this was not systematically tested against a full-precision baseline.
- Why unresolved: The 7B model was only trained in 4-bit PEFT configuration due to computational constraints, making it unclear whether the observed unlearning behaviors are intrinsic to the methods or artifacts of quantization.
- What evidence would resolve it: A controlled comparison of the same unlearning methods on 7B models in both full-precision and quantized configurations.

### Open Question 2
- Question: Why does gradient descent on the retain set alone achieve strong unlearning in 1B models but fail completely in 7B models?
- Basis in paper: [inferred] The authors report that gradient descent achieved an aggregate score of 0.410 with near-perfect unlearning (MIA 0.982) for the 1B model, but the same method yielded an aggregate of only 0.170 with MIA near 0 for the 7B model.
- Why unresolved: The paper documents this scale-dependent divergence but does not investigate whether it stems from model architecture, parameter count, data characteristics, or the PEFT configuration used for the 7B model.
- What evidence would resolve it: Ablation studies varying model scale incrementally (e.g., 1B, 3B, 7B) with identical training configurations to isolate the factor causing the performance gap.

### Open Question 3
- Question: Can effective unlearning be achieved when the forget set contains very limited samples, and what is the minimum sample threshold?
- Basis in paper: [explicit] The authors state: "availability or provision of only limited forget samples could lead to ineffective unlearning of the target model," noting that an assistant model trained on the small forget set "could not pick up the forget set due to its small quantity."
- Why unresolved: The logits-difference approach failed due to insufficient forget set size, but no systematic analysis was conducted on the relationship between forget set size and unlearning success across different methods.
- What evidence would resolve it: Experiments with artificially subsampled forget sets of varying sizes to identify the minimum threshold needed for effective unlearning across different methods.

## Limitations

- Scale dependency: Methods effective for 1B models failed completely on 7B models, indicating fundamental scaling challenges
- Data sensitivity: Unlearning effectiveness highly dependent on forget set size and data characteristics
- Hyperparameter sensitivity: Success requires precise tuning of learning rates, epochs, and weight decay
- Quantization uncertainty: 4-bit training introduces numerical instability that may help or hinder unlearning unpredictably

## Confidence

- **High Confidence**: The gradient-based unlearning mechanisms (ascent/descent) fundamentally work as described - the mathematical operations and
- **Medium Confidence**: Scale-dependent behavior (1B vs 7B) - observed but not fully explained
- **Medium Confidence**: Hyperparameter sensitivity - demonstrated but optimal ranges not systematically mapped
- **Low Confidence**: Quantization effects - mentioned but not experimentally validated against full-precision baseline

## Next Checks

1. Verify MIA score computation: Replicate the negative log likelihood calculation and ROC-AUC procedure to ensure the 0.5 target is correctly interpreted
2. Test learning rate sensitivity: Systematically vary LR from 1e-8 to 1e-4 for both gradient ascent and descent to map the stability frontier
3. Validate scale scaling: Reproduce the same method on 3B model (if available) to determine whether the 1B/7B gap is monotonic or has intermediate behavior