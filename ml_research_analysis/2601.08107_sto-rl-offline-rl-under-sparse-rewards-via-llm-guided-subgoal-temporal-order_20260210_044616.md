---
ver: rpa2
title: 'STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order'
arxiv_id: '2601.08107'
source_url: https://arxiv.org/abs/2601.08107
tags:
- learning
- subgoal
- offline
- reward
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STO-RL, an offline reinforcement learning
  framework that leverages large language models (LLMs) to generate temporally ordered
  subgoal sequences and corresponding state-to-subgoal mappings. By applying potential-based
  reward shaping that incorporates this temporal structure, STO-RL transforms sparse
  terminal rewards into dense, temporally consistent signals that promote subgoal
  progress while avoiding suboptimal solutions.
---

# STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order

## Quick Facts
- arXiv ID: 2601.08107
- Source URL: https://arxiv.org/abs/2601.08107
- Authors: Chengyang Gu; Yuxin Pan; Hui Xiong; Yize Chen
- Reference count: 40
- One-line primary result: Offline RL method using LLM-generated temporally ordered subgoals with potential-based reward shaping achieves state-of-the-art performance on four sparse-reward benchmarks

## Executive Summary
STO-RL addresses the challenge of offline reinforcement learning under sparse terminal rewards by leveraging large language models to generate temporally ordered subgoal sequences and corresponding state-to-subgoal mappings. The method applies potential-based reward shaping that incorporates this temporal structure, transforming sparse rewards into dense, temporally consistent signals that promote efficient subgoal progress. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories.

## Method Summary
STO-RL operates by first prompting an LLM with task instructions and environment descriptions to generate temporally ordered subgoal sequences and state-to-subgoal-stage mapping functions. The method then applies a potential function Φ(s_t) = -t/T · 1/k_t that incorporates both timestep and progress index, using this in potential-based reward shaping to transform sparse terminal rewards into dense signals. Finally, an IQL agent is trained on the augmented dataset with shaped rewards, preserving the optimal policy while accelerating learning through richer training signals.

## Key Results
- Outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines on four benchmarks
- Achieves faster convergence and higher success rates compared to baselines
- Produces shorter trajectories while maintaining high success rates
- Demonstrates robustness to imperfect or noisy LLM-generated subgoal sequences

## Why This Works (Mechanism)

### Mechanism 1: LLM-Guided Temporal Subgoal Decomposition
LLMs can decompose goal-reaching tasks into temporally ordered subgoal sequences that capture inherent task structure. The framework prompts an LLM with task instructions and environment descriptions, producing both a subgoal sequence {G1, G2, ..., GK} and a state-to-subgoal-stage mapping function h: S → {1, 2, ..., K} that assigns each state a progress index kt representing temporal position.

### Mechanism 2: Subgoal-Temporal-Order-Aware Potential-Based Reward Shaping
A potential function incorporating timestep and progress index transforms sparse terminal rewards into dense signals that prefer efficient goal-reaching behavior. The shaped reward is r'(st, at, st+1) = r(st, at, st+1) + γΦ(st+1) - Φ(st), where transitions achieving positive progress (kt+1 > kt) receive strictly higher shaped rewards.

### Mechanism 3: Policy Invariance Guarantee Through Potential-Based Formulation
The potential-based shaping formulation preserves optimal policy structure while accelerating learning. By using the temporal difference form γΦ(st+1) - Φ(st), the cumulative shaped reward differs from original by only a constant term dependent on initial/final states, ensuring the optimal policy remains unchanged while receiving richer training signals.

## Foundational Learning

- **Concept: Offline Reinforcement Learning**
  - Why needed here: STO-RL operates entirely on pre-collected datasets without environment interaction; understanding offline RL's distributional shift and data coverage challenges is essential
  - Quick check question: Can you explain why offline RL methods like IQL use expectile regression rather than standard Q-learning?

- **Concept: Potential-Based Reward Shaping**
  - Why needed here: The core mechanism builds on PBRS theory; without understanding why γΦ(s') - Φ(s) preserves optimality, the method's theoretical justification is opaque
  - Quick check question: Why does potential-based shaping guarantee policy invariance while intrinsic reward methods may not?

- **Concept: Goal-Conditioned RL**
  - Why needed here: STO-RL addresses goal-reaching tasks with sparse terminal rewards; understanding how goal-conditioning enables generalization across goals clarifies the problem framing
  - Quick check question: How does hindsight experience replay address sparse rewards in goal-conditioned settings?

## Architecture Onboarding

- **Component map:**
  Task instruction (text) -> LLM Prompt -> [LLM] -> Subgoal sequence G + Mapping h -> Reward Shaping Module -> Augmented dataset D' -> [IQL Agent] -> Trained policy π*

- **Critical path:**
  1. Construct prompt with task description + discretized environment map
  2. Parse LLM output into subgoal sequence and state-mapping dictionary
  3. Apply reward shaping to every transition in offline dataset
  4. Train IQL agent on augmented dataset with shaped rewards

- **Design tradeoffs:**
  - LLM choice vs. subgoal quality: Better LLMs yield more coherent temporal orderings but increase cost/latency
  - Discretization granularity: Finer discretization improves state-mapping precision but increases prompt length and LLM error potential
  - Discount factor selection: Must satisfy γ > (T-1)/T for Theorem 2; higher γ increases credit assignment horizon but may slow convergence

- **Failure signatures:**
  - Subgoals guide agent to dead ends
  - Learning curves plateau below baseline success rates
  - Trajectories longer than baseline despite higher success rates

- **First 3 experiments:**
  1. Validate LLM subgoal extraction: Manually inspect LLM-generated subgoal sequences for temporal consistency before training
  2. Ablate shaping component: Compare STO-RL against IQL with same subgoals but without temporal-order-aware shaping
  3. Test discount boundary: Run experiments with γ below and above (T-1)/T threshold to validate Theorem 2's condition

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-generated subgoal quality depends heavily on prompt engineering quality with no systematic evaluation of subgoal sequence correctness
- Offline RL performance is sensitive to dataset composition; limited details on expert policy behavior and trajectory diversity
- Potential-based shaping assumes perfect knowledge of task horizon T; real-world tasks often have unknown or variable horizons

## Confidence

**Confidence labels:**
- High: Policy invariance through potential-based formulation (theoretically sound)
- Medium: LLM subgoal generation effectiveness (empirical results strong but depend on LLM quality)
- Medium: Generalization across diverse environments (4 benchmarks show consistent improvement but may not cover all task types)

## Next Checks
1. Ablation of LLM dependency: Run STO-RL with manually designed subgoals vs. LLM-generated subgoals on the same tasks to isolate LLM contribution from shaping mechanism
2. Cross-dataset robustness: Evaluate STO-RL on datasets collected under different expert policies or with different mixture ratios to test sensitivity to offline data distribution
3. Temporal ordering sensitivity: Test STO-RL with shuffled vs. ordered subgoals to empirically validate the importance of temporal structure in the shaping potential