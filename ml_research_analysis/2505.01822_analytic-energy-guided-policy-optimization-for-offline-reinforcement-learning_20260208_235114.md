---
ver: rpa2
title: Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning
arxiv_id: '2505.01822'
source_url: https://arxiv.org/abs/2505.01822
tags:
- learning
- diffusion
- arxiv
- offline
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Analytic Energy-guided Policy Optimization (AEPO),
  a method for offline reinforcement learning using diffusion models. The core challenge
  addressed is estimating intermediate energy in energy-guided diffusion models, which
  is intractable due to the log-expectation formulation.
---

# Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.01822
- Source URL: https://arxiv.org/abs/2505.01822
- Authors: Jifeng Hu; Sili Huang; Zhejian Yang; Shengchao Hu; Li Shen; Hechang Chen; Lichao Sun; Yi Chang; Dacheng Tao
- Reference count: 40
- Primary result: AEPO achieves state-of-the-art performance on 30+ offline RL tasks from D4RL benchmarks

## Executive Summary
AEPO addresses the challenge of estimating intermediate energy in energy-guided diffusion models for offline RL. The method provides a theoretical analysis and closed-form solution for intermediate guidance under conditional Gaussian transformation, simplifying the intractable log-expectation formulation using Gaussian distribution properties. AEPO trains an intermediate energy neural network to approximate the log-expectation formulation and demonstrates superior performance compared to numerous baselines across various task types.

## Method Summary
AEPO combines diffusion models with energy-based guidance for offline RL. The method derives a closed-form approximation for intermediate energy using Taylor expansion and Gaussian moment-generating functions, then trains a neural network to approximate this formulation. During inference, the model uses guided sampling where the gradient of the intermediate energy network is combined with the diffusion model's score function, with guidance rescaling to stabilize performance. The Q-function is trained via expectile regression using only in-sample actions to avoid distribution shift.

## Key Results
- Achieves state-of-the-art performance on 30+ offline RL tasks from D4RL benchmarks
- Outperforms numerous baselines including diffusion-based and non-diffusion-based methods
- Demonstrates effectiveness across task types including Gym-MuJoCo, Pointmaze, Locomotion, and Adroit
- Shows robust performance with guidance rescaling across various guidance scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The intractable log-expectation formulation for intermediate energy can be approximated analytically by combining first-order Taylor expansion with the moment-generating function of Gaussian distributions.
- Mechanism: The exact intermediate energy Et(s,at) = log E_{a0~μ(a0|at,s)}[e^{βQ(s,a0)}] is decomposed by: (1) Taylor expanding Q(s,a0) around point ā to convert implicit dependence on a0 into explicit linear dependence; (2) applying the Gaussian moment-generating function E_{x~N(v,Σ)}[e^{a^Tx}] = e^{a^Tv + ½a^TΣa}, yielding the closed-form approximation Et(s,at) ≈ βQ(s,ā) + βQ'(s,ā)^T(μ̃_{0|t} - ā) + ½β²σ̃²_{0|t}||Q'(s,ā)||².
- Core assumption: The Q-function has low curvature between ā and a0 (second-order Taylor remainder negligible), and the posterior μ(a0|at,s) is approximately Gaussian.
- Evidence anchors:
  - [section 3.1] "We first convert the implicit dependence on the action in the exponential term to explicit dependence by applying Taylor expansion."
  - [section 3.1] "Applying the above results... we have βQ'(s,ā)^T μ̃_{0|t} + ½β²Q'(s,ā)^T Σ̃_{0|t}Q'(s,ā)"
  - [corpus] Related papers (QGPO, FlowQ) address energy guidance but use inexact approximations; corpus lacks direct evidence for this analytic derivation.

### Mechanism 2
- Claim: Posterior distribution parameters μ̃_{0|t} and Σ̃_{0|t} can be estimated directly from the trained diffusion model's noise prediction network without additional learnable components.
- Mechanism: The posterior mean μ̃_{0|t} = (at - σtεθ(s,at,t))/αt is derived from the forward process definition. The covariance Σ̃_{0|t} = (σ²t/α²t)[I - E_{μt(at|s)}[εθεθ^T]] captures uncertainty from imperfect noise prediction, estimated empirically from batch statistics.
- Core assumption: The diffusion model's noise prediction εθ is sufficiently accurate, and the isotropic Gaussian approximation Σ̃_{0|t} = σ̃²_{0|t}I is reasonable.
- Evidence anchors:
  - [section 3.2, Posterior 1] "We can use the trained diffusion model εθ to obtain the mean vector μ̃_{0|t}"
  - [Figure 1] Posterior 1 outperforms Posterior 2 across Gym-MuJoCo tasks
  - [corpus] No corpus papers provide this specific posterior derivation; related work assumes but doesn't estimate these parameters.

### Mechanism 3
- Claim: Guidance rescaling—normalizing guided score magnitude to match unguided score magnitude—stabilizes performance across guidance scale hyperparameter ω.
- Mechanism: Replace ∇at log πt(at|s) = ∇at log μt(at|s) + ω∇at Et(s,at) with ∇at log πt(at|s) = [∇at log πt(at|s)/||∇at log πt(at|s)||] · ||∇at log μt(at|s)||, preserving direction while matching the base distribution's score magnitude.
- Core assumption: The unguided score magnitude provides a stable reference; guidance direction matters more than raw magnitude.
- Evidence anchors:
  - [section 3.4] "When the guidance scale is zero, the inference performance is more stable than that when the guidance scale is non-zero."
  - [Figure 2] Rescaled guidance maintains stable performance as ω increases from 0.1 to 3.0, while non-rescaled degrades rapidly.
  - [corpus] No corpus evidence; this is a novel heuristic contribution.

### Mechanism 4
- Claim: Using expectile regression for Q-function training with in-sample actions only (IQL-style) provides better intermediate energy estimates than conservative Q-learning or in-support methods that involve generated actions.
- Mechanism: The value function Vϕ is trained via expectile regression L²τ(y) = |τ - 1(y<0)|y² on Q-values from dataset actions, avoiding OOD actions entirely. Qψ is updated via standard TD using Vϕ(s').
- Core assumption: In-sample Q-values are reliable; out-of-distribution actions should not influence training.
- Evidence anchors:
  - [section 3.3] "We leverage the expectile regression loss to train the Q function and V function"
  - [Figure 1] AEPO-IQL (default) outperforms AEPO-CQL and AEPO-ISQL variants
  - [corpus] Related work (IQL, QGPO) uses similar in-sample training but corpus provides no comparative evidence for this choice.

## Foundational Learning

- Concept: **Score-Based Diffusion Models and SDEs**
  - Why needed here: AEPO manipulates score functions ∇xt log pt(xt) to guide generation; understanding the reverse SDE dx = [f(x,t) - g(t)²∇x log qt(xt)]dt + g(t)dw̄ is essential.
  - Quick check question: Can you explain why ∇xt log qt(xt) ≈ -εθ/σt and how this connects noise prediction to score functions?

- Concept: **Energy-Based Models and KL-Constrained Optimization**
  - Why needed here: The optimal policy π*(a|s) ∝ μ(a|s)e^{βQ(s,a)} derives from KL-constrained RL; understanding this connection is foundational.
  - Quick check question: Derive why the Lagrangian multiplier β controls the trade-off between maximizing Q and staying close to behavior policy μ.

- Concept: **Offline RL Distribution Shift and Conservative Q-Learning**
  - Why needed here: AEPO addresses distribution shift through behavior constraints and in-sample Q-learning; understanding why OOD actions cause overestimation is critical.
  - Quick check question: Why does standard Q-learning fail in offline settings, and how does expectile regression address this?

- Concept: **Moment-Generating Functions of Gaussian Distributions**
  - Why needed here: The key analytic insight E_{x~N(v,Σ)}[e^{a^Tx}] = e^{a^Tv + ½a^TΣa} enables the closed-form solution.
  - Quick check question: Compute E_{x~N(μ,σ²I)}[e^{c^Tx}] for a simple 1D case.

## Architecture Onboarding

- Component map:
  - Diffusion model εθ(s,at,t) -> Q-function Qψ(s,a) -> V-function Vϕ(s) -> Intermediate energy network EΘ(s,at,t)
  - Posterior estimator (non-learned) -> Intermediate energy network EΘ(s,at,t)

- Critical path:
  1. Train εθ via diffusion loss (Eq. 7) — this is the behavior policy model
  2. Train Qψ, Vϕ via expectile regression (Eq. 27) — these provide energy function
  3. For each batch: compute μ̃_{0|t}, σ̃²_{0|t} via Posterior 1; compute target Et(s,at) via Eq. 25; train EΘ via MSE (Eq. 26)
  4. Inference: sample aT ~ N(0,I); iteratively denoise using ∇at log πt = ∇at log μt + ∇at EΘ with rescaling

- Design tradeoffs:
  - **Posterior 1 vs. Posterior 2**: Posterior 1 is simpler (variance from εθ only) and empirically better; Posterior 2 uses dataset variance but adds complexity
  - **IQL vs. CQL vs. ISQL for Q-training**: IQL (expectile) is default; CQL adds conservatism but may underestimate; ISQL uses generated actions which can introduce OOD issues
  - **Choice of ā**: Default uses ā = a - ν·(Q'/||Q'||) with ν=0.001 to minimize Taylor remainder; larger ν increases approximation error
  - **Guidance rescaling on/off**: Rescaling (default: on) improves robustness to ω; may harm tasks where optimal guidance requires different magnitudes

- Failure signatures:
  - **Exploding gradients during EΘ training**: Check σ̃²_{0|t} computation; if variance becomes negative (numerical instability in εθ statistics), the moment-generating function explodes
  - **Policy collapses to behavior cloning**: Et network not learning; check if Q-function has meaningful gradients (τ may be too low/high)
  - **High variance across seeds**: ω sensitivity; ensure rescaling is enabled; check β tuning
  - **Poor performance on sparse-reward tasks (Maze2D, Adroit)**: Q-function may not learn meaningful values; consider higher τ or different Q-training method

- First 3 experiments:
  1. **Ablation on posterior approximation**: Compare Posterior 1 vs. Posterior 2 on 2-3 Gym-MuJoCo tasks (e.g., halfcheetah-medium, hopper-medium-expert); expect Posterior 1 to match or exceed Posterior 2. Report normalized scores and training stability.
  2. **Guidance rescaling sensitivity**: Run walker2d-medium-expert with rescaling on/off across ω ∈ {0.1, 0.5, 1.0, 2.0, 3.0}. Expect rescaled version to maintain >90% of peak performance across all ω; non-rescaled to degrade sharply for ω > 1.0.
  3. **Q-training method comparison**: On a medium-difficulty task (hopper-medium), compare IQL (default, τ=0.7) vs. CQL vs. ISQL for Q-function training. Expect IQL to achieve highest scores; CQL may underperform due to conservatism on in-distribution actions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the approximation error from the first-order Taylor expansion affect the accuracy of the intermediate energy guidance in regions of the action space where the Q-function has high curvature?
- **Basis in paper:** [inferred] Equation (15) linearizes the Q-function $Q(s, a_0)$ using a Taylor expansion. Appendix I.2 mentions an error bound $R_2(a)$ but assumes it is small if $\bar{a}$ is close to $a$, without empirically testing sensitivity to Q-function non-linearity.
- **Why unresolved:** The method assumes the Q-function is locally linear. If the true Q-landscape is highly non-linear (common in sparse reward tasks), the linear approximation may provide incorrect gradient directions.
- **What evidence would resolve it:** An ablation study analyzing performance degradation when artificially increasing the non-linearity of the Q-function, or a comparison using second-order Taylor expansion terms.

### Open Question 2
- **Question:** Does modeling the posterior distribution $\mu_{0|t}$ as a full-covariance (anisotropic) Gaussian, rather than the isotropic Gaussian assumed in Equation (22), significantly improve performance on tasks with correlated action dimensions?
- **Basis in paper:** [explicit] Section 3.2 states, "For simplicity, we usually consider isotropic Gaussian distribution, where the covariance $\tilde{\Sigma}_{0|t}$ satisfies $\tilde{\Sigma}_{0|t} = \tilde{\sigma}^2_{0|t} * I$."
- **Why unresolved:** Complex robotics tasks (like Adroit) often have highly correlated joints. Forcing isotropy ignores these correlations, potentially leading to suboptimal posterior variance estimation.
- **What evidence would resolve it:** Comparing the current isotropic implementation against a variant that tracks the full covariance matrix $\tilde{\Sigma}_{0|t}$ on high-dimensional D4RL Adroit tasks.

### Open Question 3
- **Question:** Can the theoretical derivation of the intermediate energy (Equation 17) be extended to latent diffusion models where the transition kernel is not explicitly Gaussian in the data space?
- **Basis in paper:** [explicit] The abstract states the closed-form solution is derived "when the diffusion model obeys the conditional Gaussian transformation."
- **Why unresolved:** The derivation relies on the moment-generating function of Gaussian distributions (Eq. 16 to 17). Many efficient diffusion policies operate in latent space, potentially violating this assumption.
- **What evidence would resolve it:** A theoretical extension of the proof to latent spaces or empirical results applying AEPO to a latent diffusion policy baseline.

## Limitations

- The analytic derivation relies heavily on Gaussian assumptions that may not hold for complex action distributions
- The posterior estimation method, while elegant, is not extensively validated across diverse scenarios
- Guidance rescaling is presented as a heuristic fix without theoretical justification for why magnitude normalization is optimal
- The ablation studies focus on hyperparameters and architectural choices rather than fundamental assumptions about approximation quality

## Confidence

- Mechanism 1 (analytic energy approximation): **Medium** - The derivation is mathematically sound but depends on assumptions that may break in practice
- Mechanism 2 (posterior estimation): **High** - Direct implementation from diffusion model equations with empirical validation
- Mechanism 3 (guidance rescaling): **Medium** - Effective heuristic but lacks theoretical grounding
- Mechanism 4 (expectile regression for Q-training): **High** - Well-established approach in offline RL with demonstrated effectiveness

## Next Checks

1. Test the analytic energy approximation on tasks with multimodal action distributions to verify Gaussian assumption validity
2. Compare the proposed posterior estimation method against Monte Carlo estimates of the true posterior on a subset of tasks
3. Evaluate guidance rescaling across a broader range of tasks, including those requiring large policy deviations from the behavior policy