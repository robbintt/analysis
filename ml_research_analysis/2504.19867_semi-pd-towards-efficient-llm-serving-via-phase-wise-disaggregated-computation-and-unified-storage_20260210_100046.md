---
ver: rpa2
title: 'semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation
  and Unified Storage'
arxiv_id: '2504.19867'
source_url: https://arxiv.org/abs/2504.19867
tags:
- prefill
- decode
- semi-pd
- disaggregated
- tpot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semi-PD, an LLM serving system that achieves
  disaggregated computation while maintaining unified storage to overcome the storage
  inefficiencies of existing disaggregated systems. The system employs a computational
  resource controller to dynamically partition streaming multiprocessors (SMs) between
  prefill and decode phases, and a unified memory manager to handle asynchronous memory
  access for both model weights and KV cache.
---

# semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage

## Quick Facts
- arXiv ID: 2504.19867
- Source URL: https://arxiv.org/abs/2504.19867
- Reference count: 40
- Reduces average end-to-end latency per request by 1.27-2.58x on DeepSeek series models and serves 1.55-1.72x more requests while adhering to latency constraints on Llama series models compared to state-of-the-art systems

## Executive Summary
semi-PD introduces a novel approach to LLM serving that achieves disaggregated computation while maintaining unified storage to overcome the storage inefficiencies of existing disaggregated systems. The system employs SM-level partitioning between prefill and decode phases using NVIDIA MPS, combined with a unified memory manager for shared weight and KV cache access. This architecture eliminates latency interference between phases while avoiding the memory overhead of weight replication and KV cache transfers. Evaluation shows significant improvements in both latency (1.27-2.58x reduction) and throughput (1.55-1.72x increase) across multiple model families compared to state-of-the-art systems.

## Method Summary
semi-PD implements phase-wise disaggregated computation at the SM level using NVIDIA MPS to partition streaming multiprocessors between prefill and decode workers on the same GPU. The system features a unified memory manager that coordinates read-only weight access and KV cache allocation using atomic operations, enabling both workers to share the same HBM space via IPC pointers. A resident process mechanism with delayed and asynchronous switching enables low-overhead adjustment of SM ratios between workers. The SLO-aware dynamic partitioning algorithm continuously monitors TTFT and TPOT metrics to optimize resource allocation based on observed performance against latency constraints.

## Key Results
- Achieves 1.27-2.58x reduction in average end-to-end latency per request on DeepSeek series models
- Serves 1.55-1.72x more requests while adhering to latency constraints on Llama series models
- Demonstrates superior SLO attainment compared to disaggregated systems that suffer from storage inefficiencies

## Why This Works (Mechanism)

### Mechanism 1: SM-Level Disaggregation
- **Claim:** Partitioning GPU Streaming Multiprocessors (SMs) between prefill and decode workers on the same GPU may eliminate latency interference while preserving memory efficiency.
- **Mechanism:** Uses NVIDIA Multi-Process Service (MPS) to assign a configurable percentage of SMs (x, y) to prefill and decode processes, enabling asynchronous computation without GPU-level disaggregation.
- **Core assumption:** SM-level isolation is sufficient to prevent prefill-decode interference; shared memory bandwidth and cache do not reintroduce contention.
- **Evidence anchors:**
  - [abstract] "introduces phase-wise disaggregated computation at the SM level to eliminate latency interference"
  - [Section 4.3] Describes MPS-based SM partitioning and the (x, y) configuration interface.
  - [corpus] Related work "RAPID-Serve" and "Injecting Adrenaline into LLM Serving" explore intra-GPU disaggregation but use different partitioning strategies; no direct validation of SM-level isolation efficacy.
- **Break condition:** If interference re-emerges through shared HBM bandwidth, L2 cache, or interconnect, TTFT and TPOT improvements would degrade, especially under high memory-bound workloads.

### Mechanism 2: Unified Storage Architecture
- **Claim:** Unified storage for model weights and KV cache may reduce memory overhead by eliminating weight replication and KV cache transfers.
- **Mechanism:** A unified memory manager coordinates read-only weight access and uses atomic operations for KV cache block allocation, enabling both workers to share the same HBM space via inter-process communication (IPC) pointers.
- **Core assumption:** Write-after-read conflicts in KV cache allocation are rare or short enough that atomic locking does not become a throughput bottleneck.
- **Evidence anchors:**
  - [abstract] "maintaining unified storage to avoid memory imbalance and KV cache transfer overhead"
  - [Section 4.4] Describes paged KV cache management and atomic allocating to avoid WAR conflicts.
  - [corpus] No direct corpus evidence on unified storage efficacy in disaggregated contexts; related work focuses on disaggregated storage patterns.
- **Break condition:** If atomic allocation contention increases sharply under high concurrency, allocation latency could offset storage savings.

### Mechanism 3: Resident Process Switching
- **Claim:** Resident processes and delayed switching may enable low-overhead adjustment of SM ratios between prefill and decode workers.
- **Mechanism:** A resident process continuously holds weights and KV cache; new workers receive IPC pointers instead of reloading data. Delayed and asynchronous switching hides initialization latency.
- **Core assumption:** IPC pointer passing and engine initialization are fast relative to iteration latency; partial overlap of old and new workers via MPS over-subscription is tolerable.
- **Evidence anchors:**
  - [Section 4.3] Details resident process, delayed switching, and asynchronous switch mechanisms.
  - [Figure 6] Visual comparison of switch overhead with and without resident workers.
  - [corpus] Weak; no direct corpus validation of this specific switching technique.
- **Break condition:** If IPC overhead or kernel relaunch costs scale with model size or TP degree, adjustment latency may become non-negligible.

## Foundational Learning

- **Concept:** Prefill vs. decode phases in LLM inference (TTFT vs. TPOT)
  - **Why needed here:** Understanding phase-specific latency metrics is essential to interpret SLO-aware resource partitioning.
  - **Quick check question:** Can you explain why prefill is compute-bound while decode is memory-bound?

- **Concept:** NVIDIA MPS and SM-level partitioning
  - **Why needed here:** Core mechanism for disaggregated computation within a single GPU.
  - **Quick check question:** What happens to a process when its assigned SM ratio is reduced mid-execution?

- **Concept:** Paged KV cache and block allocation (vLLM-style)
  - **Why needed here:** Foundation for unified memory manager and atomic allocation logic.
  - **Quick check question:** Why does write-after-read conflict occur when two workers allocate KV cache blocks asynchronously?

## Architecture Onboarding

- **Component map:**
  - Computation Resource Controller -> Manages SM partition (x, y), handles worker lifecycle and switching
  - Unified Memory Manager -> Manages weight/KV cache sharing, atomic block allocation
  - Prefill/Decode Workers -> Separate processes with own queues, accessing shared storage via IPC
  - SLO-aware Adjusting Algorithm -> Feedback loop that updates (x, y) based on observed TTFT/TPOT

- **Critical path:**
  1. Request arrives → enqueued in prefill worker queue
  2. Prefill worker processes → allocates/writes KV cache via unified manager
  3. Request handed off to decode worker queue (no KV transfer)
  4. Decode worker reads/writes KV cache iteratively
  5. Controller monitors latencies → triggers (x, y) adjustment if SLO at risk

- **Design tradeoffs:**
  - Unified storage vs. disaggregated storage: Saves memory but requires careful concurrency control
  - SM-level disaggregation vs. GPU-level: Finer-grained control but may share memory bandwidth/cache
  - Dynamic adjustment frequency vs. switching overhead: More responsive but more frequent interruptions

- **Failure signatures:**
  - TTFT or TPOT spikes under high request rates → possible SM ratio mismatch or memory bandwidth saturation
  - Increased allocation latency or queue buildup → atomic lock contention in unified memory manager
  - Stale workers after switch → incomplete teardown or IPC pointer invalidation

- **First 3 experiments:**
  1. Baseline latency comparison: Measure P90 TTFT/TPOT for semi-PD vs. vLLM-S and DistServe at fixed request rates to validate interference elimination
  2. SM ratio sweep: Test different (x, y) partitions under varying prefill/decode workload ratios to identify optimal static configurations
  3. Dynamic adjustment stress test: Apply heterogeneous workloads (e.g., ShareGPT + long-context requests) to evaluate SLO attainment with and without the dynamic algorithm

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can semi-PD support heterogeneous parallelism strategies (e.g., different tensor or pipeline parallelism degrees) for prefill and decode phases while maintaining a unified storage architecture?
- **Basis:** [Explicit] Section 4.5 states, "The limitation of such a scheme is the parallel patterns have to be the same for prefill and decode phases," while noting that different GPU ratios impact performance more than parallelism patterns.
- **Why unresolved:** The current unified memory manager and IPC mechanism assume consistent parallelism configurations to manage weight and KV cache pointers efficiently across workers.
- **What evidence would resolve it:** An implementation allowing distinct TP/PP configurations for prefill and decode workers on the same unified memory pool without incurring data reshuffling overhead that negates latency gains.

### Open Question 2
- **Question:** Does combining chunked prefill with phase-wise disaggregated computation yield strictly additive benefits, or do they interfere with the SLO-aware resource adjustment algorithm?
- **Basis:** [Inferred] Section 6 notes that the DistServe-based implementation did not support chunked prefill, while the SGLang one did. Since chunked prefill is typically used to hide interference in unified systems, its interaction with a system that already disaggregates computation is unclear.
- **Why unresolved:** It is unconfirmed if breaking prefill into smaller chunks affects the queuing models (Eq. 2) used by the dynamic partitioning algorithm.
- **What evidence would resolve it:** A comparative analysis of semi-PD's dynamic partitioning stability and SLO attainment with chunked prefill enabled versus disabled on identical workloads.

### Open Question 3
- **Question:** How does the unified storage architecture scale on hardware with lower inter-GPU bandwidth (e.g., PCIe) where the "unified" abstraction incurs higher access latency?
- **Basis:** [Inferred] The evaluation is conducted exclusively on high-end A100/H200 clusters with NVLink (Section 7.1.1), relying on fast interconnects to make unified storage feasible across TP/PP groups.
- **Why unresolved:** The paper explicitly lists "KV cache transfer overhead" as a motivation (Section 3.2), but does not analyze if the unified memory access patterns in semi-PD degrade on systems where memory is not truly physically unified.
- **What evidence would resolve it:** Benchmark results on PCIe-connected GPU clusters comparing semi-PD's latency against disaggregated systems that might tolerate lower bandwidth better via explicit transfer scheduling.

## Limitations
- Interference at SM level may still exist through shared HBM bandwidth and L2 cache, though the evaluation doesn't isolate these effects
- Atomic allocation scalability is unproven at extreme concurrency levels where contention could become a bottleneck
- SLO-aware algorithm hyperparameters may not generalize across diverse workload patterns beyond tested scenarios

## Confidence
- **High confidence**: The architectural design of SM-level disaggregation and unified storage is well-specified and the evaluation methodology is sound
- **Medium confidence**: The mechanism claims (interference elimination, storage efficiency) are supported by the evaluation but could benefit from more granular isolation testing
- **Medium confidence**: The SLO-aware dynamic adjustment algorithm appears effective in the tested scenarios but may have limitations in extreme workload conditions

## Next Checks
1. **Interference isolation test**: Measure HBM bandwidth utilization and L2 cache miss rates for prefill vs decode workers under varying SM ratios to quantify residual interference at the memory hierarchy level
2. **Atomic contention stress test**: Scale request concurrency to 2-4x the evaluation levels while monitoring KV cache allocation latency to identify the atomic operation bottleneck threshold
3. **Model size scalability validation**: Test semi-PD on 1B-7B parameter models where TTFT/TPOT ratios differ significantly from the 8B+ models evaluated, to verify SLO-aware algorithm effectiveness across the full parameter spectrum