---
ver: rpa2
title: '"Don''t Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin
  Translation with RAG'
arxiv_id: '2511.01454'
source_url: https://arxiv.org/abs/2511.01454
tags:
- latin
- draft
- translation
- pipeline
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a reproducible draft-based refinement pipeline
  for Latin-to-English translation, combining a fine-tuned NLLB-1.3B model with zero-shot
  LLM refinement (Llama-3.3 or Qwen3), augmented by retrieval of similar examples.
  The method addresses challenges of translating morphology-rich, low-resource Latin
  while preserving philological fidelity.
---

# "Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG

## Quick Facts
- arXiv ID: 2511.01454
- Source URL: https://arxiv.org/abs/2511.01454
- Authors: Sergio Torres Aguilar
- Reference count: 21
- Primary result: The pipeline achieves statistically comparable performance to GPT-5 benchmark with a peak COMET score of 75.7 while producing more source-faithful translations for morphology-rich Latin

## Executive Summary
This paper introduces a draft-based refinement pipeline for Latin-to-English translation that combines a fine-tuned NLLB-1.3B model with zero-shot LLM refinement (Llama-3.3 or Qwen3) and retrieval-augmented generation (RAG). The approach addresses the challenges of translating morphology-rich, low-resource Latin while preserving philological fidelity through a systematic refinement process. The method demonstrates statistically comparable performance to proprietary models on both in-domain and out-of-domain test sets, with particular strength in maintaining faithfulness to source material. The pipeline is fully reproducible and provides a viable open-source alternative for specialized translation tasks.

## Method Summary
The pipeline employs a multi-stage translation process beginning with a fine-tuned NLLB-1.3B model for initial Latin-to-English translation, followed by zero-shot refinement using Llama-3.3 or Qwen3 models. Retrieval-augmented generation is integrated by retrieving similar examples from a corpus to guide the refinement process, ensuring philological accuracy. The system addresses the unique challenges of Latin's complex morphology and syntax through this iterative refinement approach, which allows for corrections and improvements while maintaining fidelity to the source text. The methodology is designed to be reproducible and adaptable to other morphology-rich, low-resource languages.

## Key Results
- Achieves statistically comparable performance to GPT-5 benchmark on both in-domain and out-of-domain test sets
- Peak COMET score of 75.7 demonstrates strong translation quality
- Produces more source-faithful translations than proprietary models according to qualitative analysis
- Successfully validates guided open-source approaches for specialized translation tasks

## Why This Works (Mechanism)
The pipeline succeeds by addressing the specific challenges of Latin translation through a multi-layered approach. The fine-tuned NLLB-1.3B model provides a strong initial translation foundation tailored to Latin's morphological complexity. The zero-shot LLM refinement leverages the advanced capabilities of large language models to improve fluency and coherence without requiring extensive fine-tuning. The RAG component retrieves contextually relevant examples to guide the refinement process, ensuring that translations maintain philological accuracy and faithfulness to the source material. This combination allows the system to handle Latin's rich morphology and complex syntax while producing natural-sounding English translations.

## Foundational Learning
- **Morphology-rich language translation**: Understanding how languages with complex inflectional systems differ from analytic languages is crucial for designing appropriate translation approaches. Quick check: Compare word order and morphological marking in Latin vs. English sentences.
- **Fine-tuning vs. zero-shot learning**: The paper demonstrates how initial fine-tuning provides domain-specific capabilities while zero-shot refinement leverages general language understanding. Quick check: Evaluate performance difference between fully fine-tuned vs. zero-shot refined outputs.
- **Retrieval-augmented generation**: RAG helps maintain faithfulness by providing contextual examples during the refinement process. Quick check: Measure how retrieved examples influence final translation choices.
- **COMET metric evaluation**: Understanding automatic evaluation metrics is essential for assessing translation quality. Quick check: Compare COMET scores with human judgments on sample translations.
- **Philological fidelity**: The concept of maintaining source text faithfulness goes beyond standard translation quality metrics. Quick check: Analyze specific cases where the system preserves source meaning differently than baseline models.

## Architecture Onboarding

**Component Map:**
Fine-tuned NLLB-1.3B -> Zero-shot LLM Refinement (Llama-3.3/Qwen3) -> RAG Retrieval -> Final Output

**Critical Path:**
The critical path involves the sequential processing through each stage: initial NLLB translation, LLM refinement, RAG-augmented guidance, and final output generation. The system's performance depends on the quality of the initial translation, the effectiveness of the refinement stage, and the relevance of retrieved examples.

**Design Tradeoffs:**
- Fine-tuning NLLB-1.3B provides domain adaptation but requires computational resources and training data
- Zero-shot refinement avoids additional fine-tuning costs but introduces variability across runs
- RAG improves faithfulness but depends on the quality and representativeness of the retrieval corpus
- The pipeline balances open-source accessibility with performance comparable to proprietary models

**Failure Signatures:**
- Poor initial translations from NLLB may propagate through refinement stages
- Irrelevant retrieved examples can mislead the refinement process
- Zero-shot LLM variability may produce inconsistent outputs across different runs
- The system may struggle with highly idiomatic or culturally specific Latin expressions

**First Experiments:**
1. Run the pipeline on a small sample of Latin sentences and compare outputs with ground truth translations
2. Test the impact of different retrieval examples by manually curating example sets and measuring output changes
3. Evaluate the zero-shot refinement variability by running the same input through multiple iterations with different random seeds

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology relies on automatic metrics (COMET-22 with ZR and ChrF++) that may not fully capture philological fidelity
- Test set size of 2,195 sentences may not represent the full complexity of Latin syntax variations
- Zero-shot LLM refinement introduces performance variability without fine-tuning
- Effectiveness of retrieval-augmented component depends on quality and representativeness of retrieval corpus

## Confidence
- **High confidence** in the technical pipeline description and reproducibility of the methodology
- **Medium confidence** in the comparative performance claims against GPT-4/5, given the limited evaluation metrics
- **Medium confidence** in the philological fidelity claims, as these are primarily qualitative observations without systematic measurement
- **Low confidence** in the scalability claims to other morphology-rich low-resource languages without additional validation

## Next Checks
1. Conduct human evaluation studies with classical scholars to systematically assess philological fidelity claims beyond automatic metrics
2. Test the pipeline on additional morphology-rich low-resource languages (e.g., Ancient Greek, Sanskrit) to evaluate generalizability
3. Implement multiple runs with different random seeds to quantify the variability in zero-shot LLM refinement performance and report confidence intervals for key metrics