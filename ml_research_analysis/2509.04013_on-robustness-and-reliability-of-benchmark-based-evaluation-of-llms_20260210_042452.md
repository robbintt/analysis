---
ver: rpa2
title: On Robustness and Reliability of Benchmark-Based Evaluation of LLMs
arxiv_id: '2509.04013'
source_url: https://arxiv.org/abs/2509.04013
tags:
- across
- question
- questions
- benchmarks
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the robustness of large language models
  (LLMs) to paraphrased benchmark questions and evaluates the reliability of benchmark-based
  assessments. The authors systematically generate paraphrases for six common benchmarks
  (ARC-C, HellaSwag, MMLU, OpenBookQA, RACE, SciQ) and evaluate 34 state-of-the-art
  LLMs across these variants.
---

# On Robustness and Reliability of Benchmark-Based Evaluation of LLMs

## Quick Facts
- arXiv ID: 2509.04013
- Source URL: https://arxiv.org/abs/2509.04013
- Reference count: 40
- Primary result: Benchmark-based LLM evaluation shows significant performance degradation on paraphrased questions, questioning current reliability

## Executive Summary
This study systematically investigates how large language models perform when benchmark questions are paraphrased, revealing significant vulnerabilities in current evaluation methodologies. The authors generate paraphrased versions of six popular benchmarks and evaluate 34 state-of-the-art models, finding that while model rankings remain relatively stable, absolute accuracy scores decline substantially. The research demonstrates that consistency across paraphrases is not always indicative of correctness, particularly for smaller models that may consistently give wrong answers. Additionally, evidence suggests potential data contamination in older benchmarks, with models showing lower agreement between original and paraphrased performances. These findings challenge the reliability of current benchmark evaluations and highlight the need for robustness-aware evaluation methodologies.

## Method Summary
The authors systematically generated paraphrased variants of six widely-used benchmarks (ARC-C, HellaSwag, MMLU, OpenBookQA, RACE, and SciQ) by employing a two-step process: first using GPT-4o to create paraphrases, then manually filtering to remove quality issues, semantic alterations, and duplicated answers. They evaluated 34 state-of-the-art LLMs across these original and paraphrased versions, measuring both absolute accuracy changes and ranking stability. The study introduced a classification system (Over, Under, Balance) based on performance differences between original and paraphrased evaluations, and developed a robustness score metric that compares model consistency with ground truth alignment. To investigate data contamination, they compared performance patterns between newer and older benchmarks, finding that older benchmarks showed more pronounced performance degradation on paraphrased variants.

## Key Results
- Absolute accuracy scores decline significantly when models are evaluated on paraphrased benchmark questions, with most models falling into the "Over" category
- Model rankings remain relatively stable across paraphrased inputs, but this consistency does not guarantee correctness, especially for smaller models
- Evidence suggests potential data contamination in older benchmarks, as these show lower agreement between original and paraphrased performances compared to newer benchmarks

## Why This Works (Mechanism)
The study's methodology works because it directly addresses the gap between controlled benchmark environments and real-world linguistic variability. By systematically generating paraphrases while maintaining semantic equivalence, the authors create a controlled experiment that isolates the effect of linguistic variation on model performance. The combination of automated generation and manual quality control ensures that performance differences stem from genuine robustness issues rather than artifacts in the paraphrased data. The use of multiple models and benchmarks provides statistical power to detect patterns in how different architectures and training regimes handle linguistic variation.

## Foundational Learning
- **Benchmark Evaluation Metrics**: Understanding accuracy, precision, recall, and ranking stability is crucial for interpreting model performance across different test conditions. Quick check: Verify that benchmark metrics align with intended use cases and real-world performance requirements.
- **Data Contamination Detection**: Recognizing signs of training data overlap with test benchmarks, including unusual performance consistency and sensitivity to input variations. Quick check: Compare performance patterns across benchmarks of different vintages and generation methods.
- **Paraphrase Quality Assessment**: Evaluating semantic equivalence while maintaining linguistic diversity requires both automated and human validation processes. Quick check: Ensure paraphrased questions preserve original meaning while introducing meaningful variation.
- **Model Ranking Stability**: Understanding when rank order preservation indicates genuine capability versus superficial consistency in incorrect responses. Quick check: Analyze correlation between consistency scores and ground truth alignment across model sizes.
- **Robustness Scoring**: Developing metrics that balance consistency with correctness to provide nuanced evaluation of model reliability. Quick check: Validate robustness scores against known model capabilities and failure modes.
- **Cross-Benchmark Generalization**: Assessing whether findings from specific benchmark families extend to broader evaluation practices and model behaviors. Quick check: Test methodology across diverse benchmark types and linguistic domains.

## Architecture Onboarding
**Component Map**: Benchmark Questions -> Paraphrase Generation -> Manual Quality Control -> Model Evaluation -> Performance Analysis -> Robustness Scoring
**Critical Path**: Original Benchmark → Paraphrased Variants → Model Inference → Accuracy Calculation → Consistency Analysis → Contamination Assessment
**Design Tradeoffs**: Automated paraphrase generation offers scalability but requires manual quality control; comprehensive model evaluation provides robust findings but limits sample size; focus on English benchmarks ensures quality but reduces generalizability.
**Failure Signatures**: Performance drops on paraphrased questions without rank changes suggest robustness issues; consistent wrong answers across variants indicate memorization; older benchmark sensitivity may signal contamination.
**First Experiments**: 1) Generate paraphrases for a single benchmark and manually validate quality; 2) Evaluate one model on both original and paraphrased versions to establish baseline patterns; 3) Calculate initial robustness scores to test metric validity.

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on English benchmarks and models, limiting generalizability to multilingual contexts
- Manual quality control process for paraphrases introduces potential subjectivity and scalability constraints
- Classification system based on absolute performance differences may oversimplify nuanced model behaviors
- Evidence for data contamination relies on performance patterns rather than direct evidence of training exposure

## Confidence
- **High Confidence**: Absolute accuracy decline on paraphrased questions is well-supported; model ranking stability despite performance drops is robustly demonstrated
- **Medium Confidence**: Consistency not guaranteeing correctness requires more systematic error analysis; data contamination evidence is suggestive but not conclusive
- **Medium Confidence**: Claims about benchmark evaluation unreliability need careful interpretation regarding sensitivity vs. fundamental flaws

## Next Checks
1. **Cross-Lingual Validation**: Test models on paraphrased versions of multilingual benchmarks to assess robustness pattern generalizability
2. **Controlled Contamination Analysis**: Experiment with models trained with and without known benchmark exposure to establish causal contamination relationships
3. **Error Analysis Framework**: Implement systematic categorization of consistent vs. inconsistent responses to determine correlation between consistency and correctness