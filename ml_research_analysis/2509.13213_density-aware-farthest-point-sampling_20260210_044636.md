---
ver: rpa2
title: Density-Aware Farthest Point Sampling
arxiv_id: '2509.13213'
source_url: https://arxiv.org/abs/2509.13213
tags:
- uni00000013
- data
- uni00000011
- da-fps
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting training data for
  machine learning regression models when labeled data is limited due to computational
  constraints or high labeling costs. The authors focus on passive and model-agnostic
  sampling methods that rely only on data feature representations.
---

# Density-Aware Farthest Point Sampling

## Quick Facts
- arXiv ID: 2509.13213
- Source URL: https://arxiv.org/abs/2509.13213
- Reference count: 40
- Key outcome: DA-FPS significantly reduces mean absolute prediction error compared to random sampling and FPS for regression tasks with limited labeled data.

## Executive Summary
This paper introduces Density-Aware Farthest Point Sampling (DA-FPS), a novel sampling strategy for selecting training data in machine learning regression models when labeled data is scarce. The method addresses the challenge of passive, model-agnostic sampling by minimizing a weighted fill distance of the training set. DA-FPS iteratively selects points at maximal weighted distance from already selected points, with weights accounting for data density. The approach provides theoretical guarantees through an upper bound on expected prediction error that depends linearly on the weighted fill distance, and demonstrates significant performance improvements over baseline methods across multiple datasets and regression models.

## Method Summary
DA-FPS is designed to minimize a weighted fill distance of the training set through iterative point selection. The algorithm begins by computing density estimates for all data points, then selects the first point randomly. In each subsequent iteration, it calculates weighted distances from each unselected point to its nearest selected neighbor, where the weight incorporates the density estimate. The point with maximum weighted distance is selected next. This process continues until the desired number of points is reached. The method proves to provide approximate minimizers for the weighted fill distance estimation, and establishes an upper bound for expected prediction error of Lipschitz continuous regression models that depends linearly on this weighted fill distance.

## Key Results
- DA-FPS significantly reduces mean absolute prediction error compared to random sampling, Farthest Point Sampling, and other baseline approaches across three datasets and two regression models
- Performance advantages are most pronounced for larger datasets and training set sizes exceeding 5% of available data
- The method maintains computational efficiency suitable for large-scale applications while improving prediction accuracy

## Why This Works (Mechanism)
DA-FPS works by strategically selecting training points that maximize coverage of the data space while accounting for local density variations. By weighting distances inversely to local density, the algorithm ensures that sparse regions receive adequate representation in the training set, preventing the model from missing important low-density areas. The farthest point sampling component guarantees good spatial coverage, while the density awareness component prevents oversampling in dense regions. This dual approach optimizes the trade-off between exploration (covering new areas) and exploitation (sampling from representative regions), resulting in training sets that better represent the underlying data distribution and lead to improved generalization performance.

## Foundational Learning
- **Weighted fill distance**: A metric that measures how well a set of points covers a space, weighted by data density. Needed to quantify the optimality of training set selection. Quick check: Verify that weighted fill distance decreases as more representative points are added.
- **Lipschitz continuity**: A property of functions where the rate of change is bounded. Needed to establish theoretical error bounds for regression models. Quick check: Confirm that the regression model satisfies Lipschitz conditions on the relevant domain.
- **Density estimation**: Methods for approximating the underlying probability density function of data. Needed to compute weights for the weighted fill distance. Quick check: Validate density estimates using cross-validation or known ground truth when available.
- **Passive sampling**: Selection of training data without active interaction with the model during training. Needed to ensure the method works with any regression model. Quick check: Confirm that sampling decisions depend only on data features, not model predictions.
- **Farthest point sampling**: An algorithm that iteratively selects points maximally distant from already selected points. Needed as the base selection mechanism for spatial coverage. Quick check: Verify that FPS provides good coverage in low-dimensional spaces.
- **Upper bound analysis**: Mathematical techniques for establishing maximum possible error given certain conditions. Needed to provide theoretical guarantees for prediction accuracy. Quick check: Test whether the bound is reasonably tight on synthetic data where ground truth is known.

## Architecture Onboarding

**Component Map**: Data points -> Density estimation -> Weighted distance computation -> Point selection -> Training set

**Critical Path**: The critical path involves computing density estimates, calculating weighted distances for all unselected points, selecting the point with maximum weighted distance, and repeating until the training set is complete. Each iteration depends on the previous selections, making the sequential nature of the algorithm essential to its functionality.

**Design Tradeoffs**: The method trades computational complexity for improved prediction accuracy. While density estimation and weighted distance calculations add overhead compared to simple FPS, the resulting performance gains justify this cost for most applications. The passive, model-agnostic nature limits the method's ability to leverage model-specific information that could potentially improve sampling further.

**Failure Signatures**: DA-FPS may underperform when density estimates are inaccurate, particularly in high-dimensional spaces where density estimation becomes challenging. The method may also struggle with highly irregular data distributions or when the assumption of Lipschitz continuity is severely violated. Poor performance on very small datasets (<5% of total data) suggests limitations in capturing sufficient diversity with minimal samples.

**First Experiments**:
1. Compare DA-FPS against random sampling and standard FPS on a simple 2D synthetic dataset with known density variations
2. Test the impact of different density estimation methods (k-NN, kernel density estimation) on DA-FPS performance
3. Evaluate DA-FPS on a real-world regression task with limited labeled data, comparing prediction accuracy across different training set sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical error bounds, while mathematically sound, may not be tight in practice across all regression tasks and data distributions
- Experimental validation is limited to three datasets and two regression models, requiring broader testing for generalization claims
- The performance advantage is most pronounced for larger datasets and training sizes above 5%, suggesting potential limitations for small-data scenarios
- Computational efficiency claims lack detailed runtime comparisons and scalability analysis across different dataset sizes

## Confidence

| Claim | Confidence |
|-------|------------|
| Core theoretical framework and DA-FPS algorithm design | High |
| Experimental methodology and results across tested scenarios | Medium |
| Computational efficiency and scalability for large-scale applications | Medium |

## Next Checks

1. Test DA-FPS across diverse regression tasks including deep neural networks and non-Euclidean data structures to assess robustness beyond the current experimental scope.

2. Conduct ablation studies to isolate the contribution of density awareness versus the farthest point sampling component, determining the relative importance of each aspect.

3. Perform runtime and scalability benchmarking comparing DA-FPS to alternatives on datasets ranging from thousands to millions of points to verify computational efficiency claims.