---
ver: rpa2
title: Reasoning Models Reason Well, Until They Don't
arxiv_id: '2510.22371'
source_url: https://arxiv.org/abs/2510.22371
tags:
- reasoning
- graph
- lookahead
- number
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large reasoning models (LRMs) show excellent performance on standard\
  \ graph and proof planning benchmarks, but their accuracy drops sharply as problem\
  \ complexity increases. Using a new synthetic dataset, DeepRD, which generates scalable\
  \ reasoning problems with controlled lookahead and branching, the authors demonstrate\
  \ that all evaluated models\u2014including DeepSeek-R1, o3-mini, GPT-4o, and DeepSeek-V3\u2014\
  experience abrupt performance collapses at high lookahead values, regardless of\
  \ whether the task is symbolic graph connectivity or natural language proof planning."
---

# Reasoning Models Reason Well, Until They Don't

## Quick Facts
- arXiv ID: 2510.22371
- Source URL: https://arxiv.org/abs/2510.22371
- Reference count: 40
- Large reasoning models (LRMs) show excellent performance on standard graph and proof planning benchmarks, but their accuracy drops sharply as problem complexity increases

## Executive Summary
Large reasoning models (LRMs) excel on standard benchmarks but experience abrupt performance collapses when reasoning complexity exceeds certain thresholds. Using a new synthetic dataset called DeepRD, which generates scalable reasoning problems with controlled lookahead and branching, the authors demonstrate that all evaluated models—including DeepSeek-R1, o3-mini, GPT-4o, and DeepSeek-V3—experience accuracy drops at high lookahead values regardless of task type. Even simple chain graphs exhibit accuracy drops at large depths, indicating reasoning limitations rather than token limits. Analysis of real-world datasets reveals that most examples fall within models' success regime, but long tails contain highly complex instances that exceed their capabilities.

## Method Summary
The paper introduces DeepRD, a synthetic dataset generator that creates directed acyclic graphs with controllable lookahead (L) and branching (B) parameters. Lookahead measures the number of BFS iterations needed to determine the correct next step toward a goal, while branching controls the number of possible paths. The authors evaluate models on two tasks: symbolic graph connectivity (finding paths between nodes) and natural language proof planning (predicting next proof steps). They analyze failure modes through manual inspection and compare performance against real-world datasets like NLGraph-hard and NaturalProofs. The key innovation is using lookahead as a complexity metric that captures the minimal computational work required for correct reasoning.

## Key Results
- All evaluated LRMs experience abrupt performance collapses at lookahead values of approximately 100-200, regardless of branching factor
- Performance drops from near-perfect accuracy to near-zero, even on chain graphs (B=1) at large depths, ruling out token limits as the primary cause
- Real-world datasets show heavy-tailed lookahead distributions, with most examples falling in models' success regime but long-tail instances exceeding capabilities
- Three failure modes dominate: omitted outgoing edges, missing branches entirely, and hallucinating non-existent edges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled complexity scaling via lookahead (L) and branching (B) metrics exposes a sharp performance cliff in reasoning models at predictable thresholds.
- Mechanism: Lookahead L measures BFS iterations required to determine correct next steps. As L increases, models must maintain coherent search state across more iterations; as B increases, they must evaluate more competing paths. This creates a bounded reasoning horizon beyond which models cannot reliably track state.
- Core assumption: Lookahead metric correctly captures minimal computational work for correct next-step prediction (per Saparov et al. 2025).
- Evidence anchors:
  - [abstract] "all evaluated models...experience abrupt performance collapses at high lookahead values"
  - [Section 4.2, Figure 2] Shows accuracy drops from ~100% to ~0% at L≈100-200 for B≥2, with earlier drops for higher B
  - [corpus] Related work "The Illusion of Thinking" (Shojaee et al., 2025) shows similar complexity-dependent failures on puzzle tasks, though confounded by token limits
- Break condition: If models computed BFS algorithmically, performance would degrade gradually rather than collapse abruptly; the cliff pattern suggests a phase transition in reasoning coherence rather than incremental computational overload.

### Mechanism 2
- Claim: Performance collapse is driven by "propagation errors" where single early local missteps cause all subsequent reasoning to be correct conditioned on incorrect premises.
- Mechanism: Models perform local reasoning at each step, but errors compound. Three observed failure modes: (I) omitting necessary outgoing edge at intermediate step, (II) missing branch at start node entirely, (III) hallucinating an edge. Once gold path is pruned or wrong branch committed to, subsequent steps are locally consistent but globally wrong.
- Core assumption: Authors' manual inspection of 40 R1 failure cases generalizes to broader model behavior (small sample).
- Evidence anchors:
  - [Section 4.4] "15/20 samples" showed Type I errors (edge omission); "13/20" incorrect-path cases showed Type III (hallucination)
  - [Section 4.4] Cites Dziri et al. (2023) "Faith and Fate" on propagation errors in compositional reasoning
  - [corpus] "On the Self-awareness of Large Reasoning Models' Capability Boundaries" notes LRMs "engage in unproductive reasoning until context limit" on hard questions—consistent with propagation into dead ends
- Break condition: If models recovered from early errors via backtracking or verification, propagation would be interrupted. Current LRMs appear to lack reliable self-correction mechanisms that operate mid-reasoning.

### Mechanism 3
- Claim: Models fail to generalize beyond complexity distribution present in training data; current benchmarks primarily sample from "head" of real-world complexity, masking this limitation.
- Mechanism: LRMs trained (via RLVR) on problems with limited lookahead/depth. They learn heuristics effective within this distribution but do not acquire general-purpose search algorithms. When test complexity exceeds training complexity, these heuristics break down. Real-world datasets have heavy-tailed complexity distributions, so most benchmark examples fall in success regime while long-tail instances fail.
- Core assumption: Training distributions for evaluated models contain limited-complexity reasoning examples; authors cannot directly verify this for closed models (o3, GPT-4o).
- Evidence anchors:
  - [Section 5, Figure 6] Shows L=100 is in 99th percentile for ogbl-wikikg2 and OGB; LRM collapse at L≈100-200 means ~1% of real-world graph queries exceed capability
  - [Section 4.1] NLGraph-hard expected lookahead ≤1.8 due to Erdős-Rényi graph properties—benchmark "hardness" comes from node count, not reasoning depth
  - [corpus] Corpus lacks direct evidence on training distributions; related work "RegexPSPACE" similarly tests LLMs on PSPACE-complete problems exceeding typical training complexity
- Break condition: If fine-tuning on higher-complexity synthetic examples transferred to natural language reasoning, distribution mismatch would be addressable via data augmentation. Paper notes this as future work but does not test it.

## Foundational Learning

- **Concept: Breadth-First Search (BFS) and Lookahead Complexity**
  - Why needed here: Paper's core complexity metric derived from BFS; understanding how lookahead differs from simple path length is essential for interpreting results. Chain graphs have L=1 regardless of depth because only one child exists; branching graphs require L iterations to determine which child leads toward goal.
  - Quick check question: For a graph with start node S having 3 children, where only one path leads to goal G after 5 steps, what is the lookahead L?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: LRMs (o3, DeepSeek-R1) distinguished from base LLMs by RLVR training, which provides incentives for step-by-step reasoning and self-verification. Understanding this helps explain why LRMs outperform LLMs on low-complexity tasks but still collapse at scale.
  - Quick check question: Why might RLVR-trained models perform better on reasoning tasks yet still fail to generalize to higher complexity?

- **Concept: Directed Acyclic Graphs (DAGs) and Proof Planning**
  - Why needed here: Paper maps graph connectivity to natural language proof planning—each node is proposition, each edge an implication. Proof planning requires same lookahead search as graph traversal but in natural language form.
  - Quick check question: How does branching factor B in a DAG correspond to number of possible inference rules applicable at a proof step?

## Architecture Onboarding

- **Component map:**
  - generate_lookahead_graphs() (Alg. 6) -> lookahead() (Alg. 2) -> graph_to_logic() (Alg. 5) -> pairwise_distance() (Alg. 4)

- **Critical path:**
  1. Generate graphs with target (L, B) using Alg. 6
  2. Convert to symbolic prompts (edge lists) or NL proof planning (Alg. 5)
  3. Query model for full path (symbolic) or next step (proof planning)
  4. Extract answer from model output; validate path/step correctness
  5. Plot accuracy vs. (L, B) to identify collapse threshold

- **Design tradeoffs:**
  - Chain graphs (B=1) isolate depth effects but cannot test search; use to separate token/depth limits from reasoning limits
  - Proof planning uses only next-step prediction (not full proof generation) due to evaluation difficulty; trades off task fidelity for tractability
  - Edge hallucination rate vs. lookahead shows models give up (incomplete paths) rather than hallucinate at high L

- **Failure signatures:**
  - "No path" responses when gold path exists (Type I/II errors: missed edges)
  - Paths containing non-existent edges (Type III errors: hallucination)
  - Responses hovering at 1/B accuracy (random guessing among B branches)
  - Token usage decreasing with increasing L (early termination rather than truncation)

- **First 3 experiments:**
  1. **Baseline calibration on NLGraph-hard:** Run model on existing benchmark, compute actual lookahead distribution using Alg. 3; verify most examples have L<5
  2. **Controlled complexity sweep:** Generate DeepRD examples with L∈{10, 25, 50, 100, 200, 400} and B∈{2, 4, 8, 16}; plot accuracy curves to identify collapse point for your model
  3. **Chain graph depth test:** Generate B=1 graphs with depth up to 1500; if accuracy drops at high depth despite no search requirement, confirms reasoning limitation vs. token limit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there specific thinking patterns in LRM reasoning traces that predict success or failure on complex reasoning tasks?
- Basis in paper: [explicit] The conclusion states: "First is a deeper analysis R1's 'thinking' tokens for traversal and reasoning: Are there any thinking patterns that are predictive of the model's performance?"
- Why unresolved: Current analysis only categorizes error types post-hoc; no predictive analysis was conducted. Access to reasoning tokens is limited for OpenAI models.
- What evidence would resolve it: Systematic analysis of reasoning traces across complexity levels, identifying patterns that correlate with correct vs. incorrect outputs before the error occurs.

### Open Question 2
- Question: Can fine-tuning models on higher-complexity symbolic reasoning tasks transfer improved performance to natural language reasoning?
- Basis in paper: [explicit] The conclusion explicitly proposes: "test whether fine-tuning models on higher complexities in symbolic cases transfer performance to natural language reasoning."
- Why unresolved: DeepRD enables unlimited synthetic data generation, but no fine-tuning experiments were conducted in this work.
- What evidence would resolve it: Fine-tune models on DeepRD examples at lookaheads beyond current collapse thresholds, then evaluate on natural language proof planning.

### Open Question 3
- Question: What architectural or algorithmic modifications could prevent propagation errors where early reasoning missteps cascade into complete failures?
- Basis in paper: [inferred] The manual inspection of failure modes (Section 4.4) reveals "propagation errors" where "after a single early local misstep, later computations are correct, conditioned on incorrect parent values," mirroring findings from Dziri et al. (2023).
- Why unresolved: The paper identifies the failure pattern but does not investigate intervention strategies such as intermediate verification steps or backtracking mechanisms.
- What evidence would resolve it: Experiments with models augmented to perform self-verification at intermediate reasoning steps, measuring whether error propagation is reduced.

## Limitations
- Training data complexity distribution is unknown for closed models (o3, GPT-4o), limiting claims about distribution mismatch as primary failure cause
- Small manual error analysis sample (40 R1 failures) may not generalize to other models or error types
- Real-world dataset lookahead computation relies on external shortest-path algorithms that may not capture human reasoning complexity

## Confidence
- High confidence: Performance collapse pattern at controlled complexity thresholds is reproducible and well-characterized
- Medium confidence: Propagation error mechanism is supported by manual analysis but may not be only failure mode
- Medium confidence: Distribution mismatch hypothesis is plausible given evidence but lacks direct training data verification

## Next Checks
1. Test fine-tuning DeepSeek-V3 on DeepRD examples with L>200 to verify if higher-complexity training data transfers to improved reasoning generalization
2. Implement and evaluate model self-correction mechanisms (backtracking, verification) during reasoning to determine if propagation errors can be interrupted
3. Compute lookahead distributions for additional real-world reasoning datasets (math word problems, code synthesis) to verify heavy-tailed complexity pattern across domains