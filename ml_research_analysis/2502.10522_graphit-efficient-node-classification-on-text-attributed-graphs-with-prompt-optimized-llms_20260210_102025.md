---
ver: rpa2
title: 'GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt
  Optimized LLMs'
arxiv_id: '2502.10522'
source_url: https://arxiv.org/abs/2502.10522
tags:
- node
- graph
- neighbors
- text
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphiT introduces an efficient framework for node classification
  on text-attributed graphs by encoding graph structures into concise textual representations
  and optimizing LLM prompts programmatically. The approach uses neighbor keyphrases
  instead of lengthy summaries to capture neighborhood information, reducing token
  usage and avoiding the "lost-in-the-middle" effect.
---

# GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs

## Quick Facts
- arXiv ID: 2502.10522
- Source URL: https://arxiv.org/abs/2502.10522
- Authors: Shima Khoshraftar; Niaz Abedini; Amir Hajian
- Reference count: 34
- Primary result: Achieves 79.84%, 93.28%, and 57.25% accuracy on Cora, PubMed, and Ogbn-arxiv datasets respectively

## Executive Summary
GraphiT introduces an efficient framework for node classification on text-attributed graphs by encoding graph structures into concise textual representations and optimizing LLM prompts programmatically. The approach uses neighbor keyphrases instead of lengthy summaries to capture neighborhood information, reducing token usage and avoiding the "lost-in-the-middle" effect. GraphiT employs the DSPy framework to automatically optimize both prompt instructions and demonstrative examples without manual tuning or labeled training data.

## Method Summary
GraphiT processes text-attributed graphs by first extracting node text attributes and 1-hop neighbor information. For each node, it concatenates the target node's text with neighbor labels and semantic keyphrases extracted from neighbor texts using KeyBERT with diversity filtering. The method then uses DSPy's COPRO and BootstrapFewShot compilers to optimize the prompt structure and demonstration examples for few-shot learning with gpt-3.5-turbo-1106. The optimized prompt is applied to classify nodes based on their textual representation and structural context, achieving competitive accuracy while maintaining efficiency through compressed neighbor encoding.

## Key Results
- Outperforms vanilla LLM and LLM-based baselines on Cora (79.84%), PubMed (93.28%), and Ogbn-arxiv (57.25%) datasets
- Achieves significant token savings by using neighbor keyphrases instead of full summaries (ratio > 1.0 in favor of keyphrases)
- Bridges performance gap between LLM-only approaches and GNNs while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
Encoding neighborhoods via extracted keyphrases rather than full summaries maintains classification performance while significantly reducing token usage. The method replaces verbose neighbor summaries with a set of semantic keyphrases derived from 1-hop neighbors, mitigating the "lost-in-the-middle" effect and lowering API costs.

### Mechanism 2
Programmatic prompt optimization (DSPy) elicits superior reasoning from the LLM compared to manual prompt engineering. The system iteratively refines prompt instructions via COPRO and selects high-quality demonstrations via Bootstrap Few-Shot, automating the search for optimal prompt structure.

### Mechanism 3
Incorporating 1-hop neighbor signals (labels and keyphrases) improves node classification by exploiting graph homophily. The model explicitly provides the LLM with structural context by concatenating target node text with neighbor labels and keyphrases, allowing it to reason about node communities.

## Foundational Learning

- **Text-Attributed Graphs (TAGs)**: Graphs where nodes possess semantic text attributes (e.g., paper abstracts). Understanding this is crucial as GraphiT operates specifically on graphs with text content.
  - Quick check: Can you identify a dataset where nodes possess both citation links (structure) and paper abstracts (text)?

- **In-Context Learning & Prompt Engineering**: The LLM's ability to learn from examples provided in the prompt without weight updates. This is essential to understanding why DSPy optimization works for GraphiT.
  - Quick check: How does providing examples of input-output pairs in a prompt differ from fine-tuning model weights?

- **KeyBERT / Keyphrase Extraction (KPE)**: The method of extracting semantic keyphrases from text to compress neighbor information. Understanding KPE is key to implementing the node feature preparation step.
  - Quick check: How does selecting top-ranked n-grams based on cosine similarity to a source document reduce context length?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> KPE Module (KeyBERT) -> Formatter -> DSPy Layer -> LLM Engine
- **Critical path**: The KPE Module is the critical efficiency componentâ€”if keyphrase extraction fails to be semantic, token savings are worthless. The DSPy Compilation is the critical performance component, distinguishing GraphiT from standard RAG or zero-shot approaches.
- **Design tradeoffs**: 1-hop vs. 2-hop neighbors (1-hop chosen for efficiency but causes performance gap vs. GCNs), Keyphrases vs. Summaries (keyphrases win on cost/speed), LLM-only vs. GNN (LLM avoids GNN training complexity but has higher inference costs)
- **Failure signatures**: High Token Usage (KPE diversity module disabled), Low Accuracy (DSPy optimizer failed), Hallucinated Labels (prompt formatting broken)
- **First 3 experiments**: 1) Vanilla Baseline: Run classification on 50 nodes using only target node text, 2) Ablation on Context: Add "Neighbors Labels" to input and measure lift, 3) Efficiency Test: Compare token count and accuracy of "Neighbors Summary" vs. "Neighbors Keyphrases"

## Open Questions the Paper Calls Out
- Can incorporating neighbors beyond 1-hop improve GraphiT's performance to match or exceed GNNs on datasets like Cora and Ogbn-arxiv?
- Can GraphiT be effectively extended to other graph prediction tasks such as link prediction?
- How does GraphiT perform on heterophilic graphs where the homophily assumption does not hold?
- Would integrating LLMs with GNNs in the GraphiT framework bridge the performance gap with pure GNN methods?

## Limitations
- Assumes access to neighbor labels during inference, creating a semi-supervised setting
- Small validation set size (2-3 nodes per class) may lead to unstable DSPy optimization
- Lacks explicit API cost and latency analysis to validate practical efficiency gains

## Confidence
- High confidence: Core efficiency mechanism (keyphrase extraction vs. full summaries) is well-supported by quantitative token count comparisons
- Medium confidence: DSPy optimization's contribution is demonstrated through ablation, but small validation set raises generalizability questions
- Medium confidence: Homophily assumption is explicitly stated and partially validated, though performance on heterophilic graphs remains untested

## Next Checks
1. Run GraphiT with and without neighbor labels using predicted (instead of ground truth) neighbor classifications to verify viability in fully unsupervised settings
2. Repeat DSPy optimization with varying validation set sizes (1, 3, 5 nodes per class) to quantify impact on accuracy and prompt stability
3. Measure actual API token usage and associated costs for GraphiT versus summary-based baselines across 1000-node inference batches to validate claimed efficiency gains in practice