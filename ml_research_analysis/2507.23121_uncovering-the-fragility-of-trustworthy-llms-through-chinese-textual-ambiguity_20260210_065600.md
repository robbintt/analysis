---
ver: rpa2
title: Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity
arxiv_id: '2507.23121'
source_url: https://arxiv.org/abs/2507.23121
tags:
- ambiguity
- llms
- language
- ambiguous
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models handle Chinese
  textual ambiguity, revealing significant limitations in their ability to detect
  and interpret ambiguous sentences. The researchers created a benchmark dataset of
  900 annotated ambiguous sentences with multiple interpretations across nine categories.
---

# Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity

## Quick Facts
- arXiv ID: 2507.23121
- Source URL: https://arxiv.org/abs/2507.23121
- Reference count: 40
- Primary result: LLMs struggle with Chinese textual ambiguity detection and interpretation, with retrieval-augmented generation showing the strongest performance improvements

## Executive Summary
This study investigates how large language models handle Chinese textual ambiguity, revealing significant limitations in their ability to detect and interpret ambiguous sentences. The researchers created a benchmark dataset of 900 annotated ambiguous sentences with multiple interpretations across nine categories. Through experiments with various open-weight models, they found that LLMs struggle to distinguish ambiguous from unambiguous text, often exhibit overconfidence in their interpretations, and show overthinking when explicitly prompted about ambiguity. The best-performing approach was retrieval-augmented generation (RAG), which improved detection and understanding tasks, particularly for medium-scale models. Larger models and reasoning-enhanced models generally performed better, but even state-of-the-art models like DeepSeek-R1 showed fragility when confronted with ambiguity.

## Method Summary
The researchers constructed a Chinese textual ambiguity dataset with 900 sentences annotated with multiple interpretations across three main categories (lexical, syntactic, semantic-pragmatic) and nine subcategories. They evaluated detection performance using a fine-tuned Chinese RoBERTa classifier and six prompting strategies across various model scales. For understanding tasks, they employed a combination of direct interpretation, knowledge-enhanced, chain-of-thought, prompted disambiguation, few-shot, and retrieval-augmented generation approaches. The dataset was split 70/15/15 for training, development, and testing with stratified sampling. Evaluation metrics included accuracy, precision, recall, F1 scores for detection, and exact match, recall, and set F1 for understanding tasks.

## Key Results
- BERT classifier achieved 94.7% accuracy on detection vs LLMs' 43-65% range
- RAG+FS improved detection F1 to 87.01 for DeepSeek-R1 and understanding Set F1 to 69.57 for Qwen3-32B
- Medium-scale models (14B-32B) showed optimal performance with RAG augmentation
- Overthinking occurred when explicit ambiguity prompting generated speculative interpretations beyond human-annotated bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) combined with few-shot prompting improves LLM performance on ambiguity detection and understanding tasks, with strongest effects for medium-scale models.
- Mechanism: Retrieved semantically similar examples provide concrete patterns that help models recognize multiple valid interpretations rather than defaulting to single-meaning commitment. Examples compensate for insufficient internal reasoning capacity in smaller models.
- Core assumption: Models can transfer ambiguity patterns from retrieved examples to new inputs without being overwhelmed by the additional context.
- Evidence anchors:
  - [abstract]: "The best-performing approach was retrieval-augmented generation (RAG), which improved detection and understanding tasks, particularly for medium-scale models."
  - [Section 3.2.2, Table 3]: RAG+FS achieved 87.01 Macro-F1 on detection for DeepSeek-R1 (vs 62.62 baseline); medium models like Qwen3-32B jumped from 54.79 to 69.57.
  - [corpus]: Limited direct validation. AmbiSQL (arxiv:2508.15276) explores interactive ambiguity for Text-to-SQL but uses clarification dialogs rather than RAG. DEBATE (arxiv:2506.07502) addresses Chinese ambiguity through speech, not retrieval.
- Break condition: RAG shows diminishing returns for already-strong reasoning models (DeepSeek-R1 showed minimal improvement on understanding tasks) and may hurt performance if retrieval quality is low or introduces noise.

### Mechanism 2
- Claim: Models optimized for reasoning tasks demonstrate improved ambiguity handling through enhanced multi-step analysis capabilities.
- Mechanism: Reasoning-enhanced training develops decomposition skills that enable models to systematically consider alternative interpretations rather than pattern-matching to single-meaning outputs.
- Core assumption: Ambiguity resolution requires explicit chain-of-thought-style reasoning that can be incentivized through training.
- Evidence anchors:
  - [abstract]: "Larger models and reasoning-enhanced models generally performed better, but even state-of-the-art models like DeepSeek-R1 showed fragility."
  - [Section 3, Table 4]: Reasoning models (Qwen3-235B-A22B, DeepSeek-R1) showed positive Δ Set F1 when explicitly prompted; non-reasoning models often showed negative deltas.
  - [corpus]: "The Illusion of Certainty" (arxiv:2511.04418) finds that uncertainty quantification methods fail under ambiguity, suggesting architectural reasoning improvements alone are insufficient without explicit uncertainty handling mechanisms.
- Break condition: Even reasoning-enhanced models exhibited fragility; reasoning capability does not guarantee awareness of when multiple interpretations are valid versus when single interpretations suffice.

### Mechanism 3
- Claim: Explicit ambiguity prompting activates latent multi-interpretation capability but can induce overthinking where models generate speculative, implausible meanings.
- Mechanism: Ambiguity-aware prompts signal that multiple interpretations exist, suppressing the default single-meaning bias. However, this can overshoot into generating interpretations beyond reasonable bounds.
- Core assumption: Models possess latent capacity for multi-interpretation reasoning that requires explicit activation signals.
- Evidence anchors:
  - [abstract]: "show overthinking when explicitly prompted about ambiguity"
  - [Section 3.1.2, Table 4]: Prompted Disambiguation framework showed mixed results—reasoning models improved (Δ Set F1 positive) but some non-reasoning models degraded (Gemma2-9B: -0.78 Δ Set F1).
  - [corpus]: AmbiK (arxiv:2506.04089) validates that explicit ambiguity detection helps embodied agents in kitchen environments but notes false positive rates remain problematic in real-world deployment.
- Break condition: Over-prompting causes models to classify unambiguous text as ambiguous; generated interpretations may exceed human-acceptable plausibility bounds (the "overthinking" phenomenon described in conclusions).

## Foundational Learning

- Concept: **Chinese Ambiguity Taxonomy (Lexical/Syntactic/Semantic-Pragmatic)**
  - Why needed here: The paper's 3-category, 9-subcategory framework (polysemy, homonymy, part-of-speech, structural, syntax-semantics, speech act, conversational implicature, deixis, sociocultural) determines which failure modes apply and what retrieval examples would help.
  - Quick check question: Given "杜鹃很漂亮" (cuckoo/azalea is beautiful), identify the ambiguity type and explain why context disambiguation differs from structural ambiguity cases like "组织人员" (organize personnel vs. organizational staff).

- Concept: **Perplexity Limitations for Ambiguity Detection**
  - Why needed here: The paper explicitly found perplexity does NOT reliably distinguish ambiguous from disambiguated sentences—a critical negative result preventing misapplication of this common metric.
  - Quick check question: If Qwen3-8B assigns similar ambiguity probabilities to an ambiguous sentence and its disambiguated version (as shown in Figure 2), what does this imply about using log-probability signals for ambiguity-aware systems?

- Concept: **Set-Level Evaluation (Exact Match, Recall, Set F1)**
  - Why needed here: Standard accuracy fails for multi-interpretation tasks; capturing ALL valid meanings matters more than avoiding marginal ones, requiring set-based metrics.
  - Quick check question: Why is recall prioritized over precision in ambiguity evaluation, and how would you interpret a model with high precision but low recall on the understanding task?

## Architecture Onboarding

- Component map:
  - Input -> Detection classifier (fine-tuned RoBERTa-wwm-ext) for ambiguous vs. unambiguous
  - If ambiguous -> RAG retrieves 3-5 similar examples by subcategory
  - Construct few-shot prompt with retrieved examples + reasoning template
  - LLM inference -> post-process for interpretation set extraction
  - Optional: clarification question generation for NLI-style probing

- Critical path:
  1. Input → Detection classifier (determine if ambiguity exists)
  2. If ambiguous → RAG retrieves 3-5 similar examples by subcategory
  3. Construct few-shot prompt with retrieved examples + reasoning template
  4. LLM inference → post-process for interpretation set extraction
  5. Optional: clarification question generation for NLI-style probing

- Design tradeoffs:
  - **BERT detector vs. LLM detection**: BERT more accurate (94.7% vs. 62.6% best LLM) but requires fine-tuning; LLM more flexible but over-predicts ambiguity.
  - **Model scale selection**: Medium-scale (14B-32B) optimal for RAG benefits; large reasoning models (671B) show diminishing returns; small models (<4B) struggle with complex examples.
  - **Prompting strategy**: CoT helps reasoning models but confuses smaller models; Knowledge-enhanced prompts provide external structure; Few-shot alone often sufficient for non-reasoning models.

- Failure signatures:
  - **Single-meaning commitment**: Model outputs one interpretation when ≥2 exist (Direct Interpretation condition shows 0.00 EM across all models).
  - **Ambiguity over-prediction**: LLMs misclassify clear unambiguous sentences as ambiguous (Table 2: Gemma2-9B accuracy 38.19%).
  - **Overthinking**: Prompted Disambiguation causes speculative interpretations beyond human-annotated bounds (Section 4.2, Figure 4 example).
  - **Misidentified ambiguity source**: Model detects ambiguity correctly but localizes to wrong element (Figure 4: focuses on emotional reasoning instead of syntactic scope).

- First 3 experiments:
  1. **Detection baseline**: Fine-tune chinese-roberta-wwm-ext on 70/15/15 stratified split; incorporate POS tags and syntactic depth features; target ≥90% F1. Run baseline LLM direct-prompting comparison to quantify detection gap.
  2. **RAG ablation on understanding**: Implement RAG+FS on Qwen3-14B (medium-scale sweet spot); ablate: (a) retrieval only, (b) few-shot only, (c) combined; measure Set F1 and Recall on interpretation generation.
  3. **Overthinking probe**: Run Direct Interpretation vs. Prompted Disambiguation on 50-sample held-out set; have native speakers rate interpretation plausibility (1-5 scale); quantify overthinking as proportion of interpretations rated ≤2 that appear only in prompted condition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What lightweight, fine-tuning-free methods can effectively mitigate LLM fragility when handling textual ambiguity in resource-constrained deployment settings?
- Basis in paper: [explicit] The conclusion states: "For future work, we plan to conduct fine-grained analysis within different categories of ambiguity and develop lightweight, effective methods to mitigate these problems."
- Why unresolved: While RAG improved performance, the paper shows it has diminishing returns for large reasoning models and may be impractical for some real-world applications requiring computational efficiency.
- What evidence would resolve it: Novel prompting strategies, adapter modules, or inference-time interventions that improve ambiguity detection F1 scores without significant computational overhead, validated across model scales.

### Open Question 2
- Question: What internal model signals, beyond perplexity, can reliably indicate when an LLM has correctly detected linguistic ambiguity?
- Basis in paper: [explicit] Section 4.1 concludes: "This observation suggests that PPL scores may not serve as a reliable signal for LLMs' ambiguity understanding ability" and "log-probabilities may not serve as a reliable signal for detecting ambiguity."
- Why unresolved: The paper systematically tested perplexity and log-probability assignments but found no consistent patterns distinguishing ambiguous from disambiguated sentences.
- What evidence would resolve it: Identification of alternative internal representations (attention patterns, hidden state activations, uncertainty estimates) that correlate with successful ambiguity detection across the benchmark categories.

### Open Question 3
- Question: Does the observed ambiguity fragility in Chinese LLMs generalize to typologically distinct languages with different ambiguity structures?
- Basis in paper: [inferred] The study focuses exclusively on Chinese textual ambiguity, with its specific linguistic features (lexical, syntactic, semantic-pragmatic categories unique to Chinese). No cross-linguistic experiments were conducted.
- Why unresolved: Chinese ambiguity types (e.g., homonymy in characters, specific syntactic structures) may not directly transfer to morphologically rich or non-tonal languages.
- What evidence would resolve it: Replication of the experimental framework on equivalent ambiguity benchmarks in languages such as English, Arabic, or Japanese, comparing performance patterns across linguistic typologies.

### Open Question 4
- Question: How can LLMs be improved to generate clarification questions that correctly identify and target the actual source of ambiguity rather than tangential interpretations?
- Basis in paper: [inferred] Section 4.2 and Figure 4 demonstrate a case where "the LLM correctly detects ambiguity, but misidentifies the source of ambiguity" and generates "a clarification question that is misaligned with human intuition."
- Why unresolved: The paper identifies this failure mode through case study but does not propose or evaluate interventions to correct clarification question generation.
- What evidence would resolve it: Systematic evaluation of clarification question quality on ambiguous premises, combined with training or prompting methods that improve alignment between model-identified and human-identified ambiguity sources.

## Limitations

- Cross-linguistic generalizability uncertainty due to exclusive focus on Chinese language ambiguity patterns
- Ambiguity taxonomy may miss language-specific patterns or underrepresent certain ambiguity types
- RAG effectiveness depends heavily on retrieval quality and assumes transferable patterns between examples

## Confidence

**High Confidence**: Detection task results showing BERT classifier's 94.7% accuracy vs LLM's 43-65% range, RAG benefits for medium-scale models (Qwen3-32B: 54.79→69.57 Set F1), and the fundamental observation that LLMs struggle with ambiguity detection.

**Medium Confidence**: The overthinking phenomenon characterization, the optimal model scale findings (14B-32B sweet spot for RAG), and the comparative reasoning model performance claims, as these depend on specific prompt engineering and evaluation choices.

**Low Confidence**: Cross-linguistic applicability, the completeness of the 9-category ambiguity taxonomy, and whether the observed patterns hold for specialized domains or non-Chinese content.

## Next Checks

1. **Cross-linguistic transferability test**: Apply the same RAG+FS approach to English ambiguous sentences from existing datasets (e.g., Winograd Schema or general ambiguity corpora) and measure whether the medium-model sweet spot and overthinking phenomena replicate.

2. **Retrieval quality ablation**: Systematically vary the number and quality of retrieved examples (0, 1, 3, 5 examples) and measure impact on both detection accuracy and interpretation quality, particularly for the overthinking effect.

3. **Real-world deployment stress test**: Implement a pilot system using the best-performing RAG+FS approach on actual Chinese NLP applications (e.g., customer service or document summarization) and measure precision/recall on downstream tasks where ambiguity handling directly impacts user outcomes.