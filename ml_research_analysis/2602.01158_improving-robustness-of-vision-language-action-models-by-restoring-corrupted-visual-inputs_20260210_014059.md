---
ver: rpa2
title: Improving Robustness of Vision-Language-Action Models by Restoring Corrupted
  Visual Inputs
arxiv_id: '2602.01158'
source_url: https://arxiv.org/abs/2602.01158
tags:
- corruption
- image
- visual
- arxiv
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Vision-Language-Action
  (VLA) models to image corruptions such as sensor noise, dead pixels, and lens contaminants.
  While existing literature focuses on physical occlusions, sensor-level artifacts
  remain under-explored and cause significant performance degradation in VLAs.
---

# Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs

## Quick Facts
- arXiv ID: 2602.01158
- Source URL: https://arxiv.org/abs/2602.01158
- Reference count: 28
- Introduces CRT module that recovers near-baseline performance (e.g., from 2% to 87%) under severe image corruption

## Executive Summary
This paper addresses the vulnerability of Vision-Language-Action (VLA) models to image corruptions such as sensor noise, dead pixels, and lens contaminants. While existing literature focuses on physical occlusions, sensor-level artifacts remain under-explored and cause significant performance degradation in VLAs. The authors introduce the Corruption Restoration Transformer (CRT), a lightweight plug-and-play module that restores corrupted visual inputs before they reach the VLA. CRT leverages advanced transformer mechanisms including Shifted Patch Tokenization, Rotary Position Embeddings, and Locality Self-Attention, trained with an adversarial objective to recover clean observations from corrupted inputs. Extensive experiments on LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, with models like π0.5 maintaining near-baseline success rates (e.g., recovering from 2% to 87% under severe corruption). While some trade-offs exist for smaller VLAs, CRT introduces minimal computational overhead while significantly enhancing robustness to image corruptions.

## Method Summary
The Corruption Restoration Transformer (CRT) is a lightweight plug-and-play module designed to restore corrupted visual inputs before they reach VLA models. It employs advanced transformer mechanisms including Shifted Patch Tokenization for efficient tokenization, Rotary Position Embeddings for improved position encoding, and Locality Self-Attention for capturing local spatial relationships. The module is trained using an adversarial objective that minimizes the difference between restored and clean observations, enabling it to effectively recover visual information lost due to sensor noise, dead pixels, and lens contaminants. CRT can be integrated into existing VLA architectures without requiring architectural changes, making it a practical solution for enhancing robustness to image-level corruptions.

## Key Results
- CRT effectively recovers near-baseline performance under severe corruption conditions (e.g., recovering from 2% to 87% success rate)
- Models like π0.5 maintain high performance with CRT integration across various corruption types
- CRT introduces minimal computational overhead while significantly enhancing robustness to image-level corruptions

## Why This Works (Mechanism)
CRT works by leveraging transformer-based mechanisms to restore corrupted visual inputs before they reach the VLA model. The key insight is that by reconstructing clean observations from corrupted inputs using Shifted Patch Tokenization, Rotary Position Embeddings, and Locality Self-Attention, the downstream VLA can operate on high-quality visual information. The adversarial training objective ensures that the restored images closely resemble clean observations, minimizing the performance degradation caused by sensor-level artifacts. This approach effectively addresses the gap in robustness against image corruptions that existing VLA models struggle with, while maintaining the original architecture's capabilities.

## Foundational Learning

**Vision-Language-Action (VLA) Models**
- Why needed: VLAs integrate visual perception, language understanding, and action generation for embodied AI tasks
- Quick check: π0.5 achieves 87% success rate on clean images but drops to 2% under severe corruption

**Image Corruption Types**
- Why needed: Understanding the specific vulnerabilities of VLAs to sensor-level artifacts versus physical occlusions
- Quick check: CRT addresses noise, dead pixels, and lens contaminants but not physical occlusions

**Transformer Mechanisms**
- Why needed: Modern restoration approaches require advanced tokenization and attention mechanisms
- Quick check: CRT uses Shifted Patch Tokenization, Rotary Position Embeddings, and Locality Self-Attention

**Adversarial Training**
- Why needed: Ensures restored images closely match clean observations for optimal VLA performance
- Quick check: CRT trained with adversarial objective to minimize difference between restored and clean images

## Architecture Onboarding

**Component Map**
Sensor -> Corruption -> CRT -> VLA -> Action

**Critical Path**
Corrupted image → CRT restoration → Clean image representation → VLA processing → Action output

**Design Tradeoffs**
- CRT adds minimal computational overhead but requires additional training
- Effective for image-level corruptions but doesn't address physical occlusions
- Works best with larger VLA models; smaller models (π0.5) show some performance trade-offs

**Failure Signatures**
- Under physical occlusions: CRT provides no benefit as it only addresses image-level artifacts
- For very small VLA models: Potential performance degradation due to CRT overhead
- When corruption is too severe: Restoration quality may degrade, limiting VLA effectiveness

**3 First Experiments**
1. Apply CRT to π0.5 on LIBERO benchmark with severe corruption to verify 2% → 87% recovery
2. Test CRT integration with different VLA architectures to measure computational overhead
3. Evaluate CRT performance when both image corruptions and physical occlusions are present simultaneously

## Open Questions the Paper Calls Out
The paper acknowledges that while CRT effectively handles image-level corruptions, it does not address physical occlusions or multi-modal perturbations that may co-occur in real-world settings. The evaluation focuses primarily on controlled corruption scenarios, with limited exploration of how CRT performs when both visual artifacts and physical occlusions are present simultaneously.

## Limitations
- CRT does not address physical occlusions or multi-modal perturbations that may co-occur in real-world settings
- Performance trade-offs observed for smaller VLAs (like π0.5) suggest benefits may not scale uniformly across all model sizes
- Evaluation focuses primarily on controlled corruption scenarios rather than complex real-world conditions

## Confidence

**High confidence**: Effectiveness of CRT in recovering performance under image corruption conditions, supported by quantitative results showing near-baseline success rates (e.g., recovery from 2% to 87% under severe corruption)

**Medium confidence**: Claim of minimal computational overhead, as the paper provides qualitative statements about CRT being "lightweight" but lacks detailed ablation studies on computational costs across different VLA architectures

**Medium confidence**: Generalizability of results across diverse real-world scenarios, given that evaluation is primarily conducted in simulation environments (LIBERO, Meta-World) with synthetically applied corruptions

## Next Checks

1. Evaluate CRT's performance when image corruptions co-occur with physical occlusions to assess real-world applicability
2. Conduct detailed computational overhead analysis across different VLA architectures to quantify latency and memory impacts
3. Test CRT's robustness on physical robot platforms in environments with varying lighting conditions and real sensor noise patterns