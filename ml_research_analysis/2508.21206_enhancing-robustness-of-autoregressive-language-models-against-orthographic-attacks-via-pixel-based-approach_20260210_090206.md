---
ver: rpa2
title: Enhancing Robustness of Autoregressive Language Models against Orthographic
  Attacks via Pixel-based Approach
arxiv_id: '2508.21206'
source_url: https://arxiv.org/abs/2508.21206
tags:
- language
- pixel
- text
- image
- pixel-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of autoregressive language
  models to orthographic attacks, where text perturbations using multilingual characters
  lead to significant performance degradation. The core problem stems from the out-of-vocabulary
  issue in subword tokenizers when handling noisy or multilingual inputs.
---

# Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach

## Quick Facts
- arXiv ID: 2508.21206
- Source URL: https://arxiv.org/abs/2508.21206
- Reference count: 14
- Pixel-based generative language model shows superior robustness to orthographic noise with 1.4x perplexity increase vs 43x for text models

## Executive Summary
This paper addresses the vulnerability of autoregressive language models to orthographic attacks, where text perturbations using multilingual characters lead to significant performance degradation. The authors propose a pixel-based generative language model that renders each word as a separate fixed-size image, replacing text-based embeddings with pixel-based representations. This approach establishes a direct one-to-one correspondence between words and their pixel representations, enabling natural next-token prediction without requiring auxiliary components like OCR.

The experimental results demonstrate that the pixel-based model maintains more stable performance under increasing noise levels compared to standard text-based models, with significantly smaller accuracy drops (0.3 vs 0.6) and perplexity increases (1.4x vs 43x). The approach shows particular advantage for non-Latin languages (Russian, Chinese, Japanese, Hindi), exhibiting significantly smaller perplexity increases compared to text-based models.

## Method Summary
The method builds a pixel-based generative language model using LLaMA-style decoder architecture. Each word token is rendered as a fixed-size grayscale image (50×20 pixels) using Noto font with adaptive font sizing based on word length. A pre-rendered lookup table stores all vocabulary tokens as images, with on-the-fly rendering for OOV tokens. The rendered images are passed through a learnable linear projector to create embeddings compatible with the transformer decoder. The model is trained using standard cross-entropy next-token prediction loss on BookCorpus + English Wikipedia, with evaluation on SST-2 sentiment classification under noise injection and multilingual LAMBADA.

## Key Results
- Pixel model shows 1.4x perplexity increase vs 43x for text model under noise on SST-2
- Accuracy drop of 0.3 vs 0.6 on SST-2 under increasing noise levels
- Superior robustness for non-Latin languages (Russian, Chinese, Japanese, Hindi) with significantly smaller perplexity increases
- Maintains autoregressive training compatibility without auxiliary components like OCR

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pixel representations preserve semantic similarity under orthographic perturbation better than subword token sequences.
- **Mechanism:** When characters are substituted (e.g., "apple" → "ápple"), subword tokenizers produce entirely different token sequences, breaking the embedding mapping. In contrast, rendered images of perturbed words retain high visual similarity (~0.89 cosine similarity in pixel embedding space per the paper), allowing the model to map noisy inputs to semantically similar representations.
- **Core assumption:** The linear projector learns to extract visually invariant features from rendered word images, generalizing across font variations and minor character substitutions.
- **Evidence anchors:** [abstract] "This design provides stronger robustness to noisy inputs"; [Section 4.3] Shows perplexity increase of only 1.4x for pixel model vs. 43x for text model under noise; accuracy drop of 0.3 vs. 0.6 on SST-2.

### Mechanism 2
- **Claim:** Word-level image rendering enables direct alignment with next-token prediction without auxiliary components.
- **Mechanism:** Prior pixel-based approaches rendered full sentences into images, then split into fixed patches—creating patch boundaries that don't align with linguistic units. By rendering each word as a discrete fixed-size image, the model establishes a one-to-one mapping between pixel representations and token positions, preserving the standard autoregressive training objective.
- **Core assumption:** Word boundaries from the tokenizer provide semantically meaningful units for language modeling; the adaptive renderer's font scaling preserves discriminative visual features across word lengths.
- **Evidence anchors:** [abstract] "establishes a direct one-to-one correspondence between words and their pixel representations, enabling natural next-token prediction without requiring auxiliary components like OCR"; [Section 3] "This architecture receives inputs in the pixel modality while remaining fully compatible with next-token prediction".

### Mechanism 3
- **Claim:** Pre-rendered vocabulary lookup provides O(1) rendering complexity while maintaining noise robustness for OOV inputs.
- **Mechanism:** For known tokens, pre-rendered images are stored in a lookup table implemented as an embedding layer. For OOV tokens (including noisy variants), on-the-fly rendering still produces pixel representations that integrate into the embedding space via the linear projector, maintaining the visual similarity property.
- **Core assumption:** The linear projector generalizes from pre-rendered vocabulary images to newly rendered OOV images—this is not explicitly validated in the paper.
- **Evidence anchors:** [Section 3.2] "Rendering a token then reduces to a simple dictionary lookup with complexity O(1)"; [Section 3.4] "if the input contains tokens not present in the pre-rendered vocabulary, these tokens can still be rendered individually into images".

## Foundational Learning

- **Concept: Subword tokenization and OOV fragility**
  - Why needed here: The paper's core motivation is that BPE/SentencePiece tokenizers produce brittle token sequences when characters are perturbed. Understanding how "apple" might tokenize as ["app", "le"] while "ápple" becomes ["√°", "pp", "le"] is essential.
  - Quick check question: Given a tokenizer with vocabulary ["app", "le", "√°", "pp"], what happens to the token sequence when "apple" is changed to "ápple"?

- **Concept: Vision Transformer patch embeddings**
  - Why needed here: The pixel model processes rendered word images through a linear projector analogous to ViT patch projections. Understanding how spatial pixel arrangements map to dense vectors clarifies how visual similarity translates to embedding similarity.
  - Quick check question: How does a linear projection differ from a convolutional layer in extracting features from a rendered word image?

- **Concept: Autoregressive language modeling objective**
  - Why needed here: The paper's innovation is maintaining next-token prediction compatibility with pixel inputs. Understanding cross-entropy loss over token vocabulary is necessary to see how pixel embeddings connect to text generation.
  - Quick check question: In a standard autoregressive LM, what does the model predict at each position, and how is the loss computed?

## Architecture Onboarding

- **Component map:**
  ```
  Input text → Tokenizer (BPE) → Token IDs
                                    ↓
                         Pre-rendered Lookup Table (vocab × H × W)
                                    ↓
                         Linear Projector (H×W → hidden_size)
                                    ↓
                         Transformer Decoder (LLaMA-style)
                                    ↓
                         Output Head → Next Token Probability
  ```

- **Critical path:** The linear projector is the only learnable bridge between pixel representations and the transformer. If it fails to extract meaningful features, the entire approach collapses. Pre-train verification should confirm the projector learns discriminative representations (e.g., via probing tasks on rendered word pairs).

- **Design tradeoffs:**
  - Image resolution vs. computational cost: 50×20 pixels chosen; larger images may capture more detail but increase projector parameters.
  - Font choice: Noto Sans provides broad script coverage but may not be optimal for specific writing systems.
  - Tokenizer vocabulary: BPE vocabulary size (32,001) limits pre-rendered coverage; larger vocabulary increases lookup table memory.

- **Failure signatures:**
  - Non-Latin scripts with tokenizer: If BPE tokenizer has poor coverage of target script, pre-rendered tokens are sparse, forcing on-the-fly rendering with potential quality variance.
  - Extremely long words: Adaptive font scaling may render text illegible.
  - Script mixing: Tokenizer may segment multilingual text unpredictably, breaking word-level alignment assumptions.

- **First 3 experiments:**
  1. **Reproduce noise robustness result:** Train both pixel and text baselines on BookCorpus subset, evaluate on LAMBADA with noise ratios [0, 0.1, 0.2, 0.3]. Confirm perplexity divergence pattern matches paper (pixel model should show ~3x increase vs. ~300x for text at highest noise).
  2. **Ablate linear projector capacity:** Train variants with projector hidden dimensions [256, 768, 2048] and measure clean vs. noisy perplexity. Identify minimum capacity for robustness.
  3. **Probe cross-script transfer:** Evaluate pixel model trained on English-only data on Russian/Chinese test sets without further training. Quantify whether visual features provide zero-shot transfer or if language-specific training is required (paper shows mixed results—advantage for non-Latin but still high perplexity).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the re-tokenization of noised sentences be handled for token-level pixel methods when noise inevitably alters tokenization in practice?
- Basis in paper: [explicit] "We regard the re-tokenization of noised sentences for the token-level pixel methods as an open problem, and leave its resolution for future work."
- Why unresolved: The experiments assume tokenization remains identical between clean and noised sentences, but in realistic settings, character-level noise changes subword segmentation, creating misaligned token sequences.
- What evidence would resolve it: A method that handles dynamic tokenization under noise, with experiments comparing pixel embeddings under original vs. re-tokenized noisy inputs.

### Open Question 2
- Question: Would combining pixel-based representations with multilingual tokenizers improve performance on Latin-script languages while maintaining advantages for non-Latin scripts?
- Basis in paper: [inferred] The authors attribute weaker Latin-script performance to using an English tokenizer, noting that "many lexical items in Latin-alphabet languages share cognates or morphemic units with English, enabling language model to effectively reuse subword units."
- Why unresolved: The current design uses an English BPE tokenizer, potentially disadvantaging the pixel model on related languages where subword overlap benefits text-based models.
- What evidence would resolve it: Experiments comparing pixel models with multilingual tokenizers (e.g., mBERT, XLM-R vocabularies) against the current English-only tokenizer across all evaluated languages.

### Open Question 3
- Question: How does the pixel-based approach scale to larger model sizes and does the robustness advantage persist at scale?
- Basis in paper: [inferred] Experiments use a small model (768 hidden size, 12 layers) trained for only 100K steps, with no comparison to larger architectures or analysis of scaling behavior.
- Why unresolved: Modern LLMs operate at much larger scales; it remains unclear whether pixel-based embeddings provide similar benefits in larger models or if computational overhead becomes prohibitive.
- What evidence would resolve it: Scaling experiments with models of varying sizes (e.g., 350M, 1B, 7B parameters) comparing convergence rates, final performance, and robustness metrics.

### Open Question 4
- Question: What is the optimal image resolution and adaptive rendering strategy for balancing information preservation with computational efficiency?
- Basis in paper: [inferred] The authors use fixed 50×20 pixel images with adaptive font scaling but provide no ablation study on how rendering parameters affect model performance or long-word handling.
- Why unresolved: Aggressive font scaling for long words may reduce visual clarity, while larger images increase computational costs; the trade-off space remains unexplored.
- What evidence would resolve it: Ablation experiments varying image dimensions, font scaling strategies, and analyzing performance on words of different lengths.

## Limitations
- Limited evaluation to a single small model architecture (12 layers, 768 hidden size)
- No validation of linear projector's ability to generalize from pre-rendered to on-the-fly rendered OOV tokens
- Underspecified adaptive font scaling mechanism for long words
- No testing against homoglyph attacks that preserve visual similarity but change semantic meaning

## Confidence
- **High Confidence:** Experimental results showing pixel models maintain lower perplexity increases (1.4x vs 43x) and accuracy drops (0.3 vs 0.6) under noise compared to text models.
- **Medium Confidence:** Claim that pixel representations provide "stronger robustness to noisy inputs" for multilingual settings, though performance varies significantly by script type.
- **Low Confidence:** Assertion that the linear projector learns to extract visually invariant features across font variations and minor character substitutions without direct probing experiments.

## Next Checks
1. **Probe Linear Projector Representations:** Conduct controlled experiments with pairs of visually similar vs. dissimilar word images (same/different characters, same/different fonts) and measure cosine similarity in the projected embedding space. This would directly validate whether the projector learns visually invariant features as claimed.

2. **Stress Test Homoglyph Robustness:** Design experiments using homoglyph attacks (e.g., replacing Latin characters with visually identical Cyrillic/Greek characters) to determine if pixel models maintain robustness when visual similarity is preserved but semantic meaning changes. This would reveal the limits of visual similarity-based defense.

3. **Evaluate OOV Rendering Generalization:** Systematically test the model's ability to handle OOV tokens by measuring embedding quality degradation as a function of token frequency in training data and visual complexity (word length, script type). This would validate the critical assumption that on-the-fly rendering produces useful embeddings for unseen tokens.