---
ver: rpa2
title: Calibrated Uncertainty Sampling for Active Learning
arxiv_id: '2510.03162'
source_url: https://arxiv.org/abs/2510.03162
tags:
- calibration
- error
- samples
- uncertainty
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of improving both calibration
  and generalization in active learning for deep neural networks. It proposes a novel
  acquisition function, CUSAL, that combines calibration error estimation with uncertainty
  sampling using lexicographic ordering: first selecting samples with highest calibration
  error, then by model uncertainty if calibration errors are equal.'
---

# Calibrated Uncertainty Sampling for Active Learning

## Quick Facts
- arXiv ID: 2510.03162
- Source URL: https://arxiv.org/abs/2510.03162
- Authors: Ha Manh Bui; Iliana Maifeld-Carucci; Anqi Liu
- Reference count: 40
- Key outcome: Proposes CUSAL method that combines calibration error estimation with uncertainty sampling, achieving better calibration and accuracy than six strong baselines across six datasets

## Executive Summary
This paper addresses the challenge of improving both calibration and generalization in active learning for deep neural networks. The authors propose a novel acquisition function called CUSAL that leverages a lexicographic ordering approach: first selecting samples with highest calibration error, then by model uncertainty if calibration errors are equal. The method employs a kernel-based calibration error estimator that remains consistent under covariate shift in active learning settings. Theoretical bounds on calibration error estimation and expected calibration error are proven for both the unlabeled pool and unseen test data. Empirically, CUSAL demonstrates superior performance across multiple image classification datasets.

## Method Summary
The CUSAL method combines calibration error estimation with uncertainty sampling through a novel acquisition function. It uses a kernel-based estimator to measure calibration error on the unlabeled pool, then applies lexicographic ordering to select samples: prioritizing those with highest calibration error first, and using model uncertainty (entropy) as a tiebreaker. The approach is theoretically grounded with bounds on calibration error estimation and expected calibration error, and is validated across six image classification datasets including MNIST, Fashion-MNIST, SVHN, CIFAR-10, CIFAR-10-LT, and ImageNet. The method shows consistent improvements in both calibration quality and classification accuracy compared to six strong baselines.

## Key Results
- CUSAL achieves lower expected calibration error (0.030) compared to baselines (0.033-0.123) on MNIST
- CUSAL reaches higher accuracy (95.9%) compared to baselines (83.1%-95.7%) on MNIST
- Method demonstrates consistent improvements across six datasets including challenging scenarios like CIFAR-10-LT
- Theoretical bounds on calibration error estimation are proven and hold in practice

## Why This Works (Mechanism)
CUSAL works by recognizing that traditional uncertainty sampling methods can select samples that are either easy or hard to calibrate, leading to suboptimal learning. By explicitly estimating calibration error and prioritizing samples with high calibration error, the method ensures the model learns to make well-calibrated predictions. The lexicographic ordering ensures that when multiple samples have similar calibration issues, the most uncertain ones are selected first. This dual focus on calibration quality and uncertainty naturally leads to better generalization and more reliable predictions.

## Foundational Learning
- **Expected Calibration Error (ECE)**: Measures the difference between predicted confidence and actual accuracy; needed to quantify calibration quality, quick check: compute ECE on validation set
- **Kernel-based calibration error estimation**: Non-parametric method for estimating calibration error; needed for consistent estimation under covariate shift, quick check: verify estimator consistency with synthetic shifts
- **Lexicographic ordering**: Mathematical concept for multi-criteria selection; needed to combine calibration error and uncertainty signals, quick check: test ordering on simple synthetic data
- **Active learning acquisition functions**: Methods for selecting informative samples; needed as baseline for comparison, quick check: implement entropy-based acquisition as reference
- **Covariate shift**: Distributional change between training and test data; needed to understand active learning dynamics, quick check: measure shift between labeled and unlabeled pools

## Architecture Onboarding

**Component Map**: Unlabeled Pool -> Calibration Error Estimator -> Uncertainty Module -> Lexicographic Selector -> Query Samples

**Critical Path**: The core pipeline flows from the unlabeled pool through calibration error estimation, uncertainty computation, and lexicographic selection to determine which samples to query. The calibration error estimator uses kernel methods to measure how well current model predictions align with true probabilities, while the uncertainty module computes entropy-based uncertainty scores.

**Design Tradeoffs**: The method trades computational complexity (kernel-based calibration estimation) for improved calibration and accuracy. Alternative designs could use parametric calibration error estimators for speed, or different uncertainty measures beyond entropy, but these would sacrifice the theoretical guarantees and empirical performance demonstrated by the current approach.

**Failure Signatures**: Poor performance may occur when calibration error and uncertainty are highly correlated, making the lexicographic ordering redundant. The method may also struggle with very high-dimensional features where kernel methods become computationally expensive. In cases of severe class imbalance, the calibration error estimator might be biased toward majority classes.

**First Experiments**: 1) Compare CUSAL against pure uncertainty sampling on MNIST with varying pool sizes, 2) Test sensitivity to kernel bandwidth parameters on Fashion-MNIST, 3) Evaluate performance degradation when calibration error and uncertainty are artificially correlated

## Open Questions the Paper Calls Out
None

## Limitations
- Kernel-based calibration error estimator may face scalability challenges with high-dimensional features or very large datasets
- Method assumes calibration error and model uncertainty are complementary signals, but their correlation in practice remains unexamined
- Experimental validation is limited to image classification tasks and may not generalize to other domains

## Confidence

**Theoretical analysis**: High - The proofs for calibration error bounds and expected calibration error are rigorous and well-established

**Empirical results**: Medium - While extensive, the experiments are limited to image classification tasks and may not generalize to other domains

**Scalability claims**: Low - Limited testing on very large-scale datasets or real-world active learning scenarios

## Next Checks
1. Test CUSAL on non-image datasets (text, tabular data) to verify generalizability beyond computer vision tasks
2. Evaluate performance under extreme class imbalance scenarios beyond the CIFAR-10-LT experiment
3. Benchmark against recently proposed active learning methods that incorporate conformal prediction or distribution shift detection