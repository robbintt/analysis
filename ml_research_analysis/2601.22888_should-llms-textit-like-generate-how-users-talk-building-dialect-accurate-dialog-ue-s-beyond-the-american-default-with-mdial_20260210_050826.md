---
ver: rpa2
title: Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate
  Dialog[ue]s Beyond the American Default with MDial
arxiv_id: '2601.22888'
source_url: https://arxiv.org/abs/2601.22888
tags:
- dialect
- english
- llms
- mdial
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MDial, a large-scale framework for generating
  multi-dialectal conversational data across nine English dialects, addressing the
  problem that over 80% of English speakers do not use Standard American English (SAE)
  yet experience higher failure rates and stereotyped responses from LLMs. The method
  involves creating a parallel dataset using rule-based transformations (RBTs) with
  native linguist annotations across lexical, orthographic, and morphosyntactic dimensions,
  while distinguishing between features appropriate for model generation versus user
  speech.
---

# Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial

## Quick Facts
- arXiv ID: 2601.22888
- Source URL: https://arxiv.org/abs/2601.22888
- Reference count: 40
- Over 80% of English speakers do not use Standard American English (SAE) yet experience higher failure rates from LLMs.

## Executive Summary
This paper introduces MDial, a framework for generating multi-dialectal conversational data across nine English dialects, addressing the problem that over 80% of English speakers do not use Standard American English (SAE) yet experience higher failure rates and stereotyped responses from LLMs. The method involves creating a parallel dataset using rule-based transformations (RBTs) with native linguist annotations across lexical, orthographic, and morphosyntactic dimensions, while distinguishing between features appropriate for model generation versus user speech. Independent evaluations show MDial outputs are preferred over prior methods in 98% of pairwise comparisons for dialect naturalness. The MDialBenchmark dataset with 50k+ dialogs (97k+ QA pairs) is used to evaluate 17 LLMs on dialect identification and response generation tasks, revealing that even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. Post-training on MDial data substantially improves dialectal capabilities, with turn 1 accuracy reaching 96.7% after fine-tuning.

## Method Summary
MDial generates dialect-accurate conversational data by first building a dialect knowledge base with native linguist annotations rating feature prevalence (1-4) and whether models should mirror each feature ("Model Mirror?" flag). Seed dialogs are generated in Standard American English using lexical seeds in natural/indirect modes, then transformed through OrthoLex (lexical + orthographic changes) followed by sequential morphosyntactic rule application (RBTs). Each morphosyntactic rule is applied one-by-one with explicit examples and linguist-curated guidelines, with transformation probabilities based on prevalence ratings (4→100%, 3→60%, 2→30%, 1→0%). Quality control reverts low-confidence transformations probabilistically. Two datasets are created: RBT_User for comprehension testing and RBT_Model for generation fine-tuning. The MDialBenchmark evaluates 17 LLMs on dialect identification and response completion tasks across 1/2/4/8 turns.

## Key Results
- MDial outputs preferred over prior methods in 98% of pairwise comparisons for dialect naturalness
- Even frontier models achieve under 70% accuracy on dialect identification, failing to reach 50% for Canadian English
- Response completion training transfers to dialect identification (+10.9 pp), but classification training degrades generation (-10.6 pp on average)
- Turn 1 accuracy reaches 96.7% after fine-tuning on MDial data

## Why This Works (Mechanism)

### Mechanism 1
Separating user-appropriate from model-appropriate dialect features produces more natural outputs than applying all attested features. Native linguists annotate which features models should mirror, filtering out up to 90% of eWAVE-listed morphosyntactic features for model generation to prevent stereotyped or antiquated outputs. This prevents outputs like "thou" in Irish English.

### Mechanism 2
Sequential, single-rule morphosyntactic transformation prevents LLMs from defaulting to SAE grammar. Rather than providing all rules simultaneously (which causes LLMs to revert to SAE patterns), each morphosyntactic rule is applied one-by-one with explicit examples and linguist-curated guidelines.

### Mechanism 3
Response completion training transfers to dialect identification, but classification training degrades generation. Response completion requires fine-grained comparison between dialectal candidates, building discriminative representations. Classification involves coarser single-label assignment, enabling shortcut learning without comparative reasoning.

## Foundational Learning

- **Concept: Three dialectal dimensions (lexical, orthographic, morphosyntactic)**
  - Why needed here: MDial transforms SAE across all three; prior work typically addressed only morphosyntax via eWAVE.
  - Quick check question: Can you explain why "apartment → flat" is lexical while "color → colour" is orthographic?

- **Concept: Dialect-parallel data generation**
  - Why needed here: Enables controlled comparison of model behavior across dialects with identical semantic content.
  - Quick check question: Why is parallel data preferred over native corpus collection for benchmarking?

- **Concept: Multi-label assignment for ambiguous dialogs**
  - Why needed here: Short dialogs may be valid in multiple dialects; single-label evaluation would penalize correct but non-primary answers.
  - Quick check question: How does the O×X×M×D matrix track valid transformations?

## Architecture Onboarding

- **Component map:** Dialect Knowledge Base → Seed Dialog Generator → OrthoLex Transformer → Morphosyntactic RBT Engine → Quality Controller → MDialBench Evaluator

- **Critical path:** Human annotation → Wordbank construction → Seed generation → OrthoLex → Morphosyntactic RBT (User and Model variants) → Quality control → Benchmark/training data

- **Design tradeoffs:**
  - Sequential rule application: More faithful transformation vs. higher inference cost
  - Separate RBT_User and RBT_Model datasets: Enables controlled studies of comprehension vs. generation at cost of 2× data generation
  - Probabilistic rule sampling (based on prevalence ratings): Natural variation vs. reproducibility challenges

- **Failure signatures:**
  - High US/GB confusion for Canadian/Philippine inputs → model defaulting to high-resource dialects
  - Archaic transformations (e.g., "thou") → eWAVE rules applied without filtering
  - Reasoning models outperform base models → dialect identification benefits from test-time compute

- **First 3 experiments:**
  1. Reproduce the Trans-EnV vs. MDial preference evaluation on 20 dialogs per dialect to validate your annotation pipeline matches the paper's 98% preference rate.
  2. Fine-tune a small model (e.g., Qwen3-0.6B) on just 0.15 epoch of RBT_Model data to verify that ~90% turn-1 accuracy is achievable with minimal supervision.
  3. Run confusion matrix analysis on your chosen base model across all 9 dialects to identify which dialects are most confused with US/GB before any fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop reliable automatic evaluation metrics for open-ended dialectal generation, given that current LLM-as-a-judge approaches lack sufficient dialectal knowledge? Authors state: "We could not develop sufficiently precise judges to properly benchmark generation performance and leave the development of generation metrics and dialectal post-training as important directions for future research."

### Open Question 2
To what extent do MDialBench classification and generation performance improvements transfer to real-world dialect usage, as opposed to rule-based synthetic dialect transformations? The paper evaluates only on synthetically transformed dialogs using RBTs, not authentic dialect corpora, raising questions about ecological validity.

### Open Question 3
How does the novel distinction between Features_User and Features_Model generalize across different conversational contexts, formality levels, and speaker populations within each dialect? Annotations were collected for a specific conversational context without exploring how appropriate features shift across different interaction types.

## Limitations
- Core mechanisms depend heavily on native linguist judgments that may not generalize across dialect speaker populations
- The 90% pruning rate for morphosyntactic features is based on subjective annotator ratings rather than empirical naturalness testing
- Sequential transformation approach assumes fixed SAE priors that may change with newer model architectures

## Confidence
- **High confidence**: Benchmark evaluation results showing systematic misclassification of non-SAE dialects as American or British
- **Medium confidence**: 98% human preference rate for MDial over prior methods, given single annotator per dialect design
- **Medium confidence**: Mechanism claims about why sequential vs. batch transformation works better

## Next Checks
1. Replicate the Trans-EnV vs. MDial human preference evaluation with at least 3 annotators per dialect to verify the 98% preference rate is robust across independent raters.
2. Conduct an ablation study comparing sequential vs. batch morphosyntactic transformation on the same seed dialogs to isolate the effect size of the sequential approach.
3. Test whether the observed transfer asymmetry between classification and completion training persists when using balanced label distributions or different dataset constructions.