---
ver: rpa2
title: Leveraging Open-Source Large Language Models for Clinical Information Extraction
  in Resource-Constrained Settings
arxiv_id: '2507.20859'
source_url: https://arxiv.org/abs/2507.20859
tags:
- tasks
- performance
- task
- dragon
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates nine open-source large language models on
  28 clinical information extraction tasks in Dutch, using a newly developed framework
  called llmextractinator. The models were tested in a zero-shot setting on the DRAGON
  benchmark, which includes tasks such as binary classification, multi-class classification,
  regression, and named entity recognition.
---

# Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings

## Quick Facts
- arXiv ID: 2507.20859
- Source URL: https://arxiv.org/abs/2507.20859
- Reference count: 40
- Open-source LLMs with ≥14B parameters achieve ~0.75 utility on Dutch clinical extraction tasks in zero-shot settings

## Executive Summary
This study evaluates nine open-source large language models on 28 clinical information extraction tasks in Dutch using a zero-shot approach. The authors develop llm_extractinator, a framework that enables structured JSON output extraction from Dutch medical reports across diverse task types including classification, regression, and named entity recognition. The research demonstrates that models with approximately 14 billion parameters or more can achieve clinically acceptable performance in resource-constrained settings, while smaller models fail. Crucially, the study finds that processing clinical text in its native Dutch language consistently outperforms translation to English, challenging conventional wisdom about leveraging English-centric models.

## Method Summary
The research employs the llm_extractinator framework to evaluate nine open-source LLMs on the DRAGON benchmark, which contains 28,824 annotated Dutch medical reports from five institutions. The framework uses zero-shot chain-of-thought prompting with structured JSON output schemas, running models in 4-bit quantization on consumer-grade hardware. Tasks span binary classification, multi-class classification, regression, and named entity recognition. The overall performance metric S_DRAGON aggregates arithmetic means across all tasks, while per-task metrics include AUC, Cohen's kappa, RSMAPES, and F1 scores.

## Key Results
- Llama-3.3-70B, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B achieved highest utility scores near 0.75
- Smaller models (Llama-3.2-3B, Gemma-2-2B) consistently failed with nonsensical outputs or invalid JSON
- Translation of Dutch reports to English before inference degraded performance significantly (∆S_DRAGON = -0.11 to -0.25)
- Regression tasks performed excellently (avg 0.971) while NER tasks failed uniformly (F1 < 0.47)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Open-source models with approximately 14 billion parameters or more possess sufficient capacity to handle domain-specific, non-English clinical text in zero-shot settings
- **Mechanism:** Larger parameter counts enable better retention of multilingual knowledge and medical reasoning patterns during pre-training
- **Core assumption:** Pre-training data contained sufficient Dutch and medical text to form robust internal representations
- **Evidence anchors:** 14B models achieved competitive results (~0.75 utility) while smaller models produced invalid JSON or near-random performance
- **Break condition:** Deploying models with fewer than 8B parameters for zero-shot clinical tasks

### Mechanism 2
- **Claim:** Processing clinical text in its native language preserves critical nuance and entity accuracy compared to translation-then-infer pipelines
- **Mechanism:** Translation introduces intermediate noise and fails to capture domain-specific jargon accurately
- **Core assumption:** Models' multilingual pre-training is sufficiently strong to outperform theoretical advantages of English-centric training data
- **Evidence anchors:** Translating Dutch reports to English consistently degraded performance (∆S_DRAGON = -0.11 to -0.25)
- **Break condition:** Relying on machine translation as a pre-processing step

### Mechanism 3
- **Claim:** Generative LLMs exhibit structural advantage in extracting numeric values via copy-and-reason capabilities, while struggling with token-level alignment required for NER
- **Mechanism:** Generative models can directly copy numbers or perform simple arithmetic reasoning, but producing sparse, token-level entity lists requires precise alignment
- **Core assumption:** NER failures are primarily due to generative architecture's difficulty with sparse token prediction rather than lack of medical knowledge
- **Evidence anchors:** Regression tasks were a relative strength (avg 0.971) while NER performance was uniformly poor (F1 < 0.47)
- **Break condition:** Expecting uniform high performance across all task types, particularly for fine-grained NER

## Foundational Learning

- **Concept: Zero-shot Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** llm_extractinator relies on CoT to elicit reasoning without labeled examples
  - **Quick check question:** How does adding "think step by step" to the prompt change model output behavior in clinical extraction tasks?

- **Concept: Model Quantization (4-bit)**
  - **Why needed here:** Models evaluated on consumer-grade hardware (12GB VRAM) using 4-bit quantization
  - **Quick check question:** What is the trade-off between inference speed/VRAM usage and output quality when loading 70B model in 4-bit vs 16-bit precision?

- **Concept: JSON-Constrained Decoding**
  - **Why needed here:** Framework enforces structured JSON output to automate evaluation
  - **Quick check question:** Why might a generative model fail to produce valid JSON even if semantic content is correct?

## Architecture Onboarding

- **Component map:** Dutch Clinical Report (Raw Text) -> Taskfile (JSON configuration) -> llm_extractinator framework (wraps LangChain & Ollama) -> Open-source LLM (e.g., Phi-4-14B) -> Automatic re-prompting (up to 3 attempts) if output schema validation fails
- **Critical path:** Designing the Taskfile, which translates clinical intent into a JSON schema the model can reliably populate
- **Design tradeoffs:**
  - Scale vs. Cost: Llama-3.3-70B offers marginal performance gains (~1%) over Phi-4-14B but requires significantly more compute/memory
  - Language Strategy: Native Processing > Translate to English for this use case
- **Failure signatures:**
  - Invalid JSON: Occurs frequently with small models (<3B)
  - Hallucination/Randomness: Occurs on tasks with vague criteria where zero-shot context is insufficient
  - NER Collapse: Consistently low F1 scores across all models for token-level tasks
- **First 3 experiments:**
  1. Reproduce Regression Success: Run llm_extractinator with Qwen-2.5-14B on single regression task to verify "copy-and-reason" capability
  2. Stress Test NER: Attempt Anonymization task (Task 25) using Llama-3.3-70B to observe token-alignment failures
  3. Translation Ablation: Run binary classification task (Task 01) in Dutch, then again with translation flag enabled, to quantify performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would few-shot prompting or retrieval-augmented generation improve open-source LLM performance on clinical information extraction tasks in resource-constrained languages?
- Basis in paper: Authors state "there is room for future research to explore the effects of few-shot prompting, lightweight instruction-tuning, or retrieval-augmented generation on model performance"
- Why unresolved: Study evaluated only zero-shot settings; impact of task-specific examples or external knowledge retrieval remains unknown
- What evidence would resolve it: Comparative experiments using llm_extractinator framework with few-shot in-context examples and RAG pipelines on DRAGON benchmark

### Open Question 2
- Question: How would larger open-source LLMs (e.g., 70B+ parameter models) perform on clinical NLP tasks compared to 14B models tested, and does scaling uniformly improve performance across task types?
- Basis in paper: Authors note "due to resource constraints, we only evaluated one model over 15B parameters and did not include any of the largest open-source LLMs"
- Why unresolved: Only Llama-3.3-70B was tested among large models, and performance gains from scaling were not uniform
- What evidence would resolve it: Systematic evaluation of additional large open-source models across all 28 DRAGON tasks

### Open Question 3
- Question: Do findings on native-language processing generalize to other mid- to low-resource languages beyond Dutch, particularly for clinical text with specialized medical terminology?
- Basis in paper: Authors state "Generalizability to other languages remains to be tested" and note open-source LLMs show "disproportionate representation of high-resource languages"
- Why unresolved: Translation to English consistently degraded performance for Dutch, but pattern unknown for other resource-constrained languages
- What evidence would resolve it: Replication using clinical datasets in other mid-resource languages with native-language vs. translation comparisons

### Open Question 4
- Question: Can alternative output formats or prompting strategies for Named Entity Recognition tasks significantly improve generative LLM performance beyond poor F1 scores (<0.47) observed?
- Basis in paper: Authors note "Generative models are not naturally suited for generating sparsely populated token-level lists"
- Why unresolved: Structured JSON output format with token-level NER requirements may have introduced conversion errors; alternative approaches were not tested
- What evidence would resolve it: Comparison experiments using span-based output formats, JSON-free structured outputs, or step-wise extraction prompting on DRAGON NER tasks

## Limitations
- Data Access Constraint: DRAGON benchmark test set requires challenge registration for ground-truth access
- Model Scope: Only 9 open-source models evaluated, all with 70B parameters or less
- Task Coverage Gap: Certain clinical extraction scenarios (temporal reasoning, multi-modal inputs) not explicitly tested

## Confidence
- **High Confidence:** Performance hierarchy between model sizes (14B+ vs <8B) is robustly supported by consistent results across all 28 tasks
- **Medium Confidence:** Specific utility threshold of ~0.75 for "acceptable" clinical performance is reasonable but somewhat arbitrary
- **Low Confidence:** Exact reasons why NER tasks fail uniformly across all models remain somewhat speculative

## Next Checks
1. Reproduce the 14B Threshold: Run same 28 tasks using 7B parameter model to empirically confirm claimed performance cliff below 14B parameters
2. NER Task Redesign: Modify NER output format to use simpler token-by-token classification approach rather than generative entity lists, then re-run Task 25
3. Translation Quality Control: Implement human-validated translation step for subset of reports and re-run inference to determine whether translation quality (rather than translation itself) drives performance degradation