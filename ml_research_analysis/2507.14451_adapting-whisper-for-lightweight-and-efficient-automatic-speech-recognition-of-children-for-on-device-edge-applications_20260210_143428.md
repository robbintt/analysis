---
ver: rpa2
title: Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition
  of Children for On-device Edge Applications
arxiv_id: '2507.14451'
source_url: https://arxiv.org/abs/2507.14451
tags:
- speech
- tiny
- data
- children
- utterances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of developing a lightweight
  and efficient Automatic Speech Recognition (ASR) system for children that can run
  locally on edge devices like a Raspberry Pi, motivated by privacy concerns and regulatory
  requirements such as COPPA. The core method involves adapting the Whisper ASR model
  using the MyST corpus of children's speech, exploring various data filtering strategies,
  and applying low-rank compression to reduce model size and inference time.
---

# Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition of Children for On-device Edge Applications

## Quick Facts
- arXiv ID: 2507.14451
- Source URL: https://arxiv.org/abs/2507.14451
- Reference count: 0
- This study fine-tunes Whisper tiny.en on the MyST children's speech corpus and applies low-rank compression to achieve 15.9% WER on Raspberry Pi with 0.23-0.41 RTF

## Executive Summary
This paper addresses the challenge of developing a lightweight Automatic Speech Recognition (ASR) system specifically for children's speech that can run locally on edge devices like the Raspberry Pi. The work is motivated by privacy concerns and regulatory requirements such as COPPA, which necessitate on-device processing rather than cloud-based solutions for educational applications. The authors fine-tune the Whisper ASR model using the MyST corpus of children's speech, explore various data filtering strategies, and apply low-rank compression to reduce model size and inference time while maintaining acceptable accuracy.

The core contribution demonstrates that a compressed Whisper tiny.en model can achieve a Word Error Rate of 15.9% on the MyST test set while running efficiently on a Raspberry Pi 5 with Real-Time Factors ranging from 0.23 to 0.41. The study validates that the system can operate within acceptable thermal limits (<80째C) and RAM usage (<15%), making it feasible for privacy-preserving educational applications. The approach balances the competing demands of accuracy, computational efficiency, and hardware constraints for on-device child ASR.

## Method Summary
The method involves adapting the Whisper ASR model for children's speech recognition through a two-stage process: fine-tuning and compression. The authors first fine-tune the Whisper tiny.en model (39M parameters) on the MyST corpus, which contains 210 hours of transcribed children's speech from grades 3-5 (ages 8-11). They explore four data versions through filtering: original data, filtered data (discarding utterances with >50% WER and <3 words), 30-second concatenated versions, and filtered 30-second versions. After fine-tuning with batch size 128 until convergence, they apply low-rank compression to the encoder using 500 calibration samples and a threshold of 0.999. The compressed model is then evaluated on a Raspberry Pi 5 for inference speed, computational complexity, and thermal performance.

## Key Results
- Fine-tuned 'tiny.en' model achieves 15.9% WER on MyST test set, improving to 11.8% WER with filtered data
- Compressed model reduces encoder size by 0.51 million parameters and increases inference speed by 1.26x on GPU
- Raspberry Pi inference requires approximately 2 GFLOPS fewer computations with Real-Time Factors of 0.23-0.41
- RAM usage remains within 15% and CPU temperature stays below thermal throttling limits (<80째C)

## Why This Works (Mechanism)
The approach works by leveraging the strong pre-trained representations in Whisper while adapting them specifically to the acoustic and linguistic characteristics of children's speech. Children's voices have distinct pitch ranges, pronunciation patterns, and speaking styles compared to adults, requiring specialized training data. The low-rank compression technique reduces model complexity while preserving the most important features learned during fine-tuning, enabling efficient inference on resource-constrained edge devices. The data filtering strategy improves quality by removing low-confidence or incomplete utterances, resulting in better generalization.

## Foundational Learning
**Whisper ASR Architecture**: A transformer-based encoder-decoder model pre-trained on multilingual and multitask data, including speech recognition, translation, and language identification. Why needed: Understanding the base model architecture is essential for knowing which components to fine-tune and compress. Quick check: Verify the model uses self-attention and feed-forward layers in both encoder and decoder.

**Low-Rank Compression**: A technique that approximates weight matrices using lower-dimensional representations to reduce parameters while maintaining functionality. Why needed: This is the primary method for achieving the target computational efficiency on edge devices. Quick check: Confirm the compression reduces parameters without catastrophic accuracy loss.

**Real-Time Factor (RTF)**: The ratio of processing time to audio duration, where RTF < 1 indicates real-time performance. Why needed: This metric determines whether the system can process live audio streams. Quick check: Measure RTF across different audio durations to ensure consistent real-time capability.

**Word Error Rate (WER)**: The standard metric for ASR accuracy, calculated as (substitutions + deletions + insertions) / total words. Why needed: This quantifies the trade-off between model size reduction and recognition accuracy. Quick check: Compare WER against baseline models to validate improvement.

**COPPA Compliance**: Children's Online Privacy Protection Act requirements that mandate parental consent and restrict data collection from children under 13. Why needed: This regulatory context drives the need for on-device processing rather than cloud-based solutions. Quick check: Verify all processing occurs locally without data transmission.

## Architecture Onboarding
**Component Map**: MyST corpus -> Data filtering (4 versions) -> Whisper tiny.en fine-tuning -> Low-rank compression -> Raspberry Pi inference

**Critical Path**: The most compute-intensive operations are the self-attention mechanisms in the encoder, which benefit most from low-rank compression. The fine-tuning process requires careful hyperparameter selection to avoid overfitting on the relatively small MyST dataset while capturing children's speech characteristics.

**Design Tradeoffs**: The study prioritizes computational efficiency and privacy over maximum accuracy, accepting a 15.9% WER to achieve real-time performance on edge hardware. The choice of Whisper tiny.en over larger variants reflects the data scarcity challenge - larger models degraded performance due to insufficient training data.

**Failure Signatures**: High WER (>25%) indicates insufficient adaptation to children's speech characteristics or inadequate filtering. RTF > 1.0 suggests the compression wasn't aggressive enough or the hardware isn't sufficient. CPU temperatures >80째C indicate thermal throttling that would degrade real-time performance.

**Three First Experiments**:
1. Fine-tune Whisper tiny.en on MyST corpus with batch size 128 and patience=5, measuring WER on test set
2. Apply low-rank compression with 500 calibration samples and threshold 0.999, measuring parameter reduction and WER change
3. Deploy compressed model to Raspberry Pi 5 and measure RTF, GFLOPS, RAM usage, and CPU temperature across 5s, 10s, 15s, and 20s audio samples

## Open Questions the Paper Calls Out
**Open Question 1**: Can alternative model compression techniques, such as quantization or pruning, achieve better WER retention for child speech than the low-rank approximation used in this study? The authors implemented one specific compression scheme which sacrificed accuracy for speed; they did not test if other "lightweight" methods could preserve the unique acoustic features of children better. Evidence to resolve: A comparison of WER and RTF on the Raspberry Pi between the low-rank compressed model and a quantized (e.g., 4-bit or 8-bit) version of the fine-tuned `tiny.en` model.

**Open Question 2**: How does the system perform in naturalistic, noisy environments (e.g., classrooms) compared to the simulated booth recording? The evaluation relies on the MyST corpus and a single speaker in a booth; the paper acknowledges noise issues but does not test the model's robustness to the "cocktail party" or background noise typical of educational settings. Evidence to resolve: WER evaluation of the compressed `tiny.en` model on a 'wild' dataset of children's speech recorded in actual home or school environments.

**Open Question 3**: Can data augmentation or synthetic data generation enable the successful fine-tuning of larger model variants (e.g., `small` or `base`) without the performance degradation observed in this study? The study identifies data scarcity as a bottleneck for using larger, potentially more robust models, but does not explore methods to artificially expand the 210-hour MyST dataset to support them. Evidence to resolve: Fine-tuning the `small.en` model using an augmented training set (e.g., with speed perturbation, noise injection, or voice conversion) and measuring if WER improves over the zero-shot baseline.

## Limitations
The low-rank compression method lacks implementation details, referencing external work without specifying layer-wise rank selection or calibration procedures. Exact training hyperparameters including learning rate, optimizer, and total training duration are unspecified. The evaluation environment uses simulated conditions rather than real-world noisy educational settings, limiting generalizability to classroom deployments.

## Confidence
**High Confidence**: The feasibility demonstration of running child ASR on Raspberry Pi with acceptable RTF (0.23-0.41) and thermal performance (<80째C) is well-supported by measured metrics and direct experimental validation on the target hardware.

**Medium Confidence**: The WER improvements from filtering strategies (15.9% to 11.8%) and fine-tuning effectiveness are reasonably supported, though exact replication depends on precise implementation of data filtering criteria and training hyperparameters.

**Low Confidence**: The compression methodology claims (1.26x speedup, 2 GFLOPS reduction) cannot be independently verified due to insufficient technical detail about the low-rank compression implementation and calibration procedure.

## Next Checks
1. **Reproduce the F1 filtering pipeline**: Implement the exact data filtering procedure using Whisper Large V2 with documented beam search parameters, temperature settings, and decoding configuration to verify that utterances with >50% WER are consistently identified and removed.

2. **Validate low-rank compression implementation**: Obtain and document the specific low-rank compression method from LiteASR [29], including layer-wise rank selection criteria, calibration sample requirements, and threshold application procedure, then verify the reported parameter reduction (0.51M) and performance improvements (1.26x speedup).

3. **Measure RTF and GFLOPS on identical hardware**: Conduct inference timing and computational complexity measurements on Raspberry Pi 5 using the same audio duration samples (5s, 10s, 15s, 20s) and confirm the reported RTF range (0.23-0.41) and GFLOPS reduction (~2 GFLOPS) while monitoring CPU temperature to ensure no thermal throttling occurs.