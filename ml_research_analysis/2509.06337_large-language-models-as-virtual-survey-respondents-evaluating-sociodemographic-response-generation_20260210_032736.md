---
ver: rpa2
title: 'Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic
  Response Generation'
arxiv_id: '2509.06337'
source_url: https://arxiv.org/abs/2509.06337
tags:
- survey
- performance
- language
- llama
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates using Large Language Models (LLMs) to
  simulate virtual survey respondents for scalable and cost-effective sociodemographic
  data collection. Two novel simulation settings are proposed: Partial Attribute Simulation
  (PAS) for predicting missing attributes from incomplete profiles, and Full Attribute
  Simulation (FAS) for generating complete synthetic datasets with or without contextual
  constraints.'
---

# Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation

## Quick Facts
- arXiv ID: 2509.06337
- Source URL: https://arxiv.org/abs/2509.06337
- Reference count: 40
- This paper investigates using Large Language Models (LLMs) to simulate virtual survey respondents for scalable and cost-effective sociodemographic data collection

## Executive Summary
This study proposes using Large Language Models (LLMs) as virtual survey respondents to generate synthetic sociodemographic data. The research introduces two novel simulation settings - Partial Attribute Simulation (PAS) for predicting missing attributes from incomplete profiles, and Full Attribute Simulation (FAS) for generating complete synthetic datasets. A comprehensive benchmark suite spanning 11 real-world datasets across four domains is developed to evaluate mainstream LLMs under zero-shot and few-shot conditions. The results demonstrate that context-aware prompts and reasoning-oriented models can improve prediction accuracy, though structured output generation remains challenging for certain model architectures.

## Method Summary
The research establishes a comprehensive evaluation framework for LLM-based sociodemographic simulation through two distinct settings: PAS predicts missing attributes from partial profiles using conditional generation, while FAS generates complete synthetic datasets either independently or with contextual constraints. The study curates the LLM-S3 benchmark suite covering 11 real-world datasets across four domains (employment, housing, healthcare, and finance) to evaluate mainstream LLMs including GPT-3.5/4 Turbo and LLaMA 3.0/3.1-8B. Performance is assessed through zero-shot and few-shot evaluation protocols using standard metrics like accuracy, precision, and recall. The methodology emphasizes context-aware prompting and explores reasoning-oriented distilled models to enhance prediction quality.

## Key Results
- GPT-4 Turbo consistently outperforms other models across both simulation settings and evaluation protocols
- Context-aware prompts and reasoning-oriented distilled models significantly improve prediction accuracy
- LLaMA models exhibit higher failure rates in structured output generation during Full Attribute Simulation
- Zero-shot and few-shot evaluation settings show consistent performance trends across different model architectures

## Why This Works (Mechanism)
The mechanism leverages LLMs' ability to capture complex attribute relationships through pretraining on diverse text corpora, enabling them to infer missing sociodemographic information from contextual patterns. By treating survey response generation as a conditional text generation task, LLMs can probabilistically complete profiles based on learned statistical correlations between attributes. The context-aware prompting strategy guides the model's reasoning process by providing relevant domain knowledge and structural constraints. The reasoning-oriented distilled models benefit from fine-tuning on specific sociodemographic reasoning tasks, improving their ability to handle complex attribute interdependencies that simple pattern matching would miss.

## Foundational Learning

### Conditional Generation
- **Why needed**: Enables prediction of missing attributes based on available information
- **Quick check**: Can the model generate coherent outputs given partial input profiles?

### Structured Output Constraints
- **Why needed**: Ensures generated data adheres to survey format requirements
- **Quick check**: Does the model maintain required output schema across different datasets?

### Zero-shot vs Few-shot Learning
- **Why needed**: Evaluates model's ability to generalize without extensive task-specific training
- **Quick check**: Compare performance differences between no examples and limited examples

### Context-aware Prompting
- **Why needed**: Guides model reasoning by providing relevant domain knowledge
- **Quick check**: Measure accuracy improvements when contextual information is added to prompts

### Attribute Interdependency Modeling
- **Why needed**: Captures complex relationships between sociodemographic variables
- **Quick check**: Assess whether predictions respect known correlations between attributes

## Architecture Onboarding

### Component Map
LLM Framework -> Benchmark Suite -> Simulation Settings -> Evaluation Metrics -> Performance Analysis

### Critical Path
1. Load dataset from LLM-S3 benchmark
2. Apply simulation setting (PAS or FAS) with appropriate prompts
3. Generate predictions using target LLM
4. Evaluate outputs against ground truth using accuracy/precision/recall
5. Analyze performance patterns across models and datasets

### Design Tradeoffs
- Zero-shot evaluation favors model generalization but may sacrifice accuracy
- Few-shot evaluation improves performance but requires labeled examples
- Context-aware prompting increases computational cost but enhances reasoning quality
- Structured output constraints reduce flexibility but ensure data quality

### Failure Signatures
- LLaMA models failing to maintain output structure in FAS
- Incorrect attribute predictions when contextual information is missing
- Overfitting to specific dataset patterns in few-shot settings
- Inconsistent performance across different sociodemographic domains

### First Experiments to Run
1. Compare zero-shot vs few-shot performance on a single dataset
2. Test context-aware prompting impact on prediction accuracy
3. Evaluate structured output generation reliability across all models

## Open Questions the Paper Calls Out

## Limitations
- Evaluation framework relies on synthetic benchmarks rather than live survey deployment
- Performance metrics may not fully capture nuanced quality of sociodemographic predictions
- Structured output generation challenges for LLaMA models require further investigation
- Cultural generalizability across different sociodemographic contexts remains uncertain

## Confidence
- Medium for simulation performance claims: Consistent trends across models provide reasonable confidence in relative rankings
- Low for structured output generation in FAS: Higher failure rates in LLaMA models not fully explained
- Medium for context-aware improvements: Demonstrated benefits need further validation across diverse contexts

## Next Checks
1. Test simulation models on live survey data where ground truth is available post-collection, comparing synthetic predictions against actual human responses
2. Evaluate model performance across different cultural contexts and languages beyond current benchmark datasets
3. Conduct multi-time-point evaluations to assess whether simulated predictions remain consistent over time or degrade