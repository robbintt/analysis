---
ver: rpa2
title: 'Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning
  of LLM Agents'
arxiv_id: '2510.24702'
source_url: https://arxiv.org/abs/2510.24702
tags:
- data
- agent
- datasets
- training
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present ADP (Agent Data Protocol), a standardized representation
  language designed to unify heterogeneous agent training datasets into a common format.
  The core method involves converting 13 existing agent datasets into ADP format and
  then converting them into training-ready formats for multiple agent frameworks (OpenHands,
  SWE-Agent, AgentLab).
---

# Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents

## Quick Facts
- arXiv ID: 2510.24702
- Source URL: https://arxiv.org/abs/2510.24702
- Reference count: 21
- The ADP framework achieves up to 40.3% accuracy on SWE-Bench Verified, 22.9% on WebArena, and 34.7% on AgentBench OS through unified dataset training

## Executive Summary
The paper introduces ADP (Agent Data Protocol), a standardized representation language that unifies heterogeneous agent training datasets into a common format. By converting 13 existing agent datasets into ADP format and then into training-ready formats for multiple agent frameworks, the authors demonstrate significant performance improvements across diverse agent tasks. The framework reduces the integration effort from quadratic to linear scaling and enables cross-task transfer benefits, with ADP-trained models outperforming base models by an average of 20% across benchmarks.

## Method Summary
The core method involves a three-stage pipeline: Raw Dataset → ADP Converter → ADP Store → ADP-to-SFT Converter → Training-ready JSON → LLaMA-Factory SFT → Fine-tuned Agent Model. The ADP schema defines structured trajectories with typed Actions (API, Code, Message) and Observations (Text, Web). The authors converted 13 heterogeneous datasets into ADP format and then trained models on the unified dataset, evaluating across multiple agent frameworks including OpenHands, SWE-Agent, and AgentLab.

## Key Results
- SWE-Bench Verified: ADP-trained models achieve up to 40.3% accuracy (vs 2.2% for base models)
- WebArena: Up to 22.9% accuracy (vs 10.9% for base models)
- AgentBench OS: Up to 34.7% accuracy (vs 9.0% for base models)
- ADP data consistently outperforms single-domain training on all benchmarks
- Cross-task transfer benefits observed when training on diverse dataset mixtures

## Why This Works (Mechanism)

### Mechanism 1: Decoupling via Intermediate Representation
ADP acts as a "hub-and-spoke" interlingua, reducing integration effort from quadratic $O(D \times A)$ to linear $O(D+A)$ by requiring only one converter per dataset and one per agent. This works because the ADP schema is expressive enough to capture semantic intent across diverse formats without catastrophic information loss.

### Mechanism 2: Cross-Task Transfer via Data Diversity
Training on a unified mixture of diverse agent tasks (coding, browsing, tool use) yields better generalization than task-specific tuning by exposing the model to wider reasoning patterns and action spaces, preventing overfitting and enabling transferable agentic behaviors.

### Mechanism 3: Structural Preservation of Agentic States
Granular Pydantic schemas for Actions and Observations preserve structural nuances required for specific agent architectures, allowing precise mapping to agent-specific interfaces through explicit typing of action types (e.g., CodeAction vs APIAction).

## Foundational Learning

**Supervised Fine-Tuning (SFT) of Agents**: Agent SFT requires learning sequential decision-making (trajectories) rather than single-shot input-output pairs. *Why needed*: This is the core task. *Quick check*: Can you explain why training on a trajectory differs fundamentally from training on a single instruction-response pair?

**Pydantic Schemas & Data Validation**: ADP is implemented as Pydantic schemas. *Why needed*: Understanding typed data structures and validation is required to extend or query the protocol. *Quick check*: If you receive a raw JSON log where an "observation" field is missing, how would strict Pydantic validation handle it versus a lenient parser?

**Agent-Computer Interface (ACI)**: The paper targets frameworks like SWE-Agent and OpenHands, which define specific ways an LLM interacts with a terminal or filesystem. *Why needed*: These interfaces determine how actions are formatted and executed. *Quick check*: Why would a CodeAction be formatted differently for an IPython-based agent compared to a pure Bash-based agent?

## Architecture Onboarding

**Component map**: Raw Dataset → Raw-to-ADP Converter → ADP Store → ADP-to-SFT Converter → Training-ready JSON → LLaMA-Factory SFT → Fine-tuned Agent Model → Agent Harness

**Critical path**: The ADP Schema Definition (Trajectory, Action, Observation) is the bottleneck. If the schema cannot represent a new dataset's modality, the entire pipeline stalls.

**Design tradeoffs**: Simplicity vs. Expressiveness. The authors chose a "lightweight" schema (Text/Web Obs, API/Code/Msg Action) to reduce complexity but may struggle with highly specialized modalities not explicitly covered.

**Failure signatures**:
- Schema Drift: Training data passes validation, but generated actions fail at inference because the target agent's API changed
- Silent Data Loss: Complex raw observations are truncated or flattened during Raw→ADP conversion, depriving the model of necessary context

**First 3 experiments**:
1. Unit Test Conversion: Take one trajectory from "SWE-smith" dataset, convert to ADP, then to SWE-Agent format, and verify executable code blocks are preserved exactly
2. Overfitting Probe: Fine-tune a small model (1B params) on a single ADP dataset and evaluate on its target task to establish a baseline before mixing data
3. Ablation on Schema: Train two models—one using full WebObservation and one using only TextObservation—and compare on WebArena to measure the value of structured web data

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- Multimodal extensions remain unexplored, with ADP currently limited to text and basic web observations
- No empirical validation of information preservation across the two-stage conversion pipeline
- Lack of theoretical explanation for why certain task combinations yield positive transfer

## Confidence
**High**: Performance improvements are empirically demonstrated across multiple benchmarks
**Medium**: The mechanism claims are supported by experimental results but lack ablation studies for transfer effects
**Low**: No validation of information preservation through the conversion pipeline

## Next Checks
1. Reproduce the three-stage conversion pipeline on a subset of datasets and verify structural preservation
2. Conduct ablation studies varying data mixture compositions to identify predictive features for transfer success
3. Implement a controlled experiment training models on raw vs. ADP-converted versions of the same dataset to validate information preservation