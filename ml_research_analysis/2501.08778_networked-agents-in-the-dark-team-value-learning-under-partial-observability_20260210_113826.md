---
ver: rpa2
title: 'Networked Agents in the Dark: Team Value Learning under Partial Observability'
arxiv_id: '2501.08778'
source_url: https://arxiv.org/abs/2501.08778
tags:
- agents
- consensus
- learning
- critic
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach for networked agents to learn
  cooperative behavior under partial observability. The key idea is to use a consensus
  mechanism to approximate a team value function from local observations and rewards,
  enabling agents to perform local gradient updates in the direction of the team advantage.
---

# Networked Agents in the Dark: Team Value Learning under Partial Observability

## Quick Facts
- **arXiv ID**: 2501.08778
- **Source URL**: https://arxiv.org/abs/2501.08778
- **Reference count**: 40
- **Primary result**: DNA-MARL enables networked agents to learn cooperative behavior under partial observability by approximating team value functions through local communication and consensus mechanisms

## Executive Summary
This paper addresses the challenge of cooperative multi-agent reinforcement learning under partial observability, where agents must work together without centralized training or full state information. The authors propose DNA-MARL (Double Networked Averaging Multi-Agent Reinforcement Learning), which combines team value consensus with parameter consensus to approximate a global team value function from local observations and rewards. Through iterative communication and averaging, agents learn to reinforce actions that benefit the team rather than just themselves. The method is validated on benchmark MARL scenarios including Level-Based Foraging and Multi-agent Particle Environments, demonstrating superior performance to previous methods in both on-policy and off-policy settings.

## Method Summary
DNA-MARL builds on actor-critic architecture where each agent maintains local policy and value networks. The key innovation is using consensus averaging to approximate a team advantage function: agents compute local temporal difference targets, then perform K rounds of averaging with neighbors to obtain a consensus-based team value estimate. This consensus target is used for local policy gradient updates. Additionally, agents periodically average their neural network parameters (actor and critic) with neighbors to share learning across the team. The approach works in a decentralized, temporally extended setting where agents only communicate through a switching communication graph, making it suitable for environments where centralized training is impractical.

## Key Results
- DNA-MARL outperforms previous methods in both on-policy (DNAA2C) and off-policy (DNAQL) settings
- The method achieves performance matching centralized training approaches while maintaining decentralized execution
- Parameter consensus combined with team value consensus improves performance in 3 out of 4 tested tasks
- DNA-MARL generalizes to deep Q-network algorithms, extending beyond actor-critic methods

## Why This Works (Mechanism)

### Mechanism 1: Team Value Consensus
Agents approximate a global team value function using only local rewards and neighbor communication. Each agent computes a local TD target based on its individual reward, then performs K rounds of consensus averaging with neighbors. This propagates reward information across the network, resulting in a consensus-based approximation of the team advantage used for local gradient updates. The communication graph must be "jointly connected" over time for convergence.

### Mechanism 2: Parameter Consensus
Periodically averaging neural network parameters across agents improves sample efficiency and performance stability. In addition to averaging value targets, agents average their actor and critic weights with neighbors, allowing them to benefit from the diverse experiences of the whole team. This requires homogeneous agents with matching observation/action spaces.

### Mechanism 3: Team Policy Gradient Factorization
The global team objective can be decomposed into a sum of individual expected returns, allowing local gradient updates to optimize the global objective. By using the approximated team advantage as the scaling factor in the local policy gradient, agents reinforce actions that benefit the team rather than just the individual.

## Foundational Learning

- **Consensus Algorithms**: Linear iterative averaging that converges to the network mean; needed to understand how local values become global estimates. Quick check: If Node A has value 10 and Node B has value 0, what is their value after one consensus step with weight 0.5? (Answer: 5)
- **Actor-Critic Architecture**: Framework distinguishing policy (actor) from value (critic) networks; needed to implement the "Double" averaging correctly. Quick check: Which network estimates the value of a state, and which network selects the action?
- **Partial Observability in Markov Games**: Framework where agents see local observations, not true states; needed to appreciate why centralized critics are impossible and approximation is required. Quick check: Why can't an agent simply use the true state to calculate its Q-value in this framework?

## Architecture Onboarding

- **Component map**: Agents (Actor-Critic units) -> Local trajectory buffers -> Communication Module (consensus logic) -> Environment (generates observations and rewards)
- **Critical path**: Rollout (agents interact, store trajectories) -> Target Calculation (compute local TD targets) -> Team Consensus (average targets with neighbors for K rounds) -> Update (local gradient descent using consensus target) -> Parameter Consensus (average weights with neighbors every I episodes)
- **Design tradeoffs**: K (higher = better approximation but slower), I (frequent = faster convergence but higher bandwidth), Homogeneity (required for parameter consensus)
- **Failure signatures**: Diverging loss (heterogeneous observations), no cooperation (agents act selfishly), non-stationarity (performance oscillates)
- **First 3 experiments**: 1) Ablation Study (DNA-MARL vs. "Distributed-V" on LBF Easy), 2) Connectivity Test (vary edge count C), 3) Scale Test (increase number of agents N)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can DNA-MARL be combined with multi-agent belief systems to enhance decision-making under partial observability?
- Basis in paper: The conclusion states the method should be combined with multi-agent belief systems
- Why unresolved: Current method is strictly model-free without joint belief state estimation
- Evidence: Hybrid algorithm integrating belief tracking into team-value consensus mechanism

### Open Question 2
- Question: How can DNA-MARL framework be adapted to support agents with heterogeneous observation or action spaces?
- Basis in paper: Section 4.1.4 notes parameter consensus requires homogeneous observation and action spaces
- Why unresolved: Current reliance on parameter averaging prevents application to diverse teams
- Evidence: Modified DNA-MARL with distinct parameter updates or network structures for heterogeneous agents

### Open Question 3
- Question: What is the theoretical lower bound on consensus steps (K) required for convergence?
- Basis in paper: Section 4.1.3 states empirical testing was done but no theoretical derivation provided
- Why unresolved: Trade-off between communication cost and accuracy is currently experimentally determined
- Evidence: Formal analysis establishing convergence rate relative to K, or empirical failure threshold identification

## Limitations

- Relies on "jointly connected" communication graph, which may be unrealistic in dynamic environments
- Requires homogeneous agents with matching observation/action spaces for parameter consensus
- Limited experimental validation to grid-based and particle-world environments only
- Convergence guarantees depend on idealized conditions that may not hold in practice

## Confidence

**High Confidence** (well-supported):
- Mathematical decomposition of team gradients into individual components
- Consensus algorithm convergence under jointly connected topology
- Empirical advantage over baselines in LBF and MPE environments

**Medium Confidence** (some gaps):
- K=5 consensus rounds sufficient for all network sizes (no ablation shown)
- Parameter consensus significantly improves performance across all tasks (3/4 tasks shown)
- Generalization to off-policy methods (DNAQL results less comprehensive)

**Low Confidence** (limited evidence):
- Scalability to large networks without increasing K (no experiments with N>5)
- Robustness to communication failures or delays (no stress tests)
- Computational overhead compared to centralized methods in wall-clock time

## Next Checks

1. **Graph Robustness Test**: Systematically vary communication graph connectivity from fully connected to sparse, measuring performance degradation to identify minimum connectivity threshold for cooperative behavior.

2. **Heterogeneity Stress Test**: Modify LBF environment so agents have different observation dimensions or action spaces, then evaluate whether parameter consensus provides benefits or causes divergence.

3. **Consensus Round Sensitivity**: Conduct ablation study varying K from 1 to 10 rounds, measuring both performance and sample efficiency to identify optimal tradeoff between communication cost and learning quality.