---
ver: rpa2
title: 'VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual
  Documents'
arxiv_id: '2602.00122'
source_url: https://arxiv.org/abs/2602.00122
tags:
- image
- editing
- text
- instruction
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VDE Bench is a new benchmark for evaluating image editing models
  on multilingual and complex visual documents. It includes high-quality datasets
  with densely textual documents in both English and Chinese, such as academic papers,
  posters, and newspapers.
---

# VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents

## Quick Facts
- **arXiv ID**: 2602.00122
- **Source URL**: https://arxiv.org/abs/2602.00122
- **Reference count**: 28
- **Primary result**: VDE Bench is a new benchmark for evaluating image editing models on multilingual visual document editing, showing significant performance gaps between English and Chinese text editing.

## Executive Summary
VDE Bench is a comprehensive benchmark for evaluating image editing models on multilingual and complex visual documents, such as academic papers, posters, and newspapers. The benchmark introduces a decoupled evaluation framework that assesses editing performance at both global (full-image) and local (cropped edit region) levels, using metrics like IoU, CDM, BLEU-4, and TEDS-like. Experiments with multiple mainstream image editing models demonstrate that all models struggle more with Chinese text and dense layouts compared to English, with text addition being a particularly challenging task. Manual verification confirms strong consistency between human judgments and automated metrics, validating the reliability of the benchmark.

## Method Summary
VDE Bench evaluates image editing models on multilingual visual document editing tasks including text replacement, addition, and deletion while preserving style and background. The benchmark uses 674 human-verified samples from the Omni Doc Bench source, covering documents in English, Chinese, and mixed languages. Evaluation employs a decoupled framework using PaddleOCR-VL for OCR parsing, with greedy Euclidean-distance matching between predicted and ground truth blocks. Metrics include IoU for spatial localization and CDM (normalized Levenshtein), BLEU-4, and TEDS-like for text correctness, evaluated at both global and local levels.

## Key Results
- All evaluated models show significantly worse performance on Chinese text editing compared to English
- Text addition tasks are a critical bottleneck, with models struggling to maintain layout and formatting when inserting new content
- Step1X achieves high global IoU but low local edit accuracy, while Qwen shows the opposite pattern, indicating a fundamental trade-off in current approaches

## Why This Works (Mechanism)
The benchmark works by decoupling global layout preservation from local text modification accuracy, allowing fine-grained analysis of model capabilities. By evaluating at both image and cropped edit region levels, it reveals whether models are genuinely editing text content or simply preserving background appearance. The use of multiple text similarity metrics (CDM, BLEU-4, TEDS-like) provides comprehensive assessment of different aspects of text editing quality.

## Foundational Learning
- **OCR parsing with PaddleOCR-VL**: Needed for extracting text regions and content from images; quick check: verify OCR accuracy on benchmark documents
- **Greedy Euclidean-distance matching**: Used to align predicted and ground truth text blocks; quick check: validate matching accuracy on sample data
- **Text similarity metrics (CDM, BLEU-4, TEDS-like)**: Required for quantifying text editing quality; quick check: compute metrics on controlled test cases
- **Multi-level evaluation framework**: Global vs local assessment needed to distinguish layout preservation from content modification; quick check: compare metric distributions across levels

## Architecture Onboarding
- **Component map**: Document images -> Image editing models -> Edited outputs -> PaddleOCR-VL parsing -> Greedy matching -> Metric computation
- **Critical path**: Image input → Model inference → OCR parsing → Block matching → Metric calculation
- **Design tradeoffs**: Global metrics favor background preservation while local metrics emphasize text modification accuracy
- **Failure signatures**: High global IoU with low local metrics indicates background preservation without actual text editing
- **First experiments**:
  1. Test baseline model on simple text replacement task
  2. Evaluate model on Chinese-only document
  3. Compare global vs local metric performance on same sample

## Open Questions the Paper Calls Out
- Can layout-aware generation mechanisms resolve the "text addition" bottleneck where models fail to reflow dense surrounding content? Current models excel at replacement/deletion but produce low-quality results when additions force significant layout shifts.
- How does the significant performance gap between English and Chinese editing generalize to other complex, non-Latin scripts? The benchmark demonstrates difficulty with Chinese characters but doesn't test scripts with different typographic challenges.
- Is the trade-off between global layout preservation (IoU) and local text fidelity (CDM) inherent to current diffusion priors or a solvable optimization issue? Models show a dichotomy where they either preserve background rigidly or edit aggressively at the cost of layout consistency.

## Limitations
- Ground truth bounding boxes for modified regions are not publicly released, requiring manual annotation with three-group consensus protocol
- Exact inference settings for Gemini 2.5-Flash-Image used in data generation and evaluation are unspecified
- PaddleOCR-VL configuration and post-processing thresholds are not documented, potentially affecting metric computation

## Confidence
- **High**: Benchmark design and evaluation framework methodology
- **Medium**: Generalizability of findings due to unspecified model versions and OCR settings
- **High**: Claims about performance differences across languages and document types

## Next Checks
1. Obtain or reconstruct the ground truth bounding box annotations using the three-group consensus protocol and verify IoU threshold application
2. Clarify and document exact inference settings (model version, guidance scale, steps, seed) for Gemini 2.5-Flash-Image used in data generation and evaluation
3. Specify and reproduce PaddleOCR-VL configuration and post-processing thresholds to ensure consistent OCR parsing and metric computation