---
ver: rpa2
title: 'MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large
  Language Models?'
arxiv_id: '2506.13065'
source_url: https://arxiv.org/abs/2506.13065
tags:
- question
- reasoning
- behavior
- motivation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MotiveBench is a new benchmark for evaluating LLMs\u2019 human-like\
  \ motivational reasoning. It includes 200 rich scenarios and 600 questions covering\
  \ Maslow\u2019s hierarchy and 16 basic desires."
---

# MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?

## Quick Facts
- arXiv ID: 2506.13065
- Source URL: https://arxiv.org/abs/2506.13065
- Reference count: 40
- Even GPT-4o achieves only 80.89% accuracy, falling short of human-level motivational reasoning

## Executive Summary
MotiveBench is a new benchmark for evaluating large language models' human-like motivational reasoning across 200 rich scenarios and 600 questions. It covers Maslow's hierarchy and 16 basic desires, testing models on three task types: Motivational Reasoning, Behavioral Reasoning, and Motive&Behavior Reasoning. Experiments with 29 models across seven families show that even the best models like GPT-4o achieve only 80.89% accuracy, with smaller models performing significantly worse at 57.16% average. The benchmark reveals that LLMs struggle especially with "love & belonging" needs and tend toward excessive rationality and idealism, demonstrating that current models are still significantly distant from human-level motivational reasoning.

## Method Summary
The benchmark evaluates LLMs using 600 multiple-choice questions across three reasoning tasks derived from 200 scenarios sourced from Persona-Hub, Amazon reviews, and Blogger posts. Each scenario is associated with three questions, and accuracy is computed by requiring all three answers to be correct per scenario. The evaluation uses both Base (vanilla) and Chain-of-Thought prompting, with results averaged over six option permutations to control for position bias. The benchmark covers Maslow's five need levels and Reiss's 16 basic desires, with average context length of ~96 tokens. Open-source models are evaluated using vLLM with temperature=0, while closed-source models use Azure OpenAI API, with experiments run on 4× A100 80GB GPUs.

## Key Results
- GPT-4o achieves only 80.89% accuracy, well below the 97.4% human baseline
- Smaller models average just 57.16% accuracy across all task types
- Chain-of-thought prompting reduces accuracy by ~6.88% for models with ≤34B parameters
- Models struggle most with "love & belonging" needs (68.32% accuracy vs 76.53% for self-actualization)

## Why This Works (Mechanism)
MotiveBench works by creating controlled scenarios that isolate specific motivational reasoning patterns, allowing systematic evaluation of how well models can infer human motivations from behavioral contexts. The three-task structure (reasoning about motives, behaviors, and their relationships) provides comprehensive coverage of the motivational reasoning process, while the MCQ format with permutation controls enables reliable quantitative comparison across diverse model families.

## Foundational Learning
- Motivational reasoning frameworks (Maslow's hierarchy, Reiss's 16 desires): Why needed - provides theoretical foundation for benchmark categories; Quick check - verify 5 Maslow levels and 16 desires are correctly represented
- Multiple-choice question design: Why needed - enables controlled evaluation and automated scoring; Quick check - confirm 600 questions with 6 options each
- Chain-of-thought prompting methodology: Why needed - tests whether explicit reasoning improves motivational inference; Quick check - verify CoT template and implementation
- Scenario-based evaluation: Why needed - captures contextual reasoning rather than isolated facts; Quick check - confirm 200 scenarios average ~96 tokens
- Permutation-based scoring: Why needed - eliminates position bias in multiple-choice options; Quick check - verify all 6 permutations are averaged

## Architecture Onboarding
**Component Map:** Scenario generation -> Question creation -> Model inference -> Scoring (permutations) -> Human evaluation
**Critical Path:** Scenarios → Questions → Model evaluation → Accuracy computation (all 3 questions per scenario must be correct)
**Design Tradeoffs:** MCQ format enables automated scoring but may constrain reasoning paths; fixed 6 options control bias but limit answer diversity
**Failure Signatures:** CoT prompting reduces accuracy (avg -6.88%); lowest performance on "love & belonging" needs (68.32%); smaller models show 23.73% gap vs GPT-4o
**First 3 Experiments:** 1) Run Base prompt evaluation on GPT-4o to verify 80.89% accuracy; 2) Compare Base vs CoT accuracy for a 7B model to observe the drop; 3) Check per-category accuracy to confirm "love & belonging" as lowest-performing category

## Open Questions the Paper Calls Out
1. Can a fully automated benchmark generation pipeline produce questions matching human-verified quality? The current pipeline requires 6 minutes per question for manual verification, limiting scalability and risking data contamination.
2. Can LLMs demonstrate human-like motivational reasoning in dynamic, multi-step simulation environments rather than single-turn question-answering? Current benchmark uses "situational question-answering" paradigm that deviates from real-world social activities.
3. What architectural modifications would improve LLM performance on "love and belonging" motivational reasoning where models currently show lowest accuracy? Models excel at surface-level emotional language but lack deep causal reasoning for emotional needs.
4. Why does chain-of-thought prompting reduce accuracy on motivational reasoning tasks, and can alternative strategies preserve reasoning benefits? CoT prioritizes logical coherence over situational cues and intuitive judgments.

## Limitations
- Prompt templates and exact model configurations are incompletely documented, affecting reproducibility
- Single human evaluation round with 31 participants may not capture full human reasoning diversity
- MCQ format may not fully represent real-world motivational reasoning complexity
- Interpretation that low "love & belonging" performance reflects "excessive rationality" remains speculative without deeper qualitative analysis

## Confidence
- CoT reduces accuracy: High confidence (robust pattern across multiple model families)
- LLMs distant from human-level reasoning: High confidence (supported by 97.4% human baseline vs 80.89% best model)
- Struggle with "love & belonging" needs: High confidence (consistent across all model families)
- Interpretation of performance gaps: Medium confidence (mechanism not fully established)

## Next Checks
1. Replicate benchmark evaluation using exact prompt templates from Appendix F and verify the ~6.88% average accuracy drop for CoT prompting across multiple model families
2. Conduct independent human evaluation with larger, more diverse participant pool to validate 97.4% human accuracy baseline
3. Perform ablation studies removing MCQ format constraints to assess whether reasoning patterns persist in open-ended motivational reasoning tasks