---
ver: rpa2
title: 'LLM-ProS: Analyzing Large Language Models'' Performance in Competitive Problem
  Solving'
arxiv_id: '2502.04355'
source_url: https://arxiv.org/abs/2502.04355
tags:
- problems
- llms
- performance
- reasoning
- icpc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-ProS evaluates five large language models (LLM) on 166 ICPC
  World Finals problems (2011-2024) using a novel performance analyzer. The study
  assesses models including GPT-4o, Mistral Large, Llama-3.1-405B, and the o1 family
  (o1-mini, o1-preview) on accuracy, resource utilization, and reasoning.
---

# LLM-ProS: Analyzing Large Language Models' Performance in Competitive Problem Solving

## Quick Facts
- arXiv ID: 2502.04355
- Source URL: https://arxiv.org/abs/2502.04355
- Reference count: 35
- Five LLMs evaluated on 166 ICPC World Finals problems (2011-2024) using pass@1 metric

## Executive Summary
LLM-ProS evaluates five large language models on 166 ICPC World Finals competitive programming problems using a novel performance analyzer. The study finds that OpenAI's o1 models (o1-mini, o1-preview) significantly outperform other models, achieving 25.0% accuracy on historical problems and 15.4-7.7% on unseen 2024 problems, while GPT-4o, Mistral Large, and Llama-3.1-405B show 0% accuracy. The research highlights the importance of contamination-free benchmarks and advanced reasoning strategies for technical problem-solving.

## Method Summary
The study uses 166 ICPC World Finals problems (2011-2024) scraped from the official ICPC website, standardized into prompts, and submitted to Codeforces Gym for automated verdicts. Five LLMs are evaluated in zero-shot pass@1 settings: GPT-4o, Mistral Large, Llama-3.1-405B, and o1 family models (o1-mini, o1-preview). Performance metrics include accuracy, resource utilization, and verdict distributions (AC, WA, TLE, RE, CE, MLE).

## Key Results
- o1 models achieve 25.0% accuracy on historical problems (2011-2023) vs. 0% for other models
- o1-mini and o1-preview achieve 15.4% and 7.7% accuracy on unseen 2024 problems
- GPT-4o shows highest compile error rate at 24.7%, followed by Llama-3.1 at 19.88%

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Reasoning Specialization
- Claim: CoT-trained models demonstrate superior performance on competitive programming tasks
- Mechanism: CoT models decompose problems into sequential sub-tasks for step-by-step logical derivation
- Core assumption: Superior performance stems from CoT training methodology, not architectural scale
- Evidence: o1 models achieve 25.0% accuracy vs. 0% for larger models without CoT training
- Break condition: If general-purpose models with similar parameter counts achieve comparable accuracy

### Mechanism 2: Dataset Contamination Detection
- Claim: Performance disparity between historical and unseen problems indicates memorization
- Mechanism: Contamination-free benchmarks reveal true generalization capability
- Core assumption: 0% accuracy on unseen 2024 problems reflects inability to generalize
- Evidence: o1-preview achieves 25.0% on 2017 problems but drops to 7.7% on 2024 problems
- Break condition: If non-o1 models achieve >0% accuracy on contamination-free benchmarks

### Mechanism 3: Verdict Distribution as Diagnostic Signal
- Claim: Compile Error rates serve as proxy for syntactic correctness and implementation reliability
- Mechanism: CoT-trained models generate more syntactically correct code, reducing CE frequency
- Core assumption: CE verdicts primarily reflect code generation quality
- Evidence: GPT-4o has 24.7% CE rate vs. o1-mini at 10 CE (16 AC) and o1-preview at 19 CE (15 AC)
- Break condition: If CE rates converge across model types under controlled prompting

## Foundational Learning
- Concept: pass@1 Evaluation
  - Why needed: Study uses pass@1 as primary metric for zero-shot generalization
  - Quick check: Can you explain why pass@1 is stricter than pass@k for evaluating LLM reasoning?
- Concept: Data Contamination in Benchmarks
  - Why needed: Performance variability attributed to overlap between problems and training data
  - Quick check: How would you design an experiment to isolate contamination effects?
- Concept: Chain-of-Thought Prompting
  - Why needed: CoT identified as primary differentiator between o1 models and alternatives
  - Quick check: What's the difference between CoT prompting at inference vs. CoT-specialized training?

## Architecture Onboarding
- Component map: Data Collection -> Data Preprocessing -> Model Testing -> Solution Generation & Submission
- Critical path: 1. Preprocess problems into standardized prompts 2. Generate solutions via model APIs 3. Submit to Codeforces Gym 4. Collect verdicts and resource metrics
- Design tradeoffs: Zero-shot evaluation limits capability exploration but ensures fair comparison; single-platform submission introduces platform-specific constraints but ensures consistency
- Failure signatures: High CE rates (>20%) indicate syntactic failure; 0% AC on unseen 2024 problems suggests lack of generalization; dominance of WA verdicts indicates reasoning errors
- First 3 experiments: 1. Replicate pass@1 evaluation on 2024 problems to validate contamination control 2. Introduce few-shot prompting to measure performance delta 3. Isolate error categories by manually inspecting WA solutions

## Open Questions the Paper Calls Out
None

## Limitations
- Contamination control relies on single-year (2024) unseen problems; broader temporal validation needed
- Zero-shot evaluation limits exploration of potential performance gains via fine-tuning
- Single-platform submission introduces potential platform-specific biases
- Verdict interpretation assumes CE rates reflect model capability rather than template compatibility

## Confidence
- High confidence in o1 models' superior performance on historical problems (2011-2023)
- Medium confidence in attributing performance to CoT reasoning specialization vs. architectural scale
- Medium confidence in contamination-free benchmark interpretation due to single-year validation
- Low confidence in distinguishing algorithmic vs. implementation failures from verdict distributions alone

## Next Checks
1. Replicate contamination-free evaluation on 2024 problems across multiple programming platforms
2. Implement controlled few-shot prompting experiments (2-3 examples) to measure performance gains
3. Conduct manual analysis of compilation errors and wrong answers to categorize failure modes