---
ver: rpa2
title: Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting
arxiv_id: '2507.22619'
source_url: https://arxiv.org/abs/2507.22619
tags:
- sparql
- llms
- ontology
- queries
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling non-experts to query
  manufacturing knowledge graphs (KGs) without requiring expertise in SPARQL. It proposes
  an LLM-based framework that translates natural language questions into SPARQL queries
  by providing the model with context-aware KG schema information.
---

# Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting

## Quick Facts
- **arXiv ID:** 2507.22619
- **Source URL:** https://arxiv.org/abs/2507.22619
- **Reference count:** 29
- **Primary result:** Context-aware content selection reduces hallucination and improves SPARQL generation accuracy by 20-30% in manufacturing KGQA.

## Executive Summary
This paper addresses the challenge of enabling non-experts to query manufacturing knowledge graphs (KGs) without requiring expertise in SPARQL. It proposes an LLM-based framework that translates natural language questions into SPARQL queries by providing the model with context-aware KG schema information. Experiments using GPT-3.5 and GPT-4 on two manufacturing ontologies (LIS and CIMM) demonstrate that context-aware content selection significantly reduces hallucination and improves accuracy by 20-30%. Including domain-specific examples in prompts yields an additional 5-8% accuracy gain. The framework shows that carefully curated KG context and targeted prompting techniques enable LLMs to generate correct and complete SPARQL queries for complex manufacturing queries.

## Method Summary
The framework uses a two-stage pipeline: preprocessing/enrichment with content selection (Entire Ontology, Naive Reduction, Context-based Reduction) and representation (Graph, Table, Table-Sorted); followed by LLM prompting with Psimple, Pexample, or Pdomain templates. Context-based reduction uses semantic similarity to retrieve top-25 relevant classes/properties from the ontology. The LLM generates SPARQL queries using GPT-4-32K or GPT-3.5-16K. Experiments run 5× per configuration on LIS and CIMM ontologies with 17 benchmark questions.

## Key Results
- Context-aware content selection improves accuracy by 20-30% over naive or full-ontology approaches
- Domain-specific examples in prompts yield 5-8% additional accuracy gains
- GPT-4-32K achieves correctness 3.15/4 and completeness 3.18/4 on LIS benchmark
- Table representation is more compact than Graph while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: Context-Based Ontology Reduction
Providing only question-relevant ontology subsets improves SPARQL generation accuracy by 20-30% compared to naive or full-ontology approaches. Semantic similarity selects top 25 matching classes/properties, then includes neighboring concepts to form a focused sub-ontology, narrowing LLM attention to relevant entities.

### Mechanism 2: Rich Ontology Semantics Enable Better Query Formulation
Including full ontology definitions (rdfs:comment, rdfs:domain/range, owl:inverseOf) improves correctness and completeness beyond shallow schema-only inputs. Rich axioms and comments provide lexical and semantic cues that help the LLM disambiguate class/property usage.

### Mechanism 3: Domain-Specific Few-Shot Prompting Reduces Hallucination
Adding a single domain-specific SPARQL example to the prompt improves hallucination accuracy by 5-8% over generic examples. The example grounds the LLM in the target ontology's naming conventions and query patterns, providing in-context schema familiarity without fine-tuning.

## Foundational Learning

- **Concept: SPARQL query syntax and triple patterns**
  - Why needed: The entire pipeline targets SPARQL generation; you must read and debug generated queries.
  - Quick check: Can you write a SPARQL SELECT query that retrieves all instances of a class matching a property filter?

- **Concept: Ontology structure (classes, properties, rdfs:domain/range, rdfs:subClassOf)**
  - Why needed: Context selection and enrichment depend on understanding what ontology elements carry semantic weight.
  - Quick check: What does rdfs:domain assert about a property, and how might an LLM misuse it if omitted from the prompt?

- **Concept: Retrieval-Augmented Generation (RAG) and embedding-based similarity**
  - Why needed: Context-based reduction relies on semantic similarity to select relevant ontology fragments.
  - Quick check: How would you retrieve the top-k ontology terms most relevant to a user question using vector embeddings?

## Architecture Onboarding

- **Component map:** Content Selection -> Content Enrichment -> Representation -> Prompt Construction -> LLM Inference -> Query Validation -> KG Execution

- **Critical path:**
  1. Receive natural language question
  2. Run embedding similarity to identify top 25 ontology classes/properties
  3. Expand to neighboring concepts; serialize as Turtle or Table-Sorted
  4. Construct prompt with domain-specific example
  5. Call LLM; validate SPARQL against ontology (hallucination check)
  6. Execute query against KG; return results

- **Design tradeoffs:**
  - Graph vs. Table representation: Graph preserves relationships but may exceed token limits; Table is compact but loses explicit edges
  - OntC vs. OntD enrichment: OntC provides rich axioms; OntD adds heuristic descriptions but showed no significant gain on CIMM
  - Generic vs. domain-specific example: Domain-specific improves accuracy but requires curation per ontology

- **Failure signatures:**
  - Query references non-existent predicates → likely insufficient context reduction or missing relevant ontology fragment
  - Syntactically correct but semantically wrong triples → LLM misunderstood ontology semantics; consider richer axiom inclusion
  - Empty or overly broad results → query lacks filters; few-shot example may not match query type

- **First 3 experiments:**
  1. Reproduce OntA vs. OntC comparison on the LIS benchmark with GPT4-32K; measure hallucination accuracy and correctness/completeness ratings
  2. Ablate representation format: compare Turtle vs. Table-Sorted on OntC with Pdomain; track token usage and accuracy
  3. Swap GPT4-32K for an open model (e.g., Llama 3) on a 5-question subset; evaluate if context-based reduction transfers without re-tuning prompts

## Open Questions the Paper Calls Out

- **Open Question 1:** How do open-source models (e.g., Llama 3, Falcon) compare to GPT-4 in performance for context-aware SPARQL generation?
  - Basis: The Conclusion states the need to "utilize Open Language Models, e.g., Llama 3, Falcon and compare their performance to GPT-based LLMs."
  - Why unresolved: The study exclusively evaluated proprietary models (GPT-3.5 and GPT-4), leaving the efficacy of open-source alternatives undetermined.

- **Open Question 2:** Can the framework be extended to handle complex, multi-hop questions involving specific constraints?
  - Basis: The authors identify the need to "handle more complex questions" as future work, specifically citing "intricate questions such as, 'List all lines that fulfill the requirements for the production of a given product'."
  - Why unresolved: The current benchmark focuses on operational questions, and the system's ability to navigate intricate logical constraints and multi-hop reasoning remains untested.

- **Open Question 3:** To what extent does fine-tuning improve accuracy over context-aware prompting for this specific domain?
  - Basis: The Discussion notes the study avoided fine-tuning due to data scarcity but states that "assembly of a comprehensive and domain-specific dataset for fine-tuning purposes constitutes a valuable avenue."
  - Why unresolved: It is currently unknown if the performance gains from prompting strategies (20-30%) represent the ceiling or if model adaptation could yield higher correctness.

## Limitations

- Results hinge on proprietary datasets (LIS benchmark) and undisclosed prompt templates, making independent replication challenging
- Token limits (32K for GPT-4) constrain scalability to very large ontologies
- External validation on open manufacturing ontologies beyond CIMM is absent

## Confidence

- **High confidence:** Context-based ontology reduction improves accuracy over naive or full-ontology approaches; domain-specific examples yield measurable gains
- **Medium confidence:** Rich ontology semantics (OntC) provide additional benefits beyond OntB; Table representation is more compact without loss of accuracy
- **Low confidence:** Generalizability to non-manufacturing domains; robustness under different LLM families; performance on extremely large or highly interconnected ontologies

## Next Checks

1. Replicate the OntA vs. OntC comparison on LIS with GPT-4-32K; measure hallucination accuracy and correctness/completeness ratings
2. Conduct ablation studies varying representation (Graph vs. Table-Sorted) and enrichment level (OntC vs. OntD) on CIMM
3. Evaluate transferability of the framework to an open manufacturing ontology (e.g., SSN) with a small benchmark of 5-10 queries, using GPT-4-32K and a different embedding model (e.g., text-embedding-3-small)