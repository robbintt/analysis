---
ver: rpa2
title: Advanced Black-Box Tuning of Large Language Models with Limited API Calls
arxiv_id: '2511.10210'
source_url: https://arxiv.org/abs/2511.10210
tags:
- proxy
- training
- black-box
- tuning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  in black-box settings where parameter access is unavailable. The proposed method
  employs a Gaussian Process surrogate trained on a minimal yet informative subset
  of data to approximate the foundation model's output logits.
---

# Advanced Black-Box Tuning of Large Language Models with Limited API Calls

## Quick Facts
- arXiv ID: 2511.10210
- Source URL: https://arxiv.org/abs/2511.10210
- Reference count: 37
- Primary result: Achieves 86.85% accuracy using only 1.38% of API calls compared to prior methods

## Executive Summary
This paper addresses the challenge of adapting large language models in black-box settings where parameter access is unavailable. The proposed method employs a Gaussian Process surrogate trained on a minimal yet informative subset of data to approximate the foundation model's output logits. This surrogate guides the training of a smaller proxy model, significantly reducing the need for expensive API calls. Extensive experiments demonstrate that the approach achieves state-of-the-art performance, improving pre-trained model accuracy from 55.92% to 86.85% while using only 1.38% of the API calls required by previous methods, offering a robust and highly efficient paradigm for black-box tuning.

## Method Summary
The method introduces a Gaussian Process surrogate model trained on a carefully selected subset of data to approximate the black-box foundation model's output logits. This surrogate acts as a proxy for the expensive API calls, enabling efficient training of a smaller proxy model that mimics the foundation model's behavior. The approach focuses on minimizing API usage while maintaining high performance through strategic data selection and surrogate-guided optimization. By reducing the dependency on direct API interactions, the method provides a scalable and cost-effective solution for black-box model adaptation.

## Key Results
- Achieves 86.85% accuracy compared to 55.92% baseline
- Reduces API calls to 1.38% of previous methods
- Demonstrates state-of-the-art performance across tested benchmarks

## Why This Works (Mechanism)
The method works by creating a Gaussian Process surrogate that approximates the foundation model's logits using a minimal but informative dataset. This surrogate serves as a cost-effective replacement for direct API calls during the training of a smaller proxy model. The Gaussian Process captures the underlying patterns in the foundation model's outputs, enabling the proxy model to learn effectively without requiring extensive API interactions. The strategic selection of the informative subset ensures that the surrogate remains accurate while minimizing computational overhead, leading to efficient and high-performance black-box tuning.

## Foundational Learning

### Gaussian Processes
- Why needed: Provides probabilistic surrogate modeling for approximating black-box model outputs
- Quick check: Verify GP kernel choice matches data characteristics and computational constraints

### Active Learning Subset Selection
- Why needed: Identifies most informative samples to minimize API calls while maximizing surrogate accuracy
- Quick check: Confirm selected subset covers diverse input patterns and edge cases

### Surrogate-Guided Optimization
- Why needed: Enables proxy model training without direct access to foundation model parameters
- Quick check: Validate that proxy model performance correlates with surrogate accuracy

## Architecture Onboarding

### Component Map
Input Data -> Informative Subset Selector -> Gaussian Process Surrogate -> Proxy Model Trainer -> Tuned Proxy Model

### Critical Path
The critical path flows from data selection through surrogate construction to proxy model training, with API calls bottlenecked at the initial data subset collection phase.

### Design Tradeoffs
The method trades computational complexity in surrogate construction for reduced API costs, with the Gaussian Process introducing overhead that must be balanced against API savings.

### Failure Signatures
- Poor subset selection leading to inaccurate surrogate modeling
- Gaussian Process overfitting to limited data samples
- Proxy model failing to generalize from surrogate guidance

### First 3 Experiments
1. Validate subset selection strategy on a small dataset with known properties
2. Test Gaussian Process surrogate accuracy against ground truth logits
3. Measure proxy model performance with varying surrogate quality levels

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to other foundation models and domains remains uncertain
- Performance depends heavily on quality of informative subset selection
- Computational overhead of Gaussian Processes not fully addressed for large-scale applications

## Confidence

| Claim | Confidence |
|-------|------------|
| 1.38% API call reduction | High for described experimental setup |
| State-of-the-art performance | High within tested benchmarks |
| Gaussian Process effectiveness | Medium - depends on data characteristics |

## Next Checks

1. Test the approach on a broader range of foundation models (e.g., Claude, LLaMA, or domain-specific models) to assess robustness.

2. Evaluate performance on non-standard tasks (e.g., long-form generation or multi-modal inputs) to identify potential limitations.

3. Conduct ablation studies to quantify the impact of the informative subset selection strategy on final performance.