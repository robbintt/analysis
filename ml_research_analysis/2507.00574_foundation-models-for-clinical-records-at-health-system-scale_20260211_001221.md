---
ver: rpa2
title: Foundation Models for Clinical Records at Health System Scale
arxiv_id: '2507.00574'
source_url: https://arxiv.org/abs/2507.00574
tags:
- dementia
- prediction
- diagnosis
- patient
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a generative pretraining framework for electronic
  health records using next-visit event prediction. The approach trains a decoder-only
  Transformer to autoregressively generate tokenized clinical events for the next
  visit, conditioned on patient history and inter-visit time, with regularization
  to penalize repeated event predictions and encourage forecasting new onsets.
---

# Foundation Models for Clinical Records at Health System Scale

## Quick Facts
- arXiv ID: 2507.00574
- Source URL: https://arxiv.org/abs/2507.00574
- Reference count: 23
- Zero-shot clinical event prediction achieves AUROC of 0.814 for dementia forecasting at 2 years

## Executive Summary
This study introduces a generative pretraining framework for electronic health records using next-visit event prediction. The approach trains a decoder-only Transformer to autoregressively generate tokenized clinical events for the next visit, conditioned on patient history and inter-visit time, with regularization to penalize repeated event predictions and encourage forecasting new onsets. Evaluated on forecasting dementia and knee osteoarthritis incidence within 2 and 5 years, the model achieved strong zero-shot performance—for dementia at 2 years, AUROC of 0.814—rivaling fully fine-tuned BERT baselines. The work highlights that repeated event tokens can inflate performance metrics and demonstrates the importance of regularization to capture meaningful new disease onsets rather than simply repeating chronic conditions.

## Method Summary
The approach uses a decoder-only Transformer architecture trained on EHR sequences to autoregressively predict clinical events for the next patient visit. The model conditions predictions on both patient history and inter-visit time, incorporating a regularization term that penalizes repeated event predictions to encourage forecasting of new disease onsets rather than simply repeating existing chronic conditions. The pretraining objective focuses on next-visit event prediction, enabling the model to learn temporal patterns in clinical trajectories. This generative pretraining enables zero-shot transfer to downstream disease forecasting tasks without requiring task-specific fine-tuning.

## Key Results
- Zero-shot performance achieved AUROC of 0.814 for dementia forecasting at 2 years
- Performance rivals fully fine-tuned BERT baselines despite no task-specific training
- Regularization approach effectively prevents inflated performance from repeated event predictions
- Demonstrated effectiveness for both dementia and knee osteoarthritis forecasting

## Why This Works (Mechanism)
The approach works by leveraging the temporal structure of clinical visits to train a generative model that learns to predict future clinical events based on historical patterns. The decoder-only Transformer architecture enables autoregressive generation of event sequences, while conditioning on inter-visit time captures the temporal dynamics of disease progression. The key innovation is the regularization term that penalizes repeated event predictions, which forces the model to focus on forecasting new disease onsets rather than simply regurgitating existing chronic conditions. This addresses a critical methodological challenge in EHR modeling where models can achieve artificially high performance by repeatedly predicting already-present conditions.

## Foundational Learning
1. **Decoder-only Transformer architecture**: Why needed - enables autoregressive generation of sequential clinical events; Quick check - verify the model can generate coherent next-visit event sequences
2. **Next-visit event prediction**: Why needed - captures temporal dynamics of disease progression between visits; Quick check - evaluate prediction accuracy across different time gaps between visits
3. **Inter-visit time conditioning**: Why needed - accounts for variable time intervals between clinical encounters; Quick check - assess performance when conditioning on actual vs. average time intervals
4. **Regularization against repeated predictions**: Why needed - prevents artificially inflated performance from predicting chronic conditions; Quick check - compare performance with and without regularization on disease onset tasks
5. **Tokenization of clinical events**: Why needed - converts heterogeneous clinical data into model-compatible format; Quick check - evaluate impact of different tokenization schemes on prediction quality
6. **Zero-shot transfer learning**: Why needed - enables application to new tasks without task-specific fine-tuning; Quick check - test performance on diseases not seen during pretraining

## Architecture Onboarding
**Component Map**: Clinical events -> Tokenizer -> Decoder-only Transformer -> Regularized loss function -> Next-visit predictions

**Critical Path**: Patient history + inter-visit time -> Transformer encoder -> Masked event prediction -> Regularization penalty -> Autoregressive generation

**Design Tradeoffs**: Decoder-only vs. encoder-decoder architecture (simplicity vs. bidirectional context), autoregressive vs. non-autoregressive generation (accuracy vs. speed), explicit vs. implicit time modeling (control vs. flexibility)

**Failure Signatures**: Over-prediction of chronic conditions, poor performance on rare disease onset, temporal inconsistency in predicted event sequences, failure to generalize across different visit intervals

**First Experiments**:
1. Baseline evaluation: Compare zero-shot performance against random and majority-class baselines for disease onset prediction
2. Ablation study: Test model performance with and without inter-visit time conditioning to quantify temporal signal importance
3. Regularization sensitivity: Evaluate different penalty strengths for repeated predictions to find optimal balance between novelty and accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two specific chronic diseases (dementia and knee osteoarthritis) within a single health system
- Performance metrics rely on AUROC without additional measures like precision-recall curves for imbalanced clinical populations
- Regularization effectiveness depends on implementation details not fully elaborated in the main text

## Confidence
- **High confidence**: Generative pretraining can achieve competitive zero-shot performance for clinical event prediction
- **Medium confidence**: Generalizability of results across different diseases and healthcare systems
- **Medium confidence**: Effectiveness of the proposed regularization approach, pending more detailed implementation validation

## Next Checks
1. Evaluate zero-shot performance on additional disease outcomes across different disease categories (acute vs. chronic, common vs. rare)
2. Test the approach on independent health system datasets to assess robustness and generalizability beyond the training institution
3. Conduct ablation studies comparing different regularization strategies and their impact on preventing repeated event prediction while maintaining predictive accuracy for disease onsets