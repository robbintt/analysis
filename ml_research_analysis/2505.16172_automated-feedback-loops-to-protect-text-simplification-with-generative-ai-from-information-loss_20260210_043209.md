---
ver: rpa2
title: Automated Feedback Loops to Protect Text Simplification with Generative AI
  from Information Loss
arxiv_id: '2505.16172'
source_url: https://arxiv.org/abs/2505.16172
tags:
- text
- simplified
- missing
- information
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of information loss during automated
  text simplification, particularly in health-related content, where missing critical
  information can impact comprehension. The authors propose an automated feedback
  loop using generative AI (ChatGPT) to detect and reinsert missing elements in simplified
  text.
---

# Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss

## Quick Facts
- arXiv ID: 2505.16172
- Source URL: https://arxiv.org/abs/2505.16172
- Reference count: 31
- Authors: Abhay Kumara Sri Krishna Nandiraju; Gondy Leroy; David Kauchak; Arif Ahmed
- Primary result: Adding all missing entities (A1) provides best document-level semantic alignment; adding all missing words (A2) excels at summary-level

## Executive Summary
This study addresses information loss during automated text simplification of health-related content, where missing critical information can impact comprehension. The authors propose an automated feedback loop using generative AI (ChatGPT) to detect and reinsert missing elements in simplified text. They employ frequency analysis and biomedical named entity recognition to identify missing information, then use five different approaches to reinsert it. The results show that comprehensive reinsertion of all missing entities or words produces significantly better semantic alignment than selective approaches, though this comes at the cost of potentially reduced readability.

## Method Summary
The method uses GPT-4 to initially simplify 50 rheumatology texts from BMJ articles. Missing information is detected through two mechanisms: (1) words appearing ≥2 times in original but <2 times in simplified, and (2) missing biomedical named entities identified via SciSpacy. Five regeneration approaches are then applied: adding all missing entities, adding all missing words, adding top-3 ranked entities, adding three random entities, or adding a random number of entities equal to missing words. The regenerated texts are evaluated using cosine similarity and ROUGE-1 scores at both document and summary levels (summaries generated via BART).

## Key Results
- Adding all missing entities (A1) provides the best overall improvement in semantic alignment at the document level
- Adding all missing words (A2) is most effective at the summary level
- Approaches involving adding a limited number of entities (ranked or random) demonstrate significantly less improvement
- Comprehensive information reinsertion is critical for retaining semantic meaning

## Why This Works (Mechanism)

### Mechanism 1: Automated Missing Element Detection via Frequency Analysis and NER
- Claim: Automated detection of missing information through frequency-based word analysis and named entity recognition can identify semantic gaps in simplified text with higher precision than random selection.
- Mechanism: The system compares original and simplified texts by (1) identifying words appearing ≥2 times in original but <2 times in simplified, and (2) extracting biomedical named entities using SciSpacy and performing set-theoretic subtraction.
- Core assumption: Words/entities that appear frequently in original text but are absent or infrequent in simplified text represent meaningful information loss rather than desirable simplification.
- Evidence anchors: [abstract] "We employ two methods to identify missing information: (1) identifying frequently occurring words in the original text but less frequent in the simplified version, and (2) detecting missing named entities using biomedical named entity recognition."
- Break condition: Frequency threshold fails for rare but critical single-occurrence terms; NER fails for entities outside biomedical domain or unrecognized by SciSpacy models.

### Mechanism 2: LLM-Powered Feedback Loop for Contextual Reinsertion
- Claim: Feeding identified missing elements back to an LLM with structured prompts enables regeneration of simplified text with improved semantic alignment while maintaining readability.
- Mechanism: Missing entities/words are passed to GPT-4 with a structured prompt instructing augmentation while preserving linguistic simplicity. The LLM rewrites the simplified text incorporating specified elements contextually.
- Core assumption: LLMs can integrate missing information without introducing hallucinations, readability loss, or factual errors beyond what was provided.
- Evidence anchors: [abstract] "The authors propose an automated feedback loop using generative AI (ChatGPT) to detect and reinsert missing elements in simplified text."
- Break condition: LLM hallucinates beyond provided information, degrades readability by forced insertion, or fails when prompt context window is exceeded for long documents.

### Mechanism 3: Comprehensive Reinsertion Outperforms Selective Addition
- Claim: Adding ALL missing entities or ALL missing words produces better semantic alignment than adding limited subsets, regardless of whether selection is ranked by LLM or random.
- Mechanism: Comprehensive reinsertion (A1: all entities, A2: all words) preserves broader semantic coverage. Selective approaches (A3: top-3 ranked, A4: 3 random, A5: k-random) miss too much information for meaningful improvement.
- Core assumption: Semantic alignment metrics (cosine similarity, ROUGE-1) correlate with information completeness, and higher scores indicate better preservation of meaning comprehensible to readers.
- Evidence anchors: [abstract] "Adding all missing entities (approach A1) provides the best overall improvement in semantic alignment at the document level, while adding all missing words (approach A2) is most effective at the summary level."
- Break condition: When original contains redundant or noisy information, comprehensive addition may degrade quality; ranking failure (A3 ≈ A4) suggests GPT-4 cannot reliably prioritize entity importance, not that selection is inherently flawed.

## Foundational Learning

- Concept: **Named Entity Recognition (NER) for Biomedical Domains**
  - Why needed here: Understanding how SciSpacy extracts biomedical entities (diseases, treatments, medications) is essential for grasping what the system identifies as "missing" and why standard NER may fail on specialized vocabulary.
  - Quick check question: Can you explain why a general-purpose NER model might miss "rheumatoid arthritis" or "methotrexate" compared to a biomedical-specific model like SciSpacy?

- Concept: **Cosine Similarity and ROUGE Metrics**
  - Why needed here: The paper's conclusions rest entirely on these metrics. Cosine similarity measures semantic vector alignment; ROUGE-1 measures unigram overlap. Understanding their differences is critical for interpreting why A1 excels at document-level while A2 excels at summary-level.
  - Quick check question: If two texts have high ROUGE-1 but low cosine similarity, what might that indicate about their relationship?

- Concept: **Prompt Engineering for Constrained Generation**
  - Why needed here: The feedback loop depends on structured prompts that instruct the LLM to add specific elements while maintaining simplicity. Poor prompt design could cause hallucination or readability degradation.
  - Quick check question: What prompt constraints would you add to prevent an LLM from introducing information not in the provided missing-element list?

## Architecture Onboarding

- Component map: Original text -> GPT-4 initial simplification -> Detection Module (frequency analysis + NER) -> Missing elements identification -> Feedback Module (LLM regeneration) -> Evaluation Module (cosine similarity & ROUGE-1) -> Output

- Critical path: Original text quality and domain specificity determine NER coverage → Detection accuracy directly limits what can be reinserted → Prompt design determines whether reinsertion improves or degrades output → Evaluation metrics guide approach selection

- Design tradeoffs: A1 (all entities) vs. A2 (all words): A1 better for semantic concepts; A2 better for fine-grained detail preservation. Frequency threshold (≥2): Higher thresholds miss more; lower thresholds add noise. Single-pass vs. iterative feedback: Paper uses single-pass; iterative could improve but increases cost/latency.

- Failure signatures: Low improvement from A1/A2: Detection module missing critical entities (NER coverage gap) or prompt not effectively guiding reinsertion. A3 ≈ A4 performance: LLM ranking provides no advantage—ranking mechanism unreliable. High ROUGE-1 but low cosine: Lexical overlap without semantic coherence (possible forced insertion). Degraded readability (not measured): Comprehensive reinsertion may undo simplification benefits.

- First 3 experiments:
  1. **Baseline replication**: Run A1 and A2 on 10 new rheumatology texts; verify cosine/ROUGE improvements replicate within ±0.02 of reported values. Check for readability degradation using Flesch-Kincaid.
  2. **Domain boundary test**: Apply detection module to non-biomedical texts (e.g., legal, financial); measure NER recall drop to identify generalization limits.
  3. **Hybrid approach prototype**: Combine A1 + A2 (entities + frequent words); compare against A1-only and A2-only to test authors' proposed future work direction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative ranking mechanisms effectively identify crucial entities for reinsertion, overcoming the limitation where GPT-4's ranking failed to outperform random selection?
- Basis in paper: [explicit] The Abstract states current tools are "not valuable in ranking" entities, and Section 8 proposes exploring "alternative ranking mechanisms" as future work.
- Why unresolved: The study found that adding top-3 ranked entities (A3) yielded no significant improvement over adding random entities (A4), indicating the model could not distinguish critical information.
- Evidence: A new ranking method resulting in semantic similarity scores that significantly exceed the random insertion baseline.

### Open Question 2
- Question: How does a hybrid approach combining missing entities and words perform compared to isolated approaches in balancing document-level semantic alignment with summary-level detail?
- Basis in paper: [explicit] Section 8 suggests "exploring hybrid approaches that combine the strengths of A1 and A2" (A1 was best for documents, A2 for summaries).
- Why unresolved: The authors evaluated A1 and A2 in isolation, finding they excel at different granularities, but did not test a combined method.
- Evidence: Comparative cosine similarity and ROUGE-1 scores for a combined A1+A2 method against the isolated baselines.

### Open Question 3
- Question: Does the insertion of missing information to maximize semantic similarity negatively impact the text's readability or simplicity?
- Basis in paper: [inferred] The study evaluates success via similarity to the original text (Section 3) but does not measure if the regenerated text remains simple; related work (Section 2.2) highlights the trade-off between simplification and retention.
- Why unresolved: The feedback loop restores information but may inadvertently restore complexity, undoing the simplification without specific validation.
- Evidence: Readability scores (e.g., Flesch-Kincaid Grade Level) calculated for the regenerated texts (A1–A5) compared to the initial simplified text.

## Limitations
- Conclusions rely entirely on automatic metrics without human evaluation of readability or comprehension
- Detection mechanisms may miss critical single-occurrence terms and depend on domain-specific NER coverage
- LLM ranking approach shows no advantage over random selection, suggesting ranking mechanism unreliability
- Approach tested only on rheumatology texts, limiting generalizability to other domains

## Confidence
- **High confidence**: Comprehensive reinsertion (A1 and A2) outperforms selective approaches in automatic metrics
- **Medium confidence**: Frequency threshold and NER detection reliably identify meaningful information loss (no direct validation shown)
- **Low confidence**: LLM can contextually reinsert missing elements without introducing hallucinations or readability degradation (not measured)

## Next Checks
1. **Human readability validation**: Have 3 independent evaluators rate A1 and A2 outputs on readability (1-5 scale) and factual accuracy; verify that semantic gains don't come at the cost of degraded simplification quality.
2. **Cross-domain generalization test**: Apply the complete pipeline to non-medical texts (legal contracts, financial reports); measure NER coverage drop and evaluate whether comprehensive reinsertion still outperforms selective approaches.
3. **Single-occurrence term coverage analysis**: Manually annotate 10 original texts for critical single-occurrence terms that frequency threshold would miss; compare against W_missing output to quantify detection completeness.