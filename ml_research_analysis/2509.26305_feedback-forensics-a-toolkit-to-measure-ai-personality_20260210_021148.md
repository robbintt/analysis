---
ver: rpa2
title: 'Feedback Forensics: A Toolkit to Measure AI Personality'
arxiv_id: '2509.26305'
source_url: https://arxiv.org/abs/2509.26305
tags:
- more
- personality
- traits
- response
- uses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Feedback Forensics is an open-source toolkit that measures AI personality
  traits using relative pairwise annotations. The method compares personality annotations
  to human feedback or target model responses to compute strength metrics indicating
  how much a trait is encouraged or exhibited.
---

# Feedback Forensics: A Toolkit to Measure AI Personality

## Quick Facts
- arXiv ID: 2509.26305
- Source URL: https://arxiv.org/abs/2509.26305
- Reference count: 40
- Feedback Forensics measures AI personality traits using relative pairwise annotations to quantify trait strength

## Executive Summary
Feedback Forensics is an open-source toolkit that measures AI personality traits by analyzing pairwise preference annotations. The method computes strength metrics showing how much each trait is encouraged by human feedback or exhibited by AI models. Using 10,000 samples from Chatbot Arena, MultiPref, and PRISM datasets, the toolkit reveals that structured formatting, verbosity, and factual correctness are most encouraged by human feedback, while conciseness and avoidant tone are discouraged. Model personality analysis across six popular models shows significant differences, with Gemini-2.5-Pro and Mistral-Medium-3.1 using notable markdown formatting while GPT-5 is more concise. Human-AI annotation agreement experiments validate the approach, with GPT-5-mini and Gemini-2.5-Flash achieving 92% choice agreement with human annotations.

## Method Summary
The toolkit uses relative pairwise annotations to measure AI personality traits. For each trait, responses are ranked by trait expression using a pairwise comparison method that determines which response exhibits more of the trait. A regression model then predicts trait strength from prompt and response content. These predicted strengths are used to compute strength metrics showing how much each trait is encouraged by human feedback (preference-weighted trait strength) or exhibited by a model (trait score). The approach normalizes results relative to the dataset distribution and provides both absolute and relative strength metrics. The toolkit supports both relative trait strength measurements and absolute trait scoring for individual responses.

## Key Results
- Structured formatting, verbosity, and factual correctness are most encouraged by human feedback; conciseness and avoidant tone are discouraged
- Gemini-2.5-Pro and Mistral-Medium-3.1 use notable markdown formatting; GPT-5 is more concise
- Chatbot Arena's Llama-4-Maverick version is 0.97 strength more verbose and 0.96 more enthusiastic than public release
- GPT-5-mini and Gemini-2.5-Flash achieve 92% choice agreement with human annotations

## Why This Works (Mechanism)
The method works by leveraging the natural human tendency to prefer responses with certain personality traits. By analyzing pairwise preferences, the toolkit captures relative trait strength without requiring absolute trait ratings. The regression-based approach allows prediction of trait strength from content features, enabling computation of feedback-weighted trait strengths that reveal which personality characteristics humans implicitly reward. The pairwise ranking system provides more reliable measurements than absolute ratings by forcing comparative judgments.

## Foundational Learning

**Pairwise Comparison Method**
- Why needed: Enables reliable relative ranking of trait expression without requiring absolute trait ratings
- Quick check: Verify that pairwise comparisons produce consistent rankings across multiple annotators

**Regression-Based Trait Prediction**
- Why needed: Allows computation of feedback-weighted trait strengths by predicting trait strength from content features
- Quick check: Validate that predicted trait strengths correlate with human judgments on held-out samples

**Relative vs Absolute Strength Metrics**
- Why needed: Distinguishes between traits encouraged by feedback versus those naturally exhibited by models
- Quick check: Confirm that relative strength metrics vary across different datasets while absolute metrics remain consistent for individual models

## Architecture Onboarding

**Component Map**
Dataset (A) -> Pairwise Annotation (B) -> Trait Strength Prediction (C) -> Strength Metrics (D) -> Model Comparison (E)

**Critical Path**
The core measurement pipeline flows from pairwise annotation through trait strength prediction to final strength metric computation. The regression model is the critical component that enables feedback-weighted trait strength calculation.

**Design Tradeoffs**
- Pairwise comparisons vs absolute ratings: Pairwise provides more reliable relative rankings but requires more annotations
- Relative vs absolute strength metrics: Relative metrics capture feedback preferences but absolute metrics enable model comparison
- Trait selection: 14 traits balance comprehensiveness with measurement reliability

**Failure Signatures**
- Inconsistent pairwise rankings indicate unreliable trait measurement
- Poor regression model performance suggests inadequate feature representation
- Extreme strength metric values may indicate dataset bias or measurement instability

**3 First Experiments**
1. Run pairwise annotation pipeline on small dataset subset to verify ranking consistency
2. Train regression model on held-out samples to validate prediction accuracy
3. Compute strength metrics for single model to verify calculation pipeline

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Do human annotators consciously select responses based on the personality traits that correlate with their preferences, or are these correlations incidental?
- Basis in paper: Section 5 states "correlation does not imply causation: whilst annotations may correlate this does not necessarily mean that the original annotators followed a certain personality-selecting criterion."
- Why unresolved: The toolkit measures trait-preference correlations but cannot establish whether traits causally influence human decisions.
- What evidence would resolve it: Ablation experiments where specific traits are systematically varied while holding content constant, combined with human annotator post-hoc explanations of their choices.

**Open Question 2**
- Question: How robust are AI personality annotations to the choice of annotator model, and what systematic biases do different annotator models introduce?
- Basis in paper: Section 5 notes "LLM judges may also introduce their own biases and issues," and Table 1 shows varying agreement rates across GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.
- Why unresolved: The paper validates only a subset of annotator configurations and observes performance differences but does not systematically characterize annotator-specific biases.
- What evidence would resolve it: Cross-annotation experiments using multiple diverse AI annotator backbones on the same dataset, analyzed for systematic disagreement patterns.

**Open Question 3**
- Question: How do encouraged personality traits generalize across different prompt distributions, user demographics, and cultural contexts?
- Basis in paper: Section 5 states "all measurements are relative to the underlying data distribution of prompts and responses," and Figure 6 shows domain-specific trait preferences (e.g., conciseness valued in emails but not songwriting).
- Why unresolved: The paper demonstrates distribution dependence but does not characterize how trait encouragement shifts across broader demographic, cultural, and task contexts.
- What evidence would resolve it: Systematic experiments measuring trait strength across stratified datasets representing diverse user populations, languages, and task types.

**Open Question 4**
- Question: Does the limited human-AI agreement observed (92% choice agreement) indicate inherent ambiguity in personality trait annotation or limitations of the AI annotators?
- Basis in paper: Section E.2 reports 92% mean choice agreement but the study used only 100 comparisons annotated by a single author, and Section 5 encourages "manual inspection to go alongside the use of our framework."
- Why unresolved: The validation study was limited in scale (1,000 trait-level judgments) and annotator diversity, leaving unclear whether disagreement represents annotation ambiguity or model error.
- What evidence would resolve it: Large-scale human annotation studies with diverse annotator pools, inter-annotator agreement analysis, and characterization of systematic disagreement patterns.

## Limitations

- Reliability depends on validity of 14 trait definitions and whether pairwise comparisons adequately capture nuanced personality differences
- Dataset composition may introduce sampling bias - Chatbot Arena reflects competitive preference scenarios while MultiPref and PRISM capture different interaction types
- Human-AI agreement metric conflates cases where AI perfectly matches human judgment with those where it simply chooses differently but with equal confidence
- Strength metric calculation assumes linear relationships between trait expression and feedback weighting

## Confidence

**Major claim clusters confidence assessment:**
- Dataset analysis findings (High): The 10k sample analysis and trait strength calculations are reproducible and internally consistent
- Model personality characterization (Medium): Cross-model comparisons show clear differences but absolute trait strength values may be influenced by interaction context
- Human-AI agreement validation (High): Choice agreement metric is well-defined and reproducible, though interpretation requires caution

## Next Checks

1. Conduct ablation studies removing individual traits to assess their relative contribution to the overall measurement framework
2. Test the consistency of strength metrics across different dataset types by re-running analysis on held-out samples
3. Evaluate inter-annotator agreement among human raters to establish baseline variability for comparison with AI annotations