---
ver: rpa2
title: 'Start Making Sense(s): A Developmental Probe of Attention Specialization Using
  Lexical Ambiguity'
arxiv_id: '2511.21974'
source_url: https://arxiv.org/abs/2511.21974
tags:
- heads
- attention
- layer
- head
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a systematic pipeline to probe attention mechanisms
  in Transformer language models using lexical ambiguity as a stimulus. The method
  identifies developmental inflection points in disambiguation performance and isolates
  attention heads whose behavior covaries with task performance across pretraining.
---

# Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity

## Quick Facts
- **arXiv ID**: 2511.21974
- **Source URL**: https://arxiv.org/abs/2511.21974
- **Reference count**: 40
- **Primary result**: This study develops a systematic pipeline to probe attention mechanisms in Transformer language models using lexical ambiguity as a stimulus. The method identifies developmental inflection points in disambiguation performance and isolates attention heads whose behavior covaries with task performance across pretraining. By stress-testing candidate heads against stimulus perturbations and performing targeted ablations, the authors distinguish between robust and fragile disambiguation mechanisms. In Pythia-14M, key heads were highly sensitive to position and part-of-speech; in Pythia-410M, multiple heads showed generalizable behavior. Ablations impaired disambiguation performance, especially in 14M, indicating causal roles for identified heads. Across random seeds, performance trajectories were consistent, though head locations varied. The results suggest larger models may encode more abstract, robust disambiguation functions, and highlight the value of developmental analysis in interpretability research.

## Executive Summary
This paper introduces a developmental probing methodology to understand how Transformer language models learn to resolve lexical ambiguity through attention mechanisms. Using the Pythia model family and RAW-C dataset, the authors track disambiguation performance across pretraining checkpoints, identify specific attention heads that contribute to this capability, and validate their causal role through targeted ablations. The study reveals that disambiguation emerges through distinct developmental phases, with smaller models relying on fragile position-based heuristics while larger models develop more abstract, robust mechanisms. The approach demonstrates how systematic perturbation and ablation can distinguish between superficial and generalizable attention patterns in neural language models.

## Method Summary
The authors develop a pipeline to probe attention mechanisms using lexical ambiguity as a stimulus. They analyze Pythia models (14M and 410M) across pretraining checkpoints, tracking disambiguation performance via regression of human relatedness judgments against cosine distances between contextualized embeddings. Key heads are identified through linear regression between attention scores (from target to disambiguating cue) and task performance, with multiple comparison correction. The methodology includes stress-testing identified heads with positional and part-of-speech perturbations, followed by targeted ablations (zero-ablation and step1-copy-ablation) to establish causal roles. The developmental analysis identifies inflection points where disambiguation capability emerges abruptly rather than gradually.

## Key Results
- Disambiguation performance in Pythia-14M showed clear developmental inflection points at training steps 1000-2000, with R² increasing from 0.07 to 0.33.
- In 14M, the critical attention heads were highly sensitive to positional perturbations (attending to "of" instead of "friendly" when "sort of" was inserted) and part-of-speech changes.
- In 410M, multiple heads showed generalizable behavior across perturbations, suggesting more abstract disambiguation mechanisms in larger models.
- Ablations impaired disambiguation performance, especially in 14M, with zero-ablations causing larger performance drops than step1-copy-ablations.
- Across random seeds, performance trajectories were consistent, though the specific head locations varied between seeds.

## Why This Works (Mechanism)

### Mechanism 1: Causal Role of Specific QK Matrices
Specific attention heads in early-to-middle layers are functionally necessary for word sense disambiguation. The Query ($W_Q$) and Key ($W_K$) matrices in specific heads (e.g., Layer 3 in Pythia-14M) learn to direct attention from an ambiguous target token (e.g., "lamb") to a disambiguating cue (e.g., "marinated"). When these weights are zero-ablated, the model's representation of the target word fails to incorporate context, degrading performance ($R^2$). The intervention isolates the specific head's contribution without triggering catastrophic interference or compensation from other layers.

### Mechanism 2: Scale-Dependent Functional Abstraction
As model scale increases, attention heads transition from implementing "surface-level" heuristics (e.g., attending to the immediately preceding token) to "abstract" disambiguation functions (e.g., attending to semantic cues regardless of position). In smaller models (14M), heads attend strongly to disambiguating cues primarily because they are in the "1-back" position (a fragile heuristic). In larger models (410M), heads emerge that maintain high attention to the semantic cue even when its position or part-of-speech is perturbed, suggesting the encoding of a generalizable "disambiguation circuit."

### Mechanism 3: Developmental Phase Transitions
Disambiguation capability emerges abruptly at specific training checkpoints ("inflection points") via coordinated changes in head attention patterns, rather than gradually. The weights in candidate heads reorganize significantly around steps 1000–2000, causing a spike in the correlation ($R^2$) between attention to the cue and the model's ability to distinguish senses. This suggests the learning of specific linguistic relationships (e.g., modifier-noun) is a discrete event.

## Foundational Learning

- **Concept: Lexical Ambiguity as a Probe**
  - **Why needed here**: This paper uses ambiguity (words with multiple meanings) as the "stressor" to force the model to use context. Understanding the difference between static embeddings (bank = bank) and contextualized embeddings (river bank vs. bank teller) is required to interpret the $R^2$ metric.
  - **Quick check question**: If a model perfectly resolves lexical ambiguity, should the cosine distance between "river bank" and "bank teller" representations be high or low?

- **Concept: Self-Attention ($Q$, $K$, $V$)**
  - **Why needed here**: The study specifically ablates $W_Q$ and $W_K$ matrices to disrupt attention. You must understand that $Q$ represents what the token is looking for and $K$ represents what the token contains to follow the intervention logic.
  - **Quick check question**: Which matrix would you modify to stop a head from paying attention to adjectives?

- **Concept: Ablation Types (Zero vs. Copy)**
  - **Why needed here**: The paper distinguishes between "Zero-Ablation" (removing the head entirely) and "Step1-Copy-Ablation" (reverting the head to an untrained state). This distinction is crucial for understanding whether a head is *currently* functional vs. whether it merely *participates* in the residual stream.
  - **Quick check question**: Why might a "Zero-Ablation" cause more performance degradation than a "Step1-Copy-Ablation" in a late-layer head?

## Architecture Onboarding

- **Component map**: Pythia-14M/410M -> RAW-C dataset (noun pairs) -> Contextualized embeddings -> Cosine distance -> Human relatedness regression -> R² metric -> Attention score analysis -> Stress tests -> Ablations

- **Critical path**:
  1. Ingest: Load model checkpoints (step 0 to 143,000)
  2. Behavioral Profiling: Calculate R² for ambiguous pairs at every checkpoint to find the inflection point (step ~2000)
  3. Head Identification: Run linear regression between attention scores (Target -> Cue) and R² to find candidate heads
  4. Stress Test: Feed perturbed inputs (e.g., "friendly *sort of* lamb") to see if candidates break
  5. Causal Verification: Ablate candidates and measure ΔR²

- **Design tradeoffs**:
  - 14M vs. 410M: 14M is tractable and interpretable (single head has large effect), but relies on "fragile" heuristics. 410M is performant and "robust," but suffers from high redundancy (ablating one head has little effect)
  - Perturbation Complexity: Increasing perturbations (inserting longer phrases) degrades 14M performance immediately, risking false negatives in identifying "robust" heads in small models

- **Failure signatures**:
  - The "1-Back" False Positive: A head appears to be a "disambiguation head" but is actually just a "previous-token" head. (Fix: Use the subtraction analysis in Phase 2)
  - Bimodality across Seeds: Replicating results on one seed works, but the specific head (e.g., 3.2) moves to a different layer (e.g., 4.1) in another seed. (Fix: Look for functional equivalence, not index equivalence)

- **First 3 experiments**:
  1. Replicate the Inflection Point: Run the 14M model on RAW-C sentences and plot R² by checkpoint to verify the performance jump at step 2000
  2. Visualize the "Baton Pass": Extract attention patterns for Layer 3 Heads 1 & 2 at steps 1000, 2000, and 5000 to observe the shift in attention from the target to the cue
  3. Run the "Sort of" Test: Input the positional perturbation ("friendly *sort of* lamb") and verify that 14M's Head (3,2) incorrectly attends to "of" instead of "friendly"

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Are the identified attention heads specialized for lexical ambiguity resolution, or do they perform general contextualization for all target words?
- **Basis in paper**: The authors ask whether the identified heads are "selective for ambiguous target words in particular, or whether they participate in contextualizing any target word."
- **Why unresolved**: The current study restricted its analysis to the RAW-C dataset, which exclusively contains ambiguous target words.
- **What evidence would resolve it**: Stress-testing head behavior by comparing attention patterns on ambiguous words (e.g., "marinated lamb") versus unambiguous controls (e.g., "marinated pork").

### Open Question 2
- **Question**: What causal role do value matrices and the residual stream play in the identified disambiguation circuit?
- **Basis in paper**: The authors note that future work "could also investigate the role of other model components in disambiguation, such as the value matrices or the residual stream."
- **Why unresolved**: The analyses focused exclusively on the query-key (QK) matrices to direct and ablate attention patterns.
- **What evidence would resolve it**: Extending the ablation pipeline to target value vectors and residual stream contributions within the identified layers.

### Open Question 3
- **Question**: Do these specialized disambiguation mechanisms generalize across different model families and languages?
- **Basis in paper**: The authors suggest carrying out analyses on "other model families" and addressing "questions regarding the generalizability... in the context of different languages."
- **Why unresolved**: This study focused only on the Pythia suite (14M and 410M) using English stimuli.
- **What evidence would resolve it**: Applying the developmental probing pipeline to non-English ambiguity datasets and architecturally distinct models (e.g., OLMo).

### Open Question 4
- **Question**: What mechanistic interactions drive the "passing the baton" phenomenon observed during training?
- **Basis in paper**: The authors describe a phenomenon where heads "play a stronger role earlier in pre-training, then appeared to lose influence," stating this is "ripe for future investigation."
- **Why unresolved**: The study documented the existence of these shifting functional roles but did not determine the underlying cause or trigger.
- **What evidence would resolve it**: Analyzing inter-layer communication dynamics and performing ablations at specific intermediate checkpoints to track the hand-off of information.

## Limitations

- The causal claims rely on targeted ablations in a small, fixed set of heads identified from one training run, with head positions shifting across random seeds suggesting the identified circuits may not be universal architectural features.
- The distinction between "robust" and "fragile" mechanisms is based on stress-test generalization using a relatively narrow perturbation set (position shifts, POS changes) that may not fully capture generalization capacity.
- The developmental analysis identifies inflection points but cannot distinguish whether these represent discrete learning events versus artifacts of checkpoint spacing or optimization dynamics.

## Confidence

- **High confidence**: The existence of developmental inflection points in disambiguation performance and the general finding that ablation impairs task performance (particularly in smaller models). The stress-test methodology for distinguishing robust vs. fragile attention patterns is sound and reproducible.
- **Medium confidence**: The specific identification of causal heads and their functional characterization as either "robust" or "fragile" disambiguation mechanisms. The seed-to-seed variability in head locations introduces uncertainty about whether these are stable circuit components.
- **Low confidence**: Claims about scale-dependent functional abstraction (smaller models using "surface heuristics" vs. larger models using "abstract circuits") require broader validation across model families and linguistic phenomena beyond the current noun-focused scope.

## Next Checks

1. **Cross-seed replication**: Run the full pipeline on 3-5 additional random seeds for Pythia-14M and Pythia-410M to assess whether identified heads consistently emerge in the same layers or whether functional equivalence patterns vary systematically.

2. **Broader perturbation suite**: Design and test a more comprehensive set of linguistic perturbations including syntactic transformations (passive voice, clefting), semantic substitutions (synonyms for disambiguating cues), and pragmatic contexts to better evaluate the claimed robustness distinctions.

3. **Control task comparison**: Apply the identical methodology to a non-ambiguous control task (e.g., consistent word usage) to establish whether the identified developmental patterns and head specializations are specific to disambiguation or reflect general attention optimization processes.