---
ver: rpa2
title: 'LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection'
arxiv_id: '2510.25799'
source_url: https://arxiv.org/abs/2510.25799
tags:
- listen-u
- listen-t
- figure
- utility
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LISTEN, a framework that uses a large language
  model (LLM) as a zero-shot preference oracle for multi-objective selection. The
  method iteratively refines a parametric utility function (LISTEN-U) or performs
  tournament-style selections (LISTEN-T) to find the most preferred item from a large
  set, guided only by high-level natural language priorities.
---

# LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection

## Quick Facts
- arXiv ID: 2510.25799
- Source URL: https://arxiv.org/abs/2510.25799
- Reference count: 40
- Primary result: LLM-based zero-shot preference elicitation achieves superior selection quality compared to z-score baselines across three multi-objective domains

## Executive Summary
This paper introduces LISTEN, a framework that uses a large language model (LLM) as a zero-shot preference oracle for multi-objective selection. The method iteratively refines a parametric utility function (LISTEN-U) or performs tournament-style selections (LISTEN-T) to find the most preferred item from a large set, guided only by high-level natural language priorities. Evaluated on flight booking, shopping, and exam scheduling tasks, LISTEN-U excels when human preferences align with linear utilities, while LISTEN-T provides robust performance across all problems. A novel concordance metric quantifies the difficulty of problems for linear methods. The results show that LLMs can effectively navigate complex trade-offs directly from natural language, reducing the cognitive burden of traditional preference elicitation.

## Method Summary
The LISTEN framework selects preferred items from large Pareto-optimal sets using an LLM as a zero-shot preference oracle. LISTEN-U iteratively elicits and refines linear utility weights by presenting the LLM with concrete solutions and their attributes, adjusting weights based on critiques. LISTEN-T uses tournament-style batch comparisons, where the LLM selects champions from small batches, culminating in a final playoff. Both methods rely on structured prompts containing persona context, metric definitions, user priorities, and candidate solutions. The framework operates without fine-tuning the LLM, instead leveraging its pre-trained knowledge to interpret natural language priorities and map them to item selections.

## Key Results
- LISTEN-T consistently outperforms z-score baselines across all three tested domains (flights, headphones, exam scheduling)
- LISTEN-U achieves superior performance on high-concordance problems where preferences align with linear utilities
- The concordance metric successfully identifies problem difficulty for linear utility methods, with low concordance correlating with LISTEN-U's degraded performance

## Why This Works (Mechanism)

### Mechanism 1
LLMs can serve as zero-shot preference oracles by interpreting natural language priorities and mapping them to item selections without explicit pairwise preference labels. The framework constructs structured prompts containing: (1) persona context, (2) metric definitions, (3) user priorities in natural language, (4) candidate solutions, and (5) format instructions. The LLM interprets the verbal description and produces either utility weights or direct selections. Core assumption: The LLM's pre-trained knowledge includes sufficient understanding of the decision domain to translate qualitative priorities into quantitative trade-offs. Evidence anchors: Abstract states LLM is "guided only by an expert's high-level priorities in natural language"; Section 3.1 confirms no fine-tuning is used; related work on LLM-as-a-judge supports feasibility but warns of framing sensitivity.

### Mechanism 2
Iterative critique of concrete solutions enables LISTEN-U to refine linear utility weights even when initial approximations are poor. After computing the best solution under current weights, the algorithm presents the unnormalized solution with all attributes to the LLM. The LLM critiques real values and proposes adjusted weights. Core assumption: Local weight adjustments guided by specific solution critiques can converge toward globally better selections, even if true preferences are non-linear. Evidence anchors: Section 3.2 emphasizes presenting true values is crucial for reasoning about real-world trade-offs; Section 4.5 shows LISTEN-U progressively refines utility function on low-concordance Exam Scheduling dataset. Break condition: When preferences are highly non-linear, linear utility cannot capture the structure regardless of iteration count—validated by Headphones-Strict experiment where concordance dropped from 0.232 to 0.053 and LISTEN-U degraded.

### Mechanism 3
Tournament-style batch comparisons provide robust selection without parametric assumptions about the utility function form. LISTEN-T samples small batches uniformly, asks the LLM to select the best within each batch (preliminary rounds), then compares all batch champions in a final playoff. This reduces the problem to O(T) LLM calls where T is the budget, avoiding context window limits. Core assumption: The LLM's relative preference judgments within small batches are more reliable than absolute scoring or large-set reasoning. Evidence anchors: Section 3.3 describes tournament-style selections over small batches; Section 4.5 confirms LISTEN-T consistently outperforms baselines. Break condition: If sampled batches fail to include high-quality candidates, the final champion set will be weak. Performance depends on batch size and number of rounds relative to solution set size.

## Foundational Learning

- **Concept: Pareto optimality in multi-objective optimization**
  - Why needed here: The candidate set S is described as "Pareto-optimal solutions" where improving one objective requires sacrificing another. Understanding that no single solution dominates all others is prerequisite to grasping why preference elicitation is necessary.
  - Quick check question: Given two flights where one is cheaper but has more stops, and another is more expensive but direct, can you explain why neither dominates the other?

- **Concept: Linear utility functions and weight vectors**
  - Why needed here: LISTEN-U assumes u(s) = w^T s_num, scoring items via dot product of weights and normalized numerical attributes. Understanding how weight adjustments shift preferences across objectives is essential.
  - Quick check question: If price weight is -0.5 and duration weight is -0.3 (both to minimize), which solution scores higher: (price=$200, duration=120min) or (price=$150, duration=180min) after normalizing both to [0,1]?

- **Concept: Preference elicitation bottleneck**
  - Why needed here: The paper positions itself against methods requiring "pairwise preferences between solutions" which demand significant human effort. Understanding this cost motivates the zero-shot NL approach.
  - Quick check question: Why might asking a user "do you prefer option A or B?" 100 times be impractical, even if it would yield accurate preferences?

## Architecture Onboarding

- **Component map:**
  Natural Language Utterance (U) -> Prompt Constructor -> LLM Oracle -> LISTEN-U (parametric) or LISTEN-T (non-parametric) -> Selected item s*

- **Critical path:**
  1. Attribute normalization (LISTEN-U only): Normalize numerical attributes to [0,1] once before iteration; use consistently across all rounds
  2. Prompt construction: Must include unnormalized values for critique phase (LISTEN-U) or exact batch items (LISTEN-T)
  3. LLM response parsing: Extract JSON weights (LISTEN-U) or selection identifier (LISTEN-T)
  4. Convergence: LISTEN-U may plateau; LISTEN-T completes in exactly T calls

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | LISTEN-U | Excels on high-concordance problems; interpretable weights | Fails on non-linear preferences; requires numerical attributes |
  | LISTEN-T | Domain-agnostic; no linearity assumption | No interpretable utility function; sampling variance |
  | More iterations | LISTEN-U can escape poor initial weights | Diminishing returns; higher API cost |
  | Larger batch size B | Better coverage per round | Larger context window needed |

- **Failure signatures:**
  - LISTEN-U flatlines near baseline: Check concordance (likely low); preference structure may be non-linear
  - LISTEN-T selects unranked items consistently: Increase batch size or number of preliminary rounds
  - Both methods degrade: Inspect prompt—missing persona/metric context may cause LLM confusion
  - LLM outputs invalid JSON: Add format enforcement or retry with explicit schema

- **First 3 experiments:**
  1. Concordance validation: Run LISTEN-U and LISTEN-T on a synthetic dataset where ground-truth utility is known to be linear vs. non-linear (e.g., threshold constraints). Confirm LISTEN-U excels only on high-concordance cases
  2. Ablation on prompt components: Remove the preference utterance (keep only persona + metrics) and measure degradation. The paper shows this matters most for subjective tasks (Headphones) and less for objective ones (Exam Scheduling)
  3. Batch size sensitivity for LISTEN-T: Vary B ∈ {3, 5, 10} with fixed budget T=10 on a medium-sized candidate set (N≈500). Identify where marginal improvement plateaus

## Open Questions the Paper Calls Out

### Open Question 1
Can non-linear or piecewise utility representations improve LISTEN-U's performance on low-concordance problems? Basis: The Conclusion states the algorithm's reliance on a linear utility function is a key limitation and suggests future work explore richer representations. Why unresolved: Linear models cannot capture complex preference structures (e.g., threshold effects) found in low-concordance datasets like Exam Scheduling. What evidence would resolve it: Benchmarking LISTEN-U modified with non-linear utilities against the current linear version on the Exam Scheduling dataset.

### Open Question 2
Does LISTEN generalize effectively to high-stakes domains such as healthcare scheduling or logistics planning? Basis: The Conclusion lists performance on a wider range of problems, specifically citing apartment hunting, healthcare, and logistics, as an open question. Why unresolved: The study is limited to three specific domains (flights, shopping, exams), leaving robustness in specialized fields untested. What evidence would resolve it: Successful application of the framework to a healthcare scheduling problem with validation from domain experts.

### Open Question 3
How can the framework be adapted to aggregate preferences from multiple users with conflicting priorities? Basis: The Conclusion identifies that ground-truth rankings reflect a single expert and suggests exploring methods for aggregating preferences from multiple users. Why unresolved: The current architecture assumes a single decision-maker and does not possess a mechanism to resolve conflicting natural language directives. What evidence would resolve it: A modified LISTEN algorithm that synthesizes multiple utterances and produces selections that maximize group consensus.

## Limitations

- The study does not fully explore scaling to larger candidate sets where batch sampling might miss high-quality items
- The non-linear preference failure mode for LISTEN-U is demonstrated but not quantified across diverse preference structures
- Batch size B for LISTEN-T is unspecified, making reproduction incomplete

## Confidence

**High confidence**: LISTEN-T consistently outperforms baselines across all tested problems; LLM can serve as zero-shot preference oracle when provided structured prompts with full context.

**Medium confidence**: LISTEN-U excels specifically on high-concordance problems where preferences align with linear utilities; iterative refinement helps on low-concordance problems but cannot overcome fundamental non-linearity.

**Low confidence**: The concordance metric reliably predicts method performance across arbitrary preference structures; the specific prompt templates shown in figures generalize to all multi-objective selection domains.

## Next Checks

1. **Scaling validation**: Test LISTEN-T with candidate sets of 10K+ items using varying batch sizes B to identify when sampling variance overwhelms selection quality, and measure how many preliminary rounds are needed for coverage.

2. **Non-linear preference mapping**: Construct synthetic datasets with known non-linear preference structures (step functions, threshold constraints, interaction effects) to quantify exactly where LISTEN-U's linear utility assumption breaks down and whether LISTEN-T consistently outperforms in these regimes.

3. **Prompt ablation study**: Systematically remove each prompt component (persona, metrics, priorities, format instructions) from the structured template to measure degradation in LLM selection accuracy, particularly comparing subjective (Headphones) versus objective (Exam Scheduling) tasks.