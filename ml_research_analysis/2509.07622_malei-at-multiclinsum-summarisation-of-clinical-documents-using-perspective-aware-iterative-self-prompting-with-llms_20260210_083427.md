---
ver: rpa2
title: 'MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware
  Iterative Self-Prompting with LLMs'
arxiv_id: '2509.07622'
source_url: https://arxiv.org/abs/2509.07622
tags:
- clinical
- prompt
- summaries
- summary
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MaLei team applied perspective-aware iterative self-prompting (PA-ISP)
  on GPT-4 and GPT-4o for clinical document summarisation in the MultiClinSUM shared
  task. The method used LLMs to generate task-specific prompts, refine them through
  example-based few-shot learning, and employed ROUGE and BERT-score for guidance.
---

# MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs

## Quick Facts
- **arXiv ID**: 2509.07622
- **Source URL**: https://arxiv.org/abs/2509.07622
- **Reference count**: 40
- **Primary result**: Achieved ROUGE-L F1=30.77 and BERTScore F1=85.46 on 3,396 English clinical case reports using perspective-aware iterative self-prompting

## Executive Summary
MaLei team applied perspective-aware iterative self-prompting (PA-ISP) on GPT-4 and GPT-4o for clinical document summarisation in the MultiClinSUM shared task. The method used LLMs to generate task-specific prompts, refine them through example-based few-shot learning, and employed ROUGE and BERT-score for guidance. On 3,396 English clinical case reports, the submission achieved ROUGE scores (P: 46.53, R: 24.68, F1: 30.77) and BERT-scores (P: 87.84, R: 83.25, F1: 85.46). High BERT-score indicates strong semantic preservation despite lower lexical overlap reflected in ROUGE scores. Qualitative analysis confirmed summaries covered key clinical aspects with logical paraphrasing. The work demonstrates PA-ISP's potential for supporting efficient clinician-patient communication through automated clinical report summarisation.

## Method Summary
The approach uses perspective-aware iterative self-prompting (PA-ISP) where LLMs generate and refine task-specific prompts through multiple epochs. A meta-prompt combines Chain-of-Thought instructions, 5 clinical perspectives (Patient Presentation, Clinical Presentation, Diagnosis, Treatment/Intervention, Outcome), evaluation metrics, and 3 few-shot examples. GPT-4 generates initial task prompts, which are applied to 50 training samples. Outputs are evaluated using ROUGE-L and BERTScore, with the worst 15 cases used to generate reflection and revision suggestions via GPT-4o. This process repeats for 5 epochs until scores plateau. The best prompt is then applied to 3,396 test documents, with length issues handled through regeneration or fallback to original text.

## Key Results
- Achieved ROUGE-L scores (P: 46.53, R: 24.68, F1: 30.77) on 3,396 test summaries
- BERTScore results (P: 87.84, R: 83.25, F1: 85.46) indicate strong semantic preservation
- 12 test summaries exceeded input length, mostly for short ~135-word inputs
- Most generated summaries were well-structured according to clinical report features
- Low ROUGE-L scores attributed to paraphrasing and structural differences rather than quality issues

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Prompting with Reflection-Based Refinement
- Claim: LLM-generated prompts refined through metric-guided self-reflection produce higher-quality summarization prompts than static hand-crafted prompts.
- Mechanism: A meta-prompt instructs the LLM to generate an initial task prompt → prompt applied to training samples → outputs evaluated against gold references → LLM receives scores + generates reflection on failures → LLM revises prompt → cycle repeats until plateau.
- Core assumption: The LLM can accurately diagnose weaknesses in its own prompts and propose effective revisions based on evaluation feedback.
- Evidence anchors: [abstract] describes ISP technique; [section 3.3] details 5-epoch process with prompt revision; [corpus] lacks direct validation for clinical documents.
- Break condition: When evaluation scores plateau across epochs or when invalid outputs persist despite revisions.

### Mechanism 2: Perspective-Based Structural Scaffolding
- Claim: Pre-defined clinical perspectives enforce consistent information coverage and produce semantically coherent summaries.
- Mechanism: Five fixed perspectives act as schema constraints, guiding the LLM to extract and organize information into predictable categories.
- Core assumption: Clinical summaries should follow a consistent structure matching clinician mental models that transfers across document types.
- Evidence anchors: [abstract] shows high BERTScore F1 (0.8546) for semantic equivalence; [section 3.2] defines 5 perspectives explicitly; [section 4.2.2] confirms well-structured outputs; [corpus] lacks direct validation.
- Break condition: When source documents lack content for specific perspectives, triggering hallucinated filler or "missing information" commentary.

### Mechanism 3: Dual-Metric Divergence as Quality Signal
- Claim: The gap between ROUGE-L (lexical overlap) and BERTScore (semantic similarity) indicates paraphrasing quality rather than summarization failure.
- Mechanism: ROUGE-L captures surface-level n-gram overlap; BERTScore captures embedding-space semantic similarity. High BERTScore + low ROUGE suggests semantic preservation through linguistic variation.
- Core assumption: Gold-standard summaries and generated summaries may convey equivalent meaning through different phrasing, and BERTScore captures this equivalence better than ROUGE.
- Evidence anchors: [abstract] links high BERTScore to semantic preservation despite low ROUGE; [section 3.3] shows stable BERTScore (>0.85) vs volatile ROUGE-L (0.12-0.52); [section 5.2] explains low ROUGE-L doesn't indicate poor quality; [corpus] lacks direct validation.
- Break condition: When optimizing for one metric degrades the other significantly, or when both metrics fail to capture factual errors.

## Foundational Learning

- **Concept: Few-Shot In-Context Learning**
  - Why needed here: The ISP approach seeds prompt generation with 3 representative examples; understanding how example selection biases the generated prompt is critical for debugging.
  - Quick check question: If your 3 few-shot examples all include "Outcome" sections but your target documents often lack outcomes, what failure mode should you anticipate?

- **Concept: ROUGE-L vs BERTScore Interpretation**
  - Why needed here: The system shows BERTScore F1=0.85 but ROUGE-L F1=0.31; engineers must understand this indicates paraphrasing, not failure.
  - Quick check question: A generated summary has ROUGE-L recall=0.25 but BERTScore recall=0.83. What does this suggest about the relationship between generated and reference text?

- **Concept: Prompt-to-Prompt (Meta-Prompting)**
  - Why needed here: The architecture uses LLMs to generate and revise prompts rather than hand-crafting them; this requires understanding how to structure meta-prompts that constrain LLM behavior.
  - Quick check question: What components must a meta-prompt include to produce useful task-specific prompts? (Hint: See Section 3.2's list of CoT questions.)

## Architecture Onboarding

**Component map:**
Meta-Prompt Constructor → GPT-4 Prompt Generator → GPT-4o Summary Generator → Evaluation Module → GPT-4o Reflection Generator → Prompt Updater → Final Prompt → Test Set Inference

**Critical path:**
1. Construct meta-prompt with perspectives, metrics, and 3 seed examples
2. Generate initial prompt, apply to 50 training pairs
3. Evaluate, extract worst-performing 15 cases
4. Generate reflections, update prompt
5. Repeat for 5 epochs (or until scores plateau)
6. Deploy best prompt (prompt_v2 in this case) to test set

**Design tradeoffs:**
- **Sample size for iteration:** Used 50 samples for speed; larger sample may improve prompt generalization but increases API costs
- **Explicit section headers:** Enforced structure improves coverage but hurts ROUGE-L due to ordering differences and extra tokens
- **Metric optimization focus:** Optimized for ROUGE-L (more volatile) while BERTScore remained stable; could reverse priority
- **Unimplemented components:** RAG-based few-shot augmentation and structure extraction from remaining 539 gold samples were designed but not executed due to time constraints

**Failure signatures:**
- **Length inversion:** 12/3,396 test summaries longer than input (avg input: 135 words vs full test avg: 527 words)—indicates model struggled with already-concise documents
- **Hallucinated commentary:** When perspectives lack source content, model generates statements like "The case report does not provide specific details on the outcome or follow-up"
- **Invalid outputs:** Early prompt versions produced 1/50 unevaluable outputs; resolved by prompt_v2
- **Over-expansion of abbreviations:** Generated summaries expand all abbreviations while gold summaries retain them, reducing ROUGE overlap

**First 3 experiments:**
1. **Baseline reproduction:** Run 5-epoch ISP on 50 samples with 3 fixed examples; log ROUGE-L, BERTScore, and prompt changes per epoch to establish reproducibility
2. **Perspective ablation:** Remove one perspective at a time (5 conditions) and measure impact on BERTScore and qualitative coverage of missing information types
3. **Failure mode analysis:** Manually review the 12 length-inversion cases and low-ROUGE subset (<0.2, 380 samples) to categorize error types (missing info vs. structural mismatch vs. paraphrasing distance)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does implementing Similar Case Retrieval-based Few-shot Augmentation improve lexical overlap (ROUGE-L) scores compared to the static few-shot examples used in the final submission?
- **Basis in paper:** [explicit] Section 3.5 describes the design for retrieval-based augmentation, while Section 3.1 and 3.6 confirm it was not executed due to time constraints.
- **Why unresolved:** The authors designed a method to retrieve semantically similar cases to use as few-shot demonstrations but did not integrate it into the pipeline used for the final results.
- **What evidence would resolve it:** A comparative evaluation on the test set showing ROUGE-L and BERTScore differences between the submitted static-prompt model and a model utilizing dynamic retrieval-based examples.

### Open Question 2
- **Question:** Can specific prompt constraints or "negative prompting" effectively eliminate the hallucination of clinical details (e.g., fabricated follow-up schedules) in structurally incomplete source texts without degrading summary utility?
- **Basis in paper:** [inferred] Section 5.1 notes that the decoder-only architecture tends to hallucinate content to fill structural slots (like "Outcome") when the information is absent in the source.
- **Why unresolved:** The paper identifies this as a limitation of structure-guided prompting but does not test methods to suppress it beyond suggesting "counterfactual constraints" for future work.
- **What evidence would resolve it:** A qualitative and quantitative error analysis comparing hallucination rates in outputs generated with standard instructions versus those with explicit instructions to omit missing sections.

### Open Question 3
- **Question:** Does the divergence between high BERTScore and low ROUGE-L scores stem primarily from the model's tendency to expand clinical abbreviations and insert section headers not present in the gold standard?
- **Basis in paper:** [inferred] Section 4.2.1 and 4.2.2 observe that the model expands abbreviations and enforces headers, creating a stylistic mismatch with the compressed gold summaries, which likely penalizes lexical overlap.
- **Why unresolved:** While the authors correlate the low ROUGE score to "varied expressions," the specific contribution of header insertion and abbreviation expansion to the score penalty was not isolated.
- **What evidence would resolve it:** An ablation study measuring ROUGE-L scores on generated summaries where headers are stripped and abbreviations are statistically normalized or restored to match the reference style.

## Limitations
- **Reproducibility constraints**: Exact meta-prompt structure, complete prompt versions, and specific few-shot examples not fully disclosed; critical parameters like model versions and temperature settings remain unspecified.
- **Scope and generalizability**: Results based on single English dataset; effectiveness on other clinical document types or multilingual settings untested.
- **Evaluation completeness**: Lacks human evaluation for factual accuracy, clinical correctness, or practical utility; no hallucination detection metrics for clinical safety assessment.

## Confidence
**High confidence**: The ISP methodology produces consistent semantic preservation (BERTScore F1=0.85) across iterations. The mechanism of using evaluation feedback to refine prompts is technically sound and reproducible.

**Medium confidence**: The interpretation that ROUGE-BERTScore divergence indicates paraphrasing quality rather than failure is plausible but not independently validated. The structural scaffolding approach shows promise but lacks comparative validation against alternative architectures.

**Low confidence**: Claims about practical impact on clinician-patient communication are speculative without human usability studies. The absence of factual consistency verification means potential clinical risks remain unassessed.

## Next Checks
1. **Error mode categorization**: Manually analyze 50 randomly selected low-ROUGE-L outputs (<0.2) to classify error types (hallucination, missing information, structural mismatch) and assess clinical safety implications.

2. **Perspective ablation study**: Systematically remove each of the 5 clinical perspectives and measure impact on both BERTScore and qualitative coverage to validate the structural scaffolding hypothesis.

3. **Cross-dataset generalization**: Apply the finalized prompt to a different clinical summarization dataset (e.g., MIMIC-III discharge summaries) and compare performance metrics to establish domain transferability.