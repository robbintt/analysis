---
ver: rpa2
title: 'CaRT: Teaching LLM Agents to Know When They Know Enough'
arxiv_id: '2510.08517'
source_url: https://arxiv.org/abs/2510.08517
tags:
- termination
- diagnosis
- information
- cart
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of teaching large language\
  \ models (LLMs) to know when to stop gathering information during multi-step reasoning\
  \ tasks. The core idea is to fine-tune LLMs using counterfactual pairs of trajectories\u2014\
  one where termination is appropriate and a minimally modified version where it is\
  \ not\u2014combined with explicit reasoning traces that justify the termination\
  \ decision."
---

# CaRT: Teaching LLM Agents to Know When They Know Enough

## Quick Facts
- arXiv ID: 2510.08517
- Source URL: https://arxiv.org/abs/2510.08517
- Reference count: 40
- Primary result: CaRT improves LLM termination timing via counterfactual pairs + reasoning traces, achieving up to 0.36 FRQ success rate in medical diagnosis

## Executive Summary
This paper addresses the challenge of teaching LLMs to know when to stop gathering information during multi-step reasoning tasks. The core method, CaRT (Counterfactuals and Reasoning for Termination), fine-tunes LLMs using counterfactual trajectory pairs and explicit reasoning traces that justify termination decisions. Evaluated in interactive medical diagnosis and math problem solving, CaRT outperforms baselines by teaching models to recognize genuine information sufficiency rather than relying on spurious heuristics like conversation length.

## Method Summary
CaRT fine-tunes LLMs using counterfactual trajectory pairs where one version is appropriate for termination and a minimally modified version is not. The method generates hard negative examples by perturbing the last question/answer pair until success rate drops below 30%, then augments these with GPT-4o-generated reasoning traces explaining the termination decision. The fine-tuning uses an 80/20 continue/terminate ratio on balanced datasets. The approach is tested on Qwen2.5-3B-Instruct with optional GRPO RL post-training.

## Key Results
- CaRT achieves up to 0.36 FRQ success rate in medical diagnosis, outperforming SFT and fixed-budget baselines
- Reasoning traces and counterfactual data are both essential—ablation studies show performance drops when either is removed
- Models generalize to OOD data (math after training on medical), performing better than SFT but worse than in-domain CaRT
- RL post-training improves in-domain performance but increases conversation length without accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual trajectory pairs isolate the causal signal for when information becomes sufficient.
- Mechanism: By creating minimally different trajectories—one where termination succeeds, one where it fails—the model cannot rely on spurious features. The only discriminative signal is whether the critical information is present.
- Core assumption: Success rate delta between counterfactual pairs reflects genuine information sufficiency rather than annotation noise.
- Evidence anchors: [abstract] "fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not"; [Section 4.1] "ensuring that the only difference between the pair is the presence or absence of genuinely necessary information"; [Figure 5] Ablation shows counterfactual data produces greatest improvement.

### Mechanism 2
- Claim: Explicit reasoning traces serve as a verbalized value function that improves generalization.
- Mechanism: Generating natural language explanations before termination forces the model to explicitly assess current information vs. expected future value, making classification boundaries more linearly separable.
- Core assumption: Reasoning traces transfer across domains; verbalized value estimation is not overfit to specific task phrasing.
- Evidence anchors: [abstract] "trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning"; [Section 4.2] "reasoning traces serve a similar role as a value function"; [Table 1] Models with reasoning show higher logistic regression test accuracy (0.774 vs. 0.645).

### Mechanism 3
- Claim: The combination of counterfactuals + reasoning smooths termination decisions and reduces brittleness.
- Mechanism: Counterfactuals teach what information matters (sharp sensitivity to critical data). Reasoning teaches why it matters (stable decision boundaries). Together, they produce termination rates that spike at optimal points but transition smoothly.
- Core assumption: The two components are complementary; neither alone is sufficient for robust termination.
- Evidence anchors: [abstract] "ablation studies confirm that both counterfactual data and reasoning traces are essential"; [Section 5.4] "Counterfactual training teaches what information matters... reasoning teaches why it matters by stabilizing decision boundaries"; [Figure 6] Shows CaRT (-reason) has spiky termination rates; full CaRT has smooth curves.

## Foundational Learning

- Concept: **Optimal Stopping / Sequential Decision Theory**
  - Why needed here: The termination problem is formally an optimal stopping problem where the agent must maximize expected reward while minimizing interaction/compute cost.
  - Quick check question: Can you explain why a discount factor γ < 1 penalizes late termination?

- Concept: **Counterfactual Data Augmentation**
  - Why needed here: Core to CaRT's training signal; understanding how to generate minimally different inputs with flipped labels is essential for implementation.
  - Quick check question: If you perturb multiple question-answer pairs instead of one, what happens to the causal isolation property?

- Concept: **Chain-of-Thought as Intermediate Computation**
  - Why needed here: Reasoning traces are not just explainability—they're a computational mechanism that reshapes representations before classification.
  - Quick check question: Why might reasoning traces improve out-of-distribution generalization even if they don't change the final decision?

## Architecture Onboarding

- Component map:
  - Trajectory Generator -> Reward Model -> Counterfactual Generator -> Reasoning Generator -> Termination Policy
  - Qwen models produce trajectories/reasoning
  - Llama-3.1-8B-Instruct labels prefixes with success rates
  - GPT-4o generates reasoning traces
  - Fine-tuned LLM outputs termination decisions

- Critical path:
  1. Generate trajectories with reward labels at each prefix
  2. Identify breakpoints (≥50% success rate increase for medical; termination-better-than-continue for math)
  3. Generate counterfactuals by perturbing final step until success drops below threshold (<30%)
  4. Augment with reasoning traces via prompted LLM
  5. SFT on balanced dataset (paper uses 80% continue / 20% terminate)
  6. Optional: RL post-training with binary reward

- Design tradeoffs:
  - Counterfactual generation cost vs. data quality: More perturbation attempts find cleaner pairs but increase API calls
  - Reasoning verbosity vs. inference overhead: Longer traces improve accuracy but slow down termination decisions at test time
  - Dataset balance ratio: Paper uses 80/20 continue/terminate; different ratios may be needed for domains with different interaction costs
  - SFT-only vs. SFT+RL: RL improves in-domain performance but tends toward longer traces without accuracy gains

- Failure signatures:
  - Termination rate monotonically increases with conversation length → model learned length heuristic, not content sensitivity
  - Termination rate stays flat near zero → model never learned to recognize sufficiency
  - Large gap between train and test logistic regression accuracy → final layer overfitting, reasoning may help
  - OOD performance drops below fixed-budget baseline → counterfactuals didn't generalize

- First 3 experiments:
  1. Validate counterfactual quality: Sample 20 generated pairs; manually verify trajectories differ by only one interaction, success rate labels are correct, and perturbed question plausibly doesn't yield critical information
  2. Ablate each component: Train three models—counterfactuals only, reasoning only, both—and plot termination rate curves over example conversations
  3. Test on held-out domain: After training on medical diagnosis, evaluate on math reasoning to measure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the termination policy be jointly optimized with the information-seeking policy (what to ask/think) rather than treating them as separate components?
- Basis in paper: [explicit] The authors state that CaRT currently assumes a fixed information-seeking policy and that "the effectiveness of termination is inherently coupled with the 'quality' of exploration."
- Why unresolved: The current method modularizes the "seeking" and "stopping" behaviors, which may limit the efficiency of the overall system.
- What evidence would resolve it: A unified training framework that simultaneously improves question quality and termination timing, outperforming the modular CaRT approach.

### Open Question 2
- Question: Does incorporating explicit value estimation or uncertainty modeling into CaRT improve robustness to distribution shifts?
- Basis in paper: [explicit] The authors note that extending CaRT with "explicit value estimation or uncertainty modeling could make termination more robust to distribution shifts."
- Why unresolved: While verbal reasoning acts as an implicit value function, it is unclear if it matches the robustness of quantitative uncertainty estimates when facing out-of-distribution inputs.
- What evidence would resolve it: Experiments comparing the current reasoning-based termination against a variant augmented with explicit uncertainty heads on significantly shifted domain data.

### Open Question 3
- Question: Is it possible to train effective termination models without relying on dense, oracle-labeled reward signals for every intermediate step?
- Basis in paper: [inferred] The methodology relies on querying an external reward model to label the success rate of every conversation prefix, which is resource-intensive.
- Why unresolved: The dependence on a pre-trained reward model for dense labeling may limit CaRT's applicability in domains where such models are unavailable.
- What evidence would resolve it: A variant of CaRT trained successfully using sparse rewards or self-supervised signals without ground-truth intermediate success labels.

## Limitations

- Counterfactual generation process may not scale to domains with high variability or ambiguous success criteria
- Reliance on external LLM (GPT-4o) for reasoning traces introduces dependency on another model's capabilities and potential biases
- Current method modularizes seeking and stopping behaviors, potentially limiting overall system efficiency

## Confidence

- High: The effectiveness of counterfactual pairs and reasoning traces in the specific domains tested (medical diagnosis, math problem solving)
- Medium: The general mechanism by which counterfactuals and reasoning improve termination decisions
- Low: Generalization to other domains and scalability of the counterfactual generation process

## Next Checks

1. Validate counterfactual quality: Sample 20 generated pairs and manually verify that (a) trajectories differ by only one interaction, (b) success rate labels are correct, and (c) the perturbed question plausibly doesn't yield the critical information

2. Ablate each component: Train three models—counterfactuals only, reasoning only, both—and plot termination rate curves over example conversations to replicate the paper's Figure 6 pattern

3. Test on held-out domain: After training on medical diagnosis, evaluate on math reasoning (or vice versa) to measure generalization; expect performance between SFT baseline and in-domain CaRT based on OOD results in Figure 3b