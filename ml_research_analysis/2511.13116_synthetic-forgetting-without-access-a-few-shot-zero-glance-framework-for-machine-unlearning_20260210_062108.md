---
ver: rpa2
title: 'Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for
  Machine Unlearning'
arxiv_id: '2511.13116'
source_url: https://arxiv.org/abs/2511.13116
tags:
- data
- unlearning
- classes
- retained
- gfoes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GFOES addresses machine unlearning under the few-shot zero-glance
  setting, where only minimal retained data is available and forget data is entirely
  inaccessible. The method introduces Optimal Erasure Samples (OES) generated by a
  Generative Feedback Network (GFN) to induce forgetting without accessing the target
  data, and employs a two-phase fine-tuning strategy to balance aggressive forgetting
  with utility preservation.
---

# Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning

## Quick Facts
- arXiv ID: 2511.13116
- Source URL: https://arxiv.org/abs/2511.13116
- Reference count: 37
- Primary result: Achieves complete forgetting (ADf=0) while maintaining high retained accuracy (ADr up to 89.23%) in few-shot zero-glance setting

## Executive Summary
GFOES addresses machine unlearning under the challenging few-shot zero-glance setting where only minimal retained data is available and forget data is entirely inaccessible. The method introduces Optimal Erasure Samples (OES) generated by a Generative Feedback Network (GFN) to induce forgetting without accessing the target data, and employs a two-phase fine-tuning strategy to balance aggressive forgetting with utility preservation. Experiments on CIFAR-10, CIFAR-100, and Fashion-MNIST show GFOES outperforms state-of-the-art methods in achieving complete forgetting while maintaining high accuracy on retained data.

## Method Summary
GFOES operates in two phases: first, a Generative Feedback Network generates Optimal Erasure Samples (OES) that maximize classification loss on forgotten classes under the original model; second, a two-phase fine-tuning strategy aggressively disrupts forgotten-class representations followed by conservative utility recovery. The GFN uses a stabilized joint objective with adaptive balancing to ensure stable training, while the fine-tuning employs asymmetric learning rates (high then low) to achieve complete forgetting without significant utility loss.

## Key Results
- Achieves complete forgetting (ADf = 0) across all benchmarks while maintaining high retained accuracy (ADr up to 89.23%)
- Outperforms state-of-the-art methods including distillation-based and anchor-based approaches
- Demonstrates significant time efficiency compared to retraining baselines
- Effectively removes class-specific representations at both logit and feature levels as verified by t-SNE and GradCAM visualizations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic samples can induce forgetting without accessing the original forget data by generating adversarial-like inputs that disrupt class-specific representations.
- **Mechanism:** Optimal Erasure Samples (OES) are generated to maximize classification loss on forgotten classes under the original model. When the model is fine-tuned on these samples (with forgotten-class labels), the decision boundaries for those classes are corrupted without directly accessing the true data distribution.
- **Core assumption:** The model's internal representations of forgotten classes can be disrupted by training on adversarially crafted samples that share labels with those classes, even when the samples do not resemble true data.
- **Evidence anchors:**
  - [Section 3.2]: "Instead of approximating the true data distribution, we deliberately generate synthetic samples that deviate from it. These adversarial-like inputs, paired with the target labels, mislead the model into associating irrelevant features with the forgotten classes."
  - [Section 4.3, Figure 3]: t-SNE visualizations show GFOES disperses forgotten-class feature clusters while preserving retained-class structure.
  - [Corpus]: Related work (OPC, arXiv:2507.07754) addresses "shallow forgetting" where methods adjust model response without removing representations—GFOES targets this limitation.
- **Break condition:** If the generator fails to produce samples that induce high loss on the original model, or if OES inadvertently damage retained-class boundaries beyond recovery in phase 2.

### Mechanism 2
- **Claim:** The stabilised joint objective prevents gradient instability during generator training by replacing subtractive loss with a reciprocal formulation.
- **Mechanism:** The GFN optimizes L_GFN = λ · (1/L_max) + (1-λ) · L_min. The reciprocal of L_max bounds gradients (∇_ϕ(1/L_max) = -1/(L_max²) · ∇_ϕ L_max), preventing divergence when the forgetting loss grows large. The trade-off coefficient λ adapts via gradient ascent to balance forgetting vs. retention dynamically.
- **Core assumption:** Cross-entropy losses remain bounded and differentiable; the forgetting loss stays above some ε > 0 to avoid division instability.
- **Evidence anchors:**
  - [Section 3.3, Eq. 8-11]: Explicit derivation of reciprocal form and gradient properties.
  - [Appendix B]: Theoretical convergence proof under Assumptions A1-A3, showing O(1/T) convergence rate.
  - [Corpus]: No direct corpus evidence for this specific stabilization technique.
- **Break condition:** If L_max → 0, the reciprocal term explodes; Assumption A3 requires L_max ≥ ε > 0.

### Mechanism 3
- **Claim:** Two-phase fine-tuning with asymmetric learning rates enables aggressive representation disruption followed by utility restoration without relearning forgotten knowledge.
- **Mechanism:** Phase 1 (Erasure) uses high LR (4e-3) on OES + retained data to aggressively overwrite forgotten-class representations. Phase 2 (Recovery) uses low LR (4e-4) on retained data only, refining decision boundaries without reintroducing erased knowledge since OES is excluded.
- **Core assumption:** Aggressive updates can damage retained-class structure, but this damage can be repaired with conservative fine-tuning that doesn't reintroduce forgotten-class information.
- **Evidence anchors:**
  - [Section 3.4]: "The strategy is motivated by the observation that aggressive updates are needed to disrupt representations of forgotten classes, yet they risk damaging the structure of retained ones."
  - [Table 3]: Ablation shows OES+Rl (uniform high LR) or OES+Rs (uniform low LR) both underperform the phased approach.
  - [Corpus]: Train Once, Forget Precisely (arXiv:2506.14515) uses anchored optimization for post-hoc unlearning but doesn't employ phased LR strategy.
- **Break condition:** If recovery phase uses too high a learning rate or includes OES, the model may relearn forgotten information; if too low, utility recovery may be insufficient.

## Foundational Learning

- **Gradient ascent for unlearning:**
  - Why needed here: GFOES's maximise branch uses gradient ascent logic (inducing high loss) on forgotten classes. Understanding how gradient direction affects knowledge retention/erasure is essential.
  - Quick check question: What happens if you perform gradient descent instead of ascent on the forget-set loss?

- **Minimax optimization with dynamic weighting:**
  - Why needed here: The GFN solves a minimax problem where λ adapts to balance competing objectives. Understanding how gradient-based λ updates work helps debug convergence issues.
  - Quick check question: Why does the paper use projected gradient ascent for λ rather than treating it as a fixed hyperparameter?

- **Feature-level vs. logit-level forgetting:**
  - Why needed here: The paper emphasizes representation-based evaluation because logit-level accuracy alone doesn't guarantee internal knowledge removal.
  - Quick check question: If a model achieves ADf=0 but retains well-separated feature clusters for forgotten classes, what attack could recover performance?

## Architecture Onboarding

- **Component map:**
  ```
  [Retained Data D_rs] ──┐
                         │
  [Random Noise z] → [Generator G(z;ϕ)] → [OES]
                         │                        │
                         ↓                        ↓
                    [Maximise Branch]        [Erasure Phase]
                    (L_max on M₀)           (High LR on OES + D_rs)
                         │                        │
                         └────→ [Stabilised       → [Recovery Phase]
                                 Joint Loss]       (Low LR on D_rs only)
                                    │                    │
                                    ↓                    ↓
                               [λ updater]         [Unlearned Model M*]
  ```

- **Critical path:** Generator training (GFN) → OES generation → Erasure phase (1 epoch) → Recovery phase (1 epoch). Total: ~20 epochs for GFN + 2 epochs for unlearning.

- **Design tradeoffs:**
  - Higher λ → more aggressive forgetting but risk to utility; lower λ → safer but incomplete forgetting. Paper uses adaptive λ to auto-balance.
  - Larger η_high in erasure → faster forgetting but more collateral damage requiring longer recovery.
  - More GFN epochs → better OES quality but higher time cost.

- **Failure signatures:**
  - ADf > 0 after unlearning: OES quality insufficient; check L_max during GFN training.
  - ADr drops significantly: Recovery phase failed; check if η_low is too aggressive or if retained data is too sparse.
  - Generator divergence: L_max approaching 0; verify loss positivity and gradient clipping.

- **First 3 experiments:**
  1. **Single-class ablation on CIFAR-10:** Train original ResNet-18, then unlearn 1 class with 5% retained data. Verify ADf=0 and ADr>85%. Compare OES+Rls vs OES+Rl vs OES+Rs to validate two-phase necessity.
  2. **λ sensitivity test:** Run GFN with fixed λ values (0.3, 0.5, 0.7) vs. adaptive λ. Plot convergence of L_max and L_min over training epochs.
  3. **Representation-level verification:** After unlearning, extract features and train a linear classifier on frozen features for forgotten classes. If accuracy recovers, representation-level knowledge persists—indicates shallow forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the two-phase fine-tuning strategy be theoretically grounded to guarantee stability and convergence?
- Basis in paper: [explicit] The paper states on Page 5 that the design of the two-phase fine-tuning procedure is "not grounded in formal theory," despite being empirically validated.
- Why unresolved: Without theoretical justification, the aggressive learning rate switch relies on heuristic tuning, which may not generalize or could cause instability in different data distributions or architectures.
- What evidence would resolve it: A formal proof or theoretical framework defining the conditions under which the erasure and recovery phases balance forgetting and utility without divergence.

### Open Question 2
- Question: Can GFOES be adapted for instance-level unlearning rather than class-specific unlearning?
- Basis in paper: [inferred] The methodology (Section 3.1) explicitly formulates the forget set $D_f$ as instances belonging to a subset of categories $Y_f$, and the OES generation relies on these class labels to synthesize erasure samples.
- Why unresolved: Real-world "Right to be Forgotten" requests often target specific data points rather than entire semantic classes, a scenario the current label-driven generative process does not address.
- What evidence would resolve it: An extension of the GFN that generates erasure samples conditional on instance-level features or embeddings rather than class labels, validated on instance-removal benchmarks.

### Open Question 3
- Question: Is the method robust to the black-box setting where model gradients are inaccessible?
- Basis in paper: [inferred] The Generative Feedback Network (Section 3.3) and the fine-tuning procedure (Section 3.4) rely fundamentally on calculating gradients $\nabla_\theta$ with respect to the model parameters.
- Why unresolved: Many Machine Learning as a Service (MLaaS) scenarios provide only query access (inputs/outputs) rather than white-box access to weights, limiting the applicability of gradient-dependent unlearning.
- What evidence would resolve it: A modification of the GFN using gradient-free optimization or zeroth-order estimation to generate Optimal Erasure Samples without accessing internal model parameters.

## Limitations
- The stability of the reciprocal loss formulation remains theoretically untested in non-convex deep learning settings, with Assumption A3 unverified in practice.
- The adaptive λ update rule lacks ablation studies showing its superiority over fixed λ scheduling.
- The GFN architecture is underspecified, making exact replication challenging.
- Time efficiency claims lack direct comparison to retraining on retained data only.

## Confidence

- **Mechanism 1 (Adversarial OES):** Medium confidence. The adversarial generation concept is sound, but the claim that synthetic samples "need not resemble" real data while still achieving perfect forgetting requires more rigorous verification.
- **Mechanism 2 (Stabilised Joint Objective):** Low confidence. The reciprocal formulation is mathematically elegant, but its necessity and stability in practice are unproven.
- **Mechanism 3 (Two-phase Fine-tuning):** High confidence. The empirical results strongly support the necessity of asymmetric learning rates, with clear ablation showing uniform LR performs worse.

## Next Checks

1. **Representation-level verification test:** After unlearning, train a linear probe on frozen features for forgotten classes. If accuracy recovers significantly, the method achieves only shallow forgetting despite ADf=0, contradicting the paper's claim of representation-level removal.

2. **L_max stability analysis:** During GFN training, plot L_max values across epochs. If L_max approaches zero or becomes unstable, the reciprocal term in the joint objective will explode, indicating the stabilization mechanism may not be robust.

3. **Retraining baseline comparison:** For the same retained data fraction (5-20%), retrain a model from scratch on retained data only. Compare both accuracy and time to GFOES. If retraining achieves similar or better ADr with lower computational cost, GFOES's efficiency advantage is questionable.