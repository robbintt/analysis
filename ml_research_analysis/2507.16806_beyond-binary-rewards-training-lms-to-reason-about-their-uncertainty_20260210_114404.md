---
ver: rpa2
title: 'Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty'
arxiv_id: '2507.16806'
source_url: https://arxiv.org/abs/2507.16806
tags:
- confidence
- answer
- movies
- calibration
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RLCR (Reinforcement Learning with Calibration
  Rewards), a method that trains language models to reason about uncertainty by generating
  both answers and calibrated confidence estimates. The key innovation is a reward
  function that combines binary correctness with a Brier score to incentivize both
  accuracy and well-calibrated confidence.
---

# Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty

## Quick Facts
- arXiv ID: 2507.16806
- Source URL: https://arxiv.org/abs/2507.16806
- Reference count: 40
- Key outcome: RLCR achieves better calibration than standard RL training while maintaining accuracy, using a reward combining correctness and Brier score

## Executive Summary
This paper introduces RLCR (Reinforcement Learning with Calibration Rewards), a method that trains language models to jointly optimize correctness and calibration by generating both answers and confidence estimates. The key innovation is a reward function that combines binary correctness with a Brier score, creating a proper scoring rule that incentivizes calibrated predictions without sacrificing accuracy. Experiments across diverse QA tasks show RLCR matches baseline task accuracy while substantially improving calibration, both in-domain and out-of-domain, outperforming standard RL training and classifier-based confidence methods.

## Method Summary
RLCR trains language models to generate structured outputs containing answers and numerical confidence estimates using a modified GRPO algorithm. The model receives a reward combining binary correctness with a Brier score term that penalizes miscalibration. The method uses a single model for both solution generation and confidence calibration, with structured prompts that include reasoning analysis components. Training involves generating multiple responses per prompt and updating the policy based on the combined reward, with format enforcement through additional reward terms.

## Key Results
- RLCR achieves better calibration than standard RL training on both in-domain and out-of-domain tasks
- The method maintains task accuracy while improving calibration, showing no accuracy-cost tradeoff
- Confidence-weighted ensembling at test time provides additional accuracy and calibration gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Brier score term creates calibration incentive through proper scoring rule properties
- **Mechanism**: The Brier score `(q - 1_{y≡y*})²` is a proper scoring rule—it is minimized when confidence `q` equals the true probability `p_y` of correctness. By including this in the reward, the model learns to output confidence estimates that reflect actual accuracy rates.
- **Core assumption**: Models can learn to map internal uncertainty states to numerical confidence values through RL optimization
- **Evidence anchors**:
  - [abstract] "They are trained to optimize a reward function that augments a binary correctness score with a Brier score—a scoring rule for confidence estimates that incentivizes calibrated prediction."
  - [section 3] "For any y, the expected reward E[1_y≡y*][R_RLCR(y, q, y*,)] is maximized when q = p_y"
  - [corpus] Related work "UCPO: Uncertainty-Aware Policy Optimization" addresses similar overconfidence issues in GRPO

### Mechanism 2
- **Claim**: Bounded scoring rules preserve correctness incentive while adding calibration
- **Mechanism**: The Brier score is bounded in `[0,1]`, ensuring that the correctness reward term `λc` can dominate the calibration term when `λ ≥ 1`. This prevents the model from sacrificing accuracy for calibration—unlike log-loss which is unbounded and can incentivize incorrect predictions.
- **Core assumption**: The bound condition `S(p,1) - S(p,0) ≤ λ` holds uniformly across all probability values
- **Evidence anchors**:
  - [section 3] "Among all calibrated predictions (y, p_y), expected reward is maximized by the prediction whose success probability p_y is greatest"
  - [appendix A] "If S(p,1) - S(p,0) is bounded, then there exists a finite λ > 0 such that the reward function...jointly incentivizes calibration and correctness"
  - [corpus] "Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision" provides theoretical counterpoint showing binary supervision alone is insufficient

### Mechanism 3
- **Claim**: Explicit uncertainty reasoning in chain-of-thought improves calibration through shared representations
- **Mechanism**: By requiring models to generate `<analysis>` tags that reason about uncertainty before outputting confidence, RLCR forces the model to develop internal representations that support both solution generation and calibration. This shared representation enables better generalization to OOD tasks.
- **Core assumption**: The uncertainty reasoning chains faithfully reflect the model's internal uncertainty states rather than being post-hoc rationalizations
- **Evidence anchors**:
  - [section 4.2] "Using a single model for both solution generation and calibration allows the calibration task to leverage internal representations used by the solution generating process"
  - [section 4.5] Analysis classifiers trained on RLCR outputs outperform baseline classifiers at smaller model sizes, suggesting reasoning chains contain calibration-relevant information

## Foundational Learning

**Concept: Proper Scoring Rules (Brier Score)**
- Why needed here: Understanding why Brier score incentivizes calibrated predictions and why alternatives like log-loss fail
- Quick check question: Given a model that's correct 70% of the time, what confidence value minimizes the expected Brier score?

**Concept: Expected Calibration Error (ECE)**
- Why needed here: Primary metric for measuring calibration quality; used throughout experiments to compare methods
- Quick check question: If a model assigns 0.9 confidence to 100 predictions and gets 90% accuracy, what's the ECE for that bin?

**Concept: GRPO (Group Relative Policy Optimization)**
- Why needed here: Base RL algorithm used; understanding its advantage normalization helps explain modifications in RLCR
- Quick check question: How does GRPO's group-based advantage estimation differ from PPO's rollout-based estimation?

## Architecture Onboarding

**Component map**: Input prompt with structured format instructions -> Model generates structured output with reasoning, answer, and confidence -> Reward computed from correctness and Brier score -> GRPO update with modified advantage calculation

**Critical path**:
1. Prompt engineering for structured output format (use Long RLCR prompt for HotpotQA, Simple for Math)
2. Generate 32 responses per prompt with temperature 0.7
3. Extract answer and confidence from structured tags
4. Compute correctness (exact match or math-verify) and Brier score
5. Update policy via GRPO with batch size 2048

**Design tradeoffs**:
- **Long vs Simple prompts**: Long prompts with detailed analysis guidelines improve calibration on complex reasoning (HotpotQA) but add token overhead
- **SFT warmup vs direct RL**: SFT warmup (500 examples with DeepSeek-R1 generated analyses) improves analysis quality but may cause catastrophic forgetting on OOD tasks
- **Single model vs classifier ensemble**: RLCR uses one model (simpler deployment) vs RLVR+classifier (two-model approach, more expensive)

**Failure signatures**:
- Generic analysis text that doesn't reference specific reasoning steps → indicates weak uncertainty reasoning
- Confidence sums > 1 for mutually exclusive answers → indicates overconfidence
- High variance in confidence across multiple reasoning chains for same answer → indicates unstable calibration

**First 3 experiments**:
1. Replicate HotpotQA-Modified experiment with 0/1/2 relevant paragraphs to test uncertainty reasoning under information asymmetry
2. Ablation study comparing Brier score vs log-loss to verify boundedness requirement
3. Test confidence-weighted majority voting on held-out datasets to validate test-time scaling benefits

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can language models be trained to maintain inter-solution consistency, ensuring they do not assign high confidence to multiple contradictory answers simultaneously?
- Basis in paper: [explicit] The Conclusion states that despite RLCR improvements, "models may still assign high confidence to multiple contradictory answers," and there remains "significant room for improvement" in this area.
- Why unresolved: While RLCR improves average calibration, it does not explicitly enforce the constraint that confidence sums for mutually exclusive answers should equal 1, leading to inconsistent belief states.
- What evidence would resolve it: A modification to the reward function or a post-hoc normalization technique that drives the "confidence sum" metric (defined in Section 4.6) closer to 1.0 without sacrificing task accuracy.

**Open Question 2**
- Question: Can the trade-off between the calibration benefits of SFT warmup and the OOD generalization costs (catastrophic forgetting) be mitigated?
- Basis in paper: [explicit] Section 4.3 notes that while SFT+RLCR achieves the best calibration, it suffers from "reduced generalization accuracy, possibly due to catastrophic forgetting," whereas pure RLCR offers a stronger trade-off.
- Why unresolved: The paper identifies the trade-off but does not propose a method to retain the high-quality uncertainty analysis from SFT while preventing the drop in out-of-distribution reasoning accuracy.
- What evidence would resolve it: Experiments showing that techniques like replay buffers or regularization during the SFT warmup phase allow SFT+RLCR models to match the OOD accuracy of the pure RLCR models.

**Open Question 3**
- Question: How does model capacity determine the utility of explicit uncertainty reasoning chains for calibration?
- Basis in paper: [explicit] Section 4.5 concludes that "broader questions about the relationship between classifier capacity and CoT contents are an important topic for future work."
- Why unresolved: Results show small classifiers benefit from uncertainty CoT while large ones do not, but it is unclear if this is due to saturation of reasoning ability or redundancy in the representations.
- What evidence would resolve it: A scaling law analysis measuring the divergence in calibration error between "Analysis Classifiers" and "Baseline Classifiers" across a continuous spectrum of model sizes.

## Limitations
- The method requires structured reasoning output formats, which may not generalize to domains where uncertainty reasoning doesn't naturally fit into a chain-of-thought format
- While RLCR shows improved calibration on OOD datasets, the improvement is modest for harder tasks like GPQA
- The reward function is sensitive to the choice of scoring rule and requires boundedness properties to maintain accuracy

## Confidence

**High Confidence**: RLCR achieves better calibration than standard RL training on both in-domain and OOD tasks; the Brier score reward function provides a proper scoring rule incentive for calibrated predictions; RLCR maintains task accuracy while improving calibration

**Medium Confidence**: Shared representation between solution generation and calibration improves OOD generalization; confidence-weighted ensembling provides additional calibration gains at test time; analysis quality improves with SFT warmup from DeepSeek-R1 but may harm OOD performance

**Low Confidence**: The uncertainty reasoning chains faithfully represent internal model uncertainty rather than being post-hoc rationalizations; single-model RLCR performs comparably to two-model approaches (RLVR + classifier) for all practical purposes

## Next Checks
1. **Ablation on Scoring Rule Choice**: Run HotpotQA experiments comparing RLCR with Brier score vs log-loss reward to verify that boundedness property is essential for maintaining accuracy while improving calibration.

2. **Out-of-Domain Generalization Stress Test**: Evaluate RLCR on a diverse set of 10+ unseen QA datasets (including different domains like science, history, and creative writing) to quantify calibration generalization limits and identify failure patterns.

3. **Analysis Faithfulness Validation**: Implement automated metrics to measure whether uncertainty reasoning chains actually influence final confidence (e.g., token importance analysis, counterfactual reasoning with perturbed analysis text) to verify the mechanism assumption about shared representations.