---
ver: rpa2
title: 'Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer
  Models'
arxiv_id: '2508.10243'
source_url: https://arxiv.org/abs/2508.10243
tags:
- head
- backdoor
- attack
- malicious
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Head-wise Pruning and Malicious Injection (HPMI),
  the first retraining-free backdoor attack on transformer models that maintains the
  original architecture. The method identifies and prunes the least important attention
  head in each transformer layer, then injects a pre-trained malicious head to establish
  a backdoor.
---

# Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models

## Quick Facts
- arXiv ID: 2508.10243
- Source URL: https://arxiv.org/abs/2508.10243
- Reference count: 21
- Primary result: First retraining-free backdoor attack on transformers that maintains original architecture while achieving >99.55% attack success rate

## Executive Summary
This paper introduces Head-wise Pruning and Malicious Injection (HPMI), a novel backdoor attack on transformer models that operates without retraining. Unlike previous approaches that modify the entire architecture or require extensive retraining, HPMI identifies and prunes the least important attention head in each layer, then injects a pre-trained malicious head to establish the backdoor. The method achieves attack success rates exceeding 99.55% while preserving clean accuracy and successfully bypasses four state-of-the-art defense mechanisms. The attack demonstrates significant resilience compared to retraining-based approaches and shows strong resistance to detection and removal by leading defense techniques.

## Method Summary
HPMI operates through a two-stage process that targets transformer attention heads. First, it identifies the least important attention head in each transformer layer using importance scores, then prunes these heads from the model. Second, it injects pre-trained malicious heads that activate on specific trigger patterns to produce attacker-desired outputs. This approach maintains the original transformer architecture while establishing a functional backdoor. The method requires only a small subset of data and basic architecture knowledge, making it practical for real-world deployment. By avoiding retraining, HPMI reduces computational overhead while maintaining high attack effectiveness.

## Key Results
- Achieves attack success rates exceeding 99.55% across multiple datasets and models
- Successfully bypasses four state-of-the-art defense mechanisms
- Maintains clean accuracy while establishing the backdoor
- Demonstrates greater resilience than retraining-based attack methods
- Requires only a small subset of data and basic architecture knowledge

## Why This Works (Mechanism)
HPMI exploits the inherent redundancy in transformer architectures where attention heads can be pruned without significant performance degradation. By identifying and removing the least important heads, the attack creates space for malicious heads to be injected while maintaining model functionality. The malicious heads are specifically trained to activate on trigger patterns and produce attacker-desired outputs, establishing a covert backdoor. The pruning process ensures the backdoor remains hidden within the normal operation of the model, while the injection of pre-trained malicious heads allows for immediate functionality without the computational cost of retraining.

## Foundational Learning

**Transformer Attention Heads**: Individual components within transformer layers that learn different attention patterns. Why needed: HPMI specifically targets and manipulates these heads for the attack. Quick check: Verify understanding of multi-head attention mechanism and head specialization.

**Attention Head Pruning**: The process of removing specific attention heads from transformer models. Why needed: HPMI uses pruning to create space for malicious head injection while maintaining functionality. Quick check: Understand criteria for determining head importance and pruning impact.

**Backdoor Attacks**: Methods of inserting hidden functionality into models that activate on specific triggers. Why needed: HPMI represents a novel approach to establishing backdoors in transformers. Quick check: Distinguish between retraining-based and architecture-preserving attack methods.

**Model Architecture Knowledge**: Understanding of the target model's structure and components. Why needed: HPMI requires knowledge of transformer architecture to identify pruning targets. Quick check: Identify key architectural components in BERT, RoBERTa, and DeBERTa models.

## Architecture Onboarding

**Component Map**: Input -> Transformer Layers (with Attention Heads) -> Output. HPMI modifies this by: Input -> [Pruning Stage] -> Transformer Layers (with Malicious Heads) -> Output.

**Critical Path**: The attack targets attention heads as the critical path for establishing the backdoor, rather than modifying the entire architecture or requiring retraining.

**Design Tradeoffs**: HPMI trades computational efficiency (no retraining) for architectural modification (head pruning and injection), achieving a balance between attack effectiveness and stealth.

**Failure Signatures**: The attack may fail if the pruning process removes heads that are more critical than estimated, or if the malicious heads cannot effectively replace the functionality of pruned heads.

**First Experiments**:
1. Test HPMI on BERT model with a simple classification task to verify basic functionality
2. Evaluate attack success rate on a dataset with clear trigger patterns
3. Assess clean accuracy preservation after HPMI application

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes knowledge of model architecture, which may not be available in practice
- Effectiveness across diverse transformer architectures beyond tested models remains unexplored
- Claim of resistance to "all" state-of-the-art defenses should be tempered as new defenses may emerge
- Computational overhead of pruning process during inference is not discussed

## Confidence

**Attack effectiveness**: High - Well-supported by experimental results showing >99.55% attack success rates
**Defense evasion**: High - Demonstrated bypassing of four state-of-the-art defense mechanisms
**Practicality and efficiency**: Medium - Claims require validation in more complex real-world scenarios
**Theoretical analysis**: Medium - Framework presented but applicability to all defense mechanisms may be limited

## Next Checks

1. Test HPMI on a broader range of transformer architectures, including those with different attention mechanisms and model sizes
2. Evaluate the attack's performance under realistic threat models where the attacker has limited knowledge of the target model's architecture and training data
3. Investigate the computational overhead introduced by the pruning process during inference and its impact on model efficiency