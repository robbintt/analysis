---
ver: rpa2
title: Deconstructing Self-Bias in LLM-generated Translation Benchmarks
arxiv_id: '2509.26600'
source_url: https://arxiv.org/abs/2509.26600
tags:
- source
- lurawinak
- translation
- self-bias
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates self-bias in LLM-generated translation
  benchmarks, showing that when models generate test sets and evaluate translations,
  they systematically favor their own outputs. The bias arises from two additive sources:
  the generated test data (LLM-as-a-testset) and the evaluation method (LLM-as-an-evaluator),
  with their combination amplifying the effect.'
---

# Deconstructing Self-Bias in LLM-generated Translation Benchmarks

## Quick Facts
- arXiv ID: 2509.26600
- Source URL: https://arxiv.org/abs/2509.26600
- Authors: Wenda Xu; Sweta Agrawal; VilÃ©m Zouhar; Markus Freitag; Daniel Deutsch
- Reference count: 27
- Primary result: Models systematically favor their own outputs when generating test sets and evaluating translations

## Executive Summary
This paper investigates self-bias in LLM-generated translation benchmarks, demonstrating that when models generate test sets and evaluate translations, they systematically favor their own outputs. The bias arises from two additive sources: the generated test data (LLM-as-a-testset) and the evaluation method (LLM-as-an-evaluator), with their combination amplifying the effect. The bias is more pronounced in into-English translation directions where the model's generation capabilities in the source language are limited. Low diversity in generated source texts contributes to self-bias, as models tend to produce homogeneous content with repetitive patterns and stylistic traits. Improving the diversity of these generated source texts can mitigate some of the observed self-bias.

## Method Summary
The study uses three state-of-the-art LLMs across six low-to-medium resource language directions and quantifies bias through systematic ranking comparisons. The research design isolates the two sources of bias - LLM-as-a-testset and LLM-as-an-evaluator - to understand their individual and combined effects. The methodology involves generating test sets with LLMs, evaluating translations using LLM-based metrics, and comparing how different models rank each other's outputs versus their own. The analysis focuses on both the quality of generated source texts and the evaluation process itself.

## Key Results
- Models systematically favor their own outputs when both generating test data and evaluating translations
- Two additive bias sources: LLM-as-a-testset and LLM-as-an-evaluator, which compound when combined
- Bias is more pronounced in into-English translation directions where source language generation is limited
- Low diversity in generated source texts contributes to self-bias through repetitive patterns and stylistic traits
- Improving source text diversity can mitigate some self-bias effects

## Why This Works (Mechanism)
The mechanism behind self-bias operates through two complementary pathways. First, when LLMs generate test data (LLM-as-a-testset), they produce source texts that reflect their own linguistic patterns, stylistic preferences, and generation capabilities. These texts tend to have limited diversity and contain repetitive structures that align with the generating model's strengths. Second, when the same or similar LLMs evaluate translations (LLM-as-an-evaluator), they use criteria and preferences that inherently favor outputs matching their own generation patterns. The combination of these two effects creates a compounding bias where models not only produce favorable test conditions but also evaluate translations through a lens that privileges their own output characteristics.

## Foundational Learning

**LLM-as-a-testset**: Understanding how LLMs generate evaluation data is crucial because the quality and diversity of source texts directly impacts translation evaluation fairness. Quick check: Analyze the entropy and vocabulary diversity of generated source texts across different models.

**LLM-as-an-evaluator**: The evaluation methodology using LLMs introduces systematic preferences based on the evaluator's own generation capabilities and linguistic patterns. Quick check: Compare evaluation scores across different model families to identify consistent preference patterns.

**Translation direction bias**: The asymmetry between source and target language capabilities affects bias manifestation, with into-English directions showing stronger self-bias due to source language generation limitations. Quick check: Test translation pairs with varying source-to-target capability ratios to map bias strength.

**Additive bias effects**: The compounding nature of combined bias sources requires understanding how individual effects interact rather than simply summing. Quick check: Design experiments that isolate and combine bias sources systematically to measure interaction effects.

**Diversity metrics**: Quantifying source text diversity through appropriate metrics is essential for understanding and mitigating self-bias. Quick check: Implement multiple diversity measures (n-gram diversity, syntactic variety, semantic coverage) and correlate with bias levels.

## Architecture Onboarding

Component map: Data Generation -> Source Text Creation -> Translation Generation -> Evaluation -> Ranking Comparison
Critical path: LLM-as-a-testset -> LLM-as-an-evaluator -> Self-Bias Manifestation
Design tradeoffs: Using LLMs for both generation and evaluation trades automation efficiency for potential bias introduction; human references would eliminate one bias source but increase cost and scalability limitations
Failure signatures: Systematic ranking inflation for self-generated outputs, reduced differentiation between model capabilities, correlation between source text homogeneity and evaluation bias
First experiments: 1) Generate identical test sets using different LLMs and compare evaluation consistency; 2) Vary source text diversity parameters systematically while holding evaluation method constant; 3) Cross-evaluate translations using different model families to isolate evaluator-specific biases

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses exclusively on low-to-medium resource language directions, limiting generalizability to high-resource pairs
- Study uses only three specific LLMs, constraining external validity across different model architectures
- Quantification relies on relative ranking comparisons rather than absolute performance metrics, potentially obscuring absolute quality differences

## Confidence
High: Self-bias exists and is measurable across tested conditions
Medium: Additive bias effects and diversity mitigation strategies are well-supported but require broader validation
Low: Generalizability to other language pairs and model families needs further investigation

## Next Checks
1. Replicate study with human-generated reference translations to establish baseline bias levels and determine if self-bias is unique to LLM-generated benchmarks
2. Extend experiments to high-resource language pairs and additional model families to test generalizability of additive bias effects
3. Conduct controlled experiments varying source text diversity parameters systematically to identify optimal diversity ranges that minimize bias without compromising translation quality