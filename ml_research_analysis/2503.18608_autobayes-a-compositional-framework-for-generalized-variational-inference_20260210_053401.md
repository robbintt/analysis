---
ver: rpa2
title: 'AutoBayes: A Compositional Framework for Generalized Variational Inference'
arxiv_id: '2503.18608'
source_url: https://arxiv.org/abs/2503.18608
tags:
- bayesian
- open
- inference
- rule
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AutoBayes, a compositional framework for generalized
  variational inference that clarifies the different parts of a probabilistic model,
  how they interact, and how they compose. The key insight is that both exact Bayesian
  inference and loss functions used in variational inference satisfy chain rules similar
  to reverse-mode automatic differentiation, enabling compositional construction and
  optimization of models.
---

# AutoBayes: A Compositional Framework for Generalized Variational Inference

## Quick Facts
- **arXiv ID**: 2503.18608
- **Source URL**: https://arxiv.org/abs/2503.18608
- **Reference count**: 7
- **Primary result**: Introduces AutoBayes, a compositional framework for generalized variational inference that clarifies the different parts of a probabilistic model, how they interact, and how they compose.

## Executive Summary
AutoBayes introduces a compositional framework for generalized variational inference that clarifies the different parts of a probabilistic model, how they interact, and how they compose. The key insight is that both exact Bayesian inference and loss functions used in variational inference satisfy chain rules similar to reverse-mode automatic differentiation, enabling compositional construction and optimization of models. The framework introduces open models (directed probabilistic models with separate observed/unobserved/latent spaces), Bayesian lenses (models paired with local inversions), and statistical games (lenses equipped with energy and entropy terms). These components compose according to a chain rule, allowing complex models to be built from simpler parts without manually deriving loss functions.

## Method Summary
The method involves defining open models as measure kernels with latent/observed/unobserved spaces, composing them sequentially or in parallel, attaching local inversions to form Bayesian lenses, equipping with energy/entropy to form statistical games, and optimizing parameterized games via gradient descent on free energy. The framework leverages categorical compositionality where models compose via ˝ (sequential) and ⊗ (parallel), lenses compose via ∘|, and games compose via ˛. Statistical games decompose losses into energy (likelihood) and entropy (regularizer) terms that compose differently, enabling automatic construction of complex loss functions from simple parts.

## Key Results
- Open models, Bayesian lenses, and statistical games form a bicategorical structure that composes according to chain rules analogous to reverse-mode automatic differentiation
- Variational free energy (VFE) decomposes into energy and entropy terms that compose differently, enabling compositional construction of complex loss functions
- Parameterized statistical games can be optimized compositionally, though with technical challenges around gradient computation due to lax functoriality
- The framework generalizes Bayesian networks and provides a mathematically rigorous foundation for building and optimizing probabilistic models in a modular way

## Why This Works (Mechanism)

### Mechanism 1: Compositional Loss Function Construction
- Claim: Complex variational loss functions can be automatically constructed from simple parts using a chain rule analogous to reverse-mode automatic differentiation.
- Mechanism: Statistical games decompose losses into energy (likelihood) and entropy (regularizer) terms. Energy terms compose by simple addition: l_dc(x,a,y,b,z) = l_c(x,a,y) + l_d(y,b,z). Entropy terms compose via an expectation-weighted chain rule: H_dc(π,z) = E[H_c(π,y)] + H_d(c*π,z). This separation preserves the variational free energy structure compositionally.
- Core assumption: Loss functions for variational inference can be meaningfully decomposed into energy and entropy components that have distinct compositional behaviors.
- Evidence anchors:
  - [abstract] "both exact Bayesian inference and the loss functions used in variational inference satisfy chain rules similar to reverse-mode automatic differentiation"
  - [section 4, Theorem 23] Proves F_dc(π,z) = E[F_c(π,y)] + F_d(c*π,z), establishing the chain rule for generalized free energy
  - [corpus] Weak direct corpus support; related work on variational inference optimization exists but doesn't address compositionality
- Break condition: If entropy and energy cannot be cleanly separated (e.g., entangled loss terms), or if the expectation under posterior samples becomes intractable, the chain rule decomposition fails to yield computable gradients.

### Mechanism 2: Bayesian Chain Rule for Inversion
- Claim: The inversion (posterior) of a composite model equals the composition of local inversions.
- Mechanism: For open models c: X ↠ Y and d: Y ↠ Z, the Bayesian chain