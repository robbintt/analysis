---
ver: rpa2
title: Teleportation With Null Space Gradient Projection for Optimization Acceleration
arxiv_id: '2502.11362'
source_url: https://arxiv.org/abs/2502.11362
tags:
- teleport
- loss
- test
- train
- teleportation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Teleportation With Null Space Gradient Projection for Optimization Acceleration

## Quick Facts
- arXiv ID: 2502.11362
- Source URL: https://arxiv.org/abs/2502.11362
- Reference count: 40
- Primary result: Teleportation with null space gradient projection accelerates convergence of SGD and Adam across MLPs, CNNs, and Transformers by 10-30% on MNIST, FashionMNIST, and CIFAR-10/100.

## Executive Summary
This paper introduces teleportation, an optimization technique that accelerates gradient descent by repositioning parameters within loss-invariant level sets to regions with larger gradient norms. The method projects teleportation gradients onto the null space of layer inputs, preserving the loss value while enabling parameter updates that increase gradient magnitude. Empirical results demonstrate consistent convergence acceleration across multiple architectures and datasets without degrading final test performance.

## Method Summary
Teleportation accelerates optimization by finding parameters with higher gradient norms within the same loss level set. For each teleportation batch, the algorithm constructs a representation matrix from layer inputs, performs SVD to identify the core gradient space (CGS) capturing top input variance, and projects the teleportation gradient onto the residual gradient space (RGS)—the orthogonal complement of CGS. This projection ensures loss preservation while enabling gradient norm growth. The method is applied during the first 5 training epochs using 32 batches with 8 teleportation steps per batch, with hyperparameters tuned per architecture and dataset.

## Key Results
- SGD on MNIST: 3-layer MLP converges 15-20% faster with teleportation versus baseline.
- CNNs: ResNet-18 on CIFAR-10 shows 10-15% acceleration with teleportation during early epochs.
- Transformers: Sequential MNIST training achieves 20-25% faster convergence using teleportation with null space projection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting teleportation gradient onto input null space preserves loss while increasing gradient norm.
- Mechanism: For each layer, the teleportation gradient lies within the input space span. Projecting onto RGS (orthogonal complement of CGS) modifies weights without altering correlations in significant representation subspace, maintaining level set membership.
- Core assumption: Teleportation gradient can be expressed as matrix product involving layer input (e.g., δ · x^T for MLPs).
- Evidence: Abstract states projection preserves teleportation within loss invariant level set; section 3.2 confirms gradient resides in input space for MLPs, CNNs, and Transformers.
- Break condition: Full-rank inputs with no null space yield zero projection and no effect; nonlinearities may violate gradient-in-input-span assumption.

### Mechanism 2
- Claim: SVD-based low-rank approximation enables controllable error in level-set approximation.
- Mechanism: Representation matrix from layer inputs undergoes SVD with threshold τ to separate CGS (top-k eigenvectors) from RGS (remaining eigenvectors). Projection onto RGS controls variance preservation versus modification.
- Core assumption: Most input variance captured by small proportion of dimensions, leaving sufficient null space for projection. Teleportation batch is representative.
- Evidence: Section 3.3, Step 2 explains CGS projection causes greatest interference while RGS causes minimal interference; Figure 6a shows majority variance captured by small input proportion.
- Break condition: τ too low (e.g., 0.5) includes loss-affecting dimensions in null space, causing loss drift; Figure 6b suggests τ≥0.99 maintains constant loss.

### Mechanism 3
- Claim: Teleportation objective (squared gradient norm) accelerates gradient descent by repositioning to higher-gradient regions.
- Mechanism: Define L_teleport = ½‖∇_W L_primary‖². Perform gradient ascent on this objective within level set via null-space projection to find parameters with higher gradient norm. Higher gradient norm regions correspond to steeper loss landscapes enabling faster progress.
- Core assumption: Larger gradient norm at teleportation target translates to faster primary task convergence. Early training epochs are appropriate phase.
- Evidence: Abstract describes navigating loss invariant level set to identify parameters with advantageous geometric properties; section 3 defines objective as squared gradient norm of primary loss.
- Break condition: Poor correlation between gradient norm and distance to minima negates acceleration benefit; teleportation overhead may not improve wall-clock time if steps are too frequent/costly.

## Foundational Learning

- **Concept: Loss-invariant level sets and parameter symmetries**
  - Why needed: Teleportation relies on multiple parameter configurations yielding identical loss; essential for understanding why projection-based movement preserves loss.
  - Quick check: For 2-layer MLP with ReLU activations, name one transformation that leaves output unchanged while modifying weights.

- **Concept: Null space and orthogonal projection**
  - Why needed: Core algorithm projects gradients onto null space of layer input; required for deriving and debugging projection operators.
  - Quick check: Given matrix A with SVD A = UΣV^T, write projection operator onto null space of A.

- **Concept: Gradient flow through linear and convolutional layers**
  - Why needed: Paper asserts ∇_W L_teleport lies in input span for MLPs, CNNs, and Transformers; requires understanding gradient propagation through each layer type.
  - Quick check: For convolutional layer x_l = σ(X_{l-1} W_l), write ∇_{W_l} L in terms of input X_{l-1} and upstream gradient δ.

## Architecture Onboarding

- **Component map**: Representation matrix constructor -> SVD module -> Projection operator π_l -> Teleportation scheduler -> Primary optimizer wrapper

- **Critical path**:
  1. Forward pass on teleportation batch → capture layer inputs X_{l-1}
  2. Construct R_l and compute SVD → obtain B_l
  3. Compute ∇_W L_teleport via backprop on primary loss gradient norm
  4. Apply π_l to project onto RGS for each layer
  5. Update weights: W_l ← W_l + η · π_l(∇_W L_teleport)
  6. Repeat for t steps per batch, b batches, then resume primary training

- **Design tradeoffs**:
  - τ threshold: Higher τ (1.0) guarantees loss preservation but limits gradient norm increase; lower τ (0.99) allows faster growth with minimal loss change
  - Teleportation batch size n: Larger batches improve null-space estimation but increase memory/compute
  - Teleportation frequency/duration: More steps/batches increase overhead; paper uses 32 batches × 8 steps for first 5 epochs
  - Layer parallelization: SVD per layer is independent; current sequential implementation, parallelization could reduce time to O(d²nb)

- **Failure signatures**:
  - Loss drift during teleportation: Indicates τ too low or incorrect projection implementation; verify τ≥0.99 and check architecture-specific formulas
  - No acceleration observed: Model underparameterized (no null space) or late teleportation schedule; try earlier/more frequent teleportation or increase model width
  - NaN or instability: Teleportation learning rate η too high; paper uses η=0.2 for MLPs, η=0.003 for CNNs/Transformers—reduce if needed
  - Excessive wall-clock time: SVD cost dominates; reduce batch count b or enable layer parallelization

- **First 3 experiments**:
  1. MLP on MNIST with SGD: Replicate Figure 2 (top-left) using 3-layer MLP [16, 10], τ=1.0, teleportation first 5 epochs, 32 batches × 8 steps. Verify training loss converges faster than baseline SGD.
  2. Ablation on τ: Transformer on sequential MNIST, vary τ ∈ {1.0, 0.99, 0.95, 0.9}. Plot gradient norm increase vs. loss change per teleport step to confirm τ≥0.99 preserves loss.
  3. CNN on CIFAR-10 with Adam: 3-layer CNN with channels [3, 16, 32, 64], τ=1.0, 40 warm-up steps then teleportation. Compare test loss trajectory to baseline to observe early acceleration without final-test-loss degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated or adaptive hyperparameter strategy be developed to select teleportation learning rates and SVD thresholds that generalize across diverse architectures?
- Basis: Conclusion states identifying generalizable hyperparameters remains difficult and developing simple strategy would enhance efficiency.
- Why unresolved: Experiments rely on distinct, manually tuned hyperparameters (η_tele, CAP) for every dataset and optimizer combination (Table 1).
- What evidence would resolve: Rule-based or adaptive algorithm maintaining acceleration performance across datasets without per-dataset tuning.

### Open Question 2
- Question: Can algorithm be implemented with layer-wise parallelization to achieve theoretical time complexity O(d²nb) in practice?
- Basis: Authors state computations can be parallelized across all layers but leave engineering optimizations for future work.
- Why unresolved: Current runtime scales linearly with number of layers (Figure 3), indicating layer-independent property not yet fully exploited.
- What evidence would resolve: Benchmarks demonstrating constant scaling with respect to network depth compared to sequential implementation.

### Open Question 3
- Question: Can alternative teleportation objective functions, such as targeting curvature of landscape, be utilized to explicitly improve generalization rather than just training speed?
- Basis: Paper notes current objective prioritizes training convergence, often resulting in test loss plateaus similar to standard training.
- Why unresolved: Current method finds parameters with larger gradient norms but does not explicitly seek minima offering better generalization.
- What evidence would resolve: Experiments showing statistically significant improvements in test loss/accuracy compared to non-teleported baselines.

## Limitations

- The assumption that teleportation gradients universally reside in input span for "MLPs, CNNs, and Transformers" is not rigorously proven and may fail for certain layer configurations or activation functions.
- Computational overhead of SVD per layer, particularly for large models, is substantial and not fully addressed in terms of wall-clock time impact.
- Effectiveness may degrade in highly overparameterized regimes where null space becomes insufficient to accommodate meaningful gradient norm increases without loss drift.

## Confidence

- **High**: Mathematical framework for SVD-based subspace identification and null-space projection is sound and reproducible.
- **Medium**: Empirical results demonstrate acceleration for small-scale models (MNIST, FashionMNIST) but lack extensive validation on large-scale architectures where method's benefits and costs would be most pronounced.
- **Low**: Claim that teleportation gradients universally reside in input span for "MLPs, CNNs, and Transformers" is not rigorously proven and may fail for certain layer configurations or activation functions.

## Next Checks

1. **Scalability Test**: Apply teleportation to ResNet-18 on CIFAR-10 and measure both convergence acceleration and wall-clock time overhead compared to standard training. This will reveal if computational costs negate theoretical benefits.

2. **Architecture Stress Test**: Implement narrow-layer MLP (input width < hidden width) and attempt teleportation. Verify algorithm gracefully handles cases with insufficient null space by limiting teleportation steps or adjusting τ dynamically.

3. **Loss Drift Quantification**: For Transformer on sequential MNIST, systematically vary τ below 0.99 and plot loss change per teleport step against gradient norm increase. Confirm threshold where loss preservation fails and quantify trade-off between acceleration and stability.