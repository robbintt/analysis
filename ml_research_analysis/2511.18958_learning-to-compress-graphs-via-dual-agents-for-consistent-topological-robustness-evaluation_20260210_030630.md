---
ver: rpa2
title: Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness
  Evaluation
arxiv_id: '2511.18958'
source_url: https://arxiv.org/abs/2511.18958
tags:
- graph
- reward
- learning
- robustness
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cutter, a dual-agent reinforcement learning
  framework for graph compression that preserves structural robustness. The method
  addresses the challenge of evaluating large graphs under adversarial attacks, which
  is computationally expensive.
---

# Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation

## Quick Facts
- arXiv ID: 2511.18958
- Source URL: https://arxiv.org/abs/2511.18958
- Reference count: 12
- Method achieves RPS of 0.832 at 0.1 compression ratio on Cora

## Executive Summary
This paper introduces Cutter, a dual-agent reinforcement learning framework for graph compression that preserves structural robustness. The method addresses the challenge of evaluating large graphs under adversarial attacks, which is computationally expensive. Cutter uses a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA) to collaboratively identify critical and redundant nodes for compression. Experiments on five real-world datasets demonstrate that Cutter achieves high RPS (Robustness Preservation Similarity) scores, outperforming baselines even under aggressive compression.

## Method Summary
Cutter is a dual-agent reinforcement learning framework that compresses graphs while preserving their robustness profiles. The framework uses a Vital Detection Agent (VDA) to identify structurally important nodes and a Redundancy Detection Agent (RDA) to remove redundant nodes. Both agents share a graph convolutional encoder but maintain task-specific sub-encoders and independent Q-networks. The method incorporates trajectory-level reward shaping, prototype-based shaping, and cross-agent imitation to improve learning efficiency and performance.

## Key Results
- Cutter achieves RPS of 0.832 at 0.1 compression ratio on Cora
- Maintains high RPS scores (0.836-0.861) across compression ratios 0.5-0.1 on Citeseer
- Outperforms baselines including SparRL, DPGS, GEC, MCGS, and HyperSampling
- Improves evaluation efficiency by up to 86.3% on Citeseer with 0.5 compression ratio

## Why This Works (Mechanism)

### Mechanism 1: Dual-Agent Asymmetric Task Decomposition
Separating vital node detection from redundancy identification enables coordinated compression that preserves robustness. VDA maximizes connectivity degradation to identify critical nodes while RDA minimizes connectivity loss while removing nodes, using VDA-identified vital nodes as constraints. Both agents share a graph convolutional encoder but maintain task-specific sub-encoders and independent Q-networks.

### Mechanism 2: Trajectory-Level Return Reward Shaping with Affine Alignment
Dense step-wise rewards derived from trajectory-level returns accelerate learning in sparse-reward graph compression tasks. A reward network predicts per-step rewards, and cumulative predictions are regressed against true trajectory returns via MSE loss. An affine transformation aligns predicted scale to [0,1] range of true returns, ensuring gradient consistency.

### Mechanism 3: Prototype-Constrained Contrastive Reward Shaping
Behavioral prototypes extracted from high/low-return trajectories provide local decision guidance that trajectory-level signals miss. Top-K and bottom-K trajectories by return are extracted, and the most critical decision steps are identified. N-step context windows are encoded via GRU to produce prototypes, and new state-action pairs are compared via cosine similarity to generate auxiliary supervision.

### Mechanism 4: Cross-Agent Active-Follow Exploration
Alternating leader-follower roles enables safe exploration and knowledge transfer between asymmetric tasks. VDA leads in Phase I while RDA follows the same action sequence, and VDA extracts top 15% nodes as importance set. In Phase II, RDA leads while VDA follows, conditioned on the importance set. Each agent stores both active and follower trajectories in its experience buffer.

## Foundational Learning

- **Deep Q-Networks (DQN) with Experience Replay**: Both VDA and RDA use DQN to approximate Q-functions for sequential node removal. Experience replay stores trajectories for sample-efficient reuse. Quick check: Can you explain why replay buffers help stabilize off-policy RL training, and how ε-greedy exploration balances exploitation?

- **Graph Convolutional Networks (GCN) for Representation Learning**: The shared encoder uses graph convolution to produce node embeddings. Quick check: Given adjacency matrix A and node features X, what does the operation AX compute, and why is mean pooling used for graph-level embeddings?

- **Reward Shaping and Policy Invariance**: The paper relies on potential-based shaping theory to guarantee that affine-transformed rewards preserve optimal policies. Quick check: Under what conditions does adding a shaping function F(s, s') to rewards preserve the optimal policy? Why does affine transformation preserve trajectory rankings?

## Architecture Onboarding

- **Component map**: Adjacency matrix A + uniform node features X → Shared GCN encoder → Task-specific encoders → Q-value decoders → ε-greedy action selection
- **Critical path**: Graph input → Shared encoder → Task-specific encoders → Q-value decoders → ε-greedy action selection → Trajectory collection → True return computation → Reward network training → Prototype extraction → Cross-agent active-follow
- **Design tradeoffs**: Shared vs. separate encoders balance parameter efficiency with task-specific adaptation; affine alignment adds complexity but stabilizes training; prototype window size affects context capture vs. signal dilution.
- **Failure signatures**: Low RDA returns with high variance indicate P_delete penalty may be too aggressive; prototype loss not decreasing suggests GRU encoder fails to capture temporal patterns; cross-agent transfer hurting performance indicates I_vda may be noisy.
- **First 3 experiments**: 1) Ablation on reward shaping comparing RDA with and without reward network; 2) Sensitivity to compression ratio testing on Cora at ρ ∈ {0.5, 0.3, 0.1}; 3) Component isolation testing VDA-only and RDA-only against full Cutter.

## Open Questions the Paper Calls Out

- **Can the Cutter framework effectively incorporate node attributes to support downstream tasks such as GCN training or link prediction while maintaining robustness profiles?** The current methodology explicitly utilizes an "all-one matrix" for node features to isolate structural compression, leaving the interaction between semantic features and robustness preservation unexplored.

- **Does the robustness preservation learned by the dual agents generalize to adversarial attack strategies not included in the optimization set X?** The optimization problem minimizes discrepancy over a specific set of strategies, and it is unclear if the compressed graph captures a universal robustness profile or overfits to the structural signatures of the specific attacks used during training.

- **Is the computational overhead of the dual-agent RL training process tractable for massive, web-scale graphs?** While the abstract motivates the work by the need to scale to "increasingly large" graphs, the experimental validation is restricted to relatively small benchmark datasets, and the complexity of training dual agents may introduce significant bottlenecks.

## Limitations

- The framework currently operates on topology-only graphs with uniform node features, limiting its applicability to attributed graphs
- The 15% importance set threshold for cross-agent transfer is arbitrary and may require tuning for different graph domains
- The computational overhead of dual-agent RL training may limit scalability to massive graphs

## Confidence

- **High**: Dual-agent decomposition into vital detection (VDA) and redundancy detection (RDA) tasks is well-specified and mechanistically sound
- **Medium**: Trajectory-level reward shaping with affine alignment is theoretically grounded but implementation details are underspecified
- **Medium**: Cross-agent active-follow exploration has conceptual merit but the 15% threshold appears arbitrary
- **Low**: Prototype-based contrastive shaping lacks direct empirical support in graph RL literature

## Next Checks

1. **Reward shaping ablation**: Train RDA with and without reward network (as in Figure 3). Compare convergence speed and final return. Expect shaped variant to achieve higher returns (0.668 vs 0.513 in paper).

2. **Compression ratio sensitivity**: Run Cutter on Cora at ρ ∈ {0.5, 0.3, 0.1}. Report RPS_mean across 8 attack strategies. Expect gradual decline but maintenance above 0.8 even at ρ=0.1.

3. **Component isolation**: Test VDA-only (no RDA, random removal) and RDA-only (no VDA guidance) against full Cutter. Isolate contribution of dual-agent coordination vs. single-agent baselines.