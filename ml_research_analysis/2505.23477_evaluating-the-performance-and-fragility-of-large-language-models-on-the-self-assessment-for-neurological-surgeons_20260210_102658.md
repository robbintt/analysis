---
ver: rpa2
title: Evaluating the performance and fragility of large language models on the self-assessment
  for neurological surgeons
arxiv_id: '2505.23477'
source_url: https://arxiv.org/abs/2505.23477
tags:
- llms
- questions
- accuracy
- medical
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated 28 large language models on 2,904 neurosurgery
  board-style questions from the CNS-SANS benchmark and tested their resilience to
  distracting, irrelevant information. Six models achieved board-passing performance,
  with top models scoring over 15.7% above the passing threshold.
---

# Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons

## Quick Facts
- arXiv ID: 2505.23477
- Source URL: https://arxiv.org/abs/2505.23477
- Reference count: 0
- Top models achieved over 15.7% above passing threshold on neurosurgery board questions

## Executive Summary
This study evaluated 28 large language models on 2,904 neurosurgery board-style questions from the CNS-SANS benchmark and tested their resilience to distracting, irrelevant information. Six models achieved board-passing performance, with top models scoring over 15.7% above the passing threshold. However, all models experienced significant accuracy drops when extraneous distractors were added, with some declining by as much as 20.4%. Proprietary models showed greater resilience than open-source variants, but even they were affected. Fragility was most pronounced in Neuropathology questions for general open-source models. These findings highlight that while LLMs can answer neurosurgical questions at expert levels, their performance is highly vulnerable to irrelevant content, underscoring the need for improved robustness before clinical deployment.

## Method Summary
The study used 2,904 text-only questions from the CNS-SANS neurosurgery board exam benchmark, categorized into 12 subspecialties. A distraction framework generated irrelevant sentences containing polysemous medical terms used in non-clinical contexts (e.g., "The patient's zodiac sign is Cancer"). Twenty-eight models were evaluated across proprietary, general open-source, and medical open-source classifications using deterministic decoding and exact string matching. Statistical analysis included ANOVA with Tukey post-hoc and Spearman correlation between baseline accuracy and accuracy loss.

## Key Results
- Six models achieved board-passing performance (>73.4% accuracy), with top models scoring 15.7-24.3% above threshold
- All models experienced accuracy drops under distraction, ranging from 1.4-6.7% for proprietary models to 5.6-20.4% for general open-source models
- Fragility was most pronounced in Neuropathology questions for general open-source models (P < 10⁻⁴ vs other sections)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polysemous medical terms used in non-clinical contexts degrade LLM accuracy by capturing attention that should be directed toward clinically relevant content.
- Mechanism: Distractor sentences containing terms like "Cancer" (zodiac vs. malignancy) exploit semantic similarity to medical knowledge, causing attention mechanisms to weight irrelevant tokens inappropriately when computing the final answer.
- Core assumption: The performance drop reflects attention dispersion rather than simple context length effects.
- Evidence anchors: Abstract mentions distractor statements with polysemous words; Methods describes extracting clinical terms from incorrect answers and embedding them in non-clinical sentences.

### Mechanism 2
- Claim: Proprietary models exhibit greater distraction resilience due to training scale and data diversity, though the precise training differences remain opaque.
- Mechanism: Larger proprietary models likely internalize more robust representations that better distinguish clinical from non-clinical contexts, reducing spurious attention to distractors.
- Core assumption: The proprietary vs. open-source performance gap stems from training differences rather than architectural innovations alone.
- Evidence anchors: Results show proprietary models had 1.4-6.7% accuracy drops vs. 5.6-20.4% for general open-source models; spearman correlation ρ = -0.66 between baseline accuracy and accuracy loss.

### Mechanism 3
- Claim: Neuropathology questions show disproportionate fragility in open-source models due to inherently polysemous and abstract terminology in that domain.
- Mechanism: Neuropathology language already requires disambiguating terms with multiple meanings; adding distractors compounds this cognitive load beyond model capacity.
- Core assumption: The domain-specific effect reflects linguistic properties rather than uneven training coverage.
- Evidence anchors: Results show Neuropathology accuracy loss significantly higher for general open-source models (P < 10⁻⁴ vs. multiple other sections), but not for proprietary models (P > .9975).

## Foundational Learning

- Concept: **Attention dispersion in transformer models**
  - Why needed here: The distractor effect fundamentally involves how self-attention weights tokens; understanding attention helps explain why irrelevant content pulls focus from clinical reasoning.
  - Quick check question: Can you explain why a token with high semantic similarity to medical terms might receive disproportionate attention even in a non-clinical context?

- Concept: **Polysemy and semantic ambiguity in medical language**
  - Why needed here: The distraction framework exploits words like "Cancer" with both clinical and non-clinical meanings; recognizing polysemy is essential for understanding the attack surface.
  - Quick check question: Name three medical terms with common non-medical meanings that could serve as distractors.

- Concept: **Robustness vs. accuracy in benchmark evaluation**
  - Why needed here: High benchmark accuracy on clean inputs does not imply robustness; this paper explicitly separates baseline performance from resilience under perturbation.
  - Quick check question: Why might a model with 90% baseline accuracy and 15% accuracy loss be more concerning for deployment than a model with 80% baseline and 2% loss?

## Architecture Onboarding

- Component map: CNS-SANS question bank -> Distraction generator (GPT-4o pipeline) -> Evaluation harness (deterministic decoding, exact string matching) -> Statistical analysis (ANOVA, Spearman correlation)

- Critical path: 1) Filter SANS questions to text-only subset 2) Categorize questions by neurosurgical subspecialty 3) Generate distractors from incorrect answer terms 4) Run all 28 models on clean and distracted questions 5) Compare accuracy deltas by model classification and question category

- Design tradeoffs:
  - Exact string matching: Simple and reproducible but cannot award partial credit for conceptually correct non-identical answers
  - Single distractor style: Controls complexity but may not generalize to real-world noise patterns (copy-forward text, transcription errors)
  - Text-only exclusion: Removes multimodal questions (26% of corpus), potentially underestimating or overestimating robustness for vision-language models

- Failure signatures:
  - High baseline accuracy + high accuracy loss under distractors indicates fragility despite apparent competence
  - Category-specific drops (especially Neuropathology) suggest domain vocabulary gaps
  - One model passing baseline but failing with distractors represents critical deployment risk

- First 3 experiments:
  1. Replicate the distraction framework on a held-out subset of SANS questions to validate reproducibility of the 5.6-20.4% open-source accuracy loss range.
  2. Test whether non-medical distractors (sentences without any clinical terminology) produce smaller accuracy drops, isolating the polysemy mechanism.
  3. Evaluate whether chain-of-thought prompting explicitly instructing models to identify and ignore irrelevant information mitigates distraction effects, or if the vulnerability persists as prior work suggests.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can technical interventions such as adversarial noise-aware fine-tuning or curated retrieval-augmented generation (RAG) effectively mitigate the performance drops caused by irrelevant distractors?
- **Basis in paper:** The Discussion states that "future systems may require technical and architectural innovations such as adversarial noise-aware fine-tuning and retrieval-augmented prompting restricted to curated templates" to suppress non-salient information.
- **Why unresolved:** The paper demonstrates the fragility of current models but does not test these specific mitigation strategies; it further cites prior work (Vishwanath et al.) suggesting standard fine-tuning has previously failed to confer resistance.
- **What evidence would resolve it:** A follow-up study benchmarking models specifically trained with these noise-aware interventions on the distracted SANS dataset, showing a statistically significant reduction in accuracy loss compared to baseline models.

### Open Question 2
- **Question:** Do multimodal LLMs exhibit similar fragility to textual distractors when processing image-containing neurosurgical board questions?
- **Basis in paper:** The Limitations section notes that items containing images were excluded and that "multimodal LLMs may yield different robustness profiles in the SANS dataset."
- **Why unresolved:** The current study isolated text-only inputs to standardize the benchmark, leaving the interaction between visual reasoning and textual noise unexplored.
- **What evidence would resolve it:** Extending the distraction framework to the excluded image-based SANS questions and evaluating the accuracy delta between clean and distracted multimodal inputs.

### Open Question 3
- **Question:** How does LLM resilience degrade when subjected to more complex, realistic clinical noise—such as copy-forward redundancy, transcription errors, or longer digressions—compared to the single-sentence distractors used here?
- **Basis in paper:** The Limitations section highlights that "real-world clinical notes may feature longer digressions, copy-and-paste redundancy and transcription errors," whereas the study only examined a "single distractor style."
- **Why unresolved:** The study utilized a controlled distraction framework (single sentences with polysemous words), which may underestimate the impact of the messy, voluminous noise found in actual electronic health records.
- **What evidence would resolve it:** Evaluating model performance on a dataset augmented with varying lengths and types of noise (e.g., full paragraphs of irrelevant history, formatting artifacts) to map the relationship between noise complexity and accuracy loss.

### Open Question 4
- **Question:** What specific linguistic or semantic features of Neuropathology questions cause the disproportionate accuracy loss observed in general open-source models?
- **Basis in paper:** The Discussion observes that Neuropathology accuracy loss was significantly higher for general open-source models and speculates this "may reflect the abstract... and often polysemous language" used in that domain.
- **Why unresolved:** The study identifies the category-specific vulnerability but does not perform a linguistic analysis to confirm if the specific ambiguity of terms in Neuropathology is the causal factor for the failure.
- **What evidence would resolve it:** A fine-grained error analysis correlating the presence of specific linguistic features (e.g., ratio of polysemous terms, abstract vs. concrete concepts) with model attention weights and accuracy in the Neuropathology subset.

## Limitations

- Proprietary CNS-SANS benchmark prevents independent verification of evaluation framework
- Text-only question exclusion removes 26% of corpus, potentially biasing results
- Exact string matching evaluation cannot account for semantically correct answers using different terminology
- Distraction framework limited to single-sentence distractors, may not generalize to complex real-world noise

## Confidence

**High Confidence**: All models experience accuracy drops under distraction conditions (directly supported by experimental results)

**Medium Confidence**: Proprietary vs. open-source performance gap and Neuropathology-specific fragility effects (statistically significant but rely on training data assumptions)

**Low Confidence**: Specific mechanism by which polysemous medical terms cause attention dispersion (inferred from related work rather than directly measured)

## Next Checks

1. Replicate the distraction framework on a held-out subset of CNS-SANS questions to validate the 5.6-20.4% open-source accuracy loss range independently.

2. Test whether non-medical distractors (sentences without clinical terminology) produce smaller accuracy drops, isolating whether the polysemy mechanism is essential to the observed effects.

3. Evaluate whether explicit chain-of-thought instructions to identify and ignore irrelevant information can mitigate distraction effects, or if the vulnerability persists as prior work suggests.