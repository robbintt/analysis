---
ver: rpa2
title: 'mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning
  in Vision-Language Models'
arxiv_id: '2511.09339'
source_url: https://arxiv.org/abs/2511.09339
tags:
- dataset
- questions
- question
- reasoning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mmJEE-Eval is a new benchmark built from India's JEE Advanced exams
  to test scientific reasoning in vision-language models. It includes 1,460 multimodal
  questions in English and Hindi across Physics, Chemistry, and Mathematics.
---

# mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2511.09339
- Source URL: https://arxiv.org/abs/2511.09339
- Reference count: 40
- Key outcome: Frontier models (GPT-5, Gemini 2.5 Pro) achieve 77-84% accuracy vs open models plateauing at 37-45% on this new JEE-based multimodal benchmark

## Executive Summary
mmJEE-Eval introduces a novel benchmark for evaluating scientific reasoning in vision-language models using India's JEE Advanced exam questions. The benchmark includes 1,460 multimodal questions in English and Hindi across Physics, Chemistry, and Mathematics. Evaluation of 17 state-of-the-art models revealed a significant performance gap: closed models achieve 77-84% accuracy while open models plateau at 37-45%, a much larger difference than observed on existing benchmarks. The benchmark is designed to avoid memorization through annual updates using held-out questions from 2025.

## Method Summary
The benchmark evaluates VLMs on 1,460 JEE Advanced questions (2019-2025) with manual bounding box annotations and six-source majority voting for ground truth. Evaluation uses pass@1 (k=10 runs), exam-style marking with negative penalties, and bilingual consistency tracking. Four ablation conditions (original images, EasyOCR text, Gemma3 OCR, diagrams-cropped) isolate visual vs. textual reliance. Metacognitive evaluation includes Error Presence (EP) and Error Correction (EC) rates, plus pass@k sampling to test self-correction capabilities.

## Key Results
- Frontier models (GPT-5, Gemini 2.5 Pro) achieve 77-84% accuracy while open models plateau at 37-45%
- Open-source models show 28-37 point gap versus 5-10 points on existing benchmarks (MMMU/MathVista)
- Models detect errors in 21-73% of cases but correct only 1.1-5.2% when explicitly prompted

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exam-style multimodal evaluation with bilingual inputs exposes capability gaps hidden by standard benchmarks.
- Mechanism: mmJEE-Eval combines native bilingual presentation, integrated STEM reasoning across domains, exam-style negative marking, and contamination-resistant annual updates. This forces models to perform joint visual-textual reasoning rather than pattern-matching, revealing a 28-37 point frontier-open gap versus 5-10 points on MMMU/MathVista.
- Core assumption: The gap reflects genuine reasoning deficiencies, not domain unfamiliarity.
- Evidence anchors: "open-source models plateau at 37-45% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks" [abstract]; "Frontier models exhibit severe OCR sensitivity... Open models exhibit opposite behavior... they fully recover with better OCR, with minimal diagram dependence" [section 4.6]

### Mechanism 2
- Claim: Training methodology dominates parameter scale for multimodal scientific reasoning.
- Mechanism: Models with similar active parameter counts (~17B sparse MoE) show 30-40% accuracy differences (Gemini Flash vs. Llama4 Scout/Maverick). Ablations show frontier models depend heavily on visual-symbolic integration (48.5 point drop with corrupted OCR), while open models recover from OCR errors but plateau regardless of diagrams—indicating textual pattern reliance.
- Core assumption: The training methodologies of frontier models include proprietary techniques for cross-modal grounding not present in open training pipelines.
- Evidence anchors: "Even with diagrams removed, Flash achieves 66.3% while Maverick reaches 40.5% and Scout 31%. Since these are all MoEs with ~17B active parameters, we hypothesize that training methodology supersedes vision capabilities" [section 4.6]

### Mechanism 3
- Claim: Metacognitive self-correction is decoupled from problem-solving accuracy in current VLMs.
- Mechanism: Models detect errors in 21-73% of cases (EP scores) but correct only 1.1-5.2% when chained (EP→EC). This contrasts with 30-40% gains from pass@k sampling, suggesting models can arrive at correct answers through retries but cannot reliably recognize and fix their own reasoning chains when explicitly prompted with prior errors.
- Core assumption: Error detection and correction should correlate if models possess genuine metacognitive awareness.
- Evidence anchors: "GPT-5 fixes just 5.2% errors" [abstract]; "Most strikingly, on average, models detect errors in 52.9% of cases but correct only 8% of those detected. The EP→EC conversion rate averages 3.5% across open models" [section 4.2]

## Foundational Learning

- Concept: **Pass@k vs. self-correction evaluation**
  - Why needed here: The paper shows pass@k gains (~30%) don't translate to self-correction gains (~5%), requiring distinct evaluation protocols.
  - Quick check question: If a model improves from 50% to 80% accuracy with pass@3, would you expect its self-correction rate to also increase significantly? (Answer: No—the paper shows these are decoupled.)

- Concept: **Visual-symbolic integration**
  - Why needed here: The ablation study isolates whether models rely on text patterns vs. genuine visual reasoning through OCR corruption tests.
  - Quick check question: What does it mean if a model's accuracy recovers fully with better OCR but doesn't improve with diagram restoration? (Answer: The model relies on textual patterns, not visual grounding.)

- Concept: **Contamination-aware benchmarking**
  - Why needed here: mmJEE-Eval uses held-out 2025 questions released after model training cutoffs to ensure evaluation reflects reasoning, not memorization.
  - Quick check question: Why compare 2019-2024 performance to 2025 performance? (Answer: To detect memorization—if models drop on held-out data, contamination is limited.)

## Architecture Onboarding

- Component map:
  Data pipeline -> Manual bounding box annotation -> Six-source answer verification -> Pass@1 evaluation -> Bilingual consistency tracking -> EP/EC metacognitive testing -> Ablation framework

- Critical path:
  1. Extract questions from PDFs with bounding boxes (manual verification required—automated parsing failed)
  2. Collect answers from 6 independent sources and compute consensus (>60% threshold)
  3. Run evaluation with k=10 samples for Pass@1 stability (95% CI width ~1.09%)
  4. Apply confidence thresholding for MCQ negative marking avoidance

- Design tradeoffs:
  - Manual vs. automated annotation: Custom tools required; automated bounding box detection failed on matching problems
  - Pass@1 vs. pass@k: Paper uses pass@1 for benchmark comparability despite pass@k showing larger gains
  - Consensus threshold: 60% chosen over higher thresholds; only 30/1,476 questions flagged for manual review

- Failure signatures:
  - Visual assumption errors: Correct mathematical reasoning with incorrect visual assumptions (Figure 1: o3 assumes uniform thickness for wedge-shaped glass)
  - Language drift: Models responding in unintended languages (Gemini 2.5 Pro responded in German to Hindi input)
  - OCR brittleness: Frontier models drop 48.5 points with corrupted OCR; open models recover, indicating different failure modes

- First 3 experiments:
  1. Baseline establishment: Run a small open model (Qwen 2.5 VL 7B or InternVL3 8B) on the 2025 held-out set with k=10 to verify your evaluation pipeline matches reported accuracy (12.4-12.5%).
  2. Ablation replication: Test the same model across the four ablation conditions (original, EasyOCR, Gemma3 OCR, cropped) to confirm open models show minimal diagram dependence.
  3. EP/EC pilot: Pick 20 incorrect responses from step 1, run the EP prompt to detect errors, then chain the EC prompt for detected errors—expect ~60% detection, ~2-5% correction.

## Open Questions the Paper Calls Out
None

## Limitations
- Corpus coverage gaps: Based on a single exam (JEE Advanced), which may not fully represent the breadth of scientific reasoning
- Training methodology opacity: Cannot verify claims about proprietary training techniques due to lack of transparency
- Confidence threshold methodology: Adaptive thresholding approach lacks systematic justification and could significantly impact MCQ performance

## Confidence
**High confidence**: The benchmark construction methodology (1,460 questions, 6-source majority voting, held-out 2025 evaluation) and the basic accuracy findings (77-84% frontier vs 37-45% open models) are well-documented and reproducible.

**Medium confidence**: The mechanism linking training methodology to performance differences and the metacognitive decoupling claim are supported by the data but rely on speculative interpretations of the ablation studies.

**Low confidence**: The assertion that the 28-37 point gap represents genuine reasoning deficiencies rather than domain unfamiliarity lacks direct evidence, as the paper doesn't test models on comparable non-JEE benchmarks.

## Next Checks
1. **Training intervention validation**: Replicate the frontier-OSS gap using a smaller open model (17B parameters) trained with documented frontier-style techniques (process supervision, outcome verifiers) to test whether training methodology alone can close the gap.

2. **Domain generalization test**: Evaluate the same models on mmJEE-Eval and comparable STEM benchmarks (MMMU, PRiSM) using identical protocols to quantify how much of the performance gap is JEE-specific versus general multimodal reasoning capability.

3. **Metacognitive intervention study**: Test whether adding explicit self-correction training (chain-of-thought with error detection and correction) to an open model improves EP→EC rates without improving pass@1, directly testing the metacognitive decoupling hypothesis.