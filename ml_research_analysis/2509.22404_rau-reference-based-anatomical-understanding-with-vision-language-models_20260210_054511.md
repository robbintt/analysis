---
ver: rpa2
title: 'RAU: Reference-based Anatomical Understanding with Vision Language Models'
arxiv_id: '2509.22404'
source_url: https://arxiv.org/abs/2509.22404
tags:
- image
- medical
- target
- segmentation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAU, a framework that leverages vision-language
  models (VLMs) for reference-based anatomical understanding in medical images. RAU
  trains a VLM to identify anatomical regions by reasoning about relative spatial
  relationships between reference and target images, then integrates these spatial
  cues with SAM2's segmentation capability for precise localization.
---

# RAU: Reference-based Anatomical Understanding with Vision Language Models

## Quick Facts
- **arXiv ID**: 2509.22404
- **Source URL**: https://arxiv.org/abs/2509.22404
- **Reference count**: 26
- **Key result**: RAU achieves 89.38% labeling accuracy on RAOS dataset and improves Dice score from 0.24 to 0.71 on average

## Executive Summary
This paper introduces RAU, a framework that leverages vision-language models (VLMs) for reference-based anatomical understanding in medical images. RAU trains a VLM to identify anatomical regions by reasoning about relative spatial relationships between reference and target images, then integrates these spatial cues with SAM2's segmentation capability for precise localization. The method achieves strong generalization across diverse in-distribution and out-of-distribution datasets, outperforming baselines on labeling accuracy (e.g., 89.38% on RAOS vs 64.11% baseline) and segmentation metrics (Dice improving from 0.24 to 0.71 on average). RAU demonstrates robust performance on challenging cases like vessel segmentation and ultrasound interpretation with minimal supervision.

## Method Summary
RAU combines reference-based spatial reasoning with VLM-SAM2 integration for anatomical understanding. The method uses DINOv2 to retrieve reference images from a bank of annotated templates, then fine-tunes a VLM (Qwen2.5-VL-7B with LoRA) using SFT followed by GRPO reinforcement learning to perform spatial reasoning between reference-target pairs. The VLM outputs special <Seg> tokens whose embeddings are projected via MLP into SAM2's memory space, enabling language-guided attention over fine-grained semantic features. For labeling tasks, RAU optionally uses optimal transport to globally assign predicted bounding boxes to anatomical labels based on spatial relationships.

## Key Results
- Achieves 89.38% labeling accuracy on RAOS dataset (64.11% baseline)
- Improves Dice score from 0.24 to 0.71 on average across datasets
- Shows strong OOD generalization: 61.87% accuracy on LERA vs 33.95% for SFT-only baseline
- Successfully handles challenging vessel segmentation and ultrasound interpretation tasks

## Why This Works (Mechanism)

### Mechanism 1: Reference-Guided Spatial Reasoning via RL
- **Claim**: Targeted GRPO-based reinforcement learning induces VLMs to perform relative spatial reasoning between reference and target images, enabling anatomical region identification without large-scale per-dataset supervision.
- **Mechanism**: The VLM receives paired reference-target images where the reference contains labeled anatomical regions. Through reward-shaped optimization combining accuracy and format correctness (Eq. 2), the model shifts from isolated region recognition to reference-guided relational reasoning. CoT inspection (Fig. A1) shows convergence toward reasoning about relative spatial relationships rather than direct visual feature matching.
- **Core assumption**: Human anatomy exhibits sufficient cross-individual similarity that relational spatial priors from one reference generalize to unlabeled targets.
- **Evidence anchors**:
  - [abstract]: "a VLM learns to identify anatomical regions through relative spatial reasoning between reference and target images, trained on a moderately sized dataset"
  - [section 3.1]: "accuracy improves from 16.62% to 64.11% with 5 epochs of SFT, and further to 70.68% with 2400 steps RL-based finetuning"
  - [section 3.1]: "accuracy improves monotonically with additional optimization steps across both ID and OOD settings"
  - [corpus]: Weak direct evidence; neighboring papers (MedVL-SAM2, Anatomy-VLM) address VLM grounding but not reference-based spatial reasoning specifically.
- **Break condition**: Dense anatomical structures with ambiguous spatial relationships (e.g., intestinal loops in abdominal CT) degrade accuracy, indicating relational reasoning becomes unreliable when reference-target correspondence is ambiguous.

### Mechanism 2: VLM-SAM2 Fusion via Learnable Token Projection
- **Claim**: Projecting VLM-generated <Seg> token embeddings into SAM2's memory bank space enables language-guided attention over fine-grained semantic features, preserving spatial reasoning while achieving pixel-level localization.
- **Mechanism**: VLM outputs special <Seg> tokens whose embeddings h<Seg> are projected via MLP (Eq. 6) into SAM2's memory bank dimension (d=256). These queries qi retrieve relevant memory slots mj through dot-product attention (Eq. 7), producing fused representations zi for SAM2's decoder. The VLM provides instruction-following and spatial reasoning; SAM2 contributes granular features and explicit memory.
- **Core assumption**: <Seg> embeddings encode sufficient spatial-semantic information to retrieve correct memory slots, and MLP projection preserves this information across representation spaces.
- **Evidence anchors**:
  - [abstract]: "VLM-derived spatial cues can be seamlessly integrated with the fine-grained segmentation capability of SAM2"
  - [section 3.3]: "averaged over all datasets, VLM+SAM2 improves Dice from 0.24 to 0.71 and gIoU from 0.14 to 0.50 compared to the stronger SAM2-Memory-Ref-SFT baseline"
  - [corpus]: MedVL-SAM2 (FMR=0.53) similarly integrates VLM with SAM2 for 3D medical segmentation, supporting plausibility of fusion approach.
- **Break condition**: Elongated/topology-fragile structures (vessels) may fail if <Seg> embedding collapses spatial extent information; Fig. 3 shows SAM2-Memory-Ref-SFT fails on vessel branches while VLM+SAM2 succeeds, suggesting the mechanism addresses this limitation.

### Mechanism 3: Global Box Matching with Optimal Transport
- **Claim**: Jointly assigning predicted bounding boxes to anatomical labels via optimal transport leverages inter-region spatial relationships as soft constraints, improving labeling accuracy over independent predictions.
- **Mechanism**: Instead of predicting each label independently, the VLM outputs all N bounding boxes in one forward pass. Optimal transport (Eq. 4) finds assignment π* minimizing spatial/semantic distance Cij between predicted boxes and label prototypes. Unmatched/low-confidence boxes trigger fallback re-generation.
- **Core assumption**: Spatial relationships across labels (relative positions between organs) provide meaningful constraints for assignment; reference image prototypes capture these relationships.
- **Evidence anchors**:
  - [section 3.2]: "on the RAOS dataset, incorporating global bounding-box matching significantly improves labeling accuracy, from 64.11% to 78.16%"
  - [section 3.2]: "this improvement does not generalize well to structurally complex datasets such as Arcade, where the bounding-box–based method yields only 41.09% accuracy"
  - [corpus]: No direct corpus evidence for OT-based anatomical matching.
- **Break condition**: Elongated structures (vessels) poorly captured by rectangular boxes; OT assignment cannot recover from fundamentally inadequate box representations.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: RAU uses GRPO (not standard SFT alone) to enhance spatial reasoning. Understanding reward shaping and policy optimization is critical for reproducing the RL training pipeline.
  - Quick check question: Can you explain why GRPO would encourage chain-of-thought reasoning over direct prediction?

- **Concept: SAM2 Memory Bank Architecture**
  - Why needed here: The VLM-SAM2 integration relies on projecting embeddings into SAM2's memory space. Without understanding memory attention and slot structure, the fusion mechanism remains opaque.
  - Quick check question: How does SAM2's memory bank differ from standard attention, and why is it suited for reference-guided segmentation?

- **Concept: Optimal Transport for Assignment**
  - Why needed here: Global matching uses OT for box-label assignment. Understanding cost matrices and transport plans is necessary to modify or debug this component.
  - Quick check question: Why would OT assignment outperform independent per-box classification when spatial relationships matter?

## Architecture Onboarding

- **Component map**: Reference retrieval (DINOv2) -> VLM backbone (Qwen2.5-VL-7B with LoRA) -> MLP projector -> SAM2 memory bank (Hiera-Large) -> Mask decoder
- **Critical path**: Reference retrieval → VLM forward pass → <Seg> token generation → MLP projection → SAM2 memory attention → Mask decoding. Failures anywhere along this path cascade to final segmentation.
- **Design tradeoffs**:
  - Two-stage training (SFT then RL) vs. end-to-end: Paper shows RL initialization critical for OOD generalization (Tab. 4: RL-VQA achieves 61.87% on LERA vs. 33.95% for SFT-VQA)
  - Bounding box vs. segmentation output: Boxes provide coarse localization but fail on vessels; segmentation handles topology but requires more supervision
  - 4-bit quantization: Reduces memory but may compress spatial information; paper does not ablate this choice
- **Failure signatures**:
  - **Hallucinated coordinates**: VLM outputs invalid bbox coordinates → check format reward enforcement
  - **Memory attention collapse**: All <Seg> tokens attend to same memory slot → inspect attention weights αij
  - **OOD degradation on dense structures**: Intestinal loops, overlapping vessels → relational reasoning becomes unreliable (Sec. 3.1 limitation)
  - **Reference mismatch**: DINOv2 retrieval fails on modality shift → verify cosine similarity threshold
- **First 3 experiments**:
  1. **VQA baseline**: Train VLM with SFT-only on RAOS VQA task; measure labeling accuracy on RAOS-test and one OOD dataset. This isolates spatial reasoning capability without SAM2.
  2. **SAM2-Memory ablation**: Run SAM2 with memory bank (no VLM) using same reference inputs; compare Dice/gIoU to VLM+SAM2. Quantifies VLM contribution.
  3. **Projection dimensionality**: Vary MLP output dimension (128, 256, 512) and measure impact on vessel segmentation (Arcade dataset). Tests hypothesis that 256D preserves sufficient spatial-semantic information.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would incorporating explicit structural priors (e.g., part–whole hierarchies or anatomical organ trees) during training significantly improve RAU's anatomical consistency, particularly for complex multi-organ reasoning tasks?
  - Basis in paper: [explicit] The authors state in Section 5: "Looking forward, we identify several promising directions: (i) incorporating structural priors (e.g., part–whole hierarchies or organ trees) during training to reinforce anatomical consistency."
  - Why unresolved: The current framework trains VLMs on reference-target pairs without explicit encoding of anatomical relationships (e.g., liver is adjacent to gallbladder), potentially missing opportunities to enforce medically valid spatial configurations.
  - What evidence would resolve it: A comparative study showing RAU with and without structural prior integration on a multi-organ dataset, measuring both segmentation accuracy and violation rates of anatomical constraints.

- **Open Question 2**: Can adaptive memory mechanisms be designed to improve reference-target alignment when viewpoint variations or pathological shape distortions exist between the reference image and target image?
  - Basis in paper: [explicit] The authors identify in Section 5: "(ii) designing adaptive memory mechanisms to better align reference and target in cases of viewpoint or shape distortion."
  - Why unresolved: The current SAM2 memory bank assumes relatively consistent spatial layouts; cases like ultrasound with probe angle variation or diseased organs with abnormal morphology may degrade memory retrieval quality.
  - What evidence would resolve it: Experiments on datasets with controlled viewpoint variations (e.g., ultrasound with varying probe angles) showing whether adaptive memory improves performance over fixed memory mechanisms.

- **Open Question 3**: How can the RAU framework be extended to 3D volumes or temporal sequences while maintaining cross-slice/cross-frame correspondence for anatomical understanding?
  - Basis in paper: [explicit] The authors state in Section 5: "(iii) extending our framework to temporal or 3D volumes, where continuity and cross-slice correspondence are essential."
  - Why unresolved: Current experiments use 2D images; extending to 3D CT/MRI volumes or ultrasound video sequences requires handling inter-slice consistency, larger memory requirements, and potentially different reference selection strategies.
  - What evidence would resolve it: Implementation of 3D RAU on volumetric datasets (e.g., AMOS, TotalSegmentator) with metrics measuring both per-slice accuracy and 3D connectivity of predicted structures.

## Limitations

- **Structural complexity dependency**: Performance degrades on datasets with dense, overlapping structures (e.g., intestinal loops, thin vessel branches)
- **Representation space assumptions**: The MLP projection from 3584D to 256D for SAM2 memory space lacks ablation or theoretical justification
- **Limited pathological generalization**: OOD datasets tested are normal anatomy; performance on pathological cases remains unknown
- **Resource intensity**: Two-stage training (SFT + GRPO) and full SAM2 integration require substantial computational resources

## Confidence

- **Mechanism 1 (Spatial reasoning via RL)**: Medium-High - quantitative improvements are compelling, but relies on cross-individual anatomical similarity assumption
- **Mechanism 2 (VLM-SAM2 fusion)**: Medium - architecture is sound and performance gains substantial, but specific projection dimension adequacy unproven
- **Mechanism 3 (Optimal transport matching)**: Medium - works well for RAOS's regular anatomy but degrades on Arcade's complex structures

## Next Checks

1. **Representation dimensionality sensitivity**: Systematically vary MLP output dimension (128, 256, 512, 1024) and measure impact on vessel segmentation accuracy in Arcade dataset
2. **Pathological generalization test**: Evaluate RAU on a dataset containing anatomical variants or pathologies (e.g., congenital abnormalities, tumors) to test assumption of cross-individual similarity
3. **Memory attention analysis**: Visualize SAM2 memory attention weights (αij) across different anatomical regions to verify that <Seg> embeddings are meaningfully distributed rather than collapsed to dominant slots