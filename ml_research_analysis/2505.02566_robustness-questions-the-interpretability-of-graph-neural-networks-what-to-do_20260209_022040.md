---
ver: rpa2
title: 'Robustness questions the interpretability of graph neural networks: what to
  do?'
arxiv_id: '2505.02566'
source_url: https://arxiv.org/abs/2505.02566
tags:
- defense
- interpretability
- graph
- methods
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically benchmarks the interplay between interpretability
  and robustness in Graph Neural Networks (GNNs). It evaluates six GNN architectures
  (GCN, SAGE, GIN, GAT) across five datasets using four interpretability metrics (Fidelity,
  Stability, Consistency, Sparsity) while testing seven defense mechanisms against
  poisoning and evasion attacks.
---

# Robustness questions the interpretability of graph neural networks: what to do?

## Quick Facts
- **arXiv ID**: 2505.02566
- **Source URL**: https://arxiv.org/abs/2505.02566
- **Reference count**: 28
- **Primary result**: Benchmark reveals interpretability and robustness can be simultaneously enhanced through defense mechanisms, with GIN models showing significantly worse Sparsity and Stability metrics

## Executive Summary
This paper systematically investigates the interplay between interpretability and robustness in Graph Neural Networks (GNNs). Through comprehensive benchmarking of six GNN architectures across five datasets, the study evaluates four interpretability metrics while testing seven defense mechanisms against poisoning and evasion attacks. The key finding is that interpretability and robustness can be simultaneously enhanced, challenging the assumption that adversarial defenses necessarily degrade explanation quality. The study provides practical guidance on selecting appropriate interpretability metrics and architectures based on application requirements.

## Method Summary
The paper evaluates six GNN architectures (GCN, SAGE, GIN, GAT) on five datasets (Cora, CiteSeer, PubMed, Computers, Photo) using four interpretability metrics (Fidelity, Stability, Consistency, Sparsity). Models are tested with seven defense mechanisms against poisoning (Jaccard, GNNGuard) and evasion attacks (Distillation, Adversarial Training, Gradient Regularization, Quantization, Autoencoder). The evaluation uses GNNExplainer for post-hoc interpretability, with metrics computed on 10 fixed nodes per configuration. Stability and Consistency are averaged over 5 runs with perturbations limited to 5% of features and nodes. Training uses Adam optimizer, NLLLoss, and 200 epochs.

## Key Results
- Consistency and Fidelity metrics remain stable across modifications, while Sparsity and Stability are more sensitive to changes
- Most defense mechanisms improve interpretability, with Jaccard defense enhancing stability but increasing Sparsity variance
- GIN-based models show significantly worse Sparsity and Stability metrics due to their focus on isomorphic graphs
- Interpretability and robustness can be simultaneously enhanced through appropriate defense mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defense mechanisms generally improve interpretability metrics through regularization-like effects
- Mechanism: Defense methods against adversarial attacks smooth decision boundaries and reduce model sensitivity to noise, which simultaneously produces more coherent explanations. The paper explicitly notes these mechanisms "operate on mathematical principles similar to regularization, which often enhances the final model."
- Core assumption: Regularization effects that improve robustness also improve explanation quality by reducing spurious feature dependencies
- Evidence anchors:
  - [abstract] "Most defense mechanisms improve interpretability"
  - [Section 4.2.3] "all examined defense mechanisms improve interpretability metrics compared to the unprotected model. The most likely explanation is that these defense mechanisms operate on mathematical principles similar to regularization"
  - [corpus] arXiv:2509.00387 confirms adversarial training acts as data augmentation for robustness
- Break condition: Adversarial training creates highly complex decision boundaries between classes, degrading Stability metric (noted as exception in Section 4.2.3)

### Mechanism 2
- Claim: GIN architectures exhibit worse Sparsity and Stability due to isomorphism-focused aggregation
- Mechanism: GIN's injective aggregation ensures isomorphic graphs receive similar embeddings, but this means "the majority of a vertex's neighborhood is important." Removing even single vertices during perturbation shifts the graph to different embedding regions, producing inconsistent explanations.
- Core assumption: Isomorphism-preserving representations make local neighborhoods globally important, reducing explanation sparsity
- Evidence anchors:
  - [abstract] "GIN-based models show significantly worse Sparsity and Stability metrics due to their focus on isomorphic graphs"
  - [Section 4.2.2] "deterioration in metrics for the GIN-based model can be explained by the method's focus on aligning the representations of isomorphic graphs"
  - [corpus] Limited direct corpus evidence on GIN interpretability trade-offs; related work focuses on robustness not explanation quality
- Break condition: When graphs have highly regular structures where isomorphism matters less for local node predictions

### Mechanism 3
- Claim: Fidelity and Consistency measure interpretation method quality; Sparsity and Stability measure sensitivity to modifications
- Mechanism: Fidelity (prediction accuracy of explanation) and Consistency (agreement across runs) are intrinsic to the interpretation method. Sparsity (features excluded) and Stability (explanation similarity under perturbation) depend on both method and data characteristics, making them responsive to architectural and defense changes.
- Core assumption: Metrics that remain stable across modifications evaluate the method; metrics that change evaluate modification impact
- Evidence anchors:
  - [abstract] "Consistency and Fidelity metrics remain stable across modifications, while Sparsity and Stability are more sensitive to changes"
  - [Section 5] "Consistency and Fidelity metrics appear to be better suited for evaluating interpretation methods... Sparsity and Stability metrics are more appropriate for analyzing how small changes impact model interpretability"
  - [corpus] arXiv:2506.04694 discusses influence functions for edge edits, supporting sensitivity-based evaluation approaches
- Break condition: When perturbations are so severe they directly alter model predictions, not just explanations

## Foundational Learning

- Concept: Message passing paradigms in GNNs (spectral vs spatial vs attentional vs isomorphism-preserving)
  - Why needed here: Understanding aggregation differences explains why GCN/SAGE outperform GIN on Stability—mean/sum aggregation is less sensitive to individual node removal than injective aggregation
  - Quick check question: Can you explain why removing one neighbor affects GIN embeddings more than GCN embeddings?

- Concept: Poisoning vs evasion attack surfaces on graphs
  - Why needed here: The paper applies defenses differently—poisoning defenses (Jaccard, GNNGuard) modify the graph before training; evasion defenses (Distillation, Adversarial Training) modify training dynamics
  - Quick check question: If an attacker can modify 5% of edges in your training data, which defense category applies?

- Concept: Interpretability metric trade-offs
  - Why needed here: Selecting metrics depends on evaluation goal—method comparison (use Fidelity/Consistency) vs. robustness analysis (use Sparsity/Stability)
  - Quick check question: Why might an explanation with perfect Fidelity still be unreliable for deployment?

## Architecture Onboarding

- Component map:
  - **Data layer**: Planetoid (Cora/CiteSeer/PubMed—citation, sparse features) vs Amazon (Computers/Photo—purchase, dense graphs)
  - **Model layer**: GCN-2l/3l, SAGE-2l/3l, GIN-2l, GAT-2l (all with 16 hidden dims, 200 training epochs)
  - **Defense layer**: Poisoning (Jaccard threshold=0.4, GNNGuard) applied pre-training; Evasion (Distillation T=5, AT ε=0.01, Gradient Reg λ=50, Quantization 8 levels, Autoencoder) applied during training
  - **Interpretation layer**: GNNExplainer (primary, epochs=100), SubgraphX (supplementary only—takes days on large graphs)
  - **Metrics layer**: Fidelity ↑, Consistency ↑, Sparsity ↓, Stability ↓

- Critical path:
  1. Load dataset, create 80/20 train/test split
  2. Fix random set of 10 nodes for consistent cross-architecture comparison
  3. Apply poisoning defense to graph structure OR configure evasion defense in training loop
  4. Train model with Adam, NLLLoss, 200 epochs
  5. Generate explanation masks via GNNExplainer
  6. Compute all four metrics; for Stability, perturb ≤5% features and remove ≤5% nodes

- Design tradeoffs:
  - GCN/SAGE vs GIN: Choose GCN/SAGE when Stability matters; avoid GIN for perturbation-sensitive applications
  - 2-layer vs 3-layer: 3-layer improves Sparsity but degrades Stability (larger receptive field amplifies perturbation effects)
  - Jaccard defense: Best for Stability but causes high Sparsity variance—unpredictable explanation compactness
  - Adversarial training: Improves robustness but worst for Stability—complex decision boundaries

- Failure signatures:
  - GIN architecture showing Sparsity >0.5 and Stability >1.0: Expected behavior from isomorphism sensitivity
  - Jaccard defense with Sparsity variance >0.15: Normal—edge removal effects are dataset-dependent
  - SubgraphX timeout on Amazon datasets: Expected—switch to GNNExplainer
  - Adversarial training with Stability >1.5 on Planetoid: Complex boundaries degrading explanation consistency

- First 3 experiments:
  1. **Baseline calibration**: Run GCN-2l on Cora with no defense; verify Consistency ≈0.998, Fidelity ≈0.97, Sparsity ≈0.07, Stability ≈0.44
  2. **Defense impact validation**: Add Jaccard defense to same setup; confirm Stability drops to ≈0.17 while Sparsity variance increases
  3. **Architecture comparison**: Run GIN-2l vs SAGE-2l on same Cora split; verify GIN shows higher Sparsity and Stability values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interpretability metrics be refined to establish a clear, context-dependent standard for when and how they should be applied?
- Basis in paper: [explicit] The conclusion states that current metrics "should be further refined—primarily to establish a clear understanding of when and how each metric should be applied," noting that they are currently either too insensitive (Consistency/Fidelity) or too sensitive (Sparsity/Stability).
- Why unresolved: The study found that Fidelity and Consistency remain stable across modifications, failing to capture nuanced changes, while Sparsity and Stability fluctuate wildly based on domain and architecture, making a unified evaluation standard difficult.
- What evidence would resolve it: The development of new metrics or normalization techniques that show distinct, measurable responses to specific model modifications without being confounded by dataset domain or graph density.

### Open Question 2
- Question: How can defense mechanisms be modified to eliminate the trade-off where improving Stability results in high variance for Sparsity?
- Basis in paper: [explicit] The results (Section 4.2.3) show that the Jaccard defense improves Stability but "significantly increases the variance of the Sparsity metric," attributed to the opposing effects of edge removal on the neighborhood size versus importance assignment.
- Why unresolved: The paper identifies this specific instability in the Jaccard defense but does not propose a method to balance the beneficial regularization effects with the negative side-effect of unpredictable explanation complexity.
- What evidence would resolve it: A modified defense algorithm or a composite metric that maintains low Stability error while constraining Sparsity variance to a narrow, predictable range across diverse graph topologies.

### Open Question 3
- Question: Can GIN-based architectures be adapted to maintain isomorphism detection capabilities without suffering from significant degradation in Stability and Sparsity?
- Basis in paper: [explicit] Section 4.2.2 notes that GIN models exhibit "significantly worse values for the Sparsity and Stability metrics" because their focus on isomorphic graphs makes them highly sensitive to small perturbations (like removing a single vertex).
- Why unresolved: The paper identifies the root cause (isomorphism alignment) as inherent to the architecture's design, suggesting a fundamental conflict between the GIN aggregation strategy and the requirements for robust post-hoc interpretability.
- What evidence would resolve it: A comparative study showing that a modified GIN convolution operator can achieve comparable classification accuracy to standard GIN while matching the Stability scores of GCN or SAGE models.

## Limitations

- The study relies primarily on GNNExplainer for interpretability, with SubgraphX used only as supplementary due to computational constraints
- The fixed 10-node evaluation strategy may not capture dataset-wide trends, particularly for large Amazon datasets with dense features
- While interpretability-robustness alignment is demonstrated, the study does not establish causation between specific defense mechanisms and improved explanation quality

## Confidence

- **High**: Consistency and Fidelity metrics remaining stable across modifications; Jaccard defense improving stability while increasing Sparsity variance
- **Medium**: GIN architecture showing worse Sparsity/Stability due to isomorphism focus (strong theoretical basis but limited empirical validation across datasets)
- **Low**: The claim that most defenses improve interpretability via regularization effects (Mechanism 1)—while intuitive, the exact regularization paths for each defense remain unexplored

## Next Checks

1. **Cross-dataset validation**: Test whether GIN's poor Sparsity/Stability persists on datasets with more regular graph structures (e.g., molecular graphs)
2. **Ablation study**: Isolate which defense mechanisms contribute most to interpretability improvements by testing each component independently
3. **Temporal consistency**: Evaluate whether interpretability metrics remain stable across training epochs, particularly for defense-enhanced models