---
ver: rpa2
title: 'ExGRPO: Learning to Reason from Experience'
arxiv_id: '2510.02245'
source_url: https://arxiv.org/abs/2510.02245
tags:
- experience
- exgrpo
- reasoning
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates what makes a reasoning experience valuable
  for reinforcement learning from verifiable rewards (RLVR). It identifies rollout
  correctness and trajectory entropy as effective indicators of experience quality,
  finding that medium-difficulty questions and low-entropy trajectories yield the
  strongest learning signals.
---

# ExGRPO: Learning to Reason from Experience

## Quick Facts
- arXiv ID: 2510.02245
- Source URL: https://arxiv.org/abs/2510.02245
- Authors: Runzhe Zhan; Yafu Li; Zhi Wang; Xiaoye Qu; Dongrui Liu; Jing Shao; Derek F. Wong; Yu Cheng
- Reference count: 0
- This paper investigates what makes a reasoning experience valuable for reinforcement learning from verifiable rewards (RLVR), finding that medium-difficulty questions and low-entropy trajectories yield the strongest learning signals, and proposes ExGRPO to organize and prioritize valuable experiences.

## Executive Summary
This paper addresses a fundamental question in RLVR: what makes reasoning experiences valuable for learning? Through empirical analysis, the authors identify two key indicators of experience quality - rollout correctness and trajectory entropy. They find that medium-difficulty questions and low-entropy trajectories provide the strongest learning signals. Based on these insights, they propose ExGRPO, a framework that organizes experiences using correctness-based bucketing and entropy-based trajectory selection, combined with a mixed-policy objective that balances exploration and exploitation.

## Method Summary
ExGRPO introduces a systematic approach to curating and utilizing valuable reasoning experiences in RLVR. The framework operates through three main components: (1) Experience bucketing based on rollout correctness, which categorizes trajectories into correct and incorrect groups to prioritize high-quality examples; (2) Entropy-based trajectory selection, which favors low-entropy paths that are more likely to be correct and generalizable; and (3) A mixed-policy objective that combines exploration (to discover new reasoning strategies) with exploitation (to reinforce proven successful patterns). The framework is designed to work across different model scales (1.5B-8B parameters) and demonstrates particular effectiveness in stabilizing training for both weaker and stronger models where traditional on-policy RLVR methods struggle.

## Key Results
- ExGRPO achieves average gains of +3.5 points (in-distribution) and +7.6 points (out-of-distribution) over on-policy RLVR
- The framework successfully stabilizes training on both weaker and stronger models where on-policy methods fail
- Performance improvements are demonstrated across five different model sizes ranging from 1.5B to 8B parameters
- Medium-difficulty questions and low-entropy trajectories are empirically validated as the strongest learning signals

## Why This Works (Mechanism)
ExGRPO works by systematically identifying and prioritizing high-quality reasoning experiences. The framework leverages the observation that not all experiences contribute equally to learning - correct rollouts and low-entropy trajectories provide clearer, more generalizable learning signals. By bucketing experiences based on correctness, the model can focus on reinforcing successful reasoning patterns while avoiding reinforcement of errors. The entropy-based selection ensures that the model learns from more deterministic, reliable reasoning paths rather than noisy or inconsistent approaches. The mixed-policy objective then balances between exploiting these valuable experiences and exploring new strategies, creating a more robust learning process that adapts to different model capabilities.

## Foundational Learning
**Reinforcement Learning from Verifiable Rewards (RLVR)**: A training paradigm where models learn through trial-and-error using reward signals from verifiable outcomes. Why needed: Provides a framework for training reasoning models where correctness can be automatically assessed. Quick check: Understanding the difference between RLVR and standard supervised fine-tuning.

**Trajectory Entropy in Reasoning**: A measure of the randomness or diversity in the reasoning paths taken by a model. Why needed: Low-entropy trajectories are more predictable and likely to be correct, making them better learning examples. Quick check: Ability to calculate entropy for a sequence of reasoning steps.

**Experience Replay and Prioritization**: Techniques for storing and selectively reusing past experiences during training. Why needed: Enables efficient learning from a diverse set of experiences while focusing on the most valuable ones. Quick check: Understanding how prioritization affects learning dynamics compared to uniform sampling.

## Architecture Onboarding

**Component Map**: Data Collection -> Experience Bucketing (Correctness-based) -> Entropy-based Trajectory Selection -> Mixed-Policy Objective (Exploration-Exploitation Balance) -> Model Update

**Critical Path**: The most important sequence is Experience Bucketing -> Entropy-based Selection -> Mixed-Policy Objective, as these directly determine which experiences are used for training and how they're weighted.

**Design Tradeoffs**: The framework trades off between exploration (discovering new reasoning strategies) and exploitation (reinforcing proven patterns). The mixed-policy objective must carefully balance these to avoid premature convergence or excessive randomness.

**Failure Signatures**: On-policy RLVR failures manifest as training instability - either divergence for weaker models or plateauing for stronger ones. ExGRPO addresses these by providing more structured experience curation.

**3 First Experiments**:
1. Validate the correlation between rollout correctness and learning effectiveness through controlled experiments
2. Test entropy-based selection by comparing low-entropy versus high-entropy trajectory performance
3. Evaluate the mixed-policy objective's balance by systematically varying the exploration-exploitation ratio

## Open Questions the Paper Calls Out
None

## Limitations
- The quality indicators (correctness and entropy) are primarily validated through correlation rather than causal experimentation
- The framework's performance shows substantial variance across different model scales and datasets
- The mixed-policy objective's balance between exploration and exploitation is determined heuristically rather than through systematic optimization
- The generalizability of the experience quality indicators to non-mathematical reasoning tasks remains unclear

## Confidence

**High confidence**: The correlation between medium-difficulty questions and stronger learning signals is well-supported by the data

**Medium confidence**: The entropy-based trajectory selection methodology and its implementation details are clearly described and reproducible

**Medium confidence**: The average performance improvements over baselines are statistically significant within the tested conditions

**Low confidence**: The generalizability of the experience quality indicators to other RLVR tasks beyond mathematical reasoning

## Next Checks
1. Conduct ablation studies isolating the effects of correctness-based bucketing versus entropy-based selection to determine their individual contributions
2. Test the framework's robustness to varying levels of label noise in correctness judgments by introducing controlled perturbations
3. Evaluate whether the identified quality indicators transfer to non-mathematical reasoning tasks (e.g., code generation, commonsense reasoning) to assess domain generality