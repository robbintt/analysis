---
ver: rpa2
title: "Breaking the $\\log(1/\u0394_2)$ Barrier: Better Batched Best Arm Identification\
  \ with Adaptive Grids"
arxiv_id: '2501.17370'
source_url: https://arxiv.org/abs/2501.17370
tags:
- complexity
- batch
- sample
- algorithm
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies batched best arm identification in multi-armed\
  \ and linear bandits, aiming to minimize both the number of samples and batches.\
  \ The key contribution is a novel instance-sensitive batch complexity that breaks\
  \ the log(1/\u22062) barrier by adaptively adjusting batch sizes based on the problem\
  \ instance."
---

# Breaking the $\log(1/Δ_2)$ Barrier: Better Batched Best Arm Identification with Adaptive Grids

## Quick Facts
- arXiv ID: 2501.17370
- Source URL: https://arxiv.org/abs/2501.17370
- Reference count: 40
- Key outcome: Achieves instance-sensitive batch complexity that breaks the log(1/Δ₂) barrier while maintaining near-optimal sample complexity for batched best arm identification.

## Executive Summary
This paper addresses the fundamental limitation in batched best arm identification where traditional algorithms require O(log(1/Δ₂)) batches regardless of problem instance structure. The authors propose Instance-Sensitive Successive Elimination (IS-SE), which adaptively adjusts batch sizes based on the problem's gap structure, achieving improved batch complexity for many instances while preserving optimal sample complexity. The approach extends to linear bandits via IS-RAGE, demonstrating the same theoretical improvements.

## Method Summary
The paper introduces an adaptive grid update rule that augments standard geometric growth with an instance-sensitive term proportional to the inverse-gap-squared of eliminated arms. The algorithm maintains successive elimination while dynamically allocating more samples when many arms have large gaps (making elimination easier) and fewer samples when gaps are small. This instance-aware batching achieves R_I batch complexity, which can be much smaller than log(1/Δ₂) for favorable instances. The method extends to linear bandits by incorporating the adaptive grid into the RAGE framework.

## Key Results
- Achieves instance-sensitive batch complexity R_I that can be O(1) for certain instances where traditional methods require Θ(log n) batches
- Maintains sample complexity of Õ(H_I) where H_I = Σ_i Δ_i⁻² is the instance complexity
- Experimental results show consistent improvement over state-of-the-art methods on both synthetic and real-world Movielens data
- The adaptive approach works effectively for both multi-armed and linear bandit settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-sensitive batch sizing reduces batch complexity by allocating larger batches when problem structure permits faster elimination of suboptimal arms.
- Mechanism: The algorithm augments the standard geometric grid update (L_{r+1} = β_grid · L_r) with an adaptive term proportional to the estimated partial instance complexity of already-eliminated arms. This term is computed as Σ_{s=1}^r Σ_{j∈O_s} (ϵ_s^j)^{-2}, where ϵ_s^j represents the empirical gap of eliminated arm j in batch s. By distributing this budget across remaining arms, the algorithm effectively "front-loads" exploration when many easily-distinguishable arms exist.
- Core assumption: The empirical gaps ϵ_s^j reliably approximate true gaps Δ_j within constant factors (specifically 1/3·Δ_j ≤ ϵ_s^j ≤ 5/3·Δ_j under high-probability events).
- Evidence anchors:
  - [abstract] "The main contribution of our algorithm is a novel sample allocation scheme that effectively balances exploration and exploitation for batch sizes."
  - [Section 3.1, Algorithm 1 Line 8] L_{r+1} ← β_grid·L_r + β_sample·Σ_{s=1}^r Σ_{j∈O_s} (ϵ_s^j)^{-2} / |S_{r+1}|
  - [corpus] The Batch Complexity of Bandit Pure Exploration (arXiv:2502.01425) provides context on batch complexity as a fundamental metric.

### Mechanism 2
- Claim: The instance-sensitive batch complexity R_I upper-bounds actual batch count by aligning algorithmic elimination with a theoretical elimination sequence based on true gaps.
- Mechanism: Definition 1.1 constructs virtual sequences {L̄_r} and {U_r} where U_r represents arms with gap Δ_j ≥ C·√(1/L̄_r). The key insight is that when many arms have large gaps, U_r grows rapidly, allowing R_I to terminate in fewer iterations than log(1/Δ_2). The proof uses induction to show L_r ≥ L̄_r and U_r ⊆ ∪_{s=1}^r O_s, meaning the algorithm eliminates arms at least as fast as the theoretical sequence.
- Core assumption: The concentration parameter C = 15√2 is sufficient to guarantee with probability 1-δ that eliminated arms are truly suboptimal.
- Evidence anchors:
  - [Section 1.1, Definition 1.1] Recursive definition of L̄_r and U_r with explicit constant C = 15√2.
  - [Section 3.2, Theorem 3.1] "Batch complexity of Algorithm 1 is bounded by R_I" with proof using induction on L_r ≥ L̄_r.
  - [corpus] No directly comparable instance-sensitive batch complexity measures found; existing work focuses on minimax bounds.

### Mechanism 3
- Claim: The additional sample complexity overhead from instance-sensitive batching is bounded by O(log(min{n, Δ_2^{-1}}) · H_I), preserving near-optimality.
- Mechanism: The proof in Appendix A.3 decomposes sample complexity into two terms: I_1^i (related to allocated budget from eliminated arms) and I_2^i (related to geometric growth). The I_1^i terms sum to at most min{log n, B} · H_I because each eliminated arm contributes its inverse-gap-squared at most logarithmically many times across batches. The I_2^i terms are bounded by O(H_I) directly using the relationship L_{r_{i-1}} ≤ 49/Δ_i^2.
- Core assumption: The number of batches B is O(log(1/Δ_2)), which holds by Theorem 3.1's batch complexity guarantee.
- Evidence anchors:
  - [Section 3.2, Theorem 3.1] Sample complexity bound O((log(n/δ) + log log Δ_2^{-1}) · log min{n, Δ_2^{-1}} · H_I).
  - [Appendix A.3] Explicit decomposition and bounding of I_1^i and I_2^i terms.
  - [corpus] Related work on streaming bandits (arXiv:2502.01067) shows similar logarithmic overhead patterns.

## Foundational Learning

- Concept: **Successive Elimination Framework**
  - Why needed here: IS-SE builds directly on classical SE; understanding the base algorithm is essential to see how the adaptive grid modification preserves correctness while improving batch efficiency.
  - Quick check question: Can you explain why SE eliminates arms when their empirical mean falls below the best by more than β_conf/√L_r?

- Concept: **Instance Complexity H_I = Σ_i Δ_i^{-2}**
  - Why needed here: The paper's main claim is achieving Õ(H_I) sample complexity with improved batch complexity; H_I quantifies problem difficulty based on gap structure.
  - Quick check question: For n=100 arms where Δ_2 = 0.1 and all other gaps are 0.5, what is H_I approximately?

- Concept: **Batched Learning Model (Fixed vs. Adaptive Grid)**
  - Why needed here: The distinction determines whether batch boundaries can depend on observed rewards; this paper exploits adaptive grids to achieve instance-sensitive results.
  - Quick check question: Why can't fixed-grid algorithms achieve better than O(log(1/Δ_2)) batch complexity even on "easy" instances?

## Architecture Onboarding

- Component map:
  - **Core loop** (Algorithm 1 Lines 2-10): Iterative batch processing with elimination
  - **Empirical mean estimator** (Line 4): Pulls each active arm L_r·log(r²n/δ₁) times
  - **Elimination filter** (Line 7): Removes arms with ϵ_r^i > β_conf/√L_r
  - **Budget allocator** (Line 8): Computes L_{r+1} using geometric growth + instance-sensitive term
  - **Gap estimator** (Line 6): Computes ϵ_r^i = p̂*_r - p̂_r^i for all active arms

- Critical path:
  1. Initialize S_1 = [n], L_1 = β_grid (default 4)
  2. Per batch: sample all active arms → compute empirical means → eliminate suboptimal arms → update L_{r+1}
  3. Terminate when |S_r| = 1, return sole remaining arm
  The instance-sensitive term in L_{r+1} computation is the critical modification; incorrect implementation will revert to standard SE behavior.

- Design tradeoffs:
  - **β_grid vs. β_sample**: Higher β_grid increases geometric growth (more samples, fewer batches); higher β_sample increases instance-sensitivity (potentially fewer batches but risk of over-allocation)
  - **β_conf (confidence parameter)**: Lower values increase elimination aggressiveness but raise error probability; paper uses 5√2 ≈ 7.07
  - **δ allocation**: Paper uses δ_r = 3δ/(π²r²) per batch; alternative allocations affect finite-sample performance

- Failure signatures:
  - **Excessive batches (approaching log(1/Δ_2))**: β_sample may be too small; instance-sensitive term not contributing
  - **Premature elimination of best arm**: β_conf too low or noise variance underestimated
  - **Sample blowup beyond Õ(H_I)**: Check that elimination threshold correctly uses √L_r denominator; verify gap estimates ϵ_r^i are computed as (max - current), not absolute

- First 3 experiments:
  1. **Replicate Ex. 1 (B1(n))**: Set n=1000, best arm μ=0.5, (n-2) arms with μ=0, 1 arm with μ=0.5-1/√n. Compare IS-SE vs SE on batch count; IS-SE should achieve O(1) batches vs SE's Θ(log n).
  2. **Ablation on β_sample**: Run IS-SE on B2(n) with β_sample ∈ {0, 0.5, 1, 1.5, 2}. β_sample=0 recovers SE; plot batch-sample frontier to find practical sweet spot.
  3. **Real data validation on Movielens**: Extract top-k items, use ratings as rewards with Gaussian noise. Compare IS-SE vs SE on actual batch complexity; verify paper's claim of consistent improvement holds on non-synthetic distributions.

## Open Questions the Paper Calls Out

- What is the average batch complexity of the IS-SE and IS-RAGE algorithms under typical input distributions?
- Can the proposed algorithms maintain their efficiency and practicality when applied to larger, real-world datasets?
- Can instance-sensitive batch complexity be achieved for the fixed-budget variant of Best Arm Identification?
- Do instance-sensitive batch complexities exist for other online learning problems, such as regret minimization or reinforcement learning?

## Limitations

- The theoretical guarantees rely on specific concentration assumptions that may not hold for heavy-tailed noise distributions
- The extension to linear bandits depends on an unspecified Round() subroutine from Allen-Zhu et al. (2021), creating potential reproducibility issues
- The practical performance depends heavily on hyperparameter tuning (β_grid and β_sample), with no clear guidance for optimal selection

## Confidence

- **High confidence**: Sample complexity bounds O((log(n/δ) + log log Δ₂⁻¹) · log min{n, Δ₂⁻¹} · H_I) - the proof structure is explicit and the decomposition follows standard techniques
- **Medium confidence**: Batch complexity improvements over log(1/Δ₂) - while theoretically sound, the practical magnitude depends heavily on problem instance structure and hyperparameter tuning
- **Medium confidence**: Extension to linear bandits - correctness of IS-RAGE depends on the unspecified Round() implementation, though the conceptual framework appears sound

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary β_grid and β_sample across the full range (2-8 for β_grid, 0.5-2 for β_sample) on synthetic instances to map the batch-sample tradeoff frontier and identify optimal configurations

2. **Noise robustness evaluation**: Test IS-SE under heavy-tailed reward distributions (e.g., t-distribution with 3 degrees of freedom) and non-Gaussian noise to quantify degradation in instance-sensitive benefits

3. **Ablation of instance-sensitive term**: Implement variants that disable the adaptive term (L_{r+1} = β_grid·L_r only) and compare batch complexity on B1(n) instances to isolate the contribution of the adaptive component