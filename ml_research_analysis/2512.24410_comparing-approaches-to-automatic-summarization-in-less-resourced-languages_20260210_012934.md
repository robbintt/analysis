---
ver: rpa2
title: Comparing Approaches to Automatic Summarization in Less-Resourced Languages
arxiv_id: '2512.24410'
source_url: https://arxiv.org/abs/2512.24410
tags:
- lr-sum
- languages
- language
- summarization
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares various approaches to automatic summarization\
  \ for less-resourced languages, finding that multilingual mT5 fine-tuning significantly\
  \ outperforms zero-shot LLM prompting. Three data augmentation strategies\u2014\
  extractive summarization, self-training, and back-summarization\u2014were tested\
  \ with Wikipedia articles, showing modest improvements over individual language\
  \ baselines but not surpassing multilingual transfer performance."
---

# Comparing Approaches to Automatic Summarization in Less-Resourced Languages

## Quick Facts
- **arXiv ID:** 2512.24410
- **Source URL:** https://arxiv.org/abs/2512.24410
- **Reference count:** 0
- **Primary result:** Multilingual mT5 fine-tuning significantly outperforms zero-shot LLM prompting for less-resourced language summarization.

## Executive Summary
This study systematically compares automatic summarization approaches for less-resourced languages, finding that multilingual fine-tuned mT5 consistently outperforms zero-shot LLM prompting despite recent advances in large language models. The research tests three data augmentation strategies—extractive summarization, self-training, and back-summarization—using Wikipedia articles, showing modest improvements over individual language baselines but not surpassing multilingual transfer performance. The study also reveals that smaller, intentionally multilingual LLMs like Aya-101 produce less English contamination than larger models, which often add English commentary when summarizing non-Roman scripts. Reference-based metrics (ROUGE, BERTScore) show more reliable results than reference-free M-Prometheus, which appears biased toward larger LLM outputs and shows higher variance for less-resourced languages.

## Method Summary
The study fine-tuned mT5-base on LR-Sum and XL-Sum datasets across 18 less-resourced languages, comparing individual-language fine-tuning against multilingual fine-tuning with upsampling. Three data augmentation strategies were tested: extractive summarization using LexRank, self-training with model-generated summaries, and back-summarization using a reverse model to create synthetic documents. Zero-shot inference was run on several LLMs including Mixtral 8x7b, Llama-3 8B, Aya-101, and larger models via VLLM. Evaluation used ROUGE-1/2/L, BERTScore, and M-Prometheus, with English content measured via CLD3. The study also tested a translate-summarize-translate pipeline for comparison.

## Key Results
- Multilingual mT5 fine-tuning significantly outperforms zero-shot LLM prompting (e.g., Sorani Kurdish: 34.7 vs 18.2 ROUGE-L).
- Smaller multilingual LLMs like Aya-101 produce minimal English contamination (<1%) versus larger models like Mixtral (64.6% English for Amharic).
- Data augmentation strategies show modest improvements over individual baselines but don't match multilingual transfer performance.
- The translate-summarize-translate pipeline underperforms direct summarization by 5-10 ROUGE-L points.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual fine-tuned mT5 outperforms zero-shot LLMs for less-resourced language summarization because shared multilingual representations enable cross-lingual knowledge transfer.
- Mechanism: mT5's encoder-decoder architecture learns language-agnostic semantic representations during pre-training across 101 languages. When fine-tuned on combined multilingual data with upsampling, patterns from relatively higher-resource languages transfer to structurally related lower-resource languages through shared subword vocabulary and attention patterns.
- Core assumption: Less-resourced languages share exploitable structural or lexical patterns with higher-resource languages present in the combined training data.
- Evidence anchors:
  - [abstract]: "multilingual mT5 fine-tuning significantly outperforms zero-shot LLM prompting"
  - [page 1, Section 1]: "Multilingual transfer has proven to be a useful strategy for less-resourced languages (Wang et al., 2021)"
  - [page 5, Table 1]: Multilingual baseline achieves ROUGE-L of 34.7 for Sorani Kurdish vs. 18.2 for Aya-101
  - [corpus]: Related work shows multilingual models leverage shared patterns (avg FMR=0.48), but corpus evidence on transfer mechanisms specifically is limited
- Break condition: When languages are linguistically isolated or have writing systems with minimal vocabulary overlap (e.g., Khmer showed smallest gains due to limited suitable Wikipedia data).

### Mechanism 2
- Claim: LLMs trained on English-dominant corpora produce unwanted English output when summarizing less-resourced languages, especially those with non-Roman scripts.
- Mechanism: LLMs' generation is biased toward high-probability token sequences from their training distribution. When encountering unfamiliar scripts or low-resource languages, uncertainty in the language modeling head increases probability of falling back to English tokens—the dominant pattern in pre-training data. Smaller, intentionally multilingual models like Aya-101 mitigate this through balanced multilingual instruction tuning.
- Core assumption: Output language fidelity correlates with the proportion and quality of target language data in the model's training distribution.
- Evidence anchors:
  - [abstract]: "Smaller LLMs like Aya-101 produced less English output than larger models, which often added extra English commentary"
  - [page 4, Table 2]: Mixtral produces 64.6% English for Amharic, 76.2% for Khmer; Aya-101 produces ~0% English across languages
  - [page 4, Section 5]: "The proportion of English appears to be highest when the target language has a non-Roman script"
  - [corpus]: Weak direct corpus evidence; related work shows LLMs exhibit different capabilities across languages (avg FMR=0.48)
- Break condition: When using models specifically trained with balanced multilingual instruction data (Aya series), or when prompting with explicit language constraints.

### Mechanism 3
- Claim: Reference-free LLM evaluators (M-Prometheus) favor outputs from larger LLMs and show reduced reliability for less-resourced languages due to training data bias.
- Mechanism: M-Prometheus is trained primarily on higher-resource languages (roughly 30, mostly well-resourced). Its evaluation criteria implicitly learn to prefer output characteristics common in larger LLMs (which share similar training distributions), creating a bias loop. For less-resourced languages, the evaluator lacks sufficient training signal to reliably judge quality, resulting in higher variance and metric misalignment.
- Core assumption: LLM-as-judge reliability is language-dependent and correlates with the evaluator's training language coverage.
- Evidence anchors:
  - [abstract]: "Reference-based metrics (ROUGE, BERTScore) showed more consistent results across languages than reference-free M-Prometheus"
  - [page 7, Section 6.2]: "M-Prometheus scores generally do not appear to increase with other scoring metrics and that there is a larger variance for less-resourced languages"
  - [page 7, Figure 6]: M-Prometheus scores show flat distribution across ROUGE-L scores for less-resourced languages
  - [corpus]: Limited corpus evidence on this specific evaluation bias; related work exists on LLM judges but doesn't directly address less-resourced language reliability
- Break condition: When evaluating higher-resource languages (English shows better alignment), or when using reference-based metrics as primary evaluation.

## Foundational Learning

- Concept: **Multilingual Transfer Learning**
  - Why needed here: The paper's central finding hinges on why multilingual fine-tuning outperforms both language-specific training and zero-shot LLMs. Understanding cross-lingual representation sharing explains when and why this approach succeeds.
  - Quick check question: Given two languages with different scripts (e.g., Georgian and Armenian), what factors determine whether multilingual transfer will help or harm performance?

- Concept: **Data Augmentation for Low-Resource NLP**
  - Why needed here: The paper tests three augmentation strategies (extractive training, self-training, back-summarization) as alternatives to scarce human annotations. Understanding their trade-offs is essential for practical deployment.
  - Quick check question: If self-training uses model-generated summaries which may contain errors, why does it still improve ROUGE scores over the baseline?

- Concept: **Evaluation Metric Selection for Multilingual NLG**
  - Why needed here: The paper demonstrates that metric choice fundamentally changes conclusions about which approach "wins." Without understanding metric biases, practitioners may select suboptimal models.
  - Quick check question: When would you choose BERTScore over ROUGE-L for evaluating summarization in a language with complex morphology?

## Architecture Onboarding

- Component map:
Data Layer:
├── LR-Sum / XL-Sum (original training pairs, 300-31k examples/language)
├── Wikipedia → Segment Any Text → Filter (>5 sentences)
└── Augmentation Module:
    ├── Extractive: LexRank (2 sentences) → ⟨D, S_extracted⟩
    ├── Self-training: mT5 generates → ⟨D, S_generated⟩
    └── Back-summarization: Reverse model → ⟨D_generated, S_extracted⟩

Model Layer:
├── mT5-base (580M params, 101 languages)
│   ├── Individual fine-tuning (per-language)
│   └── Multilingual fine-tuning (combined + upsampling 0.5)
├── Smaller LLMs: Mixtral 8x7b, Llama-3 8B, Aya-101 (quantized)
└── Larger LLMs: Gemma-3 27B, Aya-Expanse 32B, Llama-3.3 70B (VLLM)

Evaluation Layer:
├── Reference-based: ROUGE-1/2/L, BERTScore (500 bootstrap samples)
└── Reference-free: M-Prometheus (use with caution)

- Critical path:
  1. **Start with multilingual mT5 fine-tuning** on combined LR-Sum + XL-Sum data with language-wise upsampling (factor 0.5). This is your strongest baseline (3 epochs, batch size 32, LR 5e-4, label smoothing 0.1).
  2. **If training data <1k examples**, apply back-summarization augmentation (train reverse model on existing pairs, generate synthetic documents from LexRank summaries, concatenate up to 6k synthetic examples).
  3. **For LLM inference**, prefer Aya-101 (12.9B) or Aya-Expanse (32B) over Mixtral/Llama to minimize English contamination. Use explicit prompts: "Write a summary for the following article in [LANGUAGE]."
  4. **Evaluate primarily with ROUGE-L and BERTScore**; use M-Prometheus only as secondary signal and expect higher variance for less-resourced languages.

- Design tradeoffs:
  1. **Multilingual vs. Individual Fine-tuning**: Multilingual provides +5-15 ROUGE-L points but requires collecting data across languages. Individual training is faster per-language but underperforms.
  2. **Back-summarization vs. Self-training**: Back-summarization showed best results for multilingual models (Table 6, e.g., Amharic +1.0 RL), but self-training was competitive for individual models. Back-summarization requires training two models (forward + reverse).
  3. **LLM Size vs. Language Fidelity**: Aya-101 (13B) produces ~0% English; Llama-3.3 (70B) produces better summaries but with English commentary requiring post-processing. Aya-Expanse (32B) offers a middle ground.
  4. **TST Pipeline Complexity vs. Performance**: Translate-summarize-translate adds 2 inference steps but underperforms direct summarization by 5-10 ROUGE-L points (Table 4)—not worth the complexity.

- Failure signatures:
  1. **High English contamination (>30%)**: Detected via CLD3. Most severe for non-Roman scripts with Mixtral/Llama. Switch to Aya models or add explicit language constraints to prompts.
  2. **Unusually high novelty scores (>50%)**: Indicates model generating irrelevant content (e.g., Armenian back-summarization at 97.33% novelty). Check augmentation data quality.
  3. **Metric divergence (M-Prometheus >> ROUGE)**: When M-Prometheus favors a model that underperforms on reference metrics, especially for less-resourced languages. Trust reference metrics.
  4. **Very short augmentation data**: Khmer Wikipedia yielded only 4,323 suitable articles after filtering. Consider alternative data sources or skip augmentation for severely resource-constrained languages.

- First 3 experiments:
  1. **Replicate multilingual baseline**: Fine-tune mT5-base on combined LR-Sum + XL-Sum for your 5-7 target languages with upsampling. Measure ROUGE-L/BERTScore on test sets. This establishes your production baseline.
  2. **Quantify English contamination**: Run Aya-101, Aya-Expanse, and one non-Aya LLM (e.g., Llama-3) on 100 samples per language. Use CLD3 to measure English proportion. This validates language fidelity requirements for your deployment context.
  3. **Test augmentation boundary**: For your lowest-resource language (<1k training examples), train individual models with each augmentation strategy. Compare against multilingual baseline to identify when augmentation closes the gap vs. when multilingual transfer remains superior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can human evaluation confirm the findings that fine-tuned mT5 outperforms zero-shot LLM prompting for less-resourced language summarization?
- Basis in paper: [explicit] "An important limitation to this work is that the evaluation is done entirely with automated metrics... human evaluation can be expensive and especially difficult for less-resourced languages due to the added difficulty in recruitment of annotators."
- Why unresolved: The study relied entirely on automated metrics (ROUGE, BERTScore, M-Prometheus) due to cost constraints, but all have known limitations and potential biases.
- What evidence would resolve it: Human evaluations by native speakers of the studied languages rating summary quality across models.

### Open Question 2
- Question: How can reference-free LLM-based evaluation be made reliable for less-resourced languages?
- Basis in paper: [explicit] "LLM as judge may be less reliable on less-resourced languages" and M-Prometheus "has a bias towards larger LLMs... and we found evidence it may be less reliable in measuring less-resourced languages."
- Why unresolved: M-Prometheus was trained primarily on higher-resourced languages and shows different alignment patterns with reference-based metrics for less-resourced languages.
- What evidence would resolve it: Development of evaluation models specifically trained on diverse less-resourced language data with validation against human judgments.

### Open Question 3
- Question: Why does the translate-summarize-translate pipeline underperform direct summarization for less-resourced languages?
- Basis in paper: [explicit] "Overall, the TST pipeline results demonstrated worse performance across all metrics than the highest performing simple zero-shot prompting."
- Why unresolved: The paper observes the underperformance but does not investigate whether translation errors, information loss, or prompt design cause the degradation.
- What evidence would resolve it: Error analysis of translation quality at each pipeline stage and comparison of semantic preservation across approaches.

### Open Question 4
- Question: Under what conditions can individual-language fine-tuning with data augmentation outperform multilingual transfer?
- Basis in paper: [explicit] "There is evidence from Sorani Kurdish that scores that the best performing augmentation approach with individual language fine-tuning can outperform the multilingual fine-tuning baseline by a significant margin."
- Why unresolved: This finding contradicts the general trend and is not explained by language family, data size, or augmentation type.
- What evidence would resolve it: Systematic study across more languages to identify factors (script type, data quality, linguistic features) that predict when individual augmentation excels.

## Limitations

- The study relies entirely on automated metrics for evaluation, lacking human validation that could confirm quality assessments, particularly for less-resourced languages where native annotators are difficult to recruit.
- The LLM experiments lack detailed hardware and VLLM configuration specifications, creating potential reproducibility gaps for the larger models.
- M-Prometheus shows significant variance and apparent bias toward larger LLM outputs, suggesting it may not reliably measure quality for less-resourced languages, but this limitation isn't systematically validated.

## Confidence

**High Confidence:** The finding that multilingual mT5 fine-tuning significantly outperforms zero-shot LLM prompting is well-supported by consistent ROUGE/BERTScore improvements across 18 languages. The ablation showing multilingual > individual fine-tuning + augmentation is robust.

**Medium Confidence:** The claim that data augmentation strategies provide modest improvements over individual baselines is supported by the results, but the comparison lacks the multilingual baseline context that would make these findings more actionable.

**Low Confidence:** The M-Prometheus reliability assessment is based on observed variance and score patterns rather than systematic validation. The paper notes "M-Prometheus scores generally do not appear to increase with other scoring metrics" but doesn't test whether this represents true disagreement or evaluator limitations.

## Next Checks

1. **Systematic M-Prometheus Validation:** Create a controlled test suite with human-annotated quality judgments for summaries across multiple languages (both high and low-resource). Compare M-Prometheus scores against these judgments to quantify false positive/false negative rates and identify specific failure patterns for less-resourced languages.

2. **Augmentation Strategy Comparison:** Run parallel experiments testing each augmentation strategy (extractive, self-training, back-summarization) on the multilingual baseline model. This would reveal whether augmentation helps individual models specifically or if similar gains could be achieved with multilingual fine-tuning plus augmentation.

3. **English Contamination Characterization:** Systematically test LLM outputs across languages with varying scripts and resource levels using controlled prompts (e.g., "Write summary in [LANGUAGE] only" vs. "Write summary in [LANGUAGE], do not use English"). Measure English proportion via CLD3 and correlate with factors like script type, model size, and prompt specificity to identify optimal deployment configurations.