---
ver: rpa2
title: Large Language Models Show Signs of Alignment with Human Neurocognition During
  Abstract Reasoning
arxiv_id: '2508.10057'
source_url: https://arxiv.org/abs/2508.10057
tags:
- human
- llms
- reasoning
- were
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the abstract reasoning capabilities of human
  participants and eight open-source large language models (LLMs) using an abstract-pattern-completion
  task. Only the largest LLMs (~70 billion parameters) achieved human-comparable accuracy,
  with Qwen-2.5-72B and DeepSeek-R1-70B also showing similar pattern-specific difficulty
  profiles to humans.
---

# Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning

## Quick Facts
- arXiv ID: 2508.10057
- Source URL: https://arxiv.org/abs/2508.10057
- Authors: Christopher Pinier; Sonia Acuña Vargas; Mariia Steeghs-Turchina; Dora Matzke; Claire E. Stevenson; Michael D. Nunez
- Reference count: 13
- Primary result: Only 70B-parameter LLMs achieved human-comparable accuracy on abstract reasoning tasks, with Qwen-2.5-72B and DeepSeek-R1-70B showing similar human-like error patterns

## Executive Summary
This study compared human abstract reasoning capabilities with eight open-source LLMs using an abstract pattern-completion task. The largest models (~70 billion parameters) matched human accuracy, while smaller models fell significantly short. Using Representational Similarity Analysis (RSA), the researchers found that intermediate transformer layers showed the strongest clustering of abstract pattern categories, with this clustering strength scaling with task performance. Notably, task-optimal LLM layers showed moderate positive correlations with human frontal fixation-related potentials (FRPs), suggesting potential alignment in abstract reasoning mechanisms between biological and artificial neural systems.

## Method Summary
The study used 400 unique trials of abstract pattern completion with 8 pattern types (e.g., ABBACDDC, ABCDDCBA), where participants and LLMs identified missing elements from 4 multiple-choice options. Humans (N=25) completed the task while EEG and eye-tracking data were recorded, seeing icon-based visual sequences. LLMs received text-based prompts with one-word labels replacing icons. Researchers extracted hidden states from every transformer layer for sequence tokens only, building 400×400 trial-level RDMs using correlation distance. The "task-optimal" layer was identified as the one showing highest correlation with a reference RDM (perfect within-pattern similarity, maximal between-pattern dissimilarity). These optimal-layer RDMs were then correlated with human EEG signals (FRPs, response-locked ERPs, and resting state) using 10,000-iteration permutation testing.

## Key Results
- Only 70B-parameter LLMs achieved human-comparable accuracy on abstract reasoning tasks
- Intermediate transformer layers showed strongest clustering of abstract pattern categories, scaling with task performance
- Task-optimal LLM layers showed moderate positive correlations (r ≈ .17–.25) with human frontal fixation-related potentials specifically

## Why This Works (Mechanism)

### Mechanism 1
Abstract reasoning capabilities in LLMs emerge predominantly from intermediate transformer layers, which structure information in an "inverted-U" fidelity profile. Early layers process surface-level token statistics, while intermediate layers condense these into abstract relational representations. Late layers then flatten this structure to project specific output tokens. Stronger clustering of pattern categories in these intermediate layers correlates with higher task accuracy.

### Mechanism 2
LLM representational geometry shares properties with human frontal cortical activity specifically during active visual exploration (FRPs), but not during motor response or rest. Humans engage in "self-paced processing" while inspecting sequences, reflected in Fixation-Related Potentials. LLMs, when solving the text-equivalent, appear to form a similar "shared representational space" for abstract patterns in their optimal layers, leading to moderate positive correlations only with this specific EEG signal.

### Mechanism 3
Training for explicit reasoning (distillation/RL) shifts model behavior closer to human error profiles, potentially trading raw accuracy for "human-likeness." Standard high-accuracy models may solve tasks via non-human shortcuts. Models distilled to produce chain-of-thought appear to adopt processing strategies that mimic human cognitive constraints, increasing correlation with human accuracy profiles even if overall accuracy drops slightly.

## Foundational Learning

- **Concept**: **Representational Similarity Analysis (RSA)**
  - **Why needed here**: This is the core methodological bridge that allows comparison of non-comparable systems (silicon neurons vs. biological EEG) by comparing the geometry of their dissimilarity matrices rather than raw activity.
  - **Quick check question**: If two systems have different inputs (icons vs. words), can RSA still compare them? (Answer: Yes, if they process the same logical trials and you compare the relationships between trials).

- **Concept**: **Fixation-Related Potentials (FRPs)**
  - **Why needed here**: Understanding that the brain signal is locked to gaze (exploration), not just stimulus onset, is crucial. This paper argues this specific dynamic signal is what LLMs mirror, unlike standard ERPs.
  - **Quick check question**: How do FRPs differ from standard ERPs in the context of this study? (Answer: FRPs capture self-paced reasoning during visual inspection; ERPs often capture stimulus-response loops).

- **Concept**: **Inverted-U Layer Profile**
  - **Why needed here**: You must understand that information in Transformers flows from concrete → abstract → output. The "mid-point" is where abstract reasoning resides.
  - **Quick check question**: Why would the final layer of an LLM correlate worse with abstract reasoning brain signals than the middle layers? (Answer: Final layers are specialized for vocabulary projection/logits, stripping away the abstract relational geometry).

## Architecture Onboarding

- **Component map**: Text prompts (Sequence + Options) -> 8 Open-source Transformer models (variable layer depth) -> Hidden-state matrices from every layer for sequence tokens -> 400x400 Trial-RDMs per layer -> Correlation with reference RDM to find "Task-Optimal Layer" -> 8x8 Pattern-RDM from Optimal Layer -> Correlation with Human Frontal FRP RDM

- **Critical path**: Extracting token activations → Computing Correlation Distance → Building RDMs → Layer-wise correlation search. Note: The "Task-Optimal Layer" is a dynamic index, not a fixed architecture block.

- **Design tradeoffs**: 
  - **Modality Mismatch**: Humans saw icons (visual); LLMs saw words (text). This is a significant confounder but allows for testing if abstract logic transcends modality.
  - **Statistical Power**: N=25 humans is low for EEG, leading to conservative permutation tests (p > .05) despite moderate effect sizes (r ≈ .21).

- **Failure signatures**:
  - **Flat RSA Profile**: Layer correlations do not show an inverted-U, suggesting the model fails to abstract the pattern.
  - **Negative Correlation**: LLM RDMs inversely match Human RDMs (seen in resting EEG/Response ERPs), indicating distinct processing modes.

- **First 3 experiments**:
  1. **Multimodal Replication**: Run the same task using the visual inputs (pixel-based) of a multimodal LLM to resolve the text-vs-icon modality mismatch.
  2. **Layer Ablation**: Knock out the identified "task-optimal" intermediate layers and measure performance drop vs. knocking out late layers (predicts severe reasoning failure).
  3. **Noise Injection**: Add noise to the prompt instructions to see if the "inverted-U" geometry flattens or shifts, testing robustness of the representational alignment.

## Open Questions the Paper Calls Out

### Open Question 1
Does eliminating the task-modality mismatch (e.g., by using multimodal models) strengthen the alignment between human and LLM representational geometries? The authors note that humans solved a visuospatial puzzle while LLMs received text, which may distort brain-model correspondence. This is unresolved because the current study could not determine if the moderate correlations observed were suppressed by the difference in input modalities.

### Open Question 2
Do human visual fixations during reasoning correlate with token-level attention weights in transformer models? The "Limitations and future directions" section suggests that aligning fixation heat-maps with attention weights could uncover convergent attentional strategies. This is unresolved because while the study analyzed fixation-related potentials (FRPs), it did not perform a direct comparison between eye-tracking spatial data and LLM attention mechanisms.

### Open Question 3
Do the intermediate-layer representations identified as "task-optimal" generalize to other forms of abstract reasoning, or are they task-specific? The authors state that integrating causal tools could "reveal whether their internal representations generalize to other forms of (abstract) reasoning." This is unresolved because the current RSA approach only indicates where patterns are encoded for this specific task, not whether these representations form a general reasoning mechanism.

## Limitations
- **Modality Mismatch**: Fundamental difference between human (icon-based visual input) and LLM (text-based) stimuli introduces potential confounds.
- **EEG Statistical Power**: Only 25 participants limits ability to detect subtle neural effects, despite appropriate permutation testing.
- **Task-Optimal Layer Identification**: Assumes perfect within-pattern similarity and maximal between-pattern dissimilarity represents the gold standard for abstract reasoning.

## Confidence
- **High Confidence**: Large LLMs (≥70B parameters) achieve human-comparable accuracy on abstract pattern completion tasks, and intermediate transformer layers show stronger pattern clustering than early or late layers.
- **Medium Confidence**: The representational geometries of task-optimal LLM layers show moderate positive correlations with human frontal FRPs specifically, suggesting potential alignment in abstract reasoning mechanisms.
- **Low Confidence**: The claim that explicit reasoning training (RL/distillation) shifts models closer to human error profiles is supported by a single model comparison and needs broader validation.

## Next Checks
1. **Multimodal Replication**: Run the same abstract reasoning task using visual inputs (pixel-based) with a multimodal LLM to eliminate the text-vs-icon modality mismatch. Compare whether the FRP alignment persists or strengthens when both systems receive identical visual stimuli.

2. **Layer-Ablation Experiment**: Systematically disable the identified "task-optimal" intermediate layers in a 70B model and measure the impact on both accuracy and human-likeness of error patterns. Compare this to ablating early or late layers to test whether intermediate layers are uniquely critical for abstract reasoning.

3. **Cross-Task RSA Validation**: Apply the same RSA methodology to a different abstract reasoning task (e.g., Raven's Progressive Matrices) to test whether the layer-wise representational alignment and FRP correlation patterns generalize beyond the specific pattern completion paradigm used here.