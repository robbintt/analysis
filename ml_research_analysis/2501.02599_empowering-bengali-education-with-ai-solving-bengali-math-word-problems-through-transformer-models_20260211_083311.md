---
ver: rpa2
title: 'Empowering Bengali Education with AI: Solving Bengali Math Word Problems through
  Transformer Models'
arxiv_id: '2501.02599'
source_url: https://arxiv.org/abs/2501.02599
tags:
- word
- problems
- bengali
- math
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of solving Bengali math word
  problems (MWPs) using transformer-based models, a gap in low-resource language NLP.
  The authors introduce the "PatiGonit" dataset containing 10,000 Bengali MWPs and
  fine-tune transformer models (mT5, BanglaT5, mBART50, and Basic Transformer) to
  translate word problems into mathematical equations.
---

# Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models

## Quick Facts
- arXiv ID: 2501.02599
- Source URL: https://arxiv.org/abs/2501.02599
- Reference count: 19
- Key outcome: mT5 achieved 97.30% accuracy on Bengali math word problems, demonstrating transformer effectiveness for low-resource language educational AI

## Executive Summary
This study addresses a critical gap in Bengali natural language processing by developing transformer-based models capable of solving math word problems (MWPs) in Bengali, a low-resource language. The researchers created the PatiGonit dataset containing 10,000 Bengali MWPs and evaluated multiple transformer architectures including mT5, BanglaT5, mBART50, and a basic transformer model. The primary contribution is demonstrating that transformer models can effectively translate Bengali word problems into mathematical equations, with mT5 achieving 97.30% accuracy. This work advances both Bengali NLP capabilities and provides a foundation for AI-powered educational tools in low-resource language contexts.

## Method Summary
The researchers employed a transformer-based approach to solve Bengali math word problems by translating textual descriptions into mathematical equations. They introduced the PatiGonit dataset with 10,000 Bengali MWPs and fine-tuned four transformer models: mT5, BanglaT5, mBART50, and a basic transformer architecture. The models were trained to map natural language word problems to their corresponding mathematical expressions. The evaluation focused on equation generation accuracy, comparing the performance of different transformer architectures on the same dataset. The approach follows established math word problem solving methodologies while adapting them to the specific challenges of the Bengali language.

## Key Results
- mT5 achieved the highest accuracy of 97.30% in translating Bengali math word problems to mathematical equations
- Transformer models demonstrated effective performance on the PatiGonit dataset, outperforming traditional approaches
- The study established a new benchmark for Bengali math word problem solving and provided a valuable dataset for future research

## Why This Works (Mechanism)
None

## Foundational Learning

**Bengali NLP**: Understanding language-specific challenges in processing Bengali text, including script variations and morphological complexity. Why needed: Bengali has unique linguistic features requiring specialized tokenization and handling. Quick check: Verify tokenization preserves meaning across different Bengali dialects.

**Math Word Problem Solving**: Techniques for mapping natural language to mathematical expressions using sequence-to-sequence models. Why needed: This task requires understanding both linguistic and mathematical concepts simultaneously. Quick check: Ensure generated equations are mathematically valid and executable.

**Transformer Fine-tuning**: Adapting pre-trained multilingual models to specific low-resource language tasks. Why needed: Standard transformers have limited Bengali training data, requiring careful fine-tuning strategies. Quick check: Monitor for catastrophic forgetting of multilingual capabilities during fine-tuning.

## Architecture Onboarding

**Component Map**: Input Text -> Tokenizer -> Transformer Encoder -> Decoder -> Mathematical Expression

**Critical Path**: The sequence-to-sequence architecture where the encoder processes Bengali text and the decoder generates mathematical equations represents the critical path. The quality of tokenization and the attention mechanisms in the transformer layers are crucial for capturing the semantic relationships between words and mathematical concepts.

**Design Tradeoffs**: The study chose fine-tuning existing multilingual transformers over training from scratch, trading computational efficiency for potentially suboptimal Bengali-specific representations. Using established math word problem solving approaches rather than developing novel architectures prioritized practical results over innovation.

**Failure Signatures**: Performance degradation on problems with complex linguistic structures, inability to handle out-of-vocabulary mathematical terms, and sensitivity to subtle changes in problem phrasing. The models may struggle with problems requiring multi-step reasoning or those involving implicit mathematical relationships.

**First Experiments**:
1. Test model performance on progressively longer and more complex word problems
2. Evaluate zero-shot transfer to different Bengali dialects or educational contexts
3. Compare equation generation accuracy against human Bengali educators

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset generalizability concerns due to limited linguistic diversity in the PatiGonit dataset
- Evaluation focuses on equation generation accuracy without testing practical pedagogical effectiveness
- Limited comparison with general-purpose mathematical reasoning models beyond Bengali-specific variants

## Confidence
- Claim: Model accuracy on PatiGonit dataset is reliable | Confidence: High
- Claim: Models are ready for practical educational deployment | Confidence: Medium
- Claim: Approach represents significant innovation in NLP methodology | Confidence: Low

## Next Checks
1. Test trained models on independently collected Bengali math word problems from different educational sources to assess generalization beyond the PatiGonit dataset
2. Conduct a study with Bengali-speaking educators and students to evaluate the practical utility, accuracy interpretation, and pedagogical value of the generated equations in actual learning contexts
3. Systematically test model performance across different Bengali dialects, formality levels, and problem phrasings to identify potential brittleness in handling linguistic diversity