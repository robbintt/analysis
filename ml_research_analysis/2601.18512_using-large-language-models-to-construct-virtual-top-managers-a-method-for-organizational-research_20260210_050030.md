---
ver: rpa2
title: 'Using Large Language Models to Construct Virtual Top Managers: A Method for
  Organizational Research'
arxiv_id: '2601.18512'
source_url: https://arxiv.org/abs/2601.18512
tags:
- moral
- human
- foundations
- page
- personas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models can generate
  realistic virtual personas of top managers when grounded in their own communications
  and moral-psychological profiles. By constructing personas of 181 CEOs using publicly
  available texts and Moral Foundations Theory, the researchers show that these virtual
  leaders reproduce the structure of human moral reasoning in sacrificial dilemmas.
---

# Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research

## Quick Facts
- arXiv ID: 2601.18512
- Source URL: https://arxiv.org/abs/2601.18512
- Reference count: 0
- This study demonstrates that large language models can generate realistic virtual personas of top managers when grounded in their own communications and moral-psychological profiles.

## Executive Summary
This study introduces a method for constructing virtual top manager personas using large language models, grounded in publicly available communications and moral-psychological profiles. By building 181 CEO personas from YouTube interviews, news articles, and annual reports, and embedding them with Moral Foundations Theory scores, the researchers demonstrate that these virtual leaders reproduce the structure of human moral reasoning in sacrificial dilemmas. The method offers a complementary tool for organizational research, particularly when direct access to executives is limited.

## Method Summary
The researchers collected publicly available communications from 181 pharmaceutical/biotech CEOs (2000-2025), including YouTube transcripts, news articles, and annual report keynotes. They computed Moral Foundations Theory scores (harm, fairness, loyalty, authority, purity) using the Moral Strength API, creating per-CEO moral profiles. Four persona variants were tested using OpenAI Assistants API: demographics-only, text-only, MFT-only, and MFT+text. The best-performing MFT-only variant was scaled up and subjected to behavioral tasks (MFQ30 questionnaire and six sacrificial dilemmas) with five stochastic replications per CEO. Results were benchmarked against human data from Crone & Laham (2015) using correlation, Fisher's z-diff, and Chow tests.

## Key Results
- MFT-only model yielded the strongest alignment between input moral profiles and behavioral outputs
- Virtual CEOs reproduced directional structure of human moral reasoning (Harm/Purity negative on sacrificial acceptability; Authority positive) but with attenuated magnitude
- Integrated-trait prompting improved stability and internal coherence across runs compared to isolated trait prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Theoretical scaffolding (MFT scores) produces higher behavioral fidelity than raw text or demographic data alone
- Mechanism: Explicitly encoding moral-psychological scores into the system prompt constrains the LLM's reasoning space along interpretable dimensions, reducing noise from unstructured text while preserving theoretically grounded behavioral patterns
- Core assumption: LLMs can "role-play" quantitative trait profiles as if they were internal dispositions, and this mapping is more reliable than inferring behavior from raw corpora
- Evidence anchors:
  - [abstract] "MFT-only model yielded the strongest alignment between input moral profiles and behavioral outputs"
  - [section] Table 1 shows MFT-only correlations: Harm (r=.60), Fairness (r=.64), Loyalty (r=.63), Purity (r=.67), whereas Text-only and Demographics-only showed weak or non-significant correlations
  - [corpus] Related work (arxiv 2511.08565) finds LLM moral responses are susceptible to persona role-play, but robustness varies by foundation—supporting the need for explicit scaffolding

### Mechanism 2
- Claim: Integrated-trait prompting (using all foundations jointly) improves cross-run stability compared to trait-isolated prompting
- Mechanism: Activating the full moral profile as an interconnected schema mirrors how human moral reasoning integrates multiple concerns simultaneously, reducing stochastic drift when the model is forced to consider only one dimension at a time
- Core assumption: Moral foundations interact in human judgment; forcing isolation reduces ecological validity and increases variance
- Evidence anchors:
  - [section] "Integrated-trait prompting...displayed more stable and coherent moral patterns than under trait-isolated prompting" with coefficient stability (β_sd ≈ .03–.07) vs. higher variability in trait-isolated runs
  - [section] "Authority was positive in 5/5 runs" under trait-isolated, but other foundations "alternated between positive and negative effects"
  - [corpus] Weak direct evidence; corpus papers focus on persona role-play effects but not specifically on integrated vs. isolated prompting

### Mechanism 3
- Claim: Virtual CEO personas reproduce the directional structure of human moral reasoning (Harm/Purity negative on sacrificial acceptability; Authority positive) but with attenuated magnitude
- Mechanism: LLMs trained on general-population text capture average moral patterns; demographic grounding (CEO role) shifts magnitude toward a more utilitarian, outcome-focused stance observed in executive cognition literature
- Core assumption: The "CEO flavor" reflects genuine population differences rather than model artifact
- Evidence anchors:
  - [section] "Harm and Purity negatively predicting sacrificial acceptability, Authority positively—indicates that our personas reproduce the structure of human moral reasoning. At the same time, their attenuated affective intensity produced a distinct 'CEO flavor.'"
  - [section] Fisher's z-diff shows significant difference only for Harm (z=2.15, p=.031), with other foundations not significantly diverging
  - [corpus] arxiv 2504.10886 finds persona-dependent alignment in moral dilemmas, but specific executive population effects are not tested

## Foundational Learning

- Concept: **Moral Foundations Theory (MFT)**
  - Why needed here: The entire persona scaffolding relies on five foundations (Harm, Fairness, Loyalty, Authority, Purity). Without understanding these, you cannot interpret why MFT-only prompting outperforms alternatives
  - Quick check question: Can you name the five moral foundations and predict which would correlate positively vs. negatively with accepting sacrificial harm?

- Concept: **Stochastic reliability testing**
  - Why needed here: LLM outputs vary across runs; the paper ran five replications per condition to assess stability. Understanding variance is critical for distinguishing signal from noise
  - Quick check question: If you run a persona through a moral dilemma three times and get different responses, is this a bug or a feature?

- Concept: **Benchmarking against human data**
  - Why needed here: The paper uses Crone & Laham (2015) human data as ground truth. Validating LLM personas requires an external criterion, not just internal consistency
  - Quick check question: What statistical tests would you use to compare regression coefficients between virtual personas and human samples?

## Architecture Onboarding

- Component map: Data ingestion (YouTube transcripts, news articles, annual reports) -> Psychological scoring (Moral Strength API) -> Persona construction (OpenAI Assistants API) -> Behavioral elicitation (MFQ30 + dilemmas) -> Validation (correlation, Fisher's z, Chow tests)

- Critical path:
  1. Collect CEO corpora -> preprocess/clean
  2. Compute MFT scores via Moral Strength API
  3. Construct persona prompt with MFT scores (best-performing version)
  4. Run behavioral tasks (MFQ + dilemmas) with 5 replications
  5. Benchmark against human data using correlation, Fisher's z, Chow tests

- Design tradeoffs:
  - **MFT-only vs. Text-only**: MFT-only yields higher alignment but loses nuance from raw communications. Text-only captures style but introduces noise
  - **Trait-isolated vs. Integrated prompting**: Isolated tests foundation-specific effects but is unstable; integrated is more stable but weaker effect sizes
  - **Temperature=1.0**: Maximizes exploration but increases variance; lower temperature would reduce stochasticity but may flatten individual differences

- Failure signatures:
  - **Low input-output correlation**: If MFQ responses don't align with embedded MFT scores, prompt scaffolding failed
  - **High coefficient variance across runs**: If β signs flip between runs (e.g., Harm positive in run 1, negative in run 2), stochasticity dominates signal
  - **Missing human pattern**: If virtual personas show positive Harm-sacrifice correlation (opposite of humans), fundamental misalignment exists

- First 3 experiments:
  1. **Replicate Phase 2**: Build 25 CEO personas with all four versions; confirm MFT-only outperforms others on MFQ alignment
  2. **Temperature sweep**: Run integrated-trait condition at temperature [0.5, 0.7, 1.0] to test stability-efficiency tradeoff
  3. **Cross-population test**: Construct virtual non-CEO personas (e.g., nurses, engineers) to test whether "CEO flavor" is replicable population signal or artifact

## Open Questions the Paper Calls Out

- **Question:** Do alternative theoretical scaffolds (e.g., Big Five, Schwartz's values) yield higher behavioral fidelity than Moral Foundations Theory when constructing LLM-based personas?
  - **Basis in paper:** [explicit] The authors explicitly state, "Subsequent studies could examine the effects of alternative scaffolds such as the Big Five, Schwartz's values... to increase multidimensional realism."
  - **Why unresolved:** This study isolated Moral Foundations Theory as the primary psychological framework and did not compare its efficacy against other established personality or value frameworks
  - **What evidence would resolve it:** A comparative experiment constructing personas using different theoretical scaffolds and benchmarking their alignment with known human behavioral data

- **Question:** Can "ensemble agent" architectures (where multiple persona instances deliberate) reduce stochastic noise and improve result stability better than simple averaging?
  - **Basis in paper:** [explicit] The authors propose, "A particularly promising direction is the use of ensemble agents, where several instances of the same persona deliberate before a final answer is produced."
  - **Why unresolved:** The current study addressed stochasticity by averaging separate runs, but the authors note the need for methods that provide greater standardization and theoretical grounding
  - **What evidence would resolve it:** A methodological comparison of coefficient stability and response variance between single-prompt, averaged-run, and ensemble-deliberation approaches

- **Question:** Do interactions among virtual leaders in multi-agent environments generate emergent collective behaviors that mirror real-world organizational or industry-level dynamics?
  - **Basis in paper:** [explicit] The authors ask, "A natural extension is to examine whether interactions among virtual leaders produce emergent patterns that resemble organizational or industry-level dynamics over time."
  - **Why unresolved:** The current research validated individual decision-making in isolation but did not test social interactions, negotiation, or collective outcomes
  - **What evidence would resolve it:** Simulating multi-agent interactions (e.g., crises, negotiations) and validating the macro-level outputs against historical organizational data

## Limitations
- Proprietary Moral Strength API prevents independent verification of MFT scoring
- Results based on single model version and may not generalize across LLM architectures
- Corpus-based persona construction may introduce selection bias from publicly available communications

## Confidence
- High confidence: The directional structure of moral reasoning (Harm/Purity negative, Authority positive on sacrificial acceptability) is reproduced by virtual personas
- Medium confidence: MFT-only prompting produces stronger alignment than alternatives; integrated-trait prompting improves stability
- Low confidence: The "CEO flavor" represents genuine population differences versus model artifact; results generalize across model versions

## Next Checks
1. Replicate the study using an open-source MFT scoring dictionary to verify the Moral Strength API results are not driving the findings
2. Test multiple model versions (gpt-4, gpt-4-turbo, Claude, etc.) to assess generalizability of the persona method
3. Construct virtual personas from non-CEO populations (nurses, engineers, teachers) to determine if population-specific "flavors" are reproducible signals or artifacts