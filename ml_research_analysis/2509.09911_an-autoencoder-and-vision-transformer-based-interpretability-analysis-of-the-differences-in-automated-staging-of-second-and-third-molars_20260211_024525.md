---
ver: rpa2
title: An Autoencoder and Vision Transformer-based Interpretability Analysis of the
  Differences in Automated Staging of Second and Third Molars
arxiv_id: '2509.09911'
source_url: https://arxiv.org/abs/2509.09911
tags:
- tooth
- attention
- stage
- stages
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a deep learning framework combining a convolutional
  autoencoder (AE) with a Vision Transformer (ViT) to improve automated staging of
  mandibular second (tooth 37) and third (tooth 38) molars for forensic age estimation.
  While the framework improves classification accuracy for both teeth over a baseline
  ViT, increasing from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth
  38, the primary contribution lies in its diagnostic transparency.
---

# An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars

## Quick Facts
- arXiv ID: 2509.09911
- Source URL: https://arxiv.org/abs/2509.09911
- Reference count: 40
- Primary result: AE-ViT framework improves molar staging accuracy and provides diagnostic transparency for forensic age estimation

## Executive Summary
This study introduces a deep learning framework combining a convolutional autoencoder with a Vision Transformer to improve automated staging of mandibular second (tooth 37) and third (tooth 38) molars for forensic age estimation. While the framework improves classification accuracy for both teeth over a baseline ViT, the primary contribution lies in its diagnostic transparency. Analysis of the AE's latent space metrics and image reconstructions reveals that the remaining performance gap is data-centric, stemming from high intra-class morphological variability in the tooth 38 dataset. This work demonstrates that attention maps alone can be misleading, as they may appear anatomically plausible yet fail to identify underlying data issues.

## Method Summary
The framework preprocesses 224x224 grayscale OPG crops of teeth through a 5-layer convolutional autoencoder (30-dim latent space) trained with BCE + LPIPS reconstruction loss and variable-margin triplet loss to enforce ordinal relationships. The reconstructed images are then classified by a ViT (16 heads, 12 layers, patch size 32) into 10 developmental stages. The model is trained separately for tooth 37 (390 images) and tooth 38 (400 images) using 4-fold stratified cross-validation (65% train, 10% val, 25% test).

## Key Results
- AE-ViT improves accuracy from 0.712 to 0.815 for tooth 37 staging
- AE-ViT improves accuracy from 0.462 to 0.543 for tooth 38 staging
- Latent space analysis reveals high intra-class variability in tooth 38 data explains remaining performance gap
- Attention maps from reconstructed images focus more accurately on diagnostic root regions versus raw image attention

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preprocessing images with a convolutional autoencoder (AE) improves Vision Transformer (ViT) classification performance by reducing intra-class noise and emphasizing prototypical features.
- **Mechanism:** The AE forces images through a low-dimensional bottleneck (latent space) trained with a reconstruction loss (BCE + LPIPS). This acts as a filter, stripping high-frequency noise and retaining only features common to the class (developmental stage), creating "prototypes" that are easier for the classifier to separate.
- **Core assumption:** The variability hindering classification in the raw data is largely noise or irrelevant texture rather than signal; removing it simplifies the decision boundary without losing diagnostic information.

### Mechanism 2
- **Claim:** Enforcing ordinal relationships in the latent space via variable-margin triplet loss exposes underlying data quality issues.
- **Mechanism:** Unlike standard classification losses, the triplet loss explicitly penalizes the model based on the distance between stages. If the model cannot separate classes in the latent space (e.g., Stage 3 and Stage 4 overlap), it provides geometric evidence of low inter-class separability in the source data.
- **Core assumption:** Developmental stages are ordinal and should map to a smooth manifold; deviations from this manifold indicate data anomalies rather than model incapacity.

### Mechanism 3
- **Claim:** Attention maps generated from reconstructed (prototypical) images are more faithful to the diagnostic logic than those from raw images.
- **Mechanism:** Raw images contain visual clutter (neighboring teeth, bone structure) that distracts the ViT's self-attention mechanism. By feeding the ViT "cleaned" reconstructions, the attention mechanism focuses its weights on the relevant anatomical regions (e.g., roots) that differentiate the specific stages.
- **Core assumption:** The ViT's self-attention mechanism is sensitive to high-frequency noise in the background, which "confuses" the relevance scores in raw image analysis.

## Foundational Learning

- **Concept:** Triplet Loss & Metric Learning
  - **Why needed here:** The study uses a specific "variable-margin" triplet loss to structure the latent space. Understanding how Anchors, Positives, and Negatives interact is required to interpret why the Tooth 38 latent space failed to cluster effectively.
  - **Quick check question:** How does increasing the margin ($\alpha$) for non-adjacent stages theoretically affect the gradient of the latent space manifold?

- **Concept:** Vision Transformer (ViT) Inductive Bias
  - **Why needed here:** The paper contrasts ViT with DenseNet. You must understand that ViTs lack the translation invariance and locality of CNNs, making them more reliant on global context and potentially more sensitive to positional noise, which the AE helps mitigate.
  - **Quick check question:** Why does the paper suggest the AE prefix helps the ViT but hinders the DenseNet-201 baseline?

- **Concept:** Ordinal Regression vs. Nominal Classification
  - **Why needed here:** Tooth staging is ordinal (Stage 0 < Stage 1). Standard accuracy metrics are "misleadingly pessimistic" (predicting Stage 4 when ground truth is Stage 5 is better than predicting Stage 0). The framework relies on ordinal metrics (Weighted Kappa) for evaluation.
  - **Quick check question:** If the model predicts Stage 8 for a Stage 9 tooth, how does the Linearly Weighted Cohen's $\kappa$ penalize this compared to a simple Accuracy score?

## Architecture Onboarding

- **Component map:** Input -> AE Encoder (5-layer Conv) -> Latent Vector (30-dim) -> AE Decoder (5-layer) -> Reconstruction -> ViT Backbone (16 heads, 12 layers) -> CLS token -> Linear layer -> Softmax over 10 classes
- **Critical path:** The "Interpretability Pipeline" is the critical path for deployment/debugging: Input -> Latent Space Analysis (PCA check for overlap) -> Reconstruction (Visual check for blur/validity) -> ViT Attention (Check focus on roots vs. background)
- **Design tradeoffs:** DenseNet vs. ViT - DenseNet performs well on raw data (using texture) but degrades on AE reconstructions; ViT performs worse on raw data but improves on AE reconstructions. Assumption: The AE removes textural detail (noise) that DenseNet relies on, favoring the ViT's global patch-relation logic.
- **Failure signatures:** High Intra-Class Distance (latent space samples of same stage far apart indicates AE failing to find common features); Blurry Reconstructions (lack of distinct root shapes indicates high morphological variability or underfitting); Diffuse Attention (attention maps highlight whole bounding box evenly rather than specific tooth structures indicates uncertainty or lack of discriminative features)
- **First 3 experiments:**
  1. Baseline Reproduction: Train a standard ViT on the raw Tooth 37 dataset without the AE prefix to confirm the reported 0.712 accuracy and erratic attention maps
  2. Latent Space Stress Test: Train the AE on Tooth 38 data using the Ordinal Triplet Loss. Visualize the PCA plot. Verify if stages 0-9 form a "sphere" or if they collapse into a single cluster (confirming the data-centric failure)
  3. Ablation on Loss Components: Retrain the AE removing the LPIPS loss (keeping only BCE and Triplet) to see if the visual quality of reconstructions drops, and measure the impact on downstream ViT accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do automated staging methods on CBCT demonstrate superior performance and lower inter-observer variability compared to OPG for apical closure stages?
- Basis in paper: The authors suggest that analogous to how clavicle CT replaced radiographs, future work should investigate if automated methods show "superiority of CBCT over OPG," as artifacts in OPG obscure subtle morphological changes.
- Why unresolved: The current study exclusively utilized 2D panoramic radiographs (OPG), preventing a direct comparison of modalities regarding the detection of apical closure.
- What evidence would resolve it: A comparative study applying the AE+ViT framework to matched datasets of OPG and Cone Beam CT images.

### Open Question 2
- Question: Does the framework maintain its diagnostic transparency and accuracy when applied to multi-centric datasets with diverse ethnic origins?
- Basis in paper: The limitations section notes that the singular origin of the data (Belgian population) and modest sample size (~400 images) may hinder generalization to a larger population.
- Why unresolved: The model was trained and validated exclusively on data from a single institution, limiting the assessment of its robustness against population-specific morphological variations.
- What evidence would resolve it: Cross-validation performance and latent space consistency when the model is trained on a larger, globally diverse dataset collected from multiple institutions.

### Open Question 3
- Question: Can the framework be extended to combine dental and skeletal predictors to support a legally robust, multi-modal age estimation?
- Basis in paper: The conclusion states that "single-site age estimation ought to be avoided" and a holistic approach should always combine dental with skeletal predictors (e.g., clavicles).
- Why unresolved: The current implementation only assesses dental development (teeth 37 and 38) in isolation from other biological markers.
- What evidence would resolve it: An integrated model that processes both dental X-rays and skeletal imaging (hand/wrist or clavicle) to output a unified age estimate with uncertainty quantification.

## Limitations

- Interpretability benefits demonstrated qualitatively through visualizations but lack quantitative validation metrics
- Performance gap for tooth 38 attributed to data quality without systematically ruling out architectural limitations
- Claim that attention maps are "misleading" primarily supported by comparison rather than isolated ablation studies

## Confidence

- **High Confidence:** The AE-ViT framework improves classification accuracy over ViT baseline (0.712→0.815 for tooth 37, 0.462→0.543 for tooth 38)
- **Medium Confidence:** The data-centric explanation for tooth 38's remaining performance gap, based on latent space analysis
- **Low Confidence:** The interpretability benefits are quantifiable; attention maps from raw images are definitively misleading

## Next Checks

1. Conduct quantitative evaluation of interpretability by measuring attention map faithfulness to ground truth developmental landmarks across raw vs. reconstructed inputs
2. Perform ablation studies systematically varying reconstruction quality (LPIPS weight, latent dimension) to isolate its effect on ViT performance
3. Test alternative architectures (CNN-based classifiers) on the same AE-preprocessed inputs to verify if ViT-specific improvements are architecture-dependent or AE-methodology dependent