---
ver: rpa2
title: 'LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking
  Tokens'
arxiv_id: '2510.11919'
source_url: https://arxiv.org/abs/2510.11919
tags:
- translation
- cotft
- ioft
- wang
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether generating intermediate reasoning
  tokens improves machine translation performance. Experiments across 10 language
  pairs and multiple benchmarks show that allowing large reasoning models to "think"
  before translating does not improve translation quality compared to direct translation.
---

# LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens

## Quick Facts
- arXiv ID: 2510.11919
- Source URL: https://arxiv.org/abs/2510.11919
- Reference count: 40
- Primary result: Generating intermediate reasoning tokens does not improve MT performance; translation quality and dataset size matter more than reasoning process.

## Executive Summary
This paper investigates whether generating intermediate reasoning tokens improves machine translation performance. Experiments across 10 language pairs and multiple benchmarks show that allowing large reasoning models to "think" before translating does not improve translation quality compared to direct translation. Fine-tuning models to "think" using distilled chain-of-thought explanations from teachers also fails to outperform standard input-output fine-tuning. However, using traces from modular translation-specific prompting strategies as intermediate information during fine-tuning yields improvements of up to 3.5 BLEU points. Analysis reveals that the presence of translation attempts within these traces is crucial for success. The paper concludes that using teachers to improve target translations or expand parallel corpora is more impactful than distilling their reasoning processes.

## Method Summary
The study employs a teacher-student fine-tuning framework where teacher models (Llama-4-Scout-17B or gemma-3-27b-it) generate intermediate reasoning traces using either chain-of-thought prompts or modular translation prompting strategies. Students (gemma-3-4b-pt or gemma-3-1b-pt) are fine-tuned with loss computed on target outputs. The research compares three approaches: IOFT (input-output fine-tuning), CoTFT (chain-of-thought fine-tuning), and RL (reinforcement learning with GRPO). Experiments are conducted across 10 language pairs using FLORES-200 devtest and synthetic parallel data, with evaluation using BLEU, MetricX-24-Hybrid-XXL, chrF++, and XCOMET-XXL metrics.

## Key Results
- Direct translation (IOFT) consistently outperforms "thinking" models in zero-shot MT evaluation.
- CoT distillation from teachers fails to improve student MT models compared to standard fine-tuning.
- Using traces from modular prompting strategies (MAPS, TEaR, CompTra) improves translation by up to 3.5 BLEU points.
- The presence of translation attempts within intermediate traces is crucial for performance gains.
- IOFT-BOA (best-of-all translation selection) outperforms all CoTFT variants.

## Why This Works (Mechanism)

### Mechanism 1: Translation Attempt Embedding in Intermediate Traces
- Claim: Intermediate tokens improve student MT models only when they contain concrete translation attempts, not abstract reasoning explanations.
- Mechanism: Modular prompting strategies generate draft translations during their multi-step processes. When these drafts are concatenated into traces for fine-tuning, students benefit from exposure to varied translation candidates and refinement patterns—not from the explanatory text itself.
- Core assumption: Student models learn translation patterns more effectively from concrete translation examples than from meta-reasoning text describing how to translate.
- Evidence anchors: Analysis reveals that the presence of translation attempts within these traces is crucial for success.

### Mechanism 2: Sentence Decomposition as Synthetic Parallel Data
- Claim: Sub-sentential translation pairs within intermediate tokens provide valuable in-context demonstrations for the final translation.
- Mechanism: Strategies like CompTra decompose source sentences into smaller phrases, translate each independently, and embed these partial pairs in the trace. Students learn from these synthetic examples as additional parallel data without expanding the training set.
- Core assumption: Sub-sentential pairs provide implicit compositional signal that aids sentence-level translation.
- Evidence anchors: COTFT consistently outperforms IOFT across all decomposition strategies.

### Mechanism 3: Target Quality Dominates Over Reasoning Process
- Claim: Improving target translation quality or expanding parallel corpora yields greater gains than distilling reasoning processes.
- Mechanism: IOFT-BOA (replacing ground truth with the best translation attempt across all strategies) outperforms all CoTFT variants. The signal from higher-quality targets outweighs any benefit from intermediate reasoning tokens.
- Core assumption: Translation is fundamentally different from reasoning tasks—pretraining corpora lack translation reasoning patterns, so distilled CoT provides limited utility.
- Evidence anchors: Using a teacher to improve target translations or expand parallel corpora is more impactful than distilling their CoT explanations.

## Foundational Learning

- **Chain-of-Thought Distillation for Reasoning Tasks**
  - Why needed: To understand the baseline approach that succeeds in math/coding but fails in MT.
  - Quick check: Can you explain why CoT distillation helps mathematical reasoning but provides no benefit for translation tasks?

- **Input-Output Fine-Tuning (IOFT) vs CoT Fine-Tuning (CoTFT)**
  - Why needed: Core comparison framework—IOFT trains on (source, target) pairs; CoTFT adds intermediate tokens.
  - Quick check: In CoTFT, where does the loss function apply—only on target or on intermediate tokens + target?

- **Modular Translation Prompting Strategies**
  - Why needed: Understanding MAPS (multi-aspect analysis + candidates), SBYS (pre-drafting → draft → refine → proofread), TEaR (translate → annotate errors → refine), CompTra (decompose → translate phrases → recombine).
  - Quick check: Which of these strategies generates MQM-style error annotations as an intermediate step?

## Architecture Onboarding

- **Component map:** Teacher model → Generates traces using CoT prompts or modular strategies → Concatenates traces → Student fine-tuning on (source, trace, target) triples

- **Critical path:**
  1. Run teacher with modular prompting strategy (MAPS/TEaR/CompTra) on source sentences.
  2. Concatenate strategy outputs into single trace; select best translation attempt via BLASER 2.0-QE.
  3. Fine-tune student on (source, trace, target) with max_seq_length=2048 vs 512 for IOFT.
  4. Evaluate zero-shot on FLORES-200 devtest with greedy decoding (T=0.0).

- **Design tradeoffs:**
  - CoTFT requires 4x longer sequences (2048 vs 512), increasing memory and training time.
  - IOFT-BOA achieves better results than CoTFT without inference-time thinking overhead.
  - RL (GRPO) after CoTFT provides marginal gains (~+1.3 BLEU) but underperforms continued IOFT.

- **Failure signatures:**
  - CoTFT with standard CoT templates (T1–T6) consistently underperforms IOFT by 0.5–5 BLEU.
  - When teacher translation quality is poor, traces from prompting strategies provide no benefit.
  - SBYS traces can degrade performance if translation attempts are drowned in non-useful tokens.

- **First 3 experiments:**
  1. Establish IOFT baseline on your language pair (5k steps, lr=1e-5, batch_size=16 effective).
  2. Run CoTFT with MAPS traces; compare BLEU/MetricX against IOFT at 2k/5k steps.
  3. Implement IOFT-BOA: select best translation across all strategy attempts (BLASER 2.0-QE) and fine-tune—this should match or exceed CoTFT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL) reward functions be designed to effectively utilize intermediate reasoning tokens in Machine Translation (MT), rather than evaluating only the final output?
- Basis in paper: The authors note in Section 6.2 that applying GRPO failed to induce meaningful reasoning because "CoT signals fail to induce meaningful reasoning when the reward is applied only to the final translation."
- Why unresolved: Current RL approaches for MT optimize for translation quality metrics (like BLEU or MetricX) on the final string, ignoring the internal validity or helpfulness of the "thinking" process.
- What evidence would resolve it: An RL framework that uses a process-based reward model (PRM) to evaluate the consistency and utility of intermediate steps, resulting in "thinking" models that outperform standard IOFT.

### Open Question 2
- Question: Do the performance gains from modular prompting traces (e.g., MAPS, SBYS) stem primarily from the structural decomposition of the task or merely from the inclusion of high-quality translation drafts within the context?
- Basis in paper: Section 5.3 and Figure 5 show that CoT distillation succeeds only when traces contain "translation attempts," suggesting the content of the draft matters more than the reasoning structure.
- Why unresolved: While the paper proves that traces with drafts improve performance, it remains unclear if the reasoning steps provide algorithmic scaffolding or if they are simply redundant carriers for the draft text.
- What evidence would resolve it: An ablation study where the structural reasoning text is preserved but the translation drafts are removed or replaced with low-quality noise, to see if the "thinking structure" retains any utility.

### Open Question 3
- Question: Would continued pre-training on large-scale synthetic "translation reasoning" data (beyond fine-tuning) enable LLMs to natively leverage thinking tokens for MT?
- Basis in paper: Section 6.2 suggests CoT's failure in MT may be due to the "scarcity of reasoning-like data" in pre-training corpora compared to mathematics or code.
- Why unresolved: The study focuses on fine-tuning (SFT and RL) existing pre-trained models; it does not test if changing the pre-training data distribution could fundamentally alter the model's ability to reason about translation.
- What evidence would resolve it: Training a model from scratch or continual pre-training on a corpus enriched with step-by-step translation logs, followed by an evaluation of its "thinking" capabilities.

## Limitations
- Limited evaluation scope: Only tested on FLORES-200 devtest (1012 examples) and synthetic parallel data, without examining robustness to domain shifts.
- Short training duration: 5K fine-tuning steps may be insufficient to observe convergence differences between IOFT and CoTFT.
- Lack of linguistic analysis: No deeper error analysis to explain why CoT tokens fail to help beyond metric comparisons.

## Confidence
- **High Confidence**: Translation quality and dataset size are more important than intermediate reasoning tokens for MT performance; CoT distillation fails in MT while succeeding in reasoning tasks due to MT's unique training dynamics.
- **Medium Confidence**: Translation attempts within intermediate traces are the crucial signal for CoTFT success; decomposing sentences into sub-sentential pairs provides valuable synthetic parallel data.
- **Low Confidence**: The paper's conclusion that reasoning signals inherently fail in MT due to scarcity of reasoning-like data in pretraining corpora—this mechanism is asserted but not empirically validated.

## Next Checks
1. **Extended Training and Convergence Analysis**: Run IOFT and CoTFT experiments for 20K steps (4x longer) on English→Xhosa to verify whether CoTFT ever catches up to or surpasses IOFT, addressing the convergence uncertainty.

2. **Linguistic Error Analysis**: Manually analyze 50 translation outputs from best-performing IOFT-BOA and CoTFT-MAPS models on FLORES-200 to identify specific error patterns (e.g., lexical choice, reordering, fluency) and determine whether intermediate tokens actually affect error types differently.

3. **Teacher Quality Sensitivity Test**: Repeat the CoTFT-MAPS experiment with different teacher models spanning quality spectrum (e.g., Qwen2.5-7B-Instruct vs Llama-4-Scout-17B) to validate the claim that poor teacher quality prevents CoTFT benefits, directly testing the target quality dominance mechanism.