---
ver: rpa2
title: 'OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference
  Attacks on Large Vision-Language Models'
arxiv_id: '2510.16295'
source_url: https://arxiv.org/abs/2510.16295
tags:
- membership
- data
- training
- images
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenLVLM-MIA, a benchmark for evaluating
  membership inference attacks on large vision-language models (LVLMs). Existing benchmarks
  often suffer from distribution bias between member and non-member data, leading
  to inflated attack success rates.
---

# OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models

## Quick Facts
- arXiv ID: 2510.16295
- Source URL: https://arxiv.org/abs/2510.16295
- Reference count: 38
- Primary result: State-of-the-art MIA methods achieve ~0.5 AUC-ROC (chance level) on controlled LVLM benchmark, suggesting prior results were artifacts of distribution bias

## Executive Summary
This paper introduces OpenLVLM-MIA, a benchmark for evaluating membership inference attacks on large vision-language models (LVLMs). The key innovation addresses a critical flaw in existing MIA benchmarks: distribution bias between member and non-member data artificially inflates attack success rates. By creating a carefully controlled dataset of 6,000 images with balanced distributions and verified ground-truth labels across three training stages, the authors demonstrate that state-of-the-art MIA methods perform at chance level, revealing that prior results were detecting dataset artifacts rather than true membership status. The benchmark and evaluation tools are publicly released to support reproducible research.

## Method Summary
The authors created a controlled LVLM membership inference benchmark using open-source components (OpenCLIP ViT-B/32 + Vicuna-7B v1.5) to ensure complete training data transparency. The benchmark covers three training stages: vision encoder pretraining on LAION-2B-en, projector pretraining on LLaVA-Pretrain, and instruction tuning on LLaVA-Instruct. Each stage uses 1,000 member and 1,000 non-member images from distributionally matched sources. Ground-truth membership is verified through MD5 hash indexing of all training data. The benchmark includes a distribution audit using Classifier Two-Sample Test (C2ST) with DINOv2 embeddings to verify alignment, and evaluates 10 MIA methods in gray-box settings across different logit slices.

## Key Results
- C2ST with visual features alone achieved AUROC of 0.949 on existing VL-MIA benchmark, confirming severe distribution bias
- On OpenLVLM-MIA, all tested MIA methods achieved AUROC around 0.5 (chance level) across all three training stages
- Distribution audit showed minimal bias (AUROC 0.51-0.58) between member and non-member images in the controlled benchmark
- TPR@0.05FPR remained below 0.078 for all methods, indicating limited practical attack capability

## Why This Works (Mechanism)

### Mechanism 1: Distribution Bias Detection via Visual Features
Prior MIA benchmarks conflate distributional artifacts with true membership signals. A Classifier Two-Sample Test (C2ST) using only image embeddings (DINOv2) can separate member from non-member samples in existing benchmarks, indicating MIA methods may detect dataset construction artifacts rather than model memorization. If C2ST AUROC using visual features approaches 0.5, distribution bias is controlled and MIA evaluation is valid.

### Mechanism 2: Ground-Truth Membership via Fully Transparent Training Pipeline
Verified membership labels require training data fully disclosed for all model components. The authors replace closed components with open equivalents (OpenCLIP) and record which images are used at each training stage via MD5 hash indexing. If any component has undisclosed training data, membership labels become probabilistic rather than deterministic.

### Mechanism 3: Stage-Separate MIA Evaluation Across Training Pipeline
Membership signals differ across vision encoder pretraining, projector pretraining, and instruction tuning stages. The benchmark samples 1,000 member and 1,000 non-member images per stage from distributionally matched sources and evaluates MIA methods independently at each stage. If MIA performance is similar across all stages, stage-specific evaluation is unnecessary.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIA)**
  - Why needed here: This is the core threat model. You must understand that MIA aims to infer whether a sample was in training data—a privacy violation.
  - Quick check question: In a gray-box MIA setting, what information does the attacker have access to?

- **Concept: Classifier Two-Sample Test (C2ST)**
  - Why needed here: This is the diagnostic tool for detecting distribution bias. C2ST trains a classifier to distinguish two distributions; high accuracy indicates distributional difference.
  - Quick check question: If C2ST achieves 0.95 AUROC on visual features alone, what does this imply about the benchmark?

- **Concept: LVLM Training Stages (Vision Encoder → Projector → Instruction Tuning)**
  - Why needed here: The benchmark evaluates membership at each stage. Each uses different data and serves different functions.
  - Quick check question: Which stage connects the vision encoder output to the LLM input?

## Architecture Onboarding

- **Component map:** OpenCLIP ViT-B/32 (Vision Encoder) → Projector (linear projection layer) → Vicuna-7B v1.5 (LLM)

- **Critical path:**
  1. Verify training data is public for all components before building benchmark
  2. Index all training images with MD5 hashes for membership tracking
  3. Select non-member images from same time period/domain as members
  4. Run C2ST with DINOv2 embeddings to verify distribution alignment (target AUROC ≈ 0.5)
  5. Evaluate MIA methods at each stage separately

- **Design tradeoffs:**
  - Using OpenCLIP instead of OpenAI CLIP ensures transparency but may affect model performance comparability
  - 6,000 images provides statistical power but may not capture domain-specific privacy risks
  - Gray-box assumption (logit access) excludes purely black-box attacks

- **Failure signatures:**
  - C2ST AUROC > 0.6 on visual features → distribution bias not controlled
  - MIA AUROC varies significantly across stages → stage-specific analysis needed
  - TPR@0.05FPR < 0.1 across all methods → current MIA methods are ineffective for controlled settings

- **First 3 experiments:**
  1. Reproduce C2ST distribution audit on VL-MIA vs. OpenLVLM-MIA using DINOv2 embeddings
  2. Run Min-K% Prob and Max Rényi MIA methods on OpenLVLM-MIA across all three stages
  3. Test whether different logit slices (img, inst, desp, inst+desp) affect MIA performance at each training stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MIA methods be developed that explicitly leverage the interaction between vision and language modalities to overcome the limitations of single-modality approaches?
- Basis in paper: Section 7.4 states that current approaches "do not account for multimodality" and requires future methods that leverage "interaction between vision and language."
- Why unresolved: Existing single-modality methods fail on this benchmark (chance-level), but cross-modal attack vectors remain unexplored.
- What evidence would resolve it: An attack method utilizing vision-language correlation achieving significantly >0.5 AUROC on OpenLVLM-MIA.

### Open Question 2
- Question: Do the observed chance-level MIA results generalize to significantly larger model scales (e.g., 70B parameters) or alternative LVLM architectures?
- Basis in paper: Section 8 explicitly lists as a limitation that the authors "did not evaluate larger models (13B, 70B) or other architectures (e.g., BLIP-2, Flamingo)."
- Why unresolved: The difficulty of MIA might be tied to the specific 7B parameter capacity of the tested model; larger models may memorize more.
- What evidence would resolve it: Replicating the OpenLVLM-MIA evaluation pipeline on larger scale or architecturally distinct models.

### Open Question 3
- Question: Is MIA performance on general web-crawled images representative of privacy risks for sensitive, domain-specific data?
- Basis in paper: Section 8 states the dataset "does not explicitly include domain-specific data or data labeled as sensitive."
- Why unresolved: Sensitive data (e.g., medical, copyright) often has distinct features that might be memorized differently than the "high volume" web data tested.
- What evidence would resolve it: Evaluation of MIA methods on a controlled benchmark of sensitive data showing statistically significant detection rates.

## Limitations
- Focus on gray-box attacks only, excluding purely black-box scenarios where MIA methods might perform differently
- Use of open-source model components that may not generalize to proprietary LVLMs like GPT-4V or Gemini
- Limited domain coverage—6,000 natural images may not capture privacy risks in medical, legal, or copyrighted content domains

## Confidence
- Core claim that controlled benchmarks reveal prior MIA results as artifacts of dataset construction: **High confidence**
- Distribution audit mechanism validity: **High confidence**
- Finding that state-of-the-art MIA methods perform at chance level: **High confidence**
- Generalizability to real-world deployment scenarios: **Medium confidence** (limited by domain coverage and model scale)

## Next Checks
1. **Black-box extension**: Evaluate MIA performance under purely black-box conditions (only confidence scores, no logits) to test robustness of the chance-level findings
2. **Domain-specific testing**: Apply OpenLVLM-MIA methodology to sensitive domains (medical imaging, copyrighted art) to identify potential vulnerabilities in high-stakes scenarios
3. **Cross-model generalization**: Test whether the distribution bias detection mechanism applies to other LVLM architectures beyond the OpenCLIP-Vicuna pipeline used in this study