---
ver: rpa2
title: 'HARP: Hallucination Detection via Reasoning Subspace Projection'
arxiv_id: '2509.11536'
source_url: https://arxiv.org/abs/2509.11536
tags:
- reasoning
- subspace
- hallucination
- semantic
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HARP, a hallucination detection method for
  large language models (LLMs) that focuses on reasoning subspace projection. The
  key idea is that LLM hidden states can be decomposed into semantic and reasoning
  subspaces, with the reasoning subspace capturing the model's internal reasoning
  process.
---

# HARP: Hallucination Detection via Reasoning Subspace Projection

## Quick Facts
- arXiv ID: 2509.11536
- Source URL: https://arxiv.org/abs/2509.11536
- Reference count: 40
- Primary result: Achieves 92.8% AUROC on TriviaQA, outperforming previous best by 7.5%

## Executive Summary
HARP is a hallucination detection method for large language models that identifies a reasoning subspace in hidden states through Singular Value Decomposition of the Unembedding layer. The method projects hidden states onto this reasoning subspace, reducing dimensionality to ~5% while filtering semantic noise, leading to enhanced robustness and state-of-the-art performance. HARP demonstrates strong generalization across different models and datasets, with consistent improvements over baseline hallucination detection approaches.

## Method Summary
HARP performs SVD on the LLM's Unembedding layer weight matrix to identify basis vectors for semantic and reasoning subspaces. The trailing singular vectors (small singular values) form the reasoning subspace basis, which captures the model's internal reasoning process while discarding semantic information. Hidden states from the final layer are projected onto this reasoning subspace basis, reducing feature dimensionality to approximately 5% of the original. A lightweight 2-layer MLP classifier then detects hallucinations from these projected features, achieving enhanced robustness by focusing on reasoning signals rather than semantic noise.

## Key Results
- Achieves 92.8% AUROC on TriviaQA, outperforming previous best method by 7.5%
- Reduces feature dimensionality to ~5% of original while maintaining detection performance
- Demonstrates strong robustness with consistent improvements across multiple models and datasets
- Shows state-of-the-art generalization from NQ Open to TriviaQA and TyDiQA-GP

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Subspace Disentanglement via Unembedding Weights
The hidden state space of LLMs can be decomposed into a direct sum of semantic and reasoning subspaces, which are operationally separable using the Unembedding layer. By performing SVD on the Unembedding layer, basis vectors for these subspaces are identified, with the reasoning subspace corresponding to near-zero singular values.

### Mechanism 2: Noise Reduction via Projection
Projecting hidden states onto the reasoning subspace reduces feature dimensionality (~5% of original) and filters out semantic noise, leading to better generalization. Hallucinations often occur when fluency is high but reasoning is flawed, so isolating the reasoning subspace removes dominating semantic signals that might mask reasoning uncertainty.

### Mechanism 3: Causal Intervention via Reasoning Patch
The reasoning subspace causally encodes the logic of the generation process. Replacing reasoning components of a "normal" run with those from a Chain-of-Thought run forces the model to output reasoning steps, validating that the subspace captures functional reasoning logic.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: HARP relies on SVD of the weight matrix to find orthogonal bases
  - Quick check question: If all singular values of W_unemb were non-zero, how would that affect the definition of the "reasoning subspace" in this paper?

- **Concept: Direct Sum of Subspaces**
  - Why needed here: The paper posits H = S_Semantic ⊕ S_Reasoning
  - Quick check question: Why is orthogonality important here? (Hint: It ensures that projecting onto the reasoning subspace removes all semantic influence)

- **Concept: The Unembedding Layer**
  - Why needed here: This is the "Subspace Decomposer" in HARP
  - Quick check question: According to HARP, does the Unembedding layer "see" the reasoning subspace?

## Architecture Onboarding

- **Component map:** LLM Backbone -> SVD Module -> Projector -> Detector MLP
- **Critical path:** The extraction of trailing basis vectors (V_R) from SVD of W_unemb
- **Design tradeoffs:** Rank selection (k ≈ 95% of dimensions), layer selection (final layer), detector complexity (simple MLP)
- **Failure signatures:** Random projection baseline performance, semantic leakage into reasoning subspace
- **First 3 experiments:**
  1. Sanity Check: Verify removing reasoning component preserves token rankings
  2. Ablation: Compare HARP projections against random projections
  3. Cross-Dataset Generalization: Train on one dataset, test on another

## Open Questions the Paper Calls Out
- Can HARP framework be adapted for real-time hallucination mitigation rather than just detection?
- Does the identified reasoning subspace generalize to non-QA tasks or encoder-decoder architectures?
- Is there a theoretically grounded method for determining the optimal rank k for semantic/reasoning split?

## Limitations
- Subspace decomposition validity relies on untested structural assumptions about Unembedding layer behavior
- Generalization across tasks beyond QA is untested, particularly for non-semantic reasoning errors
- Detector may overfit to beam-search-perturbed reasoning patterns rather than universal failure modes

## Confidence
- High Confidence: Experimental results and subspace projection method are well-specified and reproducible
- Medium Confidence: Unembedding layer as "subspace decomposer" assumption is plausible but untested across architectures
- Low Confidence: Claim that reasoning subspace captures "model's internal reasoning process" lacks rigorous definition

## Next Checks
1. Apply HARP to a non-Transformer architecture (e.g., Mamba or RWKV) to test subspace decomposition universality
2. Evaluate HARP on a non-QA task (e.g., code generation) to assess reasoning subspace generalization
3. Perform ablation studies varying detector complexity to determine if performance is bottlenecked by model capacity or subspace projection