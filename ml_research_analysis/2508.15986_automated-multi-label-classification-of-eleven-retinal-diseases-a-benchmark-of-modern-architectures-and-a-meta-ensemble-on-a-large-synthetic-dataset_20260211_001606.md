---
ver: rpa2
title: 'Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark
  of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset'
arxiv_id: '2508.15986'
source_url: https://arxiv.org/abs/2508.15986
tags:
- dataset
- performance
- ensemble
- were
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study established a foundational benchmark for multi-label
  retinal disease classification using a large synthetic fundus image dataset (SynFundus-1M).
  The researchers trained six modern deep learning architectures and developed a meta-ensemble
  model using XGBoost to classify eleven retinal diseases.
---

# Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset

## Quick Facts
- **arXiv ID**: 2508.15986
- **Source URL**: https://arxiv.org/abs/2508.15986
- **Reference count**: 27
- **Primary result**: Meta-ensemble trained on synthetic fundus images achieves 0.9973 macro-AUC on internal validation and generalizes to real clinical datasets (AUC 0.7972 for DR, 0.9126 for glaucoma, 0.8800 for multi-label)

## Executive Summary
This study establishes a benchmark for multi-label retinal disease classification using a large synthetic fundus image dataset (SynFundus-1M) containing 1 million images across 11 disease categories. The researchers trained six modern deep learning architectures and developed a meta-ensemble model using XGBoost to classify eleven retinal diseases. The ensemble achieved exceptional performance on internal validation with a macro-average AUC of 0.9973 and an F1-score of 0.9244. Critically, the models demonstrated strong generalization to real-world clinical datasets in zero-shot evaluation, achieving AUCs of 0.7972 on diabetic retinopathy, 0.9126 on glaucoma, and 0.8800 on multi-label retinal disease classification. The study provides a robust baseline for future research on synthetic datasets and demonstrates that models trained exclusively on synthetic data can effectively classify multiple pathologies and generalize to real clinical images.

## Method Summary
The researchers generated a synthetic fundus image dataset (SynFundus-1M) using a Denoising Diffusion Probabilistic Model trained on 1.3 million private clinical images. They trained six deep learning architectures (ResNet50, EfficientNetV2-M, ConvNeXtV2-Base, ViT-Base, SwinV2-Base, and RETFound) on this dataset for multi-label classification of 11 retinal diseases. Each model used binary cross-entropy with logits loss and was evaluated using 5-fold cross-validation. The best-performing models' out-of-fold predictions were combined into a 66-dimensional feature vector (6 models × 11 diseases) and used to train an XGBoost meta-learner wrapped in a MultiOutputClassifier. The final ensemble was evaluated zero-shot on three external clinical datasets for single-label and multi-label classification tasks.

## Key Results
- Meta-ensemble achieved macro-average AUC of 0.9973 and F1-score of 0.9244 on internal SynFundus-1M validation
- Zero-shot external validation: AUC 0.7972 on diabetic retinopathy dataset, 0.9126 on glaucoma dataset, 0.8800 macro-AUC on multi-label retinal disease dataset
- ConvNeXtV2-Base achieved comparable performance to ensemble with AUC 0.9967 on internal validation
- RETFound pre-training showed significant benefits over random initialization (AUC 0.9959 vs 0.9924)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Models trained exclusively on synthetic fundus images can generalize to real clinical data in zero-shot evaluation.
- **Mechanism**: The Denoising Diffusion Probabilistic Model (DDPM) that generated SynFundus-1M was trained on 1.3M private clinical images, encoding real pathological feature distributions into synthetic samples. Deep models learning these distributions acquire transferable representations.
- **Core assumption**: The synthetic images preserve disease-relevant visual features that exist in real clinical images, and the domain shift is surmountable without fine-tuning.
- **Evidence anchors**:
  - [abstract]: "models trained exclusively on synthetic data can accurately classify multiple pathologies and generalize effectively to real clinical images"
  - [section 3.3]: Zero-shot evaluation achieved AUC 0.7972 (DR), 0.9126 (glaucoma), 0.8800 (RFMiD multi-label)
  - [corpus]: Weak—neighbor papers focus on real data training; limited corpus evidence for synthetic-to-real transfer claims
- **Break condition**: If synthetic data contains systematic artifacts or exaggerated disease presentations not found in real clinical images, generalization would degrade on atypical/subtle pathologies.

### Mechanism 2
- **Claim**: A stacking ensemble with XGBoost meta-learner improves multi-label classification by assigning different weights to base models per disease class.
- **Mechanism**: Six diverse architectures (CNNs and Vision Transformers) produce 66 prediction features (6 models × 11 diseases). XGBoost learns optimal combinations per disease, effectively creating disease-specific model selection.
- **Core assumption**: Base models make uncorrelated errors, and their predictions contain complementary information the meta-learner can exploit.
- **Evidence anchors**:
  - [section 2.4]: "feature vector of size 66 (6 models × 11 diseases)"
  - [section 3.5, Figure 6]: For DR, CNN predictions (EfficientNetV2, ResNet) were most influential; for degenerative myopia, Transformer models (SwinV2, RETFound) received higher weight
  - [corpus]: Related papers (HOG-CNN, MDF-MLLM) support feature fusion benefits but don't address stacking specifically
- **Break condition**: If base models are highly correlated or systematically weak on the same disease classes, ensemble gains diminish.

### Mechanism 3
- **Claim**: Pre-training provides measurable performance gains even when fine-tuning on large synthetic datasets.
- **Mechanism**: Pre-trained weights initialize models with learned feature extractors (edge detectors, texture patterns, anatomical priors), providing better optimization starting points than random initialization.
- **Core assumption**: The pre-training domain (ImageNet or retinal images) shares low-level and mid-level features with the target task.
- **Evidence anchors**:
  - [section 3.4, Table 8]: ConvNeXtV2 pre-trained achieved 0.9967 vs. 0.9951 from scratch; RETFound pre-trained achieved 0.9959 vs. 0.9924
  - [section 4]: "domain-specific pre-training (RETFound) and general-purpose pre-training both outperforming models trained from scratch"
  - [corpus]: Neighbor papers lack direct comparison data on pre-training vs. scratch training
- **Break condition**: If pre-training domain differs fundamentally from medical imaging (e.g., text-only pre-training), transfer benefits would be minimal.

## Foundational Learning

- **Concept: Multi-label Classification with BCEWithLogitsLoss**
  - Why needed here: Unlike multi-class (one label per image), multi-label allows co-occurring diseases (e.g., DR + glaucoma). BCEWithLogitsLoss treats each label independently.
  - Quick check question: Can you explain why softmax would fail for multi-label retinal disease classification?

- **Concept: Out-of-Fold (OOF) Predictions for Ensemble Stacking**
  - Why needed here: Stacking requires training meta-learners on predictions from models that never saw those samples during training. OOF ensures no information leakage.
  - Quick check question: Why can't you simply use in-fold predictions to train the XGBoost meta-learner?

- **Concept: Zero-Shot Generalization Testing**
  - Why needed here: External validation without fine-tuning rigorously tests domain transfer. This is the critical evidence claim for synthetic-to-real viability.
  - Quick check question: What would it mean if models required fine-tuning on external datasets before evaluation?

## Architecture Onboarding

- **Component map**: Input: 224×224 fundus images (192×192 for SwinV2) -> Six parallel architectures (ResNet50, EfficientNetV2-M, ConvNeXtV2-Base, ViT-Base, SwinV2-Base, RETFound) -> Binary classifiers per disease (11 outputs each) -> XGBoost meta-learner -> Output: 11 disease probabilities per image

- **Critical path**:
  1. Verify dataset integrity (file existence check)
  2. Apply MultilabelStratifiedKFold (preserves class balance per label)
  3. Train each backbone with optimized hyperparameters (learning rate, weight decay, dropout from Optuna)
  4. Collect OOF predictions across all folds
  5. Stack into 66-feature vectors
  6. Train XGBoost meta-learner
  7. Evaluate zero-shot on external datasets

- **Design tradeoffs**:
  - Macro-average AUC vs. micro-average: Macro treats all classes equally (appropriate for rare diseases), micro would be dominated by prevalent conditions
  - Ensemble complexity vs. inference cost: Six backbones plus XGBoost increases latency; ConvNeXtV2 alone provides near-equivalent performance on some tasks
  - Pre-trained vs. scratch: Domain-specific (RETFound) shows larger gains but requires specialized weights

- **Failure signatures**:
  - Low AUC on specific disease class → check label prevalence and augmentation appropriateness
  - Large train-validation gap → reduce dropout, increase augmentation, verify no data leakage
  - Poor external generalization → synthetic data may lack disease heterogeneity; consider fine-tuning
  - Ensemble underperforms best single model → base models too correlated; diversify architectures

- **First 3 experiments**:
  1. **Baseline single model**: Train ConvNeXtV2-Base on one fold with ImageNet pre-training; verify macro-AUC approaches ~0.996
  2. **OOF sanity check**: Confirm OOF predictions have correct shape (N×66) and no target leakage by checking that each sample's prediction came from a model trained on different folds
  3. **Minimal external validation**: Run zero-shot inference on AIROGS subset (glaucoma task); expect AUC ~0.91 as reported. If significantly lower, verify preprocessing matches training pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning the synthetically-trained ensemble on small, expert-labeled real clinical datasets yield superior performance on external cohorts compared to models initialized with general-purpose pre-training (e.g., ImageNet)?
- Basis in paper: [explicit] The Discussion section proposes using the models developed in this study as a "powerful pre-trained foundation" to be fine-tuned on real data to refine performance.
- Why unresolved: This study evaluated zero-shot generalization and ablations of pre-training on synthetic data, but did not test the transfer learning efficiency of the final synthetic-trained weights against standard baselines when fine-tuned on real data.
- What evidence would resolve it: A comparative experiment fine-tuning the synthetic-pretrained model versus an ImageNet-pretrained model on a limited real-world dataset, followed by evaluation on a held-out clinical test set.

### Open Question 2
- Question: What is the diagnostic performance and safety profile of the synthetically-trained ensemble when deployed in a prospective, multi-center clinical workflow?
- Basis in paper: [explicit] The authors state in the Limitations and Discussion that "performance in a prospective clinical workflow remains to be evaluated" and that "prospective clinical trials are the necessary next step."
- Why unresolved: The current validation relies on retrospective, static datasets (AIROGS, RFMiD), which may not capture the full variability of live clinical environments, imaging hardware differences, or workflow integration challenges.
- What evidence would resolve it: A prospective clinical trial measuring the model's sensitivity, specificity, and agreement with human graders in a live screening setting.

### Open Question 3
- Question: Does the synthetic data generation process, which may produce "exaggerated or typical" disease manifestations, result in reduced model sensitivity for subtle or early-stage pathologies in real-world images?
- Basis in paper: [inferred] The authors note a limitation that the SynFundus-1M dataset "may produce exaggerated or 'typical' disease manifestations, potentially limiting the models' sensitivity to subtle or atypical pathologies."
- Why unresolved: While the model generalized well overall, the study did not stratify external validation performance by disease severity (e.g., mild vs. severe Diabetic Retinopathy) to confirm if sensitivity drops for subtle cases.
- What evidence would resolve it: A sub-group analysis on an external clinical dataset specifically evaluating the model's detection performance on early-stage or subtle clinical findings versus distinct, advanced pathologies.

## Limitations
- **Dataset provenance**: The primary strength (synthetic data generation) is also its central limitation. SynFundus-1M was generated using a DDPM trained on 1.3M private clinical images, but neither the original clinical dataset nor the synthetic dataset are publicly available, creating fundamental reproducibility barriers.
- **Zero-shot generalization**: While reported AUCs on external datasets are promising (0.7972 for DR, 0.9126 for glaucoma, 0.8800 for multi-label), these were achieved without fine-tuning. The absolute performance levels are moderate-to-good but fall short of state-of-the-art models trained directly on real clinical data.
- **Ensemble necessity**: The meta-ensemble improved performance marginally over the best single model (ConvNeXtV2), raising questions about whether the added complexity is justified for practical deployment.

## Confidence
- **High confidence**: Internal validation performance on SynFundus-1M (macro-AUC 0.9973, F1 0.9244) - well-documented with proper cross-validation methodology.
- **Medium confidence**: External zero-shot generalization to real clinical datasets - performance is reasonable but synthetic-to-real transfer remains an open question requiring independent verification.
- **Medium confidence**: Pre-training benefits (ImageNet and RETFound) - demonstrated through ablation but limited to this specific synthetic dataset context.
- **Low confidence**: The practical necessity of the XGBoost meta-ensemble over simpler ensemble strategies or single best models.

## Next Checks
1. **Public dataset replication**: Request or create a small public synthetic fundus dataset using the same DDPM approach, then independently verify that models trained on it can achieve comparable performance on standard public datasets (DR, glaucoma, RFMiD).

2. **Ablation study on ensemble components**: Systematically remove individual base models from the ensemble and measure performance degradation per disease class to quantify the actual contribution of each architecture and justify the ensemble complexity.

3. **Fine-tuning vs. zero-shot comparison**: Fine-tune the best-performing models (ConvNeXtV2 or RETFound) on each external dataset and compare performance gains against zero-shot evaluation to establish whether the synthetic-to-real transfer gap is bridgeable through minimal adaptation.