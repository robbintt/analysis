---
ver: rpa2
title: 'MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction
  Modelling'
arxiv_id: '2508.05492'
source_url: https://arxiv.org/abs/2508.05492
tags:
- chest
- trauma
- clinical
- moma
- alcohol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Multimodal-Agents (MoMA), a novel
  architecture for clinical prediction using multimodal electronic health record (EHR)
  data. MoMA leverages specialized LLM agents to convert non-textual modalities like
  medical images and lab results into structured textual summaries, which are then
  integrated by an aggregator agent with clinical notes to form a unified multimodal
  summary.
---

# MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling

## Quick Facts
- arXiv ID: 2508.05492
- Source URL: https://arxiv.org/abs/2508.05492
- Reference count: 40
- Key outcome: MoMA achieves macro-F1 scores near 0.85 and micro-F1 scores above 0.90 for trauma tasks, and an AUROC of around 0.75 for alcohol screening

## Executive Summary
This paper introduces Mixture-of-Multimodal-Agents (MoMA), a novel architecture for clinical prediction using multimodal electronic health record (EHR) data. MoMA leverages specialized LLM agents to convert non-textual modalities like medical images and lab results into structured textual summaries, which are then integrated by an aggregator agent with clinical notes to form a unified multimodal summary. This summary is used by a predictor agent to generate clinical predictions. Evaluated on three clinical prediction tasks (chest trauma severity, multitask trauma severity, and unhealthy alcohol use screening) using real-world datasets, MoMA outperforms current state-of-the-art methods.

## Method Summary
MoMA employs a modular architecture where specialized LLM agents convert various non-textual clinical data (images, lab results) into structured textual summaries. An aggregator agent combines these summaries with clinical notes into a unified multimodal summary. A predictor agent then uses this consolidated information to make clinical predictions. The system is designed to enhance prediction accuracy while reducing the need for extensive training data compared to traditional joint fusion approaches.

## Key Results
- Macro-F1 scores near 0.85 and micro-F1 scores above 0.90 for trauma prediction tasks
- AUROC of approximately 0.75 for unhealthy alcohol use screening
- Outperforms state-of-the-art methods across all three evaluated clinical prediction tasks
- Demonstrates enhanced accuracy and flexibility across various tasks and subgroups

## Why This Works (Mechanism)
The architecture works by leveraging the natural language processing strengths of LLMs to bridge the gap between different data modalities. By converting all modalities to text, MoMA can use established NLP techniques for integration and reasoning. The modular design allows each agent to specialize in handling specific types of information, potentially improving the quality of the final prediction.

## Foundational Learning

### Multimodal Integration
**Why needed:** Clinical data comes in diverse formats (text, images, numerical values) that traditional models struggle to combine effectively
**Quick check:** Compare performance when using only single modalities versus the integrated approach

### Agent Specialization
**Why needed:** Different data types require different processing approaches and domain knowledge
**Quick check:** Evaluate performance impact of using specialized versus generic processing agents

### Textual Summarization
**Why needed:** Converting complex medical data to structured text enables leveraging advanced NLP techniques
**Quick check:** Assess summary quality and completeness through clinician review

## Architecture Onboarding

### Component Map
Specialized Modality Agents -> Aggregator Agent -> Predictor Agent

### Critical Path
Modality conversion → Text aggregation → Prediction generation

### Design Tradeoffs
- Modularity enables easier maintenance but adds communication overhead
- Text-based integration leverages mature NLP tools but may lose some modality-specific information
- Specialized agents improve accuracy but increase system complexity

### Failure Signatures
- Inconsistent modality conversion leading to poor aggregation
- Aggregator unable to reconcile conflicting information from different agents
- Predictor fails to extract relevant information from the unified summary

### First Experiments
1. Test modality conversion accuracy on a held-out validation set
2. Evaluate aggregator performance with synthetic multimodal summaries
3. Benchmark predictor performance using ground truth unified summaries

## Open Questions the Paper Calls Out
None

## Limitations
- Potential overfitting to specific clinical domains tested
- Computational overhead of using multiple LLM agents not addressed
- Generalizability to other non-textual modalities (genomic data, continuous monitoring) remains untested
- No comparison with purely unimodal approaches for individual tasks

## Confidence

| Claim | Confidence |
|-------|------------|
| Architecture design and basic functionality | High |
| Performance improvements within tested datasets | Medium |
| Computational efficiency and scalability claims | Low |

## Next Checks

1. Test MoMA on at least two additional clinical prediction tasks with different non-textual modalities (e.g., radiology reports with imaging, or genomics with lab results) to assess generalizability
2. Conduct an ablation study comparing MoMA to the best-performing unimodal model for each modality to quantify the true benefit of multimodal integration
3. Measure the computational overhead (inference time, memory usage) of MoMA relative to baseline models to evaluate deployment feasibility in clinical settings