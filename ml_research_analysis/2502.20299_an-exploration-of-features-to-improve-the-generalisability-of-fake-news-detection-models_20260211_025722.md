---
ver: rpa2
title: An exploration of features to improve the generalisability of fake news detection
  models
arxiv_id: '2502.20299'
source_url: https://arxiv.org/abs/2502.20299
tags:
- features
- news
- fake
- these
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the generalisation challenges of fake news
  detection models, which perform well under cross-validation but struggle on unseen
  datasets due to biases in coarsely labelled training data. To address this, the
  research explores stylistic features (lexical, syntactic, semantic) and novel social-monetisation
  features (ads, external links, social media elements) that are less sensitive to
  dataset biases.
---

# An exploration of features to improve the generalisability of fake news detection models

## Quick Facts
- arXiv ID: 2502.20299
- Source URL: https://arxiv.org/abs/2502.20299
- Reference count: 24
- Token-based models suffer 24-30% accuracy drops on real-world data due to coarse publisher-level labelling biases

## Executive Summary
This study investigates why fake news detection models perform well under cross-validation but struggle on unseen datasets. The core issue stems from coarsely labelled training data where articles are labelled based on publisher reputation rather than content veracity, introducing biases that token-based models like TF-IDF and BERT are sensitive to. To address this, the research explores stylistic features (lexical, syntactic, semantic) and novel social-monetisation features (ads, external links, social media elements) that are less sensitive to dataset biases. Models were trained on the NELA 2020-21 dataset and evaluated on the manually labelled Facebook URLs dataset, showing that stylistic and social-monetisation features demonstrate improved generalisation with higher accuracy and balanced recall/specificity.

## Method Summary
The study trains fake news detection models on the NELA 2020-21 dataset (22,230 true and 25,650 fake articles, filtered to 285 articles per source) and evaluates them on the Facebook URLs dataset (5,355 fake and 798 true manually labelled English articles). Models include traditional ML (Logistic Regression, SVM, Gradient Boosting, Random Forest), neural approaches (FFNN, LSTM), and LLMs (LLaMA 3.2-1B). Feature types comprise token features (CountVectorizer, TF-IDF, Word2Vec, BERT), stylistic features (lexical complexity, punctuation patterns, POS distributions), and social-monetisation features (ads, external links, social media elements). Performance is assessed via 10-fold cross-validation on NELA and external validation on Facebook URLs, with permutation feature importance analysis identifying generalisable features.

## Key Results
- Token-based models (TF-IDF, BERT) and LLMs suffer 24-30% accuracy drops when tested on real-world data versus cross-validation performance
- Stylistic features achieve more balanced recall/specificity (0.65-0.75 range) versus token models showing large imbalances (e.g., Random Forest with BoW: recall 0.88, specificity 0.34)
- Social-monetisation features rank highly in permutation feature importance across both datasets, with 'ads' consistently important
- Reduced 33-feature set achieves 76% accuracy (vs. 75% original) on cross-dataset test while being computationally more efficient than embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stylistic features reduce sensitivity to topical biases present in coarsely labelled datasets, improving cross-dataset generalisation.
- Mechanism: Token-representations encode high-frequency vocabulary that correlates with publisher identity rather than content veracity. Stylistic features capture writing characteristics that are less tied to specific topics or publishers and more reflective of deceptive writing intent.
- Core assumption: Fake news producers exhibit consistent stylistic patterns that generalise across topics and publishers, independent of subject matter.
- Evidence anchors:
  - [abstract] "This issue stems from coarsely labelled training data, where articles are labelled based on their publisher, introducing biases that token-based models like TF-IDF and BERT are sensitive to."
  - [section 5.3, p.31-33] Stylistic feature models achieved more balanced recall and specificity (0.65-0.75 range) vs. token models showing large imbalances.
  - [corpus] Limited direct support; related work focuses on content/propagation features rather than stylistic bias mitigation specifically.
- Break condition: If fake news stylistic patterns diverge significantly across languages, cultures, or time periods, this mechanism would weaken.

### Mechanism 2
- Claim: Social-monetisation features capture economically-motivated signals that generalise because financial incentives are a stable driver of fake news production.
- Mechanism: Fake news operations prioritise revenue generation through advertising impressions, affiliate marketing, and viral social media distribution, creating measurable structural signals that persist regardless of article content or topic.
- Core assumption: Monetisation-driven fake news exhibits consistent structural patterns that differ systematically from legitimate news organisations' production choices.
- Evidence anchors:
  - [abstract] "Additionally, novel 'social-monetisation' features are introduced, capturing economic incentives behind fake news, such as the presence of advertisements, external links, and social media sharing elements."
  - [section 5.4, p.33-35] Permutation Feature Importance analysis showed all four social-monetisation features ranked as relevant across both datasets; 'ads' ranked highly in both NELA and Facebook URL datasets.
  - [corpus] No direct corpus support for monetisation features specifically; related work focuses on content/social propagation features.
- Break condition: If legitimate news outlets adopt similar monetisation strategies, or if fake news producers deliberately reduce these signals to evade detection, discriminative power would decline.

### Mechanism 3
- Claim: Reduced feature sets with high permutation importance maintain generalisation while improving computational efficiency and interpretability.
- Mechanism: Comprehensive feature sets may include redundant or noise features that cause models to overfit to training dataset artefacts. By identifying features with consistent positive importance across datasets, the reduced set captures genuinely transferable signals while eliminating dataset-specific noise.
- Core assumption: Features showing positive importance across datasets from different collection methodologies represent stable, generalisable indicators rather than dataset-specific artefacts.
- Evidence anchors:
  - [section 5.4, p.35, Table 9] Reduced 33-feature set achieved 76% accuracy (vs. 75% original) on cross-dataset test while being computationally more efficient than embeddings.
  - [section 5.4, p.34-35] PFI identified overlapping features including exclamation marks, all-caps text, cardinal numbers, and all four social-monetisation features as consistently important.
  - [corpus] No direct corpus evidence for this specific feature-reduction approach.
- Break condition: If important features for new domains/contexts differ substantially from the identified 33 features, or if feature importance distributions shift significantly over time, this reduced set would underperform.

## Foundational Learning

- Concept: **Cross-dataset generalisation vs. holdout validation**
  - Why needed here: The paper demonstrates that high performance under K-fold cross-validation does not predict real-world performance. Understanding why training-set performance is misleading is essential.
  - Quick check question: Can you explain why a model achieving 99% accuracy on cross-validation might perform poorly on a different dataset from the same domain?

- Concept: **Coarse (publisher-level) vs. granular (article-level) labelling**
  - Why needed here: The fundamental problem identified is that labelling all articles from a publisher as "fake" or "reliable" introduces systematic bias. Models learn publisher-specific vocabulary rather than content-based fake news indicators.
  - Quick check question: If a training dataset labels all articles from Publisher X as "fake" and all from Publisher Y as "reliable," what spurious patterns might a model learn?

- Concept: **Feature importance analysis for bias detection**
  - Why needed here: Permutation Feature Importance (PFI) is used to identify features that contribute positively across datasets, helping distinguish genuine signals from dataset-specific artefacts.
  - Quick check question: How does shuffling a feature's values help determine if that feature captures a generalisable signal or a dataset-specific pattern?

## Architecture Onboarding

- Component map:
Raw Article URLs -> [Web Scraping Layer] BeautifulSoup + Wayback Machine API -> [Feature Extraction Layer] Token features, Stylistic features, Social-monetisation features -> [Model Training Layer] Traditional ML, Neural, LLM -> [Evaluation Layer] Internal cross-validation, External validation

- Critical path: The generalisation assessment depends entirely on the external validation step using the Facebook URLs dataset. Without this, model performance appears excellent (90-99% accuracy) but fails in deployment. Prioritise setting up the external validation pipeline before extensive feature engineering.

- Design tradeoffs:
  - **Feature complexity vs. interpretability**: Token embeddings capture rich semantics but are opaque; stylistic features are interpretable via PFI but may miss nuanced semantic patterns.
  - **Dataset size vs. label quality**: NELA provides 1.5M+ articles but with coarse labels; Facebook URLs has manually verified labels but is smaller and restricted-access.
  - **Computational cost**: Fine-tuning LLMs is expensive; the reduced 33-feature set with Gradient Boosting is computationally efficient while maintaining comparable generalisation.

- Failure signatures:
  - High training accuracy (>95%) with low external validation accuracy (<70%) indicates overfitting to training-dataset biases.
  - Large imbalance between recall and specificity suggests the model learned dataset-specific class correlations rather than genuine signals.
  - LLM zero-shot/few-shot performance near random (0.50-0.55 accuracy) indicates pre-trained knowledge does not transfer to fake news detection without domain-specific fine-tuning.

- First 3 experiments:
  1. **Replicate the generalisation gap**: Train a TF-IDF + Random Forest model on NELA, evaluate via 10-fold CV (expect ~99% accuracy), then test on a held-out external dataset (expect 25-30% drop). This establishes the baseline problem.
  2. **Compare feature types under generalisation test**: Train identical models (e.g., Gradient Boosting) on NELA using token features vs. stylistic features vs. combined stylistic + social-monetisation features. Evaluate all three on the external dataset. Expect stylistic + social-monetisation to show better balanced recall/specificity.
  3. **Feature importance consistency check**: Train on NELA, compute Permutation Feature Importance on both NELA holdout and external validation data. Identify features with positive importance in both (these are the 33 generalisable features). Retrain using only these features and verify generalisation performance is maintained or improved.

## Open Questions the Paper Calls Out

- **Question**: Can integrating stylistic and social-monetisation features with Large Language Models (LLMs) improve the generalisability of fake news detection compared to LLMs alone?
- **Question**: Do the proposed stylistic and social-monetisation features maintain their generalisability across news domains distinct from the political/general topics used in this study?
- **Question**: What additional generalisable features exist within the broader context of the webpage structure that have not yet been explored?

## Limitations
- Primary validation relies on a single external dataset (Facebook URLs) that is not publicly available without special access, limiting reproducibility
- Study uses publisher-level coarse labelling, which inherently introduces the biases the research aims to address
- Feature importance analysis depends heavily on permutation importance scores that can be sensitive to implementation details and may not capture complex feature interactions

## Confidence
- **High Confidence**: The core observation that token-based models suffer substantial performance drops (24-30%) when moving from cross-validation to external datasets
- **Medium Confidence**: The effectiveness of stylistic and social-monetisation features for improving generalisation, based on a single validation dataset
- **Low Confidence**: The proposed reduced 33-feature set as a universal solution, based on feature importance from two specific datasets

## Next Checks
1. **External Dataset Validation**: Replicate the experiment using a publicly available fake news dataset (e.g., CoAID, ReCOVery) to verify whether the observed generalisation patterns hold beyond the restricted Facebook URLs dataset

2. **Temporal Stability Test**: Train models on NELA 2020 data and test on NELA 2021 data to assess whether the stylistic and social-monetisation features maintain their discriminative power across time

3. **Multi-Domain Transfer**: Evaluate the reduced 33-feature set on a dataset from a different domain (e.g., health misinformation vs. political misinformation) to test whether the identified generalisable features truly capture universal fake news indicators rather than dataset-specific patterns