---
ver: rpa2
title: Distributionally Robust Federated Learning with Outlier Resilience
arxiv_id: '2509.24462'
source_url: https://arxiv.org/abs/2509.24462
tags:
- robust
- learning
- distribution
- distributionally
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of outlier-robust federated learning
  in the presence of data heterogeneity and distribution shifts. The authors propose
  a novel distributionally robust optimization framework that uses an unbalanced Wasserstein
  distance to construct an ambiguity set that jointly captures geometric distributional
  shifts and mitigates outlier influence through Kullback-Leibler penalization.
---

# Distributionally Robust Federated Learning with Outlier Resilience

## Quick Facts
- **arXiv ID:** 2509.24462
- **Source URL:** https://arxiv.org/abs/2509.24462
- **Reference count:** 40
- **Primary result:** Proposes DOR-FL algorithm achieving O(T^{-1/2} + ε) convergence rate for outlier-robust federated learning using unbalanced Wasserstein DRO with KL penalization.

## Executive Summary
This paper addresses the critical challenge of outlier-robust federated learning in heterogeneous, distribution-shifted environments. The authors introduce a novel distributionally robust optimization framework that uses unbalanced Wasserstein distance to construct ambiguity sets that jointly capture geometric distributional shifts while mitigating outlier influence through Kullback-Leibler penalization. The resulting min-max-max optimization problem is reformulated as a tractable Lagrangian penalty optimization that enables decentralized training. Experiments demonstrate significant improvements over existing methods on both synthetic and real-world datasets, with DOR-FL achieving 95.4% accuracy on synthetic data compared to 61.5-77.1% for baselines, and 84.6% overall accuracy on UCI Adult Income dataset.

## Method Summary
The method extends federated learning to handle outliers through distributionally robust optimization using unbalanced Wasserstein distance. The key innovation is reformulating the hard constraint UW(P||P̂_λ) ≤ r as a soft penalization in the objective, enabling tractable decentralized optimization. Each client solves an inner maximization to find worst-case perturbations, computes gradient estimates, and sends updates to the server. The server aggregates using weighted averaging and projects onto the feasible set. The approach uses an outlier scoring function h(ξ) that downweights contaminated distributions in the worst-case supremum, providing task-dependent flexibility without requiring explicit outlier removal.

## Key Results
- **Synthetic data:** DOR-FL achieves 95.4% accuracy versus 61.5-77.1% for baselines (ERM, AFL, WAFL, GDRFL)
- **UCI Adult Income:** DOR-FL achieves 84.6% overall accuracy with 0.34 excess risk, outperforming ERM (83.1%, 0.37), AFL (81.7%, 0.44), WAFL (76.7%, 0.64), and GDRFL (79.5%, 0.61)
- **Fairness:** DOR-FL shows improved accuracy across demographic groups compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The unbalanced Wasserstein distance bounds the distance between true and contaminated distributions more tightly than standard Wasserstein, enabling smaller ambiguity sets.
- **Mechanism:** UW decomposes into Wasserstein distance between true distribution P* and clean distribution P̂_clean, plus KL penalization measuring mismatch between clean and contaminated data. Since KL measures relative differences only, D_KL(P̂_clean||P̂) remains small even with outliers, allowing radius r to stay small while still capturing distributions of interest.
- **Core assumption:** Outliers cause large transportation costs in standard Wasserstein distance, requiring overly conservative large radii to include the true distribution.
- **Evidence anchors:** Lemma 1 in Section 2.1 states UW(P*||P̂) ≤ W(P*, P̄) + βD_KL(P̄||P̂), with explanation that "a small radius r in (1) can effectively include the distributions of interest."

### Mechanism 2
- **Claim:** The Lagrangian penalty reformulation preserves robustness guarantees while enabling tractable decentralized optimization.
- **Mechanism:** The hard constraint UW(P||P̂_λ) ≤ r is replaced with soft penalization ρ·UW(P||P̂_λ) in the objective. The reformulation decomposes as a weighted sum of client-specific terms, each computing exp{sup_ξ[L(θ,ξ) - ρc(ζ,ξ)]/(ρβ)} locally. The induced ambiguity set radius emerges from the maximizer, and Proposition 2 certifies that solving the penalty problem guarantees robustness over this induced set.
- **Core assumption:** Strong duality holds for the Lagrangian formulation, and the exponential transformation preserves the optimization landscape.
- **Evidence anchors:** Proposition 1 in Section 3 shows equivalence between the penalty form and decomposed weighted sum across clients.

### Mechanism 3
- **Claim:** The outlier scoring function h(ξ) downweights contaminated distributions in the worst-case supremum without requiring explicit outlier removal.
- **Mechanism:** The modified loss L(θ,ξ) = l(θ,ξ) - h(ξ) means that for any distribution P containing outliers, E_ξ~P[h(ξ)] is large, making P less likely to achieve the supremum in the worst-case optimization. This provides task-dependent flexibility: what constitutes an outlier varies by client context.
- **Core assumption:** Domain knowledge or statistical diagnostics can specify h(ξ) such that it assigns larger values to genuinely harmful outliers while not penalizing valid rare cases needed for other clients.
- **Evidence anchors:** Section 2.2 defines h(ξ) as an outlier scoring function with application-dependent design, including the medical diagnosis example.

## Foundational Learning

- **Concept: Wasserstein Distance and Optimal Transport**
  - Why needed here: The core innovation uses unbalanced Wasserstein distance to construct ambiguity sets. Without understanding how Wasserstein distance measures distributional similarity via transportation costs, the mechanism for handling geometric shifts is opaque.
  - Quick check question: Can you explain why Wasserstein distance captures geometric shifts better than KL divergence, and what the marginal constraint Γ(P,P̂) represents?

- **Concept: Distributionally Robust Optimization (DRO)**
  - Why needed here: The entire framework builds on DRO's min-max formulation optimizing worst-case performance over ambiguity sets. Understanding the tradeoff between ambiguity set size (robustness) and conservatism is essential.
  - Quick check question: If you increase the ambiguity set radius r, what happens to model conservatism and why does this tradeoff matter for outlier-contaminated data?

- **Concept: Federated Averaging and Decentralized Optimization**
  - Why needed here: DOR-FL extends FedAvg-style algorithms to handle the min-max-max structure. The gradient estimation, local updates, and server aggregation steps all build on standard FL mechanics.
  - Quick check question: In Algorithm 1, why must each client solve the inner maximization (equation 8) locally before computing gradient estimates, and what information must be communicated to the server?

## Architecture Onboarding

- **Component map:** Server -> Client modules (N clients) -> Server aggregation -> Global parameters
- **Critical path:**
  1. Server broadcasts h(ξ) and current (θ_t, λ_t) to all clients (lines 1, 13)
  2. Each client samples ζ̂_i,t from local data and finds approximate maximizer z_i,t of L(θ_t, ξ) - ρc(ξ, ζ̂_i,t) (lines 4-5)
  3. Clients compute gradient estimates and perform local θ updates (lines 6, 8)
  4. Clients send (θ_i,t+1, g^λ_i,t) to server (line 9)
  5. Server aggregates and projects (lines 11-12)
  6. Repeat for T rounds until convergence

- **Design tradeoffs:**
  - **ρ (penalty coefficient):** Larger ρ relaxes robustness for better accuracy on clean data; smaller ρ increases robustness but risks over-conservatism.
  - **β (KL weight):** Controls outlier penalization strength. Too small: outliers still distort ambiguity set. Too large: over-penalizes legitimate distribution shifts.
  - **ε (inner maximization accuracy):** Theorem 1 shows convergence has O(ε) offset. More accurate inner solves improve final performance but increase per-round computation.
  - **h(ξ) design:** Requires domain knowledge. Overly aggressive penalization loses valuable rare cases; insufficient penalization fails to suppress outliers.

- **Failure signatures:**
  - Numerical overflow in exp{(L - ρc)/(ρβ)}: Occurs when ρβ is too small relative to loss scale. Fix: normalize losses or increase ρβ.
  - Divergence of λ (client weights): If one client's g^λ_i,t dominates, aggregation becomes degenerate. Fix: add regularization or cap λ updates.
  - No improvement over baseline: Likely h(ξ) misspecified or outliers don't cause large transport costs. Check: visualize transport costs for suspected outliers vs. normal samples.
  - Convergence stalls at suboptimal point: Inner maximization (equation 8) may be stuck at poor local optima if L(θ,ξ) not concave in ξ. Fix: use stronger convexity in c(ξ,ζ) or multiple restarts.

- **First 3 experiments:**
  1. **Synthetic validation:** Replicate the Gaussian mixture experiment (Section 5.1) with known contamination. Verify that: (a) standard Wasserstein DRO requires large radius to include true distribution, (b) UW-DRO achieves same inclusion with smaller radius, (c) classification accuracy degrades gracefully as h(ξ) prior mean offset increases.
  2. **Ablation on h(ξ) sensitivity:** On UCI Adult (Section 5.2), systematically vary the sigmoid softness parameter s and threshold D. Measure: (a) overall accuracy, (b) group-wise accuracy (Figure 4), (c) excess risk (Figure 5). Identify the regime where h(ξ) is too weak (no improvement) vs. too aggressive (over-penalizes valid cases).
  3. **Scalability test:** Simulate FL with N=10, 50, 100 clients varying local dataset sizes (100-1000 samples). Measure: (a) communication rounds to reach target accuracy, (b) per-round computation time dominated by inner maximization, (c) convergence rate matches O(T^(-1/2)) theoretical bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can communication-efficient variants of DOR-FL be developed to reduce transmission overhead in large-scale networks?
- Basis: [explicit] The conclusion explicitly states future work includes "developing communication-efficient variants of DOR-FL to further reduce transmission overhead in large-scale networks."
- Why unresolved: The current algorithm requires iterative transmission of model updates and gradient estimates between clients and the server, which may be cost-prohibitive for large-scale or bandwidth-constrained systems.
- Evidence to resolve: A modified algorithm demonstrating convergence with fewer communication rounds or utilizing gradient compression techniques while maintaining robustness.

### Open Question 2
- Question: How can the method be adapted to ensure scalability in high-dimensional applications?
- Basis: [explicit] The conclusion lists "exploring scalable algorithms for high-dimensional applications" as a future research direction.
- Why unresolved: Solving the inner maximization problem (finding $\xi$) may become computationally prohibitive as the dimensionality of the feature space increases, limiting applicability to complex data.
- Evidence to resolve: Theoretical analysis or empirical results showing tractable computation times on datasets with significantly higher dimensionality than the UCI Adult dataset used in the paper.

### Open Question 3
- Question: Do the convergence guarantees hold when Assumption 1 (convexity in $\theta$) is relaxed for non-convex models?
- Basis: [inferred] Theorem 1 relies on Assumption 1, and the experimental evaluation is restricted to convex logistic regression, whereas practical FL often involves non-convex deep neural networks.
- Why unresolved: It is unclear if the $O(T^{-1/2})$ convergence rate is preserved or if the min-max-max optimization becomes unstable when the loss function is non-convex.
- Evidence to resolve: A new convergence proof for non-convex settings or empirical validation of the method's stability and accuracy when training deep neural networks (e.g., CNNs on image data).

## Limitations
- The method relies heavily on appropriate specification of the outlier scoring function h(ξ) and hyperparameter tuning (ρ, β), which may be challenging in practice without strong domain knowledge.
- The O(T^{-1/2} + ε) convergence rate requires careful inner maximization accuracy ε that scales with problem complexity, potentially limiting practical efficiency.
- Computational overhead of solving the inner maximization problem at each iteration may limit scalability to large datasets or many clients.

## Confidence
- **High Confidence:** The mechanism connecting unbalanced Wasserstein distance to outlier resilience (Mechanism 1) is well-supported by the theoretical decomposition and experimental validation.
- **Medium Confidence:** The Lagrangian reformulation and convergence analysis (Mechanism 2) are sound, but practical implementation challenges around numerical stability remain underexplored.
- **Medium Confidence:** The outlier scoring mechanism (Mechanism 3) shows promise but requires careful domain-specific design that may not generalize across applications.

## Next Checks
1. **Robustness to h(ξ) misspecification:** Systematically test DOR-FL with intentionally flawed h(ξ) functions (e.g., penalizing valid rare cases) to quantify performance degradation and identify failure thresholds.
2. **Computational overhead characterization:** Measure per-iteration time complexity and memory requirements across varying client counts and dataset sizes to establish practical scalability limits.
3. **Convergence acceleration strategies:** Investigate whether warm-starting inner maximization or using momentum-based methods can improve the ε term in the O(T^{-1/2} + ε) rate without sacrificing robustness guarantees.