---
ver: rpa2
title: 'CauKer: classification time series foundation models can be pretrained on
  synthetic data only'
arxiv_id: '2508.02879'
source_url: https://arxiv.org/abs/2508.02879
tags:
- time
- series
- data
- synthetic
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CauKer, a synthetic data generation pipeline
  for pretraining time series foundation models (TSFMs) on classification tasks. CauKer
  combines Gaussian Process kernel composition with Structural Causal Models to generate
  diverse, causally coherent time series with realistic trends, seasonality, and nonlinear
  interactions.
---

# CauKer: classification time series foundation models can be pretrained on synthetic data only

## Quick Facts
- **arXiv ID**: 2508.02879
- **Source URL**: https://arxiv.org/abs/2508.02879
- **Reference count**: 32
- **Primary result**: TSFMs pretrained solely on CauKer synthetic data achieve SOTA zero-shot classification performance, matching real-world pretraining while using up to 20× less data

## Executive Summary
CauKer introduces a synthetic data generation pipeline that enables pretraining time series foundation models (TSFMs) for classification tasks without requiring large real-world datasets. The method combines Gaussian Process kernel composition with Structural Causal Models to generate diverse, causally coherent time series with realistic trends, seasonality, and nonlinear interactions. Experiments show that models pretrained on CauKer-generated data achieve state-of-the-art zero-shot classification performance, matching the accuracy of models trained on real-world datasets while using up to 20× less data.

## Method Summary
CauKer generates synthetic time series through a 5-step pipeline: (1) sample kernels from a bank of 36 GP kernels, (2) compose them via random addition/multiplication, (3) generate root nodes from GP priors with non-zero mean functions, (4) sample activations from a bank of 6 nonlinear functions, and (5) propagate through a random DAG where each edge applies an activation to aggregated parent signals. The generated data exhibits clear scaling laws for both dataset size (10K to 10M samples) and model capacity (1M to 783M parameters), unlike real-world datasets which display irregular scaling behavior.

## Key Results
- Models pretrained on CauKer achieve SOTA zero-shot classification on 128 UCR benchmarks, matching real-world pretraining performance
- CauKer exhibits clear scaling laws for dataset size (10K to 10M samples) and model capacity (1M to 783M parameters)
- Synthetic pretraining requires up to 20× less data than real-world pretraining while achieving equivalent performance
- The generated data produces hierarchical clustering structure with clear inter-cluster distances and identifiable anomalies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic pretraining data with explicit causal structure produces more predictable scaling behavior than real-world corpora
- **Mechanism**: CauKer generates time series through a DAG where each edge applies a nonlinear transformation to linear combinations of parent signals. This creates diverse yet structured dependencies that models can progressively learn as capacity increases.
- **Core assumption**: The scaling regularity stems from the synthetic data having a learnable generative structure without the distributional noise and domain mismatches present in aggregated real-world benchmarks.
- **Evidence anchors**:
  - [abstract] "CauKer-generated datasets exhibit clear scaling laws for both dataset size (10K to 10M samples) and model capacity (1M to 783M parameters), unlike real-world datasets, which display irregular scaling behavior"
  - [Page 7, Results] "accuracy rises steadily when the models are trained on CauKer; additional epochs translate into consistent gains for both architectures. When pretrained on UEA, however, accuracy curves remain flat or fluctuate"
  - [corpus] Related work on TSFM failures (arXiv:2511.05619) identifies spectral shift as a key generalization barrier—CauKer's kernel composition may implicitly cover broader spectral diversity
- **Break condition**: Scaling laws plateau when model capacity saturates on the synthetic distribution, or when the synthetic data's spectral range doesn't match downstream task requirements.

### Mechanism 2
- **Claim**: Combining GP kernel composition with SCM nonlinearity is necessary for classification-oriented synthetic data; neither component alone suffices
- **Mechanism**: Kernel composition generates temporally coherent base patterns. SCM propagation through random DAGs injects nonlinear interactions. Mean functions provide discriminative cues that zero-mean GP lacks. Together, these create clusterable structure.
- **Core assumption**: Classification requires both temporal motifs and discriminative inter-sample structure, whereas forecasting primarily needs temporal coherence.
- **Evidence anchors**:
  - [Page 5, Results] "classification-tailored tabular data generation pipeline SCM underperforms significantly compared to all other methods. This suggests that temporal dependencies are important for time series classification"
  - [Page 5, Results] Mean+KernelSynth (adding non-zero mean) improves over KernelSynth from 77.70→78.20 (Mantis) and 69.31→72.56 (MOMENT)
  - [corpus] No direct corpus comparison for GP+SCM hybrid; synthetic TSFM literature (arXiv:2503.11411) notes most work focuses on forecasting, not classification
- **Break condition**: Removing mean functions drops discriminative signal; removing SCM edges loses nonlinear cluster boundaries; performance degrades to forecasting-oriented baselines.

### Mechanism 3
- **Claim**: Higher training loss on CauKer data correlates with better OOD generalization due to enforced diversity
- **Mechanism**: CauKer's random sampling over kernel/mean/activation banks produces high-variance training data. Models can't memorize narrow patterns; instead, they learn disentangled representations that transfer. The DTW analysis shows clear cluster emergence without collapsed diversity.
- **Core assumption**: Training difficulty acts as implicit regularization, preventing overfitting to spurious correlations that wouldn't transfer to real benchmarks.
- **Evidence anchors**:
  - [Page 8, Figure 6] "Training loss and test accuracy... show that synthetic data is harder to train on, but leads to a smoother increase of the test accuracy across epochs"
  - [Page 6, Qualitative analysis] "hierarchical clustering... reveals clear clusters (large blocks of time series having similar intra-cluster distances) as well as... anomalies obtained using the anomaly mean function"
  - [corpus] Limited direct evidence; related TSFM work doesn't compare training dynamics on synthetic vs. real data
- **Break condition**: If synthetic diversity exceeds model capacity, training never converges; if activation bank is too limited, synthetic data becomes too easy and loses transfer benefits.

## Foundational Learning

- **Concept: Gaussian Process (GP) kernels**
  - **Why needed here**: CauKer samples base time series from GP priors defined by composed kernels. Without understanding that kernels encode covariance structure (RBF→smoothness, ExpSineSquared→periodicity, DotProduct→trends), the generation pipeline is opaque.
  - **Quick check question**: If you compose RBF and ExpSineSquared kernels multiplicatively, what temporal structure would you expect in sampled paths?

- **Concept: Structural Causal Models (SCMs)**
  - **Why needed here**: CauKer propagates root signals through a DAG where edges apply random nonlinearities. This creates hierarchical dependencies beyond simple additive patterns. Understanding SCMs clarifies why the data has disentangleable structure.
  - **Quick check question**: In a 3-node DAG with edges (A→B, B→C), if edge (A→B) uses sigmoid and (B→C) uses ReLU, how does the nonlinearity compound?

- **Concept: Self-supervised pretraining (contrastive vs. masked)**
  - **Why needed here**: The paper evaluates two TSFMs with different pretraining objectives. Mantis uses contrastive learning (distinguishing positive/negative pairs); MOMENT uses masked reconstruction. Results differ by model—understanding these paradigms explains why MOMENT benefits more from CauKer's diversity.
  - **Quick check question**: Would you expect a contrastive model to be more or less sensitive to synthetic data diversity than a masked-reconstruction model, and why?

## Architecture Onboarding

- **Component map**:
  1. **Kernel bank** (36 kernels with varied hyperparameters): defines GP covariance structure
  2. **Mean function bank** (4 types: zero, linear, exponential, sparse anomalies): provides discriminative baselines
  3. **Activation bank** (6 functions): applies edge-wise nonlinearities in SCM
  4. **DAG generator**: creates random causal graph topology
  5. **Composition engine**: samples kernels, applies random +/× operations, generates root nodes
  6. **Propagation layer**: linear aggregation + activation per node

- **Critical path**:
  1. Sample K kernels → compose → define M GP priors with random means
  2. Sample root time series from each GP prior
  3. Generate random DAG with E edges, assign activations to edges
  4. Propagate: for each non-root node, apply activation to each parent, aggregate via random linear layer
  5. Interpolate to fixed length (512)

- **Design tradeoffs**:
  - **Bank size vs. compute**: Larger kernel/activation banks increase diversity but slow sampling
  - **DAG depth vs. pattern complexity**: Deeper graphs create more complex interactions but risk gradient issues if used for training
  - **Series length (512) vs. model compatibility**: Fixed to match Mantis/MOMENT input; shorter lengths may lose spectral diversity
  - **Mean function variance vs. realism**: Aggressive anomalies create discriminative signal but may deviate from real-world stationarity

- **Failure signatures**:
  - **No scaling behavior**: DAG too shallow or activation bank too limited—synthetic data lacks complexity
  - **Training loss doesn't decrease**: Synthetic diversity exceeds model capacity; reduce bank sizes
  - **Poor UCR transfer**: Kernel bank missing relevant spectral patterns; expand periodicity range
  - **Collapsed embeddings**: Clustering structure too weak; increase mean function variance or DAG nonlinearity

- **First 3 experiments**:
  1. **Ablation on mean functions**: Generate CauKer-100K datasets with (a) zero mean only, (b) non-zero mean functions, (c) full mean bank. Pretrain Mantis-8M, evaluate on UCR. Expect: (b) > (a), (c) ≈ (b) with smoother scaling.
  2. **DAG depth sweep**: Fix 100K samples, vary DAG depth (2, 4, 6, 8 layers). Measure UCR accuracy and non-linearity scores. Expect monotonic improvement then plateau.
  3. **Spectral coverage audit**: Compute FFT of CauKer-100K vs. UEA-100K. Identify frequency bands missing from synthetic data. Add targeted kernels and re-evaluate MOMENT transfer performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does synthetic pretraining on CauKer-generated data yield similar performance benefits for Large Language Model (LLM)-based TSFMs or generative architectures, which were excluded from this study?
- **Basis in paper**: [explicit] The authors state in the Limitations section that adding more models "such as [Gao et al., 2024]" would be a necessary addition, as their study was restricted to Mantis and MOMENT to manage computational costs.
- **Why unresolved**: The computational intensity of pretraining prevented the authors from validating the pipeline across the full spectrum of existing TSFM architectures, leaving the generalizability to LLM-based backbones unknown.
- **What evidence would resolve it**: Empirical results showing scaling laws and zero-shot performance for LLM-based TSFMs pretrained solely on CauKer data.

### Open Question 2
- **Question**: Can the CauKer generation pipeline be effectively extended to handle multivariate time series classification without sacrificing the sample efficiency and scaling laws observed in univariate experiments?
- **Basis in paper**: [inferred] The experiments are restricted to univariate time series because the selected backbones (Mantis and MOMENT) take univariate inputs; however, the method description mentions multivariate capabilities (e.g., "root nodes generation" implies multiple channels potentially).
- **Why unresolved**: While the pipeline generates data via a DAG which could imply multivariate structures, the paper only validates performance on univariate benchmarks (UCR/UEA), leaving the efficacy for complex multivariate dependencies unproven.
- **What evidence would resolve it**: A study evaluating TSFMs pretrained on multivariate CauKer data against real-world multivariate benchmarks.

### Open Question 3
- **Question**: What are the theoretical mechanisms driving the sub-exponential scaling laws in classification TSFMs, and why do they diverge from the patterns seen in language or vision models?
- **Basis in paper**: [explicit] The authors observe "sub-exponential rather than clean exponential growth" in accuracy and explicitly state that "a more systematic, theory-driven study of such behavior is needed to fully understand its implications."
- **Why unresolved**: This work is empirical; it identifies the phenomenon (irregular scaling on real data vs. sub-exponential on synthetic) but does not provide a formal theoretical explanation for why time series classification dynamics differ from other modalities.
- **What evidence would resolve it**: A theoretical framework or ablation study linking specific properties of the synthetic data to the observed loss scaling curves.

## Limitations
- **Exact kernel specifications unknown**: The 36 GP kernel hyperparameters are not provided, only stated as "same as Chronos"
- **DAG structure details unspecified**: Total nodes, edge probability, and final time series selection method are not detailed
- **Model architecture restrictions**: Study limited to Mantis and MOMENT, excluding LLM-based TSFMs and other architectures

## Confidence
- **High confidence**: The core finding that CauKer enables zero-shot classification with SOTA performance and clear scaling laws is well-supported by multiple experiments and ablation studies
- **Medium confidence**: The claim that synthetic pretraining beats real-world pretraining is strong but relies on specific model architectures showing different benefits
- **Medium confidence**: The mechanism explaining scaling law differences (synthetic data having learnable structure vs. real-world noise) is plausible but not directly proven

## Next Checks
1. **Kernel bank sensitivity**: Systematically vary the kernel hyperparameter ranges and measure UCR transfer performance degradation to quantify how critical the exact Chronos configuration is
2. **DAG complexity sweep**: Fix synthetic dataset size and vary DAG depth/width, measuring both UCR accuracy and non-linearity scores to validate the mechanism linking structural complexity to performance
3. **Spectral coverage audit**: Compute FFT of CauKer vs. UCR datasets to identify frequency gaps, then target missing bands with additional kernels and measure MOMENT transfer improvement