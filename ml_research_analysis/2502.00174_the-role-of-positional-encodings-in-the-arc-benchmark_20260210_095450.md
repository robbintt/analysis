---
ver: rpa2
title: The role of positional encodings in the ARC benchmark
arxiv_id: '2502.00174'
source_url: https://arxiv.org/abs/2502.00174
tags:
- encoding
- positional
- examples
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different positional encoding strategies
  affect transformer model performance on the Abstraction and Reasoning Corpus (ARC)
  benchmark, which tests abstract reasoning with minimal training data. Using custom
  transformer models trained on 100,000 examples across 10 ARC tasks, the research
  compares various positional encoding approaches including 1D sinusoidal encoding,
  2D sinusoidal encoding, Rotary Position Embedding (RoPE), and Learned Embeddings.
---

# The role of positional encodings in the ARC benchmark

## Quick Facts
- arXiv ID: 2502.00174
- Source URL: https://arxiv.org/abs/2502.00174
- Reference count: 10
- Primary result: 2D positional encoding consistently outperforms other methods in data-constrained scenarios for ARC tasks

## Executive Summary
This study investigates how different positional encoding strategies affect transformer model performance on the Abstraction and Reasoning Corpus (ARC) benchmark. Using custom transformer models trained on 100,000 examples across 10 ARC tasks, the research compares various positional encoding approaches including 1D sinusoidal encoding, 2D sinusoidal encoding, Rotary Position Embedding (RoPE), and Learned Embeddings. Results demonstrate that 2D positional encoding consistently outperforms other methods in data-constrained scenarios, with particular effectiveness for ARC tasks. The study also reveals that CodeT5+ struggles with vertical examples due to its relative positional encoding mechanism, highlighting how encoding design impacts reasoning capabilities.

## Method Summary
The study uses custom transformer models trained on 100,000 examples generated from 10 specific ARC tasks. Four positional encoding strategies are compared: 1D sinusoidal, 2D sinusoidal (separate x/y encodings), RoPE, and learned embeddings. Models are evaluated across different architectures (small, medium, large, decoder-only) and input tokenization formats (raw and bracketed). Exact match accuracy is used as the evaluation metric. The experimental design systematically varies model size, positional encoding type, and tokenization strategy to assess their impact on ARC task performance.

## Key Results
- 2D positional encoding consistently outperforms 1D sinusoidal, RoPE, and learned embeddings in data-constrained scenarios across all model architectures
- CodeT5+ shows significant performance asymmetry between horizontal and vertical examples due to its relative positional encoding mechanism
- 2D encoding enables competitive performance with smaller architectures, achieving ~90% accuracy even in decoder-only setups
- In high-data settings (>100k examples), RoPE shows slight performance advantage over 2D encoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2D positional encoding improves spatial reasoning in data-constrained scenarios
- Mechanism: Independent encoding of horizontal (x) and vertical (y) positions allows the model to capture 2D spatial relationships directly at the positional encoding level, rather than relying on the model to infer spatial structure from sequential token ordering.
- Core assumption: ARC tasks require preserving explicit grid-based spatial relationships that 1D sequential flattening may obscure.
- Evidence anchors: 2D encoding excels in data-constrained scenarios; 2D positional encoding demonstrates clear advantage over 1D encoding irrespective of model size or architectural configuration.

### Mechanism 2
- Claim: Relative positional encoding creates distance-based attention biases that impair vertical spatial reasoning
- Mechanism: CodeT20+ uses relative PE which prioritizes tokens closer in sequence. When grids are flattened row-by-row, vertically adjacent pixels become far apart in the token sequence, making cross-row relationships harder to learn than within-row relationships.
- Core assumption: Token sequence order reflects row-major flattening of 2D grids.
- Evidence anchors: Model performs considerably better on horizontal examples; goal tokens are closer in horizontal vs farther apart in vertical arrangements.

### Mechanism 3
- Claim: 2D encoding enables competitive performance with smaller architectures
- Mechanism: Explicit spatial encoding reduces the representational burden on the model, allowing fewer layers/parameters to achieve comparable performance to larger models with weaker positional encoding.
- Core assumption: Spatial reasoning capacity can be offloaded from learned weights to architectural inductive biases.
- Evidence anchors: 2D encoding achieves competitive performance against larger models even with single transformer layer; 2D encoding performs remarkably well in decoder-only architecture achieving around 90% accuracy.

## Foundational Learning

- Concept: **Sinusoidal vs Learned Positional Encodings**
  - Why needed here: The paper compares fixed sinusoidal encodings (1D, 2D) with trainable learned embeddings; understanding this distinction is essential for interpreting results.
  - Quick check question: Can you explain why fixed sinusoidal encodings generalize to longer sequences than learned embeddings?

- Concept: **Row-major vs Column-major Grid Flattening**
  - Why needed here: The CodeT20+ experiment hinges on how 2D grids become 1D sequences; the choice determines which spatial relationships become "local" in token space.
  - Quick check question: Given a 5x5 grid, which pixels are closest to position (2,2) after row-major flattening?

- Concept: **Encoder-Decoder vs Decoder-Only Architectures**
  - Why needed here: The paper shows 2D PE works well in decoder-only setups; understanding what the encoder provides (full grid visibility) clarifies why this result is notable.
  - Quick check question: In a decoder-only model processing a grid, what information is available at each generation step compared to an encoder-decoder model?

## Architecture Onboarding

- Component map: Input layer (Grid → tokenization → token embeddings + positional encodings) → Positional encoding options (1D sinusoidal, 2D sinusoidal, RoPE, Learned embeddings) → Transformer backbone (Encoder + Decoder with self-attention and cross-attention) → Output layer (Token prediction over vocabulary)

- Critical path: Tokenize grid to sequence (raw or bracketed) → Apply positional encoding → Pass through transformer layers → Generate output grid token-by-token

- Design tradeoffs: Raw vs bracketed tokenization (bracketed adds explicit row boundaries but increases sequence length); 2D vs RoPE (2D better for limited data; RoPE slightly better with 100k+ examples); Encoder-decoder vs decoder-only (former sees full grid; latter more memory-efficient but 2D PE still achieves ~90%)

- Failure signatures: Horizontal/vertical performance asymmetry → suggests distance-biased PE; poor generalization to unseen grid sizes → learned embeddings may not extrapolate; decoder-only architecture underperforms significantly → PE not compensating for lack of encoder

- First 3 experiments:
  1. Reproduce CodeT20+ horizontal/vertical asymmetry: Train on rotated versions of same task; if performance differs significantly, PE is creating spatial bias.
  2. Compare 1D vs 2D PE on single-layer transformer: Use Smaller configuration (1 layer, 4 heads); expect 2D to approach larger model performance.
  3. Sample efficiency test: Train Medium model with 10k examples comparing 2D, RoPE, and Learned; expect 2D to outperform.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of 2D positional encoding persist when models must perform relational reasoning across groups of related examples rather than independent tasks?
- Basis in paper: The authors explicitly identify this as a limitation, stating they treat ARC examples as independent, overlooking the reasoning often required between groups of related examples.
- Why unresolved: Current experimental design evaluates tasks in isolation, failing to capture few-shot learning dynamics required to infer rules from multiple demonstration pairs simultaneously.
- What evidence would resolve it: A study evaluating positional encoding strategies on tasks where the model must process a set of demonstration pairs before predicting the test output.

### Open Question 2
- Question: Can effective 2D spatial reasoning be instilled in pre-trained Large Language Models (LLMs) by retrofitting positional encodings, or is training from scratch necessary?
- Basis in paper: Authors note that modifying CodeT20+ "posed significant challenges" due to its deeply integrated mechanism, leading them to train custom models from scratch.
- Why unresolved: Unclear if poor performance of standard LLMs on ARC is inherent limitation of their architecture or simply result of inflexible pre-trained positional representations.
- What evidence would resolve it: A successful methodology for fine-tuning or modifying the positional layers of a frozen/pre-trained LLM to utilize 2D encoding without catastrophic forgetting.

### Open Question 3
- Question: How does the trade-off between 2D encoding and RoPE change as the diversity of tasks increases beyond the 10 specific tasks tested?
- Basis in paper: Conclusion suggests future work should include expanding analysis to include broader range of tasks.
- Why unresolved: Study restricted to 10 specific tasks; uncertain if 2D encoding's dominance in data-constrained scenarios holds universally across full complexity and variety of ARC dataset.
- What evidence would resolve it: A comprehensive benchmark across complete ARC training set or wider synthetic dataset, comparing sample efficiency of 2D encoding versus RoPE.

## Limitations
- The study's core claims rest on controlled experiments with synthetic data generation, but generator implementation details are not fully specified
- Comparison focuses on limited set of positional encoding strategies without exploring more recent approaches like ALiBi or complex-valued embeddings
- Decoder-only architecture results showing 90% accuracy may not fully account for trade-off between memory efficiency and modeling capacity

## Confidence

- **High Confidence:** Experimental methodology is sound with clear ablation studies and controlled comparisons across multiple architectures and tasks; horizontal/vertical performance asymmetry in CodeT20+ is directly observable and well-documented.
- **Medium Confidence:** Superiority of 2D positional encoding in data-constrained scenarios is demonstrated within experimental setup but may not generalize to all ARC-like tasks or more complex reasoning domains.
- **Low Confidence:** Claim that 2D encoding enables competitive performance with smaller architectures is based on limited architectural comparisons and may not hold for more complex tasks requiring deeper reasoning chains.

## Next Checks
1. Test 2D positional encoding on a held-out ARC task not used in training to verify generalization beyond the 10 specific tasks studied.
2. Compare 2D encoding against ALiBi or other relative positional encoding methods to establish whether the advantage is specific to the 2D approach or relative encoding generally.
3. Evaluate performance degradation when reducing training data below 10,000 examples to identify the true data threshold where 2D encoding becomes essential.