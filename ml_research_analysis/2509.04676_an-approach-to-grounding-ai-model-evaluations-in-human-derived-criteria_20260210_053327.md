---
ver: rpa2
title: An Approach to Grounding AI Model Evaluations in Human-derived Criteria
arxiv_id: '2509.04676'
source_url: https://arxiv.org/abs/2509.04676
tags:
- benchmarks
- skills
- https
- test
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a human-derived evaluation framework to augment
  traditional AI benchmarks, addressing their limitations in interpretability and
  real-world applicability. The study focuses on physical world modeling, using interviews
  and surveys to identify key cognitive skills like Prioritization, Memorizing, Discerning,
  and Contextualizing.
---

# An Approach to Grounding AI Model Evaluations in Human-derived Criteria

## Quick Facts
- arXiv ID: 2509.04676
- Source URL: https://arxiv.org/abs/2509.04676
- Authors: Sasha Mitts
- Reference count: 0
- Primary result: Human-derived evaluation framework identifies key cognitive skills (Prioritization, Memorizing, Discerning, Contextualizing) for AI model evaluation

## Executive Summary
This paper proposes augmenting traditional AI benchmarks with human-derived evaluation criteria to address limitations in interpretability and real-world applicability. The study focuses on physical world modeling, using interviews and surveys to identify cognitive skills critical for both human and AI reasoning. Participants rated these skills as essential, with high expectations for AI performance. The findings offer actionable guidelines for researchers to enhance AI model evaluations by integrating skills that reflect user expectations and real-world needs, aiming to create more interpretable and reliable benchmarks.

## Method Summary
The study employs a two-phase approach: first, in-depth interviews with 8 participants solving benchmark tasks while verbalizing their reasoning process; second, a large-scale survey with 1,515 participants rating the importance of identified cognitive skills. The methodology extracts skills from interview transcripts through qualitative coding, then validates their importance through quantitative survey ratings using Top-Two-Box scoring. The framework was developed using two benchmark datasets: Perception Test (6 videos) and OpenEQA (3 videos), focusing on tasks requiring physical world modeling and reasoning.

## Key Results
- Participants identified four critical cognitive skills: Prioritization (76% T2B), Memorizing (77% T2B), Discerning (83% T2B), and Contextualizing (80% T2B)
- Survey respondents showed high expectations for AI performance on these skills, indicating strong user interest in human-like reasoning capabilities
- The approach provides actionable guidelines for integrating human-derived criteria into benchmark design, improving alignment with real-world task requirements

## Why This Works (Mechanism)
The framework works by grounding abstract benchmark scores in concrete human reasoning processes. By decomposing how people solve physical world tasks into specific cognitive skills, the approach creates interpretable evaluation axes that connect technical performance to human cognitive capabilities. This translation from human introspection to evaluation criteria addresses the interpretability gap where traditional benchmarks show score improvements without explaining what cognitive capabilities actually improved.

## Foundational Learning

- **Concept: Benchmark Saturation and Interpretability Gap**
  - **Why needed here:** Traditional benchmarks often show score improvements without clear insight into what improved or whether performance generalizes. This paper's entire motivation is to address this limitation by adding human-derived criteria.
  - **Quick check question:** If a model's score on a benchmark increases from 80% to 85%, can you explain specifically what cognitive capability improved and how it relates to real-world tasks?

- **Concept: Introspective User Research Methods**
  - **Why needed here:** The core methodology relies on recruiting users capable of articulating their reasoning process ("introspective ability") and then coding those responses to identify skills. Understanding qualitative-to-quantitative translation is key.
  - **Quick check question:** How would you design an interview protocol to extract the reasoning steps a person uses to solve a visual puzzle, rather than just whether they got it right?

- **Concept: Top-Two Box (T2B) Scoring in Surveys**
  - **Why needed here:** The results quantify skill importance using percentages like "76% (T2B)." Understanding this common survey metric is necessary to interpret the paper's findings on which skills are most critical.
  - **Quick check question:** In a 5-point Likert scale from "Not at all important" to "Extremely important," which responses count towards the "Top-Two Box" score?

## Architecture Onboarding

- **Component map:** Benchmarks (Perception Test, OpenEQA) -> User Research Module (Qualitative interviews n=8 + Quantitative survey n=1,515) -> Skills Framework (Prioritization