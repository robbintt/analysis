---
ver: rpa2
title: 'ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus
  for LLM World Modeling and Evaluation'
arxiv_id: '2509.23979'
source_url: https://arxiv.org/abs/2509.23979
tags:
- code
- game
- arxiv
- games
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ByteSized32Refactored, a modular and extensible
  refactored version of the original ByteSized32 text game corpus, designed to evaluate
  Large Language Models' (LLMs) world modeling capabilities. The authors optimize
  the code structure by creating a centralized GameBasic.py library that abstracts
  seven base classes (e.g., GameObject, Container, World, Agent, TextGame) into reusable
  modules, reducing total lines of Python code from 20k to 10k.
---

# ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation

## Quick Facts
- arXiv ID: 2509.23979
- Source URL: https://arxiv.org/abs/2509.23979
- Reference count: 18
- Authors: Haonan Wang; Junfeng Sun; Xingdi Yuan; Ruoyao Wang; Ziang Xiao
- Primary result: Refactored ByteSized32 corpus reduces Python code from 20k to 10k lines while enabling more sophisticated game generation with mixed quality outcomes

## Executive Summary
This paper introduces ByteSized32Refactored, a modular and extensible refactored version of the original ByteSized32 text game corpus designed to evaluate Large Language Models' world modeling capabilities. The authors optimize the code structure by creating a centralized GameBasic.py library that abstracts seven base classes (GameObject, Container, World, Agent, TextGame) into reusable modules, reducing total lines of Python code from 20k to 10k. Extensive experiments with GPT-4o demonstrate that while the refactored code presents new challenges for LLMs due to its hierarchical structure, it enables generation of games that better comply with specifications and maintain winnability without sacrificing physical reality alignment under reflection.

## Method Summary
The authors refactor the original ByteSized32 text game corpus by creating a modular GameBasic.py library that centralizes seven base classes including GameObject, Container, World, Agent, and TextGame. This abstraction reduces the total Python codebase from 20,000 to 10,000 lines while maintaining extensibility. The refactored system uses JSON files to store game data and implements a hierarchical structure where objects inherit properties and behaviors from parent classes. The evaluation uses GPT-4o to generate text games from natural language descriptions, which are then assessed across four dimensions: specification compliance, winnability, technical validity, and physical reality alignment. The corpus includes a diverse set of game types including action, simulation, adventure, and puzzle games to test comprehensive world modeling capabilities.

## Key Results
- GPT-4o achieved a 62% success rate in generating text games from natural language prompts
- Generated games showed quality improvements on two of four evaluation dimensions: specification compliance and winnability
- Games showed decreased performance on technical validity and physical reality alignment, indicating hierarchical structure presents new challenges for LLMs
- Refactored code reduced Python lines from 20k to 10k while maintaining extensibility and game diversity

## Why This Works (Mechanism)
The refactored architecture works by abstracting complex game logic into reusable base classes that capture fundamental game elements and their relationships. The GameBasic.py library provides a hierarchical inheritance structure where specific game objects can inherit properties from general parent classes, reducing code duplication while enabling sophisticated game behaviors. This modular design allows LLMs to generate games that better adhere to specifications by providing clear interfaces and constraints, though the increased abstraction level requires more sophisticated reasoning to maintain technical validity and physical consistency across game states.

## Foundational Learning
- **GameObject hierarchy**: Understanding how game entities inherit properties from parent classes (why needed: enables code reuse and consistent behavior patterns; quick check: verify inheritance relationships in GameBasic.py)
- **Container relationships**: How objects can contain other objects and manage state transitions (why needed: critical for inventory and environment interactions; quick check: test container add/remove operations)
- **World state management**: Tracking game state across multiple objects and interactions (why needed: ensures consistency in generated games; quick check: verify state persistence across game turns)
- **Agent behavior modeling**: Implementing intelligent actors that respond to game events (why needed: enables dynamic gameplay and challenge; quick check: test agent decision-making in various scenarios)
- **TextGame interface**: Standardized methods for game initialization, execution, and termination (why needed: provides consistent API for LLM generation; quick check: verify game lifecycle methods work correctly)
- **JSON data serialization**: Storing and loading game configurations and state (why needed: enables persistence and sharing of game designs; quick check: test JSON save/load functionality)

## Architecture Onboarding

**Component map**: GameBasic.py (central library) -> Base Classes (GameObject, Container, World, Agent, TextGame) -> Specific Game Implementations -> JSON Configuration Files -> LLM Generation Pipeline

**Critical path**: LLM prompt -> Game generation -> JSON configuration creation -> Game execution -> State evaluation -> Quality assessment

**Design tradeoffs**: Hierarchical abstraction reduces code duplication but increases reasoning complexity for LLMs; modular design enables extensibility but requires careful interface design to maintain consistency across game types.

**Failure signatures**: 
- Specification violations indicate LLM misunderstanding of interface requirements
- Winnability failures suggest incorrect game state transitions or missing win conditions
- Technical validity issues reveal improper use of inheritance hierarchies
- Physical reality misalignment shows insufficient reasoning about object interactions

**First experiments**:
1. Generate a simple puzzle game and verify all win conditions can be achieved
2. Test inheritance by creating a derived GameObject and verifying property inheritance
3. Evaluate a simulation game's state consistency across multiple turns

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on GPT-4o, limiting generalizability to other LLM architectures
- Quality assessment relies on GPT-4o as both generator and evaluator, potentially introducing bias
- Physical reality alignment claims are supported only by limited experiments with GPT-4o
- Trade-off between hierarchical abstraction and reasoning difficulty is demonstrated but not fully characterized
- Paper doesn't address potential security implications of generating interactive text games from natural language prompts

## Confidence
- Performance generalizability to other LLM architectures: Low confidence
- Quality assessment without bias from using GPT-4o for both generation and evaluation: Medium confidence
- Physical reality alignment maintenance across different game types: Medium confidence
- Whether reasoning difficulties are fundamental to hierarchical structure or design artifacts: Medium confidence
- Security implications assessment: High confidence in significance, Low confidence in assessment

## Next Checks
1. Replicate the evaluation with diverse LLM architectures (including open-weight models) to assess generalizability of the reported performance patterns
2. Implement cross-validation where multiple independent evaluators assess game quality to address potential bias from using GPT-4o for both generation and evaluation
3. Conduct ablation studies on different abstraction levels to determine whether the observed reasoning difficulties stem from the hierarchical structure itself or specific design choices in the refactored codebase