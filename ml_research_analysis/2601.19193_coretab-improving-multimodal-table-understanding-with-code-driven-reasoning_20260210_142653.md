---
ver: rpa2
title: 'CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning'
arxiv_id: '2601.19193'
source_url: https://arxiv.org/abs/2601.19193
tags:
- table
- answer
- reasoning
- code
- coretab
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoReTab, a code-driven reasoning framework
  for multimodal table understanding that combines natural-language multi-step reasoning
  with executable Python code. Using CoReTab, the authors curate a dataset of 115K
  verified samples and fine-tune Qwen2.5-VL through a three-stage training pipeline.
---

# CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning

## Quick Facts
- **arXiv ID**: 2601.19193
- **Source URL**: https://arxiv.org/abs/2601.19193
- **Reference count**: 24
- **Primary result**: CoReTab achieves +6.2% on table question answering, +5.7% on table fact verification, and +25.6% on table structure understanding across 17 benchmarks

## Executive Summary
This paper introduces CoReTab, a code-driven reasoning framework for multimodal table understanding that combines natural-language multi-step reasoning with executable Python code. The authors curate a dataset of 115K verified samples and fine-tune Qwen2.5-VL through a three-stage training pipeline. The resulting model consistently outperforms MMTab-trained baselines across 17 benchmarks, achieving absolute improvements of +6.2% on table question answering, +5.7% on table fact verification, and +25.6% on table structure understanding. The approach enhances both accuracy and interpretability by producing transparent, verifiable reasoning traces.

## Method Summary
CoReTab uses a three-stage LoRA fine-tuning pipeline on Qwen2.5-VL 7B. First, the model undergoes table recognition pretraining on MMTab-pre (150K samples). Second, it performs instruction tuning on the CoReTab dataset (115K samples from 11 tasks). Third, GRPO reinforcement learning optimizes the reasoning capabilities. The framework generates code from tables and questions using a teacher model, executes it to verify correctness, and trains the student model to replicate this reasoning pattern. LoRA rank=8, alpha=32 is applied to all LLM linear layers with specific learning rates for each stage.

## Key Results
- **TQA**: +6.2% absolute improvement over MMTab-trained baselines
- **TFV**: +5.7% absolute improvement on table fact verification tasks
- **TSU**: +25.6% absolute improvement on table structure understanding tasks
- Consistent outperformance across 17 diverse benchmarks covering 3 categories: Table QA, Fact Verification, and Structure Understanding

## Why This Works (Mechanism)
CoReTab works by combining the interpretability of code execution with the reasoning capabilities of large language models. The framework generates executable Python code from multimodal inputs, which can be verified against ground truth answers. This creates a training signal that encourages precise, logical reasoning rather than pattern matching. The three-stage training pipeline gradually builds capabilities from basic table recognition to complex multi-step reasoning, while the GRPO optimization fine-tunes the reasoning process. The code generation provides transparency and verifiability, making errors easier to diagnose and correct.

## Foundational Learning
- **LoRA fine-tuning**: Why needed - Reduces computational cost while maintaining performance during multi-stage training. Quick check - Verify rank=8, alpha=32 settings produce stable training curves.
- **GRPO reinforcement learning**: Why needed - Optimizes reasoning quality beyond supervised learning. Quick check - Confirm temperature=0.5 and beta=0.05 settings produce reasonable exploration.
- **Multimodal table understanding**: Why needed - Tables contain both visual structure and semantic content requiring coordinated processing. Quick check - Test on varied table layouts from different domains.
- **Code generation verification**: Why needed - Provides ground truth training signal through executable reasoning. Quick check - Validate numerical comparison tolerance and string normalization.
- **Three-stage training pipeline**: Why needed - Gradually builds capabilities from recognition to reasoning. Quick check - Monitor performance progression across stages.

## Architecture Onboarding
- **Component map**: Table image -> Vision encoder -> Feature extraction -> Code generation -> Python execution -> Answer verification -> Training loss
- **Critical path**: Image → Code → Execution → Answer (verifiable reasoning chain)
- **Design tradeoffs**: Interpretability vs. inference speed (code generation overhead), dataset size vs. annotation cost (synthetic data generation), model complexity vs. computational efficiency (LoRA vs full fine-tuning)
- **Failure signatures**: 72% of TQA errors from cell extraction failures on large/dense tables, code execution mismatches due to floating-point comparison, long inference times from code generation
- **First experiments**:
  1. Validate code generation and execution pipeline on simple tables with known answers
  2. Test LoRA fine-tuning stability with rank=8, alpha=32 on small subset of CoReTab
  3. Verify numerical comparison tolerance works correctly for floating-point answers

## Open Questions the Paper Calls Out
None

## Limitations
- **Computational requirements**: Three-stage pipeline requires substantial resources (18-24 GPU-days with eight A100-80G GPUs)
- **Implementation details**: Critical parameters underspecified (prompt templates, code verification logic, tool function implementations)
- **Generalizability**: Performance on domains outside the 11 source tasks is unclear
- **OCR limitations**: 72% of TQA errors stem from cell extraction failures on large/dense tables

## Confidence
- **High Confidence**: Core methodology of code-driven reasoning is well-established and benchmark improvements are statistically significant
- **Medium Confidence**: Absolute performance numbers depend on specific implementation choices not fully detailed
- **Low Confidence**: Generalizability to new domains without additional training data is unclear

## Next Checks
1. **Code Verification Logic**: Implement and test exact numerical comparison tolerance and string normalization procedures to ensure reproducible code execution results
2. **Tool Function Implementation**: Implement complete table manipulation functions and validate on diverse table layouts, focusing on dense tables where 72% of errors occur
3. **Computational Resource Assessment**: Conduct detailed cost analysis of three-stage training pipeline to provide realistic resource requirements for replication attempts