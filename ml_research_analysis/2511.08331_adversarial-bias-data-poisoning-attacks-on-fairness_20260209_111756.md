---
ver: rpa2
title: 'Adversarial Bias: Data Poisoning Attacks on Fairness'
arxiv_id: '2511.08331'
source_url: https://arxiv.org/abs/2511.08331
tags:
- fairness
- samples
- dataset
- error
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of fairness vulnerability in machine
  learning systems through targeted data poisoning attacks. The authors propose a
  novel method called Proportional Fairness Attack (PFA) that strategically injects
  adversarial samples into training data to maximize fairness disparity while preserving
  overall performance.
---

# Adversarial Bias: Data Poisoning Attacks on Fairness

## Quick Facts
- arXiv ID: 2511.08331
- Source URL: https://arxiv.org/abs/2511.08331
- Authors: Eunice Chan; Hanghang Tong
- Reference count: 40
- Primary result: Novel PFA method achieves maximum EOD=1 in 33% of scenarios, outperforming baseline attacks

## Executive Summary
This work introduces the Proportional Fairness Attack (PFA), a data poisoning method that strategically injects adversarial samples to maximize fairness disparity while preserving overall model performance. The attack leverages a surrogate model to identify influential training samples that, when poisoned, skew decision boundaries against protected groups. PFA demonstrates significant effectiveness across multiple datasets and model architectures, achieving maximum equalized odds difference (EOD) of 1 in 33% of tested scenarios compared to 0% for baseline methods.

## Method Summary
PFA operates by iteratively building a poisoned dataset using a surrogate model's predictions to identify samples from protected groups where predictions contradict desired unfair behavior. The attack dynamically selects which protected group to poison based on Continuous Disparity Margin (CDM), samples from the identified subset, and creates poisoned samples with labels matching the sensitive attribute. Candidates are scored using a weighted combination of fairness degradation, performance preservation, and convergence metrics, with the best-scoring dataset selected as the final poisoned training set.

## Key Results
- Maximum EOD=1 achieved in 33% of tested scenarios versus 0% for baseline methods
- Attack effectiveness maintained across diverse model architectures (GNB, LR, DT, KNN)
- Competitive accuracy preservation with EOD values reaching 0.453 on German dataset
- Surrogate-guided poisoning outperforms random and density-based baseline attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surrogate model identifies influential samples whose mislabeling biases decision boundaries against protected groups
- Mechanism: PFA trains surrogate, identifies samples where Ŷ ≠ S, re-injects with label Y = S, increasing P[S=s|Y=s] while decreasing P[S=s|Y≠s]
- Core assumption: Surrogate's prediction errors indicate samples that reduce feature-label correlation fastest
- Evidence anchors: Abstract describes strategic injection of adversarial data points; Section IV-B details subset D_{S=s, Ŷ=1-s} selection
- Break condition: Surrogate-target model mismatch renders identified samples non-influential

### Mechanism 2
- Claim: Feature-label independence in poisoned samples maximizes group posterior divergence
- Mechanism: Theorem 1 proves poisoned samples with uniform features make score ratio = (prior ratio) × (group posterior ratio) × (feature likelihood ratio)
- Core assumption: Naive Bayes theoretical guarantee extends to other models learning from same correlation structure
- Evidence anchors: Section IV-B proves existence of poisoned samples guaranteeing maximally unfair behavior
- Break condition: Non-naive Bayes models using feature interactions cannot transfer theoretical guarantee

### Mechanism 3
- Claim: Dynamic group selection via CDM accelerates fairness degradation over fixed-group strategies
- Mechanism: CDM = |M_1 - M_0| approximates boundary proximity to flipping; poisoning higher-CDM group pushes both toward maximum disparity
- Core assumption: Margin-based approximation valid even with non-independent features
- Evidence anchors: Section IV-C describes CDM-based group selection; Figure 2 shows consistent superiority
- Break condition: Similar CDM values make selection essentially random with no advantage

## Foundational Learning

- Concept: **Naive Bayes decision rule and posterior decomposition**
  - Why needed here: Theoretical proof relies on decomposing classification score into priors, group posteriors, and feature likelihoods
  - Quick check question: Given P[Y=1]=0.5, P[S=1|Y=1]=0.8, and P[S=1|Y=0]=0.2, which class would a naive Bayes classifier predict for a sample with S=1?

- Concept: **Equalized Odds Difference (EOD) and Statistical Parity Difference (SPD)**
  - Why needed here: Target metrics for attack; EOD captures TPR/FPR disparity, SPD captures outcome rate disparity
  - Quick check question: If a classifier predicts Ŷ=S for all samples, what is the EOD value?

- Concept: **Surrogate model transferability in adversarial ML**
  - Why needed here: PFA uses surrogate to guide poisoning but attacks potentially different target model
  - Quick check question: Why might a decision tree surrogate provide poor guidance for poisoning a KNN classifier?

## Architecture Onboarding

- Component map: Clean dataset D → surrogate model training → prediction on D → CDM computation → group selection → sample poisoning → candidate generation → scoring → final poisoned dataset selection

- Critical path: Clean dataset D → surrogate model training → prediction on D → CDM computation → group selection → sample poisoning → candidate generation → scoring → final poisoned dataset selection

- Design tradeoffs:
  - Real samples vs. uniform features: Real samples more efficient but detectable; uniform features theoretically cleaner but require more samples
  - α, β parameters: Low values prioritize fairness degradation; high values protect accuracy (paper uses α=β=0.5 default)
  - Number of candidates N: More candidates improve results but increase computation linearly

- Failure signatures:
  - EOD/SPD plateauing below 1.0: Surrogate model mismatch; try different surrogate architecture
  - Accuracy drops precipitously: α, β too low; increase to prioritize performance preservation
  - D_s = ∅ error: No samples satisfy selection criterion; fall back to uniform generation
  - Attack works on LR/GNB but not DT/KNN: Non-differentiable models may require surrogate tuning

- First 3 experiments:
  1. Baseline validation: Run PFA on German dataset with GNB, ε=1.0, α=β=0.5. Verify EOD approaches values in Table II (~0.453).
  2. Ablation on sample selection: Compare PFA-Ŷ vs PFA-Y per Figure 4. Expect Ŷ variant to achieve higher EOD at higher ε values.
  3. Cross-model transferability test: Train surrogate as logistic regression, attack decision tree target. Compare EOD degradation to same-model case.

## Open Questions the Paper Calls Out

- Can PFA maintain efficacy against fairness-enhancing interventions (pre-processing, in-processing, post-processing)?
- Can theoretical guarantee of maximal unfairness be extended to model families beyond Naive Bayes?
- How robust is the attack when there's surrogate-target model mismatch?

## Limitations
- Theoretical guarantees proven only for naive Bayes classifiers, not other model architectures
- Reliance on surrogate model predictions introduces uncertainty with poor model matching
- CDM-based group allocation lacks strong theoretical grounding for generalization

## Confidence
- **High**: Attack successfully degrades fairness metrics in controlled experiments across multiple datasets
- **Medium**: Theoretical analysis for naive Bayes is sound but generalization unproven
- **Medium**: Empirical superiority over baselines demonstrated, though implementations may affect comparisons

## Next Checks
1. **Surrogate Transferability Test**: Vary surrogate architecture relative to target model and measure EOD degradation rates
2. **CDM Ablation Study**: Fix group allocation to one protected group and compare EOD trajectories against dynamic CDM approach
3. **Convergence Analysis**: Measure how quickly EOD approaches maximum values as poisoning proportion ε increases