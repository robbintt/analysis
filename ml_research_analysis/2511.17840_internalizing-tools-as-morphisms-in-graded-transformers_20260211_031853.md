---
ver: rpa2
title: Internalizing Tools as Morphisms in Graded Transformers
arxiv_id: '2511.17840'
source_url: https://arxiv.org/abs/2511.17840
tags:
- graded
- morphisms
- utility
- block
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a graded formulation of internal symbolic\
  \ computation for transformers, introducing a mathematical framework where symbolic\
  \ operations are realized as typed block maps (morphisms) between graded components\
  \ of the hidden space. The method uses a self-supervised graded utility functional\u2014\
  defined as the reduction in language-model loss induced by a candidate morphism\u2014\
  to govern selective activation, yielding sparse and interpretable behavior."
---

# Internalizing Tools as Morphisms in Graded Transformers

## Quick Facts
- arXiv ID: 2511.17840
- Source URL: https://arxiv.org/abs/2511.17840
- Reference count: 25
- Key outcome: A graded formulation where symbolic operations are realized as typed block maps (morphisms) between components of hidden space, selected by a self-supervised utility functional that maximizes loss reduction

## Executive Summary
This paper proposes a graded formulation of internal symbolic computation for transformers, introducing a mathematical framework where symbolic operations are realized as typed block maps (morphisms) between graded components of the hidden space. The method uses a self-supervised graded utility functional—defined as the reduction in language-model loss induced by a candidate morphism—to govern selective activation, yielding sparse and interpretable behavior. The framework unifies symbolic computation, geometry, and self-supervised learning within the graded transformer formalism, while subsuming prior external-tool paradigms as a special case via functorial internalization. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks, demonstrating that this approach enables internal, typed, composable symbolic reasoning within the model's representation manifold.

## Method Summary
The approach decomposes hidden space into orthogonal graded components $V = \bigoplus_g V_g$ and realizes symbolic operations as typed block maps $\phi_{h \leftarrow g}: V_g \to V_h$ between components. A self-supervised utility functional measures loss reduction from applying each morphism, governing activation through softmax routing with augmented logits. The framework uses residual updates after morphic application and can internalize external tools via categorical functors, preserving composition. Training involves synthetic tasks with 2-4 grades, utility estimation through dual forward passes, and regularization for sparsity.

## Key Results
- Graded hidden space decomposition enables modular symbolic operations as typed morphisms between components
- Self-supervised utility functional governs sparse, interpretable morphism selection without external supervision
- Functorial internalization subsumes external-tool paradigms as special case, enabling internal typed symbolic reasoning

## Why This Works (Mechanism)

### Mechanism 1: Graded Representation Decomposition with Typed Block Maps
- Claim: Partitioning hidden space into orthogonal graded components enables modular symbolic operations as typed morphisms between components.
- Mechanism: Hidden state $z_t = \sum_g z_t^{(g)}$ decomposes into homogeneous components $V_g$. Symbolic operations become block maps $\phi_{h \leftarrow g}: V_g \to V_h$ restricted to admissible edges $E \subseteq G \times G$. Block-orthogonality ensures decoupled gradients and identifiability per block.
- Core assumption: Block-orthogonality holds—cross-grade covariances vanish (requires gradient noise and may need explicit regularization during training).
- Evidence anchors: [abstract] "hidden space is endowed with a grading $V=\bigoplus_{g\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms)"; [Section 2.5, Lemma 2.11] Proves least-squares decoupling under orthogonality.
- Break condition: If cross-grade correlations persist after normalization, utility decomposes incorrectly, causing interference between morphisms.

### Mechanism 2: Graded Utility Functional Governs Selective Activation
- Claim: Self-supervised utility—measured as reduction in language-model loss—drives sparse, interpretable morphism selection without external supervision.
- Mechanism: Per-candidate utility $\Delta L_t(h \leftarrow g) = L_{LM}(z_t) - L_{LM}(z_t^+)$ measures loss improvement from applying $\phi_{h \leftarrow g}$. Softmax routing with augmented logits $\tilde{\ell}_t = \ell_t + \beta(\Delta L_t - \tau_{h \leftarrow g})$ selects high-utility edges. Lemma 5.2 bounds utility by alignment with negative gradient.
- Core assumption: The loss landscape is locally smooth and strongly convex along each $V_h$ (enables gradient-based utility interpretation).
- Evidence anchors: [abstract] "A self-supervised graded utility functional, defined as the loss reduction induced by a candidate morphism, governs activation"; [Section 5.2, Theorem 5.1] Derives utility bounds under smoothness/convexity.
- Break condition: If candidate morphisms produce updates orthogonal to the gradient or loss curvature is high, utilities become unreliable signals, causing random routing.

### Mechanism 3: Functorial Internalization of External Tools
- Claim: External symbolic tools embed faithfully as internal morphisms via categorical functors, preserving composition and enabling differentiable tool chains.
- Mechanism: External tool category $\mathcal{T}$ maps to internal category $\mathcal{M}$ via functor $F: \mathcal{T} \to \mathcal{M}$. Adjunction $F \dashv G$ ensures round-trips are idempotent under orthogonality.
- Core assumption: Encoders/decoders exist mapping interface types to grades such that realized blocks approximate tool behavior.
- Evidence anchors: [abstract] "subsuming prior external-tool paradigms (e.g., Toolformer) as a special case via functorial internalization"; [Section 3.4, Theorem 3.15] Constructs adjoint functors with unit/counit from encoder-decoder pairs.
- Break condition: If encoder-decoder round-trips have high reconstruction error, adjointness fails and composed morphisms diverge from intended tool behavior.

## Foundational Learning

- **Graded Vector Spaces and Direct Sum Decomposition**: Why needed: The entire architecture assumes $V = \bigoplus_g V_g$ with orthogonal projections; without this, morphisms cannot be typed or isolated. Quick check: Given a 512-dim hidden state with 4 grades of equal size, what is the projection $\pi_{g_2}$?
- **Bregman Divergences and Mirror Descent**: Why needed: Section 6.2.2 interprets utility-gated routing as a constrained mirror-descent step; understanding this clarifies why the routing promotes sparsity. Quick check: If $\Phi(u) = \frac{1}{2}\|u\|^2$, what Bregman divergence does this induce?
- **Functors and Adjunctions (Category Theory)**: Why needed: The internalization proof (Theorem 3.15, 6.4) relies on functoriality and adjoints; practitioners need to recognize when tool chains compose correctly. Quick check: What does $F \dashv G$ mean informally for round-trip tool operations?

## Architecture Onboarding

- **Component map**: Input -> Grade Projections -> Morphism Blocks -> Utility Estimation -> Router -> Weighted Update -> Output
- **Critical path**: 
  1. Project input to grades: $z^{(g)}_t = \pi_g(z_t)$
  2. Compute candidate outputs: $\tilde{z}^{(h)}_t = \sum_{g:(g,h)\in E} \phi_{h\leftarrow g}(z^{(g)}_t)$
  3. Estimate utilities $\Delta L_t(h \leftarrow g)$ via forward pass with temporary state update
  4. Compute routing weights $\alpha_t(h \leftarrow g)$ via utility-augmented softmax
  5. Apply weighted morphic update with residual: $z^{\text{new}}_t = \text{LN}(z_t + \sum_h (\tilde{z}^{(h)}_t - z^{(h)}_t))$

- **Design tradeoffs**:
  - Sparsity ($\tau$, $\beta$): High $\tau$/high $\beta$ → fewer activations, cleaner interpretation; low $\tau$ → more expressive but noisier routing
  - Edge set $E$: Banded/translation-invariant ($|\Delta| \ll |G|^2$) reduces parameters but limits cross-grade expressivity
  - Router detach: Detaching $\Delta L_t$ in $\alpha$ computation stabilizes training but may reduce routing sensitivity to morphism updates

- **Failure signatures**:
  - Grade collapse: All $\alpha_t$ concentrate on one edge → check utility margins, increase $\tau$
  - Utility oscillation: $\Delta L_t$ alternates signs across batches → reduce $\beta$, add smoothing
  - Non-identifiable blocks: Different morphisms produce similar outputs → check orthogonality, add orthogonality regularization

- **First 3 experiments**:
  1. **Mod-$p$ arithmetic sanity check** (Section 5.3.1): 2-grade model with $V_{\text{sem}} = V_{\text{num}} = \mathbb{R}^p$, morphism as cyclic shift. Verify utility is positive and routing concentrates on correct edge. Expected: $\Delta L_t \geq \gamma_p > 0$ for correct shifts.
  2. **Routing sparsity calibration**: Synthetic task with known optimal edge. Sweep $\beta \in [1, 20]$ and $\tau \in [0, 0.5]$. Monitor entropy $H_t = -\sum_e \alpha_t(e) \log \alpha_t(e)$. Expect sharp drop at correct $\beta/\tau$ threshold.
  3. **Block-orthogonality ablation**: Add explicit orthogonality penalty $\lambda \sum_{g \neq g'} \|\mathbb{E}[z^{(g)}(z^{(g')})^\top]\|_F^2$. Compare utility decomposition additivity (Prop 6.11) with/without penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can path-level utility selection for multi-step morphic programs (Π) be made consistent under bounded depth, with a defined path utility ΔLt(Π) = LLM(zt) − LLM(ΦΠ(zt)) minus a regularized cost(Π)?
- Basis in paper: [explicit] Section 7 lists "Path-level selection and program composition" as open problem (i), asking for consistency proofs for selectors maximizing ΔLt(Π) − cost(Π).
- Why unresolved: The paper focuses on single-step routing and leaves multi-step program selection for subsequent work.
- What evidence would resolve it: A theorem establishing consistency conditions under which greedy or beam-based path selectors converge to optimal morphic programs with bounded depth.

### Open Question 2
- Question: Can composition laws for morphisms be learned as graded higher structure, i.e., coefficients ck←h←g satisfying ϕk←h ∘ ϕh←g = ck←h←g ϕk←g + Rk←h←g with bounded residuals?
- Basis in paper: [explicit] Section 7 item (ii) asks for learnable program laws with identifiability guarantees on coefficients and bounds on ∥R∥.
- Why unresolved: The paper proves composition exists abstractly but does not address how to learn or identify composition coefficients from data.
- What evidence would resolve it: An algorithm that recovers ck←h←g uniquely under block-orthogonality, with sample complexity bounds for finite data.

### Open Question 3
- Question: Does utility-threshold pruning preserve top-k gains with high probability, and what characterizes optimal banded edge sets E for LGT/EGT architectures?
- Basis in paper: [explicit] Section 7 item (iii) poses this as an open problem on complexity, sparsity, and pruning with guarantees.
- Why unresolved: The paper shows sparsity reduces FLOPs but does not prove that pruning low-utility edges preserves task performance.
- What evidence would resolve it: A concentration inequality showing that edges with utility below a threshold contribute negligible loss reduction, validated on synthetic tasks.

### Open Question 4
- Question: Can identifiability of grades and blocks be guaranteed, and what diagnostics detect failures like grade collapse?
- Basis in paper: [explicit] Section 7 item (v) requests conditions for unique recovery and diagnostic design; Lemma 2.11 gives partial identifiability under block-orthogonality.
- Why unresolved: Identifiability is proven for fixed E but not when E or grading structure is learned; no empirical diagnostics are provided.
- What evidence would resolve it: Necessary/sufficient conditions for unique grade recovery plus diagnostic metrics (e.g., grade-wise covariance rank) tested on collapsed vs. healthy models.

## Limitations

- Block-orthogonality assumption lacks empirical validation; cross-grade correlations may persist during training
- Utility estimation via dual forward passes introduces computational overhead and variance that could destabilize training
- Practical benefits of functorial internalization over existing external-tool approaches remain theoretical

## Confidence

**High Confidence**: Mathematical foundations (graded vector spaces, block-orthogonality properties, utility bounds under smoothness assumptions) are rigorous and internally consistent.

**Medium Confidence**: Routing mechanism (utility-augmented softmax) is well-defined and should work as described, but empirical behavior requires more validation.

**Low Confidence**: Claims about cross-grade orthogonality emerging naturally during training lack empirical support; practical benefits of functorial internalization remain theoretical.

## Next Checks

1. **Block-Orthogonality Validation**: Implement diagnostic to measure empirical cross-grade covariance during training on mod-p arithmetic task. Plot covariance norms vs. training steps to verify orthogonality emerges naturally or requires explicit regularization.

2. **Utility Estimation Variance Study**: Systematically vary batch size (8→32→128) on memory retrieval task and measure variance of ∆L_t estimates. Compare training stability and convergence speed between detached vs. non-detached utility routing.

3. **Functorial Internalization Benchmark**: Extend Dyck task to include both internalized stack operations and external tool calls. Measure performance, routing sparsity, and composability of mixed internal/external tool chains.