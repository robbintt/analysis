---
ver: rpa2
title: A Concise Review of Hallucinations in LLMs and their Mitigation
arxiv_id: '2512.02527'
source_url: https://arxiv.org/abs/2512.02527
tags:
- hallucinations
- llms
- hallucination
- language
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review provides a concise overview of hallucinations in large
  language models (LLMs), their causes, and mitigation strategies. Hallucinations
  are defined as model-generated outputs that lack factual grounding, such as false
  statements or fabricated references.
---

# A Concise Review of Hallucinations in LLMs and their Mitigation

## Quick Facts
- arXiv ID: 2512.02527
- Source URL: https://arxiv.org/abs/2512.02527
- Reference count: 37
- This review provides a concise overview of hallucinations in large language models (LLMs), their causes, and mitigation strategies

## Executive Summary
This review provides a comprehensive overview of hallucinations in large language models (LLMs), defining them as outputs that lack factual grounding such as false statements or fabricated references. The paper categorizes existing mitigation approaches into four main categories: detection techniques, fine-tuning methods, knowledge integration strategies, and user-centered approaches. Key evaluation benchmarks like TruthfulQA and FaithDial are highlighted, and the review concludes that while complete elimination of hallucinations is unlikely, a combination of these strategies can significantly improve model reliability.

## Method Summary
The review synthesizes existing literature on LLM hallucination mitigation without conducting original experiments. The methodology involves systematic literature review of detection approaches (black-box identifiers, self-assessment, early detection), fine-tuning methods (domain-specific adaptation, F² Faithful Finetuning), knowledge integration (RAG, knowledge injection), and RLHF-based human feedback alignment. The paper references specific studies and their reported metrics but does not provide implementation details or experimental validation protocols.

## Key Results
- Four main mitigation categories identified: detection, fine-tuning, knowledge integration, and user-centered approaches
- RAG+FE achieved 15% factual errors according to referenced studies
- RLHF-V achieved 34.8% hallucination reduction with segment-level fine-grained feedback
- TruthfulQA and FaithDial benchmarks highlighted as key evaluation standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) may reduce hallucinations by grounding model outputs in externally verified knowledge sources.
- Mechanism: External knowledge bases provide factual anchors that constrain generation, reducing the model's reliance on potentially incorrect parametric knowledge.
- Core assumption: The retrieved information is accurate, up-to-date, and relevant to the query.
- Evidence anchors:
  - [abstract] "Mitigation strategies involve... retrieval-augmented generation (RAG)"
  - [section V] "[9] deals with the importance of how structured knowledge sources like databases or factual repositories within content are embedded into LLM to help them produce better and less fallacious content"
  - [corpus] Neighbor paper "Mitigating Hallucinated Translations in Large Language Models" (FMR=0.58) supports preference optimization approaches for hallucination reduction
- Break condition: RAG fails when retrieved documents are irrelevant, outdated, or when the model ignores retrieved context in favor of parametric knowledge.

### Mechanism 2
- Claim: Reinforcement Learning from Human Feedback (RLHF) can align model outputs toward truthfulness by rewarding factually accurate responses.
- Mechanism: Human annotators provide pairwise preferences or fine-grained feedback on factual accuracy; the reward model learns to penalize hallucinated content during fine-tuning.
- Core assumption: Human annotators can reliably identify hallucinations and provide consistent feedback at scale.
- Evidence anchors:
  - [abstract] "Reinforcement Learning from Human Feedback (RLHF)" listed as mitigation strategy
  - [section V, Table I] RLHF-V achieved "34.8% hallucination reduction" with segment-level fine-grained feedback
  - [corpus] Weak direct corpus evidence on RLHF specifically; neighbor papers focus more on detection than RL-based mitigation
- Break condition: RLHF effectiveness degrades when human feedback is inconsistent, when reward hacking occurs, or when the reward model fails to generalize to out-of-distribution queries.

### Mechanism 3
- Claim: Black-box hallucination detection can identify ungrounded outputs without accessing model internals.
- Mechanism: Detection tools analyze output coherence, consistency across multiple generations, or cross-reference against known facts without requiring model weights or activation patterns.
- Core assumption: Hallucinations produce detectable patterns in output text that differ from factual responses.
- Evidence anchors:
  - [abstract] "Detection techniques include black-box hallucination identifiers and early detection systems"
  - [section III] "[14] proposes the novel detection of hallucination without the need for model internals... This zero-access model is most useful in practical situations"
  - [corpus] Neighbor paper "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection" (referenced in paper) provides supporting methodology
- Break condition: Detection fails for subtle hallucinations that are internally consistent but factually wrong, or when detection latency requirements conflict with real-time generation needs.

## Foundational Learning

- Concept: **Hallucination taxonomy (factual vs. logical)**
  - Why needed here: Mitigation strategies differ depending on whether the model is generating false facts or producing contradictory reasoning chains.
  - Quick check question: Can you distinguish between a model fabricating a citation (factual hallucination) and a model contradicting itself mid-response (logical hallucination)?

- Concept: **Parametric vs. retrieved knowledge**
  - Why needed here: RAG-based approaches assume model parametric knowledge is unreliable; understanding this distinction explains why external grounding helps.
  - Quick check question: When a model answers "What is the capital of France?" from training data alone, is this using parametric or retrieved knowledge?

- Concept: **Benchmark evaluation metrics (TruthfulQA, FaithDial)**
  - Why needed here: Without standardized benchmarks, hallucination reduction claims cannot be meaningfully compared across approaches.
  - Quick check question: What dimension does TruthfulQA measure versus FaithDial?

## Architecture Onboarding

- Component map:
  User Query → [Prompt Engineering Layer] → [LLM Core] → [Detection Layer] → Output
                     ↓                           ↓
              [RAG Retrieval]              [Knowledge Base]
                                             ↓
                                        [RLHF Reward Model]

- Critical path:
  1. Query preprocessing and constraint-based prompt design
  2. Optional RAG retrieval for domain-specific queries
  3. Model generation with sampling constraints
  4. Black-box detection filtering before user delivery

- Design tradeoffs:
  - RAG adds latency but improves factual grounding
  - RLHF requires substantial annotation cost but provides continuous improvement
  - Black-box detection is model-agnostic but may have higher false positive rates than internal-state methods
  - Domain-specific fine-tuning improves accuracy in target domains but risks catastrophic forgetting

- Failure signatures:
  - High factual error rate with low uncertainty refusals → model overconfident on out-of-distribution queries
  - Consistent fabrication of citations → training data contained synthetic examples without grounding verification
  - Contradictory outputs on repeated queries → temperature too high or detection not enforcing consistency

- First 3 experiments:
  1. Establish baseline hallucination rate using TruthfulQA benchmark on your target model without any mitigation
  2. Implement RAG retrieval on a domain-specific knowledge base and measure factual error reduction (Table II shows RAG+FE achieves 15% factual errors)
  3. Add black-box detection layer (e.g., SelfCheckGPT-style consistency checking) and measure precision/recall tradeoff on flagged outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can real-time hallucination detection and correction be effectively implemented during model inference in high-stakes domains?
- Basis in paper: [explicit] The authors identify "real-time systems for the detection of hallucination" as a necessary future direction to prevent the spread of misinformation in critical contexts like legal or medical document generation.
- Why unresolved: Current detection methods often operate post-hoc or require intensive computation, making them difficult to integrate into the low-latency requirements of real-time conversational agents.
- What evidence would resolve it: The development of inference-time monitors that can flag or rectify factual errors instantly without significantly degrading the user experience or system throughput.

### Open Question 2
- Question: To what extent does incorporating multi-modal data (images, audio, structured data) improve the accuracy of hallucination detection compared to text-only analysis?
- Basis in paper: [explicit] Section VII highlights "multi-modal detection of hallucinations" as a specific area of interest, noting that hallucinations often occur when text is combined with other forms of media.
- Why unresolved: Most existing benchmarks and detection tools focus heavily on textual inconsistencies, lacking robust frameworks to verify cross-modal consistency (e.g., alignment between generated text and accompanying images).
- What evidence would resolve it: Comparative studies showing higher detection rates or reduced hallucination frequency in multi-modal models versus text-only models when processing identical information queries.

### Open Question 3
- Question: How can models be fine-tuned on new domain-specific knowledge without inducing conflicts with internal pre-trained knowledge that lead to hallucinations?
- Basis in paper: [inferred] The review cites [26] in Section V, posing the problem that fine-tuning on new knowledge might create "hallucinatory effects" if the new information conflicts with the model's existing knowledge base.
- Why unresolved: There is a lack of established protocols for "knowledge injection" that ensure factual updates are integrated seamlessly without destabilizing the model’s previously correct assertions.
- What evidence would resolve it: A fine-tuning methodology that successfully updates specific factual domains while maintaining or improving performance on static truthfulness benchmarks like TruthfulQA.

## Limitations

- No experimental data provided to validate the proposed mechanisms' effectiveness
- Implementation specifics for detection tools and mitigation strategies are absent
- Statistical significance and confidence intervals for reported metrics are not provided
- Benchmark evaluation protocols are referenced but not detailed

## Confidence

- **High confidence**: The definition of hallucinations and general categorization of mitigation strategies (detection, fine-tuning, RAG, RLHF) are well-established in the literature.
- **Medium confidence**: Specific effectiveness claims for individual methods (e.g., RLHF-V achieving 34.8% hallucination reduction) depend on cited papers' methodologies and evaluation protocols.
- **Low confidence**: Implementation details, parameter choices, and practical deployment considerations are not specified, making quantitative assessment difficult.

## Next Checks

1. Implement and evaluate RAG-based grounding on domain-specific knowledge to measure factual error reduction compared to baseline
2. Deploy SelfCheckGPT-style consistency detection on a subset of TruthfulQA questions and measure precision-recall tradeoffs
3. Test RLHF fine-tuning on pairwise preference data for a target domain and track hallucination rates versus baseline and refusal frequency