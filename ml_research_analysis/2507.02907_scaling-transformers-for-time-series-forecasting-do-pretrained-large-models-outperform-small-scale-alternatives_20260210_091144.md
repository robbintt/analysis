---
ver: rpa2
title: 'Scaling Transformers for Time Series Forecasting: Do Pretrained Large Models
  Outperform Small-Scale Alternatives?'
arxiv_id: '2507.02907'
source_url: https://arxiv.org/abs/2507.02907
tags:
- time
- series
- forecasting
- lstsms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares pretrained large-scale time series models (LSTSMs)
  with small-scale transformers for time series forecasting, analyzing their performance
  across long-term and short-term forecasting tasks. It evaluates models like Moirai,
  TimeGPT, and Timer against non-pretrained transformers such as Crossformer and PatchTST
  using benchmark datasets.
---

# Scaling Transformers for Time Series Forecasting: Do Pretrained Large Models Outperform Small-Scale Alternatives?

## Quick Facts
- **arXiv ID:** 2507.02907
- **Source URL:** https://arxiv.org/abs/2507.02907
- **Reference count:** 40
- **Primary result:** Pretrained LSTSMs outperform small-scale transformers in accuracy for time series forecasting tasks, particularly in long-term predictions, while smaller models remain competitive in computational efficiency.

## Executive Summary
This study compares pretrained Large-Scale Time Series Models (LSTSMs) with small-scale transformers for time series forecasting across long-term and short-term tasks. Using benchmark datasets and models like Moirai, TimeGPT, Timer, and Crossformer, the research finds that pretrained LSTSMs generally achieve superior accuracy (measured by MSE and MAE) due to their ability to leverage transfer learning and capture complex temporal dependencies. Large-scale models also demonstrate better generalization across domains, though smaller transformers remain competitive for real-time applications where computational efficiency is critical.

## Method Summary
The study evaluates LSTSMs (Timer, Timer_XL, Moment, Moirai, GPT4TS, LLM4TS, TimeLLM, TimeMachine) against small-scale transformers (Crossformer, ETSformer, iTransformer, PatchTST, FEDformer) on long-term datasets (ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, Traffic, Exchange) and short-term datasets (PEMS03, PEMS04, PEMS08). LSTSMs are tested under zero-shot, few-shot (50% data), and full-shot fine-tuning conditions, while small-scale transformers are trained from scratch. The evaluation uses standard lookback windows (L=672 for long-term, L=96 for short-term) and multiple prediction horizons, with metrics being MSE and MAE.

## Key Results
- Pretrained LSTSMs, particularly LLM4TS_FS and Timer_XL, outperform small-scale transformers across most forecasting tasks
- LSTSMs excel in long-term forecasting by capturing complex temporal dependencies and leveraging transfer learning
- Small-scale transformers remain competitive in computational efficiency and real-time applications despite lower accuracy
- Pretrained models show superior generalization across different domains compared to models trained from scratch

## Why This Works (Mechanism)

### Mechanism 1: Representation Stability via Pre-training
Pre-training on large, diverse corpora stabilizes the spectral properties of weight matrices, leading to better generalization. The study suggests pre-trained layers converge toward optimal $\alpha$ values (2-6), correlating with effective learning capacity. When fine-tuned, these stable representations require less adjustment to fit target data, minimizing expected loss.

### Mechanism 2: Autoregressive Processing of Temporal Causality
LSTSMs utilizing decoder-only autoregressive factorization capture temporal dependencies more effectively by strictly adhering to causal time-ordering. Unlike bidirectional attention that sees past and future simultaneously, LSTSMs predict the next time step based solely on past observations, enforcing a causal structure that reduces future information leakage during training.

### Mechanism 3: Scaling Capacity for Long-Context Dependencies
Larger parameter scale and context window size enable models to capture longer-range dependencies and reduce error accumulation in long-horizon forecasting. The study notes LSTSMs scale computationally as O(T·d), managing long sequences more efficiently than the quadratic O(T²) of standard attention.

## Foundational Learning

- **Concept:** **Autoregressive vs. Bidirectional Attention**
  - **Why needed here:** The paper frames the comparison fundamentally around this architectural difference. LSTSMs use autoregressive (causal) decoding while small-scale transformers typically use bidirectional attention.
  - **Quick check question:** Does the model mask future tokens in the attention layer to prevent information leakage, or does it see the whole input history at once?

- **Concept:** **Tokenization of Time Series (Patching)**
  - **Why needed here:** To apply Transformers to time series, continuous values must be discretized or embedded. LSTSMs like Timer and Moment utilize patching to aggregate local temporal information.
  - **Quick check question:** Is the model ingesting raw floats at every timestep, or is it embedding a window of 16 steps into a single vector (patch)?

- **Concept:** **Transfer Learning (Zero/Few/Full-Shot)**
  - **Why needed here:** The core value proposition of LSTSMs is their ability to transfer knowledge. Understanding the gradient of "shots" (amount of task-specific data used) is critical to interpreting results.
  - **Quick check question:** Is the model being trained from scratch (random weights), or are we starting with weights from a massive pre-training run (e.g., Era5)?

## Architecture Onboarding

- **Component map:** Input Layer -> Tokenizer -> Backbone (Encoder/Decoder) -> Head (Linear projection)

- **Critical path:** The choice of **Backbone + Pre-training Strategy** is the critical decision node.
  *   Path A (Small-scale): Random Init → Train on specific dataset → Deploy. (Fast, cheap, lower accuracy on complex tasks)
  *   Path B (LSTSM): Load Pre-trained Weights → Fine-tune (Full/Zero-shot) → Deploy. (Slow init, expensive, high accuracy)

- **Design tradeoffs:**
  - **Efficiency vs. Accuracy:** LSTSMs offer lower MSE but require significantly more GPU memory and inference time. Small transformers are "lightweight" and "competitive" for task-specific applications but lack broad generalization.
  - **Data Availability:** If data is scarce, LSTSM zero/few-shot capabilities dominate. If data is abundant, small transformers can close the gap but may still lag in capturing complex long-term dependencies.

- **Failure signatures:**
  - **Unstable Alpha:** If the layer α values are <2 or >6, the model is likely underfitting or overfitting.
  - **Quadratic Bottleneck:** Using standard small transformers on very long sequences (T > 1000) without efficient attention optimizations leads to O(T²) memory crashes.
  - **Zero-shot Drift:** Using LSTSMs in zero-shot mode on data distributions wildly different from the pre-training set without fine-tuning.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Implement `PatchTST` on the `ETTh1` dataset with a 96-step horizon to establish a competitive baseline MSE.
  2. **LSTSM Zero-Shot Evaluation:** Load a pre-trained `Moirai` or `Moment` model and evaluate on `ETTh1` without updating weights. Compare MSE against the PatchTST baseline.
  3. **Fine-Tuning Ablation:** Take `LLM4TS` or `Timer_XL` and run two variations: (a) Few-shot (50% data) and (b) Full-shot (100% data). Verify if the cost of fine-tuning yields diminishing returns or significant leaps over the zero-shot score.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do LSTSMs compare to small-scale transformers regarding computational efficiency during inference and model interpretability in real-time deployment scenarios?
- **Open Question 2:** Can multi-modal foundation models integrating text and time series data significantly outperform purely numerical Large-scale Time Series Models (LSTSMs)?
- **Open Question 3:** Do the superior generalization capabilities of fine-tuned LSTSMs observed in forecasting tasks transfer effectively to time series classification and anomaly detection?

## Limitations
- The paper lacks analysis on model interpretability and computational efficiency during real-time deployment scenarios
- Critical hyperparameters for small-scale transformer training remain unspecified, potentially affecting fair comparison
- The spectral analysis claims connecting α values to generalization capability require further empirical validation

## Confidence
- **High confidence:** The general observation that pretrained models outperform non-pretrained alternatives on average across benchmark datasets
- **Medium confidence:** The specific claim that LSTSMs excel particularly in long-term forecasting due to their ability to capture complex temporal dependencies
- **Medium confidence:** The assertion that LSTSMs provide superior generalization across domains

## Next Checks
1. **Spectral Analysis Replication:** Independently measure α values across different weight matrices in both pretrained and non-pretrained models to verify the claimed correlation with generalization performance.

2. **Efficiency Benchmarking:** Implement Timer_XL and a small-scale transformer (e.g., PatchTST) with identical hardware constraints and measure actual memory usage and inference time across various sequence lengths to validate the O(T·d) vs O(T²) claims.

3. **Transfer Learning Ablation:** Conduct controlled experiments where models pretrained on Era5 are fine-tuned on diverse downstream datasets (climate, energy, traffic) to systematically measure how pre-training dataset relevance affects transfer performance.