---
ver: rpa2
title: 'All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level
  Spiking Neural Networks'
arxiv_id: '2510.24637'
source_url: https://arxiv.org/abs/2510.24637
tags:
- spiking
- energy
- multi-level
- spikes
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks

## Quick Facts
- arXiv ID: 2510.24637
- Source URL: https://arxiv.org/abs/2510.24637
- Authors: Andrea Castagnetti; Alain Pegatoquet; Benoît Miramond
- Reference count: 28
- Primary result: Multi-level spiking neurons reduce quantization error, latency, and energy consumption while maintaining accuracy

## Executive Summary
This paper proposes multi-level spiking neurons as an alternative to traditional binary spiking neurons in Spiking Neural Networks (SNNs). By encoding information in the amplitude of a single spike rather than its timing, the approach achieves 2-3x energy reduction and improved sparsity. The authors demonstrate effectiveness on CIFAR-10/100 and CIFAR-10-DVS datasets using VGG16 and ResNet18 architectures.

## Method Summary
The method replaces binary spiking neurons with multi-level spiking neurons that generate a spike value z(t) ∈ [0, N] within a single timestep. For residual networks, a "barrier neuron" with Straight-Through Estimator (STE) backward pass is inserted after residual addition to prevent spike avalanche effects. The model uses sigmoid surrogate gradients for standard layers and STE for barrier neurons. Training employs SGD with learning rate decay for CIFAR-10/100 and Adam for CIFAR-10-DVS.

## Key Results
- Multi-level VGG16 (T=1, N=4) produces 57·10^3 valued spikes vs 130·10^3 binary spikes
- Sparse-ResNet with barrier neurons achieves 3x spike reduction compared to SEW-ResNet
- Maintains competitive accuracy on CIFAR-10/100 and CIFAR-10-DVS benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Information Density Exchange (Time vs. Amplitude)
Multi-level spiking neurons replace temporal binary spikes with single-timestep multi-level spikes, preserving information capacity while reducing total synaptic events. A standard IF neuron with T timesteps provides T+1 quantization levels, while the multi-level neuron achieves N×T+1 levels by subdividing a timestep into N micro-timesteps.

### Mechanism 2: Activity Regulation via Barrier Neurons
A barrier neuron with specific gradient properties is inserted after residual addition to prevent spike avalanche effects. This neuron re-quantizes aggregated membrane potentials, filtering activity before it propagates to deeper layers.

### Mechanism 3: Gradient Flow Preservation via STE
The Straight-Through Estimator for barrier neurons preserves gradient magnitude during backpropagation, solving vanishing gradient problems that occur when using standard surrogate functions on post-addition spikes.

## Foundational Learning

- **Surrogate Gradient vs. STE**: The paper uses sigmoid surrogate for standard layers but STE for barrier layers because standard surrogate gradients vanish when membrane potentials are far from threshold after residual addition. *Quick check: Why does the paper use STE for the barrier neuron but sigmoid for standard neurons?*

- **Rate Coding vs. Single-Timestep Quantization**: Binary SNNs use firing frequency over time to represent values, while this method uses instantaneous spike magnitude. *Quick check: If T=1 and N=4, how many distinct values can be represented compared to binary neuron with T=4?*

- **Spiking Residual Dynamics**: SEW-ResNet aggregates spikes through element-wise addition, causing activity to compound in deeper layers. *Quick check: In SEW-ResNet, if Layer L outputs 1 spike and residual carries 1 spike, how many spikes does Layer L+1 receive?*

## Architecture Onboarding

- **Component map**: Input -> Encoder (Direct potential) -> Block [Conv → Multi-level SN (Sigmoid SG)] -> Residual Link -> Merge Point (Summation) -> Barrier (Multi-level SN with STE) -> Decoder (Spike summation scaled by 1/N)

- **Critical path**: The barrier neuron implementation is crucial. Forward pass acts as standard multi-level IF neuron, but backward hook must replace gradient calculation with identity function (STE).

- **Design tradeoffs**: Higher N provides better accuracy and lower latency but increases per-spike transmission cost (log₂(N) bits). T is fixed at 1 in this paper but theoretically extendable.

- **Failure signatures**: Accuracy cap indicates barrier neuron surrogate slipping back to sigmoid; energy plateau suggests non-event-driven implementation; activity explosion indicates disabled barrier neuron or high threshold.

- **First 3 experiments**: 1) Implement multi-level neuron (N=4, T=1) on simple ConvNet and compare accuracy with binary SNN (T=4). 2) Train Sparse-ResNet with barrier neuron using standard SG vs STE, plotting gradient norms per layer. 3) Log total spike count for SEW-ResNet vs Sparse-ResNet to verify avalanche effect suppression.

## Open Questions the Paper Calls Out

### Open Question 1
Does the Sparse-ResNet architecture effectively mitigate spike avalanche in deeper networks like MobileNetV2 or ResNet34? The authors propose extending to these architectures but experiments were limited to ResNet18.

### Open Question 2
Do analytical energy efficiency gains translate to real-world savings on physical neuromorphic hardware? The paper uses an analytical energy model but calls for evaluation on real hardware to confirm results.

### Open Question 3
Can the 1-timestep multi-level approach scale to large-scale datasets like ImageNet without accuracy degradation? The method works well on CIFAR datasets, but scalability to high-resolution, high-class-count datasets remains untested.

## Limitations
- Surrogate gradient approximation may accumulate errors in extremely deep architectures beyond ResNet18
- Energy efficiency claims depend on theoretical memory access models rather than hardware measurements
- CIFAR-10-DVS preprocessing pipeline is not fully specified, affecting performance comparisons

## Confidence

- **High Confidence**: Multi-level spiking neurons can represent more information per timestep while maintaining accuracy
- **Medium Confidence**: Barrier neuron with STE effectively prevents spike avalanches in residual networks
- **Medium Confidence**: Energy efficiency improvements of 2-3x are achievable in principle

## Next Checks

1. Implement Sparse-ResNet and SEW-ResNet with and without STE on barrier neurons, plotting gradient norms per layer during training to confirm STE prevents vanishing gradients.

2. Run inference on CIFAR-10 with both architectures, logging spike counts at each summation point to verify exponential growth in SEW-ResNet is suppressed in Sparse-ResNet.

3. Implement the Lemaire et al. (2023) energy model and calculate theoretical energy consumption for binary vs. multi-level spiking, comparing against actual measured energy on neuromorphic hardware (e.g., Intel Loihi).