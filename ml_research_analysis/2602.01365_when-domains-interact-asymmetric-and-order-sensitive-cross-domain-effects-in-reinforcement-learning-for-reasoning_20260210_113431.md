---
ver: rpa2
title: 'When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects
  in Reinforcement Learning for Reasoning'
arxiv_id: '2602.01365'
source_url: https://arxiv.org/abs/2602.01365
tags:
- science
- training
- math
- logic
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work systematically studies cross-domain effects in Group\
  \ Relative Policy Optimization (GRPO) for reasoning tasks, revealing pronounced\
  \ asymmetry and order sensitivity in domain interactions. Training order significantly\
  \ impacts performance, with math \u2192 science achieving 83%/41% accuracy versus\
  \ 77%/25% when reversed, and mixed training proving preferable for science, logic,\
  \ and puzzle domains."
---

# When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning

## Quick Facts
- arXiv ID: 2602.01365
- Source URL: https://arxiv.org/abs/2602.01365
- Reference count: 14
- Key outcome: Cross-domain training in GRPO shows pronounced asymmetry and order sensitivity, with math→science yielding 83%/41% accuracy versus 77%/25% when reversed, and mixed training being optimal for science, logic, and puzzle domains.

## Executive Summary
This work systematically studies cross-domain effects in Group Relative Policy Optimization (GRPO) for reasoning tasks, revealing pronounced asymmetry and order sensitivity in domain interactions. Training order significantly impacts performance, with math → science achieving 83%/41% accuracy versus 77%/25% when reversed, and mixed training proving preferable for science, logic, and puzzle domains. Cross-domain generalization is highly asymmetric: math reasoning benefits from training on almost all domains (+25% accuracy), while logic and puzzle show minimal transfer. The study highlights the necessity of domain-aware and order-aware training design in multi-domain reinforcement learning for reasoning.

## Method Summary
The study uses GRPO to train Qwen3-4B-Base on four reasoning domains (math, science, logic, puzzle) with 5K training samples each. Experiments compare single-domain training, two-domain sequential training (A→B), and mixed training (A+B). Key hyperparameters include batch_size=256, lr=1e-6, 8 responses per prompt, and weight_decay=0.1. Performance is measured by accuracy on domain-specific test sets, with cross-domain transfer quantified as accuracy gains.

## Key Results
- Single-domain generalization is highly asymmetric: training on other domains improves math reasoning (+25% accuracy) while yielding negligible transfer to logic and puzzle
- Training order matters critically: math → science achieves 83% / 41% accuracy versus 77% / 25% when reversed
- Mixed training is preferable for science, logic, and puzzle domains, while sequential training benefits math performance

## Why This Works (Mechanism)

### Mechanism 1: Pretraining-Dependent Transfer Asymmetry
- **Claim:** Mathematical reasoning is uniquely activated by cross-domain training, whereas logic and puzzle reasoning require domain-specific data, likely due to pretraining data distributions.
- **Mechanism:** The base model (Qwen3-4B) appears to possess latent mathematical capabilities derived from heavy pretraining exposure. GRPO on non-math domains (science, logic) acts as a general reasoning "activator," sharpening these latent circuits. Conversely, logic/puzzle capabilities are sparse in pretraining, so cross-domain training provides insufficient signal for activation.
- **Core assumption:** The transferability is primarily determined by the density of relevant concepts in the pre-training corpus rather than the RL algorithm's architecture.
- **Evidence anchors:** "single-domain generalization is highly asymmetric... training on other domains improves math reasoning... while yielding negligible transfer to logic and puzzle"; "We hypothesize that this behavior stems from the exposure of Qwen3-4B-Base to substantial mathematical data during the pretraining stage."

### Mechanism 2: Order-Sensitive Interference via Gradient Competition
- **Claim:** Sequential training creates directional interference (e.g., science → math degrades both), while mixed training mitigates it.
- **Mechanism:** In sequential training, the policy optimization for the second domain aggressively shifts the policy parameters, potentially overwriting the heuristics learned in the first domain (catastrophic interference). Specifically, science training appears to introduce bias that conflicts with the logical structures required for math and logic tasks. Mixing data balances the gradient updates, preventing the model from shifting too far in any single domain's "direction."
- **Core assumption:** The interference is a result of parameter drift in shared attention heads or MLP weights serving both domains.
- **Evidence anchors:** "training in the order math → science achieves 83% / 41%... while reversing... degrades performance to 77% / 25%"; "science training consistently interferes with logical reasoning... leading to a 5.1% drop."

### Mechanism 3: Strategy-Dependent Optimization Landscapes
- **Claim:** Optimal training strategy (sequential vs. mixed) is domain-specific, not universal.
- **Mechanism:** Math benefits from the depth of sequential optimization (potentially "squeezing" more performance out of the high-capability domain). In contrast, science and logic benefit from the regularization effect of mixed training, which prevents overfitting to the noisier or sparser signals of a single domain.
- **Core assumption:** Math tasks offer a cleaner gradient signal for sequential "curriculum" learning, while science/logic benefit from the noise robustness of mixed batching.
- **Evidence anchors:** "mixed training is preferable for science, logic, and puzzle domains."; "For the math domain, the optimal sequential strategy yields the highest accuracy... In contrast, for science and logic, mixed training is more suitable."

## Foundational Learning

- **Concept:** **Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the core RL algorithm. It normalizes rewards within a group of responses rather than using a critic model. Understanding this is crucial because the "group" composition (mixed vs. single domain) changes the baseline for advantages.
  - **Quick check question:** If you mix easy and hard domains in one batch, how does the group-relative normalization affect the perceived advantage of the hard domain?

- **Concept:** **Catastrophic Forgetting / Interference**
  - **Why needed here:** The paper explicitly identifies that training on Domain B (Science) degrades performance on Domain A (Logic). This is a failure mode of sequential training where new knowledge overwrites old.
  - **Quick check question:** Why does Science → Math degrade performance more than Math → Science in this study?

- **Concept:** **Training Curriculum / Data Ordering**
  - **Why needed here:** The paper's central finding is that the *order* (curriculum) is a hyperparameter as critical as learning rate.
  - **Quick check question:** What is the risk of arbitrarily shuffling domains when training a multi-domain reasoning model?

## Architecture Onboarding

- **Component map:**
  - Base Model: Qwen3-4B-Base (Pre-trained, not instruction-tuned)
  - Reward Function: Verifiable rewards (Rule-based accuracy)
  - Optimizer: GRPO (Group Normalization + PPO-style clipping)
  - Data Loader: Configurable for Sequential (Domain A → B) or Mixed (A+B) sampling

- **Critical path:**
  1. Select base model with strong pre-training in target "hub" domains (here, Math)
  2. Identify transfer relationships (e.g., does Science help Math?)
  3. Sequence domains to exploit positive transfer (Math first) and avoid interference (Science before Logic)
  4. Use Mixed training for domains that interfere sequentially (Logic + Science)

- **Design tradeoffs:**
  - Sequential vs. Mixed: Sequential yields peak Math performance (84%) but risks collapse in other domains (Science → Math drops to 77%). Mixed yields balanced performance (71.9% avg) but slightly lower peak Math
  - Ordering: Math → Science maximizes joint performance; Science → Logic causes mutual interference

- **Failure signatures:**
  - Science-Logic Interference: Accuracy on Logic tests drops from 88% to ~64% if Science is introduced sequentially
  - Order Collapse: Using Science → Math order causes average accuracy to drop from ~70% to ~56%

- **First 3 experiments:**
  1. **Baseline Probe:** Train on single domains (Math, Science, Logic) independently to establish transfer asymmetry (check if training Logic improves Math)
  2. **Ablation on Order:** Compare Math → Science vs. Science → Math to verify the 6-16% gap reported
  3. **Mixed vs. Sequential:** Compare Logic → Science (sequential) vs. Logic + Science (mixed) to confirm mitigation of interference

## Open Questions the Paper Calls Out

- **Question:** What are the mechanistic origins of asymmetric cross-domain facilitation and interference?
  - **Basis in paper:** Appendix A states the work does not provide a "deep mechanistic explanation" or disentangle whether effects stem from pretraining data coverage, domain-specific knowledge overlap, or GRPO optimization dynamics
  - **Why unresolved:** The study establishes *that* order and asymmetry exist but does not explain the underlying representation shifts or optimization conflicts causing math→science to succeed while science→logic fails
  - **What evidence would resolve it:** Analyzing internal representations (e.g., using probing classifiers) to measure domain overlap during training, or correlating transfer gains with the frequency of specific reasoning patterns in the pretraining corpus

- **Question:** Do cross-domain interference effects diminish or persist in larger model scales?
  - **Basis in paper:** All experiments are conducted exclusively on Qwen3-4B-Base due to computational constraints, leaving the behavior of larger models unknown
  - **Why unresolved:** Larger models may possess sufficient capacity to mitigate the "forgetting" observed in sequential training, or they may exhibit the same sensitivity to training order
  - **What evidence would resolve it:** Replicating the sequential training orders (e.g., math→science vs. science→math) on 14B, 32B, and 70B variants to see if the performance gap between optimal and suboptimal orders narrows

- **Question:** Can an adaptive training strategy dynamically optimize domain sequencing?
  - **Basis in paper:** The paper manually compares fixed sequential and mixed strategies, noting that "no single strategy is universally optimal" and poor ordering causes significant degradation
  - **Why unresolved:** Currently, identifying the optimal order requires exhaustive combinatorial testing; there is no method to predict or adapt the sequence during training
  - **What evidence would resolve it:** Developing a curriculum learning algorithm that monitors validation performance on held-out domain data and dynamically adjusts the domain sampling ratio to prevent interference

## Limitations

- **Pretraining Data Dependency**: The observed asymmetry heavily depends on the base model's pretraining distribution, with math reasoning showing unique transferability due to Qwen3-4B's pretraining
- **Dataset Selection Bias**: The paper filters 5K samples from each domain with underspecified selection criteria, potentially affecting transfer patterns
- **Reward Function Ambiguity**: While math rewards are verifiable, science/logic/puzzle reward functions are only described as "verifiable" without precise rules

## Confidence

- **High Confidence**: Sequential order effects (math→science vs. science→math) and interference between science and logic are strongly supported by direct comparisons
- **Medium Confidence**: The pretraining-dependent transfer asymmetry is well-reasoned and aligned with observed data but depends on unverified assumptions about pretraining corpus composition
- **Low Confidence**: The claim that mixed training is universally preferable for science/logic/puzzle is based on limited comparisons and may not generalize across different model scales

## Next Checks

1. **Base Model Sensitivity Test**: Repeat the single-domain and sequential experiments using a different base model (e.g., DeepSeekMath-7B or Llama-3-8B) to verify if the transfer asymmetry persists or inverts

2. **Dataset Sampling Audit**: Document and replicate the exact filtering/sampling strategy for all domains, especially the 5K sample selection, to ensure reproducibility and rule out selection bias

3. **Reward Function Specification**: Precisely define the reward functions for science, logic, and puzzle domains (e.g., exact match rules, multiple-choice grading, rule-based verification) and verify their consistency across experiments