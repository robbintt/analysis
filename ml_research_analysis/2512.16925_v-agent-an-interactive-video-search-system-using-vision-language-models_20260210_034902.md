---
ver: rpa2
title: 'V-Agent: An Interactive Video Search System Using Vision-Language Models'
arxiv_id: '2512.16925'
source_url: https://arxiv.org/abs/2512.16925
tags:
- retrieval
- video
- search
- arxiv
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces V-Agent, a multi-agent video search system\
  \ that overcomes limitations of text-based retrieval by using vision-language models\
  \ (VLMs) to interpret both visual and spoken content in videos. The system employs\
  \ three agents\u2014routing, search, and chat\u2014that collaborate to process user\
  \ queries, retrieve relevant videos, and enable interactive conversations."
---

# V-Agent: An Interactive Video Search System Using Vision-Language Models

## Quick Facts
- arXiv ID: 2512.16925
- Source URL: https://arxiv.org/abs/2512.16925
- Reference count: 40
- Primary result: V-Agent achieves state-of-the-art zero-shot performance on MultiVENT 2.0 benchmark (nDCG@10: 0.680, Recall@10: 0.676)

## Executive Summary
V-Agent is a multi-agent video search system that overcomes limitations of text-based retrieval by using vision-language models (VLMs) to interpret both visual and spoken content in videos. The system employs three agents—routing, search, and chat—that collaborate to process user queries, retrieve relevant videos, and enable interactive conversations. A key innovation is the fine-tuning of VLMs with a small video preference dataset and the addition of a retrieval vector from an image-text retrieval model, enhancing the model's vision-text alignment. The VLM-based retrieval model jointly embeds video frames and audio transcriptions for multimodal video understanding.

## Method Summary
V-Agent uses a multi-agent architecture where a routing agent determines whether queries should go to a search agent (for retrieval) or chat agent (for conversation). The search agent uses a VLM-based retrieval model (Qwen2-VL-7B-Instruct fine-tuned with a retrieval vector from an image-text model) to embed video frames and ASR transcriptions independently, then fuses scores with weighted combination (α=0.5). An LLM reranker (gpt-4o-mini) refines top candidates. The system is indexed with pgvector (HNSW m=16, ef_construction=200) and fine-tuned using InfoNCE loss on ShareGPTVideo dataset with 17k video preference pairs.

## Key Results
- Achieves state-of-the-art zero-shot performance on MultiVENT 2.0 benchmark (nDCG@10: 0.680, Recall@10: 0.676)
- Outperforms existing models like MMMORRF on retrieval tasks
- Ablation studies confirm retrieval vector and re-ranking contribute 8 and 6 percentage points to nDCG@10 respectively
- Strong potential for both academic research and real-world video search applications

## Why This Works (Mechanism)

### Mechanism 1
Adding a retrieval vector (τ) derived from an image-text model to a video fine-tuned VLM improves vision-text alignment and retrieval quality. The retrieval vector τ is computed as the parameter difference between GME (an image-text retrieval model) and the original Qwen2-VL-Instruct model (τ = θ_GME − θ_Qwen), then added to the fine-tuned model weights (θ_MR = θ_MF + τ). This transfers general retrieval capabilities while preserving video-specific adaptations from fine-tuning on the preference dataset.

### Mechanism 2
Fusing frame-level visual embeddings with audio transcription embeddings via weighted score combination enables multimodal video understanding. Video frames (48 per video) and ASR-generated transcriptions are embedded independently by the retrieval model M_R. At query time, similarity scores are computed separately and fused: score = α · ⟨e_f, e_q⟩ + (1−α) · ⟨e_a, e_q⟩ with α=0.5. The top-k results from each modality are ranked by this combined score.

### Mechanism 3
LLM-based re-ranking significantly improves retrieval ranking quality by refining initial retrieval outputs according to query relevance. After initial retrieval, the rerank tool uses an LLM (gpt-4o-mini) guided by prompt p_rerank to reorder the top-k candidate videos. The LLM evaluates relevance between the query and each candidate's (audio transcription, description) tuple.

## Foundational Learning

- **Vision-Language Models (VLMs) and embedding spaces**: Why needed here: V-Agent's retrieval model must jointly represent video frames and text queries in a shared embedding space where semantic similarity correlates with inner product. Quick check: Can you explain why a VLM trained on image-text pairs might not directly produce good video retrieval embeddings without adaptation?

- **Contrastive learning (InfoNCE loss)**: Why needed here: The fine-tuning process uses InfoNCE loss with in-batch negatives and hard negatives to train the model to rank positive answers above rejected responses. Quick check: What is the role of hard negatives in contrastive learning, and why might they be particularly important when training data is limited?

- **Multi-agent orchestration and handoffs**: Why needed here: V-Agent routes queries through three specialized agents with explicit handoff logic; understanding this pattern is essential for debugging pipeline behavior. Quick check: In the V-Agent architecture, what determines whether the routing agent forwards a query to the search agent versus the chat agent?

## Architecture Onboarding

- **Component map**: User query → Routing Agent (gpt-4.1-mini) → (Search Agent: M_R retrieval → score fusion → LLM re-ranking → Chat Agent) OR (Chat Agent directly)
- **Critical path**: Retrieval quality depends on (1) M_R embedding quality ← fine-tuning + retrieval vector, (2) modality fusion quality ← frame count + ASR accuracy, (3) re-ranking effectiveness ← LLM relevance judgment
- **Design tradeoffs**: Frame count (16/32/48): More frames improve visual understanding modestly but increase indexing latency and storage; Re-ranking: +6% nDCG@10 but adds LLM latency; Description availability: significant drop without descriptions
- **Failure signatures**: Low recall on queries requiring visual-only content if not in transcription; poor performance on non-English content if translation fails; retrieval vector degrades performance if base model architectures mismatch
- **First 3 experiments**: 1) Reproduce ablation on retrieval vector; 2) Modality ablation (frames-only vs transcription-only); 3) Re-ranking prompt iteration

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating visual cues directly into the re-ranking module improve retrieval accuracy compared to the current text-only LLM approach? The authors state that visual information is "not sufficiently incorporated during the re-ranking phase" and hypothesize that integrating visual cues could yield better outcomes. Ablation studies comparing current text-based reranker against a multimodal VLM reranker that processes both frames and text would resolve this.

### Open Question 2
What is the optimal trade-off between retrieval effectiveness and system latency in the multi-agent pipeline? The "Limitations and Future Work" section notes that the agent-based flow introduces higher latency, and the authors aim to "establish an optimal balance." System benchmarks measuring end-to-end query latency against nDCG@10 scores would identify Pareto optimal configurations.

### Open Question 3
Does the "retrieval vector" addition remain effective when fine-tuning is scaled to larger video datasets? The authors introduce the retrieval vector specifically to compensate for the "insufficiency of training instances" (17k pairs), leaving its utility in data-rich regimes unexplored. Experiments comparing the fine-tuned model with and without the retrieval vector across varying training dataset sizes would resolve this.

## Limitations
- Visual-only queries requiring non-textual understanding cannot be answered if not captured in ASR transcriptions
- Reliance on accurate ASR and translation for non-English content introduces potential failure points
- Limited dataset size (17k samples) may constrain generalization to diverse video domains

## Confidence

- **High confidence**: Retrieval vector contribution (confirmed by ablation), re-ranking effectiveness (6% nDCG@10 improvement), and basic multi-agent routing architecture
- **Medium confidence**: VLM fine-tuning effectiveness and overall benchmark performance (limited dataset size and single benchmark suite leave some uncertainty)
- **Low confidence**: Optimal frame count (48 vs alternatives) and long-term stability of the retrieval vector transfer mechanism

## Next Checks

1. **Cross-architecture retrieval vector validation**: Test the τ transfer mechanism using different base VLM architectures (e.g., LLaVA, BLIP-2) to verify the approach generalizes beyond the Qwen2-VL/GLM combination used in the paper.

2. **Visual-only query benchmark**: Create a benchmark subset of queries that require visual content not captured in transcriptions (e.g., color, object presence, scene type) to quantify the performance gap when audio/text is insufficient.

3. **Fusion weight optimization study**: Systematically vary α (0.0 to 1.0 in increments) and measure retrieval quality on queries with known modality requirements to determine if adaptive fusion could improve performance over the fixed 0.5 weighting.