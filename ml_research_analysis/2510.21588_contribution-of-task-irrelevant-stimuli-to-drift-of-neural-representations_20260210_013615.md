---
ver: rpa2
title: Contribution of task-irrelevant stimuli to drift of neural representations
arxiv_id: '2510.21588'
source_url: https://arxiv.org/abs/2510.21588
tags:
- drift
- learning
- network
- where
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the contribution of task-irrelevant stimuli
  to representational drift in neural networks through theoretical analysis and simulations.
  The study demonstrates that even when neural networks learn to suppress task-irrelevant
  information, these stimuli can still act as a source of learning noise that causes
  gradual changes in the representation of task-relevant stimuli over time.
---

# Contribution of task-irrelevant stimuli to drift of neural representations

## Quick Facts
- **arXiv ID**: 2510.21588
- **Source URL**: https://arxiv.org/abs/2510.21588
- **Reference count**: 40
- **Primary result**: Task-irrelevant stimuli contribute to representational drift in neural networks even when suppressed by learning rules

## Executive Summary
This paper presents a theoretical and computational analysis of how task-irrelevant stimuli contribute to representational drift in neural networks. The study demonstrates that even when neural networks learn to suppress irrelevant information, these stimuli can act as a source of learning noise that causes gradual changes in the representation of task-relevant stimuli over time. Through analysis of multiple network architectures including Oja's rule, Similarity Matching networks, autoencoders, and supervised two-layer networks, the research shows that drift rates increase with both the variance and dimension of task-irrelevant stimuli. The findings reveal that learning-induced drift has distinct qualitative properties compared to drift caused by Gaussian synaptic noise, particularly in terms of geometry and dimension-dependency.

## Method Summary
The study employs theoretical analysis combined with computational simulations to examine representational drift across multiple network architectures. The approach involves deriving analytical expressions for drift rates under various learning rules and validating these predictions through simulations on synthetic Gaussian data and real MNIST datasets. The analysis covers both unsupervised networks (Oja's rule, Similarity Matching, autoencoders) and supervised networks (two-layer networks), systematically varying the variance and dimension of task-irrelevant stimuli to quantify their impact on representational drift. The theoretical framework is compared against empirical results to establish the robustness of the predictions across different learning scenarios.

## Key Results
- Task-irrelevant stimuli act as a source of learning noise that causes gradual changes in task-relevant stimulus representations
- Drift rates increase linearly with both the variance and dimension of task-irrelevant stimuli across multiple architectures
- Learning-induced drift exhibits different qualitative properties compared to drift from Gaussian synaptic noise, particularly in geometric patterns and dimension-dependency

## Why This Works (Mechanism)
The mechanism underlying this phenomenon stems from the fundamental tension between learning to represent task-relevant information while simultaneously being exposed to task-irrelevant stimuli. Even when learning rules are designed to suppress irrelevant information, the presence of these stimuli creates a form of learning noise that gradually alters the neural representations over time. This occurs because the learning dynamics must continuously negotiate between optimizing for task-relevant features and managing the interference from irrelevant inputs, leading to a slow drift in the representational space that accumulates over time.

## Foundational Learning
- **Representational drift**: The gradual changes in neural representations over time that can affect learning stability and memory retention. Why needed: Central concept being studied as it relates to learning dynamics and network stability.
- **Task-irrelevant stimuli**: Inputs that are present but not necessary for the current learning objective. Why needed: Primary focus of the study as a source of learning noise.
- **Learning noise**: Variability introduced into the learning process that affects representation stability. Why needed: Mechanism through which task-irrelevant stimuli influence drift.
- **Linear network approximations**: Simplified models used to derive analytical predictions about drift behavior. Why needed: Enable tractable mathematical analysis of complex learning dynamics.
- **Gaussian-distributed inputs**: Assumed input statistics for theoretical analysis. Why needed: Provides a tractable framework for deriving analytical results.
- **Dimensionality effects**: How the number of stimulus dimensions influences drift patterns. Why needed: Key factor determining the magnitude and nature of representational drift.

## Architecture Onboarding

**Component map**: Task-relevant stimuli -> Neural network (architecture-specific) -> Learning rule -> Task-irrelevant stimuli -> Drift in representations

**Critical path**: Input stimuli (relevant + irrelevant) → Neural representation → Learning dynamics → Representation drift

**Design tradeoffs**: The study balances theoretical tractability (using linear approximations and Gaussian assumptions) against capturing the essential dynamics of representational drift. This enables analytical predictions but may miss some complexities of real-world data distributions and nonlinear network behaviors.

**Failure signatures**: When theoretical predictions deviate from simulation results, this could indicate either breakdown of linear approximations, violation of Gaussian assumptions, or the presence of additional drift sources not accounted for in the model.

**3 first experiments**:
1. Vary the variance of task-irrelevant stimuli while keeping task-relevant stimuli constant to test the predicted linear relationship with drift rates
2. Increase the dimension of task-irrelevant stimuli in synthetic Gaussian data to verify dimensionality-dependent drift patterns
3. Compare drift patterns in networks with and without task-irrelevant stimuli to isolate the specific contribution of irrelevant inputs

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses primarily on linear and shallow network architectures, potentially missing drift mechanisms in deep networks
- Theoretical predictions assume Gaussian-distributed task-irrelevant stimuli, which may not reflect real-world data complexity
- Validation is limited to synthetic Gaussian data and MNIST, requiring testing on more diverse and naturalistic datasets

## Confidence
- **High confidence**: Core finding that task-irrelevant stimuli contribute to drift across multiple architectures, supported by strong agreement between theory and simulations
- **Medium confidence**: Dimensionality analysis and comparisons with synaptic noise, dependent on specific assumptions about network connectivity and noise characteristics

## Next Checks
1. Test theoretical predictions on deep convolutional neural networks trained on naturalistic image datasets to verify if linear approximations hold for complex architectures
2. Examine how non-Gaussian task-irrelevant stimulus distributions affect drift patterns and whether the theoretical framework requires modification
3. Conduct experiments to distinguish learning-induced drift from other drift sources in experimental neural recordings to validate model predictions in biological systems