---
ver: rpa2
title: Equality Graph Assisted Symbolic Regression
arxiv_id: '2511.01009'
source_url: https://arxiv.org/abs/2511.01009
tags:
- expressions
- expression
- search
- e-graph
- symregg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SymRegg, a symbolic regression algorithm that
  uses an equality graph (e-graph) to avoid redundant evaluations of equivalent expressions.
  Unlike traditional genetic programming, SymRegg incrementally builds and queries
  an e-graph to detect and reuse previously evaluated expressions, reducing computation
  waste.
---

# Equality Graph Assisted Symbolic Regression

## Quick Facts
- **arXiv ID:** 2511.01009
- **Source URL:** https://arxiv.org/abs/2511.01009
- **Reference count:** 40
- **Primary result:** SymRegg uses an equality graph to avoid redundant evaluations, achieving near-optimal performance compared to exhaustive search and outperforming genetic programming in efficiency and accuracy.

## Executive Summary
This paper introduces SymRegg, a symbolic regression algorithm that leverages an equality graph (e-graph) to detect and avoid redundant evaluations of semantically equivalent expressions. Unlike traditional genetic programming, which repeatedly evaluates identical expressions in different syntactic forms, SymRegg incrementally builds an e-graph to track all visited expressions and their equivalents. The method employs a simple iterative procedure: perturb sampled expressions, evaluate only unvisited ones, and apply equality saturation to generate equivalent forms. Experiments across multiple real-world datasets demonstrate that SymRegg achieves near-optimal performance compared to exhaustive search while significantly outperforming genetic programming in both efficiency and accuracy.

## Method Summary
SymRegg is a non-population-based symbolic regression algorithm that uses an equality graph (e-graph) to store visited expressions and detect semantically equivalent forms. The algorithm iteratively attempts to generate unvisited expressions using four prioritized methods: perturbing from the top 50 expressions per size, perturbing from the top 100 expressions overall, evaluating a random unevaluated e-class, and inserting a random expression. When a truly new expression is found, parameters are fit using a variable metric optimizer (100 iterations, 2 retries), and one step of equality saturation is applied to generate equivalent forms. The process continues until a maximum number of evaluations is reached, with the goal of finding expressions that minimize mean squared error on validation data.

## Key Results
- SymRegg achieves near-optimal performance compared to exhaustive search, finding expressions with MSE below threshold within 50,000 evaluations
- The algorithm significantly reduces redundant evaluations, avoiding the 60% redundancy typical in genetic programming
- Across four real-world datasets (Beer's law, Nikuradse 1, Nikuradse 2, Supernovae), SymRegg outperforms traditional genetic programming in both efficiency and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Redundancy Elimination via Equivalence Detection
The algorithm maintains an equality graph that groups expressions into equivalence classes. Before fitting parameters, it queries the e-graph to check if an expression (or equivalent variant) already exists. If found, evaluation is skipped, eliminating redundant computation. This assumes e-graph lookup overhead is lower than parameter fitting cost.

### Mechanism 2: Low-Cost Plateau Expansion via Single-Step Saturation
Instead of full saturation, SymRegg executes one step of rule application when inserting new expressions. This generates immediate syntactic variations (e.g., transforming `2x` into `x+x`) without requiring additional data fitting, expanding the neighborhood of solutions available for perturbation while preventing exponential graph growth.

### Mechanism 3: Implicit Exploration-Exploitation Balancing
The algorithm uses a four-tier generation strategy that naturally shifts between exploitation and exploration based on e-graph density. It prioritizes perturbation of top expressions when the graph is sparse, but falls back to random insertion as the graph saturates, creating a self-regulating mechanism that adapts to the current search state.

## Foundational Learning

- **Concept: Equality Saturation**
  - **Why needed here:** Equality saturation explores all equivalent rewrites of a program simultaneously, rather than iterating on a single solution
  - **Quick check question:** How does an e-graph differ from a standard Abstract Syntax Tree (AST) in terms of representing `x + x` and `2x`?

- **Concept: Neutrality in Genetic Programming**
  - **Why needed here:** The paper addresses the "neutrality problem" of navigating flat fitness landscapes where many expressions have identical fitness values
  - **Quick check question:** Why might having many equivalent expressions with the same fitness (neutrality) be both a hindrance (wasted compute) and a help (navigable plateaus) in search?

- **Concept: Symbolic Regression (SR)**
  - **Why needed here:** SR searches for mathematical formulas themselves, rather than fitting weights to fixed formulas
  - **Quick check question:** What is the primary unit of evaluation in SR that SymRegg tries to optimize (hint: it's not just the error, but the structure)?

## Architecture Onboarding

- **Component map:**
  - E-Graph Core -> Selector -> Perturber -> Evaluator
  - E-Graph Core stores all visited expressions, e-classes, and fitness metadata
  - Selector samples "top expressions" from the graph based on size/accuracy trade-offs
  - Perturber attempts mutations/recombinations, constrained to return None if the result exists in the E-Graph
  - Evaluator runs parameter optimization only when a truly new expression is confirmed

- **Critical path:**
  1. Sample parent expression from E-Graph
  2. Apply perturbation -> generate candidate
  3. Query E-Graph (Is candidate in an e-class?)
  4. If No: Fit Parameters -> Calculate Loss -> Insert Node -> Run 1-step Saturation
  5. If Yes: Discard and retry with next method (random insert, etc.)

- **Design tradeoffs:**
  - Completeness vs. Speed: Single-step saturation saves memory/CPU but may miss deep semantic equivalences
  - Memory: E-graph grows monotonically; potential mitigations include pruning or disk-spilling (not implemented)

- **Failure signatures:**
  - Stagnation: Repeatedly fails to generate unvisited expressions, defaulting to random insertion
  - OOM: E-graph grows exponentially if saturation rules are too aggressive or expression limit is too high

- **First 3 experiments:**
  1. Redundancy Baseline: Run standard GP and SymRegg on same dataset; log percentage of duplicate/skipped evaluations
  2. Saturation Sensitivity: Compare single-step vs. full saturation vs. no saturation on solution quality
  3. Scalability: Test performance as maximum expression size increases (length 10 vs. 15 vs. 20)

## Open Questions the Paper Calls Out

- Would a self-adaptive mechanism for selecting top expressions improve the exploration-exploitation balance compared to the currently empirically fixed sizes?
- Can the e-graph's pattern matching system be utilized to extract general templates of the data-generating function for mapping to first-principle models?
- How can the exponential growth of the e-graph be managed to prevent hardware memory limits from terminating the search in complex spaces?

## Limitations
- Performance claims rest on a small set of benchmark datasets and an idealized "Exhaustive" baseline
- Core efficiency assumption (lookup vs. fitting time) is not empirically validated
- Single-step saturation rule is justified by efficiency but not rigorously tested against alternatives
- Perturbation operators' exact implementation is underspecified

## Confidence
- **High:** The e-graph concept is well-established; claim that redundant evaluations exist and can be detected is sound
- **Medium:** Empirical advantage over GP is plausible but depends on benchmark representativeness
- **Low:** Claim that single-step saturation is sufficient is not experimentally validated; "natural shift" heuristic lacks ablation

## Next Checks
1. Measure actual lookup vs. fitting time ratios across expression sizes to verify core efficiency assumption
2. Compare one-step saturation against multi-step and full saturation on a synthetic neutrality test (flat fitness landscape)
3. Run ablation studies removing the top-50/top-100 tiers to test if exploitation-exploitation heuristic is necessary