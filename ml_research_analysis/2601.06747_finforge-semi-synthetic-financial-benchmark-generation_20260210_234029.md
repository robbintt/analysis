---
ver: rpa2
title: 'FinForge: Semi-Synthetic Financial Benchmark Generation'
arxiv_id: '2601.06747'
source_url: https://arxiv.org/abs/2601.06747
tags:
- financial
- question
- reasoning
- data
- finance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FinForge is a semi-synthetic pipeline for generating domain-specific
  financial benchmarks that combines expert-guided data curation with LM-based synthesis
  to address the scarcity of high-quality, contamination-free evaluation data in finance.
  The framework constructs a corpus of 100,000 verified documents (143M tokens) across
  11 subdomains, then uses controlled LM generation and rubric-based validation to
  produce FinForge-5k, a benchmark of 5,000 human-validated question-answer pairs.
---

# FinForge: Semi-Synthetic Financial Benchmark Generation

## Quick Facts
- arXiv ID: 2601.06747
- Source URL: https://arxiv.org/abs/2601.06747
- Authors: Glenn Matlin; Akhil Theerthala; Anant Gupta; Anirudh JM; Rayan Castilla; Yi Mei Ng; Sudheer Chava
- Reference count: 9
- Key outcome: Semi-synthetic pipeline generating 5,000 human-validated financial QA pairs from 100K curated documents reveals significant model performance gaps (73-77% accuracy) and validation limitations.

## Executive Summary
FinForge addresses the scarcity of high-quality, contamination-free financial benchmarks by combining expert-guided corpus curation with LM-based synthesis. The framework constructs a 143M-token corpus across 11 subdomains from authoritative sources, then uses a structured 5-stage pipeline to generate FinForge-5k, a benchmark of 5,000 human-validated question-answer pairs. Benchmarking reveals significant performance gaps in top models, particularly on quantitative and wealth management questions requiring conceptual financial reasoning, while also exposing limitations in LM-as-a-judge validation that necessitate human oversight.

## Method Summary
FinForge employs a two-stage pipeline: (1) expert-guided corpus curation from authoritative sources (textbooks, institutional research) to create a 100K document, 143M-token finance corpus; (2) controlled LM-based question synthesis using a 5-stage workflow with Gemini 2.5 Flash to generate and validate MCQA pairs. The generation pipeline includes document analysis for reasoning patterns, structured answer planning with difficulty ratings, question formulation with distractors, labeling by capability, and LM-as-judge validation filtering ~50% of initial pairs to produce 5,000 human-validated questions.

## Key Results
- Top models achieve 73-77% accuracy on FinForge-5k, with notable struggles on quantitative and wealth management subdomains
- LM-as-judge validation shows 100% approval rate versus 70% expert approval, revealing 30-point discrepancy in assessment sophistication
- 30% of expert-flagged issues involve missing contextual assumptions or ambiguous premises in generated questions
- Performance gaps highlight fundamental deficiencies in models' conceptual financial reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semi-synthetic benchmark generation from curated, authoritative sources can produce contamination-free evaluation datasets.
- **Mechanism:** By synthesizing novel question-answer pairs from verified documents excluded from common training corpora, the pipeline creates test items models are unlikely to have memorized while maintaining domain fidelity through source grounding.
- **Core assumption:** Questions from authoritative sources not typically in pretraining data will remain uncontaminated; models cannot generalize perfectly from partial exposure.
- **Evidence anchors:** Abstract states FinForge enables "scalable, contamination-free evaluation"; methodology deliberately excludes user forums and trivial stock data from authoritative sources.
- **Break condition:** If source documents appear in training corpora or models can synthesize correct answers from partial exposure.

### Mechanism 2
- **Claim:** A structured "planning-first" approach produces higher-quality, more faithful questions than direct generation.
- **Mechanism:** Multi-stage pipeline forces explicit reasoning about conceptual nucleus, difficulty level, and minimal context before question formulation, preventing surface-level pattern matching and ensuring multi-step reasoning requirements.
- **Core assumption:** Explicit planning captures reasoning structures lost in single-pass generation; LMs can reliably identify causal relationships in financial text.
- **Evidence anchors:** Abstract mentions "structured question generation and validation"; methodology describes identifying testable conceptual nucleus and assessing cognitive complexity.
- **Break condition:** If planning stage introduces systematic biases or generator fails to accurately extract conceptual nucleus from complex documents.

### Mechanism 3
- **Claim:** LM-as-a-judge validation is insufficient for complex financial reasoning; human expert oversight remains essential.
- **Mechanism:** Automated validation operates as boolean filter on surface-level criteria but lacks domain sophistication to detect missing contextual assumptions, ambiguous premises, or misaligned difficulty that human experts readily identify.
- **Core assumption:** Human experts possess tacit knowledge and contextual judgment current LMs cannot replicate for financial reasoning assessment.
- **Evidence anchors:** Abstract notes expert review shows LM validation "lacks sophistication"; methodology reveals 30-point discrepancy between LM (100%) and expert (70%) approval rates.
- **Break condition:** If human experts exhibit systematic blind spots or future LMs develop sufficient domain sophistication to match expert validation patterns.

## Foundational Learning

- **Concept: Semi-synthetic data generation**
  - **Why needed here:** FinForge's core innovation balances novelty (avoiding memorization) with domain fidelity (grounding in real financial reasoning). Understanding this tradeoff is essential for evaluating design choices.
  - **Quick check question:** If you trained a model on all publicly available finance textbooks, would questions generated from those same textbooks still constitute a valid evaluation? Why or why not?

- **Concept: Multi-stage LM pipelines with structured intermediates**
  - **Why needed here:** The 5-stage workflow is the architectural core. Understanding how structured intermediates constrain downstream generation is essential for debugging or extending the system.
  - **Quick check question:** What information must the "answer plan" stage produce to ensure the final question is self-contained and appropriately difficult?

- **Concept: Validation-reality gap in automated evaluation**
  - **Why needed here:** The 30-point discrepancy signals a fundamental limitation. Recognizing where automated validation systematically over-estimates quality prevents over-reliance on scalable but insufficient checks.
  - **Quick check question:** If you only had budget for 500 expert reviews, how would you strategically sample from 10,000 generated questions to maximize signal about pipeline quality?

## Architecture Onboarding

- **Component map:** Corpus quality → Document sampling strategy → Planning-stage accuracy → Validation filtering precision → Human expert bandwidth. The planning stage is the bottleneck; errors propagate through all downstream stages.
- **Critical path:** Expert taxonomy definition → Source whitelisting → Automated scraping → Quality filtering → Stratified sampling → Five-stage generation → LM validation → Expert review.
- **Design tradeoffs:** Gemini 2.5 Flash optimized for speed but produces ambiguous questions requiring stronger validation; full human review ensures quality but doesn't scale; self-containment ensures evaluability but may diverge from real-world scenarios.
- **Failure signatures:** Insufficient self-containment (requires external knowledge), ambiguous premises (missing assumptions), difficulty misalignment, conceptual errors (wrong methodology), arithmetic errors (calculation mistakes).
- **First 3 experiments:**
  1. Generator ablation: Run pipeline with higher-capability model (GPT-4o, Claude Sonnet) and compare expert validation rates.
  2. Validation-criteria dissection: Have experts categorize rejected questions by which validation criteria they fail.
  3. Contamination sanity check: Compare performance of models with different training data compositions on FinForge-5k.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can automated validation achieve human-expert agreement rates for complex financial reasoning questions?
- **Basis in paper:** Expert evaluation revealed 30% discrepancy between LM-as-judge (100% approval) and human experts (70% approval), with authors concluding that LM validation "lacks sophistication required to assess complex financial reasoning."
- **Why unresolved:** Speed-optimized validation creates "capabilities mismatch" with complex reasoning assessment needs.
- **What evidence would resolve it:** Validation system achieving >90% expert agreement with transparent, interpretable decision reasoning.

### Open Question 2
- **Question:** What training approaches can address conceptual financial reasoning failures versus arithmetic errors?
- **Basis in paper:** Authors distinguish conceptual failures (incorrect methodologies, flawed assumptions) from arithmetic ones, noting "Future studies should concentrate on addressing these conceptual misinterpretations."
- **Why unresolved:** External tools can mitigate arithmetic errors, but conceptual failures reflect "fundamental deficiency" in financial reasoning interpretation.
- **What evidence would resolve it:** Improved performance on multi-constraint financial tasks through targeted training interventions demonstrating transfer to novel scenarios.

### Open Question 3
- **Question:** How can generator model influence be isolated from evaluation outcomes to eliminate circular dependency?
- **Basis in paper:** Limitations section calls for "more robust strategies for isolating generator influence from evaluation outcomes" to address contamination when evaluating models from the generator's family.
- **Why unresolved:** Current approach requires excluding Gemini/Gemma models from primary results, limiting benchmark applicability to frontier models.
- **What evidence would resolve it:** Fair evaluation methodology via contamination detection, question transformation, or independent verification protocols showing equivalent performance to non-generator models.

### Open Question 4
- **Question:** Can validation filtering be made interpretable to enable targeted improvement of the question generation pipeline?
- **Basis in paper:** Black-box validation "obstructed the analysis of failed questions, thereby impeding the improvement of the generation pipeline."
- **Why unresolved:** Without understanding specific rejection reasons, systematic refinement of generation logic cannot be optimized.
- **What evidence would resolve it:** Interpretable validator providing actionable feedback enabling principled pipeline iteration.

## Limitations
- Contamination claims lack direct empirical validation against pretraining corpora
- LM-as-judge validation sophistication gap reveals fundamental limitations in automated assessment
- Generator capability tradeoffs (speed vs. quality) not empirically quantified against higher-capability models

## Confidence
- **High confidence:** Performance gaps between top models (73-77% accuracy) and struggles on quantitative/wealth management questions requiring conceptual reasoning
- **Medium confidence:** Effectiveness of semi-synthetic pipeline in producing contamination-free evaluation data, given limited direct evidence of source exclusion
- **Medium confidence:** Claim that planning-first generation produces higher-quality questions than direct generation, based on precedent but lacking direct A/B comparison
- **Medium confidence:** Conclusion that LM-as-a-judge validation is insufficient for complex financial reasoning, supported by expert validation discrepancy but not fully characterized

## Next Checks
1. **Contamination verification:** Systematically analyze whether FinForge source documents appear in common LLM training corpora by checking document fingerprinting against known pretraining datasets.
2. **Generator capability ablation:** Run the full pipeline with higher-capability models (GPT-4o, Claude Sonnet) as the question generator and compare expert validation rates.
3. **Validation-criteria dissection:** Have experts categorize rejected questions by which of the five validation criteria they fail to identify systematic blind spots in LM-as-judge scoring.