---
ver: rpa2
title: Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric
  Planning
arxiv_id: '2506.18847'
source_url: https://arxiv.org/abs/2506.18847
tags:
- learning
- keypoints
- latent
- reinforcement
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles long-horizon offline goal-conditioned reinforcement
  learning by learning a latent state space where distances model reachability. The
  method, Projective Quasimetric Planning (ProQ), jointly learns an encoder, an asymmetric
  quasimetric, and an out-of-distribution detector, then places a small set of keypoints
  that spread uniformly under Coulomb-like repulsion while staying in-distribution.
---

# Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning

## Quick Facts
- arXiv ID: 2506.18847
- Source URL: https://arxiv.org/abs/2506.18847
- Reference count: 40
- Primary result: Achieves up to 92% success rate on OGBench POINTMAZE Giant maze, outperforming six baselines

## Executive Summary
This paper tackles long-horizon offline goal-conditioned reinforcement learning by learning a latent state space where distances model reachability. The method, Projective Quasimetric Planning (ProQ), jointly learns an encoder, an asymmetric quasimetric, and an out-of-distribution detector, then places a small set of keypoints that spread uniformly under Coulomb-like repulsion while staying in-distribution. Navigation reduces to graph search between these keypoints, with actions chosen by a short-horizon policy. On the OGBench POINTMAZE suite, ProQ achieves high success rates—up to 92% on the largest maze—and outperforms six baselines in most settings. The learned mappings show crisp OOD boundaries and well-distributed keypoints, while ablation confirms the OOD barrier's importance for coverage and planning reliability.

## Method Summary
ProQ learns a latent space where asymmetric quasimetric distances approximate reachability, then places uniformly distributed keypoints within this space using Coulomb-like repulsion coupled with an out-of-distribution barrier. The method trains an encoder, quasimetric, and OOD detector jointly, then optimizes keypoint placement before constructing a graph for planning. During inference, it performs graph search to decompose long-horizon goals into sub-goals and uses a short-horizon policy to navigate between them. The approach specifically addresses the challenge of distributional shift in offline RL by ensuring keypoints remain in-distribution through the OOD classifier.

## Key Results
- Achieves 92% success rate on OGBench POINTMAZE Giant maze
- Outperforms six baselines including IQL, QRL, and TD3+BC across most maze sizes
- Ablation confirms OOD barrier prevents keypoint drift into unreachable space
- Keypoints show uniform coverage of navigable corridors while avoiding walls

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Distance as a Value Proxy
Learning an asymmetric distance (quasimetric) approximates the optimal goal-conditioned value function better than symmetric embeddings. The Interval Quasimetric Embedding (IQE) enforces the triangle inequality and non-negativity while allowing $d(x,y) \neq d(y,x)$. This captures the directional difficulty of transitions (e.g., descending vs. climbing). The loss function enforces local consistency (successors are distance 1) and global separation (non-successors are pushed apart).

### Mechanism 2: Physics-Inspired Uniform Coverage
Minimizing a Coulomb-like repulsion energy causes keypoints to uniformly cover the reachable manifold. Keypoints are treated as charged particles repelling each other via the inverse of the learned quasimetric distance ($1/d$). This pushes points apart to maximize coverage. Crucially, this is coupled with an "entropic barrier" derived from an Out-of-Distribution (OOD) classifier, preventing points from drifting into unreachable state space (walls/voids).

### Mechanism 3: Short-Horizon Graph Stitching
Decomposing long-horizon goals into a sequence of keypoint sub-goals mitigates compounding value-estimation errors. ProQ constructs a directed graph where nodes are keypoints and edge weights are quasimetric distances. It computes all-pairs shortest paths (Floyd-Warshall) once. During inference, rather than estimating value over 1000 steps, the agent identifies the best immediate keypoint neighbor that minimizes total path cost to the goal, and executes a short-horizon policy to reach it.

## Foundational Learning

- **Concept: Quasimetrics vs. Metrics**
  - **Why needed here:** Standard distances (Euclidean) assume $A \to B$ is the same cost as $B \to A$. In navigation (and RL), actions are often irreversible (e.g., falling off a ledge). You must understand that $d(x,y) \neq d(y,x)$ is a feature, not a bug.
  - **Quick check question:** If an agent moves from state $A$ to $B$ in 1 step but requires 10 steps to return, which distance metric property is violated?

- **Concept: Offline RL Distributional Shift**
  - **Why needed here:** The agent cannot interact with the environment to correct false beliefs. If the planner generates a sub-goal outside the dataset distribution (OOD), the policy will likely fail or output arbitrary actions.
  - **Quick check question:** Why does ProQ explicitly penalize keypoints that the OOD classifier $\psi$ assigns low probability to?

- **Concept: VicReg / Representation Learning**
  - **Why needed here:** Before learning distances, the encoder must map states to a latent space where meaningful distances can exist. VicReg prevents collapse (all states mapping to zero) without requiring negative pairs.
  - **Quick check question:** What would happen to the Coulomb repulsion mechanism if the encoder mapped all states to the exact same latent vector?

## Architecture Onboarding

- **Component map:** Encoder ($\phi$) -> Quasimetric ($d$) -> OOD Classifier ($\psi$) -> Keypoints ($\{z_k\}$) -> Actor ($\pi$)

- **Critical path:**
  1. Joint Training: Train $\phi, d, \psi$ concurrently using dataset transitions
  2. Negative Mining: Generate interpolated/extrapolated states to train $\psi$
  3. Keypoint Optimization: Freeze $\phi, d, \psi$; optimize $\{z_k\}$ using $L_{repel}$ and $L_{ood}$
  4. Graph Construction: Build graph $G$ using frozen keypoints and distances; run Floyd-Warshall

- **Design tradeoffs:**
  - Keypoint Count ($K$): Higher $K$ improves graph resolution but increases memory/compute for Floyd-Warshall ($O(K^3)$). Paper uses $K=100$.
  - Distance Cutoff ($\tau$): Edges with $d > \tau$ are removed to prevent using unreliable long-range distance estimates.

- **Failure signatures:**
  - Keypoint Drift: Red dots (keypoints) appearing in yellow regions (walls) in visualizations → OOD barrier $\lambda_{ood}$ is too weak
  - Teleportation Artifacts: Isolated green spots in OOD map → Stochastic transitions (portals) confusing the deterministic distance learner
  - Planning Loops: Agent oscillates between two keypoints → Graph disconnected or asymmetric distance estimates inconsistent with policy capability

- **First 3 experiments:**
  1. Visualize Latent Space: Render the OOD heatmap and keypoints on a 2D maze (e.g., `pointmaze-medium`) to verify coverage and barrier sharpness
  2. Ablate OOD Barrier: Train ProQ on `giant` maze without the $L_{ood}$ term (repulsion only) to confirm keypoints explode into unreachable space
  3. Vary Horizon: Test success rate as maze size increases (Medium → Giant) to verify that graph-based planning scales better than baseline value-bootstrapping methods

## Open Questions the Paper Calls Out

### Open Question 1
Can ProQ's keypoint learning mechanism be adapted to handle stochastic transitions without degrading coverage quality? The authors state "stochastic transitions seem to break the learning of the keypoints, as they degrade their spread" and observe in teleport environments that "the equilibrium seems harder to attain" in sparsely covered hallways. This remains unresolved because the current Coulomb-like repulsion and OOD barrier assume largely deterministic reachability; stochastic portals create inconsistent distance estimates that destabilize keypoint placement. Evidence would require demonstration of uniform keypoint coverage on stochastic navigation tasks (e.g., teleport mazes with random exits) matching deterministic baseline performance, potentially via probabilistic quasimetric variants or entropy-regularized placement.

### Open Question 2
Does ProQ scale to high-dimensional vision-based or manipulation tasks where the OOD classifier's interpolation-based negatives may fail to cover unreachable regions? The authors acknowledge "validating on richer manipulation or vision-based tasks is required for stronger evidence" and in Appendix D.1.3 note that interpolation/extrapolation sampling "only explore[s] the convex hull of pairs" leaving "unreachable points... that never get generated" in high dimensions. This is unresolved because the OOD detector's negative sampling strategy assumes low-dimensional state spaces where convex combinations approximate the boundary; pixel-based observations (n≃1000) violate this. Resolution would require successful ProQ application to vision-based benchmarks with analysis of OOD boundary sharpness, or proposal of alternative negative sampling that scales to high dimensions.

### Open Question 3
Can the energy function be modified to adapt keypoint density to local action complexity rather than enforcing uniform spacing? The authors propose as future work "adapting the energy function so that keypoints' density grows or shrinks with local action complexity would make ProQ scalable to high-DOF manipulation." This remains unresolved because the current Coulomb repulsion treats all latent regions equally; complex manipulation regions may require denser keypoints than simple corridors, but the quasimetric distance alone does not capture action-space difficulty. Evidence would require integration of action-entropy or policy-uncertainty terms into the repulsion energy, demonstrating improved performance on tasks with heterogeneous difficulty.

## Limitations
- The quasimetric learning mechanism relies on strong assumptions about the geometry of optimal value functions aligning with quasimetric structure
- The Coulomb repulsion mechanism lacks strong empirical validation compared to simpler coverage strategies
- Scalability of Floyd-Warshall ($O(K^3)$) with increasing keypoint count remains unaddressed for truly large-scale problems

## Confidence
- **High Confidence:** The overall architecture and training procedure are well-specified and reproducible. The reported success rates on OGBench POINTMAZE are likely accurate given the standardized benchmark.
- **Medium Confidence:** The quasimetric learning mechanism (IQE) is theoretically sound and the claimed advantages over symmetric embeddings are plausible based on the directional nature of RL transitions. The physics-inspired uniform coverage via Coulomb repulsion is intuitively appealing but lacks strong empirical validation compared to alternatives.
- **Low Confidence:** The long-term generalization of the learned quasimetric to unseen state pairs and the robustness of the OOD detector to complex distributional shifts in non-synthetic environments are not thoroughly tested.

## Next Checks
1. **OOD Detector Robustness:** Systematically vary the amount of out-of-distribution data in the training set and measure the impact on keypoint distribution and planning success. This tests the OOD detector's sensitivity and the method's reliance on its accuracy.
2. **Alternative Coverage Mechanisms:** Replace the Coulomb repulsion with a simpler coverage loss (e.g., maximizing pairwise distances or using a fixed grid of keypoints) and compare planning success rates and keypoint distribution uniformity. This validates the specific contribution of the physics-inspired approach.
3. **Scalability Analysis:** Incrementally increase the number of keypoints ($K$) and measure the wall-clock time for inference (Floyd-Warshall) and the marginal improvement in planning success. This quantifies the practical scalability limits of the graph-based approach.