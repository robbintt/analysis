---
ver: rpa2
title: Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation
arxiv_id: '2602.00681'
source_url: https://arxiv.org/abs/2602.00681
tags:
- audio
- text
- image
- retrieval
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of audio-to-image bird species
  retrieval in bioacoustic settings, where paired audio-image data is scarce. The
  authors propose a simple, data-efficient approach that uses text as a semantic intermediary:
  they distill the text embedding space of a pretrained image-text model (BioCLIP-2)
  into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder
  with a contrastive objective.'
---

# Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation

## Quick Facts
- **arXiv ID**: 2602.00681
- **Source URL**: https://arxiv.org/abs/2602.00681
- **Reference count**: 24
- **Primary result**: Audio-to-image bird species retrieval with 70.47% MAP on SSW60 using text distillation, outperforming zero-shot baselines

## Executive Summary
This paper addresses the challenge of audio-to-image bird species retrieval in bioacoustic applications, where paired audio-image data is scarce. The authors propose a simple, data-efficient approach that leverages text as a semantic intermediary: they distill the text embedding space of a pretrained image-text model (BioCLIP-2) into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective. This transfer enables emergent alignment between audio and image embeddings without using images during training. The resulting model achieves strong audio-to-image retrieval performance, with a mean average precision (MAP) of 70.47% on the SSW60 benchmark, outperforming baselines including random projection (3.79% MAP), text embeddings mapping (51.39% MAP), and cascaded zero-shot approaches (39.85% MAP). Crucially, the method preserves audio discriminative power while substantially improving audio-text alignment on both focal recordings and soundscape datasets.

## Method Summary
The approach involves distilling the text embedding space of a pretrained image-text model (BioCLIP-2) into a pretrained audio-text model (BioLingual). This is achieved by fine-tuning the audio encoder of BioLingual with a contrastive objective that aligns its audio embeddings with the text embeddings of BioCLIP-2. During training, only audio and text pairs are used, and no image data is involved. The fine-tuned audio encoder can then retrieve images by first mapping audio to the BioCLIP-2 text embedding space and then using the image encoder of BioCLIP-2 for image retrieval. This method leverages the semantic richness of text as an intermediary, enabling cross-modal retrieval without requiring paired audio-image data.

## Key Results
- Achieved 70.47% mean average precision (MAP) on SSW60 audio-to-image retrieval benchmark
- Outperformed random projection baseline (3.79% MAP) by a wide margin
- Surpassed text embeddings mapping (51.39% MAP) and cascaded zero-shot approaches (39.85% MAP)

## Why This Works (Mechanism)
The method works by leveraging the semantic richness of text embeddings as an intermediary between audio and image modalities. By distilling the text embedding space of a pretrained image-text model (BioCLIP-2) into a pretrained audio-text model (BioLingual), the approach transfers cross-modal alignment knowledge without requiring paired audio-image data. The contrastive fine-tuning aligns the audio encoder's output with the target text embedding space, enabling emergent audio-image alignment through shared text representation. This circumvents the data scarcity problem in bioacoustic settings while preserving the discriminative power of the audio encoder for species classification tasks.

## Foundational Learning

**Contrastive learning**
*Why needed*: To align audio embeddings with text embeddings without paired audio-image data
*Quick check*: Verify alignment loss decreases during training and retrieval performance improves

**Knowledge distillation**
*Why needed*: To transfer cross-modal alignment from a pretrained image-text model to an audio-text model
*Quick check*: Confirm retrieval performance exceeds both source and target models individually

**Cross-modal retrieval**
*Why needed*: To enable searching for images using audio queries in bioacoustic applications
*Quick check*: Validate retrieval accuracy on species not seen during training

## Architecture Onboarding

**Component map**
Audio encoder (BioLingual) -> Text encoder (BioCLIP-2) -> Image encoder (BioCLIP-2)

**Critical path**
Audio signal → BioLingual audio encoder → Aligned text embeddings → BioCLIP-2 image encoder → Retrieved images

**Design tradeoffs**
Uses text as intermediary to avoid paired audio-image data requirement, sacrificing some direct alignment precision for data efficiency and broader applicability

**Failure signatures**
Poor alignment if BioCLIP-2's text embeddings don't capture bird species semantics accurately; degraded audio discriminative power if fine-tuning is too aggressive

**First experiments**
1. Verify contrastive loss decreases during fine-tuning
2. Test retrieval on held-out species to confirm generalization
3. Compare audio classification accuracy before and after fine-tuning

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation focused on single benchmark (SSW60), limiting generalizability to other bioacoustic datasets
- Assumes BioCLIP-2's text embeddings provide taxonomically aligned representations, not empirically validated across all bird species
- Performance bounded by quality and coverage of pretrained models' training data

## Confidence

- **High confidence**: Strong empirical results show substantial improvement over baselines
- **Medium confidence**: Claims about emergent alignment and preservation of discriminative power lack cross-dataset validation and detailed ablations

## Next Checks

1. Validate the quality and taxonomic alignment of BioCLIP-2's text embeddings for bird species by comparing retrieval performance using different text embedding sources
2. Conduct cross-dataset evaluation to assess the method's robustness on unseen bioacoustic datasets and species not present in SSW60
3. Perform ablation studies to quantify the impact of text embedding quality on retrieval accuracy and confirm preservation of audio discriminative power on standard audio classification benchmarks