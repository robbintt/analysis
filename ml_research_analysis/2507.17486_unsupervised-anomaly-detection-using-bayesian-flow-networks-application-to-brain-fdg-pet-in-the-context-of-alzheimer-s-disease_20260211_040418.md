---
ver: rpa2
title: 'Unsupervised anomaly detection using Bayesian flow networks: application to
  brain FDG PET in the context of Alzheimer''s disease'
arxiv_id: '2507.17486'
source_url: https://arxiv.org/abs/2507.17486
tags:
- bayesian
- anomaly
- detection
- image
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnoBFN, the first application of Bayesian
  Flow Networks (BFNs) to unsupervised anomaly detection in medical imaging. The authors
  extend BFNs to handle conditional image generation under high spatially correlated
  noise while preserving subject specificity through a recursive Bayesian update scheme.
---

# Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease

## Quick Facts
- arXiv ID: 2507.17486
- Source URL: https://arxiv.org/abs/2507.17486
- Reference count: 34
- First application of Bayesian Flow Networks to unsupervised medical anomaly detection

## Executive Summary
This paper introduces AnoBFN, the first application of Bayesian Flow Networks (BFNs) to unsupervised anomaly detection in medical imaging. The authors extend BFNs to handle conditional image generation under high spatially correlated noise while preserving subject specificity through a recursive Bayesian update scheme. AnoBFN combines simplex noise with a novel accuracy schedule and integrates the original input image recursively throughout the generative process. Evaluated on Alzheimer's disease-related anomaly detection in FDG PET scans, AnoBFN outperforms state-of-the-art methods including β-VAE, f-AnoGAN, and AnoDDPM. On synthetic anomalies simulating hypometabolism, AnoBFN achieves an average precision of 48.17% and an IoU of 31.79%, representing significant improvements over baseline approaches.

## Method Summary
AnoBFN is a U-Net-based generative model that operates on the parameters of distributions rather than directly on images. It uses a modified Bayesian update scheme that incorporates the original input image recursively during the generative process, weighted by reconstruction error. The method employs simplex noise (spatially correlated) instead of Gaussian noise, combined with a novel accuracy schedule that maximizes variance at the start of inference. This allows the model to "fall back" onto a healthy data manifold learned during training, effectively removing anomalies while preserving subject-specific details. The model is trained only on healthy FDG PET scans and evaluated on both synthetic and real AD-related anomalies.

## Key Results
- AnoBFN achieves AP of 48.17% and IoU of 31.79% on synthetic AD-related hypometabolism anomalies
- Outperforms β-VAE (AP: 27.58%, IoU: 11.84%), f-AnoGAN (AP: 35.34%, IoU: 18.28%), and AnoDDPM (AP: 40.14%, IoU: 21.71%)
- Maintains high reconstruction quality with PSNR of 38.03 dB on healthy test scans

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured noise (simplex) enables the corruption of spatially coherent anomalies while preserving subject-specific details better than Gaussian noise.
- **Mechanism:** Simplex noise, which possesses spatial continuity and a gradient-based structure, aligns with the spatial correlation of pathological regions (e.g., hypometabolism). Unlike i.i.d. Gaussian noise, simplex noise allows the network to target specific regions for "forgetting" (anomaly removal) without erasing the subject's identity.
- **Core assumption:** Anomalies in FDG PET are spatially correlated and continuous rather than pixel-independent.
- **Evidence anchors:** Section 3 [C1.1] contrasts Gaussian noise's flat power spectrum with the spatial coherence of simplex noise; Wyatt et al. (AnoDDPM) similarly utilizes simplex noise for anomaly detection.
- **Break condition:** If anomalies are pixel-level or "salt-and-pepper" noise rather than structured regions, the spatial advantage of simplex noise diminishes.

### Mechanism 2
- **Claim:** A high-variance accuracy schedule allows the latent distribution of an abnormal image to overlap with the healthy training manifold.
- **Mechanism:** The authors modify the standard BFN accuracy schedule β(t) to ensure the probability flow distribution has maximal variance at the start of inference (t=T). This creates a "confused" prior that depends on the input but has wide uncertainty, allowing the generative process to "fall back" onto the healthy data manifold learned during training.
- **Core assumption:** A sufficiently high noise schedule creates a shared latent space where the distinction between healthy and abnormal inputs vanishes.
- **Evidence anchors:** Section 3 [C1.2] defines the schedule ensuring β(T) > 0 and maximal variance to encourage overlap.
- **Break condition:** If the anomaly is too severe or distinct, it might fall outside the high-variance prior's "cone of uncertainty," failing to revert to a healthy representation.

### Mechanism 3
- **Claim:** Recursive Bayesian feedback preserves subject specificity by selectively re-injecting input information during the generative process.
- **Mechanism:** AnoBFN modifies the standard Bayesian update function h to include a term α_{t,A} derived directly from the input image x_0, modulated by the squared error between the current prediction and the input. If the model predicts a pixel well (normal region), the input is heavily weighted (preserving specificity). If the error is high (anomaly), the input weighting drops, allowing the model to "hallucinate" a healthy alternative.
- **Core assumption:** High reconstruction error during the generative process correlates positively with the presence of an anomaly.
- **Evidence anchors:** Section 3 [C2] and Eq. (11) define the error-modulated weighting α_{t,A}.
- **Break condition:** If a normal region is difficult to reconstruct (e.g., complex texture), high error might incorrectly suppress input guidance, leading to artifacts.

## Foundational Learning

- **Concept:** **Bayesian Flow Networks (BFNs)**
  - **Why needed here:** Unlike standard diffusion (DDPM) which diffuses data x_0 directly, BFNs operate on the *parameters* θ of a distribution. Understanding that the latent variable is a belief state (mean μ and precision ρ) updated via Bayes' rule is required to comprehend the "Receiver" and "Sender" distributions.
  - **Quick check question:** Does the network predict the clean image directly, or the parameters of the distribution describing the clean image?

- **Concept:** **Unsupervised Anomaly Detection (UAD) via Reconstruction**
  - **Why needed here:** The entire premise rests on training on "healthy" data. If the model learns the distribution p(Healthy) perfectly, it should fail to reconstruct anomalies, making the residual map (Input - Reconstruction) the anomaly mask.
  - **Quick check question:** Why would a model trained only on healthy brains fail to reconstruct a tumor?

- **Concept:** **Simplex Noise**
  - **Why needed here:** Standard Gaussian noise destroys high-frequency details instantly. Simplex noise is a gradient noise function providing spatially coherent corruption. Understanding this is key to Section 3 [C1.1].
  - **Quick check question:** Does Gaussian noise introduce high-frequency or low-frequency perturbations compared to simplex noise?

## Architecture Onboarding

- **Component map:**
  - Input: FDG PET volume (128x128x128) -> 2D Slices
  - Network Ψ: Standard U-Net (3 downsampling stages, attention heads)
  - Latent State θ: Tuple of Mean μ and Precision ρ
  - Distributions: Sender p_S (adds noise to data), Receiver p_R (Network prediction), Update p_U (Bayesian fusion)
  - Novelty: The Recursive Update injects x_0 into the loop, and the Accuracy Schedule controls the noise variance

- **Critical path:**
  1. Corruption: Input x_0 is noised via Sender Distribution using Simplex noise
  2. Initialization: Initialize parameters θ_T using the new accuracy schedule (starts with high variance)
  3. Loop (T to 0):
     - Sample x̂_t from Receiver p_R (Network predicts clean image)
     - Recursive Update: Calculate error (x̂_t - x_0)² to determine weighting α_{t,A}
     - Bayesian Step: Fuse θ_{t+1}, x̂_t, and original input x_0 (weighted by α_{t,A}) to get θ_t
  4. Output: Final μ_0 is the pseudo-healthy reconstruction

- **Design tradeoffs:**
  - Noise Level vs. Identity: Increasing noise intensity helps remove large anomalies but risks changing the patient's anatomy (loss of subject specificity)
  - Error Sensitivity: The recursive update relies on reconstruction error to detect anomalies. If the network reconstructs an anomaly *too well* (low error), the mechanism fails to remove it

- **Failure signatures:**
  - Blurred Reconstructions: Indicates the model reverted to the population mean (over-regularized)
  - Retained Anomalies: Indicates the noise level was too low or the Recursive Update weighted the input too heavily in anomalous regions
  - False Positives (Normal removal): Indicates the model found high error in normal complex regions, suppressing necessary details

- **First 3 experiments:**
  1. Ablation on Noise Type: Compare standard BFN with Gaussian noise vs. AnoBFN with Simplex noise
  2. Ablation on Recursive Update: Compare AnoBFN vs. AnoBFN w/o [C2] to validate the specific contribution of the input feedback loop
  3. Schedule Validation: Visualize the trajectory of μ (as in Fig 2) to ensure the prior variance is indeed maximized at t=T and decreases appropriately

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can uncertainty quantification be formally integrated into AnoBFN's Bayesian generative process, and would it improve anomaly detection reliability?
- **Basis in paper:** [explicit] The conclusion states: "To enhance the robustness of anomaly detection, we aim to incorporate uncertainty quantification into the Bayesian generative process."
- **Why unresolved:** The current framework does not provide confidence estimates for reconstructions or anomaly maps, which could be clinically valuable.
- **What evidence would resolve it:** A modified AnoBFN with uncertainty outputs, evaluated on whether uncertainty correlates with detection accuracy and reduces false positives in ambiguous cases.

### Open Question 2
- **Question:** How well does AnoBFN generalize to real Alzheimer's disease patient scans, as opposed to synthetic anomalies?
- **Basis in paper:** [inferred] The evaluation uses only synthetic anomalies "simulating 30% AD-induced hypometabolism" due to "absence of ground truth masks," leaving real clinical performance unvalidated.
- **Why unresolved:** Synthetic anomalies may not capture the full complexity and variability of real AD-related hypometabolism patterns.
- **What evidence would resolve it:** Evaluation on real AD patient scans with clinical expert annotations or correlation with established AD biomarkers (e.g., CSF, cognitive scores).

### Open Question 3
- **Question:** How sensitive is AnoBFN's performance to the choice of accuracy schedule function and the scaling metric hyperparameters (logistic growth rate k=30, midpoint t_c=0.5)?
- **Basis in paper:** [inferred] The accuracy schedule was "inspired by" prior work and the scaling metric uses fixed hyperparameters without ablation or sensitivity analysis.
- **Why unresolved:** The schedule and scaling parameters may be suboptimal, and their impact on the balance between anomaly removal and subject specificity is unclear.
- **What evidence would resolve it:** Systematic ablation studies varying schedule functions and hyperparameters, measuring IoU and AP changes.

### Open Question 4
- **Question:** Does AnoBFN maintain its performance advantage when applied to 3D volumetric data and other medical imaging modalities beyond FDG-PET?
- **Basis in paper:** [explicit] The conclusion states: "We aim to evaluate the robustness and generalization of our approach by applying it to a broader range of medical imaging datasets, including diverse pathologies and imaging modalities."
- **Why unresolved:** Current experiments use only 2D axial slices from FDG-PET; 3D spatial context and modality-specific characteristics (e.g., MRI, CT) remain untested.
- **What evidence would resolve it:** Benchmarking AnoBFN on 3D volumes across multiple modalities (brain MRI, chest CT) with comparison to existing UAD methods.

## Limitations

- The method was evaluated only on synthetic anomalies, leaving real clinical performance on actual AD patients unvalidated
- Performance may be sensitive to specific hyperparameter choices (noise scale, accuracy schedule parameters, logistic decay parameters) that were not systematically explored
- Current implementation uses 2D slices rather than full 3D volumes, potentially missing important spatial context

## Confidence

- **High confidence:** The core mechanism of combining simplex noise with recursive Bayesian updates is well-defined and reproducible
- **Medium confidence:** The quantitative performance improvements over baselines are demonstrated but depend on specific hyperparameter choices
- **Low confidence:** The generalizability of results to other anatomical regions or imaging modalities beyond FDG PET

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the accuracy schedule parameters (s, T) and recursive update coefficients (k, tc) to identify robustness boundaries
2. **Alternative noise comparisons**: Replace simplex noise with other structured noise types (Perlin, fractal) while maintaining the recursive update mechanism to isolate the noise contribution
3. **Cross-domain validation**: Apply AnoBFN to MRI-based anomaly detection tasks (e.g., tumor segmentation) to test modality transferability beyond FDG PET