---
ver: rpa2
title: 'Advances in Pre-trained Language Models for Domain-Specific Text Classification:
  A Systematic Review'
arxiv_id: '2510.17892'
source_url: https://arxiv.org/abs/2510.17892
tags:
- text
- classification
- tasks
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review analyzed 41 articles on pre-trained
  language models for domain-specific text classification, covering 2018-2024. The
  study examined modern approaches like BERT, BioBERT, and SciBERT against traditional
  methods, focusing on challenges in specialized domains like biomedical and finance.
---

# Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review

## Quick Facts
- arXiv ID: 2510.17892
- Source URL: https://arxiv.org/abs/2510.17892
- Authors: Zhyar Rzgar K. Rostam; Gábor Kertész
- Reference count: 40
- 41 articles analyzed on PLMs for domain-specific text classification (2018-2024)

## Executive Summary
This systematic review examines the evolution and application of pre-trained language models (PLMs) for domain-specific text classification tasks across biomedical, financial, and scientific domains. The study identifies BioBERT and SciBERT as leading domain-specific models, demonstrating superior performance over general-purpose PLMs like BERT in specialized tasks. Key findings reveal that while PLMs achieve high accuracy through fine-tuning and prompt-based learning, challenges persist around computational costs, data availability, and ethical considerations. The review establishes a comprehensive taxonomy of techniques and highlights critical research directions for improving domain adaptation, model efficiency, and evaluation standardization.

## Method Summary
The review systematically analyzed 41 peer-reviewed articles published between 2018 and 2024, focusing on PLM applications in domain-specific text classification. The methodology employed PRISMA guidelines for systematic literature review, including comprehensive database searches, quality assessment, and thematic synthesis. Articles were categorized by domain (biomedical, finance, scientific), PLM architecture, fine-tuning strategies, and evaluation metrics. The review examined both traditional machine learning approaches and modern transformer-based models, with particular emphasis on comparative performance analysis across different specialized domains.

## Key Results
- BioBERT achieved highest F1 score of 0.74 in biomedical sentence classification tasks
- Domain-specific models (BioBERT, SciBERT) significantly outperform general-purpose models in specialized domains
- Computational costs and data availability remain major barriers to PLM adoption in specialized domains

## Why This Works (Mechanism)
PLMs leverage transformer architectures with attention mechanisms to capture contextual relationships in text. Domain-specific models undergo pretraining on specialized corpora (e.g., biomedical literature for BioBERT), allowing them to learn domain-specific vocabulary, terminology, and syntactic patterns. Fine-tuning adapts these pretrained representations to specific classification tasks using labeled data from the target domain. The self-attention mechanism enables models to weigh the importance of different words based on their context, capturing long-range dependencies that traditional models miss. Transfer learning allows knowledge from large general corpora to be applied to specialized tasks with limited labeled data.

## Foundational Learning
- **Transformer Architecture**: Why needed - replaces recurrent networks for better parallelization and long-range context capture; Quick check - verify model uses multi-head self-attention
- **Self-Attention Mechanism**: Why needed - captures word relationships regardless of distance; Quick check - confirm attention scores are computed for all token pairs
- **Domain-Specific Pretraining**: Why needed - adapts vocabulary and patterns to specialized terminology; Quick check - verify pretraining corpus matches target domain
- **Fine-Tuning vs. Prompting**: Why needed - different approaches for adapting pretrained models; Quick check - identify which adaptation method was used
- **BERT vs. Domain-Specific Variants**: Why needed - general vs. specialized knowledge representation; Quick check - compare F1 scores between general and domain-specific models
- **Transfer Learning**: Why needed - leverages knowledge from large corpora for specialized tasks; Quick check - verify model was pretrained on general corpus before domain adaptation

## Architecture Onboarding

**Component Map:**
General Corpus Pretraining -> Domain-Specific Pretraining -> Fine-Tuning/Prompting -> Classification Layer

**Critical Path:**
Pretraining (general) -> Pretraining (domain-specific) -> Task-Specific Fine-tuning -> Evaluation

**Design Tradeoffs:**
- Computational cost vs. model performance
- Data availability vs. model generalization
- Model size vs. inference efficiency
- Domain specificity vs. cross-domain applicability

**Failure Signatures:**
- Poor performance on domain-specific terminology
- Overfitting to training data
- High computational requirements for inference
- Bias amplification in sensitive domains

**First Experiments:**
1. Compare F1 scores of BERT vs. BioBERT on biomedical sentence classification
2. Evaluate fine-tuning vs. prompt-based learning on domain-specific datasets
3. Measure computational efficiency across different model sizes for inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain-specific PLMs be optimized to handle specialized vocabulary and unique grammatical structures while significantly reducing the computational costs associated with training and fine-tuning?
- Basis in paper: [explicit] The authors explicitly list "Optimization of Computational Resources" and "Enhanced Domain Adaptation Techniques" as necessary future research directions in Section 7.
- Why unresolved: Current state-of-the-art transformer models require substantial hardware resources, creating a barrier to entry for smaller institutions, and high computational costs remain a general challenge across domains (Section 3.5).
- What evidence would resolve it: The development of lightweight model architectures that maintain high F1 scores on domain-specific benchmarks (like BioBERT or FinBERT) but require a fraction of the current training/inference time and GPU memory.

### Open Question 2
- Question: To what extent can synthetic data generation techniques reliably supplement limited human-annotated data in low-resource domains without introducing the performance degradation associated with high-subjectivity tasks?
- Basis in paper: [explicit] In Section 7, the authors call for "Improved Data Utilization" through synthetic data generation to address data scarcity.
- Why unresolved: While Section 4.2.1 notes that synthetic data works well for low-subjectivity tasks, it performs poorly for high-subjectivity tasks, making the reliability of synthetic data for complex domain-specific applications uncertain.
- What evidence would resolve it: Empirical studies demonstrating that models trained predominantly on synthetic domain-specific data can achieve statistical parity with models trained on human-annotated data across both low and high-subjectivity tasks.

### Open Question 3
- Question: What standardized benchmarks and evaluation metrics are required to allow for consistent, comparative assessment of PLM performance across diverse specialized domains?
- Basis in paper: [explicit] Section 7 lists "Comprehensive Benchmarking and Evaluation" as a future direction, stating the need to "establish standardized benchmarks... to facilitate consistent and comparative assessment."
- Why unresolved: The review reveals that different domains use varying datasets and metrics (e.g., BLURB for biomedical vs. specific F1 scores for finance), making cross-domain comparison difficult.
- What evidence would resolve it: The creation and adoption of a unified benchmark suite containing tasks and datasets from multiple domains (e.g., biomedical, legal, nuclear) evaluated under a single standardized protocol.

## Limitations
- Analysis covers 41 articles, potentially missing emerging research in rapidly evolving field
- Focus on biomedical, finance, and scientific domains may limit generalizability to other specialized areas
- Computational cost comparisons lack standardized benchmarking across different hardware configurations

## Confidence

**High Confidence:**
- Domain-specific models significantly outperform general-purpose models in specialized tasks
- BioBERT achieves F1 score of 0.74 in biomedical classification
- Computational costs and data availability are major adoption barriers

**Medium Confidence:**
- Effectiveness of fine-tuning techniques and prompt-based learning
- Impact of domain-specific pretraining on overall performance
- Generalizability of findings across different specialized domains

## Next Checks
1. Conduct replication study with standardized datasets and evaluation metrics across different domain-specific PLMs
2. Perform ablation studies to quantify contribution of domain-specific pretraining vs. fine-tuning
3. Implement cost-benefit analysis comparing computational requirements across multiple hardware configurations