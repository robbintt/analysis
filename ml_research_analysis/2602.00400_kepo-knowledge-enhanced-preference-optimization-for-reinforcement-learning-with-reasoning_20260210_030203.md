---
ver: rpa2
title: 'KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning
  with Reasoning'
arxiv_id: '2602.00400'
source_url: https://arxiv.org/abs/2602.00400
tags:
- reasoning
- learning
- kepo
- distillation
- on-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of inducing explicit reasoning
  behaviors in vision-language models through reinforcement learning, particularly
  under sparse trajectory-level rewards. The core difficulty lies in two failure modes:
  ambiguous credit assignment due to sparse rewards and exploration collapse when
  reward-bearing trajectories are rarely encountered.'
---

# KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning

## Quick Facts
- **arXiv ID**: 2602.00400
- **Source URL**: https://arxiv.org/abs/2602.00400
- **Reference count**: 29
- **Primary result**: KEPO achieves 78.00% average OOD accuracy across 7 medical imaging modalities, outperforming GRPO (74.33%) and MM-GKD (73.67%) under single-source MRI training.

## Executive Summary
This paper addresses the challenge of inducing explicit reasoning behaviors in vision-language models via reinforcement learning, particularly under sparse trajectory-level rewards. The core difficulty lies in two failure modes: ambiguous credit assignment due to sparse rewards and exploration collapse when reward-bearing trajectories are rarely encountered. The authors propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that combines quality-gated on-policy distillation with knowledge-enhanced exploration. KEPO selectively applies dense teacher guidance only to high-quality trajectories to avoid noisy gradients from flawed reasoning contexts, and adaptively injects teacher-guided trajectories via rejection sampling when naive exploration fails. Evaluated on a challenging medical VQA benchmark with single-source training (MRI-only) and testing across seven other modalities, KEPO significantly improves training stability, accelerates coherent reasoning emergence, and achieves superior out-of-distribution generalization compared to standard reinforcement learning and on-policy distillation baselines.

## Method Summary
KEPO is a unified post-training framework that integrates quality-gated on-policy distillation with knowledge-enhanced exploration for RL with sparse rewards. The method samples G trajectories per input from a student policy, evaluates them with a binary reward function, and applies three key mechanisms: (1) quality-gated distillation that only applies dense teacher guidance to reward-positive trajectories, (2) knowledge-enhanced exploration that uses teacher hints to guide rejection sampling toward reward-bearing trajectories when all sampled trajectories fail, and (3) a unified objective that combines advantage-weighted RL with gated distillation and KL regularization. The approach addresses the learning cliff problem where policies rarely encounter reward-bearing trajectories by providing dense guidance on successful reasoning patterns while maintaining the on-policy nature of reinforcement learning.

## Key Results
- KEPO achieves 78.00% average accuracy across OOD modalities, outperforming GRPO (74.33%) and MM-GKD (73.67%) under the same low-resource setting
- Quality gating (τ=1) significantly improves performance compared to uniform distillation (τ=0), demonstrating the importance of filtering contextual noise
- Knowledge-enhanced exploration component contributes to improved OOD generalization, particularly for modalities requiring complex multi-step reasoning
- KEPO demonstrates superior training stability and faster emergence of coherent reasoning behaviors compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Quality-Gated Distillation Filters Contextual Noise
- **Claim**: Selectively applying dense teacher supervision only to reward-positive trajectories prevents gradient corruption from flawed reasoning contexts.
- **Mechanism**: An indicator function I(r_i ≥ τ) gates the distillation loss term, so only trajectories meeting the quality threshold receive token-level teacher guidance.
- **Core assumption**: Low-quality trajectories originate from early logical errors or hallucinations; distilling teacher outputs conditioned on these flawed contexts injects misaligned gradients.
- **Evidence anchors**: [abstract] "quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories"; [section 3.2.1] "distillation acts as a form of dense credit assignment that amplifies successful behaviors rather than competing with the global reward"

### Mechanism 2: Knowledge-Enhanced Exploration Escapes Learning Cliff
- **Claim**: Teacher-generated hints guide rejection sampling toward reward-bearing trajectories when naive on-policy exploration fails.
- **Mechanism**: When max(r_i) = 0 across all G sampled trajectories, the teacher generates a hint h ~ π_T(·|x, y) conditioned on input and ground-truth. The student then samples y_h ~ π_θ(·|x, h, y) and accepts only if r(y_h) > 0, repeating until success or budget exhaustion.
- **Core assumption**: Teacher can generate informative hints when given ground-truth answers, and conditioning on these hints increases the probability of producing reward-positive trajectories.
- **Evidence anchors**: [abstract] "knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories"; [section 3.2.2] "this mechanism helps the model escape the learning cliff while preserving the on-policy nature of reinforcement learning"

### Mechanism 3: Unified Objective Balances RL and Distillation
- **Claim**: Joint optimization of sparse RL rewards with quality-gated dense distillation provides complementary learning signals without conflict.
- **Mechanism**: The KEPO objective (Eq. 7) combines: (i) advantage-weighted policy optimization, (ii) quality-gated distillation KL term, and (iii) reference KL regularization.
- **Core assumption**: Dense distillation on reward-aligned samples reinforces rather than competes with the sparse RL objective.
- **Evidence anchors**: [abstract] "unified post-training framework that integrates" both components; [section 3.2.1, Eq. 7] Full mathematical formulation of the unified objective

## Foundational Learning

- **Concept: Group-based policy optimization (GRPO/RLOO)**
  - Why needed here: KEPO builds directly on group-wise advantage estimation and importance weight clipping; understanding these baselines is prerequisite.
  - Quick check question: How does GRPO compute advantages using group-wise normalization, and why does this reduce variance?

- **Concept: On-policy vs. supervised knowledge distillation**
  - Why needed here: KEPO modifies on-policy distillation with quality gating; distinguishing standard supervised KD from GKD-style on-policy methods is essential.
  - Quick check question: What is the key difference between supervised KD and on-policy KD in terms of which distribution generates the training trajectories?

- **Concept: Sparse reward RL and exploration failure**
  - Why needed here: The "learning cliff" phenomenon—where policies never encounter reward-bearing trajectories—is central to the problem motivation.
  - Quick check question: Why do sparse trajectory-level rewards cause exploration collapse, particularly in early training with cold-start policies?

## Architecture Onboarding

- **Component map**: Student policy π_θ (Qwen3-VL-2B) → Rollout buffer D (stores G=8 trajectories) → Reward function r(y) (binary format + accuracy) → Quality threshold τ (0 or 1) → Teacher policy π_T (Qwen3-VL-32B) → Rejection sampling budget B

- **Critical path**:
  1. Sample G trajectories from student → evaluate sparse rewards
  2. If max(r_i) = 0: trigger hint generation + rejection sampling loop
  3. Filter trajectories by r_i ≥ τ for distillation eligibility
  4. Compute combined loss: RL advantage term + gated distillation + KL penalty
  5. Single backward pass updates student policy

- **Design tradeoffs**:
  - τ=0 vs τ=1: Lower threshold includes more distillation samples but may introduce noise; higher threshold is stricter but provides fewer learning signals
  - Rejection budget B: Larger budget increases probability of finding positive samples but increases compute cost per batch
  - Hint conditioning uses ground-truth answer at training time (unavailable at test time)—this is exploration-time guidance, not inference-time

- **Failure signatures**:
  - All trajectories remain r=0 even with hints → hint quality poor or B too small
  - OOD accuracy degrades while ID improves → potential overfitting to source modality
  - Training instability or divergence → check KL coefficient β or distillation term scaling

- **First 3 experiments**:
  1. Reproduce GRPO baseline on 600 MRI samples to establish floor performance (reported: 74.33% OOD avg)
  2. Ablate quality gating: compare τ=0 vs τ=1 on KEPO-KE (no exploration component) to isolate distillation filtering effect
  3. Ablate exploration: compare KEPO-KE vs full KEPO to quantify contribution of hint-aware rejection sampling to OOD generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Exact operationalization of rejection sampling budget B and KL regularization coefficient β remain unspecified, limiting precise reproduction
- The paper assumes teacher hints consistently improve trajectory quality but does not validate this across all OOD modalities
- Quality gating threshold τ is stated as 0 or 1, but optimal setting across different reasoning complexity levels is unclear

## Confidence
- **High confidence**: Quality-gated distillation filtering contextual noise is well-supported by theoretical motivation and ablation studies
- **Medium confidence**: Knowledge-enhanced exploration shows empirical gains but relies on ground-truth-conditioned hints at training time
- **Low confidence**: Unified objective balance is mathematically sound but paper doesn't thoroughly explore sensitivity to hyperparameter tuning

## Next Checks
1. **Ablation of quality threshold τ**: Systematically evaluate KEPO performance across τ ∈ {0, 0.5, 1} to determine optimal gating strictness
2. **Exploration budget sensitivity**: Vary rejection sampling budget B and measure impact on training stability and OOD generalization
3. **Teacher hint quality analysis**: Measure correlation between hint informativeness and reward-positive trajectory generation rates across different OOD modalities