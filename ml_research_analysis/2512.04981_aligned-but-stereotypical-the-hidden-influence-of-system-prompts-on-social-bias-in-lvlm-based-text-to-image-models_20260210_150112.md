---
ver: rpa2
title: Aligned but Stereotypical? The Hidden Influence of System Prompts on Social
  Bias in LVLM-Based Text-to-Image Models
arxiv_id: '2512.04981'
source_url: https://arxiv.org/abs/2512.04981
tags:
- bias
- prompt
- system
- prompts
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates social bias in LVLM-based text-to-image
  models and finds that they produce significantly more biased outputs than non-LVLM-based
  models. The authors attribute this bias primarily to system prompts, which encode
  demographic priors that propagate into image generation.
---

# Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models

## Quick Facts
- arXiv ID: 2512.04981
- Source URL: https://arxiv.org/abs/2512.04981
- Reference count: 40
- LVLM-based T2I models exhibit significantly higher social bias than non-LVLM models, primarily due to system prompts encoding demographic priors

## Executive Summary
This paper reveals that system prompts—not user prompts—are the primary source of social bias in LVLM-based text-to-image models. Through extensive experiments on SANA and Qwen-Image, the authors demonstrate that default system prompts inject demographic associations that propagate through text embeddings into image generation. They introduce FAIRPRO, a training-free meta-prompting framework that leverages the LVLM's reasoning capabilities to self-audit and generate fairness-aware system prompts at test time, substantially reducing bias while maintaining text-image alignment.

## Method Summary
The authors propose FAIRPRO, a training-free framework that uses meta-prompting to generate fairness-aware system prompts. The method involves a single call to the LVLM with a meta-instruction that prompts the model to identify potential social stereotypes in the user prompt and default system prompt, then generate a revised system prompt that explicitly promotes diversity. This approach leverages the LVLM's internal representations of social concepts without requiring external classifiers or fine-tuning, allowing for dynamic, input-specific debiasing that outperforms static approaches.

## Key Results
- LVLM-based models produce significantly more biased outputs than non-LVLM-based models
- Removing system prompts shifts 27% of male-associated and 36% of female-associated occupations toward neutrality
- FAIRPRO reduces bias substantially (from 0.816 to 0.746 on SANA) while preserving text-image alignment
- Static debiasing approaches underperform dynamic, input-specific fairness prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** System prompts encode demographic priors that propagate through text embeddings into image generation.
- **Mechanism:** Default system prompts implicitly inject demographic associations when the LVLM interprets user prompts. These priors shift token-level probability distributions and reshape text embeddings used for cross-attention conditioning, ultimately biasing which demographic attributes appear in synthesized images.
- **Core assumption:** The text embeddings from the LVLM encoder causally influence demographic attributes in generated images through attention mechanisms.
- **Evidence anchors:**
  - [abstract] "Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis."
  - [Section 5.2] Agreement between decoded text bias and image bias: 64%, 55%, and 53% for occupation, simple, and context prompts.
  - [Section 5.3] Removing system prompts shifts 27% of male-associated and 36% of female-associated occupations toward neutrality.
  - [corpus] Related work (arXiv:2508.03199) confirms grammatical gender in prompts influences visual representations in T2I models.
- **Break condition:** If the LVLM encoder were bypassed entirely or if text embeddings were decorrelated from demographic priors before conditioning the image generator, this mechanism would not operate.

### Mechanism 2
- **Claim:** The LVLM's own reasoning capability can self-audit and construct fairness-aware system prompts via chain-of-thought prompting.
- **Mechanism:** A meta-instruction prompts the embedded LVLM to (1) identify potential social stereotypes in the user prompt and default system prompt, then (2) generate a revised system prompt that explicitly promotes diversity. This leverages the model's internal representations of social concepts without external classifiers or fine-tuning.
- **Core assumption:** The LVLM has sufficient world knowledge to recognize stereotypical associations and can express debiasing instructions in natural language.
- **Evidence anchors:**
  - [abstract] "FAIRPRO... leverages the LVLM's reasoning capabilities to dynamically construct debiasing instructions tailored to each input."
  - [Table 4 ablation] No-CoT variant achieves bias 0.816 (SANA) vs. FAIRPRO 0.746; removing reasoning degrades performance.
  - [corpus] Weak direct evidence—corpus papers on LLM bias focus on veracity bias and multi-agent propagation, not self-auditing for T2I.
- **Break condition:** If the LVLM's training data severely lacked fairness concepts or if the meta-instruction failed to elicit useful reasoning, self-auditing would produce superficial or ineffective prompts.

### Mechanism 3
- **Claim:** Dynamic, input-specific system prompts outperform static debiasing because bias patterns vary across occupations and contexts.
- **Mechanism:** FAIRPRO replaces the fixed `s_default` with `s_fair = LVLM(prompt_meta, u)`, generating a tailored instruction per user prompt. This allows the system to explicitly counter occupation-specific stereotypes rather than applying a one-size-fits-all correction.
- **Core assumption:** The fairness-aware system prompt can encode debiasing signals that the LVLM encoder will respect during embedding generation.
- **Evidence anchors:**
  - [Section 6.1] Equation (2): `s_fair = LVLM(prompt_meta, u)` with single-call CoT design.
  - [Table 4] Fixed hand-crafted prompt achieves bias 0.872 (SANA) vs. FAIRPRO 0.746—static approaches underperform.
  - [Table 1] Example: "An accountant" generates system prompt "ensuring representation of various genders, ages, racial and ethnic backgrounds."
  - [corpus] No direct corpus evidence on dynamic vs. static debiasing in T2I.
- **Break condition:** If the generated fairness prompt introduced contradictory signals or degraded text-image alignment beyond acceptable thresholds, the tradeoff would break.

## Foundational Learning

- **Concept: System prompts in LVLM-based T2I architectures**
  - Why needed here: The paper's central finding is that system prompts—not user prompts—are the primary bias source. Understanding their role is essential for any intervention.
  - Quick check question: Can you explain why removing the system prompt in SANA shifts many gender-skewed occupations toward neutrality?

- **Concept: Cross-attention conditioning in diffusion models**
  - Why needed here: Text embeddings condition the image generator through cross-attention; biased embeddings yield biased attention maps and thus biased images.
  - Quick check question: How would biased text embeddings manifest in the attention maps of a diffusion model?

- **Concept: Fair Discrepancy (FD) metric**
  - Why needed here: The paper quantifies bias as deviation from uniform demographic distribution; understanding FD is necessary to interpret all results tables.
  - Quick check question: Given `B_k = 1/|S_k| * Σ ||p_i,k - u_k||²`, what does a score of 0.9 indicate vs. 0.5?

## Architecture Onboarding

- **Component map:** User Prompt → Meta-Instruction → LVLM → Fairness-Aware System Prompt → Concatenation with User Prompt → LVLM Text Encoder → Text Embedding → Diffusion/Flow Model (Cross-Attention) → Generated Image

- **Critical path:** System prompt → text embedding geometry → cross-attention conditioning → demographic attributes in output. Intervening at the system prompt level (FAIRPRO's approach) is upstream of all other components.

- **Design tradeoffs:**
  - **Alignment vs. Fairness:** Pearson r = 0.948 correlation between CLIP score and bias score (Section 4.2). Reducing bias may slightly lower text-image alignment.
  - **Single-call vs. Two-call:** Two-call FAIRPRO matches single-call performance but adds inference overhead (Table 4).
  - **Input-specific vs. Fixed:** Dynamic prompts require LVLM invocation per request; fixed prompts are cheaper but less effective.

- **Failure signatures:**
  - FAIRPRO generates generic prompts like "ensure diversity" without occupation-specific reasoning → check meta-instruction formatting.
  - Alignment drops significantly (>5% CLIP score decrease) → the fairness prompt may be over-constraining; reduce CoT verbosity.
  - Bias reduction is inconsistent across demographic attributes → the LVLM may have stronger priors for some attributes (e.g., gender vs. body type).

- **First 3 experiments:**
  1. **Reproduce token probability analysis (Section 5.3):** Compare default vs. no-system-prompt conditions on 50 occupations; verify 20%+ shift toward neutrality.
  2. **Ablate CoT reasoning:** Run FAIRPRO without the "think about potential biases" instruction; expect bias score regression toward default (Δ ~0.05–0.07).
  3. **Stress-test on Rewritten prompts:** Apply FAIRPRO to Level 4 prompts that already contain injected demographic cues (Table 15); measure whether fairness prompts can override implicit stereotypes.

## Open Questions the Paper Calls Out
None

## Limitations
- **Model Generalizability:** FAIRPRO was validated only on SANA and Qwen-Image; broader validation across different LVLM architectures needed.
- **Bias Metric Validity:** Fair Discrepancy metric may not capture all forms of social bias, particularly intersectional patterns.
- **Self-Auditing Reliability:** FAIRPRO's effectiveness depends on the LVLM's training data and reasoning capabilities, which may vary.

## Confidence
- **High Confidence:** System prompts are the primary source of bias in LVLM-based T2I models (supported by multiple analytical approaches).
- **Medium Confidence:** FAIRPRO effectively reduces bias while preserving alignment (promising results but depends on LVLM's reasoning quality).
- **Low Confidence:** Generalizability to different LVLM architectures and the metric's ability to capture all forms of social bias.

## Next Checks
1. **Cross-Architecture Validation:** Test FAIRPRO on three additional LVLM-based T2I models with different system prompt formats to verify generalizability.
2. **Intersectional Bias Analysis:** Apply FAIRPRO to prompts combining multiple demographic attributes to assess effectiveness on intersectional bias patterns.
3. **Long-term Bias Stability:** Conduct a longitudinal study measuring bias scores across multiple generations of the same prompt to check for degradation or emergence of new bias patterns.