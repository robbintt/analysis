---
ver: rpa2
title: Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks
arxiv_id: '2509.24972'
source_url: https://arxiv.org/abs/2509.24972
tags:
- tasks
- visual
- target
- arxiv
- gripper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling robots to learn
  complex multi-step manipulation tasks from a single human demonstration, without
  requiring additional data collection, model training, or manual annotation. The
  authors propose a novel trajectory-replay-based imitation learning approach that
  decomposes a demonstration into atomic subtasks, aligns the robot to each subtask's
  keyframe using pre-trained vision-language models, and then executes the corresponding
  action sequence.
---

# Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks

## Quick Facts
- arXiv ID: 2509.24972
- Source URL: https://arxiv.org/abs/2509.24972
- Authors: Vijja Wichitwechkarn; Emlyn Williams; Charles Fox; Ruchi Choudhary
- Reference count: 27
- One-shot imitation learning for multi-step manipulation tasks without additional training or manual annotation, achieving 82.5% success on multi-step tasks

## Executive Summary
This paper addresses the challenge of enabling robots to learn complex multi-step manipulation tasks from a single human demonstration without requiring additional data collection, model training, or manual annotation. The authors propose a novel trajectory-replay-based imitation learning approach that decomposes a demonstration into atomic subtasks, aligns the robot to each subtask's keyframe using pre-trained vision-language models, and then executes the corresponding action sequence. The method leverages pre-trained models like Gemini 2.5 Pro for subtask decomposition and keyframe selection, and spatial perception encoders for visual alignment through keypoint matching. Evaluated on both multi-step and single-step manipulation tasks, the method achieves an average success rate of 82.5% on multi-step tasks and 90% on single-step tasks, outperforming or matching baselines that require manual annotations.

## Method Summary
The method operates through a three-component pipeline that requires no training. First, f_decompose uses Gemini 2.5 Pro VLM with gripper context to identify subtask boundaries and visual targets from the demonstration video. Second, f_keyframe selects bottleneck keyframes using Gemini 2.5 Pro. Third, f_align uses Spatial Perception Encoder features, SAM2 segmentation, and generalized ICP with RANSAC for pose estimation to align the robot to each subtask. The system records demonstration trajectories containing RGB-D wrist camera images, end-effector poses in SE(3), and gripper states. The approach is evaluated on a Franka Emika Panda robot with an Intel RealSense D435 wrist camera, with code available at https://github.com/vijja-w/AF-OSIL-MS.

## Key Results
- Achieved 82.5% average success rate on multi-step manipulation tasks
- Achieved 90% success rate on single-step manipulation tasks
- Outperformed or matched baselines requiring manual annotations
- Demonstrated computational efficiency compared to training-based approaches

## Why This Works (Mechanism)
The method works by decomposing complex multi-step tasks into atomic subtasks using vision-language models, then reconstructing the task through precise visual alignment and trajectory replay. The key insight is that pre-trained VLMs can understand the semantic structure of manipulation tasks and identify critical decision points (keyframes) without requiring manual annotation. By combining this semantic understanding with geometric alignment through spatial perception encoders and RANSAC-based pose estimation, the system can replicate human demonstrations accurately while avoiding the need for expensive retraining or annotation.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Understand manipulation tasks and decompose demonstrations into subtasks. Needed for semantic understanding without manual annotation. Quick check: Can VLM correctly identify subtask boundaries in diverse manipulation scenarios.
- **Spatial Perception Encoding**: Extract geometric features from RGB-D data for precise alignment. Needed for accurate camera-to-object pose estimation. Quick check: Does encoder produce consistent features across similar object configurations.
- **RANSAC-based Pose Estimation**: Robustly estimate camera-to-object transformations in presence of outliers. Needed for reliable alignment despite noise in point clouds. Quick check: Can RANSAC recover correct pose when significant outliers present.
- **Generalized ICP**: Refine initial pose estimates through iterative closest point matching. Needed for precise final alignment after initial VLM-guided positioning. Quick check: Does ICP converge to accurate pose within reasonable iteration count.
- **Trajectory Replay**: Execute pre-recorded end-effector trajectories after alignment. Needed to reproduce the demonstrated manipulation actions. Quick check: Can robot follow recorded trajectory accurately after alignment.

## Architecture Onboarding

**Component Map**: Gemini VLM (decompose) -> Gemini VLM (keyframe) -> Spatial Perception Encoder + SAM2 -> RANSAC ICP -> Franka Panda execution

**Critical Path**: Demonstration recording -> f_decompose (VLM) -> f_keyframe (VLM) -> f_align (geometric) -> Trajectory execution

**Design Tradeoffs**: 
- No training required but depends on external VLM API reliability
- Open-loop execution simplifies control but lacks error recovery
- Uniform mask sampling introduces noise vs. more complex keypoint extraction
- Real-time processing vs. accuracy in alignment convergence

**Failure Signatures**:
- VLM misclassification leading to incorrect subtask boundaries
- Alignment overshooting causing visual target to exit field of view
- RGB-D shadows degrading point-cloud quality for pose estimation
- Grasp slippage on irregular object geometries during execution

**First Experiments**:
1. Record demonstration of simple pick-and-place task and verify VLM decomposition accuracy
2. Test alignment component with known object poses to validate pose estimation accuracy
3. Execute single-step task to verify trajectory replay after alignment

## Open Questions the Paper Calls Out
- **3D Reconstruction Integration**: Can incorporating 3D reconstruction methods enable generalization to unseen objects and improve robustness to occlusion? The current method struggles with novel object geometries and partial occlusion.
- **Semantic Keypoint Extraction**: Does replacing uniform mask sampling with semantic keypoint extraction reduce noise and improve alignment accuracy? The current approach's uniform sampling contributes to higher iteration counts.
- **VLM Task Completion Checking**: Can VLM integration for task completion checking and re-routing handle identical object instances and out-of-view targets? The system currently cannot distinguish between identical objects or recover if the target is lost.
- **Incremental Alignment Strategy**: Does an incremental alignment strategy that recomputes relative camera motion online mitigate the overshooting failures observed during the alignment phase? The current implementation can move the camera too aggressively.

## Limitations
- Method fails with objects having irregular shapes and concave geometries due to RGB-D shadows affecting point-cloud reconstruction
- Fully open-loop execution with no error recovery mechanism limits robustness to perturbations
- Performance degrades with occlusion and objects not seen during demonstration
- Dependency on external VLM API creates potential reliability and cost concerns

## Confidence
- **High confidence** in VLM-based decomposition and keyframe selection accuracy
- **Medium confidence** in geometric alignment component given unspecified RANSAC parameters
- **Low confidence** in overall system robustness to challenging object geometries and lighting conditions

## Next Checks
1. Implement incremental alignment with online recomputation to test mitigation of overshooting failure mode
2. Evaluate performance with synthetic shadow artifacts in depth images to quantify impact on pose estimation
3. Test method on objects with irregular shapes and concave geometries to assess generalization beyond experimental setup