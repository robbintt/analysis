---
ver: rpa2
title: 'Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling'
arxiv_id: '2510.14717'
source_url: https://arxiv.org/abs/2510.14717
tags:
- batch
- size
- learning
- rate
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Seesaw introduces a principled batch size scheduling algorithm
  for large language model pretraining, addressing the lack of theoretical grounding
  in existing batch ramp strategies. The method doubles the batch size whenever the
  learning rate would halve in standard schedulers, reducing serial training steps
  while preserving loss dynamics.
---

# Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling

## Quick Facts
- arXiv ID: 2510.14717
- Source URL: https://arxiv.org/abs/2510.14717
- Authors: Alexandru Meterez; Depen Morwani; Jingfeng Wu; Costin-Andrei Oncescu; Cengiz Pehlevan; Sham Kakade
- Reference count: 40
- Primary result: Matches cosine decay at equal FLOPs while reducing wall-clock time by ~36% on 150M/300M/600M-parameter models

## Executive Summary
Seesaw introduces a principled batch size scheduling algorithm for large language model pretraining, addressing the lack of theoretical grounding in existing batch ramp strategies. The method doubles the batch size whenever the learning rate would halve in standard schedulers, reducing serial training steps while preserving loss dynamics. Theoretically, it establishes the first finite-sample equivalence between learning rate decay and batch size ramp-up for SGD on noisy linear regression, extending this to normalized SGD as a proxy for Adam under a variance-dominated regime. Empirically, Seesaw matches cosine decay performance at equal FLOPs while reducing wall-clock time by approximately 36% on 150M/300M/600M-parameter models trained at Chinchilla scale, approaching the theoretical limit implied by the analysis.

## Method Summary
Seesaw is a drop-in replacement for cosine learning rate schedulers that coordinates batch size increases with learning rate decreases. When a standard cosine scheduler would cut the learning rate by factor α, Seesaw instead cuts it by √α and multiplies the batch size by α. This maintains approximately constant effective learning rate while reducing the number of serial training steps. The method is implemented as an adapter to existing cosine schedulers, requiring only modification of the (learning rate, batch size) pairs at decay points. The theoretical foundation relies on proving finite-sample equivalence between learning rate decay and batch size ramp-up under variance-dominated conditions, with normalized SGD serving as a tractable proxy for Adam.

## Key Results
- Matches cosine decay validation loss at equal FLOPs across 150M/300M/600M models
- Reduces serial training steps by ~36%, approaching the theoretical limit of (1-2/π)
- Works effectively at or below critical batch size (CBS ≈ 256k tokens for 150M model)
- Performance degrades when batch size exceeds CBS due to violation of variance-dominated assumption

## Why This Works (Mechanism)

### Mechanism 1: SGD Learning Rate-Batch Size Equivalence
For SGD on noisy objectives, doubling the batch size produces loss dynamics within a constant factor of halving the learning rate when both process the same number of samples. First-order Taylor expansion shows the two processes are equivalent up to O(η²): (η/2, 2B) two-step process and (η, 2B) single-step process have matching deterministic drift and noise covariance. Larger batches reduce gradient variance (∝ 1/B), compensating for smaller per-step updates from lower learning rate.

### Mechanism 2: Normalized SGD Extension via Variance-Dominated Regime
For normalized SGD (NSGD, a tractable Adam proxy), equivalence requires α₁√β₁ = α₂√β₂, so halving LR (α=2) maps to doubling batch (β=2) with √2 LR reduction (η ← η/√α). Under variance dominance, E[‖gt‖²] ≈ σ²/B·Tr(H), reducing NSGD update to SGD with rescaled learning rate η̃ ∝ η√B. The √α factor in LR reduction compensates for √β factor in effective LR increase from larger batches.

### Mechanism 3: Theoretical Speedup Bound Under Cosine Decay
The maximum achievable serial step reduction under cosine decay with aggressive (α=√β) scheduling is (1-2/π) ≈ 36.3%. Continuous approximation integrates the normalized LR schedule: ∫₀ᵀ cos(πt/2T) dt = 2T/π steps for batch-ramp vs T steps baseline. Aggressive schedules are limited by divergence constraint α ≥ √β.

## Foundational Learning

- **Bias-Variance Decomposition in SGD**: The theoretical analysis decomposes risk into bias (distance from optimum) and variance (noise-induced fluctuation). Understanding when each dominates explains why batch ramp works early (variance-dominated) but fails late (bias-dominated). Quick check: At training step t, if you double the batch size, which component of risk decreases?

- **Critical Batch Size (CBS)**: Seesaw only matches cosine decay at or below CBS. Beyond CBS, Assumption 2 fails, and no batch-ramp schedule can replicate LR decay dynamics. Quick check: If your current batch size is 2× the estimated CBS, should you apply Seesaw?

- **Normalized Gradient Descent vs. Adam**: Seesaw's √2 LR reduction rule derives from NSGD, not vanilla SGD. NSGD approximates Adam by replacing per-coordinate adaptive preconditioning with scalar gradient normalization. Quick check: In NSGD, what happens to the effective learning rate if gradient norms double?

## Architecture Onboarding

- **Component map**: Standard Cosine Scheduler → [Seesaw Adapter] → Modified (η, B) pairs → Training Loop

- **Critical path**: Identifying decay points S from cosine schedule → implementing atomic (η, B) updates → ensuring data loader rescales without gradient accumulation artifacts

- **Design tradeoffs**: Aggressiveness (α vs β): α=√β is maximally aggressive but risks instability if variance assumption weak; conservative choice α=2, β=1 is safer but slower. Batch scaling limit: Must stay ≤ CBS. Memory vs. speed: Doubling batch requires proportional memory; Seesaw trades memory headroom for wall-clock reduction.

- **Failure signatures**: Divergence late in training: Likely α < √β or initial η too high. Loss mismatch vs. cosine at same FLOPs: Batch size exceeded CBS. Z-loss instability: Figure 7 shows z-loss spikes near end of 600M training.

- **First 3 experiments**:
  1. Baseline equivalence test: Train 150M model with cosine vs. Seesaw at B₀=256. Sweep η ∈ {0.003, 0.01, 0.03}. Target: final loss delta < 0.01 at same token count.
  2. CBS boundary probe: Fix B₀=512, 1024, 2048 for 150M model. Compare Seesaw vs. cosine. Expect: B₀≤512 matches, B₀≥1024 diverges.
  3. Aggressiveness ablation: At fixed α√β=2, test (α,β) ∈ {(2,1), (√2,2), (1,4)}. Target: α=√β matches baseline; α<√β diverges; α>√β slower but stable.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does a modified scaling rule exist to match cosine decay performance when gradient norms are mean-dominated (very large batch sizes), or is learning rate decay strictly necessary? The paper provides only a 1D toy example to suggest limitations but no general solution.

- **Open Question 2**: Can the finite-sample equivalence between learning rate decay and batch size ramp-up be rigorously proven for the full Adam optimizer? The paper uses NSGD as a tractable proxy but acknowledges Adam's per-coordinate adaptivity differs from scalar normalization.

- **Open Question 3**: How must the Seesaw scheduler be adapted to stabilize z-loss dynamics during the final phases of training? The paper notes "instabilities in the z-loss towards the end of training when using Seesaw" and explicitly states this requires future work.

## Limitations

- **Variance-Dominated Regime Assumption**: The theoretical extension to normalized SGD relies critically on Assumption 2—that gradient norms are dominated by label noise variance. This breaks down beyond critical batch size.

- **Critical Batch Size Estimation**: While the paper mentions Seesaw should not exceed CBS, it doesn't provide practical methods for estimating CBS during training or model development.

- **Convergence Guarantee Gaps**: The theoretical analysis establishes equivalence under idealized conditions, but the extension to full Adam training lacks formal convergence guarantees.

## Confidence

- **High Confidence**: The empirical wall-clock speedup results (~36% reduction matching theoretical limit) and basic SGD learning rate-batch size equivalence theorem are well-supported.
- **Medium Confidence**: The extension to normalized SGD and practical applicability to Adam-based training is supported by empirical results but relies on assumptions that may not hold universally.
- **Low Confidence**: The precise characterization of when variance-dominated assumption breaks down and lack of practical CBS estimation methods represent significant theoretical gaps.

## Next Checks

1. **CBS Boundary Characterization**: Systematically vary batch size across different model scales to map the precise boundary where Seesaw performance diverges from cosine decay.

2. **Variance Dominance Measurement**: Implement diagnostic tools to measure the ratio of gradient variance to mean gradient norm during training to empirically validate the key theoretical assumption.

3. **z-loss Coordination**: Investigate whether z-loss can be modified to scale appropriately with Seesaw's LR/batch coordination, or whether alternative stabilization techniques are needed for large-scale training.