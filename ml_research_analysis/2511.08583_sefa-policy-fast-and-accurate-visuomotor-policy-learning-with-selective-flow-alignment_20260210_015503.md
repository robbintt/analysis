---
ver: rpa2
title: 'SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow
  Alignment'
arxiv_id: '2511.08583'
source_url: https://arxiv.org/abs/2511.08583
tags:
- policy
- diffusion
- sefa
- action
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SeFA-Policy addresses the challenge of efficient and accurate visuomotor
  policy learning by introducing Selective Flow Alignment (SeFA), which resolves the
  observation-action inconsistency problem in flow-based policies. The method leverages
  expert demonstrations to selectively correct generated actions during the reflow
  process, maintaining consistency with visual observations while preserving action
  diversity.
---

# SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment

## Quick Facts
- arXiv ID: 2511.08583
- Source URL: https://arxiv.org/abs/2511.08583
- Reference count: 40
- SeFA-Policy achieves 98.7% acceleration vs Diffusion Policy with 62.3% success rate vs 38.9%

## Executive Summary
SeFA-Policy addresses the challenge of efficient and accurate visuomotor policy learning by introducing Selective Flow Alignment (SeFA), which resolves the observation-action inconsistency problem in flow-based policies. The method leverages expert demonstrations to selectively correct generated actions during the reflow process, maintaining consistency with visual observations while preserving action diversity. This selective alignment strategy ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference.

Extensive experiments demonstrate that SeFA-Policy achieves superior accuracy and robustness across both simulated and real-world manipulation tasks, while reducing inference latency by over 98% compared to state-of-the-art diffusion-based and flow-based policies. The method combines rectified flow for straightening probabilistic paths with a novel selective alignment mechanism that corrects observation-action mismatches during training.

## Method Summary
SeFA-Policy trains a base flow policy using flow matching loss, then generates noise-action couplings to train a reflow policy that straightens flow paths. The key innovation is Selective Flow Alignment (SeFA), which searches expert demonstrations for nearest-neighbor actions and replaces generated actions when within a threshold distance. This preserves consistency with observations while maintaining multimodality. The final policy is trained on these selectively aligned couplings for one-step inference using an Euler solver.

## Key Results
- Achieves 98.7% latency reduction compared to Diffusion Policy
- 62.3% average success rate vs 38.9% for Diffusion Policy across 66 tasks
- 1-step inference performs on par with 100-step solvers while being 100x faster
- Reduces performance variance across different noise seeds, improving robustness

## Why This Works (Mechanism)

### Mechanism 1: Straight Flow Paths via Rectified Flow (Reflow)
One-step action generation is enabled by straightening probabilistic flow paths between noise and action distributions using rectified flow. A base policy approximates the drift vector field, then a reflow step retrains using generated couplings to create straighter trajectories that can be simulated with a single Euler step.

### Mechanism 2: Selective Flow Alignment (SeFA) for Observation-Action Consistency
A selective alignment strategy corrects actions that deviate from expert demonstrations by finding nearest-neighbor actions in the dataset. When Euclidean distance falls below threshold δ, the generated action is replaced with ground-truth, preventing error accumulation during reflow while preserving multimodal actions.

### Mechanism 3: Efficiency from One-Step Inference with Observation-Aligned Actions
The combination of straight flow paths and consistency correction enables inference with a single-step Euler solver, reducing latency by over 98%. The method ensures numerical accuracy through path straightening while alignment compensates for any loss of detail from not using iterative solvers.

## Foundational Learning

- **Rectified Flow (Reflow)**: Why needed: Core technique for achieving one-step inference by straightening probabilistic paths from noise to data. Quick check: How does retraining a flow model on its own generated noise-action couplings lead to straighter sampling paths?

- **Observation-Action Consistency in Imitation Learning**: Why needed: Key problem SeFA solves is decoupling of visual observations from generated actions during distillation. Quick check: In standard reflow, why might a generated action become inconsistent with the visual observation it was conditioned on?

- **Inference Latency vs. Action Horizon**: Why needed: Paper's primary claim is dramatic speedup; understanding NFE-performance trade-off is critical. Quick check: How does number of function evaluations during inference directly impact real-time robot control capabilities?

## Architecture Onboarding

- **Component map**: Base Policy Network -> Reflow Policy Network -> SeFA Alignment Module -> SeFA Policy Network
- **Critical path**: Base Policy Training → Coupling Generation (with Base) → Reflow Policy Training → Coupling Generation (with Reflow) → SeFA Alignment → Final SeFA Policy Training
- **Design tradeoffs**: More reflow iterations straighten paths further but risk amplifying errors; alignment threshold δ balances refinement vs efficiency; solver steps tradeoff accuracy vs efficiency
- **Failure signatures**: Reflow can degrade accuracy if base policy is weak; SeFA should recover this; high variance across noise seeds indicates sampling instability; 1-step inference may fail if paths aren't sufficiently straight
- **First 3 experiments**: 1) Baseline comparison on Meta-World assembly task reproducing success rate and latency; 2) Ablation on reflow and SeFA showing performance recovery on Adroit pen; 3) Robustness to noise seeds showing reduced variance on Adroit tasks

## Open Questions the Paper Calls Out

### Open Question 1
Can the distance threshold ε be determined adaptively rather than heuristically? The paper states ε is selected heuristically to balance refinement capability and efficiency, but automation for varying action spaces remains unresolved.

### Open Question 2
How does Euclidean nearest-neighbor search scale to high-dimensional or sparse datasets? The method may fail in extremely high-dimensional action spaces where Euclidean distances become less meaningful.

### Open Question 3
Does alignment to expert ground truth limit policy's ability to outperform expert? The method may discard potentially superior generated actions or propagate errors from sub-optimal expert data.

## Limitations
- Euclidean distance in action space may not generalize well to high-dimensional or discontinuous action spaces
- Reflow process can amplify errors if base policy is insufficiently accurate
- Performance depends heavily on proper tuning of the alignment threshold δ

## Confidence
- **High confidence**: Efficiency claims (98% latency reduction, 1-step inference) well-supported by ablation studies
- **Medium confidence**: Accuracy improvements robust across benchmarks but depend on hyperparameter tuning
- **Low confidence**: Euclidean distance as proxy for observation-action consistency lacks strong empirical justification

## Next Checks
1. Systematically vary alignment threshold δ to determine optimal performance and assess sensitivity
2. Evaluate SeFA-Policy on tasks with complex, multi-modal action distributions to test limits of 1-step inference
3. Assess performance degradation when expert demonstration dataset size is reduced to evaluate robustness