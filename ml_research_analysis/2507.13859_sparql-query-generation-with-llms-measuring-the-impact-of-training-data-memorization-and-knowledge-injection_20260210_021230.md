---
ver: rpa2
title: 'SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization
  and Knowledge Injection'
arxiv_id: '2507.13859'
source_url: https://arxiv.org/abs/2507.13859
tags:
- sparql
- llms
- knowledge
- query
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates the performance of Large Language Models
  (LLMs) in generating SPARQL queries from natural language questions over Knowledge
  Graphs, focusing on the impact of memorization and knowledge injection. It introduces
  a method evaluating SPARQL query generation under three conditions: zero-shot, knowledge
  injection, and anonymized knowledge injection.'
---

# SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection

## Quick Facts
- **arXiv ID**: 2507.13859
- **Source URL**: https://arxiv.org/abs/2507.13859
- **Reference count**: 35
- **Primary result**: Knowledge injection significantly improves SPARQL query generation performance across multiple LLMs, with Qwen 2.5 72B, Mistral-Large, and Llama 3.3 70B showing the best results

## Executive Summary
This paper investigates how Large Language Models perform in generating SPARQL queries from natural language questions over Knowledge Graphs, with particular focus on the effects of training data memorization and knowledge injection techniques. The authors introduce a novel experimental framework that evaluates SPARQL query generation under three conditions: zero-shot prompting, knowledge injection, and anonymized knowledge injection. Through extensive experiments on QALD-9-plus and MCWQ datasets using 11 different LLMs ranging from 7B to 123B parameters, the study reveals that while knowledge injection significantly improves performance, models still struggle with zero-shot prompting and exhibit signs of memorization, particularly on well-known datasets.

## Method Summary
The research introduces a comprehensive evaluation framework for SPARQL query generation that systematically compares three prompting conditions: zero-shot (no additional context), knowledge injection (providing KG schema and example queries), and anonymized knowledge injection (removing identifiable information from knowledge sources). The study employs 11 LLMs of varying sizes (7B to 123B parameters) and tests them on two established datasets: QALD-9-plus and MCWQ. The evaluation measures both exact match accuracy and the presence of memorized patterns by comparing outputs across different knowledge injection conditions. The methodology includes careful prompt engineering, controlled test conditions, and systematic analysis of memorization effects through anonymized variants of knowledge graphs.

## Key Results
- Knowledge injection consistently improves SPARQL generation performance across all tested LLMs, with the largest improvements seen in models with 70B+ parameters
- Qwen 2.5 72B, Mistral-Large, and Llama 3.3 70B achieve the highest overall performance scores when knowledge injection is applied
- All models demonstrate significant memorization effects, particularly on questions from well-known datasets, even when knowledge injection is applied

## Why This Works (Mechanism)
Knowledge injection works by providing LLMs with relevant schema information and example queries that guide the generation process toward syntactically correct and semantically appropriate SPARQL statements. The mechanism relies on the model's ability to leverage contextual information to overcome knowledge gaps, particularly for complex SPARQL constructs and KG-specific patterns. Memorization effects occur because LLMs have been exposed to similar questions and queries during pre-training, leading to pattern matching rather than genuine reasoning. The anonymized knowledge injection condition helps reveal which performance gains are due to genuine understanding versus pattern recognition of familiar dataset structures.

## Foundational Learning

**SPARQL query generation**: Understanding the syntax and semantics of SPARQL is essential for evaluating generated queries. Quick check: Verify that generated queries execute correctly on the target knowledge graph and return expected results.

**Knowledge Graph schema comprehension**: Models need to understand entity relationships, predicates, and graph structure. Quick check: Ensure the model correctly identifies relevant entities and relationships from the natural language question.

**LLM prompt engineering**: Effective prompting strategies are crucial for eliciting accurate SPARQL generation. Quick check: Test different prompt formats and examples to optimize performance for specific knowledge graphs.

**Memorization detection**: Identifying when models rely on memorized patterns rather than genuine understanding. Quick check: Compare performance across anonymized and non-anonymized knowledge injection conditions.

**Semantic parsing evaluation**: Measuring the quality of generated queries beyond simple string matching. Quick check: Use both exact match and semantic equivalence metrics to evaluate query accuracy.

## Architecture Onboarding

**Component map**: Natural Language Question -> LLM (with prompting strategy) -> SPARQL Query -> Knowledge Graph Execution -> Results Evaluation

**Critical path**: The most critical sequence is the transformation from natural language questions through the LLM to SPARQL generation, as this determines the overall system accuracy. The knowledge injection step acts as a performance multiplier but depends on the base generation capability.

**Design tradeoffs**: Larger models (70B+) show better performance but require more computational resources, while smaller models are more efficient but struggle with complex queries. Knowledge injection improves accuracy but requires additional context management and may introduce memorization biases.

**Failure signatures**: Zero-shot prompting typically fails on complex queries requiring multiple joins or specific SPARQL constructs. Memorization manifests as suspiciously high performance on benchmark datasets without knowledge injection. Knowledge injection failures occur when the provided schema information is incomplete or poorly formatted.

**First experiments**: 1) Test zero-shot prompting across all models on a small subset of questions to establish baseline performance. 2) Apply knowledge injection to the same subset and measure performance improvement. 3) Run anonymized knowledge injection to detect memorization effects and compare results.

## Open Questions the Paper Calls Out
The paper identifies several open questions including how to effectively mitigate memorization effects in SPARQL generation, whether the observed improvements generalize to multilingual and domain-specific knowledge graphs, and how to handle temporal or dynamic aspects of real-world knowledge bases that are not captured in static test datasets.

## Limitations
- The study primarily focuses on English-language questions, limiting generalizability to multilingual scenarios
- Memorization analysis relies on specific anonymization techniques that may not capture all forms of memorized knowledge
- Static knowledge graphs used in experiments may not reflect the dynamic nature of real-world knowledge bases

## Confidence
- High confidence in the relative performance differences between knowledge injection conditions across tested models
- Medium confidence in the generalization of memorization findings to other datasets or knowledge graph domains
- Medium confidence in the absolute performance scores due to potential variations in prompt engineering and evaluation setup

## Next Checks
1. Replicate the experiments with additional language pairs and domain-specific knowledge graphs to test generalizability
2. Implement alternative memorization detection methods, such as using out-of-distribution test questions or synthetic query generation
3. Evaluate the practical utility of generated SPARQL queries by measuring execution accuracy and result relevance on real knowledge graph endpoints