---
ver: rpa2
title: Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large
  Language Models
arxiv_id: '2510.26577'
source_url: https://arxiv.org/abs/2510.26577
tags:
- decoding
- cast
- eagle-3
- arxiv
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating inference for
  large language models (LLMs) by introducing a cost-aware dynamic tree decoding approach.
  The method, called CAST, dynamically adjusts the tree structure during speculative
  decoding by considering inference costs such as GPU configurations and batch sizes.
---

# Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models

## Quick Facts
- arXiv ID: 2510.26577
- Source URL: https://arxiv.org/abs/2510.26577
- Reference count: 9
- This paper introduces CAST, a cost-aware dynamic tree decoding method that achieves up to 5.2x speedup over standard decoding and 5-20% improvements over previous state-of-the-art methods.

## Executive Summary
This paper addresses the challenge of accelerating inference for large language models (LLMs) by introducing a cost-aware dynamic tree decoding approach. The method, called CAST, dynamically adjusts the tree structure during speculative decoding by considering inference costs such as GPU configurations and batch sizes. This is achieved through breadth and depth pruning, along with dynamic reranking, to balance token acceptance rates and computational efficiency. Extensive experiments across six tasks and six LLMs show that CAST consistently outperforms existing state-of-the-art methods, achieving significant speedups while maintaining model quality.

## Method Summary
CAST (Cost-Aware Speculative Tree) modifies the tree construction of speculative decoding methods like EAGLE-2/3. It pre-computes inference cost lookup tables for target and draft models based on batch size, context length, and sequence length. The method then dynamically adjusts the tree structure through three main mechanisms: breadth pruning that selects nodes based on utility-cost thresholds, depth pruning that determines when to stop expanding the tree based on confidence gain trends, and reranking that finalizes the verification set based on hardware-specific latency profiles. This cost-aware approach optimizes for GPU utilization and memory bandwidth, particularly in batched scenarios where verification costs are higher.

## Key Results
- Achieves up to 5.2x speedup compared to standard autoregressive decoding
- Consistently outperforms EAGLE-2/3 by 5-20% across six diverse tasks
- Shows graceful degradation with increasing batch size, maintaining performance better than baselines
- Demonstrates effectiveness across multiple LLM architectures including Llama-3.1-8B and Vicuna-13B

## Why This Works (Mechanism)

### Mechanism 1: Cost-Aware Breadth Pruning via Diminishing Returns
- Constrains candidate nodes per layer based on a cost-utility ratio to prevent GPU resource contention
- Uses a selection algorithm that retains nodes only if marginal utility exceeds a threshold, filtering out low-probability tokens
- Prevents the construction of trees too large for efficient GPU verification

### Mechanism 2: Predictive Depth Pruning
- Dynamically determines tree depth based on historical confidence gains to prevent wasteful deep subtrees
- Maintains a buffer tracking confidence gain ratios and stops expansion when predictive quality drops
- Adapts tree depth to hardware variables rather than using fixed maximum depths

### Mechanism 3: Inference-Cost-Aware Reranking
- Finalizes verification set based on specific latency profile of target hardware rather than simple top-m selection
- Globally reranks nodes by cumulative probability and applies cost-aware selection for verification
- Adapts to batch-induced cost changes, particularly important in high-memory-pressure regimes

## Foundational Learning

### Concept: Speculative Decoding (Draft & Verify)
- **Why needed:** CAST is an optimization of speculative decoding; understanding the draft/verify paradigm is essential for grasping why tree structure and batch size affect latency
- **Quick check:** Why does verifying 10 tokens in a single batch take longer than verifying 1 token, and how does this relate to "cost" in CAST?

### Concept: Tree-Structured Attention Masks
- **Why needed:** CAST constructs a tree, not a chain; understanding how to linearize a tree and apply masks so each token attends only to ancestors is crucial for verification
- **Quick check:** In a tree structure, if a parent node is rejected, does the verification of a sibling subtree need to be discarded?

### Concept: GPU Utilization and Memory Bandwidth
- **Why needed:** The core innovation relies on optimizing for GPU bottlenecks; distinguishing between compute-bound and memory-bound scenarios explains why optimal tree structure changes
- **Quick check:** As batch size increases, does GPU utilization typically go up or down? How should this influence the size of the draft tree?

## Architecture Onboarding

### Component map:
Cost Lookup Tables (S_T, S_D) -> Expansion Engine (Breadth Pruning + Depth Pruning) -> Reranking Engine -> Tree Attention Wrapper

### Critical path:
The correctness of the Cost Lookup Tables is the most critical dependency. If pre-computed latency data does not match actual runtime hardware performance, utility calculations will be skewed, leading to suboptimal tree sizes.

### Design tradeoffs:
- **Pre-computation vs. Runtime Overhead:** Relies on offline profiling, adding setup time and assuming static hardware conditions
- **Accept Length vs. Speedup:** Explicitly trades off Average Acceptance Length for Speedup Ratio, potentially accepting fewer tokens but finishing generation faster

### Failure signatures:
- **Speedup < 1.0x:** Cost thresholds are too permissive, creating trees too large for efficient GPU verification
- **Degradation with Batch Size:** If speedup drops precipitously as batch size increases, cost modeling for target model is likely inaccurate for high-memory-pressure regimes

### First 3 experiments:
1. Profile Baseline Latency: Generate cost lookup tables S_T and S_D for your specific GPU by measuring forward pass times across varying batch sizes, context lengths, and sequence lengths
2. Ablation on Thresholds: Run CAST on HumanEval while sweeping threshold constants to observe sensitivity of tree size vs. speedup
3. Batch Scaling Comparison: Compare speedup ratio of CAST against EAGLE-3 at batch sizes 1, 4, 8, and 16 to validate graceful degradation claim

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text, but several limitations and potential areas for future work are discussed in the appendix and discussion sections.

## Limitations
- Relies heavily on accurate pre-computed cost lookup tables that may not reflect real-time GPU behavior under varying conditions
- Does not provide direct comparisons of acceptance length achieved versus baselines, making the speed vs. acceptance trade-off difficult to assess
- Experimental validation limited to one GPU architecture (NVIDIA A800), restricting generalizability

## Confidence
**High confidence**: The claim of consistent speedups over EAGLE-2/3 across diverse tasks and LLMs is well-supported by experimental results with transparent methodology.
**Medium confidence**: The theoretical basis for depth pruning is plausible but lacks direct evidence that the buffer-based stopping criterion consistently outperforms simpler alternatives.
**Low confidence**: The claim of graceful degradation with batch size is inferred from limited data (only batch size 8 tested) and may not hold for larger batches.

## Next Checks
1. **Cost Table Sensitivity**: Run CAST with artificially perturbed cost lookup tables (+/-10% error) to quantify how profiling inaccuracies affect speedup
2. **Batch Size Scaling**: Extend experiments to batch sizes 16 and 32 to plot speedup ratio vs. batch size and identify when CAST's advantage disappears
3. **Acceptance Length Comparison**: Record average acceptance length for CAST and EAGLE-3 under identical conditions to quantify the speed vs. acceptance trade-off