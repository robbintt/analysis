---
ver: rpa2
title: 'LLM-Cave: A benchmark and light environment for large language models reasoning
  and decision-making system'
arxiv_id: '2511.22598'
source_url: https://arxiv.org/abs/2511.22598
tags:
- wumpus
- reasoning
- agent
- environment
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-Cave is a lightweight benchmark and environment for evaluating
  large language models (LLMs) in sequential reasoning and decision-making. The environment
  requires agents to infer hidden dangers and navigate to a goal based on partial,
  noisy observations.
---

# LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system

## Quick Facts
- arXiv ID: 2511.22598
- Source URL: https://arxiv.org/abs/2511.22598
- Authors: Huanyu Li; Zongyuan Li; Wei Huang; Xian Guo
- Reference count: 18
- Key outcome: Chain of Speculation and Planner-Critic mechanisms improve LLM reasoning in a Wumpus World variant, with o1-mini success rate increasing from 65.33% to 78.67% using CoS and 4o-mini rising from 44.00% to 50.67% with Planner-Critic.

## Executive Summary
LLM-Cave is a lightweight benchmark for evaluating large language models in sequential reasoning and decision-making under partial observability. The environment requires agents to infer hidden dangers and navigate to a goal based on noisy observations, addressing limitations of existing QA benchmarks. The study introduces two inference-time mechanisms: Chain of Speculation, which maintains explicit reasoning hypotheses across steps, and Planner-Critic, a feedback loop where a second model validates and optimizes proposed actions. Experiments show reasoning-oriented models perform best out-of-the-box, while weaker models improve substantially with these mechanisms, though at increased computational cost.

## Method Summary
LLM-Cave is a Wumpus World variant where agents navigate n×n grids to collect gold while avoiding hidden pits and Wumpus using only partial observations (breeze near pits, stench near Wumpus). The benchmark evaluates LLMs using two inference-time mechanisms: Chain of Speculation maintains explicit hazard hypotheses across timesteps by propagating JSON-formatted guesses, while Planner-Critic uses a second LLM to validate and potentially override proposed actions based on confidence scoring. The environment provides structured JSON observations containing sensory data but never hazard positions directly, forcing models to maintain and update beliefs about danger locations.

## Key Results
- Reasoning-oriented models (o1-mini, DeepSeek-R1) perform best out-of-the-box in LLM-Cave
- Chain of Speculation improves o1-mini success rate from 65.33% to 78.67% through explicit hypothesis propagation
- Planner-Critic increases 4o-mini success rate from 44.00% to 50.67% by catching unsafe decisions before execution
- Computational costs include ~70% token overhead for CoS and doubled latency per step for Planner-Critic

## Why This Works (Mechanism)

### Mechanism 1: Chain of Speculation
The Chain of Speculation addresses sequential reasoning consistency by enabling explicit hypothesis propagation across timesteps. At each step, the LLM outputs Analysis (reasoning), Guess (JSON-formatted hazard hypotheses), and Action. The Guess is fed back as context in subsequent rounds, creating persistent reasoning memory rather than requiring re-inference from scratch. This mechanism assumes the model can generate accurate initial hypotheses and correct them when contradicted by new observations. Evidence shows o1-mini success rate increased from 65.33% to 78.67% using this approach.

### Mechanism 2: Planner-Critic Feedback Loop
The Planner-Critic mechanism reduces catastrophic failures by implementing a validation feedback loop. The Planner proposes an action; the Critic receives identical context plus the proposed action, assigns a confidence score (0–1), and optionally suggests an alternative. If the score falls below threshold (0.7), the Critic's alternative is executed instead. This assumes the Critic can identify hazards the Planner missed using the same observational data but different reasoning. Evidence shows 4o-mini success rate rose from 44.00% to 50.67% with this mechanism, though it doubles computational cost.

### Mechanism 3: Partial Observability Inference from Sensory Cues
The benchmark forces models to deduce hidden state from indirect signals (breeze→nearby pit, stench→nearby Wumpus), exposing reasoning gaps not captured by single-step QA benchmarks. The environment provides structured JSON observations with sensory data but never hazard positions directly. The agent must maintain and update beliefs about danger locations based on constraint propagation. This structure generalizes to real-world sequential decision-making under uncertainty, though the grid-world abstraction may be too simple for full real-world transfer.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: LLM-Cave is a POMDP where agents receive observations but not full state. Understanding belief-state maintenance is essential for implementing effective agents.
  - Quick check question: Can you explain why an agent detecting breeze at (2,2) cannot uniquely determine the pit location without additional observations?

- Concept: Chain-of-Thought Prompting
  - Why needed here: Both CoS and Planner-Critic extend CoT by structuring the reasoning output format and adding external validation.
  - Quick check question: What is the difference between implicit reasoning in standard prompting and explicit hypothesis propagation in Chain of Speculation?

- Concept: Multi-Agent LLM Coordination
  - Why needed here: Planner-Critic uses role-separated LLM invocations. Understanding how to architect prompts for different roles (generator vs. evaluator) is critical.
  - Quick check question: Why might using the same underlying model for both Planner and Critic still produce different outputs?

## Architecture Onboarding

- Component map: Environment -> Observation aggregator -> LLM with CoS -> Action execution OR Environment -> LLM Planner -> LLM Critic -> Action execution
- Critical path:
  1. Environment emits observation (position, breeze locations, stench locations, valid actions)
  2. CoS appends prior Guess to prompt; LLM generates Analysis + updated Guess + Action
  3. If Planner-Critic enabled: Critic scores Action; if score < 0.7, substitute Critic's alternative
  4. Execute action; environment returns next observation or terminal state
- Design tradeoffs:
  - CoS improves capable models but adds ~70% token overhead (3896→6631 tokens for o1-mini)
  - Planner-Critic doubles latency per step (4.79s→10.14s for 4o-mini)
  - Threshold tuning (currently 0.7) balances safety vs. efficiency; higher thresholds cause more Critic interventions
- Failure signatures:
  - Persistent incorrect guesses: Model fails to update hypotheses when new observations contradict prior beliefs
  - Critic amplifies errors: If Critic prompt is not carefully designed, it may approve unsafe actions or reject optimal ones
  - Step limit exhaustion: Agent exceeds 50 steps without reaching gold (indicates exploration inefficiency)
- First 3 experiments:
  1. Baseline evaluation: Run target model on LLM-Cave with standard CoT only; record success rate, steps, and token usage (150 trials across 3×3 and 4×4 configurations)
  2. Chain of Speculation ablation: Add CoS; measure success rate delta and token overhead; confirm if model benefits from explicit hypothesis propagation
  3. Planner-Critic integration: Enable Critic with threshold 0.7; compare success rate improvement against latency cost; identify if model benefits more from validation than hypothesis propagation

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed inference-time strategies (Chain of Speculation and Planner-Critic) be effectively integrated with training-time optimization methods like reinforcement learning or supervised fine-tuning? The conclusion states that integrating inference-time reasoning strategies with training-time feedback offers a promising path, but the current study evaluates these mechanisms solely as zero-shot inference-time techniques.

### Open Question 2
Can a heterogeneous architecture, where a stronger LLM serves as the Critic for a weaker Planner, significantly outperform the homogeneous setups tested? The authors note that Planner and Critic can use different models but used the same model for fairness, leaving the stronger-teacher/weaker-student dynamic untested.

### Open Question 3
Does the simultaneous application of Chain of Speculation and Planner-Critic mechanisms yield additive or interfering effects on model performance? The paper tests these mechanisms separately but does not present data on a single model utilizing both strategies concurrently.

### Open Question 4
How does LLM performance scale in this environment as grid size and hazard density increase beyond the 4×4 configurations tested? The experiments are limited to 3×3 and 4×4 grids, but the introduction emphasizes the need for environments that capture multi-step planning, leaving uncertainty about performance on larger, more complex caves.

## Limitations

- Exact Critic prompt format and API parameters (temperature, top_p) are not fully specified, making faithful reproduction challenging
- Significant computational costs limit practical deployment: ~70% token overhead for CoS and doubled latency per step for Planner-Critic
- Grid-world abstraction may be too simple compared to real-world complexity for generalization claims

## Confidence

- High confidence: Experimental results showing performance improvements for specific models are well-supported by data
- Medium confidence: Claim that structured reasoning and feedback significantly enhance LLM decision-making is supported, though computational tradeoffs are substantial
- Low confidence: Claim that improvements generalize to real-world sequential decision-making under uncertainty is not directly validated

## Next Checks

1. Implement and test the exact Critic prompt format and API parameters to verify reported performance improvements can be reproduced
2. Conduct ablation studies varying the confidence threshold (currently 0.7) to determine optimal balance between safety and efficiency across different model capabilities
3. Test the benchmark with more complex grid configurations and compare performance against established POMDP solvers to assess the benchmark's difficulty progression and validity for sequential reasoning evaluation