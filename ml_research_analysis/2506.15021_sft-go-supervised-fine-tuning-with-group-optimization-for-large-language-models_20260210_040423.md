---
ver: rpa2
title: 'SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language
  Models'
arxiv_id: '2506.15021'
source_url: https://arxiv.org/abs/2506.15021
tags:
- tokens
- token
- llama-3
- loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SFT-GO, a novel approach for supervised fine-tuning
  (SFT) of large language models (LLMs) that groups tokens based on importance and
  optimizes using a weighted combination of standard cross-entropy and worst-group
  loss. The method addresses the issue that not all tokens contribute equally to task-specific
  semantics during SFT, with semantically rich tokens often being under-optimized
  compared to common functional words.
---

# SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models

## Quick Facts
- **arXiv ID**: 2506.15021
- **Source URL**: https://arxiv.org/abs/2506.15021
- **Reference count**: 40
- **Primary result**: SFT-GO consistently outperforms baseline SFT approaches across seven benchmarks, with LLMLingua-2 and TF-IDF particularly improving performance on general reasoning tasks

## Executive Summary
This paper introduces SFT-GO, a novel approach for supervised fine-tuning (SFT) of large language models that groups tokens based on importance and optimizes using a weighted combination of standard cross-entropy and worst-group loss. The method addresses the issue that not all tokens contribute equally to task-specific semantics during SFT, with semantically rich tokens often being under-optimized compared to common functional words. Three token grouping strategies are explored: TF-IDF (statistics-based), LLMLingua-2 (semantics-based), and Rho-1 (loss-based). Experiments on LIMA and Alpaca datasets using Llama 3.2-3B and Llama 3.1-8B show that SFT-GO consistently outperforms baseline SFT approaches across seven benchmarks, with LLMLingua-2 and TF-IDF particularly improving performance on general reasoning tasks.

## Method Summary
SFT-GO introduces a novel framework for supervised fine-tuning that addresses the imbalance in token optimization during LLM training. The core innovation lies in grouping tokens by their importance using three distinct strategies: TF-IDF for statistics-based grouping, LLMLingua-2 for semantics-based grouping, and Rho-1 for loss-based grouping. The optimization combines standard cross-entropy loss with worst-group loss, weighted by importance scores to prioritize challenging token groups. This approach ensures semantically rich tokens receive adequate optimization attention during training. The method is evaluated on LIMA and Alpaca datasets using Llama 3.2-3B and Llama 3.1-8B models, demonstrating consistent improvements across multiple benchmarks.

## Key Results
- SFT-GO consistently outperforms baseline SFT approaches across seven benchmarks
- LLMLingua-2 and TF-IDF grouping strategies particularly improve performance on general reasoning tasks
- Theoretical analysis proves convergence properties, demonstrating SFT-GO's effectiveness in enhancing LLM fine-tuning

## Why This Works (Mechanism)
The paper's theoretical claims regarding convergence properties and the effectiveness of the worst-group loss formulation require further empirical validation across diverse model architectures and datasets. The evaluation is limited to Llama 3.2-3B and Llama 3.1-8B models trained on LIMA and Alpaca datasets, which may not generalize to other model families or training paradigms. The computational overhead of the three token grouping strategies (TF-IDF, LLMLingua-2, and Rho-1) is not thoroughly analyzed, particularly for longer sequences where these methods may become prohibitively expensive. Additionally, the paper does not address potential label noise or the stability of the worst-group loss when token importance estimates are imperfect.

## Foundational Learning

**Cross-entropy loss**: The standard loss function for classification tasks that measures the difference between predicted and actual probability distributions. Needed to understand the baseline optimization approach and how SFT-GO modifies it.

**Worst-group loss**: A loss function that focuses on the most challenging examples or groups during optimization. Quick check: verify that the worst-group formulation is correctly implemented and that the weighting mechanism appropriately balances between standard and worst-group losses.

**Token importance weighting**: The practice of assigning different weights to tokens based on their contribution to task-specific semantics. Quick check: validate that the importance scoring methods (TF-IDF, LLMLingua-2, Rho-1) are correctly implemented and that the weights are properly normalized.

**Group optimization**: The strategy of partitioning data into groups and optimizing for the worst-performing group. Quick check: ensure that the grouping strategies are correctly implemented and that the worst-group loss is properly computed for each group.

## Architecture Onboarding

**Component map**: Dataset -> Token Grouping Strategy (TF-IDF/LLMLingua-2/Rho-1) -> Weighted Loss Combination (Cross-entropy + Worst-group) -> LLM Fine-tuning

**Critical path**: Token grouping → Loss computation → Parameter updates → Evaluation

**Design tradeoffs**: The paper balances between computational efficiency and optimization effectiveness by using three different grouping strategies. TF-IDF is computationally efficient but may miss semantic nuances, while LLMLingua-2 captures semantics better but at higher computational cost. Rho-1 adapts to loss patterns but requires multiple training iterations.

**Failure signatures**: Poor performance on general reasoning tasks may indicate that the token grouping strategy is not capturing the right semantic features. Computational bottlenecks may occur with longer sequences due to the overhead of token grouping strategies.

**First experiments**: 1) Compare SFT-GO with baseline SFT on a small dataset to verify the core improvement. 2) Test each token grouping strategy independently to understand their individual contributions. 3) Evaluate the computational overhead of each grouping strategy on sequences of varying lengths.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims regarding convergence properties require additional mathematical rigor and empirical verification
- Evaluation limited to Llama 3.2-3B and Llama 3.1-8B models on LIMA and Alpaca datasets, limiting generalizability
- Computational overhead of token grouping strategies not thoroughly analyzed, especially for longer sequences

## Confidence

**High**: The core observation that not all tokens contribute equally to task-specific semantics during SFT is well-supported by both theoretical arguments and empirical results across multiple benchmarks.

**Medium**: The proposed SFT-GO framework with its three grouping strategies shows consistent improvements over baseline SFT, though the relative performance differences between the grouping methods warrant more extensive testing.

**Low**: The convergence analysis and theoretical guarantees for SFT-GO's optimization dynamics require additional mathematical rigor and empirical verification beyond the current scope.

## Next Checks
1. Evaluate SFT-GO's performance and computational overhead on models with 70B+ parameters and datasets with significantly longer input sequences (e.g., 8K+ tokens) to assess scalability.
2. Conduct ablation studies isolating the impact of each token grouping strategy across diverse task types, including code generation and multilingual reasoning, to identify which approaches work best for specific domains.
3. Test the robustness of SFT-GO when applied to instruction-tuned models fine-tuned on noisy or partially labeled data to understand how well the worst-group loss handles real-world data imperfections.