---
ver: rpa2
title: Agentic Learner with Grow-and-Refine Multimodal Semantic Memory
arxiv_id: '2511.21678'
source_url: https://arxiv.org/abs/2511.21678
tags:
- memory
- visual
- reasoning
- logical
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes ViLoMem, a dual-stream memory framework that
  separately encodes visual distraction patterns and logical reasoning errors to address
  the limitation of existing memory-augmented agents that store only trajectory-based
  memories without preserving multimodal reasoning. The system employs specialized
  retrieval strategies: image-similarity search followed by question-aware attention
  maps for visual memories, and problem analysis with text-embedding similarity for
  logical memories.'
---

# Agentic Learner with Grow-and-Refine Multimodal Semantic Memory

## Quick Facts
- arXiv ID: 2511.21678
- Source URL: https://arxiv.org/abs/2511.21678
- Reference count: 40
- Primary result: Dual-stream memory framework separates visual distraction and logical reasoning errors, improving multimodal reasoning accuracy across six benchmarks

## Executive Summary
This paper proposes ViLoMem, a dual-stream memory framework that separately encodes visual distraction patterns and logical reasoning errors to address the limitation of existing memory-augmented agents that store only trajectory-based memories without preserving multimodal reasoning. The system employs specialized retrieval strategies: image-similarity search followed by question-aware attention maps for visual memories, and problem analysis with text-embedding similarity for logical memories. Through extensive experiments on six multimodal benchmarks including HallusionBench, RealWorldQA, MathVista, MathVision, MMMU, and MMStar, ViLoMem consistently improves pass@1 accuracy across different model scales, achieving notable gains on mathematical reasoning tasks (e.g., +6.48 on MathVision for GPT-4.1, +4.38 on MMMU for Qwen3-VL-8B). Ablation studies confirm both memory streams are essential, with visual errors dominating generation (59-93%) but both streams contributing comparably during retrieval.

## Method Summary
ViLoMem implements a closed-loop memory cycle where problems are processed through parallel retrieval from visual and logical memory banks, solver generation, verification against ground truth, and conditional memory generation/merge. The visual memory bank stores (guideline text, source image) pairs using two-stage retrieval (image similarity then text reranking), while the logical memory bank stores guideline text only using problem analysis with text-embedding similarity. Memory generation is triggered by verification errors and attributed to either visual or logical streams based on error type. The grow-and-refine principle governs memory updates through similarity-based merge-or-create operations with thresholds τ_V and τ_L, preventing unbounded growth while preserving stable strategies.

## Key Results
- ViLoMem achieves consistent accuracy improvements across six multimodal benchmarks (HallusionBench, RealWorldQA, MathVista, MathVision, MMMU, MMStar)
- Notable performance gains on mathematical reasoning tasks: +6.48 on MathVision for GPT-4.1, +4.38 on MMMU for Qwen3-VL-8B
- Ablation studies confirm both memory streams are essential, with visual errors dominating generation (59-93%) but both streams contributing comparably during retrieval
- Cross-model and cross-benchmark analyses demonstrate effectiveness in knowledge distillation and domain-specific optimization

## Why This Works (Mechanism)

### Mechanism 1: Error Stream Separation
Separating visual distraction errors from logical hallucination errors enables more precise memory retrieval and error correction. Visual errors (object confusion, spatial misreading) and logical errors (formula misapplication, computational mistakes) are attributed to separate streams during generation; during retrieval, each stream uses modality-appropriate matching strategies rather than forcing all errors through a single text-based pathway. Core assumption: Visual and logical error patterns are sufficiently distinct that separating them yields more actionable retrieval than unifying them. Evidence: ablations confirm the necessity of dual-stream memory with explicit distraction–hallucination separation.

### Mechanism 2: Question-Aware Visual Retrieval
Two-stage visual memory retrieval (image similarity → question-conditioned text filtering) retrieves more relevant perceptual guidance than image similarity alone. Stage 1 retrieves top-k candidates via multimodal image embeddings; Stage 2 reranks using text embeddings of an enriched query combining the question with structured problem analysis, filtering for task-relevance. Core assumption: The same image region can be relevant or irrelevant depending on the question; text-based filtering captures this context. Evidence: Visual similarity alone is insufficient for semantic matching.

### Mechanism 3: Grow-and-Refine Memory Consolidation
Similarity-based merge-or-create updates prevent detail erosion while allowing incremental knowledge accumulation. Before storing a new guideline, compute similarity to existing entries; if above threshold τ, merge into existing entry; otherwise create new entry. This avoids unbounded growth and preserves stable strategies. Core assumption: Similar error patterns should be consolidated rather than stored redundantly; merged guidelines retain essential information. Evidence: following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge—preserving stable, generalizable strategies while avoiding catastrophic forgetting.

## Foundational Learning

- **Multimodal Embeddings**: ViLoMem relies on separate embedding functions (ϕT for text, ϕM for images) to compute similarity for retrieval and merge decisions. Why needed: Core to computing similarity for retrieval and merge decisions. Quick check: Can you explain why cosine similarity in embedding space might fail for cross-modal matching (e.g., similar text but dissimilar images)?

- **Catastrophic Forgetting in Sequential Learning**: The grow-and-refine strategy explicitly aims to preserve stable strategies while accumulating new knowledge. Why needed: Understanding why simple rewriting causes forgetting clarifies the design motivation. Quick check: What happens if a memory system overwrites old entries with new ones without consolidation?

- **Attention Visualization for MLLMs**: ViLoMem generates question-aware attention maps as auxiliary visual inputs. Why needed: Understanding how these are produced and their limitations (e.g., precision on small regions) is critical for interpreting ablation results. Quick check: Why might an attention heatmap fail to highlight a small numeric label in a geometry diagram?

## Architecture Onboarding

- **Component map**: Retriever → Solver → Verifier → Generator (updates both memory banks)
- **Critical path**: Problem (I, q) arrives → parallel retrieval from visual and logical banks → retrieved memories injected into solver prompt → solver generates candidate answer → verifier checks correctness → if error, generate/merge guideline in appropriate bank → next problem benefits from updated memories
- **Design tradeoffs**: Two-stage visual retrieval vs. single-stage (more compute but higher precision); separate vs. unified memory banks (clearer attribution but more complex retrieval coordination); merge vs. create threshold (higher τ consolidates more aggressively but risks over-generalization)
- **Failure signatures**: Textual bias (solver ignores visual cues → verifier cannot attribute visual errors → logical memory gets noisy updates); complex diagrams (low-quality visual descriptions → verifier defaults to logical attribution → visual memory underpopulated); cross-domain retrieval (mismatched domains cause interference)
- **First 3 experiments**: (1) Single-stream ablation: Run GPT-4.1 with visual memory only, then logical memory only on MathVista/MMMU. (2) Threshold sensitivity: Vary τV and τL; plot memory bank size vs. accuracy. (3) Cross-domain probe: Train memory on HallusionBench, evaluate on MathVision.

## Open Questions the Paper Calls Out

- **Memory Generation Mechanisms**: How can memory generation mechanisms be refined to better decouple visual distraction errors from logical reasoning errors when the underlying solver exhibits strong textual bias or low-quality visual perception? The paper states that a promising direction is to design more specialized mechanisms to further enhance the decoupling of dual memory streams.

- **Attention Visualization for Math Tasks**: Can attention visualization methods be adapted to preserve fine-grained geometric structures and chart details necessary for high-precision mathematical reasoning? The paper notes that attention maps yielded marginal improvements on math-centric datasets because current visualization methods struggle to faithfully preserve fine-grained geometric structures.

- **Cross-Domain Memory Transfer**: How can cross-domain memory transfer be managed to prevent interference when sharing memory banks between tasks with large domain gaps? The paper shows that while some domains benefit from shared memory, tasks with large domain gaps exhibit conflicts in memory utilization.

- **Ground Truth-Free Operation**: To what extent can the ViLoMem framework operate effectively in scenarios where ground truth answers are unavailable for the verifier during the memory generation cycle? The current architecture depends on explicit verification against ground truth to identify errors.

## Limitations

- Critical design parameters (merge/create thresholds τ_V, τ_L and top-k values) are underspecified, making exact reproduction difficult
- Memory generation relies on ground truth verification, limiting applicability to real-world lifelong learning scenarios
- Attention visualization methods struggle with fine-grained geometric structures in mathematical reasoning tasks
- Cross-domain memory transfer can cause interference between tasks with large domain gaps

## Confidence

- **High Confidence**: Dual-stream separation design and its effectiveness on tested benchmarks
- **Medium Confidence**: Grow-and-refine principle's role in preventing catastrophic forgetting
- **Low Confidence**: Generality of question-aware visual retrieval effectiveness

## Next Checks

1. **Merge Operation Fidelity**: Implement and test multiple merge strategies (concatenation, weighted fusion, LLM-based synthesis) on a held-out subset; measure impact on retrieval relevance and memory bank size

2. **Threshold Sweep**: Systematically vary τ_V and τ_L; plot memory bank size vs. accuracy; identify regimes where over-merging causes detail erosion or under-merging causes retrieval noise

3. **Cross-Domain Interference**: Train memory on HallusionBench, evaluate on MathVision; quantify domain-specific gains vs. cross-domain contamination to validate domain-alignment claims