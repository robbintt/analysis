---
ver: rpa2
title: 'Using Contrastive Learning to Improve Two-Way Reasoning in Large Language
  Models: The Obfuscation Task as a Case Study'
arxiv_id: '2509.05553'
source_url: https://arxiv.org/abs/2509.05553
tags:
- code
- reasoning
- bidirectional
- semantic
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses whether large language models truly understand
  code semantics or merely replicate patterns through a bidirectional reasoning framework.
  The authors propose Contrastive Fine-Tuning (CFT) to overcome "cognitive specialization"
  - where standard fine-tuning degrades reverse reasoning capabilities while improving
  forward tasks.
---

# Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study

## Quick Facts
- arXiv ID: 2509.05553
- Source URL: https://arxiv.org/abs/2509.05553
- Authors: Serge Lionel Nikiema; Jordan Samhi; Micheline Bénédicte Moumoula; Albérick Euraste Djiré; Abdoul Kader Kaboré; Jacques Klein; Tegawendé F. Bissyandé
- Reference count: 29
- Primary result: CFT enables 39-52% reverse performance on deobfuscation tasks compared to 0% with standard fine-tuning

## Executive Summary
This paper addresses whether large language models truly understand code semantics or merely replicate patterns through a bidirectional reasoning framework. The authors propose Contrastive Fine-Tuning (CFT) to overcome "cognitive specialization" - where standard fine-tuning degrades reverse reasoning capabilities while improving forward tasks. CFT trains models on semantic triplets: positive pairs (same meaning), negative pairs (different meaning), and forward obfuscation examples. Experimental results demonstrate a breakthrough: while standard fine-tuning achieves 0% success on deobfuscation tasks, CFT enables 39-52% reverse performance across multiple models and transformation types.

## Method Summary
The authors introduce Contrastive Fine-Tuning (CFT) as a solution to unidirectional reasoning in LLMs. The method trains models using semantic triplets containing positive pairs (code with same semantics), negative pairs (code with different semantics), and forward obfuscation examples. By incorporating contrastive learning objectives, CFT forces the model to learn bidirectional mappings between original and obfuscated code. The approach is evaluated across four obfuscation transformations: variable renaming, function name obfuscation, comment removal, and whitespace stripping. Multiple model sizes are tested, including GPT-4.1-Mini and Phi-3.5-mini, with semantic equivalence determined through human evaluation.

## Key Results
- GPT-4.1-Mini achieved 52.03% semantic success in reverse variable renaming with CFT, compared to 0% with standard fine-tuning
- Phi-3.5-mini reached 39.05% success rate on deobfuscation tasks using CFT methodology
- CFT maintains forward task performance while enabling bidirectional reasoning capabilities

## Why This Works (Mechanism)
Contrastive learning enables models to learn invariant semantic representations by explicitly training on both similar and dissimilar code pairs. This forces the model to distinguish between semantically equivalent and non-equivalent code, creating bidirectional mappings rather than unidirectional pattern replication.

## Foundational Learning
- Semantic equivalence in code - Understanding that different syntactic representations can have identical runtime behavior; needed for defining positive pairs in contrastive learning
- Code obfuscation transformations - Knowledge of variable renaming, function renaming, comment removal, and whitespace manipulation; needed to create realistic test scenarios
- Contrastive learning objectives - Understanding how triplet loss functions work in training neural networks; needed to implement the CFT methodology
- Bidirectional reasoning - Recognizing that models often specialize in one direction of reasoning; needed to identify the cognitive specialization problem
- Human evaluation methodology - Techniques for reducing bias in semantic equivalence judgments; needed for reliable performance measurement

## Architecture Onboarding
- Component Map: Original Code -> Forward Obfuscation -> Obfuscated Code -> Reverse Deobfuscation -> Original Code
- Critical Path: Contrastive triplet generation → Model fine-tuning with triplet loss → Bidirectional inference testing
- Design Tradeoffs: CFT requires additional computational resources for triplet generation but enables bidirectional reasoning; standard fine-tuning is faster but unidirectional
- Failure Signatures: Models producing syntactically valid but semantically different code during reverse tasks; failure to generalize across unseen obfuscation patterns
- Three First Experiments: 1) Test generalization on domain-specific code (web vs systems programming) 2) Expert developer review of model outputs 3) Computational cost comparison between CFT and standard fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Python with four specific transformation types
- 39-52% success rate still indicates substantial failure rates in practical applications
- Computational overhead of contrastive fine-tuning compared to standard approaches not quantified

## Confidence
- High Confidence: Rigorous experimental methodology with clear semantic equivalence definitions and well-documented transformation procedures
- Medium Confidence: Interpretation of 39-52% success as "substantial improvement" needs practical utility context; claim about maintaining forward performance supported but needs more ablation studies
- Low Confidence: Broader implications for overcoming unidirectional optimization in all language models extend beyond empirical evidence; general applicability to other bidirectional tasks remains speculative

## Next Checks
1. Test model generalization by evaluating performance on code snippets from different domains (web development, data science, systems programming) and with obfuscation patterns not seen during training
2. Conduct expert developer review of a stratified sample of model outputs to validate semantic equivalence assessments and identify subtle failures that non-expert judges may have missed
3. Measure and compare computational costs (training time, memory usage, inference latency) between CFT and standard fine-tuning approaches across multiple model sizes to assess practical scalability constraints