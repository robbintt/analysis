---
ver: rpa2
title: Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning
arxiv_id: '2502.14765'
source_url: https://arxiv.org/abs/2502.14765
tags:
- claims
- evidence
- linguistics
- claim
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops and evaluates an iterative fact verification
  system that leverages large language models to verify medical claims through step-by-step
  question generation and evidence collection. The system generates follow-up questions
  to gather additional context, retrieves evidence from web search or internal knowledge,
  and produces verdicts with explainable reasoning.
---

# Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning
## Quick Facts
- arXiv ID: 2502.14765
- Source URL: https://arxiv.org/abs/2502.14765
- Reference count: 19
- System achieves 3.4-4.9 point F1 improvement over traditional three-part pipelines for medical fact verification

## Executive Summary
This study develops an iterative fact verification system that uses large language models to verify medical claims through step-by-step question generation and evidence collection. The system generates follow-up questions to gather additional context, retrieves evidence from web search or internal knowledge sources, and produces verdicts with explainable reasoning. Tested on three medical fact-checking datasets, the approach significantly improves performance compared to traditional three-part pipelines. The system performs best using GPT-4o-mini with web search for scientific claims and internal knowledge for common health claims.

## Method Summary
The system implements an iterative fact verification approach where LLMs generate follow-up questions to gather additional context for medical claims. It employs two evidence retrieval strategies: web search for broad information and internal knowledge for domain-specific data. The system processes claims through multiple iterations, collecting evidence at each step before producing final verdicts with explainable reasoning. The architecture leverages GPT-4o-mini as the primary model, though other variants were tested. Predicate logic integration was explored to enhance precision for well-formed claims.

## Key Results
- Iterative LLM-based verification improves F1 performance by 3.4-4.9 points compared to traditional three-part pipelines
- GPT-4o-mini with web search performs best for scientific medical claims
- Internal knowledge sources outperform web search for common health claims
- Predicate logic integration improves precision for well-formed claims but reduces recall for informal language

## Why This Works (Mechanism)
The iterative approach works by breaking down complex medical claims into smaller, verifiable components through targeted question generation. Each follow-up question addresses specific aspects of the claim, allowing the system to gather focused evidence rather than relying on broad search results. This targeted evidence collection reduces noise and improves the relevance of supporting information. The explainable reasoning component provides transparency in the verification process, making the system's decision-making traceable and verifiable.

## Foundational Learning
- **Iterative question generation**: Needed to decompose complex claims into verifiable components; quick check: verify that generated questions address specific claim aspects
- **Evidence retrieval strategies**: Web search for broad context vs internal knowledge for domain-specific accuracy; quick check: compare retrieval precision across claim types
- **Predicate logic integration**: Formalizes claim structure for improved precision on well-formed statements; quick check: measure precision improvement on structured vs unstructured claims
- **LLM-based reasoning**: Enables complex medical concept understanding beyond keyword matching; quick check: validate reasoning accuracy on domain-specific terminology
- **Explainable verdict generation**: Provides transparency and traceability in verification decisions; quick check: assess human interpretability of reasoning chains

## Architecture Onboarding
**Component map**: Claim -> Question Generator -> Evidence Retriever -> Reasoner -> Verdict Generator
**Critical path**: The iterative loop where generated questions inform evidence retrieval, which then feeds back into question generation for the next iteration
**Design tradeoffs**: Web search provides broader coverage but less precision vs internal knowledge offering domain accuracy but limited scope; predicate logic improves precision but may miss informal claims
**Failure signatures**: System struggles with colloquial medical language, shows reduced recall when predicate logic is applied, and may over-rely on web search for domain-specific claims
**First experiments**: 1) Compare iterative vs single-pass verification on the same claim sets, 2) Test evidence retrieval accuracy between web search and internal knowledge for different claim types, 3) Measure precision-recall tradeoff when enabling/disabling predicate logic

## Open Questions the Paper Calls Out
None

## Limitations
- Small evaluation scale with only three medical fact-checking datasets tested
- Domain-specific optimization may not generalize to non-medical claims
- Mixed results from predicate logic integration suggest challenges with informal medical language
- Performance sensitivity to LLM model choices and prompting strategies remains unclear

## Confidence
- **High confidence**: Core finding that iterative LLM-based verification outperforms traditional three-part pipelines for medical claims requiring complex reasoning
- **Medium confidence**: Specific superiority of GPT-4o-mini over other models and the precision-recall tradeoffs from predicate logic integration
- **Medium confidence**: Generalizability of results to non-medical domains and mixed claim types

## Next Checks
1. Test the system's performance on non-medical domains and mixed-domain claim sets to assess generalizability beyond the medical context where it was developed
2. Evaluate the iterative question generation approach with real users who provide the follow-up answers versus using automated LLM-generated responses, measuring the actual improvement in verification accuracy
3. Conduct ablation studies comparing different evidence retrieval strategies (web search vs internal knowledge) across various claim types to identify optimal configurations for different medical claim categories