---
ver: rpa2
title: 'FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular
  Data Generation'
arxiv_id: '2508.11810'
source_url: https://arxiv.org/abs/2508.11810
tags:
- fairness
- data
- synthetic
- causal
- fairtabgen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation

## Quick Facts
- arXiv ID: 2508.11810
- Source URL: https://arxiv.org/abs/2508.11810
- Reference count: 3
- Primary result: LLM-based tabular data synthesis achieving 3.5× lower bias and 20% higher utility while using <20% of original data

## Executive Summary
FairTabGen introduces a novel framework for synthetic tabular data generation that integrates counterfactual and causal fairness constraints using large language models. The system employs explicit Structural Causal Model (SCM) role definitions within prompts to guide generation, coupled with an iterative feedback loop that refines outputs based on comprehensive fairness and utility evaluation. By using fewer than 20% of original data points through in-context learning, the framework achieves significant improvements in both fairness metrics (Total Variation, Direct/Indirect/Spurious Effects) and utility metrics (AUROC, Precision, Recall) compared to baseline methods.

## Method Summary
The method combines in-context learning with causal fairness constraints by constructing prompts that explicitly define SCM roles (sensitive attributes, mediators, outcomes) for GPT-4o. Using curated samples (40-200 rows) as in-context examples, the LLM generates synthetic data while respecting causal pathways. An orchestration loop evaluates generated data against fairness metrics and iteratively refines prompts to balance utility and fairness objectives. The framework operates in a low-data regime, achieving comparable or superior results to methods requiring full dataset training while maintaining strict privacy guarantees through differential privacy principles.

## Key Results
- Achieves 3.5× lower bias (Total Variation) compared to baseline methods
- Improves utility by 20% (AUROC, Precision, Recall) while reducing data usage to <20% of original
- Successfully balances direct, indirect, and spurious effects across multiple real-world datasets (COMPAS, LAW School, MIMIC-IV)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly defining SCM roles within LLM prompts steers generation toward causal fairness constraints.
- **Mechanism:** Encodes sensitive attributes, mediators, and outcome variables as distinct semantic roles in prompts, leveraging LLM reasoning to respect causal pathways during synthesis.
- **Core assumption:** GPT-4o can semantically interpret and adhere to causal logic defined in natural language instructions better than correlation-based generators.
- **Evidence anchors:** Abstract states integration of counterfactual and causal fairness into generation pipelines; Section 3 describes explicit SCM role definitions in prompts; related work (FairCauseSyn) supports LLM-augmented causal fairness generation.
- **Break condition:** LLM hallucinates causal relationships or ignores role definitions in favor of statistical patterns.

### Mechanism 2
- **Claim:** Iterative orchestration loop evaluating and refining prompts balances utility and fairness.
- **Mechanism:** Generates data batches, evaluates against fairness metrics, dynamically modifies prompts (reweighting groups, adding contrastive examples) to correct bias.
- **Core assumption:** Feedback from quantitative fairness metrics can be effectively mapped to prompt modifications that guide LLM away from biased outputs.
- **Evidence anchors:** Abstract mentions feedback loop that refines prompts based on fairness and utility evaluation; Section 3 describes iterative prompt refinement; related work shows general agentic workflows.
- **Break condition:** Feedback loop oscillates or fails to converge, requiring manual intervention.

### Mechanism 3
- **Claim:** High data efficiency (<20%) achieved through LLM priors combined with fairness-aware data curation for in-context learning.
- **Mechanism:** Curates small representative samples (40-200 examples) to condition LLM, relying on pre-existing knowledge of tabular structures and fairness concepts.
- **Core assumption:** GPT-4o possesses sufficient internal priors for diverse tabular domains such that few-shot ICL captures data essence without memorization.
- **Evidence anchors:** Abstract states gains using <20% of original data; Table 2 shows specific sample sizes (COMPAS 40, MIMIC 200); methodology relies on few-shot learning.
- **Break condition:** Curated samples don't cover original distribution variance, generating naive or homogenous data.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs)**
  - **Why needed here:** Framework decomposes bias into Direct, Indirect, and Spurious effects; understanding these causal pathways is essential for interpreting results and debugging prompts.
  - **Quick check question:** In COMPAS dataset, if "race" affects "priors" (mediator) which affects "recidivism," is this a direct or indirect effect?

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** Primary training mechanism where model weights are frozen; understanding IC sample formatting and selection is the main engineering lever.
  - **Quick check question:** Does increasing ICL examples always improve fairness, or might it reinforce existing biases in real data?

- **Concept: Counterfactual Fairness vs. Demographic Parity**
  - **Why needed here:** Framework unifies individual-level (counterfactual) and group-level (demographic) fairness; orchestration loop attempts to satisfy both simultaneously.
  - **Quick check question:** If dataset has perfect Demographic Parity, does it automatically satisfy Counterfactual Fairness? (Hint: No, consider qualitatively why).

## Architecture Onboarding

- **Component map:** Input (Target Schema + Raw Data + Fairness Constraints) -> Prompt Constructor (injects SCM roles) -> LLM Engine (GPT-4o) -> Evaluator (calculates TV, DE, IE, SE, AUROC) -> Controller (orchestration loop)
- **Critical path:** Mapping fairness metrics back to specific prompt modifications (e.g., high Indirect Effect → reduce correlation between X and Z)
- **Design tradeoffs:**
  - Sample Size vs. Bias: Fewer ICL samples improve efficiency but risk missing edge cases; more samples might encode historical bias
  - Temperature Setting: High temperature (0.9) encourages diversity but risks hallucinating invalid data entries
- **Failure signatures:**
  - Memorization: Synthetic data identical to ICL samples
  - Metric Oscillation: DE improves but IE degrades significantly across iterations
  - Invalid Schema: LLM generates columns not in header or wrong data types
- **First 3 experiments:**
  1. Ablation on SCM Definitions: Test generic vs. explicit SCM role prompts to measure delta in Causal Fairness metrics
  2. ICL Sample Sensitivity: Test 1%, 5%, 10%, 20% data usage to verify low-data regime efficiency
  3. Orchestration Convergence: Run loop for fixed iterations to check if metrics plateau or get stuck

## Open Questions the Paper Calls Out

- **Intersectionality:** Framework handles sensitive attributes in isolation without addressing intersectionality; modifying prompt engineering and evaluation for intersecting protected groups would resolve this.
- **Unobserved Confounders:** Assumes observed confounders are sufficient; exploring robustness to hidden variables would address this limitation.
- **AI-Agent Evolution:** Proposes evolving to autonomous AI-agent framework for prompt refinement; developing and validating such agent architecture would resolve this.

## Limitations
- Framework handles sensitive attributes in isolation, without addressing intersectionality
- Assumes observed confounders are sufficient, not addressing unobserved confounders
- Current orchestration loop relies on predefined feedback rather than autonomous correction

## Confidence
- **Mechanism validity:** High - Multiple evidence anchors support the three core mechanisms
- **Implementation feasibility:** Medium - Some critical details missing (exact orchestration logic, causal metric implementation)
- **Reproducibility:** Low - Key unknowns about feedback loop logic and causal metric implementation block faithful reproduction

## Next Checks
1. Verify prompt construction with explicit SCM role definitions improves causal fairness metrics compared to generic prompts
2. Test data efficiency claims by running experiments with varying ICL sample percentages (1%, 5%, 10%, 20%)
3. Validate orchestration loop convergence by running fixed iterations and checking for metric plateaus or oscillations