---
ver: rpa2
title: Small sample-based adaptive text classification through iterative and contrastive
  description refinement
arxiv_id: '2508.00957'
source_url: https://arxiv.org/abs/2508.00957
tags:
- category
- classification
- description
- topic
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a zero-shot text classification framework
  for domains with evolving categories and ambiguous boundaries, such as ticketing
  systems. It combines iterative topic refinement, contrastive prompting, and active
  learning to generate and refine semantic descriptions for categories using minimal
  labeled data.
---

# Small sample-based adaptive text classification through iterative and contrastive description refinement

## Quick Facts
- arXiv ID: 2508.00957
- Source URL: https://arxiv.org/abs/2508.00957
- Authors: Amrit Rajeev; Udayaadithya Avadhanam; Harshula Tulapurkar; SaiBarath Sundar
- Reference count: 39
- Primary result: 91% accuracy on AGNews (3 seen, 1 unseen class) with minimal accuracy drop after introducing unseen classes

## Executive Summary
This paper introduces a zero-shot text classification framework designed for domains with evolving categories and ambiguous boundaries, such as ticketing systems. The approach generates and refines semantic descriptions for categories using minimal labeled data, starting with just 20 samples per class. By combining iterative topic refinement, contrastive prompting, and active learning, the system can adapt to new unseen categories without retraining. Experiments on AGNews and DBpedia datasets demonstrate strong performance with minimal accuracy degradation when integrating new classes.

## Method Summary
The framework begins by curating 20 samples per category and generating comprehensive semantic descriptions using an LLM. It then iteratively refines these descriptions through contrastive prompting between similar categories and active learning using misclassified samples. When classification accuracy falls below 80% on validation samples, the system triggers automated refinement, typically converging after four iterations. The approach handles fine-grained classification with limited supervision and enables real-time integration of new categories through an intra-class adapter for persistent confusions.

## Key Results
- 91% accuracy on AGNews (3 seen, 1 unseen class)
- 84% accuracy on DBpedia (8 seen, 1 unseen)
- Minimal accuracy drop after introducing unseen classes (91% → 82% on AGNews)
- Effective handling of fine-grained classification with limited supervision

## Why This Works (Mechanism)

### Mechanism 1
Generating semantic descriptions from small samples (n=20) produces more consistent category representations than using raw label names. The LLM analyzes patterns across N similar samples to synthesize a comprehensive description that captures the "underlying theme" rather than surface-level label inconsistencies. This eliminates annotator bias and creates holistic category definitions.

### Mechanism 2
Contrastive prompting improves discrimination between semantically similar categories by explicitly teaching boundary distinctions. When two categories show high similarity, the system generates comparative descriptions that emphasize unique characteristics and explicit exclusions. This leverages the LLM's language understanding to define fine-grained boundaries.

### Mechanism 3
Iterative refinement using misclassifications produces progressively more accurate descriptions without retraining. The system validates on M additional samples per category. When accuracy falls below 80%, misclassified examples trigger automated description augmentation. The process runs for up to 4 iterations or until threshold is met.

## Foundational Learning

- **Concept: Zero-shot classification with LLMs**
  - Why needed here: The framework relies on LLMs classifying via semantic reasoning over descriptions rather than learned embeddings. Understanding how LLMs perform zero-shot inference via prompting is essential.
  - Quick check question: Can you explain why an LLM can classify a document into a category it has never explicitly trained on, given only a natural language description?

- **Concept: Contrastive learning principles**
  - Why needed here: The method adapts contrastive learning (normally applied to embeddings) to the prompt space by making descriptions mutually discriminative.
  - Quick check question: How does the objective "maximize similarity between input and correct label while minimizing similarity to incorrect labels" translate to natural language description refinement?

- **Concept: Active learning and uncertainty sampling**
  - Why needed here: The framework identifies informative misclassified samples for refinement, which is an active learning strategy. Understanding sample selection criteria matters.
  - Quick check question: Why might focusing on misclassified or low-confidence samples be more efficient than refining on all available data?

## Architecture Onboarding

- **Component map:** Sample selector (N=20) -> Description generator -> Contrastive refinement module -> Validation loop (M samples) -> Misclassification-driven refinement (up to 4 iterations) -> Intra-class adapter -> Classifier -> Human-in-the-loop interface

- **Critical path:** Sample selection (N=20) → Initial description generation → Contrastive pairwise refinement → Validation (M samples) → Misclassification-driven refinement (up to 4 iterations) → Intra-class adaptation for persistent confusions → Production classification

- **Design tradeoffs:**
  - N (initial samples): More samples provide better coverage but risk diluted specificity
  - M (validation samples): More samples yield more robust refinement triggers but increase annotation cost
  - Iteration cap: 4 iterations empirically sufficient; more iterations yield diminishing returns but add LLM API costs
  - LLM choice: Must be decoder-based with accessible token probabilities for the classification function

- **Failure signatures:**
  - Description drift: Accumulated refinements create overly complex, contradictory descriptions
  - Category explosion: Pairwise contrasts exceed context window limits with many categories
  - Bootstrap failure: Unrepresentative initial samples propagate systematic bias
  - API cost blowup: Multiple LLM calls per iteration become cost-prohibitive

- **First 3 experiments:**
  1. Reproduce AGNews baseline: Start with N=20 samples per class (3 seen classes), generate descriptions, run validation with M=10-20 samples, measure accuracy before and after refinement
  2. Test unseen class integration: Add 4th AGNews category with only N=20 samples, generate description, integrate without retraining, measure accuracy on unseen class and check for accuracy drop on seen classes
  3. Ablate contrastive refinement: Compare classification accuracy with and without the contrastive pairwise refinement step to isolate its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to many categories is unclear as pairwise contrastive refinement grows quadratically
- Performance on domains with extreme ambiguity or significantly smaller training sets (fewer than 20 samples) remains untested
- Risk of description drift from accumulated refinements is not addressed

## Confidence

**High confidence:** Core mechanism of using LLM-generated semantic descriptions from small samples for zero-shot classification is well-supported (91% on AGNews seen classes, 84% on DBpedia).

**Medium confidence:** Contrastive prompting's contribution to improved discrimination between similar categories is supported but not explicitly validated through ablation studies.

**Low confidence:** Claims about effectiveness on "evolving categories and ambiguous boundaries" in ticketing systems are aspirational rather than empirically demonstrated.

## Next Checks

1. **Ablation study on contrastive refinement:** Run AGNews experiment with identical parameters but disable the contrastive pairwise refinement step. Compare accuracy specifically on semantically similar category pairs.

2. **Stress test with category expansion:** Start with 3 seen AGNews classes, then sequentially add 4th, 5th, 6th, and 7th classes. After each addition, measure new class accuracy and accuracy on previously seen classes.

3. **Minimum sample viability test:** Systematically reduce N from 20 down to 5 samples per category and measure corresponding drop in initial description quality and final classification accuracy.