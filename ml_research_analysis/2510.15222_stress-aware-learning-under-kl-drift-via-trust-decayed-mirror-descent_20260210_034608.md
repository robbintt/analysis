---
ver: rpa2
title: Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent
arxiv_id: '2510.15222'
source_url: https://arxiv.org/abs/2510.15222
tags:
- stress
- drift
- regret
- td-md
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study online decision-making under distribution drift and propose
  entropy-regularized trust-decay, which dynamically injects stress-aware exponential
  tilting into both belief updates and mirror-descent decisions. On the simplex, belief
  tilting and decision tilting are Fenchel-dual equivalent.
---

# Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent

## Quick Facts
- arXiv ID: 2510.15222
- Source URL: https://arxiv.org/abs/2510.15222
- Authors: Gabriel Nixon Raj
- Reference count: 40
- Primary result: Achieves O(1) per-switch regret and Õ(√T) dynamic regret under KL-drift via trust-decayed mirror descent

## Executive Summary
This paper introduces a framework for online decision-making under distribution drift that dynamically adjusts learning through KL-regularized trust-decay updates. The approach injects stress-aware exponential tilting into both belief updates and mirror-descent decisions, achieving robustness to regime switches while maintaining optimal regret guarantees. By formalizing robustness through concepts like fragility, belief bandwidth, and a decision-space Fragility Index, the framework unifies dynamic-regret analysis, distributionally robust objectives, and KL-regularized control within a single adaptive mechanism.

## Method Summary
The method employs trust-decayed mirror descent (TD-MD) with entropy regularization on the simplex, where updates are weighted by both gradient information and a stress signal. The update rule is xt+1 ∝ xt ⊙ exp{-η(gt + λtσt)}, with η = Θ(√(log d/T)) and tilt λt = κ√ϵt. The stress signal σt is triggered immediately after distribution switches and remains active for H time steps. The framework establishes that belief tilting and decision tilting are Fenchel-dual equivalent, enabling dynamic regret guarantees of Õ(√T) under KL-drift path length ST = Σt≥2√(KL(Dt||Dt-1)/2). A parameter-free hedge variant adapts the tilt to unknown drift, while persistent over-tilting incurs an Ω(λ²T) stationary penalty.

## Key Results
- Achieves O(1) per-switch regret compared to Ω(1) for stress-free baselines
- Guarantees Õ(√T) dynamic regret under KL-drift path length
- Formalizes robustness via fragility, belief bandwidth, and Fragility Index
- Extends to bandit feedback, outliers, distributed optimization, and KL-drift estimation

## Why This Works (Mechanism)
The method works by dynamically adjusting the learning rate through KL-regularized trust-decay, which injects stress-aware exponential tilting into both belief updates and decisions. This adaptive mechanism allows the algorithm to respond to distribution shifts while maintaining robustness during stationary periods. The Fenchel-duality equivalence between belief tilting and decision tilting ensures that stress-aware updates remain computationally tractable while providing theoretical guarantees.

## Foundational Learning

**KL Divergence and Bregman Divergence**
*Why needed*: KL divergence quantifies distribution drift; Bregman divergence measures distance in decision space.
*Quick check*: Verify KL(Dt||Dt-1) = ½(μt-μt-1)⊤Σ⁻¹(μt-μt-1) for Gaussian case.

**Mirror Descent and Entropy Regularization**
*Why needed*: Mirror descent enables efficient updates on probability simplices; entropy regularization ensures exploration.
*Quick check*: Confirm that updates remain on simplex and converge to optimal distribution.

**Dynamic Regret and Path Length**
*Why needed*: Dynamic regret measures performance against time-varying comparators; path length characterizes drift magnitude.
*Quick check*: Verify regret scales as O(√(T · path length)) not O(T).

## Architecture Onboarding

**Component Map**
TD-MD update -> Stress signal detection -> KL-drift estimation -> Regret analysis

**Critical Path**
Stress detection → Tilt activation (λt > 0) → Adaptive update (xt+1 ∝ xt ⊙ exp{-η(gt + λtσt)}) → Dynamic regret guarantee

**Design Tradeoffs**
- Adaptive vs fixed tilt: adaptive achieves O(1) per-switch regret but requires stress signal
- Exploration vs exploitation: entropy regularization ensures exploration but may slow convergence
- Computational cost vs robustness: KL-drift estimation adds overhead but enables better adaptation

**Failure Signatures**
- Over-tilting: Ω(λ²T) linear regret during stationary periods
- Under-tilting: Persistent O(1) tails after regime switches
- Miscalibration: Per-switch regret grows with switch count instead of remaining bounded

**First 3 Experiments**
1. Implement two-expert switching environment with K regime switches
2. Compare TD-MD against Exponentiated Gradient and Fixed-Share baselines
3. Test stress signal detection robustness with noisy stress indicators

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Hyperparameters (η, λ) and stress signal triggering mechanism remain unspecified
- Assumes exact knowledge of KL drift or stress signals, limiting practical applicability
- Two-expert simplex focus provides clean theory but limits immediate generalizability

## Confidence

**High confidence**: Fenchel-duality equivalence; Õ(√T) dynamic regret guarantee; O(1) per-switch regret advantage

**Medium confidence**: Parameter-free adaptation mechanism; extension results to bandit feedback; Fragility Index characterization

**Low confidence**: Empirical performance without specific hyperparameters; plug-in KL-drift estimation effectiveness

## Next Checks

1. **Hyperparameter sensitivity analysis**: Implement two-expert switching environment and systematically vary η and λ to verify O(1) per-switch regret and Õ(√T) total regret. Test calibrated stress versus persistent over-tilting.

2. **Stress signal detection robustness**: Implement alternative stress detection mechanisms (e.g., sudden loss increases) to verify TD-MD achieves O(1) per-switch regret even with noisy stress signals.

3. **KL-drift estimation extension**: For Gaussian-distributed variant, implement plug-in KL-drift estimator and test whether trust-decay maintains O(√T) regret under estimated drift versus true drift.