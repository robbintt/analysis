---
ver: rpa2
title: Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large
  Language Models
arxiv_id: '2506.17580'
source_url: https://arxiv.org/abs/2506.17580
tags:
- knowledge
- wise
- information
- sources
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WISE (Workflow for Intelligent Scientific Knowledge Extraction)
  is a novel system that addresses the challenge of efficiently extracting and synthesizing
  knowledge from the rapidly growing volume of scientific literature. It combines
  LLMs with a structured, multi-layered workflow to iteratively filter, refine, and
  rank query-specific information.
---

# Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models

## Quick Facts
- arXiv ID: 2506.17580
- Source URL: https://arxiv.org/abs/2506.17580
- Authors: Sajratul Y. Rubaiat; Hasan M. Jamil
- Reference count: 40
- Key outcome: WISE reduces processed text by >80% while achieving 0.84 recall vs. 0.15-0.47 for baselines on HBB gene queries

## Executive Summary
WISE addresses the challenge of efficiently extracting and synthesizing knowledge from the rapidly growing volume of scientific literature. It combines LLMs with a structured, multi-layered workflow to iteratively filter, refine, and rank query-specific information. By using an LLM-powered, tree-based architecture, WISE focuses on contextually relevant and non-redundant content, employing dynamic scoring and ranking mechanisms to prioritize unique contributions from each source.

The system demonstrates significant improvements over baseline approaches, reducing processed text volume by over 80% while achieving substantially higher recall (0.84 vs. 0.15-0.47) and providing more in-depth information as measured by a novel level-based metric. Experiments on HBB gene-associated diseases show WISE's effectiveness in handling complex biomedical queries through adaptive stopping criteria that minimize processing overhead.

## Method Summary
WISE implements a four-stage pipeline: (1) LLM filtering extracts query-relevant content from raw sources, (2) scoring calculates uniqueness metrics balancing local efficiency and global contribution, (3) threshold pruning terminates exploration when additional contributions diminish, and (4) knowledge fusion merges selected sources into a growing knowledge container. The recursive Algorithm 1 explores top-k sources per layer, starting from initial retrieval and expanding through embedded links until scores drop below threshold T=20.

## Key Results
- Reduced processed text by 80.14% average (up to 96.12% in some cases)
- Achieved recall of 0.842 vs. 0.47 for ChatGPT, 0.15 for Gemini, 0.42 for ChatGPT with Search
- Demonstrated effectiveness on HBB gene-disease queries with level-based metric showing depth of information extraction

## Why This Works (Mechanism)

### Mechanism 1
LLM-driven query-specific filtering dramatically reduces text volume while preserving query-relevant information. The filtering function Γ leverages LLM contextual understanding to extract only content relevant to query q from raw source content C(s_i), discarding noise such as advertisements, unrelated sections, and boilerplate. This works because LLMs can reliably distinguish query-relevant from irrelevant content in scientific documents without losing semantically important but lexically dissimilar information.

### Mechanism 2
Scoring sources by their unique contribution to a growing knowledge container prioritizes novel information and reduces redundancy. The system calculates K(s_i) = w_filtered(s_i) - w_overlap(s_i, K_l), then normalizes via Score(s_i) = K(s_i) / log(1 + w_filtered(s_i) + |K_l|), balancing local efficiency and global contribution. This mechanism works because word-level overlap between filtered content and knowledge container accurately captures redundancy and novelty.

### Mechanism 3
Threshold-based pruning terminates exploration when additional contributions diminish, preventing wasted computation on low-yield paths. The system compares max_{s_i ∈ S_l} Score(s_i) to threshold T; if below T or no sources remain, recursion terminates. This works because score decay correlates with diminishing returns in knowledge acquisition, and the threshold value generalizes across queries.

## Foundational Learning

- **Tree-Based Recursive Information Retrieval**: Why needed - WISE expands from initial sources through embedded links, creating layers. Quick check - Starting from a UniProt entry, how would WISE determine which linked sources become second-layer nodes?
- **Knowledge Container Accumulation**: Why needed - The knowledge container K_l evolves across layers and serves as the reference for scoring novelty. Quick check - If K_l contains "sickle cell disease causes anemia," would a source discussing "sickle cell disease treatment protocols" receive a high or low uniqueness score?
- **Log-Scaled Normalization for Scoring**: Why needed - Raw word counts favor longer sources; log scaling balances source size against contribution magnitude to prevent size bias. Quick check - Why might a 100-word source with 50 unique words score differently than a 1000-word source with 500 unique words, even though both have 50% novelty?

## Architecture Onboarding

- **Component map**: Query q -> Initial retrieval Φ(q) -> Filter Γ -> Score Ψ -> Threshold check -> Top-k select -> Merge Λ -> Extract links -> Recurse until threshold met -> Return K_f
- **Critical path**: The recursive workflow processes sources layer by layer, filtering content, scoring for uniqueness, checking thresholds, selecting top-k sources, and merging into the knowledge container until termination criteria are met.
- **Design tradeoffs**: 
  1. Threshold T (empirically 20): Higher = more comprehensive but higher cost; lower = faster but may miss deep info
  2. Top-k per layer (k=2 in experiments): More sources = broader exploration; fewer = deeper but narrower
  3. Word-level vs. semantic overlap: Current approach is fast; knowledge graph integration would improve accuracy but adds complexity
- **Failure signatures**:
  1. Source access failures: Paywalls/bot detection blocked 10/34 sources - expect growing resistance from platforms
  2. Non-convergent scores: If scores don't decay, threshold never triggers - check if query is too broad or filtering failed
  3. Knowledge container bloat without recall gains: Uniqueness scoring miscalibrated; inspect overlap calculation
- **First 3 experiments**:
  1. Replicate HBB gene query against baselines (ChatGPT, ChatGPT with Search, Gemini, Google Search); measure recall, ROUGE/BLEU, and level-based detail to validate implementation
  2. Sweep threshold T from 10-50 on a held-out query; plot sources processed vs. recall to characterize sensitivity
  3. Apply unchanged architecture to a materials science query; verify domain transfer without code changes

## Open Questions the Paper Calls Out

### Open Question 1
Does replacing the text-based knowledge container with a knowledge graph structure significantly reduce information loss and improve reasoning about complex interdependencies? The current implementation relies on a text-based "knowledge container" which may struggle to maintain explicit relational continuity between entities across layers. Evidence would be a comparative study evaluating relational accuracy and completeness of answers generated via graph-based container versus current text-based method on complex multi-hop queries.

### Open Question 2
Can automatic query enhancement mechanisms effectively identify implicit constraints or underdefined goals in user queries to improve retrieval precision? The system currently processes queries as provided without iterative refinement or semantic augmentation to clarify ambiguous user intent. Evidence would be experiments measuring the change in recall and precision scores when using automated query augmentation layer versus static user inputs for ambiguous topics.

### Open Question 3
Would incorporating TF-IDF or semantic weighting into the scoring function improve the prioritization of unique, high-value sources compared to the current normalization method? The current scoring metric uses log scaling based on word counts, which may not accurately differentiate between common terminology and domain-specific, high-value terms. Evidence would be an ablation study comparing ranking quality using current scoring function against a modified version utilizing TF-IDF weighting.

## Limitations
- Unspecified LLM prompts for filtering and fusion functions are critical for reproducing reported performance
- Word-level overlap calculation may miss semantic equivalences, causing redundant information to score as novel
- Source access failures (10/34 blocked) suggest potential scalability issues with web-based data extraction

## Confidence

- **High confidence**: The overall tree-based architecture and recursive exploration mechanism are clearly specified and logically sound
- **Medium confidence**: The scoring function mechanics are detailed, but reliance on word-level overlap without semantic understanding introduces uncertainty
- **Low confidence**: Experimental results heavily depend on unspecified LLM prompts and similarity search implementation

## Next Checks

1. Replicate the HBB gene query against baselines (ChatGPT, ChatGPT with Search, Gemini, Google Search) using specified metrics to verify implementation matches reported results
2. Perform threshold sensitivity analysis by sweeping T from 10-50 on a held-out query, measuring sources processed vs. recall
3. Apply the unchanged architecture to a materials science query to test domain transfer capability without code modifications