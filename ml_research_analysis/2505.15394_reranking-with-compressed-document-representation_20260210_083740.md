---
ver: rpa2
title: Reranking with Compressed Document Representation
arxiv_id: '2505.15394'
source_url: https://arxiv.org/abs/2505.15394
tags:
- compressed
- document
- reranking
- arxiv
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RRK, a compressed representation for reranking
  that significantly improves efficiency while maintaining effectiveness. The method
  uses the PISCO compression model to convert documents into fixed-size embedding
  representations, which are then used to train a reranker through distillation from
  a state-of-the-art cross-encoder.
---

# Reranking with Compressed Document Representation

## Quick Facts
- arXiv ID: 2505.15394
- Source URL: https://arxiv.org/abs/2505.15394
- Reference count: 40
- Primary result: Compressed document embeddings enable 10-16× faster reranking with minimal effectiveness loss

## Executive Summary
This paper introduces RRK, a compressed representation for reranking that significantly improves efficiency while maintaining effectiveness. The method uses the PISCO compression model to convert documents into fixed-size embedding representations, which are then used to train a reranker through distillation from a state-of-the-art cross-encoder. The approach reduces input length to 32 tokens (8 memory tokens plus query), achieving up to 16x speedup compared to text-based rerankers. RRK matches the effectiveness of much slower text-based rerankers (ModernBERT and Mistral-7B) on standard IR benchmarks like BeIR and TREC, with only minor performance differences.

## Method Summary
RRK compresses documents into fixed-size embeddings using the PISCO model, then trains a Mistral-7B decoder with LoRA to perform reranking from these compressed inputs via distillation from a Naver-DeBERTa cross-encoder teacher. Documents are preprocessed offline with 8 memory tokens appended, producing 8-token embeddings. During training, these embeddings are concatenated with query tokens (max 24) to form a 32-token input. The student model learns to predict relevance scores via MSE loss against the teacher. Training uses MS MARCO passages with SPLADE-V3 retrieval for first-stage candidates, distilled over 2 epochs with batch size 8 on A100 hardware.

## Key Results
- Achieves 10-16× speedup over text-based reranking with only minor effectiveness loss
- Matches ModernBERT and Mistral-7B effectiveness on TREC-DL19/20 and BeIR benchmarks
- Maintains efficiency advantage even with long documents where text-based models slow down
- Underperforms on FEVER and Touché datasets, with unexplained effectiveness drops

## Why This Works (Mechanism)

### Mechanism 1: Offline Document Compression via Memory Tokens
Variable-length documents are compressed into fixed-size embeddings (8 memory tokens) while preserving sufficient semantic information for reranking decisions. The PISCO compressor appends learnable memory tokens to each document, processes the combined sequence through a frozen language model, and extracts the final hidden states of these memory tokens as the document representation. These embeddings are precomputed offline, decoupling compression cost from inference.

### Mechanism 2: Score-Based Distillation from Cross-Encoder Teacher
A large decoder model (Mistral-7B) learns to produce reranking scores from compressed inputs by distilling from a smaller but effective cross-encoder teacher. The teacher reranker (Naver-DeBERTa) produces relevance scores for query-document pairs. The student model is trained via MSE loss to match these scores, using only compressed document embeddings as input.

### Mechanism 3: Efficiency Gains from Constant Input Sequence Length
Fixing input length to 32 tokens (8 document + 24 query tokens) yields 10-16× speedup with effectiveness within 1 point NDCG@10 compared to text-based reranking. Text-based rerankers scale linearly with document length, while compressed inputs make inference time independent of original document length.

## Foundational Learning

- **Concept: Cross-Encoder Reranking**
  - Why needed here: The paper distills from a cross-encoder teacher; understanding why cross-encoders are effective but slow motivates the compression approach.
  - Quick check question: Can you explain why a cross-encoder (jointly encoding query and document) is more accurate but slower than bi-encoder retrieval?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: The entire training pipeline relies on distilling ranking knowledge from DeBERTa to Mistral-7B; understanding what transfers (scores vs. logits vs. rankings) is critical.
  - Quick check question: What information is lost when distilling only final scores rather than full probability distributions or intermediate representations?

- **Concept: Soft Prompts / Memory Tokens**
  - Why needed here: The compression mechanism uses appended learnable tokens whose hidden states become document representations—conceptually similar to soft prompting.
  - Quick check question: How do memory tokens differ from traditional pooling methods (CLS, mean pooling) for creating document embeddings?

## Architecture Onboarding

- **Component map:**
  [Document Collection] → [PISCO Compressor (frozen, offline)] → [Compressed Embeddings (8 tokens/doc)]
                                                                    ↓
  [Query] + [Top-50 Retrieved Docs] → [Load Compressed Embeddings] → [Mistral-7B Decoder + LoRA + Projection Head] → [Relevance Score]

- **Critical path:**
  1. Offline: Pre-compute and store PISCO embeddings for entire corpus (270GB for MSMARCO)
  2. Training: Distill from teacher using MSE loss on scores (24h on 1× A100, batch size 8)
  3. Inference: Load precomputed embeddings, concatenate with query tokens, forward through decoder

- **Design tradeoffs:**
  - Storage vs. speed: 270GB index is larger than ColBERT (154GB), but enables reuse for RAG generation
  - Layers vs. effectiveness: 16-layer RRK is faster but loses ~2 points NDCG; 32-layer is more competitive
  - Compression rate vs. fidelity: 8 tokens may be insufficient for some datasets (FEVER, Touché underperform)
  - Assumption: Current PISCO compressor trained on ≤128 tokens; longer documents may compress poorly

- **Failure signatures:**
  - NDCG drops >2 points on specific datasets → likely compression information bottleneck (check FEVER, Touché patterns)
  - Efficiency gains disappear → query length likely too long; verify query token count
  - Training fails to converge with smaller models → paper notes 1B models "failed miserably"; start with 7B backbone
  - Performance degrades on long documents → compressor training distribution mismatch (trained on 128 tokens)

- **First 3 experiments:**
  1. **Reproduce distillation baseline:** Train Mistral-7B with text input (no compression) using identical distillation setup to isolate compression effect; compare NDCG on TREC-DL19/20 and BeIR subset.
  2. **Ablate compression tokens:** Vary memory token count (4, 8, 16) to measure information capacity vs. efficiency tradeoff; expect diminishing returns beyond 8 but may reveal dataset-specific needs.
  3. **Long document stress test:** Evaluate on TREC-NEWS at 768 and 1024 tokens; compare RRK against ModernBERT to validate paper's claim that RRK becomes competitive when text models slow down. Monitor for the performance drop at 1024 tokens that the paper attributes to compressor training limits.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can compressed document representation reranking be effectively implemented with smaller models (e.g., 1B parameters or less)?
  - Basis in paper: "Unfortunately, our initial attempts to use smaller models, such as 1B parameter models, have not yet been successful (to say the least, they failed miserably)."
  - Why unresolved: The paper only reports success with a 7B parameter model; smaller architectures failed to learn from compressed representations despite multiple attempts.
  - What evidence would resolve it: Successful training of a sub-1B parameter model using compressed document embeddings that achieves competitive nDCG@10 scores on BeIR benchmarks within 1-2 points of the 7B variant.

- **Open Question 2:** Why does the RRK model underperform on specific datasets like FEVER and Touché, and can this weakness be remedied?
  - Basis in paper: "We can not explain yet this weakness of the RRK model with these two datasets."
  - Why unresolved: The authors observe consistent underperformance on FEVER (argument verification) and Touché (controversial topics) but offer no hypothesis for whether the issue stems from compression, training data, or task-specific factors.
  - What evidence would resolve it: Ablation studies isolating the compression component on these datasets, or domain-adapted training that closes the performance gap to within 0.5 points NDCG@10 of text-based rerankers.

- **Open Question 3:** How can compressed reranking maintain efficiency advantages when queries are long (comparable to document length)?
  - Basis in paper: "Using datasets like Su et al., where queries length is comparable to (BeIR) document length, breaks this advantage and makes the RRK model slow."
  - Why unresolved: The efficiency gains derive from fixed short input (32 tokens). Long queries eliminate this advantage, but no solution is proposed.
  - What evidence would resolve it: A method for compressing query representations similarly to documents, demonstrating constant-time reranking regardless of query length on long-query benchmarks.

## Limitations

- **Compressor Capacity Mismatch:** PISCO compressor trained on documents up to 128 tokens, yet evaluated on documents up to 1024 tokens without examining compression quality degradation.
- **Storage Overhead:** RRK requires 270GB for MS MARCO passage compression, larger than ColBERT's 154GB, creating practical deployment barriers.
- **Dataset-Specific Performance:** RRK shows significant effectiveness drops on FEVER and Touché datasets, suggesting the 8-memory-token representation has insufficient capacity for certain retrieval tasks.

## Confidence

- **High Confidence:** The efficiency claims (10-16× speedup) are well-supported by the fixed 32-token input mechanism and clear benchmarks against text-based models.
- **Medium Confidence:** The effectiveness claims hold for the majority of BeIR datasets and TREC evaluations, but the FEVER and Touché failures suggest the compression approach has fundamental limitations.
- **Low Confidence:** The claim that 8 memory tokens optimally balance compression and effectiveness is not validated - no ablation studies on memory token count are provided.

## Next Checks

1. **Compression Token Ablation:** Systematically evaluate RRK with 4, 8, and 16 memory tokens on TREC-DL19/20 and BeIR to quantify the information capacity vs. efficiency tradeoff and determine if 8 tokens is optimal or merely sufficient.

2. **Compressor Capacity Stress Test:** Evaluate RRK on long documents (768-1024 tokens) while measuring actual compression quality degradation, not just final retrieval metrics, to validate the paper's claim that compressor training limits cause performance drops.

3. **Storage-Efficiency Tradeoff Analysis:** Compare RRK's 270GB storage requirement against the runtime efficiency gains across different document collection sizes to determine the break-even point where traditional reranking becomes more practical.