---
ver: rpa2
title: 'Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis'
arxiv_id: '2504.21061'
source_url: https://arxiv.org/abs/2504.21061
tags:
- code
- specifications
- specification
- program
- pathcrawler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores how large language models (LLMs) can be used
  to generate formal specifications from C code, using Deepseek-R1 and the ACSL specification
  language. It addresses two key challenges: generating specifications from buggy
  code and combining symbolic analysis with LLM-based synthesis.'
---

# Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis

## Quick Facts
- **arXiv ID:** 2504.21061
- **Source URL:** https://arxiv.org/abs/2504.21061
- **Reference count:** 37
- **Primary result:** Deepseek-R1 can generate formal ACSL specifications from buggy, anonymized C code, with neuro-symbolic prompting shaping specification characteristics.

## Executive Summary
This paper investigates the use of large language models (LLMs) for generating formal specifications from C code, focusing on the ACSL specification language. The authors address two key challenges: inferring intended program behavior from buggy code, and integrating symbolic analysis with LLM-based synthesis. Using Deepseek-R1, they conduct experiments on 50 C programs with intentional bugs and anonymized function names, demonstrating the model's ability to identify program intent despite these obfuscations. The study also explores how augmenting LLM prompts with outputs from Frama-C tools (PathCrawler for test generation and EVA for static analysis) influences the generated specifications, showing that symbolic guidance can effectively shape specification content toward desired properties.

## Method Summary
The study employs Deepseek-R1 to generate ACSL specifications (preconditions, postconditions, assigns clauses) for C code. Experiments use two datasets: "intent_tests" (50 C programs with bugs and anonymized names) and "pathcrawler_tests" (55 C programs from tool developers). Three prompting strategies are compared: a baseline prompt, a prompt augmented with PathCrawler-generated test cases, and a prompt augmented with EVA static analysis results. The LLM is configured with temperature 0.7 and generates three specifications per program. Qualitative analysis examines reasoning traces, while quantitative counts track annotation types. Frama-C tools provide symbolic analysis outputs that are incorporated into prompts to guide specification synthesis.

## Key Results
- Deepseek-R1 effectively identifies program intent from buggy and anonymized C code, even when implementation differs from specification.
- PathCrawler-augmented prompts encourage more abstract postconditions by providing input/output examples.
- EVA-augmented prompts increase preconditions to prevent runtime errors identified by static analysis, though this can reduce focus on functional logic.

## Why This Works (Mechanism)
The approach works by leveraging the LLM's ability to parse and reason about code structure, while using symbolic analysis outputs to provide concrete constraints and examples. The LLM can infer intended behavior from code patterns even when implementation is buggy or obfuscated. Symbolic tools like PathCrawler and EVA provide domain-specific knowledge about program behavior and potential errors, which the LLM incorporates into its generated specifications. This neuro-symbolic combination allows the LLM to ground its abstract reasoning in concrete program analysis results.

## Foundational Learning
- **ACSL specification language:** Formal annotation syntax for C code; needed to understand what the LLM generates and how specifications are structured.
- **Frama-C ecosystem:** Static analysis and verification framework; needed to comprehend the symbolic tools (PathCrawler, EVA) used to augment prompts.
- **LLM prompting strategies:** Techniques for guiding model output; needed to understand how different prompt structures influence specification characteristics.
- **Test generation vs. static analysis:** Complementary verification approaches; needed to grasp why PathCrawler and EVA provide different types of guidance to the LLM.
- **Intent vs. implementation distinction:** Core challenge in specification synthesis; needed to understand why buggy code poses a unique problem for specification generation.

## Architecture Onboarding

### Component Map
Frama-C Tools (PathCrawler/EVA) -> Prompt Augmentation -> Deepseek-R1 LLM -> ACSL Specifications

### Critical Path
1. Run C code through Frama-C tools to generate analysis outputs
2. Construct augmented prompt with code + symbolic analysis results
3. Submit to Deepseek-R1 with temperature 0.7, 3 generations
4. Parse and evaluate generated ACSL specifications

### Design Tradeoffs
- **General vs. domain-specific LLMs:** Using Deepseek-R1 (general) versus fine-tuning on ACSL examples; tradeoff between flexibility and precision
- **Symbolic vs. LLM-only approaches:** Combining symbolic analysis with LLM reasoning versus pure LLM synthesis; tradeoff between control and automation
- **Prompt complexity:** More detailed prompts with symbolic analysis versus simpler prompts; tradeoff between specification quality and prompt engineering effort

### Failure Signatures
- **Buggy implementation specification:** LLM detects bug but specifies incorrect behavior anyway
- **EVA tunnel-vision:** Excessive preconditions generated while postconditions become weak or absent
- **PathCrawler over-generalization:** LLM creates overly abstract specifications based on single test case

### First 3 Experiments
1. Run baseline prompt on a sample buggy program and verify reasoning trace correctly identifies intended behavior
2. Apply EVA prompt augmentation to a program with runtime errors and check if generated preconditions prevent those errors
3. Use PathCrawler prompt on a function and verify if generated postconditions reflect abstraction from provided I/O examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning LLMs on domain-specific datasets of annotated code significantly improve the quality of generated specifications compared to general-purpose prompting?
- Basis in paper: The authors state in the Future Work section their aim to "experiment with fine-tuning models specifically for contract-based deductive verification" using datasets like ACSL or JML.
- Why unresolved: Current experiments utilized general-purpose models (Deepseek-R1, GPT-4) without domain-specific weight adjustments.
- What evidence would resolve it: A comparative benchmark evaluating specification correctness between a fine-tuned model and the baseline Deepseek-R1 model.

### Open Question 2
- Question: Does a dataset of programs specifically curated for "realistic verifiability" enable the successful integration of LLM-synthesized specifications into formal verification pipelines?
- Basis in paper: The authors note in Future Work that earlier verification attempts failed due to missing loop invariants and syntax issues, prompting a need for "a dataset that includes a larger set of programs that are realistically verifiable."
- Why unresolved: The datasets used in this study were either handcrafted for intent analysis or borrowed from existing tool test suites, limiting the evaluation of end-to-end verification.
- What evidence would resolve it: A study measuring the pass-rate of automated verification attempts on this new dataset using specifications generated by the LLM.

### Open Question 3
- Question: How must symbolic test generation tools be adapted to produce input/output examples optimized for LLM generalization rather than just minimal path coverage?
- Basis in paper: The authors note in Section 4.6.3 that PathCrawler's design goal of producing the "minimal number of test cases" is "not necessarily optimal for specification synthesis," often leading the LLM to generalize incorrectly from single examples.
- Why unresolved: The paper establishes that "high quality" examples improve results but does not define or build a tool to generate such examples specifically for LLM consumption.
- What evidence would resolve it: Experiments using test generators tuned for semantic diversity rather than structural coverage, measuring the resulting accuracy of generated specifications.

## Limitations
- Results rely heavily on qualitative analysis and partially closed datasets, limiting independent verification.
- Experiments use only one LLM (Deepseek-R1) and one specification language (ACSL), raising generalizability concerns.
- The subjective assessment of "intended behavior" detection may not scale to more complex programs.

## Confidence
- **High Confidence:** Deepseek-R1 can parse anonymized and buggy C code to produce valid ACSL specifications (directly observable from reasoning traces).
- **Medium Confidence:** Neuro-symbolic prompting effectively shapes specification characteristics (requires interpretation of qualitative results and relies on partially closed dataset).
- **Low Confidence:** Broader claims about general viability for production software verification (limited scope and lack of systematic error analysis).

## Next Checks
1. Replicate the "intent_tests" experiment with an additional LLM (e.g., GPT-4) to assess robustness of "intent prioritization" approach across models.
2. Manually construct a small, public dataset of buggy C programs with known intended behavior to test LLM's ability to distinguish intent from implementation in controlled scenarios.
3. Analyze a sample of generated specifications from the "pathcrawler_tests" suite (if accessible) to verify reported trends in precondition and postcondition density under different prompting strategies.