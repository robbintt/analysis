---
ver: rpa2
title: 'Reasoning with Preference Constraints: A Benchmark for Language Models in
  Many-to-One Matching Markets'
arxiv_id: '2509.13131'
source_url: https://arxiv.org/abs/2509.13131
tags:
- matching
- prompting
- reasoning
- llms
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new benchmark for evaluating large language\
  \ models (LLMs) on the College Admission Problem, a many-to-one matching problem\
  \ requiring reasoning under preferential and structural constraints. The benchmark\
  \ includes 369 instances with varying numbers of students (5\u201320), preference\
  \ types (complete, incomplete, flexible), and capacity settings (under-, exact-,\
  \ over-capacity)."
---

# Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets

## Quick Facts
- arXiv ID: 2509.13131
- Source URL: https://arxiv.org/abs/2509.13131
- Reference count: 40
- Primary result: Reasoning models (QwQ, GPT-oss) significantly outperform base models on feasibility, stability, and optimality in many-to-one matching, but all models struggle with larger instances and non-monotonic iterative prompting.

## Executive Summary
This paper introduces a new benchmark for evaluating large language models (LLMs) on the College Admission Problem, a many-to-one matching problem requiring reasoning under preferential and structural constraints. The benchmark includes 369 instances with varying numbers of students (5–20), preference types (complete, incomplete, flexible), and capacity settings (under-, exact-, over-capacity). Six open-weight LLMs—Llama, Mistral, Qwen, QwQ, and GPT-oss—were tested under eight prompting strategies (basic, role-based, in-context learning, chain-of-thought variants, and iterative prompting). Results show that while reasoning models (QwQ, GPT-oss) significantly outperform base models on feasibility, stability, and optimality, all models struggle as instance size increases. No single prompt strategy consistently delivers the best performance across all models. Iterative prompting does not guarantee monotonic improvement, suggesting that generating diverse outputs may be more beneficial than iterative self-correction in this context.

## Method Summary
The study evaluates six LLMs (Llama 3 8B/70B, Mistral 7B, Qwen2 7B, QwQ 32B, GPT-oss 120B) across 8 prompting strategies on a benchmark of 369 synthetic College Admission Problem instances. Each instance specifies students, colleges, capacities, and preference lists. The ground truth is computed using the Gale-Shapley Deferred Acceptance algorithm. Outputs are evaluated on four metrics: feasibility (capacity and assignment constraints), assignment stability, matching stability (no blocking pairs), and student optimality (minimizing sum of student ranks). Iterative prompting with up to 5 attempts is tested using external feedback.

## Key Results
- Reasoning-specialized models (QwQ, GPT-oss) outperform base models on all metrics, with QwQ achieving >90% feasibility even at 20 students.
- Prompt strategy effectiveness is highly model-dependent: base models benefit from detailed algorithmic prompts (ICL with steps), while reasoning models perform worse with such specificity.
- Iterative prompting improves feasibility but does not guarantee monotonic improvement; performance can peak early and decline in later attempts.
- All models struggle with larger instances (15–20 students), with feasibility dropping sharply for base models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reasoning-specialized models (QwQ, GPT-oss) outperform general-purpose base models (Llama, Mistral) on combinatorial optimization tasks with structural constraints, contingent on the task involving iterative logic rather than simple pattern matching.
- **Mechanism:** Reinforcement learning or supervised fine-tuning specifically for "thinking" appears to internalize a step-by-step verification process (similar to Chain-of-Thought) that base models require external prompting to achieve.
- **Core assumption:** The superior performance stems from learned reasoning architectures rather than merely larger parameter counts, as evidenced by QwQ (32B) outperforming Llama 70B.
- **Evidence anchors:**
  - [abstract] "Reasoning LLMs, like QwQ and GPT-oss, significantly outperform traditional models... defined here as models used without any dedicated reasoning mechanisms."
  - [section 4, Page 6] Figure 2 shows reasoning models maintaining feasibility >90% even as student numbers increase, while base models drop sharply.
  - [corpus] The neighbor paper "Matching Markets Meet LLMs" (Hosseini et al.) corroborates that algorithmic reasoning with ranked preferences remains a distinct challenge for standard LLMs.
- **Break condition:** Performance gains diminish if the problem size exceeds the model's effective context window or if the preference structure requires tracking dependencies deeper than the model's reasoning depth.

### Mechanism 2
- **Claim:** Prompting strategies (CoT, ICL) show high variance in effectiveness depending on model architecture; detailed algorithmic prompts aid base models but can degrade reasoning model performance.
- **Mechanism:** Base models rely on "scaffolding" (detailed pseudocode or ICL steps) to guide inference steps, whereas reasoning models may interpret specific constraints as conflicting signals or unnecessary noise, preferring general guidelines like "Think step by step."
- **Core assumption:** Reasoning models develop an internal planning state that is brittle to over-specification or format constraints that differ from their training distribution.
- **Evidence anchors:**
  - [section 4, Page 7] "For the base models, the best prompts... perform worst on every metric for reasoning models."
  - [section 3.3, Page 5] Describes distinct prompt strategies (CoT text, pseudocode, Python).
  - [corpus] Corpus signals (avg neighbor FMR 0.39) suggest general reasoning benchmarks struggle to capture this specific prompt-model coupling, highlighting a gap in static evaluation sets.
- **Break condition:** If a prompt forces a reasoning model to output an intermediate format (e.g., specific pseudocode syntax) not native to its reasoning trace, performance degrades.

### Mechanism 3
- **Claim:** Iterative prompting with auto-generated feedback improves feasibility but does not guarantee monotonic convergence to optimality.
- **Mechanism:** Iterative feedback acts as a diverse sampling mechanism rather than a strict error-correction gradient; models may "forget" previously satisfied constraints when correcting others (catastrophic interference in context).
- **Core assumption:** The model treats the feedback as a new context constraint rather than a cumulative patch, leading to oscillation between error states.
- **Evidence anchors:**
  - [abstract] "Iterative prompting... results are not monotonic; they can peak early and then significantly decline in later attempts."
  - [section 4, Page 8] Table 1 shows QwQ and base models fluctuating in optimality between the "Last" and "Best" attempts.
  - [corpus] Evidence is weak or missing in the provided corpus neighbors regarding the specific non-monotonicity of iterative prompting, marking this as a novel finding of this specific paper.
- **Break condition:** Improvement stops if the feedback loop introduces logical contradictions or if the token limit truncates the history required for cumulative correction.

## Foundational Learning

- **Concept:** **Gale-Shapley Deferred Acceptance (DA) Algorithm**
  - **Why needed here:** This is the ground-truth solver used to evaluate the LLMs. Without understanding that DA produces a student-optimal stable matching, one cannot interpret the "optimality" metric or the "stability" checks for blocking pairs.
  - **Quick check question:** Can you explain why a matching is considered "unstable" if a student-college pair would both prefer each other over their current assignments?

- **Concept:** **Many-to-One Matching Markets**
  - **Why needed here:** Unlike one-to-one matching, this involves capacity constraints ($q_c$) and complex preference lists. The paper specifically tests if LLMs can respect capacity (feasibility) while navigating preferences (stability).
  - **Quick check question:** In a many-to-one market, does a college's capacity have to equal the number of students it ranks, or can it be strictly less?

- **Concept:** **Blocking Pairs & Stability**
  - **Why needed here:** The paper distinguishes between "feasibility" (valid format/capacity) and "stability" (no blocking pairs). Many models achieved feasibility but failed stability.
  - **Quick check question:** If Student A is unmatched and College B has an open slot, do they necessarily form a blocking pair?

## Architecture Onboarding

- **Component map:** Benchmark Generator -> Prompting Engine -> LLM Solver -> Validator
- **Critical path:** The **Validator**. Reliability depends entirely on the correctness of the ground-truth DA implementation. If the validator is buggy, all feasibility/stability metrics are invalid.
- **Design tradeoffs:**
  - **Instance Size vs. Context Window:** The paper limits to 20 students to fit token limits for smaller models (Llama 8B).
  - **Prompt Detail vs. Reasoning Capability:** Detailed algorithmic prompts help base models but hurt reasoning models; the system must select the prompt strategy based on the target model class.
- **Failure signatures:**
  - **Format Drift:** Models outputting names like "Alice" instead of "s1" (Invalid Output, Page 14).
  - **Metric Asymmetry:** High "Assignment Stability" but low "Feasibility" (seen in Mistral), meaning the model understands blocking pairs but fails basic arithmetic on capacity counts.
  - **Non-monotonic Iteration:** Performance peaking at iteration 2 and crashing by iteration 5.
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run the 5-student "Basic" prompt on a reasoning model (QwQ) to verify if it achieves >90% optimality without specialized prompting.
  2. **Scaling Stress Test:** Run a 20-student instance on a base model (Llama 70B) using "ICL with steps" to observe the degradation rate of feasibility as solution space expands.
  3. **Iterative Oscillation Test:** Run iterative prompting on a base model for 5 iterations; check if the "Best" attempt differs significantly from the "Last" attempt to validate the non-monotonicity claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does iterative prompting improve LLM performance through genuine self-correction and feedback integration, or merely through increased answer diversity from multiple attempts?
- **Basis in paper:** [explicit] Authors state "more experiments are necessary to determine whether the performances increase as a result of potential self-correction or simply by an increase in the answer diversity."
- **Why unresolved:** The study observed non-monotonic improvements in iterative prompting, but could not isolate whether feedback was meaningfully used or if multiple independent attempts drove gains.
- **What evidence would resolve it:** Controlled experiments comparing iterative prompting with feedback versus repeated independent sampling without feedback, holding the number of attempts constant.

### Open Question 2
- **Question:** How does varying the granularity and specificity of feedback (e.g., reporting exact blocking pairs versus aggregate stability metrics) affect LLMs' ability to correct matchings?
- **Basis in paper:** [explicit] Authors propose "experimenting with different feedback forms that vary in the level of detail provided regarding the encountered errors, like specifying the exact unstable pairs rather than only reporting the total number of unstable pairs."
- **Why unresolved:** Current experiments used only binary metric-level feedback; the impact of more informative, targeted feedback on correction capability remains untested.
- **What evidence would resolve it:** Systematic evaluation of LLMs under feedback conditions differing in specificity (exact blocking pairs vs. counts vs. binary flags), measuring improvement rates per iteration.

### Open Question 3
- **Question:** Can LLMs generalize to recommend appropriate matching algorithms (e.g., Deferred Acceptance vs. Top Trading Cycles) based on user-specified priorities such as efficiency, stability, or fairness?
- **Basis in paper:** [explicit] Authors suggest LLMs could serve "as interpreters capable of suggesting appropriate approaches based on the context and users' priorities through even deeper reasoning."
- **Why unresolved:** The benchmark only evaluated LLMs executing one prescribed algorithm; their capacity for algorithm selection aligned with preferential constraints was not assessed.
- **What evidence would resolve it:** A benchmark where LLMs must choose among multiple matching mechanisms given natural language descriptions of user priorities, evaluated on alignment between selected mechanism and stated objectives.

## Limitations

- The core findings hinge on the correctness and completeness of the ground-truth Deferred Acceptance implementation. Any bugs in this validator would invalidate all feasibility/stability/optimality metrics.
- Without access to the actual dataset or model inference parameters (temperature, max_tokens), exact replication is blocked pending publication.
- The non-monotonicity of iterative prompting is identified as a novel finding, but the paper provides limited analysis of why oscillation occurs (e.g., whether it's model-specific or a general limitation of context-based feedback).

## Confidence

- **High Confidence:** The general trend that reasoning-specialized models outperform base models on combinatorial tasks requiring iterative logic (Mechanism 1). This is well-supported by the literature and the paper's results.
- **Medium Confidence:** The claim that prompt strategies are highly model-dependent (Mechanism 2). While the results are clear, the underlying cognitive mechanism (why detailed pseudocode helps base models but hurts reasoning models) remains speculative without deeper interpretability analysis.
- **Medium Confidence:** The observation that iterative prompting does not guarantee monotonic improvement (Mechanism 3). The paper shows the phenomenon, but the explanation (catastrophic interference in context) is a hypothesis, not proven.

## Next Checks

1. **Validator Cross-Check:** Implement a second, independent version of the Gale-Shapley DA algorithm and verify it produces identical student-optimal matchings on a subset of instances. This is critical before trusting any reported metric.
2. **Scaling Threshold Identification:** Systematically test a reasoning model (QwQ) and a base model (Llama 70B) across a finer-grained range of student counts (e.g., 5, 8, 10, 13, 15, 18, 20) to pinpoint the exact instance size where feasibility/stability drops below 80%.
3. **Iterative Prompting Ablation:** For a base model, run iterative prompting but with a fixed "best-of-n" strategy (keep the best attempt seen so far) rather than the "last attempt." Compare if this mitigates the non-monotonicity by preventing degradation from later, worse iterations.