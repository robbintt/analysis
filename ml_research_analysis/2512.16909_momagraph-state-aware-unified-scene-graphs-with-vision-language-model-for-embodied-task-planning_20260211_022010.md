---
ver: rpa2
title: 'MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for
  Embodied Task Planning'
arxiv_id: '2512.16909'
source_url: https://arxiv.org/abs/2512.16909
tags:
- scene
- task
- graph
- spatial
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling mobile manipulators
  to perform both navigation and manipulation in household environments by introducing
  a unified scene graph representation that integrates spatial and functional relationships
  at the part level. The authors develop MomaGraph, a task-specific scene graph that
  captures not only where objects are but also how they function and which parts are
  interactive, along with MomaGraph-Scenes, the first large-scale dataset for such
  task-driven scene graphs.
---

# MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning

## Quick Facts
- arXiv ID: 2512.16909
- Source URL: https://arxiv.org/abs/2512.16909
- Reference count: 37
- Primary result: Achieves 71.6% accuracy on MomaGraph-Bench, outperforming open-source baselines by +11.4% and matching closed-source models

## Executive Summary
This paper introduces MomaGraph, a task-oriented scene graph representation that unifies spatial and functional relationships at the part level for mobile manipulation in household environments. The authors develop MomaGraph-R1, a 7B vision-language model trained via reinforcement learning with graph-alignment rewards, to generate structured scene graphs from multi-view images and language instructions. The system demonstrates strong performance on task planning benchmarks and real-robot experiments, achieving state-of-the-art results among open-source models while maintaining competitive performance with closed-source alternatives.

## Method Summary
MomaGraph combines multi-view RGB images and natural language instructions as input to generate task-oriented scene graphs using MomaGraph-R1, a Qwen2.5-VL-7B model fine-tuned with DAPO reinforcement learning. The graph structure captures objects, part-level elements, spatial relationships (9 types), and functional relationships (6 types), along with action sequences. Training uses a composite reward function combining action type accuracy, edge similarity, node completeness, and format compliance. The system operates under a Graph-then-Plan framework where the generated graph conditions planning, with a state-aware update module to refine the graph based on observed state changes during task execution.

## Key Results
- MomaGraph-R1 achieves 71.6% accuracy on MomaGraph-Bench, outperforming open-source baselines by +11.4% and matching closed-source models
- The unified spatial-functional representation outperforms spatial-only (59.9%) and functional-only (64.9%) variants
- RL training with graph-alignment rewards improves performance from 63.9% (SFT) and 60.2% (ICL) to 71.6%
- Real-robot experiments show 70% success rate on 10 multi-step tasks with an average execution time of 186 seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured scene graph generation as an intermediate representation improves downstream task planning accuracy compared to direct planning from visual inputs.
- Mechanism: The Graph-then-Plan framework forces the model to explicitly reason about object identities, relationships, and task relevance before generating action sequences, reducing hallucination and improving reasoning traceability.
- Core assumption: Accurate scene graphs provide sufficient grounding for planning; errors in graph generation will propagate to planning.
- Evidence anchors:
  - [abstract] "MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework"
  - [section 6.1, Table 2] "Across all models, the w/ Graph setting consistently outperforms the w/o Graph baseline"
  - [corpus] ESCA paper addresses similar grounding challenges via scene-graph generation for embodied agents
- Break condition: If scene graph generation quality is low (missing nodes, wrong edges), planning accuracy degrades; observed in failure analysis (Figure 6b) where graph errors compound.

### Mechanism 2
- Claim: Unified spatial-functional scene graphs provide more complete task-relevant representations than single-relationship graphs.
- Mechanism: Spatial edges (e.g., "in front of," "touching") capture geometric constraints for navigation/reachability; functional edges (e.g., "control," "adjust") capture operability and causal dependencies for manipulation. Joint modeling enables reasoning about both "where" and "how."
- Core assumption: Tasks require reasoning about both spatial layout and functional affordances; pure spatial or pure functional representations create blind spots.
- Evidence anchors:
  - [section 3.2, Table 1] Unified representation achieves 71.6% overall vs. 59.9% (spatial-only) and 64.9% (functional-only)
  - [section 1] "Relying solely on spatial relationships captures geometric layout but overlooks operability"
  - [corpus] ZING-3D and MSGNav papers focus primarily on spatial scene graphs without unified functional integration
- Break condition: When tasks require only spatial reasoning (navigation-only) or only functional reasoning (known-object manipulation), unified representation may add unnecessary complexity.

### Mechanism 3
- Claim: Reinforcement learning with graph-alignment rewards teaches VLMs to generate more accurate task-oriented scene graphs than supervised fine-tuning or in-context learning alone.
- Mechanism: The composite reward function (action type + edge similarity + node completeness + format) provides dense, structured feedback that encourages the model to explore reasoning strategies rather than pattern-match to training examples.
- Core assumption: RL training enables generalization beyond the specific graph structures in training data; reward design captures task-relevant graph properties.
- Evidence anchors:
  - [section 4.2] Reward function R(G) combines R_action, R_edges, R_nodes with format and length penalties
  - [appendix B.1, Table 5] RL achieves 71.6% vs. 63.9% (SFT) and 60.2% (ICL) on MomaGraph-Bench
  - [corpus] Corpus lacks direct RL-vs-SFT comparisons for scene graph generation; this mechanism is primarily evidenced within the paper
- Break condition: If reward function misaligns with true task requirements (e.g., over-weighting format over semantic accuracy), model may optimize for wrong objective.

## Foundational Learning

- **Scene Graphs (3D Indoor)**
  - Why needed here: MomaGraph's core output is a structured graph G_T = (N_T, E_s, E_f) representing objects, spatial relations, and functional relations. Understanding graph terminology is essential.
  - Quick check question: Given a kitchen scene with a stove, knob, and pot, can you identify at least one spatial edge and one functional edge?

- **Vision-Language Models (VLMs) for Embodied AI**
  - Why needed here: MomaGraph-R1 is built on Qwen2.5-VL-7B-Instruct. Understanding how VLMs process multi-view images and task instructions is prerequisite to the architecture.
  - Quick check question: What is the input and output format of a VLM when used for scene understanding?

- **Reinforcement Learning with Verifiable Rewards (DAPO)**
  - Why needed here: The training methodology uses DAPO with a custom graph-alignment reward. Understanding reward shaping is critical for reproducing or extending this work.
  - Quick check question: In RL for structured output generation, why might a composite reward outperform simple accuracy-based rewards?

## Architecture Onboarding

- **Component map:**
  Multi-view images + language instruction -> MomaGraph-R1 -> Scene graph (nodes, spatial edges, functional edges, action type) -> Planner -> Action sequence

- **Critical path:** Multi-view perception quality -> Graph generation accuracy (nodes, edges, action type) -> Planning correctness. Failure at graph stage propagates; observed 80% graph success -> 87.5% planning success -> 70% overall task success (Figure 6).

- **Design tradeoffs:**
  - Model size (7B) vs. performance: Competitive with closed-source models but not superior; smaller models may sacrifice reasoning depth
  - Dataset coverage (350 scenes, 93 tasks) vs. generalization: Designed for household domains; may not transfer to industrial/outdoor settings
  - Part-level granularity vs. annotation cost: Enables fine-grained interaction but requires detailed labeling

- **Failure signatures:**
  - Missing nodes: Task-relevant objects not detected in multi-view aggregation
  - Wrong edge types: Spatial/functional relationships misclassified (e.g., "control" vs. "adjust")
  - Action sequencing errors: Correct actions in suboptimal order (Section 6.4 failure analysis)
  - State-update failures: Graph not refined correctly after interaction

- **First 3 experiments:**
  1. **Reproduce benchmark results**: Run MomaGraph-R1 on MomaGraph-Bench T1-T4, compare with Table 2 baselines (Qwen2.5-VL-7B-Instruct, LLaVA-Onevision)
  2. **Ablate unified representation**: Train spatial-only and functional-only variants on same data, verify Table 1 pattern holds
  3. **Real-robot validation**: Deploy on mobile manipulator with simple task ("Open the cabinet"), verify graph generation -> planning -> execution pipeline; measure success rate and failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an agent learn an optimal interaction policy to efficiently resolve functional relationship uncertainties (e.g., identifying which knob controls which burner) rather than relying on random exploration?
- Basis in paper: [explicit] "In this work, we do not focus on the agent's interaction policy; instead, our emphasis lies on how to capture and incorporate observed state changes in the environment into the scene graph to resolve such ambiguities."
- Why unresolved: The paper addresses state-aware graph updates once interactions occur, but leaves open how the agent should strategically select interactions to minimize uncertainty resolution cost.
- What evidence would resolve it: A method that learns to predict which interactions will maximally reduce graph uncertainty, evaluated on tasks requiring sequential hypothesis testing with efficiency metrics (e.g., number of interactions needed to resolve ambiguities).

### Open Question 2
- Question: Can the unified spatial-functional scene graph representation scale to multi-room or building-level environments while maintaining the task-relevant compactness that MomaGraph achieves for single rooms?
- Basis in paper: [inferred] The paper defines MomaGraph for "a single indoor room" and the dataset spans "350 diverse household scenes" but does not address connected multi-room planning scenarios common in real households.
- Why unresolved: Task planning often requires navigating between rooms; whether the graph-then-plan framework degrades with larger, more complex scene graphs remains untested.
- What evidence would resolve it: Experiments on tasks requiring inter-room navigation and manipulation (e.g., "bring me water from the kitchen") measuring planning accuracy and computational cost as scene graph size increases.

### Open Question 3
- Question: What mechanisms could enable recovery from the identified failure modes—spatial relation errors during graph generation and action sequencing errors during planning—without full task restart?
- Basis in paper: [explicit] "The main failure modes were: (1) spatial relation errors or missing nodes during graph generation; and (2) action sequencing errors in the planning phase."
- Why unresolved: The current system achieves 70% success on multi-step tasks but lacks explicit error detection and recovery mechanisms to handle mid-execution failures gracefully.
- What evidence would resolve it: Implementation of failure detection modules and replanning strategies, evaluated by success rate improvement on the same long-horizon tasks and analysis of recovery trajectories.

### Open Question 4
- Question: How robust is MomaGraph-R1 to domain shift when deployed in environments significantly different from the training distribution (e.g., industrial kitchens, laboratories, or outdoor-adjacent spaces)?
- Basis in paper: [inferred] The training data spans household environments (kitchen, living room, bedroom, bathroom) and experiments use "unseen environments," but these remain within the domestic domain.
- Why unresolved: Functional relationships (e.g., specialized equipment controls) and spatial configurations differ substantially across domains; zero-shot transfer to non-household settings is unverified.
- What evidence would resolve it: Cross-domain benchmarking on out-of-distribution scene types, reporting accuracy degradation curves as a function of domain similarity to training data.

## Limitations
- The evaluation relies heavily on synthetic or curated benchmarks, with real-world robot performance reported only for a small set of tasks (10 trials)
- The dataset coverage of 350+ scenes may not capture the full variability of household environments, potentially limiting generalization
- The reinforcement learning component depends on a reward function that may not fully align with real-world task success metrics

## Confidence
- **High Confidence:** The core claim that structured scene graphs improve planning accuracy compared to direct visual planning (Mechanism 1) is well-supported by the consistent performance gap in Table 2
- **Medium Confidence:** The superiority of the unified spatial-functional representation (Mechanism 2) is demonstrated on the specific MomaGraph-Bench tasks, but the results may not generalize to all household scenarios
- **Medium Confidence:** The RL training methodology (Mechanism 3) shows clear improvement over SFT and ICL baselines, but the exact impact of reward design choices on final performance is not fully explored

## Next Checks
1. **Generalization Test:** Evaluate MomaGraph-R1 on a held-out set of household tasks not seen during training to assess real-world generalization
2. **Failure Analysis:** Conduct detailed ablation studies on graph components (nodes, edges, action types) to identify which elements most impact planning success
3. **Scaling Study:** Test performance with smaller model variants (e.g., 1B or 3B parameters) to understand the trade-off between model size and accuracy