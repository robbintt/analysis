---
ver: rpa2
title: Benchmarking machine learning models for multi-class state recognition in double
  quantum dot data
arxiv_id: '2511.22451'
source_url: https://arxiv.org/abs/2511.22451
tags:
- data
- normalization
- quantum
- training
- experimental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks four modern machine learning architectures
  - convolutional neural networks (CNNs), U-Nets, vision transformers (ViTs), and
  mixture density networks (MDNs) - for multi-class state recognition in double quantum
  dot charge stability diagrams. The study systematically evaluates these models across
  different data budgets (25-100% of training data) and normalization schemes (min-max
  scaling and z-score normalization) using both synthetic and experimental datasets.
---

# Benchmarking machine learning models for multi-class state recognition in double quantum dot data
## Quick Facts
- arXiv ID: 2511.22451
- Source URL: https://arxiv.org/abs/2511.22451
- Reference count: 40
- CNNs with min-max normalization emerge as the most practical approach for quantum dot charge stability diagram analysis

## Executive Summary
This paper benchmarks four modern machine learning architectures - convolutional neural networks (CNNs), U-Nets, vision transformers (ViTs), and mixture density networks (MDNs) - for multi-class state recognition in double quantum dot charge stability diagrams. The study systematically evaluates these models across different data budgets (25-100% of training data) and normalization schemes (min-max scaling and z-score normalization) using both synthetic and experimental datasets. The results show that while U-Nets achieve the highest performance on synthetic data, CNNs provide the best trade-off for experimental data with two orders of magnitude fewer parameters than U-Nets and ViTs.

## Method Summary
The study trains and evaluates four ML architectures (CNN, U-Net, ViT, MDN) on double quantum dot charge stability diagrams using both synthetic and experimental datasets. Models are tested across three data budget levels (25%, 50%, 100% of training data) and two normalization schemes (min-max scaling and z-score normalization). Performance is measured using mean squared error (MSE) and computational efficiency metrics. The experimental data comes from two different quantum dot devices, while synthetic data is generated using a lattice-based simulator that accounts for electron-electron interactions.

## Key Results
- U-Nets achieve highest MSE scores (>0.98) on synthetic data but fail to generalize to experimental measurements
- CNNs with min-max normalization provide best trade-off for experimental data, using two orders of magnitude fewer parameters than U-Nets and ViTs
- MDNs offer most stable training with lowest computational costs but achieve lower peak performance
- Normalization critically affects model performance: min-max scaling yields higher accuracy but less stable convergence, while z-score normalization provides more predictable training dynamics at reduced accuracy

## Why This Works (Mechanism)
The performance differences between architectures stem from their fundamental design trade-offs. U-Nets' skip connections and encoder-decoder structure excel at preserving spatial information in synthetic data but overfit to noise patterns not present in real measurements. CNNs' convolutional filters naturally capture local patterns in charge stability diagrams while maintaining computational efficiency. MDNs' probabilistic output distribution provides robustness against data scarcity and measurement uncertainty. The normalization schemes affect how models learn feature representations - min-max scaling preserves relative feature importance while z-score normalization stabilizes training by centering data distributions.

## Foundational Learning
- Double quantum dot charge stability diagrams: Two-dimensional plots showing electron occupation as a function of gate voltages; why needed for visualizing quantum state transitions, quick check by identifying triple points
- Mean squared error (MSE): Metric measuring prediction accuracy as average squared difference between predicted and true values; why needed to quantify model performance, quick check by comparing predicted vs actual state assignments
- Data normalization: Scaling techniques that transform data to consistent ranges; why needed to improve model convergence and stability, quick check by verifying all features fall within expected ranges
- Model generalization: Ability of trained models to perform well on unseen data; why needed to ensure real-world applicability, quick check by testing on held-out experimental data
- Computational efficiency: Resource usage metrics including parameter count and training time; why needed to assess practical deployment feasibility, quick check by measuring memory and time requirements

## Architecture Onboarding
Component map: Input data -> Normalization -> ML Architecture (CNN/U-Net/ViT/MDN) -> Output predictions -> Performance evaluation

Critical path: Data preprocessing (normalization) → Model training → Validation on experimental data → Performance assessment

Design tradeoffs: U-Nets offer highest accuracy but excessive parameters and poor generalization; CNNs balance accuracy and efficiency; MDNs prioritize stability over peak performance; normalization scheme affects both convergence stability and final accuracy

Failure signatures: U-Nets show perfect synthetic performance but collapse on experimental data; CNNs may underfit complex patterns; MDNs may struggle with highly non-linear boundaries; improper normalization causes training instability

First experiments: 1) Train each architecture on synthetic data only to establish baseline performance 2) Test trained models on experimental data to assess generalization 3) Compare training dynamics across normalization schemes for each architecture

## Open Questions the Paper Calls Out
None

## Limitations
- U-Net performance gap between synthetic and experimental data raises questions about architectural necessity for real quantum dot systems
- Study limited to four specific architectures, preventing broader conclusions about ML approaches in this domain
- Normalization effects may not generalize to other quantum dot systems or measurement conditions

## Confidence
- CNNs as "best trade-off" for experimental data: Medium confidence, dependent on specific data budget and normalization tested
- Normalization effects on training dynamics: High confidence for observed trends within tested range
- U-Net generalizability limitations: High confidence based on consistent experimental results

## Next Checks
1. Test trained models on a third, independent experimental dataset from a different fabrication process or material system to verify generalizability
2. Evaluate model performance across a wider range of data availability scenarios (e.g., 10-25% training data) to determine if observed trends hold for extremely limited datasets
3. Investigate whether alternative normalization schemes or adaptive normalization strategies could bridge the performance gap between min-max scaling and z-score normalization while maintaining stable training