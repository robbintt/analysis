---
ver: rpa2
title: 'Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts'
arxiv_id: '2505.21556'
source_url: https://arxiv.org/abs/2505.21556
tags:
- masked
- toxicity
- toxic
- jailbreak
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Benign-to-Toxic (B2T) jailbreaking is proposed to induce harmful
  outputs from harmless prompts in large vision-language models (LVLMs). Unlike prior
  Toxic-Continuation methods that rely on already-toxic inputs, B2T optimizes adversarial
  images to force toxic outputs from benign conditioning, breaking safety alignment
  directly.
---

# Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts

## Quick Facts
- **arXiv ID**: 2505.21556
- **Source URL**: https://arxiv.org/abs/2505.21556
- **Reference count**: 40
- **Primary result**: A new jailbreaking method that induces toxic responses from harmless prompts by optimizing adversarial images for vision-language models.

## Executive Summary
Benign-to-Toxic (B2T) jailbreaking introduces a novel paradigm for breaking safety alignment in large vision-language models (LVLMs). Unlike prior methods that require already-toxic inputs, B2T optimizes adversarial images to force toxic outputs from benign conditioning prompts. The approach achieves 10-40 percentage point improvements in attack success rates over existing Toxic-Continuation methods across five safety benchmarks and four LVLMs. B2T demonstrates strong transferability in black-box settings and enhances text-based jailbreaks when combined with them, revealing fundamental vulnerabilities in multimodal safety alignment.

## Method Summary
B2T employs adversarial image optimization to break safety alignment in LVLMs. The method starts with a benign text prompt and optimizes an image such that when combined with this prompt, the model generates harmful content. Unlike Toxic-Continuation which relies on already-toxic prompts, B2T directly manipulates the visual modality to induce toxicity. The optimization uses a loss mixing parameter τ to balance benign and toxic objectives, with the attack success rate (ASR) measured as the proportion of successful jailbreaks. The approach is evaluated on safety benchmarks including jailbroken prompt generation and answering harmful questions.

## Key Results
- B2T achieves 10-40 percentage point improvements in attack success rates compared to Toxic-Continuation baselines
- Strong transferability demonstrated in black-box settings across different LVLM architectures
- When combined with text-based jailbreaks, B2T further enhances attack effectiveness
- B2T perturbations show robustness to JPEG compression while Toxic-Continuation attacks are neutralized by it

## Why This Works (Mechanism)
B2T works by exploiting the interaction between visual and textual modalities in LVLMs. By optimizing adversarial images that are semantically neutral or benign, the attack creates visual features that, when processed alongside harmless text prompts, trigger the model's generation of harmful content. This breaks safety alignment directly at the feature level rather than relying on text manipulation alone. The method leverages the continuous nature of image features for gradient-based optimization, allowing precise control over the model's output behavior.

## Foundational Learning

**Adversarial Image Optimization**: Technique for generating perturbed images that cause models to produce specific outputs. Needed to create the visual component that induces toxicity. Quick check: Verify optimization converges and produces visually plausible images.

**Multimodal Safety Alignment**: The process of training models to avoid generating harmful content across both visual and textual inputs. Needed to understand what B2T breaks. Quick check: Review safety training objectives and detection mechanisms.

**Black-box Transferability**: The ability of attacks optimized on one model to work on different models without access to their parameters. Needed to assess real-world attack potential. Quick check: Measure success rates when attacking models not used in optimization.

**Vision-Language Model Architecture**: Understanding how LVLMs process and integrate visual and textual information. Needed to identify optimization targets. Quick check: Trace input through vision encoder, fusion layers, and text decoder.

**Jailbreak Success Metrics**: Standardized ways to measure whether safety alignment has been broken. Needed for consistent evaluation. Quick check: Apply benchmark-specific scoring criteria.

## Architecture Onboarding

**Component Map**: Benign Prompt -> LVLM -> Adversarial Image Optimization -> Optimized Image -> Combined Input -> Harmful Output

**Critical Path**: The optimization loop that iteratively adjusts image features based on model outputs, using gradients from the combined text-image input to maximize toxicity while maintaining benign prompt semantics.

**Design Tradeoffs**: B2T trades computational cost of optimization for effectiveness, versus one-shot attacks. It requires white-box access for optimization but achieves better transferability than text-only methods.

**Failure Signatures**: When B2T fails, it typically produces either neutral responses or outputs that are toxic but don't match the benign prompt context. Optimization may also stall if gradients become too noisy.

**First Experiments**:
1. Test B2T on a single benign prompt with a known-safe LVLM to verify basic functionality
2. Compare attack success rates against Toxic-Continuation on the same prompt-model pair
3. Evaluate robustness by applying JPEG compression to optimized images and measuring ASR degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the Benign-to-Toxic (B2T) optimization paradigm be effectively extended to discrete text-only modalities, and how do the optimization challenges differ from continuous image perturbations?
- **Basis in paper**: [explicit] The authors state in the Limitations section that "Future work could explore B2T extensions to text, with attention to the challenges posed by its discrete modality."
- **Why unresolved**: The paper focuses on continuous image features. While B2S-GCG showed promise, the discrete nature of text complicates the gradient-based optimization used for images.
- **What evidence would resolve it**: Successful demonstration of a text-only B2T attack that achieves comparable success rates to image-based B2T without relying on continuous perturbations.

### Open Question 2
- **Question**: How does dynamic scheduling of the B2T loss mixing parameter ($\tau$) or curriculum learning impact the trade-off between attack success rates and output fluency?
- **Basis in paper**: [explicit] Section 6 notes that "Strategies like curriculum learning or adaptive $\tau$ scheduling may help balance ASR and fluency."
- **Why unresolved**: The current study uses a fixed $\tau$ (0.1 or 0.2), which successfully breaks alignment but sometimes degrades fluency (e.g., generating excessive profanity).
- **What evidence would resolve it**: Ablation studies comparing fixed $\tau$ against adaptive schedules, measuring both ASR and fluency scores (e.g., GPT-4o ratings) over optimization steps.

### Open Question 3
- **Question**: Why are Benign-to-Toxic perturbations robust to input-level defenses like JPEG compression while Toxic-Continuation attacks are effectively neutralized by them?
- **Basis in paper**: [inferred] The supplementary material (Section D) shows B2T retains high ASR under compression while Toxic-Continuation fails, but the paper does not provide a theoretical explanation for this disparity in robustness.
- **Why unresolved**: Understanding this robustness is critical for developing defenses. The mechanism—whether frequency-based or related to the semantic nature of the perturbation—is unidentified.
- **What evidence would resolve it**: Frequency-domain analysis of the perturbations or theoretical analysis of how B2T gradients survive quantization noise better than continuation-based gradients.

## Limitations
- B2T requires white-box access to LVLM architectures and parameters for optimization, limiting real-world applicability
- Attack effectiveness depends on benign prompt selection and may vary across different contexts not covered in evaluation
- Study focuses on five safety benchmarks and four LVLMs, potentially missing broader deployment scenarios
- Transferability characterization is limited to specific model pairs without comprehensive cross-architecture analysis

## Confidence
**High confidence**: The core methodology of adversarial image optimization to induce toxic outputs from benign prompts is technically sound and reproducible. The quantitative improvements over Toxic-Continuation baselines are clearly demonstrated with appropriate statistical comparison.

**Medium confidence**: The claim that B2T "breaks safety alignment directly" is supported by results but requires more analysis of how this differs mechanistically from existing approaches. The transferability results show promise but are based on limited model pairs.

**Medium confidence**: The enhancement of text-based jailbreaks when combined with B2T is demonstrated but the practical significance and consistency across different text jailbreak methods needs further validation.

## Next Checks
1. Test B2T effectiveness across a broader range of benign prompts and real-world scenarios beyond curated benchmarks to assess practical robustness.
2. Evaluate defense strategies against B2T, including adversarial training with poisoned examples and detection of adversarial images in the visual modality.
3. Investigate the transfer success rate across diverse LVLM architectures (different model families, sizes, and training paradigms) to better characterize black-box effectiveness.