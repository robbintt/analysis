---
ver: rpa2
title: Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based
  Encoders
arxiv_id: '2501.03038'
source_url: https://arxiv.org/abs/2501.03038
tags:
- sequence
- language
- music
- note
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid method combining pre-trained roll-based
  encoders with an LM decoder for automatic music transcription. The proposed approach
  employs a hierarchical prediction strategy, first predicting onset and pitch, then
  velocity, and finally offset, to reduce computational costs by breaking down long
  sequences.
---

# Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders

## Quick Facts
- arXiv ID: 2501.03038
- Source URL: https://arxiv.org/abs/2501.03038
- Authors: Dichucheng Li; Yongyi Zang; Qiuqiang Kong
- Reference count: 31
- One-line primary result: Hierarchical prediction strategy combining pre-trained roll-based encoders with LM decoder outperforms traditional piano-roll outputs by 0.01-0.022 in onset-offset-velocity F1 score

## Executive Summary
This paper introduces a hybrid approach for automatic piano transcription that combines pre-trained roll-based encoders with a language model decoder. The method employs a hierarchical prediction strategy that first predicts onset and pitch, then velocity, and finally offset, reducing computational costs by breaking down long sequences. The approach achieves state-of-the-art performance on piano transcription benchmarks, demonstrating that encoder quality significantly impacts performance more than language model size.

## Method Summary
The method uses a pre-trained roll-based encoder (CRNN or HPPNet) to extract audio features, which are then projected to match the language model's embedding dimension. A decoder-only transformer (LLaMA-style) with three separate sub-models predicts note events hierarchically: first onset and pitch, then velocity, and finally offset. The system is trained end-to-end on hierarchical sequences, with each sub-model trained separately using task-specific query tokens. The hierarchical approach reduces computational complexity from O((T + 3N)²D) to O(3(T + N)²D) by handling shorter sequences per task.

## Key Results
- Hierarchical prediction strategy outperforms flattened sequences by 0.01-0.022 in onset-offset-velocity F1 score
- Encoder choice (HPPNet vs CRNN) has larger impact on performance than language model size
- Larger language models showed no significant improvement and sometimes degraded performance due to overfitting
- Hierarchical approach achieves better computational efficiency by reducing sequence lengths per task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical token prediction strategy improves accuracy and computational efficiency
- **Mechanism:** Decomposes long note sequences into three shorter sequential tasks (Onset/Pitch → Velocity → Offset), reducing sequence length per task and time complexity
- **Core assumption:** Accuracy of subsequent predictions benefits from prior establishment of foundational elements
- **Evidence anchors:** Abstract mentions reduced computational costs; Section III provides complexity analysis; Table I shows hierarchy outperforming flatten

### Mechanism 2
- **Claim:** Pre-trained roll-based encoders effectively repurposed as frozen/fine-tuned feature extractors for LM decoder
- **Mechanism:** Roll-based encoder trained on frame-level presence/absence provides representations for LM to decode precise note attributes without manual thresholding
- **Core assumption:** Encoder representations contain sufficient information for precise velocity/offset decoding
- **Evidence anchors:** Abstract states hybrid method outperforms traditional outputs; Section II describes encoder transformation

### Mechanism 3
- **Claim:** Audio encoder quality dominates final AMT performance over LM scale
- **Mechanism:** System performance bottleneck lies in audio representation quality rather than LM capacity
- **Core assumption:** LM capacity is not limiting factor; audio feature fidelity is critical
- **Evidence anchors:** Abstract emphasizes encoder choice impact; Section V, Table III shows no LM scaling improvement

## Foundational Learning

- **Concept:** Transformer Language Modeling (Autoregressive Decoding)
  - **Why needed here:** Core engine of LM decoder for predicting next token given previous tokens and audio context
  - **Quick check question:** How does the model ensure it does not attend to future tokens when predicting current note event?

- **Concept:** Feature Extraction from Audio (STFT/CQT & CNNs)
  - **Why needed here:** Roll-based encoder operates on time-frequency representations
  - **Quick check question:** What is the difference between frame-level output (piano roll) and note-level output (token sequence), and why does latter eliminate thresholding step?

- **Concept:** Hierarchical vs. Flat Tokenization
  - **Why needed here:** Central structural innovation - splitting note attributes into distinct sub-models
  - **Quick check question:** In hierarchical setup, which model must be run first, and why can Velocity model condition on Onset-Pitch output?

## Architecture Onboarding

- **Component map:** Input Processor → Pre-trained Encoder → Projection Layer → LM Decoder (3 sub-models) → Tokenizer
- **Critical path:** Pre-train roll-based encoder → connect encoder to LM via projection → train 3 LM sub-models separately in sequence → run models sequentially at inference
- **Design tradeoffs:** Prioritize superior audio encoder over larger LM; hierarchical reduces computational cost but introduces sequential dependency; decoder-only vs encoder-decoder affects positional information handling
- **Failure signatures:** LM overfitting (training/validation loss divergence); offset collapse (drastic On-Off F1 drop); encoder bottleneck (poor Onset F1)
- **First 3 experiments:** Encoder ablation (CNN vs pre-trained); sequence length stress test (20s, 30s vs baseline); hierarchical dependency check (error propagation analysis)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can scalability of language-model-based AMT systems be improved so increasing model size yields gains rather than degradation?
- **Basis in paper:** Explicit conclusion calling for further investigation into improving scalability since larger models underperformed
- **Why unresolved:** Standard scaling laws didn't apply; increasing parameters led to overfitting
- **What evidence would resolve it:** Modified architecture or regularization technique where increasing parameter count consistently improves Onset-Offset-Velocity F1

### Open Question 2
- **Question:** Why does velocity modeling overfit significantly faster than onset-pitch or offset modeling?
- **Basis in paper:** Explicit statement that velocity modeling is more prone to overfitting with validation loss increasing after 100k steps
- **Why unresolved:** Identifies disparity in loss curves but provides no theoretical explanation or solution
- **What evidence would resolve it:** Analysis of spectral features for velocity estimation or training intervention normalizing generalization gap

### Open Question 3
- **Question:** Does non-autoregressive nature of encoder-decoder architectures provide advantage over decoder-only for handling long flattened sequences?
- **Basis in paper:** Hypothesis that encoder-decoder's non-autoregressive nature helped better encode information
- **Why unresolved:** Stated as hypothesis to explain performance difference but not tested
- **What evidence would resolve it:** Controlled ablation study comparing encoder-decoder and decoder-only on identical flattened token sequences

## Limitations

- Experimental design cannot fully isolate whether hierarchical factorization or encoder choice drives performance gains due to confounded comparisons
- Velocity modeling shows significant overfitting with larger models, suggesting inability to learn robust velocity representations
- Temporal precision evaluation (±50ms) is coarse relative to 10ms tokenization resolution, potentially masking finer timing errors

## Confidence

- **High Confidence:** Computational complexity reduction claim is mathematically sound and directly supported by hierarchical decomposition analysis
- **Medium Confidence:** Encoder choice matters more than LM size claim is well-supported but lacks cleanest experimental isolation
- **Medium Confidence:** Hierarchical strategy's superiority over flattened sequences demonstrated but results could be partially attributed to better encoder choice

## Next Checks

1. **Encoder Ablation on Flattened Task:** Train and evaluate flattened sequence model using HPPNet encoder to determine if performance gains are primarily from encoder quality or hierarchical structure

2. **Hierarchical Dependency Analysis:** Conduct error propagation study to quantify how errors in onset-pitch model affect velocity and offset predictions

3. **Temporal Precision Stress Test:** Evaluate system with tighter temporal tolerances (e.g., ±10ms) to assess whether claimed improvements hold under more demanding precision requirements