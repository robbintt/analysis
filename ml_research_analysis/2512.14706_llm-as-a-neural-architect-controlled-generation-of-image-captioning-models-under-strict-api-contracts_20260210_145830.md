---
ver: rpa2
title: 'LLM as a Neural Architect: Controlled Generation of Image Captioning Models
  Under Strict API Contracts'
arxiv_id: '2512.14706'
source_url: https://arxiv.org/abs/2512.14706
tags:
- prompt
- code
- generated
- training
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents NN-Caption, an LLM-guided neural architecture
  search pipeline that generates runnable image captioning models by composing CNN
  encoders from LEMUR's classification backbones with sequence decoders under a strict
  Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, the system automatically
  proposes novel captioning architectures and suggests hyperparameters and training
  practices.
---

# LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts

## Quick Facts
- **arXiv ID:** 2512.14706
- **Source URL:** https://arxiv.org/abs/2512.14706
- **Reference count:** 14
- **Key outcome:** DeepSeek-R1-0528-Qwen3-8B LLM-guided NAS generates dozens of image captioning models; best CNN+Transformer model reaches BLEU-4 of 0.1192 in 3 epochs, outperforming baseline.

## Executive Summary
This paper presents NN-Caption, a neural architecture search pipeline that uses an LLM to automatically generate novel image captioning models by composing CNN encoders from LEMUR classification backbones with sequence decoders (LSTM/GRU/Transformer). Using DeepSeek-R1-0528-Qwen3-8B as the generator and a strict Net API contract, the system proposes architectures, hyperparameters, and training practices. Evaluated on MS COCO, the LLM produced dozens of models with over half successfully trained, yielding meaningful captions. The best generated model (CNN encoder + Transformer decoder) achieved a BLEU-4 score of 0.1192, outperforming the baseline in just 3 epochs. The study demonstrates the promise of LLM-guided NAS for image captioning and contributes dozens of novel models to the LEMUR dataset for reproducible benchmarking.

## Method Summary
The NN-Caption pipeline uses DeepSeek-R1-0528-Qwen3-8B as an LLM-guided generator to propose novel image captioning architectures. The LLM is prompted with a baseline ResNet-50+LSTM captioning model and 5 classification model snippets from LEMUR, along with strict API rules requiring a Net class with specific methods and hyperparameter support. Generated Python code is automatically validated for syntax and API compliance, with up to 2 repair iterations for errors. Valid models are trained on MS COCO 2017 for 3 epochs using AdamW optimizer, CrossEntropyLoss, and teacher forcing. The pipeline evaluates models using BLEU-4 on the validation set, reporting success rates for runnable models.

## Key Results
- LLM-generated dozens of image captioning models using LEMUR classification backbones
- Over half of generated models successfully trained and produced meaningful captions
- Best model (CNN encoder + Transformer decoder) achieved BLEU-4 of 0.1192, outperforming baseline in 3 epochs
- Using 5 input model snippets yielded slightly better success rates than 10 snippets

## Why This Works (Mechanism)
The LLM acts as a neural architect by composing known CNN encoder patterns with sequence decoders under strict API constraints. The controlled generation approach limits hallucinations through prompt engineering and automated validation. The Net API contract ensures generated models can be trained consistently. Using classification model snippets provides architectural diversity while maintaining syntactic validity. The automated repair loop addresses common code errors, increasing the yield of runnable models.

## Foundational Learning
- **LEMUR framework**: A repository of pre-trained classification models and training infrastructure; needed for consistent model evaluation and comparison
- **Net API contract**: Strict interface requirements for generated models; ensures all models can be trained with the same pipeline
- **Teacher forcing**: Training technique where decoder input is shifted ground truth; improves caption generation quality
- **BLEU-4 metric**: Bilingual Evaluation Understudy score measuring caption quality; standard metric for image captioning
- **CrossEntropyLoss with label smoothing**: Training loss function with regularization; prevents overconfident predictions

## Architecture Onboarding

**Component map:** LLM generator -> Code validator -> Automated repair -> Training pipeline -> BLEU-4 evaluation

**Critical path:** Prompt construction → LLM generation → Syntax/API validation → Model training → Evaluation

**Design tradeoffs:** Short 3-epoch training limits comparison fairness but enables rapid iteration; strict API reduces architectural diversity but ensures trainability

**Failure signatures:** Syntax errors (hallucinations), shape mismatches (incorrect tensor dimensions), missing API methods, training instability (NaN loss)

**Three first experiments:**
1. Validate baseline ResNet-50+LSTM model reproduces expected BLEU-4 score
2. Generate and train 5 models with different encoder-decoder combinations
3. Compare success rates using 5 vs 10 input model snippets in prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Success rate metric conflates code validity with model trainability
- Short 3-epoch training duration limits fair comparison with established baselines
- Critical details underspecified (exact prompt template, vocabulary construction, decoder hyperparameters)
- Reproducibility depends heavily on specific LLM's code generation capabilities

## Confidence

**High confidence:** Core concept of LLM-guided NAS is sound; BLEU-4 results internally consistent
**Medium confidence:** Success rate statistics and baseline comparison are reproducible
**Low confidence:** Exact prompt engineering, code snippet selection, and hyperparameter distributions cannot be precisely replicated

## Next Checks

1. Test prompt template with multiple LLM instances to verify success rate independence from specific DeepSeek model
2. Implement granular error classification to distinguish syntax errors, shape mismatches, and poor architectures
3. Extend training duration to 10-15 epochs for subset of generated models to verify BLEU-4 robustness