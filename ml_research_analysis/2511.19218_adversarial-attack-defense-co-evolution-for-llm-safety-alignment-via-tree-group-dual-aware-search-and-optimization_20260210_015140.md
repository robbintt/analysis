---
ver: rpa2
title: Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group
  Dual-Aware Search and Optimization
arxiv_id: '2511.19218'
source_url: https://arxiv.org/abs/2511.19218
tags:
- attack
- jailbreak
- defense
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ACE-Safety, a co-evolutionary framework
  for joint optimization of attack and defense models in large language models (LLMs).
  The framework integrates two key components: GS-MCTS (Group-aware Strategy-guided
  Monte Carlo Tree Search) for efficient jailbreak attack exploration with adversarial
  priors and group-awareness, and AC-TGPO (Adversarial Curriculum Tree-aware Group
  Policy Optimization) for robust joint training through curriculum reinforcement
  learning.'
---

# Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization

## Quick Facts
- arXiv ID: 2511.19218
- Source URL: https://arxiv.org/abs/2511.19218
- Authors: Xurui Li; Kaisong Song; Rui Zhu; Pin-Yu Chen; Haixu Tang
- Reference count: 36
- Primary result: Introduces ACE-Safety framework achieving higher attack success rates with fewer attempts while maintaining superior defense performance, helpfulness, and responsibility

## Executive Summary
This paper presents ACE-Safety, a co-evolutionary framework that jointly optimizes attack and defense models for large language model safety alignment. The framework integrates Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS) for efficient jailbreak attack exploration with adversarial priors and group-awareness, and Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO) for robust joint training through curriculum reinforcement learning. ACE-Safety outperforms existing methods across multiple benchmarks, achieving higher attack success rates with fewer attempts while maintaining superior defense performance, helpfulness, and responsibility.

## Method Summary
ACE-Safety co-evolves attack and defense LLMs through four iterative training rounds. GS-MCTS uses PUCT-guided tree search over 8 strategy categories, generating G=6 modified queries per node with adversarial priors (combining attack and defense model confidence), backpropagating maximum harmfulness rewards. AC-TGPO implements 4 iterations of curriculum RL combining Normal samples (current round attacks), Asymmetric samples (strong attack vs weak defense), and Hard samples (persistent failures) with tree-aware hierarchical normalization and KL-constrained PPO. The framework is trained on MergedHarm dataset using Vicuna/Llama3/Mistral variants with GPT-4 judge agent scoring harmfulness, responsibility, co-relevance, and explanation.

## Key Results
- GS-MCTS achieves higher attack success rates (ASR-LR) with fewer attempts (ANA) compared to baseline methods
- AC-TGPO improves defense robustness while maintaining helpfulness and responsibility scores on standard benchmarks
- The co-evolutionary approach demonstrates superior performance across multiple evaluation datasets including XSTest, ORHard, MT-Bench, and AlpacaEval
- Tree-aware normalization and adversarial curriculum training contribute to 2-4 percentage point improvements in ASR-LR reduction

## Why This Works (Mechanism)

### Mechanism 1: Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS)
GS-MCTS extends standard MCTS by computing a prior probability P(s,a) that combines the attack model's generation confidence (P^A_{s,a}) and defense model's non-rejection likelihood (P^D_{s,a}). For each selected node, it generates G modified queries (group) rather than a single query, using the maximum harmfulness score j^h_max across the group as the reward. The PUCT algorithm selects actions balancing exploitation (Q-values) and exploration (prior × visit counts). This approach reduces variance from LLM generation randomness and yields more reliable strategy evaluation.

### Mechanism 2: Tree-aware Group Policy Optimization with Hierarchical Normalization
AC-TGPO computes group-level advantage A'_{i,t} using mean/std within each group of G outputs. It then applies tree-level normalization using composite weights W_j = (1/d_j) × γ^{d_j} × Σ Q_l (path-averaged action values). This ensures comparability across groups from different tree depths, where deeper nodes represent harder attack chains. The policy objective uses clipped importance ratios with KL regularization. This hierarchical normalization stabilizes reinforcement learning when training samples have heterogeneous difficulty.

### Mechanism 3: Adversarial Curriculum Training with Triple Sample Sets
Each iteration constructs three sample sets: (1) T^N_i: normal samples from current-round MA_i attacking MD_i; (2) T^A_i: asymmetric samples from stronger MA_i attacking weaker MD_{i-1} (generates more jailbroken samples); (3) T^H_i: hard samples re-evaluated from prior rounds that remain jailbroken. The merged dataset T_i trains both models simultaneously, with asymmetric/hard sets collected only in the first epoch per iteration. This curriculum approach induces robust co-evolution without overfitting.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS) with Upper Confidence Bound (UCB/PUCT)
  - Why needed here: GS-MCTS uses PUCT to balance exploring new attack strategies vs. exploiting known-effective ones. Without understanding selection/expansion/backpropagation, the prior probability integration and group-level reward aggregation will be opaque.
  - Quick check question: Given a node with Q(s,a)=0.7, N(s,a)=5, total visits ΣN=20, and prior P(s,a)=0.3, compute the PUCT score with c_p=1.

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: AC-TGPO builds on GRPO's group-based advantage normalization. Understanding baseline GRPO clarifies what the tree-aware extension adds (hierarchical normalization across tree nodes with depth-weighted composite scores).
  - Quick check question: In standard GRPO with group size G=4, if rewards are [0.2, 0.5, 0.8, 0.3], compute the normalized advantage for the third sample.

- Concept: Curriculum Learning with Progressive Difficulty
  - Why needed here: The asymmetric/hard sample construction mirrors curriculum learning principles—starting with current-capability samples (normal), introducing harder variants (asymmetric), and reinforcing persistent failures (hard).
  - Quick check question: If iteration 3 produces 100 normal samples (30 jailbroken) and asymmetric attack on MD_2 produces 80 jailbroken samples, how should the training batch ratio be set to avoid easy-sample dominance?

## Architecture Onboarding

- Component map: MergedHarm dataset -> GS-MCTS search -> Collect samples (Normal/Asymmetric/Hard) -> AC-TGPO training -> Attack Model (M_A) + Defense Model (M_D) -> Judge Agent (J_A) evaluation -> Next iteration

- Critical path: 1. Initialize M_A^1, M_D^1 with same LLM backbone 2. Run GS-MCTS on training queries → collect jailbroken node samples (normal set T^N_1) 3. Use M_A^1 to attack M_D^0 (vanilla model) → asymmetric set T^A_1 (first epoch only) 4. Merge samples → train both M_A^2, M_D^2 via AC-TGPO 5. Repeat iterations 2-4 with updated models 6. Evaluate: ASR-LR (attack success), ANA (attempts needed), helpfulness/responsibility scores

- Design tradeoffs:
  - Group size G: Larger G reduces variance but increases compute linearly. Paper uses G=6 based on ablation.
  - Iteration count: Most ASR reduction occurs in first 2 iterations; 4 iterations used for convergence.
  - Jailbreak threshold η=8: Setting too low misses subtle jailbreaks; too high accepts marginal cases.
  - Temperature: M_A and M_D use 0.9 for diversity; J_A uses 0.0 for consistency—different objectives require different sampling.

- Failure signatures:
  - High ASR-LR with low ANA: Defense collapses entirely; check if asymmetric samples are overwhelming training.
  - High over-refusal on XSTest/ORHard: Defense overfitted to refuse all suspicious queries; check hard sample set composition.
  - Tree search returns no jailbroken nodes after N_m cycles: Action space may be inadequate for target model; check strategy coverage or increase N_m.
  - Training divergence (KL spikes): β too low or learning rate too high; reduce to 1e-5 and increase β.

- First 3 experiments:
  1. Baseline sanity check: Run GS-MCTS with G=1 (no group aggregation) vs. G=6 on Vicuna-7B against 50 held-out queries. Expect G=6 to show lower variance in ASR across runs and fewer total attempts (ANA).
  2. Tree-aware ablation: Train defense model with (a) group-only normalization, (b) tree-aware normalization. Evaluate on Malicious-Instruct (OOD). Expect tree-aware to show smaller ASR degradation on OOD due to difficulty-aware learning.
  3. Curriculum component isolation: Train three defense variants on Vicuna-13B: (a) normal samples only, (b) normal+asymmetric, (c) full (normal+asymmetric+hard). Report ASR-LR against GCG, PAIR, TAP. Expect progressive improvement; if hard samples don't help, check if iteration 1-3 are actually producing persistent jailbreaks.

## Open Questions the Paper Calls Out
None

## Limitations
- The hierarchical normalization mechanism's empirical benefit over simpler baselines remains unclear without independent reproduction
- Fixed 8-strategy action space may limit attack diversity against increasingly sophisticated defenses
- Reliance on GPT-4 as judge agent introduces cost and potential biases that aren't fully characterized

## Confidence
- **High confidence**: Attack success rates (ASR-LR) and attempts needed (ANA) metrics, as these are directly measurable and comparable across baselines
- **Medium confidence**: Defense robustness on XSTest/ORHard, as these depend on judge agent consistency and threshold calibration (η=8)
- **Low confidence**: Generalizability to non-Vicuna/Llama3 backbones, as all experiments use specific model variants without ablation across architectures

## Next Checks
1. **Baseline comparison validation**: Implement standard GRPO (Shao et al. 2024) and MART (Ge et al. 2024) with identical model sizes and datasets to verify ACE-Safety's claimed 2-4 percentage point ASR-LR improvements
2. **Judge agent robustness**: Run ACE-Safety with different judge configurations (temperature 0.0 vs 0.2, different few-shot prompts) to measure sensitivity of ASR-LR and ANA to judge variability
3. **Cross-architecture generalization**: Train ACE-Safety on Llama3-8B and Mistral-7B, then evaluate on the same benchmarks to confirm performance gains aren't model-specific artifacts