---
ver: rpa2
title: Making Universal Policies Universal
arxiv_id: '2502.14777'
source_url: https://arxiv.org/abs/2502.14777
tags:
- agent
- agents
- learning
- action
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends universal policies to enable training a single
  policy that can control multiple agents with different action spaces but shared
  observation spaces. The method builds on a diffusion-based planner that generates
  observation sequences, paired with agent-specific inverse dynamics models to assign
  actions.
---

# Making Universal Policies Universal

## Quick Facts
- arXiv ID: 2502.14777
- Source URL: https://arxiv.org/abs/2502.14777
- Reference count: 40
- Trains a single policy that controls multiple agents with different action spaces but shared observation spaces, achieving up to 42.20% improvement in task completion accuracy compared to single-agent policies.

## Executive Summary
This paper extends universal policies to enable training a single policy that can control multiple agents with different action spaces but shared observation spaces. The method builds on a diffusion-based planner that generates observation sequences, paired with agent-specific inverse dynamics models to assign actions. By pooling datasets from different agents, the approach achieves positive transfer, outperforming policies trained on individual datasets. The planner is conditioned on agent information—such as action space encoding or example trajectories—with action space encoding yielding the best results. Experiments on BabyAI tasks show up to 42.20% improvement in task completion accuracy compared to single-agent policies. The method also demonstrates generalization to unseen agents when agent diversity is increased.

## Method Summary
The method decouples policy learning into a diffusion-based planner that predicts observation sequences and agent-specific inverse dynamics models that convert these observations to actions. The planner is trained on pooled datasets from multiple agents, enabling positive transfer. Agent conditioning is achieved through binary action space encodings, which outperform other conditioning strategies. The approach is evaluated on BabyAI gridworld environments with 8 different agent types, showing significant improvements in task completion rates.

## Key Results
- Achieves up to 42.20% improvement in task completion accuracy compared to single-agent policies on BabyAI tasks.
- Action space encoding as conditioning information outperforms other methods including example trajectories and no conditioning.
- Demonstrates generalization to unseen agents, with performance improving from 49.4% to 90.6% when training on 6,595 diverse agent types.
- Positive transfer from pooled datasets enables the universal policy to outperform individual agent policies.

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Observation Planning with Shared Latent Transfer
The architecture separates policy learning into a diffusion-based planner (predicting observation sequences) and inverse dynamics models (predicting actions). Because all agents share the observation space, the planner learns a joint distribution over observations, leveraging statistical strength from the entire mixture dataset to improve planning accuracy compared to single-agent datasets.

### Mechanism 2: Agent-Conditioning via Action Space Embeddings
Conditioning the shared planner on explicit encoding of the agent's action space (binary vector of capabilities) yields superior performance and generalization. The denoising network is conditioned on this binary vector, forcing it to learn the correlation between specific action constraints and resulting feasible observation dynamics.

### Mechanism 3: Compositional Generalization via Agent Diversity
Generalization to unseen agents is possible only if the training distribution contains sufficient agent diversity. The model treats action space encoding as compositional instruction, synthesizing plans for novel combinations by recombining learned dynamics from atomic skills present in the training set.

## Foundational Learning

- **Diffusion Models (Score-based Generative Models)**: Used for the core planner as a generative denoising model. Understand how to corrupt data with Gaussian noise and train a network to reverse it. *Quick check*: Can you explain why a diffusion model is preferable to a standard autoregressive transformer for generating multi-step observation sequences?

- **Inverse Dynamics Models (IDM)**: The policy executes actions by inferring them from observation pairs. If the planner hallucinates an impossible transition, the IDM is the component that will fail. *Quick check*: Given a dataset of (s, a, s'), how would you construct the training loss for an IDM?

- **Cross-Embodiment / Heterogeneous-Agent Learning**: Addresses the setup where agents differ in action space but share observation space. Understanding why "Union of Action Spaces" baselines often fail highlights why the decoupled approach is necessary. *Quick check*: Why is training a single policy on the union of all possible actions typically inferior to agent-specific heads or universal planning?

## Architecture Onboarding

- **Component map**: Inputs (Instruction T5 embedding, Start Obs, Agent Info) -> Diffusion Planner (UCAP) -> Output (Observation sequence) -> Agent-specific IDMs -> Output (Action sequence)

- **Critical path**: Data Prep (Pool datasets, generate binary action masks) -> Planner Training (Conditional diffusion model on mixture dataset) -> IDM Training (Separate IDMs on agent-specific datasets) -> Inference (Planner generates observations, IDMs convert to actions)

- **Design tradeoffs**: Use Action Space Encoding for best accuracy on known agents. Use Example Trajectories if you need OOD generalization and can afford high training diversity. Avoid unconditioned mixtures (negative transfer risk). Standard is 1-step planning; 2-step improves OOD generalization but requires additional training.

- **Failure signatures**: Negative Transfer (performance drops below single-agent baselines - check observation space alignment), IDM Mismatch (planner generates valid video but agent acts randomly - check agent conditioning), Slow Inference (diffusion sampling is slow - consider distillation).

- **First 3 experiments**: 1) Overfit Sanity Check: Train planner on single agent's data to verify coherent video generation. 2) Conditioning Ablation: Compare "No Agent Info" vs "Agent ID" vs "Action Space Vector" on mixture dataset. 3) OOD Stress Test: Hold out specific agent combination from training and measure zero-shot success rate.

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework effectively extend to agents with significantly heterogeneous observation spaces? The current method relies on a shared planner generating observation sequences, which breaks down if visual inputs are not semantically aligned across agents. Evidence would be successful application to datasets like Open X-Embodiment where agents possess fundamentally different visual perspectives.

### Open Question 2
Does positive transfer persist when scaling to high-dimensional, real-world robotics? Experiments were restricted to BabyAI gridworld environment, which lacks visual noise and complexity of real-world robotic manipulation. Evidence would be evaluating the method on complex physical simulation or real-world benchmarks demonstrating comparable percentage improvements.

### Open Question 3
Can the inference efficiency be improved to enable real-time control? The authors acknowledge increased training and inference time (128 neural function evaluations vs. 1 for imitation learning) but leave implementation of acceleration techniques as future work. Evidence would be implementing consistency models to reduce sampling steps and measuring the trade-off between latency and task completion accuracy.

## Limitations
- Assumes shared observation space across agents; breaks down if agents perceive environments differently
- Diffusion-based planner introduces computational overhead at inference time (64 Heun sampling steps)
- Real-world applicability depends on robust observation alignment techniques not yet developed

## Confidence

- **High**: Positive transfer effect when pooling agent data and superiority of action space encoding over other conditioning strategies
- **Medium**: Compositional generalization claim for OOD agents relies on computationally intensive scaling to 6,595 agent types
- **Low**: Claim that diffusion models are inherently superior to autoregressive transformers is asserted but not directly tested

## Next Checks

1. **Observation Space Validation**: Test the planner on agents with deliberately misaligned observation spaces (e.g., different camera angles) to quantify the breaking point of the shared X assumption.

2. **Scalability of OOD Generalization**: Replicate the 6,595-agent experiment to verify the 90.6% generalization claim, or test the scaling law with a smaller but controlled increase in agent diversity.

3. **Runtime Efficiency Analysis**: Measure wall-clock time per inference step with 64 Heun steps and compare it to a distilled or autoregressive baseline to assess practical deployment feasibility.