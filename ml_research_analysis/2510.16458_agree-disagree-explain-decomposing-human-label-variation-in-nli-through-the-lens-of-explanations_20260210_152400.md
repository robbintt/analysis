---
ver: rpa2
title: 'Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through
  the Lens of Explanations'
arxiv_id: '2510.16458'
source_url: https://arxiv.org/abs/2510.16458
tags:
- label
- reasoning
- annotators
- livenli
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses a linguistic taxonomy of free-text explanations
  (LITEX) to analyze label variation in Natural Language Inference (NLI) datasets.
  Unlike prior work focused on within-label variation, this study applies LITEX to
  two NLI datasets (LiveNLI and VariErr) to examine how annotators diverge in both
  reasoning and labeling steps.
---

# Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations

## Quick Facts
- arXiv ID: 2510.16458
- Source URL: https://arxiv.org/abs/2510.16458
- Reference count: 36
- Primary result: Agreement in reasoning categories (LiTEx) correlates more strongly with explanation semantic similarity than label agreement alone

## Executive Summary
This paper analyzes human label variation in Natural Language Inference (NLI) by decomposing the annotation process into two steps: reasoning and labeling. Using the LiTEx taxonomy to categorize free-text explanations, the study reveals that annotators exhibit distinct reasoning-type and label preferences, and that alignment in reasoning categories better reflects semantic similarity of explanations than label agreement alone. The findings highlight the value of explanations in revealing semantic similarity beyond surface-level label disagreement and caution against treating labels as ground truth.

## Method Summary
The study uses two NLI datasets (LiveNLI and VariErr) with annotator explanations. Each explanation is categorized into one of eight LiTEx reasoning types using a single trained annotator. Agreement is computed at three levels: NLI label (four classes: Full/Partial/Two Pairs/Divergent), taxonomy category (Jaccard similarity), and explanation text similarity (cosine/Euclidean using sentence embeddings). The analysis examines individual annotator preferences and the relationship between reasoning alignment and label agreement through conditional Cohen's κ measures.

## Key Results
- Annotators exhibit stable preferences in both labeling tendencies and reasoning-type strategies
- Agreement in reasoning categories correlates more strongly with explanation semantic similarity than label agreement alone
- The two-step decomposition (reasoning → labeling) reveals distinct divergence points that single-label analysis obscures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing NLI annotation into a two-step process (reasoning → labeling) reveals distinct divergence points that single-label analysis obscures.
- Mechanism: By categorizing free-text explanations via the LiTEx taxonomy (8 reasoning types across Text-Based and World-Knowledge), the framework isolates whether annotators diverge at the reasoning step (same label, different rationale) or labeling step (same rationale, different label).
- Core assumption: Annotators' written explanations faithfully reflect their internal reasoning process, and the two-step decomposition accurately models human decision-making.
- Evidence anchors: [abstract]: "We use explanations as a lens to decompose the reasoning process underlying NLI annotation and to analyze individual differences."; [section 1]: Figure 1 illustrates the two-step framework with examples showing divergence at each step.; [corpus]: Weak direct support; related work (LiTEx, Threading the Needle) focuses on within-label variation or CoT reasoning, not the specific two-step decomposition.
- Break condition: If annotators post-hoc rationalize labels rather than report genuine reasoning, or if explanation writing introduces artifacts, the decomposition becomes unreliable.

### Mechanism 2
- Claim: Alignment in reasoning categories (taxonomy agreement) correlates more strongly with explanation semantic similarity than label agreement alone.
- Mechanism: Shared reasoning types (e.g., both annotators use "Absence of Mention") constrain the semantic space of explanations, producing higher textual similarity even when final labels differ. Label agreement alone permits diverse rationales.
- Core assumption: The LiTEx taxonomy categories capture meaningful reasoning distinctions that map onto semantic content of explanations.
- Evidence anchors: [abstract]: "Agreement in reasoning types better reflects the semantic similarity of free-text explanations than label agreement alone."; [section 5.1]: Table 1 shows cosine similarity patterns align more closely with category agreement than label agreement; κ(L|T) > κ(T|L) in Figure 5.; [corpus]: No direct corpus support for this specific correlation claim.
- Break condition: If explanation text similarity is driven by surface lexical overlap rather than reasoning content, or if the taxonomy categories are too coarse/fine-grained, the correlation weakens.

### Mechanism 3
- Claim: Individual annotators exhibit stable preferences in both labeling tendencies and reasoning-type strategies that persist across items.
- Mechanism: Annotators bring priors (e.g., preference for "neutral" labels, reliance on world knowledge vs. textual evidence) that systematically bias their annotations, creating structured disagreement patterns.
- Core assumption: Annotator preferences are consistent across the annotation task and not purely item-driven.
- Evidence anchors: [abstract]: "Our analysis reveals individual preferences in explanation strategies and label choices."; [section 4]: Figure 3 shows annotator-specific label distributions (e.g., Annotator 2 in VariErr assigns 60% neutral); Figure 4 shows reasoning-type preferences.; [corpus]: Weak support; corpus papers address label variation broadly but not individual annotator preference tracking.
- Break condition: If preferences are confounded by item ordering, fatigue, or instruction effects, or if the annotator sample is too small to generalize, the mechanism reflects noise rather than stable traits.

## Foundational Learning

- **Concept: Natural Language Inference (NLI) task structure**
  - Why needed here: The entire analysis presupposes understanding the three-way classification (entailment, neutral, contradiction) and premise-hypothesis pairs.
  - Quick check question: Given "A dog is sleeping" (premise) and "An animal is resting" (hypothesis), what is the NLI label and why?

- **Concept: Human Label Variation (HLV) vs. annotation error**
  - Why needed here: The paper distinguishes plausible disagreement from noise; conflating these would invalidate the analysis.
  - Quick check question: If 5 annotators split 3-2 on an NLI label with coherent explanations for both sides, is this error or variation?

- **Concept: Explanation taxonomy structure (LiTEx)**
  - Why needed here: Interpreting results requires knowing the 8 categories (Coreference, Syntactic, Semantic, Pragmatic, Absence of Mention, Logic Conflict, Factual Knowledge, Inferential Knowledge) and their Text-Based vs. World-Knowledge division.
  - Quick check question: Is "The premise doesn't mention X" a Text-Based or World-Knowledge explanation? Which specific category?

## Architecture Onboarding

- **Component map:** Input (NLI instances + explanations + labels) -> LiTEx annotation layer (categorize into 8 categories) -> Agreement computation (label, category, text similarity) -> Conditional κ analysis (κ(T|L) and κ(L|T)) -> Output (distributions, correlation analyses)

- **Critical path:** LiTEx annotation quality → agreement metric computation → interpretation of reasoning-label relationships. The taxonomy annotation is the bottleneck; poor category assignment cascades through all downstream analyses.

- **Design tradeoffs:**
  - Single-category assignment per explanation (current) vs. multi-label (future): simpler analysis but may miss hybrid reasoning
  - Fixed 4-annotator analysis (for comparability) vs. full pool: limits generalizability but enables pairwise analysis
  - Sentence-embedding similarity vs. manual review: scalable but may miss pragmatic nuances

- **Failure signatures:**
  - Low inter-annotator agreement on LiTEx categories (κ<0.6): taxonomy unclear or annotators undertrained
  - κ(T|L) ≈ κ(L|T): no systematic relationship between reasoning and labeling; two-step model unsupported
  - Category distribution identical across all annotators: preference analysis uninformative, may indicate instruction-driven convergence

- **First 3 experiments:**
  1. Replicate LiTEx annotation on a held-out subset of VariErr (50–100 items) to verify κ≥0.75; if lower, revise category definitions or training.
  2. Compute agreement metrics (Table 1) on a new dataset (e.g., ChaosNLI subset with explanations) to test generalizability of the category-similarity correlation.
  3. Run ablation: randomly permute explanation-text pairings within items to verify that category agreement correlates with genuine explanation similarity, not just label distributions.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the taxonomy be extended to account for multiple reasoning strategies within a single explanation?
  - Basis in paper: [explicit] The Conclusion states future work can extend the taxonomy to allow multiple reasoning strategies to capture interactions between explanation categories.
  - Why unresolved: The current single-category annotation may oversimplify complex human reasoning that combines distinct inferential steps (e.g., combining world knowledge with syntactic evidence).
  - What evidence would resolve it: A comparative study measuring the coverage and inter-annotator agreement of a multi-label taxonomy version versus the single-label baseline on complex NLI instances.

- **Open Question 2:** To what extent do annotator backgrounds (e.g., linguistic expertise, cultural knowledge) systematically influence reasoning preferences and label variation?
  - Basis in paper: [explicit] The Conclusion suggests modeling annotator backgrounds to uncover systematic sources of disagreement, addressing the limitation that the current method does not account for annotator pool biases.
  - Why unresolved: The current analysis tracks individual preferences but lacks metadata to explain *why* specific reasoning strategies or biases emerge.
  - What evidence would resolve it: Correlation analysis between detailed annotator demographic profiles and their specific reasoning-type distributions across the datasets.

- **Open Question 3:** Does the correlation between reasoning alignment and label agreement generalize to other domains, such as commonsense reasoning or dialogue?
  - Basis in paper: [explicit] The Conclusion proposes applying the approach to other domains to test its generality and reveal new patterns of explanatory behavior.
  - Why unresolved: The study is limited to NLI; reasoning patterns like "Absence of Mention" may function differently in interactive contexts or tasks requiring deep commonsense.
  - What evidence would resolve it: Replicating the LiTEx annotation and agreement analysis on a dialogue or commonsense reasoning dataset (e.g., Social IQa) and comparing the reasoning-label correlation metrics.

- **Open Question 4:** Can integrating this framework with explanation generation models improve the quality or faithfulness of model rationales?
  - Basis in paper: [explicit] The Conclusion suggests integrating the framework with explanation generation models could improve both the quality and evaluation of model rationales.
  - Why unresolved: It is currently unknown if human-derived reasoning categories act as an effective inductive bias for training or evaluating language models.
  - What evidence would resolve it: Training a model to generate taxonomy-compliant explanations and evaluating the output via human annotation or automated semantic similarity metrics.

## Limitations

- The study relies on single-annotator categorization for all explanations, despite strong inter-annotator agreement (κ≈0.79-0.83)
- Analysis is restricted to two NLI datasets, limiting generalizability to other domains or task formats
- The two-step model assumes explanations faithfully capture reasoning processes, which may not hold if explanations are post-hoc rationalizations

## Confidence

- **High confidence:** The existence of structured annotator preferences in both labels and reasoning types; the correlation between reasoning agreement and explanation similarity; the value of explanations for understanding semantic similarity beyond labels
- **Medium confidence:** The specific two-step decomposition model; the universality of reasoning-type preferences across datasets; the relative strength of κ(T|L) vs κ(L|T)
- **Low confidence:** The claim that explanations faithfully capture reasoning processes; the stability of individual annotator preferences over time or across task variations

## Next Checks

1. Replicate LiTEx annotation with multiple raters on a subset of both datasets to verify IAA stability and identify category ambiguity hotspots that single-annotator assignment might miss.

2. Cross-dataset validation by applying the full analysis pipeline to a third NLI dataset (e.g., MNLI with explanations or ChaosNLI subset) to test whether reasoning-type preference patterns and κ(T|L) > κ(L|T) relationships replicate.

3. Temporal stability test by re-annotating a random sample of 50 items from both datasets after a 4-week interval to assess whether individual annotator reasoning preferences persist or are task-specific artifacts.