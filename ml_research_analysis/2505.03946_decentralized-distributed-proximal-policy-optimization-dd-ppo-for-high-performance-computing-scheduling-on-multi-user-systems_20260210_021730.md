---
ver: rpa2
title: Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance
  Computing Scheduling on Multi-User Systems
arxiv_id: '2505.03946'
source_url: https://arxiv.org/abs/2505.03946
tags:
- scheduling
- policy
- dd-ppo
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to job scheduling in high-performance
  computing (HPC) environments using decentralized distributed proximal policy optimization
  (DD-PPO). Traditional rule-based scheduling algorithms are challenged by the increasing
  heterogeneity and scale of HPC systems.
---

# Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems

## Quick Facts
- **arXiv ID**: 2505.03946
- **Source URL**: https://arxiv.org/abs/2505.03946
- **Reference count**: 40
- **Primary result**: DD-PPO-based HPC scheduler outperforms PPO and rule-based algorithms on average waiting time, turnaround time, bounded slowdown, and resource utilization.

## Executive Summary
This paper introduces a novel approach to job scheduling in high-performance computing (HPC) environments using decentralized distributed proximal policy optimization (DD-PPO). Traditional rule-based scheduling algorithms struggle with the increasing heterogeneity and scale of modern HPC systems. The proposed DD-PPO scheduler leverages large-scale distributed training across multiple workers without requiring parameter synchronization at every step, enhancing scalability, training efficiency, and sample utilization. The scheduler is trained on a dataset containing over 11.5 million real HPC job traces spanning six years, demonstrating improved scheduling performance compared to both rule-based schedulers and existing reinforcement learning-based scheduling algorithms.

## Method Summary
The DD-PPO algorithm is implemented using Ray with RLlib for distributed training. The method employs an Actor-Critic architecture with policy and value networks trained on real HPC job traces in Standard Workload Format (SWF). The training leverages Population-Based Training (PBT) and Generalized Advantage Estimation (GAE) for improved convergence and hyperparameter optimization. The scheduler is evaluated on four key optimization objectives: average waiting time, average turnaround time, average bounded slowdown, and resource utilization. Training is performed on merged Falcon and Lemhi traces, while evaluation uses Lublin-256 and SDSC-SP2 datasets.

## Key Results
- DD-PPO scheduler outperforms PPO and rule-based baselines (FCFS, SJF, F1, WFP, UNI) on all four optimization metrics
- Improved scalability through distributed training without per-step parameter synchronization
- Enhanced sample utilization through PBT and GAE integration
- Generalization demonstrated across different evaluation datasets

## Why This Works (Mechanism)
The DD-PPO approach works by enabling scalable distributed training while maintaining sample efficiency through decentralized parameter updates. The use of PBT allows for dynamic hyperparameter adaptation during training, while GAE provides more accurate advantage estimation. The architecture's ability to handle large-scale training data (11.5M jobs) while maintaining generalization to unseen workloads demonstrates its effectiveness for real-world HPC scheduling challenges.

## Foundational Learning

**Proximal Policy Optimization (PPO)**: A policy gradient method that uses clipped objective functions to ensure stable learning. Needed for baseline comparison and as foundation for DD-PPO extension.

**Distributed Reinforcement Learning**: Training RL agents across multiple workers to improve sample efficiency and training speed. Critical for handling large-scale HPC scheduling problems.

**Standard Workload Format (SWF)**: Industry standard format for representing HPC job traces, including job submission times, resource requirements, and runtime information. Essential for training and evaluation.

**Population-Based Training (PBT)**: A hyperparameter optimization method that allows parameters to evolve during training by copying parameters from better-performing workers. Improves convergence and final performance.

**Generalized Advantage Estimation (GAE)**: A method for computing advantage estimates that reduces variance while maintaining reasonable bias. Improves policy gradient estimation accuracy.

## Architecture Onboarding

**Component map**: Workload traces -> SchedGym environment -> DD-PPO agent -> Policy network -> Value network -> Ray RLlib distributed training -> PBT optimization

**Critical path**: Job trace ingestion → Environment simulation → Policy action selection → Reward computation → Advantage estimation → Policy update → PBT parameter evolution

**Design tradeoffs**: Distributed training vs. parameter synchronization overhead; model complexity vs. training efficiency; exploration vs. exploitation in scheduling decisions.

**Failure signatures**: Poor convergence indicates improper PBT configuration or missing GAE; overfitting suggests insufficient diversity in training data or reward function misalignment.

**First experiments**: 1) Verify environment simulation with small trace subsets; 2) Test baseline PPO implementation before DD-PPO extension; 3) Validate reward function design with simple scheduling scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified hyperparameters for DD-PPO training may affect reproducibility
- Performance validation on relatively small evaluation datasets compared to massive training corpus
- Potential overfitting to training traces despite demonstrated generalization

## Confidence

**Methodological framework**: High confidence in the overall approach and reported improvements over baselines

**Absolute performance metrics**: Medium confidence due to unspecified hyperparameters and scale differences between training and evaluation datasets

## Next Checks

1. Implement DD-PPO with baseline PPO hyperparameters and tune systematically to establish convergence behavior
2. Validate reward function design through multiple formulations on small-scale training data
3. Conduct ablation studies removing PBT and GAE components to quantify individual contributions to performance improvements