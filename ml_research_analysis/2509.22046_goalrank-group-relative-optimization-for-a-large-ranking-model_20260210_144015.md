---
ver: rpa2
title: 'GoalRank: Group-Relative Optimization for a Large Ranking Model'
arxiv_id: '2509.22046'
source_url: https://arxiv.org/abs/2509.22046
tags:
- ranking
- policy
- generator
- goalrank
- evaluator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GoalRank, a one-stage generator-only ranking
  framework for recommender systems. It addresses the inefficiency of multi-generator
  evaluator paradigms by proving that a sufficiently large generator-only model can
  strictly outperform any finite mixture of small generators combined with an evaluator,
  while also exhibiting favorable scaling laws.
---

# GoalRank: Group-Relative Optimization for a Large Ranking Model

## Quick Facts
- arXiv ID: 2509.22046
- Source URL: https://arxiv.org/abs/2509.22046
- Reference count: 29
- One-line result: Generator-only large model outperforms finite mixtures with Evaluator via group-relative optimization

## Executive Summary
This paper introduces GoalRank, a one-stage generator-only ranking framework for recommender systems. It addresses the inefficiency of multi-generator evaluator paradigms by proving that a sufficiently large generator-only model can strictly outperform any finite mixture of small generators combined with an evaluator, while also exhibiting favorable scaling laws. The core method leverages group-relative optimization, using a reward model trained on user feedback to construct a reference policy over groups of candidate lists. This allows effective training of a large ranker without requiring an external evaluator. Extensive offline experiments on public datasets (ML-1M, Amazon-Book) and industrial data show consistent improvements over state-of-the-art baselines, with online A/B tests confirming significant gains in key business metrics such as app stay time and watch time. GoalRank achieves 18.6 ms latency versus 34.2 ms for multi-stage pipelines, and its MFU is 12.65% versus 2.03%, demonstrating both superior performance and efficiency.

## Method Summary
GoalRank is a generator-only ranking framework that trains a large Transformer-based ranker to directly output ranked lists without an external evaluator. The method uses group-relative optimization: during training, candidate lists are generated by auxiliary policies (heuristic models and lightweight neural models) to form diverse groups. A reward model scores these lists, and rewards are normalized within each group to create a reference policy. The generator is trained to minimize KL divergence to this reference policy, effectively learning to rank lists based on relative quality signals rather than absolute reward values. At inference, the trained generator directly produces ranked lists, eliminating the need for an evaluator and reducing latency by over 45% compared to traditional Generator-Evaluator pipelines.

## Key Results
- GoalRank achieves 18.6 ms latency versus 34.2 ms for multi-stage pipelines, reducing end-to-end latency by over 45%
- Offline experiments show 4.41%-6.43% improvement in Hit Ratio and 3.75%-4.70% improvement in NDCG across ML-1M and Amazon-Book datasets
- Online A/B tests confirm significant gains in business metrics: app stay time (+2.7%), watch time (+2.1%), and revenue (+1.2%)
- Scaling law experiments demonstrate that larger generators consistently outperform mixtures of smaller generators as model size increases

## Why This Works (Mechanism)

### Mechanism 1: Expressivity Advantage of Unified Policy Space
- **Claim:** A single, sufficiently large generator-only model can approximate the optimal ranking policy $\pi^*$ with strictly smaller error than any finite mixture of smaller generators combined with an evaluator.
- **Mechanism:** Theoretical analysis (Theorem 1) demonstrates that the policy space of a large generator $F_M$ is strictly larger than the $k$-mixture policy space $C_k^m$ of small generators. While mixtures saturate due to their finite combinatorial nature, the large generator acts as a universal approximator (under UAT assumptions), allowing its approximation error to approach zero as width increases.
- **Core assumption:** The activation functions of the generators satisfy universal approximation properties (e.g., standard MLP/Transformer activations), and the optimal policy $\pi^*$ is fully supported.
- **Evidence anchors:**
  - [abstract]: "theoretically prove that... there always exists a generator-only model that achieves strictly smaller approximation error"
  - [section] 3.1 Theorem 1: "E(F_M) < E(C_k^m)... lim_{n\to\infty} E(F_M) = 0"
  - [corpus]: No direct corpus validation of this specific approximation theory was found; related work focuses on value estimation rather than policy space topology.
- **Break condition:** If the target policy requires discontinuous mappings that the specific generator architecture cannot represent, the approximation advantage may vanish.

### Mechanism 2: Bias Mitigation via Group-Relative Normalization
- **Claim:** Normalizing rewards within a group of candidate lists mitigates the impact of absolute reward prediction bias, enabling stable knowledge distillation.
- **Mechanism:** The method constructs a reference policy $\pi_{ref}$ using softmax over rewards centered by the group mean and scaled by group standard deviation. This removes the global bias $b(l)$ as long as the bias is consistent within the group, preserving the relative order (partial order) of list quality.
- **Core assumption:** The bias $b(l)$ in the reward model $\hat{r}(l)$ is smaller than the reward gaps within a constructed group $B$ (i.e., signal-to-noise ratio is sufficient).
- **Evidence anchors:**
  - [abstract]: "leverage a reward model... to construct a reference policy in a group-relative manner"
  - [section] 3.2: "if max |r_hat(l_i) - r_hat(l_j)| > \sigma^*, we can exploit this order-invariance"
  - [corpus]: Related work (e.g., "From Generation to Consumption") highlights the difficulty of list value estimation, supporting the need for bias handling.
- **Break condition:** If the auxiliary policies used to construct the group generate lists with very similar rewards (small gaps), the normalization amplifies noise rather than signal.

### Mechanism 3: Surrogate KL Minimization
- **Claim:** Minimizing the KL divergence between the generator policy $\pi_\theta$ and the group-relative reference policy $\pi_{ref}$ serves as a tractable upper bound for optimizing the intractable optimal policy.
- **Mechanism:** The training objective (Eq. 5) minimizes cross-entropy between $\pi_\theta$ and $\pi_{ref}$. Since $\pi_{ref}$ is derived from reward signals that correlate with the optimal policy, aligning to $\pi_{ref}$ implicitly aligns to $\pi^*$ without requiring an external evaluator during inference.
- **Core assumption:** The reward model captures the user's true utility function sufficiently well to guide the generator.
- **Evidence anchors:**
  - [section] 3.2: "This objective provides a tractable surrogate for minimizing KL(\pi_\theta || \pi^*)"
  - [abstract]: "reference policy serves as a practical surrogate of the optimal policy"
- **Break condition:** If the reward model fails to correlate with the true business metric (e.g., watch time), the generator learns a "optimal" policy for the wrong objective.

## Foundational Learning

- **Concept: Universal Approximation Theorem (UAT)**
  - **Why needed here:** The theoretical guarantee of GoalRank rests on the ability of a single large network to approximate any continuous function (or policy) better than a mixture of smaller ones. Without this theoretical backing, the "scaling law" argument is weak.
  - **Quick check question:** Does the paper assume depth scaling, width scaling, or both for the approximation guarantee? (Answer: Primarily width in the main text, but notes depth works too).

- **Concept: Entropy-Regularized Reinforcement Learning**
  - **Why needed here:** The definition of the optimal policy $\pi^*$ in Section 3.2 is derived from an entropy-regularized objective (Eq. 1), resulting in a Boltzmann distribution. Understanding this is necessary to grasp why KL divergence is the loss function of choice.
  - **Quick check question:** Why is entropy regularization added to the reward objective? (Answer: To avoid greedy instability and encourage exploration).

- **Concept: Generator-Evaluator (G-E) Paradigm**
  - **Why needed here:** GoalRank is defined in opposition to the standard G-E pipeline. Understanding that G-E decouples list generation from value estimation is essential to see why GoalRank merging them reduces latency and MFU.
  - **Quick check question:** In the G-E paradigm, what component does GoalRank eliminate to achieve its latency reduction? (Answer: The explicit Evaluator stage).

## Architecture Onboarding

- **Component map:** Generator ($g_\theta$) -> Auxiliary Policies ($M$) -> Reward Model ($\hat{r}$) -> Reference Policy Module -> Generator Training

- **Critical path:**
  1. Retrieve candidate set $V_u$.
  2. **Training Only:** Sample lists from Auxiliary Policies + Current Generator to form Group $B_u$.
  3. Score all lists in $B_u$ using Reward Model.
  4. Compute $\pi_{ref}$ (Softmax over normalized rewards).
  5. Train Generator to minimize KL($\pi_\theta || \pi_{ref}$).
  6. **Inference:** Generator produces list directly (No Group, No Reward Model, No Evaluator).

- **Design tradeoffs:**
  - **Latency vs. Flexibility:** GoalRank is faster (18.6ms vs 34.2ms) but the paper notes it is "less flexible in adapting to... changing business objectives" compared to G-E models where you just swap the evaluator.
  - **Group Size $|B|$:** Table 2 shows a sweet spot (8-20). Too small = high variance; too large = reward gap condition fails (bias dominates).

- **Failure signatures:**
  - **Reward Hacking:** If the reward model has spurious correlations, the generator will aggressively exploit them, potentially lowering real user satisfaction despite high proxy scores.
  - **Mode Collapse:** If the auxiliary policies are too similar, the group $B_u$ lacks diversity, providing no gradient signal for relative improvement.

- **First 3 experiments:**
  1. **Scaling Validation:** Replicate Figure 3 by varying hidden dimensions (1M to 0.1B parameters) on a subset of data to confirm the "scaling law" holds for your specific data distribution.
  2. **Group Size Ablation:** Test group sizes $|B| \in \{3, 10, 50, 100\}$ to find the local optimum where reward gaps are maximized relative to noise (validating Table 2).
  3. **Latency/MFU Profiling:** Deploy the model in a staging environment to verify the claimed latency reduction (~18ms) and MFU improvement (>10%) compared to a baseline multi-generator pipeline.

## Open Questions the Paper Calls Out

- **Question:** How can GoalRank be extended to maintain flexibility when adapting to diverse and frequently changing business objectives?
  - **Basis in paper:** [explicit] The authors state in the "Limitation and Future Work" section that compared to Generator-Evaluator models, a generator-only framework is "less flexible in adapting to such shifts" in business goals.
  - **Why unresolved:** The current framework optimizes towards a reference policy derived from a specific reward model; a mechanism for dynamically adjusting to shifting constraints or objectives without retraining is not defined.
  - **What evidence would resolve it:** A modified GoalRank framework that accepts contextual business constraints as input and successfully optimizes for dynamically weighted multi-objective functions (e.g., balancing watch time vs. revenue) in real-time.

- **Question:** Can large retrieval models and the GoalRank ranking model be jointly optimized to capture list-wise dependencies end-to-end?
  - **Basis in paper:** [explicit] The conclusion notes that recent large retrieval models "overlook list-wise modeling" and suggests future work explore how these models can be "jointly optimized."
  - **Why unresolved:** GoalRank currently treats the retrieval candidate set as a fixed input ($V_u$); the potential for the ranker's list-wise loss to improve the retriever's candidate selection remains untested.
  - **What evidence would resolve it:** An end-to-end trainable architecture where gradients from the GoalRank loss are backpropagated to the retrieval stage, resulting in improved recall of list-compatible items.

- **Question:** How can the construction of the auxiliary policy group $B$ be automated to ensure optimal reward gaps without manual tuning?
  - **Basis in paper:** [inferred] While the ablation study (Table 2) shows performance is sensitive to group size $|B|$ (peaking at 10-20), the selection of the auxiliary policy set $M$ is described heuristically in Section 3.3 and Appendix C.
  - **Why unresolved:** The paper demonstrates that incorrect group sizing leads to failure (insufficient samples) or bias amplification, yet offers no adaptive principle for constructing the group.
  - **What evidence would resolve it:** A learning-based or adaptive algorithm that constructs the group $B$ by maximizing the estimated reward variance or information gain, consistently outperforming fixed-size configurations.

## Limitations
- **Approximation Theory Generalizability:** The proof relies on strong assumptions about universal approximation properties and full support of the optimal policy that may not hold in practical recommendation settings with discontinuous or multimodal policies.
- **Reward Model Dependency:** GoalRank's performance fundamentally depends on the quality of the pre-trained reward model, with no quantification of sensitivity to reward model errors or robustness to misalignment with actual user satisfaction.
- **Scaling Law Validation:** The scaling law experiments are conducted on relatively small public datasets (ML-1M, Amazon-Book), with no validation on industrial-scale datasets with different characteristics.

## Confidence
- **High Confidence:** The empirical latency and MFU improvements (18.6ms vs 34.2ms, 12.65% vs 2.03%) are directly measurable from the described architecture and validated through online A/B tests.
- **Medium Confidence:** The offline performance improvements (4.41%-6.43% on Hit Ratio, 3.75%-4.70% on NDCG) are statistically significant on the tested datasets, but the general applicability to different domains remains uncertain.
- **Low Confidence:** The theoretical claim that "there always exists a generator-only model that achieves strictly smaller approximation error" is mathematically proven but relies on assumptions that may not hold in practice, particularly regarding policy space topology and universal approximation conditions.

## Next Checks
1. **Scaling Robustness Test:** Validate the scaling law on a large industrial dataset (e.g., 100M+ interactions) by training GoalRank with varying model sizes (1M to 100M parameters) and measuring whether the approximation advantage persists.

2. **Reward Model Sensitivity Analysis:** Systematically degrade the reward model quality (e.g., by training on reduced data or adding noise) and measure the impact on GoalRank's performance to quantify the robustness to reward model errors.

3. **Cross-Domain Transferability:** Apply GoalRank to a different recommendation domain (e.g., short-video recommendation or news feed) with different user behavior patterns to test whether the group-relative optimization mechanism generalizes beyond e-commerce and movie datasets.