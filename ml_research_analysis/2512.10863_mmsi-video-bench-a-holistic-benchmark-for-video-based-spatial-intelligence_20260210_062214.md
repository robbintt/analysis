---
ver: rpa2
title: 'MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence'
arxiv_id: '2512.10863'
source_url: https://arxiv.org/abs/2512.10863
tags:
- spatial
- video
- reasoning
- wang
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMSI-Video-Bench is a fully human-annotated benchmark designed
  to evaluate video-based spatial intelligence in multimodal large language models.
  It operationalizes a four-level framework covering Perception, Planning, Prediction,
  and Cross-Video Reasoning through 1,106 questions grounded in 1,278 video clips
  from 25 diverse datasets and in-house recordings.
---

# MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence

## Quick Facts
- **arXiv ID**: 2512.10863
- **Source URL**: https://arxiv.org/abs/2512.10863
- **Reference count**: 40
- **Primary result**: Fully human-annotated benchmark revealing near-chance model performance (38.0% best) vs. human accuracy (96.4%) on video-based spatial reasoning tasks

## Executive Summary
MMSI-Video-Bench is a comprehensive benchmark designed to evaluate video-based spatial intelligence in multimodal large language models through a four-level framework covering Perception, Planning, Prediction, and Cross-Video Reasoning. The benchmark consists of 1,106 carefully designed multiple-choice questions grounded in 1,278 video clips from 25 diverse datasets and in-house recordings, each reviewed by 3D vision experts. Current models perform near chance level, with the best reasoning model lagging humans by nearly 60%, revealing fundamental limitations in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence.

## Method Summary
The benchmark employs a human-expert annotation protocol where eleven 3D vision researchers manually design each question, selecting clips and writing questions/answers/rationales that undergo cross-evaluation with 100% approval required on three criteria: clear (unambiguous), correctness (unique accurate answer), and challenge (non-trivial reasoning). The task hierarchy cascades from Spatial Construction (inferring global layouts from partial observations) through Motion Understanding (temporal dynamics) to Planning/Prediction (decision-making under hypothetical conditions) and Cross-Video Reasoning (memory integration across discontinuous observations). Evaluation uses both uniform frame sampling (50 frames) and sufficient-coverage approaches, with error analysis categorizing failures into five non-overlapping types.

## Key Results
- Current models achieve near-chance accuracy (~24-25%) compared to human performance (96.4%), revealing a ~60% performance gap
- Prediction tasks are hardest (average ~30% accuracy) while other categories average ~35%, indicating particular struggles with hypothetical reasoning
- Geometric Reasoning errors dominate Spatial Construction tasks, while Detailed Grounding errors dominate Motion Understanding
- Neither 3D spatial cues nor chain-of-thought prompting yields meaningful improvements in model performance

## Why This Works (Mechanism)

### Mechanism 1: Human-Expert Annotation Protocol
- Core assumption: Expert-designed questions with explanatory rationales capture spatial reasoning failure modes that automated generation cannot replicate
- Evidence: Each annotation required 100% approval on clear/correctness/challenge criteria with detailed rationales ensuring precise grounding
- Break condition: If automated generation achieves comparable diagnostic power, the annotation overhead may not be justified

### Mechanism 2: Hierarchical Task Framework
- Core assumption: Spatial intelligence decomposes into hierarchical levels that can be independently measured and improved
- Evidence: Four-level framework (Perception → Planning → Prediction → Cross-Video) enables structured diagnosis of capability gaps at different reasoning depths
- Break condition: If task failures are primarily downstream effects of a single root cause, the hierarchical diagnosis provides limited independent signal

### Mechanism 3: Error Categorization Mapping
- Core assumption: Five-category error taxonomy represents separable failure modes that can be independently targeted
- Evidence: Error analysis on 520 incorrect cases shows Geometric Reasoning errors dominate Spatial Construction; Detailed Grounding errors dominate Motion Understanding
- Break condition: If error types are highly correlated or fixing one category doesn't improve downstream performance, the taxonomy may not guide effective interventions

## Foundational Learning

- **Concept: Video-Based Spatial Intelligence**
  - Why needed here: Evaluates spatial understanding from continuous visual input requiring temporal integration (e.g., inferring 3D layout from sequential partial views)
  - Quick check question: Why can't single-frame spatial reasoning solve "How many separate rooms are visible?" in a panning video?

- **Concept: Frame Sampling Tradeoffs**
  - Why needed here: Shows uniform sampling outperforms consecutive sampling, and AKS transfers poorly—key insights for deploying video MLLMs
  - Quick check question: Why might semantic-similarity-based keyframe selection fail on questions like "What happened while the object was out of view?"

- **Concept: Human-AI Performance Gap Interpretation**
  - Why needed here: ~60% gap between best model (38.0%) and humans (96.4%) signals fundamental capability limitations, not just benchmark difficulty
  - Quick check question: Does a large human-AI gap indicate a flawed benchmark or a genuine capability limitation? How would you distinguish?

## Architecture Onboarding

- **Component map**: 25 public datasets + 140 in-house videos → 11 3DV annotators → cross-validation → 1,106 MCQ questions over 1,278 clips → evaluation tracks (Uniform-50 vs Sufficient-Coverage)
- **Critical path**: Data preprocessing (downsample FPS, timestamp frames) → Annotation (select clip → design question → write answer + distractors + rationale) → Quality control (cross-evaluation against clear/correctness/challenge) → Evaluation (run inference → extract answers → compute exact-match accuracy) → Error analysis (sample failures → categorize → compute distribution)
- **Design tradeoffs**: MCQ format enables automated scoring but constrains open-ended reasoning; 50-frame limit required for API constraints but may truncate long videos; human annotation ensures quality but isn't scalable (400+ hours for 1,106 questions)
- **Failure signatures**: Near-chance accuracy (~24-25%) indicates model lacks basic spatial reasoning; Prediction hardest category (avg ~30% vs ~35% others); Camera-Instance Spatial Relation hardest subtype (ego-to-scene transformation + fine-grained grounding); Sufficient-Coverage ≤ Uniform-50 (more frames introduce noise; model lacks selective attention)
- **First 3 experiments**: 1) Run baseline evaluation (e.g., QwenVL2.5-7B) under both Uniform-50 and Sufficient-Coverage to quantify sampling sensitivity on your target model class; 2) Perform per-category error analysis on 50+ failures to identify whether your model's error profile matches reported patterns; 3) Test one intervention (3D cues via VGGT or CoT prompting) on held-out subset—expect <1% gain per paper findings—to establish whether your model responds differently

## Open Questions the Paper Calls Out
None

## Limitations
- The human-AI performance gap interpretation assumes benchmark difficulty is appropriate rather than reflecting design choices
- Error taxonomy, while comprehensive, hasn't been validated across different model architectures or real-world applications
- The 50-frame limit may truncate long videos and introduce sampling bias for certain spatial reasoning tasks

## Confidence

- **High confidence**: Human-annotation protocol produces high-quality questions with clear grounding and expert review
- **Medium confidence**: Hierarchical task framework effectively diagnoses capability gaps at different reasoning depths
- **Medium confidence**: Five-category error taxonomy reveals distinct task-specific failure patterns

## Next Checks
1. Cross-validate the error taxonomy by applying it to a completely different video reasoning dataset to test whether the same categories emerge or if new patterns appear
2. Test whether targeted training on specific error categories (e.g., geometric reasoning drills for Spatial Construction) yields measurable improvements, validating whether these represent independent failure modes
3. Compare performance on MMSI-Video-Bench questions versus equivalent real-world scenarios (e.g., robotics navigation tasks) to determine if the benchmark generalizes beyond controlled evaluation conditions