---
ver: rpa2
title: 'Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in
  Indian Languages'
arxiv_id: '2506.03884'
source_url: https://arxiv.org/abs/2506.03884
tags:
- languages
- language
- speech
- synthesis
- sanskrit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building TTS systems for
  low-resource Indian languages that use scripts from one language family but have
  phonotactic properties from another. The authors propose a method that augments
  a shared phone representation (CLS) to cover missing sounds and modifies text parsing
  rules to match the phonotactics of target languages.
---

# Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages

## Quick Facts
- arXiv ID: 2506.03884
- Source URL: https://arxiv.org/abs/2506.03884
- Reference count: 0
- Key outcome: The paper proposes a method for zero-shot TTS in low-resource Indian languages by augmenting a shared phone representation (CLS) and modifying text parsing rules to match target phonotactics, achieving high naturalness and intelligibility without training data.

## Executive Summary
This paper addresses the challenge of building TTS systems for low-resource Indian languages that use scripts from one language family but have phonotactic properties from another. The authors propose a method that augments a shared phone representation (CLS) to cover missing sounds and modifies text parsing rules to match the phonotactics of target languages. This approach enables rapid adaptation without requiring training data in the target language. The method was evaluated on Sanskrit, Maharashtrian and Canara Konkani, Maithili, and Kurukh. Results show significantly improved naturalness (MOS scores up to 4.48), intelligibility (WER as low as 4.6%), and lower Mel-cepstral distortion compared to baseline systems that did not account for linguistic properties. The approach demonstrates the potential for expanding speech technology access to under-represented languages.

## Method Summary
The method involves training separate FastSpeech2 acoustic models and HiFi-GAN vocoders for each source language (Hindi, Kannada, Marathi, Telugu). For zero-shot synthesis of a target language, the text is parsed into a sequence of phones from the Common Label Set (CLS) using rules specific to the target language's phonotactics, not its script. The parsed phoneme sequence is then fed into a pre-trained source language acoustic model that is linguistically similar to the target, which generates the mel-spectrogram that a vocoder converts to speech. The CLS is expanded to map absent sounds in the target language to the closest acoustic equivalent (e.g., /q/ → /k/).

## Key Results
- MOS scores up to 4.48 for naturalness across target languages
- WER as low as 4.6% for intelligibility in target languages
- Lower Mel-cepstral distortion compared to baseline systems
- Sanskrit synthesized with Dravidian model (Kannada) achieved MOS 4.12 vs 3.25 with Indo-Aryan model (Hindi)
- Schwa retention in Sanskrit confirmed through mel-spectrogram comparison

## Why This Works (Mechanism)

### Mechanism 1
Text-to-speech systems can generate intelligible speech for a low-resource "target" language without training data by borrowing an acoustic model from a resource-rich "source" language, provided the text parsing rules are modified to match the target's phonotactics. The approach decouples the front-end text processing from the back-end acoustic model. Target language text is parsed into a shared phone representation (Common Label Set - CLS) using rules specific to the target's phonotactics, not the source's script. This phoneme sequence is then fed into a pre-trained source language acoustic model which generates the mel-spectrogram, which a vocoder converts to speech. This mechanism fails if the target language contains phonemes or critical prosodic features that are not present in the source language's acoustic model.

### Mechanism 2
A unified, language-agnostic phone representation (CLS) is essential for zero-shot cross-lingual TTS, acting as an interlingua that maps graphemes from any supported script to a standardized set of phonemic labels. Instead of training on characters from a specific script, the system maps all graphemes from various Indian scripts to a Common Label Set (CLS) of about 72 tokens. Text from an unseen language is first converted to this CLS. The TTS acoustic model is trained to generate speech from these CLS labels, not raw script, making the model script-agnostic and allowing it to synthesize any language that can be mapped to the CLS. This mechanism breaks down if the CLS lacks a symbol for a crucial phonemic contrast in the target language.

### Mechanism 3
Selecting the source TTS system based on linguistic similarity—specifically phonotactics and prosody—rather than just script similarity is crucial for generating natural-sounding speech. Different languages have different prosodic patterns. A model trained on Language A will impose its prosody onto the generated speech. To synthesize Language B zero-shot, the source model from Language A should be chosen such that its natural prosody aligns with Language B's. The paper shows this by using a Dravidian model (with schwa retention) for Sanskrit instead of an Indo-Aryan model (with schwa deletion). This mechanism may fail if the source and target languages, despite sharing phonotactics, have vastly different intonation patterns or stress rules.

## Foundational Learning

- **Concept:** Phonotactics and Schwa Deletion
  - **Why needed here:** This is the central problem the paper addresses. Understanding that languages like Hindi (Indo-Aryan) delete the inherent vowel 'a' (schwa) in certain positions, while languages like Kannada (Dravidian) and Sanskrit generally do not, is crucial for understanding the paper's method of aligning parsing rules with the target language's properties.
  - **Quick check question:** What is the key phonotactic difference between Hindi and Sanskrit that makes a default Hindi TTS model unsuitable for direct Sanskrit synthesis?

- **Concept:** Common Label Set (CLS) / Unified Phone Representation
  - **Why needed here:** The CLS is the foundational technical artifact that enables the method. It acts as the bridge between any Indian script and the TTS model. Without it, the text-to-phoneme conversion for unseen scripts would not be possible within a single framework.
  - **Quick check question:** What is the primary function of the Common Label Set (CLS) in the context of this paper's architecture?

- **Concept:** Zero-Shot Learning / Cross-Lingual Transfer
  - **Why needed here:** The entire goal is "zero-shot synthesis" for unseen languages. This concept defines the problem space: generating speech for a language with *no training data* by transferring knowledge from a related, resource-rich language.
  - **Quick check question:** In this paper, what does the term "zero-shot synthesis" specifically refer to in terms of training data for the target language?

## Architecture Onboarding

- **Component map:** Text Input -> Unified Parser (Modified G2P) -> Phone-to-CLS Mapper -> Pre-trained Acoustic Model -> Vocoder (HiFi-GAN)

- **Critical path:**
  1. Identify the target language's script and its phonotactic family
  2. Configure the Unified Parser to use the correct rule set based on target language properties
  3. Pass the target text through the configured parser to get a CLS phone sequence
  4. Select a suitable pre-trained acoustic model based on linguistic similarity
  5. Generate the mel-spectrogram using the selected model and synthesize audio with its paired vocoder

- **Design tradeoffs:**
  - Speed vs. Quality: The zero-shot approach is extremely fast and requires no new training data, but it may inherit prosodic characteristics of the source language, potentially sounding accented
  - CLS Coverage vs. Granularity: A compact CLS (72 tokens) enables rapid adaptation but may neutralize subtle phonemic distinctions

- **Failure signatures:**
  - Incorrect Prosody/Schwa Deletion: If the wrong parser rule is applied, the output will sound broken and words will be truncated
  - Unintelligible Phonemes: If the CLS mapping is poor or the source model lacks a specific phone, the output will substitute sounds
  - Glottal Stop Issues: Specific sounds like glottal stops may not be handled well if the source model does not have a strong representation for them

- **First 3 experiments:**
  1. Phonotactic Ablation: Synthesize Sanskrit using a Hindi model with both default IA rules (schwa deletion) and modified DR rules (schwa retention). Compare MOS and WER to quantify the impact of phonotactic alignment.
  2. Source Model Comparison: For a single target language, synthesize identical text using models from different source languages. Perform MOS tests and MCD analysis to correlate linguistic similarity with output quality.
  3. CLS Coverage Stress Test: Select a language with a unique phoneme not explicitly in the CLS. Synthesize words containing this phoneme using the fallback mapping. Evaluate via native speaker listening test to determine if the substitution is perceptible and causes intelligibility errors.

## Open Questions the Paper Calls Out

### Open Question 1
What specific acoustic or linguistic factors cause the Marathi TTS system to generate artifacts in Maharashtrian Konkani synthesis, leading to higher Mel-cepstral distortion (MCD) despite better subjective naturalness? The paper identifies the discrepancy between the MOS and MCD scores but does not perform an analysis to isolate the source of these specific acoustic artifacts.

### Open Question 2
How can the framework be extended to handle suprasegmental features, such as intonation contours for questions versus assertions, which currently fail in languages like Kurukh? The proposed method focuses on phonotactics and segmental features but relies on the source synthesizer's implicit prosody, which does not transfer correctly for all sentence types.

### Open Question 3
Can the process of selecting the optimal source synthesizer and determining the appropriate phonotactic rule set (IA vs. DR) be automated for an unseen language without manual linguistic analysis? The methodology relies on the authors' manual analysis to determine phonotactics and select synthesizers, but does not provide a mechanism to predict the best existing synthesizer for a new, undocumented language automatically.

## Limitations
- Limited Phoneme Coverage in CLS: The method relies on mapping unseen phonemes to acoustically similar ones, which could lead to intelligibility errors
- Source Model Selection Heuristic: The process is largely manual and based on linguistic intuition, which may not generalize to other languages with mixed linguistic features
- Evaluation Scope: The evaluation focuses on naturalness, intelligibility, and acoustic similarity but does not assess long-form speech quality or mixed-script scenarios

## Confidence

**High Confidence**: The core mechanism of using phonotactically aligned parsing rules to override script-based defaults is well-demonstrated through the Sanskrit schwa retention experiments with clear visual evidence from mel-spectrogram comparisons.

**Medium Confidence**: The claim that the CLS mapping enables zero-shot synthesis is supported by quantitative results, but the robustness of the phoneme substitution strategy across diverse languages remains uncertain without broader testing.

**Low Confidence**: The generalizability of the source model selection heuristic to languages not tested in this study is uncertain, as the underlying criteria for this decision are not formalized or validated across a wider range of languages.

## Next Checks

1. **Phoneme Substitution Stress Test**: Systematically test the CLS mapping strategy by synthesizing words containing every phoneme not in the original CLS. Have native speakers rate the acceptability of each substitution to quantify the intelligibility cost of the approximation strategy.

2. **Cross-Lingual Model Transferability**: For a single target language, synthesize the same text using all four source models (Hindi, Kannada, Marathi, Telugu). Compare MOS scores and WER to build a quantitative model of how linguistic similarity predicts synthesis quality.

3. **Mixed-Script and Long-Form Evaluation**: Test the system on languages that use multiple scripts and synthesize extended passages rather than isolated sentences. Measure whether prosodic consistency and speaker identity are maintained over longer utterances.