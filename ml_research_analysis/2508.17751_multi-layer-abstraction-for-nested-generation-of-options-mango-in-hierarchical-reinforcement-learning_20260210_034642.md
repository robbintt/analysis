---
ver: rpa2
title: Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical
  Reinforcement Learning
arxiv_id: '2508.17751'
source_url: https://arxiv.org/abs/2508.17751
tags:
- options
- abstract
- layer
- agent
- abstraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MANGO (Multi-layer Abstraction for Nested Generation of Options)
  is a hierarchical reinforcement learning framework designed to address the challenges
  of long-term sparse reward environments. It decomposes complex tasks into multiple
  layers of abstraction, where each layer defines an abstract state space and employs
  options to modularize trajectories into macro-actions.
---

# Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.17751
- Source URL: https://arxiv.org/abs/2508.17751
- Authors: Alessio Arcudi; Davide Sartor; Alberto Sinigaglia; Vincent François-Lavet; Gian Antonio Susto
- Reference count: 2
- One-line primary result: Multi-layer hierarchical RL framework that decomposes tasks into nested abstractions achieves substantial improvements in sample efficiency and generalization for sparse-reward environments

## Executive Summary
MANGO is a hierarchical reinforcement learning framework designed to tackle long-term sparse reward environments by decomposing complex tasks into multiple layers of abstraction. Each layer defines an abstract state space using concept functions that progressively compress the state space, with options at each level modularizing trajectories into macro-actions. The framework employs nested options across layers, allowing efficient reuse of learned movements and improved sample efficiency. Experiments in procedurally-generated grid environments demonstrate significant performance gains over standard RL methods, though the approach is sensitive to hyperparameter tuning and requires careful scheduling to prevent error propagation between layers.

## Method Summary
MANGO constructs a hierarchy of options where each layer i has its own abstract state space S^ϕi defined by a concept function ϕi, and an intra-layer policy that selects options from the layer below. The framework uses layer-wise training with freezing, where base-layer options are trained first and then frozen before training higher layers. Task-specific components like reward functions are integrated at the top layer through task actions. The abstract transition dynamics are learned at each layer, and intrinsic rewards are shaped by alignment between achieved and intended abstract transitions. The approach is tested on procedurally-generated grid environments, demonstrating improved sample efficiency and generalization compared to flat RL baselines.

## Key Results
- Substantial improvements in sample efficiency and generalization capabilities compared to standard RL methods in procedurally-generated grid environments
- Near-perfect completion rates (approximately 100%) achieved in 16x16 Frozen Lake maps when properly configured
- Method demonstrates sensitivity to hyperparameter tuning, particularly for the base layer, with aggressive learning rates or early termination potentially hindering convergence

## Why This Works (Mechanism)

### Mechanism 1: State Compression via Progressive Abstractions
Layered concept functions reduce effective state space at higher levels, accelerating credit assignment. A concept function ϕ : S → S^ϕ partitions the state space into connected regions (Definition 3.2). These partitions are progressive refinements (S^ϕi ⊃ S^ϕi+1), so a transition in layer i+1 compresses multiple transitions in layer i into one abstract edge. The core assumption is that manually designed concept functions preserve task-relevant structure; poorly chosen partitions break connectivity and policy coherence. Break condition: If partitions are not connected or misaligned with task structure, agents cannot reliably reach abstract states, and higher layers inherit incoherent transition models.

### Mechanism 2: Nested Options Recursion
Allowing options to invoke lower-level options enables compositional reuse and shortens effective horizon. Intra-layer policies ˆπo^ϕi : S → P(O^ϕi-1) select lower-level options until termination β^ϕi-1 triggers (Definition 3.5). This yields a recursive execution tree where each macro-action expands into subroutines. The core assumption is that lower-level options have converged sufficiently; otherwise, composing unstable subroutines propagates errors upward. Break condition: If base-layer options have high failure rates, composition amplifies error; the paper notes "if the base layer does not thoroughly learn [...] higher layers inherit an unreliable subroutine."

### Mechanism 3: Intrinsic Reward Shaping via Abstract Transitions
Reward functions shaped by abstract transition alignment provide dense feedback even in sparse-reward environments. When executing an abstract action a^ϕi+1, the intrinsic reward r^¯a^ϕi+1 is nonzero only at termination and weighted by (1 - 2d^T_a^ϕi+1(δ)), which measures alignment between the achieved and intended abstract transition (Equation 5). The core assumption is that the abstract transition dynamics Ta^ϕi+1 are learnable and sufficiently deterministic for the shaping signal to be meaningful. Break condition: If transition dynamics are highly stochastic or the abstraction is too coarse, the shaping signal becomes noisy, potentially misleading learning.

## Foundational Learning

- **Markov Decision Processes (MDPs) and Semi-MDPs**: Why needed here: MANGO builds on MDPs at each layer and SMDPs for options with multi-step duration. Quick check question: Can you explain why an option induces a Semi-MDP rather than a standard MDP?

- **Options Framework (Sutton et al., 1999)**: Why needed here: The entire hierarchy is constructed from options ⟨πo, βo⟩; understanding termination and policy composition is essential. Quick check question: What happens if an option's termination condition is too permissive?

- **State Abstraction and Bisimulation**: Why needed here: Concept functions are state abstractions; their quality determines whether hierarchical decomposition is even possible. Quick check question: How would you verify that two abstract states are behaviorally equivalent?

## Architecture Onboarding

- **Component map**: Base Layer (ϕ0: identity abstraction) -> Intermediate Layers (ϕ1...ϕn-1: progressively coarser concept functions) -> Top Layer (ϕn + task action τ: task policy integrates environment reward R)

- **Critical path**:
  1. Define concept functions ϕ0...ϕn ensuring connected partitions
  2. Train base-layer options until convergence (hole avoidance, local navigation)
  3. Freeze base layer; train layer 1 using frozen base options
  4. Repeat until top task policy converges
  5. Execute via top-level task option cascading down

- **Design tradeoffs**:
  - Abstraction granularity: Coarser abstractions reduce state space but may obscure critical transitions; finer abstractions retain detail but increase sample cost
  - Layer-wise training budget: Under-training lower layers causes error propagation; over-training delays overall convergence
  - Learning rate scheduling: Aggressive rates yield fast initial gains but prevent fine-tuning; conservative rates improve stability

- **Failure signatures**:
  - Base layer never exceeds ~70% local success: higher layers cannot converge
  - Q-values at higher layers collapse or oscillate: likely due to non-stationarity from unfrozen lower layers
  - Episode terminates early without reaching goal: check if abstract transitions are unreachable under current partition

- **First 3 experiments**:
  1. **Sanity check**: Train single-layer MANGO (identity abstraction only) on 4×4 Frozen Lake; verify it matches standard Q-learning performance
  2. **Ablation**: Compare 2-layer vs 3-layer hierarchy on 8×8 Frozen Lake; measure sample efficiency and final success rate to isolate abstraction benefit
  3. **Stress test**: Randomize hole density in 16×16 Frozen Lake; sweep base-layer training budgets to identify minimum viable convergence threshold before error propagation dominates

## Open Questions the Paper Calls Out
None

## Limitations

- **Abstraction Quality Dependency**: Framework assumes manually designed concept functions preserve task-relevant structure; poor abstractions can prevent convergence
- **Error Propagation Sensitivity**: Layer-wise training creates fragile dependency chain where base-layer failures amplify through higher layers
- **Hyperparameter Sensitivity**: Method requires careful learning rate scheduling and training budget allocation, particularly for the base layer

## Confidence

- **State Compression Mechanism**: High confidence - The progressive abstraction framework is well-defined and grounded in established HRL theory
- **Nested Options Recursion**: High confidence - The recursive composition mechanism is theoretically sound and directly builds on the options framework
- **Intrinsic Reward Shaping**: Medium confidence - The mechanism is novel but lacks empirical validation of its contribution relative to other factors

## Next Checks

1. **Abstraction Robustness Test**: Systematically vary concept function granularity and connectivity in procedurally-generated environments to quantify the relationship between abstraction quality and learning efficiency, measuring failure rates when partitions disconnect key state transitions.

2. **Error Propagation Quantification**: Design experiments where base-layer success rates are artificially constrained (70%, 80%, 90%) and measure the resulting performance decay at higher layers, establishing quantitative bounds on error tolerance.

3. **Reward Shaping Ablation**: Implement a variant of MANGO that removes the intrinsic reward component while preserving the hierarchical structure, then compare sample efficiency and convergence speed across environments with varying reward sparsity to isolate the shaping contribution.