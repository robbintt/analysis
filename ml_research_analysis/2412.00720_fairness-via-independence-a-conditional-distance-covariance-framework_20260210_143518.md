---
ver: rpa2
title: 'Fairness via Independence: A (Conditional) Distance Covariance Framework'
arxiv_id: '2412.00720'
source_url: https://arxiv.org/abs/2412.00720
tags:
- dual
- fairness
- distance
- covariance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Fairness via Independence: A (Conditional) Distance Covariance Framework

## Quick Facts
- **arXiv ID**: 2412.00720
- **Source URL**: https://arxiv.org/abs/2412.00720
- **Authors**: Ruifan Huang; Haixia Liu
- **Reference count**: 40
- **Primary result**: Proposes and validates empirical Distance Covariance (DC) and Conditional Distance Covariance (CDC) as fairness regularization terms, showing favorable accuracy-fairness trade-offs on tabular and image datasets.

## Executive Summary
This paper introduces a novel fairness regularization framework using empirical Distance Covariance (DC) and Conditional Distance Covariance (CDC) to enforce independence or conditional independence between model predictions and sensitive attributes. The method is applicable to both Demographic Parity (DP) and Equalized Odds (EO) fairness constraints. Experiments on tabular datasets (Adult, ACSIncome) and image datasets (CelebA, UTKFace) demonstrate that the approach achieves competitive accuracy while reducing fairness gaps, with particular effectiveness in the low-fairness-gap regime.

## Method Summary
The authors propose using empirical Distance Covariance (DC) for Demographic Parity and Conditional Distance Covariance (CDC) for Equalized Odds as regularization terms in the classification loss. These statistics measure statistical independence (or conditional independence) between predictions and sensitive attributes. The optimization is performed using a Lagrangian dual approach, where model weights are updated via SGD and the regularization multiplier λ is dynamically adjusted using a gradient-based update rule. The method is tested on both tabular data (using MLPs) and image data (using ResNet-18), showing effective control over fairness metrics with minimal accuracy loss.

## Key Results
- Empirical DC and CDC effectively regularize fairness constraints on both tabular and image datasets.
- The method achieves favorable accuracy-fairness trade-offs, particularly excelling at low fairness gap values.
- Dynamic adjustment of the regularization multiplier λ via the dual update rule provides stable convergence.

## Why This Works (Mechanism)
The framework leverages the statistical properties of distance covariance to quantify dependence between predictions and sensitive attributes. DC measures overall dependence for DP, while CDC conditions on true labels to enforce EO. The Lagrangian dual formulation allows for effective trade-off control between accuracy and fairness through the dynamic λ update.

## Foundational Learning
- **Distance Covariance (DC)**: A measure of dependence between random variables based on pairwise distances. *Why needed*: Core statistic for enforcing independence in fairness constraints. *Quick check*: Verify DC=0 for independent variables on synthetic data.
- **Conditional Distance Covariance (CDC)**: Extension of DC that measures conditional independence given another variable. *Why needed*: Enables enforcement of Equalized Odds by conditioning on true labels. *Quick check*: Confirm CDC captures conditional independence in controlled scenarios.
- **Lagrangian Dual Optimization**: Mathematical framework for constrained optimization using penalty multipliers. *Why needed*: Enables dynamic adjustment of fairness-accuracy trade-off during training. *Quick check*: Ensure λ update stabilizes during training.

## Architecture Onboarding
- **Component Map**: Input Data -> Model (MLP/ResNet) -> Predictions -> DC/CDC Regularization -> Loss -> Gradients -> Model Update
- **Critical Path**: Forward pass → DC/CDC computation → Loss calculation → Backpropagation → Model weight update → λ update
- **Design Tradeoffs**: Uses O(n²) memory for distance matrices (accuracy vs. memory), dynamic λ update vs. fixed penalty (adaptivity vs. stability).
- **Failure Signatures**: High memory usage with large batches, unstable λ values, slow convergence due to expensive CDC computation.
- **First Experiments**:
  1. Implement and validate empirical DC on a small synthetic dataset.
  2. Test DC regularization on tabular data with fixed λ.
  3. Implement CDC for EO constraint and validate on a simple binary classification task.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the empirical distance covariance framework be effectively extended to regression tasks with continuous sensitive attributes? The paper notes this as essential but provides no experimental validation beyond classification.
- **Open Question 2**: What is the minimum batch size required to ensure the empirical (conditional) distance covariance is a sufficiently accurate proxy for the population statistic during mini-batch training? The paper provides convergence bounds but no practical lower bound guidance.
- **Open Question 3**: Does the iterative update rule for the Lagrangian multiplier λ guarantee convergence to a saddle point in the non-convex optimization landscape of deep neural networks? The paper acknowledges this challenge but provides no theoretical convergence guarantees.

## Limitations
- Requires O(n²) memory for distance matrix computation, limiting scalability to large datasets.
- Convergence behavior of the dual variable update in non-convex settings is not theoretically guaranteed.
- All experimental validation is restricted to classification tasks; applicability to regression remains unexplored.

## Confidence
- **High**: The core algorithmic framework (DC/CDC regularization with dual updates) is well-specified and implementable.
- **Medium**: The empirical methodology (averaging over 10 runs, use of tabular and image datasets) is clear, but exact numerical outcomes are uncertain due to missing hyperparameters.
- **Low**: The specific performance metrics (e.g., exact accuracy and fairness gap values) cannot be confidently reproduced without the exact data preprocessing and training hyperparameters.

## Next Checks
1. Reimplement and validate the empirical DC/CDC formulas (Propositions 3 and 4) on a small, controlled dataset to ensure the matrix operations are correct and produce expected independence test results.
2. Run a pilot experiment on tabular data (e.g., Adult dataset) with a simplified MLP, using a fixed random seed and reasonable default hyperparameters for λ and β, to test the training stability and verify the fairness-accuracy trade-off curve.
3. Conduct a sensitivity analysis on the dual variable update (λ update rule) by varying β to observe its effect on convergence and fairness constraint satisfaction, comparing against the stated ablation values.