---
ver: rpa2
title: A Systematic Study of Compositional Syntactic Transformer Language Models
arxiv_id: '2506.22978'
source_url: https://arxiv.org/abs/2506.22978
tags:
- slms
- composition
- trees
- language
- syntactic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically studies compositional syntactic language
  models (SLMs), which enhance Transformers by incorporating syntactic biases through
  modeling linearized syntactic parse trees alongside surface sentences. The authors
  propose a unified framework encompassing four key design choices: parse tree binarization,
  linearization methods, composition functions, and sub-constituent masking.'
---

# A Systematic Study of Compositional Syntactic Transformer Language Models

## Quick Facts
- arXiv ID: 2506.22978
- Source URL: https://arxiv.org/abs/2506.22978
- Authors: Yida Zhao; Hao Xve; Xiang Hu; Kewei Tu
- Reference count: 20
- One-line primary result: Compositional syntactic language models with explicit bottom-up composition show significantly improved syntactic generalization and downstream performance while maintaining reasonable language modeling capabilities.

## Executive Summary
This paper presents a systematic study of compositional syntactic language models (SLMs) that enhance Transformers by incorporating syntactic biases through modeling linearized syntactic parse trees alongside surface sentences. The authors propose a unified framework encompassing four key design choices: parse tree binarization, linearization methods, composition functions, and sub-constituent masking. Through comprehensive experiments across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency, they demonstrate that while compositional SLMs may underperform traditional LMs in language modeling, the best variants significantly improve syntactic generalization, summarization, and dialogue performance. The study provides concrete design recommendations including using external composition for efficiency, combining specialized composition functions with binary parse trees, and avoiding sub-constituent masking for most tasks.

## Method Summary
The unified framework represents sentences and their linearized constituency parse trees as joint action sequences, where each action is either a terminal token or a bracketing operation. At each closing-nonterminal action, the model performs explicit bottom-up composition of constituent representations using either internal composition (attention-restricted forward pass) or external composition (separate 4-layer Transformer module). The framework explores 16 variants across four design dimensions: binarization (Bi/Nb), linearization (Dn/Up), composition function (In/Ex), and sub-constituent masking (M/Nm). Training uses GPT-2 small backbone with 300-sample tree marginalization for language modeling, syntactic generalization test suites for structural evaluation, and finetuning on Xsum and DailyDialog for downstream tasks.

## Key Results
- Compositional SLMs achieve significantly improved syntactic generalization scores (up to 82.1 SG) compared to GPT-2 token baseline (78.0 SG) and GPT-2 tree baseline (78.3 SG)
- Top-performing variants demonstrate improved downstream performance on summarization (ROUGE-1 +1.2) and dialogue tasks while maintaining reasonable language modeling perplexity
- External composition with binary parse trees provides optimal efficiency-performance tradeoff, achieving 80% of internal composition's syntactic generalization with 50% inference time reduction
- Sub-constituent masking benefits syntactic modeling when composition is effective but harms generation tasks due to information bottleneck

## Why This Works (Mechanism)

### Mechanism 1: Explicit Composition Improves Syntactic Generalization
Bottom-up composition of constituent representations enhances syntactic generalization capabilities by forcing the model to learn hierarchical structure relationships explicitly rather than relying solely on sequential patterns. This works because syntactic competence requires representing hierarchical constituent structure, not just surface sequential patterns.

### Mechanism 2: Binary Trees Align External Composition Capacity with Task Demands
Binary parse trees enable small external composition modules to match or exceed internal composition performance by constraining each composition to exactly two sub-constituents. This fixed arity allows compact external Transformers to learn effective binary composition functions, avoiding the variable-arity challenge that degrades non-binary external composition.

### Mechanism 3: Sub-Constituent Masking Creates Task-Dependent Information Bottleneck
Masking composed sub-constituents creates a syntactic bottleneck beneficial for syntax-focused tasks but harmful for generation tasks by preventing subsequent tokens from attending to sub-constituent representations after composition. This pressures composition quality but removes granular context useful for surface generation.

## Foundational Learning

- Concept: Constituency Parse Trees and Binarization
  - Why needed here: The entire framework depends on understanding how unlabeled constituency trees represent phrase structure and how Chomsky normal form binarization transforms N-ary trees to binary while preserving coverage
  - Quick check question: Given "(S (NP John) (VP (V ate) (NP (Det the) (N apple))))", what is the left-binarized form?

- Concept: Autoregressive Action Sequence Generation
  - Why needed here: SLMs generate joint (sentence, tree) pairs via action sequences p(a) = ∏p(aᵢ|a<ᵢ), where actions include terminals and bracketing operations. Understanding this factorization is essential for training and inference
  - Quick check question: For bottom-up linearization of "Write an essay quickly", what action sequence generates both the sentence and its parse?

- Concept: Attention Masking for Structural Induction
  - Why needed here: Both internal composition (restricting attention to sub-constituents during composition) and sub-constituent masking (hiding composed elements from future tokens) manipulate attention patterns to enforce structural constraints
  - Quick check question: Why does internal composition require a duplicate ")" token with full-context attention for next-action prediction?

## Architecture Onboarding

- Component map:
  Main Transformer (GPT-2 backbone, 12 layers, 768-dim) -> Linearization Layer -> External Composition Module (optional, 4 layers, 256-dim) -> Attention Masking System -> Start Position Predictor (Nb-Up only)

- Critical path:
  1. Input sentence → CRF parser → silver constituency tree
  2. Tree binarization (Bi) or keep non-binary (Nb)
  3. Linearize: Dn produces "(" + token + ")" sequence; Up produces token + ")" sequence
  4. Tokenize actions, embed, feed to Transformer
  5. At each ")" prediction: compose sub-constituents via internal (attention-restricted forward pass) or external (separate module) function
  6. Optionally mask composed sub-constituents (M) or retain access (Nm)
  7. Inference: word-synchronous beam search with nc (max nonterminals) and pc (max consecutive "(") constraints

- Design tradeoffs:
  - Internal (In) vs External (Ex) composition: In simplifies implementation, enables parallel training, but slower inference (2×+ forward calls); Ex adds module complexity but faster inference (1.28s vs 4.28s at bsz-10 for Bi-Up)
  - Binary (Bi) vs Non-binary (Nb) trees: Bi restricts expressiveness but aligns with fixed-capacity external composition; Nb preserves linguistics but requires variable-arity handling (Nb-Up-Ex-# fails catastrophically)
  - Masking (M) vs No mask (Nm): M may improve syntactic modeling when composition works (Bi-#-#-M > Bi-#-#-Nm on SG for some configs); Nm consistently better for generation tasks (R-L: 23.01 vs 18.84 for Bi-Up-Ex on Xsum)

- Failure signatures:
  - SG scores ~40-52 (vs 78-82 baseline): Nb-#-Ex-# configurations—external module underfits variable-arity composition
  - Perplexity >25 (vs ~20): #-#-#-M configurations—information bottleneck too restrictive for language modeling
  - Inference time >7s at bsz-300: Nb-Up-#-# configurations—word-synchronous beam search synchronization overhead from variable composition counts
  - Excessive consecutive "(": #-Dn-#-# without pc constraint—structure generation entropy too low

- First 3 experiments:
  1. **Validate composition necessity**: Compare GPT2-tree (no explicit composition) vs Bi-Up-Ex-Nm on SG test suites. Expect ~7-10 point SG improvement confirming composition benefits.
  2. **Identify optimal efficiency-performance point**: Benchmark Bi-Up-Ex-Nm vs Bi-Up-In-Nm across inference beam sizes (10, 30, 100, 300) measuring time-per-token and SG. Expect Ex to maintain 80% SG with 50% inference time reduction.
  3. **Characterize masking effects**: Run Bi-Up-Ex-Nm vs Bi-Up-Ex-M on Xsum (summarization) and SG (syntax). Expect Nm to win summarization (R-L +4) but M to potentially edge SG (+2-3 points if composition is effective).

## Open Questions the Paper Calls Out

### Open Question 1
Can an explicit learning target or architectural redesign resolve the suboptimal performance of compositional SLMs when modeling non-binary constituency trees? The authors state that current composition functions underperform on non-binary trees due to "the simplicity of their architecture and the absence of an explicit learning target to guide the composition process beyond the language modeling loss." This remains unresolved because the current framework relies solely on the language modeling loss to supervise composition, which appears insufficient for capturing the complex interactions in non-binary structures.

### Open Question 2
Do the observed benefits of compositional SLMs over standard Transformers persist when scaling to larger corpora and more advanced model backbones? The paper notes in the Limitations section that the framework is "tested on a relatively small corpus using a GPT-2 backbone due to limited computational resources" and suggests future research should explore "larger corpora, and more advanced Transformer backbones." This is unresolved because improved syntactic generalization and downstream performance are verified only on a GPT-2 scale; it is unknown if larger standard LLMs simply "learn" these syntactic biases implicitly.

### Open Question 3
How can the specific attention masking patterns of compositional SLMs be adapted to utilize modern hardware optimizations like Flash-Attention? The authors highlight that "most compositional SLMs are unable to readily leverage recent advancements in Transformer efficiency, such as Flash-Attention... due to their specific attention patterns." This remains unresolved because custom attention masks required for internal composition and sub-constituent masking prevent use of optimized standard kernels, creating an efficiency bottleneck despite potential of external composition functions.

## Limitations

- The study is limited to a relatively small corpus (BLLIP-LG) using a GPT-2 backbone, with potential scaling effects on compositional benefits untested
- Critical implementation details remain unspecified, including exact external composition module architecture, batch sizes per variant, and optimizer configurations
- The framework cannot readily leverage modern Transformer efficiency optimizations like Flash-Attention due to custom attention masking patterns

## Confidence

**High Confidence**: The core finding that explicit composition improves syntactic generalization while harming language modeling is well-supported across multiple configurations. The Bi-Up-Ex-Nm variant consistently achieving top-5 syntactic generalization scores provides robust evidence for this claim.

**Medium Confidence**: Design recommendations regarding optimal combinations (external composition + binary trees + specialized composition functions) are supported but depend on specific implementation details not fully specified. The sub-constituent masking effects show clear patterns in the data but the magnitude of benefits varies across tasks in ways that suggest implementation sensitivity.

**Low Confidence**: The claim that specific composition function variants (e.g., In-#-1-1) are optimal requires the most trust in unreported architectural details. The paper identifies patterns but doesn't systematically compare all composition function variants.

## Next Checks

1. **External Composition Architecture Verification**: Implement the Transformer-based external composition module with multiple architectural configurations (varying hidden sizes, attention heads, activation functions) and measure how performance varies across Bi-#-Ex-# variants.

2. **Batch Size Sensitivity Analysis**: Systematically vary batch sizes across the 16 SLM variants while holding other hyperparameters constant, measuring impacts on syntactic generalization scores and language modeling perplexity.

3. **Word-Synchronous Beam Search Fidelity Test**: Implement the word-synchronous beam search with beam sizes {10, 30, 100, 300} and verify that consecutive "(" generation remains bounded (pc=3-5) and that token-length grouping produces correct synchronization across variable composition counts.