---
ver: rpa2
title: 'Scaling Down Semantic Leakage: Investigating Associative Bias in Smaller Language
  Models'
arxiv_id: '2501.06638'
source_url: https://arxiv.org/abs/2501.06638
tags:
- qwen2
- leakage
- b-instruct
- prompts
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether smaller language models exhibit\
  \ less semantic leakage\u2014a phenomenon where learned associations from training\
  \ data unexpectedly influence model outputs\u2014compared to larger models. Building\
  \ on prior work, the study evaluates Qwen2.5 models ranging from 500M to 7B parameters\
  \ using both an existing dataset and a new color-focused prompt set."
---

# Scaling Down Semantic Leakage: Investigating Associative Bias in Smaller Language Models

## Quick Facts
- arXiv ID: 2501.06638
- Source URL: https://arxiv.org/abs/2501.06638
- Authors: Veronika Smilga
- Reference count: 2
- Primary result: Smaller language models exhibit less semantic leakage, though the relationship is not strictly linear with model size

## Executive Summary
This study investigates whether smaller language models exhibit less semantic leakage compared to larger models. Building on prior work, the study evaluates Qwen2.5 models ranging from 500M to 7B parameters using both an existing dataset and a new color-focused prompt set. Results show that smaller models do exhibit less semantic leakage overall, though the trend is not strictly linear, with medium-sized models sometimes surpassing larger ones. The study also finds that models tend to leak more for prompts requiring non-color-related outputs rather than color-to-color mappings. While semantic leakage can enhance lexical diversity, it may also introduce bias, highlighting the need for further research on detection and mitigation strategies.

## Method Summary
The study evaluates semantic leakage using Qwen2.5-Instruct models (0.5B, 1.5B, 3B, 7B-GPTQ-Int4) on two datasets: a 109-prompt dataset from Gonen et al. (2024) and a new color-focused dataset with 720 prompts. Generation uses temperature=0.5, max_tokens=10, and truncation at sentence-ending punctuation. Leak-Rate is calculated by comparing BERT-score and SentenceBERT similarities between concept-to-test and concept-to-control generations, with scores of 100, 50, or 0 based on which similarity is higher, then averaged across prompts.

## Key Results
- Smaller models (0.5B-1.5B) consistently show less semantic leakage than larger models
- The relationship between model size and leakage is not strictly linear; the 3B model sometimes shows more leakage than 7B
- Models leak more on prompts requiring non-color-related outputs versus color-to-color mappings
- The 0.5B model's low leak-rate likely results from generating repetitive outputs regardless of context

## Why This Works (Mechanism)

### Mechanism 1: Capacity-Constrained Association Learning
Smaller models exhibit less semantic leakage because limited parameter capacity constrains complex association learning. Model capacity determines the richness of learnable training associations; smaller capacity → fewer internalized associations → fewer opportunities for those associations to surface unexpectedly in generation. This assumes semantic leakage requires models to have sufficiently encoded complex training associations that can be triggered by prompt concepts.

### Mechanism 2: Repetitive Generation Masking
The 0.5B model's low leak-rate reflects context-insensitive generation, not genuine leakage reduction. Very small models generate repetitive outputs regardless of prompt context; without diverse context-responsive generation, there are fewer opportunities for prompt concepts to influence output. This assumes low leak-rate from repetition is a measurement artifact, not evidence of better leakage control.

### Mechanism 3: Echo-Based Leakage Inflation
The 3B model's elevated leak-rate stems from a specific repetition tendency rather than richer associative capacity. The 3B model learned a pattern of echoing prompt concepts directly into generation; similarity metrics count this as leakage even when repetition is contextually appropriate. This assumes high leak-rate from repetition is measurement noise, not genuine semantic association surfacing.

## Foundational Learning

- **Concept: Semantic Leakage**
  - Why needed: Understanding what counts as leakage vs. normal coherence is essential for interpreting results
  - Quick check: Given "Ivory is a student. Her favorite color is... white," can you explain why this counts as leakage?

- **Concept: Mean Leak-Rate Computation**
  - Why needed: The evaluation methodology determines what conclusions can be drawn from the data
  - Quick check: If sim(concept, test) > sim(concept, control), what Leak-Rate score does that prompt receive?

- **Concept: Capacity-Association Tradeoff**
  - Why needed: Understanding that association-learning capacity is double-edged—beneficial for coherence, problematic for leakage
  - Quick check: Why might reducing model capacity reduce leakage while also reducing output quality?

## Architecture Onboarding

- **Component map:**
  Qwen2.5-Instruct models (0.5B, 1.5B, 3B, 7B-GPTQ-Int4) -> BERT-score (distilbert-base-uncased) + SentenceBERT (all-MiniLM-L6-v2) -> Mean Leak-Rate calculation

- **Critical path:**
  1. Load Qwen2.5-Instruct model from HuggingFace
  2. Prepare test prompts with leakage-triggering concepts + control prompts without
  3. Generate with temperature=0.5, max_tokens=10, truncate at sentence-ending punctuation
  4. Compute similarity scores using both BERT-score and SentenceBERT
  5. Calculate Mean Leak-Rate: average of per-prompt scores (100/0/50 based on similarity comparison)

- **Design tradeoffs:**
  - Temperature 0.5: balances diversity vs. high-probability sequence preservation
  - BERT-score vs SentenceBERT: may capture different similarity aspects; paper uses both
  - GPTQ-Int4 quantization on 7B: necessary for P100 GPU but may introduce quantization artifacts (not analyzed)

- **Failure signatures:**
  - 0.5B model: Identical outputs across varied prompts → low leakage is context-insensitivity
  - 3B model: Excessive prompt repetition in outputs → inflated leak-rate
  - 7B-GPTQ: Potential quantization effects uncharacterized

- **First 3 experiments:**
  1. Repetition-filtered evaluation: Exclude generations that directly echo prompt concepts to isolate genuine associative leakage from echo artifacts
  2. Cross-family replication: Run the same color dataset on Llama-3.2 or Mistral families to test whether size-leakage patterns generalize beyond Qwen
  3. Intermediate size probing: Test whether 2B or 4B models (if available) show linear interpolation between 1.5B and 7B, or whether the 3B peak persists

## Open Questions the Paper Calls Out

### Open Question 1
Does the trend of increased semantic leakage with model size continue for models larger than 7B parameters (14B, 32B, 72B)? Authors state: "one may want to test larger models of the same family, Qwen2.5-14B-Instruct, Qwen2.5-32B-Instruct, and Qwen2.5-72B-Instruct, to determine whether the tendency of larger models to leak more holds for 14B+ models." The study only tested models up to 7B parameters, leaving larger models unexplored.

### Open Question 2
What explains the non-linear relationship between model size and semantic leakage, where the 3B model sometimes exhibits more leakage than the 7B model? The 3B model showed the highest BERT-score-based Mean Leak-Rate (83.03) on the original dataset, and the authors note this tendency is "hard to explain" despite using identical hyperparameters across all models.

### Open Question 3
How can the evaluation methodology distinguish between legitimate contextually appropriate concept references and actual undesirable semantic leakage? The authors note a limitation: "In some cases, the repetition of the concept from the prompt in the generation is justified as it is used to refer to a previously mentioned concept... However, generations like that... get classified as instances of semantic leakage, unjustly increasing the Mean Leak-Rate."

### Open Question 4
What methods can effectively detect and mitigate undesirable semantic leakage in language models? The final sentence of future work states: "it may be interesting to investigate the ways of detecting and mitigating undesirable leaking behavior in language models." The paper focuses on measurement and characterization rather than mitigation strategies.

## Limitations

- Reliance on a single model family (Qwen2.5) limits generalizability to other architectures
- The 7B model's use of GPTQ-Int4 quantization introduces uncharacterized artifacts that may influence leakage patterns
- Similarity-based metrics may conflate contextually appropriate repetition with genuine semantic leakage

## Confidence

- **High confidence**: The finding that smaller models exhibit less semantic leakage overall (0.5B model consistently shows lowest rates)
- **Medium confidence**: The observation that leakage trends are not strictly linear with model size, particularly the 3B model's elevated leak-rate due to repetition patterns
- **Medium confidence**: The distinction between color-to-color versus color-to-non-color leakage patterns, though this requires careful interpretation given the repetition artifacts

## Next Checks

1. **Cross-family replication**: Evaluate the same color dataset on Llama-3.2 or Mistral models to determine whether the size-leakage relationship generalizes beyond Qwen2.5.

2. **Repetition-filtered analysis**: Implement a post-processing step to exclude generations that directly echo prompt concepts, isolating genuine associative leakage from echo artifacts.

3. **Intermediate size probing**: Test 2B and 4B parameter models (if available) to determine whether the 3B peak represents a genuine local maximum or an artifact of the Qwen2.5 family's specific training dynamics.