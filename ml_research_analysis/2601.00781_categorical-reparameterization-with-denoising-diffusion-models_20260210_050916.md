---
ver: rpa2
title: Categorical Reparameterization with Denoising Diffusion models
arxiv_id: '2601.00781'
source_url: https://arxiv.org/abs/2601.00781
tags:
- diffusion
- categorical
- distribution
- where
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces REDGE, a diffusion-based soft reparameterization
  for categorical distributions that leverages the fact that, for categorical targets,
  the denoising distribution is discrete and its denoiser can be computed in closed
  form. By connecting this denoiser to a continuous relaxation, REDGE defines a differentiable
  sampling map from Gaussian noise to the target distribution without requiring training
  of a denoiser network.
---

# Categorical Reparameterization with Denoising Diffusion models

## Quick Facts
- arXiv ID: 2601.00781
- Source URL: https://arxiv.org/abs/2601.00781
- Reference count: 40
- Primary result: Introduces REDGE, a training-free differentiable sampling method for categorical distributions using diffusion models

## Executive Summary
This paper presents REDGE (Reparameterization via Diffusion for Gradient Estimation), a novel approach for differentiable sampling from categorical distributions. The method leverages the closed-form denoiser of categorical distributions within a diffusion framework, eliminating the need to train a denoiser network. By connecting this denoiser to continuous relaxations, REDGE provides a differentiable sampling map from Gaussian noise to categorical samples. The method recovers STRAIGHT-THROUGH and REINMAX estimators as special cases while offering improved gradient quality and robustness.

## Method Summary
REDGE constructs a deterministic sampling map from Gaussian noise to categorical samples using the DDIM framework. For categorical distributions, the denoiser has a closed-form expression as a softmax of perturbed logits. This enables training-free differentiable sampling by backpropagating through the deterministic DDIM reverse process. The method introduces a time cutoff parameter $t_1$ that controls the trade-off between bias and gradient quality, recovering STRAIGHT-THROUGH for large $t_1$ and REINMAX for appropriate settings. The approach is theoretically grounded with analysis of gradient vanishing in the small-noise regime.

## Key Results
- REDGE matches or outperforms existing gradient estimators across Sudoku solving, variational inference, and reward-guided image generation benchmarks
- The method shows improved robustness and less hyperparameter sensitivity compared to alternatives like Gumbel-Softmax and REINMAX
- Theoretical analysis explains the emergence of uninformative gradients and informs hyperparameter choices
- REDGE recovers STRAIGHT-THROUGH and REINMAX estimators as one-step special cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** REDGE enables training-free differentiable sampling for categorical distributions by exploiting the closed-form structure of the categorical denoiser.
- **Mechanism:** For categorical distributions on simplex vertices, the posterior-mean denoiser $\hat{x}_0(x_t, t)$ is a matrix of posterior probabilities. The paper derives that this denoiser reduces to a softmax function of the logits perturbed by the noisy sample (Eq. 11: $\text{softmax}(\phi_\theta + \alpha_t x_t / \sigma_t^2)$). Because this map is explicit, the reverse diffusion process (DDIM) becomes differentiable without training a neural network to approximate the denoiser.
- **Core assumption:** The target distribution $\pi_\theta$ factorizes into independent categorical components (Eq. 2), allowing the denoiser to be computed per dimension.
- **Evidence anchors:**
  - [Section 3.2] "...the denoiser $\hat{x}^\theta_0(x_t, t)$ has a closed-form expression due to the factorized categorical structure."
  - [Abstract] "...denoiser can be computed in closed form... yielding a training-free diffusion sampler through which we can backpropagate."
  - [Corpus] Weak direct support; related work (MissHDD) addresses heterogeneous data but relies on learned models, highlighting the uniqueness of the analytic solution here.
- **Break condition:** If the target distribution $\pi_\theta$ has strong correlations between dimensions violating the factorization assumption, the closed-form denoiser formula (Eq. 11) theoretically no longer holds exactly, though the paper notes the mean-field assumption is standard.

### Mechanism 2
- **Claim:** REDGE recovers existing estimators (STRAIGHT-THROUGH, REINMAX) as limiting cases while providing a continuum of estimators controlled by the number of diffusion steps ($n$) and the time cutoff ($t_1$).
- **Mechanism:** The method constructs a sampling map $T^\theta_0$ from noise to the target. If using a single diffusion step ($t_1=1$), the sample reduces to $\mathbb{E}_{\pi_\theta}[X]$, recovering the STRAIGHT-THROUGH estimator. By extending the diffusion chain ($n > 1$), the method interpolates toward the true categorical sampling path, reducing bias compared to a single step but potentially increasing variance.
- **Core assumption:** The loss function $f$ is differentiable with respect to the continuous relaxation of the categorical variables.
- **Evidence anchors:**
  - [Section 3.2] "...with a single diffusion step... we recover the STRAIGHT-THROUGH estimator... as a special case."
  - [Section 3.3] "REINDGE... recovers REINMAX as a special case."
  - [Corpus] Corpus papers on Discrete VAEs (arXiv:2509.24716) similarly struggle with the bias-variance trade-off in discrete reparameterization, validating the problem setup.
- **Break condition:** If the downstream objective $f$ is discontinuous or non-differentiable in the relaxed space, the pathwise gradient estimator loses validity.

### Mechanism 3
- **Claim:** The "temperature" parameter $t_1$ (cutoff time) governs a trade-off between gradient vanishing and bias, where excessively small $t_1$ causes gradients to vanish almost surely.
- **Mechanism:** Proposition 1 proves that as $t_1 \to 0$, the Jacobian $\|J_\theta T^\theta_0(X_1)\| \to 0$ almost surely. This occurs because the denoising distribution collapses sharply to a one-hot vector, creating near-constant transport regions and sharp decision boundaries where gradients are uninformative.
- **Core assumption:** The schedule $\alpha_t / \sigma_t^2$ diverges as $t \to 0$ (Assumption A1).
- **Evidence anchors:**
  - [Section 3.2] Proposition 1 and the subsequent analysis.
  - [Section A.1] Formal proof of the vanishing gradient bound involving the margin function.
  - [Corpus] Not explicitly addressed in corpus neighbors.
- **Break condition:** Setting $t_1$ too low (e.g., $< 0.3$) in pursuit of lower bias will empirically degrade performance due to gradient signal loss, as shown in Figure 4 and 8.

## Foundational Learning

- **Concept:** **The Reparameterization Trick**
  - **Why needed here:** REDGE is fundamentally a proposal for reparameterizing discrete variables. Understanding that gradients $\nabla_\theta \mathbb{E}[f(X)]$ normally require moving $\nabla_\theta$ inside the expectation (which is blocked by discreteness) is necessary to grasp the value of the "training-free" claim.
  - **Quick check question:** Why can we not simply backpropagate through a sample drawn from a standard Categorical distribution?

- **Concept:** **Denoising Diffusion Implicit Models (DDIM)**
  - **Why needed here:** The method uses the DDIM framework to define a deterministic map from noise $X_1$ to data $X_0$. You must understand that DDIM provides a non-Markovian reverse process that is deterministic given a denoiser, which allows for the "soft reparameterization."
  - **Quick check question:** How does the deterministic nature of the DDIM sampler (when $\eta=0$) enable the backpropagation required for REDGE?

- **Concept:** **Continuous Relaxations (e.g., Gumbel-Softmax)**
  - **Why needed here:** REDGE is benchmarked against these methods. Understanding that Gumbel-Softmax introduces a temperature parameter to smooth the discrete distribution helps contextualize REDGE's use of $t_1$ as a similar hyperparameter.
  - **Quick check question:** In Gumbel-Softmax, what happens to the gradient variance as the temperature $\tau \to 0$?

## Architecture Onboarding

- **Component map:** Inputs (logits $\phi_\theta$, Gaussian Noise $X_1$) -> Core Loop (DDIM Steps with analytic denoiser) -> Denoiser (softmax of perturbed logits) -> Hard/Soft Split (hard sample for forward pass, relaxed proxy for backward gradient)

- **Critical path:** The implementation hinges on **Algorithm 1**. Specifically, the efficiency comes from the fact that the loop (lines 3-7) is shallow (typically $n=3$ to $9$ steps). The Jacobian is not calculated analytically in code but rather via automatic differentiation through this loop.

- **Design tradeoffs:**
  - **Steps ($n$) vs. Accuracy:** Increasing steps $n$ improves sample fidelity (closer to $\pi_\theta$) but increases memory/compute. The paper finds $n=3$ or $5$ is often sufficient (Appendix E.1).
  - **Time Cutoff ($t_1$):** High $t_1$ (e.g., 0.9) behaves like Straight-Through (fast, potentially biased). Low $t_1$ (<0.3) approaches the true categorical sample but risks gradient vanishing (Prop 1).

- **Failure signatures:**
  - **Vanishing Gradients:** If $t_1$ is set too small, gradient norms approach zero (Figure 8). The model trains very slowly or not at all.
  - **Objective Mismatch:** In Variational Inference (VAEs), if $n$ is too small, the sampling distribution differs from the model distribution used in the KL term, decoupling the loss (Tables 1-4).
  - **High Variance:** If the reward function is extremely sparse, the diffusion path might not provide informative gradients (though REDGE generally outperforms REINFORCE here).

- **First 3 experiments:**
  1. **Gradient Magnitude Check:** Plot $\| \nabla_\theta L \|$ vs. $t_1$ on a toy optimization task (like the polynomial benchmark in Appendix F) to verify the vanishing gradient regime predicted by Prop 1.
  2. **Sample Fidelity Test:** Compare the empirical distribution of samples from REDGE (with varying $n$) against the true $\pi_\theta$ using the Wasserstein distance (following Figure 7) to determine the minimum steps $n$ needed for your specific $L$ and $K$.
  3. **Guidance Integration:** Implement the "Direct Optimization" (Sudoku without MDM) from Section 4.1.1 to test if the gradient estimator can enforce hard constraints better than STRAIGHT-THROUGH.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis assumes mean-field factorization and does not account for potential smoothing effects from finite batch sizes or adaptive optimizers
- Empirical study does not explore very high-dimensional categorical spaces (e.g., >100 categories) or structured dependencies beyond the Sudoku benchmark
- One-step REDGE estimator may still suffer from high variance in highly multimodal target distributions

## Confidence
- **High Confidence:** The closed-form denoiser derivation and its connection to existing estimators are mathematically sound and empirically validated. The theoretical analysis of gradient vanishing is rigorously proven.
- **Medium Confidence:** The practical recommendation of $n=3$ to $5$ steps is well-supported within the tested domains but may not generalize to all categorical reparameterization tasks without tuning.
- **Low Confidence:** The paper does not provide theoretical or empirical analysis of REDGE's performance in the presence of strong categorical correlations that violate the mean-field assumption.

## Next Checks
1. **Correlation Stress Test:** Evaluate REDGE on a synthetic categorical distribution with explicitly introduced pairwise correlations between dimensions. Measure the degradation in sample fidelity and gradient quality compared to the independent case.

2. **High-Dimensional Scalability:** Apply REDGE to a task with categorical variables having 50-100 categories each (e.g., large vocabulary language modeling). Track memory usage, runtime, and performance relative to Gumbel-Softmax as dimension increases.

3. **Multimodal Distribution Analysis:** Construct a target distribution $\pi_\theta$ with two well-separated modes. Visualize the DDIM sampling paths and measure the probability of the sampler switching between modes for different $t_1$ values.