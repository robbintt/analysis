---
ver: rpa2
title: 'MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality'
arxiv_id: '2502.14509'
source_url: https://arxiv.org/abs/2502.14509
tags:
- language
- translation
- directions
- languages
- slavic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multilingual neural machine translation (NMT)
  for Slavic languages, testing whether extending the data regime through cross-lingual
  knowledge transfer improves performance. The authors trained several model variants,
  including bi-directional, pivot-based, and many-to-many multilingual models, on
  parallel data for Czech, Polish, Slovak, Slovene, and English.
---

# MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality

## Quick Facts
- arXiv ID: 2502.14509
- Source URL: https://arxiv.org/abs/2502.14509
- Reference count: 19
- Primary result: Multilingual models trained on Slavic languages achieve superior translation quality, including zero-shot directions, without the curse of multilinguality

## Executive Summary
This paper evaluates multilingual neural machine translation for Slavic languages, demonstrating that cross-lingual knowledge transfer improves translation quality even for low-resource language pairs and zero-shot scenarios. The authors train several model variants—bi-directional, pivot-based, and many-to-many multilingual models—on parallel data for Czech, Polish, Slovak, Slovene, and English. Their MultiSlav+ENG model achieves an average COMET score of 89.2 across all 20 translation directions, outperforming baselines while confirming that adding related languages enhances rather than degrades performance.

## Method Summary
The study uses transformer-based multilingual NMT with 6-layer encoder-decoder architecture (1024 model dimension, 4096 FFN, 16 attention heads) and three-way tied embeddings. Models are trained using MarianNMT with target language tokens prepended to source sentences. Tokenization uses SentencePiece unigram models with language-balanced sampling (16k tokens per language). The training corpus includes 578M sentences from multiple parallel datasets, filtered using language identification, length ratios, and character whitelists. Evaluation uses the Flores-101 dev set with COMET as the primary metric.

## Key Results
- Multilingual models outperform bi-directional baselines even in zero-shot translation scenarios
- Adding English to Slavic language set increased overall translation quality without causing the curse of multilinguality
- MultiSlav+ENG achieved the best average COMET score of 89.2 across all 20 translation directions
- Zero-shot ablation experiments confirmed effective cross-lingual transfer between closely related languages

## Why This Works (Mechanism)

### Mechanism 1
Training multilingual NMT models on closely related languages from the same language group enables cross-lingual knowledge transfer, improving quality on low-resource directions even in zero-shot settings. Shared linguistic features between related languages create overlapping representations in the shared embedding space, allowing patterns learned from one direction to generalize to unseen pairs.

### Mechanism 2
Adding a high-resource pivot language (English) to a related-language multilingual model increases total training signal without causing capacity saturation or quality degradation. The additional signal is complementary rather than competitive for model capacity because English uses the same Latin script and shares vocabulary with Slavic languages through loanwords.

### Mechanism 3
Directional zero-shot translation emerges when a model learns source-language comprehension from one translation direction and target-language fluency from another, with the shared representation enabling composition. The encoder and decoder learn separable source-understanding and target-generation capabilities that can be recombined across training directions.

## Foundational Learning

- **Curse of Multilinguality vs. Cross-lingual Transfer**: Understanding the tension between adding languages (which could improve quality through transfer) versus causing degradation (through capacity constraints or negative interference) is central to evaluating multilingual NMT systems.
  - *Why needed*: The paper positions itself as testing whether adding languages improves or degrades quality—a central tension in multilingual NMT.
  - *Quick check*: When you add a new language pair to a multilingual model, what two opposing effects could occur, and what determines which dominates?

- **Tokenizer vocabulary sizing for multilingual models**: Proper vocabulary sizing ensures morphologically rich languages receive adequate representation without overwhelming the model capacity.
  - *Why needed*: The paper uses 16k tokens per language as a heuristic; understanding why this matters is critical for reproducing or extending the work.
  - *Quick check*: Why would a tokenizer with too small vocabulary hurt morphologically rich languages like Czech or Polish more than English?

- **Encoder-Decoder Transformer with three-way weight tying**: This architectural choice affects how cross-lingual representations form and how information flows between languages.
  - *Why needed*: The architecture uses shared embeddings for source, target, and output layers—this design choice affects how cross-lingual representations form.
  - *Quick check*: What is the potential benefit of tying embeddings across languages in a multilingual translation model, and what constraint does this impose?

## Architecture Onboarding

- **Component map**: Data filtering -> Tokenizer training -> Model training with language tokens -> Evaluation on FLORES-101
- **Critical path**: Data filtering (language ID, length ratios, character whitelists) → clean bitext → Tokenizer training on 40M equal-sampled sentences → shared vocabulary → Model training with language tokens → many2many translation capability → Evaluation on FLORES-101 dev set → COMET/chrF metrics
- **Design tradeoffs**: Equal vs. proportional sampling (equal prevents English dominance but may underutilize high-resource signal); Target-only vs. source+target language tokens (no performance difference found; target-only chosen for simplicity); Pivot vs. direct many2many (pivot increases bridge-language fluency but loses direct bitext signal)
- **Failure signatures**: Pivot model outperformed on bridge-language directions but degraded on non-bridge pairs; Training instability with 10-layer models (abandoned in favor of 6-layer); LLM refusal to translate controversial content (0.12-0.55% of examples)
- **First 3 experiments**: Baseline comparison (train bi-directional models for each language pair); Zero-shot ablation (train MultiSlav excluding specific direction, then evaluate on held-out direction); English addition test (compare MultiSlav vs. MultiSlav+ENG on Slavic-only directions)

## Open Questions the Paper Calls Out

### Open Question 1
Does cross-lingual knowledge transfer manifest effectively across different scripts within the same language family (e.g., adding Cyrillic-script languages to Latin-script Slavic models)? The authors explicitly state that extending the model to include Cyrillic Slavic languages is necessary to determine if knowledge transfer is "inherent to in-language-family scenarios despite different alphabets."

### Open Question 2
Can adding geographically co-located languages from different language families (e.g., Hungarian, Romanian) improve translation quality through cross-lingual transfer, or does it introduce noise? The paper suggests future research on "using geographically co-located languages outside of a single-family."

### Open Question 3
Why does excluding the reverse translation direction sometimes result in superior translation performance for the remaining direction? The authors report an unexpected observation where a model trained without the SLV→SLK direction achieved better performance than the full model, noting this phenomenon requires further investigation.

## Limitations

- The study's conclusions about avoiding the curse of multilinguality are based on a single dataset and small language group (4-5 languages), limiting generalizability to larger multilingual systems.
- Evaluation uses COMET as the primary metric, which may not fully capture translation quality aspects for morphologically rich languages.
- The data filtering pipeline includes thresholds that are referenced but not fully specified in code, creating potential reproducibility gaps.
- The study does not systematically analyze which language pairs benefit most from transfer versus which may experience negative interference.

## Confidence

- **Cross-lingual knowledge transfer improves low-resource translation quality**: High confidence
- **Adding English to Slavic multilingual models does not cause the curse of multilinguality**: Medium confidence
- **Zero-shot translation quality emerges from composition of learned source and target representations**: Medium confidence

## Next Checks

1. **Cross-script transfer validation**: Train and evaluate a multilingual model that includes both Latin-script Slavic languages and Cyrillic-script Slavic languages to test whether cross-lingual transfer benefits extend beyond same-script language groups.

2. **Capacity scaling study**: Systematically vary model capacity while training on the same language sets to identify the threshold where the curse of multilinguality emerges.

3. **Negative transfer analysis**: Conduct a detailed error analysis comparing baseline and multilingual model outputs to identify specific translation errors that worsen in the multilingual setting.