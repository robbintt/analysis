---
ver: rpa2
title: 'Not All Explanations are Created Equal: Investigating the Pitfalls of Current
  XAI Evaluation'
arxiv_id: '2511.03730'
source_url: https://arxiv.org/abs/2511.03730
tags:
- explanations
- user
- users
- explanation
- placebic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether any explanation\u2014regardless\
  \ of quality\u2014improves user satisfaction compared to no explanation, a flaw\
  \ in current XAI evaluation methods. The authors conduct a between-subjects experiment\
  \ with 108 participants using chess puzzles to test three conditions: no explanation,\
  \ placebic (superficial) explanations, and actionable (informative) explanations."
---

# Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation

## Quick Facts
- arXiv ID: 2511.03730
- Source URL: https://arxiv.org/abs/2511.03730
- Authors: Joe Shymanski; Jacob Brue; Sandip Sen
- Reference count: 24
- Primary result: User satisfaction alone is an inadequate metric for evaluating explanation quality

## Executive Summary
This study reveals a critical flaw in current XAI evaluation methods: any explanation—regardless of quality—improves user satisfaction compared to no explanation. Through a controlled between-subjects experiment with 108 participants solving chess puzzles, the researchers found that while placebic and actionable explanations yielded similar satisfaction ratings, only actionable explanations significantly improved task performance. This demonstrates that satisfaction-based evaluation fails to capture the true value of explanations and calls for more comprehensive evaluation techniques that combine subjective and objective measures.

## Method Summary
The study employed a between-subjects design with 108 participants recruited via Amazon Mechanical Turk. Participants were randomly assigned to one of three conditions: no explanation, placebic explanations (superficial statements restating obvious information), or actionable explanations (causal reasoning with tactical concepts). All participants completed 10 training puzzles followed by 5 testing puzzles without explanation support. Performance was measured using a geometric mean formula incorporating both moves taken and time elapsed, while subjective satisfaction and explanatory power were captured through 7-point Likert surveys. Data analysis used ANOVA with Tukey's HSD for post-hoc comparisons.

## Key Results
- Placebic and actionable explanations produced similar satisfaction ratings (both around 5.5/7)
- Only actionable explanations significantly improved task performance (+2.2% from practice to testing)
- Placebic explanations led to a 21.3% performance drop from practice to testing
- No significant differences were found in subjective metrics across conditions

## Why This Works (Mechanism)

### Mechanism 1: The Satisfaction-Performance Decoupling
- Claim: User satisfaction ratings do not predict objective learning outcomes from explanations
- Mechanism: Users respond positively to any coherent communication from an agent, interpreting responsiveness as competence regardless of informational content
- Core assumption: Users cannot consciously distinguish explanation quality through self-report when they lack domain expertise
- Evidence: Placebic and actionable explanations yielded similar satisfaction but divergent performance outcomes

### Mechanism 2: The Placebic Explanation Effect
- Claim: Superficial explanations produce equivalent satisfaction to informative explanations but fail to transfer learnable skills
- Mechanism: Placebic explanations activate a social reciprocity response—users perceive communicative effort as trustworthy without providing actionable reasoning
- Core assumption: The agent's willingness to explain signals credibility without requiring logical content validity
- Evidence: Placebic group showed 21.3% performance drop compared to 2.2% gain for actionable group

### Mechanism 3: Actionable Knowledge Transfer via Domain-Specific Reasoning
- Claim: Explanations that explicitly state causal relationships enable users to independently apply learned strategies
- Mechanism: Actionable explanations decompose decisions into transferable components, creating explicit mental models users can retrieve
- Core assumption: Users will encode and retrieve the causal structure when explicitly articulated
- Evidence: Actionable explanations provided new information beyond the fact that a move was best

## Foundational Learning

- **Between-subjects experimental design**: Each participant experiences only one explanation type, preventing individual learning curves from confounding group-level comparisons
  - Quick check: If you see one user's practice score and their testing score, can you infer which explanation type they received? (Answer: No—you need group aggregates)

- **Placebic vs. Actionable Explanations**: Superficial reassurance ("This is good") vs. causal reasoning ("This achieves X by Y")
  - Quick check: An explanation states "The model recommends this loan approval because it meets our criteria." Is this placebic or actionable? (Answer: Placebic—restates the decision without explaining the causal factors)

- **Ceiling Effects in Satisfaction Metrics**: Likert scales may saturate quickly, masking quality differences that performance metrics reveal
  - Quick check: If satisfaction scores for placebic and actionable explanations both cluster around 5.5/7, does this prove equivalent quality? (Answer: No—performance metrics are needed to distinguish)

## Architecture Onboarding

- **Component map**: User authentication -> Demographics collection -> Key information briefing -> Practice section (10 puzzles with explanations) -> Testing section (5 puzzles without explanations) -> Survey completion -> Exit

- **Critical path**: 1) User authentication → Demographics collection → Key information briefing 2) Practice section: 10 puzzles with explanations triggered on errors 3) Testing section: 5 puzzles without explanations 4) Survey completion → Bonus calculation → Exit

- **Design tradeoffs**: Human-generated explanations ensured consistency but sacrificed realism; blur effect standardized exposure time but may have depressed satisfaction scores; geometric mean scoring balanced speed and accuracy but may conflate deliberation with poor performance

- **Failure signatures**: Exponential score distribution with heavy right skew indicates brute-force attempts or non-engagement; filter incomplete, >5 min duration, or >10 mistakes before analysis

- **First 3 experiments**:
  1. Replication with chess experts (rated ≥1800) to test whether expertise enables users to consciously distinguish explanation quality in satisfaction ratings
  2. Add post-puzzle concept identification questions to measure explicit concept learning alongside performance
  3. Within-subjects design with counterbalancing to control for individual aptitude differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the differential impacts of placebic versus actionable explanations generalize to dynamic, real-time collaborative domains?
- Basis: Authors intend to extend research to domains where explanations are key to improving human-agent interactions, such as search-and-rescue teams
- Why unresolved: Current study limited to static, turn-based chess environment
- What evidence would resolve it: Replication in high-pressure, real-time simulation showing similar performance gaps

### Open Question 2
- Question: Does removing interface friction reveal that placebic explanations actually provide higher satisfaction than no explanations?
- Basis: Authors note H1 was unsupported likely because "users assigned the None protocol could make the correct move more quickly"
- Why unresolved: UI design introduced confounding variable where receiving explanations physically delayed users
- What evidence would resolve it: Follow-up study with identical interface timing across all conditions

### Open Question 3
- Question: How does a within-subjects design affect user preference and performance when evaluating explanation types?
- Basis: Authors state "It can be instructive to evaluate a within-subjects study design to see how the same user responds to different explanation types"
- Why unresolved: Between-subjects design prevents understanding how a single user compares utility of different explanation styles
- What evidence would resolve it: Study where participants experience multiple explanation types in sequence with counterbalancing

## Limitations

- Study used a constrained domain (chess puzzles) with pre-generated human explanations rather than real-world AI systems
- 108-participant sample size may miss subtler interaction effects
- Satisfaction ratings clustered tightly around scale midpoint, suggesting potential ceiling effects
- Geometric mean scoring formula may conflate deliberate strategy with poor performance

## Confidence

- **High confidence**: The decoupling between satisfaction and performance metrics is well-supported by data
- **Medium confidence**: Generalizability of placebic explanation effects to other domains requires further validation
- **Medium confidence**: Assumption that users cannot consciously distinguish explanation quality through self-report needs verification across expertise levels

## Next Checks

1. **Expert vs. novice comparison**: Replicate with chess experts (rated ≥1800) to test whether domain expertise enables conscious distinction of explanation quality in satisfaction ratings

2. **Transfer task validation**: Add immediate post-puzzle questions asking users to identify the tactic used (fork/pin) to measure explicit concept learning alongside performance metrics

3. **Real-world deployment**: Test the same explanation framework with a deployed ML system (e.g., loan approval or medical diagnosis) to validate findings beyond the controlled chess domain