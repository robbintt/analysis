---
ver: rpa2
title: 'Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses'
arxiv_id: '2506.21842'
source_url: https://arxiv.org/abs/2506.21842
tags:
- quantum
- circuit
- adversarial
- attacks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper surveys adversarial threats in Quantum Machine Learning
  (QML), identifying vulnerabilities across the QML pipeline including data encoding,
  circuit structure, and training processes. Key attack vectors include model stealing
  via query-based extraction, data poisoning through quantum-specific perturbations,
  reverse engineering of proprietary variational quantum circuits, and backdoor attacks
  exploiting NISQ hardware noise and QML-as-a-Service (QMLaaS) environments.
---

# Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses

## Quick Facts
- **arXiv ID**: 2506.21842
- **Source URL**: https://arxiv.org/abs/2506.21842
- **Reference count**: 26
- **Key outcome**: Comprehensive survey identifying adversarial threats across QML pipeline including model stealing, data poisoning, reverse engineering, and backdoor attacks, with proposed defenses leveraging quantum properties and hardware-aware randomization.

## Executive Summary
This survey systematically examines adversarial threats in Quantum Machine Learning systems, identifying vulnerabilities across the entire QML pipeline from data encoding through training and deployment. The authors categorize attack vectors by access model (black-box, gray-box, white-box) and propose defense mechanisms that exploit quantum-specific properties like hardware noise signatures and secure circuit partitioning. The study highlights how NISQ-era constraints and QML-as-a-Service environments create unique attack surfaces requiring integrated security frameworks that combine classical and quantum techniques.

## Method Summary
The paper surveys multiple distinct attack and defense methods from the literature rather than implementing a single unified approach. Attacks include query-based model extraction using surrogate datasets, quantum-specific data poisoning via perturbations, reverse engineering of variational circuits, and backdoor attacks exploiting hardware noise. Defenses leverage quantum logic locking (E-LoQ), hardware-induced noise signatures for watermarking, secure partitioning across distributed backends (QuMoS), and hardware-aware variance injection (HVIP). Implementation details depend on specific methods chosen from the survey, with requirements including Qiskit, access to IBMQ backends or noise models, and standard ML datasets like MNIST.

## Key Results
- Model stealing attacks can achieve high clone fidelity through systematic query extraction, particularly effective against QMLaaS deployments
- Data poisoning via quantum perturbations can significantly degrade classification accuracy while maintaining stealthy characteristics
- Hardware watermarking using quantum generative adversarial networks provides robust ownership verification against reverse engineering attempts
- Secure partitioning strategies can reduce attack success rates by introducing variance across distributed quantum backends

## Why This Works (Mechanism)
The effectiveness of both attacks and defenses stems from exploiting the fundamental differences between classical and quantum information processing. Attacks leverage the probabilistic nature of quantum measurements and the limited visibility into quantum circuit operations, while defenses use hardware-specific noise signatures and circuit structure obfuscation that are difficult to replicate without physical access. The NISQ-era noise characteristics that typically hinder QML performance can be repurposed as security features through techniques like hardware watermarking and variance injection.

## Foundational Learning

**Quantum Circuit Ansatzes**: Variational circuit structures used as feature maps in QML; needed for implementing PQCs that attackers target or defenders protect. Quick check: Verify circuit depth and entanglement patterns match target QML architecture.

**Hardware Noise Models**: Characterization of gate errors, decoherence, and crosstalk specific to quantum backends; essential for realistic attack/defense simulation. Quick check: Compare simulator noise statistics against baseline backend calibration data.

**Quantum Generative Adversarial Networks**: Hybrid classical-quantum models for generating synthetic quantum states; used in watermarking and data poisoning attacks. Quick check: Validate generated states maintain statistical properties within TVD tolerance of target distribution.

**Measurement Statistics**: Probability distributions from quantum measurements used for classification; exploited in model extraction attacks. Quick check: Ensure shot counts provide sufficient resolution for distinguishing class probabilities.

**Query Budget Management**: Limited number of allowed interactions with target model; constrains attack surface and defense resource allocation. Quick check: Track query counts against theoretical bounds for successful extraction.

## Architecture Onboarding

**Component Map**: Classical preprocessing -> Quantum encoding/ansatz -> Measurement/expectation -> Classical postprocessing -> (Attack/Defense layer) -> Output

**Critical Path**: Data encoding → quantum circuit execution → measurement → classical classification → output decision. Attacks intercept measurement statistics; defenses modify circuit structure or backend selection.

**Design Tradeoffs**: Circuit depth vs. noise resilience, query efficiency vs. stealth, watermark robustness vs. classification accuracy, defense overhead vs. attack success rate.

**Failure Signatures**: High variance in measurement outcomes indicating successful HVIP defense, inconsistent clone model accuracy suggesting over-perturbation, watermark detection failures indicating compromised ownership verification.

**First 3 Experiments**:
1. Implement basic QNN classifier with angle encoding on MNIST (2-4 qubits), train with SPSA, attempt query-based model extraction measuring clone fidelity
2. Deploy HVIP defense routing queries across 2-3 IBMQ backends, measure TVD and classification accuracy degradation under extraction attempts
3. Test qGAN watermarking by training quantum classifier with watermark dataset, measure detection accuracy against reverse engineering under varying noise conditions

## Open Questions the Paper Calls Out

None specified in the provided materials.

## Limitations

- Survey synthesizes methods across multiple independent works, creating uncertainty around implementation specifics and parameter choices
- Effectiveness of defenses depends heavily on noise model accuracy and backend availability, introducing variability not captured in the survey
- Focus on NISQ-era constraints may not fully account for how fault-tolerant quantum architectures could alter attack surfaces and defense efficacy

## Confidence

- **High confidence**: Identification of attack vectors and their categorization by access model
- **Medium confidence**: Proposed defense mechanisms and their theoretical soundness
- **Low confidence**: Specific quantitative effectiveness metrics and success rates across different hardware configurations

## Next Checks

1. Implement QNN classifier with angle encoding on MNIST using 2-4 qubits, train with SPSA optimizer, attempt query-based model extraction to measure clone fidelity under different query budgets
2. Set up HVIP defense by routing classification queries across 2-3 IBMQ backends with hardware-aware variance injection, measuring TVD and classification accuracy degradation for model stealing attempts
3. Test qGAN-based watermarking by training quantum classifier with watermark dataset, measuring detection accuracy against extraction attempts under varying hardware noise and query volume