---
ver: rpa2
title: 'FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program
  Synthesis'
arxiv_id: '2511.06522'
source_url: https://arxiv.org/abs/2511.06522
tags:
- code
- recursion
- depth
- reasoning
- fractal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FractalBench evaluates whether multimodal AI can infer symbolic
  generative rules from fractal images, requiring abstraction of recursive mathematical
  structure from visual patterns. Using 12 canonical fractals spanning distinct mathematical
  challenges, the benchmark tests models' ability to recognize scale invariance, infer
  geometric transformations, and achieve recursive abstraction through code synthesis.
---

# FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis

## Quick Facts
- **arXiv ID**: 2511.06522
- **Source URL**: https://arxiv.org/abs/2511.06522
- **Reference count**: 40
- **Primary result**: Current MLLMs achieve 76% syntactically valid code but only 4% visually correct fractal generation, revealing fundamental gaps in recursive mathematical abstraction

## Executive Summary
FractalBench evaluates whether multimodal large language models can infer symbolic generative rules from fractal images, requiring abstraction of recursive mathematical structure from visual patterns. Using 12 canonical fractals spanning distinct mathematical challenges, the benchmark tests models' ability to recognize scale invariance, infer geometric transformations, and achieve recursive abstraction through code synthesis. Evaluating four leading MLLMs across 7,320 images reveals a striking disconnect: while 76% generate syntactically valid code, only 4% produce visually correct fractals. Performance systematically varies by mathematical challenge—Koch curves achieve 17-21% accuracy through iterative geometric transformations, Sierpiński fractals reach 3-18% by recognizing multi-scale self-similarity, but tree fractals catastrophically fail at <2%, revealing fundamental gaps in branching recursion.

## Method Summary
FractalBench presents models with 610 unique 1,024×1,024 pixel fractal images across 12 types, 5 colors, and 4-12 recursion depths. Models must generate Python code using a MinimalTurtle API (4 commands: move, turn, pen_up/down, goto) that reproduces the fractal exactly. The evaluation measures execution success rate and visual correctness via IoU ≥ 95% between generated and ground truth binary masks. Four MLLMs (GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, Qwen 2.5-VL) are tested with three prompt strategies: Direct Code Generation, Reasoning Then Code, and Recursive Structure Focus. The benchmark isolates specific mathematical reasoning capabilities by selecting fractals with distinct structural requirements that map to separable cognitive operations.

## Key Results
- 76% of code generations are syntactically valid but only 4% produce visually correct fractals
- Koch curves achieve highest accuracy (17-21%) through iterative geometric transformations
- Tree fractals fail catastrophically at <2%, revealing fundamental gaps in branching recursion
- Explicit verbal reasoning degrades performance, inverting typical chain-of-thought advantages
- Performance systematically varies by mathematical challenge type, creating a capability hierarchy

## Why This Works (Mechanism)

### Mechanism 1: Diagnostic Granularity Through Mathematical Hierarchy
The benchmark isolates specific mathematical reasoning capabilities by selecting fractals with distinct structural requirements that map to separable cognitive operations. Twelve canonical fractals span five computational patterns—linear recursion (Cantor), geometric transformations (Koch), multi-scale self-similarity (Sierpiński), space-filling navigation (dragons), and branching recursion (trees). Performance systematically varies by pattern type, creating a capability hierarchy that reveals which reasoning primitives models possess versus lack.

### Mechanism 2: Semantic Gap Detection via Syntax-Semantics Disconnect
The 76% execution vs 4% correctness gap reveals models possess syntactic competence without semantic understanding of generative rules. The benchmark requires executable Python code as output, then measures visual correctness via IoU ≥ 95%. This two-stage evaluation separates "can write valid code" from "infers the correct generative rule." Models produce executable code that generates recursive patterns—but the wrong fractal structure.

### Mechanism 3: Contamination Resistance Through Parameterized Complexity
Variable recursion depths (4-12 levels) and color variants (5 colors) create contamination-resistant evaluation by preventing memorization of canonical examples. Color variants prevent pretrained MLLMs from relying on cached visual embeddings of canonical black fractals. The 610 unique images across depth/color combinations require models to infer generative rules rather than retrieve memorized patterns.

## Foundational Learning

- **Iterated Function Systems (IFS)**: FractalBench defines all 12 fractals as attractors of contractive IFS—finite sets of 2-8 similarity maps that generate arbitrarily complex patterns through recursive application. Understanding IFS is essential to grasp what "inferring the generative rule" means.
- **Scale Invariance vs Visual Similarity**: Models must distinguish between "looks similar at different scales" and "is composed of exact scaled copies with specific contraction ratios." This distinction separates pattern recognition from mathematical abstraction.
- **Branching vs Linear Recursion**: Tree fractals fail catastrophically (<2%) while Cantor sets perform better (3.7%), isolating branching recursion as the critical bottleneck. Understanding why exponential state management differs from linear recursion is essential for interpreting results.

## Architecture Onboarding

- **Component map**: Input images → Fractal type and depth recognition → Geometric parameter extraction → Recursive rule inference → MinimalTurtle implementation → Visual rendering → IoU evaluation
- **Critical path**: Image perception → fractal type and depth recognition → Visual analysis → geometric parameter extraction (angles, contraction ratios, transformations) → Mathematical abstraction → recursive rule inference (self-similar structure) → Code synthesis → MinimalTurtle implementation with correct recursion → Execution → visual rendering → IoU comparison against ground truth
- **Design tradeoffs**: MinimalTurtle constraint isolates visual-to-symbolic reasoning from library recall but limits expressiveness; IoU ≥ 95% threshold enables objective evaluation but misses "near-miss" structures; single generation per image is faster but misses stochastic variance; Direct Code Generation outperforms reasoning prompts, suggesting verbose reasoning interferes with tight visual-geometric coupling
- **Failure signatures**: Tree fractals <2% (models substitute iterative loops or single-branch recursion); Dragon curves 1.6-1.9% (space-filling navigation errors); Koch curves 17-21% (best performance but 80% failure rate); Reasoning prompts hurt performance (verbal analysis difficult to translate into exact numerical parameters)
- **First 3 experiments**: 1) Establish baseline by fractal type to replicate hierarchical performance pattern; 2) Probe branching recursion specifically with controlled variations; 3) Prompt strategy ablation comparing DCG vs RTC vs RSF on single fractal type

## Open Questions the Paper Calls Out

### Open Question 1
Why does explicit verbal reasoning degrade performance in visual-mathematical synthesis? The authors observe that "verbose intermediate reasoning may interfere with precise visual-to-code synthesis," inverting the typical chain-of-thought advantage. This remains unresolved as the findings are "observational without stepwise ablations," leaving the cause (e.g., attention exhaustion vs. semantic gap) unidentified.

### Open Question 2
Can model architectures be modified to successfully handle branching recursion? The paper notes tree fractals fail at <2% because models "substitute iterative loops" for true branching recursion, identifying this as a "fundamental" gap. While the paper diagnoses the failure to manage exponential complexity, it does not propose or test architectural solutions.

### Open Question 3
Do specialized reasoning models overcome the failure modes identified in standard MLLMs? The authors excluded "reasoning-specialized systems such as OpenAI o1 or DeepSeek-R1" and explicitly query whether the benchmark exposes "universal failure modes or distinguishes genuinely stronger capabilities." The evaluation is limited to four standard MLLMs, leaving the performance of specialized reasoning architectures unknown.

## Limitations
- Unknown MLLM API parameters (temperature, max_tokens, top_p) not specified
- Unknown binary mask extraction method from colored fractals for IoU computation
- Unknown exact render_turtle() implementation details for path-to-image conversion

## Confidence
- **High**: Syntax-semantics disconnect finding (76% vs 4%) - well-documented through two-stage evaluation
- **Medium**: Contamination resistance through parameterization - plausible but weak corpus evidence
- **Medium**: Reasoning prompts hurt performance - counterintuitive but well-documented finding

## Next Checks
1. Replicate the 76%/4% syntax-semantics gap by running all four MLLMs on black fractals with DCG prompt
2. Test tree fractal variations (binary vs ternary, symmetric vs asymmetric) to isolate whether failure is branching-specific or exponential-complexity-specific
3. Compare DCG vs RTC vs RSF on Koch curve with controlled output length to determine if reasoning prompts interfere due to attention exhaustion or semantic gap