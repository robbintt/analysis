---
ver: rpa2
title: 'Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP'
arxiv_id: '2508.08005'
source_url: https://arxiv.org/abs/2508.08005
tags:
- graph
- algorithm
- features
- instances
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of selecting the best exact algorithm
  for the Maximum Clique Problem (MCP) across diverse graph instances, as no single
  solver consistently outperforms others. The authors propose a dual-channel GAT-MLP
  model that integrates a Graph Attention Network (GAT) to capture local structural
  patterns with a Multilayer Perceptron (MLP) to encode global graph statistics.
---

# Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP

## Quick Facts
- **arXiv ID:** 2508.08005
- **Source URL:** https://arxiv.org/abs/2508.08005
- **Authors:** Xiang Li; Shanshan Wang; Chenglong Xiao
- **Reference count:** 34
- **Primary result:** A dual-channel GAT-MLP model achieves 90.43% accuracy in selecting the optimal exact MCP solver.

## Executive Summary
This work addresses the challenge of selecting the best exact algorithm for the Maximum Clique Problem (MCP) across diverse graph instances, as no single solver consistently outperforms others. The authors propose a dual-channel GAT-MLP model that integrates a Graph Attention Network (GAT) to capture local structural patterns with a Multilayer Perceptron (MLP) to encode global graph statistics. They construct a labeled dataset by running four state-of-the-art exact MCP solvers on a diverse collection of graphs and extract both local and global features. Extensive experiments show that GAT-MLP significantly outperforms classical ML baselines and single-channel graph neural networks, achieving 90.43% accuracy in choosing the optimal solver. The study demonstrates the effectiveness of combining local and global information for instance-aware algorithm selection in MCP.

## Method Summary
The proposed approach uses a dual-channel architecture where a GAT branch processes node-level features (degree, k-core) to capture local topology, while an MLP branch processes 12 global graph statistics. These embeddings are concatenated and passed through a classifier to predict which of four exact MCP solvers (CliSAT, LMC, MoMC, dOmega) will perform best. The model is trained on a dataset of 572 graphs with labels indicating the winning solver based on maximum clique size and runtime. The architecture is trained using Adam optimizer with learning rate 0.001, dropout 0.5, and batch size 16 for 50 epochs.

## Key Results
- GAT-MLP achieves 90.43% accuracy in predicting the optimal MCP solver, significantly outperforming classical ML baselines (82.22%) and single-channel GNN models (56.52-83.48%).
- Feature importance analysis shows graph density and maximum k-core number are the most predictive metrics for solver performance.
- The dual-channel architecture is superior to single-channel approaches, with GAT-only achieving 56.52% and MLP-only achieving 83.48% accuracy.
- Ablation studies confirm that both local topology and global statistics contribute valuable information for solver selection.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dual-channel architecture improves prediction accuracy by preventing the information loss that occurs when forcing global and local features into a single processing stream.
- **Mechanism:** The model decouples representation learning into a Graph Attention Network (GAT) branch for local neighborhood topology and a Multilayer Perceptron (MLP) branch for global statistical summaries. These distinct embeddings are concatenated (late fusion), allowing the classifier to weigh dense connectivity signals against macro-level graph metrics independently.
- **Core assumption:** Local topology (e.g., k-core neighborhoods) and global statistics (e.g., density) provide non-redundant, complementary predictive signals for solver performance.
- **Evidence anchors:**
  - [abstract] "GAT-MLP... integrates a Graph Attention Network (GAT) to capture local structural patterns with a Multilayer Perceptron (MLP) to encode global graph statistics."
  - [section 4.3] Ablation study shows "GAT-Only" scores 56.52% accuracy while "MLP-Only" scores 83.48%, confirming that global features carry the bulk of the signal, but the full dual-channel model reaches 90.43%.
  - [corpus] "Geometric Learning in Black-Box Optimization" supports the use of GNNs for performance prediction, but notes that integrating problem characterizations (features) is an active research area.
- **Break condition:** If global features (like density) fully determine the local structure (e.g., in Erdős–Rényi graphs), the GAT branch provides no new information, rendering the dual-channel overhead useless.

### Mechanism 2
- **Claim:** Solver performance correlates strongly with specific structural metrics, allowing tree-based models to establish a high baseline via feature importance.
- **Mechanism:** Tree-based classifiers (Random Forest) prioritize splits on high-variance features. The analysis reveals that graph Density ($D$) and Max $k$-core number ($K$) are the dominant predictors, effectively mapping graph instances to the algorithmic strengths of specific solvers (e.g., dOmega for sparse/low-degeneracy graphs vs. CliSAT for dense ones).
- **Core assumption:** The four candidate solvers have distinct, non-overlapping "performance profiles" based on graph density and core structure.
- **Evidence anchors:**
  - [section 4.2] "Feature importance analysis... shows graph density $D$ consistently shows the highest importance, followed by... $K$."
  - [section 2.1] The paper details solver biases: "dOmega performs well on sparse graphs... LMC is suitable for... high graph density."
  - [corpus] Evidence is limited in the direct corpus regarding these specific MCP solvers; the claim rests on the internal experimental analysis.
- **Break condition:** If a new solver is introduced that performs robustly across *all* density regimes, the correlation between density features and "best solver" labels will weaken, collapsing the model's predictive power.

### Mechanism 3
- **Claim:** Avoiding label noise is more critical for this task than maximizing training data volume.
- **Mechanism:** The authors compare splitting multi-label instances (Method 1) vs. removing them (Method 2). Method 1 creates identical feature vectors with contradictory labels (confusing the decision boundary), whereas Method 2 sacrifices data quantity for boundary clarity.
- **Core assumption:** The feature space is sufficiently distinct for single-label instances that removing ambiguous examples improves generalization.
- **Evidence anchors:**
  - [section 4.2] "Method 1 yields markedly inferior performance... identical feature vectors are assigned conflicting labels, effectively introducing severe label noise."
  - [section 3.1] Describes the three labeling strategies and the trade-offs involved.
- **Break condition:** If the dataset were significantly smaller, losing the ambiguous instances (Method 2) might result in under-fitting, necessitating a soft-label or multi-label approach.

## Foundational Learning

- **Concept: Per-Instance Algorithm Selection (Rice's Framework)**
  - **Why needed here:** The paper frames the problem not as solving MCP, but as a meta-optimization task: mapping "Problem Space" (graph features) to "Algorithm Space" (solvers) via a "Performance Model."
  - **Quick check question:** Can you explain why the "No Free Lunch" theorems necessitate an instance-aware selection strategy for NP-hard problems?

- **Concept: Graph Degeneracy and K-Core Decomposition**
  - **Why needed here:** The feature analysis identifies $k$-core (degeneracy) as a top predictor. Understanding that a $k$-core is a maximal subgraph where every vertex has degree $\ge k$ is essential to interpreting why solvers like dOmega succeed on low-degeneracy graphs.
  - **Quick check question:** How does the "clique-core gap" influence the complexity of exact maximum clique algorithms?

- **Concept: Attention Mechanisms in GNNs (GAT)**
  - **Why needed here:** The proposed model uses GAT rather than a simpler GCN. Understanding that GAT assigns learnable weights to neighbor nodes allows the model to focus on "important" structural patterns (like clique-dense regions) rather than averaging all neighbors equally.
  - **Quick check question:** How does the attention coefficient in GAT differ from the normalized sum in GCN when processing a node with both high-degree and low-degree neighbors?

## Architecture Onboarding

- **Component map:**
  Raw `.clq` graph file -> Extract 12 global stats (MLP input) + Node Degree/Core 2D features (GAT input) -> GAT Branch (2-layer GAT -> Global Mean Pool) -> MLP Branch (2-layer Linear -> ReLU) -> Concatenate([$z_{struct}$, $z_{stat}$]) -> Linear layer -> Softmax

- **Critical path:**
  The fusion of the GAT embedding and the statistical embedding is the system bottleneck. If the global features are normalized incorrectly (Z-score) or the node features are missing, the classifier receives misaligned signals, degrading accuracy from ~90% to ~56% (GAT-only baseline).

- **Design tradeoffs:**
  - **Fusion Strategy:** The paper tests Weighted, Gated, and Direct Concatenation. Direct concatenation wins, suggesting the features are already semantically aligned and learnable gates add unnecessary complexity/overfitting risk on a small dataset (572 graphs).
  - **Labeling:** Removing multi-label instances (Method 2) is preferred over splitting (Method 1) to maintain decision boundary integrity.

- **Failure signatures:**
  - **Accuracy ~55-60%:** Likely using "Method 1" (split labels) or GAT-only mode (ignoring global density).
  - **Low Macro-F1 (despite high accuracy):** The model is biased toward the majority class solver; check class weights or dataset balance.

- **First 3 experiments:**
  1.  **Baseline Verification:** Train a Random Forest on the 12 global features using Method 2 (remove multi-label). Confirm ~82% accuracy to validate dataset integrity.
  2.  **Ablation Run:** Train GAT-MLP with the GAT branch disabled (MLP-only) and compare against the full model to quantify the contribution of local structure (expected delta: ~7% accuracy).
  3.  **Hyperparameter Sensitivity:** Run a grid search on Learning Rate ($10^{-3}$ vs $10^{-4}$) and Dropout (0.5 vs 0.7) to ensure the reported 90.43% is stable and not a result of a lucky random seed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dual-channel GAT-MLP framework be effectively generalized to other NP-hard combinatorial optimization problems beyond the Maximum Clique Problem (MCP)?
- **Basis in paper:** [explicit] The conclusion explicitly states future work will focus on "exploring ways to transfer the proposed framework to related combinatorial optimization problems."
- **Why unresolved:** The current model relies on features and architectural choices (like $k$-core attention) tailored specifically to clique topology; it is unclear if the dual-channel GAT-MLP architecture is inherently robust or requires significant re-engineering for problems with different structural constraints (e.g., TSP, SAT).
- **What evidence would resolve it:** Successful application of the specific dual-channel architecture to a distinct combinatorial problem (e.g., Graph Coloring) using a similar pipeline, demonstrating comparable performance gains over single-channel baselines.

### Open Question 2
- **Question:** How does the inclusion of weighted or approximate solvers impact the classification accuracy and the "optimal solver" labeling strategy?
- **Basis in paper:** [explicit] The authors identify the limitation to four exact algorithms and propose incorporating "weighted and approximate MCP solvers" as a primary direction for future work.
- **Why unresolved:** The current labeling relies on a strict hierarchy (clique size $\rightarrow$ runtime) for exact solvers. Introducing approximate or weighted solvers introduces trade-offs between solution quality, optimality guarantees, and speed that the current binary labeling logic cannot capture.
- **What evidence would resolve it:** An extension of the dataset including approximate solvers, coupled with a modified loss function or labeling scheme that handles multi-objective trade-offs (e.g., Pareto efficiency), showing whether the model can learn to predict these more complex preferences.

### Open Question 3
- **Question:** Does the reliance on a modest dataset of 572 instances limit the model's ability to generalize to newer, more complex graph topologies?
- **Basis in paper:** [explicit] The paper admits the "dataset size is modest" and lists "expanding the dataset with more diverse graph instances" as a key future step.
- **Why unresolved:** While the model achieves 90.43% accuracy, training on a relatively small sample of existing benchmarks (DIMACS, BHOSLIB) risks overfitting to specific structural artifacts of these legacy datasets, potentially failing on modern, massive-scale networks.
- **What evidence would resolve it:** A rigorous out-of-distribution generalization test where the model is trained on the current 572 instances and tested on a newly generated set of large-scale synthetic or real-world networks not derived from the original benchmark families.

## Limitations
- The labeling method's impact on model performance is significant but not fully explored - while Method 2 (removing multi-label instances) performs best, the underlying causes of label ambiguity and whether a soft-label approach could perform better remain unclear.
- The generalizability to solvers beyond the four tested (CliSAT, LMC, MoMC, dOmega) is unknown, as is the model's performance on graphs with different characteristics than the 572-instance dataset.
- The specific number of attention heads (H) in the GAT encoder is not specified in the hyperparameters, creating a potential reproduction barrier.

## Confidence
- **High confidence:** The dual-channel architecture's superiority over single-channel models (90.43% vs 56.52-83.48%) is well-supported by ablation studies.
- **Medium confidence:** The correlation between solver performance and specific structural metrics (density, k-core) is supported by feature importance analysis but relies on internal experimental results.
- **Medium confidence:** The labeling strategy's impact (Method 2 vs Method 1) is demonstrated, though the dataset's size and composition may influence these findings.

## Next Checks
1. Conduct a systematic study of the "label noise" problem by testing soft-label approaches and analyzing the distribution of multi-label instances across different graph types.
2. Test the model's transferability by training on graphs from one domain (e.g., Network Repository) and evaluating on graphs from another (e.g., CSPLIB), measuring performance degradation.
3. Perform a hyperparameter sensitivity analysis focusing on the attention head count (H) in the GAT layer and its interaction with the hidden dimension (h=32) to identify optimal configurations.