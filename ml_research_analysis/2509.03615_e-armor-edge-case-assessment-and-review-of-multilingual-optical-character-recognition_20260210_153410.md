---
ver: rpa2
title: 'E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character
  Recognition'
arxiv_id: '2509.03615'
source_url: https://arxiv.org/abs/2509.03615
tags:
- text
- arxiv
- recognition
- image
- deployment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of OCR systems across
  multilingual, noisy, and real-world images, comparing five state-of-the-art Large
  Vision-Language Models (LVLMs) with two traditional OCR systems. The study introduces
  Sprinklr-Edge-OCR, a lightweight OCR pipeline optimized for edge deployment.
---

# E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition

## Quick Facts
- arXiv ID: 2509.03615
- Source URL: https://arxiv.org/abs/2509.03615
- Reference count: 34
- Primary result: Sprinklr-Edge-OCR outperforms LVLMs in edge deployment with 35× faster processing (0.17s vs 5.83s) and 0.006 USD per 1,000 images cost

## Executive Summary
This paper presents a comprehensive evaluation of OCR systems across multilingual, noisy, and real-world images, comparing five state-of-the-art Large Vision-Language Models (LVLMs) with two traditional OCR systems. The study introduces Sprinklr-Edge-OCR, a lightweight OCR pipeline optimized for edge deployment. Using a proprietary dataset of 54 languages, the authors benchmark models on accuracy, latency, memory usage, and cost. Results show that Sprinklr-Edge-OCR outperforms all models in overall F1 score (0.46), latency (0.17s per image), and cost (0.006 USD per 1000 images), processing images 35× faster than LVLM alternatives. Qwen LVLM achieved the highest precision (0.54). The study concludes that optimized traditional OCR systems remain superior for edge deployment due to their efficiency, low resource demands, and affordability.

## Method Summary
The study benchmarks five LVLMs (Qwen2-VL-2B-OCR, Llama-3.2-11B-Vision-Instruct-4Bit, MiniCPM-V-2.6-int4, InternVL2.5-1B, GOT-OCR2.0) against two traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary dataset spanning 54 languages. Evaluation metrics include accuracy (F1, Precision, Recall, WER, CER), efficiency (latency, memory, GPU utilization, cost), and semantic similarity scores generated by Qwen3-8B LLM judge. Sprinklr-Edge-OCR uses a two-stage detection-recognition architecture with TensorRT acceleration and ~0.15B parameters, optimized for edge deployment scenarios with CPU-only inference requirements.

## Key Results
- Sprinklr-Edge-OCR achieved best overall F1 score (0.46) and outperformed others in efficiency, processing images 35× faster (0.17 seconds per image) and at less than 0.01× of the cost (0.006 USD per 1,000 images)
- Qwen achieved the highest precision (0.54) while processing at significantly higher cost and latency
- Peak VRAM usage decreased from 9.7 GiB to 1.8 GiB when compared to traditional Paddle structure pipeline
- CPU-only inference: Sprinklr-Edge-OCR 4.36s vs Qwen-VL 69.38s

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimized traditional OCR pipelines outperform LVLMs for edge deployment in resource-constrained multilingual scenarios.
- Mechanism: Sprinklr-Edge-OCR uses a two-stage detection-recognition architecture with TensorRT-accelerated inference, reducing model complexity from multi-billion parameter transformers to ~0.15B parameters while maintaining competitive accuracy through task-specific optimization rather than general reasoning.
- Core assumption: Edge deployment scenarios prioritize latency, memory footprint, and cost over semantic understanding or zero-shot generalization across unseen languages.
- Evidence anchors:
  - [abstract] Sprinklr-Edge-OCR delivered best overall F1 score (0.46) and outperformed others in efficiency, processing images 35× faster (0.17 seconds per image) and at less than 0.01× of the cost (0.006 USD per 1,000 images).
  - [section] Peak VRAM usage decreased from 9.7 GiB to 1.8 GiB when compared to traditional Paddle structure pipeline; CPU-only inference: Sprinklr-Edge-OCR 4.36s vs Qwen-VL 69.38s.
  - [corpus] Related work on Korean character recognition (arXiv:2504.05770) similarly emphasizes lightweight fusion approaches for real-world OCR under computational constraints.
- Break condition: If application requires deep semantic reasoning across unseen document types or zero-shot multilingual generalization beyond training distribution, this mechanism fails.

### Mechanism 2
- Claim: LVLMs achieve higher precision through end-to-end semantic understanding but incur prohibitive computational costs for edge deployment.
- Mechanism: LVLMs integrate a Vision Transformer encoder with an LLM via projection layers, enabling contextual reasoning that reduces hallucinations and improves text fidelity (Qwen achieved 0.54 precision, lowest extra words at 4.03), but require 5-13 seconds inference time and 9-14 GiB memory.
- Core assumption: The unified vision-language representation enables better disambiguation of similar characters and handling of noisy inputs through learned priors.
- Evidence anchors:
  - [abstract] Qwen achieved the highest precision (0.54) while processing at significantly higher cost and latency.
  - [section] Qwen consistently produced output with high textual fidelity and minimal hallucinations, making it suitable for applications where precision is critical without compute restrictions.
  - [corpus] MultiOCR-QA work (arXiv:2502.16781) documents how OCR errors cascade into downstream QA tasks, suggesting LVLM semantic understanding may help—but corpus lacks direct LVLM vs. traditional OCR efficiency comparisons.
- Break condition: If inference must complete in <1 second on CPU-only hardware with <2 GiB RAM, LVLMs are unsuitable regardless of accuracy benefits.

### Mechanism 3
- Claim: LLM-as-judge evaluation captures semantic similarity better than traditional character-level metrics for multilingual OCR assessment.
- Mechanism: The benchmark uses Qwen3-8B to score predictions 0-9 based on word presence ignoring order/grammar, addressing limitations of WER/CER which penalize valid variations in spelling or word ordering common in multilingual contexts.
- Core assumption: LLM judges can reliably distinguish between acceptable variations and genuine errors without bias toward particular languages or output styles.
- Evidence anchors:
  - [section] The Similarity Score employs Qwen3-8B to evaluate word-level similarity, returning 0-9 where 9 indicates all ground truth words present; this captures nuances traditional metrics miss.
  - [section] Sprinklr-Edge-OCR achieved similarity score of 7.2 vs. InternVL's 4.8, correlating with F1 performance differences.
  - [corpus] Insufficient corpus evidence on LLM-as-judge validity for OCR evaluation; related benchmarks use standard metrics without semantic scoring.
- Break condition: If LLM judge exhibits systematic bias toward certain languages or output formats, composite scores become unreliable for cross-model comparison.

## Foundational Learning

- Concept: **Detection-Recognition Pipeline Architecture**
  - Why needed here: Understanding that traditional OCR separates text localization from character transcription explains why Sprinklr-Edge-OCR achieves lower latency—each stage can be independently optimized and quantized.
  - Quick check question: Can you explain why separating detection and recognition might fail on documents where text regions overlap with graphics?

- Concept: **Vision-Language Model Fusion**
  - Why needed here: LVLMs project visual embeddings into LLM token space, enabling reasoning but incurring cross-attention overhead; this explains the 15-80× latency gap.
  - Quick check question: What happens to inference cost if you increase image resolution in a ViT-MLP-LLM architecture versus a CNN-based detection head?

- Concept: **TensorRT and Quantization for Edge Deployment**
  - Why needed here: The paper attributes Sprinklr-Edge-OCR's efficiency gains partly to TensorRT acceleration; understanding INT8/INT4 quantization is essential for reproducing these results.
  - Quick check question: How does quantization affect character-level error rates versus semantic similarity scores?

## Architecture Onboarding

- Component map: Input preprocessing -> Detection module -> Recognition module -> TensorRT runtime -> Evaluation layer
- Critical path:
  1. Image → detection bounding boxes (largest latency contributor in traditional OCR)
  2. Cropped regions → recognition model (parallelizable)
  3. Aggregated text → LLM judge scoring (evaluation only, not production path)
- Design tradeoffs:
  - **Accuracy vs. latency**: Sprinklr-Edge-OCR trades ~0.08 F1 vs. Qwen for 35× speed improvement
  - **Language coverage vs. model size**: LVLMs support zero-shot multilingual; Sprinklr-Edge-OCR explicitly supports 5 languages with extensibility via fine-tuning
  - **Memory vs. batch size**: Sprinklr-Edge-OCR at 1.8 GiB enables 4-thread processing; LVLMs limited to single-thread on same hardware
- Failure signatures:
  - High WER with low Levenshtein distance suggests systematic character substitutions (check language model post-processing)
  - High extra words count with high precision indicates over-generation (common in LVLMs without strict prompting)
  - Inference time >10s with low GPU utilization suggests CPU bottleneck or memory swapping
- First 3 experiments:
  1. **Baseline latency test**: Run Sprinklr-Edge-OCR and Qwen-VL on 100 images from the evaluation set; confirm reported 0.17s vs. 5.83s on T4 GPU; if latency differs >20%, check TensorRT optimization and batch size settings.
  2. **CPU-only stress test**: Deploy both models on target edge hardware (e.g., 8-core CPU, 8 GiB RAM); measure peak memory and inference time; verify Sprinklr-Edge-OCR stays under 4s per image as reported.
  3. **Language subset evaluation**: Test on 5 held-out languages not in Sprinklr-Edge-OCR's explicit support list; compare F1 drop vs. Qwen to quantify zero-shot generalization gap.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on a proprietary 54-language dataset that is not publicly available, preventing independent validation of the benchmark methodology
- Sprinklr-Edge-OCR architecture is described as "PaddleOCR-based with proprietary enhancements" but specific modifications, code, or model weights are not released
- LLM-as-judge evaluation mechanism using Qwen3-8B is innovative but lacks external validation and established benchmark comparison

## Confidence
- **High confidence**: Traditional OCR efficiency claims (35× speedup, cost reduction) - supported by clear hardware measurements and TensorRT acceleration documentation
- **Medium confidence**: F1 score comparisons between models - depends on proprietary dataset quality and evaluation methodology
- **Medium confidence**: LLM-as-judge validity - novel approach without established benchmark comparison
- **Low confidence**: Generalization to languages outside the 5 explicitly supported by Sprinklr-Edge-OCR - zero-shot performance not characterized

## Next Checks
1. **Hardware profiling validation**: Deploy Sprinklr-Edge-OCR and Qwen-VL on identical edge hardware (8-core CPU, 8 GiB RAM) to verify reported 4.36s vs 69.38s CPU-only inference times and 1.8 GiB vs 9.7 GiB memory usage; any deviation >20% indicates optimization gaps.

2. **Cross-dataset benchmark**: Test both models on public multilingual OCR datasets (OCRBench v2, Common Crawl) to verify F1 score correlations and assess whether proprietary dataset results generalize to open benchmarks.

3. **LLM judge reliability assessment**: Run Qwen3-8B judge evaluations on a small subset with human-annotated ground truth; measure inter-annotator agreement and identify systematic scoring biases across language families to validate the semantic similarity scoring approach.