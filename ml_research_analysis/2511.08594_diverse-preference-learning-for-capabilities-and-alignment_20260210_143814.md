---
ver: rpa2
title: Diverse Preference Learning for Capabilities and Alignment
arxiv_id: '2511.08594'
source_url: https://arxiv.org/abs/2511.08594
tags:
- diversity
- temperature
- policy
- best-of-n
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a loss of diversity in LLM outputs caused
  by the KL-divergence regularizer in standard alignment algorithms like RLHF and
  DPO. The authors prove this causes systematic overrepresentation of majority preferences,
  harming both representation and performance.
---

# Diverse Preference Learning for Capabilities and Alignment

## Quick Facts
- arXiv ID: 2511.08594
- Source URL: https://arxiv.org/abs/2511.08594
- Reference count: 40
- Key outcome: SPL decouples KL penalty into entropy (α) and cross-entropy (β) terms, enabling fine-grained control over LLM generation diversity while improving calibration and maintaining quality.

## Executive Summary
This paper identifies a fundamental limitation in standard alignment algorithms like RLHF and DPO: the KL-divergence regularizer causes systematic overrepresentation of majority preferences and loss of diversity in LLM outputs. The authors prove this occurs because the KL penalty couples two independent functions, leading to majority preferences being exponentiated to the 10th-100th power. To address this, they propose Soft Preference Learning (SPL), which decouples these terms, enabling fine-grained control over generation diversity. Empirically, SPL increases semantic and lexical diversity while maintaining or improving quality, outperforms temperature scaling in best-of-N problem-solving accuracy, and achieves better logit calibration on multiple-choice benchmarks.

## Method Summary
SPL modifies the standard preference learning objective by decoupling the entropy and cross-entropy terms in the KL penalty. The SPL RLHF objective becomes E[r(x,y)] + αH(π) - βH(π, π_ref), where α controls the entropy bonus and β controls the KL penalty. This yields an optimal policy π(y) ∝ π_ref(y)^(β/α) × p^(1/α). The α/β ratio acts as a "global temperature" that scales entire sequence probabilities rather than token-by-token. For DPO-style training, SPL uses the closed-form objective E[log σ(α log(π(y)/π(y')) - β log(π_ref(y)/π_ref(y')))]. The method is implemented as a LoRA fine-tune with no additional computational overhead compared to standard DPO.

## Key Results
- SPL increases semantic and lexical diversity by 2-3x compared to standard DPO while maintaining or improving quality on Arena-Hard benchmarks
- On GSM8K hard split, SPL with α/β=1.2 achieves ~10% improvement over DPO in best-of-N accuracy at N=128 samples
- SPL models show significantly lower Expected Calibration Error (ECE) and better Brier scores on TruthfulQA and MMLU compared to DPO
- SPL produces more diverse outputs while maintaining coherence even at high temperatures (α/β=11 remains coherent vs token-level t=1.4 producing aberrations)

## Why This Works (Mechanism)

### Mechanism 1
The KL-divergence regularizer in RLHF/DPO causes systematic overrepresentation of majority preferences, leading to mode collapse. The KL penalty couples two independent functions: maximizing cross-entropy with the reference policy and maximizing entropy. When population preferences are split p vs (1-p), standard RLHF/DPO produces π(y) ∝ π_ref(y)^(p^(1/β)). With typical β ∈ [0.01, 0.1], this exponentiates preferences to the 10th-100th power—for 80%/20% splits with β=0.1, the majority receives 99.9999% probability.

### Mechanism 2
Decoupling entropy (α) and cross-entropy (β) terms enables proportional representation of preferences at α=1. SPL modifies the objective to maximize E[r(x,y)] + αH(π) - βH(π, π_ref). This yields optimal policy π(y) ∝ π_ref(y)^(β/α) × p^(1/α). The ratio α/β acts as a "global temperature" that scales entire sequence probabilities. When α=1, SPL becomes a proper scoring rule weighted by the reference prior, recovering proportional representation.

### Mechanism 3
Sequence-level temperature scaling preserves relative probability orderings across complete outputs, avoiding quality degradation seen in token-level temperature scaling. Token-level temperature renormalizes at each step, disrupting coherent multi-token planning. SPL's global scaling applies π'(y|x) = π_DPO(y|x)^(1/(α/β)) / Z, which flattens the distribution over complete sequences while maintaining that preferred sequences remain most likely. This preserves fluency even at high temperatures.

## Foundational Learning

- Concept: KL divergence and its decomposition
  - Why needed here: Understanding that D_KL(π||π_ref) = -H(π) + H(π, π_ref) reveals why standard RLHF couples entropy and cross-entropy, which SPL explicitly separates.
  - Quick check question: If H(π) increases while H(π, π_ref) stays constant, does D_KL increase or decrease?

- Concept: Bradley-Terry preference modeling
  - Why needed here: The theoretical analysis assumes rewards satisfy p(y≻y') = σ(r(y)-r(y')), which enables the closed-form policy derivation connecting population preferences p to policy probabilities.
  - Quick check question: Given two items with rewards r(A)=2, r(B)=1, what probability does Bradley-Terry assign to preferring A over B?

- Concept: Temperature scaling as distribution sharpening/flattening
  - Why needed here: SPL's α/β ratio acts as a global temperature; understanding how temperature affects entropy vs mode concentration clarifies why SPL outperforms token-level approaches.
  - Quick check question: Does increasing temperature increase or decrease the entropy of a softmax distribution?

## Architecture Onboarding

- Component map: SPL RLHF Objective -> E[r(x,y)] + αH(π) - βH(π_ref, π) -> Closed-form policy π(y) ∝ π_ref(y)^(β/α) × p^(1/α)
- Critical path: 1) Start with SFT base model 2) Train with SPL DPO objective on preference pairs 3) Select α/β based on task 4) Inference uses standard sampling
- Design tradeoffs: α/β=1 gives proportional representation but may overfit to noise; α/β≈1.1-1.5 balances quality and diversity; β=0.1 performs better than standard 0.01 for math tasks
- Failure signatures: Quality degradation at very high α/β; reward model mismatch amplifying low-quality outputs; check α scaling is applied to log ratios not logits directly
- First 3 experiments: 1) Train DPO and SPL (α/β=2) on HH-RLHF; measure embedding cosine distance vs Arena-Hard win rate 2) On GSM8K hard split, compare DPO, DPO t=1.2, and SPL α/β=1.2 at N=128 samples 3) Evaluate on TruthfulQA/MMLU measuring ECE and accuracy

## Open Questions the Paper Calls Out
- Can semantically grounded diversity metrics (e.g., based on embeddings or LLM judges) be directly integrated into the SPL objective to optimize for meaningful diversity rather than relying on entropy as a proxy?
- How does the optimal global temperature (α/β) scale with model size, and does SPL's advantage over token-level temperature scaling persist for larger models?
- Can the relationship between problem difficulty and optimal SPL temperature be formalized to enable automatic temperature selection for inference-time scaling?

## Limitations
- The theoretical analysis relies on idealized Bradley-Terry modeling assumptions that may not hold in practice, particularly distinguishing legitimate minority preferences from noise
- Empirical results show best performance at α/β ∈ [1.1, 1.5] rather than the theoretically optimal α=1, suggesting practical limitations to the theoretical framework
- Superiority over temperature scaling depends heavily on comparing specific configurations and may be task-dependent

## Confidence
- High Confidence: Empirical observation that SPL increases semantic and lexical diversity while maintaining quality is well-supported by diversity-quality tradeoff experiments
- Medium Confidence: Theoretical mechanism linking KL-divergence amplification to mode collapse is mathematically sound but relies on idealized preference modeling
- Low Confidence: Claim that SPL provides a Pareto improvement over temperature scaling depends on specific configuration comparisons

## Next Checks
1. **Noise vs. Legitimate Disagreement**: Re-run HH-RLHF experiments with synthetic noise injection to distinguish whether SPL's diversity gains come from representing legitimate minority preferences versus amplifying noisy preferences
2. **Cross-Reward Model Generalization**: Train SPL models using different reward models on the same preference datasets to test whether diversity gains transfer across reward models
3. **Long-Term Stability Analysis**: Track model performance over extended training (10K+ steps) to detect potential overfitting or catastrophic forgetting that might emerge with the entropy bonus