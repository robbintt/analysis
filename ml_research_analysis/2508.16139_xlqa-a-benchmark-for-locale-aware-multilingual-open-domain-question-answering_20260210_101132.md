---
ver: rpa2
title: 'XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering'
arxiv_id: '2508.16139'
source_url: https://arxiv.org/abs/2508.16139
tags:
- question
- questions
- multilingual
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of culturally aware evaluation in
  multilingual open-domain question answering (ODQA) benchmarks. It argues that existing
  benchmarks assume locale-invariant answers across languages, which fails to capture
  the cultural and regional variations that affect question understanding and correct
  responses.
---

# XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering

## Quick Facts
- **arXiv ID**: 2508.16139
- **Source URL**: https://arxiv.org/abs/2508.16139
- **Reference count**: 8
- **Primary result**: Introduced XLQA benchmark to evaluate culturally aware multilingual ODQA, revealing significant performance gaps on locale-sensitive questions across five state-of-the-art multilingual LLMs

## Executive Summary
This paper addresses a critical gap in multilingual open-domain question answering (ODQA) benchmarks by introducing XLQA, a benchmark designed to evaluate locale-aware understanding. The authors argue that existing benchmarks fail to capture cultural and regional variations that affect question understanding and correct responses, assuming locale-invariant answers across languages. XLQA contains 3,000 English seed questions expanded to eight languages, with human-verified annotations distinguishing between locale-invariant and locale-sensitive cases. The benchmark reveals that current state-of-the-art multilingual LLMs notably fail on locale-sensitive questions, with average exact match scores of 26.02% and F1 scores of 36.33% across non-English languages.

## Method Summary
XLQA was constructed using a scalable pipeline involving multilingual question generation, locale-aware answer generation via LLM with retrieval-augmented generation, and human verification. The benchmark uses English seed questions expanded to eight languages, with human annotators verifying and distinguishing between locale-invariant and locale-sensitive cases. The evaluation methodology involves testing five state-of-the-art multilingual LLMs on the benchmark, measuring performance gaps between English and other languages. The authors attribute these gaps to a lack of locale-grounding knowledge in the models, with disparities in training data distribution contributing to differences in both linguistic competence and locale-awareness across models.

## Key Results
- XLQA benchmark reveals significant performance gaps on locale-sensitive questions, with average EM scores of 26.02% and F1 scores of 36.33% across non-English languages
- Five state-of-the-art multilingual LLMs notably fail on locale-sensitive questions, demonstrating limitations in culturally nuanced query handling
- Performance gaps between English and other languages attributed to lack of locale-grounding knowledge in models
- Disparities in training data distribution contribute to differences in both linguistic competence and locale-awareness across models

## Why This Works (Mechanism)
The mechanism works by explicitly separating locale-invariant from locale-sensitive questions through human verification, creating a benchmark that tests both general knowledge and cultural context understanding. The scalable pipeline combining multilingual generation with LLM-based answer generation and human verification ensures comprehensive coverage while maintaining quality control.

## Foundational Learning
- **Locale-aware ODQA**: Understanding that questions may require culturally specific knowledge beyond literal translation
  - Why needed: Cultural context significantly affects correct answers in multilingual settings
  - Quick check: Can models distinguish between questions requiring local knowledge versus universal answers?

- **Multilingual question generation**: Creating diverse questions across multiple languages from seed questions
  - Why needed: Ensures comprehensive language coverage and cultural variation
  - Quick check: Are generated questions natural and culturally appropriate in target languages?

- **Human verification pipeline**: Annotators distinguishing locale-invariant vs. locale-sensitive cases
  - Why needed: Ensures benchmark quality and captures nuanced cultural distinctions
  - Quick check: What's the inter-annotator agreement on locale-sensitive classifications?

## Architecture Onboarding
**Component Map**: Seed Questions -> Multilingual Generation -> Locale-Aware Answer Generation (LLM+RAG) -> Human Verification -> Benchmark
**Critical Path**: The human verification stage is critical as it determines the locale-sensitive vs. invariant distinction that defines the benchmark's purpose
**Design Tradeoffs**: Using LLM-based answer generation enables scalability but introduces potential circularity with evaluation models
**Failure Signatures**: Low performance on locale-sensitive questions indicates insufficient cultural grounding in training data
**3 First Experiments**:
1. Test models on pure locale-invariant subset to establish baseline multilingual capability
2. Evaluate models on locale-sensitive questions with and without retrieval augmentation
3. Compare performance across languages with varying cultural distance from English

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Benchmark construction using LLM-based answer generation introduces potential circularity with evaluation models
- Reliance on English seed questions may not fully capture native speaker intuitions about locale-specific queries
- Human verification process may introduce inconsistencies in determining what constitutes "locale-sensitive" answers

## Confidence
**High Confidence**: The core claim that existing multilingual ODQA benchmarks fail to capture locale-specific variations is well-supported by the construction methodology and evaluation results.
**Medium Confidence**: The assertion that training data distribution disparities directly cause differences in both linguistic competence and locale-awareness across models is plausible but not conclusively demonstrated.
**Low Confidence**: The claim that current multilingual LLMs "notably fail" on locale-sensitive questions should be tempered, as the evaluation is limited to a specific set of models and tasks.

## Next Checks
1. Test XLQA benchmark with a broader range of multilingual ODQA systems, including non-LLM approaches and models trained on different data distributions
2. Conduct inter-annotator agreement studies on the locale-sensitive versus locale-invariant classifications
3. Perform controlled experiments varying training data composition to establish causality between data distribution and locale-awareness capabilities