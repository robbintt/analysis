---
ver: rpa2
title: An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems
arxiv_id: '2511.02525'
source_url: https://arxiv.org/abs/2511.02525
tags:
- which
- routing
- depot
- location
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DRLHQ, an end-to-end deep reinforcement learning
  method to solve capacitated location-routing problems (CLRPs) and open CLRPs (OCLRP).
  The key innovation is reformulating CLRPs as a Markov decision process (MDP) that
  integrates location and routing decisions into a unified framework, enabling simultaneous
  optimization.
---

# An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems

## Quick Facts
- **arXiv ID:** 2511.02525
- **Source URL:** https://arxiv.org/abs/2511.02525
- **Reference count:** 40
- **Primary result:** DRLHQ achieves superior solution quality and generalization for capacitated location-routing problems through unified MDP formulation and heterogeneous querying attention.

## Executive Summary
This paper introduces DRLHQ, a deep reinforcement learning method that solves capacitated location-routing problems (CLRPs) by reformulating them as a unified Markov decision process. The method integrates facility location and vehicle routing decisions into a single end-to-end framework, using a heterogeneous querying attention mechanism to handle the distinct temporal characteristics of each decision type. The approach demonstrates strong performance on both synthetic and benchmark datasets, outperforming traditional and DRL-based baselines while maintaining better generalization across problem scales.

## Method Summary
DRLHQ reformulates CLRPs as a Markov decision process where solutions are constructed through sequential node selection. The method uses an encoder-decoder architecture with multi-head self-attention to process node embeddings and heterogeneous queries to guide decision-making. Location decisions employ GRU-based queries for temporal dependencies, while routing decisions use context embeddings for immediate spatial relationships. Dynamic masking enforces feasibility constraints throughout the solution construction process. The model is trained using POMO-style multi-trajectory sampling with REINFORCE and a shared baseline.

## Key Results
- DRLHQ achieves superior solution quality compared to traditional and DRL-based baselines on CLRP datasets
- The method demonstrates better generalization performance across problem scales (10-200 customers)
- Heterogeneous querying attention mechanism shows 5-15% improvement in routing costs when combined with dynamic masking
- Cross-distribution generalization maintains competitive performance on real-world benchmark instances

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating CLRPs as a single unified MDP rather than solving location and routing sequentially improves solution quality by capturing interdependencies between decision stages.
- **Mechanism:** The solution construction is decomposed into subtours, where each subtour implicitly contains a location decision (selecting a depot) followed by routing decisions (selecting customers). The depot's open/closed status is determined only after the full solution is constructed—depots included in any subtour are considered open. This allows the policy to jointly optimize facility opening costs, vehicle costs, and routing costs in a single objective function.
- **Core assumption:** Location and routing decisions are sufficiently coupled that sequential optimization leaves exploitable optimality gaps.
- **Evidence anchors:** [abstract], [Section IV.A], [corpus]

### Mechanism 2
- **Claim:** Using heterogeneous query vectors—GRU-based queries for location decisions and context-embedding queries for routing decisions—enables the attention mechanism to better handle the distinct temporal characteristics of each decision type.
- **Mechanism:** The decoder constructs location queries using a GRU that conditions on the previous subtour's starting depot embedding, capturing sequential dependencies among depot selections across subtours. Routing queries use the last visited node's context embedding directly, emphasizing immediate node-to-node relationships. The indicator state $I_t$ switches between query types at each step.
- **Core assumption:** Location decisions exhibit longer-range temporal dependencies, while routing decisions depend more on local spatial relationships and remaining capacity.
- **Evidence anchors:** [abstract], [Section IV.C], [corpus]

### Mechanism 3
- **Claim:** Dynamic masking based on the indicator state $I_t$ and capacity constraints ensures solution feasibility while enabling flexible switching between location and routing phases without a fixed decision order.
- **Mechanism:** Five masking rules enforce constraints during decoding: (1) mask visited customers, (2) mask non-departure depots during routing, (3) mask infeasible customers by vehicle/depot capacity, (4) mask departure depot at subtour start, (5) mask depots with insufficient capacity for remaining demand during location selection. Capacity values update immediately after each action.
- **Core assumption:** All feasibility constraints can be expressed as node-level masks at each step.
- **Evidence anchors:** [abstract], [Section IV.D], [corpus]

## Foundational Learning

- **Concept:** Markov Decision Processes for constructive combinatorial optimization
  - **Why needed here:** The entire method reformulates CLRP as an MDP with state, action, transition, reward, and policy components.
  - **Quick check question:** Can you explain how a solution tour is built step-by-step as a sequence of state-action transitions?

- **Concept:** Attention mechanisms and encoder-decoder architectures (Transformer basics)
  - **Why needed here:** The encoder uses multi-head self-attention to compute node embeddings; the decoder uses cross-attention with heterogeneous queries to select nodes.
  - **Quick check question:** Given node embeddings $h_i$, how would you compute attention scores using a query vector $q$ and apply masking?

- **Concept:** Policy gradient methods (REINFORCE with baseline)
  - **Why needed here:** The model is trained using REINFORCE with a shared baseline across trajectories.
  - **Quick check question:** What is the role of the baseline in reducing variance for REINFORCE, and how does POMO's multi-trajectory sampling provide it?

## Architecture Onboarding

- **Component map:** Instance input -> feature extraction -> encoder (L self-attention layers) -> node embeddings + graph embedding -> decoder (heterogeneous querying) -> node selection with dynamic masking -> solution construction

- **Critical path:**
  1. Instance input → feature extraction → encoder → node embeddings + graph embedding
  2. Decoder initializes with placeholder context; $I_t = 1$ triggers location query → depot selection
  3. $I_t = 0$ triggers routing query → customer selection loop until vehicle returns to depot
  4. Return to step 2 until all customers served; compute reward (negative total cost)
  5. Training: REINFORCE with shared baseline across POMO trajectories

- **Design tradeoffs:**
  - **GRU in location query:** Adds temporal modeling capacity but increases parameters and sequential dependency; ablation shows marginal improvement without masking but meaningful gain when combined.
  - **Instance augmentation (rotation):** Improves inference quality at cost of $g \times m$ forward passes; general augmentation extends POMO's flip-only approach.
  - **Simulation-based beam search:** Post-hoc search improves solutions but adds latency; not used during training.

- **Failure signatures:**
  - **Infeasible solutions:** If masking rules are incomplete or capacity updates lag, constraints may be violated.
  - **Degenerate routing:** If routing queries fail to capture spatial structure, routes may zigzag.
  - **Poor generalization across scales:** If trained only on small instances, larger instances may exceed capacity patterns seen during training.

- **First 3 experiments:**
  1. **Validate masking correctness:** Generate small instances (10 customers, 2 depots) and run greedy decoding; manually verify all constraints (each customer visited once, capacities respected, no depot-to-depot edges). Log mask status at each step.
  2. **Ablate heterogeneous queries:** Train two variants—one with GRU-based location queries, one with uniform context queries—on CLRP50; compare objective values and routing costs as in Tables VI/VII. Expect 5-15% degradation without GRU.
  3. **Test cross-scale transfer:** Train on CLRP50, evaluate on CLRP100 and CLRP200 without fine-tuning; compare to POMO* and MTA* baselines. Measure gap degradation relative to in-distribution performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the DRLHQ framework be effectively adapted to handle stochastic or dynamic environments, such as real-time fluctuations in customer demand or travel costs?
- **Basis in paper:** The conclusion explicitly identifies "considering uncertain factors in real-world applications" as a primary direction for future work.
- **Why unresolved:** The current MDP formulation and dynamic masking mechanism assume deterministic inputs where demands and costs are fixed and known a priori.
- **Evidence to resolve it:** Demonstrating stable performance on benchmarks where parameters change dynamically during the solution construction phase.

### Open Question 2
- **Question:** Can this end-to-end architecture generalize to CLRP variants with more intricate constraints, such as time windows or multi-echelon structures?
- **Basis in paper:** The authors state an intention to apply the method to "more complex variants of routing problems" in future research.
- **Why unresolved:** The current heterogeneous query mechanism is tailored for standard CLRP/OCLRP constraints; it is unclear if the attention mechanism can capture temporal dependencies required for time windows without significant structural changes.
- **Evidence to resolve it:** Applying DRLHQ to a CLRP with Time Windows (CLRPTW) dataset and comparing the solution gap against specialized heuristic baselines.

### Open Question 3
- **Question:** How can the cross-distribution generalization gap be further reduced when applying a model trained on uniform synthetic data to diverse real-world instances?
- **Basis in paper:** The paper lists "enhancing the cross-size and cross-distribution generalization performance" as a key future objective.
- **Why unresolved:** While the method outperforms baselines on the Prins benchmark, the average gap to Best Known Solutions remains at roughly 7.59%, indicating the model struggles to fully bridge the distribution shift from synthetic training data.
- **Evidence to resolve it:** Modifications to the training scheme that result in a significantly reduced average gap on the Prins benchmark without retraining on that specific distribution.

## Limitations

- Heterogeneous querying mechanism's effectiveness is supported by ablation but lacks comparison to unified query baselines in the literature.
- Dynamic masking ensures feasibility under stated assumptions, but subtour elimination relies on MDP construction rather than explicit constraint handling.
- Cross-scale generalization is demonstrated empirically, but performance degradation at larger instances (100+ customers) is not quantified relative to specific architectural bottlenecks.

## Confidence

- **High:** Reformulation as unified MDP enables joint optimization of location and routing decisions (empirical results show consistent improvement over sequential methods).
- **Medium:** Heterogeneous querying improves temporal modeling for location decisions (GRU ablation shows 5-15% gain when combined with masking).
- **Medium:** Dynamic masking ensures feasibility under incremental constraint checking (five rules are explicitly defined and validated on synthetic instances).

## Next Checks

1. **Validate masking correctness:** Run greedy decoding on small instances (10 customers, 2 depots); manually verify all constraints and log mask status at each step.
2. **Ablate heterogeneous queries:** Train two variants—GRU-based location queries vs. uniform context queries—on CLRP50; compare objective values and routing costs.
3. **Test cross-scale transfer:** Train on CLRP50, evaluate on CLRP100/200 without fine-tuning; measure gap degradation relative to in-distribution performance.