---
ver: rpa2
title: 'Active-O3: Empowering Multimodal Large Language Models with Active Perception
  via GRPO'
arxiv_id: '2505.21457'
source_url: https://arxiv.org/abs/2505.21457
tags:
- active
- perception
- object
- reward
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of enabling Multimodal Large\
  \ Language Models (MLLMs) to perform active perception\u2014the ability to actively\
  \ select where and how to look in order to gather task-relevant visual information.\
  \ The authors propose ACTIVE-O3, a reinforcement learning-based training framework\
  \ built on GRPO, which equips MLLMs with a two-stage sensing and task execution\
  \ policy."
---

# Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO

## Quick Facts
- arXiv ID: 2505.21457
- Source URL: https://arxiv.org/abs/2505.21457
- Authors: Muzhi Zhu; Hao Zhong; Canyu Zhao; Zongze Du; Zheng Huang; Mingyu Liu; Hao Chen; Cheng Zou; Jingdong Chen; Ming Yang; Chunhua Shen
- Reference count: 40
- Primary result: Introduces ACTIVE-O3, a GRPO-based training framework enabling MLLMs to actively select informative image regions, achieving significant improvements on open-world small/dense object grounding (+2.7/+3.5 AP/AR on LVISdense), domain-specific detection (+8.5 APs on SODA-A), and fine-grained interactive segmentation tasks.

## Executive Summary
This paper addresses the challenge of enabling Multimodal Large Language Models (MLLMs) to perform active perception—the ability to selectively gather task-relevant visual information through sensor control. The authors propose ACTIVE-O3, a reinforcement learning framework built on Group Relative Policy Optimization (GRPO) that trains MLLMs to first sense where to look (proposing crop regions) and then execute tasks (detection/segmentation) on those regions. The method uses a dual-form reward design combining heuristic constraints (format compliance, non-overlap, area bounds, ground-truth coverage) with task-aware rewards (detection AP/AR, segmentation mIoU). Results show ACTIVE-O3 significantly outperforms baselines like Qwen2.5-VL and Qwen2.5-VL-CoT across multiple benchmarks, while also demonstrating strong zero-shot reasoning capabilities on the V* benchmark without explicit reasoning data.

## Method Summary
ACTIVE-O3 implements a two-stage policy for active perception in static 2D images, where a sensing model MO proposes crop regions and a task model MA performs detection or segmentation on those regions. The framework uses GRPO to optimize MO, with rewards computed from both heuristic constraints (ensuring valid, non-overlapping, appropriately-sized regions that cover ground truth) and task performance metrics. During training, both MO and MA share the same Qwen2.5-VL-7B-Instruct backbone, while inference can optionally decouple MA to use a stronger task-specific model like GDINO. The sensing prompt is structured to elicit bbox_2d JSON responses, which are parsed, cropped, resized to 840×840, and evaluated by the task model to compute task-aware rewards. Group-relative advantages are computed from multiple sampled responses per image, eliminating the need for a separate critic model.

## Key Results
- On LVISdense validation: ACTIVE-O3 achieves APs of 4.4 and ARs of 4.5, significantly outperforming Qwen2.5-VL (APs 2.5, ARs 2.7) and Qwen2.5-VL-CoT (APs 2.2, ARs 2.6)
- On SODA-A dataset: ACTIVE-O3+GDINO achieves 24.0 APs, 24.3 APs_50, and 23.5 APs_75, outperforming all baselines including Qwen2.5-VL-CoT (8.5, 11.5, 8.5)
- On SODA-D dataset: ACTIVE-O3+GDINO achieves 23.9 APs, 24.0 APs_50, and 23.8 APs_75, demonstrating strong performance on real-world aerial imagery
- On V* benchmark: ACTIVE-O3 achieves 67.9%, outperforming Qwen2.5-VL (54.9%) and Qwen2.5-VL-CoT (58.7%) without explicit reasoning data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating sensing policy from task execution enables targeted learning of "where to look" without requiring explicit supervision of optimal attention regions.
- Mechanism: The framework factorizes action into acam (sensing: proposing crop regions) and aenv (task: detection/segmentation). The sensing model MO receives heuristic and task-aware rewards based on downstream performance, not direct region labels. GRPO optimizes πθ to produce better region proposals by comparing multiple sampled responses.
- Core assumption: The base MLLM has sufficient instruction-following capability to produce parseable region proposals given structured prompts, and task model MA provides reliable reward signals.
- Evidence anchors:
  - [abstract] "introduces a two-stage policy for sensing and task execution, guided by dual-form rewards (task-aware and heuristic)"
  - [Section 4] "We treat active perception in this static 2D setting as a single-step decision problem (T = 1)"
  - [corpus] GRPO-CARE (2506.16141) confirms GRPO adaptation to MLLMs is viable but notes evaluation rigor gaps; methodological similarity supports plausibility but not uniqueness of this approach.
- Break condition: If task model MA has high error rates on cropped regions, reward signals become noisy and sensing policy may converge to suboptimal but reward-hacking behaviors.

### Mechanism 2
- Claim: Group-based advantage estimation removes the need for a separate critic model while providing stable gradient signals for policy improvement.
- Mechanism: GRPO samples N responses per input, computes rewards for each, then normalizes via An = (rn - mean) / std. This relative scoring within groups provides implicit baseline without learned value function. KL regularization against πref prevents catastrophic forgetting.
- Core assumption: Reward variance within groups is meaningful (not uniform); sampling diversity is sufficient to produce contrastive advantage estimates.
- Evidence anchors:
  - [Section 4.2] "An = rn − mean({r1, . . . , rN}}) / std({r1, . . . , rN}})" — explicit formula
  - [Section 4.2] "eliminates the need for a separate critic model"
  - [corpus] DeepSeek-R1 and DeepSeek-Math (cited) established GRPO; corpus shows recent extensions (GRPO-CARE) but no direct replication of active perception use case.
- Break condition: If all sampled responses receive similar rewards (low variance), advantage estimates become unstable or near-zero, stalling learning.

### Mechanism 3
- Claim: Combining heuristic constraints with task performance rewards balances exploration (spatial diversity, format compliance) with exploitation (actual detection accuracy).
- Mechanism: Heuristic rewards enforce non-overlapping regions (IoU ≤ 0.3), valid area ranges (1–50% of image), format compliance (parseable JSON), and coverage of GT regions. Task rewards compute AP/AR for detection or mIoU for segmentation. Weighted combination guides policy toward both well-formed and effective proposals.
- Core assumption: Heuristic constraints correlate with useful sensing behavior; violating them indicates poor proposals independent of task success.
- Evidence anchors:
  - [Section 4.3] "The final heuristic reward Rheuristic(y) is computed as a weighted sum of the above components"
  - [Table 4] Combined reward achieves APs 4.4 vs. 3.6 (task-only) and 3.0 (heuristic-only)
  - [corpus] No direct corpus comparison; mechanism is paper-specific.
- Break condition: If heuristic constraints systematically exclude optimal regions (e.g., optimal zoom is 60% of image), combined reward creates conflicting gradients.

## Foundational Learning

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: GRPO relies on rewards computed from deterministic metrics (AP, IoU) rather than learned reward models. Understanding how verifiable signals differ from preference-based RLHF clarifies why no critic is needed.
  - Quick check question: Can you explain why GRPO uses group-relative advantages instead of a learned value function baseline?

- Concept: **Active Perception / Active Vision**
  - Why needed here: The paper's core contribution is framing zoom-in selection as active perception. Prior work (Aloimonos 1988, Ballard 1991) established that sensor control can transform ill-posed problems into well-posed ones.
  - Quick check question: What makes a perception problem "active" vs. "passive," and why does static image analysis qualify under this framing?

- Concept: **Detection Metrics (AP, AR, mIoU)**
  - Why needed here: Task-aware rewards are computed directly from standard detection/segmentation metrics. Misunderstanding these metrics will lead to incorrect reward interpretation.
  - Quick check question: What does Average Recall @ IoU=0.5 measure, and why is it suitable as a reward signal for dense object grounding?

## Architecture Onboarding

- Component map:
  - Qwen2.5-VL-7B-Instruct (policy backbone) -> Sensing prompt IO (structured bbox_2d JSON elicitation) -> Task prompt IA (detection/segmentation instruction) -> Reward computation (heuristic + task-aware) -> GRPO optimizer (group size 8, KL coef 0.04, lr 1e-6, AdamW) -> Updated policy

- Critical path:
  1. Image → resize to shorter side 1024 → MLLM with sensing prompt → N sampled responses
  2. Parse bboxes from JSON in each response → compute heuristic rewards
  3. Crop each proposed region → resize to 840×840 → task model inference → compute task rewards
  4. Combine rewards → compute group-relative advantages → GRPO update with KL penalty
  5. At test time: MO proposes K regions → MA (MLLM or GDINO) produces final detections

- Design tradeoffs:
  - **Unified vs. decoupled MA/MO**: Training uses same MLLM for both; inference can use GDINO for MA. Unified simplifies training but may limit task model strength.
  - **Heuristic vs. task reward weighting**: Paper sets all λi = 1. Imbalanced weighting could over-constrain (too much heuristic) or under-regularize (too much task reward).
  - **Group size N = 8**: Larger groups improve advantage estimation but increase compute (N forward passes + N task evaluations per image).
  - **Fixed K = 3 regions**: Limits coverage for very dense scenes; adjustable per task but requires prompt modification.

- Failure signatures:
  - **Low reward variance across samples**: GRPO stalls; check if task model produces identical outputs for all crops.
  - **JSON parse failures**: Heuristic Rformat = 0 dominates; review prompt compliance or add format-specific SFT warmstart.
  - **Degenerate region proposals** (all same location, extreme sizes): Rno-overlap or Rarea = 0; verify reward weights and constraint thresholds.
  - **Domain gap on specialized data** (SODA-A aerial): Task model misidentifies objects → task reward unreliable; consider domain-specific task model or lower IoU threshold (paper uses 0.1 for SODA).

- First 3 experiments:
  1. **Baseline sanity check**: Run Qwen2.5-VL-CoT (prompt-based MO only, no RL) on LVIS_small validation. Confirm reported APs ~1.2–2.5 before training.
  2. **Reward component ablation**: Train three variants (task-only, heuristic-only, combined) on LVIS_small subset. Verify combined outperforms either alone per Table 4.
  3. **Cross-domain transfer test**: Train on LVIS only, evaluate zero-shot on SODA-D. Confirm generalization gap and whether ACTIVE-O3+GDINO closes it vs. ACTIVE-O3 alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating a memory mechanism to store past actions and observations improve decision-making for trajectory-level planning and long-horizon search tasks?
- Basis in paper: [explicit] "The input to the sensing model is limited to the current observation. In practice, incorporating a memory mechanism... could enable more informed decision-making... This extension may support more sophisticated strategies, such as trajectory-level planning, long-term search, and rollback operations." (Section D.1)
- Why unresolved: The current framework only uses the current observation for sensing decisions, limiting its ability to perform coherent multi-step reasoning or recover from erroneous region selections.
- What evidence would resolve it: Experiments comparing the current single-step formulation against a memory-augmented variant on tasks requiring sequential decision-making, measuring improvements in search efficiency and task success rate.

### Open Question 2
- Question: How can the sensing model and task model be jointly optimized in a bootstrapping manner, rather than fixing the task model and only training the sensing policy?
- Basis in paper: [explicit] "In principle, the action model MA and the sensing model MO can be jointly optimized... A common alternative is to perform staged or iterative optimization, where one alternately updates MA and MO in a bootstrapping manner." (Remark D.2)
- Why unresolved: Joint optimization requires the task model to already possess sufficient baseline capability, which is not guaranteed. The interdependence between the two modules creates a chicken-and-egg problem for end-to-end training.
- What evidence would resolve it: Ablation studies showing whether alternating optimization of MA and MO improves performance over the current fixed-MA approach, and analysis of the stability of such training.

### Open Question 3
- Question: To what extent does ACTIVE-O3 generalize to truly embodied 3D scenarios with camera pose control, rather than the simplified 2D static-image setting?
- Basis in paper: [inferred] The paper acknowledges that the 2D setting "preserves the core challenge of active perception—selecting informative views—while simplifying execution" and notes that general embodied settings "are often difficult to deploy and evaluate in a reproducible manner." (Section 3)
- Why unresolved: The transition from axis-aligned rectangular regions to full 6-DOF camera control introduces significantly larger action spaces, continuous control requirements, and complex state transitions not addressed in this work.
- What evidence would resolve it: Evaluation on embodied navigation or manipulation benchmarks where the agent must actively control viewpoint through physical movement, comparing against the 2D zoom-in baseline.

### Open Question 4
- Question: How does performance scale with the number of allowed sensing regions, and can dynamic budget allocation based on scene complexity improve efficiency?
- Basis in paper: [explicit] "Our framework only allows zooming into three target regions per step. However, certain applications may require more flexible control, such as selecting a larger number of regions..." (Section D.1)
- Why unresolved: The fixed budget of three regions is a design choice without systematic exploration of alternative budgets or adaptive allocation strategies. Different scenes may benefit from different sensing budgets.
- What evidence would resolve it: Experiments varying K (number of regions) from 1 to 10+ across diverse scenes, analyzing the accuracy-efficiency trade-off curve and identifying whether scene complexity metrics can predict optimal budget.

## Limitations

- The paper's evaluation focuses heavily on detection and segmentation tasks, with limited validation of reasoning capabilities on the V* benchmark.
- Claims about zero-shot reasoning capabilities and generalization to unseen domains require more rigorous validation beyond the single V* benchmark experiment.
- The decoupled inference setup (using GDINO for MA) shows stronger results but was not used during training, creating a gap between training and deployment configurations.

## Confidence

- **High Confidence**: The two-stage sensing-task architecture and GRPO implementation details are well-specified and technically sound. Results on LVISdense and SODA datasets show consistent improvements over baselines.
- **Medium Confidence**: The dual-form reward design and its contribution to performance improvements are supported by ablation studies, though the exact weighting scheme (λi=1 for all components) lacks sensitivity analysis.
- **Low Confidence**: Claims about zero-shot reasoning capabilities and generalization to unseen domains require more rigorous validation beyond the single V* benchmark experiment.

## Next Checks

1. Conduct systematic reasoning capability tests on multiple reasoning-focused datasets (e.g., VSR, ScienceQA) to validate the V* benchmark results.
2. Perform a sensitivity analysis on the heuristic-to-task reward weighting (λi values) to determine optimal balance for different task types.
3. Evaluate cross-dataset generalization by training on LVIS only and testing on SODA-D and aerial imagery datasets to measure domain adaptation capabilities.