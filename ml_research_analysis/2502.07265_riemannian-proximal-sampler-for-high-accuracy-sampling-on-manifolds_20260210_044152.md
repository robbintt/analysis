---
ver: rpa2
title: Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds
arxiv_id: '2502.07265'
source_url: https://arxiv.org/abs/2502.07265
tags:
- riemannian
- grad
- have
- sampling
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Riemannian Proximal Sampler, a method\
  \ for sampling from densities on Riemannian manifolds using two key oracles: Manifold\
  \ Brownian Increments (MBI) and Riemannian Heat-kernel (RHK). The authors establish\
  \ high-accuracy sampling guarantees, showing that generating samples with \u03B5\
  -accuracy requires O(log(1/\u03B5)) iterations in Kullback-Leibler divergence with\
  \ exact oracles and O(log\xB2(1/\u03B5)) iterations in total variation metric with\
  \ sufficiently accurate inexact oracles."
---

# Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds

## Quick Facts
- **arXiv ID**: 2502.07265
- **Source URL**: https://arxiv.org/abs/2502.07265
- **Reference count**: 40
- **Primary result**: Achieves O(log(1/ε)) KL convergence and O(log²(1/ε)) TV convergence for manifold sampling

## Executive Summary
This paper introduces the Riemannian Proximal Sampler, a method for high-accuracy sampling from densities on Riemannian manifolds using two key oracles: Manifold Brownian Increments (MBI) and Riemannian Heat-kernel (RHK). The method addresses the challenge that traditional approaches struggle with high-accuracy sampling on Riemannian manifolds due to the complexity of implementing manifold Brownian motion and evaluating heat kernels. The proposed solution provides both theoretical guarantees and practical implementations through heat-kernel truncation and Varadhan's asymptotics.

## Method Summary
The Riemannian Proximal Sampler generates samples through an iterative two-step process. First, the MBI oracle samples from the heat kernel using manifold Brownian motion via rejection sampling with a Riemannian Gaussian proposal. Second, the RHK oracle samples from the product of the target density and heat kernel, implemented via rejection sampling by finding the minimum of f(x) - log ν_l(η, x, y_k). The method achieves ε-accuracy in O(log(1/ε)) iterations for KL divergence with exact oracles and O(log²(1/ε)) iterations for TV distance with sufficiently accurate inexact oracles. Practical implementations use heat kernel truncation and Varadhan's asymptotics, which can be interpreted as discretizing the entropy-regularized Riemannian Proximal Point Method on the associated Wasserstein space.

## Key Results
- Achieves O(log(1/ε)) iterations for ε-accuracy in KL divergence with exact oracles
- Achieves O(log²(1/ε)) iterations for ε-accuracy in TV distance with sufficiently accurate inexact oracles
- Demonstrates superior stability compared to Riemannian LMC on non-convex target functions, particularly for sampling from positive definite matrices
- Shows effective performance on hyperspheres (S²) and positive definite matrices (P₂) with Fréchet variance metrics

## Why This Works (Mechanism)
The method works by leveraging the properties of heat kernels and Brownian motion on manifolds. The heat kernel serves as a natural proposal distribution that smooths the target density, while the proximal iteration structure ensures convergence to the target distribution. The entropy-regularized interpretation connects the method to optimal transport theory, providing both theoretical justification and practical implementation strategies through Varadhan's asymptotics.

## Foundational Learning
- **Riemannian manifolds**: Smooth manifolds with a Riemannian metric g, essential for defining geometric operations like gradients, Hessians, and geodesics
  - *Why needed*: Provides the geometric framework for defining densities and sampling procedures on curved spaces
  - *Quick check*: Verify that the target density is defined on a complete, connected Riemannian manifold

- **Heat kernel**: Fundamental solution to the heat equation on manifolds, representing the probability density of Brownian motion
  - *Why needed*: Serves as both a proposal distribution and a smoothing kernel in the proximal iteration
  - *Quick check*: Confirm heat kernel satisfies the semigroup property and integrates to 1

- **Rejection sampling**: Monte Carlo method for generating samples from a target distribution using a proposal distribution
  - *Why needed*: Core algorithmic technique for implementing both MBI and RHK oracles when exact sampling is intractable
  - *Quick check*: Verify acceptance probability V(x) ≤ 1 throughout the high-density region

- **Wasserstein space**: Space of probability measures with finite second moments equipped with the Wasserstein metric
  - *Why needed*: Provides the optimal transport interpretation connecting proximal methods to entropy regularization
  - *Quick check*: Confirm the cost function satisfies the triangle inequality and the space is complete

## Architecture Onboarding

**Component Map**: Target density → MBI oracle (Brownian motion) → Heat kernel → RHK oracle (rejection sampling) → Samples

**Critical Path**: The most timing-sensitive path is the heat kernel evaluation and rejection sampling loop. Each iteration requires: (1) generating manifold Brownian increments via rejection sampling with Riemannian Gaussian proposal, (2) evaluating the heat kernel for acceptance/rejection, and (3) finding the minimizer x* = argmin(f(x) - log ν_l(η, x, y_k)). The bottleneck is typically the heat kernel computation, especially for high truncation levels or high-dimensional manifolds.

**Design Tradeoffs**: The method trades off between approximation accuracy (higher truncation levels for heat kernel) and computational cost. Exact heat kernel evaluation is expensive for high-dimensional manifolds, leading to the use of Varadhan's asymptotics which provides a good approximation with O(d(x,y)²) complexity. The rejection sampling approach provides theoretical guarantees but may have poor acceptance rates for multimodal distributions or when proposal and target have poor overlap.

**Failure Signatures**: 
- Divergence on P_m due to f not being gradient Lipschitz (Riemannian LMC requires step size ≤ 0.0001; proximal sampler remains stable)
- Acceptance rate V(x) > 1 in rejection sampling when proposal variance t or constants C are poorly chosen
- Poor performance on highly multimodal distributions where the heat kernel smoothing may create disconnected modes

**First Experiments**:
1. Implement and verify heat kernel series expansion for S² with l = 10 terms, comparing against numerical integration
2. Test MBI oracle with varying proposal variances t to find optimal acceptance rate on a simple manifold
3. Compare proximal sampler performance against Riemannian LMC on P_2 with non-convex f(X) = d(X,I_m)^4/(2σ²)

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on accurate heat kernel computation which becomes expensive for high-dimensional manifolds
- Sensitivity to step size selection without clear guidance beyond theoretical bounds
- Need for gradient information of f which may be unavailable in some applications

## Confidence
- **Theoretical guarantees**: High confidence in convergence rates (O(log(1/ε)) for KL, O(log²(1/ε)) for TV)
- **Practical implementation**: Medium confidence due to unspecified hyperparameters (step sizes η, proposal variance t, truncation level l)
- **Numerical validation**: Medium confidence - promising results on simple manifolds (S² and P₂) but limited testing across different geometries

## Next Checks
1. Implement automated hyperparameter tuning for η, t, and truncation level l to achieve target accuracy without manual intervention
2. Test performance on non-convex manifolds with varying curvature and compare with state-of-the-art manifold MCMC methods
3. Analyze computational complexity scaling with manifold dimension and sample accuracy requirements beyond the theoretical bounds