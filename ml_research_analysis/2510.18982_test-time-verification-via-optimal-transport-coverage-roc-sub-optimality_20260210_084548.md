---
ver: rpa2
title: 'Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality'
arxiv_id: '2510.18982'
source_url: https://arxiv.org/abs/2510.18982
tags:
- sub-optimality
- coverage
- sampling
- transport
- verifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes test-time scaling through the lens of optimal
  transport, framing verifiable test-time scaling as a sampling problem where the
  goal is to transport a generator''s proposal distribution to an optimal target distribution
  using a potentially imperfect verifier. The study reveals three distinct regimes
  in the sub-optimality-coverage curve: transport (sub-optimality increases with coverage),
  policy improvement (sub-optimality decreases with coverage depending on verifier''s
  Youden''s index), and saturation (sub-optimality plateaus).'
---

# Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality

## Quick Facts
- arXiv ID: 2510.18982
- Source URL: https://arxiv.org/abs/2510.18982
- Reference count: 40
- Key outcome: Test-time scaling performance depends on generator coverage, verifier accuracy (Youden's index), and sampling algorithm choice, with three distinct regimes in the sub-optimality-coverage curve

## Executive Summary
This paper analyzes test-time scaling through the lens of optimal transport, framing verifiable test-time scaling as a sampling problem where the goal is to transport a generator's proposal distribution to an optimal target distribution using a potentially imperfect verifier. The study reveals three distinct regimes in the sub-optimality-coverage curve: transport (sub-optimality increases with coverage), policy improvement (sub-optimality decreases with coverage depending on verifier's Youden's index), and saturation (sub-optimality plateaus). Two classes of sampling algorithms - sequential and batched - are analyzed, with theoretical results showing that SRS and SMC achieve the same computational complexity and sub-optimality, while AiC violates coverage constraints in low-coverage regimes. Empirical results across Qwen, Llama, and Gemma models confirm the theoretical predictions, demonstrating that rejection sampling-type algorithms are advantageous under low coverage while best-of-N approaches excel under liberal coverage. The work provides a unified framework for understanding how generator coverage, verifier accuracy, and sampling algorithms jointly determine test-time scaling performance.

## Method Summary
The framework frames test-time verification as an optimal transport problem between proposal distribution μ and target distribution ν*. Coverage constraints are imposed via χ²-divergence bounds, limiting how far optimal policies can deviate from reference. Four sampling algorithms are analyzed: Acceptance-in-Context (AiC), Sequential Rejection Sampling (SRS), Sequential Maximal Coupling (SMC), Best-of-N (BoN), and Batched Rejection Sampling (BRS). The analysis decomposes sub-optimality into optimal transport cost (OTC) plus policy improvement (PI), revealing three coverage regimes. Sequential algorithms (SRS, SMC) achieve identical computational complexity despite different derivations, while batched algorithms (BoN, BRS) show complementary strengths across coverage regimes.

## Key Results
- The sub-optimality-coverage curve exhibits three distinct regimes: transport (OTC dominates), policy improvement (PI reduces sub-optimality when J > 0), and saturation
- SRS and SMC achieve identical computational complexity and sub-optimality despite different derivations
- AiC violates coverage constraints when β < 1/s_ver, limiting its applicability to high-coverage regimes
- BoN outperforms BRS under liberal coverage (large β), while BRS excels under conservative coverage (small β)
- Empirical validation across Qwen, Llama, and Gemma models confirms theoretical predictions with consistent three-regime behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-optimality decomposes into optimal transport cost (OTC) plus policy improvement, yielding three distinct coverage regimes.
- Mechanism: OTC captures intrinsic difficulty of moving from reference distribution μ to target ν*. When coverage constraint β is tight, OTC dominates (transport regime). As β relaxes, a sufficiently accurate verifier (positive Youden's index J = TPR−FPR) enables policy improvement (PI regime). Eventually both terms saturate (saturation regime).
- Core assumption: Verifier reward is binary; optimal policy lies within χ²-ball of radius β−1 around reference policy.
- Evidence anchors:
  - [abstract] "uncovers that the sub-optimality–coverage curve exhibits three regimes... transport regime... policy improvement regime... saturation regime"
  - [Section 3.1, Theorem 3.6] "SubOpt(A) = OTC(β)·(1−α_k J)" with α_k varying by regime
  - [corpus] Dorner et al. (ROC-n-reroll) studies verifier imperfection but lacks unified coverage–ROC decomposition
- Break condition: If J ≤ 0 (verifier no better than random), policy improvement regime disappears; sub-optimality never decreases with coverage.

### Mechanism 2
- Claim: Sequential Rejection Sampling (SRS) and Sequential Maximal Coupling (SMC) achieve identical computational complexity despite different derivations.
- Mechanism: Both algorithms require expected proposals E[τ] = (1∧m_β(s_ver))/s_ver. SMC's potential efficiency gain from optimal transport is negated by inability to access residual measure directly—must instead resample until verification passes.
- Core assumption: The mass s_ver = μ(Ŝ) is known or estimable; envelope M is correctly specified.
- Evidence anchors:
  - [Section 3.1, Theorem 3.5] "for both algorithms A∈{SRS,SMC}, the computational complexity is identical"
  - [Figure 4] empirical validation showing overlapping complexity curves for SRS and SMC
  - [corpus] Related work lacks direct comparison of sequential methods under coverage constraints
- Break condition: If s_ver is severely mis-specified, SRS may violate coverage; SMC degrades differently (see sensitivity analysis in Figure 5).

### Mechanism 3
- Claim: Batched Rejection Sampling (BRS) outperforms Best-of-N (BoN) in low-coverage regimes; BoN excels under liberal coverage.
- Mechanism: BoN violates coverage constraints when β < s_r*(1−s_r*), limiting admissible batch size N_max. BRS satisfies coverage for all N, with sub-optimality decaying exponentially as OTC(β)·(1−1/M)^N. Under large β, BoN's N can grow unboundedly, achieving vanishing sub-optimality.
- Core assumption: Accurate verifier for batched analysis (extended to approximate verifiers in Appendix O).
- Evidence anchors:
  - [Section 3.2, Theorem 3.7] "ν_BoN satisfies the coverage constraint only if N ≤ ⌊N_max⌋"
  - [Section 3.2, Theorem 3.10] "SubOpt(BRS) = OTC(β)·(1−1/M)^N"
  - [corpus] Setlur et al. argue verifier-based methods outperform verifier-free, but don't compare batched algorithms across coverage regimes
- Break condition: When β ≥ (1−s_r*)/s_r*, N_max → ∞ and BoN becomes unconstrained; BRS still requires proper envelope specification.

## Foundational Learning

- Concept: **Optimal Transport with Hamming Cost**
  - Why needed here: Frames rejection sampling as minimizing transport cost between proposal and target distributions; OTC = total variation distance.
  - Quick check question: Can you explain why Hamming cost (probability of y≠z) naturally captures rejection probability?

- Concept: **χ²-Divergence Coverage Constraint**
  - Why needed here: Constrains how far optimal policy can deviate from reference; defines feasible policy set Π(β|x).
  - Quick check question: How does χ² constraint relate to the likelihood ratio bound E[π(Y)/π_ref(Y)] ≤ β?

- Concept: **Youden's Index (J = TPR − FPR)**
  - Why needed here: Single metric capturing verifier quality; determines whether policy improvement regime exists.
  - Quick check question: If a verifier has TPR=0.7 and FPR=0.3, what is J? Does positive J guarantee sub-optimality reduction?

## Architecture Onboarding

- Component map:
  Generator (π_ref) → Proposals (Y₁, Y₂, ...) → Verifier (r̂/Ŝ) → Acceptance Logic → Output

- Critical path:
  1. Estimate s_ver = μ(Ŝ) from validation data (required for SRS/SMC envelope M)
  2. Compute m_β(s) = s + √(s(1−s)(β−1)) for acceptance thresholds
  3. Run sampling algorithm; track acceptance rate vs. theoretical prediction

- Design tradeoffs:
  - **Coverage β**: Tighter β → lower compute but higher sub-optimality in transport regime
  - **Sequential vs. Batched**: Sequential adapts to acceptance; batched exploits parallelism but may hit N_max
  - **Verifier threshold γ**: Controls TPR/FPR tradeoff; higher γ reduces FPR but may miss correct responses
  - **s_ver estimation**: Mis-estimation affects SRS/SMC asymmetrically (Figure 5 shows crossover behavior)

- Failure signatures:
  - **AiC constraint violation**: Sub-optimality unexpectedly low but χ²(μ‖ν_AiC) > β−1 (Theorem 3.3)
  - **BoN batch overflow**: Using N > N_max causes coverage violation
  - **SMC sensitivity**: Under-estimating s causes SMC to underperform SRS; over-estimating reverses this

- First 3 experiments:
  1. **Validate three-regime curve**: Sweep β across [1, 1/s_r*∨1/s_ver] range; plot sub-optimality vs. β; verify transport (√β growth), PI (bend downward), saturation (plateau) regimes using Qwen/Llama models on GSM8K
  2. **Compare SRS vs. SMC under s mis-specification**: Ablate assumed s ∈ {0.1, 0.3, 0.5, 0.7, 1.0}; confirm crossover at true s_ver (replicate Figure 5 right panel)
  3. **BRS vs. BoN across coverage regimes**: Fix N ∈ {2, 4, 8, 16}; sweep β; confirm BRS advantage when β < (1−s_r*)/s_r* and BoN advantage when β large (replicate Figure 9 vs. Figure 10)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework be extended to general reward models for inference-time alignment rather than just binary verifiable rewards?
  - Basis: [explicit] Section 5 states "moving beyond verifiable rewards toward general reward models for inference-time alignment is an important next step."
  - Why unresolved: The current theoretical derivations rely on the binary structure of the verifier $r(x,y)$.
  - What evidence would resolve it: A derivation of the sub-optimality-coverage trade-off for continuous rewards.

- **Open Question 2**: How does the analysis change when using difference-based coverage constraints instead of the current ratio-based ($\chi^2$) constraints?
  - Basis: [explicit] Section 5 notes "extending from ratio-based to difference-based coverage remains unexplored."
  - Why unresolved: The paper's specific three-regime geometry is derived using $\chi^2$-divergence.
  - What evidence would resolve it: Theoretical results characterizing the optimal transport cost and sub-optimality under $\ell_\infty$ or Total Variation constraints.

- **Open Question 3**: How can an algorithm optimally balance exploration and exploitation when the target-to-proposal likelihood ratio is unknown?
  - Basis: [explicit] Section 5 highlights a "fundamental open problem in sampling" regarding unknown likelihood ratios and conjectures a necessary balance between estimation and acceptance decisions.
  - Why unresolved: Current algorithms assume access to the ground truth mass $s_{ver}$ or a known density ratio.
  - What evidence would resolve it: An algorithm with provable regret bounds that jointly estimates the likelihood ratio while sampling.

## Limitations

- **Verifier imperfection generalization**: The binary TPR/FPR model may not capture complex real-world verifier failure modes or context-dependent accuracy
- **Sampling algorithm assumptions**: Theoretical results assume known proposal mass s_ver and proper envelope specification, but practical estimation introduces uncertainty
- **Coverage constraint trade-offs**: The framework treats coverage β as a tunable hyperparameter but doesn't address multi-objective considerations like latency, cost, or downstream evaluation metrics

## Confidence

**High Confidence** - Theoretical framework: The optimal transport decomposition, three-regime characterization, and computational complexity results are mathematically rigorous with clear proofs. The framework correctly identifies when policy improvement is possible (J > 0) versus transport-limited regimes.

**Medium Confidence** - Algorithm comparisons: While the theoretical analysis shows SRS and SMC have identical complexity, and BRS vs BoN comparisons are clear under ideal assumptions, empirical validation is limited to a specific model class and task. Results may not generalize to other architectures or problem domains.

**Low Confidence** - Verifier modeling: The assumption that verifiers can be fully characterized by TPR/FPR on fixed proposal sets may break down for more sophisticated verification approaches or when verifier accuracy varies with input difficulty.

## Next Checks

1. **Cross-domain verification dynamics**: Apply the framework to non-arithmetic tasks (e.g., commonsense reasoning, creative writing) to test whether the three-regime structure holds when optimal policies differ substantially from reference distributions.

2. **Finite-sample estimator performance**: Conduct a systematic study of s_ver estimation error under different sample sizes and distribution shifts. Quantify how estimator uncertainty propagates to algorithm performance, particularly the SRS/SMC crossover behavior.

3. **Multi-objective coverage optimization**: Extend the framework to handle competing constraints beyond coverage (e.g., computational budget, response diversity, downstream task performance). Develop practical guidelines for selecting β in real-world deployments.