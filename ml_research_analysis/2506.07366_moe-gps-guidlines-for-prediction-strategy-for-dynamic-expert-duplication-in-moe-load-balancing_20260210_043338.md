---
ver: rpa2
title: 'MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication
  in MoE Load Balancing'
arxiv_id: '2506.07366'
source_url: https://arxiv.org/abs/2506.07366
tags:
- prediction
- expert
- latency
- overhead
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses load imbalance in multi-GPU Mixture-of-Experts
  (MoE) inference caused by skewed token-to-expert distributions. The authors propose
  MoE-GPS, a framework that guides the selection of optimal prediction strategies
  for dynamic expert duplication to minimize end-to-end inference latency.
---

# MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing

## Quick Facts
- arXiv ID: 2506.07366
- Source URL: https://arxiv.org/abs/2506.07366
- Reference count: 40
- One-line primary result: Distribution-Only Prediction improves end-to-end inference performance by more than 23% compared to Token-to-Expert Prediction on Mixtral 8×7B MMLU dataset.

## Executive Summary
This paper addresses load imbalance in multi-GPU Mixture-of-Experts (MoE) inference caused by skewed token-to-expert distributions. The authors propose MoE-GPS, a framework that guides the selection of optimal prediction strategies for dynamic expert duplication to minimize end-to-end inference latency. The key insight is that Distribution-Only Prediction, which predicts only overall expert usage without token-level routing, offers a lightweight alternative to traditional Token-to-Expert Prediction. On the Mixtral 8×7B MMLU dataset, Distribution-Only Prediction improves end-to-end inference performance by more than 23% compared to Token-to-Expert Prediction, especially when communication is not the bottleneck.

## Method Summary
The method evaluates two prediction strategies: Distribution-Only Prediction using multinomial MLE on expert counts, and Token-to-Expert Prediction using probability-based, conditional probability, FFN, or LSTM models. Predictors are trained on token embedding–expert label pairs with cross-entropy loss. Expert duplication is performed via iterative greedy rebalancing (Algorithm 1). Performance is simulated using an extended LLMCompass framework that models runtime with skewness, accuracy, overhead, and interconnect type. The framework quantifies trade-offs between prediction accuracy, overhead, and system performance to guide optimal predictor design.

## Key Results
- Distribution-Only Prediction achieves 23% speedup compared to best Token-to-Expert configuration on Mixtral 8×7B MMLU dataset
- Prediction accuracy and overhead exhibit a U-shaped relationship with end-to-end performance
- Token-to-Expert Prediction becomes more favorable under high skewness and low-bandwidth interconnects like PCIe

## Why This Works (Mechanism)

### Mechanism 1
Distribution-Only Prediction outperforms Token-to-Expert Prediction when communication is not the bottleneck and load imbalance is moderate. By estimating only aggregate token distribution across experts (not exact token-to-expert mappings), the predictor achieves sufficient accuracy for compute load balancing while incurring near-zero runtime overhead. Expert duplication uses these estimates to replicate popular experts across GPUs, equalizing per-GPU token counts without requiring per-token routing decisions. Core assumption: Expert selection follows a multinomial distribution where historical token-to-expert frequencies predict future distribution patterns reasonably well (Assumption: i.i.d. token behavior across batches). Evidence anchors: [abstract] "Distribution-Only Prediction improves end-to-end inference performance by more than 23% compared with Token-to-Expert Prediction"; [Section 4] "With skewness = 1.4... Distribution-Only Prediction achieves 23% speedup compared to the best configuration of Token-to-Expert Prediction". Break condition: When prediction error rate becomes too high (e.g., with high skewness datasets like SST2 at 16% error), distribution estimation degrades, reducing load balancing effectiveness.

### Mechanism 2
Token-to-Expert Prediction becomes favorable when communication costs dominate and skewness is high. Exact token-level predictions enable direct routing to the GPU hosting the predicted expert, bypassing the scatter phase entirely. This eliminates communication overhead from the All-to-All shuffle, which becomes critical under low-bandwidth interconnects (e.g., PCIe vs. NVLink). Core assumption: Higher accuracy predictors can be trained at acceptable computational cost, and the saved communication time outweighs prediction overhead. Evidence anchors: [abstract] "exact token-level prediction optimizes both computation and communication at the cost of higher overhead, which becomes more favorable when communication cost dominates"; [Section 4, Figure 7] "Token-to-Expert Prediction becomes more effective as skewness increases... particularly beneficial under low-bandwidth interconnects like PCIe". Break condition: When prediction overhead exceeds the communication savings (typically at low skewness where prediction accuracy is harder to achieve), overall performance degrades.

### Mechanism 3
Prediction accuracy and overhead exhibit a U-shaped relationship with end-to-end performance—higher accuracy improves load balancing but increases overhead, creating an optimal intermediate accuracy. Better predictions yield better expert placement, reducing FFN compute time and communication bottlenecks. However, achieving higher accuracy requires more complex models (e.g., neural networks vs. simple probability models), which increase prediction latency and offset the gains. Core assumption: The relationship between accuracy and overhead can be modeled (fitted with exponential and polynomial functions), and the optimum is stationary for a given configuration. Evidence anchors: [Section 3.2.2, Figure 4] "Each curve exhibits a U-shape: higher accuracy improves load balancing... but also increases overhead. The optimal configuration is the one with the lowest total latency"; [Section 4] "the best configuration of Token-to-Expert Prediction (the bottom of the U-shape)". Break condition: If the accuracy-overhead relationship does not hold (e.g., with fundamentally different predictor architectures), the U-shape may not appear, and the optimal point may shift unpredictably.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Routing and Expert Parallelism**
  - **Why needed here:** Understanding how tokens are routed to specific experts and how experts are distributed across GPUs is essential to grasp why load imbalance occurs and how prediction strategies mitigate it.
  - **Quick check question:** If Expert 1 receives 75% of tokens on a 4-GPU system with each expert on one GPU, what is the skewness value, and which component's runtime is most affected? (Answer: skewness = 3; FFN compute and communication on GPU 1 dominate.)

- **Concept: Load Imbalance Quantification and Skewness**
  - **Why needed here:** Skewness directly determines the severity of load imbalance and influences which prediction strategy is optimal.
  - **Quick check question:** A workload has 1000 tokens, 4 experts. Expert 1 receives 500 tokens. What is the skewness? (Answer: 500 / (1000/4) = 2.0.)

- **Concept: Prediction Overhead vs. Load Balancing Benefit Tradeoff**
  - **Why needed here:** The core contribution of the paper is quantifying this tradeoff and showing that lighter predictors can outperform heavier ones depending on system constraints.
  - **Quick check question:** If a predictor achieves 95% accuracy but adds 20% runtime overhead, while a simpler predictor achieves 85% accuracy with 2% overhead, which is likely better when communication is not a bottleneck? (Answer: The simpler predictor, as per Distribution-Only Prediction insights.)

## Architecture Onboarding

- **Component map:** Input tokens -> Ring Reduce-Scatter (after Attention) -> Predictor -> Expert Duplication Engine -> Routing & All-to-All Shuffle -> FFN Layers -> LLMCompass Simulator
- **Critical path:** Predictor inference on batched inputs (before Attention) -> Expert duplication decision (compute per-GPU load, replicate overloaded experts) -> Token routing (either direct for Token-to-Expert or via scatter for Distribution-Only) -> FFN compute (now balanced across GPUs) -> All-to-All shuffle (post-FFN communication)
- **Design tradeoffs:** Distribution-Only vs. Token-to-Expert: Distribution-Only has near-zero overhead but no communication savings; Token-to-Expert saves communication but incurs high overhead. Prediction accuracy vs. overhead: Higher accuracy requires complex models (LSTMs, FFNs), increasing latency; simpler models (probability-based) have lower accuracy but minimal overhead. Interconnect bandwidth: NVLink (high bandwidth) favors Distribution-Only; PCIe (low bandwidth) may favor Token-to-Expert when skewness is high. Skewness level: Higher skewness makes prediction easier (higher accuracy at lower cost), shifting optimal strategy toward Token-to-Expert.
- **Failure signatures:** High skewness with Distribution-Only: Error rates increase (e.g., 16% for SST2), reducing load balancing effectiveness and potentially degrading performance vs. no prediction. Low skewness with Token-to-Expert: Overhead dominates (complex models needed for accuracy), causing worse performance than Distribution-Only. Long sequences with Token-to-Expert: LSTM-based predictors parallelize poorly; FFN predictors hit accuracy lower bounds. Small batch/sequence with PCIe: Communication savings from Token-to-Expert may still not offset overhead if FFN compute is short.
- **First 3 experiments:** Baseline profiling: Run MoE-GPS simulation (or real inference) on Mixtral 8×7B with MMLU dataset, batch=1, seq_len=512, 4×A100 NVLink. Measure baseline latency without prediction, breakdown by Attention, FFN, communication. Distribution-Only test: Implement multinomial MLE estimator on training split (80/20), predict per-batch expert distribution, run expert duplication per Algorithm 1. Compare end-to-end latency vs. baseline, report skewness, error rate, and FFN compute reduction. Token-to-Expert sweep: Train simple FFN predictor (128-dim hidden) for token-to-expert classification. Sweep accuracy levels (by varying model complexity or training epochs), measure prediction overhead and simulated latency. Identify U-shape optimum and compare to Distribution-Only result.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance trade-off between Distribution-Only and Token-to-Expert prediction strategies shift as sequence lengths increase significantly beyond 512 tokens?
- Basis: [inferred] Section 5 ("Long sequence lengths") notes that longer sequences introduce new tradeoffs, specifically a "lower bound on achievable accuracy" for token-level predictors and poor parallelism for LSTMs, suggesting Distribution-Only might become more favorable.
- Why unresolved: The study limited experiments to a sequence length of 512, and the scalability of the prediction overhead relative to the increased FFN computation time in long-context scenarios remains unquantified.
- What evidence would resolve it: Simulation and empirical measurements of MoE-GPS on sequence lengths of 2k, 8k, and 32k tokens.

### Open Question 2
- Question: Do the guidelines for selecting prediction strategies hold in non-fully-connected network topologies, such as Mesh, Torus, or Tree structures?
- Basis: [inferred] Section 5 ("Generality across hardware systems") explicitly states the study assumed fully connected GPUs and that different topologies are "orthogonal to our core insights" but were not modeled.
- Why unresolved: In topologies with limited bandwidth between specific nodes, the communication savings from Token-to-Expert prediction might be prioritized differently compared to the fully connected NVLink setup tested.
- What evidence would resolve it: Performance simulation results using MoE-GPS on clusters configured with Mesh or Torus interconnects.

### Open Question 3
- Question: How does the introduction of hybrid parallelism (TP+EP) for FFN layers affect the efficacy of dynamic expert duplication strategies?
- Basis: [inferred] Section 5 ("Generality across hardware systems") notes that while the study assumes EP-only FFN, "hybrid parallelism... have been proven to be useful in certain settings."
- Why unresolved: Hybrid parallelism splits expert weights across devices, complicating the duplication and load balancing mechanics assumed in the proposed EP-only duplication algorithm (Algorithm 1).
- What evidence would resolve it: An evaluation of end-to-end latency when integrating MoE-GPS into a system utilizing Tensor Parallelism for FFN layers.

## Limitations
- Simulation dependency: Core performance claims rely on LLMCompass simulation rather than direct measurement on real hardware, with unverified extensions for MoE/EP modeling.
- Dataset representativeness: Evaluation focuses on three specific datasets (MMLU, Alpaca Eval, SST2) with batch size 1 and sequence length 512, limiting generalizability.
- Predictor training details: Critical hyperparameters for neural predictors and calibration set assumptions are not specified.

## Confidence
- **High confidence**: The fundamental insight that prediction accuracy and overhead trade off against each other in a U-shaped relationship with end-to-end performance is well-supported by the theoretical framework and quantified examples.
- **Medium confidence**: The specific performance numbers (23% improvement for Distribution-Only Prediction on MMLU) are plausible given the described mechanisms, but simulation-based nature introduces uncertainty about real-world applicability.
- **Low confidence**: The generalizability of the optimal strategy selection framework across different hardware configurations, MoE architectures, and workload characteristics is asserted but not empirically validated beyond the specific experimental setup.

## Next Checks
1. **Hardware validation**: Implement the Distribution-Only and Token-to-Expert predictors on actual 4×A100 GPUs with Mixtral 8×7B, measuring end-to-end prefill latency under NVLink and PCIe interconnects. Compare against LLMCompass simulation predictions to quantify modeling accuracy.

2. **Predictor generalization test**: Train and evaluate the proposed predictors on an additional MoE model (e.g., DeepSeekMoE or a custom 8-expert configuration) and a different dataset family (e.g., code generation tasks) to assess the framework's applicability beyond the Mixtral-MMLU setup.

3. **Varying workload characterization**: Systematically vary batch size (1→32) and sequence length (512→2048) to identify at which points each prediction strategy breaks down or becomes optimal, particularly focusing on when predictor overhead dominates or when communication savings become negligible.