---
ver: rpa2
title: Trojan Cleansing with Neural Collapse
arxiv_id: '2411.12914'
source_url: https://arxiv.org/abs/2411.12914
tags:
- neural
- data
- trojan
- attacks
- cleansing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of trojan attacks on neural networks,
  which embed backdoor triggers to force specific outputs on triggered inputs. The
  authors connect trojan attacks to Neural Collapse, a phenomenon where over-parameterized
  neural networks' feature representations converge to a simple geometric structure.
---

# Trojan Cleansing with Neural Collapse

## Quick Facts
- arXiv ID: 2411.12914
- Source URL: https://arxiv.org/abs/2411.12914
- Reference count: 40
- Primary result: Novel trojan cleansing method ETF-FT achieves high clean accuracy and low attack success rate using minimal clean data

## Executive Summary
This paper addresses the problem of trojan attacks on neural networks by leveraging insights from Neural Collapse, a phenomenon where over-parameterized networks' feature representations converge to a simple geometric structure. The authors demonstrate that trojan attacks systematically disrupt this convergence, particularly affecting classifier weight symmetry. Building on this observation, they propose ETF-FT, a novel cleansing method that resets final layer weights to a simplex Equiangular Tight Frame and fine-tunes the remaining model parameters on clean data. Experimental results show ETF-FT effectively removes trojan triggers while maintaining clean data accuracy, outperforming other methods on various attacks and architectures including transformers.

## Method Summary
The ETF-FT method works by first constructing a simplex Equiangular Tight Frame (ETF) matrix based on the number of classes, then replacing the final layer's weight matrix with this ETF. The final layer weights are frozen, and the remaining model parameters are fine-tuned on a small clean dataset using AdamW optimizer with ExponentialLR scheduler. The method requires no knowledge of trigger types and works with limited clean data (1-5% of original training size). The ETF structure provides a symmetric decision boundary that forces the feature extractor to reorganize, removing trojan associations while preserving classification capability.

## Key Results
- ETF-FT reduces attack success rate to <5% while maintaining clean accuracy >88% on CIFAR-10 with 5% clean data
- The method outperforms vanilla fine-tuning and other baseline cleansing approaches across multiple datasets and architectures
- ETF-FT maintains effectiveness even with only 1% clean data, though accuracy degrades slightly
- The method is effective on both ResNet and ViT architectures, including scenarios where other methods fail

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Trojan attacks systematically weaken Neural Collapse (NC) metrics, with the strongest effect on classifier weight symmetry (NC2Norm).
- **Mechanism:** Data poisoning creates asymmetric feature distributions—the target class receives triggered samples from multiple source classes, disrupting the equinorm and equiangular convergence that characterizes NC. The target class weights develop systematically smaller norms than other classes.
- **Core assumption:** The trojan must map heterogeneous triggered inputs to a single target class, which structurally contradicts the uniform geometry of simplex ETF convergence.
- **Evidence anchors:**
  - [abstract]: "trojan attacks disrupt this convergence for a variety of datasets and architectures"
  - [section 3, p.7-8]: Figure 3a shows increasing poison rates correlate with higher NC2Norm values; "the source of this deviation is actually that the norm of the target class is consistently smaller than those of other classes"
  - [corpus]: Weak direct support; neighbor papers focus on detection rather than NC-based cleansing mechanisms.
- **Break condition:** If trojan attacks preserved NC symmetry (e.g., distributed triggers uniformly across classes), the NC-based detection and cleansing signal would disappear.

### Mechanism 2
- **Claim:** Resetting the final layer to a random simplex ETF (Equiangular Tight Frame) removes the trojan's learned weight deformation while preserving a geometry compatible with classification.
- **Mechanism:** The final layer weights are the primary locus of trojan asymmetry. By replacing W with W_ETF (Algorithm 2) and freezing it, the method: (1) destroys the trojan-specific weight configuration, and (2) imposes a maximally symmetric decision boundary structure that forces the feature extractor to reorganize.
- **Core assumption:** The trojan primarily manifests in final-layer weight distortions rather than deeply embedded feature transformations.
- **Evidence anchors:**
  - [section 4.1, p.9-10]: "first over-writing the weights of the final layer to a randomly generated simplex ETF, freezing those weights, and then fine-tuning"
  - [section 3, p.8]: "benign model shows the most uniformity in norm across both feature means and classifier weights, and as the percentage of trojaned data increases, the equinorm property weakens. This trend is particularly profound in the classifier weights."
  - [corpus]: MergeGuard (neighbor) also uses weight manipulation but via model merging rather than NC-based geometry.
- **Break condition:** If trojan triggers operated through feature-space distortions before the final layer (e.g., embedding triggers in intermediate representations), resetting only the final layer would be insufficient.

### Mechanism 3
- **Claim:** Fine-tuning with frozen ETF weights allows the feature extractor to re-learn clean class separations using minimal clean data.
- **Mechanism:** With W_ETF fixed, backpropagation can only adjust earlier layers. The ETF structure provides a stable, symmetric target for feature reorganization. This is more parameter-efficient than full fine-tuning and prevents re-learning trojan associations.
- **Core assumption:** Clean data availability (even 1-5%) is sufficient for feature reorganization when the decision boundary is pre-structured as an ETF.
- **Evidence anchors:**
  - [section 4.3, Table 2]: ETF-FT with 1% clean data achieves 90.24% ACC and 4.22% ASR on CIFAR-10/ResNet18 vs. vanilla fine-tuning at 90.20% ACC and 22.04% ASR
  - [section 4.3.1, p.13]: ETF-FT maintains 87.99% ACC with corrupted data vs. 81.66% for vanilla FT
  - [corpus]: Limited comparison; neighbor papers do not report NC-based fine-tuning baselines.
- **Break condition:** If clean data is highly imbalanced or from a shifted distribution that doesn't represent all classes, the ETF-imposed symmetry may not find appropriate feature representations.

## Foundational Learning

- **Concept: Neural Collapse (NC)**
  - **Why needed here:** The entire cleansing mechanism relies on understanding NC's four metrics and how trojans disrupt them. Without this, ETF-FT appears arbitrary.
  - **Quick check question:** On a trained classifier, if you compute the covariance of within-class features vs. between-class means, which should dominate as NC progresses?

- **Concept: Simplex Equiangular Tight Frame (ETF)**
  - **Why needed here:** ETF is the geometric target of NC and the core construct used in the cleansing algorithm. Understanding its equinorm and equiangular properties is essential.
  - **Quick check question:** For K classes in m-dimensional space, what is the maximum pairwise cosine similarity achievable for a set of K unit vectors that form a simplex ETF?

- **Concept: Data Poisoning and Trigger Functions**
  - **Why needed here:** The threat model defines the attack surface; understanding how poisoning proportion δ affects NC metrics guides expectations about cleansing difficulty.
  - **Quick check question:** If δ=0.10 and trigger function κ(x) overlays a patch, what fraction of training samples in non-target classes are modified and relabeled?

## Architecture Onboarding

- **Component map:** Input -> ETF Construction -> Weight Replacement -> Frozen Fine-tuning -> Cleansed Model
- **Critical path:**
  1. Verify model architecture has a separable final linear layer (W, b decomposition)
  2. Extract feature dimension m and number of classes K
  3. Construct valid ETF (ensure m ≥ K for full-rank simplex)
  4. Replace weights, verify Frobenius norm scaling
  5. Configure optimizer (AdamW: lr=0.0001 for ResNet, 0.001 for ViT) and scheduler (gamma=0.95 ResNet, 0.85 ViT)
  6. Fine-tune with frozen final layer for same epochs as original training or until convergence

- **Design tradeoffs:**
  - **ETF randomness:** Random initialization works well, but optimal ETF selection is unexplored (noted in Future Work)
  - **Data requirements:** Works with 1% clean data but accuracy degrades; 5% is more robust
  - **Architecture generality:** Outperforms baselines on ViT where other methods fail, but may have ~4% ACC drop (Table 2b); worth accepting for security
  - **Overtraining dependency:** Paper shows ETF-FT works without TPT overtraining (Table 8), though NC is strongest with it

- **Failure signatures:**
  - High ASR post-cleansing (>20%): Likely insufficient fine-tuning data or extreme class imbalance; increase X_clean or verify data quality
  - ACC drop >10%: May indicate m < K (ETF underconstrained) or learning rate too high; reduce lr or check feature dimensions
  - Method fails to converge: Check that final layer is actually frozen; verify gradient flow to earlier layers

- **First 3 experiments:**
  1. **Baseline validation:** Take a clean (unpoisoned) ResNet18 on CIFAR-10, apply ETF-FT with 5% held-out clean data. Verify ACC remains within 2% of original (should be ~91-92%). This confirms the ETF structure doesn't degrade benign models.
  2. **Trojan removal test:** Train ResNet18 on CIFAR-10 with BadNets poisoning (δ=0.10, patch trigger). Verify ASR >90%. Apply ETF-FT with 5% clean data. Target: ASR <5%, ACC >88%.
  3. **Data scarcity stress test:** Repeat experiment 2 with only 1% clean data. Compare ASR and ACC to vanilla fine-tuning baseline. Expect ETF-FT to maintain lower ASR than FT (which should show high variance and potential ASR spikes).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a formal theoretical framework be established to prove the connection between trojan attacks and the disruption of Neural Collapse?
- Basis in paper: [explicit] The authors explicitly state in Section 5.1 that "developing a theoretical framework" is an important area of future work to support their experimental evidence.
- Why unresolved: The current paper relies entirely on experimental evidence to demonstrate that trojans weaken Neural Collapse metrics, lacking a mathematical proof.
- What evidence would resolve it: A theoretical derivation showing how trojan optimization objectives inherently conflict with the symmetric convergence criteria of Neural Collapse.

### Open Question 2
- Question: Can the observed disruption of Neural Collapse be utilized to build a robust trojan detection method?
- Basis in paper: [explicit] Section 5.1 lists "building a trojan detection method" as a key area for future work derived from the insights in this paper.
- Why unresolved: The current work focuses on mitigation (cleansing) rather than detection, despite the observation that trojaned models exhibit distinct NC metric signatures.
- What evidence would resolve it: A detection algorithm that uses NC metrics (e.g., NC2Norm, NC3) to successfully classify models as benign or trojaned without prior knowledge of the trigger.

### Open Question 3
- Question: How does the specific initialization of the simplex ETF matrix impact the performance of the ETF-FT cleansing method?
- Basis in paper: [explicit] Section 5.1 notes that the authors used a randomly generated ETF and suggests "characterizing the impact of the initial ETF selection as well as optimizing this" for future study.
- Why unresolved: The paper does not ablate or optimize the construction of the $W_{ETF}$ matrix, leaving its influence on clean accuracy and attack success rate unexplored.
- What evidence would resolve it: A comparative study of ETF-FT performance using various ETF initializations (e.g., orthogonal vs. random) or learned geometric structures.

## Limitations

- The connection between trojan attacks and Neural Collapse disruption is primarily correlational rather than mechanistically established
- The method's effectiveness on extreme class imbalance scenarios or with non-standard trigger functions remains untested
- The paper does not explore how trojan mechanisms that operate through feature-space transformations (rather than final layer weights) would affect the method's performance

## Confidence

- **High Confidence:** ETF-FT's ability to reduce ASR while maintaining clean accuracy across multiple datasets and architectures
- **Medium Confidence:** The claim that trojan attacks systematically weaken NC metrics (supported by correlation but lacking theoretical proof)
- **Medium Confidence:** The assertion that ETF structure is universally beneficial for feature reorganization (empirically supported but theoretically under-explained)

## Next Checks

1. **NC-Mechanism Verification:** Design an experiment where trojan triggers are distributed uniformly across all classes rather than concentrated on one target. Test whether ETF-FT still shows the same ASR reduction benefit, which would validate or challenge the core NC-disruption mechanism.

2. **Architecture Stress Test:** Apply ETF-FT to transformer-based architectures with different tokenization strategies (e.g., Swin Transformer, DeiT) and evaluate whether the method maintains its performance advantage over vanilla fine-tuning.

3. **Trigger Diversity Test:** Implement triggers that operate through feature-space transformations (e.g., input-dependent perturbations learned via generative models) rather than simple overlay patches. Evaluate whether ETF-FT remains effective when trojan mechanisms extend beyond the final layer.