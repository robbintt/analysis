---
ver: rpa2
title: 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency
  at Matched Compute'
arxiv_id: '2511.02309'
source_url: https://arxiv.org/abs/2511.02309
tags:
- reasoning
- sequential
- parallel
- across
- chains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing parallel self-consistency
  paradigm for test-time scaling in LLM reasoning. The authors systematically compare
  sequential iterative refinement with parallel independent chains under matched compute
  budgets across five state-of-the-art models and three reasoning benchmarks.
---

# The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute

## Quick Facts
- **arXiv ID:** 2511.02309
- **Source URL:** https://arxiv.org/abs/2511.02309
- **Reference count:** 32
- **Primary result:** Sequential reasoning with inverse-entropy voting outperforms parallel self-consistency across multiple models and benchmarks at matched compute

## Executive Summary
This paper challenges the dominance of parallel self-consistency for test-time scaling in LLM reasoning. Through systematic empirical comparisons across five state-of-the-art models and three reasoning benchmarks, the authors demonstrate that sequential iterative refinement consistently outperforms parallel independent chains under matched compute budgets. Sequential reasoning achieved accuracy gains up to 46.7% and outperformed parallel approaches in 95.6% of configurations. The paper introduces inverse-entropy weighted voting, which leverages Shannon entropy from token-level logprobs to weight answers by model confidence, establishing it as the universally superior aggregation method across both sequential and parallel configurations.

## Method Summary
The study systematically compares sequential iterative refinement with parallel independent chains under matched compute budgets. Sequential reasoning processes answers iteratively, allowing error correction and context accumulation, while parallel approaches sample independent chains simultaneously. The authors introduce inverse-entropy weighted voting, which uses Shannon entropy from token-level logprobs to weight answers by model confidence. Experiments were conducted across five state-of-the-art models and three reasoning benchmarks, with efficiency analysis identifying optimal chain configurations and ablation studies on creative tasks to explore complementary advantages between approaches.

## Key Results
- Sequential reasoning outperformed parallel approaches in 95.6% of configurations with accuracy gains up to 46.7%
- Inverse-entropy weighted voting achieved optimal performance in 97% of sequential configurations and 100% of parallel configurations
- 6-chain configurations were identified as optimal, achieving 13.8 accuracy points per 1K tokens
- Parallel approaches showed greater semantic diversity while sequential methods demonstrated superior lexical diversity in creative tasks

## Why This Works (Mechanism)
Sequential reasoning succeeds because iterative refinement enables error correction and context accumulation that independent parallel sampling cannot achieve. Each iteration builds upon previous reasoning, allowing the model to identify and correct mistakes while incorporating new information. The inverse-entropy voting mechanism works by weighting answers based on the model's confidence at the token level, with lower entropy (higher confidence) answers receiving greater weight. This approach effectively filters out uncertain or contradictory outputs, selecting answers that the model is most confident about during generation.

## Foundational Learning
- **Test-time scaling**: Inference-time computation strategies that improve reasoning without additional training; needed to maximize existing model capabilities
- **Shannon entropy**: Information-theoretic measure of uncertainty in probability distributions; quick check: lower entropy indicates higher confidence in predictions
- **Logprob distributions**: Token-level probability scores from language models; quick check: examine how entropy correlates with prediction accuracy
- **Compute-matched comparison**: Evaluating methods under equal computational budgets; quick check: verify token counts and processing steps are equivalent
- **Semantic vs lexical diversity**: Different measures of output variation; quick check: understand how diversity metrics capture different aspects of creativity
- **Chain-of-thought reasoning**: Step-by-step problem solving approach; quick check: assess how intermediate reasoning steps affect final accuracy

## Architecture Onboarding
- **Component map:** Input Prompt → Model Chain(s) → Answer Generation → Entropy Calculation → Voting Aggregation → Final Answer
- **Critical path:** Reasoning chains → Entropy-weighted voting → Answer selection
- **Design tradeoffs:** Sequential vs parallel computation (latency vs accuracy), chain count optimization (efficiency vs coverage), entropy weighting (confidence calibration vs simplicity)
- **Failure signatures:** Poor entropy calibration leading to incorrect weightings, suboptimal chain count causing inefficiency, task-specific limitations in sequential reasoning
- **First experiments:** 1) Test inverse-entropy voting on a simple reasoning benchmark, 2) Compare 3-chain vs 6-chain sequential configurations, 3) Evaluate semantic diversity metrics on creative outputs

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Study primarily focused on reasoning tasks with only brief ablation on creative tasks, leaving uncertainty about generalization to code generation, fact retrieval, and open-ended generation
- Optimal chain count of 6 may be model-dependent and not universal across different architectures or reasoning task complexities
- Inverse-entropy weighting relies on token-level logprob distributions that may not accurately reflect true model confidence for models with calibration issues
- Complementary advantages between parallel and sequential approaches require deeper investigation for practical hybrid implementations

## Confidence
- **Sequential superiority over parallel**: High confidence (supported by extensive empirical comparisons across multiple models and benchmarks)
- **Inverse-entropy voting effectiveness**: High confidence (universal superiority claims supported by comprehensive evaluation)
- **Optimal chain count**: Medium confidence (may be model-dependent)
- **Generalization to other task types**: Low confidence (limited evaluation beyond reasoning tasks)

## Next Checks
1. Systematic evaluation of sequential versus parallel approaches across diverse task categories including code generation and fact verification
2. Testing inverse-entropy voting method on models with known calibration issues to assess robustness
3. Conducting efficiency analysis across different model scales and reasoning task complexities to identify whether 6-chain optimal configuration is universal or context-dependent