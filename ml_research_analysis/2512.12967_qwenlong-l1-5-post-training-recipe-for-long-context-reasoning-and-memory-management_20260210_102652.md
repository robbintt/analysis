---
ver: rpa2
title: 'QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory
  Management'
arxiv_id: '2512.12967'
source_url: https://arxiv.org/abs/2512.12967
tags:
- reasoning
- memory
- long-context
- training
- qwenlong-l1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QwenLong-L1.5 introduces a post-training recipe for long-context
  reasoning and memory management, addressing the gap in mature post-training systems
  for long-context reasoning. The key innovations include a long-context data synthesis
  pipeline that generates multi-hop reasoning tasks requiring grounding over globally
  distributed evidence, stabilized reinforcement learning with task-balanced sampling
  and adaptive entropy-controlled policy optimization to mitigate training instability,
  and a memory-augmented architecture for ultra-long contexts using iterative memory-based
  processing.
---

# QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management

## Quick Facts
- arXiv ID: 2512.12967
- Source URL: https://arxiv.org/abs/2512.12967
- Reference count: 40
- Key outcome: Achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing baseline by 9.90 points on average

## Executive Summary
QwenLong-L1.5 introduces a post-training recipe specifically designed for long-context reasoning and memory management, addressing the gap in mature post-training systems for this domain. The system achieves performance comparable to leading models like GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks. By implementing a memory-augmented architecture and stabilized reinforcement learning approach, QwenLong-L1.5 demonstrates significant improvements in both standard and ultra-long context scenarios (1M–4M tokens), with the acquired reasoning capabilities extending to general domains including scientific reasoning, tool-use agents, and extended dialogue.

## Method Summary
The method centers on three key innovations: a long-context data synthesis pipeline that generates multi-hop reasoning tasks requiring grounding over globally distributed evidence, stabilized reinforcement learning with task-balanced sampling and adaptive entropy-controlled policy optimization to mitigate training instability, and a memory-augmented architecture for ultra-long contexts using iterative memory-based processing. Built upon Qwen3-30B-A3B-Thinking, the system integrates these components through a post-training framework that enhances both reasoning depth and memory management capabilities across extended contexts.

## Key Results
- Achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks
- Surpasses baseline by 9.90 points on average across long-context reasoning tasks
- Memory-agent framework yields 9.48-point gain over agent baseline on ultra-long tasks (1M–4M tokens)

## Why This Works (Mechanism)
The system's effectiveness stems from addressing three critical bottlenecks in long-context reasoning: data scarcity for training, training instability in reinforcement learning, and memory limitations for ultra-long contexts. The data synthesis pipeline creates diverse multi-hop reasoning scenarios that require the model to navigate and integrate information across distributed evidence. The stabilized RL approach maintains training stability through adaptive entropy control and balanced task sampling, preventing catastrophic forgetting and mode collapse. The memory-augmented architecture enables iterative processing of ultra-long contexts by maintaining and updating working memory, allowing the model to reason effectively even when information spans millions of tokens.

## Foundational Learning
- Long-context reasoning fundamentals: Understanding how models process and reason across extended sequences is essential for building effective long-context systems. Quick check: Can the model maintain coherence and accuracy when reasoning spans multiple non-contiguous segments?
- Reinforcement learning stabilization: Critical for preventing training collapse in complex reasoning tasks. Quick check: Does entropy control maintain sufficient exploration while preventing reward hacking?
- Memory-augmented architectures: Necessary for handling ultra-long contexts beyond transformer attention limits. Quick check: Can the memory system effectively retrieve and integrate relevant information across millions of tokens?

## Architecture Onboarding

Component map: Data Synthesis -> Stabilized RL Training -> Memory-Augmented Architecture -> Long-Context Reasoning

Critical path: The data synthesis pipeline generates training examples that feed into the stabilized RL system, which then optimizes the memory-augmented architecture for both standard and ultra-long context scenarios.

Design tradeoffs: The system balances computational efficiency with reasoning depth by using iterative memory processing rather than full-context attention, trading some latency for scalability to ultra-long contexts.

Failure signatures: Training instability without proper entropy control, memory retrieval failures in ultra-long contexts, and reasoning degradation when evidence is sparsely distributed across the context.

First experiments:
1. Test data synthesis pipeline on creating diverse multi-hop reasoning tasks with varying evidence distribution patterns
2. Evaluate stabilized RL training with different entropy control parameters on benchmark reasoning tasks
3. Benchmark memory-augmented architecture on ultra-long context retrieval and reasoning accuracy

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation primarily focuses on benchmarks that may not fully capture real-world complexity in ultra-long context scenarios
- Performance gains rely on synthetic data generation that may not perfectly reflect natural long-context reasoning patterns
- Computational overhead and latency implications of iterative memory processing remain unexplored

## Confidence

**Performance claims vs. GPT-5/Gemini-2.5-Pro:** High - based on standardized benchmarks with clear metrics
**RL stability improvements:** Medium - methodology is sound but lacks extensive hyperparameter sensitivity analysis
**Memory-augmented architecture effectiveness:** Medium - results are positive but limited to specific ultra-long context ranges

## Next Checks

1. Conduct ablation studies isolating the contributions of data synthesis, RL stabilization, and memory architecture to quantify their individual impact on performance gains
2. Test the memory-agent framework on real-world ultra-long documents (legal contracts, technical manuals) beyond synthetic benchmarks to assess practical utility
3. Measure computational overhead and inference latency of the iterative memory processing compared to baseline approaches across different context lengths