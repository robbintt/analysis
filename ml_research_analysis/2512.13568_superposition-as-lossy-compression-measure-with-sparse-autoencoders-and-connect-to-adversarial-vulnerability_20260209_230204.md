---
ver: rpa2
title: 'Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect
  to Adversarial Vulnerability'
arxiv_id: '2512.13568'
source_url: https://arxiv.org/abs/2512.13568
tags:
- features
- feature
- superposition
- networks
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled information-theoretic framework
  for measuring neural network superposition as lossy compression. The core idea measures
  effective degrees of freedom by applying Shannon entropy to sparse autoencoder activations,
  computing how many interference-free neurons would be needed to encode the observed
  feature distribution.
---

# Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability

## Quick Facts
- arXiv ID: 2512.13568
- Source URL: https://arxiv.org/abs/2512.13568
- Reference count: 26
- Core contribution: Information-theoretic framework measuring neural superposition as lossy compression via sparse autoencoder activation entropy

## Executive Summary
This paper introduces a principled information-theoretic framework for measuring neural network superposition as lossy compression. The core idea measures effective degrees of freedom by applying Shannon entropy to sparse autoencoder activations, computing how many interference-free neurons would be needed to encode the observed feature distribution. This yields a superposition metric as the ratio of effective features to actual neurons. The framework achieves r=0.94 correlation with ground truth in toy models, detects minimal superposition in algorithmic tasks (effective features approximately equal neurons), reveals systematic reduction under dropout, and captures developmental dynamics during grokking. Surprisingly, adversarial training effects depend on task complexity and network capacity: simple tasks with ample capacity enable feature expansion (abundance regime) while complex tasks or limited capacity force reduction (scarcity regime), contradicting the hypothesis that superposition universally causes vulnerability.

## Method Summary
The method applies Shannon entropy to sparse autoencoder (SAE) activation distributions to quantify neural superposition. For a layer with N neurons, activations are passed through an SAE with dictionary size D (typically 4×-8×N), producing sparse codes z. The probability distribution over features is computed as p_i = |z_i| / Σ|z_j|, and effective features F = exp(H(p)) where H(p) is Shannon entropy. The superposition ratio ψ = F/N indicates compression: ψ < 1 means the network uses fewer than N interference-free neurons to represent the observed feature distribution. This approach is validated against ground truth in toy models with known superposition patterns and applied to analyze dropout effects, grokking dynamics, and adversarial training impacts across multiple architectures.

## Key Results
- Achieves r=0.94 correlation between SAE-based superposition measure and ground truth in toy models with known superposition
- Detects minimal superposition in algorithmic tasks (effective features approximately equal neurons) compared to dense networks on multi-task parity
- Reveals systematic superposition reduction under dropout (up to 50% effective feature loss) with capacity-dependent sensitivity
- Captures developmental dynamics during grokking: effective features grow from 25% to 75% of neuron count as networks transition from memorization to generalization
- Challenges universal superposition-vulnerability link: adversarial training effects bifurcate by task complexity/capacity—abundance regime (simple tasks, ample capacity) enables feature expansion while scarcity regime (complex tasks, limited capacity) forces feature reduction

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Effective Feature Quantification
The exponential of Shannon entropy applied to SAE activation distributions measures the minimum neurons required for interference-free encoding of the observed feature distribution. SAE activations define a probability distribution over features via their relative activation magnitudes. The quantity F = e^H(p) equals the effective alphabet size—the number of equally-likely channels that would transmit identical information. When F > N (actual neurons), the network must accept interference as the price of lossy compression. This assumes SAE dictionary elements correspond one-to-one with genuine computational features that combine primarily through linear superposition with independent information contributions.

### Mechanism 2: Task Complexity / Network Capacity Bifurcation
Adversarial training's effect on superposition depends on the ratio of task demands to network capacity, not a universal relationship. In the abundance regime (simple tasks + ample capacity), networks expand effective features for robustness. In the scarcity regime (complex tasks + constrained capacity), networks prune to fewer, potentially more orthogonal features. This bifurcation is quantified by the sign of the normalized slope of feature count vs. adversarial training strength (ε). The relationship is mediated by whether the network has representational slack to elaborate defensive features.

### Mechanism 3: Dropout Forces Redundancy-Induced Feature Competition
Dropout monotonically reduces effective features by forcing each feature to occupy multiple neurons for survival, consuming capacity and leaving fewer total features. To survive random deactivation, features must distribute across neurons (redundant encoding). Under a fixed dimensional budget, this leaves room for fewer total features. Networks respond by pruning less essential features per competitive resource allocation. This assumes polysemanticity via redundancy is distinct from superposition (compression beyond ψ=1).

## Foundational Learning

- **Shannon entropy and perplexity (Hill numbers)**: The core measurement uses exp(H(p)) to count effective features. Without understanding that perplexity quantifies effective alphabet size, the metric appears arbitrary. Quick check: Given p = [0.5, 0.25, 0.25], what is exp(H(p))? (Answer: 2.83 effective outcomes)

- **Sparse autoencoders as dictionary learning**: The measurement depends entirely on SAE feature extraction. The L1 penalty creates explicit budget competition that gives activation magnitudes semantic meaning. Quick check: Why does L1 (vs L2) regularization preserve the interpretation of |z_i| as budget allocation? (Answer: L1 maintains linear connection to reconstruction contribution; L2 squares and overweights outliers)

- **Superposition vs. polysemanticity**: The paper distinguishes compression beyond ψ=1 (superposition) from multiple features per neuron via other mechanisms (polysemanticity). Confusing these leads to wrong predictions about adversarial training. Quick check: Can polysemanticity exist without superposition? (Answer: Yes—e.g., via dropout-induced redundancy where ψ < 1 but neurons are still polysemantic)

## Architecture Onboarding

- Component map: Network activations (N dims) → SAE encoder → Sparse codes z (D dims, D > N) → Aggregate |z_i| across samples → Compute p_i = |z_i| / Σ|z_j| → F = exp(H(p)), ψ = F/N

- Critical path:
  1. Collect ≥2×10^4 activation samples from the target layer (convergence analysis in Section 6.4)
  2. Train SAE with appropriate L1 (≥0.1) to avoid feature splitting; dictionary size 4×-8× layer width
  3. Compute feature probabilities from activation magnitudes (Eq. 6), not SAE weights
  4. For CNNs: aggregate across spatial positions (Eq. 18), measure across channels

- Design tradeoffs:
  - Higher L1 → fewer spurious features but risks suppressing genuine weak features
  - Larger dictionary → finer granularity but risks feature splitting; requires L1 ≥ 0.1 for convergence
  - Weight tying (W_dec = W_enc^T) → conceptual clarity at potential reconstruction cost
  - Comparative interpretation is more reliable than absolute counts due to SAE hyperparameter sensitivity

- Failure signatures:
  - Non-converging feature counts with dictionary scaling → L1 too weak (feature splitting)
  - Near-zero feature counts → L1 too strong (suppression)
  - r < 0.9 in toy model validation → SAE training issues or insufficient samples
  - F ≈ D (dictionary size) → uniform distribution suggests SAE failed to find sparse structure

- First 3 experiments:
  1. **Toy model validation**: Replicate Section 5.1—train toy models with known superposition, verify r ≥ 0.94 correlation between your SAE-based measure and the Frobenius norm baseline on W_toy.
  2. **Dictionary scaling sweep**: On multi-task sparse parity, verify that feature counts plateau with L1 ≥ 0.1 across 2×-16× dictionary expansion (Figure 4a pattern).
  3. **Layer-wise pilot on pretrained model**: Apply to Pythia-70M with existing SAEs; verify you recover the "hunchback" pattern (peak in early MLP layers) per Section 6.4 before running custom experiments.

## Open Questions the Paper Calls Out

- Can we generalize the notion of superposition to arbitrary function classes beyond linear subspaces? While the current work focuses on linear superposition, the authors suggest that the information-theoretic approach could potentially be extended to measure interference in nonlinear representations, though this remains an open theoretical challenge.

## Limitations

- The SAE-based measure assumes one-to-one correspondence between SAE features and computational features, which may break down with hierarchical representations or feature splitting
- The adversarial training bifurcation mechanism lacks direct validation and relies on correlation rather than controlled ablation studies
- Dropout findings show correlation but lack mechanistic proof isolating the redundancy mechanism from other dropout effects
- The approach is sensitive to SAE hyperparameters, making absolute superposition quantification less reliable than relative comparisons

## Confidence

- **High**: Shannon entropy as effective feature count (validated in toy models with r=0.94)
- **Medium**: Dropout monotonically reduces superposition (observed correlation, limited mechanistic proof)
- **Low**: Adversarial training effects depend on task-complexity/capacity regime (theoretical prediction, minimal direct validation)

## Next Checks

1. **SAE Feature Splitting Validation**: Systematically test whether increasing L1 regularization beyond 0.1 continues to suppress features without improving correlation with ground truth, confirming the existence of a threshold where feature splitting stops being a concern.

2. **Adversarial Training Ablation**: Run controlled experiments varying task complexity (e.g., parity vs. modular addition) and network capacity (MLP width) while keeping adversarial training strength constant to directly test whether the sign of superposition change flips as predicted by the abundance/scarcity framework.

3. **Hierarchical Representation Stress Test**: Design experiments with known hierarchical feature structures (e.g., features composed of sub-features) to test whether the SAE-based measure correctly identifies effective features or gets confused by the hierarchy, validating the assumption that SAE activations capture computational features directly.