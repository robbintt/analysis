---
ver: rpa2
title: 'GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache
  Eviction'
arxiv_id: '2509.00388'
source_url: https://arxiv.org/abs/2509.00388
tags:
- tokens
- cache
- graphkv
- nodes
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraphKV, a graph-based framework for dynamic
  key-value (KV) cache management in large language models (LLMs). The key insight
  is that conventional token eviction methods relying on static importance scores
  fail to capture evolving token dependencies, leading to redundant retention of semantically
  similar tokens.
---

# GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction

## Quick Facts
- **arXiv ID:** 2509.00388
- **Source URL:** https://arxiv.org/abs/2509.00388
- **Reference count:** 28
- **Key outcome:** GraphKV achieves up to 45.88% better performance than suboptimal methods and approximately 3% improvement over state-of-the-art methods with the same KV cache size

## Executive Summary
GraphKV introduces a graph-based framework for dynamic key-value (KV) cache management in large language models (LLMs). The paper addresses the limitation of conventional token eviction methods that rely on static importance scores, which fail to capture evolving token dependencies and lead to redundant retention of semantically similar tokens. GraphKV models tokens as graph nodes with edges representing cosine similarity between their key vectors, using a decay-signal-propagation mechanism to iteratively refine importance scores. This approach suppresses redundancy while preserving contextually significant and diverse tokens. The framework can be seamlessly integrated with existing KV cache eviction methods in a plug-and-play manner, requiring only linear additional computation.

## Method Summary
GraphKV operates by first obtaining initial importance scores from a base method (e.g., SnapKV, PyramidKV), then constructing a sparse graph where edges represent key-vector cosine similarity. The algorithm selects top-$K$ source nodes based on the initial scores and propagates a "decay signal" to their semantically similar neighbors, reducing the neighbors' importance scores proportional to similarity. This process iterates for a small number of rounds (typically T=1) to refine token importance while suppressing redundancy. The final step retains the top-scoring tokens within the cache budget. The method requires only linear additional computation (O(N × K_source)) compared to the attention computation cost, making it efficient for long-context scenarios.

## Key Results
- GraphKV achieves up to 45.88% better performance than suboptimal methods on LongBench and Needle-in-a-Haystack benchmarks
- The method provides approximately 3% improvement over state-of-the-art methods with the same KV cache size
- GraphKV reduces inference latency in some cases while improving accuracy, demonstrating practical efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-importance tokens often exhibit high semantic similarity (redundancy), and removing this redundancy improves retention efficiency.
- **Mechanism:** GraphKV constructs a sparse graph where edges represent key-vector cosine similarity. It identifies top-$K$ source nodes and propagates a "decay signal" to their nearest neighbors, reducing the neighbors' importance scores proportional to similarity.
- **Core assumption:** Key-vector cosine similarity is a valid proxy for semantic redundancy and information overlap in the context of the current query.
- **Evidence anchors:** [abstract] "suppresses redundancy while preserving diversity" via "propagating decay signals." [Section 3] Figure 2 analysis notes that "high-scoring tokens... usually contains similar semantics," and top-10 tokens show high cosine similarity. [corpus] *OBCache* and *CAOTE* corroborate the general need to move beyond simple attention magnitude.

### Mechanism 2
- **Claim:** Dynamic score refinement via iterative propagation creates a more optimal retention set than single-pass static selection.
- **Mechanism:** The mechanism updates node scores $s_j^{(t)}$ iteratively (up to $T$ rounds). In each round, the score of a token adjacent to a source node is multiplied by $(1 - \alpha \cdot e_{ij})$. This suppresses tokens that are similar to multiple high-scoring source nodes.
- **Core assumption:** A token's importance is conditional; if a token's information is already represented by a higher-scoring neighbor, its marginal value decreases.
- **Evidence anchors:** [Section 4.3] Equation 5 defines the cumulative decay over $T$ rounds. [Section 6.3] Ablation study shows performance jumps significantly from $T=0$ to $T=1$ (e.g., PyramidKV 42.51 $\to$ 44.48). [corpus] *G-KV* also utilizes global attention for decoding.

### Mechanism 3
- **Claim:** The framework acts as a plug-and-play diversifier for existing importance scoring methods (e.g., SnapKV, PyramidKV).
- **Mechanism:** GraphKV does not define its own initial importance score. Instead, it accepts $s_j^{(0)}$ from any base method, builds the graph, applies decay, and re-ranks the tokens.
- **Core assumption:** The overhead of computing cosine similarity for the sparse graph ($O(N \times K_{source})$) is negligible compared to the attention computation cost.
- **Evidence anchors:** [Section 1] "The only additional computation... shows linear complexity as $O(N \times K) \approx O(N)$." [Section 5.1] Demonstrates integration with 5 distinct baseline methods (CAKE, SnapKV, PyramidKV, H2O, KNorm).

## Foundational Learning

- **Concept: KV Cache Mechanics & Memory Constraints**
  - **Why needed here:** You must understand that KV caches scale linearly with sequence length ($O(n \times d)$) to grasp why eviction is critical and why "token budgeting" is the primary constraint.
  - **Quick check question:** If you double the context window of an LLM, what happens to the memory footprint of the KV cache, assuming constant batch size?

- **Concept: Attention Scores vs. Key Similarity**
  - **Why needed here:** GraphKV distinguishes between *importance* (often derived from attention scores, i.e., "what the query looks at") and *redundancy* (derived from Key cosine similarity, i.e., "what the keys represent"). The algorithm uses these two distinct signals.
  - **Quick check question:** Why does a high attention score for Token A not guarantee that Token A contains unique information compared to Token B?

- **Concept: Sparse Graph Construction**
  - **Why needed here:** The paper claims efficiency by building a sparse graph rather than a fully connected one. Understanding how nodes and edges are selected (Top-K source nodes) is necessary to implement the algorithm correctly.
  - **Quick check question:** In GraphKV's sparse graph, does an edge exist between every pair of tokens, or only between specific subsets?

## Architecture Onboarding

- **Component map:** Initial scores $S$ -> Graph Builder (select top-$K$ sources, compute cosine similarity) -> Propagation Engine (iterative decay) -> Selector (retain top-$N$ tokens)

- **Critical path:** The **cosine similarity computation** between the $K$ source keys and the remaining $N-K$ keys. This must be optimized (vectorized) to ensure the overhead remains $O(N)$ and does not bottleneck inference.

- **Design tradeoffs:**
  - **Source Node Count ($K$):** Too few source nodes misses redundancy clusters; too many introduces noise (paper suggests $0.3 \times$ budget).
  - **Propagation Rounds ($T$):** $T=1$ gives the best ROI; $T>1$ yields diminishing returns or minor degradation.
  - **Sparsity:** Fully connecting the graph is $O(N^2)$; the paper enforces sparsity by only connecting *from* top-scoring sources.

- **Failure signatures:**
  - **Semantic Collapse:** If decay factor $\alpha$ is too high, all neighbors of a high-score token are zeroed out, losing context.
  - **Latency Spike:** If graph construction is not sparse enough, the similarity calculation will dominate inference time.
  - **Stagnation:** If $T=0$ or similarity edges are near zero, GraphKV defaults to the baseline method with no gain.

- **First 3 experiments:**
  1. **Validation of Observation (Fig 2):** Before implementing the full algo, verify that top-k keys in your target model actually have high cosine similarity (redundancy). If keys are already diverse, GraphKV's premise breaks.
  2. **Budget Ablation (Needle-in-a-Haystack):** Run the "Needle" test with a strict cache budget (e.g., 128 tokens) to verify the "diversity" mechanism actually aids retrieval of isolated facts better than standard Top-K.
  3. **Integration Test (SnapKV + GraphKV):** Implement GraphKV as a wrapper around a strong baseline like SnapKV on LongBench. specifically checking the "Retrieval-Based Passage" task cited in the paper as a key win.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GraphKV's eviction strategy scale regarding performance and latency when applied to models significantly larger than 8B parameters (e.g., 70B+)?
- Basis: [Explicit] Section 9 states evaluations are "confined to 8B models, leaving the scalability and effectiveness of GraphKV on larger models unexplored."
- Why unresolved: Larger models exhibit different attention head distributions and capabilities, which may alter the relationship between key similarity and semantic redundancy that GraphKV relies on.
- Evidence: Benchmarking GraphKV on architectures like LLaMA-70B to verify if the 3% accuracy gain holds without inducing memory or latency bottlenecks.

### Open Question 2
- Question: Can a theoretical framework be established to determine optimal propagation hyperparameters (decay strength, rounds) without empirical tuning?
- Basis: [Explicit] Section 9 notes the propagation mechanism "depends on empirically determined hyperparameters... lacking a rigorous theoretical foundation."
- Why unresolved: The current reliance on manual tuning (e.g., setting T=3) suggests the method may not be robust across diverse context complexities without search.
- Evidence: A mathematical analysis or adaptive algorithm that dynamically sets decay rates based on input graph density, outperforming fixed hyperparameters.

### Open Question 3
- Question: Does the O(N) complexity of graph construction introduce non-trivial latency during the prefilling stage for extreme context lengths (e.g., 128k–1M tokens)?
- Basis: [Inferred] Section 4.2 claims the overhead is "ignorable" based on O(N) complexity, but Table 3 only validates latency on contexts around 10k tokens.
- Why unresolved: Linear overhead scales with sequence length; at extreme lengths, the constant factors of computing cosine similarities for graph edges might dominate inference time.
- Evidence: Profiling the end-to-end latency of the graph construction phase specifically on inputs exceeding 100k tokens.

## Limitations
- The paper does not specify critical hyperparameters (decay factor λ, neighbor count m), making exact reproduction challenging
- Performance gains are validated primarily on synthetic benchmarks and specific LongBench tasks, with limited real-world generalization evidence
- The method's efficiency claims rely on sparse graph construction but lack comprehensive latency measurements across varying sequence lengths

## Confidence

- **High Confidence:** The core observation that high-attention tokens often exhibit semantic redundancy is well-supported by Figure 2's empirical analysis. The mechanism of using cosine similarity between key vectors as a proxy for semantic overlap is theoretically sound.
- **Medium Confidence:** The integration methodology (plug-and-play with existing eviction methods) and the claim of linear computational overhead are plausible but lack direct empirical validation through scaling studies.
- **Low Confidence:** The exact values for critical hyperparameters (λ decay factor, neighbor count m) are not specified, making it difficult to assess whether reported results are reproducible or sensitive to these choices.

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary λ (decay factor) across {0.1, 0.3, 0.5, 0.7} and m (neighbor count) across {32, 64, 128} to determine the stability of performance improvements and identify the true optimal configuration.

2. **Latency Benchmarking:** Measure end-to-end inference latency with GraphKV enabled versus baseline methods across varying sequence lengths (1K, 4K, 8K tokens) to empirically verify the claimed O(N) complexity and assess real-world overhead.

3. **Generalization Test:** Evaluate GraphKV on additional long-context tasks beyond LongBench, particularly document summarization with extended contexts and multi-turn dialogue systems, to verify that the diversity gains generalize beyond the specific benchmark tasks reported.