---
ver: rpa2
title: 'Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based
  Classification'
arxiv_id: '2511.03830'
source_url: https://arxiv.org/abs/2511.03830
tags:
- prompting
- dichotomic
- text
- label
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces dichotomic prompting, a method for efficient
  multi-label text classification with LLMs that reformulates the task as a sequence
  of binary (yes/no) decisions. This approach uses independent queries for each label,
  combined with prefix caching to reduce inference costs, particularly for short texts.
---

# Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification

## Quick Facts
- arXiv ID: 2511.03830
- Source URL: https://arxiv.org/abs/2511.03830
- Authors: Mikołaj Langner; Jan Eliasz; Ewa Rudnicka; Jan Kocoń
- Reference count: 40
- Primary result: Dichotomic prompting achieves comparable or better accuracy than structured JSON prompting for multi-label classification, with improved efficiency and robustness

## Executive Summary
This paper introduces dichotomic prompting, a method that reformulates multi-label text classification as a sequence of binary (yes/no) decisions. By using independent queries for each label combined with prefix caching, the approach reduces inference costs while maintaining or improving accuracy. The method is demonstrated on affective text analysis across 24 dimensions in Polish, showing that it achieves comparable or better results than traditional structured JSON prompting approaches.

## Method Summary
Dichotomic prompting reformulates multi-label classification by asking independent binary questions for each possible label, rather than requiring the model to output all labels simultaneously. The method leverages prefix caching to store intermediate computations, reducing redundant processing during inference. The approach is validated through an LLM-to-SLM distillation pipeline where a large model (DeepSeek-V3) generates annotations that are then used to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The experiments focus on affective text analysis, examining 24 emotional and sentiment dimensions in Polish text.

## Key Results
- Dichotomic prompting achieves comparable or better accuracy than structured JSON prompting for multi-label classification
- The approach demonstrates improved efficiency through prefix caching, particularly beneficial for short texts
- Shows robustness advantages in zero-shot scenarios compared to traditional prompting methods
- Successfully generalizes across different model sizes through the LLM-to-SLM distillation pipeline

## Why This Works (Mechanism)
Dichotomic prompting works by breaking down complex multi-label classification into simpler binary decisions. Each label is treated as an independent yes/no question, allowing the model to focus on one dimension at a time rather than simultaneously predicting all possible labels. This decomposition reduces cognitive load and allows for more precise reasoning about each label. The prefix caching mechanism stores intermediate results from the binary decision process, eliminating redundant computations when similar prefixes appear across different queries. This combination of task decomposition and computational optimization results in both accuracy improvements and efficiency gains.

## Foundational Learning
- **Binary classification decomposition**: Breaking multi-label problems into independent yes/no decisions for each label; needed to understand how the task is reformulated and why it improves performance; quick check: verify that each binary decision operates independently
- **Prefix caching in transformer models**: Storing and reusing intermediate computation states; needed to understand the efficiency mechanism; quick check: confirm caching reduces token generation for repeated prefixes
- **LLM-to-SLM distillation**: Using large model outputs to train smaller models; needed to understand the experimental validation approach; quick check: verify that smaller models achieve similar performance to the large teacher model
- **Multi-label classification metrics**: Understanding precision, recall, and F1-score in multi-label contexts; needed to evaluate the claimed performance improvements; quick check: confirm appropriate metrics are used for multi-label scenarios
- **Zero-shot prompting robustness**: Evaluating model performance without task-specific fine-tuning; needed to assess the robustness claims; quick check: verify that dichotomic prompting maintains performance without examples

## Architecture Onboarding

**Component Map:** Input Text -> Dichotomic Prompt Generator -> LLM with Prefix Cache -> Binary Decision Outputs -> Label Aggregation -> Final Multi-Label Classification

**Critical Path:** The critical path flows from input text through the dichotomic prompt generator, which creates individual binary questions for each label. These questions are processed by the LLM with prefix caching enabled, producing binary decisions that are then aggregated into the final multi-label output.

**Design Tradeoffs:** The main tradeoff is between the number of queries (one per label) and the simplicity of each query. While more queries are required compared to single structured outputs, each query is simpler and benefits from caching. The prefix caching adds implementation complexity but provides significant efficiency gains.

**Failure Signatures:** Potential failures include inconsistent binary decisions for related labels, where the model might answer "yes" to mutually exclusive categories. Another failure mode could be cache misses if the prefix caching implementation doesn't properly handle variations in query formulation.

**First Experiments:**
1. Test binary decision accuracy for individual labels compared to direct multi-label output
2. Measure token savings from prefix caching across different query patterns
3. Validate that label aggregation from binary decisions produces coherent multi-label outputs

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several implicit questions remain about the approach's broader applicability and limitations.

## Limitations
- The empirical validation is limited to affective text classification in Polish, making generalization claims to other domains and languages largely untested
- The LLM-to-SLM distillation pipeline may propagate systematic errors from the large model to smaller models
- Efficiency gains from prefix caching may vary significantly across different model architectures and implementations
- Claims about robustness in zero-shot scenarios lack rigorous comparative analysis against alternative few-shot approaches

## Confidence
- **High**: The core technical contribution of reformulating multi-label classification as binary decisions and the associated implementation details
- **Medium**: The efficiency improvements and accuracy comparisons within the studied affective text classification task
- **Low**: Claims about broad generalizability across domains, languages, and the robustness advantage in zero-shot scenarios

## Next Checks
1. Test dichotomic prompting on multi-label classification tasks in entirely different domains (e.g., scientific document classification, product categorization) to assess generalizability beyond affective text analysis
2. Implement the same experiments using different base LLM architectures (e.g., LLaMA, Mistral) to verify that the observed efficiency gains are not specific to DeepSeek-V3's implementation details
3. Conduct ablation studies removing the prefix caching component to isolate its contribution to efficiency gains and verify it doesn't introduce any classification inconsistencies