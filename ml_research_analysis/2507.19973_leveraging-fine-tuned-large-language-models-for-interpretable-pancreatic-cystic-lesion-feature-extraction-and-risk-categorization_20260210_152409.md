---
ver: rpa2
title: Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic
  Lesion Feature Extraction and Risk Categorization
arxiv_id: '2507.19973'
source_url: https://arxiv.org/abs/2507.19973
tags:
- pancreatic
- risk
- report
- cyst
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed and evaluated fine-tuned open-source large\
  \ language models (LLMs) for automated extraction of pancreatic cystic lesion (PCL)\
  \ features from radiology reports and assignment of risk categories based on clinical\
  \ guidelines. Using chain-of-thought (CoT) supervision and QLoRA fine-tuning on\
  \ GPT-4o-generated labels, two open-source models\u2014LLaMA and DeepSeek\u2014\
  achieved feature extraction accuracy of 97\u201398% and risk categorization F1 scores\
  \ of 0.93\u20130.95, matching the performance of GPT-4o (97% accuracy, F1=0.97)."
---

# Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization

## Quick Facts
- arXiv ID: 2507.19973
- Source URL: https://arxiv.org/abs/2507.19973
- Reference count: 39
- Fine-tuned open-source LLMs achieved 97-98% feature extraction accuracy and 0.93-0.95 F1 scores for risk categorization, matching GPT-4o performance.

## Executive Summary
This study developed and evaluated fine-tuned open-source large language models (LLMs) for automated extraction of pancreatic cystic lesion (PCL) features from radiology reports and assignment of risk categories based on clinical guidelines. Using chain-of-thought (CoT) supervision and QLoRA fine-tuning on GPT-4o-generated labels, two open-source models—LLaMA and DeepSeek—achieved feature extraction accuracy of 97–98% and risk categorization F1 scores of 0.93–0.95, matching the performance of GPT-4o (97% accuracy, F1=0.97). A radiologist reader study confirmed that model-assigned risk categories achieved near-perfect agreement with expert interpretations (Fleiss' kappa 0.893–0.897). These findings demonstrate that fine-tuned open-source LLMs can deliver accurate, interpretable, and cost-effective phenotyping for large-scale PCL research, offering a scalable alternative to proprietary models while maintaining clinical reliability.

## Method Summary
The study fine-tuned LLaMA-3.1-8B-Instruct and DeepSeek-R1-Distill-Llama-8B models using QLoRA on 6,000 radiology reports with GPT-4o-generated chain-of-thought (CoT) labels. The pipeline extracted 9 structured PCL features (size, nodules, septations, etc.) from CT/MRI reports and mapped them to risk categories (1-3) per 2017 ACR/IAP guidelines. CoT training included observation (quoted text), reasoning, and JSON output. Models were evaluated on 281 held-out reports using exact match accuracy for feature extraction and macro-averaged F1 for risk categorization, with radiologist validation confirming clinical reliability.

## Key Results
- Fine-tuned open-source models achieved 97–98% exact match accuracy for feature extraction, matching GPT-4o performance.
- Risk categorization F1 scores of 0.93–0.95 demonstrated strong clinical reliability.
- Radiologist reader study showed near-perfect agreement (Fleiss' kappa 0.893–0.897) between model-assigned and expert risk categories.

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation via Synthetic Labels
Smaller open-source models (8B parameters) approximate proprietary model performance by fine-tuning on synthetically generated labels. GPT-4o acts as a "teacher" to generate high-quality CoT rationales and structured JSON labels, which open-source "student" models (LLaMA, DeepSeek) learn to mimic through QLoRA fine-tuning without requiring human annotation.

### Mechanism 2: Reasoning Elaboration via Chain-of-Thought (CoT)
Enforcing explicit reasoning traces improves accuracy on features requiring multi-step logic or calculation. Training models to generate "Observation" (quoting text) and "Reasoning" (step-by-step logic) before final JSON output allocates compute to resolve ambiguities like calculating time intervals between dates.

### Mechanism 3: Deterministic Mapping via Rule-Based Risk Stratification
Separating feature extraction (LLM) from risk categorization (Rules) ensures guideline adherence and interpretability. The LLM outputs structured JSON features (e.g., size_mm, thickened_wall), and a deterministic script maps these features to risk categories based on the 2017 ACR White Paper, preventing the LLM from misapplying complex clinical guidelines.

## Foundational Learning

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed: Full fine-tuning of 8B+ parameter models is computationally expensive. QLoRA allows efficient adaptation by freezing model weights and only training small adapter layers.
  - Quick check: Can you explain why QLoRA is critical for keeping the "fixed cost" of deployment low in this architecture?

- **Concept: Chain-of-Thought (CoT) Distillation**
  - Why needed: Small models often struggle with implicit reasoning. Training them on the *process* (CoT traces) of a larger model improves their ability to handle complex clinical logic.
  - Quick check: How does the "Observation" step in the CoT prompt reduce the risk of hallucination?

- **Concept: Exact Match Accuracy vs. F1 Score**
  - Why needed: The paper evaluates extraction using "Exact Match" (strict) and categorization using "F1" (tolerant to class imbalance). Understanding this distinction is vital for interpreting the 97% vs 0.93 scores correctly.
  - Quick check: Why is Exact Match a more rigorous metric for structured JSON extraction than simple token overlap?

## Architecture Onboarding

- **Component map:** Data Engine -> Label Factory (GPT-4o with CoT) -> Trainer (QLoRA fine-tuning) -> Inference Server (vLLM) -> Logic Layer (Rule-based mapping)
- **Critical path:** The "Label Factory" quality is the bottleneck. If GPT-4o produces flawed CoT data for the 6,000 training reports, the fine-tuned model will fail to generalize to the held-out test set.
- **Design tradeoffs:**
  - Latency vs. Accuracy: CoT prompting improves accuracy by ~17% but increases inference time from 0.3s to 1.5s per report.
  - Cost vs. Control: Open-source fine-tuning requires an upfront $354 training cost and GPU setup, but becomes cheaper than GPT-4o after ~9,100 reports.
  - Recall vs. Precision: Models show "recall-dominant" tendency, identifying high-risk cases at the cost of false positives due to RLHF biases.
- **Failure signatures:**
  - Object Identification Error: Misidentifying the "most worrisome" cyst when multiple are present.
  - Calculation Error: 16% error rate on date calculations for LLaMA-FT-CoT.
  - Negation Miss: Misinterpreting reports with typographical errors like missing negations.
- **First 3 experiments:**
  1. Zero-Shot Baseline: Test LLaMA-3.1-8B-Instruct on 50 reports to establish floor performance (expect ~80% accuracy).
  2. Synthetic Data Audit: Manually review 20 random GPT-4o CoT labels to verify teacher model quality before fine-tuning.
  3. Feature Ablation: Retrain model removing "CoT" component to verify performance drop on multi-step features like "Time Interval."

## Open Questions the Paper Calls Out

### Open Question 1
Can iterative reasoning frameworks like Self-Taught Reasoner (STaR) improve performance by overcoming limitations of GPT-4o-generated supervision? The authors note that STaR could iteratively improve both reasoning traces and final outputs through bootstrapped self-improvement, addressing the performance ceiling created by inherited teacher model errors.

### Open Question 2
How do fine-tuned open-source LLMs perform on external datasets with different reporting standards and templates? The study used a single-center dataset (NYU Langone Health), and external validation at institutions with different reporting standards would strengthen generalizability findings.

### Open Question 3
Can training strategies be optimized to improve precision for high-risk PCL features without sacrificing the high recall observed in current models? The results show a significant precision drop in the highest risk category (Category 3) compared to low-risk cases, indicating a trade-off where models prioritize sensitivity over specificity.

## Limitations
- Performance relies heavily on quality of synthetic labels generated by GPT-4o, with no independent human validation before fine-tuning.
- Cost comparison assumes Azure HIPAA compliance pricing for GPT-4o, which may not reflect all deployment scenarios.
- 6,000 training examples come from a single institution, raising questions about generalizability across different radiology report styles.

## Confidence

- **High Confidence:** Performance equivalence between fine-tuned open-source models and GPT-4o (97-98% accuracy, F1=0.93-0.95). Directly supported by reported experimental results and radiologist validation.
- **Medium Confidence:** Cost-effectiveness claim (cheaper after 9,100 reports). Calculation shown but depends on specific pricing assumptions that may vary by deployment context.
- **Medium Confidence:** Interpretability benefit of CoT supervision. Mechanism well-described but practical clinical utility for auditability requires further validation.

## Next Checks

1. **Label Quality Audit:** Have 2-3 independent radiologists review 50 randomly selected GPT-4o-generated CoT labels against original reports to assess hallucination rates and guideline adherence.

2. **Cross-Institutional Generalization:** Fine-tune the same models on 1,000 reports from a different institution with distinct radiology report templates and re-evaluate performance to assess domain shift effects.

3. **Clinical Impact Simulation:** Create a simulated clinical workflow where the model flags 100 high-risk cases from a test set, then have radiologists assess whether the model's risk categorization would have changed clinical management decisions.