---
ver: rpa2
title: 'RecTable: Fast Modeling Tabular Data with Rectified Flow'
arxiv_id: '2503.20731'
source_url: https://arxiv.org/abs/2503.20731
tags:
- data
- rectable
- https
- learning
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RecTable introduces rectified flow to tabular data generation,
  using a simple architecture of stacked gated linear unit (GLU) blocks. It employs
  a mixed-type noise distribution and logit-normal timestep distribution, avoiding
  complex transformer architectures for faster training.
---

# RecTable: Fast Modeling Tabular Data with Rectified Flow

## Quick Facts
- arXiv ID: 2503.20731
- Source URL: https://arxiv.org/abs/2503.20731
- Reference count: 33
- Primary result: RecTable achieves competitive performance compared to state-of-the-art diffusion and score-based models while reducing training time by 3-4×

## Executive Summary
RecTable introduces rectified flow to tabular data generation, using a simple architecture of stacked gated linear unit (GLU) blocks. It employs a mixed-type noise distribution and logit-normal timestep distribution, avoiding complex transformer architectures for faster training. Experiments on six real-world datasets show RecTable achieves competitive performance compared to state-of-the-art diffusion and score-based models while significantly reducing training time. For example, on the adult dataset, RecTable trains in 1,800 seconds versus 6,231 seconds for TabDiff, while maintaining comparable machine learning efficiency scores. The GLU-based architecture also shows robustness in high-dimensional datasets, outperforming MLP-based models like TabDDPM.

## Method Summary
RecTable uses rectified flow modeling for synthetic tabular data generation, employing a simple architecture of 4 stacked gated linear unit (GLU) blocks with hidden sizes [1024, 2048, 1024, 1024]. The model uses mixed-type noise distribution (Gaussian for numerical, uniform for categorical) and samples timesteps from a logit-normal distribution. Training minimizes L2 loss between predicted and target velocity vectors, and generation occurs via ODE solving using RK45. The model is trained on six UCI datasets with numerical features transformed via QuantileTransformer and categorical features one-hot encoded.

## Key Results
- RecTable trains in 1,800 seconds on adult dataset versus 6,231 seconds for TabDiff
- Achieves comparable machine learning efficiency scores (AUC ~0.906) to state-of-the-art models
- GLU-based architecture shows robustness on high-dimensional datasets (News with 46 features)
- Mixed-type noise distribution provides ~1.6% AUC improvement over Gaussian-only noise

## Why This Works (Mechanism)

### Mechanism 1: Straight ODE Trajectories
Rectified flow learns velocity fields vθ that transport noise to data via linear interpolation z_t = t·z₁ + (1-t)·z₀. The model minimizes ||vθ(z_t, t) - (z₁ - z₀)||², directly predicting optimal transport direction. This contrasts with diffusion models that learn score functions requiring iterative denoising through stochastic differential equations. The straight-line assumption provides efficient transport for tabular data distributions.

### Mechanism 2: GLU Architecture Benefits
GLU blocks apply a gating mechanism: GLU(x) = Dropout((xW₁ + b₁) ⊗ Sigmoid(xW₂ + b₂)). The sigmoid gate adaptively controls information flow, allowing selective activation of feature pathways. This provides richer function approximation than standard MLP layers while maintaining fewer parameters than attention-based transformers. The gating mechanism captures meaningful feature dependencies without requiring explicit attention mechanisms.

### Mechanism 3: Mixed-Type Noise Distribution
RecTable uses z₀ = [z_num, z_cat] where z_num ~ N(0,1) and z_cat_i ~ U(1, K_i). This matches preprocessing (QuantileTransformer normalizes numerical data) and one-hot encoding structure of categorical variables. Aligning noise distribution with data modality reduces the complexity of the transport problem, improving generation quality.

## Foundational Learning

- **Rectified Flow / Flow Matching**: Understanding ODE-based generative models versus diffusion (SDE-based) models clarifies why RecTable achieves faster training. Quick check: Can you explain why learning a velocity field v(z_t, t) might require fewer iterations than learning a score function ∇log p(z_t)?
- **Gated Linear Units (GLU)**: GLU is the architectural backbone replacing both MLP and attention. Understanding gating helps debug feature interaction learning. Quick check: How does the sigmoid gate in GLU differ from a ReLU activation in terms of gradient flow and feature selection?
- **Mixed-Type Tabular Data Preprocessing**: RecTable's noise distribution depends on proper preprocessing. Quick check: Why might applying Gaussian noise to one-hot encoded categorical features create invalid intermediate states during transport?

## Architecture Onboarding

- **Component map**: Input (preprocessed tabular + timestep) -> 4 GLU blocks (sizes 1024, 2048, 1024, 1024) -> MLP head -> velocity output
- **Critical path**: Sample real data z₁ -> Sample noise z₀ from mixed distribution -> Sample timestep t from LogitNormal -> Compute z_t = t·z₁ + (1-t)·z₀ -> Forward pass through GLU blocks -> Compute loss ||vθ - (z₁ - z₀)||² -> Generation via ODE solving dz/dt = vθ(z, t) from t=1 to t=0 using RK45
- **Design tradeoffs**: GLU vs. Transformer (fewer parameters vs. potential missed dependencies), Logit-normal vs. Uniform t (minor quality gain but 3× training time), No Reflow (simplifies training but may limit inference speedup)
- **Failure signatures**: Categorical collapse (generated samples cluster toward uniform), high-dimensional degradation (performance drops on >40 features), numerical outliers (extreme values in generated features)
- **First 3 experiments**: 1) Reproduce adult dataset results (AUC ~0.906, training time ~1800s), 2) Ablate noise distribution on adult (expect 1-2% AUC gap), 3) Scale to News dataset and compare GLU vs. MLP backbone

## Open Questions the Paper Calls Out

### Open Question 1
Can attention-based mechanisms or more sophisticated architectures close the performance gap in distributional fidelity compared to transformer-based state-of-the-art models? The paper notes RecTable lags in Shape error and that "capturing complex column-to-column relationships remains a challenge." Experiments combining rectified flow with transformer backbones could measure if Shape/Trend metric gaps close without losing speed benefits.

### Open Question 2
Does the omission of the "reflow" process result in suboptimal inference efficiency for tabular data? While reflow is standard in image generation for reducing sampling steps, its utility for mixed-type tabular data within the RecTable framework remains untested. An ablation study measuring generation quality versus inference speed after applying 1-2 steps of reflow could resolve this.

### Open Question 3
Why does RecTable achieve state-of-the-art Machine Learning Efficiency despite lagging in column-wise distributional fidelity metrics? Tables show RecTable ranking 3rd or lower in Shape and C2ST but 1st or 2nd in MLE. The disconnect between high utility and low fidelity metrics suggests GLU blocks may capture latent non-linear dependencies that standard column-wise metrics miss.

## Limitations

- GLU architecture for tabular generation is novel with no supporting corpus evidence
- Mixed-type noise distribution implementation details are ambiguous, particularly for categorical features
- Cross-dataset generalization remains unproven, limited to UCI datasets with modest feature dimensions
- Performance in distributional fidelity metrics (Shape, C2ST) lags behind transformer-based methods

## Confidence

- **High**: Training time reduction claims (directly measured), basic model architecture implementation
- **Medium**: Performance parity claims (MLE scores depend on downstream model choice), ablation study interpretations
- **Low**: Claims about GLU superiority on high-dimensional data (limited to one dataset), mixed-type noise benefits (single ablation)

## Next Checks

1. Reproduce adult dataset results (AUC ~0.906, training time ~1800s) with default config; verify against Table 6
2. Compare mixed-type vs. Gaussian-only noise on adult dataset; expect ~1-2% AUC gap per Table 9
3. Train on News dataset (46 numerical features) and compare GLU vs. MLP backbone; expect larger performance gap at high dimensionality per section 4.3 observations