---
ver: rpa2
title: 'Dale meets Langevin: A Multiplicative Denoising Diffusion Model'
arxiv_id: '2510.02730'
source_url: https://arxiv.org/abs/2510.02730
tags:
- multiplicative
- mnist
- samples
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a biologically inspired generative model
  based on geometric Brownian motion (GBM) and multiplicative score-matching. The
  key idea is to connect exponentiated gradient descent, which respects Dale's law
  and produces log-normally distributed synaptic weights, to sampling from GBM.
---

# Dale meets Langevin: A Multiplicative Denoising Diffusion Model

## Quick Facts
- arXiv ID: 2510.02730
- Source URL: https://arxiv.org/abs/2510.02730
- Reference count: 40
- Key outcome: A biologically inspired generative model based on geometric Brownian motion with multiplicative score-matching, achieving competitive generative performance while maintaining biological plausibility through log-normal distributions and Dale's law

## Executive Summary
This paper presents a novel generative model that bridges biologically motivated optimization principles with diffusion-based sampling. The authors develop a framework that connects exponentiated gradient descent (which respects Dale's law and produces log-normally distributed synaptic weights) to sampling from geometric Brownian motion (GBM). By developing a reverse-time stochastic differential equation (SDE) framework for GBM and proposing a multiplicative denoising score-matching loss function, they create a generative model that is both theoretically sound and biologically plausible. The model demonstrates competitive performance on standard image datasets while maintaining the desirable property of producing non-negative, log-normally distributed outputs.

## Method Summary
The proposed method introduces a multiplicative denoising diffusion model based on geometric Brownian motion. The key innovation is connecting exponentiated gradient descent to GBM sampling through a reverse-time SDE framework. The authors develop a novel multiplicative denoising score-matching loss that generalizes Hyvärinen's approach for non-negative data. This allows the model to learn score functions that respect the multiplicative nature of biological neural updates. The training process involves optimizing parameters to minimize the proposed loss function, while sampling is performed by solving the reverse-time SDE with learned score functions. The framework maintains biological plausibility by ensuring that all updates are multiplicative and that the resulting distributions are log-normal, consistent with empirical observations of neural connectivity patterns.

## Key Results
- Generated samples on MNIST, Fashion-MNIST, and Kuzushiji MNIST are diverse with reasonable quality
- Achieved FID scores of 28.96, 116.15, and 50.78 on respective datasets
- Demonstrated competitive generative performance while maintaining biological plausibility through log-normal distributions
- Proposed multiplicative denoising score-matching loss generalizes Hyvärinen's approach for non-negative data

## Why This Works (Mechanism)
The model works by leveraging the mathematical connection between exponentiated gradient descent and geometric Brownian motion. Exponentiated gradient descent naturally respects Dale's law (neurons are either excitatory or inhibitory) and produces log-normally distributed weights, which are empirically observed in biological neural networks. By connecting this optimization principle to GBM sampling, the model inherits these biologically plausible properties while maintaining the generative capabilities of diffusion models. The multiplicative nature of both the updates and the resulting distributions ensures that the generated data remains non-negative and follows realistic distributions, addressing a key limitation of standard diffusion models that use additive noise and can produce negative values.

## Foundational Learning
- **Geometric Brownian Motion**: A continuous-time stochastic process where the logarithm of the randomly varying quantity follows a Brownian motion with drift. Needed to model multiplicative noise processes that naturally produce log-normal distributions. Quick check: Verify that the solution to GBM SDE is indeed log-normally distributed.

- **Exponentiated Gradient Descent**: An optimization algorithm that uses multiplicative updates instead of additive ones, ensuring parameters remain positive. Required to maintain biological plausibility and connect to GBM sampling. Quick check: Confirm that exponentiated gradient descent produces log-normally distributed parameters in practice.

- **Dale's Law**: The principle that each neuron in the mammalian brain releases only one type of neurotransmitter. Essential for biological plausibility claims and motivates the multiplicative update structure. Quick check: Verify that the model's output distributions respect the excitatory/inhibitory dichotomy.

- **Score-Matching**: A method for learning probability distributions by matching the score function (gradient of log-density) rather than the density itself. Needed to train the generative model without requiring explicit density evaluation. Quick check: Ensure the multiplicative score-matching loss properly handles the non-negativity constraints.

- **Reverse-Time SDEs**: Stochastic differential equations that describe the reverse-time dynamics of diffusion processes. Required to transform the forward diffusion process into a generative sampling procedure. Quick check: Validate that the reverse-time SDE correctly inverts the forward GBM process.

## Architecture Onboarding

Component Map: Data -> Forward GBM Process -> Noisy Data -> Score Network -> Reverse SDE Solver -> Generated Samples

Critical Path: The critical path involves the forward GBM process adding multiplicative noise to data, the score network learning to denoise at various noise levels, and the reverse SDE solver using these learned scores to generate samples from noise.

Design Tradeoffs: The model trades computational efficiency for biological plausibility. Multiplicative operations are more computationally expensive than additive ones but produce more realistic distributions. The reverse-time SDE approach requires careful numerical integration but enables exact sampling from the learned distribution.

Failure Signatures: The model may fail when the learned score functions are inaccurate, leading to mode collapse or poor sample quality. Over-regularization can cause the model to underfit and produce overly smooth samples. Underfitting occurs when the score network capacity is insufficient to capture complex score functions.

First Experiments:
1. Train the model on synthetic log-normal data to verify that it can recover the correct distribution
2. Compare sample quality and diversity between multiplicative and additive diffusion models on MNIST
3. Analyze the learned score functions to verify they respect the multiplicative structure and produce log-normal outputs

## Open Questions the Paper Calls Out
None

## Limitations
- The generative performance, while competitive, shows room for improvement with FID scores not reaching state-of-the-art levels
- The evaluation is limited to relatively simple datasets (MNIST, Fashion-MNIST, Kuzushiji MNIST), with performance on more complex datasets untested
- Certain mathematical assumptions about the reverse-time SDE for GBM require more rigorous verification

## Confidence

Theoretical Framework: High
Mathematical Derivations: High
Biological Plausibility Claims: Medium
Generative Performance: Medium
Practical Implementation: High

## Next Checks

1. Test the model on more complex datasets (e.g., CIFAR-10, CelebA) to evaluate scalability and performance on high-dimensional data
2. Conduct ablation studies comparing multiplicative vs. additive updates on real neural data to validate biological plausibility claims
3. Perform systematic analysis of the model's behavior under different parameter settings to better understand its limitations and failure modes