---
ver: rpa2
title: Protecting Bystander Privacy via Selective Hearing in Audio LLMs
arxiv_id: '2512.06380'
source_url: https://arxiv.org/abs/2512.06380
tags:
- bystander
- audio
- speaker
- main
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SH-Bench, the first benchmark to evaluate\
  \ selective hearing in audio LLMs\u2014their ability to attend to an intended main\
  \ speaker while refusing to process or reveal information about incidental bystander\
  \ speech. The benchmark includes 3,968 multi-speaker audio mixtures and 77k multiple-choice\
  \ questions covering real and synthetic scenarios."
---

# Protecting Bystander Privacy via Selective Hearing in Audio LLMs

## Quick Facts
- arXiv ID: 2512.06380
- Source URL: https://arxiv.org/abs/2512.06380
- Authors: Xiao Zhan; Guangzhi Sun; Jose Such; Phil Woodland
- Reference count: 18
- Primary result: State-of-the-art audio LLMs leak bystander privacy; BPFT training achieves 47% higher bystander accuracy and 16% higher SE

## Executive Summary
This paper introduces SH-Bench, the first benchmark to evaluate selective hearing in audio LLMs—their ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. The benchmark includes 3,968 multi-speaker audio mixtures and 77k multiple-choice questions covering real and synthetic scenarios. A novel metric, Selective Efficacy (SE), is proposed to measure both comprehension and privacy protection. Evaluation of state-of-the-art models shows significant bystander privacy leakage, with strong audio understanding failing to translate into selective protection. To address this, the authors present Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries. BPFT achieves a 47% higher bystander accuracy under selective mode and 16% higher SE compared to Gemini 2.5 Pro, demonstrating substantial improvements in privacy protection without degrading main-speaker comprehension.

## Method Summary
BPFT fine-tunes only the LLM backbone using LoRA (rank 32) with paired instructions per question: one instructing general answering, another instructing refusal for bystander content. Training data consists of 3,768 audio mixtures created by combining AMI corpus segments with attenuated bystander audio (-10dB) and reverberation. MCQs with speaker descriptions are generated via GPT-4o. The approach freezes other components and relies on the model learning to semantically distinguish speaker roles from audio features and instruction context.

## Key Results
- State-of-the-art audio LLMs show only 45-48% refusal rates for bystander questions without BPFT
- BPFT achieves 92-96% refusal rates under selective mode, a 47% improvement in bystander accuracy
- Selective Efficacy improves by 16-17% compared to baseline models
- Performance degrades on real recordings (90-92.8%) compared to synthetic test scenarios (99.4-99.6%)
- Main-speaker comprehension drops from 98.3% to 93.9% under selective mode

## Why This Works (Mechanism)

### Mechanism 1: Instruction-conditioned refusal learning
Supervised fine-tuning with paired refusal instructions teaches models to conditionally ignore bystander content while preserving main-speaker comprehension. BPFT creates training pairs where each question has two instruction variants—one instructing general answering, another instructing refusal for bystander content. LoRA adaptation (rank 32) on the LLM backbone learns this conditional behavior without full model retraining. The model can learn to semantically distinguish speaker roles from audio features and instruction context, then map this to refusal behavior. Break condition: If main-speaker and bystander audio are acoustically indistinguishable or if instruction-following capacity is insufficient, refusal learning may fail or over-generalize.

### Mechanism 2: Content-conditioned speaker descriptions
Content-conditioned speaker descriptions enable the model to identify and isolate the main speaker from background bystanders. A natural language description of the main speaker is provided as context, allowing the model to ground its attention to matching acoustic characteristics. The model can perform cross-modal matching between textual descriptions and acoustic features to localize specific speakers. Break condition: If descriptions are ambiguous, speakers have similar voices, or the model lacks robust audio-text grounding, localization degrades.

### Mechanism 3: Explicit refusal as classification target
The "I don't know" option as an explicit refusal class provides a learnable, measurable pathway for selective hearing behavior. The 5-way MCQ format includes an explicit IDK option. Under selective mode, selecting IDK for bystander questions is scored as correct. This transforms refusal from implicit uncertainty into a concrete classification target. Explicit refusal options are necessary for reliable privacy behavior; without them, models may hallucinate answers even when uncertain. Break condition: In open-ended generation (no MCQ), refusal rates drop; pre-BPFT models refuse only 45-48% vs 92-96% post-BPFT.

## Foundational Learning

- Concept: Multi-speaker audio representation
  - Why needed here: Understanding how audio LLMs encode overlapping speech and separate speaker identities is prerequisite to manipulating which speaker receives attention.
  - Quick check question: Can you explain how an audio encoder might represent two simultaneous speakers versus sequential speakers?

- Concept: Instruction-tuning for conditional behavior
  - Why needed here: BPFT relies on the model learning different behaviors based on instruction context; understanding instruction-following mechanics is essential.
  - Quick check question: How does adding task-specific instructions to training data change model behavior at inference time?

- Concept: Privacy-utility tradeoffs in ML
  - Why needed here: SE metric explicitly balances comprehension (utility) against privacy (refusal); practitioners must navigate this tradeoff.
  - Quick check question: If a model achieves 100% bystander refusal but 50% main-speaker accuracy, is it practically useful?

## Architecture Onboarding

- Component map: Audio input → Audio encoder (frozen) → LLM backbone with LoRA adapters (rank 32, trainable) → MCQ output
- Critical path: (1) Generate audio mixtures with main speaker + attenuated bystander (-10dB); (2) Create MCQs + speaker descriptions via GPT-4o; (3) Format training pairs with dual instructions; (4) Fine-tune LLM backbone only; (5) Evaluate under both general and selective modes using SE metric
- Design tradeoffs: Synthetic training data generalizes well to synthetic test scenarios (99.4-99.6% bystander accuracy) but less to real scenarios (90.0-92.8%); LoRA rank 32 balances capacity vs overfitting risk; 5-way MCQ format enables measurable refusal but doesn't fully reflect open-ended deployment
- Failure signatures: (1) Over-refusal: Model selects IDK for main-speaker questions (Llama-Omni-2: 32.9% main accuracy under selective mode); (2) Under-refusal: Model answers bystander questions despite instructions (pre-BPFT models: 45-48% refusal rate); (3) Dependency collapse: Performance degrades significantly without speaker descriptions (Gemini drops from 59.2% to 45.6% bystander accuracy)
- First 3 experiments:
  1. Baseline assessment: Run existing audio LLM (e.g., Qwen-2.5-Omni-7B) on SH-Bench under both modes to measure initial SE and identify failure mode (over/under-refusal)
  2. Ablation on description dependency: Test model with and without speaker descriptions to quantify reliance on this signal before investing in description generation systems
  3. BPFT with curriculum: Train with progressive bystander difficulty (clear separation → overlapping speech) to test if curriculum improves robustness on real-scenario test partition

## Open Questions the Paper Calls Out

- How can selective hearing mechanisms be effectively adapted for group discussions involving multiple intended main speakers?
  - The current focus is restricted to single main speaker scenarios and suggests that "more challenging scenarios such as group discussions... can be explored in the future." The current Bystander Privacy Fine-Tuning (BPFT) pipeline and SH-Bench are designed around a single main speaker; the authors note the model "may fail when there are more than one main speaker."

- Can bystander privacy protection be extended to audio-visual large language models without introducing visual privacy leaks?
  - The authors explicitly state in the limitations that since the examined models receive omni-modal inputs, "the bystander privacy could also be extended to audio-visual scenarios." The current study and benchmark (SH-Bench) restrict their scope exclusively to the audio modality.

- Can the trade-off between privacy protection and main speaker comprehension be optimized to prevent accuracy degradation in the primary task?
  - Section 6.1 notes that while BPFT improves privacy, it "might cause a slight degradation to the main speaker accuracy" (e.g., Qwen-2.5-Omni dropped from 96.0% to 93.3%). The paper demonstrates the existence of this trade-off but does not propose a method to fully mitigate the loss of comprehension for the main speaker while maximizing privacy.

## Limitations

- Synthetic-to-real generalization gap: BPFT achieves 99.4-99.6% bystander accuracy on synthetic test scenarios but performance drops to 90.0-92.8% on real recordings, suggesting synthetic training data doesn't fully capture real-world acoustic variability
- Description dependency fragility: BPFT models show reduced sensitivity to missing speaker descriptions but still suffer performance drops, revealing fundamental dependency on accurate speaker descriptions
- Over-refusal risk under selective mode: BPFT achieves impressive selective efficacy but at a cost of main-speaker comprehension dropping from 98.3% to 93.9% under selective mode, with some models showing potential over-refusal

## Confidence

**High confidence**: The core observation that existing audio LLMs leak bystander information (45-48% refusal rates without BPFT) is robustly demonstrated through multiple models and evaluation modes. The synthetic data generation pipeline is well-specified and reproducible. The SE metric effectively captures the privacy-utility tradeoff.

**Medium confidence**: The BPFT training methodology and its improvements over baselines are credible, but the exact LoRA hyperparameters and training duration aren't specified, making exact replication uncertain. The synthetic-to-real performance gap is documented but not deeply analyzed.

**Low confidence**: The claim that BPFT enables "selective hearing" in a generalizable sense is overstated. The method shows strong results when speaker descriptions are available and acoustic conditions are favorable, but the dependency on these factors and the synthetic-to-real gap suggest limited robustness.

## Next Checks

1. **Description robustness evaluation**: Systematically vary description quality (ambiguous, incomplete, inaccurate) and measure SE degradation across all BPFT models to quantify real-world robustness

2. **Open-ended generation benchmark**: Implement parallel evaluation using open-ended generation instead of MCQs to verify that refusal behavior generalizes beyond the constrained 5-way choice format

3. **Acoustic robustness stress test**: Create test scenarios with varying acoustic conditions (different SNR levels, overlapping speech durations, similar speaker voices) to identify specific acoustic factors causing synthetic-to-real performance gap