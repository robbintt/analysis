---
ver: rpa2
title: 'ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via
  Dual Reasoning'
arxiv_id: '2511.06057'
source_url: https://arxiv.org/abs/2511.06057
tags:
- stance
- mind
- experience
- reasoning
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal stance detection, aiming to identify
  user attitudes towards targets using both text and image data. Existing methods
  often simply fuse modalities without explicitly reasoning about their varying contributions,
  which can lead to errors, especially in cases involving irony or logical fallacies.
---

# ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning

## Quick Facts
- arXiv ID: 2511.06057
- Source URL: https://arxiv.org/abs/2511.06057
- Reference count: 13
- Outperforms existing models on MMSD benchmark with state-of-the-art results

## Executive Summary
This paper addresses multimodal stance detection by proposing ReMoD, a framework that employs dual reasoning inspired by human cognition. The method tackles the limitations of existing approaches that simply fuse modalities without explicit reasoning about their varying contributions. ReMoD uses an intuitive stage to form initial stance hypotheses through experience pools, then refines these hypotheses in a reflective stage using modality-aware and semantic reasoning chains. Extensive experiments on the MMSD benchmark demonstrate significant performance improvements, particularly in zero-shot settings, showing strong generalization capabilities across different domains.

## Method Summary
ReMoD implements a dual-reasoning paradigm with intuitive (System 1) and reflective (System 2) stages. The intuitive stage uses Modality and Semantic Experience Pools to generate initial stance hypotheses through modal synergistic retrieval. The reflective stage employs Modality-CoT to resolve inter-modal dynamics and Semantic-CoT to refine semantic understanding, updating the experience pools. The framework uses MLLM for entity extraction, Wikipedia for knowledge grounding, and SAM for visual segmentation. Experience retrieval uses bimodal embeddings with weighted similarity (α=0.7, τ=0.7, top-k=3). The final prediction integrates insights from both reasoning chains.

## Key Results
- Achieves state-of-the-art performance on MMSD benchmark across all five domains
- Demonstrates strong generalization in zero-shot settings with consistent improvements
- MEP removal causes largest performance drop (71.19→66.95) in ablation studies
- Performance degrades at both extremes of retrieval threshold (τ<0.3 or τ>0.9)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual experience pools (MEP and SEP) enable context-aware stance decisions by accumulating and recalling past reasoning patterns across training samples.
- Mechanism: The Modality Experience Pool stores adaptive fusion strategies while the Semantic Experience Pool stores deeper contextual insights. At inference, modal synergistic retrieval ranks experiences using weighted combination of text and vision similarity scores (α=0.7, τ=0.7, top-k=3), then uses them as in-context exemplars for the MLLM agent.
- Core assumption: Past reasoning patterns transfer to new samples sharing similar bimodal embeddings; experience quality depends on retrieval relevance thresholds balancing noise vs. coverage.
- Evidence anchors: [abstract] "dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions"; [section 3.2.1] "experiences are ranked in descending order based on their MRS"
- Break condition: If retrieval threshold too low (τ<0.3), noisy experiences degrade reasoning; if too high (τ>0.9), insufficient retrieval limits knowledge leverage.

### Mechanism 2
- Claim: Modality-CoT resolves inter-modal dynamics by diagnosing prediction failures across unimodal and multimodal views, then distilling adaptive fusion strategies.
- Mechanism: The reflective stage evaluates accuracy of three initial predictions (text-only, vision-only, multimodal) with their rationales. By analyzing failure patterns, it generates a modality insight Iₘ—an adaptive fusion strategy dictating how to balance unimodal vs. multimodal cues. This insight updates the MEP via re-reasoning with retrieved experiences.
- Core assumption: Cross-modal irony and intra-modal logical fallacies can be detected by comparing predictions across perspectives; fusion strategies are sample-dependent rather than fixed.
- Evidence anchors: [abstract] "Modality-CoT for resolving inter-modal dynamics and Semantic-CoT for refining semantic understanding"; [section 3.3.1] "distills a high-level modality insight Iₘ. This insight is not merely a summary, but an adaptive fusion strategy"
- Break condition: If initial predictions are uniformly wrong across perspectives, CoT lacks discriminative signal to derive useful insights.

### Mechanism 3
- Claim: Semantic-CoT grounds reasoning in external world knowledge by synthesizing internal rationales with Wikipedia-retrieved entity knowledge.
- Mechanism: Textual and visual entities are extracted via MLLM, then used to query Wikipedia. Raw knowledge is distilled into concise summaries. Semantic-CoT interrogates initial reasoning's semantic grounding, identifies content-level biases, and generates semantic insight Iₛ—an actionable directive for deeper contextual understanding.
- Core assumption: Entity-level Wikipedia knowledge suffices to resolve semantic traps (irony, cultural context); MLLM can effectively distill noisy retrieved text.
- Evidence anchors: [abstract] "Semantic-CoT for refining semantic understanding"; [section 3.1.1-3.1.2] "retrieve corresponding background information... distill it into a concise, semantically-rich summary"
- Break condition: If entities are misidentified or Wikipedia lacks relevant entries, grounding fails; if distillation loses critical nuance, semantic insight degrades.

## Foundational Learning

- Concept: **Dual-Process Cognitive Theory (System 1 / System 2)**
  - Why needed here: The entire MIND architecture is structured around Kahneman's dual-process theory—intuitive reasoning (fast, heuristic-based) vs. reflective reasoning (slow, analytical). Understanding this distinction is essential to grasp why experience pools enable "rapid hypothesis" while CoT enables "scrutiny and refinement."
  - Quick check question: Can you explain why the paper uses experience retrieval for System 1 but structured CoT for System 2, rather than both using the same mechanism?

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Modality-CoT and Semantic-CoT are the reflective reasoning engines. CoT structures multi-step reasoning to surface intermediate judgments, rationales, and their failures—critical for distilling insights Iₘ and Iₛ.
  - Quick check question: How does Modality-CoT differ from standard CoT in what it produces (hint: it's not just a final answer)?

- Concept: **Experience/Memory Retrieval in LLMs**
  - Why needed here: MIND relies on retrieving top-k experiences from MEP/SEP using bimodal embeddings and cosine similarity. This is a form of retrieval-augmented generation (RAG) specialized for reasoning patterns rather than raw documents.
  - Quick check question: Why does the paper use a weighted combination (α=0.7) of text and vision similarity rather than a single multimodal embedding?

## Architecture Onboarding

- Component map:
  ```
  [Input: (text u, image v, target t)]
          ↓
  [Multimodal Knowledge Perception]
    ├─ Textual Knowledge Attainment (MLLM entity extraction → Wikipedia → distillation → Kᵤ)
    └─ Visual Information Grounding (MLLM caption → entities Eᵥ → SAM segmentation → V)
          ↓
  [Experience-Driven Intuitive Reasoning (System 1)]
    ├─ Modal Synergistic Retrieval (BGE-VL-CLIP Enc → query vectors → MRS ranking → top-k experiences)
    └─ Experience-Driven Stance Prediction (Agent Aᴿ generates (ŷᶜ, rᶜ) for each perspective)
          ↓
  [Meta-cognitive Reflective Reasoning (System 2)]
    ├─ Modality-CoT → Iₘ (adaptive fusion strategy) → update MEP
    ├─ Semantic-CoT → Iₛ (contextual directive) → update SEP
    └─ Stance Inference (final prediction y)
  ```

- Critical path:
  1. Entity extraction accuracy determines Wikipedia grounding quality
  2. SAM segmentation quality determines visual entity alignment
  3. Retrieval relevance (τ, α) determines experience quality
  4. CoT reasoning quality determines insight usefulness
  5. Experience pool update mechanism determines learning accumulation

- Design tradeoffs:
  - **Computational cost vs. reasoning depth**: Multiple MLLM calls (entity extraction, distillation, intuitive prediction, CoT reflection, experience update) cause significant latency—the paper explicitly notes this limitation.
  - **Retrieval threshold (τ=0.7)**: Balances noise (low τ) vs. insufficient recall (high τ); ablation shows performance degrades at both extremes.
  - **Experience pool size**: Not specified; implicitly controlled by relevance threshold and update strategy.

- Failure signatures:
  - **Cross-modal irony missed**: Model predicts FAVOR when text and image appear consistent but contain sarcasm—MEP should weight contextual textual evidence over surface terms.
  - **Intra-modal logical fallacy**: Model predicts AGAINST based on keywords like "sues" without recognizing "What if..." framing makes it COMMENT—SEP should provide deeper semantic grounding.
  - **Vision noise overwhelming text**: Simpler fusion models (CLIP, TMPT) degrade when visual data is irrelevant—MIND's adaptive fusion should downweight vision in such cases.

- First 3 experiments:
  1. **Reproduce ablation on single domain** (e.g., MTSE): Remove MEP, SEP, and CoT separately to verify relative contributions (Table 3 shows MEP removal causes largest zero-shot drop: 71.19→66.95).
  2. **Vary retrieval threshold (τ)**: Test τ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on held-out domain to reproduce Figure 3 curve and identify optimal threshold for your compute budget.
  3. **Test different backbones**: Run MIND with smaller backbone (Qwen-VL-7B) vs. full Qwen2.5-VL-32B to quantify framework's ability to compensate for weaker base models (Table 1-2 show consistent ↑ improvements across backbones).

## Open Questions the Paper Calls Out
The paper explicitly identifies computational cost and inference latency as primary limitations, noting the framework is "less suitable for real-time applications" due to multiple calls to the large multimodal model. The authors speculate that techniques like knowledge distillation might create a lightweight version without testing it.

## Limitations
- **Knowledge acquisition dependency**: Performance heavily depends on entity extraction accuracy and Wikipedia's coverage of relevant entities
- **Computational overhead**: Multiple MLLM calls create significant latency, making the framework less suitable for real-time applications
- **Experience pool dynamics**: The paper mentions continuous refinement but doesn't specify pool size limits, aging mechanisms for outdated experiences, or strategies for handling contradictory experiences

## Confidence
- **High confidence**: Dual-reasoning paradigm effectiveness (superior performance vs baselines on MMSD benchmark)
- **Medium confidence**: Experience pool mechanisms (retrieval parameters α=0.7, τ=0.7, top-k=3 reported but not extensively validated)
- **Medium confidence**: Knowledge grounding approach (Wikipedia integration shown effective but dependency risks acknowledged)

## Next Checks
1. **Retrieval threshold sensitivity**: Systematically vary τ from 0.1 to 0.9 on a held-out domain to reproduce Figure 3's performance curve and identify optimal threshold for different computational budgets.

2. **Cross-domain generalization stress test**: Apply MIND to completely unseen domains (e.g., political discourse, product reviews) to validate zero-shot capabilities beyond MMSD's five domains.

3. **Knowledge source ablation**: Replace Wikipedia with domain-specific knowledge bases or fine-tuned retrieval models to quantify dependency on Wikipedia and test robustness to knowledge source variations.