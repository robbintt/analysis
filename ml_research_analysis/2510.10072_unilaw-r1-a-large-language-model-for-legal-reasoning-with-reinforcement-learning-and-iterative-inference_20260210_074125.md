---
ver: rpa2
title: 'Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning
  and Iterative Inference'
arxiv_id: '2510.10072'
source_url: https://arxiv.org/abs/2510.10072
tags:
- legal
- reasoning
- data
- answer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Unilaw-R1 is a 7-billion parameter legal reasoning language model
  designed to address key challenges in legal AI: insufficient legal knowledge, unreliable
  reasoning logic, and weak generalization. The authors propose a two-stage training
  approach combining Supervised Fine-Tuning (SFT) with Reinforcement Learning (RL)
  using a novel legal validity reward function integrated into Group Relative Policy
  Optimization (GRPO).'
---

# Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference

## Quick Facts
- **arXiv ID**: 2510.10072
- **Source URL**: https://arxiv.org/abs/2510.10072
- **Reference count**: 13
- **Primary result**: Unilaw-R1 achieves 53.2% accuracy on LawBench, outperforming Qwen-2.5-7B-Instruct by 6.6% and matching 32B-scale model performance

## Executive Summary
Unilaw-R1 is a 7-billion parameter legal reasoning model addressing key challenges in legal AI: insufficient legal knowledge, unreliable reasoning logic, and weak generalization. The authors propose a two-stage training approach combining Supervised Fine-Tuning (SFT) with Reinforcement Learning (RL) using a novel legal validity reward function integrated into Group Relative Policy Optimization (GRPO). The model incorporates iterative multi-agent inference with Assessor-Reviser agents to refine reasoning outputs. Unilaw-R1 achieves strong performance on authoritative legal benchmarks, exceeding Qwen-2.5-7B-Instruct by 6.6% on LawBench and LexEval, and matching much larger models like DeepSeek-R1-Distill-Qwen-32B.

## Method Summary
The authors develop a two-stage training pipeline combining Supervised Fine-Tuning on distilled chain-of-thought data with Group Relative Policy Optimization reinforcement learning. They construct Unilaw-R1-Data, a high-quality dataset of ~17K chain-of-thought samples filtered for legal validity, and Unilaw-R1-Eval, a dedicated benchmark covering multiple legal domains. The model uses iterative multi-agent inference with Assessor-Reviser agents to refine reasoning outputs. Training employs LoRA for parameter efficiency and a composite reward function capturing correctness, format compliance, and legal validity. The inference pipeline samples multiple reasoning chains, scores them with an outcome reward model, and applies self-consistency voting to select final answers.

## Key Results
- Unilaw-R1 achieves 53.2% accuracy on LawBench, exceeding Qwen-2.5-7B-Instruct baseline (46.6%) by 6.6%
- Outperforms all models of similar scale and matches DeepSeek-R1-Distill-Qwen-32B (54.9%) performance
- Iterative inference improves zero-shot CoT accuracy from 39.5% to 51.0% after three iterations
- Training stages show progressive gains: SFT-only (48.0%) → RL-only (50.4%) → Full model (53.2%)

## Why This Works (Mechanism)

### Mechanism 1
A two-stage training pipeline combining Supervised Fine-Tuning (SFT) on distilled chain-of-thought data followed by Group Relative Policy Optimization (GRPO) reinforcement learning yields higher legal reasoning accuracy than either stage alone. SFT establishes structured legal reasoning patterns (producing outputs in `<think>...</think` and `<answer>...</answer>` formats), while RL refines policy using a composite reward that captures correctness, format compliance, and legal validity. GRPO normalizes rewards within groups of sampled outputs, emphasizing candidates that exceed group average quality. Legal reasoning benefits from explicit structured CoT learning followed by reward-driven policy refinement where legal soundness is directly incentivized.

### Mechanism 2
Iterative inference with Assessor-Reviser agents improves final answer quality through structured feedback and targeted revision. Given an input, the model samples k diverse chains. An Assessor scores each step and generates actionable feedback. Low-scoring chains are revised by a Reviser agent using the feedback. After n iterations, the top k chains (by outcome reward model score) are retained and the final answer is selected via self-consistency voting. Legal reasoning errors can be localized to specific steps and corrected via targeted feedback without introducing new errors.

### Mechanism 3
High-quality, filtered chain-of-thought data distilled from a strong reasoning model improves downstream legal reasoning performance. Questions are distilled via DeepSeek-R1 to generate CoT traces. A multi-stage filter (answer check, chain rewriting, explanation generation, reasoning selection) retains only reasoning paths that are correct, legally coherent, and aligned with syllogistic legal analysis. This reduces noise and ensures training on legally sound trajectories. Distilled reasoning from a larger/general model, when rigorously filtered, transfers effectively to a smaller domain-specific model.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Unilaw-R1 relies on structured CoT traces (`<think>` blocks) for both training and inference. Understanding CoT is essential to grasp how reasoning is externalized and refined. Quick check: Can you explain how CoT differs from standard prompting and why step-by-step reasoning matters for legal tasks?

- **Reinforcement Learning with Human/AI Feedback (RLHF/RLAIF)**: The GRPO stage uses a model-based verifier to provide legal validity rewards, a variant of RL from AI feedback. Quick check: What is the role of a reward model in RLHF, and how might reward hacking occur?

- **Multi-Agent Inference Systems**: The iterative inference pipeline depends on Assessor and Reviser agents collaborating to refine outputs. Quick check: How does separating evaluation (Assessor) from generation (Reviser) change the failure modes compared to a single model?

## Architecture Onboarding

- **Component map**: JEC-QA + proprietary data → DeepSeek-R1 distillation → filtering (answer check, chain rewriting, explanation generation, reasoning selection) → Unilaw-R1-Data (SFT) + RL subset → Qwen2.5-7B-Instruct → SFT (LoRA, rank 8) → GRPO RL (group size 4, composite reward: accuracy + format + legal validity)

- **Critical path**: Build and validate Unilaw-R1-Data with rigorous filtering → Run SFT to establish CoT format and basic legal reasoning → Train GRPO with composite reward; verify legal validity reward alignment → Implement iterative inference; tune k, n, and thresholds

- **Design tradeoffs**: Stricter filtering improves quality but may reduce coverage and diversity; RL improves accuracy but requires careful reward design to avoid hacking; more iterations and larger k improve outputs but increase latency and cost; gains saturate quickly (marginal after 1–2 iterations)

- **Failure signatures**: SFT fails to learn CoT format → malformed outputs (missing tags); GRPO reward hacking → legally plausible but incorrect answers; Assessor produces vague feedback → Reviser makes cosmetic rather than substantive changes; over-aggressive filtering → poor generalization to unseen question types

- **First 3 experiments**: Validate data pipeline: Sample 100 distilled+filtered chains and manually verify legal soundness and CoT coherence; Ablate training stages: Compare SFT-only, RL-only, and SFT+RL on Unilaw-R1-Eval to isolate contribution of each stage; Tune iterative inference: Run grid on k ∈ {5, 10, 15} and n ∈ {1, 2, 3} to identify cost-quality sweet spot; monitor ORM score trends and error types per iteration

## Open Questions the Paper Calls Out

- **Question**: How can the validity of the model's step-by-step legal reasoning be rigorously evaluated to ensure it adheres to syllogistic legal standards rather than just matching the correct answer?
- **Question**: Why does the performance of the iterative inference mechanism plateau after the first iteration, and does the Assessor-Reviser loop introduce error propagation?
- **Question**: To what extent does restricting training data to objective multiple-choice questions limit the model's generalization to open-ended legal tasks?

## Limitations

- The proprietary judicial exam dataset (~1,700 samples) is not publicly available, limiting exact reproduction and raising questions about data representativeness
- The legal validity reward model and its calibration are not independently verified; potential reward hacking could bias RL outcomes
- Iterative inference gains are marginal after 1–2 iterations, suggesting diminishing returns and sensitivity to Assessor feedback quality

## Confidence

- **High**: Unilaw-R1 improves over baseline Qwen-2.5-7B-Instruct by 6.6% on LawBench/LexEval; structured CoT and reward-based RL are well-established techniques
- **Medium**: Claims about superiority over similar-scale models rely on single-authority benchmarks; broader cross-dataset validation is limited
- **Medium**: Iterative inference improves performance but gains plateau quickly; exact contribution of each component is not isolated via ablation

## Next Checks

1. Replicate the SFT → RL pipeline using a public legal QA dataset (e.g., JEC-QA only) to assess data dependency
2. Perform ablation: test SFT-only, RL-only, and SFT+RL to isolate training stage contributions
3. Conduct robustness testing across multiple legal reasoning benchmarks (e.g., FLITE, LexGLUE) to verify generalization beyond the primary evaluation set