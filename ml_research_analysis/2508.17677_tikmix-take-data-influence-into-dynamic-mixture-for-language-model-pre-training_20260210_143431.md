---
ver: rpa2
title: 'TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training'
arxiv_id: '2508.17677'
source_url: https://arxiv.org/abs/2508.17677
tags:
- data
- influence
- arxiv
- mixture
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TiKMiX introduces a dynamic data mixing framework for large language
  model pre-training that addresses the limitations of static mixing strategies. It
  uses Group Influence, an efficient metric to evaluate the collective impact of data
  domains on model performance, and formulates data mixing as an influence-maximization
  optimization problem.
---

# TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training

## Quick Facts
- arXiv ID: 2508.17677
- Source URL: https://arxiv.org/abs/2508.17677
- Reference count: 16
- 1B-7B parameter models trained up to 1 trillion tokens show 2% average performance gain over baselines

## Executive Summary
TiKMiX introduces a dynamic data mixing framework that addresses static mixing limitations in LLM pre-training by optimizing data mixtures based on their collective influence on downstream performance. The method uses Group Influence, an efficient metric extending influence functions to evaluate the collective impact of data domains, and formulates data mixing as an influence-maximization optimization problem. Experiments demonstrate that TiKMiX-D outperforms state-of-the-art methods like REGMIX while using only 20% of the computational resources, and TiKMiX-M achieves an average performance gain of 2% across 9 downstream benchmarks.

## Method Summary
TiKMiX calculates Group Influence for data domains using gradient accumulation and Hessian approximation, then optimizes mixture weights through two approaches: TiKMiX-D directly solves a constrained optimization problem to maximize normalized influence while ensuring Pareto improvement, and TiKMiX-M uses a regression model trained on sampled candidates to capture non-linear interactions. The framework dynamically adjusts data mixtures every 200B tokens based on evolving model preferences, partitioning RefinedWeb into 26 domains and evaluating on 9 downstream benchmarks.

## Key Results
- TiKMiX-D outperforms REGMIX while using only 20% of computational resources
- TiKMiX-M achieves 45.9% average performance vs 45.5% for TiKMiX-D across 9 benchmarks
- Pearson correlation of 0.789 between calculated Group Influence and actual downstream performance

## Why This Works (Mechanism)

### Mechanism 1
Aggregating gradients over a data domain provides a tractable proxy for collective impact on downstream validation performance. TiKMiX extends influence functions from individual samples to groups by approximating parameter change induced by upweighting specific domains, computing the product of validation set gradients and inverse Hessian-vector products. Core assumption: influences are linearly additive for the direct optimization variant. Evidence: Figure 4 shows Pearson correlation of 0.789 between calculated Group Influence and actual downstream performance.

### Mechanism 2
Formulating data mixing as a constrained optimization problem prevents "under-digestion" of data and ensures balanced capability gains. TiKMiX-D frames mixing as maximizing total normalized influence while minimizing standard deviation across tasks, with a Pareto improvement constraint ensuring no task degrades relative to the prior mixture. Core assumption: the optimization landscape is smooth enough for Sequential Least Squares Quadratic Programming to find valid solutions. Evidence: Table 1 shows balanced improvements across both in-domain and out-of-domain tasks.

### Mechanism 3
A regression surrogate can capture non-linear cross-domain interactions that direct linear optimization misses. While TiKMiX-D assumes linearity, TiKMiX-M samples perturbations using Latin Hypercube Sampling, trains a LightGBM model to predict aggregate influence, then iteratively searches this surrogate space. Core assumption: the relationship between mixture ratios and performance is learnable by a tree-based model within the local neighborhood of initial weights. Evidence: Table 1 shows TiKMiX-M achieving 45.9% average performance vs 45.5% for TiKMiX-D.

## Foundational Learning

- **Concept: Influence Functions**
  - Why needed here: This is the theoretical engine of the paper, providing the foundation for Group Influence metric
  - Quick check question: How does the paper justify calculating the Hessian inverse for large models?

- **Concept: Constrained Optimization (SLSQP)**
  - Why needed here: TiKMiX-D is solving a mathematical program, requiring understanding of constraints
  - Quick check question: Why is the entropy term H(w) included in the objective function?

- **Concept: Surrogate-based Optimization**
  - Why needed here: TiKMiX-M relies on training a cheap predictor to guide the search
  - Quick check question: Why use Dirichlet sampling in the iterative search?

## Architecture Onboarding

- **Component map:** RefinedWeb data partitioned into 26 domains -> Gradient Accumulator (collects domain gradients) -> Influence Calculator (estimates domain utility) -> TiKMiX-D (SLSQP Solver) OR TiKMiX-M (LightGBM Surrogate) -> Dataloader (updates sampling probabilities for Stage N+1)

- **Critical path:** The most latency-sensitive step is the Influence Calculation, as computing the Hessian-vector product across multiple validation sets for 26 domains is the computational gatekeeper.

- **Design tradeoffs:** TiKMiX-D uses only 20% of compute of baselines and assumes linear additivity, while TiKMiX-M adds a regression layer to capture non-linearity but requires a sampling budget. The tradeoff is engineering complexity versus model accuracy.

- **Failure signatures:** Domain Collapse (optimizer assigns 100% weight to one domain), Metric Gaming (model improves on validation sets but degrades on unseen tasks), Gradient Noise (noisy influence estimation from small batch sizes).

- **First 3 experiments:**
  1. Replicate Figure 4 by training small proxy models on random mixtures and verifying Group Influence correlates with actual downstream loss (Pearson > 0.7)
  2. Compare wall-clock time of calculating Group Influence for 1 epoch vs training a REGMIX proxy model to verify claimed efficiency
  3. Run TiKMiX-D with and without the Pareto constraint to check if unconstrained version causes catastrophic forgetting on specific benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
How does the correlation between Group Influence and downstream performance change when scaling models significantly beyond 7B parameters? The paper states plans for further experiments on larger-scale models to validate Group Influence effectiveness. Current experiments only validate up to 7B parameters, leaving uncertainty about whether the observed linear correlation holds for much larger architectures.

### Open Question 2
To what extent does the selection of validation sets introduce bias into the optimized data mixture? The method calculates Group Influence relative to specific validation tasks, but the paper doesn't ablate how varying these target sets alters learned data weights. Optimizing for specific benchmarks could over-represent relevant domains while under-representing others, potentially limiting generalizability.

### Open Question 3
How does the granularity of domain definition impact the accuracy of Group Influence estimation? The paper relies on a fixed 26-domain taxonomy, assuming these groupings are optimal units for calculating collective influence. If domains are too broad or too narrow, the linearity assumption might fail. Experiments comparing different domain granularities would determine sensitivity to taxonomy choices.

## Limitations

- Theoretical foundation rests on influence function approximations that may not scale cleanly to trillion-token regimes
- Validation benchmark suite (9 tasks) may not fully capture downstream capabilities, creating potential blind spots
- Group Influence metric requires gradient accumulation across entire domains, which could be memory-intensive

## Confidence

**High Confidence**: Static data mixing leads to "under-digestion" of certain domains is well-supported by experimental results; computational efficiency claims for TiKMiX-D are specific and measurable.

**Medium Confidence**: Superiority of TiKMiX-M over TiKMiX-D relies on quality of LightGBM surrogate model, which depends on sampling choices and may not generalize across different dataset characteristics.

**Low Confidence**: Pareto improvement constraint ensures balanced capability gains assumes validation sets are representative and influence estimation is accurate enough to capture task-specific degradation.

## Next Checks

1. **Hessian Approximation Scalability**: Measure actual wall-clock time and memory usage for Group Influence calculation on 1B, 3B, and 7B parameter models to verify claimed efficiency improvements.

2. **Domain Interaction Validation**: Design controlled experiments where domains have known synergistic or antagonistic relationships to test whether TiKMiX-M correctly identifies and leverages these interactions.

3. **Robustness to Validation Set Choice**: Train models using different subsets of the 9 validation benchmarks and measure performance variance on held-out downstream tasks to reveal potential overfitting to specific benchmarks.