---
ver: rpa2
title: An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems
arxiv_id: '2511.02525'
source_url: https://arxiv.org/abs/2511.02525
tags:
- which
- routing
- depot
- location
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DRLHQ, an end-to-end deep reinforcement learning
  method to solve capacitated location-routing problems (CLRPs) and open CLRPs (OCLRP).
  The key innovation is reformulating CLRPs as a Markov decision process (MDP) that
  integrates location and routing decisions into a unified framework, enabling simultaneous
  optimization.
---

# An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems

## Quick Facts
- arXiv ID: 2511.02525
- Source URL: https://arxiv.org/abs/2511.02525
- Authors: Changhao Miao; Yuntian Zhang; Tongyu Wu; Fang Deng; Chen Chen
- Reference count: 40
- Primary result: End-to-end DRL method (DRLHQ) outperforms traditional and DRL baselines on CLRP and OCLRP

## Executive Summary
This paper proposes DRLHQ, an end-to-end deep reinforcement learning method to solve capacitated location-routing problems (CLRPs) and open CLRPs (OCLRP). The key innovation is reformulating CLRPs as a Markov decision process (MDP) that integrates location and routing decisions into a unified framework, enabling simultaneous optimization. A heterogeneous querying attention mechanism is introduced to handle interdependencies between decision stages, with distinct query construction methods for location and routing. Experiments on synthetic and benchmark datasets show that DRLHQ achieves superior solution quality and better generalization performance across problem scales compared to representative baselines.

## Method Summary
DRLHQ reformulates CLRP as a unified MDP where location decisions (selecting a depot) occur only when a previous subtour completes. A heterogeneous querying attention mechanism uses distinct query construction methods for location and routing decisions. The method follows an encoder-decoder structure with dynamic masking to ensure solution feasibility during autoregressive generation. Training uses REINFORCE with a shared baseline, employing POMO for multiple trajectories.

## Key Results
- DRLHQ outperforms representative traditional and DRL-based baselines on CLRP and OCLRP
- Achieves superior solution quality with better generalization across problem scales
- Demonstrates effectiveness on both synthetic and benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Unified MDP Formulation
Integrating facility location and vehicle routing into a single MDP may yield superior solutions compared to sequential decomposition by capturing interdependencies between decision stages. The authors reformulate CLRP as a unified MDP where location decisions occur only when a previous subtour completes, with a depot implicitly "opened" only if selected during route construction. Core assumption: The optimal solution can be constructed incrementally via subtours without requiring global pre-planning of all facility locations.

### Mechanism 2: Heterogeneous Querying Attention
Distinct attention query mechanisms for location vs. routing decisions allow the model to prioritize different contextual features. The decoder generates a Location Query using a GRU when starting a new subtour ($I_t=1$), and a Routing Query using standard context embeddings during traversal ($I_t=0$). Core assumption: Location decisions benefit from sequential historical context, whereas routing decisions rely more on immediate capacity and spatial context.

### Mechanism 3: Dynamic Masking for Feasibility
Dynamic masking ensures solution feasibility during autoregressive generation without needing complex penalty terms. At each decoding step, the model applies specific rules based on indicator state $I_t$, enforcing hard constraints by setting logits to large negative values. Core assumption: The action space remains sufficiently large to allow gradient flow, and constraints can be checked locally at each step.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) in Routing**
  - Why needed here: The paper reformulates a coupled optimization problem (CLRP) into an MDP. Understanding state transitions and reward structures is prerequisite to grasping the training loop.
  - Quick check question: Can you explain how the "state" changes when a vehicle visits a customer vs. returns to a depot?

- **Concept: Encoder-Decoder Architectures (Transformers)**
  - Why needed here: DRLHQ relies on a Transformer-style encoder for embeddings and a custom attention-based decoder.
  - Quick check question: How does the self-attention mechanism in the encoder capture the relationship between a customer node and a potential depot node?

- **Concept: Policy Gradient (REINFORCE)**
  - Why needed here: The model is trained using REINFORCE with a shared baseline, a standard DRL technique for combinatorial optimization.
  - Quick check question: Why is a "baseline" used in the reward calculation, and how does it reduce variance during training?

## Architecture Onboarding

- **Component map:** Input Feature Engineering (CCR/CDR) -> Encoder Embedding -> Context Construction -> Heterogeneous Query Generation -> Dynamic Masking -> Action Sampling

- **Critical path:** Input Feature Engineering (CCR/CDR) -> Encoder Embedding -> Context Construction -> **Heterogeneous Query Generation** -> Dynamic Masking -> Action Sampling

- **Design tradeoffs:**
  - End-to-end vs. Sequential: Unified MDP handles interdependencies better but complicates state space and masking logic
  - GRU in Decoder: Using a GRU for location queries preserves sequential depot history but increases decoder latency

- **Failure signatures:**
  - Capacity Overflow: Dynamic masking logic failing to account for depot capacity correctly
  - Mode Collapse: The model repeatedly selects the same subset of depots or gets stuck in loops
  - Dead Ends: Aggressive masking leaves no valid nodes to visit, resulting in crashes or incomplete solutions

- **First 3 experiments:**
  1. **Sanity Check (Small Scale):** Run DRLHQ on CLRP10/20 instances against Gurobi to verify the model learns valid, feasible solutions (Gap should be near 0%)
  2. **Ablation (Heterogeneous Query):** Replace the GRU-based location query with a standard linear query. Compare performance on CLRP100
  3. **Generalization Test:** Train on N=50 (uniform distribution) and test on the Prins et al. benchmark dataset to verify cross-distribution robustness

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the DRLHQ framework be effectively adapted to solve more complex variants of Location-Routing Problems, such as multi-echelon or time-windowed versions? [Basis: Section VI states plan to investigate "applying to solve more complex variants of routing problems"]

- **Open Question 2:** How can the cross-size and cross-distribution generalization capabilities of DRLHQ be further enhanced to minimize performance gaps on real-world data? [Basis: Section VI lists "enhancing the cross-size and cross-distribution generalization performance" as primary direction]

- **Open Question 3:** How can the model be modified to handle uncertain factors and stochastic elements inherent in real-world applications? [Basis: Section VI explicitly mentions "considering uncertain factors in real-world applications" as future research]

## Limitations
- Capacity parameters (exact ranges for synthetic data) are not specified, affecting model behavior and benchmarking fairness
- Heterogeneous query details (specific GRU hidden size and projection dimensions) are not detailed
- Generalization claims based on single test on Prins et al. benchmark may not represent all problem variations

## Confidence
- **High Confidence:** Unified MDP formulation and dynamic masking for feasibility are well-grounded and directly supported
- **Medium Confidence:** Heterogeneous querying attention mechanism's effectiveness is plausible but implementation details are sparse
- **Low Confidence:** Superiority claims over baselines rely on results not fully detailed in the provided text

## Next Checks
1. **Sanity Check (Small Scale):** Run DRLHQ on CLRP10/20 instances against Gurobi to verify the model learns valid, feasible solutions (Gap should be near 0%)
2. **Ablation (Heterogeneous Query):** Replace the GRU-based location query with a standard linear query. Compare performance on CLRP100 to measure the specific contribution of the "Heterogeneous" component
3. **Generalization Test:** Train on N=50 (uniform distribution) and test on the Prins et al. benchmark dataset (different distribution/demands) to verify cross-distribution robustness claims