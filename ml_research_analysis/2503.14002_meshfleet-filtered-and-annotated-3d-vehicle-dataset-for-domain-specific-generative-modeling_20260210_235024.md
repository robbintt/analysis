---
ver: rpa2
title: 'MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative
  Modeling'
arxiv_id: '2503.14002'
source_url: https://arxiv.org/abs/2503.14002
tags:
- objects
- dataset
- quality
- data
- high-quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MeshFleet, a high-quality 3D vehicle dataset
  extracted and filtered from Objaverse-XL for domain-specific generative modeling.
  The authors propose an automated pipeline that combines a manually labeled subset
  of 3D vehicle models with a quality classifier trained on DINOv2 and SigLIP embeddings.
---

# MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling

## Quick Facts
- **arXiv ID:** 2503.14002
- **Source URL:** https://arxiv.org/abs/2503.14002
- **Reference count:** 40
- **Primary result:** Automated pipeline combining manual labeling, multi-view embeddings, and active learning produces a 1,620-model 3D vehicle dataset that improves domain-specific generative model fine-tuning.

## Executive Summary
This work introduces MeshFleet, a high-quality 3D vehicle dataset extracted and filtered from Objaverse-XL for domain-specific generative modeling. The authors propose an automated pipeline that combines a manually labeled subset of 3D vehicle models with a quality classifier trained on DINOv2 and SigLIP embeddings. The classifier is iteratively refined using caption-based analysis and uncertainty estimation. The resulting MeshFleet dataset contains 1,620 curated vehicle models with detailed textures, metadata, and generated captions. Fine-tuning experiments with SV3D demonstrate that models trained on MeshFleet produce higher-quality and more multi-view-consistent outputs compared to those trained on datasets filtered using only captions or aesthetic scores. The study underscores the importance of targeted data selection and quality filtering for effective domain-specific 3D generative modeling. MeshFleet and the associated tools are released to support further research in this area.

## Method Summary
The authors created MeshFleet by developing an automated pipeline to filter and annotate high-quality 3D vehicle models from Objaverse-XL. The process began with a manually labeled subset of 6,200 objects (3D-Car-Quality Dataset) rated on a 1-5 quality scale. For each object, four orbital views were rendered (500x500px) to extract DINOv2 (structural) and SigLIP (semantic) embeddings. DINOv2 features were reduced from 4×257×768 dimensions to 3072 via PCA (preserving 92% variance) to match SigLIP's 4×768 dimensions. An MLP-Mixer classifier was trained on these combined features to distinguish high-quality (Labels 4-5) from low-quality (Labels 1-3) vehicles. The classifier was iteratively refined using Monte Carlo dropout uncertainty estimation to flag ambiguous samples for manual review, combined with caption-based misclassification analysis using a BART classifier. After refinement, the pipeline processed Objaverse-XL to extract 1,620 high-quality vehicles with detailed metadata including GPT-4o-mini captions, Florence-2 wheelbase measurements, and BART category classifications. Downstream validation involved fine-tuning SV3D on subsets of the filtered data, demonstrating superior performance compared to baseline approaches using caption-only or aesthetic-score filtering.

## Key Results
- MeshFleet contains 1,620 curated 3D vehicle models with detailed textures, metadata, and generated captions
- Binary classifier trained on manually labeled subset achieved 95.0% validation accuracy using combined DINOv2 and SigLIP embeddings
- SV3D fine-tuned on 380 Label 4-5 objects (CLIP-S=0.923, MSE=0.0218) outperformed models trained on larger but lower-quality datasets
- MeshFleet-generated captions achieved 94.7% exact match rate with human-labeled captions in downstream classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Multi-View Feature Fusion with MLP-Mixer
- Claim: Combining DINOv2 and SigLIP embeddings from multiple orbital viewpoints enables automated quality assessment that aligns with human judgment.
- Mechanism: Four rendered views (500×500px, equidistant orbital trajectory) provide complementary coverage. DINOv2 captures structural features; SigLIP provides semantic embeddings. PCA reduces concatenated DINOv2 features (4×257×768) to 3072 dimensions (92% explained variance). MLP-Mixer processes combined sequence (8×768 features), treating views as patches to exploit sequential structure rather than processing independently.
- Core assumption: Four external views capture sufficient 3D quality information without requiring direct mesh analysis.
- Evidence anchors:
  - [abstract] "classifier is trained on a manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings"
  - [section 3.4] "Our final binary car quality classifier, utilizing the combined features and the MLP-Mixer architecture, achieved a validation accuracy of 95.0%"
  - [corpus] Weak corpus evidence—no direct neighbors address multi-view 3D quality classification.
- Break condition: Degrades for objects with complex internal structures invisible from external views, or when texture quality diverges significantly from geometry.

### Mechanism 2: Uncertainty-Guided Active Learning for Iterative Refinement
- Claim: Monte Carlo dropout uncertainty estimation efficiently prioritizes samples for manual labeling, reducing annotation burden while improving classifier robustness.
- Mechanism: Dropout layers remain active during inference across 500 stochastic forward passes. High entropy in prediction distributions flags samples at decision boundaries for review. Caption-based analysis (BART classifier) provides orthogonal misclassification signal. Four refinement cycles expanded labeled data from ~4,000 to 6,200 objects.
- Core assumption: Model uncertainty correlates with information gain for training set expansion.
- Evidence anchors:
  - [abstract] "classifier is iteratively refined using caption-based analysis and uncertainty estimation"
  - [section 3.5] "The entropy of this distribution served as a measure of model uncertainty...Objects exhibiting high uncertainty...were flagged for manual review"
  - [corpus] Neighbor paper "Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering" supports uncertainty-based filtering as a general principle.
- Break condition: Fails if model is consistently overconfident on wrong predictions, or if caption quality is systematically poor.

### Mechanism 3: Quality-Over-Quantity Data Selection for Domain Fine-Tuning
- Claim: Training on fewer high-quality domain-specific samples outperforms larger datasets with lower relevance or domain alignment.
- Mechanism: Fine-tuning SV3D on 380 Label 4+5 objects achieved CLIP-S=0.923, MSE=0.0218, outperforming 1,265 Label 3+4+5 objects (CLIP-S=0.913, MSE=0.0228). Label 3 samples introduce geometric imprecision and specialized types (e.g., police cars) that may not generalize.
- Core assumption: Manual quality labels correlate with downstream fine-tuning performance.
- Evidence anchors:
  - [abstract] "models trained on MeshFleet produce higher-quality and more multi-view-consistent outputs compared to those trained on datasets filtered using only captions or aesthetic scores"
  - [section 3.2, Table 1] Label 4: MSE=0.0218, CLIP-S=0.923; Label 3: MSE=0.0228, CLIP-S=0.913
  - [corpus] Neighbor papers (TrustDataFilter, SLearnLLM) support targeted data selection for domain adaptation.
- Break condition: May not hold when task robustness requires exposure to diverse or degraded samples.

## Foundational Learning

- Concept: Principal Component Analysis (PCA) for Feature Dimensionality Reduction
  - Why needed here: DINOv2 features from 4 views yield 4×257×768 dimensions. PCA compresses to 3072 dims (matching SigLIP), preserving 92% variance for efficient classification.
  - Quick check question: If explained variance dropped to 0.75 after PCA reduction, would this indicate acceptable or problematic information loss for this application?

- Concept: Monte Carlo Dropout for Uncertainty Estimation
  - Why needed here: Active learning requires knowing which samples the model finds ambiguous. MC-dropout provides uncertainty estimates without ensembling or Bayesian networks.
  - Quick check question: Why must dropout layers remain active during inference for this technique to provide meaningful uncertainty?

- Concept: CLIP Score as Perceptual Quality Metric
  - Why needed here: Fine-tuning evaluation uses CLIP-S to measure semantic alignment between generated and reference images.
  - Quick check question: Would high CLIP-S combined with high MSE suggest good semantic matching but poor pixel-level consistency?

## Architecture Onboarding

- Component map:
  1. Rendering Pipeline (Blender): 4 orbital views per object, 500×500px, normalized objects
  2. Feature Extraction: DINOv2 (structural) + SigLIP (semantic) embeddings
  3. Dimensionality Reduction: PCA on concatenated DINOv2 → 3072 dims
  4. Classifier: MLP-Mixer (8×768 input → avg pooling → binary output)
  5. Active Learning Loop: MC-dropout uncertainty → manual review → retrain
  6. Metadata Extraction: GPT-4o-mini captions, Florence-2 wheelbase, BART category classification

- Critical path:
  Rendering → Feature Extraction → PCA Reduction → MLP-Mixer Classification → High/Low Quality Decision → (if high) Manual Review → MeshFleet Inclusion

- Design tradeoffs:
  1. 4 views vs. more: Paper reports marginal accuracy gains beyond 4 views with significant computational cost increase.
  2. Binary vs. 5-point labels: Collapsed labels 4-5 to positive, 1-3 to negative for stronger training signal.
  3. 2D rendering features vs. 3D-native: Point cloud features did not improve accuracy in preliminary experiments.

- Failure signatures:
  1. False positives: Toy cars, fictional vehicles, partial models pass aesthetic thresholds but fail domain requirements.
  2. Domain overfitting: Classifier trained on Objaverse-XL assigned 0% high-quality labels to 3DRealCar real-world scans (Section 10.2).
  3. Caption-only filtering: CarCaption800 (aesthetic ≥6.5) degraded SV3D performance below baseline (CLIP-S=0.862 vs. 0.897).

- First 3 experiments:
  1. Reproduce Label 4 vs. Label 3 fine-tuning comparison on held-out validation to confirm quality-over-quantity finding in your environment.
  2. Run MeshFleet classifier on your own 3D vehicle assets to identify false positive patterns specific to your domain.
  3. Compare caption-based filtering (custom LLM prompts) vs. MeshFleet classifier outputs on a sample of Objaverse-XL objects to map divergence zones.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Vision-Language Models (VLMs) be fine-tuned on the 3D-Car-Quality Dataset to perform direct filtering and description generation, effectively eliminating the need for manual labeling cycles?
- Basis in paper: [explicit] Section 4.1 states that "Leveraging the labeled data from the 3D-Car-Quality Dataset to perform such optimization represents a promising avenue for future research."
- Why unresolved: The current pipeline relies on general-purpose BART-based models for text filtering and requires an iterative active learning loop with manual review, rather than using the labeled dataset to train a specialized filtering model.
- What evidence would resolve it: Experiments demonstrating that a VLM fine-tuned on the dataset can filter Objaverse-XL with precision comparable to the manual+active-learning pipeline.

### Open Question 2
- Question: Do 3D-native geometric embeddings (e.g., structured latents) offer a better accuracy-to-computation trade-off than 2D multi-view rendering features (DINOv2/SigLIP) for quality assessment?
- Basis in paper: [explicit] Section 5 discusses that "3D-native embedding models... may offer improved performance" but notes the "computational cost associated with these models must be considered" since they require significantly more renderings (150 vs 4).
- Why unresolved: The authors utilized 2D features for efficiency, and preliminary tests with point clouds did not show improvements. The potential gain in geometric understanding from 3D-native features versus their high computational cost remains unquantified.
- What evidence would resolve it: A comparative benchmark showing classification accuracy and processing time for 3D-native embeddings versus the current 2D MLP-Mixer approach on the same dataset.

### Open Question 3
- Question: Can the definition of "high-quality" be generalized to include real-world scanned vehicle data (like 3DRealCar), or must synthetic CAD and real-world scans remain separate domains for generative fine-tuning?
- Basis in paper: [inferred] Section 10.2 reveals the classifier assigns a "low-quality" label to all tested 3DRealCar objects because the manual training data prioritized "clean, synthetic" models over "noisy" scans.
- Why unresolved: The current classifier is explicitly biased against scan artifacts (noise, occlusions), preventing the dataset from leveraging real-world scans, which the authors acknowledge as a complementary resource.
- What evidence would resolve it: A modified classifier or distinct labeling schema that successfully identifies high-utility vehicles within real-world scan datasets for use in fine-tuning.

## Limitations
- Classifier shows near-zero effectiveness on real-world scan datasets like 3DRealCar, indicating strong overfitting to synthetic data distributions
- Binary quality labels collapse nuanced human judgments (1-5 scale) into two categories, potentially losing fine-grained quality distinctions
- The 4-view approach may miss critical quality indicators visible only from other perspectives or through direct mesh analysis

## Confidence
- **High confidence:** The quality-over-quantity finding (Label 4+5 vs. Label 3+4+5 fine-tuning results) is well-supported by quantitative metrics (CLIP-S 0.923 vs 0.913, MSE 0.0218 vs 0.0228) and consistent with established data selection principles
- **Medium confidence:** The automated filtering pipeline shows strong validation accuracy (95.0%) but limited testing across diverse domains beyond synthetic vehicles
- **Low confidence:** Generalization claims to other 3D object categories or real-world scan data lack empirical validation

## Next Checks
1. Test classifier performance on a held-out validation set from Objaverse-XL with known quality labels to verify the 95% accuracy claim
2. Apply the MeshFleet pipeline to a different 3D category (e.g., furniture or animals) to assess cross-domain applicability
3. Evaluate the fine-tuning performance of SV3D using progressively larger subsets of MeshFleet data to determine the optimal training set size for this domain