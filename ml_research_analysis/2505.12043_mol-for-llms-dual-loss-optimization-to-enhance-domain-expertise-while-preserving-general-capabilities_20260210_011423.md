---
ver: rpa2
title: 'MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving
  General Capabilities'
arxiv_id: '2505.12043'
source_url: https://arxiv.org/abs/2505.12043
tags:
- uni00000013
- general
- uni00000011
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of Losses (MoL), a dual-loss training
  framework for large language models that addresses catastrophic forgetting during
  domain-specific adaptation. The method applies cross-entropy loss to domain-specific
  corpora for knowledge acquisition while using Kullback-Leibler divergence with the
  base model for general corpora to preserve foundational capabilities.
---

# MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities

## Quick Facts
- arXiv ID: 2505.12043
- Source URL: https://arxiv.org/abs/2505.12043
- Reference count: 13
- Primary result: Dual-loss training achieves 27.9% higher accuracy on Math-500 and 83.3% improvement on AIME25 by decoupling domain knowledge acquisition from general capability preservation

## Executive Summary
This paper introduces Mixture of Losses (MoL), a dual-loss training framework for large language models that addresses catastrophic forgetting during domain-specific adaptation. The method applies cross-entropy loss to domain-specific corpora for knowledge acquisition while using Kullback-Leibler divergence with the base model for general corpora to preserve foundational capabilities. Experiments demonstrate that a 1:1 domain-to-general corpus ratio optimally balances training, achieving significant improvements on mathematical reasoning benchmarks while maintaining general language skills. The framework effectively maintains general language skills while enhancing domain expertise without requiring extensive hyperparameter tuning.

## Method Summary
MoL decouples optimization objectives for domain-specific and general corpora by applying cross-entropy loss to domain corpora and KL divergence to general corpora. The framework uses a 1:1 sample ratio between domain and general corpora, with each sample tagged by corpus type. During training, samples are routed to appropriate loss functions, with small mixing coefficients (α≈0.01) for numerical stability. The method preserves general capabilities by aligning the training model's distribution with the frozen base model's distribution on general data, while acquiring domain knowledge through token-level cross-entropy loss on domain-specific data.

## Key Results
- 27.9% higher accuracy on Math-500 benchmark in non-think reasoning mode compared to traditional approaches
- 83.3% improvement on challenging AIME25 subset in think mode
- 1:1 domain-to-general corpus ratio identified as optimal for balancing training

## Why This Works (Mechanism)

### Mechanism 1: Dual-Loss Objective Decoupling
- Claim: Separating loss functions by corpus type allows simultaneous domain knowledge acquisition and general capability preservation
- Mechanism: Cross-entropy loss on domain corpora drives "hard" token-level learning for factual knowledge. KL divergence on general corpora enforces "soft" alignment to the base model's probability distribution, preventing distribution shift in foundational skills
- Core assumption: Domain and general corpora serve fundamentally different training purposes that interfere when optimized jointly under a single loss
- Evidence anchors:
  - [abstract]: "decouples optimization objectives for domain-specific and general corpora"
  - [section 3]: "CE loss is applied to domain-corpus to ensure knowledge acquisition, while KL divergence aligns general-corpus training with the base model's foundational capabilities"
  - [corpus]: Weak direct evidence; neighbor papers address domain injection but not dual-loss specifically
- Break condition: If domain and general data distributions overlap significantly, decoupled losses may provide conflicting gradient signals

### Mechanism 2: KL Divergence as Distributional Anchor
- Claim: Treating the base model's output distribution as "soft labels" preserves context-dependent generation patterns better than hard-label CE loss
- Mechanism: Unlike CE loss, which forces deterministic token prediction, KL divergence minimizes the divergence between the training model's distribution p_θ and the frozen base model's distribution p_0. This retains reasoning patterns (e.g., chain-of-thought) rather than memorizing fixed sequences
- Core assumption: The base model's probability distribution encodes valuable generalizable reasoning patterns worth preserving during domain adaptation
- Evidence anchors:
  - [abstract]: "KL divergence aligns general-corpus training with the base model's foundational capabilities"
  - [section 3]: "This approach allows the target model to learn context-dependent generation patterns rather than memorizing fixed token sequences"
  - [corpus]: No direct corpus support for this specific mechanism in neighbors
- Break condition: If the base model has weak foundational capabilities, anchoring to it may propagate or amplify existing weaknesses

### Mechanism 3: Gradient Modulation via Reverse KL
- Claim: KL divergence produces smaller gradient magnitudes than CE loss on general corpora, creating an implicit regularization effect
- Mechanism: The paper observes that KL-based gradients remain consistently smaller than CE-based gradients until convergence, suggesting a "negative feedback regression" that stabilizes general capability retention. Reverse KL (used here) further avoids over-emphasizing low-probability regions in the teacher distribution
- Core assumption: Smaller gradients on general corpora reduce interference with domain learning while maintaining alignment
- Evidence anchors:
  - [section 5.2.1]: "The gradients of the KL divergence-based framework remain consistently smaller than those of the CE-based alternative until convergence"
  - [section 3]: "We adopt the proposal of reverse KL divergence to mitigate overestimation of low-probability regions"
  - [corpus]: No external corpus validation of this gradient dynamics claim
- Break condition: If domain corpus dominates (e.g., >2:1 ratio), gradient asymmetry may cause training instability or under-regularization

## Foundational Learning

- **Catastrophic Forgetting**:
  - Why needed here: MoL directly addresses this phenomenon where LLMs lose previously learned general capabilities when trained on domain-shifted data
  - Quick check question: Why might fine-tuning a general-purpose LLM exclusively on medical QA pairs degrade its ability to write Python code?

- **KL Divergence (Forward vs. Reverse)**:
  - Why needed here: The paper uses reverse KL for alignment; understanding the distinction clarifies why this choice matters for avoiding low-probability token overemphasis
  - Quick check question: Given two distributions P (base model) and Q (training model), which KL direction (KL[P||Q] vs. KL[Q||P]) penalizes Q assigning probability mass where P has near-zero probability?

- **Knowledge Distillation (White-Box)**:
  - Why needed here: MoL's use of KL divergence borrows from white-box distillation, where both teacher and student probability distributions are accessible
  - Quick check question: Why does white-box distillation require vocabulary alignment between teacher and student models?

## Architecture Onboarding

- **Component map**:
  Data loader -> Loss router -> Base model reference -> Training loop

- **Critical path**:
  1. Prepare domain and general corpora at ~1:1 sample ratio (not token ratio)
  2. Tag each sample with corpus-type metadata during preprocessing
  3. Load frozen base model for KL computation (requires memory overhead)
  4. During training, route each sample to appropriate loss function
  5. Apply α-blending: for domain samples, L = (1-α)L_CE + αL_KL; for general samples, L = αL_CE + (1-α)L_KL
  6. Monitor validation CE loss on domain data; stop near convergence (~1.3 epochs observed in paper)

- **Design tradeoffs**:
  - **Memory**: Requires keeping base model in memory; truncation to top-31 tokens reduces but doesn't eliminate overhead
  - **Corpus ratio**: 1:1 is empirically optimal but may not hold for all domains; deviation causes either forgetting (too much domain) or underfitting (too much general)
  - **α value**: Near-zero α critical; α=0.5 showed significant degradation in math reasoning

- **Failure signatures**:
  - General benchmark scores drop >5% from base model → corpus ratio likely too domain-heavy
  - Domain performance plateaus early while general loss remains flat → KL may be over-constraining; check α and learning rate
  - Instruction-following errors (e.g., wrong multiple-choice format) → insufficient general corpus exposure

- **First 3 experiments**:
  1. **CE-only baseline**: Train on 1:1 mixed corpus using only CE loss to measure catastrophic forgetting magnitude on general benchmarks
  2. **Corpus ratio ablation**: Run MoL with domain:general ratios of 1:0.5, 1:1, 1:1.5, 1:2 to verify 1:1 optimality for your specific domain
  3. **α sensitivity test**: Compare α=0.01 vs α=0.5 to confirm near-zero α is required; expect ~10-20% math reasoning degradation at α=0.5

## Open Questions the Paper Calls Out
None

## Limitations
- Data Representation Bias: Assumes domain and general corpora can be balanced at a 1:1 sample ratio, ignoring differences in token density and information content
- Distributional Overlap Assumption: Dual-loss decoupling relies on domain and general corpora having sufficiently distinct distributions, without quantitative analysis of corpus overlap
- KL Divergence Fidelity: Truncation to top-31 tokens for KL computation introduces approximation error that scales with base model output diversity

## Confidence
**High Confidence Claims** (Experimental Results):
- The 27.9% accuracy improvement on Math-500 non-think mode is well-supported by controlled experiments with clear baselines
- The 83.3% improvement on AIME25 think mode demonstrates measurable domain expertise gains
- The 1:1 corpus ratio showing optimal balance is empirically validated across multiple benchmarks

**Medium Confidence Claims** (Theoretical Mechanisms):
- The catastrophic forgetting mitigation is demonstrated but relies on indirect evidence (benchmark comparisons rather than ablation studies)
- The KL divergence preservation mechanism is theoretically sound but lacks ablation studies comparing against alternative preservation methods
- The gradient dynamics explanation for stability is observed but not rigorously analyzed

**Low Confidence Claims** (Generalizability):
- Claims about applicability to "any domain" lack validation beyond mathematical reasoning
- The framework's effectiveness for domains with different data characteristics (sparse vs. dense information) is untested
- The long-term stability of preserved general capabilities beyond immediate post-training evaluation is not assessed

## Next Checks
1. **Corpus Distribution Analysis**: Compute Jensen-Shannon divergence between domain and general corpora for your target domain. If divergence < 0.3, the dual-loss assumption may be invalid and a unified loss function might perform better.

2. **α Parameter Sensitivity Grid Search**: Systematically evaluate α values in {0.001, 0.01, 0.1, 0.5} on a held-out validation set from your domain. Plot domain performance vs. general capability retention to identify the optimal trade-off point specific to your data characteristics.

3. **Token-Level Attribution Study**: Use integrated gradients or attention-based attribution to identify which tokens most contribute to performance gains vs. losses when switching from CE-only to MoL training.