---
ver: rpa2
title: Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified
  Clustering on Incomplete and Noise Multi-View Data
arxiv_id: '2512.21516'
source_url: https://arxiv.org/abs/2512.21516
tags:
- clustering
- multi-view
- incomplete
- learning
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel Global-Local Graph-guided Contrastive
  Learning (GLC) framework for unified clustering on incomplete and noise multi-view
  data. The key contributions include: (1) A global-graph guided contrastive learning
  module that constructs a global-view affinity graph to discover indirect semantic
  correlations and generate additional positive pairs, addressing the rare-paired
  issue in incomplete data; (2) A local-graph weighted contrastive learning module
  that leverages local neighbors to generate pair-wise weights, adaptively strengthening
  reliable pairs while suppressing noise, addressing the mis-paired issue; (3) An
  imputation-free unified framework that integrates both modules for simultaneous
  handling of incomplete and noise multi-view clustering.'
---

# Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data

## Quick Facts
- arXiv ID: 2512.21516
- Source URL: https://arxiv.org/abs/2512.21516
- Reference count: 40
- GLC achieves 75.5% ACC on DHA under incomplete setting (vs. 32.0% baseline)

## Executive Summary
This paper introduces GLC (Global-Local Graph-guided Contrastive Learning), a unified framework for clustering incomplete and noisy multi-view data without imputation. The method addresses two key challenges: the rare-paired issue in incomplete data and the mis-paired issue in noisy data. By integrating global-graph guided contrastive learning with local-graph weighted contrastive learning, GLC discovers indirect semantic correlations and adaptively weights reliable pairs while suppressing noise.

## Method Summary
GLC operates through two complementary modules that work synergistically. The global-graph guided module constructs a global-view affinity graph to discover indirect semantic correlations across views, generating additional positive pairs that address the rare-paired issue in incomplete data. The local-graph weighted module leverages local neighbors to generate pair-wise weights, adaptively strengthening reliable pairs while suppressing noise to address the mis-paired issue. These modules are integrated into an imputation-free unified framework that performs simultaneous clustering on incomplete and noisy multi-view data.

## Key Results
- GLC achieves 75.5% ACC on DHA dataset under incomplete setting (vs. 32.0% baseline)
- GLC achieves 80.5% ACC on ProteinFold dataset under noise setting (vs. 25.7% baseline)
- GLC demonstrates robust performance across four datasets (DHA, LandUse-21, ProteinFold, ALOI) in incomplete, noise, and combined settings

## Why This Works (Mechanism)
The effectiveness of GLC stems from its dual approach to handling multi-view data challenges. The global-graph module addresses the fundamental issue that incomplete data creates few positive pairs by discovering indirect semantic correlations across views, effectively expanding the positive pair pool. The local-graph module tackles the noise problem by learning adaptive weights for pairs based on local neighborhood consistency, allowing the model to downweight unreliable pairs that could mislead the contrastive learning process. Together, these mechanisms create a robust learning environment that can handle both data quality issues simultaneously.

## Foundational Learning
- **Multi-view clustering**: Clustering where each data instance has multiple representations from different views; needed because real-world data often comes from heterogeneous sources
- **Contrastive learning**: Learning representations by pulling similar samples together and pushing dissimilar ones apart; needed to learn discriminative features without labels
- **Graph-guided learning**: Using graph structures to guide representation learning; needed to capture complex relationships between samples
- **Imputation-free methods**: Approaches that work directly with incomplete data without filling missing values; needed to avoid introducing bias from imputation
- **Pair-wise weighting**: Assigning different importance to different sample pairs during training; needed to handle noisy and unreliable data pairs

## Architecture Onboarding
**Component Map**: Global Graph Module -> Local Graph Module -> Contrastive Learning -> Clustering Output

**Critical Path**: Data Input -> Global Affinity Graph Construction -> Local Neighbor Graph Construction -> Weighted Contrastive Loss -> Clustering Head

**Design Tradeoffs**: The framework trades computational complexity for robustness - the global graph construction adds overhead but enables handling of incomplete data without imputation. The local weighting mechanism adds flexibility but requires careful neighbor selection.

**Failure Signatures**: 
- Poor performance on datasets with >50% missing data
- Sensitivity to graph construction parameters
- Computational bottlenecks with large numbers of views
- Over-smoothing when local neighborhoods are too densely connected

**3 First Experiments**:
1. Test on a simple multi-view dataset with controlled missing rates (10%, 30%, 50%)
2. Compare GLC against a strong single-view baseline on a clean dataset
3. Evaluate the contribution of global vs local modules using ablation on a noise-free dataset

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- No systematic sensitivity analysis provided to support claims about hyperparameter robustness
- Performance on extreme missing rates (>50%) was not thoroughly evaluated
- Computational complexity and scalability for large-scale datasets with hundreds of views remains unclear
- Lacks detailed qualitative analysis of what learned representations capture

## Confidence
**High Confidence**: Core architectural contributions and integration strategy are clearly described and sound.

**Medium Confidence**: Quantitative results showing substantial performance improvements, though baseline implementation details are not fully specified.

**Low Confidence**: Claims about hyperparameter insensitivity and stable optimization lack empirical support.

## Next Checks
1. Conduct comprehensive ablation study isolating contributions of global-graph module, local-graph module, and their interaction with intermediate validation metrics.

2. Evaluate GLC's performance across a broader spectrum of missing data rates (10%, 30%, 50%, 70%) on at least two datasets to determine breaking points.

3. Test GLC on a larger dataset with more views (10+ views) to assess computational scalability and compare memory usage and runtime against baseline methods.