---
ver: rpa2
title: Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk
  Detection
arxiv_id: '2507.00693'
source_url: https://arxiv.org/abs/2507.00693
tags:
- suicide
- risk
- speech
- features
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting suicide risk in
  adolescents using spontaneous speech. The authors propose a multimodal framework
  that integrates acoustic embeddings, text embeddings, and interpretable features
  extracted by a large language model (LLM).
---

# Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection

## Quick Facts
- arXiv ID: 2507.00693
- Source URL: https://arxiv.org/abs/2507.00693
- Reference count: 0
- Primary result: Proposed multimodal framework achieves 74% accuracy and 0.740 F1 on test set, ranking first in SW1 challenge

## Executive Summary
This study tackles adolescent suicide risk detection using spontaneous speech by combining acoustic, textual, and LLM-extracted interpretable features. The authors employ a structured DeepSeek-R1 prompt to extract five clinically-relevant behavioral indicators from transcripts, which are then classified alongside acoustic and semantic embeddings. The multimodal approach, particularly the integration of interpretable LLM features with traditional speech analysis, achieves state-of-the-art performance on the SW1 dataset. The framework emphasizes transparency by providing human-readable explanations for risk assessments through the LLM's extracted markers.

## Method Summary
The method processes spontaneous speech recordings through a three-branch multimodal framework. Audio data undergoes noise reduction with Koala, segmentation into 30-second chunks with 10% overlap, and pooling per speaker. Acoustic embeddings are extracted using pretrained models (HuBERT, Wav2Vec2, Whisper) and classified with MLPs. Text transcripts from Whisper ASR are embedded with BERT/XLM-RoBERTa and classified similarly. The LLM branch uses DeepSeek-R1 with a structured prompt to extract five binary indicators (self-harm, pressure, social support, unhealthy outlets, exercise) from transcripts, which are then classified with Random Forest. Final predictions are made via weighted voting across selected model outputs, with higher weight given to DeepSeek-R1 features.

## Key Results
- Achieved 74% accuracy and 0.740 F1 on the SW1 test set, ranking first in the challenge
- LLM-extracted interpretable features achieved 0.652 accuracy and 0.679 F1 on ER task, outperforming BERT/RoBERTa embeddings
- Audio models outperformed text models on PR and ED tasks, demonstrating complementary acoustic signal
- Weighted voting ensemble combining audio, text, and LLM features improved performance over unimodal approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured LLM prompting extracts interpretable risk indicators from transcripts with higher discriminative power than embedding-based text models alone.
- Mechanism: DeepSeek-R1 is prompted to identify five suicide-relevant behavioral indicators and return binary labels plus supporting text spans. These sparse, clinically-grounded features are classified via Random Forest, achieving 0.652 accuracy and 0.679 F1 on the ER taskâ€”outperforming BERT/RoBERTa embeddings.
- Core assumption: The selected indicators (self-harm, pressure, social support, unhealthy outlets, exercise) are both present in spontaneous speech and sufficiently discriminative for adolescent suicide risk.
- Evidence anchors: Abstract states LLMs extract interpretable features enhancing transparency; section 3.4 shows structured prompt with binary labels plus bracketed evidence; corpus addresses similar LLM interpretability goals but lacks direct replication for speech.
- Break condition: If transcripts lack explicit mentions of these behaviors, the binary features become uninformative; embedding models may then dominate.

### Mechanism 2
- Claim: Acoustic models capture emotional and prosodic cues that transcribed text omits, yielding complementary signal across speech tasks.
- Mechanism: Pretrained audio encoders (HuBERT, Wav2Vec2, Whisper) extract embeddings reflecting intonation, energy, and timing. These are pooled per speaker and classified via MLP. Audio models outperform text models on PR and ED tasks, suggesting non-semantic channels carry risk-relevant variance.
- Core assumption: Suicide risk manifests in measurable acoustic deviations beyond linguistic content, and these deviations are captured by models pretrained on general speech.
- Evidence anchors: Abstract mentions integration of acoustic embeddings; section 4.2 states audio models capture emotional cues embedded in speech; corpus supports multimodal benefit with WavLM audio embeddings for depression.
- Break condition: If recording conditions introduce noise or if affective flattening masks prosodic cues, acoustic signal may degrade; the Koala denoising step partially mitigates this.

### Mechanism 3
- Claim: Weighted voting across modalities improves generalization by balancing complementary weaknesses.
- Mechanism: Final prediction aggregates outputs from audio models (ER/PR/ED), RoBERTa (ER), and DeepSeek-R1 (ER), with higher weight assigned to DeepSeek-R1. The combination achieves 0.74 accuracy/F1 on test, surpassing audio-only voting.
- Core assumption: Errors across modalities are partially uncorrelated, and interpretable features provide a stabilizing signal for out-of-distribution cases.
- Evidence anchors: Abstract reports 74% accuracy and 0.740 F1; section 4.2 shows voting ablation with substantial test performance improvement; corpus lacks direct replication of weighted voting for this exact task.
- Break condition: If test distribution shifts, the learned weighting may not transfer; cross-dataset validation is needed.

## Foundational Learning

- Concept: **Prompt engineering for structured extraction**
  - Why needed here: The LLM must output consistent binary labels + text spans; unstructured outputs break downstream classification.
  - Quick check question: Can you write a prompt that forces JSON-like output with exactly five binary fields and optional evidence strings?

- Concept: **Speaker-level pooling from segmented audio**
  - Why needed here: Raw recordings vary in length; models require fixed-dimensional inputs per participant.
  - Quick check question: Given 30-second segments with 10% overlap, what pooling operation preserves speaker identity while discarding segment-level noise?

- Concept: **Late fusion via voting vs. early concatenation**
  - Why needed here: The paper uses voting (decision-level fusion); understanding when this outperforms feature-level concatenation is critical for architecture choices.
  - Quick check question: If one modality has much higher confidence calibration, should you weight votes equally or proportionally to validation performance?

## Architecture Onboarding

- Component map:
  - Raw audio -> Koala denoiser -> 30s segments (10% overlap) -> mean pooling per speaker
  - Branch 1 (Acoustic): HuBERT/Wav2Vec2/Whisper embeddings -> MLP classifier
  - Branch 2 (Text): Whisper ASR -> BERT/RoBERTa embeddings -> MLP classifier
  - Branch 3 (LLM): Transcript -> DeepSeek-R1 (structured prompt) -> 5 binary features -> Random Forest
  - Fusion: Weighted voting across selected branch outputs -> final prediction

- Critical path:
  1. Noise reduction must complete before segmentation; otherwise boundaries shift.
  2. ASR quality limits both Branch 2 and Branch 3; transcription errors propagate to LLM.
  3. DeepSeek-R1 inference is the latency bottleneck; batch prompts if deploying in real-time.

- Design tradeoffs:
  - Interpretability vs. performance: DeepSeek-R1 features are transparent but sparse; dense embeddings may capture more signal but are uninterpretable.
  - Model complexity vs. data size: With only 400 training samples, deep classifiers risk overfitting; MLP with dropout and early stopping is used.
  - Task selection: ER task yields best LLM performance; PR/ED rely more on audio. Fusing all tasks helps but increases pipeline complexity.

- Failure signatures:
  - All LLM features return 0 -> Random Forest defaults to majority class; check prompt adherence and transcript quality.
  - Audio-only voting generalizes poorly to test -> add text/LLM components.
  - Large dev-test accuracy gap -> possible overfitting to dev; reduce model capacity or increase regularization.

- First 3 experiments:
  1. Ablate DeepSeek-R1: Remove Branch 3 and re-run voting to quantify interpretability contribution (expect ~2-3% F1 drop based on Table 3 patterns).
  2. Swap ASR model: Replace Whisper transcription with a faster ASR and measure Branch 2/3 degradation.
  3. Cross-validate indicator set: Add or remove one LLM indicator and measure ER task performance to validate indicator relevance.

## Open Questions the Paper Calls Out

- **Question**: Can the speech markers and LLM-extracted features identified in this cross-sectional study be validated for the longitudinal prediction of future suicidal behavior?
- **Basis in paper**: [explicit] The authors explicitly state the findings reflect participants' "immediate responses" and do not "serve as a prediction of future suicidal behavior."
- **Why unresolved**: The study design relies on the SW1 challenge dataset, which provides a static assessment of current risk rather than tracking behavioral outcomes over time.
- **What evidence would resolve it**: A longitudinal study correlating baseline acoustic and semantic features with subsequent behavioral incidents or changes in clinical status over a defined follow-up period.

- **Question**: To what extent does the reliance on the MINI-KID scale as ground truth introduce noise into the model, given the potential for adolescent underreporting?
- **Basis in paper**: [explicit] The conclusion notes that the MINI-KID's reliance on self-reported data "can lead to underreporting or misinterpretation of symptoms" and may fail to capture the "dynamic nature" of ideation.
- **Why unresolved**: The model optimizes to match the MINI-KID labels, so if the labels themselves are flawed due to psychological barriers in the adolescents, the model may learn to detect "performative compliance" rather than genuine internal states.
- **What evidence would resolve it**: Comparative validation against implicit association tests, clinical interviews, or third-party observer reports to verify the accuracy of the "at risk" labels used for training.

- **Question**: Is the superior performance of the DeepSeek-R1 component attributable to the specific reasoning capabilities of the model or the manual definition of risk indicators in the prompt?
- **Basis in paper**: [inferred] While the paper highlights DeepSeek-R1's success, it does not conduct an ablation study on the prompt engineering or compare it against standard rule-based extraction methods for the defined indicators.
- **Why unresolved**: It is unclear if the LLM is providing complex semantic reasoning or simply executing a keyword-adjacent search defined by the detailed prompt instructions.
- **What evidence would resolve it**: An analysis comparing the LLM's feature extraction accuracy against human annotators using the same criteria, and testing the framework with different LLMs to see if the performance is architecture-dependent.

## Limitations

- The study relies on a single challenge dataset (SW1) with only 400 training samples, limiting external validity and generalizability.
- Exact voting weights for ensemble fusion are not specified, making faithful replication difficult without access to challenge-specific tuning.
- The ASR model used to generate transcripts for LLM input is not named, which is critical since transcription quality directly impacts the interpretable features.

## Confidence

- **High confidence**: The multimodal fusion approach and general architecture design are well-documented and reproducible. The observation that audio models outperform text models on certain tasks is directly supported by Table 2.
- **Medium confidence**: The reported 74% accuracy and 0.740 F1 on the test set, as these results depend on unspecified voting weights and challenge dataset access. The claim that interpretable features enhance transparency is supported but not empirically validated against human-annotated ground truth.
- **Low confidence**: The generalizability of the five selected suicide risk indicators beyond the SW1 dataset, as no cross-dataset validation is performed and the indicator selection criteria are not fully justified.

## Next Checks

1. Perform ablation studies removing each of the five LLM indicators individually to quantify their independent contribution and validate their clinical relevance.
2. Test the weighted voting ensemble on an external suicide risk dataset to assess generalizability beyond the SW1 challenge.
3. Compare the interpretability of DeepSeek-R1 features against human-annotated transcripts to verify that the LLM is capturing clinically meaningful indicators rather than spurious correlations.