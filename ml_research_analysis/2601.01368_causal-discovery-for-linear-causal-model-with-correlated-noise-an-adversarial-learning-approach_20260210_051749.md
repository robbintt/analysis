---
ver: rpa2
title: 'Causal discovery for linear causal model with correlated noise: an Adversarial
  Learning Approach'
arxiv_id: '2601.01368'
source_url: https://arxiv.org/abs/2601.01368
tags:
- causal
- structure
- distribution
- data
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fGAN-CD, a novel method for causal structure
  learning in linear systems with unmeasured confounding. The approach reformulates
  structure learning as minimizing Bayesian free energy, which is shown to be equivalent
  to minimizing the KL divergence between the true and model-generated data distributions.
---

# Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach

## Quick Facts
- arXiv ID: 2601.01368
- Source URL: https://arxiv.org/abs/2601.01368
- Reference count: 6
- Method outperforms differentiable baseline ABIC on structural recovery metrics in 4-node confounded graphs

## Executive Summary
This paper introduces fGAN-CD, a novel method for causal structure learning in linear systems with unmeasured confounding. The approach reformulates structure learning as minimizing Bayesian free energy, which is shown to be equivalent to minimizing the KL divergence between the true and model-generated data distributions. By leveraging the f-GAN framework, the problem is transformed into a min-max adversarial optimization task, with Gumbel-Softmax relaxation enabling gradient-based search in the discrete graph space. The method aims to recover the binary causal structure (DAG and confounding correlation) rather than specific weight parameters.

## Method Summary
The fGAN-CD method treats causal structure learning as minimizing Bayesian free energy, equivalent to KL divergence minimization. Using the f-GAN framework, this intractable objective is converted into a min-max adversarial optimization problem. The discrete graph structure is parameterized by logits and sampled via Gumbel-Softmax relaxation, enabling gradient-based optimization. A discriminator network distinguishes real from generated data, while the generator updates its structure logits to minimize the adversarial loss plus an acyclicity penalty. After training, soft structures are thresholded to obtain binary causal graphs.

## Key Results
- fGAN-CD achieves lower structural Hamming distance (2.17 vs 3.4) than baseline ABIC on 4-node confounded graphs
- Higher arrowhead F1 score (0.667 vs 0.1) demonstrates better confounding edge recovery
- Correctly identifies conditional independence constraints in high-confounding scenarios where ABIC fails

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Free Energy as Model Selection Criterion
Minimizing expected Bayesian free energy over causal structures is equivalent to minimizing KL divergence between true data distribution and model's marginal likelihood. This allows treating structure learning as distribution matching rather than parameter estimation. Core assumption: The causal model is singular (contains latent confounders), making BIC unreliable. Evidence: Mathematical decomposition shown in Section 3.1; Watanabe's singular learning theory provides foundation. Break condition: If model were regular, standard BIC methods would be simpler and equivalent.

### Mechanism 2: Variational Divergence Minimization via f-GAN
The intractable KL divergence minimization is converted into a tractable min-max adversarial optimization problem using f-GAN framework. The f-divergence (with KL as special case) is lower-bounded by a variational objective involving discriminator network. Maximizing this bound while minimizing w.r.t. generator yields adversarial training loop. Core assumption: Discriminator is sufficiently expressive for tight variational bound. Evidence: Standard GAN objective derivation from f-GAN; Nowozin et al. (2016) provides foundation. Break condition: Under-parameterized discriminator or training instability degrades structure learning.

### Mechanism 3: Gumbel-Softmax Relaxation for Differentiable Discrete Search
Discrete binary graph structures are learned via gradient-based optimization through continuous relaxation. Gumbel-Softmax produces differentiable continuous approximation that approaches {0,1} as temperature τ → 0, enabling backpropagation through structure sampling. Core assumption: Temperature annealing properly transitions from exploration to exploitation without gradient explosion. Evidence: Gumbel-Softmax formula defined in Section 3.4; Jang et al. (2017) provides foundation. Break condition: If temperature too high, structures remain soft; if too low too fast, gradient variance prevents learning.

## Foundational Learning

- Concept: **Linear Structural Equation Models (SEMs) with Unmeasured Confounding**
  - Why needed here: The method assumes data generated from X = XB + E where E has correlated covariance representing latent confounders. Understanding this is essential for grasping the distinction between SB (directed edges) and SΣ (confounding correlations).
  - Quick check question: Given a 3-variable system where X₁ ← L → X₂ and X₂ → X₃, can you write the B matrix and Σ matrix that represent this ADMG?

- Concept: **Bayesian Free Energy vs. BIC for Model Selection**
  - Why needed here: The paper's central motivation is that BIC fails for singular models with latent variables. Understanding why free energy is the "correct" criterion for singular models is essential to grasp why this approach is theoretically justified.
  - Quick check question: Explain in one sentence why the likelihood function of a singular model cannot be approximated by a Gaussian, and what consequence this has for BIC.

- Concept: **f-divergence and its Variational Lower Bound**
  - Why needed here: The method relies on the mathematical result that f-divergence can be lower-bounded by an expectation involving a variational function (discriminator). This enables GAN-style training without explicitly computing the intractable KL divergence.
  - Quick check question: Why can't we compute D_KL(P_data || Q_model) directly, and how does the variational bound solve this?

## Architecture Onboarding

- Component map: Logits A_B, A_Σ -> Gumbel-Softmax layer -> Soft structures S̃_B, S̃_Σ -> Weight sampling -> Masked B*, Σ* -> Data generation X_fake -> Discriminator D_ω -> Adversarial loss

- Critical path:
  1. Initialize A_B, A_Σ (small random), discriminator weights ω
  2. Per batch: Sample Z ~ N(0, I), generate S̃_B, S̃_Σ via Gumbel-Softmax at current τ
  3. Sample weight matrices B', Σ' from prior; mask with soft structures to get B*, Σ*
  4. Generate X_fake = Z L*^T (I - B*)^{-1} where L* = Cholesky(Σ*)
  5. Update discriminator: L_D = -E[log D(X_real)] - E[log(1 - D(X_fake))]
  6. Update generator logits: L_G = -E[log D(X_fake)] + λ_acyc · h(S̃_B)
  7. Anneal τ from τ_start → τ_end over training
  8. Post-training: threshold sigmoid(A_B), sigmoid(A_Σ) at δ to get final structure

- Design tradeoffs:
  - Temperature schedule: Faster annealing reaches discrete structures sooner but risks gradient instability; slower annealing is more stable but may not fully commit to discrete edges
  - Prior distribution on weights: Narrow prior constrains search; wide prior allows more expressive models but harder optimization
  - Discriminator architecture: Too weak → poor gradient signal; too strong → generator can't improve
  - Acyclicity penalty λ_acyc: Too low → cycles may appear; too high → overly constrains structure search

- Failure signatures:
  - Dense output graph: Threshold δ too low, or generator loss dominated by adversarial term, acyclicity too weak
  - Empty/minimal output graph: Threshold δ too high, or discriminator too strong (generator gradients vanish)
  - Cycles in output: λ_acyc too small or not properly enforced
  - Training instability (oscillating losses): Learning rate mismatch between generator and discriminator, or temperature too low too early
  - Incorrect confounding detection: Prior on Σ* may be mis-specified; confounding signal overwhelmed by directed edges

- First 3 experiments:
  1. Reproduce Case Study A: Implement the 4-node ground truth (edges {(2,1), (2,3), (0,3), (3,1)} + confounding {(0,1)}), generate N=2000 samples, verify SHD < 3.0 and arrowhead F1 > 0.5 as sanity check
  2. Ablation on temperature schedule: Test τ_start ∈ {2.0, 1.0, 0.5} with τ_end = 0.1, measure final SHD and training stability to find robust schedule
  3. Vary confounding strength: Generate data with Σ off-diagonal sampled from U([-0.3, -0.1] ∪ [0.1, 0.3]) vs. U([-0.7, -0.4] ∪ [0.4, 0.7]), compare confounding edge recovery (SΣ F1) to understand sensitivity

## Open Questions the Paper Calls Out

- Does the fGAN-CD method remain computationally efficient and accurate when applied to high-dimensional causal structures (e.g., >20 nodes)?
  - Basis in paper: Experimental evaluation restricted to small-scale, 4-node synthetic graphs
  - Why unresolved: Combinatorial complexity increases rapidly with node count; Gumbel-Softmax relaxations struggle to maintain gradient fidelity in high-dimensional discrete search spaces
  - What evidence would resolve it: Benchmarking results on standard larger synthetic datasets (30-100 nodes) comparing SHD and training time against existing baselines

- Can the generative framework be generalized to handle non-linear causal mechanisms while preserving the theoretical connection to Bayesian free energy?
  - Basis in paper: Method explicitly derived for Linear SEMs and relies on linearity of X = XB + E for generator definition
  - Why unresolved: Real-world systems often exhibit non-linear relationships; current generator formulation depends on linear matrix inverses which don't translate directly to non-linear settings
  - What evidence would resolve it: Theoretical extension of generator loss for non-linear functions or empirical recovery from non-linear additive noise models

- Is the method robust to violations of the multivariate Gaussian noise assumption, specifically for non-Gaussian or heavy-tailed error distributions?
  - Basis in paper: Section 2.1 explicitly assumes noise vector E follows multivariate Gaussian distribution
  - Why unresolved: Method addresses unmeasured confounding through correlated noise but doesn't test robustness to non-normal error terms
  - What evidence would resolve it: Ablation studies on synthetic datasets with uniform, exponential, or multimodal noise distributions

## Limitations
- Critical hyperparameters (discriminator architecture, learning rates, λ_acyc, δ) are not specified, making faithful reproduction challenging
- Theoretical claims rely on singular learning theory, but direct empirical validation of KL divergence equivalence is limited
- GAN training instability is a known issue in broader literature, though not explicitly addressed for this specific application

## Confidence
- High: The core mechanism of using Bayesian free energy as model selection criterion for singular models with latent variables is theoretically sound and well-motivated
- Medium: The f-GAN framework application for transforming KL minimization into adversarial optimization is standard, but empirical validation in causal discovery context is limited
- Medium: Gumbel-Softmax relaxation for differentiable discrete structure search is established, but its effectiveness for causal graph recovery specifically requires further validation

## Next Checks
1. **Ablation study on temperature schedule**: Systematically vary τ_start and τ_end to quantify impact on final SHD and training stability
2. **Cross-method comparison**: Implement ABIC as the direct baseline and compare performance on identical synthetic datasets across multiple graph topologies
3. **Real-world data validation**: Apply fGAN-CD to a semi-synthetic dataset (e.g., real clinical measurements with simulated confounding) to assess practical applicability beyond synthetic scenarios