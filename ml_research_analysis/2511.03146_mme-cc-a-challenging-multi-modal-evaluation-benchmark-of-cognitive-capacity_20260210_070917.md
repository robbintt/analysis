---
ver: rpa2
title: 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity'
arxiv_id: '2511.03146'
source_url: https://arxiv.org/abs/2511.03146
tags:
- reasoning
- answer
- visual
- spatial
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MME-CC is a new benchmark designed to test multimodal large language\
  \ models (MLLMs) on vision-based reasoning tasks. It organizes 11 representative\
  \ tasks into three categories\u2014spatial, geometric, and visual knowledge reasoning\u2014\
  using 1,173 carefully annotated questions."
---

# MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity

## Quick Facts
- arXiv ID: 2511.03146
- Source URL: https://arxiv.org/abs/2511.03146
- Reference count: 40
- Primary result: MME-CC is a new benchmark designed to test multimodal large language models (MLLMs) on vision-based reasoning tasks across spatial, geometric, and visual knowledge dimensions.

## Executive Summary
MME-CC is a new benchmark designed to test multimodal large language models (MLLMs) on vision-based reasoning tasks across spatial, geometric, and visual knowledge dimensions. It organizes 11 representative tasks into three categories using 1,173 carefully annotated questions. The benchmark aims to address limitations in existing evaluations, which either overemphasize textual reasoning or lack systematic coverage of vision-centric cognitive behaviors. Experiments with 16 models show that closed-source systems like Gemini-2.5-Pro lead overall, but spatial and geometric reasoning remain weak (≤30%). Common errors include orientation mistakes, poor cross-view consistency, and difficulty following counterfactual instructions.

## Method Summary
MME-CC evaluates MLLMs on vision-centric cognitive tasks across three dimensions: spatial reasoning, geometric reasoning, and visual knowledge reasoning. The benchmark consists of 1,173 questions across 11 subtasks using diverse image sources including Google Maps, real estate listings, game screenshots, and web images. Models are evaluated using binary correctness (0/1) per question, with responses scored by an LLM-as-a-judge (DeepSeek-V3-0324) comparing outputs to gold references. The evaluation uses inference-only methodology with proprietary models accessed via APIs and open-source models configured with temperature=1.0 and top-p=0.7.

## Key Results
- Closed-source models like Gemini-2.5-Pro lead overall, but spatial and geometric reasoning remain weak (≤30%)
- Chain-of-Thought reasoning typically follows a three-stage process (extract → reason → verify) with heavy reliance on visual extraction
- Common errors include orientation mistakes, poor cross-view consistency, and difficulty following counterfactual instructions
- Forcing explicit visual description upfront improves reasoning performance by stabilizing textual alignment

## Why This Works (Mechanism)

### Mechanism 1: Vision-Grounding Through Language-Independent Task Design
The benchmark's design isolates visual reasoning by ensuring textual prompts contain no solution information. Tasks require extracting all task-critical information from images while prompts provide only instructions and constraints. This addresses the core assumption that existing benchmarks permit solutions relying on textual cues, format priors, or OCR rather than genuine visual reasoning.

### Mechanism 2: Three-Dimensional Cognitive Taxonomy for Fine-Grained Diagnostics
Partitioning 11 tasks into spatial, geometric, and visual knowledge categories enables dimension-level failure attribution. Each category isolates specific cognitive behaviors—spatial relations, geometric manipulation, instruction-conditioned reasoning—allowing targeted diagnosis. This assumes these three dimensions represent fundamentally distinct visual cognitive capabilities rather than a monolithic "reasoning" ability.

### Mechanism 3: Layered Chain-of-Thought with Continuous Visual Extraction
Reasoning-oriented models outperform non-reasoning counterparts through a three-stage process (extract → reason → verify) with iterative image revisiting. Extended CoT chains provide multiple checkpoints to verify recognition outcomes and revise intermediate inferences. This mechanism assumes visual extraction is not a one-time front-loading operation but occurs throughout reasoning as needed.

## Foundational Learning

- **Cross-view identity persistence**: Needed for Indoor Deduplication Counting where the same furniture item appears across multiple overlapping photos. Quick check: Given three photos of a living room with overlapping regions, how many distinct sofas are present?
- **Counterfactual instruction following**: Needed for tasks requiring reversed answers rather than literal descriptions. Quick check: If an image shows a red ball and instruction says "report colors using mapping red↔blue," what color should you answer?
- **Spatial reference frame propagation**: Needed when an anchor view provides orientation and models must infer orientation of query views. Quick check: If View A faces North and shows a window on its right wall, and View B is rotated 90° clockwise from View A, which direction does View B face?

## Architecture Onboarding

- **Component map**: Vision encoder → Vision-language adapter → LLM backbone
- **Critical path**: Image tokenization must preserve fine-grained spatial relationships; adapter must enable cross-modal grounding; LLM must support multi-step verification loops. Current bottleneck: Steps 1–2 appear fragile for spatial/geometric tasks; step 3 improves accuracy but reduces efficiency
- **Design tradeoffs**: Longer CoT improves accuracy but increases latency and can cause "stalling" from excessive verification; forcing explicit visual description upfront stabilizes reasoning but adds compute
- **Failure signatures**: Orientation errors (flipping left/right or cardinal directions), entity double-counting across views, literal description override of counterfactual instructions, local vs. global path planning failures
- **First 3 experiments**: Ablate CoT length to measure accuracy-efficiency frontier; probe cross-view binding with synthetic Indoor Deduplication variant; instruction-conditioned fine-tuning on counterfactual tasks only

## Open Questions the Paper Calls Out

- **Does performance gain from explicit textual description prompts imply MLLMs lack intrinsic visual grounding?** The authors note adding "first describe the relevant content" yields consistent gains, suggesting improvements stem from "better textual alignment rather than stronger intrinsic visual reasoning." This raises whether this dependency is a fundamental architectural limitation or training objective failure.

- **Why does extending chain-of-thought length degrade performance in long-horizon planning tasks like Maze?** The paper hypothesizes that while CoT helps verification, "long reasoning chains dilute attention, obscure crucial visual details, and ultimately degrade outcomes," specifically noting Maze task failure below 2%.

- **How can architectures be modified to maintain object identity persistence across multiple views?** The paper identifies "Entity identity consistency under multi-view settings" as a recurring error pattern where models fail to maintain identity, leading to redundant counts.

## Limitations

- The benchmark's ecological validity is uncertain - high-performing models may achieve results through pattern matching or dataset biases rather than true visual cognition
- The relatively small dataset size (1,173 questions) may not provide sufficient statistical power for fine-grained analysis
- The benchmark focuses exclusively on image-based reasoning without audio or video modalities

## Confidence

- **High confidence**: Closed-source models significantly outperform open-source models (Gemini-2.5-Pro achieving 23.80% spatial reasoning accuracy vs. best open-source at 18.46%)
- **Medium confidence**: The three-dimensional cognitive taxonomy effectively partitions distinct reasoning capabilities
- **Low confidence**: The claim that existing benchmarks "overemphasize textual reasoning" - quantitative comparison was not provided

## Next Checks

1. **Adversarial perturbation test**: Systematically distort images (rotation, scaling, partial occlusion) to measure robustness of model performance across all three cognitive dimensions, isolating whether errors stem from visual extraction failures versus reasoning breakdowns

2. **Cross-dataset transfer validation**: Evaluate whether models trained to excel on MME-CC maintain or lose performance on established benchmarks like MMMU and MathOPEval, establishing whether MME-CC measures complementary or overlapping capabilities

3. **Human baseline establishment**: Conduct controlled experiments with human participants on a subset of spatial and geometric tasks to establish absolute performance floors and ceilings, particularly for the lowest-performing tasks (≤30% accuracy) to determine if models are approaching human-level reasoning or if the tasks represent genuinely hard problems