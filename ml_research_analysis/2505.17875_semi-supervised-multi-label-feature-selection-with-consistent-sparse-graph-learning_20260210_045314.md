---
ver: rpa2
title: Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph
  Learning
arxiv_id: '2505.17875'
source_url: https://arxiv.org/abs/2505.17875
tags:
- feature
- label
- selection
- learning
- sgmfs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a consistent sparse graph learning method
  for semi-supervised multi-label feature selection (SGMFS). The method addresses
  two main challenges in semi-supervised multi-label scenarios: (1) difficulty in
  evaluating label correlations without enough labeled samples, and (2) suboptimal
  similarity graph structures derived from original feature spaces.'
---

# Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning

## Quick Facts
- **arXiv ID**: 2505.17875
- **Source URL**: https://arxiv.org/abs/2505.17875
- **Reference count**: 40
- **Primary result**: Proposes SGMFS method for semi-supervised multi-label feature selection that learns label correlations and adaptive sparse graphs, achieving superior performance on 7 benchmarks

## Executive Summary
This paper addresses semi-supervised multi-label feature selection by proposing a consistent sparse graph learning method (SGMFS). The approach tackles two key challenges: difficulty evaluating label correlations with limited labeled samples, and suboptimal similarity graph structures from original features. SGMFS learns a low-dimensional label subspace and performs sparse reconstruction in both label space and learned subspace to maintain consistency. The method combines semi-supervised learning, label correlation capture, and adaptive sparse graph learning through an alternating optimization framework. Extensive experiments on seven multi-label benchmarks validate SGMFS's superiority over state-of-the-art methods, particularly showing improved stability and robustness when labeled data is limited.

## Method Summary
SGMFS operates by first projecting features to label space using weight matrix W, then learning a low-dimensional orthogonal subspace Q that captures label correlations. It performs sparse reconstruction of samples using learned weights M, enforcing consistency between original label space and subspace representations. The algorithm iteratively updates four components: subspace Q via eigendecomposition, feature weights W via closed-form solution with l2,1-regularization, soft labels F via constrained projection, and sparse graph M via fixed-point iteration. The method uses alternating minimization with convergence guarantees, where l1-regularization induces sparsity in the graph reconstruction and orthogonality constraints ensure independent subspace dimensions.

## Key Results
- SGMFS achieves state-of-the-art performance on seven multi-label benchmarks with improvements in Hamming Loss, Ranking Loss, Macro/Micro F1, and Average Precision
- The method shows particular advantage when labeled data is limited (10-35% labeled), outperforming baselines in stability and robustness
- Extensive sensitivity analysis demonstrates SGMFS's effectiveness across different feature selection proportions (2-30%) and parameter settings

## Why This Works (Mechanism)

### Mechanism 1
Learning a low-dimensional shared label subspace enables effective label correlation capture even with incomplete label information. The method maps features to label space via weight matrix W, then projects to a shared subspace Q where dimensions are constrained to be orthogonal (Q^T Q = I). Each subspace dimension integrates correlated original labels into independent shared labels. The optimization jointly minimizes reconstruction error between projected features and subspace representation. Core assumption: Label correlations can be captured through low-rank structure; shared subspace dimensions meaningfully encode label groups.

### Mechanism 2
Joint sparse graph learning with consistency constraints produces more reliable soft labels than fixed kNN graphs. Instead of pre-computing a fixed similarity graph, the method adaptively learns sparse reconstruction weights M by solving with l1-norm to induce sparsity, automatically selecting optimal neighbors. Consistency is enforced by requiring similar reconstructions in both original label space and learned subspace. Core assumption: Data manifold structure in label space and subspace should be consistent; sparse reconstruction better captures true neighborhood structure than kernel-based kNN.

### Mechanism 3
Iterative alternating optimization with convergence guarantees produces stable feature rankings. The algorithm alternates between updating subspace Q via eigendecomposition, feature weights W via closed-form solution with l2,1-regularization, soft labels F via constrained projection, and sparse graph M via fixed-point iteration. Theorem proves monotonic decrease of objective; convergence typically achieved in ~20 iterations. Core assumption: Joint convexity of subproblems ensures convergence to meaningful local optimum; l2,1-norm on W provides feature-level sparsity for selection.

## Foundational Learning

- **Sparse representation / l1-regularization**: Core to adaptive graph learning; l1-norm induces sparsity in reconstruction weights, enabling automatic neighbor selection without tuning k. Quick check: Can you explain why l1-norm produces sparse solutions while l2-norm does not?

- **Manifold learning / label propagation**: Semi-supervised learning relies on assumption that nearby points (on data manifold) share labels; graph structure encodes this. Quick check: How does label propagation differ from semi-supervised learning via self-training?

- **Multi-label evaluation metrics**: Paper uses Hamming Loss, Ranking Loss, Macro/Micro F1—understanding these is essential for interpreting experimental results. Quick check: When would Micro F1 be preferred over Macro F1 for evaluating multi-label feature selection?

## Architecture Onboarding

- **Component map**:
  - X (d×n): Feature matrix → projected via W (d×c) → label predictions F (n×c)
  - F decomposes: labeled portion F_l = Y_l fixed; unlabeled F_u learned
  - Q (n×lsd): Shared labels in subspace, constrained orthogonal
  - P (lsd×c): Maps subspace back to original label space
  - M (n×n): Sparse symmetric graph with zero diagonal
  - Parameters: α (subspace learning weight), β (consistency weight), γ (sparsity regularization)

- **Critical path**:
  1. Initialize: W random, M via kNN, F = [Y; 0]
  2. Loop until convergence: Q ← eigendecomposition → W ← closed-form → b ← mean correction → F ← constrained solve → M ← fixed-point sparse update
  3. Output: Rank features by ||W_{i,:}||_2 descending

- **Design tradeoffs**:
  - Subspace dimension lsd: Paper suggests c/2; too small loses label correlation info, too large reduces regularization benefit
  - β (consistency vs. sparsity): High β enforces stronger consistency but may over-constrain graph; low β may produce inconsistent soft labels
  - Feature selection proportion: Paper tests 2-30%; below 15% performance degrades more for SGMFS than baselines

- **Failure signatures**:
  - Convergence stalls (>100 iterations): Check parameter scale; normalize features; verify M initialization
  - All features ranked equally: W may be under-regularized; increase γ
  - Soft labels F all zero or all one: Constraint clipping too aggressive; check labeled sample proportion
  - Memory issues on large n: M is n×n; for n > 50K, consider approximation or mini-batch variant

- **First 3 experiments**:
  1. Sanity check: Run on Emotions (small, 72 features, 6 labels) with 20% labeled; expect convergence in <30 iterations, Macro F1 > 0.65 at 10% features
  2. Parameter sensitivity: Vary α, β, γ ∈ {0.01, 0.1, 1, 10, 100} on Scene dataset; plot Average Precision vs. parameter—expect flat regions indicating robustness
  3. Ablation: Compare full SGMFS vs. SGMFS\lc (no label correlation) vs. SGMFS\sc (no space consistency); expect full model to outperform both on Yeast dataset with 35% labeled data

## Open Questions the Paper Calls Out

- **Automatic feature selection proportion**: Future research will focus on developing semi-supervised methods capable of automatically determining optimal feature selection proportions, as the current work requires manual sweeping from 2% to 30%.

- **Multi-view scenarios**: Addressing multi-view scenarios where data is collected from diverse sources is identified as a primary direction, as the current method assumes a single feature matrix X.

- **Adaptive subspace dimensionality**: The dimensionality of the shared label subspace (lsd) is currently set manually (c/2), but an adaptive mechanism that determines the optimal subspace dimension during training is needed.

- **Scalability on massive datasets**: The O(n^3) complexity for updating the sparse graph matrix impacts scalability, limiting the method's applicability to datasets with millions of samples despite being termed "large-scale."

## Limitations

- **Initialization sensitivity**: Performance depends on random initialization of W and kNN-based initialization of M, with no specified initialization distributions or robustness analysis to different starting points.

- **Scalability constraints**: The n×n sparse matrix M creates memory bottlenecks for datasets with >50K samples, though experiments are limited to smaller benchmarks.

- **Label correlation assumption**: The method's effectiveness relies on meaningful label correlations existing in the data, which may not hold for datasets with nearly independent labels.

## Confidence

- **High confidence**: Experimental superiority over baselines on seven tested datasets is well-supported by quantitative metrics (Macro F1, Average Precision). Convergence proof and alternating optimization framework are mathematically sound.
- **Medium confidence**: Mechanism claims (low-dimensional subspace capturing correlations, adaptive graph learning) are theoretically plausible but lack complete ablation studies isolating each component's contribution.
- **Low confidence**: Claims about general robustness to limited labeled data are based on 10-35% labeled proportions; performance at extreme sparsity (<5% labels) or with noisy labels is untested.

## Next Checks

1. **Ablation on mechanism contribution**: Run controlled experiments comparing SGMFS to variants with (a) fixed kNN graph instead of learned M, (b) no subspace projection Q, (c) no consistency constraint β=0. Quantify each component's marginal gain on Yeast dataset.

2. **Scalability test**: Implement SGMFS on a larger multi-label dataset (e.g., Wiki10-31K with ~30K samples) and measure memory usage and runtime. Compare against approximate graph methods or mini-batch variants.

3. **Extreme label sparsity**: Evaluate SGMFS with 1-5% labeled data on Scene dataset. Compare convergence behavior and feature selection quality against a simple supervised baseline trained on the same tiny labeled set.