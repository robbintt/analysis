---
ver: rpa2
title: 'PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation'
arxiv_id: '2510.07964'
source_url: https://arxiv.org/abs/2510.07964
tags:
- data
- uncertainty
- should
- prior
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRESCRIBE is a framework that predicts single-cell responses to
  gene perturbations while quantifying both model and data uncertainty. It jointly
  estimates epistemic uncertainty (model unfamiliarity) and aleatoric uncertainty
  (biological variability) using a multivariate deep evidential regression approach.
---

# PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation

## Quick Facts
- arXiv ID: 2510.07964
- Source URL: https://arxiv.org/abs/2510.07964
- Reference count: 40
- Primary result: PRESCRIBE predicts single-cell perturbation responses while quantifying both epistemic and aleatoric uncertainty, achieving >3% accuracy improvements when filtering lowest-confidence predictions

## Executive Summary
PRESCRIBE is a framework that predicts single-cell gene expression responses to perturbations while quantifying both model uncertainty (epistemic) and biological variability (aleatoric). It uses a multivariate deep evidential regression approach with Normal-Inverse-Wishart posteriors to estimate full distributional predictions. The model learns to predict post-perturbation gene expression profiles and assigns confidence scores based on training data support. Experiments on three benchmark datasets show strong correlation between uncertainty estimates and prediction accuracy, with calibration performance superior to baseline methods.

## Method Summary
PRESCRIBE jointly estimates epistemic uncertainty (model unfamiliarity) and aleatoric uncertainty (biological variability) using a multivariate deep evidential regression approach. The model learns to predict post-perturbation gene expression profiles and assigns confidence scores based on how well-supported each prediction is by training data. It uses Normal-Inverse-Wishart posteriors for calibrated distributional predictions over gene expression states, with a normalizing flow in latent perturbation space providing evidence for epistemic uncertainty. The framework predicts full distributional responses rather than point estimates, enabling uncertainty quantification for both model knowledge gaps and inherent biological noise.

## Key Results
- PRESCRIBE's uncertainty estimates strongly correlate with prediction accuracy (Spearman correlation rs_perf,conf)
- Filtering out the least confident 10% of predictions achieves steady accuracy improvements of over 3% compared to baseline methods
- Superior calibration performance with confidence scores that reliably indicate prediction reliability
- Pseudo E-distance serves as a unified surrogate for both epistemic and aleatoric uncertainty without requiring ground-truth post-perturbation data

## Why This Works (Mechanism)

### Mechanism 1
- Pseudo E-distance serves as a unified surrogate for both epistemic and aleatoric uncertainty without requiring ground-truth post-perturbation data
- Combines normalized posterior evidence (quantifies training-data support) with negative normalized predictive entropy (captures biological variability)
- Assumes rank-ordering of true E-distance is preserved by this proxy under fixed prior conditions
- If training-data density in latent space doesn't correlate with functional similarity to unseen perturbations, evidence becomes uninformative for epistemic uncertainty

### Mechanism 2
- Multivariate Normal-Inverse-Wishart posteriors enable calibrated distributional predictions over gene expression states
- Decoder outputs sufficient statistics for NIW distribution, serving as conjugate prior for multivariate Gaussian gene expression
- Assumes post-perturbation gene expression follows approximately multivariate Gaussian distribution
- If gene expression distributions are strongly non-Gaussian, the conjugate prior assumption mis-specifies uncertainty

### Mechanism 3
- Normalizing flow density in latent perturbation space provides evidence for epistemic uncertainty
- High-density regions yield high evidence, weighting posterior toward decoder's prediction; low-density regions cause posterior to revert toward prior
- Assumes perturbations can be embedded in continuous latent space where density correlates with functional similarity
- If OOD perturbations lie in high-density latent regions, evidence misestimates epistemic uncertainty

## Foundational Learning

- Concept: Conjugate priors and Bayesian posterior updates (exponential family)
  - Why needed here: PRESCRIBE's NIW formulation relies on analytical updates from prior → posterior; understanding Eq. 2 is essential
  - Quick check question: Given a Normal-Inverse-Wishart prior and Gaussian likelihood, how do posterior parameters update after observing M data points?

- Concept: Energy distance (E-distance) for comparing distributions
  - Why needed here: Pseudo E-distance is motivated by the original E-distance metric (Eq. 11); understanding the original clarifies the proxy
  - Quick check question: How does E-distance balance inter-group distance (δ_XY) against intra-group dispersion (σ_X, σ_Y)?

- Concept: Normalizing flows for density estimation
  - Why needed here: The evidence term requires tractable density evaluation in latent space; flows provide this via invertible transformations
  - Quick check question: Why must a normalizing flow be bijective, and how does this enable density computation?

## Architecture Onboarding

- Component map: Gene embeddings (scGPT) + control state → Encoder f_α → latent z → Normalizing Flow f_ψ → evidence ν + Decoder f_β → sufficient statistics χ_out → Bayesian Update → posterior ω

- Critical path:
  1. Input: Perturbation(s) x + control expression c
  2. Embed: z = f_α(x, c)
  3. Dual output: ν = f_ψ(z) for evidence; χ_out = f_β(z) for distribution statistics
  4. Posterior: ν_post, χ_post computed via Bayesian update (Eq. 2, 8)
  5. Output: Predictive distribution + pseudo E-distance (confidence score)

- Design tradeoffs:
  - PCA dimension N=10: Lower N preserves Student's t heavy-tailed properties; higher N risks Gaussian approximation
  - Evidence normalization to [N, 2N]: Ensures valid degrees of freedom for predictive t-distribution
  - Loss weights: λ_1/λ_3 ratio trades calibration (rs_perf,conf) vs. accuracy (r_pred,truth)—Figure 5 shows inverse relationship

- Failure signatures:
  - Zero gradients in low-evidence regions (mitigated by L_4 loss—Proposition 4)
  - Poor calibration if prior uses zero vectors instead of control statistics (Ours-NOINFO ablation)
  - Overconfident OOD predictions if flow density doesn't penalize unfamiliar regions

- First 3 experiments:
  1. Correlation validation: Compute pseudo E-distance on validation set; correlate with ground-truth E-distance (Table 1 shows positive correlation strengthening with sample size)
  2. Calibration curve: Bin predictions by confidence percentile; plot vs. accuracy (Figure 4—expect monotonic increase for PRESCRIBE, not for baselines)
  3. Uncertainty-guided filtering: Remove bottom 5%/10% by confidence; verify accuracy improvement vs. random filtering (Table 3 shows random filtering fails to improve)

## Open Questions the Paper Calls Out

- How does the choice of the configurable "null" state (reference) influence the direction and magnitude of the learned perturbation effects?
- Can integrating a generative latent space (e.g., scVI) to handle discrete counts improve performance over the current continuous Gaussian assumption?
- Does the latent density-based evidence score capture true epistemic uncertainty, or is it merely a surrogate for model sensitivity?

## Limitations

- Assumes continuous Gaussian space while single-cell data is often represented as discrete counts
- Limited exploration of how the choice of reference/null state influences predictions
- Latent density-based evidence is an indirect measurement of epistemic uncertainty
- Requires careful hyperparameter tuning (λ1, λ2, λ3) for optimal calibration vs. accuracy tradeoff

## Confidence

- High confidence in the mathematical framework (conjugate priors, E-distance formulation)
- Medium confidence in empirical results (benchmark datasets, correlation metrics)
- Medium confidence in generalizability (three datasets, specific preprocessing choices)
- Medium confidence in practical implementation details (specific flow architecture, loss weight optimization)

## Next Checks

1. Reproduce the correlation between pseudo E-distance and prediction accuracy on validation set
2. Generate calibration curves showing confidence vs. accuracy for PRESCRIBE vs. baselines
3. Validate accuracy improvements from uncertainty-guided filtering (5%/10% lowest confidence) vs. random filtering