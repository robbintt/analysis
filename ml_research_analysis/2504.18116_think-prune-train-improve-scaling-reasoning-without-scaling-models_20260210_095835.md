---
ver: rpa2
title: 'Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models'
arxiv_id: '2504.18116'
source_url: https://arxiv.org/abs/2504.18116
tags:
- data
- stamps
- pass
- reasoning
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Think, Prune, Train (TPT) framework for
  improving reasoning in smaller language models without relying on external supervision
  or reinforcement learning. The method involves prompting the model to generate structured
  reasoning traces, pruning incorrect outputs using ground-truth correctness filtering,
  and performing supervised fine-tuning on validated solutions.
---

# Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models

## Quick Facts
- arXiv ID: 2504.18116
- Source URL: https://arxiv.org/abs/2504.18116
- Authors: Caia Costello; Simon Guo; Anna Goldie; Azalia Mirhoseini
- Reference count: 40
- Gemma2-2B improves from 41.9% to 57.6% Pass@1 on GSM8K using self-generated data

## Executive Summary
This paper introduces the Think, Prune, Train (TPT) framework for improving reasoning in smaller language models without relying on external supervision or reinforcement learning. The method involves prompting the model to generate structured reasoning traces, pruning incorrect outputs using ground-truth correctness filtering, and performing supervised fine-tuning on validated solutions. Experimental results show that on GSM8K, Gemma2-2B improves from 41.9% to 57.6% Pass@1, Gemma2-9B reaches 82% Pass@1 matching LLaMA-3.1-70B, and LLaMA-3.1-70B attains 91% Pass@1 surpassing GPT-4o. The method demonstrates that structured data selection and correctness-based pruning are more effective than simply scaling dataset size.

## Method Summary
The TPT framework works through three phases: Think (generate multiple Chain-of-Thought solutions at temperature 0.8), Prune (verify each solution against ground truth and keep only correct ones), and Train (supervised fine-tuning on pruned dataset for one epoch). The process iterates up to 4 rounds, replacing the dataset each time with the model's own correct solutions. For GSM8K, 2000 pruned examples are used per round; for CodeContests, 1000 examples. Ground-truth verification is the critical gating factor that prevents model collapse. The method shows that SFT on pruned self-generated data can approximate policy gradient updates when rewards are binary and errors are localized.

## Key Results
- Gemma2-2B: Pass@1 improves from 41.9% to 57.6% on GSM8K
- Gemma2-9B: Pass@1 reaches 82% matching LLaMA-3.1-70B performance
- LLaMA-3.1-70B: Pass@1 reaches 91% surpassing GPT-4o
- Pass@20 remains stable while Pass@1 improves, indicating controlled mode collapse

## Why This Works (Mechanism)

### Mechanism 1: SFT as RL Approximation
Supervised fine-tuning on correctness-filtered self-generated data approximates policy gradient reinforcement learning when rewards are binary and errors are localized. The gradient from training only on positive examples equals half the gradient from comparing positive vs. negative pairs, simplifying to standard cross-entropy loss on correct outputs only.

### Mechanism 2: Ground-Truth Pruning Prevents Collapse
Ground-truth verification acts as a hard constraint ensuring only valid reasoning patterns propagate, preventing the knowledge forgetting and hallucination that occurs with recursive training on unfiltered synthetic data.

### Mechanism 3: Iterative Refinement Without Diversity Loss
Each round trains on the model's own correct solutions, reinforcing successful reasoning paths. Pass@1 improves while Pass@20 remains stable, indicating the model converges toward reliable outputs without complete mode collapse.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) vs. Reinforcement Learning:** The paper claims SFT on pruned data substitutes for RL. Can you explain why training on correct-only examples might approximate policy gradient with binary rewards?

- **Model Collapse vs. Mode Collapse:** The paper distinguishes theseâ€”model collapse degrades capability; mode collapse narrows output diversity. If Pass@1 improves but Pass@50 stays flat, which phenomenon is occurring?

- **Pass@k Metrics:** Results are reported as Pass@1 (single-attempt accuracy) and Pass@20/50 (best-of-k). Why might improving Pass@1 be more valuable than improving Pass@50 for production deployment?

## Architecture Onboarding

- **Component map:** Think (generate CoT solutions) -> Prune (verify against ground truth) -> Train (SFT on pruned data) -> Iterate (repeat with updated model)

- **Critical path:** Ground-truth verification is the gating factor. Without it, unfiltered self-training degrades performance.

- **Design tradeoffs:** 2k examples sufficient; temperature 0.8 trades exploration vs. correctness; single-epoch training prevents overfitting; replacing (not accumulating) data each round isolates improvement from data scaling.

- **Failure signatures:** Performance degrades when using unpruned data; mixed real/synthetic data fails in recursive training; very small models (<1B) may lack sufficient correct generations to bootstrap.

- **First 3 experiments:** 1) Compare fine-tuning Gemma-2B on 2k human GSM8K vs. 2k self-generated pruned examples. 2) Compare training with no pruning vs. ground-truth pruning vs. soft-positive. 3) Run TPT for 1, 2, 3, 4 rounds on same model to confirm diminishing returns pattern.

## Open Questions the Paper Calls Out

- **Question 1:** Does TPT generalize to reasoning domains beyond mathematical problem-solving and code generation? The paper only tested GSM8K and CodeContests, acknowledging GPQA involves similar reasoning but requires domain knowledge.

- **Question 2:** At what point does prioritizing correctness over diversity become detrimental to model capabilities? The paper observes controlled mode collapse but doesn't establish boundaries where it harms generalization or creativity.

- **Question 3:** Why does mixing human-generated and self-generated data disrupt recursive self-improvement? The paper documents the failure but offers only speculative explanation about distribution mismatch.

- **Question 4:** What determines the optimal number of TPT iterations for a given model size? Results show Gemma-2B needed 4 rounds, Gemma-9B improved faster, while LLaMA-70B peaked after 1 round, but no theoretical framework is provided.

## Limitations

- Relies critically on ground-truth verification, limiting applicability to domains with unambiguous correctness criteria
- Mechanism claiming SFT approximates RL relies on assumptions about error localization that may not generalize
- Claims about matching larger models are based on a single task (GSM8K) and may not generalize to other reasoning domains
- Paper doesn't extensively explore quality of diverse solution paths lost through mode collapse

## Confidence

**High Confidence:** Core empirical findings well-supported; iterative improvement pattern clearly demonstrated across multiple model sizes and tasks.

**Medium Confidence:** Mechanism claiming SFT approximates RL is mathematically sound but relies on assumptions about error localization not extensively validated across domains.

**Low Confidence:** Claim that "2k examples sufficient" is based on observed plateaus rather than systematic scaling studies; assertion that this approach "matches or exceeds" much larger models based on single task.

## Next Checks

1. **Ground-truth independence validation:** Apply TPT to a domain where ground-truth verification is available but imperfect (e.g., spelling correction) and measure performance degradation as verification reliability decreases from 100% to 80%.

2. **Diversity preservation analysis:** After 4 rounds of TPT, generate 100 solutions for 50 test problems and measure lexical and structural diversity to quantify mode collapse severity.

3. **Error localization study:** Manually analyze 100 incorrect solutions from baseline and post-TPT models to measure error span and determine whether the assumption of localized errors holds.