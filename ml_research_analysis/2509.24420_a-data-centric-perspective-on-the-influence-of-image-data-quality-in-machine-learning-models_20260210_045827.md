---
ver: rpa2
title: A Data-Centric Perspective on the Influence of Image Data Quality in Machine
  Learning Models
arxiv_id: '2509.24420'
source_url: https://arxiv.org/abs/2509.24420
tags:
- image
- images
- quality
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of systematically assessing
  image dataset quality and its impact on machine learning model performance. The
  authors investigate common image quality issues and develop a pipeline integrating
  two tools, CleanVision and Fastdup, to automatically detect and filter problematic
  images.
---

# A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models

## Quick Facts
- arXiv ID: 2509.24420
- Source URL: https://arxiv.org/abs/2509.24420
- Reference count: 35
- Primary result: Automated pipeline using Li's thresholding and hierarchical clustering improves F1 scores for image quality detection from 0.6794→0.9468 (single perturbations) and near-duplicate detection from 0.4576→0.7928

## Executive Summary
This study systematically evaluates how image dataset quality impacts CNN performance and develops an automated pipeline to detect and filter problematic images. The authors integrate CleanVision and Fastdup tools, enhancing them with automatic threshold selection using Li's Minimum Cross-Entropy Thresholding and improved near-duplicate detection via hierarchical clustering on perceptual hashes. Experiments on the CIFAKE dataset demonstrate that while CNNs are resilient to some distortions, they are particularly vulnerable to blurring and severe downscaling. The proposed automatic thresholding method significantly improves detection performance, with F1 scores increasing from 0.6794 to 0.9468 under single perturbations.

## Method Summary
The study employs a compact CNN architecture trained on the CIFAKE dataset (50,000 training, 10,000 test images) with perturbations applied via OpenCV and Albumentations. The detection pipeline combines CleanVision (computing 9 quality scores including blur, brightness, duplicates) and Fastdup (ONNX feature vectors with cosine similarity). Key innovations include Li's Minimum Cross-Entropy Thresholding for automatic threshold selection and hierarchical clustering on pHash Hamming distances for near-duplicate detection. The methodology evaluates detection F1 scores and classification accuracy under various perturbation types, comparing proposed improvements against baseline tool implementations.

## Key Results
- Automatic thresholding improves F1 from 0.6794 to 0.9468 under single perturbations and from 0.7447 to 0.8557 under dual perturbations
- Hierarchical clustering on pHash Hamming distances increases near-duplicate F1 from 0.4576 to 0.7928
- Blurring causes the most severe accuracy drops (54.70% with kernel size 5), eroding high-frequency information like edges and textures
- CNNs show resilience to grayscale conversion but vulnerability to brightness extremes and severe downscaling

## Why This Works (Mechanism)

### Mechanism 1: Li's Minimum Cross-Entropy Thresholding
- **Claim:** Adaptively separates low-quality from normal images by minimizing cross-entropy between score distributions
- **Core assumption:** Score distributions are bimodal (low-quality vs. normal), which holds for single perturbations but degrades with multimodal distributions
- **Evidence anchors:** F1 improvement from 0.6794→0.9468 under single perturbations; bimodal score distribution observed in experiments
- **Break condition:** Highly multimodal distributions from overlapping perturbations or initialization sensitivity

### Mechanism 2: Laplacian Variance for Blur Detection
- **Claim:** Combines Laplacian variance with grayscale histogram standard deviation to capture edge sharpness and texture variation
- **Core assumption:** Edges and texture are primary discriminative features for CNNs; degrading these directly harms feature extraction
- **Evidence anchors:** Blurring caused accuracy to plummet to 54.70% with kernel size 5; algorithm combines edge detection and tonal variation metrics
- **Break condition:** Images with intentional soft-focus aesthetics or artistic blur may be incorrectly flagged

### Mechanism 3: Hierarchical Clustering for Near-Duplicates
- **Claim:** Improves near-duplicate detection using hierarchical clustering with Hamming distance on perceptual hash values
- **Core assumption:** Near-duplicates differ in low-level attributes but share high-level visual structure captured by pHash
- **Evidence anchors:** F1 score increased from 0.4576 to 0.7928; hierarchical clustering with single linkage achieved best results
- **Break condition:** Semantic duplicates (same object, different viewpoints) may not cluster together due to pHash position sensitivity

## Foundational Learning

- **Concept:** Perceptual Hashing (pHash)
  - **Why needed here:** Core to near-duplicate detection; understanding how pHash differs from cryptographic hashing explains why clustering approach works
  - **Quick check question:** Given two images—(A) original and (B) same image with brightness increased 20%—will their pHash Hamming distance be small or large?

- **Concept:** Histogram Thresholding Methods (Otsu, Li, Entropy-based)
  - **Why needed here:** Automatic threshold selection is the key innovation; knowing when each method fails prevents misapplication
  - **Quick check question:** If quality scores form three peaks instead of two (e.g., clean, mildly degraded, severely degraded), which thresholding method from the paper would likely fail?

- **Concept:** Laplacian Operator for Edge Detection
  - **Why needed here:** Blur detection relies on Laplacian variance; understanding second-derivative edge response explains why blurry images score low
  - **Quick check question:** An image with uniform gray (all pixels = 128) is processed through a Laplacian filter. What is the expected variance of the output?

## Architecture Onboarding

- **Component map:** Input images → CleanVision module (9 quality scores) → Fastdup module (ONNX features) → Thresholding layer (Li's method) → Deduplication layer (hierarchical clustering) → Output dataframe with flags and cluster assignments

- **Critical path:** Score computation → Threshold selection → Flagging → Deduplication clustering → User review. The threshold selection step is the highest-leverage improvement.

- **Design tradeoffs:**
  - Li vs. GHT: Li achieves highest average F1 (0.9468) but is initialization-sensitive; GHT is more stable across perturbation types
  - pHash vs. Feature vectors: pHash is faster and pixel-focused; Fastdup's ONNX features capture semantic similarity but require more computation
  - Single vs. dual perturbation detection: Tools handle single perturbations well; dual perturbations cause multimodal distributions that degrade MET

- **Failure signatures:**
  - High false positive rate on blur: Likely threshold too aggressive for domain; check if "artistic blur" is common
  - Near-duplicates missed: pHash clustering may need min_cluster_size tuning; check if duplicates span different aspect ratios
  - Grayscale images not flagged: Ensure channel-equality check runs after PIL mode detection

- **First 3 experiments:**
  1. **Baseline validation:** Run CleanVision with default thresholds on CIFAKE subset; verify F1 scores match paper benchmarks (blur: 0.4735, low-info: 0.0000)
  2. **Threshold ablation:** Compare Li, GHT, and Otsu on a held-out validation set with known injected degradations; measure per-issue F1
  3. **Deduplication stress test:** Inject 12% near-duplicates with brightness/contrast variations; compare CleanVision default vs. proposed hierarchical clustering

## Open Questions the Paper Calls Out
- How does the proposed automatic thresholding pipeline perform when applied to non-synthetic, real-world image datasets? (Future work to extend to real-world datasets)
- Can the proposed thresholding algorithms maintain robustness when images suffer from three or more concurrent quality issues? (Study considered at most two concurrent perturbations)
- To what extent does the initialization strategy influence the stability and accuracy of Li's Minimum Cross-Entropy Thresholding? (Li's method is sensitive to initialization)
- Does the correlation between specific image degradations and model performance generalize to diverse architectures such as Vision Transformers? (Study used only compact CNN)

## Limitations
- Evaluation confined to synthetic CIFAKE dataset, leaving real-world dataset robustness unverified
- Li's thresholding fails under dual perturbations, indicating sensitivity to multimodal score distributions
- pHash-based detection may struggle with semantic duplicates and heavily augmented images
- Study does not address class imbalance effects on quality detection performance

## Confidence
- **High Confidence:** CNN vulnerability to blurring and downscaling (directly measured experimental results)
- **Medium Confidence:** Automatic thresholding improvements (demonstrated but relies on synthetic data)
- **Medium Confidence:** Near-duplicate detection improvements (validated but pHash limitations acknowledged)

## Next Checks
1. **Domain Transfer Validation:** Apply pipeline to naturally occurring image dataset (e.g., ImageNet subset) with manually verified quality labels to test real-world performance
2. **Perturbation Robustness Testing:** Generate validation datasets with mixed perturbation types to quantify degradation in automatic thresholding under realistic contamination scenarios
3. **Semantic Duplicate Evaluation:** Construct test cases with semantically identical images under different transformations to measure pHash clustering effectiveness and identify failure modes