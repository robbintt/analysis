---
ver: rpa2
title: 'LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge
  Graph Construction -- A Case Study on SDGs'
arxiv_id: '2602.02090'
source_url: https://arxiv.org/abs/2602.02090
tags:
- uni00000048
- knowledge
- uni00000003
- uni00000057
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LEC-KG is a collaborative framework for domain-specific knowledge
  graph construction that integrates LLM-based semantic extraction with KGE-based
  structural validation. The framework addresses challenges of heterogeneous entity
  mentions, long-tail relation distributions, and absent schemas through bidirectional
  refinement: KGE provides structure-aware feedback to guide LLM extraction via evidence-grounded
  reasoning, while validated triples progressively improve KGE representations.'
---

# LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs

## Quick Facts
- **arXiv ID:** 2602.02090
- **Source URL:** https://arxiv.org/abs/2602.02090
- **Reference count:** 40
- **Primary result:** 36.8% Micro-F1 on Chinese SDG reports, improving 11.2 points over LLM few-shot extraction and doubling tail relation performance (13.3% vs 6.7%).

## Executive Summary
LEC-KG addresses the challenge of constructing domain-specific knowledge graphs from unstructured text by integrating large language model (LLM) semantic extraction with knowledge graph embedding (KGE) structural validation. The framework iteratively refines both semantic and structural representations through bidirectional feedback: KGE provides structure-aware guidance to the LLM, while validated triples progressively improve KGE representations. Evaluated on Chinese SDG reports, LEC-KG achieves significant performance gains over baseline extraction methods, particularly for long-tail relations and entities absent from initial training data.

## Method Summary
The LEC-KG framework employs a four-iteration bidirectional refinement loop. Initially, an LLM (DeepSeek-V3) extracts candidate triples from document chunks using hierarchical coarse-to-fine relation classification with 89 fine-grained relations across 8 categories. These candidates are validated by RotatE KGE models using a semantic initialization strategy that projects RoBERTa embeddings for unseen entities. The validation process tri-partitions triples into accept, feedback, and reject sets based on percentile thresholds. Accepted triples update the KGE model (Channel 2), while feedback triples trigger evidence-grounded Chain-of-Thought reasoning to guide the LLM toward more accurate relations (Channel 1). This closed-loop system progressively improves both extraction accuracy and structural validation capability.

## Key Results
- Micro-F1 of 36.79% on Chinese SDG test set, improving 11.2 points over LLM few-shot extraction baseline
- Tail relation F1 improves from 6.7% (baseline) to 13.3% through hierarchical classification and iterative refinement
- Semantic initialization enables structural validation for unseen entities, with 8.49-point Micro-F1 drop when ablated
- Four iterations show diminishing returns, with convergence achieved by T=4

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional LLM-KGE Feedback Loop
The framework's core innovation is the iterative exchange between semantic extraction and structural validation. KGE provides structure-aware feedback to guide LLM extraction via evidence-grounded reasoning, while validated triples progressively improve KGE representations. This addresses the complementary failure modes where LLMs produce locally coherent but globally inconsistent triples, while KGE captures global patterns but cannot extract from text.

### Mechanism 2: Hierarchical Coarse-to-Fine Relation Classification
By reducing the relation search space from 89 to ~11 per coarse category, the framework mitigates long-tail bias inherent in severe class imbalance (top 10 relations = 68.2% of instances). The LLM first predicts coarse category (8 options), then fine-grained relation from that category's subset, allowing tail relations to compete only within their semantic neighborhood.

### Mechanism 3: Semantic-Guided Entity Initialization for Unseen Entities
The framework projects RoBERTa embeddings into RotatE space via learned linear alignment, enabling structural validation for entities absent from training data. This addresses the cold-start problem by leveraging semantic similarity from pre-trained language models to approximate structural similarity in knowledge graphs.

## Foundational Learning

- **Knowledge Graph Embeddings (RotatE):** RotatE models relations as rotations in complex vector space, capturing symmetric/antisymmetric/inverse/compositional patterns. Quick check: Given h=(1+0i), r=(0+1i) (90° rotation), t=(0+1i), what's the plausibility score? (High—rotation of h by r produces t.)

- **Long-tail Distribution:** SDG relations exhibit severe class imbalance where top relations dominate training data. Quick check: If "hasValue" has 685 instances and "hasInput" has 3, what happens to a flat classifier? (It will overwhelmingly predict "hasValue," ignoring tail types.)

- **Cold-Start Bootstrapping:** The framework initializes KGE without labeled data by using LLM-extracted, schema-validated triples as seeds. Quick check: Why can't we just start with empty KGE and validate after one LLM pass? (RotatE requires training data to learn entity/relation embeddings; without seeds, it cannot score any triple.)

## Architecture Onboarding

- **Component map:** Document Corpus → Chunking → LLM Extraction → Candidate Triples → RotatE Validation → ACCEPT/FEEDBACK/REJECT → Add to KG/Update KGE → Channel 1 Feedback → Channel 2 KGE Update

- **Critical path:** Cold-start seed generation → Initial KGE training → First extraction pass → Tri-partition validation → Channel 1 feedback (iterations 1-2) → Channel 2 KGE updates (iterations 2-4) → Convergence at T=4

- **Design tradeoffs:** Precision vs. Recall (conservative thresholds favor precision; evidence-guided feedback recovers recall), Compute vs. Quality (16 hours total; acceptable for offline KG construction), Schema rigidity vs. Coverage (out-of-schema relations discarded, potentially losing valid but novel relation types)

- **Failure signatures:** Early iterations reject valid triples (KGE undertrained → cold-start protection disables relation suggestions), Infinite feedback loops (retry limit K=3 prevents triple cycling), Hallucinated evidence (LLM generates plausible but ungrounded triples → structural validation catches globally inconsistent patterns)

- **First 3 experiments:** 1) Baseline sanity check: Run LLM Few-shot extraction without KGE validation on 100-chunk sample (expect ~25% Micro-F1 with high hallucination rate). 2) Semantic initialization validation: Ablate RoBERTa projection (use random initialization for unseen entities; expect ~8-10 point Micro-F1 drop on temporal test set). 3) Iteration convergence test: Run full pipeline but stop at T=1, T=2, T=3, T=4; plot cumulative valid triples and Micro-F1 to verify diminishing returns by T=4.

## Open Questions the Paper Calls Out

### Open Question 1
Can LEC-KG maintain its extraction performance if the hierarchical schema is generated automatically via clustering rather than being manually designed? The current framework relies on a manually constructed schema with 89 relation types, which acts as a strict constraint for the LLM but requires significant domain expertise. An experiment comparing automated schema generation against the current manual-schema baseline would validate generalizability.

### Open Question 2
Does jointly optimizing the semantic projection layer and the KGE model improve structural validation of unseen entities compared to the current separate training strategy? The semantic initialization currently trains the linear projection layer separately from RotatE embeddings, which may result in suboptimal alignment between semantic and structural spaces. An ablation study comparing two-stage training against end-to-end differentiable training would measure potential improvements.

### Open Question 3
Can replacing exact-match evidence retrieval with dense retrieval methods improve recovery of long-tail relations expressed through paraphrasing? The current feedback mechanism relies on entity-anchored exact matching, which guarantees precision but may fail to retrieve evidence for relations lacking explicit lexical markers. Evaluating recall rate using dense vector retrieval versus exact-match would validate this enhancement.

## Limitations

- **Schema rigidity:** Out-of-schema relations are discarded without recovery mechanisms, limiting the framework's ability to discover novel relation types or adapt to schema evolution.
- **Cold-start dependency:** Framework performance critically depends on initial LLM extraction pass quality; poor seed triples lead to unreliable structural feedback.
- **Domain specificity:** Current evaluation on Chinese SDG reports may not generalize to domains with different entity distributions, relation patterns, or document structures.

## Confidence

- **High Confidence:** Bidirectional refinement mechanism and hierarchical relation classification are well-specified with direct empirical validation showing 11.2-point improvement over baseline.
- **Medium Confidence:** Semantic initialization approach shows significant ablation impact but linear projection assumption lacks extensive cross-domain validation.
- **Low Confidence:** Framework's ability to handle truly unseen relation types or dynamic schema evolution is unclear due to out-of-schema relation discard policy.

## Next Checks

1. **Schema Robustness Test:** Run the framework on documents from different SDG targets (e.g., Health vs. Education) and measure performance degradation to validate whether the 89-relation schema is truly domain-general.

2. **Cold-Start Sensitivity Analysis:** Vary initial LLM extraction pass size (50, 100, 500 seed triples) and measure impact on Micro-F1 at T=4 to reveal minimum viable training set for reliable structural feedback.

3. **Temporal Generalization Stress Test:** Apply fully trained model (T=4) to a third-year SDG report (2025, if available) and compare performance to 2024 test set to validate whether iterative refinement learns domain-invariant patterns or report-specific structures.