---
ver: rpa2
title: 'On Recipe Memorization and Creativity in Large Language Models: Is Your Model
  a Creative Cook, a Bad Cook, or Merely a Plagiator?'
arxiv_id: '2506.23527'
source_url: https://arxiv.org/abs/2506.23527
tags:
- ingredients
- found
- annotation
- recipe
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-annotated dataset of 36,000 instances
  to evaluate memorization, creativity, and nonsense in LLM-generated recipes. Using
  20 recipes from Mixtral, the study annotates ingredients and tasks against 18 online
  documents to determine whether elements are memorized, creative, or nonsensical.
---

# On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?

## Quick Facts
- arXiv ID: 2506.23527
- Source URL: https://arxiv.org/abs/2506.23527
- Reference count: 15
- Primary result: LLM-as-judge pipeline achieves 78% accuracy on ingredient matching in recipe memorization detection

## Executive Summary
This paper introduces a human-annotated dataset of 36,000 instances to evaluate memorization, creativity, and nonsense in LLM-generated recipes. Using 20 recipes from Mixtral, the study annotates ingredients and tasks against 18 online documents to determine whether elements are memorized, creative, or nonsensical. Human annotators achieved 85% accuracy for ingredients and 74% for tasks. An automated "LLM-as-judge" pipeline was developed, with Llama 3.1+Gemma 2 9B achieving 78% accuracy on ingredient matching. The results suggest Mixtral strongly relies on memorized content, with no evidence of creative ingredient invention.

## Method Summary
The paper evaluates memorization in LLM-generated recipes by comparing recipe elements against retrieved online documents. Mixtral 8x7B generates 20 recipes, which are parsed into structured elements (ingredients, tasks, tools). For each recipe, 18 online documents are retrieved via search engines. Human annotators then classify each element as "Found," "Found (not perfect)," or "Not found" in the retrieved documents. An automated "LLM-as-judge" pipeline using Cloze Formulation with Llama 3.1 extraction and Gemma 2 9B annotation was developed to scale this annotation process.

## Key Results
- Human annotators achieved 85% accuracy for ingredient matching and 74% for task matching
- Best automated pipeline (Llama 3.1 extraction + Gemma 2 9B annotation) achieved 77.8% accuracy on ingredient matching
- 98% of ingredients were classified as memorized (Found or Found not perfect)
- All 8 ingredients initially marked as "Not found" were later located in other online recipes, suggesting no creative invention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization in LLM-generated recipes can be detected by matching recipe elements against potentially-seen training documents.
- Mechanism: Parse generated recipes into structured elements (ingredients, tasks, tools), retrieve Nd=18 online documents per recipe via search engines, then annotate each element's presence across documents. Elements found in documents are classified as potentially memorized; unfound elements warrant further scrutiny.
- Core assumption: If an ingredient or task appears in a retrieved online document, it was potentially present in the model's training data and thus represents memorization rather than creative synthesis.
- Evidence anchors:
  - [abstract]: "assess which elements are memorized—i.e., directly traceable to online sources possibly seen during training"
  - [Section 5]: "The annotation is based on locating each model-generated ingredient verbatim in a parsed document's ingredient list"
  - [corpus]: No direct corpus support for this specific overlap-based memorization detection method.
- Break condition: If documents containing the element existed but were not retrieved (non-exhaustivity), elements may be falsely classified as "creative." Section 6.3 shows saturation curves mitigate this for ingredients (5 documents → ~100% coverage).

### Mechanism 2
- Claim: LLMs can approximate human annotation judgments for recipe element matching using probability-based classification.
- Mechanism: Present N prompts differing only in final choice (Cloze Formulation), compute token probabilities, and select highest-probability answer. Gemma 2 9B with Llama 3.1 extraction achieved 78% accuracy vs. human annotations for 3-class ingredient matching.
- Core assumption: LLM probability distributions over annotation choices correlate with human judgment patterns.
- Evidence anchors:
  - [Section 7]: "The automated annotation utilises the Cloze Formulation (CF) approach... The LLM probability for the chosen tokens is then used to predict the answer"
  - [abstract]: "Llama 3.1+Gemma 2 9B achieving up to 78% accuracy on ingredient matching"
  - [corpus]: No corpus papers validate this specific CF approach for annotation automation.
- Break condition: Task annotation proved harder (35-50% accuracy with 4 classes), requiring reduction to 2 classes to reach ~75%. Complex multi-class distinctions degrade performance.

### Mechanism 3
- Claim: High memorization rates indicate Mixtral lacks genuine creative synthesis for recipe ingredients.
- Mechanism: For all 8 "not found perfectly" ingredients, manual expanded searches located them in other online recipes, suggesting they appeared in training data rather than being invented.
- Core assumption: Creative invention would produce ingredients not findable in any online recipe corpus.
- Evidence anchors:
  - [Section 6.1]: "In case of every missing ingredient, we managed to find a relevant document with recipe containing such an ingredient"
  - [Section 6.1]: "we conjecture that hypothesis (ii) is correct, and Mixtral model uses ingredients only and only if they were seen in the training set"
  - [corpus]: Related work (Hartmann et al., 2023, cited in paper) discusses memorization types but doesn't validate this inference method.
- Break condition: Cannot definitively prove absence of creativity—only that ingredients exist somewhere online. Recipe-specific creative combinations remain unexamined.

## Foundational Learning

- Concept: **Inter-annotator agreement (Cohen's κ)**
  - Why needed here: Determines annotation reliability before using human labels as ground truth for LLM training/evaluation.
  - Quick check question: Can you explain why κ=0.77 (ingredients) indicates "substantial" agreement while κ=0.63 (tasks) suggests only "moderate" reliability?

- Concept: **Cloze Formulation for LLM evaluation**
  - Why needed here: Enables automated multi-class annotation by comparing token probabilities across prompt variants.
  - Quick check question: How does CF differ from standard prompting when asking an LLM to choose among "Found," "Found (not perfect)," and "Not found"?

- Concept: **Exhaustivity in information retrieval**
  - Why needed here: Understanding that Nd=18 documents may miss relevant sources; saturation analysis estimates coverage adequacy.
  - Quick check question: What does Figure 1's rapid saturation (5 documents → ~100%) vs. Figure 2's slower saturation imply for ingredient vs. tool annotation reliability?

## Architecture Onboarding

- Component map:
```
Recipe Generator (Mixtral) → Filter (nonsense detection) → Parser (ingredients/tasks) → 
Document Retriever (search APIs) → Annotator (human or LLM-as-judge) → Statistics Aggregator
```

- Critical path: Annotation accuracy. All downstream conclusions depend on reliable element-to-document matching. Human baseline (85% ingredients, 74% tasks) sets the ceiling; LLM annotators (78% best) introduce additional error.

- Design tradeoffs:
  - Nd=18 documents: Balances annotation cost vs. exhaustivity. Ingredients saturate quickly; tools need more.
  - 3-class vs. 2-class annotation: More classes capture nuance but degrade LLM accuracy (35-50% → 75%).
  - Human vs. automated: Humans achieve higher accuracy but cost 7.4 hours/recipe; LLMs trade accuracy for scale.

- Failure signatures:
  - Recursive generation (Appendix B.3): Model loops endlessly—"Stir in 1/2 cup chopped [X]" repeated infinitely.
  - Prompt misunderstanding: Model treats prompt as letter reply prefix rather than instruction.
  - Extraction errors: Llama 3.1 8B extraction with Llama 3.1 8B annotation achieved only 35.5% accuracy (Table 7).

- First 3 experiments:
  1. Replicate ingredient annotation on 5 recipes with Nd=5 documents to validate saturation claim before scaling.
  2. Test whether prompt type (Appendix C's 5 variants) affects memorization rates—detailed prompts may trigger different training distributions.
  3. Compare Gemma 2 9B vs. Gemma 2 27B annotation accuracy to determine if model scale justifies computational cost for large-scale deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Mixtral generate genuinely creative recipe steps, or are all tasks also traceable to training data when exhaustively searched?
- Basis in paper: [explicit] "This hints at a considerable amount of potential creativity when it comes to Tasks in recipes. We have yet to analyze whether the lack of 'Task Found' selections is caused by a lack of exhaustivity—suggesting Mixtral is also a task plagiarator."
- Why unresolved: The 29 tasks never marked as "Task Found" were not subjected to targeted follow-up searches as was done for missing ingredients.
- What evidence would resolve it: Extend the targeted search methodology (appending task names to queries) to the 29 unfound tasks to determine if they exist in online documents.

### Open Question 2
- Question: Can the LLM-as-judge pipeline reliably detect nonsensical ingredients and tasks in generated recipes?
- Basis in paper: [explicit] "Our next steps are to... (iv) address nonsense and creativity detection."
- Why unresolved: Nonsense detection is defined but not yet implemented or evaluated; only memorization annotation is complete.
- What evidence would resolve it: Implement the proposed 4-step nonsense detection pipeline and evaluate against human judgments of nonsensical elements.

### Open Question 3
- Question: Do memorization patterns observed in recipe generation generalize to other creative domains like story writing?
- Basis in paper: [explicit] "If a model is found to be capable of mimicking the semantics of a recipe, it may be capable of doing the same in other areas, such as the creation of stories."
- Why unresolved: This paper only studies recipe generation with one model (Mixtral); cross-domain and cross-model validation is needed.
- What evidence would resolve it: Apply the same annotation methodology to LLM-generated stories or other procedural content across multiple models.

## Limitations

- Limited to recipe generation domain; results may not generalize to other creative tasks
- Cannot definitively prove absence of creativity—only that ingredients exist somewhere online
- Task annotation accuracy significantly lower than ingredient annotation (74% vs. 85%)

## Confidence

- High: Memorization detection mechanism (matching elements to documents) is sound and validated
- Medium: LLM-as-judge pipeline achieves reasonable accuracy but introduces error compared to human baseline
- Low: Conclusions about lack of creativity are inferential rather than definitively proven

## Next Checks

1. Replicate the automated annotation pipeline on a small subset (5 recipes) to verify the 78% accuracy claim before scaling
2. Validate the saturation analysis by comparing annotation results with Nd=5 vs. Nd=18 documents for both ingredients and tasks
3. Test the automated pipeline on recipes from a different model (e.g., GPT-4) to assess generalizability across models