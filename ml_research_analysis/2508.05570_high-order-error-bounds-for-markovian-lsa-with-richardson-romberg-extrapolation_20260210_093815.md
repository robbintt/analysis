---
ver: rpa2
title: High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation
arxiv_id: '2508.05570'
source_url: https://arxiv.org/abs/2508.05570
tags:
- term
- where
- lemma
- proposition
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation

## Quick Facts
- **arXiv ID:** 2508.05570
- **Source URL:** https://arxiv.org/abs/2508.05570
- **Reference count:** 40
- **Primary result:** High-probability error bounds for Richardson-Romberg (RR) extrapolated Markovian LSA showing linear bias cancellation and optimal fluctuation recovery.

## Executive Summary
This paper analyzes Linear Stochastic Approximation (LSA) with Markovian noise under constant step sizes, demonstrating that the Polyak-Ruppert averaged estimate has a bias linear in the step size $\alpha$. The authors introduce Richardson-Romberg (RR) extrapolation, combining two LSA processes with step sizes $\alpha$ and $2\alpha$ using the same noise trajectory, to eliminate this leading bias term. Theoretical results show the RR estimator achieves a residual bias of $O(\alpha^{3/2})$ while preserving the asymptotically optimal covariance, with explicit high-probability error bounds derived under uniform geometric ergodicity assumptions.

## Method Summary
The method involves running two parallel LSA processes with step sizes $\alpha$ and $2\alpha$ using the same Markovian noise sequence $\{Z_k\}$. The Polyak-Ruppert averaged iterates from each process are combined via RR extrapolation: $\bar{\theta}^{(\alpha, RR)}_n = 2\bar{\theta}^{(\alpha)}_n - \bar{\theta}^{(2\alpha)}_n$. This construction cancels the linear bias term while maintaining optimal fluctuation properties. The analysis uses perturbation expansions to decompose the error and Rosenthal-type inequalities for Markov chains to bound high-order terms.

## Key Results
- The leading bias in constant-step Polyak-Ruppert averaging is exactly linear in $\alpha$ (Proposition 2)
- RR extrapolation eliminates the linear bias, reducing residual bias to $O(\alpha^{3/2})$
- The RR estimator preserves the asymptotically optimal covariance $\Sigma_\infty = \bar{A}^{-1} \Sigma_\epsilon^{(M)} \bar{A}^{-T}$
- High-probability error bounds are derived under uniform geometric ergodicity assumptions

## Why This Works (Mechanism)

### Mechanism 1: Linearization of Markovian Bias
The Polyak-Ruppert averaged LSA with Markovian noise has a bias linear in step size $\alpha$. This arises from the fluctuation term in the error decomposition, which can be expanded into a series $\sum J_n^{(\ell)}$. The term $J^{(1)}$ captures the dominant linear bias component, shown to scale as $O(\alpha)$ under uniform geometric ergodicity and Hurwitz stability conditions.

### Mechanism 2: Richardson-Romberg (RR) Extrapolation
Since the leading bias is linear in $\alpha$, combining two processes with step sizes $\alpha$ and $2\alpha$ via the RR estimator $\bar{\theta}^{(\alpha, RR)}_n = 2\bar{\theta}^{(\alpha)}_n - \bar{\theta}^{(2\alpha)}_n$ cancels the first-order bias terms: $(2 \cdot \alpha\Delta) - (2\alpha \Delta) = 0$. This requires both processes to share the same noise trajectory to ensure higher-order terms remain correlated.

### Mechanism 3: Optimal Fluctuation Recovery
After bias elimination, the RR procedure preserves the asymptotically optimal covariance structure. The leading fluctuation term scales with $\Sigma_\infty = \bar{A}^{-1} \Sigma_\epsilon^{(M)} \bar{A}^{-T}$, which is the Rao-Cramer optimal covariance for this problem class. The transformation removes bias without inflating the variance of the optimal leading term.

## Foundational Learning

- **Markov Chain Mixing Times ($t_{mix}$)**: Essential for bounding convergence rates; determines how quickly the Markov chain forgets its past. Quick check: If $Z_k$ is periodic or has multiple disconnected modes, does $t_{mix}$ exist as defined?
- **Hurwitz Stability (Lyapunov Equation)**: Required for exponential stability and convergence of the LSA iterates. Quick check: Why does small $\|\bar{A}\|$ help bound the transient term $\tilde{\theta}_n^{(tr)}$?
- **Perturbation Expansion / Linearization**: The error decomposition into terms $J^{(0)}, J^{(1)}, J^{(2)}$ ordered by powers of $\alpha$ is key to understanding how RR isolates and cancels specific terms. Quick check: In the decomposition, which term is responsible for the linear bias that RR removes?

## Architecture Onboarding

- **Component map:** LSA Iterator ($\theta_k^{(\alpha)}$) $\rightarrow$ PR Averager ($\bar{\theta}_n^{(\alpha)}$) $\rightarrow$ RR Combiner (weighted sum with Track B)
- **Critical path:** Two parallel LSA tracks must consume the same noise sample $Z_k$ at each step. Both tracks run simultaneously with different step sizes, sharing the same Markovian noise trajectory.
- **Design tradeoffs:** Memory doubles (storing $2d$ parameters), gradient computation doubles per step, step size $\alpha$ must be carefully chosen relative to mixing time.
- **Failure signatures:** Divergence if $\bar{A}$ not Hurwitz, high variance if noise variance $\|\varepsilon\|_\infty$ is extreme, stagnation if Markov chain doesn't mix.
- **First 3 experiments:**
  1. Linear Bias Verification: Run vanilla PR-averaging, plot error vs. $n$ to confirm bias scales with $\alpha$
  2. RR Cancellation: Implement RR estimator, compare MSE vs. PR to verify error floor reduction
  3. Sensitivity Analysis: Vary mixing time $t_{mix}$, confirm error bounds degrade as mixing time increases

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the high-order error bounds and bias cancellation properties be extended to non-linear Markovian SA or settings with state-dependent noise?
- **Open Question 2:** Can the remainder term in the error bounds for RR iterates be sharpened from $O(\alpha^{3/2})$ to $O(\alpha^2)$?
- **Open Question 3:** Do the derived error bounds and bias decomposition hold under mixing conditions weaker than Uniform Geometric Ergodicity (UGE)?

## Limitations
- Analysis assumes uniform geometric ergodicity of the Markov chain, which may not hold in complex real-world environments
- High-probability bounds rely on Rosenthal-type inequalities that can be loose in practice
- Requires maintaining two parallel processes with coupled noise, doubling computational and memory requirements

## Confidence

- **Bias Structure and Decomposition (Proposition 2):** High confidence - explicit mathematical derivation
- **Richardson-Romberg Bias Cancellation:** Medium confidence - theory sound but implementation sensitive to noise coupling
- **Optimal Covariance Preservation:** Medium confidence - optimality derived under strong assumptions that may be violated

## Next Checks

1. **Noise Coupling Validation:** Implement RR with both coupled and independent noise sequences, measure MSE difference to confirm coupling is essential
2. **Perturbation Expansion Verification:** For a simple linear system, numerically compute terms $J^{(0)}, J^{(1)}, J^{(2)}$ to verify $J^{(1)}$ is the dominant linear bias term
3. **Mixing Time Sensitivity Test:** Construct Markov chains with varying mixing times, run LSA and RR algorithms, plot error scaling to confirm degradation as mixing time increases