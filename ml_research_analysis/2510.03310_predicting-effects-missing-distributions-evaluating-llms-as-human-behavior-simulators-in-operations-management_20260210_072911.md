---
ver: rpa2
title: 'Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior
  Simulators in Operations Management'
arxiv_id: '2510.03310'
source_url: https://arxiv.org/abs/2510.03310
tags:
- human
- min-p0
- llms
- data
- top-p0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can replicate most hypothesis-test
  outcomes in behavioral operations management, but their response distributions diverge
  significantly from human data. The study tested 11 LLMs across nine OM lab experiments,
  measuring distributional alignment using Wasserstein distance.
---

# Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management

## Quick Facts
- arXiv ID: 2510.03310
- Source URL: https://arxiv.org/abs/2510.03310
- Reference count: 40
- Primary result: LLMs replicate most hypothesis-level effects in behavioral operations management but show large distributional gaps from human data, partially mitigated by prompting and hyperparameter tuning

## Executive Summary
This study evaluates large language models (LLMs) as simulators of human behavior in operations management (OM) lab experiments. Testing 11 LLMs across nine OM experiments, researchers found that while models successfully replicate hypothesis-level effects and capture key decision biases, their response distributions significantly diverge from human data. The distributional gap is particularly pronounced in multi-agent settings. Lightweight interventions including Chain-of-Thought prompting and hyperparameter tuning substantially improved alignment, sometimes enabling smaller models to match larger ones. However, higher temperatures that reduce distributional gaps also increase computational costs and hallucination risks. The findings support using LLMs for hypothesis-level prototyping but caution against treating them as direct substitutes for human data in distributional-sensitive applications.

## Method Summary
The study tested 11 LLMs (DeepSeek, GPT-3.5/4/4o-mini, Llama-3.3, Qwen-2.5) on nine behavioral OM experiments using zero-shot prompting that mirrored original lab instructions. Researchers compared hypothesis replication (direction and statistical significance of effects) and distributional alignment (Wasserstein distance) against human benchmark data from Davis et al. (2023). They systematically varied generation hyperparameters (temperature, top-p, top-k, min-p) and tested Chain-of-Thought prompting as interventions. Simulations matched original sample sizes, with single-agent tasks running independent chains and multi-agent tasks managing shared history states. Analysis included hypothesis tests (t-tests, regressions) and Wasserstein distance calculations between LLM and human distributions.

## Key Results
- LLMs successfully replicated most hypothesis-level effects, capturing key decision biases in OM experiments
- Response distributions showed large Wasserstein distances from human data, particularly in multi-agent settings
- Chain-of-Thought prompting and hyperparameter tuning (especially temperature increases) substantially improved distributional alignment
- Higher temperatures improved alignment but reduced completion rates and increased hallucination risks
- Smaller models with optimal tuning could match larger models' performance

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Pattern Recognition Over Distributional Fidelity
- Pre-training enables recognition of decision-making patterns documented in academic literature, allowing models to replicate the sign of effects even without seeing specific experimental setups
- Next-token prediction objective and default sampling parameters optimize for high-probability tokens, causing outputs to cluster around limited values rather than matching human response dispersion, tails, and multi-modality
- Behavioral patterns must be sufficiently represented in pre-training data; if experimental setup is too novel or domain coverage is insufficient, models may fail to recognize relevant patterns

### Mechanism 2: Sampling Hyperparameters as a Proxy for Behavioral Heterogeneity
- Adjusting generation hyperparameters (temperature, top-p, min-p, top-k) can artificially inject variance into LLM outputs, partially mimicking human heterogeneity and reducing distributional misalignment
- Increasing temperature flattens the probability distribution over next tokens, increasing probability of lower-probability tokens and output variance
- Distributional misalignment is primarily driven by lack of dispersion in LLM outputs rather than fundamental misunderstanding of decision context

### Mechanism 3: Chain-of-Thought Prompting for Improved Reasoning Alignment
- Chain-of-Thought prompting improves distributional alignment by forcing explicit reasoning traces, steering final numerical outputs toward more human-like decisions
- CoT decomposes problems into intermediate reasoning steps, activating more relevant pre-trained knowledge and encouraging decision-making processes mirroring human cognitive steps
- Improved alignment stems from more accurate simulation of human reasoning processes rather than just increased output length or randomness introduction

## Foundational Learning

- **Wasserstein Distance**
  - Why needed here: Primary metric quantifying distributional misalignment between LLM and human data; measures "cost" of transforming one probability distribution into another, capturing location, spread, and shape differences
  - Quick check question: If an LLM produces order quantities with identical mean to humans but much smaller variance (tightly clustered), would Wasserstein distance be large or small, and why?

- **Sampling Hyperparameters in LLMs (Temperature, Top-p, Top-k, Min-p)**
  - Why needed here: Main intervention for improving distributional alignment; practitioners must understand what each parameter controls to apply this strategy and manage trade-offs
  - Quick check question: What fundamental trade-off does the paper identify when increasing temperature to improve distributional alignment?

- **Behavioral Operations Management Experiments**
  - Why needed here: Evaluation is grounded in this domain; understanding experiments (newsvendor problem, bullwhip effect, auctions) and hypotheses (mental accounting, over/under-reaction) is necessary to interpret replication results
  - Quick check question: Why was the multi-agent Kremer and Debo (2016) experiment uniquely difficult for all tested LLMs compared to single-agent experiments?

## Architecture Onboarding

- **Component map:** Human Subject Instructions -> Prompt Engineering Module -> LLM Simulation Engine -> Analysis Pipeline -> Hypothesis Test + Wasserstein Distance
- **Critical path:**
  1. Select experiment, retrieve original design/sample size
  2. Construct prompt encoding instructions, role, payoff structure
  3. Run simulation for required rounds/participants, save numerical outputs
  4. Analyze: replicate original hypothesis test on LLM data; compute Wasserstein distance between LLM and human distributions
- **Design tradeoffs:**
  - Prompt fidelity vs. leakage: Zero-shot prompts avoid "steering" but may underutilize model capacity
  - Completion rate vs. alignment: Higher temperature improves Wasserstein distance but lowers completion rate via hallucinations, increasing cost/time
  - Model scale vs. resource cost: Larger models perform best but require more compute; smaller, well-tuned models can match them
- **Failure signatures:**
  1. Consistent multi-agent failure: LLMs not updating behavior based on inferred agent states, indicating strategic reasoning breakdown
  2. Hallucination under high temperature: Temperatures >1.5 causing frequent invalid numerical outputs
  3. Distributional clustering: Sharp peaks on few integer values while human data is smooth/dispersed, yielding high Wasserstein distance
- **First 3 experiments:**
  1. Establish baseline: Run simplest single-agent experiment (Chen et al. 2013) with default GPT-4/Llama-70B; compute hypothesis test and Wasserstein distance
  2. Tune hyperparameters: Systematically vary temperature (0.5-2.5) and sampling method; plot Wasserstein distance and completion rate vs. temperature
  3. Test Chain-of-Thought: Re-run with CoT prompt; compare quantities, reasoning traces, and Wasserstein distances to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Can the distributional gap between LLM and human responses be fully closed without fine-tuning, or is some level of distributional misalignment inherent to current LLM architectures?
- The authors note that while interventions reduced Wasserstein distances, they did not eliminate them, and it remains unclear whether remaining gaps reflect fundamental architectural limitations or could be addressed with more sophisticated lightweight methods

### Open Question 2
- What mechanisms explain why multi-agent experiments are consistently harder for LLMs to replicate than single-agent experiments, and can targeted interventions mitigate this gap?
- The paper documents that multi-agent tasks are consistently harder but does not isolate whether the issue stems from limited theory-of-mind reasoning, insufficient context retention across agent turns, or other specific mechanisms

### Open Question 3
- How can optimal hyperparameter settings (temperature, sampling methods) be systematically determined for a given behavioral simulation task without exhaustive grid search?
- The current approach required testing 144 hyperparameter combinations across nine experiments (over four million LLM interactions), which is computationally expensive and impractical for routine use

## Limitations
- Only one of nine experimental prompts is fully specified, raising concerns about replicability and sensitivity to minor wording changes
- Wasserstein distance may not fully represent multi-modal response patterns or contextual decision structures that characterize human behavior
- Static LLM simulations may not adequately capture learning effects that humans exhibit in multi-round experiments

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| LLMs can replicate hypothesis-level effects in single-agent behavioral OM experiments | High |
| Lightweight interventions meaningfully improve distributional alignment | Medium |
| LLMs are viable substitutes for human data in distributional-sensitive applications | Low |

## Next Checks
1. Systematically vary key prompt components (instruction framing, historical information presentation) for one experiment to quantify impact on hypothesis replication and distributional alignment
2. Implement learning mechanisms in LLM simulations and compare distributional evolution against human learning curves in multi-round experiments
3. Compute additional distributional similarity measures (e.g., Kolmogorov-Smirnov, Jensen-Shannon divergence) alongside Wasserstein distance to validate robustness of distributional misalignment findings