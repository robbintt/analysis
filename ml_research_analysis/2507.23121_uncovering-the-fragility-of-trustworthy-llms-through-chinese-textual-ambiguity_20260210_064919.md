---
ver: rpa2
title: Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity
arxiv_id: '2507.23121'
source_url: https://arxiv.org/abs/2507.23121
tags:
- ambiguity
- llms
- language
- ambiguous
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models handle Chinese
  textual ambiguity, revealing significant limitations in their ability to detect
  and interpret ambiguous sentences. The researchers created a benchmark dataset of
  900 annotated ambiguous sentences with multiple interpretations across nine categories.
---

# Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity

## Quick Facts
- **arXiv ID**: 2507.23121
- **Source URL**: https://arxiv.org/abs/2507.23121
- **Reference count**: 40
- **Primary result**: Large language models show significant limitations in detecting and interpreting Chinese textual ambiguity, with overconfidence and overthinking behaviors.

## Executive Summary
This study systematically investigates how large language models handle Chinese textual ambiguity through a carefully constructed benchmark of 900 annotated ambiguous sentences. The researchers evaluated multiple open-weight models across detection, interpretation, and overthinking tasks, revealing fundamental limitations in LLM trustworthiness when faced with linguistic uncertainty. The work demonstrates that even state-of-the-art models struggle with ambiguity detection and often exhibit problematic behaviors like overconfidence and overthinking, particularly when explicitly prompted about ambiguity. Retrieval-augmented generation (RAG) emerged as the most effective approach for improving performance, especially for medium-scale models.

## Method Summary
The researchers constructed a benchmark dataset of 900 Chinese sentences containing nine types of ambiguity, including polysemy, homonymy, and contextual vagueness. They employed human annotators to label sentences with multiple interpretations and built a comprehensive evaluation framework testing models across three tasks: ambiguity detection (identifying whether sentences are ambiguous), ambiguity interpretation (providing correct interpretations), and overthinking behavior (assessing how models respond when explicitly prompted about ambiguity). The study tested various approaches including direct fine-tuning, retrieval-augmented generation (RAG), and chain-of-thought prompting across multiple model families and sizes, from small models to state-of-the-art systems like DeepSeek-R1.

## Key Results
- LLMs struggle significantly to distinguish ambiguous from unambiguous Chinese text, with detection performance well below human-level accuracy
- Models exhibit overconfidence in their interpretations of ambiguous sentences, often providing single interpretations when multiple valid readings exist
- RAG approach showed the most promise, particularly improving medium-scale models' performance on both detection and interpretation tasks
- Larger models and reasoning-enhanced models generally performed better, but even top models like DeepSeek-R1 showed fragility when confronted with ambiguity

## Why This Works (Mechanism)
The study's methodology works by creating a controlled experimental environment that isolates textual ambiguity as a specific challenge for LLMs. By using carefully constructed sentences with clear ambiguity categories and multiple human-annotated interpretations, the researchers can measure model performance against ground truth rather than subjective judgments. The multi-task evaluation framework (detection, interpretation, overthinking) reveals different failure modes that single-task evaluations might miss. The comparison of different approaches (fine-tuning vs RAG vs prompting) provides insights into which architectural choices best handle linguistic uncertainty.

## Foundational Learning

**Chinese textual ambiguity categories**: Understanding the nine types of ambiguity (polysemy, homonymy, etc.) is essential for interpreting results and designing better benchmarks. Quick check: Can you classify a given Chinese sentence into one of the nine ambiguity types?

**Ambiguity detection vs interpretation**: These are distinct tasks requiring different capabilities - detection needs sensitivity to uncertainty, while interpretation requires knowledge to generate multiple valid readings. Quick check: Does a model correctly identify ambiguity before attempting interpretation?

**Overthinking behavior**: When models explicitly consider ambiguity, they may generate excessive reasoning that degrades rather than improves performance. Quick check: Does prompting about ambiguity improve or worsen model outputs?

**RAG effectiveness**: Retrieval-augmented generation can provide external context that helps resolve ambiguity, but effectiveness varies by model scale. Quick check: Does adding relevant context from retrieval improve ambiguity handling?

**Model scale effects**: Larger models generally perform better on ambiguity tasks, but diminishing returns and different failure modes emerge at different scales. Quick check: How does performance scale with model parameters for ambiguity detection?

## Architecture Onboarding

**Component map**: Raw text → Ambiguity detection module → Interpretation module → Overthinking analysis → Performance evaluation

**Critical path**: Text input → Ambiguity detection → Interpretation generation → Multiple interpretation validation → Confidence scoring

**Design tradeoffs**: The study balances controlled experimental conditions (clean, annotated data) against real-world applicability (natural language complexity). Fine-tuning provides better adaptation but requires more resources than RAG, which depends on external knowledge availability.

**Failure signatures**: Overconfidence manifests as single interpretations for clearly ambiguous sentences; overthinking appears as verbose reasoning without improved accuracy; detection failures occur when models misclassify unambiguous text as ambiguous or vice versa.

**First 3 experiments**: 
1. Test baseline model on ambiguity detection task without any prompting
2. Apply RAG approach to same models and compare detection accuracy
3. Evaluate overthinking behavior by explicitly prompting models about ambiguity and measuring reasoning quality vs accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond those addressed in the study's discussion and future work sections.

## Limitations

The dataset construction relies on nine predefined ambiguity categories that may not capture all real-world ambiguity forms. Human annotation introduces potential subjectivity despite high inter-annotator agreement. The evaluation focuses on detection and interpretation without extensive testing of downstream application impacts. RAG effectiveness depends on external knowledge base availability that may not suit all use cases.

## Confidence

**High Confidence**: LLMs struggle with Chinese textual ambiguity detection is well-supported by comprehensive benchmark testing. Overconfidence in interpretations has strong empirical backing.

**Medium Confidence**: Medium-scale models benefit more from RAG than large models, while large models benefit more from direct fine-tuning. This finding depends on specific experimental conditions.

**Medium Confidence**: Overthinking behavior when explicitly prompted about ambiguity. The psychological framing may oversimplify observed complex reasoning patterns.

## Next Checks

1. Test benchmark on additional Chinese dialect variants and informal language styles to assess generalizability beyond controlled dataset.

2. Conduct real-world application testing in domains like legal documents, medical instructions, or customer service where ambiguity naturally occurs.

3. Evaluate temporal stability by testing models on same sentences after incremental fine-tuning with non-ambiguity-specific data to assess catastrophic forgetting effects.