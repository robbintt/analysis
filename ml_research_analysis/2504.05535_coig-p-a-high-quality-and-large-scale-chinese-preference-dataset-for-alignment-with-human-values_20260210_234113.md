---
ver: rpa2
title: 'COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment
  with Human Values'
arxiv_id: '2504.05535'
source_url: https://arxiv.org/abs/2504.05535
tags:
- chinese
- llms
- datasets
- zhang
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COIG-P, a large-scale Chinese preference
  dataset designed to align large language models with human values. The dataset contains
  1,006k samples spanning six domains (Chat, Code, Math, Logic, Novel, and Role) and
  was curated using an LLM-based pipeline without human intervention.
---

# COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values

## Quick Facts
- arXiv ID: 2504.05535
- Source URL: https://arxiv.org/abs/2504.05535
- Reference count: 40
- Dataset size: 1,006k Chinese preference pairs across 6 domains

## Executive Summary
This paper introduces COIG-P, a large-scale Chinese preference dataset designed to align large language models with human values. The dataset contains 1,006k samples spanning six domains (Chat, Code, Math, Logic, Novel, and Role) and was curated using an LLM-based pipeline without human intervention. The method involves collecting 92k high-quality Chinese queries, generating responses with 15 diverse LLMs, and scoring them using 8 LLMs to form chosen-rejected pairs with a score threshold of 2. The dataset was used to train a 8B Chinese Reward Model (CRM) and construct a Chinese Reward Benchmark (CRBench) for efficient scoring. Evaluation on AlignBench shows COIG-P significantly improves model performance by 2%–12% across Qwen2/2.5 and Infinity-Instruct-3M-0625 series.

## Method Summary
COIG-P was constructed through a multi-stage LLM pipeline. First, 92k high-quality Chinese queries were collected and filtered. These queries were then sent to 15 diverse LLMs to generate responses. An ensemble of 8 LLMs scored each response on a 1-10 scale using domain-specific prompts. Chosen-rejected pairs were formed when the score difference exceeded a threshold of 2. Half of the resulting 1M pairs were used to train an 8B Chinese Reward Model (CRM) using Bradley-Terry loss, while the other half served as a held-out test set. The CRM was then used to filter low-quality samples from a 430k sample set, with results comparable to GPT-4o filtering but achieved in 40 GPU hours versus API costs.

## Key Results
- Dataset contains 1,006k chosen-rejected response pairs across 6 domains
- CRM achieves 5.26 Overall score on AlignBench filtering vs GPT-4o's 5.28
- Alignment improves performance by 2%–12% on Qwen2/2.5 and Infinity-Instruct-3M-0625 series
- Domain mixing outperforms single-domain training, with only mixed training consistently improving all subtasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model generation and scoring creates robust preference pairs.
- Mechanism: 15 diverse LLMs generate responses to 92k queries, then 8 different LLMs score each response on a 1-10 scale using domain-specific prompts. Chosen-rejected pairs are formed only when score difference exceeds threshold of 2, filtering ambiguous cases.
- Core assumption: Model diversity in generation and scoring reduces individual model bias and captures genuine quality differences.
- Evidence anchors:
  - [abstract]: "employed 15 mainstream LLMs to generate and score chosen-rejected response pairs... form chosen and rejected response pairs with a score threshold of 2"
  - [section 5.3]: Shows threshold of 2 optimizes AlignBench performance vs other thresholds
  - [corpus]: AIR paper analyzes annotation/instruction/response components in preference datasets, supporting multi-component approach

### Mechanism 2
- Claim: Multi-domain mixing outperforms single-domain preference training.
- Mechanism: Dataset spans Chat (702k pairs), Math (156k), Logic (55k), Code (40k), Novel (34k), and Role (19k). Models trained on full mix outperform those trained on any single domain alone.
- Core assumption: Diverse preference signals prevent overfitting to narrow conversational patterns and improve generalization.
- Evidence anchors:
  - [section 5.2]: Ablation study shows single-domain training often harms overall performance; only mixed training consistently improves all subtasks
  - [table 4]: Novel-only training improves Fund. score by 0.71 but mixed training achieves better overall results (5.47 vs 5.29)

### Mechanism 3
- Claim: Bradley-Terry reward model on preference data enables efficient inference-time scoring.
- Mechanism: CRM (8B parameters) trained on half of COIG-P using Bradley-Terry loss learns to predict which response humans prefer. At inference, it scores single responses without needing pairwise comparison.
- Core assumption: Reward model generalizes from training preference pairs to score unseen responses consistently with human preferences.
- Evidence anchors:
  - [section 6.1]: "we use the Bradley-Terry (BT) Reward Modeling method"
  - [section 6.3]: CRM achieves 5.26 Overall score on AlignBench filtering vs GPT-4o's 5.28, while being "notably faster" (40 GPU hours vs API costs for 430k samples)

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Core training algorithm used throughout experiments (beta=0.1, lr=1e-6 to 1e-7)
  - Quick check question: Can you explain why DPO eliminates the need for explicit reward model sampling during policy optimization?

- Concept: **Bradley-Terry Model**
  - Why needed here: Mathematical foundation for CRM training (P(a1≻a2|x,a1,a2) = σ(r*(x,a1) - r*(x,a2)))
  - Quick check question: Given two responses with reward scores 7.2 and 5.8, what's the probability the first is preferred?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: Entire pipeline relies on 8 LLMs scoring responses; evaluation uses GPT-4o-0806 as AlignBench judge
  - Quick check question: What biases might occur when LLMs evaluate responses from models in their own family?

## Architecture Onboarding

- Component map:
  - Query Pipeline: Crawl → Deduplicate (SentenceBERT) → Filter (Qwen2-72B scoring + rules) → 92k queries
  - Response Pipeline: Query → 15 LLMs generate → 8 LLMs score (domain-specific prompts) → Pair formation (threshold ≥2) → 1M pairs
  - CRM Pipeline: Half of COIG-P → Bradley-Terry loss → Llama3.1-8B-Instruct backbone → 8B reward model
  - Evaluation: AlignBench (GPT-4o judge) + CRBench (1,040 human-verified samples)

- Critical path:
  1. Query quality determines downstream value (Section 3.1 deduplication + Qwen2-72B filtering)
  2. Score threshold calibration (Section 5.3 shows threshold=2 optimal; test on 1k sample subset)
  3. CRM training split (Section 6.1 uses 50/50 split to enable held-out testing)

- Design tradeoffs:
  - Scale vs. Quality: 15 LLMs for generation increases cost but ensures diversity; 8 scorers balances robustness vs. expense
  - Threshold sensitivity: Higher threshold yields fewer but cleaner pairs (Figure 3 shows performance peaks at 2.0)
  - CRM size: 8B model chosen for efficiency; paper shows 40 GPU hours for 430k samples vs. GPT-4o API costs

- Failure signatures:
  - Single-domain training degrades performance (Table 4: Code-only drops Overall from 4.96 to 4.72)
  - Low threshold (<1.5) creates ambiguous pairs; high threshold (>2.5) reduces dataset size excessively
  - Existing Chinese datasets (CVALUES, RLHF-CN) actively harm alignment (Table 5: 3.54 and 3.79 Overall scores)

- First 3 experiments:
  1. **Threshold sweep**: Replicate Figure 3 on 1k query subset across thresholds [0.5, 1.0, 1.5, 2.0, 2.5, 3.0] to validate optimal threshold for your model
  2. **Domain ablation**: Train separate models on Chat-only vs. mixed-domain subsets (Table 4 protocol) to confirm mixing benefit
  3. **CRM validation**: Score held-out test split with CRM vs. GPT-4o, train models on each filtered set, compare AlignBench scores (Section 6.3 protocol)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal score-difference threshold for constructing preference pairs generalize across different model architectures, or does it require re-calibration for each new base model?
- Basis in paper: [inferred] Section 5.3 selects a threshold of 2 based on experiments with Infinity-Instruct-3M-0625-Qwen2-7B, but does not test this threshold on other backbone models.
- Why unresolved: The threshold is empirically chosen on a single model; whether it is dataset-intrinsic or model-dependent remains untested.
- What evidence would resolve it: Ablation experiments repeating the threshold sweep on multiple diverse backbone models (e.g., Llama, Mistral) and reporting variance in optimal threshold.

### Open Question 2
- Question: Does using a pool of 8 diverse LLMs as scorers effectively mitigate the "narcissistic evaluation" bias where models favor their own outputs?
- Basis in paper: [explicit] Related Work mentions that "evaluation models may favor responses that resemble their own outputs" as a known issue in LLM-based annotation, but the paper does not analyze bias in its own scorer pool.
- Why unresolved: The paper uses multiple scorers to increase robustness but provides no analysis of scorer-specific biases or overlap between scorers and generators.
- What evidence would resolve it: A per-scorer bias analysis measuring whether each scorer systematically ranks responses from specific generator models higher, followed by debiasing experiments.

### Open Question 3
- Question: What is the optimal domain mixing ratio for preference dataset training, and does it vary based on the base model's pre-existing capabilities?
- Basis in paper: [inferred] The ablation study (Section 5.2) shows mixed-domain training outperforms single-domain training, but uses a fixed ratio (derived from the collected data distribution) without exploring alternative proportions.
- Why unresolved: The current distribution (Chat and Math dominant) is determined by collection availability, not optimization; whether this is universally optimal is unknown.
- What evidence would resolve it: Systematic experiments varying domain proportions (e.g., upsampling underrepresented domains) and measuring effects on both strong and weak backbone models.

## Limitations

- Dataset construction relies entirely on LLM-generated and LLM-scored data without human verification
- Scoring mechanism's independence is questionable due to potential correlated biases among judge LLMs
- Domain mixing benefit needs clearer theoretical justification beyond empirical observation

## Confidence

- **High confidence**: Dataset scale and domain coverage are well-documented (1M samples, 6 domains); CRM achieves GPT-4o-comparable filtering performance; DPO alignment improves AlignBench scores by 2-12%
- **Medium confidence**: The claimed human values alignment based on LLM scoring alone; the generalizability of domain mixing benefits across different model architectures
- **Low confidence**: The absolute quality of preference pairs without human verification; the stability of LLM scoring across different API versions and model families

## Next Checks

1. **Human verification study**: Select 100 randomly sampled chosen-rejected pairs from COIG-P and have human annotators evaluate whether the "chosen" response genuinely reflects human preferences, comparing agreement rates with LLM scoring

2. **Cross-dataset transfer test**: Train the same CRM architecture on a subset of COIG-P and evaluate on CVALUES and RLHF-CN to measure cross-dataset generalization and potential overfitting to COIG-P's generation patterns

3. **Bias audit**: Analyze score distributions across judge LLM families (e.g., GPT vs. Claude vs. Qwen) to identify systematic biases where certain models consistently score their own outputs higher, quantifying the magnitude and impact on pair quality