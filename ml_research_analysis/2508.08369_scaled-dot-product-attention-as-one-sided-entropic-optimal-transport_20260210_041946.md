---
ver: rpa2
title: Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport
arxiv_id: '2508.08369'
source_url: https://arxiv.org/abs/2508.08369
tags:
- attention
- optimal
- problem
- transport
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a first-principles justification for scaled-dot-product
  attention (SDPA), revealing it as the solution to a degenerate, one-sided Entropic
  Optimal Transport (EOT) problem. The EOT formulation seeks a distribution maximizing
  similarity to a query while being maximally entropic.
---

# Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport

## Quick Facts
- arXiv ID: 2508.08369
- Source URL: https://arxiv.org/abs/2508.08369
- Reference count: 34
- Primary result: SDPA is the solution to a one-sided Entropic Optimal Transport problem, with backward pass implementing advantage-based policy gradients

## Executive Summary
This paper establishes a first-principles justification for scaled-dot-product attention (SDPA), revealing it as the solution to a degenerate, one-sided Entropic Optimal Transport (EOT) problem. The EOT formulation seeks a distribution maximizing similarity to a query while being maximally entropic. Crucially, the backward pass implements an advantage-based policy gradient, a variance-reduced update from reinforcement learning. The key outcome is that the shared Fisher Information geometry between the forward optimization and the space of attention distributions dictates this learning rule, unifying the perspectives of optimal transport, information geometry, and reinforcement learning.

## Method Summary
The paper provides an analytical derivation showing that softmax attention weights are the unique solution to a constrained optimization problem balancing similarity maximization with entropy maximization. The forward pass solves one-sided entropic optimal transport by minimizing J(p) = Σⱼ pⱼCⱼ - εH(p) over the probability simplex. The backward pass computes gradients that factor into probability times advantage, revealing an implicit policy gradient structure. The Fisher Information Matrix of the attention distribution equals the Hessian of the dual potential, establishing a unified geometric interpretation connecting the forward and backward passes through information geometry.

## Key Results
- Softmax attention weights emerge as the analytical solution to one-sided entropic optimal transport
- Standard backpropagation through attention computes variance-reduced advantage-based policy gradients
- The Fisher Information Matrix of attention distributions equals the Hessian of the dual potential, unifying forward and backward dynamics

## Why This Works (Mechanism)

### Mechanism 1: Forward Pass Solves One-Sided Entropic Optimal Transport
The softmax attention weights are the unique analytical solution to a constrained optimization problem that balances similarity maximization with entropy maximization. The EOT objective J(p) = Σⱼ pⱼCⱼ - εH(p) is minimized over the probability simplex. The cost Cⱼ = -⟨q,kⱼ⟩ makes similarity maximization equivalent to cost minimization. With ε = τ (temperature), the KKT conditions yield pⱼ ∝ exp(⟨q,kⱼ⟩/τ)—exactly the softmax. Core assumption: The entropy regularizer must be Shannon entropy; alternative regularizers (Tsallis, L2) produce different attention mechanisms (Sparsemax, α-entmax).

### Mechanism 2: Backward Pass Implements Advantage-Based Policy Gradient
Standard backpropagation through attention computes a variance-reduced reinforcement learning update, not a naive gradient. The gradient ∂L/∂sⱼ = -(pⱼ*/τ)(uⱼ - E[u]) where uⱼ = -⟨∇cL, vⱼ⟩ is the marginal utility. The term Aⱼ = uⱼ - E[u] is the advantage—how much better key j performs vs. the current policy's average. This is REINFORCE with baseline. Core assumption: Assumes a differentiable downstream loss L(c) and that the softmax Jacobian structure (diag(p) - pp^T) holds.

### Mechanism 3: Fisher Information Geometry Governs Learning Dynamics
The softmax Jacobian diag(p) - pp^T is proportional to the Fisher Information Matrix, making standard gradient a manifold-aware natural gradient variant. The dual potential φ*(s) = τ·logsumexp(s/τ) has Hessian ∇²φ* = (1/τ)(diag(p) - pp^T) = F(s)/τ. Since ∇sL = -τF(s)u, the standard gradient equals the natural gradient direction (u) preconditioned by the FIM. Core assumption: Strong duality holds for the EOT problem (satisfied due to convexity and Slater's condition).

## Foundational Learning

- Concept: **Lagrangian Duality / KKT Conditions**
  - Why needed here: The paper derives attention weights as solutions to constrained optimization; understanding how Lagrange multipliers enforce the simplex constraint is essential for following Theorem 3.3.
  - Quick check question: Given a minimization over the simplex, what role does the multiplier λ play in the solution?

- Concept: **Policy Gradient / REINFORCE**
  - Why needed here: The backward pass equivalence requires recognizing the advantage-based update form; without RL background, Theorem 5.2 appears as algebraic manipulation rather than a principled learning rule.
  - Quick check question: Why does subtracting the expected value baseline reduce gradient variance without introducing bias?

- Concept: **Fisher Information Matrix / Natural Gradient**
  - Why needed here: Section 7 unifies the paper through information geometry; the FIM describes curvature of the probability distribution manifold and explains why standard gradient has the specific form it does.
  - Quick check question: How does the FIM relate to the Hessian of the log-sum-exp potential?

## Architecture Onboarding

- Component map:
  - Forward: Query q + Keys {kⱼ} → Scores s = ⟨q,kⱼ⟩ → Softmax(p) → Context c = Σⱼ pⱼvⱼ
  - Backward: ∇cL → Marginal utilities uⱼ = -⟨∇cL, vⱼ⟩ → Advantage Aⱼ = uⱼ - E[u] → Gradient ∂L/∂sⱼ = -(pⱼ/τ)Aⱼ
  - Geometry: Dual potential φ* = τ·LSE(s/τ) with Hessian = FIM scaled by τ

- Critical path:
  1. Understand EOT formulation (Section 3) → why softmax emerges from entropy-similarity trade-off
  2. Trace gradient derivation (Section 5) → recognize advantage-based structure
  3. Connect via FIM (Section 7) → unified geometric interpretation

- Design tradeoffs:
  - Regularizer choice: Shannon entropy → softmax (dense); Tsallis entropy → sparse attention (α-entmax); L2 norm → Sparsemax
  - Temperature τ: Large → uniform attention (high entropy); Small → focused attention (cost-dominated); Standard τ = √dₖ normalizes score variance
  - Structural bias: Adding linear penalty (ALiBi) modifies cost to C'ⱼ = -sⱼ + γ|i-j|, encouraging locality

- Failure signatures:
  - Gradient explosion: If τ → 0 and scores have high variance, softmax saturates → near-zero gradients for most positions
  - Rank collapse: If all queries attend uniformly (high τ or similar keys), context vectors become nearly identical
  - Advantage instability: If marginal utilities uⱼ have high variance across keys, gradient updates become high-variance despite baseline

- First 3 experiments:
  1. **Temperature sweep**: Train with varying τ (fixed and learnable) and measure entropy of attention distributions vs. task performance to verify the cost-entropy trade-off prediction.
  2. **Gradient structure validation**: Log the advantage-weighted gradient vs. standard gradient during training; confirm they produce identical parameter updates numerically.
  3. **Regularizer ablation**: Replace Shannon entropy with Tsallis (α=1.5) and verify sparse attention emerges while maintaining the FIM-gradient relationship holds for the modified geometry.

## Open Questions the Paper Calls Out

### Open Question 1
Does the geometric relationship established for SDPA (where the Hessian of the dual potential equals the Fisher Information Matrix) generalize to alternative attention mechanisms like Sparsemax or $\alpha$-entmax? The paper demonstrates the unified geometry for standard SDPA but does not verify if the "shared Fisher Information geometry" dictates the learning rule for the sparsity-inducing regularizers discussed.

### Open Question 2
Can the "advantage-based policy gradient" interpretation of the backward pass be extended to two-sided Entropic Optimal Transport attention mechanisms? The derivation relies on the "one-sided" nature where the target marginal is unconstrained; enforcing a target marginal constraint likely changes the optimization geometry and thus the interpretation of the gradient.

### Open Question 3
Does the implicit inclusion of the Fisher Information Matrix in the gradient provide empirical convergence benefits typically associated with Natural Gradient methods? While the paper establishes the mathematical identity, it does not validate if this structural property translates to the training stability or efficiency improvements observed in explicit natural gradient descent.

## Limitations
- Temperature parameterization: Assumes τ = √dₖ is optimal without empirical validation across different model architectures or data modalities
- Non-differentiable losses: RL interpretation may not apply to attention applications with discrete decisions or non-differentiable objectives
- Fisher Information curvature: FIM may become ill-conditioned as attention distributions become sparse during training

## Confidence
- High confidence: The EOT formulation and softmax solution (Section 3). The Lagrangian derivation is mathematically rigorous with uniqueness guarantees.
- Medium confidence: The backward pass as advantage-based policy gradient (Section 5). While the algebraic derivation is correct, practical variance reduction benefits depend on downstream task characteristics.
- Medium confidence: The FIM geometry connection (Section 7). The mathematical relationship is established, but practical implications for optimization dynamics require empirical validation.

## Next Checks
1. **Empirical temperature sweep**: Systematically vary τ (fixed and learnable) across diverse attention tasks and measure the entropy-cost trade-off to validate the EOT interpretation predicts optimal attention behavior.
2. **Variance analysis**: Compare the empirical variance of advantage-based gradients vs. naive gradients during training across multiple datasets to quantify the practical benefit of the RL interpretation.
3. **Regularizer ablation study**: Implement Tsallis and L2 regularizers in attention mechanisms and verify that the corresponding sparse attention patterns emerge while maintaining the FIM-gradient relationship for their respective geometries.