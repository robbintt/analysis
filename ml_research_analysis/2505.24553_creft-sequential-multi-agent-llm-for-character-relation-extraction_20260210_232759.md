---
ver: rpa2
title: 'CREFT: Sequential Multi-Agent LLM for Character Relation Extraction'
arxiv_id: '2505.24553'
source_url: https://arxiv.org/abs/2505.24553
tags:
- character
- name
- list
- characters
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CREFT, a sequential multi-agent LLM framework
  for extracting and refining character relationships in long-form narratives. CREFT
  iteratively improves character composition, relation extraction, role identification,
  and group assignments through specialized agents, outperforming single-agent baselines
  on a Korean drama dataset.
---

# CREFT: Sequential Multi-Agent LLM for Character Relation Extraction

## Quick Facts
- **arXiv ID:** 2505.24553
- **Source URL:** https://arxiv.org/abs/2505.24553
- **Reference count:** 40
- **Primary result:** Sequential multi-agent LLM framework achieves 18.8% improvement in character recall and 16.5% increase in group match F1-score for Korean drama character relation extraction.

## Executive Summary
This paper introduces CREFT, a sequential multi-agent LLM framework for extracting and refining character relationships in long-form narratives. CREFT iteratively improves character composition, relation extraction, role identification, and group assignments through specialized agents, outperforming single-agent baselines on a Korean drama dataset. Results show an 18.8% absolute improvement in character recall and a 16.5% increase in group match F1-score. The method also advances entity and relation extraction in NLP by addressing challenges in identity resolution, implicit relationship detection, and group assignment accuracy, with benefits for script evaluation in entertainment and education.

## Method Summary
CREFT employs a sequential multi-agent approach where specialized LLM agents handle distinct sub-tasks: character merging (resolving aliases using treatment summaries), relation extraction (identifying explicit/implicit links), irrelevant character filtering, role identification, and grouping by affiliation. The framework uses knowledge distillation to train a Korean LLM on GPT-4o-generated SPO triplets, builds a base character graph, applies Personalized PageRank (PPR) for character selection (threshold=0.02), and refines the graph through the agent pipeline. Evaluation uses KURE-v1 embeddings for semantic similarity on 15 human-annotated Korean dramas.

## Key Results
- 18.8% absolute improvement in character recall over single-agent baselines
- 16.5% increase in group match F1-score for affiliation clustering
- Demonstrated effectiveness in resolving aliases and extracting implicit relationships

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequentially decomposing a complex extraction task into specialized sub-tasks appears to reduce error accumulation compared to monolithic single-pass prompting.
- **Mechanism:** Instead of one LLM handling merging, extraction, and grouping simultaneously, CREFT chains agents. Agent A outputs a clean character list, which reduces the context load and error surface for Agent B (Relation Extraction).
- **Core assumption:** Compounded error rates in a single large prompt exceed the sum of error rates in sequential, focused prompts.
- **Evidence anchors:**
  - [abstract] Mentions "iteratively refines character composition, relation extraction..." via "specialized LLM agents."
  - [section 3.2] Describes the sequential chain: "Each agent builds upon the output of the previous agent."
  - [corpus] NexusSum (arXiv:2505.24575) similarly uses hierarchical agents for long-form narratives, supporting the efficacy of decomposition.
- **Break condition:** If an early agent (e.g., Merge) hallucinates or removes a critical entity, subsequent agents lack the context to recover it, leading to a cascading failure.

### Mechanism 2
- **Claim:** Using explicit "treatment" context (meta-data) allows for more robust entity resolution (alias merging) than graph topology alone.
- **Mechanism:** A specialized agent cross-references the base graph against external summaries (treatments) to determine if "Professor Cha" and "Young-min Cha" are the same node.
- **Core assumption:** The provided treatment/summary text is accurate and contains sufficient identity cues (names/titles) to resolve ambiguities present in the raw script.
- **Evidence anchors:**
  - [section 3.2] States the agent "leveraged contextual information from treatments... to accurately merge nodes."
  - [figure 2] Visually demonstrates the single-agent failing to merge aliases (producing redundant nodes E', O') while the multi-agent approach succeeds.
  - [corpus] MAQInstruct (arXiv:2502.03954) implies structured instructions aid extraction, though CREFT specifically relies on external context for identity.
- **Break condition:** If the input narrative lacks accompanying treatment files, the Merging agent loses its primary ground-truth source, likely degrading to simple string matching.

### Mechanism 3
- **Claim:** Propagating user-defined importance via Personalized PageRank (PPR) can identify narratively significant characters who appear late or infrequently, where frequency-based counts fail.
- **Mechanism:** Initial seeds (Main/Sub characters) are assigned high scores. The algorithm propagates these scores to connected nodes, surfacing characters connected to key figures even if their raw dialogue count is low.
- **Core assumption:** Narrative significance is a function of proximity to known main characters, not just interaction volume.
- **Evidence anchors:**
  - [section 5.2] Notes "PPR yielded more accurate results" specifically when "key characters appeared less frequently in the first four episodes."
  - [table 2] Shows PPR significantly outperforming Count in specific dramas (e.g., Drama 1, 12, 13), though the aggregate average is comparable.
  - [corpus] No direct corpus neighbor uses PPR for this specific extraction task; this appears to be a domain-specific adaptation.
- **Break condition:** If main characters are isolated (structurally disconnected) in the initial graph, PPR cannot propagate importance effectively, causing recall to drop below simple frequency counting.

## Foundational Learning

- **Concept:** **Personalized PageRank (PPR)**
  - **Why needed here:** Used in the Character Selection phase to weigh character importance based on their relationship to user-defined "main" characters, rather than just counting lines.
  - **Quick check question:** If you increase the damping factor in PPR, how does it affect the weight given to the initial "seed" nodes vs. the graph structure?

- **Concept:** **Knowledge Distillation**
  - **Why needed here:** Used to build the Base Character Graph. Since scripts are confidential, a small local model is taught to mimic GPT-4's SPO extraction capabilities, allowing offline processing.
  - **Quick check question:** In this setup, what serves as the "ground truth" labels for the student model during the distillation training phase?

- **Concept:** **SPO (Subject-Predicate-Object) Triplets**
  - **Why needed here:** The fundamental unit of the Base Character Graph. Agents refine these raw triplets into a structured Character Relation Structure (CRS).
  - **Quick check question:** How does a raw SPO triplet (e.g., "He strikes She") differ from the final Explicit Relation labeled by the Relation Extraction agent?

## Architecture Onboarding

- **Component map:**
  1. **Input:** Drama Script + Treatment/Summary.
  2. **Base Constructor:** Fine-tuned LLM extracts SPO triplets -> Builds Base Graph.
  3. **Character Selector:** PPR algorithm prunes graph to top relevant nodes.
  4. **Sequential Agents:**
     - *Agent 1 (Merge):* Resolves aliases using Treatment.
     - *Agent 2 (Relation):* Identifies Explicit/Implicit links.
     - *Agent 3 (Filter):* Removes generic characters (e.g., "Doctor").
     - *Agent 4 (Role):* Assigns professions.
     - *Agent 5 (Group):* Clusters by affiliation.
  5. **Output:** Character Relation Structure (CRS).

- **Critical path:** The **Agent 1 (Merge)** phase is critical. If aliases are not consolidated (as seen in the single-agent baseline), the graph becomes fragmented, and subsequent Relation/Role agents process duplicate nodes, wasting tokens and skewing the final visualization.

- **Design tradeoffs:**
  - **PPR vs. Frequency Count:** The paper shows PPR is safer for late-intro characters, but simple Edge Counting actually performed slightly better on average in the aggregate (Table 2). The team chose PPR to hedge against "edge case" narratives.
  - **Latency vs. Accuracy:** The sequential chain introduces significant latency (6 stages) compared to a single call, traded for higher precision and F1 scores.

- **Failure signatures:**
  - **Cascading Fragmentation:** Redundant nodes in the final graph (e.g., "E" and "E'") indicate a failure in the *Merging Agent*.
  - **Over-pruning:** Missing key characters indicates the *PPR threshold* (0.02) was likely too aggressive or seeds were incorrect.
  - **Generic Labeling:** Roles labeled as "Doctor" instead of "Cardiothoracic Surgeon" suggest insufficient context in the *Role Agent* prompts.

- **First 3 experiments:**
  1. **Metric Baseline:** Run the single-agent pipeline on 3 dramas to establish the baseline F1/Recall scores mentioned in Table 1.
  2. **Ablation on Context:** Run the Merge Agent *without* the Treatment text to quantify the drop in alias resolution accuracy.
  3. **Selector Tuning:** Vary the PPR threshold (e.g., 0.01, 0.05) to find the optimal balance between precision and recall for different genre types (e.g., rom-com vs. thriller).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive refinement strategies be developed to handle evolving character affiliations and shifting narratives more effectively?
- **Basis in paper:** [explicit] The conclusion states that "challenges persist—particularly in robust group matching for evolving or ambiguous affiliations—highlighting the need for advanced inference techniques."
- **Why unresolved:** The current sequential pipeline processes the narrative in a largely static manner, struggling to capture dynamic changes in relationships that occur as the plot progresses.
- **What evidence would resolve it:** A modification of the CREFT framework that incorporates temporal tracking of relationships, showing improved Group Match F1-scores on narratives with rapidly changing alliances.

### Open Question 2
- **Question:** How does the dependency on user-provided "main" and "sub" character seeds impact the robustness of the Personalized PageRank (PPR) extraction method?
- **Basis in paper:** [inferred] Section 3.2 states that the PPR method begins by collecting "user-provided information about main and sub-characters," assigning them fixed initial importance scores.
- **Why unresolved:** The paper does not evaluate the sensitivity of the character recall (78.5%) to the accuracy or presence of these manual seeds, leaving the scalability of the method uncertain.
- **What evidence would resolve it:** An ablation study testing the PPR performance with noisy or missing user-provided seeds compared to a fully automated initialization.

### Open Question 3
- **Question:** Can a hybrid or adaptive selection mechanism outperform the isolated use of PPR versus node-edge count methods?
- **Basis in paper:** [explicit] Section 5.1 notes that "the effectiveness of each method may depend on specific narrative characteristics," with PPR excelling when key characters appear less frequently early on.
- **Why unresolved:** The current framework requires a fixed choice between algorithms based on the specific drama's pacing structure rather than dynamically adjusting the selection strategy.
- **What evidence would resolve it:** A meta-classifier or heuristic that analyzes narrative structure to automatically select the optimal character selection algorithm, achieving higher average precision and recall than either method alone.

## Limitations

- **Domain Specificity:** Evaluation relies on 15 Korean dramas, limiting generalizability to other languages or narrative forms
- **External Dependency:** Method requires treatment/summary files for alias resolution, creating fragility when such context is unavailable
- **Sequential Latency:** Multi-agent pipeline introduces significant processing latency compared to single-pass approaches

## Confidence

- **High Confidence:** The mechanism of decomposing a complex extraction task into specialized sub-tasks reducing error accumulation compared to monolithic prompting
- **Medium Confidence:** The claim that PPR propagates narrative significance more effectively than frequency counting for late-intro characters, given mixed results in the aggregate
- **Low Confidence:** The 18.8% absolute improvement in character recall and 16.5% increase in group match F1-score are specific to the Korean drama dataset and may not transfer to other domains or languages

## Next Checks

1. **Generalizability Test:** Evaluate CREFT on a multi-genre corpus (e.g., novels, screenplays from different cultures) to assess performance consistency
2. **Ablation on External Context:** Run the Merge Agent without treatment text to quantify the drop in alias resolution accuracy and test robustness
3. **Latency Analysis:** Measure the end-to-end latency of the sequential chain and compare it against a single-pass agent baseline to quantify the accuracy-latency tradeoff