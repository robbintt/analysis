---
ver: rpa2
title: 'MAGIC: Achieving Superior Model Merging via Magnitude Calibration'
arxiv_id: '2512.19320'
source_url: https://arxiv.org/abs/2512.19320
tags:
- task
- magnitude
- feature
- calibration
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model merging in deep learning,
  where the goal is to combine the capabilities of multiple specialized models into
  a single unified model. The authors identify that existing model merging approaches
  primarily focus on aligning the directional components of features, while neglecting
  the magnitude component.
---

# MAGIC: Achieving Superior Model Merging via Magnitude Calibration

## Quick Facts
- arXiv ID: 2512.19320
- Source URL: https://arxiv.org/abs/2512.19320
- Reference count: 40
- Primary result: Achieves +4.3% accuracy on eight CV datasets and +8.0% on Llama tasks

## Executive Summary
This paper addresses a critical gap in model merging approaches by focusing on magnitude calibration alongside directional alignment. The authors identify that existing methods primarily optimize for directional components of features while neglecting magnitude, leading to significant performance degradation when combining specialized models. MAGIC (MAGnItude Calibration) introduces a novel framework that calibrates both feature and weight spaces through three variants: FSC (Feature Space Calibration), WSC (Weight Space Calibration), and DSC (Dual Space Calibration).

The framework demonstrates consistent performance improvements across diverse computer vision and natural language processing tasks, outperforming state-of-the-art merging approaches. By identifying magnitude-sensitive layers and applying layer-wise scaling coefficients, MAGIC effectively realigns merged models with their specialized counterparts, addressing the fundamental issue that merging operations inherently alter feature magnitudes.

## Method Summary
MAGIC addresses model merging by calibrating the magnitude of features and weights in both feature and weight spaces. The framework operates through three variants: FSC calibrates feature magnitudes during forward passes using task-specific data, WSC performs data-free calibration in weight space using a hyperellipsoid constraint, and DSC combines both approaches sequentially. The method identifies magnitude-sensitive layers through layer-wise sensitivity analysis and applies optimal scaling coefficients to realign merged model features with specialized models. The conservative calibration strategy only scales up non-sensitive layers and scales down sensitive layers, preventing degradation in critical components.

## Key Results
- Achieves +4.3% average accuracy improvement across eight computer vision datasets
- Demonstrates +8.0% improvement on Llama tasks for natural language processing
- FSC alone achieves up to +4.9% accuracy gains, validating the importance of feature magnitude calibration
- Consistent performance improvements across different merging scenarios and model architectures

## Why This Works (Mechanism)

### Mechanism 1: Magnitude Deviation from Merging Operations
Merging operations inherently alter task vector magnitudes through parameter fusion and sparsification. The generalized triangle inequality shows averaged task vectors have smaller norms than expected, while weight disentanglement means feature magnitude depends on both task vector norm and alignment with inference data. Merging introduces task-irrelevant vectors that reduce this alignment.

### Mechanism 2: Layer-wise Sensitivity Heterogeneity
Different layers exhibit vastly different sensitivities to magnitude rescaling due to varying Jacobians through downstream layers. Magnitude-sensitive layers in sharp loss landscape regions degrade severely under upward scaling, while other layers can tolerate larger adjustments. This heterogeneity makes uniform global scaling suboptimal.

### Mechanism 3: Calibration via Optimal Scaling Coefficients
Under ideal weight disentanglement, optimal calibration coefficients equal the ratio of specialized to merged feature magnitudes. The conservative strategy applies scaling only when beneficial—scaling up non-sensitive layers while scaling down sensitive layers—ensuring performance improvements without introducing degradation.

## Foundational Learning

- **Concept: Task Vectors**
  - Why needed here: Fundamental unit of manipulation; defined as Δθ_k = θ_k - θ_pre. All calibration operates on these vectors.
  - Quick check question: Given pre-trained weights θ_pre = [1,2] and fine-tuned θ_k = [1.5, 2.5], what is the task vector?

- **Concept: Weight Disentanglement**
  - Why needed here: Explains why weight magnitude ≠ feature magnitude. Critical for understanding when WSC succeeds or fails.
  - Quick check question: If task vector aligns poorly with inference data Jacobian, will feature magnitude increase or decrease?

- **Concept: Hyperellipsoid Constraint**
  - Why needed here: Foundation for WSC data-free calibration. Merged vector constrained to lie on hyperellipsoid with task vectors as axes.
  - Quick check question: Why does the constraint include both the projection term and orthogonal residual term?

## Architecture Onboarding

- **Component map**:
  ```
  MAGIC Framework
  ├── FSC (Feature Space Calibration)
  │   ├── Input: θ_merge, unlabelled data {x_k}
  │   ├── Compute: ξ^f_l = avg(‖Δh^l_k(x_k)‖ / ‖Δh^l_merge(x_k)‖)
  │   └── Output: Calibrated features during forward pass
  ├── WSC (Weight Space Calibration)
  │   ├── Input: θ_pre, task vectors {Δθ^l_k}
  │   ├── Compute: ξ^w_l via hyperellipsoid constraint (Eq. 22)
  │   └── Output: Calibrated task vectors → recomposed weights
  └── DSC (Dual Space Calibration)
      └── Sequential: WSC first → FSC second
  ```

- **Critical path**:
  1. Identify magnitude-sensitive layers using general dataset (CIFAR100/ImageNet) via Eq. 10, 17 (α=10 default)
  2. Compute layer-wise calibration coefficients (FSC needs 1 unlabelled sample/task; WSC is data-free)
  3. Apply conservative calibration: only apply ξ_l when (ξ_l > 1) ⊕ (l ∈ A) is true
  4. For DSC: WSC first modifies weights, then FSC modifies forward pass

- **Design tradeoffs**:
  - FSC vs WSC: FSC is more accurate (accounts for data alignment) but requires 1+ unlabelled samples per task; WSC is data-free but may fail when weight/feature magnitude correlation breaks
  - Sensitive layer threshold α: Higher α = more layers suppressed = safer but potentially under-calibrated. Paper uses α=10 robustly.

- **Failure signatures**:
  - Performance drops when α=0 and sensitive layers are scaled up
  - WSC alone degrades Iso-C/Iso-CTS by ~1% (use DSC instead)
  - Task-agnostic data for FSC (DSC-A) underperforms task-specific samples

- **First 3 experiments**:
  1. Reproduce Table I on 2-3 datasets with existing baseline (TA or TIES) + DSC wrapper; verify +3-5% gains
  2. Ablate α ∈ {0, 5, 10, 20} on held-out task to confirm robustness (expect flat curve with peak near 10)
  3. Compare FSC vs WSC vs DSC on a new backbone (e.g., ViT-B/16) to identify when WSC alone suffices vs when dual calibration is needed

## Open Questions the Paper Calls Out
None

## Limitations
- The weight disentanglement assumption may break down in highly interleaved tasks where task-irrelevant weights contribute significantly to feature magnitude
- The local linearity assumption for first-order approximations may not hold for deep networks with complex loss landscapes
- Empirical evaluation focuses primarily on computer vision and language tasks, leaving cross-modal generalization questions open

## Confidence

- **High Confidence**: Magnitude deviations occur during merging operations (well-supported by Eq. 4 and Fig. 3). Layer-wise sensitivity heterogeneity is convincingly demonstrated through Jacobian analysis (Theorem 1) and empirical results (Fig. 5).

- **Medium Confidence**: The theoretical framework for optimal scaling coefficients (Theorem 2) is mathematically sound, but practical effectiveness depends on weight disentanglement strength. Conservative calibration strategy shows robust performance, but α=10 threshold appears somewhat arbitrary.

- **Low Confidence**: Comparison with state-of-the-art baselines is limited, benchmarking only a few existing methods. Ablation studies for hyperparameter sensitivity (α) are promising but not exhaustive.

## Next Checks

1. **Task Interference Analysis**: Systematically vary task similarity and measure how weight disentanglement breaks down to identify when magnitude calibration provides diminishing returns.

2. **Cross-Modal Generalization**: Apply MAGIC to domains not covered (e.g., speech, medical imaging) to test generality across different data modalities and model architectures.

3. **Long-Tailed Distribution Testing**: Evaluate performance when merged tasks have highly imbalanced data distributions to reveal whether magnitude calibration helps or hinders handling rare classes.