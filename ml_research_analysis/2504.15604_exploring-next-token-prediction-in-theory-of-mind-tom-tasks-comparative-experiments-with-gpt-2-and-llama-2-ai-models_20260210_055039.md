---
ver: rpa2
title: 'Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative
  Experiments with GPT-2 and LLaMA-2 AI Models'
arxiv_id: '2504.15604'
source_url: https://arxiv.org/abs/2504.15604
tags:
- infills
- temp
- room
- storage
- next
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the next-token prediction performance of GPT-2
  and Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. The models were evaluated
  on a dataset created from 10 short stories with varying levels of contextual complexity
  (0, 1, 4, 16, and 64 additional sentences) and three reasoning levels (zero-order,
  first-order, and second-order).
---

# Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models

## Quick Facts
- arXiv ID: 2504.15604
- Source URL: https://arxiv.org/abs/2504.15604
- Reference count: 40
- Primary result: LLaMA-2 consistently outperforms GPT-2 in next-token prediction for ToM tasks, with accuracy declining as contextual complexity increases.

## Executive Summary
This study investigates the next-token prediction capabilities of GPT-2 and LLaMA-2-7b-chat-hf on Theory of Mind tasks using a dataset of 10 short stories with varying contextual complexity. The models are evaluated under four temperature settings across three reasoning levels (zero-order, first-order, and second-order). The findings reveal that increased contextual complexity slightly reduces prediction accuracy due to higher ambiguity, while LLaMA-2 demonstrates superior performance and confidence compared to GPT-2. The study highlights the impact of model architecture, temperature, and reasoning complexity on prediction outcomes, providing insights into the strengths and limitations of current language models in ToM tasks.

## Method Summary
The study employs a comparative experimental design to evaluate next-token prediction in Theory of Mind tasks. A dataset of 10 short stories is constructed, each with varying levels of contextual complexity (0, 1, 4, 16, and 64 additional sentences). The models are tested across three reasoning levels: zero-order (no reasoning), first-order (one level of mental state inference), and second-order (nested mental state inference). Four temperature settings (0.01, 0.5, 1.0, and 2.0) are used to assess the models' sensitivity to randomness in token selection. Prediction accuracy is measured by comparing the models' outputs to ground truth responses, with a focus on how contextual complexity and reasoning levels affect performance.

## Key Results
- LLaMA-2 consistently outperforms GPT-2 in next-token prediction accuracy across all temperature settings.
- Increasing contextual complexity (number of infill sentences) slightly reduces prediction accuracy due to higher ambiguity.
- Higher reasoning complexity (first- and second-order) leads to greater variability in model predictions, with both models showing divergent responses.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Theory of Mind (ToM)**: The ability to attribute mental states (beliefs, desires, intentions) to others and understand that these may differ from one's own. Why needed: ToM is central to the study's focus on modeling human-like reasoning in AI. Quick check: Can the model infer a character's false belief in a story?
- **Next-token prediction**: The task of predicting the most likely next word or token in a sequence. Why needed: This is the core task used to evaluate the models' reasoning capabilities. Quick check: Does the model correctly predict the next token in a sentence?
- **Temperature in language models**: A parameter that controls the randomness of token selection, with lower values favoring more deterministic outputs. Why needed: Temperature affects the models' confidence and variability in predictions. Quick check: How does changing the temperature affect the model's output diversity?
- **Contextual complexity**: The amount of additional information provided in a narrative to influence reasoning. Why needed: Higher complexity increases the challenge of accurate prediction. Quick check: Does adding more context improve or hinder the model's performance?
- **Reasoning levels (zero-order, first-order, second-order)**: Hierarchical levels of mental state inference, from no reasoning to nested beliefs. Why needed: These levels test the models' ability to perform increasingly complex ToM tasks. Quick check: Can the model handle nested beliefs (e.g., "Alice thinks Bob believes...")?

## Architecture Onboarding

**Component Map**
Input Story -> Contextual Complexity Layer -> Reasoning Level Processor -> Temperature Parameter -> Output Token

**Critical Path**
Input Story -> Contextual Complexity Layer -> Reasoning Level Processor -> Output Token

**Design Tradeoffs**
- Model size (1.5B vs. 7B parameters) affects computational efficiency and performance.
- Temperature settings balance determinism (low temp) and creativity (high temp).
- Contextual complexity increases reasoning difficulty but may improve realism.

**Failure Signatures**
- High variability in predictions at higher temperatures indicates overfitting to noise.
- Declining accuracy with increased contextual complexity suggests difficulty in handling ambiguity.
- Divergent responses at higher reasoning levels indicate limitations in nested belief inference.

**3 First Experiments**
1. Compare prediction accuracy of GPT-2 and LLaMA-2 on zero-order reasoning tasks with no contextual complexity.
2. Evaluate the impact of temperature (0.01 vs. 2.0) on prediction confidence and accuracy.
3. Test the models' performance on second-order reasoning tasks with maximum contextual complexity.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability due to the use of only 10 short stories, which may not represent diverse ToM scenarios.
- Temperature parameter interpretation lacks clarity on the relationship between randomness, confidence, and accuracy.
- Model size differences (1.5B vs. 7B parameters) confound the comparison between GPT-2 and LLaMA-2.

## Confidence
- **High confidence**: Increasing contextual complexity reduces prediction accuracy, supported by data and theoretical expectations.
- **Medium confidence**: LLaMA-2 consistently outperforms GPT-2, plausible but requiring further validation with larger datasets.
- **Low confidence**: Higher temperatures yield better results in some cases, based on limited evidence and unclear mechanisms.

## Next Checks
1. Expand the dataset to include a more diverse set of ToM scenarios, such as longer narratives or different cultural contexts, to assess generalizability.
2. Conduct ablation studies to isolate the effects of model size, temperature, and contextual complexity on prediction accuracy.
3. Incorporate human evaluation to validate the plausibility of the models' next-token predictions in the context of the given stories.