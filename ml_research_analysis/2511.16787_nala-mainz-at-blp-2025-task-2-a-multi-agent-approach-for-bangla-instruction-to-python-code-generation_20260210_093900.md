---
ver: rpa2
title: 'NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction
  to Python Code Generation'
arxiv_id: '2511.16787'
source_url: https://arxiv.org/abs/2511.16787
tags:
- code
- test
- tests
- unit
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation

## Quick Facts
- arXiv ID: 2511.16787
- Source URL: https://arxiv.org/abs/2511.16787
- Reference count: 3
- Primary result: 95.4% Pass@1 on hidden tests using multi-agent pipeline with selective debugging

## Executive Summary
This paper presents NALA_MAINZ's approach to the BLP-2025 Shared Task 2, which involves generating Python code from Bangla instructions. The team employs a multi-agent pipeline that first generates code using a low-reasoning LLM, then selectively invokes a high-reasoning "debugger agent" only on failed cases. The debugger is conditioned on concrete error traces and failing test cases, significantly improving code repair capabilities. External unit tests from the MBPP dataset are incorporated to reduce overfitting to the sparse provided tests.

## Method Summary
The method uses a two-stage pipeline: Stage 1 generates Python code from Bangla instructions using GPT-5 with low reasoning effort, executing the code against provided and external unit tests. Failed cases proceed to Stage 2, where a debugger agent (GPT-5 with high reasoning) receives the instruction, failing tests, error trace, and current code to generate a repaired solution. The pipeline achieves selective debugging by only invoking the expensive debugger on approximately 35% of cases that fail initial execution. External unit tests from Austin et al. (2021) are matched to 480/500 test samples to provide stronger test coverage.

## Key Results
- 95.4% Pass@1 on hidden tests using multi-agent pipeline with selective debugging
- 47.67% improvement in Pass@1 for GPT-5 when using debugger agent with error traces
- 9.4% absolute boost from external unit tests versus generated tests alone

## Why This Works (Mechanism)

### Mechanism 1
Conditioning a secondary LLM agent on concrete execution feedback (error traces and failing test cases) significantly improves code repair capabilities compared to generation alone. The "Debugger Agent" receives the specific assertion failure and stack trace, grounding the repair process in actual runtime behavior rather than guessing.

### Mechanism 2
Targeted "Selective Debugging" optimizes inference resources by isolating the repair agent's workload to only failed instances. By splitting the pipeline into code generation and debugging stages, the system executes the expensive debugging prompt only on problems that fail unit tests (approximately 35% for GPT-5), reducing API costs and latency.

### Mechanism 3
Augmenting the prompt with external unit tests improves generalization and reduces overfitting to the sparse provided tests. The task provided only 1-3 tests per problem, but retrieving additional tests from MBPP/Austin et al. gives the debugger stronger signals for edge cases.

## Foundational Learning

- **Pass@1 vs. Pass@k**: The paper optimizes for Pass@1 (getting it right in one shot), measuring functional correctness rather than code similarity. Quick check: Does a higher BLEU score guarantee a higher Pass@1? (Answer: No, functional correctness is distinct from textual similarity).

- **Unit Testing / Pytest**: The entire feedback loop relies on executing `assert` statements. Understanding how Python traces errors (AssertionError vs. SyntaxError) is required to parse the debugger's input. Quick check: What signal does the debugger agent receive if an `assert` fails without a custom message?

- **Execution Sandboxing**: To run generated code safely. The paper implies executing untrusted code to extract error traces. Quick check: Why can't you just ask the LLM if the code is correct without running it? (Answer: LLMs cannot reliably execute code mentally).

## Architecture Onboarding

- **Component map**: Input (Bangla Instruction + Function Signature) -> Stage 1 (Code Generation Agent: LLM -> Code) -> Executor (Sandbox runs Pytest) -> Router (If Pass -> Output; If Fail -> Stage 2) -> Stage 2 (Debugger Agent: LLM + Context -> Repaired Code)

- **Critical path**: The Error Trace Extraction in the Executor. If the sandbox fails to capture the specific traceback or truncates it, the Debugger Agent loses its "conditioning" signal and reverts to guessing.

- **Design tradeoffs**: Model Selection shows GPT-5 vastly outperforms others in debug stage (+47% gain), suggesting debug agent requires stronger model than initial coder. External Dependency on Austin et al. dataset provided ~9.4% absolute boost, creating hard dependency on external data alignment.

- **Failure signatures**: Overfitting shows high local scores (99.8 dev) but lower hidden test scores (95.4), suggesting model learns to satisfy specific tests rather than general instruction. Translation Loss shows Bangla->English translation hurts GPT-5 performance, implying model may leverage non-transferred linguistic nuances.

- **First 3 experiments**:
  1. Establish Baseline: Run Stage 1 agent alone on dev set to determine initial "Pass" rate and identify volume of data requiring Stage 2.
  2. Ablate the Trace: Run Stage 2 but strip the "Error Trace" from prompt to quantify value of traceback versus just showing failing test code.
  3. Test Generalization: Evaluate Stage 2 output on held-out set of "hidden" tests to measure overfitting to visible asserts.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed multi-agent pipeline be effectively replicated using state-of-the-art open-source code models without significant performance degradation? Basis: Authors explicitly list exclusive use of proprietary models as limitation. Unresolved because pipeline relies on specific API features and frontier model capabilities. Evidence would resolve it: Running identical pipeline on BLP-2025 test set using top-tier open models (e.g., Qwen2.5-Coder) and comparing Pass@1 scores.

### Open Question 2
To what extent does the debugger agent "overfit" to provided unit tests rather than learning generalizable program logic? Basis: Authors note drop from 99.2 (local) to 95.4 (hidden tests) and state possibility of overfitting. Unresolved because system optimizes for immediate passage of visible tests. Evidence would resolve it: "Semantic robustness" test with new hidden tests designed for logical edge cases absent from provided suites.

### Open Question 3
How can automated test case generation be improved to close performance gap with human-curated external datasets? Basis: Generated tests yield 84.0% accuracy versus 95.4% for external tests. Unresolved because generated tests failed to provide necessary signal for roughly 10% of problems. Evidence would resolve it: Qualitative analysis comparing coverage and edge-case complexity of generated versus external tests.

### Open Question 4
Does translating Bangla instructions to English impair model's ability to infer implicit type constraints or variable relationships? Basis: Authors observe translation hurts GPT-5 performance and infer "task-specific information is lost." Unresolved because paper only measures aggregate effect of translation. Evidence would resolve it: Error analysis comparing types of failures (type errors vs. logic errors) in translated versus native instruction runs.

## Limitations

- The paper relies exclusively on proprietary models (GPT-5), limiting reproducibility and generalizability of findings
- High performance on dev set (99.8) versus hidden tests (95.4) suggests potential overfitting that isn't fully explored
- Translation from Bangla to English may degrade performance, but underlying cause (translation quality vs. model behavior) remains unclear

## Confidence

- **High Confidence**: The two-stage multi-agent architecture with selective debugging is well-documented and reproducible
- **Medium Confidence**: Reported performance gains (+47% for GPT-5 in debug stage) are credible but may be partially attributable to specific model choice
- **Low Confidence**: Claims about Bangla-to-English translation degradation lack empirical validation within the paper

## Next Checks

1. **Ablation on Error Trace Value**: Implement Stage 2 but systematically remove the error trace from the debugger prompt to quantify how much of the improvement comes from the traceback versus other contextual signals.

2. **External Test Alignment Verification**: Independently verify the mapping between Austin et al. (2021) tests and BLP-2025 samples by implementing the function name matching algorithm and validating that extracted tests are functionally appropriate.

3. **Translation Impact Experiment**: Create controlled experiment comparing GPT-5 performance on Bangla instructions versus their English translations using a small subset of the dev set to empirically test the claimed translation degradation effect.