---
ver: rpa2
title: "Mod\xE8les de Fondation et Ajustement : Vers une Nouvelle G\xE9n\xE9ration\
  \ de Mod\xE8les pour la Pr\xE9vision des S\xE9ries Temporelles"
arxiv_id: '2511.22674'
source_url: https://arxiv.org/abs/2511.22674
tags:
- pour
- vision
- ries
- donn
- temporelles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive review of foundation models
  for time series forecasting and investigates the impact of fine-tuning these models
  on specific datasets. The authors examine three main components: architecture (Transformer-based
  encoder-only, encoder-decoder, and decoder-only models), pretraining strategies
  (self-supervised learning on large datasets), and adaptation through fine-tuning.'
---

# Modèles de Fondation et Ajustement : Vers une Nouvelle Génération de Modèles pour la Prévision des Séries Temporelles

## Quick Facts
- arXiv ID: 2511.22674
- Source URL: https://arxiv.org/abs/2511.22674
- Reference count: 0
- Time series foundation models (Moirai, Chronos, TimesFM) consistently improve forecasting performance when fine-tuned on specific datasets, particularly for long-term horizons and smaller datasets.

## Executive Summary
This paper presents a comprehensive review of foundation models for time series forecasting and investigates the impact of fine-tuning these models on specific datasets. The authors examine three main components: architecture (Transformer-based encoder-only, encoder-decoder, and decoder-only models), pretraining strategies (self-supervised learning on large datasets), and adaptation through fine-tuning. They evaluate three models - Moirai (encoder-only), Chronos (encoder-decoder), and TimesFM (decoder-only) - across 92 unique configurations from the GIFT-Eval benchmark covering 15 univariate and 8 multivariate datasets. Their experiments show that fine-tuning consistently improves zero-shot forecasting performance, particularly for long-term horizons and smaller datasets.

## Method Summary
The study evaluates three pre-trained time series foundation models (Moirai, Chronos, TimesFM) on the GIFT-Eval benchmark. The authors conduct full fine-tuning (FFT) experiments using fixed hyperparameters (learning rates, batch sizes, 4000 gradient iterations) without task-specific optimization. They measure performance using Mean Weighted Quantile Loss (MWQL) and Mean Absolute Percentage Error (MAPE), comparing fine-tuned results against zero-shot baselines. The evaluation spans 23 datasets across multiple domains, with metrics normalized relative to Seasonal Naive baselines.

## Key Results
- Fine-tuning consistently improves zero-shot forecasting performance across all three models
- Improvement is particularly marked on small and medium-sized datasets
- Full fine-tuning can reduce MWQL by up to 20% compared to zero-shot predictions
- Performance gains are more pronounced for long-term forecasting horizons

## Why This Works (Mechanism)

### Mechanism 1: Transfer of Generalizable Temporal Representations
Pretraining on large, diverse corpora allows models to learn universal time series dynamics (trend, seasonality) that transfer to unseen datasets. By segmenting series into patches and applying attention mechanisms, models map raw time-series data to a latent space where structural patterns are disentangled from domain-specific noise. This "universal vocabulary" of temporal dynamics allows the model to initialize effectively on new tasks. Break condition: If the target dataset exhibits regime shifts or non-stationary behaviors completely absent from the pretraining distribution, zero-shot transfer may fail.

### Mechanism 2: Frequency-Dependent Fine-Tuning Efficacy
Full Fine-Tuning (FFT) improves performance primarily by allowing the model to overfit slightly to the specific statistical nuances of smaller target datasets. With a fixed number of gradient iterations, smaller datasets are effectively "oversampled" (seen multiple times), allowing the model to shift its weights from the general pretraining distribution to the specific target distribution. Break condition: If regularization is applied too strongly or learning rates are too low, the model may fail to adapt to the small dataset, remaining stuck in the pretraining weights.

### Mechanism 3: Objective Function Asymmetry (MWQL vs. MAPE)
Optimizing for probabilistic metrics (Quantile Loss/MWQL) during fine-tuning does not guarantee improvement in point-forecast metrics (MAPE), and can degrade them. Quantile loss optimizes the entire distribution (median, tails). MAPE relies solely on the median prediction. A model optimizing Quantile Loss might improve its estimation of extreme values (variance) or skew, effectively "sacrificing" median accuracy to better fit the overall probabilistic profile. Break condition: If the target distribution is Gaussian and symmetric, median and mean optimization often converge, reducing this asymmetry.

## Foundational Learning

- **Patching (Segmentation)**: Time series are continuous and high-frequency. Standard tokenization is inefficient. Patching (grouping time steps) reduces sequence length for the Transformer, lowering the quadratic attention cost and allowing the model to attend to "semantic" time blocks rather than individual points. Quick check question: How does changing the patch size affect the model's ability to capture high-frequency seasonality vs. long-term trends?

- **Autoregressive vs. Masked Modeling**: The paper contrasts Decoder-only (TimesFM, predicting next patch) vs. Encoder-only (Moirai, predicting masked patches). Understanding this distinction is critical for inference latency (autoregressive is slower) and accuracy trade-offs. Quick check question: In which architecture (Encoder vs. Decoder) can you parallelize the generation of the entire forecast horizon?

- **Quantile Regression / CRPS**: The paper emphasizes probabilistic forecasting (MWQL). You must understand that the model predicts a distribution (via quantiles 0.1 to 0.9) rather than a single scalar value, providing uncertainty estimates. Quick check question: If a model predicts the 0.5 quantile perfectly but fails on 0.1 and 0.9, will the MWQL score be high or low?

## Architecture Onboarding

- **Component map**: Input (Multivariate series) -> Variable Index Embedding/Flattening Strategy -> Transformer Backbone (Encoder/Decoder/Enc-Dec) -> Projection Layer (distribution parameters)
- **Critical path**: The Variable Index Embedding (in Moirai) or Flattening Strategy is the critical path for handling multivariate data. If the model cannot distinguish between different sensors/variables in the input, it fails to capture cross-dimensional dependencies.
- **Design tradeoffs**: Chronos (Tokenization) discretizes values into bins (like words). Tradeoff: Good for distribution shapes, but fixed bins limit precision on out-of-range values or strong trends. TimesFM (Decoder) predicts output patch longer than input patch. Tradeoff: Faster inference for long horizons, but potentially accumulates errors compared to non-autoregressive Encoders.
- **Failure signatures**: Zero-Shot Stagnation (High error on small datasets that contradicts general trends), Metric Divergence (MWQL improves but MAPE worsens during training)
- **First 3 experiments**: 1) Baseline Zero-Shot: Evaluate Moirai, Chronos, and TimesFM on GIFT-Eval without modification, 2) Sensitivity Analysis: Run FFT on Transport and Energy domains vs. Econ/Fin to verify domain-specific transferability, 3) Hyperparameter Ablation: Test different learning rates on a small dataset to determine if fixed iteration constraint drives performance gains

## Open Questions the Paper Calls Out

1. **What is the impact of task-specific hyperparameter optimization compared to fixed defaults on the fine-tuning performance of time series foundation models?** The authors explicitly state they did not perform a search for optimal hyperparameters for each task, leaving this study for future work. Ablation studies showing performance variance across different learning rate schedules, iteration counts, and batch sizes for specific datasets would resolve this.

2. **How can fine-tuning strategies be adapted to ensure probabilistic metrics (MWQL) and point metrics (MAPE) improve concurrently?** Section 4.3 notes that while MWQL improves after fine-tuning, MAPE often degrades on large datasets or specific domains. Experiments utilizing multi-objective loss functions or specifically targeting median optimization during fine-tuning would resolve this.

3. **What specific regularization techniques or architectural adjustments are necessary to prevent performance degradation when fine-tuning on domains with high distribution shift, such as Energy?** The authors discuss degraded performance of Chronos in the Energy domain, attributing it to overfitting caused by distribution shifts. A comparative analysis of fine-tuning with specific regularization methods applied specifically to Energy datasets would resolve this.

4. **What theoretical frameworks can best explain the generalization and information compression capabilities of time series foundation models given data heterogeneity?** Section 5 explicitly states these models lack theoretical and empirical studies on generalization and information compression. Theoretical papers establishing generalization bounds for time series transformers would resolve this.

## Limitations

- The paper uses fixed hyperparameters across 92 configurations without task-specific optimization, potentially masking optimal performance levels
- The fixed iteration count of 4000 may lead to overfitting on smaller datasets without monitoring or early stopping
- Potential data contamination from pre-training complicates interpretation, as some test data may have been seen during pretraining

## Confidence

- **High Confidence**: The general claim that fine-tuning improves zero-shot performance across most datasets and horizons is well-supported by experimental results
- **Medium Confidence**: The mechanism explaining why smaller datasets benefit more from fine-tuning (fixed iterations leading to multiple passes) is plausible but requires validation through ablation studies
- **Medium Confidence**: The observation that MAPE can degrade when optimizing for MWQL is clearly documented, but the paper doesn't explore whether this trade-off is inherent or could be mitigated

## Next Checks

1. Run fine-tuning with varying iteration counts (2000, 4000, 8000) on a small dataset to quantify the relationship between dataset size and iteration count, confirming whether the "oversampling" effect drives performance gains

2. Generate calibration plots for quantile predictions on datasets where MAPE degrades, examining whether the model shifts probability mass from the median to tails, validating the proposed mechanism for objective function asymmetry

3. Conduct fine-tuning experiments on datasets from domains not present in the pretraining corpus (LOTSA) to test the hypothesis that pretraining provides truly generalizable temporal representations rather than domain-specific memorization