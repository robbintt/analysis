---
ver: rpa2
title: Efficient numeracy in language models through single-token number embeddings
arxiv_id: '2510.06824'
source_url: https://arxiv.org/abs/2510.06824
tags:
- number
- reasoning
- language
- llms
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models require external tools or lengthy reasoning
  chains to perform even basic arithmetic, which limits their numerical intuition
  and scalability. Existing single-token number embeddings suffer from inefficiencies,
  instability, or poor arithmetic learning.
---

# Efficient numeracy in language models through single-token number embeddings

## Quick Facts
- **arXiv ID**: 2510.06824
- **Source URL**: https://arxiv.org/abs/2510.06824
- **Reference count**: 40
- **Key outcome**: BitTokens encodes numbers as IEEE 754 floating-point bit vectors, enabling efficient, trainable, and arithmetically meaningful representations; achieves near-perfect performance on comparison and single-step calculation tasks, outperforming traditional subword and digit tokenization, as well as previous single-token methods like xVal and FoNE, with log-sMAPE scores exceeding 0.98 on addition and multiplication and exact match accuracy above 0.99 on sorting and interval tasks.

## Executive Summary
Large language models typically rely on external tools or lengthy reasoning chains to perform even basic arithmetic, limiting their numerical intuition and scalability. BitTokens addresses this by encoding numbers as IEEE 754 floating-point bit vectors, enabling efficient, trainable, and arithmetically meaningful single-token representations. This approach allows models to learn arithmetic operations directly, without needing multi-token decompositions or auxiliary tools.

Empirical evaluations show that BitTokens significantly outperforms both traditional subword/digit tokenization and prior single-token number embedding methods across a range of controlled arithmetic tasks. The method achieves near-perfect accuracy on comparison, sorting, and single-step calculation problems, with log-sMAPE scores exceeding 0.98 for addition and multiplication. These results suggest that bit-level numeric representations can greatly enhance language model numeracy, though further validation on more complex, real-world problems is needed.

## Method Summary
BitTokens represents numbers using IEEE 754 floating-point bit vectors, encoding each number as a single token. This representation captures both magnitude and arithmetic relationships inherent in the bit structure, allowing models to learn number properties and operations directly. The method is trainable and designed to be integrated into language models, replacing traditional subword or digit tokenization for numeric inputs. Training is conducted on a range of arithmetic and comparison tasks, with ablation studies showing the importance of bit-level detail over scalar encodings.

## Key Results
- BitTokens achieves log-sMAPE scores exceeding 0.98 on addition and multiplication tasks.
- Exact match accuracy above 0.99 on sorting and interval tasks.
- Outperforms traditional subword and digit tokenization, as well as previous single-token methods like xVal and FoNE, on multi-task arithmetic evaluations.

## Why This Works (Mechanism)
BitTokens leverages the intrinsic arithmetic structure of IEEE 754 floating-point representations, encoding numbers as bit vectors that preserve numerical relationships. This enables language models to learn arithmetic operations directly from the bit-level structure, rather than relying on external tools or multi-token decompositions. The bit-level granularity captures both magnitude and arithmetic semantics, allowing models to generalize to unseen numbers and operations within the tested ranges.

## Foundational Learning
- **IEEE 754 floating-point format**: Standard for representing real numbers in computers; essential for understanding how BitTokens encodes numbers.
  - *Why needed*: BitTokens' effectiveness relies on the arithmetic properties of this format.
  - *Quick check*: Verify that floating-point bit patterns encode both magnitude and sign, enabling arithmetic operations.

- **Single-token number embeddings**: Encodes numbers as single tokens rather than sequences.
  - *Why needed*: Reduces tokenization complexity and enables direct learning of arithmetic.
  - *Quick check*: Confirm that single-token encodings are trainable and improve arithmetic performance.

- **Tokenization in language models**: Process of converting text to model inputs.
  - *Why needed*: BitTokens is a novel tokenization method for numbers.
  - *Quick check*: Understand how traditional subword and digit tokenization handle numbers and their limitations.

- **Log-sMAPE (log symmetric mean absolute percentage error)**: Metric for evaluating numeric prediction accuracy.
  - *Why needed*: Key evaluation metric for arithmetic tasks in the paper.
  - *Quick check*: Ensure log-sMAPE values near 1 indicate near-perfect predictions.

- **Exact match accuracy**: Proportion of predictions that exactly match the target.
  - *Why needed*: Used for tasks like sorting and interval comparisons.
  - *Quick check*: High exact match accuracy indicates strong performance on discrete tasks.

## Architecture Onboarding
- **Component map**: BitTokens (number encoding) -> Language model tokenizer -> Model training -> Arithmetic task evaluation
- **Critical path**: Encoding numbers as IEEE 754 bit vectors, integrating BitTokens into the tokenizer, training on arithmetic tasks, evaluating with log-sMAPE and exact match accuracy
- **Design tradeoffs**: BitTokens trades traditional subword efficiency for improved arithmetic learning; relies on IEEE 754 bit structure, which may not generalize to all numeric domains
- **Failure signatures**: Poor performance on multi-step or out-of-distribution arithmetic; inefficiency on non-numeric text; limited scalability to very large or complex numbers
- **3 first experiments**:
  1. Compare BitTokens vs. subword/digit tokenization on single-step addition/multiplication (log-sMAPE).
  2. Test BitTokens on sorting and interval tasks (exact match accuracy).
  3. Ablation: scalar vs. bit-level encodings for arithmetic learning.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to narrow, single-step arithmetic and comparison problems; scalability to larger numbers, multi-step reasoning, or real-world complexity is untested.
- The analysis does not fully disentangle the relative contributions of bit-level detail versus floating-point semantics versus training task design.
- Comparison with traditional tokenization is limited to fixed small ranges (0-100), which may not reflect broader numeric distributions.
- The claim of "efficiency" is based on token-level metrics, not actual computational cost or memory footprint in deployed models.
- Results depend on specific training protocols and task definitions, which may not transfer to open-ended or noisy numerical contexts.

## Confidence
- **High confidence**: BitTokens achieves near-perfect performance on the specific single-step arithmetic and comparison tasks tested, outperforming baselines in log-sMAPE and exact match metrics.
- **Medium confidence**: BitTokens generalizes to unseen numbers and outperforms previous single-token methods, but this is demonstrated only within constrained numeric ranges and simple operations.
- **Medium confidence**: The ablation results indicate bit-level embeddings are more effective than scalar encodings, though the underlying reasons and robustness across tasks are not fully characterized.

## Next Checks
1. Evaluate BitTokens on multi-step arithmetic problems and larger numeric ranges (e.g., numbers beyond 1,000, floating-point operations) to test scalability and robustness.
2. Compare computational efficiency and memory usage of BitTokens against subword/digit tokenization in full model training and inference, not just token-level metrics.
3. Test BitTokens on real-world datasets with noisy or out-of-distribution numerical values (e.g., financial tables, scientific measurements) to assess practical utility.