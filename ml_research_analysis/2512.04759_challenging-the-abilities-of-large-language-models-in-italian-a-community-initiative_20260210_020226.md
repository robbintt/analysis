---
ver: rpa2
title: 'Challenging the Abilities of Large Language Models in Italian: a Community
  Initiative'
arxiv_id: '2512.04759'
source_url: https://arxiv.org/abs/2512.04759
tags:
- italian
- computational
- linguistics
- llama3
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CALAMITA is a community-driven benchmarking initiative coordinated
  by the Italian Association for Computational Linguistics to evaluate Large Language
  Models (LLMs) on Italian language tasks. The project assembled over 80 contributors
  to design and evaluate a diverse set of 22 challenges with 95 subtasks, covering
  linguistic competence, reasoning, factual knowledge, fairness, summarization, translation,
  and code generation.
---

# Challenging the Abilities of Large Language Models in Italian: a Community Initiative

## Quick Facts
- **arXiv ID:** 2512.04759
- **Source URL:** https://arxiv.org/abs/2512.04759
- **Reference count:** 40
- **Primary result:** CALAMITA assembled over 80 contributors to evaluate four open-weight LLMs across 22 challenges with 95 subtasks, highlighting systematic strengths and weaknesses in Italian language tasks.

## Executive Summary
CALAMITA is a community-driven benchmarking initiative coordinated by the Italian Association for Computational Linguistics to evaluate Large Language Models (LLMs) on Italian language tasks. The project assembled over 80 contributors to design and evaluate a diverse set of 22 challenges with 95 subtasks, covering linguistic competence, reasoning, factual knowledge, fairness, summarization, translation, and code generation. Results were reported for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities. The study emphasizes the importance of fine-grained, task-representative metrics, harmonized evaluation pipelines, and broad community engagement, positioning CALAMITA as a rolling benchmark and a sustainable, inclusive framework for rigorous LLM evaluation in underrepresented languages.

## Method Summary
The CALAMITA initiative brought together over 80 contributors to collaboratively design, assemble, and evaluate a suite of 22 challenges with 95 subtasks targeting diverse Italian language abilities. Evaluation covered linguistic competence, reasoning, factual knowledge, fairness, summarization, translation, and code generation, using four open-weight LLMs. The benchmark employed fine-grained, task-representative metrics and harmonized evaluation pipelines to ensure rigor and comparability. Community-driven task design and inclusive participation were central to the project’s approach, with results highlighting systematic strengths and weaknesses of the tested models.

## Key Results
- CALAMITA evaluated four open-weight LLMs across 22 challenges with 95 subtasks in Italian language abilities.
- Results revealed systematic strengths and weaknesses in linguistic competence, reasoning, factual knowledge, fairness, summarization, translation, and code generation.
- The initiative demonstrated the importance of fine-grained metrics, harmonized pipelines, and community engagement for rigorous LLM evaluation in underrepresented languages.

## Why This Works (Mechanism)
The initiative’s success stems from leveraging broad community expertise to create diverse, representative tasks, ensuring coverage of multiple language abilities. Fine-grained metrics and harmonized evaluation pipelines increase rigor and comparability. The rolling benchmark model supports continuous improvement and sustainability, while inclusive participation fosters engagement and relevance to the target language community.

## Foundational Learning
- **Community-driven task design**: Ensures diverse, representative challenges; needed for comprehensive coverage; quick check: task diversity and coverage metrics.
- **Fine-grained, task-representative metrics**: Improves evaluation precision; needed for nuanced ability assessment; quick check: metric granularity and relevance.
- **Harmonized evaluation pipelines**: Ensures comparability and reproducibility; needed for reliable benchmarking; quick check: pipeline standardization and documentation.
- **Rolling benchmark model**: Supports continuous updates and sustainability; needed for long-term relevance; quick check: update frequency and community involvement.
- **Inclusive participation framework**: Broadens contributor base and perspectives; needed for balanced evaluation; quick check: contributor diversity and engagement metrics.
- **Focus on underrepresented languages**: Addresses evaluation gaps; needed for equitable LLM assessment; quick check: language coverage and task relevance.

## Architecture Onboarding
**Component Map:** Task Design -> Metric Development -> Pipeline Harmonization -> LLM Evaluation -> Result Analysis
**Critical Path:** Task Design → Metric Development → Pipeline Harmonization → LLM Evaluation → Result Analysis
**Design Tradeoffs:** Community-driven diversity vs. standardization; fine-grained metrics vs. scalability; open-weight models vs. proprietary benchmarks.
**Failure Signatures:** Inconsistent task quality, metric misalignment, pipeline errors, evaluation bias, or limited community engagement.
**First Experiments:**
1. Replicate evaluations with newer or proprietary LLM versions.
2. Conduct cross-lingual transfer tests for generalization.
3. Perform inter-annotator agreement studies on fine-grained metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to larger or proprietary LLMs beyond the four open-weight models tested.
- Potential bias from community-driven task design toward specific linguistic or cultural perspectives.
- Heavy focus on Italian may limit insights for multilingual or cross-lingual generalization.

## Confidence
- **Core claims about CALAMITA's structure and community-driven approach:** High
- **Reported LLM performance trends:** Medium
- **Long-term sustainability as a rolling benchmark:** Low

## Next Checks
1. Replicate key evaluations with newer or proprietary LLM versions to assess result stability.
2. Conduct cross-lingual transfer tests to determine if Italian-specific findings generalize.
3. Perform inter-annotator agreement studies on fine-grained metrics to validate scoring reliability.