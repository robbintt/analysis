---
ver: rpa2
title: 'SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks,
  and Mitigation'
arxiv_id: '2506.12699'
source_url: https://arxiv.org/abs/2506.12699
tags:
- privacy
- data
- arxiv
- llms
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This SoK identifies four categories of privacy challenges in LLM\
  \ systems: training data, prompts, outputs, and agents. While prior work focused\
  \ on training data privacy, we highlight underexplored areas\u2014privacy risks\
  \ from user interactions and LLM outputs."
---

# SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation

## Quick Facts
- **arXiv ID:** 2506.12699
- **Source URL:** https://arxiv.org/abs/2506.12699
- **Reference count:** 40
- **Key outcome:** Identifies four categories of privacy challenges in LLM systems: training data, prompts, outputs, and agents; highlights underexplored areas and calls for adaptive frameworks.

## Executive Summary
This systematic literature review (SoK) categorizes privacy risks in Large Language Models (LLMs) across four dimensions: training data, user prompts, generated outputs, and LLM agents. While prior work has focused heavily on training data privacy, this study emphasizes underexplored risks from user interactions and LLM outputs. The authors analyze mitigation strategies like differential privacy and unlearning, noting that many approaches involve trade-offs between privacy protection and model utility. They advocate for unified, adaptive frameworks to address evolving privacy risks unique to LLMs.

## Method Summary
The authors conducted a systematic literature review following PRISMA guidelines, starting from 17,900 initial records across ACM, IEEE Xplore, Springer, ScienceDirect, and Google Scholar. Through snowballing and strict exclusion criteria (e.g., papers before 2022, non-A/A*/B venues), they selected 116 relevant papers. The categorization framework identifies privacy risks in four areas and maps mitigation strategies to these categories, creating a comprehensive landscape analysis of LLM privacy challenges and defenses.

## Key Results
- Privacy risks extend beyond training data to include prompt-based inference, output generation, and agentic tool interactions
- Differential privacy and unlearning provide theoretical guarantees but often incur significant utility costs
- Agent-based systems introduce novel privacy vulnerabilities through autonomous data propagation to external tools
- Current mitigation strategies are nascent and face fundamental trade-offs between privacy protection and model performance

## Why This Works (Mechanism)

### Mechanism 1: Contextual Inference via Advanced NLU
LLMs can infer sensitive personal attributes from user prompts even when explicit PII is absent or redacted. The model leverages advanced natural language understanding to correlate seemingly innocuous contextual details (e.g., location-specific slang like "hook turn") to infer protected attributes (e.g., geography). This works because the adversary has sufficient prior world knowledge to map low-sensitivity inputs to high-sensitivity attributes. The break condition occurs when prompts contain strictly generic information with no statistical correlation to sensitive attributes in the model's training distribution.

### Mechanism 2: Privacy-Utility Trade-off in Differential Privacy (DP)
Applying Differential Privacy during LLM training or fine-tuning reduces data extraction attack risks but often degrades model performance on specific downstream tasks. DP mechanisms add calibrated noise to gradients or outputs during training, theoretically limiting any single data point's contribution. However, this noise can obscure subtle linguistic patterns required for high-precision generation. The break condition occurs if model architecture or training curriculum is modified to be inherently robust to noise, or if utility metrics are loosely defined.

### Mechanism 3: Agentic Data Propagation
LLM agents amplify privacy risks by autonomously propagating sensitive user data to external third-party APIs without explicit user awareness. A main agent decomposes user prompts into sub-tasks and passes necessary context to secondary agents or external tools. If context includes sensitive data from the prompt or memory, it leaks outside the user's trusted environment. The break condition occurs if strict access controls (like AirGapAgent) are enforced to isolate private data from tool-calling logic.

## Foundational Learning

- **Concept:** **Differential Privacy (DP)**
  - **Why needed here:** Cited as primary defense against training data memorization, but utility costs are significant
  - **Quick check question:** If you increase noise added to gradients during training, does the privacy guarantee strengthen or weaken, and what happens to model accuracy?

- **Concept:** **In-Context Learning (ICL)**
  - **Why needed here:** Identified as vector for privacy leakage in outputs where sensitive examples might be memorized or regurgitated
  - **Quick check question:** In ICL, does the model update its weights based on prompt examples, or process them solely during forward pass?

- **Concept:** **Named Entity Recognition (NER)**
  - **Why needed here:** Discussed as common sanitization technique but often insufficient against inference attacks
  - **Quick check question:** Can a standard NER system detect "I always get stuck waiting for a hook turn" as a privacy risk (location inference)?

## Architecture Onboarding

- **Component map:** User Prompt -> LLM Core -> Agent Orchestrator -> External Tools
- **Critical path:** The flow of sensitive data from User Prompt -> LLM Context -> Agent Tool Arguments
- **Design tradeoffs:**
  - Sanitization vs. Utility: Aggressive redaction preserves privacy but breaks semantic understanding
  - Local vs. Remote: Small local model for sanitization protects data but increases client-side compute
  - Robustness vs. Cost: Multi-agent debate reduces adversarial behavior but increases latency and costs
- **Failure signatures:**
  - Regurgitation: Model outputs verbatim text from prompt or training data
  - Inference Leakage: Model correctly answers questions about user attributes never explicitly stated
  - Unintended Tool Calls: Agent passes raw API key or password to third-party tool as parameter
- **First 3 experiments:**
  1. Prompt Inference Test: Feed model redacted prompts (via NER) and attempt to infer user's location or age to test sanitization limits
  2. Membership Inference Attack (MIA): Fine-tune small model on known dataset, then query to distinguish included vs. holdout data points
  3. Agent Tool Leakage Audit: Deploy simple LLM agent with dummy API, prompt with sensitive task, and monitor payload sent to verify credential exfiltration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can differential privacy and deduplication techniques be effectively scaled to LLMs trained on trillions of tokens while maintaining utility?
- Basis in paper: Most approaches are applied to smaller text corpora; scaling to LLM-scale datasets presents challenges
- Why unresolved: Current empirical evaluations are limited in scale; existing techniques incur prohibitive computational costs or severe utility degradation
- What evidence would resolve it: Empirical benchmarks measuring privacy-utility trade-offs when applying DP and deduplication to models trained on datasets exceeding 100B tokens

### Open Question 2
- Question: Where should adversarial inference detection modules be deployed—client-side or service-provider-side—to optimally balance privacy protection against prompt-based attribute inference with response utility?
- Basis in paper: Adversarial feedback-guided anonymization doesn't fully address where the LLM adversary should be located
- Why unresolved: Client-side deployment may lack computational resources; server-side deployment reintroduces trust assumptions
- What evidence would resolve it: Comparative studies quantifying detection accuracy, latency, and utility loss for both deployment configurations

### Open Question 3
- Question: Can adaptive, context-aware privacy mechanisms dynamically adjust obfuscation or filtering levels in real-time based on inferred privacy risk, without disrupting conversational fluency?
- Basis in paper: Authors call for developing adaptive privacy mechanisms that dynamically assess conversation context
- Why unresolved: Real-time risk assessment requires lightweight inference; aggressive dynamic filtering may degrade user experience
- What evidence would resolve it: Prototypes demonstrating adaptive filtering with measurable thresholds, evaluated on privacy leakage reduction and user satisfaction

### Open Question 4
- Question: How can fully homomorphic encryption (FHE) or secure multi-party computation be optimized to support inference for billion-parameter LLMs within practical latency bounds?
- Basis in paper: FHE has been applied to smaller models; scaling to larger models remains challenging due to computational complexity
- Why unresolved: Current FHE implementations impose latency orders of magnitude higher than plaintext inference
- What evidence would resolve it: System-level optimizations achieving sub-second inference latencies for 1B+ parameter models under FHE

## Limitations

- Reliance on published literature may systematically underrepresent negative results or emerging privacy attacks not yet peer-reviewed
- Categorization framework may oversimplify complex interdependencies between privacy risks
- Utility-privacy trade-offs discussed are largely theoretical with limited empirical quantification of real-world impact

## Confidence

- **High Confidence:** Taxonomy of four privacy dimensions and identification of differential privacy as core mitigation strategy
- **Medium Confidence:** Claims about semantic privacy inference and agentic data propagation are plausible but lack extensive empirical validation
- **Low Confidence:** Effectiveness of specific mitigation strategies (e.g., AirGapAgent, multi-agent debate) presented without detailed performance metrics

## Next Checks

1. **Empirical Trade-off Analysis:** Conduct controlled experiments measuring accuracy degradation when applying differential privacy to models of varying sizes (7B, 70B parameters) across multiple downstream tasks

2. **Semantic Inference Benchmark:** Design dataset of prompts with carefully controlled contextual clues and test whether fine-tuned models can reliably infer sensitive attributes not explicitly stated

3. **Agent Security Audit:** Implement minimal agent system with tool-calling capabilities and systematically test whether it leaks sensitive information from prompts to external APIs under various privacy-preserving configurations<|end_of_text|><|begin_of_text|><|begin_of_text|>