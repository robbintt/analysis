---
ver: rpa2
title: Pretraining Scaling Laws for Generative Evaluations of Language Models
arxiv_id: '2509.24012'
source_url: https://arxiv.org/abs/2509.24012
tags:
- scaling
- wang
- compute
- chen
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies pretraining scaling laws for generative evaluations
  using pass-at-k metrics on math benchmarks. The authors fit and predict performance
  using three scaling laws based on pretraining compute, model parameters plus tokens,
  and gold reference likelihoods.
---

# Pretraining Scaling Laws for Generative Evaluations of Language Models

## Quick Facts
- arXiv ID: 2509.24012
- Source URL: https://arxiv.org/abs/2509.24012
- Reference count: 40
- This paper studies pretraining scaling laws for generative evaluations using pass-at-k metrics on math benchmarks.

## Executive Summary
This paper presents a comprehensive study of pretraining scaling laws for generative language models evaluated on math benchmarks. The authors introduce and compare three scaling laws based on pretraining compute, model parameters plus tokens, and gold reference likelihoods, finding that all perform comparably in prediction accuracy. They demonstrate that increasing the number of attempts per problem (k) reduces irreducible error and steepens scaling exponents, with generative evaluations introducing new hyperparameters that control scaling behavior. The study provides theoretical insights into compute-optimal scaling for generative tasks and proves that the compute scaling law is the compute-optimal envelope of the parameters-and-tokens scaling law.

## Method Summary
The authors conduct controlled pretraining runs across a range of compute budgets, training models from scratch on math datasets. They evaluate performance using pass-at-k metrics on GSM8K and MATH benchmarks, where models generate multiple solutions and are scored based on whether any attempt passes. Three scaling laws are fitted to the data: compute-based, parameters-and-tokens-based, and gold reference likelihood-based. The study systematically varies k (number of attempts) to analyze its impact on scaling behavior and irreducible error. Theoretical analysis establishes relationships between the scaling laws, including proving that compute scaling law represents the optimal allocation envelope.

## Key Results
- All three scaling laws (compute, parameters-and-tokens, gold reference likelihood) perform comparably in prediction accuracy across ~5 orders of magnitude of compute
- Increasing k reduces irreducible error and steepens scaling exponents, with GSM8K reaching near-zero irreducible error at k≈100 while MATH retains significant error even at k=10,000
- The compute scaling law is theoretically proven to be the compute-optimal envelope of the parameters-and-tokens scaling law, with quantifiable misallocation penalties for suboptimal scaling

## Why This Works (Mechanism)
The mechanism underlying the scaling laws relates to how generative models leverage multiple attempts to overcome the stochastic nature of sampling and the difficulty of individual problems. As k increases, the probability that at least one attempt succeeds increases, effectively reducing the irreducible error floor. The gold reference likelihood scaling law captures this by measuring the probability that any of k samples has higher likelihood than the reference solution, providing a theoretically grounded metric that accounts for the multi-attempt evaluation setting. The steepening of scaling exponents with higher k reflects the fact that generative models can more efficiently explore the solution space when given multiple chances, making additional compute more effective at pushing performance toward the theoretical limits.

## Foundational Learning

**Scaling Laws**
*Why needed:* Understanding how model performance scales with compute, parameters, and data is crucial for efficient resource allocation in large-scale training.
*Quick check:* Verify that performance follows predictable power-law relationships with resource increases.

**Gold Reference Likelihoods**
*Why needed:* Provides a theoretically grounded metric for evaluating generative models that accounts for the multi-attempt nature of pass-at-k evaluations.
*Quick check:* Confirm that likelihood-based metrics correlate well with actual pass rates across different k values.

**Irreducible Error**
*Why needed:* Quantifies the fundamental limitations of model performance even with infinite resources, helping distinguish between optimization and expressivity constraints.
*Quick check:* Measure performance saturation points at very high k values to estimate irreducible error floors.

## Architecture Onboarding

**Component Map**
Data → Model (parameters, tokens) → Compute → Scaling Law (Compute/Params&Tokens/Gold Reference) → Performance (Pass@k)

**Critical Path**
The critical path for understanding scaling behavior is: Pretraining Compute → Model Capacity → Training Dynamics → Evaluation Performance. The study systematically varies compute while controlling other factors to isolate the relationship between resources and outcomes.

**Design Tradeoffs**
The main tradeoff is between model size and training tokens for a fixed compute budget. The study shows that optimal allocation depends on the specific scaling regime and evaluation metric (k value). Higher k values favor parameter-heavy scaling, while lower k values may benefit more from additional training data.

**Failure Signatures**
Scaling law predictions break down when: (1) training data becomes insufficient quality or quantity relative to model size, (2) numerical precision limits are reached, (3) the evaluation metric (k) is too low to capture the model's true capabilities, or (4) the task domain differs significantly from the training distribution.

**First 3 Experiments**
1. Vary k systematically from 1 to 10,000 to map the relationship between attempt count and irreducible error reduction
2. Train models with intentionally suboptimal parameter-token allocations to measure the practical misallocation penalty
3. Apply the three scaling laws to a non-math domain to test generalizability across task types

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies on controlled pretraining runs within limited model family and domain (math benchmarks), constraining generalizability
- Assumes gold reference likelihoods can be accurately computed or approximated, which may not hold in complex domains
- Synthetic pass-at-k metrics may not fully capture real-world usage patterns of generative models
- Theoretical results assume idealized scaling behavior that may break down near practical constraints

## Confidence
High confidence: Comparative performance of the three scaling laws and their prediction accuracy; relationship between increasing k and reduced irreducible error.
Medium confidence: Theoretical derivation of compute scaling law as envelope of parameters-and-tokens scaling law; parameter stability claims across 5 orders of magnitude.
Low confidence: Extrapolation of scaling behavior to extreme compute regimes, particularly for MATH benchmark with persistent irreducible error.

## Next Checks
1. Validate scaling laws on non-math domains (code generation, commonsense reasoning) to test domain transferability and identify domain-specific scaling behaviors.
2. Conduct ablation studies on training distribution and data quality variations to understand their impact on scaling exponents and irreducible error estimates.
3. Test theoretical compute-optimal allocation predictions with actual pretraining runs that deliberately misallocate compute between parameters and tokens to measure real-world misallocation penalty.