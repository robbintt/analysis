---
ver: rpa2
title: 'Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning
  with LLMs'
arxiv_id: '2509.24377'
source_url: https://arxiv.org/abs/2509.24377
tags:
- reasoning
- strategy
- routing
- across
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PRISM introduces a problem-aware strategy routing framework that
  dynamically selects the optimal mathematical reasoning approach for each problem
  instance. The method trains a lightweight Strategy Adapter on a curated multi-strategy
  preference dataset to predict the relative effectiveness of four reasoning strategies:
  natural language reasoning, code-augmented reasoning, tool-integrated reasoning,
  and ensemble-based reasoning.'
---

# Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs

## Quick Facts
- arXiv ID: 2509.24377
- Source URL: https://arxiv.org/abs/2509.24377
- Reference count: 21
- Primary result: PRISM achieves 53.2% accuracy on MATH benchmark, outperforming best single-strategy baseline by 3.1% absolute

## Executive Summary
PRISM introduces a problem-aware strategy routing framework that dynamically selects the optimal mathematical reasoning approach for each problem instance. The method trains a lightweight Strategy Adapter on a curated multi-strategy preference dataset to predict the relative effectiveness of four reasoning strategies: natural language reasoning, code-augmented reasoning, tool-integrated reasoning, and ensemble-based reasoning. At inference time, PRISM uses confidence-based adaptive routing to execute single-strategy, dual-strategy verification, or comprehensive multi-strategy exploration depending on prediction certainty. Evaluated across five mathematical reasoning benchmarks with three base models, PRISM achieves 53.2% accuracy on the challenging MATH benchmark, outperforming the best single-strategy baseline (TIR) by 3.1% absolute and the ensemble baseline (EBR) by 2.5% absolute.

## Method Summary
PRISM builds a multi-strategy preference dataset (MathStrat) by executing four reasoning strategies (NLR, CAR, TIR, EBR) on mathematical problems and collecting multi-faceted scores (correctness, process quality, efficiency). A lightweight Strategy Adapter (~1.5B params) is trained to predict strategy suitability using KL divergence loss against soft targets derived from these scores. At inference, the adapter outputs strategy probabilities that are routed through a confidence-based policy: confident predictions use single strategies, competitive predictions trigger dual-path verification, and uncertain predictions explore all strategies. The approach demonstrates consistent improvements ranging from 0.9% to 7.6% across different models and datasets.

## Key Results
- Achieves 53.2% accuracy on MATH benchmark, surpassing TIR baseline by 3.1% absolute
- Outperforms ensemble baseline EBR by 2.5% absolute accuracy on MATH
- Demonstrates consistent improvements of 0.9%-7.6% across different models and datasets
- Shows effective routing with 75.8% confident predictions, 21.8% deliberative, and 2.4% exploratory on MATH500

## Why This Works (Mechanism)

### Mechanism 1: Strategy Suitability Prediction via Distributional Learning
The lightweight adapter predicts which reasoning strategy best suits a given problem instance by learning from soft supervision targets derived from multi-faceted performance scores. The training objective combines KL divergence for distribution matching with cross-entropy for top-strategy ranking, enabling the model to capture nuanced strategy preferences rather than binary success/failure labels.

### Mechanism 2: Multi-Faceted Preference Signal Aggregation
Strategy effectiveness is evaluated across three dimensions—correctness, process quality, and computational efficiency—which are aggregated into a unified suitability score using fixed weights. This comprehensive evaluation provides richer supervision than binary success labels, capturing the trade-offs between accuracy, reasoning quality, and computational cost.

### Mechanism 3: Confidence-Calibrated Adaptive Routing
The routing policy dynamically allocates computational resources based on prediction confidence. High-confidence predictions trigger streamlined single-path execution, while competitive scores between strategies invoke dual-path verification, and low-confidence cases explore all strategies comprehensively. This principled approach balances accuracy gains against computational overhead.

## Foundational Learning

- **Reasoning Strategy Paradigms (NLR/CAR/TIR/EBR)**: PRISM routes between four fundamentally different approaches—understanding their trade-offs (precision vs. interpretability, computational cost vs. robustness) is essential for diagnosing routing failures. *Quick check*: Can you explain why code-augmented reasoning (CAR) might fail on problems requiring symbolic manipulation but succeed on numerical computation?

- **Distribution Matching via KL Divergence**: The Strategy Adapter learns via KL divergence against soft targets; understanding why soft targets preserve ranking information better than hard labels is critical for debugging training dynamics. *Quick check*: If the adapter outputs uniform distributions across strategies on held-out data, what does this suggest about the training signal?

- **Confidence Calibration in Neural Networks**: The routing policy assumes prediction confidence reflects correctness probability; miscalibration would cause systematic routing errors. *Quick check*: What techniques could detect if the Strategy Adapter is overconfident on out-of-distribution problem types?

## Architecture Onboarding

- **Component map**: Problem → Strategy Adapter → [pmax, p2nd] → Routing Decision → Strategy Execution(s) → Answer Aggregation

- **Critical path**: The Strategy Adapter processes problem text to predict strategy probabilities, which the routing policy uses to select execution mode, then executes chosen strategies and aggregates their outputs.

- **Design tradeoffs**: Lightweight adapter (1.5B) vs. larger predictor for faster inference but potentially lower routing accuracy; fixed scoring weights vs. learned weights for simplicity but potential miscalibration; three routing modes vs. continuous scaling for interpretability but possible missed intermediate optima.

- **Failure signatures**: Overconfident routing to wrong strategy on specific problem subtypes; excessive exploratory routing causing computational overhead; low strategy diversity in routing decisions suggesting adapter collapse.

- **First 3 experiments**: 1) Verify Strategy Adapter achieves >random accuracy on held-out MathStrat validation split for top-1 strategy prediction; 2) Log routing mode distribution on validation set and adjust τc if >80% confident routing with poor downstream accuracy; 3) Compute PRISM accuracy contribution by routed strategy to identify systematically misrouted strategies.

## Open Questions the Paper Calls Out

### Open Question 1
Can the PRISM framework generalize to non-mathematical reasoning domains, such as logical deduction or code generation, without extensive re-engineering of the strategy definitions? The paper restricts evaluation to five mathematical benchmarks and math-centric strategies. It's unclear if process quality and efficiency metrics transfer effectively to domains where reasoning steps are less structured than mathematical proofs.

### Open Question 2
How sensitive is the optimal routing policy to the fixed weighting hyperparameters (w_C, w_Q, w_U) used in the multi-faceted scoring function? The paper demonstrates a single operating point but doesn't analyze how varying these weights impacts the accuracy-vs-latency trade-off across different user constraints.

### Open Question 3
Can the Strategy Adapter effectively support the integration of novel reasoning strategies that were not present in the MathStrat training data? The framework relies on a closed set of four strategies for both training and inference. The model's utility is limited if it cannot accommodate new strategies without retraining on expanded preference data.

## Limitations
- Dataset generalization gap: MathStrat construction depends heavily on specific base models and problem distributions, potentially limiting adapter generalization
- Scoring weight calibration: Fixed weights for aggregating correctness, quality, and efficiency are not learned or validated, risking systematic misalignment
- Confidence calibration assumption: Routing policy relies on prediction confidence correlating with correctness probability, which may fail on out-of-distribution problems

## Confidence

- **High Confidence**: Architectural design is sound and improvements over single-strategy baselines are substantial and consistent
- **Medium Confidence**: Reported accuracy gains are strong but hinge on dataset construction quality and generalization to unseen problems
- **Low Confidence**: Specific hyperparameter values and process quality evaluator implementation are not specified, limiting reproducibility

## Next Checks

1. **Calibration Analysis**: Compute Expected Calibration Error (ECE) and reliability diagrams for the Strategy Adapter on validation and test sets to verify routing policy assumptions.

2. **Weight Sensitivity Ablation**: Perform ablation study varying (w_C, w_Q, w_U) across multiple combinations to identify if fixed weights are near-optimal.

3. **Distribution Shift Robustness**: Test PRISM on held-out problems from different sources (e.g., Olympiad problems) to quantify generalization limits and routing mode changes.