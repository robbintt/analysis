---
ver: rpa2
title: 'DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification
  under Domain Shift'
arxiv_id: '2510.17345'
source_url: https://arxiv.org/abs/2510.17345
tags:
- training
- ddsc
- acoustic
- curriculum
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses domain shift in acoustic scene classification
  (ASC) caused by recording device variation, which particularly impacts performance
  under limited labeled data. Existing curriculum learning methods use static data
  ordering or weighting strategies that do not adapt to changing example difficulty
  during training.
---

# DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift

## Quick Facts
- **arXiv ID:** 2510.17345
- **Source URL:** https://arxiv.org/abs/2510.17345
- **Reference count:** 0
- **Primary result:** DDSC improves cross-device acoustic scene classification under domain shift, especially at low label budgets (4.2% average accuracy gain at 5% budget).

## Executive Summary
This paper tackles the challenge of domain shift in acoustic scene classification (ASC) caused by recording device variation, particularly problematic under limited labeled data. The authors propose Dynamic Dual-Signal Curriculum (DDSC), which computes adaptive domain-invariance and learning-progress scores each epoch to prioritize training samples. Evaluated on DCASE 2024 Task 1 across various label budgets, DDSC consistently improves cross-device performance and is lightweight, architecture-agnostic, and inference-free.

## Method Summary
DDSC addresses domain shift in ASC by dynamically ordering training samples based on two adaptive signals computed each epoch. The domain-invariance score uses prototype entropy from device prototypes, while the learning-progress score tracks smoothed loss changes. A time-varying scheduler fuses these signals to generate per-example weights, prioritizing domain-invariant samples early and gradually emphasizing more difficult, device-specific cases. The method is evaluated on DCASE 2024 Task 1 across multiple label budgets and four diverse ASC baselines, showing consistent improvements particularly under low-resource conditions.

## Key Results
- DDSC achieves 4.2% average accuracy improvement at 5% label budget
- 3.9% better unseen-device accuracy compared to baselines
- Largest gains occur under low-resource conditions across four diverse ASC baselines

## Why This Works (Mechanism)
The method works by dynamically adapting sample difficulty during training through two complementary signals. Domain-invariance scoring via prototype entropy identifies samples that generalize across devices early in training, while learning-progress tracking ensures the curriculum adapts to the model's evolving capabilities. The time-varying scheduler balances these signals, allowing the model to first learn robust, device-agnostic features before tackling device-specific variations.

## Foundational Learning
- **Curriculum Learning**: Gradual difficulty ordering of training samples; needed to handle domain shift systematically; quick check: compare static vs dynamic ordering
- **Domain Generalization**: Model performance across different recording devices; needed for real-world ASC deployment; quick check: test on unseen devices
- **Prototype-based Representation**: Using cluster centroids to represent device characteristics; needed for domain-invariance scoring; quick check: evaluate prototype quality
- **Entropy-based Scoring**: Measuring uncertainty in prototype assignments; needed to quantify domain-invariance; quick check: correlate entropy with cross-device performance
- **Loss Smoothing**: Tracking training progress over time; needed for adaptive curriculum adjustment; quick check: compare smoothed vs raw loss signals
- **Time-varying Scheduling**: Dynamic weighting adjustment during training; needed to balance exploration/exploitation; quick check: ablate scheduler components

## Architecture Onboarding

**Component Map:** Input Data -> Feature Extraction -> Domain-Invariance Scoring -> Learning-Progress Tracking -> Time-varying Scheduler -> Weighted Sampling -> Model Training

**Critical Path:** The core innovation lies in the dual-signal computation and fusion mechanism. Domain-invariance scoring via prototype entropy runs in parallel with learning-progress tracking, both feeding into the time-varying scheduler that generates sample weights. This weighted sampling then guides the standard model training process.

**Design Tradeoffs:** The method trades minimal computational overhead during training for significant generalization improvements. The dual-signal approach adds complexity but provides more robust curriculum adaptation than single-signal methods. The prototype-based domain scoring may become less effective with many device types.

**Failure Signatures:** Performance degradation occurs when device prototypes overlap significantly, making entropy-based scoring unreliable. The method may also struggle when domain shift is minimal or when the dataset contains insufficient device diversity to learn meaningful prototypes.

**First Experiments:** 1) Ablate domain-invariance signal to test its contribution; 2) Test with static vs dynamic scheduling; 3) Evaluate on datasets with varying numbers of device types.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on prototype entropy may become less effective with many device types or overlapping prototypes
- Performance improvements primarily evaluated on DCASE 2024 Task 1, raising generalizability questions
- Scalability concerns for datasets with 10+ recording devices

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Cross-device generalization improvements | Medium |
| Data-efficiency benefits across label budgets | High |
| Architecture-agnostic and inference-free design | High |

## Next Checks
1. Test DDSC on additional ASC datasets (e.g., DCASE 2023, UrbanSound8K) to verify cross-dataset generalization.
2. Evaluate performance when scaling to datasets with 10+ recording devices to assess prototype entropy robustness.
3. Conduct ablation studies isolating the contributions of domain-invariance versus learning-progress signals under varying domain shift conditions.