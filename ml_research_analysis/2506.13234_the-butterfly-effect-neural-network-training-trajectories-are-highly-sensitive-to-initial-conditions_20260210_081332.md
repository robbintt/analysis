---
ver: rpa2
title: 'The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive
  to Initial Conditions'
arxiv_id: '2506.13234'
source_url: https://arxiv.org/abs/2506.13234
tags:
- training
- barriers
- batch
- figure
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sensitivity of neural network training
  trajectories to initial conditions by systematically perturbing networks at different
  training stages. The authors demonstrate a "butterfly effect" where even single-weight
  perturbations early in training reliably cause networks to diverge into distinct
  loss basins, as measured by training loss barriers.
---

# The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive to Initial Conditions

## Quick Facts
- arXiv ID: 2506.13234
- Source URL: https://arxiv.org/abs/2506.13234
- Authors: Devin Kwok; Gül Sena Altıntaş; Colin Raffel; David Rolnick
- Reference count: 40
- One-line primary result: Early training perturbations cause neural networks to diverge into distinct loss basins, while late perturbations have minimal effect.

## Executive Summary
This paper investigates how sensitive neural network training trajectories are to initial conditions by systematically perturbing networks at different training stages. The authors demonstrate a "butterfly effect" where even single-weight perturbations early in training reliably cause networks to diverge into distinct loss basins, as measured by training loss barriers. They quantify this divergence using L2 distance between parameters, loss barriers, barriers after permutation alignment, and representational similarity via Angular CKA. Their findings reveal that stability increases rapidly during early training, with later perturbations causing minimal divergence even at large scales.

## Method Summary
The method involves training a base model to time t, then creating a copy and applying a small perturbation (Gaussian or batch-direction) scaled relative to initialization variance. Both networks continue training with identical seeds and data order. The key measurement is the loss barrier - the maximum increase in loss along the linear interpolation between final weights. The authors also employ permutation alignment to distinguish functional divergence from weight-space permutations, and use Angular CKA to measure representational similarity. Experiments span vision tasks (CIFAR-10/100 with ResNet/ViT) and language tasks (GLUE with MultiBERT, GSM8K with OLMo-1B).

## Key Results
- Early perturbations (t < 2% of training) at σ ≥ 10^-4 cause reliable divergence with non-zero loss barriers, while late perturbations have minimal effect
- Divergence reflects landing in distinct convex loss basins, not merely weight-space permutations, as permutation alignment fails to reduce barriers
- Fine-tuning stability depends heavily on pre-training task and duration, with some language model experiments showing reduced stability after extended pre-training

## Why This Works (Mechanism)

### Mechanism 1
Neural network training exhibits an early "chaotic" phase where arbitrarily small perturbations cause trajectory divergence, followed by rapid stabilization. The loss landscape near initialization contains many diverging gradient flow paths. As training progresses, the optimizer moves into regions where the training map T^t becomes more contractive—perturbations are dampened rather than amplified. The paper measures this via L2 divergence and loss barriers, finding both diminish rapidly after the first 0.5-2% of training.

### Mechanism 2
Divergence due to early perturbations reflects landing in distinct convex loss basins, not merely weight-space permutations. Two identically-initialized networks diverge when perturbed early, and permutation alignment (weight/activation matching) fails to reduce loss barriers between them. This indicates functional divergence in the learned solution, not just re-indexing of equivalent neurons.

### Mechanism 3
Pre-training improves fine-tuning stability for vision models, but extended pre-training of language models can reduce stability on some downstream tasks. Pre-training moves the initial point into a more stable region of parameter space. However, for language models, extended pre-training may cause "catastrophic overfitting" to the pre-training distribution, making the model brittle to perturbations during fine-tuning.

## Foundational Learning

- **Concept: Linear Mode Connectivity (LMC) and Loss Barriers**
  - Why needed here: The paper's central measurement—barriers—quantifies whether two networks are in the same convex loss basin. Without this, "divergence" is ambiguous (weight distance vs. functional difference).
  - Quick check question: Given two trained networks θ_A and θ_B, what does a non-zero barrier on the linear interpolation αθ_A + (1-α)θ_B indicate about their relationship?

- **Concept: Dynamical Systems and Lyapunov Exponents**
  - Why needed here: The paper frames training as a deterministic map and tests for chaotic sensitivity. Understanding exponential divergence rates helps interpret why barriers don't grow exponentially despite early chaos.
  - Quick check question: In a linear dynamical system, what does a positive Lyapunov exponent imply about perturbation growth, and how does the paper's findings differ?

- **Concept: Permutation Symmetry in Neural Networks**
  - Why needed here: Networks have equivalent neurons that can be re-indexed without changing function. The paper tests whether divergence is "real" or just permutation, making this concept essential.
  - Quick check question: Why might two networks with identical initialization diverge into permuted vs. functionally different solutions, and how would you distinguish these cases?

## Architecture Onboarding

- **Component map:** Training procedure T -> spawn-and-perturb module (copies θ_t, adds ε) -> barrier computation (linear interpolation) -> permutation alignment (weight/activation matching) -> representational similarity (Angular CKA)
- **Critical path:** Perturbation timing and scale are the most sensitive levers. Early perturbations (t < 2% of training) at σ ≥ 10^-4 of initialization scale reliably produce barriers. The measurement pipeline (barriers → permutation alignment → CKA) must be run in sequence to distinguish functional vs. permutation divergence.
- **Design tradeoffs:** (1) Layer normalization vs. batch normalization—the paper uses LayerNorm to simplify permutation alignment but notes slight performance degradation. (2) Deterministic training eliminates noise but may underestimate stochastic training instability. (3) Wider/shallower architectures improve stability but may trade off representational capacity.
- **Failure signatures:** (1) Non-monotonic barrier curves with perturbation magnitude suggest numerical instability or learning rate issues. (2) Permutation alignment producing identity-only maps for transformer architectures indicates the matching algorithm's limitations, not necessarily functional equivalence. (3) Barriers increasing with pre-training duration on some tasks but not others signals task-specific instability requiring separate analysis.
- **First 3 experiments:**
  1. Replicate the core butterfly effect on a small model (ResNet-20 on CIFAR-10): train deterministically, apply batch perturbations at t=0%, 0.5%, 2%, 10%, 50% with σ ∈ [10^-4, 10^-1], measure barriers. Confirm early perturbations cause high barriers while late perturbations do not.
  2. Test stability vs. hyperparameters: compare standard SGD with (a) 10x warm-up, (b) wider architecture, (c) Adam. Quantify which settings reduce barriers at initialization without eliminating them entirely.
  3. Fine-tuning stability check: take a pre-trained checkpoint (e.g., MultiBERT 200k vs. 2000k), fine-tune on a GLUE task with early perturbations, measure whether extended pre-training increases or decreases barriers.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can specific hyperparameter combinations completely eliminate the "butterfly effect" instability at initialization? While the paper found that settings like shallow-wide architectures and increased warm-up reduce barriers, they failed to eliminate them entirely.
- **Open Question 2:** Can specific perturbations be designed to reliably improve model ensemble performance? The paper found that while ensemble accuracy scaled with representational dissimilarity in ResNets, this trend did not hold for fine-tuned ViT models, leaving the generalizability uncertain.
- **Open Question 3:** Why does extended pre-training sometimes reduce stability during fine-tuning in language models? The authors only speculate that "catastrophic overfitting" might be the cause, but task-specific effects could have multiple explanations beyond the proposed mechanism.

## Limitations

- The study relies on deterministic training, which may underestimate stability in stochastic settings where batch noise can dampen sensitivity to perturbations.
- The loss barrier metric captures only one aspect of functional divergence and may miss subtler differences in decision boundaries or robustness.
- Permutation alignment methods have known limitations with transformer architectures and may not find optimal matchings in all cases.
- Findings on fine-tuning stability are particularly sensitive to pre-training task distribution and optimization hyperparameters.

## Confidence

**High Confidence:** The core butterfly effect finding - that early perturbations cause trajectory divergence while late perturbations do not - is robustly demonstrated across multiple architectures, tasks, and perturbation scales.

**Medium Confidence:** The claim that divergence reflects landing in distinct convex loss basins (not permutations) is supported but could be strengthened by additional functional tests beyond barrier measurements.

**Low Confidence:** The mechanism for why extended pre-training sometimes reduces fine-tuning stability is speculative. The connection to catastrophic overfitting is plausible but not directly tested.

## Next Checks

1. **Stochastic Training Validation:** Repeat the perturbation experiments with varying batch sizes and learning rates to determine if stochastic training dampens the butterfly effect. Compare barrier distributions between deterministic and stochastic runs.

2. **Functional Divergence Tests:** Beyond loss barriers and permutation alignment, test whether diverged networks show different decision boundary geometries or adversarial robustness profiles. This would strengthen the claim that divergence is functional rather than representational.

3. **Pre-training Distribution Analysis:** Systematically vary the pre-training task distribution (e.g., different corpus sizes, domain shifts) to isolate whether fine-tuning instability correlates with distributional mismatch rather than pre-training duration alone.