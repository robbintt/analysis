---
ver: rpa2
title: Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved
  Sample Complexity
arxiv_id: '2510.04189'
source_url: https://arxiv.org/abs/2510.04189
tags:
- have
- critic
- actor
- algorithm
- termi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces the first natural critic-actor algorithm
  with function approximation for the long-run average cost setting under inequality
  constraints. The method employs a three-timescale update scheme: average cost and
  actor on the fastest timescale, critic on a slower timescale, and Lagrange multiplier
  on the slowest timescale.'
---

# Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity

## Quick Facts
- arXiv ID: 2510.04189
- Source URL: https://arxiv.org/abs/2510.04189
- Reference count: 40
- Introduces first natural critic-actor algorithm for long-run average cost under inequality constraints with $\tilde{O}(\epsilon^{-2})$ sample complexity

## Executive Summary
This paper presents the first natural critic-actor algorithm with function approximation for constrained reinforcement learning under the long-run average cost criterion. The method employs a three-timescale update scheme where the average cost and actor operate on the fastest timescale, the critic on an intermediate timescale, and the Lagrange multiplier on the slowest timescale. The authors establish non-asymptotic convergence guarantees with sample complexity bounds of $\tilde{O}(\epsilon^{-(2+\delta)})$ for standard learning rates and $\tilde{O}(\epsilon^{-2})$ for modified learning rates with logarithmic factors. Experimental evaluation on Safety-Gym environments demonstrates competitive performance compared to existing algorithms.

## Method Summary
The Constrained Natural Critic-Actor (C-NCA) algorithm uses three timescales with reversed roles compared to standard critic-actor methods: the actor operates on the fastest timescale while the critic operates on an intermediate timescale. The method employs natural gradient updates preconditioned by the Fisher information matrix, with the critic using TD(0) learning with linear function approximation. The algorithm handles inequality constraints through Lagrangian relaxation, with Lagrange multipliers updated on the slowest timescale. A key innovation is the use of modified learning rates with logarithmic factors that improve sample complexity from $\tilde{O}(\epsilon^{-(2+\delta)})$ to $\tilde{O}(\epsilon^{-2})$ while maintaining convergence guarantees.

## Key Results
- Establishes first natural critic-actor algorithm for long-run average cost under inequality constraints
- Achieves sample complexity bound of $\tilde{O}(\epsilon^{-(2+\delta)})$ with standard learning rates
- Improves sample complexity to $\tilde{O}(\epsilon^{-2})$ using logarithmic learning rate modification
- Demonstrates competitive performance on three Safety-Gym environments

## Why This Works (Mechanism)

### Mechanism 1: Three-Timescale Separation with Reversed Critic-Actor Roles
The three-timescale hierarchy creates a quasi-static decomposition where faster processes appear constant to slower ones and vice versa. With the actor on the fastest timescale and critic on intermediate timescale, this mimics value iteration rather than policy iteration. The Lagrange multiplier on the slowest timescale allows constraint violation signals to accumulate before influencing policy. Assumption 4 (uniform ergodicity) ensures Markov chain mixing is fast enough for the timescale separation to be meaningful.

### Mechanism 2: Natural Gradient Preconditioning via Fisher Information Matrix
The actor update uses the inverse Fisher information matrix $G(t)^{-1}$ to precondition policy gradient directions, providing more efficient updates than vanilla gradients. The Fisher matrix is recursively updated and required to be well-conditioned (Assumption 5(d)). This natural gradient approach provides compatibility with the compatible features $\Psi_{sa} = \nabla \log \pi(a|s)$, improving optimization efficiency in the policy space.

### Mechanism 3: Logarithmic Learning Rate Modification for Sample Complexity Improvement
Adding logarithmic factors to actor and average cost learning rates (e.g., $a(t) = c_a (\ln(t+1))^{1/2}/(1+t)^\nu$) improves sample complexity from $\tilde{O}(\epsilon^{-(2+\delta)})$ to $\tilde{O}(\epsilon^{-2})$. The modified rates maintain the required timescale separation properties while allowing the effective timescales to become closer more slowly. This modification requires $t \geq 2\tau_t + 1$ for the averaging bounds to apply.

## Foundational Learning

- **Concept: Temporal Difference (TD(0)) Learning with Function Approximation**
  - Why needed here: The critic uses TD(0) with linear function approximation to estimate the differential value function. Understanding TD convergence properties under Markov sampling is essential for interpreting critic error bounds.
  - Quick check question: Given feature matrix $\Phi$ and stationary distribution $\mu_\theta$, can you derive the TD fixed point equation $Av^* + b = 0$ and explain why negative definiteness of $A$ ensures a unique solution?

- **Concept: Lagrangian Relaxation for Constrained Optimization**
  - Why needed here: The constrained MDP is transformed into an unconstrained problem via the Lagrangian $L(\pi, \gamma) = J(\pi) + \sum_{k=1}^N \gamma_k(G_k(\pi) - \alpha_k)$. The Lagrange multiplier update implements primal-dual optimization.
  - Quick check question: In the Lagrange multiplier update $\gamma_k(n+1) = \hat{\Gamma}(\gamma_k(n) + c(n)(U_k(n) - \alpha_k))$, what happens to $\gamma_k$ if constraint $G_k(\pi)$ is consistently violated? How does projection $\hat{\Gamma}$ to $[0, M]$ affect convergence?

- **Concept: Markov Chain Mixing and Ergodicity**
  - Why needed here: Assumption 4 (uniform ergodicity) with geometric mixing rate is critical for handling non-i.i.d. samples. The analysis uses mixing time bounds to control gradient estimate variance.
  - Quick check question: If policy induces Markov chain with geometric mixing rate $k = 0.9$, approximately how many steps $\tau$ are needed for total variation distance to fall below $0.01$? Why does analysis require $t \geq 2\tau_t + 1$ for finite-time bounds?

## Architecture Onboarding

- **Component map:**
  Environment → (s_n, a_n, s_{n+1}, q(n), h_k(n))
         ↓
  Average Cost Estimator ← d(n) step-size (fastest)
         ↓ L_n
  Critic: v-parameter ← b(n) step-size (intermediate)
         ↓ δ_n = TD error
  Actor: θ-parameter ← a(n) step-size (fastest, natural gradient)
         ↓
  Fisher Matrix G(n) ← updated with a(n)
         ↓
  Constraint Estimators U_k(n) ← a(n) step-size
         ↓
  Lagrange Multipliers γ_k(n) ← c(n) step-size (slowest)

- **Critical path:**
  1. Sample transition (s_n, a_n, s_{n+1}) and costs (q(n), h_1(n), ..., h_N(n))
  2. Update average cost: $L_{n+1} = L_n + d(n)(q(n) + \sum_k \gamma_k(n)(h_k(n) - \alpha_k) - L_n)$
  3. Compute TD error: $\delta_n = q(n) + \sum_k \gamma_k(n)(h_k(n) - \alpha_k) - L_n + v_n^T(f_{s_{n+1}} - f_{s_n})$
  4. Update critic: $v_{n+1} = \Gamma(v_n + b(n)\delta_n f_{s_n})$
  5. Update actor: $\theta_{n+1} = \theta_n + a(n)\delta_n G(n)^{-1}\Psi_{s_n a_n}$
  6. Update constraint estimates: $U_k(n+1) = U_k(n) + a(n)(h_k(n) - U_k(n))$
  7. Update Lagrange multipliers: $\gamma_k(n+1) = \hat{\Gamma}(\gamma_k(n) + c(n)(U_k(n) - \alpha_k))$
  8. Update Fisher matrix: $G(n+1) = (1-a(n))G(n) + a(n)\Psi_{s_n a_n}\Psi_{s_n a_n}^T$

- **Design tradeoffs:**
  - Standard vs. Modified Learning Rates: Standard rates provide stronger timescale separation and conservative convergence; modified rates achieve better sample complexity but may have larger hidden constants and require careful tuning.
  - Projection set size M: Larger M allows stronger constraint enforcement but may slow convergence; smaller M may prevent finding feasible solutions for tight constraints.
  - Fisher matrix initialization p: Initial $G(0) = pI$ affects early gradient directions; too small p may cause large updates; too large p may slow early learning.

- **Failure signatures:**
  - Diverging critic values: Check if Assumption 2 (negative definiteness of $A$) is violated by feature choice; add regularization or adjust features.
  - Constraint violation at convergence: Lagrange multipliers hitting upper bound M indicates M may be too small or constraint thresholds $\alpha_k$ are infeasible.
  - Unstable actor updates: Monitor Fisher matrix condition number; if $\lambda_G \to 0$, consider adding entropy regularization to prevent policy collapse.
  - Slow convergence: Verify mixing time $\tau_t$ satisfies $t \geq 2\tau_t + 1$; if not, reduce exploration or use experience replay (though this changes theoretical guarantees).

- **First 3 experiments:**
  1. **Baseline sanity check on SafetyPointPush1-v0:** Implement C-NCA with standard learning rates ($\nu=0.5, \sigma=0.5+\delta, \beta=1$, small $\delta=0.01$). Verify critic MSE decreases as $O(\log^2 t \cdot t^{-(0.5-2\delta)})$, constraint costs converge below threshold $\alpha_k$, and average cost decreases monotonically after initial exploration. Compare against paper's Figure 1 (bottom-right) for constraint cost trajectory.
  2. **Learning rate ablation:** Compare standard vs. modified learning rates on SafetyCarGoal1-v0. Measure wall-clock time to reach $\epsilon$-accurate critic, samples to satisfy constraints, and final average reward. Expectation: modified rates should reach target accuracy ~10-100x faster in sample count, but hidden logarithmic factors may reduce wall-clock gains.
  3. **Fisher matrix sensitivity analysis:** Vary initial diagonal value $p \in \{0.1, 1.0, 10.0\}$ and monitor early policy entropy, Fisher matrix condition number over time, and actor gradient magnitudes. Expectation: smaller p leads to larger initial steps but potential instability; larger p is more stable but slower. Also test adding $\epsilon I$ regularization to $G(n)$ before inversion to handle near-singularity.

## Open Questions the Paper Calls Out

- **Question 1:** Can theoretical guarantees be extended to non-linear function approximation like neural networks?
  - Basis in paper: [explicit] Analysis explicitly restricts critic to linear function approximation (Section 3.2)
  - Why unresolved: Convergence proofs rely on bounded approximation error of linear features and negative definiteness of matrix $A$, which generally don't hold for non-linear, over-parameterized function approximators
  - What evidence would resolve it: Finite-time analysis of C-NCA with neural network critic without requiring linear assumptions on feature space

- **Question 2:** Can uniform ergodicity assumption be relaxed to accommodate Markov chains with slower mixing times?
  - Basis in paper: [explicit] Analysis depends on Assumption 4 (Section 4.1) requiring uniform ergodicity
  - Why unresolved: Uniform ergodicity implies exponentially fast mixing, a strong condition that excludes many practical environments; slower mixing introduces complex bias terms in sample complexity analysis
  - What evidence would resolve it: Modified finite-time bound where convergence rate explicitly depends on Markov chain mixing time rather than assuming uniform bounds

- **Question 3:** Does algorithm converge to global optimum or strictly to stationary point?
  - Basis in paper: [inferred] Theorems 2 and 5 bound gradient norm, implying convergence to stationary point rather than global minimum
  - Why unresolved: Optimization landscape of Lagrangian for constrained MDPs is generally non-convex, making global convergence guarantees difficult without further structural assumptions
  - What evidence would resolve it: Finite-time bound demonstrating global convergence under specific structural conditions, such as assuming MDP is linearly controllable

## Limitations

- The logarithmic learning rate modification may have large hidden constants in the $\tilde{O}(\epsilon^{-2})$ bound that make it impractical compared to standard rates
- Specific hyperparameter constants (Fisher matrix initialization, projection bound M) are not provided, creating significant uncertainty in reproducing experimental results
- Neural network implementation details are unclear, particularly how linear function approximation theory maps to NN architecture used in experiments

## Confidence

- **High confidence:** Three-timescale convergence theory and sample complexity bounds for standard learning rates ($\tilde{O}(\epsilon^{-(2+\delta)})$). Core algorithm mechanics and proof structure are well-established in constrained RL literature.
- **Medium confidence:** Modified learning rates achieving $\tilde{O}(\epsilon^{-2})$ complexity. While theoretical proof exists, practical performance may be limited by logarithmic factors and hyperparameter sensitivity.
- **Low confidence:** Exact experimental reproduction without knowing specific hyperparameter constants and neural network implementation details.

## Next Checks

1. **Theoretical validation:** Verify timescale separation conditions $2\sigma - \nu < \beta$ and $2\sigma < 3\nu$ are satisfied for chosen learning rate parameters ($\nu=0.5, \sigma=0.5+\delta, \beta=1$) and that critic error bound $\tilde{O}(\log^2 t \cdot t^{-(0.5-2\delta)})$ holds empirically on SafetyPointPush1-v0.

2. **Sample complexity validation:** Compare samples required to reach $\epsilon$-accurate critic (MSE < 0.01) between standard and modified learning rates on SafetyCarGoal1-v0. Modified version should require significantly fewer samples, though wall-clock time may not show proportional improvement.

3. **Fisher matrix stability validation:** Monitor condition number of $G(n)$ throughout training and test different initialization values $p \in \{0.1, 1.0, 10.0\}$. Verify Fisher matrix remains positive definite and gradient magnitudes are stable, particularly during early training when policy is exploring.