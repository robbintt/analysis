---
ver: rpa2
title: Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy
  Networks via LLM-Enhanced Optimization
arxiv_id: '2510.10028'
source_url: https://arxiv.org/abs/2510.10028
tags:
- reward
- latency
- inference
- power
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying vision-language
  models (VLMs) on UAVs for real-time multimodal inference in Low-Altitude Economy
  Networks (LAENets). The main problem is optimizing resource allocation to balance
  inference accuracy, latency, and power consumption under dynamic network conditions
  and limited onboard resources.
---

# Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization

## Quick Facts
- arXiv ID: 2510.10028
- Source URL: https://arxiv.org/abs/2510.10028
- Reference count: 40
- Primary result: Hierarchical ARPO-LLaRA framework achieves up to 45% latency reduction and 13.7% accuracy improvement in UAV-based vision-language inference

## Executive Summary
This paper addresses the challenge of deploying vision-language models (VLMs) on UAVs for real-time multimodal inference in Low-Altitude Economy Networks (LAENets). The authors propose a hierarchical optimization framework that jointly optimizes image resolution, transmit power, and UAV trajectory while leveraging LLM-augmented reward design for reinforcement learning. The solution aims to balance inference accuracy, latency, and power consumption under dynamic network conditions and limited onboard resources, demonstrating significant improvements over baseline approaches through simulations.

## Method Summary
The paper presents a two-level hierarchical optimization framework combining Alternating Resolution and Power Optimization (ARPO) with Large Language Model-augmented Reinforcement Learning Approach (LLaRA). The ARPO algorithm optimizes image resolution and transmit power for each user, while LLaRA uses an LLM to design reward functions offline for UAV trajectory optimization. The LLM serves as an offline reward designer, generating and refining reward functions to improve convergence and stability without introducing real-time latency. The framework operates in dynamic environments with time-varying network conditions and resource constraints.

## Key Results
- ARPO-LLaRA framework achieves up to 45% latency reduction compared to random policies
- Demonstrates 13.7% improvement in optimization performance over manual reward designs
- Shows stable convergence and effective multi-round service adaptation in VLM inference

## Why This Works (Mechanism)
The hierarchical approach works by separating concerns: ARPO handles local resource optimization at the user level while LLaRA manages global UAV mobility decisions. The LLM-augmented reward design addresses the challenge of designing effective reward functions for reinforcement learning in complex, multimodal environments. By generating rewards offline, the system avoids computational overhead during real-time operation while still benefiting from sophisticated reward shaping that accounts for the nuanced trade-offs between accuracy, latency, and power consumption.

## Foundational Learning
1. **Alternating Resolution and Power Optimization (ARPO)**: Jointly optimizes image resolution and transmit power per user to balance quality and resource usage - needed because VLM inference quality depends on both input quality and communication reliability; quick check: verify that power allocation doesn't exceed UAV battery constraints.
2. **Large Language Model-augmented Reinforcement Learning**: Uses LLM to generate and refine reward functions offline - needed because designing effective rewards for multimodal optimization is complex and time-consuming; quick check: validate that offline reward generation doesn't become outdated in dynamic environments.
3. **Hierarchical Optimization Framework**: Separates local resource optimization from global trajectory planning - needed because joint optimization of all variables becomes computationally intractable; quick check: ensure information flow between levels doesn't create bottlenecks.

## Architecture Onboarding

**Component Map**: Users -> ARPO -> Local Optimization -> LLaRA -> UAV Trajectory -> Network Communication -> VLM Inference

**Critical Path**: User request → ARPO optimization → Resource allocation → LLaRA decision → UAV movement → Data transmission → VLM inference → Response delivery

**Design Tradeoffs**: The framework trades computational complexity for performance by using offline LLM processing, accepts potential reward staleness for real-time efficiency, and balances accuracy-latency trade-offs through resolution adaptation.

**Failure Signatures**: Poor reward design leading to suboptimal trajectories, communication bottlenecks causing inference delays, insufficient power allocation resulting in dropped connections, and resolution settings that don't match network conditions.

**First Experiments**:
1. Baseline comparison with random UAV trajectory policies under identical network conditions
2. Performance evaluation with manual vs. LLM-augmented reward designs
3. Stress testing with rapidly changing network conditions and user demands

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on specific VLM architectures may limit generalizability across different UAV deployments
- Offline reward generation raises concerns about adaptability to rapidly changing real-world conditions
- Computational overhead of ARPO algorithm and LLaRA framework complexity may challenge real-time implementation on resource-constrained UAVs

## Confidence
- High confidence in theoretical soundness of hierarchical optimization framework and demonstrated improvements over baselines in controlled simulations
- Medium confidence in practical feasibility of LLM-augmented reward design given offline nature and potential challenges in dynamic environments
- Medium confidence in latency and accuracy trade-offs based on simulated network conditions that may not fully capture real-world variability

## Next Checks
1. Conduct field tests with actual UAV deployments to validate ARPO-LLaRA framework performance under real-world network conditions and resource constraints
2. Perform comprehensive analysis of computational overhead and energy consumption on resource-constrained UAVs to assess practical deployment feasibility
3. Evaluate adaptability of LLM-augmented reward design to rapidly changing environments by testing response to unexpected network dynamics or VLM inference requirements