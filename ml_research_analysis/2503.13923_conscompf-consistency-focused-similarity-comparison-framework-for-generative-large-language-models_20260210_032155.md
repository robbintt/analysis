---
ver: rpa2
title: 'ConSCompF: Consistency-focused Similarity Comparison Framework for Generative
  Large Language Models'
arxiv_id: '2503.13923'
source_url: https://arxiv.org/abs/2503.13923
tags:
- similarity
- comparison
- conscompf
- scores
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConSCompF addresses the challenge of comparing generative large
  language models (LLMs) when developers do not disclose training data or model weights.
  The method generates multiple responses per instruction, encodes them with SBERT,
  calculates consistency scores to account for response variability, and uses weighted
  cosine similarity to compare models.
---

# ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models

## Quick Facts
- **arXiv ID**: 2503.13923
- **Source URL**: https://arxiv.org/abs/2503.13923
- **Reference count**: 5
- **Primary result**: ConSCompF enables effective LLM comparison without requiring model access or labeled data

## Executive Summary
ConSCompF addresses the challenge of comparing generative large language models (LLMs) when developers do not disclose training data or model weights. The method generates multiple responses per instruction, encodes them with SBERT, calculates consistency scores to account for response variability, and uses weighted cosine similarity to compare models. Two experiments demonstrate ConSCompF can detect LLM performance degradation due to quantization and identify similarities between models trained on identical data.

## Method Summary
ConSCompF generates multiple responses per instruction to capture response variability, then encodes these responses using SBERT embeddings. The framework calculates consistency scores that account for the inherent variability in LLM responses, addressing the challenge that generative models produce different outputs for identical inputs. Weighted cosine similarity is then applied to compare models based on their response distributions. The approach enables model comparison without requiring labeled data or access to model weights, making it particularly useful for evaluating commercial LLMs where training data and architecture details are proprietary.

## Key Results
- ConSCompF detects LLM performance degradation due to quantization with strong correlation to traditional benchmarks
- The framework identifies similarities between models trained on identical data using similarity matrices
- In few-shot scenarios with as few as 50 samples, ConSCompF produces similarity scores highly correlated with ROUGE-L scores

## Why This Works (Mechanism)
ConSCompF leverages the principle that models trained on similar data or with similar architectures will exhibit consistent response patterns when evaluated on the same instructions. By generating multiple responses per instruction, the framework captures the stochastic nature of LLM outputs and uses consistency scoring to normalize for this variability. The weighted cosine similarity then quantifies how closely two models' response distributions align, providing a robust measure of similarity that is invariant to minor performance variations or architectural differences.

## Foundational Learning
- **SBERT embeddings**: Why needed - To convert variable-length text responses into fixed-dimensional vectors for mathematical comparison; Quick check - Verify embedding dimensions match across all model responses
- **Consistency scoring**: Why needed - To account for the inherent variability in LLM outputs and normalize for stochastic behavior; Quick check - Calculate variance in response embeddings for the same instruction across multiple generations
- **Weighted cosine similarity**: Why needed - To provide a normalized measure of similarity that accounts for response consistency; Quick check - Ensure similarity scores fall between 0 and 1 for all model pairs

## Architecture Onboarding
**Component Map**: Instruction Generator -> Response Generator -> SBERT Encoder -> Consistency Calculator -> Similarity Matrix Generator

**Critical Path**: The most critical path is Instruction Generator -> Response Generator -> Consistency Calculator, as the quality and diversity of generated responses directly impacts the reliability of consistency scores and subsequent similarity calculations.

**Design Tradeoffs**: The framework trades computational efficiency for accuracy by generating multiple responses per instruction, which increases processing time but provides more robust similarity estimates. Using SBERT rather than task-specific embeddings simplifies the approach but may miss domain-specific semantic nuances.

**Failure Signatures**: If models show artificially high similarity scores despite being trained on different data, this may indicate insufficient response diversity or SBERT embedding collapse. Low consistency scores across all instructions could suggest instructions are too ambiguous or the model's response space is too broad.

**First 3 Experiments**:
1. Generate 10 responses per instruction for two identical models to establish baseline consistency and similarity
2. Compare quantized vs. full-precision versions of the same model to validate detection of performance degradation
3. Evaluate models trained on disjoint datasets to confirm low similarity scores for unrelated training data

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on SBERT embeddings which may not capture all semantic nuances across diverse domains
- Assumes response variability is primarily random noise rather than meaningful model uncertainty
- Performance may vary significantly with more complex or domain-specific tasks beyond the tested instruction types

## Confidence
**High confidence**: Similarity detection claims for models trained on identical data and quantization degradation detection are well-supported by experimental results.

**Medium confidence**: Few-shot effectiveness claims need broader validation across diverse domains, as current experiments focus on specific instruction types.

**Low confidence**: Investment fraud detection application remains aspirational without empirical validation or demonstrated real-world performance.

## Next Checks
1. Evaluate ConSCompF on diverse instruction sets spanning multiple domains (legal, medical, technical) to assess generalization beyond current dataset
2. Conduct blinded human assessments of model similarities using the same instructions to validate alignment with SBERT-based consistency scores
3. Test whether ConSCompF produces consistent similarity scores across different time periods and SBERT model versions to validate longitudinal comparison reliability