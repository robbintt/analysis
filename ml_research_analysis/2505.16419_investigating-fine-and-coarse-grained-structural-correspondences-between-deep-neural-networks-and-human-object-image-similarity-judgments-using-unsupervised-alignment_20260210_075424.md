---
ver: rpa2
title: Investigating Fine- and Coarse-grained Structural Correspondences Between Deep
  Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment
arxiv_id: '2505.16419'
source_url: https://arxiv.org/abs/2505.16419
tags:
- human
- learning
- matching
- object
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how deep neural networks trained with different
  learning paradigms align with human object representations at both fine-grained
  and coarse-grained levels. Using an unsupervised alignment method based on Gromov-Wasserstein
  Optimal Transport, the researchers compared 27 DNN models across four learning paradigms
  (supervised, self-supervised, self-sup+sup, and CLIP) with human similarity judgments
  for 1,854 objects from the THINGS dataset.
---

# Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment

## Quick Facts
- arXiv ID: 2505.16419
- Source URL: https://arxiv.org/abs/2505.16419
- Reference count: 40
- Key outcome: CLIP models achieve 20% fine-grained alignment with human similarity judgments, while self-supervised models capture only coarse category structure

## Executive Summary
This study investigates how deep neural networks trained with different learning paradigms align with human object representations at both fine-grained (object-level) and coarse-grained (category-level) levels. Using Gromov-Wasserstein Optimal Transport for unsupervised alignment, researchers compared 27 DNN models across four paradigms (supervised, self-supervised, self-sup+sup, and CLIP) with human similarity judgments for 1,854 objects from the THINGS dataset. CLIP models achieved the strongest fine-grained alignment (20% top-1 matching rate), significantly outperforming other paradigms. Self-supervised models showed modest alignment at the coarse-grained level (25% category matching) but failed at fine-grained alignment, suggesting linguistic information is crucial for precise object representations while visual-only learning captures basic categorical structures.

## Method Summary
The study uses Gromov-Wasserstein Optimal Transport to align DNN embeddings with human similarity judgments without requiring labeled correspondences. Human representations are derived from 4.7 million odd-one-out judgments using SPoSE embeddings, creating a 1,854×1,854 similarity matrix. Model representations are obtained from final-layer embeddings of 27 pre-trained DNNs across four learning paradigms. The alignment process optimizes a transportation plan between human and model similarity structures, measuring fine-grained alignment via object-level matching rates and coarse-grained alignment via category matching rates. Entropy regularization is tuned via Optuna, and results are validated against chance-level baselines and traditional RSA comparisons.

## Key Results
- CLIP models achieved 20% fine-grained alignment (object-level matching), significantly outperforming all other paradigms
- Self-supervised models showed 25% coarse-grained alignment (category matching) but remained at chance level for fine-grained alignment
- Self-supervised contrastive learning models (SimCLR, SwAV) demonstrated better coarse-grained alignment than reconstruction-based models (MAE)
- Clustering analysis confirmed self-supervised models formed human-like coarse categories despite failing at fine-grained alignment

## Why This Works (Mechanism)

### Mechanism 1: Linguistic information enables fine-grained object representation alignment
Linguistic supervision in CLIP provides disambiguating semantic signals that separate visually similar objects into distinct representations, enabling precise distinctions (e.g., specific animal species rather than just "animal vs. vehicle"). Human fine-grained object representations rely partially on linguistic/semantic knowledge rather than purely visual features.

### Mechanism 2: Self-supervised contrastive learning captures coarse category structure through visual pattern discovery
Contrastive objectives push augmented views of the same image together while separating different images. Without labels, models discover statistical regularities in visual features that naturally correlate with semantic categories (e.g., animals share texture/motion patterns), producing coarse-grained clusters but lacking disambiguating signals for fine distinctions.

### Mechanism 3: Unsupervised alignment via GWOT reveals fine-grained differences that RSA obscures
GWOT optimizes a transportation plan that maps objects based on structural similarity alone, unlike RSA which computes correlation between fixed-pairwise dissimilarities. If a model only captures coarse structure, GWOT will correctly map categories but fail at object-level matching—revealing the granularity gap.

## Foundational Learning

- **Optimal Transport / Gromov-Wasserstein Distance:** Core method for unsupervised alignment. Standard OT matches distributions with known point-to-point costs; GWOT matches distributions by aligning their internal distance structures when no direct correspondence exists.
  - *Quick check:* Given two sets of points with only pairwise distances (no shared coordinates), how would you measure their structural similarity?

- **Representational Dissimilarity Matrices (RDMs):** The paper compares RDMs from human similarity judgments vs. DNN embeddings. Understanding that RDMs compress representational structure into pairwise distances is essential for interpreting both RSA and GWOT results.
  - *Quick check:* If two systems have RDMs with correlation r=0.9, what can and cannot you conclude about their representations?

- **Contrastive Learning Objectives:** Self-supervised models (SimCLR, SwAV, MAE) use different objectives with different inductive biases. Understanding what each objective encourages (instance discrimination vs. reconstruction vs. clustering) explains why contrastive methods capture category structure while MAE does not.
  - *Quick check:* Why might predicting masked image patches (MAE) lead to different representations than pushing positive pairs together and negative pairs apart (SimCLR)?

## Architecture Onboarding

- **Component map:** Human behavioral data (THINGS odd-one-out judgments) → SPoSE embedding → Human RDM → DNN models → Final layer embeddings → Model RDM → GWOT alignment → Transportation plan Γ → Metrics (matching rate/category matching rate/AMI)

- **Critical path:** The matching rate calculation depends entirely on the quality of the transportation plan. Poor initialization or suboptimal ε leads to local minima that underestimate alignment. The paper mitigates this by: (1) random initialization with multiple restarts, (2) reporting both minimum-GWD and highest-matching-rate solutions, (3) chance-level simulation for interpretation.

- **Design tradeoffs:**
  - **Entropy regularization (ε):** Higher ε → smoother transportation plans, easier optimization, but potentially less precise mappings. The paper tunes ε per model-dataset pair rather than fixing it.
  - **Reporting highest vs. minimum-GWD solution:** Minimum-GWD is principled but noisy; highest-matching-rate is metric-optimized but risks overfitting to the evaluation. Reporting both provides bounds.
  - **Final layer vs. intermediate layers:** Final layer captures task-optimized representations; intermediate layers might show different alignment patterns (not explored in this paper).

- **Failure signatures:**
  - Chance-level matching rate (~0.27%) with above-chance RSA correlation → model captures coarse structure only
  - High category matching but low object matching → semantic categories preserved but instance distinctions lost
  - High variance across random initializations → optimization landscape has many local minima; need more restarts
  - MAE underperforming other self-supervised methods → reconstruction objective doesn't enforce instance discrimination

- **First 3 experiments:**
  1. **Reproduce alignment on a subset:** Take 100 objects from THINGS, extract embeddings from a pre-trained CLIP and SimCLR model, compute RDMs, run GWOT with 50 random initializations, verify CLIP matching rate > SimCLR.
  2. **Ablate linguistic information:** Compare CLIP image encoder alone vs. full CLIP (image+text) alignment to isolate the contribution of linguistic supervision.
  3. **Test layer-wise alignment:** Extract embeddings from early, middle, and late layers of a supervised ResNet to test whether fine-grained alignment emerges progressively through the network.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do self-supervised DNNs accurately model prelinguistic categorical object representation in infants? The authors explicitly propose that self-supervised learning mirrors how infants organize their environment before language acquisition and state that "future research could directly test this by comparing model behavior with infant developmental data."

- **Open Question 2:** To what extent do architectural variations versus training data characteristics drive the observed differences in fine-grained alignment? The authors acknowledge in the Limitations section that the "high computational cost" restricted the number of models, meaning they "were unable to systematically control for the effects of architectural variations or dataset-specific factors."

- **Open Question 3:** Does the semantic focus of the THINGS odd-one-out task penalize self-supervised models by failing to capture their purely visual representational power? The authors note that the THINGS task encourages decisions based on "semantic similarities rather than visual appearance," arguing this favors linguistic models (CLIP).

## Limitations
- Limited model diversity due to computational constraints prevented systematic control of architectural variations and dataset-specific factors
- Results based on a single dataset (THINGS) and specific human judgment methodology may not generalize across domains
- Cannot definitively prove linguistic supervision causes fine-grained alignment versus confounding factors like model size or training data diversity

## Confidence

- **High confidence:** Self-supervised models capture coarse category structure but fail at fine-grained alignment. This is supported by multiple metrics (AMI scores, category matching rates) and aligns with established understanding of contrastive learning's inductive biases.

- **Medium confidence:** CLIP's linguistic information enables fine-grained alignment. While results strongly support this, alternative explanations (more training data, different architectures) cannot be fully ruled out without controlled ablations.

- **Medium confidence:** GWOT reveals granularity differences invisible to RSA. The methodological argument is sound, but the claim that RSA "fails to distinguish qualitative differences" may overstate its limitations in practice.

## Next Checks

1. **Dataset generalization test:** Run the same alignment pipeline on a different human similarity dataset (e.g., ImageNet similarity judgments) to verify CLIP's advantage persists across domains.

2. **Ablation of linguistic input:** Compare CLIP image encoder only vs. full CLIP alignment to isolate the contribution of text supervision to fine-grained matching rates.

3. **Controlled training experiment:** Train a CLIP-like model with identical architecture but without text supervision on the same data as CLIP to test whether linguistic information (versus scale or architecture) drives the alignment advantage.