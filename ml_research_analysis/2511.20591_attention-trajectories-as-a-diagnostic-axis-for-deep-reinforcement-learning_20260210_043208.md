---
ver: rpa2
title: Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning
arxiv_id: '2511.20591'
source_url: https://arxiv.org/abs/2511.20591
tags:
- attention
- agents
- saliency
- agent
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scientific methodology for analyzing deep
  reinforcement learning (DRL) agent learning through quantitative analysis of saliency
  maps. The key innovation is the hierarchical-attention profile (h-profile), which
  aggregates saliency information at the object and modality level to quantify how
  agents allocate attention over time.
---

# Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.20591
- Source URL: https://arxiv.org/abs/2511.20591
- Reference count: 40
- Primary result: Introduces hierarchical-attention profiles (h-profiles) as a quantitative diagnostic tool for analyzing deep RL agent learning through saliency map aggregation

## Executive Summary
This paper introduces a scientific methodology for analyzing deep reinforcement learning (DRL) agent learning through quantitative analysis of saliency maps. The key innovation is the hierarchical-attention profile (h-profile), which aggregates saliency information at the object and modality level to quantify how agents allocate attention over time. The methodology was applied across three case studies: comparing four DQN algorithms on Atari benchmarks, custom Pong environments with different reward designs, and biomechanical simulations with multimodal inputs. Results demonstrate that attention trajectories reveal algorithm-specific learning dynamics and vulnerabilities invisible to performance metrics alone.

## Method Summary
The methodology computes hierarchical-attention profiles by first extracting saliency maps using Layer-wise Relevance Propagation (LRP) from trained DRL agents. Relevance scores are propagated from the output layer through the network to the input, where they are aggregated within predefined object boundaries to create object-level attention profiles. These profiles are normalized and averaged across datasets to produce h-profiles that quantify attention allocation over training time. The approach was validated using multiple saliency methods (gradient, SmoothGrad, perturbation) and applied to analyze attention trajectories in Atari games, custom Pong variants, and multimodal biomechanical tasks.

## Key Results
- Attention profiles diverge systematically between algorithms even at matched performance levels, with DQN/QR-DQN showing vulnerability to perturbations of highly-attended features
- Reward structure shapes agent attention, with corresponding behavioral preferences in multi-object environments
- Agents dynamically reallocate attention across modalities during sequential tasks, with potential overfitting to redundant sensory channels
- Attention patterns correspond to measurable behavioral differences, establishing empirical links between attention profiles, learning dynamics, and agent behavior

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating pixel-level saliency into object-level attention profiles (h-profiles) enables systematic, quantitative comparison of agent attention across training.
- **Mechanism:** Layer-wise Relevance Propagation (LRP) computes relevance scores per pixel; these are summed within predefined object boundaries and normalized across objects. The h-profile `h(o_j)` measures average relevance allocated to object `o_j` across a fixed dataset, yielding interpretable attention distributions.
- **Core assumption:** LRP relevance at the input layer meaningfully reflects the agent's decision-reliance on those features; object boundaries are correctly annotated.
- **Evidence anchors:** [abstract] "This approach aggregates saliency information at the object and modality level into hierarchical attention profiles, quantifying how agents allocate attention over time." [section 3.1] Equation (1) formalizes h-profile as weighted sum of neuron-level relevance maps aggregated over object pixel sets.
- **Break condition:** If saliency methods produce contradictory object rankings for the same agent; if object annotations are incomplete or overlapping (violates methodology constraints).

### Mechanism 2
- **Claim:** Algorithm-specific attention patterns predict differential robustness to environmental perturbations.
- **Mechanism:** DQN/QR-DQN agents progressively increase attention to static, reward-correlated features (bricks in Breakout); A2C/PPO maintain focus on dynamic task-relevant objects (ball, paddle). When brick color is perturbed, brick-attending agents suffer performance collapse; others remain robust.
- **Core assumption:** Attention to a feature implies dependence; perturbing attended features disrupts behavior more than perturbing unattended features.
- **Evidence anchors:** [abstract] "DQN/QR-DQN agents over-attended to bricks, making them vulnerable to perturbations, while A2C/PPO agents remained robust." [section 4.1] "In Breakout, altering the color of the bricks... caused severe degradation, with Δr approaching –1... In contrast, A2C and PPO... remained robust (Δr ≈ 0)."
- **Break condition:** If perturbation effects are confounded by perceptual similarity changes rather than attention-dependency; if agents attend to features they do not causally rely on (Clever Hans artifacts).

### Mechanism 3
- **Claim:** Reward structure shapes attention allocation, which in turn shapes behavioral preferences in multi-object environments.
- **Mechanism:** In custom Pong variants, agents trained with different reward structures develop different ball attention profiles (B1 vs B2). In a forced-choice discrimination test, agents preferentially interact with the higher-attention ball, linking attention to action selection.
- **Core assumption:** Attention differences are not merely epiphenomenal but causally influence policy output; behavioral tests capture this causal link.
- **Evidence anchors:** [abstract] "In custom Pong, reward structure shaped attention, leading to distinct ball preferences confirmed via behavioral tests." [section 4.2] "In 100 controlled trials, agents consistently interacted most with the ball that had received the most attention."
- **Break condition:** If attention differences reflect spurious correlations without behavioral consequence; if forced-choice test design introduces confounds (e.g., position bias).

## Foundational Learning

- **Concept: Layer-wise Relevance Propagation (LRP)**
  - **Why needed here:** LRP is the primary saliency method; understanding its conservation property and propagation rules is essential for interpreting h-profiles and replicating results.
  - **Quick check question:** Given a network output `f(x)`, how does LRP redistribute relevance backward through layers while maintaining conservation?

- **Concept: Deep RL Algorithm Taxonomy (Value-based vs Policy Gradient)**
  - **Why needed here:** The paper compares DQN/QR-DQN (value-based) vs A2C/PPO (policy gradient); understanding their learning dynamics explains divergent attention profiles.
  - **Quick check question:** Why might value-based methods exhibit different feature-learning trajectories than actor-critic methods under identical environments?

- **Concept: Object Segmentation and Labeling in RL Environments**
  - **Why needed here:** H-profiles require predefined object mappings; accurate labeling (RAM extraction for Atari, color-mapping for custom Pong, MuJoCo labels for biomechanical tasks) determines analysis validity.
  - **Quick check question:** What constraints must labeled datasets satisfy (object presence, non-overlap) and why?

## Architecture Onboarding

- **Component map:** Saliency extraction (LRP, gradient, SmoothGrad, perturbation) → Neuron filtering (retain top 90% relevance neurons) → Object aggregation (sum relevance within object pixel sets) → H-profile computation (normalize and average across dataset) → Trajectory tracking (evaluate at training intervals) → Behavioral validation

- **Critical path:** Saliency extraction → neuron relevance computation → input-space relevance maps → object-level aggregation → h-profile normalization → longitudinal trajectory analysis → behavioral validation experiments

- **Design tradeoffs:**
  - Fixed vs online dataset: Fixed enables cross-algorithm comparison; online captures training-specific state distributions (paper finds similar results with both)
  - Saliency method choice: LRP produces sharper, more object-focused maps; gradient/perturbation methods are noisier but offer validation
  - Dataset size (N): Larger N reduces variance but increases computation; N=50 found sufficient for stable patterns

- **Failure signatures:**
  - High variance in h-profile across runs with same algorithm (indicates unstable feature learning)
  - Contradictory object rankings across saliency methods (method-dependent artifacts)
  - Behavioral tests not matching attention predictions (attention-behavior decoupling)
  - Missing objects in generated datasets (agent policy doesn't visit required states)

- **First 3 experiments:**
  1. Reproduce Atari attention divergence: Train A2C and DQN agents on Breakout; compute h-profiles every 1M steps; verify DQN shows increasing brick attention while A2C does not.
  2. Validate with alternative saliency: Re-compute h-profiles using gradient and perturbation methods; confirm algorithm-specific patterns persist (ANOSIM R > 0.8 between algorithms).
  3. Behavioral perturbation test: Apply brick-color perturbation to trained agents; measure Δr; confirm DQN/QR-DQN degrade (Δr → -1) while A2C/PPO remain stable (Δr ≈ 0).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the h-profile methodology be effectively scaled to complex, unstructured inputs by integrating automatic segmentation models?
- **Basis in paper:** [explicit] The authors state, "Combining our methodology with automatic segmentation tools (e.g., SAM...), relational abstractions..., or causal probing methods could extend attention analysis beyond object-level attribution."
- **Why unresolved:** The current implementation relies on predefined, non-overlapping object labels or RAM extraction, limiting the method's applicability to environments where object boundaries are ambiguous or unknown.
- **What evidence would resolve it:** A study integrating a model like SAM into the h-profile pipeline, demonstrating that automatically generated object masks produce trajectories consistent with manually annotated ground truths.

### Open Question 2
- **Question:** Do early-stage attention trajectories reliably predict an agent's future robustness and generalization capabilities before performance plateaus?
- **Basis in paper:** [explicit] The discussion suggests that "attention trajectories could be used to predict robustness and generalization, inform algorithm selection, and guide reward shaping..."
- **Why unresolved:** The paper demonstrates that attention profiles diagnose *current* vulnerabilities (e.g., brick over-reliance in DQN), but does not show that early trajectories can forecast these failures or final OOD performance before they manifest in the reward signal.
- **What evidence would resolve it:** A longitudinal experiment correlating specific early trajectory patterns (e.g., initial fixation on distractors) with final performance on perturbed environments or out-of-distribution tasks across multiple random seeds.

### Open Question 3
- **Question:** Can hierarchical-attention profiles be modified to quantify the relevance of relational features, such as distance or relative velocity, rather than just object presence?
- **Basis in paper:** [explicit] The authors acknowledge that relying on object-level aggregation "may obscure relational features such as distances or velocities."
- **Why unresolved:** The current h-profile sums relevance scores over pixels within an object mask, which destroys information about the spatial relationships or dynamic interactions between objects that are often critical for control tasks.
- **What evidence would resolve it:** An extension of the h-profile formulation that successfully attributes relevance to interaction terms (e.g., ball-paddle distance) and validates this through perturbation tests targeting those specific relational cues.

## Limitations

- Methodology relies on accurate object segmentation and labeling, which is nontrivial across environments and limits applicability to complex 3D scenarios
- LRP saliency method, while providing sharper maps, remains an approximation whose faithfulness to actual feature importance is not directly validated
- Paper focuses on aggregated metrics rather than individual trajectory analysis, potentially missing nuanced differences in how agents solve tasks

## Confidence

**High Confidence:** Claims about algorithm-specific attention patterns (DQN/QR-DQN over-attending to static features vs A2C/PPO maintaining focus on dynamic objects) are well-supported by quantitative evidence across multiple games and validated through perturbation experiments.

**Medium Confidence:** The reward-attention-behavior chain demonstrated in custom Pong is convincing but limited to a simplified environment.

**Low Confidence:** Claims about overfitting to redundant modalities in biomechanical tasks are based on limited examples and require broader testing.

## Next Checks

1. **Cross-saliency validation:** Recompute attention profiles using Integrated Gradients and Occlusion methods across all three case studies to verify algorithm-specific patterns persist (target: ANOSIM R > 0.8 between algorithms).

2. **Attention-perturbation ablation study:** Systematically perturb attended vs unattended features in trained agents across all games, measuring performance impact to establish causal relationship between attention and behavior (target: perturbation effect magnitude correlates with attention allocation).

3. **Generalization to complex environments:** Apply the methodology to 3D environments (e.g., DeepMind Control Suite) with automatic object detection to test scalability and identify labeling challenges in realistic scenarios.