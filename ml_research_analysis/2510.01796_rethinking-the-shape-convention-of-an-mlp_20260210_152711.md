---
ver: rpa2
title: Rethinking the shape convention of an MLP
arxiv_id: '2510.01796'
source_url: https://arxiv.org/abs/2510.01796
tags:
- hourglass
- input
- architectures
- wang
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the conventional narrow-wide-narrow design
  of MLPs by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
  operate at expanded dimensions while residual computation flows through narrow bottlenecks.
  The authors hypothesize that incremental refinement is more effective in higher-dimensional
  spaces and demonstrate that the input projection can be fixed at random initialization
  without performance loss.
---

# Rethinking the shape convention of an MLP

## Quick Facts
- arXiv ID: 2510.01796
- Source URL: https://arxiv.org/abs/2510.01796
- Authors: Meng-Hsi Chen; Yu-Ang Lee; Feng-Ting Liao; Da-shan Shiu
- Reference count: 17
- Primary result: Hourglass MLP blocks (wide-narrow-wide) achieve superior parameter efficiency and Pareto frontiers compared to conventional narrow-wide-narrow designs on MNIST and ImageNet-32.

## Executive Summary
This paper challenges the conventional narrow-wide-narrow design of MLPs by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. The authors hypothesize that incremental refinement is more effective in higher-dimensional spaces and demonstrate that the input projection can be fixed at random initialization without performance loss. Experiments on MNIST and ImageNet-32 show Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs, with optimal configurations favoring deeper networks and wider skip connections as parameter budgets increase.

## Method Summary
The Hourglass architecture introduces an input projection that maps from the input dimension dx to a higher dimensional space dz, followed by residual blocks where each computes zi+1 = zi + W2·σ(W1·norm(zi)) with W1 ∈ R^(dh×dz) and W2 ∈ R^(dz×dh). The key constraint is dz > dh, opposite to conventional MLPs. The input projection Win can be either learned or fixed random (Gaussian or Rademacher). The architecture is evaluated on generative image tasks including classification, denoising, and super-resolution on MNIST and ImageNet-32 using AdamW optimizer with linear learning rate scheduling.

## Key Results
- Hourglass architectures achieve better Pareto frontiers (higher PSNR at lower parameter counts) than conventional MLPs on both MNIST and ImageNet-32
- Optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks as parameter budgets increase
- Fixed random input projections perform comparably to learned projections when expansion factors are large (dz >> dx), enabling ~35% parameter reduction
- The conventional narrow-wide-narrow design consistently underperforms Hourglass across all tested parameter budgets

## Why This Works (Mechanism)

### Mechanism 1
Skip connections at higher dimensions enable more effective incremental refinement than those at lower dimensions. When residual blocks learn corrections at expanded dimensionality (dz > dx), the higher-dimensional space provides richer feature representations. The skip connection preserves information at this wider dimension while the bottleneck computes a targeted correction, allowing additive updates with greater representational capacity.

### Mechanism 2
A fixed random input projection can substitute for a learned projection with negligible performance loss when expansion factors are large. Random projections from appropriate distributions preserve geometric structure with high probability (Johnson-Lindenstrauss lemma). When dz >> dx, the projection acts as a universal feature expansion that requires no task-specific learning.

### Mechanism 3
Hourglass scaling favors depth over bottleneck width as parameter budgets increase, unlike conventional MLPs. By constraining computation to narrow bottlenecks (dh << dz), more blocks can be stacked within the same parameter budget. The wide skip dimension maintains expressivity while the narrow bottleneck controls parameter cost per layer.

## Foundational Learning

- Concept: Skip connections and residual learning (ResNet paradigm)
  - Why needed here: The entire Hourglass design builds on the principle that networks learn corrections, not complete transformations. Understanding y = x + ΔF(x) is essential.
  - Quick check question: Can you explain why skip connections help with gradient flow in deep networks?

- Concept: Johnson-Lindenstrauss lemma and random projections
  - Why needed here: The paper relies on theoretical justification that random projections preserve distances/information in high dimensions, enabling fixed input projections.
  - Quick check question: What does JL lemma guarantee about distances after random projection to sufficiently high dimensions?

- Concept: Pareto frontiers and multi-objective optimization
  - Why needed here: The paper evaluates architectures on performance-parameter trade-offs, not just raw performance. Understanding Pareto optimality is required to interpret results.
  - Quick check question: If model A has higher accuracy than model B but also more parameters, can both be Pareto-optimal?

## Architecture Onboarding

- Component map: Input → Win (dx→dz) → L residual blocks → Wout (dz→output) → Output
- Critical path:
  1. Initialize Win with random matrix (Gaussian or Rademacher)
  2. Stack L residual blocks with dz >> dh
  3. Apply task-specific Wout at output
  4. Train end-to-end with Win optionally frozen

- Design tradeoffs:
  - Larger dz: More expressive skip dimension, higher memory/compute for skip path
  - Smaller dh: Fewer parameters per block, enables deeper networks, but may bottleneck information
  - More blocks (L): Deeper refinement, but diminishing returns (plateau around L=5 in experiments)
  - Fixed vs trainable Win: ~35% parameter reduction, minimal performance loss at large expansion

- Failure signatures:
  - Performance degrades when dh is too small (Figure 6a shows diminishing returns below dh=270)
  - No benefit when dz ≈ dx (insufficient expansion)
  - Learned Win significantly outperforms fixed Win (suggests expansion factor too small or task requires structured projection)

- First 3 experiments:
  1. Replicate MNIST denoising with (dz=784, dh=64, L=5) vs (dz=64, dh=784, L=5) to verify wide-narrow-wide advantage on small scale
  2. Ablation: Train same architecture with fixed vs trainable Win at dz/dx ratios of 2x, 4x, 8x to identify threshold where fixed projection suffices
  3. Depth scaling: Compare (dz=3072, dh=256, L=8) vs (dz=3072, dh=1024, L=2) at matched parameter count to test depth vs width tradeoff

## Open Questions the Paper Calls Out

- Question: Does the wide-narrow-wide design improve efficiency when integrated into Transformer architectures?
  - Basis in paper: The "Future Work" section explicitly proposes extending the intuition to Transformers, suggesting coordinated modifications to self-attention and feed-forward layers to operate at expanded dimensionalities.
  - Why unresolved: The paper evaluates only pure MLP stacks on image tasks; it remains unverified if the architectural benefits persist when combined with self-attention mechanisms in large-scale language or vision models.
  - What evidence would resolve it: Empirical comparisons of standard Transformers versus modified Transformers with high-dimensional residual streams on benchmarks like ImageNet or language modeling tasks.

- Question: Can the observed parameter efficiency be maintained when scaling to high-resolution inputs?
  - Basis in paper: The authors identify limited computational capacity and the prohibitive cost of naive MLPs on high-resolution inputs as a limitation, suggesting integration into architectures like MLP-Mixer as a future direction.
  - Why unresolved: All empirical validation was conducted on low-dimensional datasets (MNIST, ImageNet-32); the behavior of the Hourglass design on high-dimensional visual data remains untested.
  - What evidence would resolve it: Performance analysis of hybrid architectures (e.g., an MLP-Mixer utilizing Hourglass blocks) on high-resolution datasets such as ImageNet-224.

- Question: What is the precise theoretical mechanism linking high-dimensional skip connections to improved incremental refinement?
  - Basis in paper: The authors hypothesize that incremental improvement is more effective in higher dimensions, motivated by theories like Cover's theorem, but rely on empirical Pareto frontier results rather than a formal proof.
  - Why unresolved: While the paper demonstrates empirical gains, it does not formally derive why this specific architectural inversion yields better gradient flow or representation capacity than the conventional design.
  - What evidence would resolve it: A formal theoretical analysis or ablation study isolating the mathematical properties (e.g., linear separability, information preservation) that drive the performance improvement.

## Limitations
- Theoretical justification for fixed random projections relies on asymptotic properties of Johnson-Lindenstrauss lemma without precise characterization of practical thresholds
- ImageNet-32 experiments use only 2 epochs, raising questions about whether advantages persist with longer training
- Architectural search space excludes modern elements like batch normalization placement variations and advanced activation functions

## Confidence

- **High Confidence**: The experimental demonstration that Hourglass architectures achieve better Pareto frontiers on MNIST and ImageNet-32
- **Medium Confidence**: The claim that fixed random input projections can replace learned projections with minimal performance loss
- **Medium Confidence**: The hypothesis that wide-narrow-wide provides more effective incremental refinement than narrow-wide-narrow

## Next Checks

1. **Dimensionality Threshold Validation**: Systematically vary the expansion ratio dz/dx from 2× to 16× while keeping other parameters fixed to empirically determine when fixed random projections become competitive with learned projections.

2. **Extended Training Duration**: Repeat the ImageNet-32 denoising experiments with 20-50 epochs to assess whether the Hourglass advantage persists or diminishes with longer training, particularly for configurations using fixed input projections.

3. **Architectural Component Ablation**: Create controlled experiments that isolate the effect of skip connection dimensionality by testing: (a) learned narrow-wide-narrow with fixed wide-narrow-narrow, (b) random wide-narrow-narrow, and (c) learned wide-narrow-narrow, all at matched parameter counts.