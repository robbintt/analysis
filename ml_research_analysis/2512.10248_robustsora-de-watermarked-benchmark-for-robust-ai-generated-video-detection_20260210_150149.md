---
ver: rpa2
title: 'RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection'
arxiv_id: '2512.10248'
source_url: https://arxiv.org/abs/2512.10248
tags:
- video
- watermark
- videos
- detection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the vulnerability of AI-generated video detectors
  to watermark manipulation, introducing RobustSora, a benchmark designed to evaluate
  watermark robustness. The authors systematically construct a dataset of 6,500 videos
  across four types: Authentic-Clean, Authentic-Spoofed with fake watermarks, Generated-Watermarked,
  and Generated-DeWatermarked.'
---

# RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection

## Quick Facts
- **arXiv ID**: 2512.10248
- **Source URL**: https://arxiv.org/abs/2512.10248
- **Reference count**: 3
- **Primary result**: Introduces RobustSora benchmark to evaluate watermark robustness in AIGC detection, revealing 2-8pp performance variations across architectures when watermarks are manipulated

## Executive Summary
This paper addresses the vulnerability of AI-generated video detectors to watermark manipulation by introducing RobustSora, a comprehensive benchmark designed to evaluate watermark robustness. The benchmark constructs 6,500 videos across four types - Authentic-Clean, Authentic-Spoofed with fake watermarks, Generated-Watermarked, and Generated-DeWatermarked - and introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models reveal performance variations of 2-8pp under watermark manipulation, with transformer-based models showing consistent moderate dependency (6-8pp) and MLLMs exhibiting diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies.

## Method Summary
RobustSora systematically constructs a dataset of 6,500 videos to evaluate watermark robustness in AI-generated video detection. The dataset includes four video types: Authentic-Clean (A-C, 3,000 videos from Vript/DVF/UltraVideo), Generated-Watermarked (G-W, 3,500 from Sora/Sora2/Pika/KLing/Open-Sora2), Generated-DeWatermarked (G-DeW, 3,500 via DiffuEraser), and Authentic-Spoofed (A-S, 3,000 with fake Sora2 watermarks). The benchmark introduces two tasks: Task-I evaluates detection on watermark-removed AI videos (G-DeW), while Task-II assesses false alarm rates on authentic videos with fake watermarks (A-S). Ten models are evaluated including three specialized detectors (DeCoF, NSG-VD, D3), four transformer models (TimeSformer, VideoSwin-T, MViT V2, DuB3D-FF), and three MLLMs (Qwen2.5-VL-3B/7B, Video-LLaVA-7B). Models are trained on A-C + G-W splits with standard augmentation and AdamW optimization.

## Key Results
- Transformer-based models show consistent 6-8pp performance drops when watermarks are removed (Task-I) or spoofed (Task-II)
- MLLMs exhibit more diverse response patterns with drops ranging from 2-8pp, and some models (e.g., Qwen2.5-VL-3B) show counterintuitive improvements on Task-II
- The benchmark reveals that AI-generated video detectors partially rely on watermark patterns rather than exclusively learning generation artifacts
- Watermark-aware training strategies are needed to reduce detector vulnerability to watermark manipulation attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Watermark patterns function as spurious detection signals that partially substitute for learning genuine generation artifacts.
- Mechanism: When models train on watermarked AI-generated videos (G-W), they learn statistical correlations with embedded watermark signatures rather than exclusively learning temporal/spatial inconsistencies unique to generative processes. This creates a shortcut that degrades when watermarks are absent or spoofed.
- Core assumption: Watermark patterns have consistent statistical properties that neural networks can exploit during supervised learning.
- Evidence anchors:
  - [abstract] "detectors may partially rely on these patterns"
  - [Table 2] Transformer-based models show 6-8pp accuracy drops on Task-I (G-DeW) and 7-8pp drops on Task-II (A-S)
  - [corpus] Watermark-based Attribution paper confirms commercial systems deploy watermark-based detection, validating this as a real-world concern

### Mechanism 2
- Claim: Architecture choice determines watermark dependency magnitude and consistency.
- Mechanism: Transformer-based video models (TimeSformer, VideoSwin, MViT V2) process spatiotemporal attention uniformly, leading to consistent watermark feature absorption. MLLMs leverage pretrained vision-language representations that may already encode watermark-agnostic features, creating variable dependency patterns.
- Core assumption: The inductive biases of different architectures cause differential sensitivity to watermark vs. content features.
- Evidence anchors:
  - [Table 2] Transformers show consistent 6-8pp variation; MLLMs show 2-8pp with divergent Task-II behaviors (Qwen2.5-VL-3B improves 3pp while Video-LLaVA drops 6pp)
  - [Discussion] "Transformer-based models exhibit consistent moderate dependency (6-8pp), while MLLM-based methods show more diverse response patterns (2-8pp)"

### Mechanism 3
- Claim: Watermark removal tools introduce confounding visual artifacts that may themselves become detection signals.
- Mechanism: DiffuEraser removes watermarks through inpainting/interpolation, which can leave subtle reconstruction traces. Models may detect these tool-specific artifacts rather than proving they relied on original watermarks.
- Core assumption: The de-watermarking process is not artifact-free and creates its own detectable signature.
- Evidence anchors:
  - [Limitations] "The watermark removal process might inadvertently introduce subtle visual artifacts or modify generation traces, acting as confounding variables"
  - [Figure 2] Shows visual comparison of watermark removal, but no quantitative artifact analysis

## Foundational Learning

- Concept: **Confounding variables in benchmark design**
  - Why needed here: Understanding that Task-I results may reflect removal-tool artifacts rather than pure watermark dependency is essential for correct interpretation
  - Quick check question: Can you explain why a performance drop on de-watermarked videos does not definitively prove the model relied on watermarks?

- Concept: **Spurious correlation in supervised learning**
  - Why needed here: The core finding is that detectors learn watermark shortcuts; understanding shortcut learning explains why this occurs and how to mitigate it
  - Quick check question: What is the difference between a model learning a shortcut feature vs. the intended signal, and how would you detect each?

- Concept: **False alarm rate vs. detection accuracy trade-offs**
  - Why needed here: Task-II specifically measures false alarms (TNR/Acc_real) because spoofing attacks exploit this asymmetric vulnerability
  - Quick check question: Why is Task-II evaluated using Acc_real (TNR) rather than Acc_ai (TPR)?

## Architecture Onboarding

- Component map: A-C/Vript/DVF/UltraVideo → A-S (watermark extraction + overlay); G-W/Sora2/Pika/KLing → G-DeW (DiffuEraser removal)
- Critical path: 1) Replicate A-C + G-W training split (2,400 + 2,800 videos) 2) Establish baseline on Standard Test (600 A-C + 700 G-W) 3) Run Task-I evaluation on G-DeW (700 videos, measure Acc_ai drop) 4) Run Task-II evaluation on A-S (600 videos, measure Acc_real drop)
- Design tradeoffs: Dataset scale (6,500) vs. coverage: Smaller than GenVidBench (143K) but enables controlled watermark manipulation; may limit generalization; Single removal tool vs. ecological validity: DiffuEraser only; may not represent real-world evasion diversity; Single watermark pattern (Sora 2) vs. comprehensiveness: Limits generalization to other generator watermarks
- Failure signatures: Model shows >10pp drop on Task-I → Strong watermark dependency, likely overfitting to watermark patterns; Model shows >10pp drop on Task-II → High spoofing vulnerability, classifier decision boundary aligned with watermark presence; MLLM shows Task-II improvement (like Qwen2.5-VL-3B's +3pp) → May indicate reasoning-based detection less sensitive to visual watermark cues, but requires investigation
- First 3 experiments: 1) Baseline replication: Train DeCoF or MViT V2 on A-C + G-W, verify Standard Test accuracy matches reported values (~0.76-0.80) before proceeding 2) Watermark ablation: Compare models trained on G-W vs. G-DeW to isolate how much training-set watermark presence affects Task-I/II performance 3) Multi-removal-tool test: Apply alternative watermark removal methods beyond DiffuEraser to disentangle tool-artifact effects from true watermark dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent are the performance degradations observed in Task-I attributable to the absence of watermark patterns versus the introduction of visual artifacts caused by the removal tool (DiffuEraser)?
- Basis in paper: [explicit] The authors state in "Limitations and Future Work": "The watermark removal process might inadvertently introduce subtle visual artifacts or modify generation traces, acting as confounding variables."
- Why unresolved: The current experimental design detects the combined effect of watermark removal and tool-induced artifacts, but it does not isolate the specific contribution of the removal artifacts to the drop in detection accuracy.
- What evidence would resolve it: A controlled ablation study comparing detection performance on synthetic videos with "ideal" (lossless) watermark removal versus those processed by DiffuEraser, specifically analyzing feature maps for tool-related noise.

### Open Question 2
- Question: What specific visual or semantic features explain the counterintuitive performance improvement observed in MLLMs (e.g., Qwen2.5-VL-3B) when detecting authentic videos containing spoofed watermarks?
- Basis in paper: [explicit] The paper notes in "Limitations and Future Work" that "The counterintuitive improvements observed in Task-II (e.g., Qwen2.5-VL-3B ↑3pp) require deeper investigation into MLLM decision mechanisms."
- Why unresolved: While the paper documents the phenomenon where some MLLMs better classify real videos when fake watermarks are present, the interpretability analysis required to explain the model's logic is outside the scope of the benchmarking results.
- What evidence would resolve it: Attention visualization and feature attribution analysis (e.g., Grad-CAM) on the MLLM layers to determine if the model ignores the watermark or identifies inconsistencies between the watermark and the high-fidelity real content.

### Open Question 3
- Question: Can the observed watermark dependency (2-8pp) be generalized across different watermark removal techniques and diverse commercial watermarking standards beyond the single tool (DiffuEraser) and single pattern (Sora 2) tested?
- Basis in paper: [explicit] The authors list as a limitation: "We rely on a single removal tool (DiffuEraser) and one watermark pattern (Sora 2), which may not capture the full spectrum of watermark manipulation techniques."
- Why unresolved: The robustness of detectors might vary significantly depending on the complexity of the watermark structure or the specific artifacts left by different removal algorithms (e.g., inpainting vs. diffusion-based erasure).
- What evidence would resolve it: Expanding the benchmark to include videos processed by multiple removal tools (e.g., various inpainting models) and videos containing watermarks from other major generators (e.g., Runway, Stable Video Diffusion).

### Open Question 4
- Question: How can "watermark-aware" training strategies be optimized to decouple detectors from reliance on watermarks without significantly compromising their accuracy on standard watermarked AI content?
- Basis in paper: [inferred] The Discussion and Conclusion state that findings "highlight the need for watermark-aware training strategies," but the paper only evaluates existing models rather than proposing or testing a specific training solution.
- Why unresolved: It is unclear if standard data augmentation (random watermark injection/removal) is sufficient to force models to learn generation traces, or if novel architectural constraints are required to ignore provenance signals.
- What evidence would resolve it: Comparative experiments training detectors on the RobustSora dataset with and without watermark-augmentation techniques, measuring the delta between Task-I (DeWatermarked) and Standard Test performance.

## Limitations
- Single watermark pattern (Sora 2) limits generalization to other generator watermarking schemes
- Single de-watermarking tool (DiffuEraser) cannot capture full spectrum of watermark manipulation techniques
- De-watermarking artifacts may confound Task-I results by introducing tool-specific detection signals
- Limited dataset scale (6,500 videos) compared to larger benchmarks may affect generalization

## Confidence

- **High Confidence**: Transformer models showing consistent 6-8pp performance drops across both tasks - this pattern is robustly observed and architecturally coherent with their uniform spatiotemporal processing.
- **Medium Confidence**: The architectural dependency claim (Transformers vs MLLMs) - while observed patterns exist, external validation is limited and the mechanism requires more comparative evidence.
- **Low Confidence**: Task-I results as pure evidence of watermark dependency - without controlling for DiffuEraser artifacts, these results may overstate true watermark reliance.

## Next Checks

1. **Ablation study on training data**: Train identical models on G-W vs G-DeW datasets to isolate how watermark presence during training affects Task-I/II performance, controlling for evaluation-time manipulation.

2. **Multi-tool de-watermarking analysis**: Apply alternative watermark removal methods (e.g., Inpaint-Anything, custom interpolation) to disentangle DiffuEraser-specific artifacts from general watermark removal effects.

3. **Cross-watermark generalization test**: Create A-S datasets with watermarks from different generators (Pika, KLing) to assess whether Task-II vulnerability generalizes beyond the Sora 2 pattern.