---
ver: rpa2
title: 'KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino'
arxiv_id: '2509.06065'
source_url: https://arxiv.org/abs/2509.06065
tags:
- filipino
- english
- questions
- language
- truthfulqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KatotohananQA, a Filipino translation of
  the English TruthfulQA benchmark, to evaluate the truthfulness of large language
  models (LLMs) in a low-resource language. Using a binary-choice framework, seven
  free-tier proprietary models were assessed, revealing a significant performance
  gap between English and Filipino, with average accuracies of 94.72% and 83.87%,
  respectively.
---

# KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino

## Quick Facts
- arXiv ID: 2509.06065
- Source URL: https://arxiv.org/abs/2509.06065
- Reference count: 18
- Key outcome: KatotohananQA benchmark reveals significant performance gap between English (94.72%) and Filipino (83.87%) on truthfulness evaluation

## Executive Summary
This paper introduces KatotohananQA, a Filipino translation of the English TruthfulQA benchmark, to evaluate the truthfulness of large language models (LLMs) in a low-resource language. Using a binary-choice framework, seven free-tier proprietary models were assessed, revealing a significant performance gap between English and Filipino, with average accuracies of 94.72% and 83.87%, respectively. Newer OpenAI models (GPT-5 and GPT-5 mini) demonstrated strong multilingual robustness, showing minimal performance differences between languages. Results also highlighted disparities across question types, categories, and topics, indicating that certain types of questions are less robust to multilingual transfer. The study underscores the need for broader multilingual evaluation to ensure fairness and reliability in LLM usage.

## Method Summary
The study created a Filipino translation of the TruthfulQA benchmark, consisting of 816 questions across 38 categories and 99 topics. The evaluation used a binary-choice framework where models responded with "YES" or "NO" to questions. Seven free-tier proprietary models were tested on both English and Filipino versions of the benchmark. The study analyzed performance differences across languages, question types, categories, and topics, comparing results between English and Filipino versions to assess multilingual transfer capabilities.

## Key Results
- Significant performance gap between English (94.72% accuracy) and Filipino (83.87% accuracy) versions
- GPT-5 and GPT-5 mini demonstrated strong multilingual robustness with minimal performance differences between languages
- Performance disparities across question types, categories, and topics suggest certain question types are less robust to multilingual transfer

## Why This Works (Mechanism)
The study's approach works by creating a controlled environment to measure truthfulness in LLMs through standardized questions that test whether models can distinguish true from false information. By using a binary-choice framework and comparing performance across languages, the methodology isolates language-specific effects from other factors. The translation of a well-established benchmark ensures comparability while addressing the multilingual evaluation gap in existing literature.

## Foundational Learning
- **Truthfulness benchmarks**: Why needed: To establish standardized evaluation metrics for LLM reliability; Quick check: Verify benchmark questions cover diverse topics and difficulty levels
- **Multilingual evaluation**: Why needed: To ensure LLMs perform consistently across languages and avoid bias; Quick check: Test models on multiple language pairs
- **Translation quality assessment**: Why needed: To distinguish between true language weaknesses and translation artifacts; Quick check: Conduct back-translation verification
- **Binary-choice frameworks**: Why needed: To simplify evaluation and reduce subjective interpretation; Quick check: Ensure questions have clear, unambiguous answers
- **Cross-lingual transfer learning**: Why needed: To understand how knowledge transfers between languages in LLMs; Quick check: Compare performance on similar questions across languages
- **Cultural context adaptation**: Why needed: To ensure questions maintain meaning across cultural boundaries; Quick check: Conduct human evaluation of question equivalence

## Architecture Onboarding
Component map: Translation -> Question Categorization -> Model Input -> Binary Response -> Accuracy Calculation -> Performance Analysis
Critical path: The translation and categorization of questions determines the quality of evaluation, while the binary response format ensures consistent measurement across models.
Design tradeoffs: The study chose professional translation over automated methods for accuracy, but this introduces potential human bias. The binary format simplifies evaluation but may not capture nuanced responses.
Failure signatures: Poor translation quality could artificially depress performance; cultural differences might affect question interpretation; binary format might miss partial truths.
First experiments: 1) Test translation accuracy through back-translation, 2) Validate question equivalence across languages with human evaluators, 3) Compare performance on subset questions to identify specific failure patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Translation quality and cultural equivalence concerns not fully addressed
- Limited sample size of seven models restricts generalizability
- Cannot definitively distinguish between true language weaknesses and translation artifacts

## Confidence
- Core finding of performance gap: High confidence
- Interpretation of results: Medium confidence
- Multilingual robustness claims for GPT-5 models: High confidence

## Next Checks
1. Conduct back-translation of the Filipino questions to verify semantic equivalence with the original English questions
2. Test models on additional low-resource languages to determine if the observed performance gap is specific to Filipino or represents a broader multilingual challenge
3. Perform human evaluation studies to assess whether the questions maintain their intended meaning and difficulty level across languages and cultural contexts