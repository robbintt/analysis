---
ver: rpa2
title: On Stealing Graph Neural Network Models
arxiv_id: '2511.07170'
source_url: https://arxiv.org/abs/2511.07170
tags:
- victim
- query
- encoder
- graph
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of extracting Graph Neural Network\
  \ (GNN) models under strict query limits, a scenario not well studied in prior work.\
  \ The authors propose a novel method that first recovers the encoder backbone locally\
  \ without querying the victim model\u2014using a randomly initialized encoder in\
  \ the inductive setting or self-supervised learning in the transductive setting."
---

# On Stealing Graph Neural Network Models

## Quick Facts
- arXiv ID: 2511.07170
- Source URL: https://arxiv.org/abs/2511.07170
- Reference count: 36
- Primary result: Novel method achieves 91% accuracy on GNN model extraction with only 100 queries, outperforming existing approaches requiring more queries or access to intermediate embeddings

## Executive Summary
This paper addresses the challenge of extracting Graph Neural Network (GNN) models under strict query limits, a scenario not well studied in prior work. The authors propose a novel method that first recovers the encoder backbone locally without querying the victim model—using a randomly initialized encoder in the inductive setting or self-supervised learning in the transductive setting. They then strategically select the most informative queries based on clustering the encoder's node embeddings, and finally train an MLP head on top of the frozen encoder using the selected query responses. Evaluated on eight real-world datasets in both transductive and inductive settings, their approach consistently achieves high accuracy and fidelity under query budgets as low as 100 queries.

## Method Summary
The proposed approach tackles GNN model extraction under hard-label access by decoupling the attack into two stages. First, it recovers the encoder backbone locally without victim queries—using a randomly initialized encoder for inductive settings or self-supervised learning for transductive settings. Second, it employs a query selection strategy based on clustering the encoder's node embeddings to identify the most informative nodes for querying. The final surrogate model is trained by freezing the encoder and training an MLP head using responses from the selected queries. This approach is evaluated on eight real-world datasets in both transductive and inductive settings, achieving high accuracy and fidelity even under strict query budgets.

## Key Results
- Achieves 91% accuracy targeting a SAGE model on the Physics dataset with only 100 queries
- Outperforms existing approaches requiring significantly more queries or access to intermediate embeddings
- Maintains effectiveness even under defense mechanisms that flip prediction labels with 10% probability
- Consistent performance across eight real-world datasets in both transductive and inductive settings

## Why This Works (Mechanism)
The method's effectiveness stems from its two-stage approach that separates encoder recovery from query-based training. By first obtaining a reasonable approximation of the victim's encoder locally (via random initialization or self-supervised learning), the attack can identify the most informative nodes to query through clustering. This targeted query strategy maximizes information gain under strict budget constraints, allowing the MLP head to be trained effectively even with minimal victim interactions. The approach exploits the fact that GNN encoders capture structural patterns that can be approximated independently of the final classification head.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Neural networks designed to operate on graph-structured data by propagating and aggregating information between nodes. Why needed: The entire attack targets GNN models, requiring understanding of how they process graph data.
- **Transductive vs Inductive Learning**: Transductive learning operates on the same graph for training and inference, while inductive learning generalizes to unseen graphs. Why needed: The attack works in both settings with different encoder recovery strategies.
- **Node Embeddings**: Low-dimensional representations of nodes learned by GNNs that capture structural and feature information. Why needed: The query selection strategy relies on clustering these embeddings to identify informative nodes.
- **Hard-Label Model Extraction**: The process of stealing a model using only the predicted class labels, without access to confidence scores or intermediate representations. Why needed: This attack operates under the strictest model extraction threat model.
- **Self-Supervised Learning**: Training methods that create supervisory signals from the data itself without manual labels. Why needed: Used to recover the encoder in the transductive setting without victim queries.
- **Query Selection via Clustering**: The strategy of selecting data points for querying based on their representation in feature space to maximize information gain. Why needed: Enables effective model extraction under strict query budget constraints.

## Architecture Onboarding

### Component Map
Local Encoder (Random/SSL) -> Node Embedding Clustering -> Query Selection -> Victim Querying -> Frozen Encoder + MLP Head Training -> Surrogate Model

### Critical Path
The most critical path for attack success is: Local Encoder Recovery -> Node Embedding Clustering -> Query Selection. If the local encoder fails to capture meaningful structure, the query selection becomes ineffective, and the attack cannot succeed within the query budget.

### Design Tradeoffs
The approach trades query efficiency for pre-attack computation. While existing methods query extensively to directly learn the model, this method invests computational resources upfront to recover the encoder locally, then minimizes victim queries through strategic selection. This makes it suitable for scenarios with strict query limits but higher local computation budgets.

### Failure Signatures
The attack fails when: the local encoder cannot capture the victim's structural patterns (poor random initialization or ineffective self-supervised learning), the node embeddings are not discriminative enough for effective clustering, or the query budget is insufficient even after optimal selection. Poor performance on datasets with highly specialized features or unusual graph structures may indicate these failures.

### First 3 Experiments
1. Compare surrogate model accuracy when using random node selection versus the proposed clustering-based query selection under identical query budgets.
2. Evaluate the impact of different self-supervised learning objectives (e.g., node reconstruction vs. contrastive learning) on transductive encoder recovery quality.
3. Test the attack's success rate when the adversary's auxiliary graph has significant domain shift from the victim's training graph.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can robust defenses be designed for hard-label GNN APIs that prevent model extraction without significantly degrading the model's utility for legitimate users?
**Basis in paper:** [explicit] The authors conclude by highlighting "the urgent need for improved security measures" and note that existing defenses like prediction flipping come at the cost of reduced model accuracy.
**Why unresolved:** The paper demonstrates that the attack succeeds even when the victim flips 10% of labels, and other standard defenses (like noise injection) require access to output probabilities or embeddings, which are unavailable in the hard-label setting studied.
**What evidence would resolve it:** A novel defense mechanism that maintains high victim accuracy (utility) while reducing the fidelity or accuracy of the surrogate model below a usable threshold in this specific threat model.

### Open Question 2
**Question:** Can the proposed query selection strategy be effectively adapted for link prediction or graph classification tasks?
**Basis in paper:** [inferred] The introduction lists link prediction and graph classification as key GNN applications, but the methodology and experiments are restricted exclusively to node classification.
**Why unresolved:** The current query selection relies on clustering node embeddings to find representative nodes. It is unclear how this clustering logic translates to selecting edges (link prediction) or entire graphs/subgraphs (graph classification) for querying.
**What evidence would resolve it:** An extension of the method demonstrating high fidelity in stealing models trained for link prediction or graph classification tasks using similar query budgets.

### Open Question 3
**Question:** How does the performance of the surrogate model degrade when the adversary's auxiliary data distribution differs significantly from the victim's private training data?
**Basis in paper:** [inferred] The threat model in Section 3.1 explicitly assumes the adversary's graph $G_D$ is drawn from the same distribution as the victim's graph $G_V$, and ablation studies (Table 11) only test feature/edge perturbations rather than domain shifts.
**Why unresolved:** The effectiveness of the self-supervised encoder (transductive) and random encoder (inductive) may rely on the semantic proximity of the features. If the domains differ (e.g., training on a citation network to steal a molecular network), the encoder's embeddings might not align, rendering the query selection ineffective.
**What evidence would resolve it:** Experimental results showing attack fidelity when the surrogate is trained on a dataset from a distinct domain (different feature space or graph topology) than the victim model.

## Limitations
- Evaluation focuses on relatively small graph datasets, with scalability to larger, more complex graphs remaining uncertain
- The claim that the method "remains effective" under defense mechanisms that flip prediction labels with 10% probability needs clearer definition of what "effective" means
- While the approach works in both transductive and inductive settings, the comparison between these settings could be more explicit

## Confidence
- **High** confidence in core methodology and experimental results on tested datasets
- **Medium** confidence in claims about effectiveness under defenses and generalizability to larger-scale graphs
- **Low** confidence in claims about real-world applicability without additional validation on industry-scale graphs or more sophisticated defense mechanisms

## Next Checks
1. Test the approach on larger-scale graph datasets with millions of nodes to assess scalability and identify potential bottlenecks.
2. Evaluate against more advanced defense mechanisms beyond simple label flipping, such as gradient-based defenses or differential privacy approaches.
3. Conduct ablation studies to quantify the individual contributions of the encoder recovery step and query selection strategy to overall success.