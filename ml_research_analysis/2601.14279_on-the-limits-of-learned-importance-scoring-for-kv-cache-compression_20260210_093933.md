---
ver: rpa2
title: On the Limits of Learned Importance Scoring for KV Cache Compression
arxiv_id: '2601.14279'
source_url: https://arxiv.org/abs/2601.14279
tags:
- importance
- learned
- arxiv
- cache
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rigorously tests whether learned importance scoring
  for KV cache compression outperforms simple heuristics. Despite designing a sophisticated
  1.7M parameter Speculative Importance Prediction (SIP) model with multi-horizon
  lookahead and cross-attention, the learned approach does not outperform simple baselines
  across 5 seeds, 4 retention levels, and 3 tasks.
---

# On the Limits of Learned Importance Scoring for KV Cache Compression
## Quick Facts
- arXiv ID: 2601.14279
- Source URL: https://arxiv.org/abs/2601.14279
- Authors: Brady Steele
- Reference count: 13
- Primary result: Learned importance scoring for KV cache compression does not outperform simple heuristics

## Executive Summary
This paper rigorously tests whether learned importance scoring for KV cache compression provides benefits over simple heuristics. Despite designing a sophisticated 1.7M parameter Speculative Importance Prediction (SIP) model with multi-horizon lookahead and cross-attention, the learned approach fails to outperform simple baselines across 5 seeds, 4 retention levels, and 3 tasks. The paper finds that position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches, and prefill attention provides equivalent signal to complex learned scorers. The primary implication is that simple heuristics should be preferred for KV cache compression, with position-based methods best for aggressive compression and prefill attention for moderate compression.

## Method Summary
The paper proposes a learned importance scoring approach using a Speculative Importance Prediction (SIP) model that predicts the importance of each token in the KV cache for future token prediction. The SIP model uses a decoder-only transformer architecture with 1.7M parameters, taking as input the current hidden state, KV representations, and an attention mask to capture temporal dependencies. It predicts importance scores for tokens at various lookahead distances (1-5 tokens ahead) and is trained using a cross-entropy loss on the attention weights. The model is evaluated against multiple baselines including random retention, keep-first/last strategies, and prefill attention, across 4 retention levels (5%, 10%, 25%, 50%) and 3 tasks (Wikitext-2, WMT, OpenWebText).

## Key Results
- Learned importance scoring (SIP) does not outperform simple heuristics across 5 seeds, 4 retention levels, and 3 tasks
- Position-based heuristics (keep first 4 + last N tokens) perform competitively or better than learned approaches
- Prefill attention provides equivalent signal to complex learned scorers for importance prediction

## Why This Works (Mechanism)
The paper finds that KV representations contain limited marginal information beyond position and prefill attention for importance prediction. Circular dependence between future queries and generation trajectories may make non-query-aware learned scoring inherently difficult. The KV cache format used (head-concatenated, as in LLaMA models) may not provide optimal signal for learned importance scoring.

## Foundational Learning
- **KV Cache Compression**: Reducing memory footprint by selectively retaining tokens in key/value cache; needed because full context windows are memory-intensive
- **Importance Scoring**: Predicting which tokens are most valuable for future predictions; quick check: does learned scoring outperform heuristics?
- **Attention Mechanisms**: Self-attention operations that scale quadratically with sequence length; quick check: does prefill attention capture sufficient signal?
- **Position Encoding**: Positional information that helps models understand token order; quick check: do position-based heuristics perform well?

## Architecture Onboarding
**Component Map**: Input Hidden State -> SIP Model -> Importance Scores -> Token Retention Decision -> Compressed KV Cache

**Critical Path**: The SIP model takes current hidden states and KV representations, applies cross-attention to understand token relationships, predicts importance scores for lookahead distances 1-5, and these scores determine which tokens to retain.

**Design Tradeoffs**: Head-concatenated KV format (used in LLaMA) vs head-separated formats (used in OPT); learned scoring complexity (1.7M parameters) vs heuristic simplicity; lookahead distance vs computational overhead.

**Failure Signatures**: Learned approaches matching or underperforming simple heuristics; high variance across seeds; limited improvement from increasing model complexity.

**First Experiments**: 1) Compare learned vs heuristic retention at 25% level on Wikitext-2; 2) Ablate position information from KV representations; 3) Test prefill attention as importance signal.

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on LLaMA-style head-concatenated KV cache format may not generalize to other architectures
- Evaluation focuses on perplexity, may not capture all aspects of generation quality
- Only evaluates greedy decoding, not sampling-based methods which may be more sensitive to cache compression

## Confidence
- **High confidence**: Simple heuristics perform competitively with learned approaches across multiple tasks and retention levels
- **Medium confidence**: KV representations contain limited marginal information beyond position and prefill attention
- **Low confidence**: Learned importance scoring is fundamentally limited for KV cache compression

## Next Checks
1. Test learned importance scoring with head-separated KV cache formats (OPT-style) and other model architectures
2. Evaluate compressed caches using comprehensive generation metrics (factuality, coherence, relevance) rather than perplexity alone
3. Repeat evaluation with temperature-based sampling (top-k, nucleus sampling) to assess learned scoring value