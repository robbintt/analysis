---
ver: rpa2
title: Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal
  LLMs
arxiv_id: '2506.17782'
source_url: https://arxiv.org/abs/2506.17782
tags:
- relevance
- retrieval
- judgments
- article
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to expand relevance judgments for
  medical case-based retrieval using multimodal large language models. The approach
  employs Gemini 1.5 Pro to automatically assess relevance between medical articles
  and patient cases, using a structured prompting strategy with binary scoring.
---

# Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2506.17782
- **Source URL**: https://arxiv.org/abs/2506.17782
- **Reference count**: 40
- **Primary result**: Achieved Cohen's Kappa of 0.6 using multimodal LLMs to expand medical IR relevance judgments from 15,028 to 558,653 annotations

## Executive Summary
This paper presents a method to expand relevance judgments for medical case-based retrieval using multimodal large language models. The approach employs Gemini 1.5 Pro to automatically assess relevance between medical articles and patient cases, using a structured prompting strategy with binary scoring. The method achieved a Cohen's Kappa of 0.6, comparable to inter-annotator agreement, when compared against human judgments. Starting from 15,028 manual judgments, the MLLM-based approach expanded the dataset to 558,653 judgments, increasing relevant annotations from 709 to 5,950. On average, each query received 15,398 new annotations, with 99% being non-relevant. The study demonstrates the potential of multimodal LLMs to scale relevance judgment collection for medical IR evaluation, offering a promising direction for supporting retrieval evaluation in multimodal contexts.

## Method Summary
The method uses Gemini 1.5 Pro with a 4-part structured prompt: system instructions defining task and evaluation criteria, case presentation with description and images, few-shot example of a relevant article, and article evaluation prompt. Binary output (0 or 1) is used for pointwise relevance judgments. The approach starts with ImageCLEFmed 2013 dataset containing 74,654 articles, 306,538 images, 35 patient cases, and 15,028 original manual judgments. The MLLM is first evaluated on a curated test subset to optimize prompt structure, achieving 0.6 Cohen's Kappa against human judgments. The best-performing prompt is then applied to unjudged articles for expansion, with safety-filtered content conservatively marked as non-relevant.

## Key Results
- Achieved Cohen's Kappa of 0.6 between MLLM and human judgments, comparable to inter-annotator agreement
- Expanded relevance judgments from 15,028 to 558,653 annotations (37x increase)
- Increased relevant annotations from 709 to 5,950 across the dataset
- Average of 15,398 new annotations per query, with 99% being non-relevant

## Why This Works (Mechanism)

### Mechanism 1: Structured Prompting Reduces Ambiguity in Relevance Assessment
A multi-part prompt structure with explicit criteria improves reliability by constraining the model's interpretation. Separating system instructions, case presentation, and evaluation into distinct prompts establishes a consistent evaluative context. Explicit criteria and binary scoring reduce possible outputs, limiting subjectivity. The MLLM must consistently interpret medical terminology and visual information across prompts to mirror human clinical reasoning.

### Mechanism 2: Few-Shot In-Context Learning Anchors Relevance Standards
Providing a single relevant example per query topic improves inter-annotator agreement by calibrating the model to specific domain relevance standards. The example acts as an in-context reference point, allowing the model to infer implicit criteria for "relevance" in medical case-based retrieval. The model must generalize from the example to new articles without introducing bias.

### Mechanism 3: Multimodal Input Enables Clinically Realistic Assessment
Using an MLLM capable of processing both text and images is essential since clinical relevance judgments depend on both modalities. The model integrates textual case descriptions with visual evidence from both queries and documents. The MLLM must have sufficient visual processing capabilities to interpret medical imagery meaningfully, not just surface-level image-text correlations.

## Foundational Learning

- **Concept: Cohen's Kappa (Inter-Annotator Agreement)**
  - Why needed here: Primary metric for evaluating MLLM judge performance; Kappa of 0.6 indicates "substantial agreement" comparable to human inter-annotator agreement
  - Quick check question: What is the maximum possible value for Cohen's Kappa, and what does a value of 0 represent?

- **Concept: Pooling in Information Retrieval (IR)**
  - Why needed here: Paper frames contribution as solution to limitations of traditional pooling, which creates "only partially labeled datasets"
  - Quick check question: Why does relying solely on pooling potentially create a biased evaluation dataset for modern retrieval systems?

- **Concept: MLLM-as-a-Judge Framework**
  - Why needed here: Core methodological contribution; understanding "pointwise, score-based" evaluation paradigm is critical for implementation
  - Quick check question: In the "pointwise" evaluation paradigm described, does the model compare two candidate documents at once, or assess a single document in isolation?

## Architecture Onboarding

- **Component map**: Source Data (ImageCLEFmed 2013) -> Curation & Sampling (test subset creation) -> Prompt Engineering Module (4-part structure) -> The Judge (Gemini 1.5 Pro) -> Aggregation & Expansion (qrels file creation)

- **Critical path**: 1) Define evaluation criteria and draft 4-part prompt template, 2) Run MLLM on test subset and calculate Cohen's Kappa against human judgments, 3) Iterate on prompt structure to maximize Kappa, 4) Apply finalized prompt to full set of candidate articles to generate expanded relevance judgments

- **Design tradeoffs**:
  - Prompt Structure vs. Context Window: Separating prompts improves clarity but increases API calls
  - Few-Shot Example Selection: Ground-truth example improves performance but risks data leakage
  - Conservative Safety Filter Handling: Marking blocked content as "not relevant" trades recall for automated processing

- **Failure signatures**:
  - Low Agreement (Kappa < 0.4): Prompt fails to guide model effectively
  - All-Zeros Output: Model defaulting to "not relevant" score
  - Safety Filter Triggers: High rate of blocked prompts halts expansion process

- **First 3 experiments**:
  1. Baseline Kappa Measurement: Run MLLM with simple zero-shot prompt on test subset
  2. Ablation Study on Prompts: Compare single-prompt vs. separate system/user vs. system/user with few-shot
  3. Full Expansion & Distribution Analysis: Apply best prompt to unjudged articles and analyze label distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can incorporating original task guidelines provided to human assessors into the prompt further improve MLLM agreement beyond achieved Cohen's Kappa of 0.6?
- **Basis in paper**: Authors state in conclusion that "having access to the original task guidelines... could enhance our approach"
- **Why unresolved**: Current study relied on iteratively refined prompts but lacked specific instructions originally given to human clinicians
- **What evidence would resolve it**: Ablation study comparing current prompt performance against setup including exact textual guidelines used by ImageCLEFmed 2013 assessors

### Open Question 2
- **Question**: How does Gemini 1.5 Pro performance compare to other state-of-the-art multimodal models or those fine-tuned for medical domain?
- **Basis in paper**: Conclusion explicitly suggests "experimenting with different MLLMs, including those fine-tuned to medical domain" as necessary future step
- **Why unresolved**: Study utilized single model, leaving generalizability and relative performance across different model architectures untested
- **What evidence would resolve it**: Comparative evaluation using same dataset and prompts across multiple MLLMs to benchmark Kappa scores

### Open Question 3
- **Question**: To what extent did model's safety filters bias expanded dataset by automatically marking valid medical content as non-relevant?
- **Basis in paper**: Authors note 1,195 judgments classified as "not relevant" because model flagged prompts as prohibited content
- **Why unresolved**: Unclear if these instances were truly non-relevant or contained sensitive medical imagery triggering false positives
- **What evidence would resolve it**: Manual review of 1,195 filtered items to determine ground-truth relevance compared to model's forced classification

## Limitations
- Cohen's Kappa of 0.6 represents only moderate reliability for precise clinical judgment tasks
- Conservative handling of safety-filtered content may introduce systematic false negatives
- Single-example few-shot approach risks overfitting to specific relevance patterns without robust validation

## Confidence
- **High confidence**: Methodological framework for prompt engineering and structured evaluation is clearly articulated and reproducible; expansion scale is verifiable
- **Medium confidence**: Kappa statistic interpretation and comparison to human agreement is reasonable but requires domain context; few-shot example assumption is supported but not rigorously validated
- **Low confidence**: Clinical validity of MLLM visual processing for medical images remains unproven; conservative bias from safety filters is acknowledged but not quantified

## Next Checks
1. **Visual Comprehension Validation**: Design ablation study comparing MLLM judgments with and without medical images to empirically verify whether model genuinely integrates visual information or merely correlates image presence with relevance

2. **Safety Filter Impact Analysis**: Quantify proportion of blocked prompts and conduct sensitivity analysis to determine how conservative labeling affects precision-recall tradeoff in expanded judgment set

3. **Multi-Example Few-Shot Evaluation**: Systematically test impact of using multiple few-shot examples per topic (2, 3, 4+) to assess whether single-example approach is optimal or introduces overfitting