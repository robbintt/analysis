---
ver: rpa2
title: 'Auditing Human Decision-Making in High-Stakes Environments via Prescriptive
  AI: A Stress-Test on Real-Time Tactical Management'
arxiv_id: '2512.04480'
source_url: https://arxiv.org/abs/2512.04480
tags:
- human
- decision
- prescriptive
- system
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Prescriptive AI framework designed to audit
  human decision-making in real-time, high-stakes environments by decoupling decision
  quality from stochastic outcomes. Using elite soccer as a test case, the system
  detects latent performance risks (e.g., decision latency, status quo bias) through
  role-aware, interpretable fuzzy logic.
---

# Auditing Human Decision-Making in High-Stakes Environments via Prescriptive AI: A Stress-Test on Real-Time Tactical Management

## Quick Facts
- **arXiv ID:** 2512.04480
- **Source URL:** https://arxiv.org/abs/2512.04480
- **Reference count:** 15
- **Primary result:** Introduces Prescriptive AI framework to audit human decisions in real-time by decoupling decision quality from outcomes, detecting latent risks (latency, status quo bias) via role-aware fuzzy logic

## Executive Summary
This paper introduces a Prescriptive AI framework designed to audit human decision-making in real-time, high-stakes environments by decoupling decision quality from stochastic outcomes. Using elite soccer as a test case, the system detects latent performance risks (e.g., decision latency, status quo bias) through role-aware, interpretable fuzzy logic. Applied to 2018 FIFA World Cup data, the system identifies critical tactical vulnerabilities missed by human experts—such as delayed substitutions following salient positive events (e.g., an assist) and sustained defensive exposure. Unlike predictive models that inherit human biases, the framework functions as a normative auditor, surfacing decision-relevant risk before failure. This establishes Prescriptive AI as a paradigm for human-AI interaction that prioritizes epistemic accountability over predictive mimicry in safety-critical domains.

## Method Summary
The framework uses a three-stage preprocessing pipeline to generate temporal performance metrics from soccer event data, then applies role-aware normalization and a Mamdani fuzzy inference system to compute substitution priority scores. Key innovations include replacing cumulative sum metrics with cumulative mean percentiles to eliminate exposure bias, and using interpretable fuzzy rules to detect decision-relevant risks like latency and status quo bias. The system was implemented in Python using scikit-fuzzy and validated on 2018 FIFA World Cup matches, comparing system outputs against actual human substitution decisions to quantify decision latency gaps.

## Key Results
- System detects critical tactical vulnerabilities missed by human experts, including delayed substitutions following salient positive events
- Identifies "Fagner Paradox" where system flags maximum risk but human retains player, validating detection of status quo bias
- Demonstrates 50-minute decision latency in Brazil vs Belgium match, where system flagged risk immediately but human action was delayed
- Framework functions as normative auditor rather than predictor, surfacing risk before failure

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Mean Performance Metric (Exposure Bias Elimination)
- **Claim:** A cumulative mean with role-aware normalization exposes performance decay invisible to cumulative sum metrics
- **Mechanism:** Replaces additive scoring (a low-pass filter that inflates scores for longer play time) with a cumulative mean percentile normalized by tactical role. This allows the performance curve to reflect true temporal variability and negative momentum, functioning as a "high-pass audit" (Section 2.2)
- **Core assumption:** Performance decay is detectable through normalized per-slice contributions, and role-based normalization provides a fair baseline
- **Evidence anchors:**
  - [abstract] "decoupling decision quality from stochastic outcomes, we quantify 'decision latency' and status quo bias"
  - [section 2.2] "By calculating the average contribution up to time t and normalizing it against the historical distribution of the agent's specific tactical role, our metric functions as a high-pass audit"
  - [corpus] Weak/missing: Corpus papers focus on LLM biases and general high-stakes decision making; one (SimBank) touches on prescriptive monitoring but without this metric
- **Break condition:** Breaks if performance cannot be meaningfully normalized by role (e.g., highly fluid tactical systems) or if slice granularity (5 min) is too coarse for rapid shifts

### Mechanism 2: Fuzzy Rule-Based Auditing Layer (Bias Auditing)
- **Claim:** A role-aware fuzzy inference system (FIS) surfaces decision-relevant risks that humans miss due to cognitive biases
- **Mechanism:** The FIS takes temporally-resolved, normalized performance and contextual signals (age, cards, goals/assists, tactical role) as inputs. It applies interpretable linguistic rules (e.g., "IF Defender AND YellowCard Yes THEN Modifier = Medium Positive") to compute a priority score. This symbolic layer acts as an auditor by mapping observed states to normative risk categories
- **Core assumption:** Human cognitive biases (status quo bias, outcome bias) are systematic and can be counteracted by explicit, interpretable rules focused on state-based risk
- **Evidence anchors:**
  - [abstract] "role-aware, interpretable fuzzy logic... detects latent performance risks (e.g., decision latency, status quo bias)"
  - [section 2.3.2] "The most significant finding is the disagreement regarding right-back Fagner... the system assigned him Maximum Priority (100.0)... validates the system's 'Critical Risk' diagnosis"
  - [corpus] Partially supported: Papers on high-stakes human-AI collaboration discuss trust; "Large Language Newsvendor" examines biases in LLMs, indirectly supporting focus on bias but not the fuzzy mechanism
- **Break condition:** Underperforms if the rule base is incomplete or poorly calibrated, or if inputs are too noisy for reliable fuzzy inference

### Mechanism 3: Decoupling Decision Quality from Outcomes (Normative Evaluation)
- **Claim:** Evaluating decisions based on epistemic justification at time t, rather than stochastic outcomes at time t+n, enables auditing of cognitive biases like outcome bias
- **Mechanism:** Adopts a decision-theoretic view where decision quality is a property of the epistemic update at the moment of commitment. The system assesses whether an action (e.g., substitution) is justified given the risk state at that time, independent of later results. Case studies (e.g., Lukaku) show humans delaying action due to salient positive events (an assist), which the system flags as risk
- **Core assumption:** Decision quality can be defined normatively based on available information, and retrospective outcome evaluation is fundamentally biased
- **Evidence anchors:**
  - [abstract] "delayed substitutions following salient positive events (e.g., an assist)"
  - [section 2.3.3] "This 50-minute latency demonstrates how prescriptive systems can decouple decision quality from 'highlight moments'"
  - [corpus] Supported broadly: "Large Language Newsvendor" and "Incorporating Cognitive Biases into RL" explicitly study cognitive biases, though not this decoupling mechanism
- **Break condition:** Breaks if no clear distinction exists between controllable risk states and stochastic outcomes, or if key variables are unobservable at decision time

## Foundational Learning

- **Fuzzy Logic and Membership Functions**
  - Why needed here: The system uses fuzzy inference as its core symbolic reasoning engine to handle uncertainty and imprecision in performance signals
  - Quick check question: Can you explain how a triangular membership function differs from a trapezoidal one, and why "Low" performance uses a trapezoid [0, 0, 0.10, 0.35]?

- **Role-Aware Normalization**
  - Why needed here: Performance metrics must be normalized against the historical distribution of each tactical role to ensure fair comparison (e.g., a defender vs. a forward)
  - Quick check question: If raw performance scores are normalized within each (match, position) group, what happens to a defender's score if all defenders in that match perform poorly?

- **Outcome Bias vs. Process Validity**
  - Why needed here: The framework's core philosophical stance is that decision quality must be evaluated based on information available at time t, not on results at time t+n
  - Quick check question: A coach substitutes a star player and the team loses. Is this automatically a bad decision under the paper's framework? Why or why not?

## Architecture Onboarding

- **Component map:** Raw event logs → Temporal slicing (5-min) → Cumulative mean with role-aware percentile → Fuzzy rule evaluation → Substitution Priority (P_final)
- **Critical path:** Raw event logs → Temporal slicing (5-min) → Cumulative mean with role-aware percentile → Fuzzy rule evaluation → Substitution Priority (P_final)
- **Design tradeoffs:**
  - Static fuzzy rules (interpretable but may require recalibration for different tactical philosophies) vs. adaptive/neuro-fuzzy (more flexible but less transparent)
  - 5-minute slice granularity (balances noise and responsiveness) vs. finer resolution (more data, more noise)
  - Using "minutes played" as a fatigue proxy (simple, available) vs. biometric telemetry (more accurate, not in current data)
- **Failure signatures:**
  - "Boiling frog" false negatives: gradual decay not caught if slices are too coarse or rules are mis-tuned
  - "Salience masking" false negatives: a single highlight event (goal/assist) over-influences human decision; the system should counteract this
  - "Fagner Paradox" disagreement: system flags critical risk but human retains player—this is a feature, not a bug, exposing status quo bias
- **First 3 experiments:**
  1. **Metric validation:** Compute both cumulative sum and cumulative mean for a set of players with known performance drops (e.g., after 70' fatigue). Confirm the mean metric shows decline where the sum does not
  2. **Rule base audit:** Walk through the rule base with domain experts. For each rule, validate the antecedent thresholds and consequent outputs against their tactical intuition
  3. **Case study replication:** Replicate the Brazil-Belgium analysis on a different match. Compare system priority scores with actual substitutions and post-match expert commentary. Quantify "decision latency" as the time difference between system flagging (>90) and human action

## Open Questions the Paper Calls Out

- **Can integrating real-time biometric telemetry replace temporal proxies to more accurately model fatigue?**
  - Basis in paper: [explicit] The authors explicitly identify the "Integration of Biometric Telemetry" as a future research direction to address the limitations of using "minutes played" as a fatigue proxy
  - Why unresolved: The current model assumes quasi-linear workload accumulation, failing to capture inter-individual physiological differences or variations in match intensity
  - What evidence would resolve it: Validation of the system using high-resolution wearable data to correlate physiological thresholds with the fuzzy system's risk triggers

- **Can an Adaptive Neuro-Fuzzy Inference System maintain interpretability while generalizing across tactical philosophies?**
  - Basis in paper: [explicit] The paper suggests developing ANFIS to allow dynamic calibration of membership functions, addressing the limitation that static rules may not generalize across different coaching styles
  - Why unresolved: Current static heuristics require manual recalibration for different teams, and it is unclear if machine learning calibration would compromise the "structural requirement" of interpretability
  - What evidence would resolve it: A comparative study showing ANFIS adapting rule weights for distinct tactical profiles while retaining the ability to provide linguistic justifications

- **Can the framework prescribe specific replacement profiles based on detected tactical deficiencies?**
  - Basis in paper: [explicit] The authors propose extending the system to recommend "how to intervene" via tactical profile matching, moving beyond the current binary "who/when" substitution priority
  - Why unresolved: The current system outputs a risk score for active players but lacks the logic to map detected gaps (e.g., defensive exposure) to the attributes of available bench players
  - What evidence would resolve it: An extension of the algorithm that successfully correlates specific performance risks with the skill profiles of potential substitutes

## Limitations

- The framework's generalizability beyond soccer remains untested; validation is confined to a single sport with unique substitution dynamics
- Rule base calibration is manual and sport-specific; no systematic procedure for adapting to different tactical philosophies or sports is provided
- The 5-minute slice granularity represents a design tradeoff that may miss rapid performance shifts in faster-paced environments
- The system assumes observable performance signals are sufficient proxies for decision quality, which may not hold in domains with different observability constraints

## Confidence

- **High Confidence:** The core premise that decision quality should be evaluated independently of stochastic outcomes, and the general architecture of fuzzy rule-based auditing layered on normalized performance metrics
- **Medium Confidence:** The specific implementation details (5-minute slicing, α = 0.25, rule formulations), which are technically specified but may require sport-specific calibration
- **Low Confidence:** Claims about the framework's applicability to non-sport domains without empirical validation in those contexts

## Next Checks

1. **Cross-Sport Validation:** Apply the framework to a different team sport with substitution dynamics (e.g., basketball or American football) and compare detection accuracy against domain experts

2. **Temporal Sensitivity Analysis:** Vary slice granularity (1, 3, 5, 10 minutes) and measure impact on decision latency detection rates and false positive/negative ratios

3. **Blind Expert Validation:** Present system-identified risk cases to domain experts without revealing system outputs, measuring agreement rates and decision changes before/after system consultation