---
ver: rpa2
title: 'MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification
  for Multilingual Hallucination Detection'
arxiv_id: '2505.20880'
source_url: https://arxiv.org/abs/2505.20880
tags:
- span
- hallucination
- spans
- verification
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Our system addresses hallucination detection in multilingual LLM-generated
  text using a hybrid approach combining task-specific prompt engineering with an
  LLM ensemble verification mechanism. We implement a multi-stage pipeline where one
  model extracts potential hallucination spans while three independent LLMs adjudicate
  their validity through probability-based voting, simulating human annotation workflows.
---

# MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection

## Quick Facts
- arXiv ID: 2505.20880
- Source URL: https://arxiv.org/abs/2505.20880
- Reference count: 13
- Primary result: Ranked 1st in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French for multilingual hallucination detection

## Executive Summary
This paper presents a hybrid approach for multilingual hallucination detection in LLM-generated text using task-specific prompt engineering combined with an LLM ensemble verification mechanism. The system implements a multi-stage pipeline where one model extracts potential hallucination spans while three independent LLMs adjudicate their validity through probability-based voting. Fuzzy matching refines span alignment to ground truth annotations. The approach demonstrates that ensemble verification with majority voting and post-processing refinement effectively reduces model bias and improves detection accuracy across diverse languages.

## Method Summary
The system employs a four-LLM ensemble (Gemini-2.0-Flash-Exp, Qwen-2.5-Max, GPT-4o, DeepSeek-V3) where one model extracts candidate hallucination spans from question-answer pairs, while three adjudicator models independently score each span's hallucination probability. Final labels are assigned via averaged probability with threshold τ=0.7. The process rotates so each model serves as span extractor once while the other three adjudicate. Post-processing applies fuzzy matching (Levenshtein distance, threshold 0.9) to refine span boundaries. This approach avoids traditional supervised fine-tuning, instead using weak supervision through prompt engineering based on annotation guidelines.

## Key Results
- Achieved top rankings across multiple languages: 1st in Arabic and Basque, 2nd in German/Swedish/Finnish, 3rd in Czech/Farsi/French
- Demonstrates effectiveness of ensemble verification with majority voting (threshold 0.7) for reducing model bias
- Shows fuzzy matching improves span alignment precision by correcting LLM boundary errors
- Performance varies significantly across languages, with morphologically rich languages presenting greater challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM ensemble adjudication with iterative role rotation reduces single-model bias in hallucination span detection.
- Mechanism: One LLM extracts candidate hallucination spans while three other independent LLMs adjudicate validity by assigning probabilities. The process rotates so each model serves as span extractor once while the other three adjudicate. Final labels are assigned via averaged probability with threshold τ=0.7.
- Core assumption: Errors made by one model (missed hallucinations or false positives) will be caught by adjudicators with different error patterns and biases.
- Evidence anchors: [abstract] "three independent LLMs adjudicate their validity through probability-based voting"; [section 3.3] "This iterative model selection ensures robustness by reducing individual model biases and leveraging diverse perspectives from different LLMs"

### Mechanism 2
- Claim: Fuzzy matching post-processing improves span alignment precision by correcting LLM boundary errors.
- Mechanism: After span extraction, Levenshtein-distance-based fuzzy matching (threshold 0.9) refines predicted spans to align with ground truth, correcting for minor LLM inconsistencies like capitalization, spacing, or boundary mismatches.
- Core assumption: LLMs correctly identify hallucination presence but are imprecise at character-level boundaries.
- Evidence anchors: [abstract] "Fuzzy matching refines span alignment to ground truth annotations"; [section 3.4] "LLMs frequently introduce minor inconsistencies in span extraction, such as variations in capitalization, extra spaces, or incomplete word boundaries"

### Mechanism 3
- Claim: Prompt engineering based on annotation guidelines provides effective weak supervision without fine-tuning.
- Mechanism: The system analyzes validation data to distill human annotator instructions into a few-shot prompt, which guides an LLM to generate initial candidate spans. This replaces traditional supervised training.
- Core assumption: Advanced LLMs can perform accurate span extraction in a few-shot setting when given detailed instructions matching the task's annotation schema.
- Evidence anchors: [abstract] "combining task-specific prompt engineering with an LLM ensemble verification mechanism"; [section 3.1] "We analyzed the validation dataset to extract annotator instructions and identify patterns, enabling the construction of a fine-grained prompt with few-shot examples"

## Foundational Learning

- **Concept: Span-Level Hallucination Detection**
  - Why needed here: This is the core problem definition. Unlike binary classification ("is this hallucinated?"), the task requires identifying precise character sequences constituting hallucinations, critical for downstream correction.
  - Quick check question: Why is detecting the exact span of a hallucination more challenging and useful than detecting its presence alone?

- **Concept: Weak Supervision / Weak Labeling**
  - Why needed here: The system avoids traditional supervised fine-tuning, instead using a prompted LLM to generate noisy initial labels. Understanding this distinction is essential to grasp the pipeline's data source.
  - Quick check question: How does using a prompted LLM for initial extraction differ from standard supervised fine-tuning?

- **Concept: Ensemble Adjudication**
  - Why needed here: This is the paper's primary reliability mechanism. Understanding why combining multiple models reduces variance and bias is essential to appreciate the multi-stage architecture.
  - Quick check question: What are the two distinct roles an LLM plays in this architecture, and how do their responsibilities differ?

## Architecture Onboarding

- **Component map:**
  Prompt Engineering Module -> Span Extractor (SEM) -> Voting Adjudicators (VAMs) -> Consensus-Based Labeling (CBL) -> Post-Processing Module

- **Critical path:**
  1. Construct few-shot prompt from validation data annotation guidelines
  2. Select one LLM as Span Extractor
  3. SEM processes question-answer pairs → outputs candidate spans S = {s₁, s₂, ..., sₖ}
  4. Three adjudicator LLMs independently score each span's hallucination probability
  5. CBL averages scores (pᵢ ≥ 0.7) → outputs verified spans
  6. Post-processor applies fuzzy matching → final aligned hallucination spans

- **Design tradeoffs:**
  - **Cost/Latency vs. Accuracy:** Running 4 LLMs iteratively (4 extraction runs × 3 adjudicators each = 12 adjudications per sample) is computationally expensive vs. single-model approaches
  - **Threshold Selection (τ=0.7):** Lower thresholds (0.5) increase recall but raise false positives; higher thresholds (0.8) miss subtle hallucinations. Authors empirically found 0.7 optimal for this task.
  - **Model Selection:** Vectara Hallucination Leaderboard guided selection toward models with strong factual accuracy, but this may not generalize to all hallucination types

- **Failure signatures:**
  - **Morphologically Rich Language Degradation:** Performance drops in languages with complex morphology and annotation inconsistency (e.g., Czech IoU=0.507 vs. Italian IoU=0.736)
  - **High Annotator Variability Collapse:** English performance degraded (IoU=0.531, rank 6/44) due to up to 12 annotators per sample creating inconsistent ground truth
  - **Ensemble Unanimous Failure:** If all four models share a systematic error pattern, adjudication cannot correct it
  - **Boundary Precision Loss:** Without fuzzy matching, predicted spans may have correct semantic content but misaligned character boundaries

- **First 3 experiments:**
  1. **Ablation on Ensemble Size:** Run pipeline with single-model self-verification vs. full 4-model ensemble. Measure IoU/Correlation delta to quantify ensemble benefit.
  2. **Threshold Sensitivity Analysis:** Sweep τ from 0.5 to 0.9 on validation set. Plot Precision-Recall tradeoff to validate 0.7 optimality claim.
  3. **Fuzzy Matching Ablation:** Run full pipeline with fuzzy matching disabled. Compare IoU scores to isolate post-processing contribution to span alignment accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating external knowledge sources (e.g., knowledge graphs, retrieval-augmented generation) improve hallucination verification accuracy beyond the current LLM-ensemble-only approach?
- Basis in paper: [explicit] Conclusion explicitly states: "Future work could focus on integrating external knowledge for hallucination verification"
- Why unresolved: Current system relies entirely on LLM-based probability voting without grounding in verified knowledge bases
- What evidence would resolve it: Comparative evaluation of knowledge-grounded verification vs. ensemble-only approach on Mu-SHROOM test set

### Open Question 2
- Question: Does task-specific fine-tuning of LLMs improve alignment with human annotations beyond what prompt engineering and ensemble verification achieve?
- Basis in paper: [explicit] Discussion states that "task-specific fine-tuning or alternative verification strategies could further improve detection accuracy" given varying LLM-human alignment
- Why unresolved: Current approach uses few-shot prompting without model weight updates
- What evidence would resolve it: Fine-tuned models evaluated on same multilingual benchmark with correlation to human annotations

### Open Question 3
- Question: Would language-specific voting thresholds (rather than a universal τ = 0.7) improve performance given varying annotation consistency across languages?
- Basis in paper: [inferred] Threshold chosen empirically on validation data; performance varies substantially across languages (IoU: 0.396–0.736), with English performing poorly due to annotation variability
- Why unresolved: Single threshold applied uniformly despite morphological and annotation differences
- What evidence would resolve it: Per-language threshold optimization showing statistically significant improvements

## Limitations
- Performance degrades significantly in morphologically rich languages, suggesting architecture limitations for complex linguistic structures
- English performance is notably lower due to inconsistent ground truth from up to 12 annotators per sample
- The ensemble mechanism fails if all four models share systematic error patterns, though this scenario is not empirically validated
- Implementation details for fuzzy matching and adjudication prompts are underspecified, creating reproducibility challenges

## Confidence
- **High Confidence:** Multi-stage pipeline combining prompt engineering with LLM ensemble verification effectively reduces single-model bias; fuzzy matching improves span alignment precision; achieved top rankings across multiple languages
- **Medium Confidence:** Threshold τ=0.7 was empirically optimal for balancing precision and recall; morphologically rich languages present greater challenges due to annotation inconsistency; ensemble approach provides superior reliability compared to single-model approaches
- **Low Confidence:** Exact adjudication prompt format and its impact on probability elicitation accuracy; long-term generalization beyond the specific Mu-SHROOM dataset; computational cost-effectiveness given 12 adjudication runs per sample requirement

## Next Checks
1. **Adjudication Prompt Validation:** Reconstruct and test the missing adjudication prompt format with probability elicitation to verify its impact on ensemble reliability and span classification accuracy.

2. **Morphological Robustness Testing:** Systematically evaluate the pipeline on morphologically complex language samples with controlled annotation consistency to isolate linguistic vs. annotation variability effects.

3. **Ensemble Failure Mode Analysis:** Design adversarial test cases where all four models share systematic biases to empirically validate the ensemble's robustness claims and identify failure thresholds.