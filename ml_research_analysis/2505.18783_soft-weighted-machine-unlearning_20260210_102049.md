---
ver: rpa2
title: Soft Weighted Machine Unlearning
arxiv_id: '2505.18783'
source_url: https://arxiv.org/abs/2505.18783
tags:
- fairness
- robustness
- unlearning
- utility
- i255
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a soft-weighted framework to mitigate over-unlearning
  in machine unlearning, a problem where removing data for fairness or robustness
  inadvertently harms model utility. Unlike traditional binary removal schemes, it
  assigns fine-grained weights to samples via convex quadratic programming, optimizing
  for target task performance while preserving utility.
---

# Soft Weighted Machine Unlearning

## Quick Facts
- arXiv ID: 2505.18783
- Source URL: https://arxiv.org/abs/2505.18783
- Reference count: 40
- Primary result: Soft weighting mitigates over-unlearning by assigning fine-grained sample weights via convex QP, achieving improved fairness/robustness with minimal utility loss.

## Executive Summary
The paper introduces a soft-weighted framework to address over-unlearning in machine unlearning—where removing data for fairness or robustness inadvertently harms model utility. Unlike traditional binary removal schemes, it assigns continuous weights to samples via convex quadratic programming, optimizing for target task performance while preserving utility. The method integrates with most unlearning algorithms and is validated across diverse datasets and models, showing consistent improvements in fairness/robustness metrics and reduced utility loss.

## Method Summary
The framework operates in three steps: (1) Compute influence functions I_util, I_fair, I_robust for all training samples using validation set and Hessian inverse; (2) Solve a convex QP optimization problem to find optimal sample weights ε* that minimize target metric influence while preserving utility; (3) Apply weights through chosen unlearning algorithm (IF, GA, FT, SCRUB, etc.) via gradient updates. The optimization uses an analytical solution with four piecewise conditions based on the relationship between influence vectors. The approach maintains negligible computational overhead while achieving "free lunch" cases where both target performance and utility improve simultaneously.

## Key Results
- Soft weighting consistently improves fairness (DP, EOP) and robustness metrics while reducing utility loss compared to hard-weighted baselines
- Achieves "free lunch" outcomes on multiple datasets where both target metrics and utility improve simultaneously
- Computational overhead is minimal (38.6% additional time) while maintaining scalability across model types (LR, NN, ResNet)
- Analytical solution via KKT conditions enables efficient weight computation without iterative optimization

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Leave-One-Out Analysis Reveals Over-Unlearning Root Causes
The paper identifies three root causes of over-unlearning: (1) fairness/robustness and utility are uncorrelated, (2) borderline samples are treated equivalently to highly detrimental ones, and (3) most detrimental samples remain in the dataset after limited removal. By training leave-one-out models for each sample and measuring changes in fairness, robustness, and utility, the authors show Spearman correlations of -0.11 (fairness-utility) and -0.16 (robustness-utility), demonstrating that removing harmful samples does not automatically improve utility. The binary framework forces uniform treatment of samples with vastly different characteristics.

### Mechanism 2: Weighted Influence Functions Enable Fine-Grained Multi-Objective Control
Extending influence functions with continuous weights (rather than binary -1/0) allows simultaneous optimization of target task performance and utility preservation. The weighted influence function I(z_j; ε_j) = -ε_j · I(z_j; -1) scales each sample's influence linearly with its assigned weight. By computing separate influence estimates for utility, fairness, and robustness, the framework can quantify tradeoffs before committing to removal decisions. The validation set T serves as a proxy for estimating these effects.

### Mechanism 3: Constrained Convex Quadratic Programming with Analytical Solution
The weight optimization problem has a closed-form analytical solution via KKT conditions, avoiding iterative optimization overhead while guaranteeing constraint satisfaction. The optimization minimizes target metric influence + L2 regularization subject to: (1) minimum improvement constraint preventing under-correction, and (2) utility preservation constraint preventing over-unlearning. Four piecewise conditions determine which analytical solution applies based on the relationship between I_metric and I_util vectors.

## Foundational Learning

- **Concept: Influence Functions (Koh & Liang, 2017)**
  - Why needed here: Core mathematical tool for estimating sample contributions without retraining; enables efficient computation of I_util, I_fair, I_robust for all training samples.
  - Quick check question: Given a trained logistic regression model, can you derive the influence of a single training point on test loss using I(z_j) = ∇θℓ_test^T · H^{-1} · ∇θℓ(z_j)?

- **Concept: Convex Quadratic Programming and KKT Conditions**
  - Why needed here: Required to understand why the weight optimization has an analytical solution and how to implement the piecewise conditions correctly.
  - Quick check question: For a QP with inequality constraints, what are the complementary slackness conditions, and how do they determine which constraints are active?

- **Concept: Machine Unlearning Paradigms (Gradient Ascent, Fine-Tuning, Fisher Forgetting)**
  - Why needed here: The soft-weighted framework integrates with existing algorithms; understanding baseline methods (GA, FT, IF, Fisher, NTK, SCRUB) is necessary to modify their loss weighting.
  - Quick check question: How does Gradient Ascent differ from Influence Function-based unlearning in terms of computational cost and removal precision?

## Architecture Onboarding

- **Component map:**
  Influence Evaluation Module -> Weight Optimization Module -> Model Correction Module

- **Critical path:**
  1. Compute/estimate Hessian H^{-1}_θ for the model's last layer (convex surrogate for non-convex models)
  2. Evaluate influence for each sample on validation set (O(n·d²) for n samples, d parameters)
  3. Determine which of 4 analytical conditions applies to the influence vectors
  4. Compute optimal weights ε* via analytical solution
  5. Apply weights via gradient updates: θ_{t+1} = θ_t - ε*·η·∇θℓ

- **Design tradeoffs:**
  - Exact Hessian vs. diagonal approximation: Exact provides better influence estimates but O(d³) inversion cost; diagonal (σI) scales but loses curvature information. Paper uses exact for linear models, diagonal for neural networks.
  - Validation set size: Larger validation improves influence estimation accuracy but increases Step 1 cost (dominates ~38.6% of runtime per Fig. 6).
  - Constraint strictness: Tighter Δ (minimum improvement) may cause infeasibility; looser constraints reduce target metric gains.

- **Failure signatures:**
  - Over-correction: Fairness metric increases (reverse bias) after unlearning → check if utility constraint is too loose or Δ bound is violated.
  - Utility collapse despite constraints: Influence estimation error accumulates → validate correlation between estimated and actual influence (Fig. 3 diagnostic).
  - Numerical instability in H^{-1}: Non-convex models with poorly conditioned Hessian → add damping factor or use diagonal approximation.
  - Infeasible optimization: Empty feasible region → relax utility constraint (allow small positive I_util sum) or reduce target improvement Δ.

- **First 3 experiments:**
  1. Influence estimation validation: On Adult dataset with logistic regression, compute leave-one-out actual changes vs. influence estimates for 100 random samples; verify Spearman correlation >0.9 (replicate Fig. 3).
  2. Hard vs. soft comparison on single algorithm: Implement soft-weighted Influence Function (IF) unlearning; compare fairness (DP), robustness (adversarial loss), and utility (test accuracy) against hard-weighted IF on Adult/Bank datasets with 20% forgetting set.
  3. Ablation on constraint relaxation: Systematically vary Δ (minimum improvement bound) and observe tradeoff between target metric improvement and utility preservation; identify "free lunch" region where both improve.

## Open Questions the Paper Calls Out

### Open Question 1
Can the soft-weighted unlearning framework be effectively adapted for Large Language Models (LLMs) where computational costs and non-convexity are significantly higher? The authors state in the "Limitation and Societal Impacts" section that "Future research should prioritize exploring the applicability and performance of this framework in LLM-related tasks," as their evaluation was constrained by resources and a lack of established benchmarks.

### Open Question 2
Does the framework generalize to individual fairness concepts, or is it strictly limited to the group fairness metrics (DP, EOP) utilized in the experiments? The authors note that "An important avenue for future research involves investigating whether the findings of this study can be applied to other fairness concepts, such as individual fairness."

### Open Question 3
Can this soft-weighting approach be extended to correct other data quality issues, such as the removal of poisoned data or the management of outdated data? The authors claim the "implications of this work extend to critical areas such as the removal of poisoned data and management of outdated data, which warrants further investigation."

### Open Question 4
How can the cumulative estimation error inherent in influence functions be mitigated to ensure utility preservation in very large-scale datasets? The paper notes in Section 5.2 that in larger datasets, "cumulative estimation error becomes more pronounced, which can lead to a slight utility decline," suggesting the current approximation limits scalability.

## Limitations
- Scalability concerns due to Hessian computation cost and potential influence estimation error accumulation in very large datasets
- Current framework limited to group fairness metrics (DP, EOP) and does not address individual fairness concepts
- Assumes convex or well-conditioned Hessian approximations, which may not hold for all deep learning architectures
- Open question about effective adaptation to LLMs due to computational and non-convexity challenges

## Confidence

- Mechanism 1 (Counterfactual analysis revealing over-unlearning causes): High - Strong empirical evidence from correlation coefficients and visual inspection
- Mechanism 2 (Weighted influence functions enabling multi-objective control): Medium - Validated on leave-one-out but limited corpus support for weighted approach specifically
- Mechanism 3 (Analytical convex QP solution): Medium - KKT derivation appears sound but feasibility conditions in non-convex settings untested

## Next Checks

1. Validate influence estimation accuracy on deep learning models: Compare Spearman correlation between estimated and actual influences on 100 random samples for ResNet-18 on CIFAR-100, expecting >0.8 correlation.

2. Test constraint infeasibility handling: Systematically vary Δ from 0.1 to 0.9 of current metric value on Adult dataset; document when QP becomes infeasible and which fallback procedure is needed.

3. Benchmark computational overhead scaling: Measure runtime breakdown across dataset sizes (10K, 50K, 100K samples) for influence computation vs. model training time; verify claimed ~38.6% overhead holds across scales.