---
ver: rpa2
title: TopClustRAG at SIGIR 2025 LiveRAG Challenge
arxiv_id: '2506.15246'
source_url: https://arxiv.org/abs/2506.15246
tags:
- passages
- system
- retrieval
- cluster
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TopClustRAG, a retrieval-augmented generation
  system developed for the LiveRAG Challenge, which evaluates end-to-end question
  answering over large-scale web corpora. The system employs a hybrid retrieval strategy
  combining sparse (BM25) and dense (embedding-based) indices, followed by K-Means
  clustering to group semantically similar passages.
---

# TopClustRAG at SIGIR 2025 LiveRAG Challenge

## Quick Facts
- arXiv ID: 2506.15246
- Source URL: https://arxiv.org/abs/2506.15246
- Reference count: 4
- Primary result: 2nd in faithfulness, 7th in correctness on FineWeb Sample-10BT dataset

## Executive Summary
TopClustRAG is a retrieval-augmented generation system developed for the SIGIR 2025 LiveRAG Challenge, designed to answer questions over large-scale web corpora. The system employs a hybrid retrieval strategy combining sparse (BM25) and dense (embedding-based) indices, followed by K-Means clustering to group semantically similar passages. Representative passages from each cluster are used to construct cluster-specific prompts for a large language model, generating intermediate answers that are filtered, reranked, and synthesized into a final response. This multi-stage pipeline enhances answer diversity, relevance, and faithfulness to retrieved evidence. Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in faithfulness and 7th in correctness on the official leaderboard, demonstrating the effectiveness of clustering-based context filtering and prompt aggregation in large-scale RAG systems.

## Method Summary
TopClustRAG implements a hybrid retrieval-augmented generation pipeline that combines both sparse (BM25) and dense (embedding-based) retrieval methods to capture complementary aspects of relevance. Retrieved passages are then clustered using K-Means to group semantically similar content, enabling the system to identify representative passages from each cluster. These cluster representatives are used to construct specialized prompts for a large language model, which generates intermediate answers. These intermediate responses undergo filtering and reranking before being synthesized into a final answer. This approach addresses the challenge of information overload in large-scale retrieval by organizing retrieved passages into semantically coherent groups, allowing the LLM to focus on diverse aspects of the query while maintaining faithfulness to the evidence.

## Key Results
- Ranked 2nd in faithfulness and 7th in correctness on FineWeb Sample-10BT dataset
- Demonstrated effectiveness of clustering-based context filtering in RAG systems
- Showed improved answer diversity through cluster-specific prompt generation

## Why This Works (Mechanism)
The clustering mechanism works by organizing retrieved passages into semantically coherent groups, which prevents the LLM from being overwhelmed by redundant information while ensuring coverage of diverse perspectives. By selecting representative passages from each cluster, the system provides focused context that enables the LLM to generate more faithful and comprehensive answers. The hybrid retrieval strategy captures both keyword-based and semantic similarity signals, improving the quality of passages fed into the clustering stage. The multi-stage pipeline with filtering and reranking ensures that only the most relevant and reliable intermediate answers contribute to the final response, reducing hallucination and improving faithfulness to the source material.

## Foundational Learning
- **Hybrid Retrieval (BM25 + Dense Embeddings)**: Combines exact keyword matching with semantic similarity to capture different relevance signals. Why needed: Neither sparse nor dense retrieval alone provides complete coverage of relevance. Quick check: Verify both components contribute to retrieval quality by ablation testing.

- **K-Means Clustering**: Groups semantically similar passages to identify representative content from each semantic cluster. Why needed: Prevents information redundancy and ensures diverse coverage of the topic. Quick check: Confirm clusters capture distinct semantic themes using coherence metrics.

- **Prompt Engineering for LLMs**: Constructs specialized prompts using cluster representatives to guide answer generation. Why needed: Directs the LLM to focus on specific aspects of the query while maintaining evidence-based reasoning. Quick check: Evaluate prompt effectiveness through answer quality metrics.

- **Answer Filtering and Reranking**: Selects the most relevant and reliable intermediate answers for final synthesis. Why needed: Reduces noise and hallucination by prioritizing faithful responses. Quick check: Measure faithfulness improvement after filtering stage.

- **Multi-stage Synthesis**: Combines filtered intermediate answers into a coherent final response. Why needed: Aggregates diverse perspectives while maintaining consistency. Quick check: Assess coherence and completeness of final answers.

## Architecture Onboarding

**Component Map:**
BM25 + Dense Retrieval -> K-Means Clustering -> Cluster Prompt Generation -> LLM Intermediate Answers -> Filtering/Reranking -> Final Synthesis

**Critical Path:**
Query -> Hybrid Retrieval -> Clustering -> Prompt Generation -> LLM Inference -> Answer Filtering -> Final Answer

**Design Tradeoffs:**
- Clustering adds computational overhead but improves answer diversity and faithfulness
- Hybrid retrieval increases complexity but captures complementary relevance signals
- Multiple intermediate stages may increase latency but improve answer quality
- Cluster size and number parameters require tuning for optimal performance

**Failure Signatures:**
- Poor retrieval quality propagates through clustering, leading to irrelevant clusters
- Oversized clusters may contain heterogeneous content, confusing the LLM
- Undersized clusters may lack sufficient context for meaningful answer generation
- Aggressive filtering may eliminate useful intermediate answers

**First 3 Experiments to Run:**
1. Measure faithfulness and correctness with and without the clustering component to quantify its contribution
2. Test different cluster sizes and numbers to optimize the tradeoff between diversity and coherence
3. Evaluate the impact of varying the balance between BM25 and dense retrieval contributions

## Open Questions the Paper Calls Out
None identified in the available documentation.

## Limitations
- Evaluation limited to single benchmark dataset without external validation
- Computational cost of clustering pipeline not addressed for large-scale deployment
- Performance on complex queries requiring multi-hop reasoning not evaluated
- Proprietary LLM nature may limit reproducibility of results

## Confidence
- Evaluation scope: Medium (single dataset only)
- Methodology transparency: Medium (some components underspecified)
- Reproducibility: Low (proprietary LLM components)
- Performance claims: Medium (leaderboard results but limited context)

## Next Checks
1. Evaluate TopClustRAG on at least two additional large-scale QA benchmarks to assess generalizability beyond FineWeb Sample-10BT
2. Conduct ablation studies removing the clustering component to quantify its specific contribution to faithfulness and correctness scores
3. Measure computational efficiency and latency compared to standard RAG pipelines to determine practical deployment viability