---
ver: rpa2
title: 'Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature
  Review'
arxiv_id: '2507.07741'
source_url: https://arxiv.org/abs/2507.07741
tags:
- speech
- modeling
- data
- code-switching
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This systematic literature review examines 127 papers on end-to-end\
  \ automatic speech recognition for code-switching, identifying key trends, challenges,\
  \ and research gaps. The analysis reveals that research is heavily concentrated\
  \ on three language pairs\u2014Mandarin-English (55%), Hindi-English (11%), and\
  \ Arabic-English (9%)\u2014driven by dataset availability."
---

# Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review

## Quick Facts
- **arXiv ID**: 2507.07741
- **Source URL**: https://arxiv.org/abs/2507.07741
- **Reference count**: 40
- **Primary result**: Systematic review of 127 papers reveals Mandarin-English dominates research (55%), most use monolingual transfer learning, and evaluation lacks standardization

## Executive Summary
This systematic literature review examines 127 papers on end-to-end automatic speech recognition for code-switching, identifying key trends, challenges, and research gaps. The analysis reveals that research is heavily concentrated on three language pairs—Mandarin-English (55%), Hindi-English (11%), and Arabic-English (9%)—driven by dataset availability. Most papers employ monolingual modeling with transfer learning, while only 36% use multilingual approaches. Large pretrained models like Whisper are increasingly adopted, and data augmentation is common due to scarcity of code-switched data. Evaluation metrics vary, with WER, CER, and MER being most prevalent, but no standardized framework exists. Key challenges include limited language coverage, lack of consistent benchmarks, and disparities in dataset accessibility. The review highlights the need for more inclusive, reproducible research and standardized evaluation to advance the field.

## Method Summary
The study conducted a systematic literature review of 127 peer-reviewed papers from 2018-2024, using Semantic Scholar API queries to identify research on end-to-end code-switching ASR. Five annotators manually coded papers using a structured schema covering problem setup, model design choices, training/evaluation settings, and performance. The review analyzed languages, datasets, modeling architectures, evaluation metrics, and reported performance across four dimensions, identifying trends and gaps in the field.

## Key Results
- Research concentrates heavily on three language pairs: Mandarin-English (55%), Hindi-English (11%), and Arabic-English (9%)
- Most papers (60%) use monolingual pretraining with transfer learning rather than multilingual approaches (36%)
- Large pretrained models like Whisper are increasingly adopted, with mixed zero-shot performance results
- Data augmentation is common (~30% of papers) but shows mixed effectiveness across techniques
- No standardized evaluation framework exists, with WER, CER, and MER being most prevalent metrics

## Why This Works (Mechanism)

### Mechanism 1
Transfer learning from monolingual pretrained models to code-switched data appears to improve CS performance by leveraging existing acoustic representations. Models first learn language-specific acoustic patterns from large monolingual corpora, then adapt to code-switched speech through fine-tuning. This staged approach addresses the scarcity of CS training data (~77% of papers use accessible datasets, per Section 3). Core assumption: Monolingual acoustic representations transfer meaningfully to mixed-language speech without severe interference. Evidence anchors: Section 4.1 shows ~60% of papers use monolingual data, Wang et al. (2024) demonstrates tri-stage training success. Break condition: Catastrophic forgetting—Shah et al. (2020) find fine-tuning for CS "may impair performance on monolingual datasets."

### Mechanism 2
Language identification (LID) integration can reduce recognition errors by routing or conditioning on language-specific information during decoding. LID operates as (a) pre-processing to separate languages, (b) auxiliary task with frame-level prediction, (c) loss term in attention components, or (d) explicit output token. Frame-level LID via multi-task learning outperforms utterance-level prediction for intra-sentential switching. Core assumption: Language boundaries in code-switched speech are detectable from acoustic signals and align with useful segmentation points for recognition. Evidence anchors: Section 4.2 shows ~33% of papers incorporate LID, Shan et al. (2019) finds attention-related components yield best results. Break condition: LID prediction at sequence start "is more appropriate for inter-sentential switching than intra-sentential switching" (Section 4.2).

### Mechanism 3
Data augmentation via SpecAugment, speed perturbation, and TTS synthesis partially compensates for limited CS training data. Synthetic CS speech generation and acoustic perturbations expand training distributions. Combination approaches (TTS + speed perturbation + SpecAugment) yield best results, though individual techniques show mixed effectiveness. Core assumption: Augmented data approximates the acoustic and linguistic distribution of natural code-switched speech sufficiently to improve generalization. Evidence anchors: Section 5.1 shows ~30% of papers explicitly mention data augmentation, Liang et al. (2022) finds "TTS and speed perturbation" favorable. Break condition: Distribution gap between synthetic and real CS speech—Sharma et al. (2020) require Mixup regularization "to bridge the distribution gap."

## Foundational Learning

- **Concept: Code-switching typology (inter-sentential vs. intra-sentential)**
  - Why needed here: Intra-sentential switching (within a sentence) is identified as "most challenging for speech systems" (Section 1.1) and drives architectural choices like frame-level LID.
  - Quick check question: Can you explain why utterance-level language prediction fails for intra-sentential code-switching?

- **Concept: End-to-end ASR architectures (CTC, encoder-decoder, transducers)**
  - Why needed here: Section 4.4 notes encoder-decorator architectures (47% of papers) outperform CTC-only approaches; understanding the tradeoffs is prerequisite to model selection.
  - Quick check question: Why might CTC's output independence assumption be problematic for code-switched speech with language transitions?

- **Concept: Mixed-script evaluation metrics (WER, CER, MER, TER)**
  - Why needed here: Standard WER fails when languages use different tokenization (e.g., Chinese characters vs. English words). MER combines WER for word-based segments and CER for character-based segments (Section 5.4).
  - Quick check question: For Mandarin-English code-switching, which metric would you use and why?

## Architecture Onboarding

- **Component map**: Audio signal → Feature extraction (mel-spectrogram) → Encoder (Transformer/Conformer or pretrained backbone) → Optional LID module → Decoder (autoregressive with attention or transducer) → Output transcription with text units

- **Critical path**: 1) Identify language pair and available datasets (Table 3 lists accessible datasets by matrix language) 2) Select architecture: Encoder-decoder recommended over CTC-only 3) Choose text units: Mixed units standard for zho-eng 4) Design training pipeline: Monolingual pretraining → CS fine-tuning with LWF if needed 5) Select evaluation metric: MER for mixed-script pairs

- **Design tradeoffs**:
  - Monolingual vs. multilingual modeling: Monolingual transfer learning dominates (~60%) but risks catastrophic forgetting; multilingual approaches (~36%) capture cross-lingual patterns but require more CS data
  - Pretrained vs. from-scratch: Pretrained models (Whisper, Wav2Vec2) increasingly common but may not support CS explicitly—zero-shot evaluation shows mixed results
  - Data augmentation: TTS synthesis helps but quality varies; SpecAugment + speed perturbation more reliable

- **Failure signatures**:
  - Catastrophic forgetting: Monolingual performance degrades after CS fine-tuning—requires regularization (Shah et al., 2020)
  - Cross-language confusion: Shared character sets cause "greater confusion between cross-language targets" (Section 4.3)
  - Script ambiguity: Predictions penalized for wrong script when matrix/embedded languages use different writing systems (Section 5.4)
  - Language imbalance: Multi-graph decoding without Kleene-closure "does not support intra-sentence switching" (Section 4.6)

- **First 3 experiments**:
  1. Baseline Whisper fine-tuning: Take Whisper Large V2, fine-tune on SEAME or ASRU 2019 dataset, evaluate with MER. Compare against zero-shot prompting with concatenated language tokens.
  2. Ablate LID integration: Implement frame-level LID as auxiliary task vs. no LID. Test on intra-sentential CS subset to validate Shan et al. (2019) findings.
  3. Data augmentation comparison: Train identical Conformer encoder-decoder with (a) no augmentation, (b) SpecAugment only, (c) SpecAugment + speed perturbation, (d) TTS-synthesized CS. Measure WER/MER gap and analyze error patterns by language segment.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the field establish a standardized benchmarking framework to enable consistent comparison of End-to-End ASR performance across diverse code-switched language pairs? Basis: [explicit] The authors identify a "lack of consistent evaluation framework" and sporadic efforts that prevent robust comparison with past work. Why unresolved: Current research is fragmented, driven largely by dataset availability for only three dominant language pairs. What evidence would resolve it: Creation and adoption of a unified benchmark suite applied to multiple languages with shared training, development, and test splits.

- **Open Question 2**: Do emerging metrics like Mixed Error Rate (MER) and PolyWER provide a more valid assessment of code-switching ASR performance than standard Word Error Rate (WER) when handling distinct writing systems? Basis: [explicit] The authors highlight "shortcomings of existing metrics" due to script differences and lack of standardization. Why unresolved: While WER, CER, and MER are prevalent, differences in tokenization and orthographic norms create conceptual challenges in evaluation. What evidence would resolve it: Correlation studies comparing these metrics against human perceptual evaluations of intelligibility across different language pairs.

- **Open Question 3**: To what extent do data augmentation techniques like TTS synthesis generalize from high-resource pairs (e.g., Mandarin-English) to under-represented languages? Basis: [inferred] The paper notes data augmentation is a common response to scarcity, but points out a lack of insight into the "interaction of different methods" and their "applicability across language pairs." Why unresolved: Most augmentation studies focus on the dominant languages; it is unclear if these methods effectively bridge the distribution gap for low-resource languages. What evidence would resolve it: Ablation studies applying identical augmentation strategies to both high-resource and low-resource datasets to measure relative performance gains.

## Limitations

- Dataset coverage heavily skewed toward Mandarin-English (55%), Hindi-English (11%), and Arabic-English (9%), limiting generalizability
- Evaluation metrics vary widely with no standardized framework, making direct comparisons difficult
- Dataset accessibility information based on paper descriptions rather than direct verification
- Transfer learning effectiveness claims rely on aggregate trends rather than controlled ablation studies

## Confidence

**High Confidence**:
- Dataset concentration (Mandarin-English dominance at 55%) - supported by explicit statistics from 127 papers
- Architecture preferences (encoder-decoder at 47%, transformer/conformer backbones) - directly extracted from paper annotations
- Evaluation metric diversity - clearly documented across the corpus

**Medium Confidence**:
- Transfer learning effectiveness - supported by 60% of papers using monolingual pretraining, but lacks controlled experiments
- Data augmentation benefits - reported in ~30% of papers with mixed results, suggesting variability in effectiveness
- LID integration benefits - 33% adoption rate with reported improvements, but effectiveness varies by switching type

**Low Confidence**:
- Whisper zero-shot performance claims - limited to 6 papers, with mixed results not extensively validated
- Catastrophic forgetting prevalence - mentioned in specific papers but not systematically measured across the corpus
- Multilingual vs. monolingual modeling tradeoffs - preference for monolingual transfer learning reported, but long-term effectiveness unclear

## Next Checks

1. **Controlled ablation study on transfer learning**: Replicate Wang et al. (2024)'s tri-stage training (monolingual pretraining → CS fine-tuning) against (a) direct CS training from scratch and (b) multilingual joint training, using identical datasets and architectures. Measure performance degradation on monolingual tasks post-CS fine-tuning to quantify catastrophic forgetting.

2. **Standardization experiment for evaluation metrics**: Select 10 papers with overlapping datasets (e.g., SEAME for Mandarin-English) and recalculate their reported performances using a unified metric framework (MER for mixed-script pairs, WER for uniform scripts). Compare ranking stability and performance gaps to assess current metric diversity's impact on reproducibility.

3. **Dataset accessibility verification**: For the 77% of papers reporting accessible datasets, attempt to download and preprocess each dataset to verify current availability and usability. Document changes in licensing, format, or accessibility that could affect reproducibility of results from older papers.