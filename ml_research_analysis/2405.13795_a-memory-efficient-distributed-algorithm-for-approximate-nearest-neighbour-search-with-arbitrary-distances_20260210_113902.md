---
ver: rpa2
title: A Memory-Efficient Distributed Algorithm for Approximate Nearest Neighbour
  Search with Arbitrary Distances
arxiv_id: '2405.13795'
source_url: https://arxiv.org/abs/2405.13795
tags:
- search
- index
- pdasc
- distance
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PDASC introduces a memory-efficient distributed algorithm for approximate
  nearest neighbour search with arbitrary distances. The method builds a hierarchical
  index using clustering mechanisms that are agnostic to distance properties, enabling
  support for non-metric and domain-specific similarities while naturally partitioning
  indexing and search across multiple computing nodes.
---

# A Memory-Efficient Distributed Algorithm for Approximate Nearest Neighbour Search with Arbitrary Distances

## Quick Facts
- arXiv ID: 2405.13795
- Source URL: https://arxiv.org/abs/2405.13795
- Reference count: 40
- Primary result: PDASC achieves competitive accuracy-efficiency trade-offs while consistently requiring lower per-node memory compared to state-of-the-art ANN methods

## Executive Summary
PDASC introduces a memory-efficient distributed algorithm for approximate nearest neighbour search with arbitrary distances. The method builds a hierarchical index using clustering mechanisms that are agnostic to distance properties, enabling support for non-metric and domain-specific similarities while naturally partitioning indexing and search across multiple computing nodes. By preserving locally informative neighbourhood structure, the index mitigates practical manifestations of the curse of dimensionality in high-dimensional spaces. Experimental evaluation across multiple benchmark datasets and distance functions shows that PDASC achieves competitive accuracy-efficiency trade-offs while consistently requiring lower per-node memory compared to state-of-the-art ANN methods.

## Method Summary
PDASC constructs a hierarchical index through distributed bottom-up clustering using k-medoids (FastPAM) to partition data and select prototypes. The algorithm splits datasets across multiple nodes, recursively clusters groups at each level, and promotes a configurable number of prototypes per group to form the index hierarchy. Search proceeds via top-down radius-constrained traversal, where candidates within a global radius are explored recursively across levels, with results merged and globally ranked. The method supports arbitrary distance functions without metric assumptions, enabling applications to non-metric and domain-specific similarities. Key parameters include the number of computing nodes, group size, prototypes per group ratio, and search radius.

## Key Results
- PDASC consistently requires lower per-node memory compared to state-of-the-art ANN methods
- High prototype-to-group ratios (20-30%) preserve local neighborhood structure in high dimensions
- Distributed computation across nodes provides favorable accuracy-efficiency trade-offs

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical clustering with distance-agnostic index construction enables support for arbitrary dissimilarity functions including non-metric distances. PDASC uses k-medoids clustering at each level to partition data and select prototypes, operating purely on distance matrices rather than geometric assumptions. The construction method decouples from metric properties like triangle inequality. Core assumption: clustering algorithm can converge meaningfully even with non-metric distances. Evidence: clustering mechanisms agnostic to distance properties, construction independent of underlying distance properties. Break condition: distance functions producing degenerate pairwise matrices.

### Mechanism 2
High prototype-to-group ratios (typically 20-30%) preserve local neighborhood structure, mitigating distance concentration effects in high dimensions. Rather than selecting few representatives per cluster, PDASC promotes many prototypes, maintaining denser local partitions where relative distances remain discriminative. The coarse-to-fine hierarchy progressively isolates neighborhoods where distance relationships stay informative. Core assumption: local geometric relationships remain meaningful when global distances concentrate. Evidence: selecting larger sets of prototypes preserves local neighborhood structure and improves separability at higher levels. Break condition: excessive ratios over-propagate representatives, causing premature pruning.

### Mechanism 3
Top-down radius-constrained traversal distributes computation across nodes while controlling recall-efficiency trade-offs. NSA starts at the highest index level, computing distances from query to all prototypes, exploring only child prototypes within radius r recursively. This pruning occurs independently on each node; final candidates are merged and globally ranked. Core assumption: distance to parent prototypes correlates with distance to descendant data points. Evidence: global radius r controls the explore/prune boundary. Break condition: if r is too restrictive, valid neighbors are pruned early; if too permissive, search degrades to near-exhaustive traversal.

## Foundational Learning

Concept: **Curse of dimensionality and distance concentration**
- Why needed here: PDASC's design justification is that global distances become uninformative in high-D, but local relationships remain useful
- Quick check question: In a 1000-dimensional space with uniform random points, what happens to the ratio (max_distance - min_distance) / min_distance as n grows?

Concept: **k-medoids clustering vs k-means**
- Why needed here: PDASC uses k-medoids specifically because it operates on distance matrices and selects actual data points as prototypes
- Quick check question: Why can't k-means work with a precomputed distance matrix alone?

Concept: **Approximate vs exact nearest neighbor trade-offs**
- Why needed here: PDASC relaxes exact retrieval guarantees for efficiency; np/gl ratio and radius r are levers controlling this approximation
- Quick check question: If recall@10 is 0.85, what fraction of the true 10-nearest neighbors are recovered on average?

## Architecture Onboarding

Component map: MSA (index builder) -> recursively clusters groups -> promotes np prototypes per group -> builds hierarchical levels; NSA (searcher) -> top-down radius-filtered traversal per node -> merge candidate sets -> global sort -> return top-k

Critical path:
1. Index build time scales with (n / nNodes) × clustering_cost(gl, np) × nLevels
2. Query time dominated by distance computations during traversal; NDC per node decreases as nNodes increases
3. Memory per node is (n / nNodes) × overhead_factor where overhead depends on np/gl ratio

Design tradeoffs:
- Low np/gl: Higher recall, higher NDC, smaller index → use when accuracy critical
- High np/gl: Lower recall, lower NDC early pruning, larger index → can fail if pruning too aggressive
- More nodes: Lower per-node memory and NDC, but diminishing returns and increased coordination overhead
- Larger radius r: Higher recall at cost of more distance computations

Failure signatures:
- Very low recall with high NDC: np/gl too low or radius poorly calibrated
- Very low recall with low NDC: np/gl too high or radius too restrictive
- Clustering divergence: Distance function produces unstable medoids
- Memory higher than expected: Residual groups at lower levels promoting all elements

First 3 experiments:
1. Parameter sweep on np/gl: Fix nNodes=1, gl=50, r=moderate; vary np/gl ∈ {0.1, 0.2, 0.3, 0.5} on MNIST; plot recall vs NDC to identify sweet spot
2. Radius sensitivity: At optimal np/gl, vary r across [0.5×, 0.75×, 1.0×, 1.25×] of the 90th percentile pairwise distance; measure recall-NDC frontier
3. Scaling with nNodes: Fix other parameters; vary nNodes ∈ {1, 3, 5, 10}; measure per-node index size, per-node NDC, and total wall-clock query time

## Open Questions the Paper Calls Out

1. How can the global search radius parameter r be replaced with a dynamic, level-specific threshold that adapts to local data density?
   - Basis: The authors state that identifying an optimal radius at each index level according to local data distribution is a promising direction
   - Why unresolved: Current NSA algorithm relies on predefined global radius which may not be optimal across different levels or varying local cluster densities
   - What evidence would resolve it: An adaptive radius selection strategy demonstrating improved recall or lower NDC compared to static r baseline

2. Would incorporating core-set techniques for prototype selection provide better representativeness and accuracy than current k-medoids clustering approach?
   - Basis: Section 5 suggests improving representative point selection by incorporating core-set techniques that capture underlying data distribution more accurately
   - Why unresolved: Current method uses k-medoids which may not theoretically guarantee preservation of geometric properties as effectively as core-sets
   - What evidence would resolve it: Comparative study between k-medoids and core-set based prototype selection within PDASC

3. Can the PDASC index structure be efficiently updated dynamically (insertions/deletions) in distributed environment without triggering full index rebuild?
   - Basis: MSA algorithm described as static batch process, experimental evaluation conducted on fixed training/test splits
   - Why unresolved: Hierarchical clustering-based indices typically require expensive re-clustering to incorporate new data
   - What evidence would resolve it: Algorithm for incremental updates maintaining recall consistency and memory efficiency

## Limitations

- Algorithm complexity not explicitly analyzed as functions of dataset size and dimensionality
- Experimental evaluation focuses on datasets up to 290K points, with only one dataset exceeding 65K points
- Performance claims on non-metric distances rely heavily on theoretical reasoning rather than extensive empirical validation

## Confidence

**High Confidence**: The core algorithmic framework is well-specified and theoretically sound; memory-efficiency advantage consistently demonstrated

**Medium Confidence**: Experimental results showing competitive accuracy-efficiency trade-offs are well-supported, though limited dataset sizes and lack of complexity analysis introduce uncertainty about large-scale performance

**Low Confidence**: Claims about performance on non-metric distances rely heavily on theoretical reasoning rather than extensive empirical validation across diverse non-metric functions

## Next Checks

1. Implement time and memory profiling to empirically measure how index construction time and memory usage scale with dataset size (n) and dimensionality (d) for fixed parameter settings

2. Test PDASC on datasets with 1M+ points to validate whether memory-efficiency advantage and recall-NDC trade-offs hold at production scales

3. Evaluate PDASC on multiple non-metric distance functions (asymmetric, non-symmetric, semi-metric) to assess robustness of clustering quality and search effectiveness under various degeneracy conditions