---
ver: rpa2
title: 'WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects'
arxiv_id: '2502.12404'
source_url: https://arxiv.org/abs/2502.12404
tags:
- uni00000012
- uni00000015
- uni00000016
- uni0000001c
- uni0000001d
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'WMT24++ expands the language coverage of the WMT24 benchmark to
  55 languages by collecting new human-written references and post-edits, as well
  as post-edits for 8 of the original WMT24 languages. The dataset includes 998 English
  sources across four domains: literary, news, social, and speech.'
---

# WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects

## Quick Facts
- arXiv ID: 2502.12404
- Source URL: https://arxiv.org/abs/2502.12404
- Reference count: 36
- Key outcome: WMT24++ expands WMT24 benchmark to 55 languages using new human references and post-edits, finding frontier LLMs outperform both traditional MT and human translations according to automatic metrics

## Executive Summary
WMT24++ significantly expands the WMT24 machine translation benchmark by adding 55 new languages and dialects, creating the first large-scale evaluation of translation quality across this diverse language set. The benchmark includes 998 English sources across literary, news, social, and speech domains, with both traditional reference-based evaluation and innovative quality estimation metrics for languages lacking human references. Results show that frontier large language models like OpenAI o1, Claude 3.5, and Gemini-1.5 Pro achieve the highest automatic metric scores across all languages, often surpassing traditional MT providers and human translations.

## Method Summary
The authors expanded WMT24's language coverage by collecting new human-written references and post-edits for 55 languages and dialects, including many previously excluded due to low resource availability. They evaluated a diverse set of translation systems including traditional MT providers (Google, DeepL, ModernMT, Amazon, Microsoft) and large language models (OpenAI o1, Claude 3.5, Gemini-1.5 Pro, GPT-4 Turbo) using multiple automatic metrics: BLEU, chrF, neural metrics (MetricX, COMET, COMETKiwi), and LLM-based evaluation (Gemini-DA). For languages without human references, they employed quality estimation metrics that predict translation quality directly from source and hypothesis text.

## Key Results
- Frontier LLMs (OpenAI o1, Claude 3.5, Gemini-1.5 Pro) achieved highest automatic metric scores across all 55 languages
- GPT-4 Turbo scored above human translations in 9 languages, while Claude 3.5 and Gemini-1.5 Pro surpassed humans in 4 and 3 languages respectively
- Traditional MT providers (Google, DeepL, ModernMT) consistently outperformed Amazon and Microsoft across all metrics
- Significant performance gaps exist between closely related language varieties (e.g., Persian varieties differ by 7+ BLEU points)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Expanding to previously excluded low-resource languages reveals significant performance gaps between frontier models.
- **Mechanism**: By systematically including dialects and low-resource languages using the same evaluation pipeline, the benchmark surfaces systematic weaknesses that are invisible when testing only high-resource languages.
- **Core assumption**: Performance on high-resource languages does not predict performance on related low-resource languages (evidence shows Persian varieties differ by 7+ BLEU points).
- **Evidence anchors**:
  - [abstract] "In this work, we expand the language coverage of WMT24 to 109 languages... including languages and dialects that were previously excluded from the evaluation in WMT24 due to low resources."
- **Break condition**: If low-resource language performance strongly correlates with related high-resource languages, the expansion provides less new signal.

### Mechanism 2
- **Claim**: Using source-target analysis (without reference translations) enables evaluation of languages without professional translators.
- **Mechanism**: QE metrics (MetricX-QE, COMETKiwi, Gemini-DA-QE) predict translation quality directly from source and hypothesis, bypassing the need for human references that are expensive for low-resource languages.
- **Core assumption**: QE metrics maintain reliability across language families and scripts (the paper finds generally consistent ranking between QE and reference-based metrics).
- **Evidence anchors**:
  - [section 2.3] "QE metrics do not require reference translations... they aim to predict the quality of the translation directly from the source and hypothesis."
- **Break condition**: If QE metrics show systematic bias against certain language families or scripts, expansion quality becomes unreliable.

### Mechanism 3
- **Claim**: Combining multiple metrics with different theoretical bases provides robust ranking that survives metric-specific weaknesses.
- **Mechanism**: By using BLEU (surface overlap), chrF (character n-grams), neural metrics (COMET/MetricX), and LLM-based evaluation (Gemini-DA), the benchmark reduces dependence on any single quality assessment theory.
- **Core assumption**: Metrics that agree on system rankings are capturing genuine quality differences rather than shared artifacts.
- **Evidence anchors**:
  - [section 2.3] "We used the following metrics... BLEU, chrF, MetricX-24, XCOMET, COMETKiwi-23, and Gemini-DA... each is described below."
- **Break condition**: If all metrics share the same systematic blind spots (e.g., all trained on similar data), the apparent consensus is misleading.

## Foundational Learning

- **Concept: Machine Translation Evaluation Paradigms**
  - Why needed here: The benchmark uses fundamentally different evaluation approaches (reference-based vs. QE) that engineers must understand to interpret results correctly.
  - Quick check question: Can you explain why a system might rank differently on BLEU vs. COMET for the same language pair?

- **Concept: Low-Resource Language Challenges**
  - Why needed here: The core contribution is evaluating languages where standard assumptions (availability of professional translators, training data) break down.
  - Quick check question: What are three distinct challenges that arise when evaluating MT for languages with <100k speakers?

- **Concept: Statistical Significance in Benchmarking**
  - Why needed here: Section 2.4 mentions statistical testing - engineers need to understand when score differences are meaningful vs. noise.
  - Quick check question: If System A scores 0.5 BLEU higher than System B on a 1000-sentence test set, is this a meaningful difference?

## Architecture Onboarding

- **Component map**: Source data → preprocessing → system translation → metric scoring → aggregation → ranking
  - Two parallel paths: reference-based metrics (requires human translation) and QE metrics (source-only)

- **Critical path**: Source quality filtering → human translation (for reference-based) → metric computation → statistical testing → final leaderboard

- **Design tradeoffs**: 
  - Broader language coverage vs. evaluation quality (more languages means fewer human references available)
  - Metric diversity vs. computational cost (Gemini-DA requires API calls, BLEU is essentially free)

- **Failure signatures**:
  - Empty output detection (Section 3.3 shows models generate text beyond translation for problematic inputs)
  - Metric disagreement (when neural metrics and surface metrics diverge significantly)
  - Script/encoding issues (visible in score tables as anomalously low scores)

- **First 3 experiments**:
  1. Validate pipeline by reproducing WMT24 results for overlapping languages - should match published scores exactly
  2. Spot-check low-resource language translations with native speakers to verify metric reliability
  3. Compare QE metric rankings vs. reference-based rankings per language family to identify systematic biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the OpenAI o1 model refuse to translate profanity in Gujarati (gu_IN) at a significantly higher rate than in other languages?
- Basis in paper: [explicit] Figure 19 states that OpenAI o1 refused to translate input text for gu_IN due to profanity but "did not refuse to translate in other languages at the same rate."
- Why unresolved: The paper observes this discrepancy but does not provide a technical explanation for the safety alignment behavior specific to Gujarati.
- What evidence would resolve it: An ablation study analyzing safety triggers across different languages or a detailed report from the model developers regarding language-specific safety filters.

### Open Question 2
- Question: How do the performance rankings of the translation systems differ when evaluated specifically on the "Speech" domain versus the "Social" domain?
- Basis in paper: [explicit] The text identifies "Speech source" (Figure 20) and "Social source" (Figure 21) as distinct data types, while the evaluation tables (Figures 10-18) provide only aggregate scores per language.
- Why unresolved: The provided scores average performance across domains, making it impossible to determine if certain systems (e.g., speech-optimized models) outperform general-purpose LLMs in the Speech category.
- What evidence would resolve it: A breakdown of metric scores for each system, separated by the "Speech" and "Social" source domains.

### Open Question 3
- Question: Which of the reported automated metrics (BLEU, ChrF, MetricX, XCOMET, or Gemini-DA) best correlates with human judgment for this year's submissions?
- Basis in paper: [explicit] The paper lists scores for five distinct metrics, and the "Avg." rankings of systems vary between them (e.g., comparing Figure 10 vs. Figure 14).
- Why unresolved: Without accompanying human evaluation scores (e.g., Direct Assessment) in the provided text, the relative validity and reliability of these metrics for the WMT24 systems remain unconfirmed.
- What evidence would resolve it: Correlation coefficients calculated between these automated metrics and human evaluation ground truth.

## Limitations
- Automatic metrics may be unreliable for low-resource languages due to lack of training data and validation
- Small dataset size (998 sentences) limits statistical power for detecting meaningful differences
- Quality estimation metrics without references cannot be validated against ground truth for most languages

## Confidence
- Claim: Automatic metrics accurately rank translation quality across all 55 languages -> Low confidence
- Claim: Frontier LLMs outperform human translations -> Low confidence (requires human validation)
- Claim: Traditional MT providers consistently outperform Amazon/Microsoft -> Medium confidence
- Claim: Closely related languages show significant performance differences -> High confidence

## Next Checks
1. Conduct human evaluation for a stratified sample of 10-15 low-resource languages to validate automatic metric rankings and identify systematic biases.
2. Perform ablation studies comparing QE metric rankings against reference-based rankings on the subset of languages where human references exist to quantify QE metric reliability.
3. Analyze metric agreement patterns by language family to identify whether certain linguistic features (script, morphology, word order) systematically affect metric reliability.