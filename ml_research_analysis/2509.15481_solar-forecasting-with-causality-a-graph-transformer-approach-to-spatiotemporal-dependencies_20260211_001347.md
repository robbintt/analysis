---
ver: rpa2
title: 'Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal
  Dependencies'
arxiv_id: '2509.15481'
source_url: https://arxiv.org/abs/2509.15481
tags:
- forecasting
- solar
- time
- data
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate solar irradiance
  forecasting, which is critical for effective renewable energy management. The authors
  propose SolarCAST, a causally informed model that predicts future global horizontal
  irradiance (GHI) at a target site using only historical GHI data from the target
  and nearby stations, avoiding reliance on specialized hardware like sky cameras
  or satellite imagery.
---

# Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies

## Quick Facts
- **arXiv ID:** 2509.15481
- **Source URL:** https://arxiv.org/abs/2509.15481
- **Reference count:** 29
- **Key result:** SolarCAST achieves 25.9% error reduction vs. Solcast with only historical GHI data, no specialized hardware needed.

## Executive Summary
This paper addresses the challenge of accurate solar irradiance forecasting, which is critical for effective renewable energy management. The authors propose SolarCAST, a causally informed model that predicts future global horizontal irradiance (GHI) at a target site using only historical GHI data from the target and nearby stations, avoiding reliance on specialized hardware like sky cameras or satellite imagery. SolarCAST models three types of confounding factors—observable synchronous variables, latent synchronous factors, and time-lagged influences—using scalable neural components: an embedding layer, a spatio-temporal graph neural network, and a gated transformer. The model outperforms leading time-series and multimodal baselines across diverse geographic conditions and achieves a 25.9% error reduction compared to the top commercial forecaster, Solcast. This demonstrates SolarCAST's practical value as a lightweight, generalizable solution for localized solar forecasting.

## Method Summary
SolarCAST is a causally-informed neural network for predicting future global horizontal irradiance (GHI) at a target location using historical GHI from the target and nearby stations. It uses a Structural Causal Model framework to identify three types of confounding factors behind observed correlations: observable synchronous variables (e.g., time-of-day), latent synchronous factors (e.g., regional weather patterns), and time-lagged influences (e.g., cloud movement). These are modeled by three scalable neural components: an embedding layer (for C1), a spatio-temporal graph neural network (for C2), and a gated transformer with fixed temporal gaps (for C3). The components are combined via a learnable fusion mechanism, and the model is trained end-to-end to minimize MSE loss, using AdamW optimizer with poly decay.

## Key Results
- SolarCAST achieves 25.9% error reduction vs. Solcast on 2-hour ahead GHI prediction using only historical GHI data.
- Outperforms all baselines (Medium, GCNN, DCRNN, SOLAR-GUIDE) across all geographic regions (Urban, Valley, Lake, Mountain).
- Ablation studies confirm contributions of each component: removing STGL causes largest error increase (70.787 → 80.037 RMSE).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing correlations between target and auxiliary stations into three causal confounder types—observable synchronous, latent synchronous, and time-lagged—and mapping each to specialized neural components improves forecasting accuracy.
- **Mechanism:** The model uses a Structural Causal Model (SCM) framework to identify that correlations between target site X and nearby stations S arise from: (C1) time-of-day and station identity affecting all sensors simultaneously, (C2) unobserved regional weather patterns creating shared influences, and (C3) moving clouds introducing delayed directional dependencies. Each confounder type is routed to a dedicated architectural component.
- **Core assumption:** The correlation structure is separable into these three distinct causal pathways, and each pathway benefits from different modeling approaches rather than a monolithic architecture.
- **Evidence anchors:**
  - [abstract]: "SolarCAST models three classes of confounding factors behind X-S correlations using scalable neural components: (i) observable synchronous variables... (ii) latent synchronous factors... (iii) time-lagged influences..."
  - [section 2]: Figure 1 illustrates C1, C2, C3 causal relationships; text states "we identify three types of confounders that may underlie the observed correlations between X and S"
  - [corpus]: Limited direct validation; CauSTream (arxiv 2512.16046) applies causal decomposition to spatiotemporal streamflow forecasting with similar rationale but different domain
- **Break condition:** If confounders are not practically separable (e.g., C2 and C3 interact non-linearly), or if additional confounding pathways exist unmodeled, performance gains may not replicate.

### Mechanism 2
- **Claim:** Learning soft, data-driven adjacency matrices captures spatial dependencies in solar networks better than fixed distance-based graphs because spatial proximity does not guarantee predictive relevance.
- **Mechanism:** The Spatio-Temporal Graph Learner (STGL) computes a continuous adjacency matrix via node embeddings and trainable weights (Equations 3-5), producing dense, asymmetric connectivity. This avoids underfitting from static distance-based adjacency and overfitting from fully end-to-end learned graphs on small networks.
- **Core assumption:** True spatial dependencies in solar irradiance are dynamic and not purely distance-driven; local cloud dynamics create non-intuitive spatial relationships that data-driven methods can recover.
- **Evidence anchors:**
  - [abstract]: "latent synchronous factors (e.g., regional weather patterns), captured by a spatio-temporal graph neural network"
  - [section 3.2]: "spatial proximity doesn't guarantee predictive relevance due to local cloud dynamics. We adopt a continuous, data-driven adjacency formulation... to learn soft, unidirectional dependencies"
  - [corpus]: No direct corpus validation for solar-specific adjacency learning; related STGNN work in traffic forecasting is cited but domains differ
- **Break condition:** If learned adjacency overfits to training periods and fails to generalize across seasons or geographic regions; if physical constraints (e.g., prevailing wind direction) are necessary but not encoded.

### Mechanism 3
- **Claim:** Enforcing a fixed temporal gap between query and support patches in a transformer with top-k sparse attention and gating explicitly captures delayed dependencies such as cloud movement across stations.
- **Mechanism:** The Segment Gated Transformer (SGT) extracts a query patch from the target node's recent data and support patches from all nodes' historical data with a fixed gap r (e.g., r=3 time steps). Top-k attention restricts each query to attend only to the most relevant support tokens, and a gated residual suppresses noisy connections, emphasizing high-impact delayed relationships.
- **Core assumption:** Informative predictive signals arrive with consistent delays (e.g., clouds propagate at quasi-regular speeds), and a fixed gap r plus attention can capture this; spurious correlations can be filtered by gating.
- **Evidence anchors:**
  - [abstract]: "time-lagged influences (e.g., cloud movement across stations), modeled with a gated transformer that learns temporal shifts"
  - [section 3.3]: "A fixed temporal gap r is enforced between the query and support segments to encourage the model to learn delayed dependencies between current and past information"
  - [corpus]: Weak corpus evidence; related causal spatiotemporal models (CauSTream, air quality forecasting paper) mention temporal dependencies but not transformer-specific gap mechanisms
- **Break condition:** If optimal delays vary significantly by weather regime (e.g., fast-moving fronts vs. stationary fog) and a single fixed gap cannot adapt; if top-k attention discards weak but causally informative connections.

## Foundational Learning

- **Concept: Structural Causal Models (SCM) for time series**
  - **Why needed here:** The paper explicitly uses SCM to decompose observed correlations into confounder types and guide architectural design, not just for interpretability.
  - **Quick check question:** Given the three confounder types (C1: observable synchronous, C2: latent synchronous, C3: time-lagged), which neural component in SolarCAST handles each, and why would combining them in one monolithic module be suboptimal?

- **Concept: Learned vs. fixed graph adjacency in GNNs**
  - **Why needed here:** Unlike traffic or power grids with fixed topology, solar sensor networks lack intrinsic connectivity; adjacency must be inferred from data.
  - **Quick check question:** Why might a distance-based adjacency matrix underperform for solar forecasting compared to a learned adjacency, even though sensors are geographically distributed?

- **Concept: Sparse attention and gating in transformers**
  - **Why needed here:** Full self-attention over long spatiotemporal sequences is costly and may learn spurious correlations; top-k attention and gating enforce selectivity.
  - **Quick check question:** How does enforcing a temporal gap r between query and support patches differ from standard self-attention, and what type of causal pattern does this target?

## Architecture Onboarding

- **Component map:**
Input: Historical GHI from target X and nearby stations S (n × T × d tensor)
   ↓
Embedding Layer (EMB): Concat(GHI, time-of-day embedding, node identity embedding)
   ↓
[PARALLEL BRANCHES]
   ├→ STGL: TGCL (temporal gated conv, k=3) → SMP (spatial message passing, K=3) → LN
   │         Captures: latent synchronous dependencies (C2)
   └→ SGT: Segment Extractor (query + support patches, gap r) → Top-k Attention → Gated Residual
             Captures: time-lagged dependencies (C3)
   ↓
Learnable Fusion: H_out = α * H_tr + (1-α) * H_gr,  α ∈ (0,1) computed per sample
   ↓
Output: Predicted GHI at target site, horizon h (2-hour default)

- **Critical path:**
  1. Embedding encodes C1 confounders (time, node identity) directly into features.
  2. STGL and SGT process in parallel—no sequential dependency between branches.
  3. Learnable α weights the two pathways per sample based on current query patch, allowing dynamic emphasis.

- **Design tradeoffs:**
  - **Fixed gap r vs. adaptive delays:** Fixed gap simplifies training but may not match variable cloud propagation speeds; an adaptive mechanism could improve flexibility at cost of complexity.
  - **Data-driven adjacency vs. physical constraints:** Learned adjacency is flexible but ignores domain knowledge (e.g., prevailing winds); hybrid approaches could encode physics priors.
  - **Top-k sparsity vs. complete attention:** Top-k reduces computation and noise but risks missing weak signals; k is a hyperparameter sensitive to dataset scale.

- **Failure signatures:**
  - **STGL overfitting:** Learned adjacency generalizes poorly to new regions or seasons—monitor performance drift when deploying to unseen locations.
  - **SGT gap mismatch:** If actual delays diverge from r, attention attends to irrelevant history—inspect attention maps for systematic misalignment.
  - **α imbalance:** If α saturates near 0 or 1, one pathway dominates—log α distributions during training to detect collapse.
  - **Sparse auxiliary data:** With only 7-11 nearby stations, missing or faulty sensors may degrade performance—validate robustness to station dropout.

- **First 3 experiments:**
  1. **Ablation by component:** Remove EMB, STGL, and SGT individually (Table 3 shows w/o STGL causes largest RMSE increase: 70.787 → 80.037). Confirm each confounder pathway contributes measurably.
  2. **Geographic transfer:** Train on one region (e.g., Urban), test on others (Valley, Lake, Mountain) to probe generalization of learned adjacency and temporal gaps.
  3. **Gap sensitivity:** Sweep r ∈ {1, 3, 6, 12} time steps to characterize how performance varies with assumed delay; identify if optimal r correlates with typical cloud speeds in each region.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SolarCAST's performance degrade as the density of nearby auxiliary stations decreases below the 7-11 stations evaluated?
- Basis in paper: [inferred] The abstract claims the model is "generalizable" and "scalable" for "sparse or uneven sensor distributions," but the experimental datasets (Table 1) consistently utilize a relatively dense network of 7 to 11 nearby stations.
- Why unresolved: The model's graph neural network component relies on spatial message passing; it is unclear if the learned graph structure remains effective when the spatial context is critically under-sampled.
- What evidence would resolve it: An ablation study systematically reducing the number of auxiliary stations N to determine the lower bound of sensor density required to outperform univariate baselines.

### Open Question 2
- Question: Does the Segment Gated Transformer (SGT) capture time-lagged dependencies effectively for prediction horizons significantly longer than the tested 2-hour window?
- Basis in paper: [inferred] The experimental setup restricts the prediction horizon h to 2 hours, justified by the need for "short-term forecasting," despite the broader energy management need for day-ahead predictions.
- Why unresolved: The SGT is designed to model cloud movement (time-lagged confounders), a dynamic which inherently possesses a short predictive lifetime; the utility of this mechanism may diminish for longer horizons.
- What evidence would resolve it: Benchmarking SolarCAST against baselines using horizons of 6, 12, and 24 hours to observe the decay rate of the error reduction benefits.

### Open Question 3
- Question: To what extent do the learned dynamic adjacency matrices (A) correspond to physical reality, such as actual wind direction or topography?
- Basis in paper: [inferred] The paper claims "strong model interpretability" and uses a Structural Causal Model to frame the problem, yet the adjacency matrix is purely data-driven (Eq. 5) without physical constraints.
- Why unresolved: The model optimizes for predictive accuracy, which allows it to learn useful correlations that may not align with physical causality (e.g., connecting stations that share a common weather driver rather than a direct causal link).
- What evidence would resolve it: A qualitative analysis comparing the learned edge weights against external physical data, such as historical wind rose diagrams or topographical maps, during specific weather events.

## Limitations

- **Learned adjacency validation:** The data-driven adjacency mechanism lacks direct validation against physical cloud dynamics or wind patterns, making it unclear whether performance gains reflect true spatial dependencies or overfitting.
- **Fixed temporal gap assumption:** The SGT uses a single fixed temporal gap (r=3), which may not adapt to varying cloud propagation speeds across different weather regimes or geographic regions.
- **Limited geographic diversity:** Experiments use data from only one geographic region (Switzerland), limiting generalizability claims to other climates and topographies.

## Confidence

- **High Confidence:** The empirical performance claims showing 25.9% error reduction versus Solcast and consistent outperformance across all baselines (Medium, GCNN, DCRNN, SOLAR-GUIDE) are well-supported by the experimental results.
- **Medium Confidence:** The three-confounder causal decomposition framework is logically sound and architecturally coherent, but limited validation exists for whether the separable modeling approach is necessary versus monolithic alternatives.
- **Low Confidence:** The specific choices of fixed gap r=3 and top-k=5 attention parameters lack sensitivity analysis, and it's unclear if these are optimal or even appropriate across different geographic regions and weather patterns.

## Next Checks

1. **Causal Component Isolation:** Design an experiment that removes the causal decomposition framework entirely (using a single monolithic architecture) while keeping all other components identical. This would directly test whether the three-confounder separation provides measurable benefit beyond architectural complexity.

2. **Geographic Transfer Robustness:** Train SolarCAST on one geographic region (e.g., Urban) and evaluate on all other regions without fine-tuning. This would validate whether the learned adjacency and temporal gap generalize across different physical environments or overfit to specific local patterns.

3. **Temporal Gap Sensitivity Analysis:** Systematically sweep the temporal gap parameter r across a wider range (e.g., r ∈ {1, 2, 3, 6, 12}) and correlate optimal values with measured cloud propagation speeds in each region. This would test whether the fixed gap assumption holds and identify if adaptive mechanisms are needed.