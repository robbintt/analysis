---
ver: rpa2
title: 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search'
arxiv_id: '2509.07969'
source_url: https://arxiv.org/abs/2509.07969
tags:
- arxiv
- turns
- visual
- reasoning
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces Mini-o3, a visual search system that generates
  deep, multi-turn reasoning trajectories (up to tens of steps) and achieves state-of-the-art
  performance on challenging visual search tasks. The method employs three key components:
  a challenging Visual Probe Dataset designed to require trial-and-error exploration,
  an iterative cold-start data collection pipeline that synthesizes diverse reasoning
  patterns via in-context learning, and an over-turn masking strategy in reinforcement
  learning that avoids penalizing over-turn trajectories to enable test-time scaling
  of interaction depth.'
---

# Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search

## Quick Facts
- arXiv ID: 2509.07969
- Source URL: https://arxiv.org/abs/2509.07969
- Reference count: 40
- Primary result: Achieves 48.0% accuracy on VisualProbe-Hard, significantly outperforming DeepEyes (35.1%) and other open-source models

## Executive Summary
Mini-o3 introduces a visual search system that generates deep, multi-turn reasoning trajectories up to tens of steps, achieving state-of-the-art performance on challenging visual search tasks. The system employs three key components: a Visual Probe Dataset requiring trial-and-error exploration, an iterative cold-start data collection pipeline synthesizing diverse reasoning patterns via in-context learning, and an over-turn masking strategy in reinforcement learning that avoids penalizing over-turn trajectories. Despite training with only six interaction turns, Mini-o3 produces trajectories that naturally scale to tens of turns at inference time, with accuracy improving as interaction depth increases.

## Method Summary
Mini-o3 addresses the challenge of scaling reasoning depth in visual search through an innovative approach that decouples training and inference interaction lengths. The method uses a challenging Visual Probe Dataset designed to require extensive exploration, an iterative data collection pipeline that synthesizes diverse reasoning patterns through in-context learning, and a unique over-turn masking strategy during reinforcement learning training. This masking prevents the model from being penalized for over-turn trajectories, enabling the system to learn complex reasoning patterns without overfitting to shorter paths. The key innovation is that the model trains on only six interaction turns but can naturally extend to tens of turns during inference, with performance improving as reasoning depth increases.

## Key Results
- Achieves 48.0% accuracy on VisualProbe-Hard, significantly outperforming DeepEyes (35.1%) and other open-source models
- Successfully generates reasoning trajectories that scale from 6 training turns to 10-30 inference turns
- Over-turn masking strategy proves essential for enabling complex reasoning without premature termination

## Why This Works (Mechanism)
Mini-o3's success stems from its ability to learn rich reasoning patterns through synthetic data generation and its strategic approach to handling trajectory length during training. The over-turn masking technique allows the model to explore complex reasoning paths without being penalized for turns that might seem unnecessary in shorter trajectories but prove valuable in deeper reasoning chains. This enables the model to develop flexible reasoning strategies that can adapt to the complexity of the task at hand, naturally extending its reasoning depth when needed during inference.

## Foundational Learning
- Visual Search Reasoning: Understanding how AI agents navigate and search through visual information spaces through sequential decision-making
  - Why needed: Visual search requires systematic exploration of complex visual spaces, not just single-step recognition
  - Quick check: Can the model plan multi-step search strategies given visual input and constraints?
- In-Context Learning: The ability to learn from demonstrations provided within the input prompt without gradient updates
  - Why needed: Enables synthesis of diverse reasoning patterns from limited human demonstrations
  - Quick check: Does the model generate appropriate reasoning patterns when given different demonstration styles?
- Reinforcement Learning with Masking: Training strategies that selectively ignore certain penalties to shape desired behaviors
  - Why needed: Prevents premature termination of potentially valuable reasoning paths during training
  - Quick check: Does the masking strategy improve final performance compared to standard RL approaches?

## Architecture Onboarding
**Component Map**: Visual Probe Dataset -> Cold-Start Data Collection -> In-Context Learning Synthesis -> Over-turn Masking RL Training -> Inference Scaling

**Critical Path**: The most critical sequence is Visual Probe Dataset → Cold-Start Data Collection → In-Context Learning Synthesis → Over-turn Masking RL Training, as this determines the quality and diversity of reasoning patterns the model can generate.

**Design Tradeoffs**: The system trades computational efficiency during training (by masking over-turns) for improved reasoning depth and accuracy at inference time. This allows for natural scaling of interaction turns but may introduce inefficiencies in shorter reasoning scenarios.

**Failure Signatures**: The model may generate unnecessarily long reasoning paths in simple scenarios, or struggle with visual search tasks that require fundamentally different reasoning strategies than those captured in the Visual Probe Dataset.

**First Experiments**:
1. Validate that over-turn masking during training enables longer inference trajectories without catastrophic forgetting of shorter path reasoning
2. Test whether the synthetic reasoning patterns generated through in-context learning are diverse enough to handle novel visual search scenarios
3. Measure the relationship between inference turn count and accuracy to confirm the claimed performance improvement with deeper reasoning

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Scalability to real-world visual search tasks beyond controlled VisualProbe-Hard dataset remains uncertain
- Over-turn masking strategy may introduce biases favoring longer, potentially inefficient search paths
- Heavy reliance on in-context learning from limited human demonstrations raises questions about diversity of synthesized reasoning patterns
- Computational overhead of generating and evaluating deep reasoning trajectories may limit practical applicability

## Confidence
- **High Confidence**: The performance improvement of Mini-o3 over baselines like DeepEyes on VisualProbe-Hard is well-supported by the reported results.
- **Medium Confidence**: The effectiveness of the over-turn masking strategy in enabling complex reasoning is plausible but requires further empirical validation on diverse datasets.
- **Low Confidence**: The claim that Mini-o3's approach generalizes to real-world visual search tasks without significant modifications or retraining is speculative and lacks supporting evidence.

## Next Checks
1. Evaluate Mini-o3's performance on real-world visual search datasets with varying levels of complexity, noise, and domain specificity to assess generalizability.
2. Conduct ablation studies to isolate the impact of the over-turn masking strategy on trajectory efficiency and accuracy, particularly in scenarios where shorter paths may be optimal.
3. Measure the computational overhead and inference latency of Mini-o3 compared to baseline models, especially as the number of interaction turns scales up, to determine practical feasibility.