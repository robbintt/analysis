---
ver: rpa2
title: Multilingual Routing in Mixture-of-Experts
arxiv_id: '2510.04694'
source_url: https://arxiv.org/abs/2510.04694
tags:
- experts
- layers
- routing
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates how multilingual tokens are routed in Mixture-of-Experts
  (MoE) large language models. Through a systematic analysis of routing divergence
  from English across multiple layers, languages, and models, the authors reveal a
  consistent U-shaped pattern: higher divergence in early and late layers, with lower
  divergence (i.e., more cross-lingual expert sharing) in middle layers.'
---

# Multilingual Routing in Mixture-of-Experts

## Quick Facts
- arXiv ID: 2510.04694
- Source URL: https://arxiv.org/abs/2510.04694
- Reference count: 30
- Authors: Lucas Bandarkar; Chenyuan Yang; Mohsen Fayyaz; Junlin Hu; Nanyun Peng
- Primary result: Simple inference-time steering interventions yield 1-2% performance gains on multilingual benchmarks across three MoE models

## Executive Summary
This work investigates how multilingual tokens are routed in Mixture-of-Experts (MoE) large language models. Through a systematic analysis of routing divergence from English across multiple layers, languages, and models, the authors reveal a consistent U-shaped pattern: higher divergence in early and late layers, with lower divergence (i.e., more cross-lingual expert sharing) in middle layers. This layer-wise behavior correlates strongly with language performance, suggesting that middle-layer expert sharing is crucial for multilingual generalization. Building on this insight, the authors introduce inference-time steering interventions that promote activation of task-specific experts frequently used in English, particularly in the middle layers. These simple, model-agnostic interventions yield consistent 1-2% performance gains on two multilingual benchmarks (MGSM and Global-MMLU) across three state-of-the-art MoE models and 15+ languages.

## Method Summary
The authors analyze routing divergence patterns across layers by computing entropy-normalized JS-divergence between expert activation distributions for different languages versus English. They identify task-specific experts by comparing activation frequencies between task data (GSM8K-Instruct for math, AlpaCare-MedInstruct for medicine) and a baseline (FLORES-English), selecting experts where the difference exceeds a threshold τ. Inference-time interventions modify router logits before softmax: soft interventions add λ·std(z) to selected expert logits, while hard interventions force activation by setting logits to max(z)+ε. These interventions are applied only in middle layers identified from divergence plots where cross-lingual alignment occurs. The approach is evaluated on MGSM (math reasoning, 10 languages, 250 samples) and Global-MMLU medicine subset (13 languages, 420 samples) across three MoE models with different architectures.

## Key Results
- MoE models exhibit consistent U-shaped routing divergence: high divergence in early/late layers, low divergence (cross-lingual alignment) in middle layers
- Routing alignment with English in middle layers correlates strongly with language performance (higher alignment → better performance)
- Inference-time steering of middle-layer task experts (identified from English) yields 1-2% performance gains on MGSM and Global-MMLU
- Interventions targeting multilingual-specialized experts degrade performance, while task-specific expert steering succeeds
- Effectiveness is consistent across three different MoE architectures and 15+ languages

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Routing Divergence Pattern
MoE models exhibit a U-shaped routing divergence pattern across layers, with language-specific routing in early/late layers and cross-lingual alignment in middle layers. Tokens from different languages are routed to different experts during input embedding (early layers) and output generation (late layers), but converge on similar experts during semantic processing (middle layers). This mirrors dense LLM parameter-sharing patterns but with discrete, observable expert selection. The observed routing alignment reflects functional sharing of semantic processing, not just statistical artifact. Evidence includes consistent U-shaped patterns across all languages and models tested, with higher divergence in first and last layers than intermediate layers. The pattern may break if languages with very different scripts/typologies show no middle-layer convergence.

### Mechanism 2: Routing Alignment Correlates with Language Performance
A language's routing similarity to English in middle layers correlates with the model's performance on that language. Languages the model "understands" better are mapped more effectively to the shared semantic space (middle layers), resulting in expert activation patterns similar to English. Poorly understood languages fail to reach this shared space and maintain high routing divergence throughout. Correlation reflects causal limitation—the model's ability to leverage shared experts constrains multilingual generalization. Evidence includes strong correlation between routing divergence and performance metrics across multiple languages, with languages showing lower divergence achieving better results. The correlation may be spurious if confounded by language family/script similarity.

### Mechanism 3: Middle-Layer Expert Steering Improves Multilingual Performance
Inference-time intervention that promotes task-specific experts (identified from English data) in middle layers improves multilingual performance by 1-2%. By boosting logits for experts frequently activated for a task in English, non-English inputs are forced through the same semantic processing pathway. This works only in middle layers where language-universal computation occurs; interventions elsewhere degrade performance. The task experts identified from English data encode language-universal reasoning that transfers across languages. Evidence includes statistically significant improvements when steering middle-layer task experts, with no gains from intervening in other layers. Gains may disappear on tasks requiring different reasoning patterns.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**
  - Why needed: The entire paper analyzes router behavior; you must understand how tokens are assigned to experts via softmax over logits.
  - Quick check: Given a 128-expert layer with top-8 routing, what happens to tokens when you boost one expert's logit by +0.5σ?

- **Cross-lingual Transfer in LLMs**
  - Why needed: The paper builds on prior findings that middle layers contain shared semantic representations across languages.
  - Quick check: Why might intervening on early/late layers fail for cross-lingual transfer, even if those layers show strong language signals?

- **Jensen-Shannon Divergence with Entropy Normalization**
  - Why needed: The paper's core metric for comparing routing distributions across layers; raw JS-divergence is confounded by layer-wise entropy changes.
  - Quick check: If routing entropy drops from 5.0 to 2.0 across layers, why does raw JS-divergence also drop even if the true routing difference is constant?

## Architecture Onboarding

- **Component map**: Router logits (E-dimensional vector per token) → softmax → top-K selection → expert outputs → weighted aggregation
- **Critical path**: 1) Compute expert importance q per sequence via mean-pooled routing weights; 2) Identify task experts using English in-domain data and Δ threshold (τ=0.3-0.5); 3) Apply soft intervention (λ=0.5) or hard intervention during inference on multilingual data; 4) Evaluate on parallel benchmarks (MGSM, Global-MMLU)
- **Design tradeoffs**: Soft vs. hard intervention (soft more stable, hard more invasive); threshold τ (higher = fewer experts, more conservative; lower = more experts, risk of degradation); layer selection (must target language-universal middle layers)
- **Failure signatures**: Intervening on >10% of experts in a layer → model degrades; activating multilingual experts instead of task experts → no gains, often degradation; targeting early/late layers → performance drops; combining interventions across layers → gains nullified
- **First 3 experiments**: 1) Replicate divergence plots: Run FLORES-200 through your MoE model, compute entropy-normalized JS-divergence per layer vs. English for 5-10 languages. Verify U-shape; 2) Expert identification sanity check: For GSM8K-Instruct vs. FLORES baseline, plot Δ distribution. Confirm right-skewed with clear specialized experts above τ=0.3; 3) Single-layer intervention pilot: Pick one middle layer, identify top-5 task experts, apply soft intervention (λ=0.5) on MGSM-Bengali. Measure delta vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanism causes the anomalous routing behavior in the first layer of Phi-3.5-MoE, where a small subset of experts is activated for all languages?
- Basis in paper: Section 4.3 (Finding 1) states, "PHI-3.5-MOE perplexingly activates the same few experts in the first layer for all languages... We are unable to find an explanation for this."
- Why unresolved: The authors verified the behavior but offered only a hypothesis regarding "very poor load-balancing" without confirming the root cause.
- What evidence would resolve it: An ablation study analyzing the distribution of router weights versus expert norms in the first layer, or a comparison of training dynamics with different load-balancing loss coefficients.

### Open Question 2
- Question: Why does boosting "multilingual-specialized experts" result in performance degradation, while boosting "task experts" improves performance?
- Basis in paper: Section 5.4 and the Abstract note that "interventions... targeting multilingual-specialized experts only yield performance degradation," creating a contrast with the success of task-specific steering that is not mechanistically explained.
- Why unresolved: While the paper establishes that distinct experts exist for languages and tasks, it does not explain why utilizing the language-specific experts is detrimental to performance during inference.
- What evidence would resolve it: Analysis of the hidden state representations when multilingual experts are forced active to determine if they push the representation out of the language-universal semantic space identified in middle layers.

### Open Question 3
- Question: Can the cross-lingual routing alignment in middle layers be enforced during pre-training via an auxiliary loss function to improve multilingual generalization natively?
- Basis in paper: Section 7 states the findings "motivate future work on methods that enhance cross-lingual routing alignment... [and] training approaches that exploit this natural division."
- Why unresolved: The current work demonstrates that inference-time intervention works, but it leaves unexplored whether this alignment can be learned as a fundamental property of the model during training.
- What evidence would resolve it: Training a new MoE model with a loss term penalizing routing divergence between English and other languages in middle layers, then evaluating zero-shot performance without inference interventions.

### Open Question 4
- Question: Does the correlation between middle-layer routing alignment and performance generalize to open-ended generative tasks like translation or summarization?
- Basis in paper: Section 5.1 restricts evaluation to MGSM (math) and Global-MMLU (medicine), which are discriminative tasks involving reasoning, leaving generative capabilities untested.
- Why unresolved: Generative tasks may rely more heavily on the language-specific experts in the final layers, which could reduce the efficacy of interventions targeting only the language-universal middle layers.
- What evidence would resolve it: Applying the proposed intervention method to generative benchmarks (e.g., FLORES translation) and measuring the change in metrics like BLEU or ROUGE scores.

## Limitations
- Architectural specificity: The U-shaped routing pattern and intervention effectiveness may be specific to the particular MoE architectures tested
- Causal inference gap: While correlation is shown, definitive causation between routing alignment and performance is not established
- Generalization beyond tested tasks: Effectiveness for other reasoning types, creative tasks, or generation tasks remains unexplored

## Confidence
- High Confidence: The empirical observation of U-shaped routing divergence across layers is robust across all three tested models and 15+ languages
- Medium Confidence: The correlation between routing alignment and performance is statistically significant but the causal mechanism remains inferential
- Low Confidence: The generalizability of these findings to other MoE architectures, task types, or to causal claims about routing alignment's role in multilingual performance

## Next Checks
1. **Architectural Transfer Test**: Apply the same methodology to a different MoE architecture (e.g., DeepSeekMoE) to verify whether the U-shaped pattern and intervention effectiveness generalize beyond the three tested models.

2. **Causal Intervention Validation**: Design an experiment that directly manipulates routing alignment (e.g., force-alignment of non-English languages to English routing patterns in middle layers) and measures whether this causes performance changes independent of language similarity factors.

3. **Task Generalization Test**: Apply the expert steering methodology to a completely different task domain (e.g., code generation, creative writing, or commonsense reasoning) to determine whether task-specific expert identification and steering generalize beyond math and medical reasoning tasks.