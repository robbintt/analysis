---
ver: rpa2
title: 'White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency
  Cost of Symbolic Auditability in Adaptive NMPC'
arxiv_id: '2602.01516'
source_url: https://arxiv.org/abs/2602.01516
tags:
- symbolic
- neural
- adaptation
- control
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies the computational trade-off of enforcing
  symbolic auditability in adaptive neural NMPC for autonomous vehicles. The authors
  propose a white-box architecture using a convex ensemble of frozen neural specialists
  (Modular Sovereignty) within CasADi SX, maintaining full symbolic Jacobians for
  verification.
---

# White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC

## Quick Facts
- **arXiv ID**: 2602.01516
- **Source URL**: https://arxiv.org/abs/2602.01516
- **Reference count**: 19
- **One-line primary result**: White-box convex ensemble of frozen neural specialists incurs 72-102× solver slowdown versus physics models but achieves rapid adaptation (7.3 ms) and up to 97.5% position drift mitigation under compound regime shifts.

## Executive Summary
This paper investigates the computational trade-off of enforcing symbolic auditability in adaptive neural NMPC for autonomous vehicles. The authors propose a white-box architecture using a convex ensemble of frozen neural specialists (Modular Sovereignty) within CasADi SX, maintaining full symbolic Jacobians for verification. Benchmarking shows this incurs a 72-102× slowdown versus compiled parametric physics models, with 96% of solver time spent traversing neural graphs. Functionally, the ensemble achieves rapid adaptation (7.3 ms) and high tracking fidelity under severe compound regime shifts (friction, mass, drag), validating plasticity. Synchronous simulation demonstrates mitigation of position drift (up to 97.5%) versus non-adaptive baselines, though real-time deployment is currently infeasible. The study establishes the structural cost of transparency and identifies avenues for optimization (pruning, asynchronous execution, KANs).

## Method Summary
The approach combines convex combinations of frozen neural specialists with symbolic NMPC to maintain plasticity while enabling formal verification. The method trains multiple neural specialists covering different parameter regimes, then uses a QP governor to blend their outputs based on real-time residuals. The entire system is implemented in CasADi SX to preserve symbolic Jacobians for auditability. Key technical choices include using MLP architectures with tanh activation, Z-score normalization, and a two-phase training protocol (Adam followed by L-BFGS refinement). The NMPC uses a 15-step horizon with specific weight tuning, while the governor employs a sliding window approach with EMA smoothing to select specialist weights.

## Key Results
- 72-102× solver slowdown versus parametric physics models, with 96% of time spent on neural graph traversal
- Adaptation latency of 7.3 ms achieved through rapid specialist weight updates
- Up to 97.5% reduction in position drift compared to non-adaptive baselines under compound regime shifts
- Maintains RMSE tracking error around 4×10⁻⁶ while handling friction, mass, and drag variations

## Why This Works (Mechanism)
The method works by maintaining a portfolio of specialized neural models, each trained on specific parameter regimes. During operation, a QP governor dynamically weights these specialists based on real-time state-derivative residuals, enabling rapid adaptation to changing conditions. The white-box implementation in CasADi SX ensures full symbolic Jacobians are available for formal verification while the convex combination structure guarantees stability through weighted averaging of proven specialists.

## Foundational Learning
**Single-Track Bicycle Model**: Vehicle dynamics representation with Pacejka tire forces. Needed for generating realistic training data across parameter variations. Quick check: Verify state derivatives match analytical solutions for simple maneuvers.
**Modular Sovereignty**: Architecture pattern where frozen specialists are combined via convex weights. Needed to enable rapid adaptation without retraining. Quick check: Confirm weight simplex constraints are properly enforced in QP.
**Symbolic vs Numerical Jacobians**: Trade-off between auditability and computational efficiency. Needed for formal verification requirements. Quick check: Compare CasADi SX vs MX evaluation times for identical models.

## Architecture Onboarding

**Component Map**: Dynamic Model -> Ensemble Network -> NMPC Solver -> Vehicle Plant -> Governor QP -> Ensemble Weights

**Critical Path**: State measurements → Governor QP → Specialist weights → Ensemble dynamics → NMPC optimization → Control inputs

**Design Tradeoffs**: 
- White-box (SX) ensures auditability but incurs 72-102× slowdown
- Frozen specialists enable rapid adaptation but require careful coverage of parameter space
- Convex combination guarantees stability but may limit expressiveness

**Failure Signatures**:
- RMSE plateauing at O(10⁻²) indicates training failure
- Solver time dominated by neural graph traversal (>95%) indicates architectural inefficiency
- Position drift exceeding 10% indicates adaptation failure

**First Experiments**:
1. Generate synthetic training data with uniform sampling and chirp trajectories
2. Train single specialist and verify two-phase protocol convergence
3. Implement NMPC with fixed ensemble weights and verify tracking performance

## Open Questions the Paper Calls Out
1. Can formal stability guarantees, such as Lyapunov bounds or dwell-time conditions, be derived for the Governor's switching logic?
2. To what extent can sensitivity-based pruning reduce the "cost to compute zeros" (72-102× slowdown) without losing functional plasticity?
3. Can an asynchronous architecture successfully decouple the rapid adaptation latency (7.3 ms) from the heavy solver latency to enable real-time deployment?

## Limitations
- Real-time deployment currently infeasible due to 72-102× computational overhead
- White-box constraint precludes more efficient numerical implementations
- No formal stability proofs provided for the adaptation mechanism

## Confidence
- **High Confidence**: Core architectural concept and qualitative findings (adaptation speed, drift mitigation)
- **Medium Confidence**: Quantitative metrics requiring full implementation details (RMSE targets, solver times)
- **Low Confidence**: Absolute performance gap versus black-box alternatives without direct comparison

## Next Checks
1. Implement the Single-Track Bicycle Model and generate training dataset across 8 regime variants
2. Train the 8 PINN specialists with two-phase protocol and verify RMSE convergence to target
3. Benchmark full NMPC solver to confirm neural graph traversal dominates computation time (>95%)