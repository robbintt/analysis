---
ver: rpa2
title: Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining
arxiv_id: '2410.00564'
source_url: https://arxiv.org/abs/2410.00564
tags:
- games
- offline
- learning
- table
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JOWA, a jointly-optimized world-action model
  for scaling offline model-based reinforcement learning across multiple Atari games.
  JOWA uses a shared transformer backbone to jointly optimize both world modeling
  and Q-value criticism, enabling large-scale training with improved stability through
  auxiliary regularization.
---

# Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining

## Quick Facts
- arXiv ID: 2410.00564
- Source URL: https://arxiv.org/abs/2410.00564
- Authors: Jie Cheng; Ruixi Qiao; Yingwei Ma; Binhua Li; Gang Xiong; Qinghai Miao; Yongbin Li; Yisheng Lv
- Reference count: 37
- Primary result: JOWA achieves 78.9% human-level performance on Atari games using only 10% of the dataset, outperforming baselines by 71.4%

## Executive Summary
This paper introduces JOWA (Jointly-Optimized World-Action model), a novel approach for scaling offline model-based reinforcement learning across multiple Atari games. The key innovation is a shared transformer backbone that jointly optimizes world modeling and Q-value criticism, enabling large-scale training with improved stability through auxiliary regularization. A provably efficient and parallelizable planning algorithm compensates for Q-value estimation errors at inference time. The model is pretrained on 15 Atari games with 6 billion tokens, and the largest variant (150M parameters) demonstrates state-of-the-art performance while enabling sample-efficient transfer to unseen games.

## Method Summary
JOWA employs a jointly-optimized world-action model architecture using a shared transformer backbone for both world modeling and Q-value criticism. The model is trained with auxiliary regularization for improved stability and includes a provably efficient, parallelizable planning algorithm that addresses Q-value estimation errors during inference. The approach is scaled through pretraining on 15 Atari games (6 billion tokens), with systematic evaluation of different model capacities. The method demonstrates strong performance on Atari benchmarks and enables effective transfer learning to unseen games with minimal fine-tuning data.

## Key Results
- Achieves 78.9% human-level performance on Atari games using only 10% of the dataset
- Outperforms state-of-the-art baselines by 71.4% on average
- Enables sample-efficient transfer to unseen games, achieving 64.7% DQN-normalized scores with just 5k fine-tuning transitions per game
- Performance scales favorably with model capacity (evaluated across 3 parameter sizes)
- Ablation studies validate the importance of joint optimization and planning components

## Why This Works (Mechanism)
JOWA works by addressing key limitations in offline model-based RL through architectural and algorithmic innovations. The joint optimization of world modeling and Q-value criticism through a shared transformer backbone enables efficient information sharing and stable training at scale. The auxiliary regularization provides additional training signals that improve model robustness. The planning algorithm specifically compensates for Q-value estimation errors that are common in offline settings, while its parallelizable design enables efficient inference. The large-scale pretraining on multiple Atari games creates rich representations that generalize well to unseen environments.

## Foundational Learning
- **Offline RL**: Learning from fixed datasets without environment interaction; needed because real-world data collection is expensive and potentially dangerous; quick check: model must handle distributional shift between training and target data
- **Model-based RL**: Using learned world models for planning; needed to enable imagination and counterfactual reasoning; quick check: model must accurately predict future states and rewards
- **Joint optimization**: Training multiple objectives simultaneously through shared parameters; needed for efficient information sharing and stable training; quick check: loss terms should be balanced and not cause gradient conflicts
- **Transformer architectures**: Self-attention mechanisms for handling variable-length sequences; needed for capturing long-range dependencies in sequential decision-making; quick check: attention patterns should reflect meaningful temporal relationships
- **Transfer learning**: Applying knowledge from source tasks to target tasks; needed to reduce sample complexity for new environments; quick check: pretraining should improve performance on unseen tasks

## Architecture Onboarding

**Component Map**: Input Frames -> Transformer Backbone -> World Model Head + Q-Value Head -> Planning Algorithm -> Action Selection

**Critical Path**: The critical execution path runs from input observation through the shared transformer backbone, splits to both world model and Q-value heads, then feeds into the planning algorithm for action selection. The planning algorithm is essential as it compensates for Q-value estimation errors.

**Design Tradeoffs**: The paper trades increased model complexity (joint optimization, larger transformers) for improved stability and performance. The shared backbone reduces parameter count compared to separate models but requires careful balancing of objectives. The planning algorithm adds computational overhead but enables more reliable performance in offline settings.

**Failure Signatures**: Potential failures include collapse of joint optimization (one head dominating training), planning algorithm failure to compensate for Q-value errors, or pretraining not generalizing to target tasks. The auxiliary regularization is specifically designed to mitigate training instability.

**First Experiments**:
1. Run ablation removing joint optimization to confirm performance degradation
2. Test planning algorithm with synthetic Q-value errors to verify error compensation
3. Evaluate transfer learning with varying amounts of fine-tuning data to establish sample efficiency curve

## Open Questions the Paper Calls Out
None

## Limitations
- Results primarily evaluated on Atari games, which represent a narrow domain with discrete actions and short horizons
- Pretraining corpus of 15 Atari games represents a curated subset that may not capture full diversity needed for robust generalization
- 10% dataset usage claim should be interpreted carefully without full details on selection criteria
- Scaling benefits may not directly translate to more complex environments with continuous action spaces or longer temporal dependencies

## Confidence
- High confidence in: Core architectural contribution and basic performance improvements on Atari games; ablation studies are methodologically sound
- Medium confidence in: Sample-efficient transfer claims (minimal 5k transitions evaluation); scaling relationship with model capacity (limited parameter sweep)
- Low confidence in: Generalization to non-Atari domains, continuous control tasks, or real-world applications

## Next Checks
1. Evaluate JOWA's performance on continuous control benchmarks (DMC Suite, Mujoco) to test generalization beyond discrete action spaces
2. Conduct transfer learning experiments with systematically varied pretraining corpus sizes and game diversities
3. Perform extensive ablation studies on planning algorithm components to isolate contribution of parallelization, error compensation, and planning horizon