---
ver: rpa2
title: Simple and Effective Baselines for Code Summarisation Evaluation
arxiv_id: '2505.19392'
source_url: https://arxiv.org/abs/2505.19392
tags:
- summary
- code
- metrics
- reference
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for evaluating code summarization
  by directly asking a large language model (LLM) to rate the quality of a summary.
  Unlike traditional metrics that only compare text similarity, this approach can
  also consider the code context.
---

# Simple and Effective Baselines for Code Summarisation Evaluation

## Quick Facts
- arXiv ID: 2505.19392
- Source URL: https://arxiv.org/abs/2505.19392
- Reference count: 40
- The authors introduce an LLM-based evaluation method that considers code context and achieves state-of-the-art correlation with human judgments on code summarization quality.

## Executive Summary
This paper introduces a novel method for evaluating code summarization by directly asking a large language model (LLM) to rate the quality of a summary. Unlike traditional metrics that only compare text similarity, this approach can also consider the code context. The authors test their method on two standard human-annotated datasets and find that it consistently outperforms or matches existing metrics like BLEU, METEOR, ROUGE-L, SIDE, and embedding-based approaches. They also introduce a reference-free variant that works just as well and could be used for other tasks, such as flagging poor-quality documentation in codebases. However, they observe that the LLM may favor its own outputs, so they recommend using their method alongside embedding-based metrics to avoid bias. Overall, the method is simple, effective, and offers a solid baseline for future work in code summarization evaluation.

## Method Summary
The authors propose a method called "Ask-LLM" that directly evaluates code summaries using an LLM. The LLM is prompted to rate the quality of a summary by considering both the code snippet and the generated summary (and optionally a reference summary). The prompt includes a role definition (professional software engineer), evaluation criteria, and a chain-of-thought request. The LLM's response is mapped to a numerical score, and Spearman's rank correlation is computed with human ratings. Two variants are tested: reference-based (code + reference + generated summary) and reference-free (code + generated summary only). The method is evaluated on four human-annotated datasets covering Java and Python code.

## Key Results
- Ask-LLM consistently outperforms or matches traditional metrics (BLEU, METEOR, ROUGE-L, SIDE) and embedding-based approaches on human-annotated datasets.
- The reference-free variant (ask-LLM-no-ref) performs just as well as the reference-based variant, with no statistically significant difference.
- Ask-LLM shows severe self-preference bias, with Claude rating its own summaries highest in 92.7% of cases.
- The method is simple to implement and provides a solid baseline for future work in code summarization evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct LLM evaluation captures semantic alignment between code and summary by processing both inputs jointly.
- Mechanism: The prompt provides the function code alongside the generated summary, allowing the LLM to verify whether summary claims are supported by code semantics—a capacity n-gram and embedding metrics lack since they compare text to text only.
- Core assumption: The LLM has sufficient code comprehension to detect factual inconsistencies between summary claims and actual code behavior.
- Evidence anchors:
  - [abstract] "Unlike n-gram and embedding-based baselines, our approach is able to consider the code when giving a score."
  - [section 6] "One explanation could be that our LLM metrics have access to the code, providing a signal that is independent of the poor quality reference summaries."
  - [corpus] Weak direct corpus support; related work (Summary-Mediated Repair) suggests LLMs can surface high-level intent from code, indirectly supporting semantic understanding claims.
- Break condition: If evaluating code in languages poorly represented in LLM training data, semantic verification accuracy may degrade.

### Mechanism 2
- Claim: Reference-free evaluation maintains effectiveness because the LLM uses internalized quality criteria rather than comparison-based scoring.
- Mechanism: Without a reference, the LLM rates summaries against an implicit model of what constitutes adequate documentation—focusing on whether the summary accurately describes the code's purpose and behavior.
- Core assumption: The LLM's internal quality model aligns sufficiently with human evaluator preferences for code summaries.
- Evidence anchors:
  - [abstract] "This allows us to also make a variant that does not consider the reference summary at all"
  - [section 6] "ask-LLM-no-ref is just as effective... no statistically significant difference between the two"
  - [corpus] No direct corpus comparison for reference-free code summarization evaluation; this appears novel to the task.
- Break condition: If summary quality criteria diverge significantly from common documentation patterns in LLM training data (e.g., domain-specific conventions), calibration may suffer.

### Mechanism 3
- Claim: Combining ask-LLM with embedding-based metrics mitigates self-preference bias through orthogonal error modes.
- Mechanism: Embedding metrics compare textual similarity independently of any model identity, while ask-LLM provides semantic verification. Disagreement between them flags potential bias cases.
- Core assumption: Embedding metrics do not share the same self-preference bias as the evaluating LLM.
- Evidence anchors:
  - [abstract] "we recommend using it in conjunction with embedding-based methods to avoid the risk of LLM-specific bias"
  - [section 6.1] "in 92.7% of cases it [Claude] gives its summaries the highest possible rating"
  - [corpus] Corpus does not directly address bias mitigation strategies for code evaluation.
- Break condition: If the same LLM generates summaries used to create the embedding model, correlated biases could persist.

## Foundational Learning

- Concept: Spearman's rank correlation
  - Why needed here: The paper uses this to measure how well metric scores predict human quality ratings; unlike Pearson, it doesn't assume normal distribution.
  - Quick check question: If a metric ranks summaries A > B > C and humans rank them C > A > B, would you expect high or low Spearman correlation?

- Concept: Reference-free vs. reference-based evaluation
  - Why needed here: Understanding when you need a ground-truth summary versus when you can evaluate quality directly affects deployment flexibility.
  - Quick check question: Can you use BLEU to flag poor documentation in a codebase that has no reference summaries?

- Concept: LLM evaluator bias (self-preference)
  - Why needed here: Without awareness, using an LLM to evaluate its own outputs produces inflated scores, invalidating comparisons.
  - Quick check question: If you fine-tune a model on Claude-generated summaries, then use ask-Claude to evaluate it, would you trust the results?

## Architecture Onboarding

- Component map:
  - Input layer: Code snippet + generated summary (+ optional reference summary)
  - Prompt template: Role definition → evaluation criteria → data → chain-of-thought request → response format
  - Scoring: Map LLM text response (Strongly agree → Strongly disagree) to numeric scale
  - Aggregation: Spearman correlation computed across dataset samples

- Critical path:
  1. Format inputs consistently (the authors standardized datasets from multiple sources)
  2. Select LLM and prompt variant (reference vs. no-ref)
  3. Collect scores and compute correlation with human ratings for validation

- Design tradeoffs:
  - Claude/GPT vs. OLMo: Higher cost ($0.012–0.024/query) vs. open-source with compute costs; Claude performed best but prompt was tuned on it (unfair comparison)
  - Reference vs. reference-free: Reference-free enables new use cases (flagging bad docs) but may miss subtle quality dimensions
  - Chain-of-thought: Adds reasoning transparency but increases token cost

- Failure signatures:
  - 92%+ highest-possible ratings when evaluating same-model outputs → self-preference bias
  - High scores for summaries that are fluent but factually incorrect → semantic verification failure
  - Near-zero correlation with length → distinguishes from other NLG evaluation bias patterns

- First 3 experiments:
  1. Reproduce the ask-Claude vs. ask-Claude-no-ref comparison on Roy et al. dataset to verify no statistical difference claim (p=0.6779 reported)
  2. Test self-preference bias: generate summaries with GPT-4, evaluate with ask-GPT vs. ask-Claude, compare score distributions
  3. Hybrid metric test: define a combined score (e.g., geometric mean of ask-LLM and voyage-code-3), measure correlation improvement on held-out portion of Haque et al. data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Ask-LLM evaluation method be adapted to accurately assess specific quality dimensions (conciseness, fluency) rather than predominantly reflecting adequacy?
- Basis in paper: [explicit] Section 6.1 states: "Ask-LLM method can't easily be adapted to different quality dimensions... Looking at specific examples, we found two issues. First, mentioning unrelated issues... Second, inconsistency."
- Why unresolved: The authors attempted prompt variations targeting different dimensions but found the LLM consistently reverted to evaluating adequacy regardless of the prompt framing.
- What evidence would resolve it: Development of a prompting strategy or model architecture that shows statistically significant differentiation between quality dimension scores, validated against human ratings for each dimension separately.

### Open Question 2
- Question: How can self-preference bias (LLMs rating their own outputs higher) be effectively mitigated in LLM-based evaluation?
- Basis in paper: [explicit] Section 6.1 reports: "in 92.7% of cases [Claude] gives its summaries the highest possible rating" and the authors "recommend using these metrics in combination with embedding based methods."
- Why unresolved: The paper identifies the bias and suggests a workaround (combining metrics) but does not propose a solution to eliminate the underlying bias.
- What evidence would resolve it: A debiasing technique that reduces self-preference rates to statistically insignificant levels, or a calibration method that normalizes scores across different generator-evaluator pairings.

### Open Question 3
- Question: How does Ask-LLM evaluation performance generalize to programming languages beyond Java and Python, particularly lower-resource languages?
- Basis in paper: [explicit] The Limitations section states: "our method may be less effective on languages that are less widely used and so less well understood by LLMs."
- Why unresolved: The authors were constrained by available human evaluation datasets, which only covered Java and Python.
- What evidence would resolve it: Evaluation results on human-annotated datasets for languages such as Rust, Go, or Kotlin, showing whether correlation with human judgments remains consistent.

### Open Question 4
- Question: Do summaries rated highly by Ask-LLM metrics actually improve performance on downstream software engineering tasks?
- Basis in paper: [explicit] The Limitations section notes: "All of the studies we use ask raters to assess the quality of the summary directly, rather than assess the impact of the different summaries on downstream tasks."
- Why unresolved: The paper evaluates intrinsic quality (human perception) rather than extrinsic utility (task performance).
- What evidence would resolve it: A user study measuring developer productivity, bug-fixing speed, or code comprehension accuracy when using summaries rated highly by the metric versus those rated poorly.

## Limitations
- The method shows severe self-preference bias, with LLMs rating their own outputs highest in 92.7% of cases.
- All evaluation datasets use Java or mixed Java/Python code, limiting generalization to other programming languages.
- Prompt tuning on Claude makes cross-LLM comparisons potentially unfair.

## Confidence
- High confidence: The core mechanism (LLM can consider code context for evaluation) and empirical finding (ask-LLM matches or exceeds traditional metrics) are well-supported by the data.
- Medium confidence: The reference-free variant's effectiveness is statistically supported but lacks comparative context from other tasks.
- Medium confidence: The self-preference bias finding is robust, but the proposed mitigation strategy lacks direct experimental validation.

## Next Checks
1. Hybrid metric validation: Implement the recommended combination of ask-LLM + voyage-code-3 on a held-out dataset subset. Measure whether the combined score shows improved correlation while reducing bias compared to either metric alone.
2. Cross-LLM fairness test: Generate summaries using GPT-4, evaluate with both ask-GPT and ask-Claude. Compare score distributions to quantify how much performance differences stem from prompt familiarity versus model capability.
3. Language generalization test: Apply ask-LLM to a dataset of non-Java/Python code (e.g., R statistical code or MATLAB engineering scripts). Measure correlation with human ratings to assess whether semantic verification degrades on underrepresented languages.