---
ver: rpa2
title: 'CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs'
arxiv_id: '2510.00579'
source_url: https://arxiv.org/abs/2510.00579
tags:
- vectors
- reasoning
- vector
- arxiv
- learnable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CoT Vectors, compact representations that
  distill task-general multi-step reasoning knowledge from support sets of (Question,
  CoT, Answer) triplets. Inspired by task vectors, CoT Vectors are injected into specific
  layers during inference to guide LLM reasoning.
---

# CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs

## Quick Facts
- **arXiv ID:** 2510.00579
- **Source URL:** https://arxiv.org/abs/2510.00579
- **Authors:** Li Li; Ziyi Wang; Yongliang Wu; Jianfei Cai; Xu Yang
- **Reference count:** 34
- **Primary result:** Compact CoT Vectors outperform baselines and match LoRA while using far fewer parameters, revealing a three-stage reasoning process in LLMs.

## Executive Summary
This work introduces CoT Vectors, compact representations that distill task-general multi-step reasoning knowledge from support sets of (Question, CoT, Answer) triplets. Inspired by task vectors, CoT Vectors are injected into specific layers during inference to guide LLM reasoning. We explore both extracted vectors, derived from activation differences, and learnable vectors, optimized via a teacher-student framework. Extracted vectors reveal a U-shaped layer-wise performance curve, reflecting a three-stage reasoning process in LLMs (perception, reasoning, expression), but suffer from instability in middle layers due to sample-specific representations. Learnable vectors address this by actively distilling generalizable reasoning signals, achieving greater stability and performance across layers. Extensive evaluations on Qwen2.5-Math-7B and LLaMA-3.1-8B-Instruct across GSM8K, MATH, and MMLU-Pro show that CoT Vectors outperform baselines and match parameter-efficient fine-tuning (LoRA) while using far fewer parameters (3.6K–4.2K vs. 10–13M). CoT Vectors also serve as a probe, revealing that their effectiveness depends on latent space structure, information density, acquisition mechanism, and model pre-training differences, offering insights into the functional organization of multi-step reasoning in LLMs.

## Method Summary
CoT Vectors are compact representations that encode multi-step reasoning knowledge, extracted either by averaging activation differences between CoT and non-CoT examples (Extracted), or learned via teacher-student distillation (Learnable). The method involves a support set of (Question, CoT, Answer) triplets. Extracted vectors compute the mean difference in activation at answer tokens across layers. Learnable vectors are trained by aligning a student model's hidden states to a teacher model's, with the vector injected into the student. Vectors are injected at specific transformer layers during inference, with μ=1.0 scaling for Extracted and learned scaling for Learnable. The framework uses KL divergence alignment loss plus cross-entropy, optimized with AdamW and weight decay.

## Key Results
- CoT Vectors outperform zero-shot CoT and match LoRA performance while using 3.6K–4.2K parameters vs. 10–13M for LoRA.
- Extracted vectors show a U-shaped performance curve, with best results at shallow and deep layers, worst in middle layers due to sample-specific representations.
- Learnable vectors achieve stable performance across all layers, overcoming the instability of extracted vectors.
- Effectiveness depends on latent space structure, information density, acquisition mechanism, and pre-training differences between models.

## Why This Works (Mechanism)

### Mechanism 1: Additive Attention Shift
The effect of Chain-of-Thought (CoT) reasoning can be mathematically formalized as a vector addition (shift) to the model’s attention outputs. The paper theorizes that the presence of CoT tokens alters the self-attention mechanism. By decomposing the attention calculation, the authors show that the "CoT path" can be isolated as a residual term (Equation 1 & 8). This term, the "CoT Vector," is added to the standard attention output. Core assumption: task-specific reasoning knowledge can be linearly decoupled from the base attention computation and represented as a constant offset in activation space. Evidence anchors: abstract mentions CoT Vectors are "compact representations that encode task-general... knowledge"; section 3.1 derives the formalization: $SA_{CoT} = SA_{Standard} + \mu \cdot \vec{v}_{CoT}$. Corpus evidence for this specific mathematical derivation is weak; related papers focus on planning or bias rather than the vector arithmetic of reasoning. Break condition: if reasoning relies on complex, non-linear state transitions that cannot be approximated by a static additive vector at a single layer, this mechanism fails.

### Mechanism 2: Three-Stage Layer-Wise Specialization
LLMs process reasoning in a functional pipeline (Perception → Reasoning → Expression), creating distinct "zones" where external vectors are more or less effective. The paper observes a U-shaped performance curve. Shallow layers (Perception) and deep layers (Expression) have unified, generalizable representations where vectors work well. Middle layers (Reasoning) hold high-dimensional, sample-specific states with "no dominant direction," causing extracted vectors to fail due to noise. Core assumption: the functional organization of the network is hierarchical, where "reasoning" complexity peaks in the middle layers and creates a representational bottleneck for transferable vectors. Evidence anchors: abstract notes a "U-shaped layer-wise performance curve" and a "three-stage reasoning process"; section 4.2.1 PCA analysis shows middle layers require significantly more principal components to explain variance, confirming high information density and lack of a unified direction. Corpus: no direct corpus confirmation of this specific U-shaped reasoning mechanism. Break condition: if a model is architecturally homogeneous (no distinct layer specialization), or if reasoning occurs uniformly across all layers, the U-shaped curve will not appear, and middle-layer injection would succeed.

### Mechanism 3: Optimization over Averaging
Actively optimizing a vector (Learnable) yields robust, stable reasoning guidance, whereas passively averaging activation differences (Extracted) is fragile. Extracted vectors merely average the noise of sample-specific activations. Learnable vectors use a teacher-student framework to distill the "reasoning signal" into the vector parameters. This allows the vector to "overcome representational limitations" and avoid the sample-specific noise of middle layers. Core assumption: there exists a generalizable "reasoning direction" that can be isolated via gradient descent, even in layers where the raw signal appears chaotic or high-dimensional. Evidence anchors: abstract states learnable vectors "achieve greater stability and performance"; section 4.2.2 shows learnable vectors maintain a plateau across layers, while extracted vectors fluctuate wildly. Corpus: corpus literature discusses distillation, but does not validate this specific "vector vs. extraction" trade-off for reasoning. Break condition: if the reasoning task is entirely novel and has no overlap with the pre-trained knowledge distribution, the learned vector may overfit to the support set (over-steering) and degrade performance.

## Foundational Learning

- **Concept: Residual Stream & Activation Steering**
  - **Why needed here:** The paper injects vectors directly into the residual stream (hidden states) at specific layers. Understanding how signals propagate and accumulate in transformers is essential to grasp *why* adding a vector changes the output.
  - **Quick check question:** Does adding a vector to layer $L$'s output affect the keys/values of layer $L+1$?

- **Concept: Task Vectors**
  - **Why needed here:** This work extends the "Task Vector" paradigm (Ilharco et al.) from simple classification to complex reasoning. You need to understand the premise that "knowledge = direction in activation space" to follow the paper's intuition.
  - **Quick check question:** How does a "task vector" typically differ from standard fine-tuning weights?

- **Concept: Teacher-Student Distillation**
  - **Why needed here:** The "Learnable CoT Vector" is trained by forcing a student (without CoT context + vector) to mimic a teacher (with CoT context). You must understand soft-label alignment to interpret the loss function ($L_{Align}$).
  - **Quick check question:** Why align hidden states (KL divergence) rather than just matching the final answer token?

## Architecture Onboarding

- **Component map:**
  - Support Set: Triplets of (Question, CoT, Answer)
  - Extraction Module: Computes $\vec{v}_{Extracted} = \text{Avg}(\alpha_{CoT} - \alpha_{Non-CoT})$
  - Learning Module: Teacher model (frozen, full CoT) vs. Student model (frozen, no CoT + learnable vector $\vec{v}_L$)
  - Injection Hook: Intercepts forward pass at layer $k$ and adds $\vec{v}$ to the hidden state

- **Critical path:** The efficacy of the system depends on the **Injection Layer Selection**. For "Extracted" vectors, this is brittle and requires a search (U-curve). For "Learnable" vectors, the paper suggests layer 0 (shallow) is consistently near-optimal.

- **Design tradeoffs:**
  - **Extracted:** Zero training cost, high inference instability, layer-sensitive
  - **Learnable:** Low training cost (3.6K params), robust inference, but risks "over-steering" (collapsing reasoning paths) if trained too long on deep layers

- **Failure signatures:**
  - **Mid-Layer Collapse:** Injecting extracted vectors into middle layers causes performance to drop below baseline due to sample-specific noise
  - **Over-Steering:** Learnable vectors trained with high learning rates on deep layers cause the model to output repetitive or collapsed reasoning (Section 4.2.2, Figure 4)

- **First 3 experiments:**
  1. **Layer Sensitivity Scan:** Run inference on GSM8K while injecting a zero-vector (or noise) into every layer individually to establish a baseline for sensitivity.
  2. **Extracted vs. Baseline:** Extract a vector from a 100-sample support set and inject it into Layer 0 vs. Layer 15 (middle). Verify the U-shape performance dip at Layer 15.
  3. **Learnable Vector Overfit:** Train a learnable vector on a small dataset (e.g., 50 samples) with a high learning rate and observe the "aggressive shift" in the t-SNE plot (clustering/collapse).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive or instance-specific vectorization strategies overcome the limitations of static task-level vectors in capturing intra-task diversity?
- **Basis in paper:** [explicit] The Conclusion states: "Future work could explore finer-grained or adaptive vectorization strategies to improve robustness and generalization," noting that task-level vectors may miss intra-task diversity.
- **Why unresolved:** The current method averages vectors across a support set, which smoothes out sample-specific nuances that might be critical for difficult or outlier instances.
- **What evidence would resolve it:** A study comparing static CoT Vectors against dynamic, input-conditional vectors on datasets with high variance in problem types.

### Open Question 2
- **Question:** To what extent does the "information density" of a model's latent space—shaped by pre-training data curation—dictate the transferability and stability of CoT Vectors?
- **Basis in paper:** [inferred] The paper notes LLaMA suffers from "high information density" compared to Qwen, leading to weaker results. The authors "conjecture that this structural disparity stems from differences in training data," but do not isolate this variable.
- **Why unresolved:** The paper compares only two models with different architectures and training recipes, making it difficult to isolate pre-training data curation as the sole cause of latent space structure.
- **What evidence would resolve it:** Controlled experiments training models from scratch on curated vs. broad datasets, followed by CoT Vector injection analysis.

### Open Question 3
- **Question:** Does the observed three-stage reasoning process (perception, reasoning, expression) and the U-shaped injection performance curve persist in non-logical domains?
- **Basis in paper:** [inferred] The analysis relies heavily on mathematical and logical benchmarks (GSM8K, MATH). The paper generalizes the "functional organization" of LLMs but does not verify if this mechanism applies to creative or open-ended tasks.
- **Why unresolved:** Mathematical reasoning often follows a rigid structure that may impose a specific layer-wise hierarchy not present in tasks like narrative generation or social reasoning.
- **What evidence would resolve it:** Applying the layer-wise extraction and probing methodology to diverse tasks such as creative writing or theory of mind benchmarks.

## Limitations
- Extracted vectors require layer-wise search with no predictable pattern across datasets, making them impractical for new tasks.
- Learnable vectors risk overfitting or "over-steering" if trained too aggressively on deep layers, collapsing reasoning diversity.
- The effectiveness of CoT Vectors depends heavily on the latent space structure and information density of the pre-trained model, which is shaped by training data curation.

## Confidence
- **Mechanism 1 (Additive Attention Shift):** Medium - mathematical derivation is sound but lacks direct corpus validation
- **Mechanism 2 (Three-Stage Layer-Wise Specialization):** High - PCA evidence and consistent U-shaped curves across datasets support this
- **Mechanism 3 (Optimization over Averaging):** High - clear performance gap between extracted and learnable vectors, with learnable showing stability

## Next Checks
1. **Layer Sensitivity Scan:** Run inference on GSM8K while injecting a zero-vector (or noise) into every layer individually to establish a baseline for sensitivity.
2. **Extracted vs. Baseline:** Extract a vector from a 100-sample support set and inject it into Layer 0 vs. Layer 15 (middle). Verify the U-shape performance dip at Layer 15.
3. **Learnable Vector Overfit:** Train a learnable vector on a small dataset (e.g., 50 samples) with a high learning rate and observe the "aggressive shift" in the t-SNE plot (clustering/collapse).