---
ver: rpa2
title: Accelerating Neural Network Training Along Sharp and Flat Directions
arxiv_id: '2505.11972'
source_url: https://arxiv.org/abs/2505.11972
tags:
- training
- subspace
- dominant
- bulk-sgd
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates Bulk-SGD, an optimizer that restricts updates
  to the orthogonal complement of the Dominant Hessian subspace, as an alternative
  to standard SGD. Through extensive ablation studies, the authors identify key hyperparameters
  (learning rate, subspace dimension, Hessian estimation frequency) and reveal an
  instability issue at larger learning rates, linked to escalating sharpness and gradient
  norms.
---

# Accelerating Neural Network Training Along Sharp and Flat Directions

## Quick Facts
- arXiv ID: 2505.11972
- Source URL: https://arxiv.org/abs/2505.11972
- Reference count: 4
- Primary result: Bulk-SGD restricts updates to the orthogonal complement of the Dominant Hessian subspace, accelerating training but requiring interpolation with Dom-SGD for stability.

## Executive Summary
This work investigates Bulk-SGD, an optimizer that projects gradients onto the orthogonal complement of the top Hessian eigenspace to accelerate training. Through extensive ablation studies, the authors identify key hyperparameters (learning rate, subspace dimension, Hessian estimation frequency) and reveal an instability issue at larger learning rates, linked to escalating sharpness and gradient norms. They propose an interpolated gradient method that combines Bulk-SGD and Dom-SGD components to balance faster convergence with stability. Empirical analysis shows that Bulk subspace updates accelerate training and reduce loss, while the Dominant subspace promotes stability and generalization. Further, the study finds that Hessian energy concentrates in the Dominant subspace, with the Generalized Gauss-Newton term dominating over the Functional Hessian.

## Method Summary
The method uses Hessian eigendecomposition to split the gradient into Dominant ($P_k g$) and Bulk ($P_k^\perp g$) components. Bulk-SGD restricts updates to the Bulk subspace for faster convergence, while Dom-SGD uses only the Dominant subspace for stability. An interpolated method combines both with tunable weights ($\alpha_{Dom}, \beta_{Bulk}$). Hessian eigenvectors are computed on a holdout set every 10-100 steps using power iteration or Lanczos. The optimizer applies weighted updates based on the interpolation coefficients. The approach is tested on MNIST-5k and CIFAR10-5k with MLPs and CNNs, comparing training loss, test accuracy, sharpness (top eigenvalues), and gradient norms.

## Key Results
- Bulk subspace updates accelerate convergence and reduce training loss, while Dominant subspace promotes stability and generalization
- Instability emerges at higher learning rates when using pure Bulk-SGD, causing sharpness explosion and gradient norm divergence
- Hessian energy concentrates in the Dominant subspace, with the Generalized Gauss-Newton term dominating over the Functional Hessian
- Interpolated gradient method balances faster convergence (Bulk) with stability (Dominant) through adaptive weighting

## Why This Works (Mechanism)

### Mechanism 1
Restricting updates to the orthogonal complement of the top Hessian eigenspace (Bulk subspace) accelerates convergence by enabling larger step sizes along flatter directions. The Bulk subspace ($P_k^\perp$) exhibits lower curvature than the Dominant subspace ($P_k$). By projecting the gradient onto this flatter manifold, the optimizer minimizes interference from high-curvature directions, potentially reducing gradient noise and allowing more aggressive descent. Core assumption: The flatter curvature in the Bulk subspace persists long enough to permit stable, larger steps before the landscape geometry shifts significantly (eigenvector stability). Evidence anchors: [abstract] "Updates along the Bulk subspace... can accelerate convergence." [section 4] "Bulk-SGD supports faster initial optimization at higher learning rates." Break condition: Instability emerges if the learning rate is too high or the projection is not updated frequently enough to track landscape changes.

### Mechanism 2
The Dominant subspace acts as a stabilizer and generalization promoter, even though it is less effective at reducing training loss. Updates along the high-curvature Dominant subspace ($P_k$) inherently constrain step size (standard gradient descent behavior). The authors observe that excluding these directions (pure Bulk-SGD) causes "sharpness explosion" (unbounded top eigenvalues) and gradient norm divergence. Retaining a fraction of the Dominant component suppresses this growth. Core assumption: The generalization benefits are causally linked to the control of sharpness (top eigenvalues) rather than just the specific path taken. Evidence anchors: [section 3.3] "Instability of Bulk-SGD is caused by the sharpness explosion." [section 4.1] "The highest test accuracies are achieved when the Dominant subspace is relatively more emphasized." Break condition: If the interpolation weight for the Dominant component ($\alpha_{Dom}$) is too high, the optimizer reverts to the slow convergence of standard SGD.

### Mechanism 3
The Generalized Gauss-Newton (GGN) term drives the concentration of energy in the Dominant subspace. The Hessian decomposes into GGN ($H_o$) and Functional Hessian ($H_f$). The paper empirically shows that $H_o$ aligns with the Dominant subspace energy, while $H_f$ does not. This suggests the structural curvature captured by GGN defines the "sharp" directions. Core assumption: The Functional Hessian contributes primarily to the Bulk subspace dynamics, though this is presented as an empirical observation rather than a proven theorem. Evidence anchors: [abstract] "Hessian energy is largely concentrated in the Dominant subspace." [section 5] "The energies of H and $H_o$ are significantly larger than that of $H_f$... [and] concentrate within the Dominant subspace." Break condition: This structural alignment may change in architectures with significantly different connectivity or loss landscapes (e.g., transformers vs. CNNs).

## Foundational Learning

- **Hessian Eigendecomposition & Subspace Projection**
  - Why needed here: The entire method relies on splitting the gradient into $P_k g$ (Dominant) and $P_k^\perp g$ (Bulk). You must understand that projecting onto eigenvectors with large eigenvalues means moving along "sharp" (high curvature) directions.
  - Quick check question: If the top eigenvalue is 10x larger than the bulk eigenvalues, how does the loss surface geometry differ between the Dominant and Bulk subspaces?

- **Generalized Gauss-Newton (GGN) vs. Functional Hessian**
  - Why needed here: Section 5 uses this decomposition to explain *why* energy concentrates in the Dominant subspace. Understanding that GGN relates to the model's functional output curvature helps interpret the empirical results.
  - Quick check question: Which component of the Hessian ($H_o$ or $H_f$) is guaranteed to be positive semi-definite in a classification setting, and how does that relate to "sharpness"?

- **Gradient Alignment & Noise**
  - Why needed here: The paper debates whether gradient alignment with the Dominant subspace is "learning" or "noise." Understanding this distinction is critical for the Interpolated Gradient method.
  - Quick check question: According to the paper, why does high alignment $\chi_k(\theta) \approx 1$ not necessarily imply that the Dominant subspace is the best direction for minimizing loss?

## Architecture Onboarding

- **Component map:** Hessian Estimator -> Projection Module -> Gradient Scaler
- **Critical path:**
  1. Initialize weights.
  2. Every $t$ steps, compute Hessian eigenvectors on holdout batch.
  3. For each training batch: Compute gradient $g$.
  4. Project $g \to g_{Dom}, g_{Bulk}$.
  5. Apply weighted update based on $\alpha_{Dom}, \beta_{Bulk}$.

- **Design tradeoffs:**
  - **Frequency of Hessian Update:** Frequent updates (every step) are accurate but computationally expensive. Infrequent updates rely on eigenvector stability but may fail if the landscape shifts rapidly.
  - **Holdout Size ($l$):** A small holdout reduces data for training but is necessary to prevent information leakage. The paper finds $l$ has "little observable impact," suggesting smaller holdouts are viable.
  - **Interpolation Weights:** High $\beta_{Bulk}$ (promoting Bulk) speeds up training loss reduction but risks divergence. High $\alpha_{Dom}$ stabilizes but slows convergence.

- **Failure signatures:**
  - **Divergence at High LR:** If using pure Bulk-SGD ($\alpha_{Dom} \to \infty$) with a high learning rate, the loss will spike and diverge due to sharpness explosion.
  - **Saturation:** If using Dom-SGD, training loss will plateau early and fail to decrease further.
  - **Stale Projections:** If the Hessian is not recomputed frequently enough, the "Dominant" directions may no longer align with the true current curvature, reducing the method's effectiveness.

- **First 3 experiments:**
  1. **Switch Point Reproduction:** Train a standard MLP on MNIST/CIFAR subset. Switch to pure Bulk-SGD at step 1000. Verify if training loss drops faster than the SGD baseline before potentially destabilizing.
  2. **Interpolation Sweep:** Run a grid search over $\alpha_{Dom}$ and $\beta_{Bulk}$ (as in Fig 4). Confirm that low training loss correlates with high Bulk contribution, while high test accuracy often requires a balance.
  3. **Stability Analysis:** Run Bulk-SGD with a learning rate of 0.1. Plot the top eigenvalue (sharpness) and gradient norm over time. Confirm the exponential growth reported in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
Can regularization techniques such as weight decay, normalization, or adaptive learning rate scheduling effectively mitigate the sharpness explosion and instability observed in Bulk-SGD? Basis: [explicit] The conclusion states, "In future work, we plan to conduct further experiments aimed at addressing this issue, exploring techniques such as weight decay, normalization, and adaptive scheduling." Why unresolved: While the paper identifies the mechanism of instability (escalating sharpness and gradient norms in Section 3.3), the proposed interpolated method is a static blend. The efficacy of dynamic stabilization techniques on pure Bulk-SGD remains untested. What evidence would resolve it: A demonstration that Bulk-SGD, when combined with specific regularization or scheduling, maintains stable gradient norms and converges without divergence at learning rates where it currently fails.

### Open Question 2
Does a time-varying schedule for the interpolation weights ($\alpha_{Dom}$ and $\beta_{Bulk}$) outperform static interpolation by sequentially exploiting early acceleration and late-stage generalization? Basis: [explicit] Section 4.1 notes that results "point to the possibility of a principled hybrid approach that adapts the interpolation weights throughout training to exploit early acceleration while preserving generalization." Why unresolved: The experiments rely on fixed interpolation coefficients, revealing a trade-off (Phenomenon 5) but not resolving it through temporal adaptation. What evidence would resolve it: Experiments showing that a scheduled decay of the Bulk component (or growth of the Dominant component) achieves a lower final test accuracy and faster convergence than any fixed $\alpha_{Dom}/\beta_{Bulk}$ ratio.

### Open Question 3
Is the geometry of the Bulk subspace causally determined by the residual structure of the Functional Hessian ($H_f$)? Basis: [explicit] Section 5 states that findings regarding the Functional Hessian's low energy in the Dominant subspace "motivate further study of the interactions between curvature, sharpness, and optimization behavior in the Bulk and Dominant subspaces." Why unresolved: The paper provides empirical evidence that $H_f$ is unaligned with the Dominant subspace (Figures 5-7) and hypothesizes it shapes the Bulk, but does not isolate $H_f$ to prove it drives the "flat" optimization dynamics. What evidence would resolve it: An analysis showing that interventions altering the Functional Hessian directly modify the efficacy or curvature profile of the Bulk-SGD updates.

## Limitations
- The method's core claims about Bulk-SGD acceleration and Hessian subspace energy concentration are tested only on MLP/CNN architectures with small datasets (MNIST-5k, CIFAR10-5k)
- The proposed interpolation method addresses instability but lacks theoretical justification for the specific weighting scheme
- Several key implementation details remain unspecified, including exact Hessian estimation parameters and initialization schemes

## Confidence
- **High confidence**: Bulk-SGD accelerates training on MLP/CNN architectures for small datasets
- **Medium confidence**: Hessian energy concentration in Dominant subspace generalizes to other architectures
- **Medium confidence**: Interpolated method effectively balances acceleration and stability

## Next Checks
1. **Architectural Transfer**: Test Bulk-SGD on ResNet/ViT architectures to verify if the Bulk subspace acceleration persists in modern networks.
2. **Loss Landscape Evolution**: Track top-k eigenvalue trajectories during training to confirm whether sharpness explosion is truly an instability mechanism or a transient phase.
3. **Projection Update Frequency**: Systematically vary Hessian recomputation intervals (every 1, 10, 100 steps) to quantify the tradeoff between computational cost and optimization performance.