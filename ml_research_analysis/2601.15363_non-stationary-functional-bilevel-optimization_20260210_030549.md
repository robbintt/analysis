---
ver: rpa2
title: Non-Stationary Functional Bilevel Optimization
arxiv_id: '2601.15363'
source_url: https://arxiv.org/abs/2601.15363
tags:
- bilevel
- optimization
- non-stationary
- hypergradient
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SmoothFBO, the first algorithm for non-stationary
  functional bilevel optimization that achieves sublinear regret bounds. The method
  uses a time-smoothed stochastic hypergradient estimator to reduce variance in non-stationary
  settings, where both inner and outer loss functions evolve over time.
---

# Non-Stationary Functional Bilevel Optimization

## Quick Facts
- **arXiv ID:** 2601.15363
- **Source URL:** https://arxiv.org/abs/2601.15363
- **Authors:** Jason Bohne; Ieva Petrulionyte; Michael Arbel; Julien Mairal; Paweł Polak
- **Reference count:** 40
- **Primary result:** Introduces SmoothFBO, the first algorithm achieving sublinear regret bounds for non-stationary functional bilevel optimization through time-smoothed stochastic hypergradient estimation.

## Executive Summary
This paper addresses the challenge of non-stationary functional bilevel optimization, where both inner and outer loss functions evolve over time. The authors introduce SmoothFBO, an algorithm that achieves sublinear regret bounds by employing a time-smoothed stochastic hypergradient estimator to reduce variance in dynamic environments. The method is theoretically grounded and empirically validated on controlled regression tasks and model-based reinforcement learning with drifting dynamics, demonstrating superior performance compared to existing functional and parametric bilevel methods.

## Method Summary
SmoothFBO addresses non-stationary functional bilevel optimization by incorporating temporal smoothing into the stochastic hypergradient estimation process. The algorithm maintains a window of past observations and computes hypergradients using a weighted average over this window, which reduces variance while allowing the method to track changing objectives. The theoretical analysis establishes regret bounds that depend on the smoothing window size w, with sublinear regret achieved when w = o(T). The method balances the need to track non-stationarity against the variance reduction benefits of temporal smoothing.

## Key Results
- SmoothFBO achieves sublinear bilevel local regret O(T/w + V₁,T + Tσ²/w) with window size w = o(T)
- Empirically outperforms existing functional and parametric bilevel methods on controlled non-stationary regression tasks
- Demonstrates improved regret and reduced variance in model-based reinforcement learning with drifting environment dynamics
- Increasing the smoothing window consistently reduces variance and improves regret in practice

## Why This Works (Mechanism)
The success of SmoothFBO stems from its ability to reduce the high variance inherent in stochastic hypergradient estimation in non-stationary settings. By averaging hypergradients over a temporal window, the method smooths out noise while maintaining sufficient responsiveness to changing objectives. The theoretical analysis shows that this approach enables sublinear regret growth, which is critical for long-term optimization in dynamic environments.

## Foundational Learning

**Bilevel Optimization:** Optimization problems where one optimization task is embedded within another, requiring solution of an inner problem to evaluate the outer objective.
*Why needed:* Forms the fundamental problem structure being addressed.
*Quick check:* Can you identify the inner and outer problems in a given bilevel formulation?

**Functional Optimization:** Optimization over function spaces rather than finite-dimensional parameter vectors.
*Why needed:* Distinguishes this work from standard parametric bilevel optimization.
*Quick check:* How does functional optimization differ from optimizing over finite-dimensional parameters?

**Regret Analysis:** Framework for evaluating online optimization algorithms by comparing cumulative performance against a benchmark.
*Why needed:* Provides the theoretical foundation for analyzing algorithm performance in non-stationary settings.
*Quick check:* What does sublinear regret imply about long-term algorithm performance?

**Stochastic Hypergradient Estimation:** Approximation of gradients in bilevel problems using stochastic samples from inner and outer problems.
*Why needed:* Enables practical optimization in bilevel settings where exact gradients are intractable.
*Quick check:* What are the main sources of variance in stochastic hypergradient estimation?

## Architecture Onboarding

**Component Map:** Outer loss function -> Inner optimization loop -> Hypergradient estimation -> Temporal smoothing -> Parameter update

**Critical Path:** The temporal smoothing of hypergradients is the core innovation, as it directly addresses the variance challenge that prevents sublinear regret in non-stationary settings.

**Design Tradeoffs:** The window size w must balance tracking non-stationarity (requiring small w) against variance reduction (benefiting from large w), creating an exploration-exploitation tension.

**Failure Signatures:** Poor performance may manifest as either excessive variance (w too small) or failure to track changing objectives (w too large), both leading to linear regret growth.

**First Experiments:**
1. Test SmoothFBO on a simple non-stationary quadratic bilevel problem with known ground truth
2. Compare variance and tracking performance across different window sizes on synthetic data
3. Evaluate sensitivity to the Lipschitz continuity assumption by testing on problems with varying smoothness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the analysis: How to adaptively select the window size w in practice? Can the method be extended to more complex function spaces? How does SmoothFBO perform in high-dimensional applications like deep learning hyperparameter optimization?

## Limitations
- Limited practical guidance on window size selection beyond theoretical scaling requirements
- Empirical validation restricted to two domains (regression and RL), limiting generalizability claims
- Assumes bounded stochastic gradients and Lipschitz continuity, which may not hold in complex applications
- Does not address practical strategies for estimating or adapting to objective variation V₁,T online

## Confidence
- **Theoretical claims:** High - rigorous regret analysis with clear proofs
- **Empirical claims:** Medium - promising results on controlled tasks but limited scope and baselines
- **Practical applicability:** Medium - theoretical framework is sound but practical implementation details need further development

## Next Checks
1. Implement an adaptive window selection strategy that balances tracking non-stationarity with variance reduction, and test its performance across multiple non-stationary benchmarks
2. Evaluate SmoothFBO on high-dimensional functional bilevel problems, such as deep neural network hyperparameter optimization with drifting data distributions
3. Develop and test variance reduction techniques specifically tailored to the functional bilevel setting, such as control variates that exploit the structure of the hypergradient estimator