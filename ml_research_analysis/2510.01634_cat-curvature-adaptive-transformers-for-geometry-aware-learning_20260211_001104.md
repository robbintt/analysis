---
ver: rpa2
title: 'CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning'
arxiv_id: '2510.01634'
source_url: https://arxiv.org/abs/2510.01634
tags:
- geometric
- learning
- routing
- attention
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAT (Curvature-Adaptive Transformer), a novel
  architecture that dynamically routes each token through Euclidean, hyperbolic, or
  spherical attention branches based on learned routing weights. Unlike prior geometric
  transformers that commit to a single geometry, CAT learns per-token curvature preferences,
  enabling adaptive geometric specialization.
---

# CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning

## Quick Facts
- arXiv ID: 2510.01634
- Source URL: https://arxiv.org/abs/2510.01634
- Authors: Ryan Y. Lin; Siddhartha Ojha; Nicholas Bai
- Reference count: 40
- One-line primary result: ~10% relative MRR improvement on knowledge graph completion via per-token curvature routing

## Executive Summary
This paper introduces CAT (Curvature-Adaptive Transformer), a novel architecture that dynamically routes each token through Euclidean, hyperbolic, or spherical attention branches based on learned routing weights. Unlike prior geometric transformers that commit to a single geometry, CAT learns per-token curvature preferences, enabling adaptive geometric specialization. The model uses differentiable routing to softly select the most appropriate geometric attention mechanism for each token, with minimal overhead (5% parameter increase, comparable inference time). Evaluated on knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves approximately 10% relative improvements in MRR and Hits@10 over fixed-geometry baselines. The routing mechanism provides interpretability by revealing geometric structure preferences across tokens, demonstrating that learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning.

## Method Summary
CAT uses a lightweight MLP to compute per-token routing weights over three geometric attention branches (Euclidean, hyperbolic via Poincaré ball, spherical via unit hypersphere). These weights softly mix the outputs of each branch, with each branch performing manifold-consistent attention using geometry-specific distance metrics and transformations. The hyperbolic branch uses exponential/logarithmic maps and Möbius operations, the spherical branch uses geodesic similarity, and the Euclidean branch uses standard dot-product attention. All branches project back to Euclidean space before mixing. Entropy regularization encourages exploration of all geometries during early training, with the regularization weight annealed over time to allow later-stage specialization.

## Key Results
- ~10% relative improvement in MRR and Hits@10 on FB15k-237 and WN18RR knowledge graph completion
- Routing weights show selective activation of hyperbolic attention for hierarchical triples, with minimal spherical usage
- ~5% parameter increase from routing MLP, with inference time ~5–6× slower than fixed-Euclidean
- Entropy regularization during early training prevents premature collapse to Euclidean routing

## Why This Works (Mechanism)

### Mechanism 1: Per-Token Geometry Routing via Soft Gating
- Claim: Adaptive per-token geometry selection outperforms committing to any single fixed geometry when data exhibits heterogeneous relational structure.
- Mechanism: A lightweight MLP computes softmax-normalized routing weights α ∈ R^{B×N×3} from input tokens, producing per-token distributions over Euclidean, hyperbolic, and spherical attention branches. These weights softly mix branch outputs via weighted summation, enabling gradients to flow through all branches simultaneously.
- Core assumption: Tokens within the same sequence exhibit locally varying geometric properties (e.g., hierarchical vs. flat vs. cyclical patterns) that benefit from different inductive biases.
- Evidence anchors:
  - [abstract] "dynamically learns per-token routing across three geometric attention branches through a lightweight, differentiable gating mechanism"
  - [section 3] "α = softmax(MLP(X)) ∈ R^{B×N×3} ... where α_{b,n,g} denotes the routing weight for geometry g at token n"
  - [corpus] Weak direct evidence; neighbor papers address geometry-aware learning but not token-level curvature routing in transformers.
- Break condition: If tokens in a domain exhibit uniform geometric structure (e.g., purely flat relational patterns), routing entropy will collapse to a dominant geometry and provide no benefit over fixed-geometry baselines.

### Mechanism 2: Manifold-Consistent Attention Computations
- Claim: Performing attention intrinsically on each manifold (using geometry-appropriate distance metrics and transformations) preserves the inductive bias of each curvature before mixing.
- Mechanism: Each branch uses principled operations—hyperbolic attention uses Poincaré ball exponential/logarithmic maps with Möbius-weighted aggregation and hyperbolic distance d_{B_c}; spherical attention uses exponential map at a base point with geodesic similarity; Euclidean uses standard dot-product attention. All branches project back to Euclidean space via logarithmic maps before mixing.
- Core assumption: Intra-branch manifold consistency is necessary for each geometry to encode its intended structural prior; cross-branch mixing in Euclidean space is a sufficient approximation for combining complementary representations.
- Evidence anchors:
  - [abstract] "each branch employs geometry-specific operations optimized for its respective manifold"
  - [section 3.2] "Pairwise attention weights are computed via hyperbolic distances ... followed by Möbius-weighted aggregation and logarithmic projection back to Euclidean space"
  - [corpus] No direct corpus validation; manifold-specific attention remains an active research direction without established benchmarks.
- Break condition: If the projection back to Euclidean space before mixing destroys curvature-specific information (e.g., via distortion in logarithmic maps), the inductive biases may not complement each other meaningfully.

### Mechanism 3: Entropy-Regularized Routing Exploration
- Claim: Early-training entropy regularization on routing weights prevents premature collapse and enables discovery of complementary geometric specializations.
- Mechanism: An auxiliary loss term L_entropy = (1/BN) Σ -α log(α) encourages high-entropy routing distributions early; the weight λ_ent is annealed (multiplied by 0.95 per epoch) to allow later-stage sharpening.
- Core assumption: All three geometries provide non-redundant signal, and early exploration is needed to discover which tokens benefit from which geometry.
- Evidence anchors:
  - [section 4.3] "entropy regularization ... encourages high-entropy routing distributions ... promoting usage of multiple geometries ... annealing λ_ent allows the model to sharpen the routing as appropriate"
  - [section 5] "Entropy regularization during early training prevents premature collapse to Euclidean routing"
  - [corpus] Not addressed in neighbor papers.
- Break condition: If one geometry dominates the data distribution universally, entropy regularization may simply delay convergence without improving final performance.

## Foundational Learning

- Concept: **Riemannian manifolds and curvature types**
  - Why needed here: CAT routes between Euclidean (zero curvature), hyperbolic (negative curvature), and spherical (positive curvature) spaces; understanding why each geometry suits particular data structures is essential for interpreting routing behavior.
  - Quick check question: Can you explain why hyperbolic space is well-suited for hierarchical tree structures while spherical space suits cyclical patterns?

- Concept: **Mixture-of-Experts (MoE) routing**
  - Why needed here: The CAT routing mechanism is an MoE-style architecture; grasping soft vs. hard routing and load balancing concerns helps understand design choices and failure modes.
  - Quick check question: What is the difference between hard routing (discrete expert selection) and soft routing (weighted combination), and why does CAT use the latter?

- Concept: **Exponential and logarithmic maps on manifolds**
  - Why needed here: These operations transfer vectors between Euclidean tangent spaces and curved manifolds; they are the computational primitives enabling geometry-specific attention.
  - Quick check question: Given a point on a manifold and a vector in its tangent space, what does the exponential map produce?

## Architecture Onboarding

- Component map:
  - Input embeddings (entity + relation) → Dropout → CATBlock
  - Within CATBlock: Routing MLP produces α ∈ R^{N×3} → Three parallel attention branches (Euclidean, Hyperbolic via Poincaré ball, Spherical via unit hypersphere) → Geometry mixing via α-weighted summation → Output
  - Each branch contains: geometry-specific attention + LayerNorm + feed-forward network

- Critical path:
  1. Routing MLP must produce meaningful α distributions (not uniform, not collapsed).
  2. Hyperbolic branch requires stable exponential/logarithmic maps and Möbius operations (numerical sensitivity near manifold boundary).
  3. Entropy regularization schedule must be tuned; too aggressive annealing causes early collapse, too slow delays convergence.

- Design tradeoffs:
  - **Parameter overhead vs. adaptivity**: ~5% parameter increase (from routing MLP) for ~10% relative MRR improvement; acceptable for knowledge graphs, unclear for larger-scale domains.
  - **Parallel branch computation vs. runtime**: All three branches compute in parallel; inference time ~5–6× slower than Fixed-Euclidean (Table 2), though still modest in absolute terms (~5ms for 1M parameters).
  - **Soft mixing vs. interpretability**: Soft routing preserves gradients but dilutes clear attribution; routing weights still provide coarse interpretability.

- Failure signatures:
  - **Routing collapse**: α converges to near-deterministic selection of one geometry (often Euclidean); check routing weight distributions during training.
  - **Numerical instability in hyperbolic branch**: Points approaching Poincaré ball boundary cause gradient explosion; monitor ||x|| and apply projection proj_{B_c} aggressively.
  - **Overfitting on small datasets**: Near-perfect training accuracy with moderate test accuracy observed on CORA (Appendix B); regularization may be insufficient for homogeneous, low-data regimes.

- First 3 experiments:
  1. **Reproduce baseline comparison on FB15k-237**: Train CAT vs. Fixed-Euclidean/Hyperbolic/Spherical with identical hyperparameters (d=64, batch=512, AdamW, λ_ent=0.01 annealing). Verify ~10% relative MRR improvement holds.
  2. **Routing weight analysis**: Log α distributions per epoch on FB15k-237; confirm entropy decreases over time and that hyperbolic weights activate selectively (not uniformly zero).
  3. **Ablate entropy regularization**: Train CAT with λ_ent=0 (no entropy regularization) and compare routing behavior and final performance; expect earlier collapse and potentially degraded MRR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CAT's curvature-adaptive advantage scale to larger model sizes and deeper architectures, or is the ~10% relative improvement primarily a benefit for low-capacity regimes?
- Basis in paper: [explicit] The authors evaluate on "compact, single-block architectures" with ~1M parameters to "isolate the effect of curvature adaptivity from confounding factors such as depth, parameter scaling," asking whether benefits extend when scaled.
- Why unresolved: All reported experiments use shallow single-block models; the scaling behavior to state-of-the-art model sizes remains untested.
- What evidence would resolve it: Evaluation of CAT variants on standard benchmarks using models with 10x–100x more parameters and multiple stacked CAT blocks.

### Open Question 2
- Question: Can conditional branch execution (compute only the highest-weighted geometry per token) recover the ~5–6x inference slowdown while preserving most accuracy gains?
- Basis in paper: [inferred] Table 2 shows CAT inference latency is 4.9–5.6ms versus 0.8–1.0ms for fixed-Euclidean. The authors explicitly list "branch pruning or distillation for fast inference" as future work but do not explore it.
- Why unresolved: All three branches currently execute in parallel for every token; the routing mechanism could theoretically skip low-weight branches but this is unexplored.
- What evidence would resolve it: Ablation studies measuring accuracy-latency trade-offs when pruning branches below routing-weight thresholds (e.g., α < 0.1).

### Open Question 3
- Question: In domains where spherical geometry should theoretically dominate (e.g., temporal sequences with strong periodicity, 360° imagery), does CAT learn meaningfully higher spherical routing weights, and does this correlate with larger performance gains?
- Basis in paper: [explicit] The authors note "the low spherical usage should be read as a dataset property, not a general indictment of spherical attention" and call for "domain-specific case studies where spherical inductive biases are expected to dominate."
- Why unresolved: On FB15k-237 and WN18RR, spherical routing weights remain near zero (Figure 2), leaving the question of whether CAT can exploit spherical structure when present unanswered.
- What evidence would resolve it: Evaluation on datasets with known cyclic structure (e.g., hourly traffic patterns, molecular conformations, astronomical data) with routing-weight analysis.

## Limitations
- The assumption that tokens exhibit heterogeneous geometric properties within the same sequence is asserted but not empirically validated through structure-routing correlation analysis.
- The projection back to Euclidean space before mixing may destroy curvature-specific information, but ablation studies on manifold-consistent mixing strategies are absent.
- The entropy regularization schedule is described but sensitivity to initialization and annealing rate is not explored through systematic hyperparameter sweeps.

## Confidence
**High confidence**: The architectural description is complete and reproducible, with explicit equations for routing, manifold-specific attention, and mixing operations. The empirical results on FB15k-237 and WN18RR are clearly presented with appropriate metrics (MRR, Hits@10) and competitive baselines. The routing mechanism provides interpretable outputs that correlate with geometric structure types.

**Medium confidence**: The claims about why per-token routing outperforms fixed geometries rely on theoretical arguments about heterogeneous relational structure but lack direct empirical validation. The mechanism of preserving curvature-specific inductive biases through manifold-consistent attention is plausible but not rigorously tested against alternatives. The entropy regularization's role in preventing premature collapse is demonstrated qualitatively but not quantitatively.

**Low confidence**: The paper assumes the three chosen geometries (Euclidean, hyperbolic, spherical) are sufficient to capture all relevant structural patterns without testing additional manifolds or adaptive curvature parameters. The numerical stability of hyperbolic operations is addressed only through projection tricks without reporting failure rates or sensitivity analysis.

## Next Checks
1. **Structure-Routing Correlation Analysis**: For FB15k-237, extract triples and classify them by structural properties (hierarchical vs. cyclic vs. flat using graph-theoretic metrics), then compute the correlation between triple structure type and routing weight distributions. This would directly validate whether the model learns to match geometry to structure.

2. **Manifold-Consistent Mixing Ablation**: Implement an alternative mixing strategy where branch outputs are combined in tangent space before projecting back to Euclidean space, and compare against the current post-projection mixing on the same benchmarks. This would test whether the current mixing approach loses curvature-specific information.

3. **Routing Entropy Sensitivity Sweep**: Train CAT with varying initial λ_ent values (0.001, 0.01, 0.1) and annealing schedules (0.8, 0.9, 0.95 per epoch) on FB15k-237, measuring both routing entropy trajectories and final MRR. This would quantify how sensitive performance is to entropy regularization and whether the current schedule is optimal.