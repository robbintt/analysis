---
ver: rpa2
title: The more polypersonal the better -- a short look on space geometry of fine-tuned
  layers
arxiv_id: '2501.05503'
source_url: https://arxiv.org/abs/2501.05503
tags:
- bert
- space
- language
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how BERT's internal representations change\
  \ when fine-tuned on a new grammatical feature (polypersonal agreement) in Russian.\
  \ The authors compare two model architectures\u2014one with a linear layer and another\
  \ with an LSTM layer\u2014trained on data with polypersonal affixes added as either\
  \ prefixes or suffixes to verbs."
---

# The more polypersonal the better -- a short look on space geometry of fine-tuned layers

## Quick Facts
- arXiv ID: 2501.05503
- Source URL: https://arxiv.org/abs/2501.05503
- Reference count: 20
- Primary result: Fine-tuning BERT with polypersonal grammar layers creates topologically distinct representations while preserving base language geometry

## Executive Summary
This paper investigates how BERT's internal representations change when fine-tuned to process a novel grammatical feature (polypersonal agreement) in Russian. The authors compare two model architectures—one with a linear layer and another with an LSTM layer—trained on data with polypersonal affixes added to verbs. They measure changes in latent space geometry using topological data analysis and evaluate perplexity and masked language modeling performance across layers. Results show successful adaptation to the new grammatical system with polypersonal sentences becoming distinguishable in latent space, while base language geometry is preserved.

## Method Summary
The method involves fine-tuning RuBERT (a Russian BERT variant) with an additional grammar module that learns to process polypersonal agreement—a linguistic feature where verbs agree with both subject and direct object. Two model variants are tested: a linear projection head and an LSTM-based architecture. The training procedure freezes the base RuBERT layers and only trains the grammar module and MLM head. Evaluation uses pseudo-perplexity, topological data analysis (Vietoris-Rips filtration and bottleneck distance), and layer-wise MLM probing to measure correct inflection prediction across layers.

## Key Results
- Fine-tuning successfully adapts BERT to polypersonal grammar, making polypersonal sentences topologically distinguishable from base Russian
- The LSTM model produces more clustered representations with smaller geometric shifts compared to the linear layer model
- Correct polypersonal inflection prediction probability increases progressively from middle to later BERT layers, consistent with syntactic processing emergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a fine-tuned grammatical layer creates topologically distinct representations for new grammatical features while preserving base language geometry.
- Mechanism: The grammar module learns to shift polypersonal sentence representations away from standard Russian representations in latent space, enabling the model to process both grammatical systems without catastrophic interference.
- Core assumption: Topological separation in latent space correlates with functional differentiation in processing capability.
- Evidence anchors: abstract states "fine-tuning successfully adapts BERT to the new grammatical system, with polypersonal sentences becoming more distinguishable in the latent space"; section 4.3 notes the grammar module "disentangles texts with different grammar in a rather diverse way."

### Mechanism 2
- Claim: Syntactic feature prediction emerges progressively in middle-to-late BERT layers.
- Mechanism: Probing intermediate layers with the MLM head reveals that correct polypersonal inflection probability increases from layer 5-6 onward, peaking at the final layer for suffix models.
- Core assumption: Layer-wise prediction probability reflects where syntactic features are encoded and processed.
- Evidence anchors: abstract mentions "MLM probing reveals that correct polypersonal inflection prediction increases in later layers, consistent with syntactic processing in BERT"; section 4.2 describes probability growing "from the middle layers with an obvious peak at the last for the suffix models."

### Mechanism 3
- Claim: LSTM-based grammar modules produce more clustered representations with smaller geometric shifts compared to linear layers.
- Mechanism: The recurrent structure of LSTM constrains representation variance to prevent gradient explosion, resulting in tighter clustering but reduced separation distance between grammatical categories.
- Core assumption: LSTM's sequential processing imposes representational stability constraints absent in feed-forward layers.
- Evidence anchors: abstract states "The LSTM-based model produces more clustered representations but smaller geometric shifts compared to the linear layer model"; section 4.3 notes "LSTM is characterized with the same tendency to disjoin polypersonal representations from base ones, the distances between them are extremely smaller."

## Foundational Learning

- **Topological Data Analysis (Vietoris-Rips filtration, persistence diagrams, bottleneck distance)**: Why needed here: The paper uses these tools to quantify geometric changes in BERT's latent space. Without understanding that bottleneck distance measures maximum topological difference between point cloud structures, results are uninterpretable. Quick check question: If two sentence sets have bottleneck distance of 0.5 vs 0.1, which pair is more geometrically similar?

- **Polypersonal Agreement (Linguistic)**: Why needed here: The artificial grammatical feature being tested. Understanding that polypersonality means verb agreement with multiple arguments (subject + object) explains why affixes encode person/number of the direct object. Quick check question: In "she hears-me every word" (modified English analogy), what argument does "-me" agree with?

- **Weight Tying in Language Models**: Why needed here: The MLM head ties output embedding weights to input embeddings, reducing parameters. This affects how to interpret the projection layer's role in the grammar module. Quick check question: If output embeddings were not tied to input embeddings, how would parameter count change for the linear model?

## Architecture Onboarding

- **Component map**: Input tokens → Frozen RuBERT (12 layers) → Grammar Module (Linear OR LSTM) → MLM Head (tied embeddings)
- **Critical path**: Prepare minimal pairs dataset → Pretrain RuBERT 10 epochs with ALL layers unfrozen → Freeze RuBERT, train only grammar module + MLM head → Evaluate: pseudo-perplexity, topology, layer-wise probing
- **Design tradeoffs**: 
  - Suffix vs Prefix affixes: Suffix more linguistically natural for Russian; prefix creates easier local token-level subsystem but higher overall perplexity
  - LSTM vs Linear: LSTM gives more stable/clusters representations; Linear gives larger geometric separation
  - Frozen vs Unfrozen BERT: Freezing prevents polypersonal knowledge from contaminating base representations but may limit adaptation
- **Failure signatures**: Perplexity gap between polypersonal and base text does not narrow after fine-tuning; early layers show higher inflection prediction than later layers; base text topology shifts significantly
- **First 3 experiments**:
  1. Measure pseudo-perplexity on 10K test sentences for unfine-tuned RuBERT to establish initial gap
  2. Pass 2000 masked samples through each BERT layer (1-12) with grammar module attached, plot correct inflection probability per layer
  3. Compute bottleneck distances between (base, polypersonal) representations at BERT output vs grammar module output to verify disentanglement occurs in added layer

## Open Questions the Paper Calls Out
- To what extent do tokenization effects versus linguistic improbability contribute to differences between prefix-based and suffix-based polypersonal models?
- Does the necessity for recurrent models to preserve consistent representations explain why LSTM-based modules exhibit smaller geometric shifts than linear layers?
- How can this method of analyzing latent space geometry via perplexity and topology be adapted for generative architectures like T5 or GPT?

## Limitations
- Lack of detailed architectural specifications and training hyperparameters blocks exact reproduction
- Topological analysis relies on abstract geometric distance measures without establishing direct functional correlations to language processing performance
- The claim about syntactic feature emergence in middle-to-late layers assumes linear progression without testing for alternative feature integration patterns

## Confidence
- **High Confidence**: Fine-tuning successfully adapts BERT to process polypersonal agreement (supported by perplexity improvements and inflection prediction accuracy)
- **Medium Confidence**: Layer-wise emergence of syntactic features (supported by probing results but could be influenced by other factors)
- **Low Confidence**: Mechanism by which topological separation correlates with functional processing capability (remains speculative without validation)

## Next Checks
1. **Ablation Study on Architecture**: Train four model variants (frozen BERT + linear, frozen BERT + LSTM, unfrozen BERT + linear, unfrozen BERT + LSTM) to isolate whether architectural choices or BERT adaptation drive performance differences.
2. **Layer-wise Gradient Attribution**: Apply integrated gradients to determine which BERT layers contribute most to polypersonal inflection prediction and compare attribution patterns between suffix and prefix models.
3. **Cross-linguistic Generalization Test**: Evaluate trained models on polypersonal Russian sentences from held-out domains and closely related languages to assess whether geometric separation corresponds to functional generalization beyond training distribution.