---
ver: rpa2
title: 'CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven
  Planning Chain of Thought in AI Counseling'
arxiv_id: '2509.25733'
source_url: https://arxiv.org/abs/2509.25733
tags:
- client
- counselor
- counseling
- dialogue
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CATCH addresses low therapy fidelity and lack of decision-making
  transparency in AI counseling by proposing a Progressive Dialogue Synthesis (PDS)
  strategy and a Memory-driven Dynamic Planning (MDP) Chain-of-Thought (CoT) framework.
  PDS incrementally generates stage-aligned counseling dialogues from client self-reports,
  while MDP explicitly models counselor reasoning through a collaborative multi-agent
  system.
---

# CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling

## Quick Facts
- **arXiv ID:** 2509.25733
- **Source URL:** https://arxiv.org/abs/2509.25733
- **Reference count:** 40
- **Primary result:** Novel framework combining Progressive Dialogue Synthesis (PDS) and Memory-driven Dynamic Planning (MDP) CoT achieves 91.7% goal identification accuracy and 87.1% working stage accuracy in AI counseling

## Executive Summary
CATCH addresses the critical challenges of low therapy fidelity and opaque decision-making in AI counseling through a two-pronged approach. The framework introduces Progressive Dialogue Synthesis (PDS) to incrementally generate stage-aligned counseling dialogues from client self-reports, and Memory-driven Dynamic Planning (MDP) to explicitly model counselor reasoning through a collaborative multi-agent system. By training Qwen3 models on 233 high-quality synthetic dialogues with embedded CoT reasoning, CATCH demonstrates significant improvements in therapeutic skills including solution focus, resource activation, and goal orientation. Human evaluations confirm the framework's effectiveness, while ablation studies validate the contributions of both PDS and MDP components.

## Method Summary
The CATCH framework employs a two-stage data synthesis approach using GPT-4o and Doubao-1.5-pro-256k models. First, Progressive Dialogue Synthesis (PDS) incrementally generates dialogues by extracting therapeutic elements (goals, resources, solutions) from client self-reports, then creating stage-specific outlines and dialogues for Goal Identification, Working, and Ending stages. Second, Memory-driven Dynamic Planning (MDP) attaches Chain-of-Thought reasoning to each dialogue turn through a multi-agent system: Memory Agent (summarizes history), Global Plan Agent (identifies stage status), Strategy Agent (selects intervention tactics), and Checking Agent (validates consistency). The resulting 6,898 responses are used to fine-tune Qwen3-8B and Qwen3-14B models via LoRA (rank 16, LR 1e-4, 3 epochs) using LLaMA-Factory, with input format combining SST knowledge, dialogue history, and MDP CoT.

## Key Results
- Achieves 91.7% and 87.1% accuracy in goal identification and working stages respectively, outperforming one-time generation baselines
- MDP CoT and PDS each significantly enhance model performance across solution focus, resource activation, and goal orientation metrics
- Human evaluations confirm improved therapeutic skill scores compared to baselines
- Demonstrates robust performance across varying client attitudes and linguistic styles

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Constraint Propagation in Dialogue Synthesis
Decomposing dialogue generation into stage-specific outlines (PDS) reduces "therapy drift" by enforcing structural constraints derived from client self-reports. The progressive approach creates causal dependencies where each stage depends on the previous one's accuracy, preventing inconsistent goal generation mid-dialogue.

### Mechanism 2: Externalized Case Conceptualization via MDP
The MDP framework operationalizes psychological theory into computational steps, forcing explicit synthesis of "thinking process" through Memory Capture, Global Planning, and Strategy Reasoning agents. This models the causal link between client history and counselor intervention that standard LLMs may lose in long contexts.

### Mechanism 3: Adversarial Verification for Alignment
A dedicated Checking Agent in the synthesis loop acts as a regulator to filter out logical inconsistencies or jargon, improving data quality for supervised fine-tuning. The iterative refinement process serves as a noise filter, leveraging LLMs' discriminative capabilities to verify generated logic.

## Foundational Learning

- **Concept: Single-Session Therapy (SST)**
  - **Why needed here:** The entire PDS architecture is built on SST protocols focusing on "resource activation" rather than deep psychoanalysis, essential for interpreting evaluation metrics.
  - **Quick check question:** Does the model understand why it must transition from "Goal Identification" to "Working Stage" only after a goal is mutually confirmed?

- **Concept: Case Conceptualization**
  - **Why needed here:** This is the theoretical basis for the MDP thinking pattern, automating the cognitive process where therapists integrate client history to form treatment plans.
  - **Quick check question:** Can you map the three MDP agents to the cognitive steps a human therapist takes between listening and speaking?

- **Concept: Supervised Fine-Tuning (SFT) with CoT**
  - **Why needed here:** The paper trains models on `<SST Knowledge, History, MDP CoT, Response>`, learning to predict both reasoning and text.
  - **Quick check question:** If MDP CoT is removed during inference, would the model still output valid responses, and how would performance compare to models trained without CoT?

## Architecture Onboarding

- **Component map:** Input (Client Self-Report + Big Five Traits) -> PDS Module (Element Acquisition -> Outline Generator -> Dialogue Generator) -> MDP Module (Memory Agent -> Global Plan Agent -> Strategy Agent -> Checking Agent -> Fusion Agent) -> Training (Qwen3-8B/14B fine-tuned on `<Context, MDP CoT + Response>`)

- **Critical path:** The PDS Element Acquisition phase is critical; incorrect extraction of the "Counseling Goal" causes the entire pipeline to generate coherent but irrelevant dialogues.

- **Design tradeoffs:**
  - Data Scale vs. Fidelity: 233 high-quality dialogues provide high logical coherence but risk overfitting compared to larger datasets
  - Complexity vs. Latency: Multi-agent MDP system requires multiple inference calls per turn, unsuitable for real-time inference without distillation

- **Failure signatures:**
  - Therapy Drift: Model repeats questions or fails to transition stages (mitigated by PDS)
  - Jargon Leakage: Uses technical terms with clients (mitigated by keyword filtering)
  - Negative Attitude Collapse: Fails with uncooperative clients (addressed in robustness analysis)

- **First 3 experiments:**
  1. Ablation on PDS: Compare one-time generation vs. PDS for Goal Identification accuracy
  2. MDP Stress Test: Feed conflicting dialogue history to verify Checking Agent catches inconsistencies
  3. Inference Mode Comparison: Compare full MDP CoT generation vs. direct response for latency and performance

## Open Questions the Paper Calls Out

- **Cross-session memory integration:** How can hierarchical memory architectures be developed to facilitate coherent, multi-session therapeutic tracking within CATCH? [explicit] Current architecture lacks cross-session memory integration for longitudinal dynamic modeling.

- **Scaling training corpus:** To what extent does scaling beyond 233 dialogues improve robust generalization across diverse psychosocial contexts and linguistic styles? [explicit] Current dataset size may not ensure robust generalization.

- **Real client interaction:** How does CATCH performance transfer to real human clients compared to simulated GPT-4o-mini agents? [inferred] Evaluation relies entirely on simulated clients without validation against human behavior.

## Limitations

- Small training corpus (233 dialogues) may limit performance in rare or complex counseling scenarios
- Evaluation relies on GPT-4o automated judgment, introducing potential bias and not fully capturing nuanced therapeutic effectiveness
- Human evaluation sample size (30 dialogues from 30 experts) is modest for establishing generalizability

## Confidence

- **High Confidence:** Technical implementation of PDS and MDP frameworks, ablation study methodology, and multi-agent synthesis system
- **Medium Confidence:** Performance metrics (91.7% goal identification, 87.1% working stage) based on human evaluation with limited generalizability
- **Medium Confidence:** SST metric evaluations using GPT-4o show consistent improvements but lack independent validation

## Next Checks

1. **Robustness Testing:** Evaluate CATCH on client self-reports with varying complexity and emotional distress to assess performance degradation
2. **Cross-Validation with Independent Experts:** Have separate counseling professionals assess dialogue samples to verify GPT-4o judgment consistency
3. **Longitudinal Coherence Analysis:** Test model's ability to maintain therapeutic goals across extended sessions (10+ turns) to validate MDP memory retention