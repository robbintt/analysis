---
ver: rpa2
title: 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers'
arxiv_id: '2511.01617'
source_url: https://arxiv.org/abs/2511.01617
tags:
- video
- retrieval
- fusion
- zero-shot
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vote-in-Context (ViC) is a training-free framework that uses a
  Vision-Language Model (VLM) to jointly rerank and fuse video retrieval results by
  serializing both visual content (via S-Grid) and retriever metadata into the VLM
  prompt. This enables adaptive, list-wise reasoning over heterogeneous retrievers
  without fixed fusion formulas.
---

# Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers

## Quick Facts
- arXiv ID: 2511.01617
- Source URL: https://arxiv.org/abs/2511.01617
- Reference count: 29
- Zero-shot VLM-based framework achieving up to +40 points Recall@1 improvement over traditional fusion methods

## Executive Summary
Vote-in-Context (ViC) is a training-free framework that leverages Vision-Language Models (VLMs) to jointly rerank and fuse video retrieval results from multiple heterogeneous retrievers. The approach serializes both visual content (via S-Grid) and retriever metadata into VLM prompts, enabling adaptive list-wise reasoning without fixed fusion formulas. Tested across four major video retrieval benchmarks, ViC demonstrates dramatic improvements in single-list reranking and consistently outperforms traditional fusion baselines like CombSUM and RRF.

## Method Summary
ViC transforms VLMs into zero-shot rank fusers by serializing both visual content and retriever metadata into the VLM prompt. The framework uses S-Grid to capture visual features from videos, which are combined with retrieval scores and metadata in a structured prompt format. The VLM then performs joint reranking and fusion of results from multiple heterogeneous retrievers. This approach eliminates the need for training or fixed fusion formulas, enabling adaptive reasoning over ranked lists. The system is evaluated on four benchmark datasets (MSR-VTT, DiDeMo, ActivityNet, VATEX) using zero-shot prompting without fine-tuning.

## Key Results
- Achieves Recall@1 scores of 87.1% (text-to-video) and 89.0% (video-to-text) on MSR-VTT in zero-shot settings
- Delivers 99.6% Recall@1 (video-to-text) on VATEX, representing up to +40 points improvement over prior SOTA
- Consistently outperforms traditional fusion baselines like CombSUM and RRF across all evaluated datasets

## Why This Works (Mechanism)
ViC works by leveraging the reasoning capabilities of VLMs to perform joint reranking and fusion tasks simultaneously. Instead of using fixed mathematical formulas like CombSUM or RRF, the VLM can adaptively weigh and combine results based on both visual content and metadata. The S-Grid serialization captures spatial-temporal visual features efficiently, while the prompt structure allows the VLM to consider multiple retrieval sources holistically. This enables more nuanced decision-making than traditional fusion methods that treat each source independently.

## Foundational Learning
- Vision-Language Models (VLMs): Pre-trained models that understand both visual and textual inputs, enabling multimodal reasoning
  - Why needed: Required to perform joint reranking and fusion of heterogeneous retrieval results
  - Quick check: Verify VLM can process both visual and textual prompts simultaneously

- S-Grid Serialization: Method for efficiently capturing and representing visual features from videos in a compact format
  - Why needed: Enables compact visual representation for VLM prompting without overwhelming the context window
  - Quick check: Confirm S-Grid preserves key visual features across different video types

- List-wise Ranking: Approach that considers the entire ranked list simultaneously rather than individual document scores
  - Why needed: Allows VLMs to make holistic decisions based on relative positioning and content
  - Quick check: Compare list-wise vs. point-wise performance on sample data

- Zero-shot Prompting: Using pre-trained models without fine-tuning or task-specific training
  - Why needed: Eliminates need for labeled data and training infrastructure
  - Quick check: Test prompt effectiveness across multiple retriever combinations

## Architecture Onboarding

**Component Map:** Visual Content (S-Grid) -> Retriever Metadata -> VLM Prompt -> Joint Reranking & Fusion -> Final Ranked List

**Critical Path:** The core workflow involves extracting S-Grid visual features from input videos, combining them with retriever scores and metadata in the prompt structure, and passing this to the VLM for joint reranking and fusion. The VLM's output becomes the final ranked list.

**Design Tradeoffs:** The framework trades computational efficiency for potential information loss through S-Grid serialization, which may not capture all fine-grained details. However, this enables the use of large VLMs with limited context windows and maintains the zero-shot advantage.

**Failure Signatures:** Poor performance may occur with videos containing rapid scene changes where S-Grid loses temporal coherence, or when retriever metadata quality is low. The framework may also struggle with domain shift if benchmark datasets differ significantly from target deployment scenarios.

**First Experiments:**
1. Test VLM prompt effectiveness with single retriever vs. multiple retrievers on MSR-VTT
2. Compare S-Grid serialized visual features against raw video frames for a subset of queries
3. Evaluate zero-shot performance on out-of-distribution video content (e.g., surveillance footage)

## Open Questions the Paper Calls Out
None

## Limitations
- Relies entirely on zero-shot prompting without fine-tuning, potentially limiting generalization to significantly different video distributions
- S-Grid serialization may lose fine-grained spatial-temporal information for videos with rapid scene changes or complex action sequences
- Performance evaluation is limited to benchmark datasets that may not represent real-world retrieval scenarios with noisy or ambiguous queries

## Confidence

**High Confidence:** Claims about outperforming traditional fusion baselines (CombSUM, RRF) are well-supported by reported Recall@1 improvements of up to +40 points across multiple datasets.

**Medium Confidence:** Claims about establishing "new Pareto frontier in retrieval efficiency vs. performance" are supported but lack comparison with more recent end-to-end trained approaches.

**Medium Confidence:** Zero-shot effectiveness (87.1% t2v / 89.0% v2t on MSR-VTT) is impressive but requires verification across additional datasets and retriever combinations.

## Next Checks

1. Test ViC's zero-shot performance on out-of-domain video collections (e.g., surveillance footage, educational content) to assess generalization beyond benchmark datasets

2. Evaluate ViC against state-of-the-art end-to-end trained fusion models on the same datasets to confirm true Pareto efficiency claims

3. Conduct ablation studies removing S-Grid serialization to quantify information loss impact on retrieval quality across different video complexity levels