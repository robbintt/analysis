---
ver: rpa2
title: ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network
arxiv_id: '2506.08629'
source_url: https://arxiv.org/abs/2506.08629
tags:
- feature
- semantic
- segmentation
- network
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient semantic segmentation
  in resource-constrained environments by proposing ECMNet, a lightweight hybrid network
  combining CNNs and Mamba. The core method involves a U-shaped encoder-decoder structure
  with an Enhanced Dual-Attention Block (EDAB) for lightweight feature extraction,
  a Multi-Scale Attention Unit (MSAU) for multi-scale feature aggregation, and a Mamba-enhanced
  Feature Fusion Module (FFM) for global context modeling.
---

# ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network

## Quick Facts
- arXiv ID: 2506.08629
- Source URL: https://arxiv.org/abs/2506.08629
- Reference count: 40
- Primary result: Achieves 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid with only 0.87M parameters and 8.27G FLOPs

## Executive Summary
ECMNet addresses the challenge of efficient semantic segmentation in resource-constrained environments by proposing a lightweight hybrid network that combines CNNs and Mamba. The architecture features a U-shaped encoder-decoder structure with specialized modules for efficient feature extraction and global context modeling. Through extensive experiments on Cityscapes and CamVid datasets, ECMNet demonstrates state-of-the-art performance among lightweight methods, achieving high accuracy while maintaining minimal computational requirements suitable for mobile and embedded applications.

## Method Summary
ECMNet employs a U-shaped encoder-decoder architecture that leverages hybrid CNN-Mamba processing. The encoder uses Enhanced Dual-Attention Blocks (EDAB) with asymmetric convolutions and atrous kernels for efficient local feature extraction. Multi-Scale Attention Units (MSAU) enhance skip connections by aggregating multi-scale context. A Mamba-based Feature Fusion Module (FFM) with SS2D blocks captures global dependencies before the decoder reconstructs the final segmentation map. The network is designed to operate at 1024×1024 resolution for Cityscapes and 360×480 for CamVid, targeting inference on resource-constrained devices.

## Key Results
- Achieves 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid datasets
- Maintains only 0.87M parameters and 8.27G FLOPs
- Outperforms other lightweight methods in the balance between performance and efficiency
- Demonstrates effectiveness of hybrid CNN-Mamba architecture for semantic segmentation

## Why This Works (Mechanism)

### Mechanism 1: Efficient Local-Global Decoupling in EDAB
The Enhanced Dual-Attention Block reduces computational cost while maintaining receptive field diversity by factorizing convolutions and separating local/global processing paths. The module uses a bottleneck structure with decomposed $3 \times 1$ and $1 \times 3$ convolutions for local features, while a parallel branch employs atrous convolutions for global features. Channel and Dual-Direction Attention refine these features before fusion.

Core assumption: Spatial features can be effectively approximated by asymmetric convolution kernels without significant loss of fidelity compared to standard $3 \times 3$ kernels.

### Mechanism 2: Multi-Scale Feature Rectification via MSAU
MSAU restores spatial details lost during downsampling by aggregating context at multiple scales through skip connections. The unit processes features through parallel depth-wise convolutions with kernel sizes $3 \times 3$, $5 \times 5$, and $7 \times 7$, generating spatial and channel attention maps that refine the feature map before fusion.

Core assumption: Low-level features contain noise, and raw skip connections benefit from a filtering mechanism that prioritizes specific spatial regions and channel responses.

### Mechanism 3: Global Sequence Modeling via Mamba FFM
The Mamba-based Feature Fusion Module allows for efficient long-range dependency modeling with linear complexity by replacing Transformer-based fusion. The FFM concatenates multi-level features and processes them using a 2D-Selective-Scan block that captures global context through selective scanning, followed by a Feed-Forward Network for non-linear transformation.

Core assumption: The selective scanning mechanism of Mamba can adequately model 2D spatial dependencies in flattened sequences better than quadratic self-attention mechanisms of Transformers.

## Foundational Learning

- **State Space Models (SSMs) / Mamba**: Why needed: The core innovation relies on the FFM using SS2D to replace quadratic attention with linear complexity sequence modeling. Quick check: How does computational complexity of Mamba block scale with sequence length compared to standard Vision Transformer self-attention?

- **Asymmetric Convolution**: Why needed: EDAB heavily relies on $3 \times 1$ and $1 \times 3$ convolutions to reduce parameters while maintaining wide receptive field. Quick check: What is the parameter count ratio between standard $3 \times 3$ convolution and pair of $3 \times 1$ and $1 \times 3$ convolutions with same channel depth?

- **Atrous (Dilated) Convolution**: Why needed: Used in EDAB's global branch to expand receptive field without increasing parameters or downsampling resolution. Quick check: How does atrous rate $R$ affect effective kernel size of $3 \times 3$ convolution?

## Architecture Onboarding

- **Component map**: Input → Encoder (EDABs) → [Concat Encoder Output + MSAU Outputs] → FFM (Mamba) → Decoder

- **Critical path**: Input → Encoder (EDABs) → [Concat Encoder Output + MSAU Outputs] → FFM (Mamba) → Decoder

- **Design tradeoffs**: Accuracy vs. Efficiency (sacrifices potential accuracy gains from heavier backbones to maintain <1M parameters); Complexity vs. Receptive Field (uses asymmetric/atrous convolutions to simulate large kernels cheaply)

- **Failure signatures**: High FLOPs, Low mIoU (if FLOPs exceed reported ~8.27G, check if standard convolutions replaced depth-wise/asymmetric); Segmentation Artifacts (if long-range dependencies broken, verify SS2D implementation in FFM)

- **First 3 experiments**:
  1. Baseline vs. Full Model: Reproduce Table 1 (A3 vs C3) to verify +3.7% mIoU gain from MSAU and FFM modules
  2. Module Ablation (EDAB): Replace EDAB with standard residual block to quantify dual-attention design contribution
  3. Resolution Sensitivity: Test on Cityscapes (1024x1024) vs. CamVid (360x480) to observe Mamba FFM high-resolution linear complexity handling

## Open Questions the Paper Calls Out

- **Generalization to other domains**: How does ECMNet generalize to semantic segmentation tasks outside urban driving scenes, such as medical imaging or aerial remote sensing? The specific hybrid architecture is tuned for street scenes; transferability to medical or satellite imagery remains unclear.

- **Edge device efficiency**: Does the Mamba-based architecture maintain its efficiency advantage on resource-constrained edge devices or is it reliant on high-end GPU optimization? Reported speed metrics are based on RTX 3090 GPU, not mobile CPUs.

- **Encoder/decoder integration**: Can performance be further improved by integrating Mamba blocks directly into encoder or decoder stages, rather than limiting them to the FFM? The ablation studies validate FFM contribution but don't explore replacing CNNs in primary feature extraction phases.

## Limitations

- **Hyperparameter dependency**: Reported performance tightly coupled to specific training settings (optimizer, scheduler, augmentation, loss weighting) not disclosed in paper
- **Architecture specificity**: Exact encoder/decoder depth, number of EDAB blocks per stage, and precise channel progression are unspecified
- **Mamba implementation gap**: SS2D block details (state dimension, window size, selective scanning parameters) are not provided

## Confidence

- **High confidence**: Core hybrid design (CNN-Mamba fusion) and ablation study trends (MSAU + FFM contribution of ~3.7% mIoU)
- **Medium confidence**: Claimed parameter (0.87M) and FLOPs (8.27G) counts, dependent on precise implementation of depthwise/atrous convolutions
- **Low confidence**: Exact training recipe and SS2D block specifics, critical for reproducing stated performance

## Next Checks

1. **Ablation replication**: Reproduce Table 1 (A3 vs C3) to verify the 3.7% mIoU gain from MSAU and FFM modules
2. **Parameter audit**: Verify 0.87M parameter count by implementing EDAB with 1×1 bottleneck, asymmetric/atrous convolutions, and depthwise layers
3. **SS2D verification**: Test SS2D block implementation on synthetic sequence task to confirm linear complexity and global context modeling capability before integrating into FFM