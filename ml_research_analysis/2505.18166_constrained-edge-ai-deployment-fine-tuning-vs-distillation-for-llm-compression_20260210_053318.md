---
ver: rpa2
title: 'Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression'
arxiv_id: '2505.18166'
source_url: https://arxiv.org/abs/2505.18166
tags:
- pruning
- accuracy
- self-distillation
- fine-tuning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two post-pruning retraining strategies for
  compressing large language models (LLMs) for edge deployment. The authors use a
  fixed layer-wise L2-norm pruning on only the MLP blocks of the OLMo2-7B-SFT model
  and then compare two recovery approaches: (i) fine-tuning with cross-entropy loss
  (L2PFT), which requires labeled data, and (ii) self-distillation with KL-divergence
  (L2PSD), which uses only teacher logits.'
---

# Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression

## Quick Facts
- arXiv ID: 2505.18166
- Source URL: https://arxiv.org/abs/2505.18166
- Reference count: 29
- Compares KL-based self-distillation vs CE fine-tuning for compressed LLM recovery

## Executive Summary
This paper investigates two post-pruning retraining strategies for compressing large language models (LLMs) for edge deployment. The authors focus on a fixed layer-wise L2-norm pruning applied only to MLP blocks of the OLMo2-7B-SFT model, then compare two recovery approaches: fine-tuning with cross-entropy loss (requiring labeled data) and self-distillation with KL-divergence (using only teacher logits). Under identical pruning schedules on CommonsenseQA, KL-based self-distillation consistently matches or exceeds CE fine-tuning, achieving 3-5% higher test accuracy at 50% parameter retention.

## Method Summary
The authors employ a fixed layer-wise L2-norm pruning strategy targeting only MLP blocks within the OLMo2-7B-SFT model. After pruning, two recovery methods are compared: L2PFT (fine-tuning with cross-entropy loss) and L2PSD (self-distillation with KL-divergence). Both methods are evaluated under identical pruning schedules on the CommonsenseQA dataset. The study focuses on comparing loss functions for compressed model recovery, with self-distillation offering superior performance and reduced prediction uncertainty.

## Key Results
- KL-based self-distillation achieves 3-5% higher test accuracy than CE fine-tuning at 50% parameter retention
- Self-distillation consistently matches or exceeds CE fine-tuning performance across tested pruning levels
- Self-distillation demonstrates reduced prediction uncertainty compared to CE fine-tuning

## Why This Works (Mechanism)
The superiority of KL-based self-distillation stems from its ability to transfer the teacher model's probability distribution rather than just hard labels. This approach captures the relative confidence between different classes, which is particularly valuable when dealing with compressed models that may struggle with calibration. The KL-divergence loss function encourages the student model to match not just the correct predictions but also the nuanced probability patterns that encode semantic relationships between classes. This is especially effective for edge deployment where data may be sparse and model calibration is critical for reliable performance.

## Foundational Learning
- **Layer-wise L2-norm pruning**: A magnitude-based pruning technique that removes weights with smallest L2 norms. Why needed: Enables systematic model compression while maintaining structural integrity. Quick check: Verify pruning mask maintains connectivity patterns.
- **KL-divergence vs cross-entropy**: KL-divergence measures distributional similarity while cross-entropy measures prediction accuracy. Why needed: Different objectives for knowledge transfer vs task-specific optimization. Quick check: Compare loss surfaces during training.
- **Self-distillation vs fine-tuning**: Self-distillation uses teacher logits as soft targets, fine-tuning uses ground truth labels. Why needed: Different data requirements and knowledge transfer mechanisms. Quick check: Evaluate calibration curves.
- **MLP block compression**: Focus on multi-layer perceptron components within transformer architecture. Why needed: MLP layers typically contain majority of parameters. Quick check: Profile parameter distribution across model components.
- **CommonsenseQA task**: Multiple-choice question answering requiring world knowledge. Why needed: Challenging task that tests reasoning capabilities. Quick check: Analyze error patterns on different question types.
- **Edge deployment constraints**: Resource limitations including memory, compute, and power. Why needed: Real-world deployment considerations. Quick check: Measure inference latency and memory usage.

## Architecture Onboarding

**Component map**: Input -> Embedding -> Encoder (with pruned MLP blocks) -> Output Head -> Loss Function (KL or CE)

**Critical path**: Token embedding → transformer encoder (with pruned MLP layers) → logits generation → loss computation → parameter update

**Design tradeoffs**: Fixed MLP-only pruning offers computational efficiency but may miss optimization opportunities in attention layers. KL-divergence requires teacher logits but provides better calibration. Cross-entropy requires labeled data but may not transfer knowledge effectively in compressed models.

**Failure signatures**: CE fine-tuning shows higher prediction uncertainty and lower accuracy on compressed models. MLP-only pruning may lead to attention mechanism degradation. Self-distillation requires teacher model availability and sufficient capacity.

**First experiments**: 1) Compare accuracy vs pruning ratio for both methods, 2) Analyze prediction uncertainty distributions, 3) Evaluate calibration curves on validation set

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Single model architecture (OLMo2-7B-SFT) and task (CommonsenseQA) limits generalizability
- Fixed MLP-only pruning may not be optimal for all model families or deployment scenarios
- Does not address full edge deployment pipeline including latency, memory footprint, and power consumption
- Absence of ablation studies on pruning granularity and layer selection

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| KL-based self-distillation superiority for MLP-pruned OLMo2-7B-SFT on CommonsenseQA | High |
| Self-distillation advantage in data-sparse edge environments | Medium |
| MLP-only pruning as optimal general strategy | Low |

## Next Checks
1. Test L2PFT vs L2PSD across multiple model architectures (Llama, Mistral) and diverse task types (NLI, QA, classification)
2. Evaluate full edge deployment profile including inference latency, memory usage, and power consumption on representative hardware
3. Conduct ablation study varying pruning strategies (layer selection, granularity) beyond fixed MLP-only pruning