---
ver: rpa2
title: Evidential Deep Active Learning for Semi-Supervised Classification
arxiv_id: '2505.20691'
source_url: https://arxiv.org/abs/2505.20691
tags:
- learning
- uncertainty
- active
- samples
- edalssc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an evidential deep active learning approach
  for semi-supervised classification (EDALSSC) that addresses the problem of traditional
  methods ignoring uncertainty estimation during learning, leading to model overconfidence
  and ineffective sample selection. EDALSSC introduces a comprehensive uncertainty-aware
  framework that quantifies uncertainty for both labeled and unlabeled samples.
---

# Evidential Deep Active Learning for Semi-Supervised Classification

## Quick Facts
- arXiv ID: 2505.20691
- Source URL: https://arxiv.org/abs/2505.20691
- Authors: Shenkai Zhao, Xinao Zhang, Lipeng Pan, Xiaobin Xu, Danilo Pelusi
- Reference count: 31
- One-line primary result: EDALSSC achieves 0.3-2.4% accuracy improvements over state-of-the-art semi-supervised and supervised active learning methods on CIFAR-10, CIFAR-100, SVHN, and Fashion-MNIST datasets.

## Executive Summary
This paper proposes EDALSSC, an evidential deep active learning approach for semi-supervised classification that addresses overconfidence issues in traditional methods by integrating comprehensive uncertainty estimation. The method quantifies uncertainty for both labeled and unlabeled samples using evidence-based Dirichlet distributions, combining ignorance and conflict information via a T-conorm operator. EDALSSC includes a dynamic scaling mechanism for Dirichlet parameters to prevent counterintuitive uncertainty estimates in high-class-count scenarios, and selects samples based on uncertainty only when training loss increases in the latter half of training cycles. Experimental results demonstrate consistent improvements over state-of-the-art methods across multiple datasets, with ablation studies confirming the effectiveness of each component.

## Method Summary
EDALSSC implements an iterative semi-supervised active learning framework using ResNet-18 as backbone with an evidence head outputting non-negative Dirichlet parameters. The method combines supervised evidential cross-entropy loss with unsupervised uncertainty loss and consistency regularization using an EMA teacher. Uncertainty is computed by aggregating ignorance (derived from Dirichlet parameters) and conflict (from belief differences) using a T-conorm operator. A dynamic scaling factor adjusts Dirichlet parameters to prevent counterintuitive uncertainty in high-class scenarios. Sample selection occurs only in the latter half of training when loss increases, choosing top 5% highest-uncertainty samples from the unlabeled pool. The framework runs for 6 active learning cycles, starting with 10% labeled data.

## Key Results
- EDALSSC achieves classification accuracy improvements of 0.3-2.4% over state-of-the-art methods across all tested datasets
- The full framework shows 0.3-2.1% accuracy gains over variants missing key uncertainty estimation mechanisms
- Ablation studies confirm each component's effectiveness, with the uncertainty module being critical for stability (SVHN crash from 96.93% to 78.87% when removed)
- Dynamic scaling factor prevents counterintuitive uncertainty in high-class-count scenarios like CIFAR-100

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** T-conorm aggregation of ignorance and conflict provides more reliable uncertainty estimation than Shannon entropy alone
- **Mechanism:** Standard entropy often remains low even for misclassified samples. EDALSSC models uncertainty as $u_i = I_i + C_i - I_i \times C_i$, combining ignorance (lack of evidence) with conflict (ambiguous class boundaries) via probabilistic OR. This ensures boundary cases with high internal conflict are flagged as high-uncertainty.
- **Core assumption:** Standard Shannon entropy fails to capture conflict aspect of uncertainty in semi-supervised settings
- **Evidence anchors:** Section 3.2 defines conflict metric and T-conorm aggregation; Table 2 shows ablation performance drop
- **Break condition:** If conflict metric correlates poorly with decision boundary proximity (e.g., due to label noise), selector may prioritize noisy samples

### Mechanism 2
- **Claim:** Dynamic scaling factor $r$ prevents counterintuitive uncertainty in high-class-count scenarios
- **Mechanism:** In CIFAR-100, standard evidential methods can be overly conservative, assigning high uncertainty even with strong evidence for one class. The heuristic scaling $r = (e_{max} + e_{second})^2 / (2 \cdot (e_{max}^2 + e_{second}^2))$ relaxes Dirichlet parameters when evidence dominance is clear.
- **Core assumption:** Ratio between top two evidence scores sufficiently proxies for adjusting Dirichlet prior strength across varying class counts
- **Evidence anchors:** Section 3.1 introduces scaling; Table 2 shows performance drop when removed, especially in later cycles
- **Break condition:** If class count to evidence strength relationship is non-linear or varies by dataset distribution, fixed heuristic may over-correct

### Mechanism 3
- **Claim:** Selecting samples only when training loss increases in latter half ensures high-value updates
- **Mechanism:** Instead of continuous selection, EDALSSC restricts selection to latter training half where loss exhibits upward tick, capturing model's exploration phase most receptive to boundary information.
- **Core assumption:** High uncertainty is most informative when loss fluctuates upward in later stages; early high-uncertainty samples are unreliable
- **Evidence anchors:** Section 3.3 specifies timing strategy; Table 2 validates with ablation 4 showing reduced performance
- **Break condition:** If loss decreases monotonically without fluctuation, selection triggering may fail or fire too rarely

## Foundational Learning

- **Concept: Subjective Logic & Evidential Deep Learning (EDL)**
  - **Why needed here:** Builds on EDL framework replacing softmax with Dirichlet distributions to model "evidence"
  - **Quick check question:** Can you explain how the paper derives "ignorance" ($I$) from sum of Dirichlet parameters ($S$) and number of classes ($K$)?

- **Concept: Semi-Supervised Active Learning (SSCAL) Cycles**
  - **Why needed here:** Architecture relies on iterative training-selection cycles; heuristics depend on cycle progression
  - **Quick check question:** In which part of training cycle does EDALSSC trigger sample selection strategy, and why?

- **Concept: T-conorm Operators**
  - **Why needed here:** Uses T-conorm to aggregate uncertainty components as fuzzy logic "OR" operation
  - **Quick check question:** Why choose T-conorm (probabilistic sum) over simple arithmetic mean for combining ignorance and conflict?

## Architecture Onboarding

- **Component map:** Input Image → ResNet → Evidence Vector $e$ → Dynamic Scaling → Dirichlet Parameters $\alpha$ → Loss Calculation → If (Cycle > Half) AND (Loss Increases): $\alpha$ → Uncertainty $u$ → Sample Selection
- **Critical path:** 1. Input Image → ResNet → Evidence Vector $e$. 2. $e$ → Dynamic Scaling → Dirichlet Parameters $\alpha$. 3. $\alpha$ → Loss Calculation (Supervised & Unsupervised $U$). 4. If (Cycle > Half) AND (Loss Increases): $\alpha$ → Uncertainty $u$ → Sample Selection
- **Design tradeoffs:** Heuristic complexity vs. calibration (dynamic scaling adds complexity but prevents counterintuitive uncertainty); selection timing reduces querying frequency but aims to improve quality
- **Failure signatures:** Ablation 1/SVHN shows 96.93% → 78.87% accuracy crash without uncertainty module; stagnant uncertainty if scaling factor not implemented properly
- **First 3 experiments:**
  1. Reproduce Ablation 2 (Scaling): Train CIFAR-100 with/without scaling factor $r$, verify counterintuitive uncertainty appears in baseline
  2. Visualization of Selection: Run t-SNE on selected samples, confirm clustering near decision boundaries vs. class centers
  3. Timing Validation: Force selection in first vs. second half of training to validate early uncertainty unreliability claim

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Dynamic scaling factor relies on heuristic formulation without theoretical guarantee of optimality
- T-conorm aggregation lacks extensive empirical validation against alternative uncertainty combination methods
- Selection timing strategy based on loss monitoring may not generalize across different training regimes and datasets

## Confidence

- **High Confidence:** Core evidential deep learning framework, overall architecture combining supervised/unsupervised losses, basic active learning cycle implementation
- **Medium Confidence:** Dynamic scaling factor effectiveness, T-conorm formulation for uncertainty aggregation
- **Low Confidence:** Generalizability of selection timing strategy across different training regimes and datasets

## Next Checks
1. Test dynamic scaling factor $r$ on datasets with varying class distributions to validate robustness beyond CIFAR-100
2. Compare T-conorm uncertainty aggregation against alternatives (geometric mean, max) to establish optimal combination
3. Evaluate selection timing strategy under different learning rate schedules and batch sizes to determine sensitivity to training hyperparameters