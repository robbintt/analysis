---
ver: rpa2
title: 'MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga
  Understanding'
arxiv_id: '2505.20298'
source_url: https://arxiv.org/abs/2505.20298
tags:
- text
- manga
- mangavqa
- mangaocr
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MangaVQA and MangaOCR, two benchmarks designed
  to evaluate multimodal understanding of Japanese comics (manga). MangaVQA consists
  of 526 manually constructed question-answer pairs that test a model's ability to
  comprehend narrative context from both visual and textual information in manga spreads.
---

# MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding

## Quick Facts
- **arXiv ID**: 2505.20298
- **Source URL**: https://arxiv.org/abs/2505.20298
- **Reference count**: 40
- **Primary result**: MangaLMM achieves over 70% OCR performance and competitive VQA results on manga-specific benchmarks

## Executive Summary
This paper introduces MangaVQA and MangaOCR, two benchmarks designed to evaluate multimodal understanding of Japanese comics (manga). MangaVQA consists of 526 manually constructed question-answer pairs that test a model's ability to comprehend narrative context from both visual and textual information in manga spreads. MangaOCR consolidates text detection and recognition annotations from manga datasets to assess in-page text extraction accuracy. To establish a strong baseline, the authors develop MangaLMM, a specialized model fine-tuned from Qwen2.5-VL that jointly handles both OCR and VQA tasks on manga. Extensive experiments show that while even state-of-the-art proprietary models struggle with these tasks, MangaLMM achieves over 70% OCR performance and competitive VQA results, demonstrating effective multimodal manga understanding through joint task training.

## Method Summary
The authors develop a specialized model (MangaLMM) by continuing finetuning Qwen2.5-VL-7B-Instruct jointly on two manga-specific tasks: text detection/recognition (MangaOCR) and visual question answering (MangaVQA). Training uses Manga109 dataset consolidated with manga onomatopoeia annotations, providing ~170K text instances for OCR and 39,837 synthetic VQA pairs. The model is trained for 1 epoch on 4 A100 GPUs with batch size 32. Post-processing removes repeated OCR segments appearing more than 10 times. Synthetic VQA data is generated using GPT-4o with OCR annotations provided in the prompt.

## Key Results
- MangaLMM achieves over 70% OCR performance on manga text detection and recognition
- Proprietary models like GPT-4o and Gemini 2.0 Flash show surprisingly low performance on both OCR and VQA tasks
- Even models with near-zero explicit OCR performance can answer text-based VQA questions with non-trivial accuracy, suggesting implicit textual semantic extraction
- Joint training on both OCR and VQA tasks improves performance on both benchmarks compared to single-task training

## Why This Works (Mechanism)
The paper does not explicitly analyze the underlying mechanism for why MangaLMM works, but the results suggest that joint training on OCR and VQA tasks enables the model to develop specialized capabilities for manga understanding. The counterintuitive finding that models can answer text-based VQA questions despite poor OCR performance indicates that LMMs may extract relevant textual information through implicit semantic understanding rather than explicit character recognition.

## Foundational Learning
- **Manga-specific visual understanding**: Needed because manga has unique visual conventions (panels, speech bubbles, onomatopoeia) different from natural images. Quick check: Evaluate on both manga and natural image datasets to measure specialization.
- **Joint task learning**: OCR and VQA are trained together to enable cross-task knowledge transfer. Quick check: Compare performance with separate OCR and VQA models.
- **Synthetic data generation**: GPT-4o generates VQA pairs from manga images with OCR annotations. Quick check: Verify synthetic question diversity and quality through human evaluation.
- **Spatial reasoning**: Manga understanding requires comprehending relationships between text and visual elements across spreads. Quick check: Test on questions requiring cross-panel reasoning.
- **Text detection and recognition**: Accurate bounding box localization and character recognition in manga's unique typography. Quick check: Measure IoU and normalized edit distance on held-out test set.
- **Multimodal integration**: Combining visual and textual information for comprehensive understanding. Quick check: Ablation studies removing either modality.

## Architecture Onboarding

**Component map**: Qwen2.5-VL-7B-Instruct -> Joint OCR+VQA finetuning -> MangaLMM

**Critical path**: Input manga spread → OCR detection/recognition → Visual understanding → VQA reasoning → Answer generation

**Design tradeoffs**: The authors chose joint training over separate models to enable cross-task knowledge transfer, accepting potential catastrophic forgetting of general capabilities in exchange for manga specialization.

**Failure signatures**: Near-zero OCR scores with nonsensical repeated outputs indicate improper post-processing or bbox format issues. Dramatic drops on general vision benchmarks (87.8→1.5%) indicate catastrophic forgetting.

**First experiments**: 
1. Evaluate MangaLMM on MMBench/MMMU before and after finetuning to quantify capability preservation
2. Test joint training with natural image data (LLaVA-OneVision) to recover general performance
3. Conduct ablation studies varying manga-to-natural-image data ratios during training

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the underlying mechanism that enables LMMs to answer text-based VQA questions with non-trivial accuracy despite near-zero explicit OCR performance on manga? The paper identifies this as counterintuitive and provides qualitative analysis showing GPT-4o detects text in nonsensical locations yet partially reads content, but does not explain how implicit textual semantic extraction occurs without accurate localization.
- **Open Question 2**: How can positional information from OCR annotations be effectively incorporated to improve synthetic VQA data generation for manga? The authors report that using only text content was more effective than including both text and positional information, yet explicitly state "leveraging it remains a promising direction for future work."
- **Open Question 3**: What architectural or training modifications would enable LMMs to achieve both strong manga specialization and competitive performance on general natural image benchmarks? The paper demonstrates feasibility of joint training but does not characterize how much general capability can be preserved at what cost to manga performance.

## Limitations
- The model suffers from catastrophic forgetting, dropping from 58.6% to 25.8% on MMMU after specialization to manga tasks
- Joint training with natural image data partially recovers general performance but the optimal trade-off curve remains unexplored
- The effectiveness of positional information in synthetic VQA generation was not fully resolved, with current experiments showing it does not help under tested conditions

## Confidence
- **High confidence**: The overall experimental methodology and task definitions are clearly specified
- **Medium confidence**: The training procedure and model architecture are well-documented, but hyperparameter details are missing
- **Low confidence**: The data preprocessing pipeline lacks sufficient procedural detail for exact replication

## Next Checks
1. Verify the exact data preprocessing steps by requesting the complete pipeline from authors or testing with the provided OCR format specifications
2. Conduct ablation studies on learning rate and optimizer parameters to understand their impact on OCR performance
3. Test the model on MMBench/MMMU benchmarks before and after finetuning to quantify catastrophic forgetting effects and validate the joint training mitigation approach