---
ver: rpa2
title: An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers
arxiv_id: '2505.05828'
source_url: https://arxiv.org/abs/2505.05828
tags:
- users
- user
- chatbot
- about
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a chatbot-based system to engage Spanish teenagers
  in mental health awareness using a self-disclosure technique. The system integrates
  controlled dialogue with GPT-3 to facilitate open conversations about mental disorders.
---

# An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers

## Quick Facts
- arXiv ID: 2505.05828
- Source URL: https://arxiv.org/abs/2505.05828
- Reference count: 40
- Primary result: 70% of teenagers emotionally engaged with the chatbot, sharing mental health concerns through self-disclosure reciprocity

## Executive Summary
This study developed a hybrid chatbot system that combines controlled psychological prompts with open GPT-3 dialogue to engage Spanish teenagers in mental health conversations. The system uses a self-disclosure technique where the chatbot shares concerns as a peer, encouraging reciprocal vulnerability from users. Tested with 44 teenagers aged 12-18, the chatbot achieved significant emotional engagement with 1,860 messages exchanged, demonstrating that AI-based systems can create safe spaces for discussing mental health disorders among adolescents.

## Method Summary
The chatbot uses a Telegram interface connected to a dialogue manager that orchestrates interactions between users and GPT-3 davinci-002. Messages flow through a Spanish-English translation pipeline using DeepL API, with GPT-3 generating responses at temperature 0.9 and maximum 170 tokens. The system alternates between open dialogue and psychologist-designed controlled prompts every 5 interactions to prevent hallucination while maintaining engagement. Conversations are logged in MongoDB with sensitivity scores and linguistic feature extraction via TextFlow toolkit for analysis of mental health indicators.

## Key Results
- 70% of users emotionally engaged with the chatbot, sharing their mental health concerns
- 50% of users reported positive attitudes toward the system and willingness to continue interaction
- Linguistic analysis revealed correlations between specific features (coordinating conjunctions, fear emotion, lexical diversity) and mental health risk status

## Why This Works (Mechanism)

### Mechanism 1: Self-Disclosure Reciprocity
Chatbot self-disclosure encourages users to share their own concerns through virtual-empathy. The bot presents as a vulnerable teenager sharing worries, creating a safe space for reciprocal disclosure. Core assumption: users will mirror the bot's self-disclosure behavior. Evidence: 100% of users expressing concerns had previously engaged in empathetic interactions with the chatbot.

### Mechanism 2: Controlled-Open Dialogue Oscillation
Alternating controlled prompts with open GPT-3 dialogue maintains safety while enabling natural conversation. Every 5 open interactions, psychologist-designed prompts redirect conversation to prevent hallucination and repetition. Core assumption: GPT-3 drifts toward repetition without periodic grounding. Evidence: Controlled dialogue helps reset context when users respond with few words.

### Mechanism 3: Linguistic Marker Correlation with Risk Status
Specific linguistic features correlate with mental health risk indicators. Indicated users show higher coordinating conjunction frequency, fear emotion, and particular lexical diversity patterns. Core assumption: language production differences reflect underlying psychological states. Evidence: coordinating conjunction frequency shows highest correlation (r>0.4) with risk status.

## Foundational Learning

- **Social Penetration Theory (SPT)**: Underpins self-disclosure design by explaining how peripheral disclosures (name, age) precede central disclosures (feelings). Quick check: Why should the bot share surface-level worries before asking about deep concerns?

- **GPT Temperature Parameter**: Temperature=0.9 chosen for creative, varied responses in empathetic dialogue. Quick check: What would happen to response variety if temperature were set to 0.1?

- **Translation Pipeline Latency and Error Modes**: Spanish→English→Spanish introduces gender mismatches and colloquialism failures. Quick check: Why would native Spanish LLM support reduce certain error types?

## Architecture Onboarding

- **Component map**: Telegram Bot API -> Dialogue Manager -> GPT-3 Davinci-002 -> DeepL API -> MongoDB
- **Critical path**: User message arrives via Telegram → Dialogue manager classifies phase → If open: Spanish→English → GPT-3 with prompt+context → English→Spanish → send → Every 5th open interaction: inject controlled prompt → Log to MongoDB with timestamp, alias, topic, sensitivity
- **Design tradeoffs**: Translation vs. native Spanish (English GPT-3 performed better but adds errors), controlled prompt frequency (every 5 interactions balances hallucination prevention vs. immersion), peer persona vs. therapist persona (peer encourages reciprocal disclosure)
- **Failure signatures**: Repetition loops with monosyllabic responses, colloquial Spanish mistranslation (e.g., "apoyargao"), gender pronoun mismatches, rare negation comprehension failures
- **First 3 experiments**: A/B test GPT-3 via translation vs. native multilingual model on error rate and user satisfaction, vary controlled dialogue frequency (3, 5, 7 interactions) and measure engagement depth, test bot personas (peer vs. mentor vs. neutral) on self-disclosure rates

## Open Questions the Paper Calls Out

### Open Question 1
Replacing the translation pipeline with a native Spanish Large Language Model (LLM) could improve understanding of colloquialisms and control gender inflections. The current DeepL pipeline caused errors with colloquial language and gender pronouns, such as mistranslating "apoyargao." A comparative study measuring error rates between translation-based and native Spanish systems would resolve this.

### Open Question 2
The study measured engagement but not therapeutic efficacy or improvements in mental health status over time. While 70% engaged and 50% reported positive attitudes, there was no pre- and post-intervention clinical evaluation to measure changes in mental health indicators. A longitudinal study with psychologist evaluations would resolve this gap.

### Open Question 3
Multimodal inputs (voice, facial expressions) could impact empathic accuracy and engagement. The current text-only study lacks non-verbal cues critical to human empathy. A prototype implementing IoT sensors to correlate non-verbal cues with text sentiment could measure increases in empathic response accuracy compared to the text-only version.

### Open Question 4
The linguistic markers (coordinating conjunctions, lexical diversity) show correlations with risk status in a small sample (44 users) but require validation in larger populations. While promising correlations were found, the sample size is insufficient to validate these as robust diagnostic tools. Training and testing on a significantly larger corpus would resolve this.

## Limitations
- Sample size of 44 participants limits generalizability across diverse populations and cultures
- Translation pipeline from Spanish to English and back introduces gender mismatches and colloquialism failures
- Mental health detection relies on self-reported screening tools rather than clinical diagnosis, introducing potential measurement error

## Confidence

- **High confidence**: The chatbot successfully engaged users emotionally (70% shared concerns) and created a comfortable environment for mental health discussions
- **Medium confidence**: The linguistic markers distinguishing healthy from indicated users show promising correlations but require larger validation samples
- **Low confidence**: The specific translation errors and their impact on conversation quality cannot be fully quantified from the reported data

## Next Checks

1. **Clinical validation study**: Test the chatbot with a larger, more diverse sample including clinical diagnostic interviews to validate the screening accuracy of the linguistic markers and engagement metrics.

2. **Translation pipeline optimization**: Compare the current translation-based approach against native Spanish LLM implementations to quantify error rates and measure impact on engagement quality and diagnostic accuracy.

3. **Longitudinal engagement analysis**: Track users over extended periods to assess whether initial engagement translates to sustained mental health awareness, behavioral changes, or actual help-seeking behavior.