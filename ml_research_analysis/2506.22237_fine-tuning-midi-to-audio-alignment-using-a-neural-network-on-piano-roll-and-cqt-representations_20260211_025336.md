---
ver: rpa2
title: Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and
  CQT Representations
arxiv_id: '2506.22237'
source_url: https://arxiv.org/abs/2506.22237
tags:
- piano
- alignment
- midi
- audio
- unaligned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neural network approach for synchronizing
  audio recordings of human piano performances with their corresponding loosely aligned
  MIDI files. The task is addressed using a Convolutional Recurrent Neural Network
  (CRNN) architecture, which effectively captures spectral and temporal features by
  processing an unaligned piano roll and a spectrogram as inputs to estimate the aligned
  piano roll.
---

# Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations

## Quick Facts
- arXiv ID: 2506.22237
- Source URL: https://arxiv.org/abs/2506.22237
- Reference count: 30
- Neural network improves MIDI-to-audio alignment accuracy by up to 20% over DTW baseline

## Executive Summary
This paper introduces a Convolutional Recurrent Neural Network (CRNN) for aligning audio recordings of human piano performances with their corresponding loosely aligned MIDI files. The model processes paired piano roll and Constant Q Transform (CQT) representations to estimate aligned note positions, addressing the limitations of Dynamic Time Warping (DTW) in handling non-monotonic timing errors. Trained on augmented synthesized data, the CRNN achieves up to 20% higher alignment accuracy than DTW across various tolerance windows. The study also demonstrates that combining DTW preprocessing with CRNN fine-tuning provides additional robustness and consistency.

## Method Summary
The method employs a CRNN architecture with two parallel CNN branches that independently extract spectral-temporal features from symbolic (piano roll) and acoustic (CQT) domains. These embeddings are concatenated and processed by a bidirectional LSTM to learn temporal correspondences between unaligned and aligned note positions. The model is trained on synthesized piano data augmented with random timing errors (±100ms shifts), using binary cross-entropy loss. Post-processing involves thresholding outputs and matching extracted notes to input MIDI by pitch and order. A hybrid approach combining DTW preprocessing with CRNN fine-tuning is also explored to leverage global alignment and local refinement.

## Key Results
- CRNN achieves up to 20% higher alignment accuracy than DTW across tolerance windows
- CQT representation provides ~2% improvement over Mel spectrogram at 10ms tolerance
- Hybrid DTW+CRNN approach improves real-world recording accuracy by 50% over DTW alone
- Temporal modeling is essential: CNN-only version shows ~50% performance drop at tight tolerances

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal feature extraction from paired piano roll and CQT representations enables frequency-specific alignment correction that DTW cannot achieve. Two parallel CNN branches independently extract spectral-temporal features from symbolic and acoustic domains. These embeddings are concatenated and passed to a BiLSTM that learns temporal correspondence between unaligned and aligned note positions. Because the network processes pitch dimensions independently before temporal modeling, it can correct notes individually rather than warping entire timeframes uniformly.

### Mechanism 2
Bidirectional LSTM temporal modeling captures non-monotonic alignment corrections that DTW's monotonic path constraint cannot handle. The BiLSTM processes concatenated feature sequence in both forward and backward directions, learning to predict aligned note activations from local audio-MIDI correspondence patterns. Unlike DTW which enforces monotonic warping, the network can predict non-sequential corrections when unaligned MIDI contains random timing errors.

### Mechanism 3
Sequential DTW preprocessing followed by CRNN fine-tuning combines global monotonic alignment with local frequency-specific refinement. DTW provides coarse alignment that roughly synchronizes audio and MIDI, reducing search space. The CRNN then refines note onset positions through frequency-specific corrections, correcting systematic errors where certain pitch ranges were misaligned.

## Foundational Learning

- **Constant Q Transform (CQT)**: Why needed here: The paper uses log-scaled CQT specifically because its frequency bins match piano roll's 88-pitch layout (12 bins/octave from 27.5 Hz), enabling direct feature correspondence. Mel spectrograms showed worse results. Quick check: Can you explain why CQT's logarithmic frequency spacing is preferable to Mel spectrogram for piano-specific tasks?

- **Piano Roll Representation**: Why needed here: This binary N×88 matrix encodes note activations over time. Understanding that velocity is ignored and one-frame gaps (10ms) are inserted before repeated note onsets is critical for interpreting model's input/output. Quick check: Why would inserting 10ms gaps before repeated note onsets help the network distinguish consecutive notes?

- **Dynamic Time Warping (DTW) Constraints**: Why needed here: The paper positions CRNN as addressing DTW's limitations—particularly monotonicity constraint and inability to perform frequency-specific alignment. Understanding DTW helps contextualize when CRNN excels vs. fails. Quick check: If a MIDI file has 10% tempo deviation from audio, why might DTW struggle more than CRNN, and why does the paper show both failing at this task?

## Architecture Onboarding

- Component map: Input: Piano Roll (N×88) -> Conv2D×3 (16,16,32) -> Dense(256) -> Concat(N×512) -> BiLSTM(512→256) -> Dense(88, sigmoid) -> Output Piano Roll
- Critical path: Input preprocessing (100 FPS, 88 pitch bins, log CQT) -> CNN feature extraction -> BiLSTM temporal alignment -> sigmoid output -> thresholding -> MIDI reconstruction via note matching
- Design tradeoffs: CNN-only (no RNN) drops 10ms accuracy from 47.41% to 22.06% (Table 2)—temporal modeling is essential. GRU vs. LSTM: LSTM outperforms GRU by ~1.5% at tight tolerances—worth minor complexity increase. 100 FPS vs. 200 FPS: No significant improvement beyond 100 FPS (Table 4)—10ms frame size is sufficient. CQT vs. Mel: Log-CQT provides ~2% improvement at 10ms tolerance—justifies computational cost.
- Failure signatures: Repeated notes merged into single long activations—requires post-processing note matching to recover. Tempo changes >5% cause onset errors to exceed 40ms—model cannot extrapolate beyond training augmentation range. Random non-monotonic timing errors defeat DTW but CRNN handles.
- First 3 experiments: 1) Baseline replication: Train CRNN on MAPS with 100ms timing variation, evaluate on held-out test split. Verify you achieve ~40-43% accuracy at 10ms tolerance before proceeding. 2) Ablation: CNN-only vs. CRNN: Remove BiLSTM, replace with direct Dense(88) from concatenated features. Expect ~50% performance drop at tight tolerances—confirms temporal modeling necessity. 3) Hybrid DTW+CRNN validation: Apply MrMsDTW to preprocess, then run CRNN. Target >60% accuracy at 10ms on real-world recordings. If not achieved, check DTW implementation matches paper's DLNCO features.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed CRNN alignment approach be effectively generalized to polyphonic instruments other than the piano? The conclusion states, "In future work, this approach could be extended to other instruments using datasets like e.g. GuitarSet." Training the model on a multi-instrument dataset like GuitarSet and comparing alignment accuracy against piano baseline would resolve this.

### Open Question 2
Would adopting transformer architectures improve alignment performance compared to the current CRNN? The conclusion suggests, "Exploring alternative architectures such as transformers could also further enhance performance." A comparative evaluation of alignment accuracy between proposed CRNN and transformer-based model on Unaligned MAPS dataset would resolve this.

### Open Question 3
Does using this model for dataset alignment improve the onset precision of downstream Automatic Music Transcription (AMT) models? The conclusion hypothesizes, "applying the model during AMT training may improve onset precision." Fine-tuning an AMT model using data pre-aligned by proposed CRNN and measuring its transcription onset accuracy against baseline trained on DTW-aligned data would resolve this.

## Limitations
- Cross-modal generalization remains untested—model only validated on synthesized data and limited real-world examples from MAPS
- Augmentation approach (uniform ±100ms shifts) may not capture all types of human timing variations
- Note matching post-processing assumes input MIDI is reasonably accurate

## Confidence

- **High confidence**: CRNN architecture provides measurable improvements over DTW for MAPS dataset with controlled timing variations
- **Medium confidence**: Frequency-specific correction mechanism is well-supported by CQT design choice but lacks direct ablation studies
- **Low confidence**: Hybrid DTW+CRNN approach shows promising results but has minimal ablation evidence

## Next Checks

1. **Ablation on representation choice**: Train exact same CRNN architecture using Mel spectrogram instead of CQT on MAPS dataset. Compare 10ms accuracy to verify ~2% improvement claimed for CQT is consistent and not dataset-specific.

2. **Generalization test**: Apply trained model to real-world piano recordings from different sources (MAESTRO dataset or commercial recordings). Measure accuracy drop compared to MAPS results to quantify cross-domain performance.

3. **Tempo variation stress test**: Systematically evaluate model performance on MIDI files with tempo variations beyond ±5% (up to ±20%). Compare CRNN vs. DTW vs. hybrid approaches to identify failure thresholds and validate claim that CRNN handles non-monotonic timing better.