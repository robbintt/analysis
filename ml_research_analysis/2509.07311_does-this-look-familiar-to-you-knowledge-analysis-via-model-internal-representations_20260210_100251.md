---
ver: rpa2
title: Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations
arxiv_id: '2509.07311'
source_url: https://arxiv.org/abs/2509.07311
tags:
- data
- training
- knowledge
- unfamiliar
- familiar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of effective training data selection
  for supervised fine-tuning (SFT) of large language models (LLMs), where increasing
  data volume does not guarantee performance gains and prompt-based knowledge detection
  methods are sensitive to variations. The proposed method, Knowledge Analysis via
  Model Internal Representations (KAMIR), analyzes data by computing similarity scores
  between hidden states at each layer and the final output vector, forming an awareness
  vector that captures the model's familiarity with the input.
---

# Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations

## Quick Facts
- arXiv ID: 2509.07311
- Source URL: https://arxiv.org/abs/2509.07311
- Authors: Sihyun Park
- Reference count: 18
- Key outcome: Training LLMs on "unfamiliar" data selected via internal representation analysis consistently outperforms training on "familiar" data, with improvements linked to higher gradient norms and entropy.

## Executive Summary
This study introduces KAMIR (Knowledge Analysis via Model Internal Representations), a method for selecting effective training data during supervised fine-tuning of large language models. Unlike prior approaches that rely on prompt-based knowledge detection, KAMIR analyzes the model's internal processing by computing similarity scores between hidden states at each layer and the final output vector. This forms an "awareness vector" that captures the model's familiarity with input data. Experiments across machine reading comprehension, multiple-choice QA, and summarization tasks demonstrate that training with unfamiliar data yields consistent performance improvements, particularly in tasks with concise, unambiguous answers.

## Method Summary
KAMIR extracts hidden states from all layers at the final generated token, then computes cosine similarity between each intermediate layer's hidden state and the final layer's hidden state to form an awareness vector. This vector is used to train a simple classifier distinguishing familiar from unfamiliar data. The method was tested using Qwen3-4B-Base with LoRA fine-tuning, comparing performance when training on data classified as familiar, unfamiliar, or random. The approach operates without task-specific prompts, enabling application to generative tasks like summarization where correctness is hard to verify automatically.

## Key Results
- Training on unfamiliar data consistently outperformed training on familiar data across multiple tasks
- Improvements were linked to reduced loss, increased prediction entropy, and higher gradient norms during training
- The effect was most pronounced in tasks with concise, unambiguous answers (MRC, MCQA)
- Unfamiliar data selection enabled effective training data filtering even with small datasets

## Why This Works (Mechanism)

### Mechanism 1: Representation Convergence as Familiarity Signal
The method computes an "awareness vector" by calculating cosine similarity between hidden states at each layer and the final layer's hidden state at the final generated token. High similarity across layers implies the model has pre-existing knowledge of the input, while low similarity indicates the model is working harder to resolve unfamiliar content. This assumes the final hidden state represents the model's "conclusion" and deviation from this conclusion reflects computational effort or novelty.

### Mechanism 2: Gradient Signal Amplification via Novelty
Training on unfamiliar data generates higher gradient norms and entropy compared to familiar data, driving better generalization. Familiar data produces low entropy and minimal loss, resulting in small gradient updates that provide little learning signal. Unfamiliar data maintains higher entropy and loss, forcing meaningful parameter updates that expand the model's functional capability rather than reinforcing memorization.

### Mechanism 3: Task-Agnostic Knowledge Probing
By analyzing processing trajectories (hidden states) rather than outputs (logits/text), KAMIR bypasses the need for task-specific prompt engineering. This enables data selection for generative tasks like summarization where automatic correctness verification is difficult, unlike prior methods limited to multiple-choice formats.

## Foundational Learning

- **Logit Lens / Representation Analysis**: Understanding how to extract and compare hidden states at specific layers is fundamental to KAMIR. Quick check: Can you write a script to extract normalized hidden state vectors for the last token at different layers using PyTorch or HuggingFace?

- **Cosine Similarity in High-Dimensional Space**: The awareness vector relies entirely on cosine similarity measuring orientation alignment, not magnitude. Quick check: If vector A is [1, 1] and vector B is [100, 100], what is their cosine similarity? (Answer: 1.0). Does KAMIR account for magnitude? (Answer: No).

- **Memorization vs. Generalization in SFT**: Understanding why training on familiar data leads to diminishing returns while novel data forces generalization is crucial. Quick check: If a model achieves 99% accuracy on training data without updates, what is the expected gradient norm? (Near zero). Does this help the model learn? (No).

## Architecture Onboarding

- **Component map**: Inference Engine -> State Hook -> Awareness Calculator -> Classifier -> Data Filter
- **Critical path**: The State Hook is most fragile - must extract hidden state of final generated token, not final input token
- **Design tradeoffs**: Computing awareness vectors requires full forward pass (expensive for large datasets); using a simple MLP for the classifier prevents overfitting to calibration data
- **Failure signatures**: Uniform awareness vectors (no separation between familiar/unfamiliar), performance collapse when training on unfamiliar data, task mismatch showing high performance on MRC but failure on summarization
- **First 3 experiments**:
  1. Reproduce the separation: Gather 100 pre-2020 documents (Familiar) and 100 post-release papers (Unfamiliar), compute awareness vectors, and visualize with t-SNE to check clustering
  2. Calibrate the classifier: Train MLP on 80% of vectors from experiment 1, test accuracy on remaining 20%. If <70%, signal is too weak
  3. Proxy ablation: Compare "High Loss" filtering against "KAMIR Unfamiliar" training to determine if KAMIR adds value beyond simple uncertainty measures

## Open Questions the Paper Calls Out

- How can evaluation and training methods be adapted to maximize benefits of unfamiliar data in generative tasks with high answer variability like summarization?
- Can integrating awareness-based sampling and clustering with KAMIR improve computational efficiency of SFT?
- Why does unfamiliar data improve extractive MRC tasks but degrade extractive summarization tasks like CNN/DailyMail?

## Limitations
- The method's effectiveness across diverse domains and model families remains unverified
- The correlation between high gradient norms/entropy and better generalization is shown but not definitively proven to be causal
- Performance on generative tasks like summarization was mixed, with some datasets showing degradation despite generalization benefits

## Confidence
- **Low**: Core claim rests heavily on assumption that awareness vectors accurately capture semantic familiarity; limited validation across architectures
- **Medium**: Mechanism linking gradient norms/entropy to generalization is plausible but causation not established
- **High**: Architectural implementation is clearly specified and reproducible; empirical results are internally consistent

## Next Checks
1. Conduct ablation study removing individual layer similarity scores from awareness vector to identify which layers contribute most to classification accuracy
2. Test cross-domain generalization by applying KAMIR-trained classifiers to legal documents, code repositories, or scientific literature
3. Compare KAMIR's unfamiliar data selection against simple heuristics like high-loss filtering and conduct human evaluation of selected data quality