---
ver: rpa2
title: 'From Slides to Chatbots: Enhancing Large Language Models with University Course
  Materials'
arxiv_id: '2510.22272'
source_url: https://arxiv.org/abs/2510.22272
tags:
- course
- materials
- llms
- text
- slide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates enhancing LLMs for educational chatbots
  using university course materials, focusing on lecture slides and transcripts. Two
  strategies are compared: Retrieval-Augmented Generation (RAG) and Continual Pre-Training
  (CPT).'
---

# From Slides to Chatbots: Enhancing Large Language Models with University Course Materials

## Quick Facts
- arXiv ID: 2510.22272
- Source URL: https://arxiv.org/abs/2510.22272
- Reference count: 9
- Primary result: RAG outperforms CPT for enhancing LLMs with small, specialized university course materials

## Executive Summary
This paper investigates how to enhance large language models for educational chatbots using university course materials, specifically lecture slides and transcripts. The authors compare two strategies - Retrieval-Augmented Generation (RAG) and Continual Pre-Training (CPT) - across six computer science topics from the SciEx benchmark. They find that RAG is more effective and efficient than CPT for small datasets, and that incorporating slides as images in a multimodal RAG approach significantly improves performance over text-only retrieval.

## Method Summary
The study compares RAG and CPT approaches for enhancing LLMs with university course materials. RAG uses M3-Embedding retriever (559M params) to retrieve relevant chunks from a vector database containing slide and transcript content, which are then passed to generator LLMs. CPT involves fine-tuning base models on course materials with replay and instruction residuals. Multimodal RAG extends this by retrieving via text but presenting slide images to vision-capable generators like Qwen2-VL. The dataset includes 3.9K slides (~0.2M tokens) and 3.3M transcript tokens across six CS topics, evaluated on 154 exam questions using automatic grading.

## Key Results
- RAG outperforms CPT for small, specialized datasets due to catastrophic forgetting risks in CPT
- Multimodal RAG using slide images improves performance over text-only retrieval, particularly for visually-rich content
- RAG provides larger gains on difficult questions where baseline LLM knowledge is weakest
- Slide transcripts degrade performance while polished transcripts help; slide chunks (1 per page) work best

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG outperforms CPT for small, specialized course material datasets
- Mechanism: RAG dynamically retrieves relevant content at inference time, avoiding catastrophic forgetting risks inherent in CPT when training data is limited (~0.2M slide tokens, 3.3M transcript tokens). The model accesses external knowledge without modifying its weights.
- Core assumption: The retrieval system can accurately identify relevant chunks; course materials contain the knowledge needed to answer exam questions.
- Evidence anchors:
  - [abstract] "given the relatively small size of university course materials, RAG is more effective and efficient than CPT"
  - [Section 6.2.2] "CPT performs worse than both the baseline and RAG... The overall loss in performance is likely due to catastrophic forgetting"
  - [corpus] Related work (AI-University, ChemTAsk) confirms RAG effectiveness in educational domains, though specific comparisons to CPT are limited
- Break condition: When retrieval fails to surface relevant content (poor chunking, weak embeddings, or questions requiring synthesis across distant chunks).

### Mechanism 2
- Claim: Multimodal RAG (passing slide images instead of extracted text) improves performance on visually-rich course materials.
- Mechanism: Text extraction from PDFs discards visual information (images, formulas, layout, spatial relationships). By retrieving via text embeddings but presenting actual slide images to vision-capable LLMs (e.g., Qwen2-VL), the generator preserves visual context that aids comprehension.
- Core assumption: The generator LLM has strong vision capabilities; visual elements in slides are relevant to exam questions.
- Evidence anchors:
  - [abstract] "incorporating slides as images in the multi-modal setting significantly improves performance over text-only retrieval"
  - [Section 6.1.4] Table 5 shows Qwen2-VL with slide images (42.50 avg) outperforms slide text (40.87 avg)
  - [Figure 3] Concrete example where slide image preserves temporal arrows distinguishing "usability" vs "user experience"
  - [corpus] Weak direct corpus evidence for image-based RAG specifically; related work focuses on text-based educational RAG
- Break condition: When slides are text-heavy with minimal visuals, or when vision models lack document understanding capabilities.

### Mechanism 3
- Claim: RAG provides larger gains on difficult questions where baseline LLM knowledge is weakest.
- Mechanism: Easy questions may be answerable from pre-trained knowledge; hard questions require course-specific information retrieval. RAG fills knowledge gaps selectively.
- Core assumption: Difficulty annotations correlate with domain-specific knowledge requirements; retrieved content is relevant to hard questions.
- Evidence anchors:
  - [Section 6.1.3] Table 4: Hard questions improved +10.79 points vs. +1.92 for easy questions
  - [Section 6.1.2] HCI showed +6.11 improvement, aligning with prior findings that LLMs lack HCI context
  - [corpus] No direct corpus confirmation of difficulty-stratified RAG effects
- Break condition: When hard questions require reasoning beyond retrieval, or when retrieval corpus lacks sufficient coverage.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) Architecture**
  - Why needed here: Core approach compared against CPT; requires understanding of retriever-generator separation, embedding models, and chunking strategies.
  - Quick check question: Can you explain why RAG avoids catastrophic forgetting that plagues CPT?

- Concept: **Vision-Language Models (VLMs)**
  - Why needed here: Multimodal RAG requires understanding how VLMs process images alongside text, and their limitations vs. text-only models.
  - Quick check question: What information might be lost when extracting text from a slide containing diagrams and formulas?

- Concept: **Catastrophic Forgetting & Instruction Residuals**
  - Why needed here: CPT experiments use instruction residuals (IR = θ_i - θ_b) to restore instruction-following after domain adaptation; understanding this tradeoff is critical.
  - Quick check question: Why does CPT on small datasets risk degrading general capabilities, and how do instruction residuals mitigate this?

## Architecture Onboarding

- Component map:
  - **Retriever**: M3-Embedding (559M params) for multilingual text embeddings
  - **Vector Database**: Stores chunked course materials (slides as 1 chunk/page; transcripts as 300-token chunks with 10% overlap)
  - **Generator LLMs**: LLaMA 3.1/3.3, Qwen2.5, Qwen2-VL (vision-capable)
  - **Knowledge Base**: 3.9K slides, 3.3M tokens of transcripts across 6 CS topics
  - **CPT Pipeline** (alternative): Base LLM → CPT with replay → Instruction Residuals addition

- Critical path:
  1. **Chunking**: Slides = 1 page/chunk; Transcripts = sequential 300-token chunks, 10% overlap
  2. **Embedding**: M3-Embedding for all chunks → vector database
  3. **Retrieval**: Query embedding → cosine similarity → top-k chunks (k=4 optimal)
  4. **Generation**: Retrieved chunks + question → LLM prompt (text or multimodal)
  5. **Multimodal variant**: Retrieve via text, but pass slide *images* (not text) to vision-capable generator

- Design tradeoffs:
  - **RAG vs CPT**: RAG is faster to implement, no training required, but requires inference-time retrieval; CPT internalizes knowledge but risks forgetting with small datasets
  - **Text vs Multimodal RAG**: Images preserve visuals but require VLMs and higher compute; text-only is faster but loses visual context
  - **Chunk size**: 300 tokens for transcripts optimal; smaller may lose context, larger may dilute relevance
  - **k (retrieval count)**: k=4 optimal for slides; higher k adds noise without gains

- Failure signatures:
  - **CPT catastrophic forgetting**: Validation perplexity on general data (Wikipedia) increases while domain perplexity decreases—stop early when domain perplexity stabilizes
  - **Poor retrieval**: Questions requiring cross-chunk synthesis fail; symptoms include off-topic retrieved chunks
  - **Vision model limitations**: VLMs may underperform text-only models on text-heavy slides if document understanding is weak
  - **Transcript noise**: Raw transcripts degrade performance; polish with LLM or prefer slides

- First 3 experiments:
  1. **Baseline RAG with slide text**: Use M3-Embedding to retrieve top-4 slide chunks; prompt generator with retrieved text + question. Measure against baseline (no retrieval).
  2. **Multimodal RAG with slide images**: Same retrieval, but convert retrieved slides to images for vision-capable generator (Qwen2-VL). Compare to text-only.
  3. **Ablate by difficulty/topic**: Stratify results by question difficulty (easy/medium/hard) and topic (HCI vs. DLNN). Expect larger gains on hard questions and weaker baseline topics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would CPT outperform RAG on larger models (70B+ parameters) with the same limited course materials?
- Basis in paper: [explicit] "we did not perform CPT on models other than Llama 3.1 8B, on which CPT might have a more positive effect."
- Why unresolved: Larger models may exhibit different adaptation dynamics with small, specialized datasets.
- What evidence would resolve it: CPT experiments on 70B+ models with direct RAG comparison.

### Open Question 2
- Question: Do RAG improvements generalize beyond computer science to other academic disciplines?
- Basis in paper: [explicit] "our experiments are limited to computer science courses from a single institution."
- Why unresolved: Different disciplines have varying course material structures and knowledge types.
- What evidence would resolve it: Cross-disciplinary evaluation with humanities and natural science materials.

### Open Question 3
- Question: What systematic biases does LLM-based grading introduce compared to human experts?
- Basis in paper: [explicit] Automatic grading "may introduce systematic biases or grading inconsistencies."
- Why unresolved: Only correlation with human scores was measured, not bias patterns.
- What evidence would resolve it: Fine-grained human vs. LLM grading comparison across question types.

### Open Question 4
- Question: What transcript characteristics cause performance degradation, and can preprocessing mitigate this?
- Basis in paper: [inferred] Authors offer only a "likely explanation" for transcript failure without empirical validation.
- Why unresolved: No ablation isolating specific causes (verbosity, disfluency, structure).
- What evidence would resolve it: Controlled experiments with varied transcript preprocessing strategies.

## Limitations

- Evaluation dataset is small and domain-specific (154 questions across 6 CS topics)
- CPT experiments limited to one base model (LLaMA 3.1 8B) with minimal hyperparameter tuning
- No ablation studies on retriever quality or embedding model alternatives
- Multimodal RAG assumes access to high-quality slide images and vision-capable LLMs

## Confidence

- RAG vs CPT effectiveness: **High** - multiple experiments consistently show RAG superiority with clear mechanism (catastrophic forgetting)
- Multimodal RAG improvement: **Medium** - statistically significant but based on single vision model comparison; visual benefits are demonstrated but magnitude depends on slide design
- Difficulty-stratified gains: **Low-Medium** - compelling but based on single dataset with limited question count; correlation between difficulty and domain knowledge needs broader validation

## Next Checks

1. Test RAG effectiveness on non-CS educational domains (e.g., humanities, sciences) to assess domain generalization
2. Compare M3-Embedding retriever against alternative embedding models (BGE, Jina) to isolate retrieval contribution
3. Implement cross-validation with question-level stratification to assess statistical significance of multimodal RAG gains across different slide designs and visual content densities