---
ver: rpa2
title: Offline-to-online hyperparameter transfer for stochastic bandits
arxiv_id: '2501.02926'
source_url: https://arxiv.org/abs/2501.02926
tags:
- learning
- algorithm
- problem
- complexity
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of transferring hyperparameters
  from offline data to online stochastic bandit problems. The authors consider a multi-task
  setting where problem instances are drawn from an unknown distribution, and the
  learner has access to historical data from similar tasks.
---

# Offline-to-online hyperparameter transfer for stochastic bandits

## Quick Facts
- **arXiv ID**: 2501.02926
- **Source URL**: https://arxiv.org/abs/2501.02926
- **Reference count**: 40
- **Primary result**: Provides theoretical bounds and practical algorithms for transferring hyperparameters from offline multi-task data to online stochastic bandit problems

## Executive Summary
This paper addresses the challenge of transferring hyperparameters from offline data to online stochastic bandit problems through a multi-task learning framework. The authors propose a method that leverages derandomization to analyze piecewise constant structure in the loss function, enabling data-driven hyperparameter selection with theoretical guarantees. The approach provides both inter-task and intra-task sample complexity bounds, showing that O(n log T) logarithmic dependence on time horizon is achievable for UCB hyperparameter tuning.

## Method Summary
The method uses derandomization to transform the expected loss into a piecewise constant function of the hyperparameter, then applies Rademacher complexity bounds to derive sample complexity guarantees. Algorithm 3 recursively computes critical points where the argmax arm selection changes, making empirical risk minimization tractable over continuous parameter spaces. The approach assumes i.i.d. arm rewards and requires offline data collection with sufficient exploration to estimate the loss function at each piece.

## Key Results
- Log QD = O(n log T) for UCB hyperparameter tuning with arbitrary distributions
- Inter-task sample complexity of Õ(n log T/ε²) and intra-task complexity of O(nT)
- Sharpened bounds of O(log KT) for Bernoulli/categorical rewards
- Strong empirical performance on CIFAR-10/100 for neural network hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Derandomization Exposes Piecewise Constant Structure
- Claim: Fixing random seeds for arm rewards transforms the expected loss from a complex function into a piecewise constant function of the hyperparameter.
- Mechanism: For UCB-style algorithms, the derandomized dual function l_{P,z}(α) has finitely many discontinuities where the argmax arm selection changes. Each "piece" corresponds to a fixed sequence of arm pulls. Massart's lemma bounds Rademacher complexity by O(H√(log m / N)) where m is the number of pieces.
- Core assumption: Arm rewards are drawn i.i.d. from fixed distributions; without this, the number of discontinuities can be 2^Ω(T) even for 2-arm bandits.
- Evidence anchors:
  - [abstract] "The core method uses derandomization to analyze piecewise constant structure in the loss function"
  - [Section 5.1, Definition 1] Formal definition of derandomized dual complexity Q_D = E_{P~D}E_{z~D_P}[q(l_{P,z,T}(·))]
  - [corpus] Weak direct support; neighbor papers focus on RL transfer, not this specific derandomization technique.
- Break condition: Non-stochastic rewards or adversarial corruption destroy the piecewise structure guarantees.

### Mechanism 2: Rademacher Complexity Bounds via Discontinuity Count
- Claim: The sample complexity scales with log(Q_D), not the size of the hyperparameter space.
- Mechanism: Since the loss is constant on each piece, the function class has bounded Rademacher complexity even for continuous parameter spaces. Theorem 6.1 shows N = O((H/ε)²(log Q_D + log 1/δ)) samples suffice for uniform convergence.
- Core assumption: The number of pieces Q_D is polynomial in T for fixed n (Theorem 7.1 shows log Q_D = O(n log T) for UCB).
- Evidence anchors:
  - [Section 6, Theorem 6.1] Full proof uses Massart's lemma and Jensen's inequality to bound R(F, D).
  - [Section 7.1, Theorem 7.1] "log Q_D = O(n log T)" for arbitrary distributions over n-arm MAB.
  - [corpus] No direct corpus validation of this specific complexity analysis.
- Break condition: If Q_D grows exponentially (possible with non-i.i.d. rewards), sample complexity bounds become vacuous.

### Mechanism 3: Critical Point Computation Makes ERM Tractable
- Claim: Empirical Risk Minimization over continuous α is achieved by evaluating loss only at critical points.
- Mechanism: Algorithm 3 recursively finds discontinuity points by detecting when a different arm becomes optimal. Expected runtime is O(Q_D) since it visits each piece once.
- Core assumption: Offline data includes full reward vectors (or sufficient exploration) to compute loss at each piece.
- Evidence anchors:
  - [Section 7, Algorithm 3] α-CRITICAL POINTS procedure with recursive subdivision.
  - [Section 6, Theorem 6.2] Shows E[T_o] = min{n, Q_D}·T offline pulls suffice.
  - [corpus] Weak; related work on hyperparameter transfer [1231] uses different mechanisms (μP parameterization).
- Break condition: Bandit feedback only (no counterfactual rewards) requires longer offline horizons or importance weighting.

## Foundational Learning

- **Rademacher Complexity and Uniform Convergence**
  - Why needed here: The entire sample complexity argument hinges on bounding Rademacher complexity of the loss function class.
  - Quick check question: Can you explain why Rademacher complexity of O(log m / N) implies O(1/√N) sample complexity?

- **Multi-Armed Bandits and Regret Decomposition**
  - Why needed here: Understanding how UCB's exploration parameter affects arm selection is essential to see why critical points exist.
  - Quick check question: For 2-armed UCB, derive the condition on α where the selected arm switches at time t.

- **Data-Driven Algorithm Configuration Paradigm**
  - Why needed here: This work extends the framework from [Bal20, BNVW17] to stochastic bandits with inter/intra-task distinctions.
  - Quick check question: How does this setting differ from online algorithm configuration where you learn during deployment?

## Architecture Onboarding

- **Component map**: Offline Phase -> Tuning Phase -> Online Phase

- **Critical path**:
  1. Estimate Q_D empirically from pilot runs (Table 1 shows Q_D ≈ 30 for Bernoulli with T=100, far below theoretical bound)
  2. Collect offline data with T_o ≥ T per task (or use sequential policy if n > Q_D)
  3. Run Algorithm 3 to enumerate critical points
  4. Return argmin over finite set {critical points} ∪ {boundaries}

- **Design tradeoffs**:
  - More offline tasks (larger N) → better generalization but higher data cost
  - Longer offline horizons (larger T_o) → more accurate loss estimation but slower offline phase
  - Assumption: Tasks are i.i.d. from D; distribution shift invalidates guarantees

- **Failure signatures**:
  - Learned α performs worse than theory-default (α=1): Check if task distribution has shifted or offline data insufficient
  - Runtime blowup in Algorithm 3: Q_D may be underestimated; add early stopping
  - Regret doesn't improve with more N: Check if hyperparameter truly matters for this task family

- **First 3 experiments**:
  1. **Synthetic 2-arm Bernoulli with small gap**: Replicate Figure 2 setting (p₁=0.5, p₂~0.51) with N=200, T_o=20; verify learned α > 1 (high exploration needed)
  2. **Ablation on N**: Plot test regret vs. number of offline tasks (as in Figure 5); confirm 1/√N decay
  3. **Estimate Q_D on your domain**: Run Algorithm 3 on 1000 random seeds; compare empirical Q_D to theoretical O(nT) bound to validate tractability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can one strategically design offline data collection policies to minimize intra-task sample complexity?
- Basis in paper: [explicit] The authors state: "An important question that our work doesn't yet address is: how to strategically collect offline data to minimize the intra-task sample complexity?"
- Why unresolved: The paper uses simple policies (pull all arms T times when n ≤ QD, or run Aρ sequentially when QD < n) but defers optimizing this to future work.
- What evidence would resolve it: An algorithm with provably optimal or improved intra-task complexity, along with lower bounds showing optimality.

### Open Question 2
- Question: Can the theoretical intra-task complexity bounds (O(nT) for general distributions, O(QD·T) otherwise) be tightened?
- Basis in paper: [explicit] "We believe the intra-task sample complexity bounds provided in our work can be improved with more careful arguments."
- Why unresolved: Current bounds may be loose; the paper's experiments show empirically that QD is typically much smaller than worst-case theoretical bounds.
- What evidence would resolve it: Sharpened bounds matching empirical observations, or instance-dependent bounds that adapt to problem structure.

### Open Question 3
- Question: How do the transfer learning guarantees degrade under distribution shift between offline and online tasks?
- Basis in paper: [inferred] The paper assumes tasks are drawn i.i.d. from a fixed distribution D, but practical applications often involve non-stationarity or distribution shift.
- Why unresolved: The theoretical analysis relies critically on the identical distribution assumption; robustness to shift is unexplored.
- What evidence would resolve it: Bounds characterizing performance degradation as a function of divergence between training and test distributions (e.g., Wasserstein or KL distance).

## Limitations

- The theoretical bounds rely on piecewise constant structure which may not hold for non-i.i.d. reward distributions, potentially leading to exponential growth in critical points
- The empirical evaluation focuses on relatively simple bandit structures and does not extensively test performance degradation under distribution shift
- The critical point computation algorithm assumes full counterfactual information, which may be impractical in pure bandit feedback settings

## Confidence

- **High confidence**: The derandomization mechanism and piecewise constant structure analysis (Mechanism 1) - well-defined theoretical framework with clear conditions
- **Medium confidence**: The Rademacher complexity bounds and sample complexity guarantees (Mechanism 2) - solid theoretical derivation but empirical validation of Q_D bounds is limited
- **Medium confidence**: The practical critical point computation and its runtime guarantees (Mechanism 3) - algorithm is specified but sensitivity to numerical precision not fully explored

## Next Checks

1. **Test Q_D scaling empirically**: For a 3-armed Bernoulli bandit with varying gaps, measure the actual number of critical points across multiple random seeds and compare to the O(n log T) theoretical bound.

2. **Evaluate distribution shift robustness**: Take the CIFAR-10 dataset and artificially shift the task distribution between offline and online phases. Measure how quickly the learned hyperparameter degrades.

3. **Validate importance weighting**: Implement the importance-weighted estimator for the case where offline rewards are only available for the arm that would have been played. Compare its performance to the full information assumption.