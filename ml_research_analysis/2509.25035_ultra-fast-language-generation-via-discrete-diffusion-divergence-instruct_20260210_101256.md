---
ver: rpa2
title: Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct
arxiv_id: '2509.25035'
source_url: https://arxiv.org/abs/2509.25035
tags:
- didi-instruct
- diffusion
- nfes
- student
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DiDi-Instruct, a training-based method for
  ultra-fast language generation that distills a pre-trained discrete diffusion language
  model (dLLM) into a few-step student model. The core method is based on integral
  KL-divergence minimization, enabling the student to match the teacher's generation
  ability with significantly improved efficiency.
---

# Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct

## Quick Facts
- arXiv ID: 2509.25035
- Source URL: https://arxiv.org/abs/2509.25035
- Authors: Haoyang Zheng, Xinyang Liu, Cindy Xiangrui Kong, Nan Jiang, Zheyuan Hu, Weijian Luo, Wei Deng, Guang Lin
- Reference count: 40
- Primary result: DiDi-Instruct achieves new SOTA on OpenWebText with over 20× faster distillation and consistently lower perplexity across 8-128 function evaluations compared to prior accelerated dLLMs and GPT-2 baseline

## Executive Summary
This paper introduces DiDi-Instruct, a training-based method for ultra-fast language generation that distills pre-trained discrete diffusion language models into few-step student models. The core innovation is integral KL-divergence minimization, which enables the student to match the teacher's generation ability while achieving significantly improved efficiency. The method demonstrates state-of-the-art performance on the OpenWebText benchmark with consistently lower perplexity compared to both accelerated dLLMs and traditional GPT-2 baselines across a wide range of function evaluations.

## Method Summary
DiDi-Instruct operates by training a student model to approximate the behavior of a pre-trained discrete diffusion language model (dLLM) teacher through integral KL-divergence minimization. This approach distills the teacher's generation capabilities into a more efficient student architecture that requires significantly fewer inference steps while maintaining comparable output quality. The training process focuses on minimizing the divergence between teacher and student distributions across the entire generation trajectory, rather than just at the final output.

## Key Results
- Achieves new state-of-the-art performance on OpenWebText benchmark
- Consistently lower perplexity than prior accelerated dLLMs and GPT-2 baseline across 8-128 function evaluations
- Demonstrates over 20× faster distillation compared to baseline methods
- Maintains negligible entropy loss while achieving speed improvements

## Why This Works (Mechanism)
The method leverages integral KL-divergence minimization to capture the full generation trajectory of the teacher model rather than just final outputs. This comprehensive distillation approach allows the student model to learn the nuanced probabilistic transitions that occur during the diffusion process, enabling it to generate high-quality text with fewer inference steps while maintaining the statistical properties of the teacher model.

## Foundational Learning
- **Discrete diffusion language models**: Why needed - form the teacher architecture being distilled; Quick check - verify understanding of forward/reverse diffusion processes in discrete token spaces
- **KL-divergence minimization**: Why needed - core optimization objective for matching teacher/student distributions; Quick check - confirm understanding of integral form vs point-wise divergence
- **Knowledge distillation**: Why needed - framework for transferring teacher capabilities to efficient student; Quick check - validate understanding of temperature scaling and soft target usage
- **Perplexity metrics**: Why needed - primary evaluation metric for language model quality; Quick check - ensure ability to compute and interpret perplexity scores
- **Function evaluations**: Why needed - measures computational efficiency in diffusion models; Quick check - understand relationship between inference steps and generation quality
- **Pre-training paradigms**: Why needed - context for teacher model capabilities; Quick check - verify understanding of self-supervised training objectives for language models

## Architecture Onboarding
- **Component map**: Pre-trained dLLM teacher -> Integral KL-divergence loss -> Student model training -> Fast inference generation
- **Critical path**: Teacher forward pass → KL-divergence calculation → Student backward pass → Parameter update
- **Design tradeoffs**: Fewer inference steps (speed) vs. entropy preservation (quality) - the method optimizes both simultaneously
- **Failure signatures**: Degraded perplexity indicates insufficient distillation; high entropy loss suggests over-aggressive acceleration
- **First experiment**: 1) Train student on OpenWebText with varying function evaluation budgets; 2) Measure perplexity vs. teacher model; 3) Analyze entropy preservation across different distillation temperatures

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on pre-trained teacher models raises questions about generalization to different architectures or domains
- 20× speedup measurement lacks ablation studies isolating individual training component contributions
- Evaluation focuses primarily on perplexity metrics without qualitative analysis of generation quality or diversity

## Confidence
High confidence in: Core training methodology (integral KL-divergence minimization), reported perplexity improvements on OpenWebText, basic implementation feasibility
Medium confidence in: 20× speedup measurement methodology, generality of improvements across different dLLM architectures, practical significance of entropy loss trade-off
Low confidence in: Long-range generation quality assessments, behavior on out-of-distribution prompts, real-world application performance beyond perplexity metrics

## Next Checks
1. **Ablation study on training components**: Systematically remove or modify individual elements of the DiDi-Instruct training pipeline (e.g., different divergence measures, alternative distillation objectives) to isolate which components contribute most to performance gains.

2. **Cross-architecture generalization test**: Apply the same distillation method to dLLMs with different backbone architectures (e.g., different transformer sizes or attention mechanisms) and evaluate whether similar speed-accuracy trade-offs are maintained.

3. **Human evaluation of generation quality**: Conduct controlled human studies comparing outputs from DiDi-Instruct models against the teacher model and other accelerated baselines, focusing on coherence, diversity, and task-specific quality metrics beyond perplexity.