---
ver: rpa2
title: 'Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations'
arxiv_id: '2509.18408'
source_url: https://arxiv.org/abs/2509.18408
tags:
- sequence
- uni00000013
- uni00000011
- geometric
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reversible Chaos Game Representation (R-CGR),
  a novel method that addresses the fundamental information loss problem in traditional
  Chaos Game Representation by enabling perfect sequence reconstruction through explicit
  path encoding and rational arithmetic precision control. The method generates geometric
  representations of biological sequences while maintaining complete sequence information,
  making it both interpretable and suitable for deep learning applications.
---

# Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations

## Quick Facts
- arXiv ID: 2509.18408
- Source URL: https://arxiv.org/abs/2509.18408
- Reference count: 24
- Primary result: R-CGR with VGG16 achieves 79.07% classification accuracy on synthetic biological sequences

## Executive Summary
This paper introduces Reversible Chaos Game Representation (R-CGR), a novel method that addresses the fundamental information loss problem in traditional Chaos Game Representation by enabling perfect sequence reconstruction through explicit path encoding and rational arithmetic precision control. The method generates geometric representations of biological sequences while maintaining complete sequence information, making it both interpretable and suitable for deep learning applications. Experimental results on synthetic biological datasets (DNA and protein sequences) show that R-CGR with VGG16 achieves 79.07% classification accuracy, outperforming established baselines including ProtT5 (73.48%) and ESM2 (74.51%).

## Method Summary
R-CGR encodes biological sequences by iteratively computing midpoints between current positions and alphabet-specific corner points, storing each step's character, position, and corner point in a path trace. Unlike traditional CGR which stores only the final point, R-CGR maintains complete reversibility through this explicit path storage. The method uses exact rational arithmetic with bounded denominators to prevent precision loss during midpoint calculations, applying continued fraction approximation when denominators exceed a specified precision bound. Path traces are rasterized to 224×224 images with gradient coloring based on sequence position, then classified using a pretrained VGG16 CNN architecture.

## Key Results
- R-CGR achieves 79.07% classification accuracy on 7-class synthetic biological sequence classification
- Outperforms ProtT5 (73.48%) and ESM2 (74.51%) baselines
- Perfect reversibility demonstrated: sequences can be exactly reconstructed from their geometric representations
- VGG16 architecture outperforms ResNet50 (45.86%) and EfficientNet-B0 (14.29%) on R-CGR images

## Why This Works (Mechanism)

### Mechanism 1: Explicit Path Trace Storage Enables Perfect Reversibility
Storing the complete encoding path allows lossless sequence reconstruction from geometric representation. During encoding, each iteration records a 4-tuple (character sⱼ, previous position pⱼ₋₁, current position pⱼ, corner point Cₖ) in path trace T. Reconstruction simply concatenates characters from T in order—no inverse computation required. Storage overhead of O(n(log|Σ| + log P)) bits per sequence is acceptable for downstream tasks.

### Mechanism 2: Rational Arithmetic Preserves Exact Coordinates Under Iterated Midpoint Operations
Using exact rational arithmetic instead of floating-point prevents cumulative precision loss during repeated midpoint calculations. Corner points are defined with bounded-denominator rationals (Equation 3: Cᵢ = (⌊q·cos(2πi/k)⌉/q, ⌊q·sin(2πi/k)⌉/q) with q = 2^⌈log₂(4k)⌉). Midpoint operations use fraction addition with GCD reduction (Equation 4, Algorithm 3). If denominator exceeds precision bound P, continued fraction approximation is applied.

### Mechanism 3: CNN Architecture Choice Determines Whether Spatial CGR Patterns Are Exploitable
VGG16's sequential convolutional hierarchy captures CGR spatial patterns more effectively than ResNet50's residual connections for this task. R-CGR paths are rasterized to 224×224 images with gradient coloring based on sequence position. VGG16's uniform convolutional stack extracts hierarchical spatial features; ResNet50's skip connections may bypass or dilute fine-grained CGR texture signals.

## Foundational Learning

- Concept: **Chaos Game Representation (CGR) Basics**
  - Why needed here: R-CGR extends traditional CGR; understanding the original midpoint-to-corner iteration is prerequisite.
  - Quick check question: Given alphabet {A,T,G,C} with corners at (0,0), (0,1), (1,1), (1,0), what is the second point for sequence "AT" starting from (0.5, 0.5)?

- Concept: **Rational Arithmetic and Continued Fractions**
  - Why needed here: Precision control via rational operations is central to R-CGR's reversibility guarantee.
  - Quick check question: Compute (1/3 + 1/6)/2 as a reduced fraction; what is the denominator?

- Concept: **Image Rasterization for CNNs**
  - Why needed here: R-CGR paths must be converted to fixed-size images for VGG16/ResNet50 input.
  - Quick check question: If a path point has coordinates (0.3, 0.7) in unit square, what pixel (row, col) does it map to in a 224×224 image assuming (0,0) at top-left?

## Architecture Onboarding

- Component map: Sequence input → corner point assignment (Equation 3) → rational midpoint iteration (Equation 4) → path trace storage → image rasterization → VGG16 forward pass → class prediction
- Critical path: Sequence input → corner point assignment → rational midpoint iteration → path trace storage → image rasterization → VGG16 forward pass → class prediction
- Design tradeoffs:
  - Storage: Path trace adds O(n log P) overhead vs. traditional CGR's O(1) final point
  - Precision vs. speed: Higher precision bound P ensures uniqueness but increases rational arithmetic cost
  - Architecture: VGG16 outperforms ResNet50 on this task (79.07% vs. 45.86%)
- Failure signatures:
  - ResNet50 underperforms dramatically (45.86%): Likely residual connections skip CGR-relevant early features
  - EfficientNet-B0 at 14.29% (random for 7 classes): Model too small / insufficient training signal for CGR patterns
  - Reversibility fails if path trace T is lost or if rational precision is insufficient (coordinate collisions)
- First 3 experiments:
  1. Encode 100 random DNA/protein sequences, apply Algorithm 2 for reconstruction, verify 100% character-level match
  2. Vary precision bound P (e.g., 2^8, 2^16, 2^32) and measure reconstruction accuracy + classification performance
  3. Train VGG16, ResNet50, and a shallow 3-conv-layer custom CNN on same R-CGR images; compare accuracy and training time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does R-CGR perform on real biological datasets compared to synthetic benchmarks?
- Basis in paper: "Future work will explore applications to real biological datasets"
- Why unresolved: All experiments used synthetic sequences with controlled compositional biases; real biological data contains complex patterns, noise, and structural dependencies not captured in synthetic settings
- What evidence would resolve it: Evaluation on standard benchmarks (e.g., Pfam, UniRef) with comparisons to baseline methods under identical conditions

### Open Question 2
- Question: Can integrating R-CGR representations into LLM architectures improve downstream task performance?
- Basis in paper: "integrating the CGR within LLM could also be an interesting future extension"
- Why unresolved: The geometric encoding operates independently from transformer-based models; no fusion architecture has been proposed or tested
- What evidence would resolve it: A hybrid model combining R-CGR embeddings with transformer attention, evaluated on protein function prediction or sequence-to-sequence tasks

### Open Question 3
- Question: What theoretical guarantees govern the precision-storage tradeoff, and how does precision bound P affect reconstruction fidelity and classification accuracy?
- Basis in paper: From Algorithm 3 and Eq. 6, which introduce precision control but lack empirical sensitivity analysis
- Why unresolved: The paper states storage scales with log P but provides no experiments varying P to assess impact on accuracy or storage efficiency
- What evidence would resolve it: Systematic experiments varying P across multiple orders of magnitude, measuring reconstruction error, storage requirements, and classification performance

## Limitations
- Lack of critical implementation details including image rasterization parameters (line width, colormap, normalization)
- Unspecified training hyperparameters (batch size, learning rate, optimizer, epochs, early stopping, data augmentation, weight decay)
- Missing precision bound P used in experiments and continued fraction approximation routine details

## Confidence
- **High confidence**: The reversibility mechanism (explicit path storage + rational arithmetic) is mathematically rigorous and well-defined through Algorithms 1-3
- **Medium confidence**: The 79.07% accuracy claim for VGG16, as it depends on unspecified implementation choices that could substantially affect results
- **Low confidence**: Comparative performance claims against ProtT5 (73.48%) and ESM2 (74.51%) due to unclear experimental setup and potential differences in evaluation methodology

## Next Checks
1. Encode 100 random DNA/protein sequences through R-CGR, apply Algorithm 2 reconstruction, and verify perfect character-level sequence recovery
2. Systematically vary precision bound P (2^8, 2^16, 2^32) and measure both reconstruction accuracy and classification performance to identify the minimal viable precision
3. Train VGG16, ResNet50, and a simple 3-layer CNN on identical R-CGR images with controlled hyperparameters to validate the claimed 79.07% vs 45.86% performance gap