---
ver: rpa2
title: Robust Watermarking on Gradient Boosting Decision Trees
arxiv_id: '2511.09822'
source_url: https://arxiv.org/abs/2511.09822
tags:
- watermark
- conf
- dataset
- watermarking
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first robust watermarking framework for\
  \ Gradient Boosting Decision Trees (GBDTs), addressing their sequential and non-differentiable\
  \ structure. Four novel embedding strategies\u2014Wrong Prediction Flip, Outlier\
  \ Flip, Cluster Center Flip, and Confidence Flip\u2014are proposed, leveraging in-place\
  \ fine-tuning to embed resilient watermarks with minimal accuracy impact."
---

# Robust Watermarking on Gradient Boosting Decision Trees

## Quick Facts
- arXiv ID: 2511.09822
- Source URL: https://arxiv.org/abs/2511.09822
- Authors: Jun Woo Chung; Yingjie Lao; Weijie Zhao
- Reference count: 22
- Primary result: Introduces first robust watermarking framework for GBDTs using in-place fine-tuning with four novel embedding strategies.

## Executive Summary
This work introduces the first robust watermarking framework for Gradient Boosting Decision Trees (GBDTs), addressing their sequential and non-differentiable structure. Four novel embedding strategies—Wrong Prediction Flip, Outlier Flip, Cluster Center Flip, and Confidence Flip—are proposed, leveraging in-place fine-tuning to embed resilient watermarks with minimal accuracy impact. Experiments across diverse datasets demonstrate high watermark embedding success (up to 100%), strong resistance to post-deployment fine-tuning, and competitive adjusted model accuracy.

## Method Summary
The framework trains an initial GBDT model on the training set, then selects watermark candidates using one of four strategies (Wrong Prediction Flip, Outlier Flip, Cluster Center Flip, or Confidence Flip) from a candidate dataset. Watermark samples are chosen based on Lowest Confidence or Maximum Distance criteria, with their labels modified to embed the watermark. An in-place fine-tuning algorithm updates the tree structure using the watermark dataset, ensuring deep integration and robustness. The method is evaluated on six UCI datasets with various watermark ratios and candidate selection scenarios.

## Key Results
- Four novel embedding strategies achieve watermark effectiveness up to 100% across diverse datasets
- In-place fine-tuning provides superior robustness to post-deployment fine-tuning compared to additive methods
- Cluster Center Flip and Confidence Flip strategies maintain high adjusted model accuracy while embedding watermarks
- Framework demonstrates effectiveness for both training data reuse and separate watermark datasets

## Why This Works (Mechanism)

### Mechanism 1: In-Place Fine-Tuning for Structural Integration
Modifying internal tree structure during fine-tuning integrates watermarks more deeply than tree addition, improving robustness to pruning-based removal. The in-place update algorithm recomputes split gains using pseudo-residuals from the watermark dataset, retraining subtrees when optimal splits change.

### Mechanism 2: Strategic Sample Selection for Minimal Accuracy Impact
Selecting watermark candidates from regions near decision boundaries, in sparse feature space, or already misclassified minimizes accuracy degradation while maintaining embedding effectiveness. Each strategy targets samples where label modifications have limited collateral impact.

### Mechanism 3: Anchoring Watermarks with Neighboring Samples (Cluster Center Flip)
Including correctly labeled neighbors alongside flipped centroid confines decision boundary shift to localized region. Neighbors act as anchors to prevent broader boundary drift during fine-tuning.

## Foundational Learning

- **Gradient Boosting Sequential Dependency**: GBDT builds trees sequentially, where each tree fits pseudo-residuals of previous ensemble. This dependency makes direct modification risky as changes can cascade and degrade accuracy.
  - Quick check: If you modify a leaf value in tree 10 of a 100-tree GBDT model, which subsequent trees might be affected and why?

- **Softmax Probability and Prediction Confidence**: Confidence Flip and Wrong Prediction Flip strategies rely on prediction probabilities (via softmax) to identify low-confidence or misclassified samples for watermarking.
  - Quick check: Given a 3-class model output F(x) = [0.5, 1.2, 0.3], what is predicted class and its softmax probability?

- **In-Place vs. Additive Fine-Tuning**: Framework explicitly uses in-place updates to improve robustness. Understanding distinction is critical for correct implementation.
  - Quick check: In XGBoost, what is default behavior when calling `.train()` on existing model with new data, and how does this differ from in-place updating?

## Architecture Onboarding

- **Component map**: GBDT Model (Pre-trained) -> Candidate Selection Module -> Watermark Selection Module -> Label Modification Module -> In-Place Fine-Tuning Engine -> Evaluation Metrics

- **Critical path**: 1) Train initial GBDT model on Dtrain. 2) Generate predictions on Dcand. 3) Apply candidate selection strategy to form C. 4) Apply watermark selection criterion to form W. 5) Modify labels for W to create W'. 6) Run in-place fine-tuning using W'. 7) Evaluate effectiveness, accuracy, and robustness.

- **Design tradeoffs**: Dcand = Dtrain avoids new data collection but requires duplication factor (dcand=train=5) to overcome gradient conflicts. Separate data is cleaner but requires holdout set. Higher watermark ratios increase success but risk accuracy loss.

- **Failure signatures**: Low Awm (<0.7) may indicate insufficient duplication factor or labels conflicting with learned boundaries. Accuracy drop (>5-10%) may indicate watermarks not in "safe" regions. Low robustness after fine-tuning may indicate watermarks in easily overwritten regions.

- **First 3 experiments**: 1) Baseline Effectiveness Test: Implement Confidence Flip with Dcand = Dtrain and |W|/|Dtrain|=0.01. 2) Comparative Strategy Test: Compare all four strategies with Dcand ≠ Dtrain, |W|/|Dtrain|=0.01. 3) Robustness Stress Test: Apply aggressive post-deployment fine-tuning and measure change in Awm.

## Open Questions the Paper Calls Out

### Open Question 1
How robust are the proposed watermarking methods against targeted adversarial attacks such as model pruning, distillation, or model extraction, rather than just benign fine-tuning? The paper evaluates robustness against post-deployment fine-tuning but not against malicious attacks designed specifically to remove watermarks.

### Open Question 2
Can the in-place fine-tuning algorithm be modified to guarantee gradient alignment with target watermark class? The paper acknowledges that for arbitrary r, gradient for watermark class k cannot be strictly guaranteed to be negative, relying instead on heuristic duplication factor.

### Open Question 3
How can Wrong Prediction Flip strategy be adapted for high-accuracy models where supply of initially misclassified training samples is scarce or non-existent? This constraint severely limits capacity of this specific embedding strategy in well-regularized or high-performing models.

## Limitations
- Limited empirical evidence comparing robustness against tree deletion or model distillation attacks
- Framework effectiveness highly dependent on availability of suitable samples in "safe" regions
- Evaluation limited to small-to-medium UCI datasets with fixed tree structure

## Confidence

- **High Confidence**: Technical feasibility of in-place fine-tuning for GBDT models, mathematical formulation of watermarking objective
- **Medium Confidence**: Robustness claims against post-deployment fine-tuning, comparative advantage over additive fine-tuning
- **Low Confidence**: Generalizability across diverse real-world datasets and model scales

## Next Checks

1. **Robustness Against Alternative Attacks**: Implement and test watermark resilience against tree deletion and model distillation attacks to validate claimed superiority of in-place updates.

2. **Scalability and Performance on Large Models**: Apply framework to larger, industrial-scale GBDT models to assess performance and computational overhead.

3. **Ablation Study on Candidate Selection**: Systematically vary duplication factor and neighbor count in Cluster Center Flip to quantify impact on watermark effectiveness and accuracy.