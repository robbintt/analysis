---
ver: rpa2
title: Analysis on distribution and clustering of weight
arxiv_id: '2509.19122'
source_url: https://arxiv.org/abs/2509.19122
tags:
- clustering
- vector
- weights
- different
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new ways to characterize and compare
  large language models based on their weight distributions. First, it constructs
  a standard-deviation vector from the normalized standard deviations of key weight
  matrices (Query, Key, Value, etc.), revealing distinct patterns between different
  model families and similarities within the same family.
---

# Analysis on distribution and clustering of weight

## Quick Facts
- arXiv ID: 2509.19122
- Source URL: https://arxiv.org/abs/2509.19122
- Authors: Chunming Ye; Wenquan Tian; Yalan Gao; Songzhou Li
- Reference count: 4
- Primary result: Introduces standard-deviation and clustering vectors as discriminative fingerprints for LLM model families and LoRA fine-tuning analysis

## Executive Summary
This paper introduces two new ways to characterize and compare large language models based on their weight distributions. First, it constructs a standard-deviation vector from the normalized standard deviations of key weight matrices (Query, Key, Value, etc.), revealing distinct patterns between different model families and similarities within the same family. Second, it clusters singular values from these matrices using K-Means to form a clustering vector, which similarly distinguishes models by family. The study also examines how these vectors change after LoRA fine-tuning, finding that the standard-deviation vector is strongly influenced by the fine-tuning dataset while the clustering vector remains stable, preserving the original model's internal correlations.

## Method Summary
The method extracts seven types of projection matrices (Query, Key, Value, Output, Gate, Up, Down) from LLM layers, concatenates them by type, and computes normalized standard deviations to form a 7D standard-deviation vector. For clustering vectors, it performs SVD on each concatenated matrix, extracts top-r singular values, normalizes them, applies K-Means clustering (n=2), and averages cluster labels by projection type to form another 7D vector. LoRA fine-tuning experiments use batch_size=2, grad_accum=2, epochs=1, lr=1e-4, dropout=0.2, warmup_ratio=0.1.

## Key Results
- Standard-deviation vectors effectively distinguish model families (LLaMA vs. SmolLM2 vs. Qwen2.5) while showing similarity within families
- Clustering vectors reveal that Q/K matrices consistently cluster together separately from V/O/Up/Down matrices across all models
- LoRA fine-tuning causes standard-deviation vectors to converge across different base models when trained on the same dataset
- Clustering vectors remain stable during LoRA fine-tuning, preserving architectural correlation structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard-deviation vectors derived from normalized weight distribution spreads serve as discriminative fingerprints for model families
- Mechanism: Different projection matrices exhibit distinct variance patterns that are consistent within a model family but diverge across architectures
- Core assumption: Weights in projection matrices approximately follow Normal distributions with μ≈0
- Evidence anchors: Abstract confirms Vstd values differ significantly between distinct model families; section 2.2 shows models within same family exhibit marked similarity in Vstd values

### Mechanism 2
- Claim: Clustering vectors derived from singular-value groupings encode structural correlations that persist across fine-tuning
- Mechanism: SVD extracts singular values from each projection matrix; K-Means clustering reveals that matrices of the same type share cluster membership patterns
- Core assumption: Inter-weight correlation structure is determined by architectural role rather than training data
- Evidence anchors: Abstract states correlations remain unaffected during LoRA; section 3.1 shows Q/K consistently cluster together vs V/O/Up/Down

### Mechanism 3
- Claim: LoRA fine-tuning perturbs weight distribution spread (dataset-driven) while preserving inter-weight correlation structure (architecture-driven)
- Mechanism: LoRA adds low-rank updates to frozen pre-trained weights; the standard-deviation vector of these updates reflects dataset statistics
- Core assumption: LoRA rank is sufficiently low that it modifies distributional spread without restructuring dominant singular-value modes
- Evidence anchors: Abstract notes standard deviation vector is directly influenced by dataset; section 4.1 shows different pretrained models yield similar weight distributions when fine-tuned on same dataset via LoRA

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD)**
  - Why needed here: The clustering vector construction depends on extracting top-r singular values from each projection matrix as structural features
  - Quick check question: Given a 4096×4096 weight matrix, what does the first singular value represent versus the 64th?

- Concept: **K-Means Clustering Convergence**
  - Why needed here: Cluster assignments for singular-value vectors must be stable across runs; randomized centroid initialization requires multiple repetitions
  - Quick check question: Why does K-Means with n_cluster=2 versus n_cluster=3 produce different heatmaps for the same weight set?

- Concept: **Normal Distribution Parameterization**
  - Why needed here: The standard-deviation vector assumes μ≈0 and uses σ as the sole shape descriptor for weight distributions
  - Quick check question: If weights were bimodal rather than Normal, would comparing σ values alone remain meaningful?

## Architecture Onboarding

- Component map: Input -> Load weights -> Extract projection matrices -> Compute std (Vstd) -> SVD + K-Means (Clustering Vector) -> Visualization

- Critical path:
  1. Load model weights and identify projection matrix names (varies by architecture: LLaMA vs. Qwen naming conventions)
  2. Concatenate same-type matrices across all layers
  3. Compute σ for each concatenated matrix (standard-deviation vector)
  4. Perform SVD, extract top-r singular values per layer-matrix
  5. Run K-Means with multiple random seeds, select stable partition
  6. Aggregate cluster labels by projection type (clustering vector)

- Design tradeoffs:
  - **Rank selection (r=16 vs. r=64)**: Lower rank is faster and more stable; higher rank captures more spectral detail but may introduce noise
  - **Cluster count (k=2 vs. k=3)**: k=2 yields cleaner separation between attention and MLP matrices; k=3 introduces ambiguity
  - **Normalization strategy**: Standard deviations are normalized within-model; singular-value vectors are normalized before clustering

- Failure signatures:
  - **Flat standard-deviation vector**: Indicates all projection matrices have similar variance → may occur in severely quantized or pruned models
  - **Inconsistent clustering across seeds**: Suggests singular-value vectors lack clear structure → check rank truncation or data loading
  - **Clustering vector shifts after LoRA**: Indicates high-rank updates or overtraining → reduce LoRA rank or training steps

- First 3 experiments:
  1. **Reproduce standard-deviation vectors**: Load LLaMA-3.2-1B, extract 7 projection types, compute normalized σ vector, compare against paper's Figure 3a curve shape
  2. **Validate clustering stability**: Run K-Means 10 times with different seeds on LLaMA-3.2-1B singular values (rank=16), verify Q/K consistently cluster together across runs
  3. **LoRA dataset ablation**: Fine-tune two different models (e.g., LLaMA-3.2-1B and SmolLM2-1.7B) on the same dataset, confirm standard-deviation vectors converge while clustering vectors remain distinct

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific profiles of the standard-deviation or clustering vectors correlate with measurable improvements in model performance, such as lower perplexity or higher accuracy?
- Basis in paper: [inferred] The introduction states that uncovering weight signatures would "provide more insights for model optimization," but the paper only validates these vectors for model identification and family grouping, not for predicting capability
- Why unresolved: The experiments focus on distinguishing model families and tracking changes during LoRA fine-tuning, without establishing a quantitative link between the vector shapes and the model's functional benchmarks
- What evidence would resolve it: A statistical analysis correlating the specific values of these vectors against downstream task performance (e.g., MMLU scores) across a wide range of models

### Open Question 2
- Question: Does the stability of the clustering vector persist during full parameter fine-tuning, or is this stability a unique property of the low-rank structure inherent to LoRA updates?
- Basis in paper: [inferred] The study finds that the clustering vector "remains frozen" during LoRA fine-tuning, implying it is tied to the pre-trained model's architecture
- Why unresolved: LoRA updates only low-rank adapters, potentially leaving the singular value structure of the main weights untouched. Full fine-tuning alters the entire matrix, which might disrupt the "inter-weight correlational structure"
- What evidence would resolve it: Repeating the fine-tuning experiments using full parameter tuning on the same datasets and comparing the resulting clustering vectors to the pre-trained baselines

### Open Question 3
- Question: How can the standard-deviation and clustering vectors be actively utilized as constraints or targets to improve model training or merging efficiency?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work will continue to analyze the role of these characteristics in model improvement"
- Why unresolved: The current study treats these vectors as passive descriptors (fingerprints) for analysis after training is complete, rather than as active variables to be manipulated for better convergence or compatibility
- What evidence would resolve it: Experiments that regularize the loss function to maintain or achieve specific vector profiles during training, or using vector similarity to determine optimal strategies for model merging

## Limitations
- Weight Distribution Assumptions: The methodology assumes projection weights follow Normal distributions with near-zero mean, which may not hold for quantized or specialized architectures
- K-Means Stability: Clustering vector depends on K-Means partitioning that can be sensitive to initialization and cluster count selection, lacking extensive robustness testing
- LoRA Generalization: Dataset-driven standard-deviation convergence is demonstrated on only two datasets, with unproven generalization across diverse datasets and LoRA configurations

## Confidence
- High Confidence: Core observation that projection matrices exhibit distinct variance patterns is well-supported across multiple model families
- Medium Confidence: Clustering vector's ability to preserve architectural correlations across fine-tuning is demonstrated but relies on limited experimental validation
- Low Confidence: Generalization of LoRA dataset-driven effects beyond two tested datasets and claim that clustering vectors remain completely stable under all reasonable LoRA fine-tuning conditions

## Next Checks
1. **Distribution Assumption Validation**: Test the standard-deviation vector methodology on models with known non-Normal weight distributions (e.g., post-quantization, post-pruning, or binary-weight models) to assess sensitivity to distribution shape deviations

2. **Clustering Stability Analysis**: Systematically vary K-Means parameters (n_cluster=2,3,4; multiple random seeds; different SVD ranks) across the model set to quantify clustering vector stability and identify parameter configurations that produce consistent architectural signatures

3. **LoRA Configuration Space Exploration**: Extend LoRA fine-tuning experiments to include multiple datasets (varying in size, domain, and complexity), different LoRA ranks (1,2,4,8), and training durations (1,3,5 epochs) to map the boundaries where standard-deviation convergence holds and clustering stability breaks