---
ver: rpa2
title: Multimodal Evaluation of Russian-language Architectures
arxiv_id: '2511.15552'
source_url: https://arxiv.org/abs/2511.15552
tags:
- dataset
- answer
- audio
- questions
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERA Multi is the first multimodal evaluation benchmark for Russian
  language, introducing 18 tasks across text, image, audio, and video modalities.
  It proposes a unified taxonomy of multimodal abilities and combines public and private
  datasets to assess general-purpose and modality-specific models.
---

# Multimodal Evaluation of Russian-language Architectures

## Quick Facts
- **arXiv ID**: 2511.15552
- **Source URL**: https://arxiv.org/abs/2511.15552
- **Reference count**: 40
- **Primary result**: MERA Multi is the first multimodal benchmark for Russian language, evaluating 50+ models across 18 tasks with unified taxonomy and dual metrics (Exact Match + Judge Score).

## Executive Summary
MERA Multi introduces a comprehensive multimodal evaluation framework specifically designed for Russian-language models. The benchmark addresses the gap in culturally-grounded evaluation by creating 18 tasks across text, image, audio, and video modalities, combining public datasets with private watermarked sets for leakage protection. Using block-prompting methodology and dual metrics (Exact Match and Judge Score), the evaluation reveals significant performance gaps between modalities, with image understanding being most mature while audio and video lag behind. Baseline results show that general-purpose models outperform specialists in multimodal tasks, though human baselines remain substantially higher across all categories.

## Method Summary
MERA Multi employs a block-prompting evaluation framework using 10 prompt variants per task to reduce sensitivity and ensure robust scoring. The methodology combines Exact Match metrics for format adherence with Judge Score metrics calculated by a fine-tuned RuModernBERT classifier to measure semantic equivalence. Evaluation uses the lm-evaluation-harness framework adapted for multimodal inputs, with watermarked private datasets protected via Multimodal SMIA (MSMIA) to detect training contamination. The unified taxonomy organizes 18 tasks into perception, reasoning, and knowledge categories, enabling systematic comparison across models and modalities.

## Key Results
- General-purpose models like Qwen3-Omni-30B-A3B-Instruct achieve highest overall scores (0.5 Total Score), while specialist models lag in audio and video tasks.
- Performance gaps show image understanding most mature, with audio and video significantly underperforming despite parameter count.
- Human baselines reach 0.80-0.92 depending on task type, highlighting substantial room for improvement in current models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal performance gaps (image > audio/video) stem from uneven maturity in model training focus and data availability across modalities.
- Mechanism: Vision-language pretraining has received more research attention and larger-scale datasets, whereas audio and video understanding remain underrepresented in both model development and evaluation resources.
- Core assumption: Performance differentials primarily reflect training and data disparities rather than fundamental architectural limitations.
- Evidence anchors: [abstract] "Baseline results across 50+ models show performance gaps between modalities, with image understanding being most mature while audio and video lag behind."
- Break condition: If audio/video-specific models begin matching or exceeding image performance with comparable parameter counts and training data, the maturity gap hypothesis weakens.

### Mechanism 2
- Claim: Dual-level evaluation metrics (Exact Match + Judge Score) provide complementary signals about model capabilities.
- Mechanism: Exact Match captures format adherence and factual correctness, while Judge Score measures semantic equivalence via an LLM-as-judge, together revealing cases where models produce correct answers in wrong formats or vice versa.
- Core assumption: Judge model generalizes reliably across diverse Russian-language model outputs and question types.
- Evidence anchors: [section 3.3.1] "Exact Match (EM) serves as a generative analog of accuracy... Complementing this, the Judge Score (JS) measures semantic similarity via an LLM-as-a-judge."
- Break condition: If Judge Score consistently diverges from human judgment or shows systematic bias, the complementary mechanism breaks down.

### Mechanism 3
- Claim: Data protection (watermarking + leakage detection) may help maintain benchmark validity for future model comparisons.
- Mechanism: Watermarking enables tracing benchmark data in training corpora, while Multimodal SMIA (MSMIA) detects if models were trained on benchmark examples, reducing risk of artificially inflated scores.
- Core assumption: Watermarks are imperceptible to models and humans, and MSMIA generalizes across model architectures.
- Evidence anchors: [abstract] "A methodology for preventing benchmark leakage, including watermarking for private sets."
- Break condition: If watermarks degrade evaluation quality or MSMIA shows high false-positive rates, the protection mechanism could hinder rather than help fair evaluation.

## Foundational Learning

- **Multimodal Evaluation Taxonomy**: Ability to classify skills across perception, reasoning, and knowledge.
  - Why needed here: MERA Multi organizes 18 tasks under a unified taxonomy to map model strengths/weaknesses.
  - Quick check question: Can you categorize a VQA task requiring object counting and spatial reasoning into the taxonomy?

- **Evaluation Metrics (EM vs. JS)**: Understanding tradeoffs between exact string matching and semantic similarity.
  - Why needed here: Scores differ significantly between metrics, indicating format adherence issues.
  - Quick check question: When would Judge Score be preferred over Exact Match?

- **Russian Cultural/Linguistic Context**: Awareness of domain-specific references (folklore, Soviet media) and morphological complexity.
  - Why needed here: Direct translation of English benchmarks is inadequate for evaluating Russian models.
  - Quick check question: Why might a model trained on English data perform poorly on culturally grounded Russian tasks?

## Architecture Onboarding

- **Component map**: Evaluation pipeline → Block-prompting (10 variants) → Dual metrics (EM/JS) → Score aggregation → Leaderboard
- **Critical path**: 
  1. Clone repository, prepare submission files
  2. Run inference on public tasks
  3. Submit for automatic scoring
  4. Request publication after verification
- **Design tradeoffs**: 
  - Public vs. private datasets (reproducibility vs. leakage prevention)
  - Single vs. multi-prompt evaluation (bias vs. robustness)
  - Modality-specific vs. unified scoring (specialization vs. comparability)
- **Failure signatures**: 
  - EM ≈ 0 with JS > 0: Format adherence issues
  - Large variance across prompt variants: Prompt sensitivity
  - High scores on public but low on private tasks: Potential data contamination
- **First 3 experiments**:
  1. Evaluate a baseline model (e.g., Qwen2.5-VL-7B) on a subset of image tasks to understand metric behavior.
  2. Compare performance across modalities to identify gaps relative to human baselines.
  3. Test the LLM-as-judge on a small held-out set to validate agreement with human labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do state-of-the-art MLLMs exhibit biases toward underrepresented minorities when evaluated specifically within the context of Russian cultural and linguistic norms?
- Basis in paper: [explicit] The Ethical Statement explicitly notes that "we did not perform explicit evaluation of any bias of these models, e.g., toward any kind of underrepresented minorities," identifying this as an "extremely important direction of future work."
- Why unresolved: The current benchmark evaluates semantic abilities and safety but lacks specific datasets or metrics designed to detect bias against minority groups.
- What evidence would resolve it: An extension of MERA Multi with bias-specific test cases and an analysis of model performance discrepancies across these demographic categories.

### Open Question 2
- Question: To what extent does high performance on the 18 MERA Multi tasks predict success in specialized domains or abilities potentially underrepresented in the current benchmark?
- Basis in paper: [explicit] The Limitations section states the set of 18 tasks "may be underrepresenting some abilities" and notes that a model excelling on the benchmark might "perform poorly on a specialized domain or task."
- Why unresolved: The paper focuses on general-purpose evaluation and does not validate the correlation between benchmark scores and performance in niche, specialized applications.
- What evidence would resolve it: A comparative study measuring the correlation between MERA Multi scores and model performance on specific, high-stakes professional tasks (e.g., medical diagnosis or legal analysis in Russian).

### Open Question 3
- Question: What architectural or data-centric interventions are most effective at elevating the performance of Russian-language models on audio and video tasks to match the maturity of image understanding?
- Basis in paper: [explicit] The results highlight a significant performance gap, with "image understanding being most mature while audio and video lag behind."
- Why unresolved: The paper identifies the gap and suggests audio and video are "underrepresented" in terms of models and datasets but does not isolate the specific causes (data scarcity vs. architectural limitations).
- What evidence would resolve it: Ablation studies using MERA Multi to evaluate models trained with varied data scaling strategies or modified encoder architectures specifically targeting audio and video modalities.

## Limitations

- **Limited Multimodal Coverage**: MERA Multi focuses exclusively on Russian language and culture, which may limit generalizability to other low-resource languages.
- **Judge Model Reliability**: The RuModernBERT-based LLM-as-a-judge for semantic scoring has not been extensively validated across all task types.
- **Data Leakage Protection Efficacy**: The watermarking and membership inference systems (MSMIA) are novel implementations that lack independent verification.

## Confidence

- **High Confidence** (Well-supported by evidence):
  - Performance gaps between modalities are real and measurable
  - Dual-metric approach provides complementary information
  - Russian-specific evaluation is necessary for cultural grounding

- **Medium Confidence** (Reasonable but with gaps):
  - Watermarking and MSMIA will effectively prevent leakage
  - Judge Score generalizes reliably across all task types
  - Block-prompting with 10 variants sufficiently reduces prompt sensitivity

- **Low Confidence** (Weak or missing evidence):
  - Generalizability to other Slavic or low-resource languages
  - Long-term effectiveness of data protection measures
  - Judge model's performance on highly specialized tasks

## Next Checks

1. **Judge Model Validation**: Conduct blind human evaluation on a stratified sample of responses across all 18 tasks to measure agreement with the LLM-as-a-judge, particularly for audio and video modalities where performance gaps are largest.

2. **Watermark Robustness Test**: Attempt to detect and remove watermarks from private datasets using state-of-the-art watermark removal techniques, then evaluate whether model performance on watermarked data changes significantly.

3. **Cross-Lingual Generalization**: Adapt 3-5 representative MERA Multi tasks for Ukrainian or Polish (similar Slavic languages) and evaluate whether the same performance patterns emerge, testing the benchmark's applicability beyond Russian.