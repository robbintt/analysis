---
ver: rpa2
title: Improving watermelon (Citrullus lanatus) disease classification with generative
  artificial intelligence (GenAI)-based synthetic and real-field images via a custom
  EfficientNetV2-L model
arxiv_id: '2508.10156'
source_url: https://arxiv.org/abs/2508.10156
tags:
- images
- synthetic
- real
- were
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether combining real and synthetic images
  improves watermelon disease classification performance. Five training treatments
  were evaluated using an EfficientNetV2-L model: only real images, only synthetic
  images, 1:1 real-to-synthetic ratio, 1:10 real-to-synthetic ratio, and 1:10 real-to-synthetic
  with an unknown class.'
---

# Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model

## Quick Facts
- arXiv ID: 2508.10156
- Source URL: https://arxiv.org/abs/2508.10156
- Reference count: 8
- Primary result: Hybrid training with 1:10 real-to-synthetic image ratio achieved weighted F1-score of 1.00, significantly outperforming real-only (0.65) or synthetic-only (0.74) approaches.

## Executive Summary
This study investigates whether combining real and synthetic images improves watermelon disease classification performance using an EfficientNetV2-L model. Five training treatments were evaluated: only real images, only synthetic images, 1:1 real-to-synthetic ratio, 1:10 real-to-synthetic ratio, and 1:10 real-to-synthetic with an unknown class. The results demonstrate that even a small number of real images (1:10 ratio) significantly enhanced model performance and generalizability, with weighted F1-score improving from 0.65 to 1.00. Synthetic images alone could not substitute for real images; instead, a hybrid approach yielded the best results. The study validates that combining real and synthetic data maximizes classification accuracy for crop disease diagnosis.

## Method Summary
The methodology employed a two-stage transfer learning strategy using EfficientNetV2-L pre-trained on ImageNet. Real images (750 total, 480×480 resolution) were captured using a Sony DSLR camera, while synthetic images were generated using Stable Diffusion 3.5 with LoRA and DreamBooth fine-tuning on 30-35 real samples per class. The model architecture featured a custom classifier head with three dense blocks (1024, 512, 256 units) with Swish activation and dropout regularization. Training followed a progressive approach: first training the head with frozen backbone, then fine-tuning deeper layers with lower learning rate. Five experimental treatments were evaluated, with 112 test images held constant across all treatments for direct comparison.

## Key Results
- Weighted F1-score improved from 0.65 (only real images) to 1.00 (1:10 real-to-synthetic ratio)
- Synthetic images alone achieved F1-score of 0.74 but misclassified real virus symptoms as fungal disease
- Including an "unknown" class reduced false positives on background clutter and improved robustness
- t-SNE and UMAP visualization showed better feature separation in hybrid models compared to real-only training

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Real-Synthetic Generalization
A hybrid dataset containing a small fraction of real-field images (1:10 ratio) combined with synthetic images significantly improves classification F1-scores compared to using real or synthetic images alone. Real images introduce essential domain-specific noise (occlusion, motion blur, lighting artifacts) that synthetic models often smooth out, while synthetic images provide volume and feature consistency. The hybrid approach allows the model to learn robust feature representations by seeing consistent synthetic examples at scale while grounding those features in the "edge-case" variability of real data.

### Mechanism 2: Unknown Class Regularization
Including an "unknown" class (random non-disease images) during training acts as a regularization technique, reducing false positives on background clutter (soil, mulch, weeds). By training with an explicit "unknown" category, the model learns a decision boundary for "non-target" inputs rather than forcing irrelevant visual data into one of the disease classes. This functions as a hard-negative mining strategy, teaching the model to recognize patterns that don't belong to any crop-centered category.

### Mechanism 3: Progressive Transfer Learning with Custom Heads
A two-stage transfer learning strategy (freezing backbone → fine-tuning deeper layers) with a custom dense classifier head effectively adapts a generic pre-trained model to specialized disease classification. Freezing the initial backbone layers preserves generic low-level feature extraction (edges, textures) learned from ImageNet, while training the custom head first allows the model to establish a mapping for the specific new classes without disrupting the pre-trained weights. Subsequent fine-tuning adjusts higher-level features to the specific nuances of watermelon pathology.

## Foundational Learning

- **Concept**: Transfer Learning & Fine-Tuning
  - **Why needed here**: The study relies on EfficientNetV2-L, a model pre-trained on ImageNet. You must understand that you cannot train a state-of-the-art model from scratch with limited agricultural data; you must adapt existing knowledge.
  - **Quick check question**: Why did the authors freeze the base layers first and unfreeze them later, rather than training all layers simultaneously from the start?

- **Concept**: The F1-Score vs. Accuracy
  - **Why needed here**: The study cites a massive jump in the weighted F1-score (0.65 to 1.00). Accuracy can be misleading in imbalanced datasets (e.g., if "healthy" images dominate), whereas F1-score balances precision and recall, which is critical when diagnosing specific diseases.
  - **Quick check question**: If a model predicts "Healthy" for everything and the dataset is 90% healthy, accuracy is 90%, but what happens to the F1-score for the "Fungal" class?

- **Concept**: Latent Space & Clustering (t-SNE/UMAP)
  - **Why needed here**: The paper uses t-SNE and UMAP to prove that the hybrid model creates better feature separation. Understanding this helps you visualize why the model classifies better—distinct clusters mean the model sees clear differences between "Fungal" and "Virus."
  - **Quick check question**: In Figure 6, why does the "Only Real" (H0) plot show overlapping clusters while the "Hybrid" (H3) plot shows distinct, separated clusters?

## Architecture Onboarding

- **Component map**: Input (480×480 RGB Images) → EfficientNetV2-L Backbone (Pre-trained on ImageNet) → Global Average Pooling → Dense(1024, Swish, Dropout 0.5, BN) → Dense(512, Swish, Dropout 0.5, BN) → Dense(256, Swish, Dropout 0.4, BN) → Output (Softmax, 3 or 4 classes) → AdamW Optimizer

- **Critical path**: The data pipeline is the critical path. The specific ratio of Real:Synthetic (1:10) and the inclusion of the Unknown class are the architectural "secret sauce" reported here. If you skip the synthetic data generation step (using SD 3.5 + Dreambooth), the model performance collapses (F1 drops to 0.65).

- **Design tradeoffs**:
  - Real vs. Synthetic Volume: Using only real data yields poor generalization (overfitting); using only synthetic data yields poor realism (domain shift). You must balance them.
  - Unknown Class: Adding the "Unknown" class adds robustness but increases model complexity and computational load slightly. It is a tradeoff between precision on known classes and safety against false positives.

- **Failure signatures**:
  - High False Positives on "Healthy": Observed in H0 (Real only) where precision dropped to 0.42, indicating the model is over-predicting "Healthy" due to lack of diverse training examples.
  - Domain Shift: Observed in H1 (Synthetic only) where the model misclassifies real virus symptoms as fungal, likely due to color bias in synthetic images.

- **First 3 experiments**:
  1. **Baseline Reality Check**: Train EfficientNetV2-L on the available real images only (H0). Establish the lower bound of performance (F1 ~0.65) to justify the need for synthetic data.
  2. **Synthetic Viability Test**: Train on synthetic images only (H1). If performance is poor (F1 ~0.74 but high misclassification), prove that synthetic data cannot fully replace real field data.
  3. **Ratio Optimization**: Test the hybrid ratios (1:1 vs 1:10). Validate that increasing the synthetic volume relative to real images (up to the 1:10 point) improves cluster separability (t-SNE) and F1-score.

## Open Questions the Paper Calls Out

1. **Incremental Ratio Testing**: Does incrementally increasing the ratio of real-to-synthetic images (e.g., 1:2, 1:3) result in a predictable linear improvement in model robustness compared to the large jumps observed between 1:1 and 1:10 ratios? The authors note that future studies must consider incrementing real images in smaller steps rather than skipping directly from 1:1 to 1:10.

2. **Cross-Geographic Validation**: Can the hybrid training approach maintain high classification accuracy when validated on external datasets collected from diverse geographic regions, seasons, and commercial imaging hardware? The authors state that broader utility requires external validation using independently collected samples from single experimental sites.

3. **Multi-Symptom Generation**: How can generative AI models be adapted to generate authentic images of leaves with multiple co-occurring disease symptoms rather than single pathologies? The authors highlight that current Stable Diffusion training used single-class examples, whereas real-field conditions often present multiple disease symptoms on a single leaf.

4. **Color Domain Shift**: To what degree does the observed color discrepancy between synthetic (dark green) and real (light green) images induce a domain shift that limits model generalization? The authors note an inconsistency where synthetic images appeared dark green while real images were light green, suggesting this may have affected accuracy.

## Limitations
- The optimal 1:10 real-to-synthetic ratio may be dataset-specific and could vary with different disease types or image quality levels
- Synthetic image generation relies heavily on prompt engineering quality, introducing potential bias if prompts don't capture field variability
- The "unknown" class regularization mechanism lacks strong validation in agricultural contexts
- Study focus on three specific watermelon diseases limits applicability to other crops or broader disease spectra

## Confidence

- **High Confidence**: The core finding that hybrid real-synthetic training (1:10 ratio) significantly improves classification performance compared to either data source alone. The 0.65 to 1.00 F1-score improvement is well-documented and directly measured.
- **Medium Confidence**: The mechanism explanations for why hybrid training works (domain-specific noise vs. synthetic consistency). While logical and supported by cluster analysis, the specific causal pathways are inferred rather than experimentally isolated.
- **Low Confidence**: The effectiveness of the "unknown" class as a regularization technique in agricultural settings. This mechanism is mentioned but lacks comparative analysis against alternative regularization methods.

## Next Checks

1. **Cross-Crop Validation**: Test the 1:10 hybrid ratio and unknown class approach on different crop diseases (e.g., tomato blight, rice blast) to verify if the optimal ratio generalizes across agricultural domains.

2. **Synthetic Generation Robustness**: Systematically vary prompt engineering parameters (lighting conditions, disease severity levels) to quantify how synthetic image quality affects classification performance and identify failure modes.

3. **Alternative Regularization Comparison**: Compare the unknown class approach against standard techniques like dropout, data augmentation, and focal loss to determine if the explicit background class provides unique benefits for agricultural disease classification.