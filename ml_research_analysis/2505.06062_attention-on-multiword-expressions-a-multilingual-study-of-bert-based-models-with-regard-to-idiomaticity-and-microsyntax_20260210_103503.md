---
ver: rpa2
title: 'Attention on Multiword Expressions: A Multilingual Study of BERT-based Models
  with Regard to Idiomaticity and Microsyntax'
arxiv_id: '2505.06062'
source_url: https://arxiv.org/abs/2505.06062
tags:
- attention
- idioms
- msus
- language
- mwes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how fine-tuning BERT-based models affects
  their attention patterns towards idioms and microsyntactic units (MSUs) across six
  Indo-European languages. By extending Jang et al.'s attention analysis methodology,
  the authors examine pre-trained versus fine-tuned models on syntactic and semantic
  tasks.
---

# Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax

## Quick Facts
- arXiv ID: 2505.06062
- Source URL: https://arxiv.org/abs/2505.06062
- Reference count: 11
- This study investigates how fine-tuning BERT-based models affects their attention patterns towards idioms and microsyntactic units across six Indo-European languages.

## Executive Summary
This study investigates how fine-tuning BERT-based models affects their attention patterns towards idioms and microsyntactic units (MSUs) across six Indo-European languages. By extending Jang et al.'s attention analysis methodology, the authors examine pre-trained versus fine-tuned models on syntactic and semantic tasks. Results show that syntactic fine-tuning increases attention to MSUs in lower layers, aligning with syntactic processing, while semantic fine-tuning distributes attention more evenly across layers for idioms. Attention to both MWE types generally decreases after fine-tuning, especially in higher layers. The study reveals that attention mechanisms adapt during fine-tuning to better reflect linguistic properties of the target task and language. Cross-linguistic differences are observed, with Germanic languages showing more uniform patterns compared to Slavic languages.

## Method Summary
The study analyzes attention patterns of 24-layer monolingual BERT variants across six Indo-European languages (English, German, Dutch, Polish, Russian, Ukrainian). Researchers fine-tune each model on four tasks (dependency parsing, POS tagging, NER, topic classification) for 10 epochs, then extract L×H attention matrices, average across heads per layer, and aggregate subword attention for MWE spans. The analysis compares pre-trained versus fine-tuned models, computing attention from context tokens to MWE tokens and within MWE tokens. The dataset includes 227 idioms per language from the ID10M corpus and 227 Slavic MSUs from Zaitova et al. (2023), with downstream tasks using Universal Dependencies, WikiANN, and SIB-200 datasets.

## Key Results
- Fine-tuning redistributes attention to MWEs based on task type—syntactic tasks concentrate MSU attention in lower layers, while semantic tasks distribute idiom attention more evenly across layers.
- Lower layers preferentially attend to MSUs due to their syntactic irregularity, while idioms receive more distributed attention reflecting semantic non-compositionality.
- Morphological complexity moderates attention uniformity—Germanic languages show more uniform patterns, Slavic languages more varied.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning redistributes attention to MWEs based on task type—syntactic tasks concentrate MSU attention in lower layers, while semantic tasks distribute idiom attention more evenly.
- Mechanism: Task-specific loss gradients reweight attention patterns during fine-tuning, causing the model to prioritize either local syntactic features (lower layers) or integrate information across layers for semantic reasoning.
- Core assumption: The task signal is sufficiently strong to override pre-trained attention priors within 10 epochs of fine-tuning.
- Evidence anchors:
  - [abstract] "Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements."
  - [section 5.1] "Fine-tuning on syntactic tasks (Topic and DepRel) leads to a sharp decrease in middle to upper layers... Fine-tuning on semantic tasks (NER and Topic) results in mixed changes."
  - [corpus] Weak direct corpus support; related work on MWEs in LLMs (e.g., "Evaluating Large Language Models on Multiword Expressions") confirms MWE difficulty but doesn't validate attention redistribution.

### Mechanism 2
- Claim: Lower layers preferentially attend to MSUs due to their syntactic irregularity, while idioms receive more distributed attention reflecting semantic non-compositionality.
- Mechanism: BERT's hierarchical processing—lower layers encode local syntactic structure, higher layers abstract semantics—causes MSUs (syntactic anomalies) to peak at layers 3-4, while idioms require cross-layer integration.
- Core assumption: MSUs are primarily syntactic phenomena and idioms are primarily semantic phenomena, which may not hold universally.
- Evidence anchors:
  - [section 2] "Lower layers of BERT are better at encoding local syntactic information, while higher layers progressively engage with more complex semantic processing."
  - [section 5.2] "A consistent pattern where attention peaks at the lower layers (3-4) when processing MSUs is observed across all models for all languages."
  - [corpus] No direct corpus validation; assumption relies on linguistic theory cited in paper (Iomdin, 2015; Avgustinova and Iomdin, 2019).

### Mechanism 3
- Claim: Morphological complexity moderates attention uniformity—Germanic languages show more uniform patterns, Slavic languages more varied.
- Mechanism: Richer morphology in Slavic languages increases token-level variation, causing attention to distribute less uniformly during fine-tuning.
- Core assumption: The observed differences are due to linguistic properties rather than model or dataset artifacts.
- Evidence anchors:
  - [section 5.3] "With the Germanic languages (EN, DE, NL), the models produce more uniform attention patterns... In contrast, the models for Slavic languages (PL, RU, UK), especially for PL, display more varied attention patterns."
  - [section 5.3] "This may be attributed to lower morphological complexity in Germanic languages."
  - [corpus] No corpus papers validate this cross-linguistic attention hypothesis directly.

## Foundational Learning

- Concept: Multi-head attention aggregation
  - Why needed here: The paper averages attention across H heads per layer to produce a single T×T matrix; understanding this simplification is critical for interpreting results.
  - Quick check question: If one head strongly attends to an MWE while others do not, what happens to the aggregated attention score?

- Concept: Subword tokenization alignment
  - Why needed here: MWEs like "spill the beans" may be split into subwords; the paper aggregates attention weights across subword tokens to treat MWEs as single units.
  - Quick check question: How would you handle an MWE that tokenizes to [ spill, the, bean, ##s ] when computing attention from context?

- Concept: Fine-tuning vs. frozen representations
  - Why needed here: The study compares pre-trained (frozen attention) to fine-tuned models; the mechanism depends on gradient-based updates to attention weights.
  - Quick check question: If you freeze all attention weights during fine-tuning, what would happen to the observed attention patterns toward MWEs?

## Architecture Onboarding

- Component map: 24-layer encoder-only transformer (BERT-large architecture) -> Multi-head attention per layer (averaged post-hoc) -> Layer-wise attention extraction -> Token-to-MWE attention aggregation -> Cross-language comparison (EN, DE, NL, PL, RU, UK).

- Critical path: (1) Tokenize sentences with MWE annotations; (2) Forward pass through pre-trained or fine-tuned model; (3) Extract L×H attention matrices; (4) Average across heads; (5) Compute attention from all tokens to MWE tokens and within MWE tokens; (6) Compare layer-wise distributions across tasks and languages.

- Design tradeoffs: Averaging across heads loses individual head specialization; using 227 MWE instances per language limits statistical power; restricting to Indo-European languages reduces typological generalizability.

- Failure signatures: (1) Attention peaks that appear only in one language (e.g., PL lower-layer spike) may indicate dataset artifacts; (2) Uniformly decreased attention post-fine-tuning may suggest task-MWE misalignment rather than improved processing; (3) High variance across runs with small datasets.

- First 3 experiments:
  1. Replicate the attention extraction pipeline on a single language (e.g., English) with pre-trained BERT-large, computing attention to idioms and verifying lower-layer MSU peaks exist.
  2. Fine-tune BERT-large on POS tagging (syntactic) and NER (semantic), then compare layer-wise attention shifts to a held-out idiom set.
  3. Extend the analysis to one non-Indo-European language (e.g., Arabic or Finnish) to test whether morphological complexity correlates with attention uniformity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do deep semantic tasks (e.g., summarization, paraphrasing) elicit different attention patterns towards idioms compared to the shallow semantic tasks (NER, Topic Classification) analyzed in this study?
- Basis in paper: [explicit] The Limitations section states that the chosen semantic tasks "may involve only shallow semantic processing" and suggests that tasks like summarization could "provide additional insights."
- Why unresolved: The study restricted its evaluation to classification-based downstream tasks, leaving generative or deep-understanding tasks unexplored.
- What evidence would resolve it: Replicating the fine-tuning and attention extraction methodology using datasets for abstractive summarization or paraphrase detection that contain idiomatic expressions.

### Open Question 2
- Question: Are the observed lower-layer attention peaks for Microsyntactic Units (MSUs) consistent across non-Slavic languages?
- Basis in paper: [explicit] The Limitations section notes that the "dataset of microsyntactic units includes only the Slavic language groups," preventing cross-group comparison for MSUs.
- Why unresolved: The authors could not analyze MSU attention patterns for Germanic languages (English, German, Dutch) due to a lack of suitable datasets.
- What evidence would resolve it: Creating or identifying MSU datasets for Germanic languages and analyzing the layer-wise attention distribution in the respective BERT-based models.

### Open Question 3
- Question: How do attention mechanisms in non-encoder-only architectures (e.g., decoder-only or encoder-decoder models) allocate focus to MWEs during fine-tuning?
- Basis in paper: [explicit] The Conclusion suggests "Future work could explore... different model architectures" to build upon the findings regarding encoder-only BERT-based models.
- Why unresolved: The study focused exclusively on 24-layer encoder-only transformer models, so the behavior of other architectures remains unknown.
- What evidence would resolve it: Applying the same attention analysis methodology to fine-tuned models like GPT (decoder-only) or T5 (encoder-decoder).

## Limitations
- The study's primary limitation is its reliance on averaged attention across all heads per layer, which obscures potential head-level specialization for MWE processing.
- The analysis is constrained to 227 MWE instances per language, which may limit statistical power for detecting subtle attention patterns.
- The exclusive focus on Indo-European languages restricts generalizability to typologically diverse language families.

## Confidence

**High Confidence**: The observation that fine-tuning reduces overall attention to MWEs in higher layers is well-supported by consistent patterns across all six languages and multiple task types.

**Medium Confidence**: The cross-linguistic differences between Germanic and Slavic languages regarding attention uniformity are plausible but lack direct corpus validation.

**Low Confidence**: The claim that MSUs peak at layers 3-4 specifically due to syntactic irregularity requires more direct evidence linking layer position to syntactic processing depth.

## Next Checks

1. **Head-level attention analysis**: Repeat the analysis examining individual attention heads rather than averaging across all heads per layer to reveal whether specific heads specialize in MWE processing.

2. **Typological extension**: Apply the same methodology to one non-Indo-European language family (e.g., Turkish or Japanese) to test whether morphological complexity predicts attention uniformity patterns across broader typological diversity.

3. **Controlled token-level analysis**: For MSUs that tokenize into multiple subwords, conduct a controlled experiment varying subword segmentation to determine whether observed attention patterns are robust to tokenization choices.