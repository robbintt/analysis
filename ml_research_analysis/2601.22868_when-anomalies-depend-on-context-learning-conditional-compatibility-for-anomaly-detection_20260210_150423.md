---
ver: rpa2
title: 'When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly
  Detection'
arxiv_id: '2601.22868'
source_url: https://arxiv.org/abs/2601.22868
tags:
- anomaly
- detection
- context
- contextual
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of contextual anomaly detection,
  where the same object or action can be normal or anomalous depending on its surrounding
  environment. Unlike traditional anomaly detection that focuses on intrinsic appearance
  deviations, this work proposes a conditional compatibility learning framework that
  models the relationship between subjects and their contexts.
---

# When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection

## Quick Facts
- **arXiv ID:** 2601.22868
- **Source URL:** https://arxiv.org/abs/2601.22868
- **Reference count:** 40
- **Primary result:** Introduces CoRe-CLIP, a conditional compatibility learning framework that models subject-context relationships, achieving state-of-the-art performance on contextual anomaly detection benchmarks and strong zero-shot transfer to standard datasets.

## Executive Summary
This paper addresses the challenge of contextual anomaly detection, where the same object or action can be normal or anomalous depending on its surrounding environment. Unlike traditional anomaly detection that focuses on intrinsic appearance deviations, this work proposes a conditional compatibility learning framework that models the relationship between subjects and their contexts. The authors introduce CAAD-3K, a benchmark designed to isolate contextual anomalies by controlling subject identity while varying contextual conditions. They propose CoRe-CLIP, a vision-language framework that decomposes visual representations into subject-focused, context-focused, and global views, and performs text-conditioned reasoning to assess compatibility. CoRe-CLIP substantially outperforms existing approaches on CAAD-3K, achieving state-of-the-art performance on MVTec-AD and VisA. It demonstrates strong cross-context generalization and robust zero-shot transfer to standard anomaly detection benchmarks, showing that modeling context dependence complements traditional structural anomaly detection.

## Method Summary
CoRe-CLIP is a vision-language framework for contextual anomaly detection that learns to assess compatibility between subjects and their contexts. The method uses a frozen CLIP ViT backbone with three lightweight Context-Selective Residual (CSR) branches that decompose visual representations into subject-focused, context-focused, and global views. A text refinement module produces paired normal/anomalous text embeddings for each class, which are then used by a Context-Refined Module (CRM) to perform text-conditioned attention-based fusion of the three branches. The framework is trained in two stages: first refining text embeddings with disentanglement objectives, then jointly training CSR and CRM modules. The approach achieves compatibility assessment through cosine similarity margins between fused visual embeddings and the normal/anomalous text embeddings.

## Key Results
- CoRe-CLIP achieves 87.3 I-AUROC on CAAD-3K cross-context split, substantially outperforming CLIP (38.1) and WinCLIP (40.1) baselines
- The framework demonstrates strong zero-shot transfer, achieving state-of-the-art performance on MVTec-AD (86.2 I-AUROC) and VisA benchmarks
- CSR decomposition provides critical gains, with tri-branch architecture outperforming single-branch approaches by 22-23 I-AUROC points on CAAD-3K
- CRM text-conditioned fusion outperforms static averaging/concatenation by 6-7 I-AUROC points, confirming adaptive reasoning's importance

## Why This Works (Mechanism)

### Mechanism 1: Representation Decomposition via Context-Selective Residuals
Separating visual representations into subject, context, and global views enables detection of relational mismatches that intrinsic (entangled) representations cannot identify. Three parallel CSR branches receive the same image but apply branch-specific residual adapters to the first K=6 transformer layers. Subject and context views are constructed via pixel-wise masking using segmentation masks during training only; all branches process the full image at inference. Contextual anomalies arise from subject-context incompatibility rather than intrinsic appearance defects; thus, representations that conflate these sources are fundamentally non-identifiable. CSR provides the largest gain (44.3 → 72.8 I-AUROC), indicating disentanglement is critical. If anomaly labels depend only on intrinsic appearance, decomposition provides no benefit and adds overhead.

### Mechanism 2: Text Disentanglement for Contextual States
Refining text embeddings into orthogonal normal/anomalous pairs per class enables context-sensitive semantic comparison without requiring class-specific training. The text encoder is adapted via gated residual pathways in the first L=3 layers. Three losses operate jointly: orthogonality (separates t₀ and t₁), consistency (preserves shared class identity), and grounding (aligns prototype with visual semantics). The semantic space learned by CLIP can be locally perturbed to encode contextual compatibility states while preserving class identity. Removing orthogonality or grounding substantially degrades performance (87.3 → 82.1–82.7 I-AUROC). If prompt templates cannot meaningfully capture normal vs. anomalous contextual states, disentanglement may not improve over standard prompts.

### Mechanism 3: Text-Conditioned Compatibility Reasoning (CRM)
Adaptive fusion of subject, context, and global branches via text-conditioned attention enables reasoning about which representation is relevant for detecting incompatibility. The anomaly text embedding t₁ forms a query q; branch embeddings are projected to keys K. Attention weights α = softmax(q^T K / √d) fuse branches: z_crm = Σ α_b z_b. This yields context-sensitive weighting—e.g., emphasizing subject branch when context mismatch is the anomaly signal. The anomaly text embedding encodes sufficient semantic signal to guide branch selection; cross-branch consistency can be regularized via entropy and consistency losses. CRM (87.3) outperforms averaging (80.3) and concatenation (81.5), confirming adaptive fusion matters. If attention weights collapse to uniform distribution or over-rely on a single branch, CRM degrades to static fusion.

## Foundational Learning

- **Vision-Language Alignment (CLIP)**
  - **Why needed here:** CoRe-CLIP builds on frozen CLIP embeddings; understanding contrastive image–text pretraining is prerequisite for interpreting why cosine similarity works as a compatibility proxy.
  - **Quick check question:** Can you explain why CLIP embeddings enable zero-shot classification via cosine similarity?

- **Residual Adapters / Parameter-Efficient Fine-Tuning**
  - **Why needed here:** CSR modules adapt only the first K=6 layers via lightweight residual pathways; this preserves pretrained alignment while enabling branch specialization.
  - **Quick check question:** How do residual adapters differ from full fine-tuning in terms of gradient flow and catastrophic forgetting risk?

- **Attention-Based Feature Fusion**
  - **Why needed here:** CRM uses single-head attention over three branch embeddings; understanding query–key–value formulation is necessary to debug attention patterns.
  - **Quick check question:** Given query q and keys K, what does a high attention weight on the context branch suggest about the anomaly?

## Architecture Onboarding

- **Component map:** Image → CSR branches (subject, context, global) → {z_s, z_c, z_g} → CRM attention (using t₁) → z_crm → compatibility score
- **Critical path:** Image → three CSR branches → {z_s, z_c, z_g} → Class prompt → refined text encoder → {t₀, t₁} → t₁ as query → CRM attention → z_crm → Cosine margin: s(x) = σ(sim(z_crm, t₁) - sim(z_crm, t₀)) → Pixel scores via same attention weights on patch tokens
- **Design tradeoffs:** Three branches vs. one: Tri-branch enables relational reasoning but increases forward-pass cost; paper shows single branches are insufficient (Table 5: 64.2–70.4 I-AUROC vs. 87.3). Mask requirement during training: Segmentation masks are required to construct subject/context views during training but not inference; SAM-generated masks work comparably (Table 6: 87.3 → 85.3 I-AUROC). CRM vs. static fusion: CRM adds parameters and complexity but yields ~6–7 point I-AUROC gain over averaging/concatenation (Table 11).
- **Failure signatures:** Attention collapse: If CRM entropy penalty (λ_fuse-ent) is too weak, α becomes uniform; fusion reduces to averaging. Text embedding collapse: If orthogonality loss is underweighted, t₀ and t₁ converge, eliminating semantic separation. Branch imbalance: If CSR adapters overfit to one branch, CRM cannot recover balanced reasoning; monitor branch-level I-AUROC during training.
- **First 3 experiments:** 1) Baseline verification: Run frozen CLIP on CAAD-CC (1-shot) and reproduce ~38 I-AUROC to validate pipeline; this isolates implementation issues from architecture effects. 2) CSR ablation: Train with only global branch (no CSR) vs. full tri-branch; expect ~44 → 87 I-AUROC gap per Table 4, confirming decomposition necessity. 3) Fusion mechanism comparison: Replace CRM with simple averaging and verify performance drop to ~80 I-AUROC; this confirms attention-based fusion is non-redundant.

## Open Questions the Paper Calls Out

### Open Question 1
Can conditional compatibility learning be extended to partial or mixed-state contextual anomalies, where only specific attributes of an object violate scene context rather than the entire subject? The current formulation models contextual mismatch with respect to a dominant subject entity, assuming binary normal/anomalous labels at the subject level. Extending this to partial or mixed-state anomalies would enable finer-grained reasoning. A benchmark with fine-grained annotations for attribute-level contextual violations, combined with modified architecture that can isolate incompatible object components, would resolve this.

### Open Question 2
How does the reliance on CLIP pretrained representations affect contextual compatibility judgments across different cultural or geographic contexts? CoRe-CLIP relies on a pretrained CLIP backbone, which may encode social, cultural, or geographic biases from its web-scale training data; as a result, contextual compatibility judgments may reflect such normative assumptions. The paper does not evaluate performance stratified by context types that may carry cultural or geographic specificity, and no debiasing mechanisms are proposed. Systematic evaluation on geographically or culturally diverse contextual anomaly datasets, with analysis of failure modes attributable to pretrained representation biases, would resolve this.

### Open Question 3
Would multi-hop inference or graph-based reasoning over subject–context relationships improve detection accuracy compared to the single-step CRM fusion mechanism? Exploring richer mechanisms such as multi-hop inference, iterative refinement, or graph-based propagation could strengthen subject–scene coherence modeling. The CRM employs single-step, text-conditioned fusion to combine multi-branch evidence, which may not capture complex relational structures. Ablation studies comparing single-step fusion against iterative or graph-structured reasoning modules on complex multi-object contextual anomaly scenarios would resolve this.

## Limitations
- The synthetic CAAD-3K benchmark may not fully capture real-world contextual anomaly complexity, potentially limiting practical applicability
- The framework's effectiveness depends heavily on prompt engineering quality and semantic descriptions, which may not generalize to ambiguous domains
- Text refinement mechanism shows significant impact in ablations, but limited direct evidence from related work in the corpus raises questions about robustness

## Confidence

- **High confidence:** Core architecture design (CSR decomposition, CRM fusion) and effectiveness on CAAD-3K benchmark, supported by multiple ablations
- **Medium confidence:** Zero-shot transfer results to MVTec-AD and VisA, as contextual anomalies are less common in these datasets
- **Medium confidence:** Text disentanglement mechanism, as orthogonality and grounding losses show significant impact but limited direct evidence from related work

## Next Checks

1. **Cross-dataset robustness:** Evaluate CoRe-CLIP on a real-world contextual anomaly detection dataset (if available) to verify that synthetic benchmark performance translates to practical scenarios.

2. **Prompt sensitivity analysis:** Systematically vary prompt templates and measure performance degradation to quantify the robustness of text-conditioned reasoning to prompt engineering quality.

3. **Real-time inference cost:** Benchmark the computational overhead of the tri-branch architecture with CRM fusion against single-branch baselines to assess practical deployment feasibility.