---
ver: rpa2
title: 'From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs
  via Building Zero-Shot Discriminative Embedding Model'
arxiv_id: '2508.00955'
source_url: https://arxiv.org/abs/2508.00955
tags:
- prompt
- embedding
- retrieval
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of adapting Multimodal Large
  Language Models (MLLMs) for universal embedding tasks without the computational
  burden of large-scale contrastive pre-training. The proposed framework leverages
  the MLLM''s innate instruction-following capabilities through two key components:
  (1) a hierarchical embedding prompt template with system and user-level instructions
  that enforces discriminative representation, and (2) self-aware hard negative sampling
  (SaHa) that uses the model''s own understanding to mine effective hard negatives
  while filtering out false negatives.'
---

# From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model

## Quick Facts
- arXiv ID: 2508.00955
- Source URL: https://arxiv.org/abs/2508.00955
- Reference count: 40
- Primary result: Zero-shot performance competitive with contrastively trained baselines using hierarchical prompts

## Executive Summary
This paper addresses the challenge of adapting Multimodal Large Language Models (MLLMs) for universal embedding tasks without the computational burden of large-scale contrastive pre-training. The proposed framework leverages the MLLM's innate instruction-following capabilities through two key components: a hierarchical embedding prompt template with system and user-level instructions that enforces discriminative representation, and self-aware hard negative sampling (SaHa) that uses the model's own understanding to mine effective hard negatives while filtering out false negatives. Experiments demonstrate that the hierarchical prompt alone achieves zero-shot performance competitive with contrastively trained baselines. When combined with SaHa for fine-tuning, the approach achieves state-of-the-art results on the MMEB benchmark, outperforming methods that rely on extensive contrastive pre-training.

## Method Summary
The method repurposes pre-trained MLLMs for discriminative embedding through hierarchical prompting and self-aware hard negative sampling. The hierarchical prompt template consists of a system-level prompt applied universally to enforce semantic compression ("summarize the provided image in one word"), combined with user-level representation prompts for queries. The SaHa mechanism preprocesses training data by clustering anchors with hard negatives selected based on semantic dissimilarity between the anchor query and the "owner queries" of candidate positives, reducing false negative contamination. Fine-tuning uses InfoNCE loss with LoRA adapters, and the approach is validated on the MMEB benchmark across 36 datasets including classification, VQA, retrieval, and visual grounding tasks.

## Key Results
- Zero-shot hierarchical prompt achieves 43.3 average Precision@1 on MMEB, competitive with contrastively trained baselines
- SaHa fine-tuning improves performance to 67.1 average Precision@1, outperforming simple in-batch negative baselines by 4.8 points
- State-of-the-art results on MMEB benchmark without requiring large-scale contrastive pre-training
- Significantly reduced training time compared to traditional contrastive learning approaches

## Why This Works (Mechanism)

### Mechanism 1: System Prompts as Global Embedding Controllers
- Claim: System prompts receive substantially higher attention weights than user prompts, enabling them to act as global controllers that enforce consistent embedding structure across all inputs.
- Mechanism: The hierarchical prompt template separates instructions into two levels: (1) a system-level prompt applied universally to queries and candidates that enforces semantic compression, and (2) a representation prompt appended to queries to reinforce the embedding objective. This exploits the differential attention patterns where system tokens receive 5-54× more attention than user tokens across model layers.
- Core assumption: The attention differential observed in Qwen2-VL generalizes to other decoder-based MLLMs.
- Evidence anchors:
  - [abstract] "hierarchical embedding prompt template with system and user-level instructions that enforces discriminative representation"
  - [Section III-B-1] "ϵsys is substantially greater than ϵusr, suggesting the system prompt acts as a global controller"
  - [corpus] Related work (Think Then Embed, U-MARVEL) explores generative context for embeddings but does not specifically analyze system prompt attention mechanisms.
- Break condition: If system prompts do not exhibit privileged attention in your target MLLM architecture, the mechanism may not transfer.

### Mechanism 2: Self-Aware Hard Negative Sampling via Query-Target Coherence
- Claim: Hard negatives mined from candidates whose "owner queries" are semantically dissimilar to the anchor query reduce false negative contamination compared to similarity-based thresholding.
- Mechanism: SaHa retrieves semantically similar candidates for an anchor, then identifies which training queries those candidates serve as positives for. It selects candidates whose owner queries are most dissimilar to the anchor, exploiting the intuition that similar queries share targets. This filters false negatives without external teacher models.
- Core assumption: Training data contains sufficient query-candidate mappings for the owner query identification to be meaningful.
- Evidence anchors:
  - [abstract] "self-aware hard negative sampling (SaHa) that uses the model's own understanding to mine effective hard negatives while filtering out false negatives"
  - [Section III-C] "semantically similar queries are more likely to share the same target and thus may not serve as reliable negatives"
  - [corpus] Related work (VIRTUE, Vela) addresses multimodal embeddings but relies on external models or does not address hard negative contamination explicitly.
- Break condition: If your training corpus has sparse query-candidate associations or high semantic overlap across distinct classes, owner query dissimilarity may not effectively filter false negatives.

### Mechanism 3: Zero-Shot Discriminative Capability via Prompt-Induced Compression
- Claim: Instruction-following capabilities from LLM pre-training can be redirected toward discriminative embedding through prompts that explicitly request compressed ("one word") representations.
- Mechanism: The system prompt instructs the model to "summarize the provided image in one word" or "describe the text in one word," forcing semantic compression into a fixed representation. Last-token pooling then extracts this compressed embedding. This avoids contrastive pre-training by repurposing existing instruction-following abilities.
- Core assumption: The MLLM has sufficiently strong instruction-following capabilities from its base pre-training.
- Evidence anchors:
  - [abstract] "leverages the MLLM's innate instruction-following capabilities"
  - [Section IV-B-3] "system prompt is the single most critical component; configurations that incorporate the system prompt consistently outperform those that omit it"
  - [corpus] U-MARVEL and Think Then Embed similarly explore leveraging MLLM capabilities but focus on generative context rather than prompt hierarchy.
- Break condition: If your MLLM lacks strong instruction-following (e.g., insufficient pre-training scale), the zero-shot baseline may underperform significantly.

## Foundational Learning

- **Contrastive Learning & InfoNCE Loss**
  - Why needed here: SaHa fine-tuning uses InfoNCE loss with hard negatives; understanding how negative sampling affects representation quality is essential for debugging training.
  - Quick check question: Can you explain why false negatives degrade contrastive learning objectives?

- **Attention Mechanisms in Decoder-Only Transformers**
  - Why needed here: The hierarchical prompt relies on system prompts receiving privileged attention; understanding causal attention patterns helps validate whether this transfers to your architecture.
  - Quick check question: Why might early layers vs. late layers show different attention distributions for system vs. user tokens?

- **Hard Negative Mining Trade-offs**
  - Why needed here: SaHa is positioned against traditional HNS methods; understanding computational costs and false negative risks helps evaluate when SaHa is appropriate.
  - Quick check question: What is the computational complexity difference between in-batch negatives and global hard negative mining?

## Architecture Onboarding

- **Component map:**
  - Vision encoder (ViT) -> Projection layer -> LLM backbone with hierarchical prompt template -> Last-token pooling -> L2 normalization

- **Critical path:**
  1. Validate system prompt attention patterns on your target MLLM before full implementation
  2. Implement hierarchical prompt template for zero-shot baseline
  3. Run SaHa preprocessing on training corpus (one-time offline)
  4. Fine-tune with LoRA using preprocessed clusters

- **Design tradeoffs:**
  - Pool multiplier (4-8 tested): Higher values increase candidate pool but show marginal degradation; default to 4
  - Hard negatives per anchor (k=7 vs k=15): Higher k slightly improves IND but similar OOD; k=7 is more efficient
  - Batch size vs. GradCache: Sub-batch size 16 (2B model) or 4 (7B model) with gradient caching for memory constraints

- **Failure signatures:**
  - Zero-shot baseline underperforms contrastively trained models -> Check if system prompt attention differential exists in your architecture
  - SaHa underperforms in-batch negatives -> Likely false negative contamination; reduce pool multiplier or inspect cluster assignments
  - Training instability -> Temperature τ=0.02 is sensitive; verify GradCache implementation if using large effective batch sizes

- **First 3 experiments:**
  1. **Attention Analysis:** Measure ϵsys/ϵusr ratio across layers on your target MLLM with sample inputs; if ratio < 5×, hierarchical prompt may not transfer effectively.
  2. **Zero-Shot Baseline:** Apply hierarchical prompt to MMEB evaluation without fine-tuning; compare against Table II baselines to validate prompt-only performance.
  3. **Ablation on k and pool multiplier:** Fine-tune with SaHa using k∈{7,15} and multiplier∈{4,6,8}; identify stable configuration before full training runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hierarchical prompting and SaHa framework effectively generalize to temporal modalities like video and audio?
- Basis in paper: [explicit] The authors state in the conclusion, "For future work, we plan to extend our framework to additional modalities such as video and audio."
- Why unresolved: The current study strictly validates the method on image-text pairs (MMEB benchmark); the impact of temporal dependencies on the static "hierarchical embedding prompt" and the SaHa clustering mechanism remains untested.
- What evidence would resolve it: Experimental results on video-text (e.g., MSR-VTT) and audio-text retrieval benchmarks showing competitive performance without architectural changes.

### Open Question 2
- Question: To what extent does the proposed discriminative fine-tuning degrade the model's innate generative capabilities?
- Basis in paper: [explicit] The conclusion notes the need to "investigate methods to preserve the model’s generative capabilities alongside its discriminative performance, particularly under limited computational resources."
- Why unresolved: While the method avoids joint training with generation loss (unlike baselines such as CAFe or VladVA), the paper does not quantify if "catastrophic forgetting" of generation skills occurs during the embedding-specific fine-tuning.
- What evidence would resolve it: A comparative evaluation on standard generation benchmarks (e.g., visual question generation) before and after applying the SaHa fine-tuning.

### Open Question 3
- Question: Does the "global controller" role of system prompts hold across diverse MLLM architectures with different attention mechanisms?
- Basis in paper: [inferred] The hierarchical prompt design is motivated by an attention analysis (Fig. 1) showing system prompts receive higher weights ($\epsilon_{sys} \gg \epsilon_{usr}$), but this analysis is limited to specific layers of the Qwen2-VL model.
- Why unresolved: The assumption that system prompts function as dominant anchors may not transfer to MLLMs with different attention patterns, sparse attention, or non-transformer backbones.
- What evidence would resolve it: A cross-architecture analysis replicating Figure 1 on models like InternVL or Mamba-based MLLMs to verify if the attention efficiency gap persists.

## Limitations

- The hierarchical prompt mechanism relies on system prompts receiving privileged attention weights (5-54× higher than user tokens) across all decoder-based MLLM architectures, which may not generalize beyond Qwen2-VL
- The approach assumes sufficient query-candidate mappings exist in training data for SaHa to effectively filter false negatives, potentially limiting applicability to sparse datasets
- The "one word" semantic compression instruction may not be optimal for all embedding tasks, restricting the method's generalizability to different domains

## Confidence

**High Confidence:**
- Zero-shot hierarchical prompt performance (43.3 avg Precision@1) - directly measured and reproducible with the provided prompt template
- SaHa fine-tuning improves over in-batch negative baseline (67.1 vs 62.3 avg Precision@1) - statistically significant improvement demonstrated across multiple datasets
- System prompt as critical component - ablation studies show consistent superiority when system prompt is included

**Medium Confidence:**
- Attention differential mechanism generalization across MLLM architectures - based on Qwen2-VL observations but requires validation on other models
- Self-aware hard negative filtering effectiveness - demonstrated empirically but the underlying semantic coherence assumption needs broader validation
- "One word" instruction optimality - shown effective but may not be universally optimal for all embedding tasks

**Low Confidence:**
- Computational efficiency claims relative to contrastive pre-training - training time comparisons not directly measured or controlled
- Long-term stability of SaHa-filtered embeddings - no temporal validation or cross-dataset generalization beyond MMEB scope

## Next Checks

1. **Architecture Transfer Test:** Measure attention weight ratios (ϵsys/ϵusr) for system vs user tokens across at least two additional MLLM architectures (e.g., GPT-4V, Gemini) to verify if the hierarchical prompt mechanism transfers beyond Qwen2-VL.

2. **False Negative Contamination Analysis:** Implement systematic evaluation of SaHa's false negative filtering by comparing mined negatives against ground truth negative labels in a subset of MMEB datasets, quantifying the reduction in false negative contamination relative to traditional similarity-based thresholding.

3. **Instruction Prompt Generalization:** Replace the "one word" semantic compression instruction with alternative formulations (e.g., "describe in three words," "create semantic summary") and measure impact on zero-shot and fine-tuned performance to determine instruction sensitivity and optimal formulations for different task types.