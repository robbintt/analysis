---
ver: rpa2
title: 'Deep Causal Behavioral Policy Learning: Applications to Healthcare'
arxiv_id: '2503.03724'
source_url: https://arxiv.org/abs/2503.03724
tags:
- provider
- clinical
- policy
- actions
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces deep causal behavioral policy learning (DC-BPL),
  a methodology that leverages large clinical behavioral models (LCBMs) to learn and
  emulate optimal clinical decision-making patterns. The approach addresses the challenge
  of capturing tacit clinical knowledge at scale by modeling provider behavior as
  a sequence-to-sequence learning task using transformer architectures.
---

# Deep Causal Behavioral Policy Learning: Applications to Healthcare

## Quick Facts
- arXiv ID: 2503.03724
- Source URL: https://arxiv.org/abs/2503.03724
- Authors: Jonas Knecht; Anna Zink; Jonathan Kolstad; Maya Petersen
- Reference count: 36
- Primary result: Transformer-based Large Clinical Behavioral Models (LCBMs) achieve 83.22% q-accuracy in predicting clinical actions when conditioned on high learned separation

## Executive Summary
This paper introduces Deep Causal Behavioral Policy Learning (DC-BPL), a framework that combines causal inference with transformer-based machine learning to learn and emulate optimal clinical decision-making patterns. The methodology leverages provider assignment as a conditional instrumental variable to identify causal effects of clinical actions on patient outcomes, then uses transformer models to learn provider-specific behavioral policies from EHR data. The authors demonstrate their approach through a proof-of-concept analysis using emergency department data, showing that their simple transformer model achieves strong predictive performance while introducing a novel "learned separation" metric that enables safer deployment by identifying high-confidence predictions.

## Method Summary
The DC-BPL framework consists of three main components: First, it identifies the causal impact of provider assignment on outcomes using instrumental variable methods, treating provider assignment as a conditional instrument that affects outcomes only through recorded clinical actions. Second, it learns provider-specific behavioral policies via transformer-based Large Clinical Behavioral Models (LCBMs) trained on clinical action sequences, where the transformer maps longitudinal EHR tokens to future actions using next-action prediction. Third, it combines these components to emulate optimal provider behavior for any patient type by estimating the optimal provider assignment function. The proof-of-concept uses a unimodal transformer architecture trained on UCSF emergency department order-set pairs, with recursive context windows and next-action prediction objectives.

## Key Results
- Simple transformer model achieves top-1 accuracy ranging from 24.7% to 57% depending on context length
- q-accuracy reaches 83.22% overall, with 99% top-10 accuracy when conditioning on high learned separation
- Action-level learned separation (Δπ) proves highly predictive of model accuracy and enables safer deployment
- Performance improves monotonically with context length, with steep accuracy gains at longer sequences

## Why This Works (Mechanism)

### Mechanism 1: Conditional Instrumental Variable for Provider Quality
The framework isolates the causal impact of provider quality on patient outcomes by treating provider assignment as a conditional instrument. If provider assignment is quasi-random conditional on observed patient history (the "provider information set"), then variation in outcomes between providers can be attributed to the providers' behavioral policies rather than patient severity. The core assumption requires conditional exchangeability and the exclusion restriction, meaning providers affect outcomes only through their recorded clinical actions. This fails if provider assignment is systematically biased by unmeasured patient severity.

### Mechanism 2: Sequence Modeling as Tacit Knowledge Encoding
A transformer (LCBM) pre-trained on clinical action sequences encodes implicit clinical reasoning ("tacit knowledge") into probabilistic policies. By training on the "next-action prediction" task across massive EHR data, the model learns the probability distribution π₀(Aₜ|I₀:ₜ), effectively compressing the decision-making patterns of providers into model weights. This assumes clinical expertise is revealed through action trajectories and can be approximated by a sequence-to-sequence function. The approach breaks down if clinical decisions rely heavily on unrecorded context not present in EHR action logs.

### Mechanism 3: Learned Separation for Calibration
The difference between the predicted probability of an action when it occurs versus when it does not ("learned separation") serves as a proxy for model certainty and accuracy. The model calculates Δπ(a) to filter predictions, where high separation indicates the model has successfully distinguished the specific clinical context requiring action a, reducing hallucinations. This assumes accurate predictions are characterized by high probability assignments to true actions and low assignments to false ones (separation), rather than just high raw probability. The metric may fail if the model is overconfident on spurious correlations.

## Foundational Learning

### Concept: Instrumental Variables (IV)
**Why needed:** The paper's entire causal claim rests on provider assignment acting as an instrument. You must understand exclusion restrictions to diagnose validity.
**Quick check:** Does the provider assignment directly affect patient health, or only through the orders/tests they initiate?

### Concept: Sequence-to-Sequence (Seq2Seq) Modeling
**Why needed:** The LCBM maps a history of actions/states to a future sequence of actions.
**Quick check:** How does the "next-token (action)" pre-training objective force the model to learn conditional probabilities?

### Concept: G-Computation / Counterfactuals
**Why needed:** To estimate the outcome Y had a patient been assigned to a different provider J=j.
**Quick check:** Can you compute the expected outcome if we intervene to set J = BestProvider, given the observed data?

## Architecture Onboarding

### Component map:
EHR tokens (Actions A, States X) -> Encoder-Decoder Transformer -> Next action prediction -> Provider-specific fine-tuning -> Optimal provider assignment

### Critical path:
Data Tokenization -> Pre-training (General Policy πₚᵣₑ) -> Fine-tuning (Provider Policy πʲ) -> Optimal Matching (d*ⱼ)

### Design tradeoffs:
- **Context Window:** Larger t increases accuracy (up to 400+ tokens) but increases compute and input space complexity
- **Unimodal vs. Multimodal:** The proof-of-concept uses unimodal action tokens; adding patient states X improves theoretical completeness but increases embedding complexity

### Failure signatures:
- **Low Learned Separation:** Model assigns similar probabilities to valid and invalid actions → indicates insufficient context or model capacity
- **Positivity Violation:** Rare providers with few data points cannot be fine-tuned reliably
- **Data Leakage:** Shared encounters between train/test splits inflate metrics

### First 3 experiments:
1. **Baseline Accuracy:** Train the unimodal transformer on order sequences and plot Mean Top-K accuracy vs. Context Length (t) to reproduce the steep accuracy gains seen in Section 4
2. **Separation Analysis:** Calculate Δπ for common vs. rare actions to verify that the model learns better separation for frequent events
3. **Provider Clustering:** Fine-tune the model on two distinct provider groups and measure the divergence in their predicted action distributions for the same patient history

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the DC-BPL framework be extended to identify the specific causal contributions of individual clinical actions rather than just the aggregate behavioral policy?
**Basis:** The authors state ongoing work aims to "discover which lower-dimensional features of a given provider's complex clinical action mechanism are causal drivers of patient outcomes," moving from identifying optimal providers to identifying key decisions.
**Why unresolved:** The current methodology identifies the causal effect of the provider (and their policy package) but does not isolate the causal effect of specific actions, presenting a "black box" of optimal care.

### Open Question 2
**Question:** How can the structural causal model and estimation strategy be modified to handle settings where multiple providers sequentially or jointly manage a patient?
**Basis:** The authors note the simplifying assumption that a single provider is responsible for decisions until the outcome is measured "is unlikely to hold in many healthcare settings," and list addressing this challenge as "ongoing work."
**Why unresolved:** The current model relies on Jₖ being constant, which fails in environments like emergency departments where handoffs occur, complicating the assignment of credit for outcomes.

### Open Question 3
**Question:** What are the asymptotic statistical properties (e.g., consistency) of transformer-based estimators when applied to high-dimensional behavioral policy learning?
**Basis:** The authors state, "To our knowledge there is no established asymptotic theory for transformer style networks as statistical estimators of this kind," relying instead on universal approximation results and empirical scaling laws.
**Why unresolved:** While transformers are universal approximators, formal statistical guarantees for the DC-BPL procedure's convergence and consistency are missing.

### Open Question 4
**Question:** What specific transportability assumptions and adjustments are required to safely deploy an LCBM trained on one health system's data to a different clinical setting?
**Basis:** The authors note that for practical applications like provider coaching, "additional assumptions (such as robust transportability over time and place) would be needed."
**Why unresolved:** Clinical practice patterns and patient populations vary significantly across institutions (distribution shift), and the current empirical work is limited to a single system (UCSF).

## Limitations

- The proof-of-concept relies on observational EHR data with potential unmeasured confounding, and the validity of provider assignment as a conditional instrument cannot be fully verified
- The framework assumes clinical actions are the only pathway through which providers affect outcomes (exclusion restriction), which may not hold if providers influence outcomes through unrecorded behaviors
- The unimodal action-only approach omits potentially important patient state information that could improve both predictive accuracy and causal validity

## Confidence

**High Confidence:**
- Transformer architecture can learn predictive clinical action sequences from EHR data
- Learned separation (Δπ) is a useful metric for identifying high-confidence predictions
- Provider assignment can be used as a conditional instrument under certain assumptions

**Medium Confidence:**
- LCBM approach captures tacit clinical knowledge effectively
- Causal framework can identify optimal provider assignments
- Action-level separation is highly predictive of model accuracy

**Low Confidence:**
- Exclusion restriction assumption holds perfectly in practice
- Provider assignment is truly quasi-random conditional on observed variables
- Unimodal approach captures sufficient clinical reasoning complexity

## Next Checks

1. **Instrument Validity Assessment:** Conduct falsification tests by applying the IV framework to outcomes known to be unaffected by clinical actions (like genetic markers) to verify the exclusion restriction holds approximately in practice.

2. **Multimodal Extension Validation:** Implement the multimodal version incorporating patient states X alongside actions A to quantify the performance gain and assess whether the additional complexity is justified by improved accuracy and causal validity.

3. **External Validation:** Test the pre-trained LCBM on an independent healthcare system's EHR data to evaluate generalization beyond the training institution and assess whether provider behavioral patterns are transferable across different clinical environments.