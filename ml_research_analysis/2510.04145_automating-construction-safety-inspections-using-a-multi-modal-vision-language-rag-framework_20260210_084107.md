---
ver: rpa2
title: Automating construction safety inspections using a multi-modal vision-language
  RAG framework
arxiv_id: '2510.04145'
source_url: https://arxiv.org/abs/2510.04145
tags:
- safety
- construction
- audio
- siteshield-image
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SiteShield, a multi-modal vision-language
  model (LVLM) framework that integrates image and audio inputs with Retrieval-Augmented
  Generation (RAG) to automate construction safety inspections. By retrieving relevant
  regulatory guidelines and safety standards in real time, SiteShield addresses the
  inefficiencies of traditional manual inspections, which rely on extensive human
  labor and inconsistent assessments.
---

# Automating construction safety inspections using a multi-modal vision-language RAG framework

## Quick Facts
- arXiv ID: 2510.04145
- Source URL: https://arxiv.org/abs/2510.04145
- Reference count: 0
- Primary result: SiteShield LVLM framework with RAG achieves F1=0.82 for automated construction safety inspection reports with regulatory citations

## Executive Summary
This study introduces SiteShield, a multi-modal vision-language model (LVLM) framework that integrates image and audio inputs with Retrieval-Augmented Generation (RAG) to automate construction safety inspections. By retrieving relevant regulatory guidelines and safety standards in real time, SiteShield addresses the inefficiencies of traditional manual inspections, which rely on extensive human labor and inconsistent assessments. SiteShield outperformed standard LVLMs without RAG, achieving an F1 score of 0.82, precision of 0.76, recall of 0.96, and hamming loss of 0.04. Incorporating audio data further improved regulatory compliance and reduced false positives. The framework enables rapid, accurate safety report generation, significantly enhancing inspection efficiency and compliance with construction safety regulations.

## Method Summary
SiteShield combines GPT-5-based vision-language models with LangChain's RAG framework, using Colpali document indexing to process the NSW Code of Practice on Construction Work into 16×16 pixel image patches with 128-dimensional embeddings. The system ingests construction site images and paired audio recordings (20-40 seconds each) with time/location metadata describing hazards. Semantic matching aligns image-audio pairs through timestamp filtering and cosine similarity on location vectors. For each matched pair, the combined text description is vectorized and used to retrieve relevant regulatory citations via MaxSim retrieval, which are then incorporated into safety inspection reports generated by the LVLM. The framework was evaluated on 25 construction site images from SODA and Kaggle Worksite Safety datasets.

## Key Results
- SiteShield achieved F1 score of 0.82, precision of 0.76, recall of 0.96, and hamming loss of 0.04 for regulatory compliance detection
- Audio integration improved inspection report quality compared to image-only input
- Standard LVLMs without RAG showed lower performance across all evaluation metrics
- The framework demonstrated capability for rapid, automated safety report generation with regulatory citations

## Why This Works (Mechanism)
The integration of multi-modal inputs with RAG enables the system to ground safety assessments in authoritative regulatory sources while maintaining context from visual and auditory site observations. By using Colpali's document chunking and embedding approach, SiteShield can efficiently retrieve relevant regulatory passages that match the specific safety concerns identified in site images and audio descriptions. The semantic matching of image-audio pairs through timestamp and location vectors ensures comprehensive hazard coverage, while the LVLM's generation capability produces coherent reports that cite appropriate regulations. This combination addresses the core challenge of manual inspections: inconsistent assessment and time-consuming cross-referencing with regulatory documents.

## Foundational Learning
- Vision-Language Models (VLMs): Neural networks that process and understand both visual and textual information simultaneously; needed for analyzing construction site images and generating safety reports; quick check: ability to generate accurate image captions from site photos
- Retrieval-Augmented Generation (RAG): Framework that retrieves relevant documents and incorporates them into generated responses; needed to ground safety assessments in actual regulatory requirements; quick check: retrieval of relevant Code of Practice sections for given safety scenarios
- Colpali document indexing: Method that segments documents into image patches and generates embeddings; needed for efficient regulatory document retrieval; quick check: successful retrieval of relevant regulatory passages using document patches
- Multi-modal semantic matching: Technique for aligning different data types (images, audio) based on shared semantic features; needed to ensure comprehensive hazard identification from complementary data sources; quick check: accurate pairing of site images with corresponding audio descriptions

## Architecture Onboarding

**Component Map:**
Construction site images and audio recordings -> LVLM (GPT-5) for description generation -> Colpali document indexer (Code of Practice) -> Vector database -> RAG retrieval (MaxSim) -> LVLM report generation -> Safety inspection report with regulatory citations

**Critical Path:**
1. Image and audio input collection with metadata
2. Semantic matching of image-audio pairs using timestamps and location vectors
3. Combined text description generation and vectorization
4. MaxSim retrieval of relevant regulatory passages
5. LVLM generation of safety inspection report with citations

**Design Tradeoffs:**
- Proprietary GPT-5 vs open-source LVLM alternatives: GPT-5 provides superior performance but limits reproducibility and accessibility
- Document chunking granularity: 16×16 pixel patches balance retrieval efficiency with semantic coherence but may miss contextual relationships across chunks
- Audio processing pipeline: ASR transcription introduces potential errors but enables integration of auditory hazard information
- Recall vs precision optimization: Current framework prioritizes high recall (0.96) at the cost of precision (0.76), accepting false positives to ensure comprehensive regulatory coverage

**Failure Signatures:**
- Image-audio mismatch due to timestamp drift or inaccurate location extraction results in incomplete hazard coverage
- High false positive rate in regulation retrieval (precision=0.57 for image-only) produces reports with irrelevant regulatory citations
- ASR errors in audio transcription degrade semantic matching accuracy between audio descriptions and site images
- Document indexing errors cause retrieval of irrelevant regulatory passages or miss applicable regulations

**3 First Experiments:**
1. Implement Colpali document chunking pipeline with configurable patch sizes and embedding dimensions to evaluate sensitivity to chunking parameters
2. Test semantic matching algorithm using synthetic image-audio pairs with controlled timestamp and location metadata to isolate matching accuracy from data quality issues
3. Conduct ablation study comparing retrieval quality metrics (precision, recall, F1) using different embedding models and top-K retrieval values on held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the retrieval mechanism be optimized to reduce false positives and improve precision (currently 0.57–0.76) without sacrificing the high recall rate (0.96) observed in multi-modal inspections?
- Basis in paper: [inferred] The results (Table 7) show that while recall is high (0.96), precision remains at 0.76 even with audio input, and the authors note that "many of the regulations in the retrieved citations are irrelevant" (Page 26).
- Why unresolved: The current framework prioritizes retrieving all potentially relevant regulations (high recall) but has not balanced this against the inclusion of irrelevant regulatory references (false positives).
- What evidence would resolve it: A modified RAG ranking algorithm or threshold setting that achieves a precision score above 0.90 while maintaining a recall above 0.90 on the same dataset.

### Open Question 2
- Question: How does SiteShield's performance and latency scale when applied to a significantly larger dataset of construction images and audio files compared to the 25-image case study?
- Basis in paper: [inferred] The validation was conducted using a small sample of "25 high-quality construction site images" selected from public datasets (Page 12).
- Why unresolved: Real-world construction sites generate vast amounts of data; the framework's efficiency and accuracy with larger, noisier datasets remain unverified.
- What evidence would resolve it: Evaluation results from a dataset containing hundreds or thousands of images and audio files, measuring both F1 scores and processing time per report.

### Open Question 3
- Question: To what extent does environmental background noise or linguistic accents in audio inputs degrade the accuracy of the semantic matching between audio descriptions and site images?
- Basis in paper: [inferred] The authors acknowledge that "text information generated by the audio conversion may not precisely match the actual audio content" (Page 8), yet the study relies on specific audio files that may not represent chaotic site conditions.
- Why unresolved: The semantic similarity matching relies on accurate transcription; the system's robustness against the high decibel levels and diverse accents typical of construction sites has not been tested.
- What evidence would resolve it: A comparative study of matching accuracy using audio samples with varying signal-to-noise ratios and speaker demographics.

## Limitations
- Proprietary GPT-5 model limits reproducibility and accessibility of the framework
- Small evaluation dataset (25 images) may not represent real-world construction site diversity
- Precision-recall tradeoff results in significant false positive rates in regulatory retrieval
- Audio processing pipeline details remain underspecified beyond general ASR claims

## Confidence
- LVLM + RAG framework effectiveness: Medium
- Regulatory compliance metrics: Medium
- Audio enhancement claims: Low
- Generalizability across sites/regulations: Low

## Next Checks
1. Implement Colpali-based document chunking and embedding pipeline with configurable parameters to test sensitivity to chunk size and embedding dimensions
2. Create synthetic audio transcripts paired with site images to test the timestamp/location matching algorithm independently of ASR quality
3. Conduct ablation study comparing retrieval quality (precision/recall) using different embedding models and top-K retrieval values on a held-out validation set