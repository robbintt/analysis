---
ver: rpa2
title: Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning
  Preference Optimization
arxiv_id: '2504.18080'
source_url: https://arxiv.org/abs/2504.18080
tags:
- arxiv
- reasoning
- medical
- japanese
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles the challenge of developing reliable medical
  large language models (LLMs) for Japanese, specifically addressing the instability
  of reasoning explanations that undermines trust in clinical settings. The authors
  propose a two-stage fine-tuning approach on Qwen2.5-72B: Continued Pretraining (CPT)
  on a comprehensive Japanese medical corpus for deep domain knowledge, followed by
  Reasoning Preference Optimization (RPO) to enhance stable and high-quality reasoning
  generation.'
---

# Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization

## Quick Facts
- **arXiv ID:** 2504.18080
- **Source URL:** https://arxiv.org/abs/2504.18080
- **Reference count:** 21
- **Key result:** Preferred-MedLLM-Qwen-72B achieves 0.868 accuracy on IgakuQA, maintaining performance when explanations are requested versus 11.5% drop in CPT-only baseline

## Executive Summary
This work addresses the critical challenge of developing trustworthy Japanese medical large language models by stabilizing reasoning explanations. The authors propose a two-stage fine-tuning approach: Continued Pretraining (CPT) on a comprehensive Japanese medical corpus for deep domain knowledge, followed by Reasoning Preference Optimization (RPO) to enhance stable and high-quality reasoning generation. Evaluated on the Japanese Medical Licensing Exam benchmark (IgakuQA), their model achieves state-of-the-art accuracy (0.868) while maintaining this performance when explanations are requested, unlike CPT-only models that show significant accuracy degradation (up to 11.5%).

## Method Summary
The authors developed a two-stage fine-tuning pipeline for medical LLMs. First, they applied Continued Pretraining (CPT) on a comprehensive Japanese medical corpus to build deep domain knowledge. Second, they applied Reasoning Preference Optimization (RPO) to enhance stable and high-quality reasoning generation. The approach was implemented on the Qwen2.5-72B architecture, resulting in the Preferred-MedLLM-Qwen-72B model. The RPO component specifically targets the instability issue where models show degraded performance when prompted to provide reasoning explanations.

## Key Results
- Achieved state-of-the-art accuracy of 0.868 on Japanese Medical Licensing Exam (IgakuQA) benchmark
- Outperformed proprietary models like GPT-4o (0.866) on the same benchmark
- Maintained accuracy at 0.868 when explanations were requested, compared to 11.5% drop in CPT-only baseline

## Why This Works (Mechanism)
The two-stage approach works by first establishing robust domain knowledge through CPT, then refining the model's reasoning behavior through RPO. CPT ensures the model has deep medical knowledge from the Japanese corpus, while RPO specifically optimizes for stable reasoning generation. This combination addresses the fundamental instability where medical LLMs often degrade in accuracy when asked to provide explanations, as the explanation generation process can interfere with the core answer prediction. By separating knowledge acquisition from reasoning behavior optimization, the model can maintain consistent performance across different prompting conditions.

## Foundational Learning
- **Continued Pretraining (CPT)**: Fine-tuning on domain-specific data to build deep knowledge; needed to establish medical expertise in Japanese context; quick check: verify domain-specific vocabulary and concepts are properly learned
- **Preference Optimization**: Reinforcement learning from human feedback; needed to align model outputs with desired reasoning quality; quick check: evaluate preference model accuracy on reasoning quality
- **Medical Licensing Exam Benchmarks**: Standardized evaluation for medical knowledge; needed to measure real-world clinical reasoning capability; quick check: ensure benchmark covers full medical curriculum scope
- **Reasoning Generation Stability**: Ability to maintain accuracy across different prompting conditions; needed for trustworthy clinical deployment; quick check: test accuracy consistency with/without explanation prompts
- **Japanese Medical Corpus**: Domain-specific dataset for CPT; needed to build culturally and linguistically appropriate medical knowledge; quick check: verify corpus coverage of Japanese medical curriculum
- **Multi-stage Fine-tuning**: Sequential application of different optimization objectives; needed to separately address knowledge and reasoning stability; quick check: ensure proper task separation between stages

## Architecture Onboarding
- **Component Map**: Qwen2.5-72B base -> CPT on Japanese medical corpus -> RPO for reasoning stability -> Preferred-MedLLM-Qwen-72B
- **Critical Path**: Base model fine-tuning sequence where CPT establishes medical knowledge foundation that RPO builds upon for stable reasoning
- **Design Tradeoffs**: CPT-only provides knowledge but unstable reasoning (11.5% accuracy drop), while RPO adds stability at potential cost of computational overhead
- **Failure Signatures**: Accuracy degradation when explanations are requested (11.5% drop in CPT-only), inconsistent reasoning quality across different prompts
- **3 First Experiments**: 1) Compare accuracy with/without explanation prompts to measure stability, 2) Evaluate on other medical reasoning tasks beyond IgakuQA, 3) Test cross-lingual generalization to non-Japanese medical questions

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Generalizability beyond Japanese medical domain and Qwen2.5-72B architecture remains uncertain
- Performance evaluation limited to single benchmark (IgakuQA) without broader medical reasoning task coverage
- Computational cost and scalability of RPO compared to alternative stabilization methods not characterized
- Preference signals for RPO not fully detailed, limiting replication and comparison to other techniques

## Confidence
- **High confidence**: RPO prevents accuracy degradation when explanations are requested (controlled comparison shows 11.5% drop vs no drop)
- **Medium confidence**: State-of-the-art performance claim (limited comparison set, no statistical significance testing)
- **Medium confidence**: CPT necessity assertion (no benchmarking against RPO-only or alternative domain adaptation approaches)

## Next Checks
1. Replicate stability findings on non-Japanese medical QA datasets and different base LLM architectures to test cross-domain and cross-model generalizability
2. Conduct ablation studies isolating CPT versus RPO contributions to determine whether both components are necessary for observed stability
3. Compare RPO against alternative preference optimization and calibration techniques on the same stability metrics to benchmark relative effectiveness