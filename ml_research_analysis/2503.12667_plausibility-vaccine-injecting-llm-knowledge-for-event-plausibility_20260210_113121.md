---
ver: rpa2
title: 'Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility'
arxiv_id: '2503.12667'
source_url: https://arxiv.org/abs/2503.12667
tags:
- association
- plausibility
- language
- adapters
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to improve semantic plausibility prediction
  by injecting latent knowledge from large language models using parameter-efficient
  fine-tuning. The authors automate data generation with GPT-4o to create datasets
  for physical properties (e.g., size, weight, temperature) and selectional association
  scores between verbs and their arguments.
---

# Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility

## Quick Facts
- arXiv ID: 2503.12667
- Source URL: https://arxiv.org/abs/2503.12667
- Authors: Jacob Chmura; Jonah Dauvet; Sebastian Sabry
- Reference count: 8
- One-line primary result: Property-based adapters improve plausibility prediction by 8-2% over direct fine-tuning, with selectional association providing marginal gains of 2-3%.

## Executive Summary
This paper presents a method to improve semantic plausibility prediction by injecting latent knowledge from large language models using parameter-efficient fine-tuning. The authors automate data generation with GPT-4o to create datasets for physical properties (e.g., size, weight, temperature) and selectional association scores between verbs and their arguments. They then train task adapters on these datasets and perform adapter fusion to compose latent semantic knowledge on top of pre-trained ALBERT embeddings. The approach is evaluated on two plausibility datasets, showing that property-based adapters improve performance by 8-2% compared to direct fine-tuning, with selectional association adapters providing additional marginal gains of 2-3%.

## Method Summary
The method injects semantic knowledge into transformer models using parameter-efficient adapters. GPT-4o generates property labels for 1,000 items across 10 physical properties and selectional association scores for verb-subject and verb-object pairs. These are used to pre-train 12 sequential bottleneck adapters (10 property + 2 association) on frozen ALBERT-base-v2 embeddings. AdapterFusion composes these representations, which are then fed to a single-layer MLP classifier trained on plausibility datasets (PEP-3K and 20Q). The property adapters ground entity representations in physical world attributes, while selectional association adapters capture how verbs semantically constrain their arguments.

## Key Results
- Property-based adapters improve plausibility prediction by 8-2% compared to direct fine-tuning
- Selectional association adapters provide additional marginal gains of 2-3%
- ALBERT-base-v2 with fused adapters achieves 0.65 accuracy on 20Q dataset (vs 0.55 baseline)
- P+V model outperforms N baseline (0.55 vs 0.65) on 20q dataset

## Why This Works (Mechanism)

### Mechanism 1: Property-Based Semantic Grounding
Injecting physical property knowledge through adapters improves plausibility discrimination over direct fine-tuning. Ten property adapters (size, weight, temperature, etc.) are pre-trained on GPT-4o-generated property labels, learning to ground entity representations in physical world attributes. These representations are fused before classification. Physical properties are causally coupled with plausibility labels; mutual information between object features and plausibility signals predictive utility. Evidence: property-based adapters improve performance by 8-2% compared to direct fine-tuning, and weight, size, texture exhibit higher mutual information with plausibility. Break condition: if property-relevant vocabulary is out-of-distribution or property labels from LLM are systematically biased.

### Mechanism 2: Selectional Association as Verb-Constraint Signal
Verb-argument association scores provide marginal gains by encoding how verbs semantically constrain subjects/objects. Two adapters learn verb-subject and verb-object selectional association scores (via KL divergence over semantic classes). These signals capture tendency for verbs to select certain argument types. Low association correlates with implausibility; the relationship is non-linear, warranting learned representations. Evidence: low association correlates with lower plausibility probability, and selectional association adapters provide additional marginal gains of 2-3%. Break condition: if verb-argument pairs are novel or rare (low corpus coverage), association estimates become unreliable.

### Mechanism 3: Non-Destructive Knowledge Composition via Adapter Fusion
Composing multiple adapter representations preserves pre-trained knowledge while injecting task-specific signals. Sequential bottleneck adapters (reduction factor 16) at every transformer layer; adapter fusion learns to weight and combine representations without modifying frozen ALBERT weights. Task-specific knowledge can be composed without catastrophic forgetting or interference. Evidence: adapter fusion composes latent semantic knowledge from each task on top of pre-trained AlBERT embeddings, and P+V outperforms N baseline on 20Q dataset. Break condition: if adapter representations conflict (e.g., property predicts plausible, association predicts implausible), fusion weights may not resolve correctly.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (Adapters)**
  - Why needed here: Understanding bottleneck adapters and fusion is essential to grasp how knowledge is injected without full fine-tuning.
  - Quick check question: Can you explain why a reduction factor of 16 means the adapter has ~6% of the hidden dimension parameters?

- **Selectional Preference / Association**
  - Why needed here: The verb-constraint mechanism relies on selectional association theory from computational linguistics.
  - Quick check question: Given a verb "eat" and object "rock," would you expect high or low selectional association? Why?

- **Mutual Information for Feature Selection**
  - Why needed here: The paper uses MI to justify which properties are most predictive of plausibility.
  - Quick check question: If weight has higher MI with plausibility than sentience, what does that imply for adapter prioritization?

## Architecture Onboarding

- **Component map:** ALBERT-base-v2 backbone (frozen) → 12 task adapters (10 property + 2 association) at every layer → AdapterFusion layer → Pooled representation → Single-layer MLP classifier → Binary plausibility output

- **Critical path:**
  1. Generate property/association datasets via GPT-4o prompting
  2. Pre-train each adapter on its auxiliary task (classification for properties, regression for association)
  3. Fuse adapters and freeze
  4. Train MLP classifier on plausibility data (PEP-3K or 20Q)

- **Design tradeoffs:**
  - ALBERT vs. BERT/RoBERTa: Table 5 shows ALBERT outperforms RoBERTa on 20q (0.65 vs. 0.63), but BERT edges ahead on PEP (0.57 vs. 0.54). Assumption: ALBERT's parameter efficiency may help in low-data regimes.
  - Adapter accuracy vs. downstream utility: Table 3 shows some adapters have low accuracy (Shape: 0.30), but fusion uses representations—not predictions—so low accuracy does not directly break downstream performance.
  - Combined training vs. single-dataset: Table 4 shows minimal benefit from combined training; hypothesis: plausibility data is too small relative to pre-training scale.

- **Failure signatures:**
  - If property adapter training diverges (e.g., F1 near 0 for multiple adapters), check label quality in GPT-4o-generated data.
  - If fusion provides no gain over property-only, verify association adapter MSE (Table 3: verb_subject 0.07, verb_object 0.05)—high MSE may indicate signal weakness.
  - If combined training underperforms, inspect vocabulary overlap between 20Q and PEP datasets.

- **First 3 experiments:**
  1. **Baseline replication**: Train N (direct MLP on ALBERT), P (property adapters only), P+V (property + association) on 20Q. Verify 0.55 → 0.63 → 0.65 accuracy progression.
  2. **Adapter ablation**: Remove low-MI property adapters (e.g., sentience, phase) and measure impact on plausibility accuracy to validate MI-based prioritization.
  3. **Backbone swap**: Replace ALBERT with BERT-base and compare P+V performance to test whether findings generalize (refer to Table 5 for expected range).

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the minimal set of adapters required to maximize plausibility prediction without degrading performance in a multi-task setting? The current study fuses all 12 trained adapters without investigating potential redundancy or negative interference between specific adapters. A systematic ablation study removing individual adapters or combinations thereof would identify the optimal subset for transfer learning.

- **Open Question 2**: Can advanced data generation techniques like prompt chaining or RAG produce higher-quality proxy datasets than direct GPT-4o prompting? The current method relies on automated single-step prompting, which may contain hallucinations or inconsistencies that more complex pipelines could mitigate. A comparative study of dataset fidelity and downstream plausibility accuracy when using chained prompts or retrieval-augmented generation versus the baseline prompting method would provide evidence.

- **Open Question 3**: How does the unverified nature of 70% of the LLM-generated property labels impact the reliability of the injected knowledge? While the LLM labels showed low divergence from human annotations on overlapping data, "70% of the labels unverified" remains a limitation. Human evaluation of the non-overlapping 70% of the dataset to determine the error rate of the LLM labels and its correlation with model accuracy would resolve this uncertainty.

## Limitations
- Reliance on GPT-4o-generated property labels with 70% unverified labels introduces potential bias and noise
- Ablation study only removes the least accurate adapter (shape), providing weak test of MI-based prioritization hypothesis
- Reported "negligible" difference between fused and directly fine-tuned models on PEP-3K lacks formal statistical testing

## Confidence
- **High**: The core mechanism of using parameter-efficient adapters to inject physical property knowledge is well-established and the 8-2% improvement over direct fine-tuning is clearly demonstrated.
- **Medium**: The marginal 2-3% gain from selectional association adapters is reported but the corpus evidence is weak (no direct replication in neighbors).
- **Low**: The effectiveness of MI-based adapter prioritization is suggested but not rigorously validated through comprehensive ablation studies.

## Next Checks
1. Perform a systematic ablation study removing multiple low-MI property adapters (e.g., sentience, phase, rigidity) to validate whether MI-based prioritization improves downstream plausibility accuracy.
2. Conduct statistical significance testing (e.g., paired t-tests) on the reported accuracy differences between fused and directly fine-tuned models on PEP-3K to verify the "negligible" difference claim.
3. Evaluate the impact of GPT-4o-generated label quality by creating an alternative property dataset with manually verified labels and measuring changes in adapter pre-training accuracy and downstream plausibility performance.