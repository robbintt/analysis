---
ver: rpa2
title: 'Tracing the Heart''s Pathways: ECG Representation Learning from a Cardiac
  Conduction Perspective'
arxiv_id: '2512.24002'
source_url: https://arxiv.org/abs/2512.24002
tags:
- leads
- learning
- lead
- clear
- clear-hug
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLEAR-HUG addresses limitations in ECG representation learning
  by incorporating cardiac conduction insights into a two-stage framework. The method
  introduces Conduction-LEAd Reconstructor (CLEAR) for pretraining, which uses sparse
  attention to reconstruct ECG signals guided by heartbeat conduction and lead-specific
  views.
---

# Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective

## Quick Facts
- arXiv ID: 2512.24002
- Source URL: https://arxiv.org/abs/2512.24002
- Reference count: 10
- Primary result: 6.84% improvement over state-of-the-art methods on six ECG tasks

## Executive Summary
CLEAR-HUG addresses limitations in ECG representation learning by incorporating cardiac conduction insights into a two-stage framework. The method introduces Conduction-LEAd Reconstructor (CLEAR) for pretraining, which uses sparse attention to reconstruct ECG signals guided by heartbeat conduction and lead-specific views. For fine-tuning, it employs a Hierarchical lead-Unified Group (HUG) head that mirrors clinical diagnostic workflows by integrating lead combinations progressively. Experiments across six tasks show a 6.84% improvement over state-of-the-art methods, with notable gains in few-shot settings (8.25% improvement with 1% training data).

## Method Summary
CLEAR-HUG is a two-stage framework for ECG representation learning. First, the Conduction-LEAd Reconstructor (CLEAR) pretrains a transformer encoder-decoder using a masked autoencoder approach with specialized sparse attention masks. The sparse attention enforces conduction-guided (same heartbeat, different leads) and view-guided (same lead, different heartbeats) reconstruction. Second, the Hierarchical lead-Unified Group (HUG) head fine-tunes the pretrained encoder using seven linear probes across three clinical lead groups (limb leads, augmented limb leads, precordial leads) that progressively combine information to mimic clinical diagnostic workflows. The model is trained on MIMIC-IV-ECG for pretraining and evaluated on PTB-XL, CPSC2018, and CSN for downstream tasks.

## Key Results
- 6.84% overall improvement over state-of-the-art methods across six ECG tasks
- 8.25% improvement in few-shot learning with 1% of training data
- Hierarchical lead grouping aligns with clinical diagnostic workflows

## Why This Works (Mechanism)

### Mechanism 1: Conduction-View Sparse Attention
CLEAR uses custom attention masks that constrain tokens to attend only to same-heartbeat tokens across different leads (conduction-guided) and same-lead tokens across different heartbeats (view-guided). This prevents interference from other heartbeats during reconstruction while preserving both synchronized electrical activity and lead-specific morphology.

### Mechanism 2: Heartbeat-Centric Tokenization
ECG signals are segmented into heartbeat tokens using R-peak detection rather than fixed time windows. This creates physiologically meaningful units that capture diagnostic signatures within individual heartbeats, analogous to how words function in language models.

### Mechanism 3: Hierarchical Lead Grouping Mimicking Clinical Workflow
The HUG head progressively integrates lead combinations in three stages: individual clinical groups (limb, augmented limb, precordial), pairwise group combinations, and final aggregation. This architecture mirrors how clinicians interpret ECGs, capturing condition-specific lead patterns (e.g., PVCs in V1-V2, RBBB patterns across different leads).

## Foundational Learning

- **Masked Autoencoder (MAE) for Sequences**: CLEAR is fundamentally an MAE variant; understanding how masking + reconstruction learns representations is prerequisite. Quick check: Can you explain why MAE uses high mask ratios (75-80%) and how the decoder reconstructs from sparse visible tokens?

- **Multi-head Self-Attention with Custom Masks**: The core innovation is a custom attention mask that enforces sparse, physiologically-motivated attention patterns. Quick check: Given attention mask M where M[i,j] = -∞ blocks attention from position i to j, how would you mask so position i only attends to positions {k₁, k₂}?

- **ECG Lead Configuration and Clinical Interpretation**: HUG's grouping follows clinical guidelines; understanding why leads are grouped (limb vs precordial, electrical views) is essential for debugging. Quick check: Why might a condition show abnormal patterns in V1-V2 but not in limb leads, and how would HUG capture this differently than a flat classifier?

## Architecture Onboarding

- **Component map**: Raw 12-lead ECG → heartbeat detection → tokenize to 15 beats/lead → 12 cls tokens → total 192 tokens → CLEAR Encoder (12-layer transformer with sparse attention) → CLEAR Decoder (4-layer transformer with full sparse mask) → HUG Head (7 linear layers in 3 stages: group → pairwise → aggregate) → final classifier

- **Critical path**: 1) Preprocessing: Downsample to 100Hz, detect heartbeats, create 15 tokens/lead; 2) Masking: Randomly mask 80% of beat tokens (cls tokens never masked); 3) Encoder forward: Apply Mc mask, produce latent representations; 4) Decoder forward: Apply M mask, reconstruct all 192 tokens; 5) Loss computation: Compare reconstructed beat tokens to ground truth; 6) Fine-tuning: Replace decoder with HUG head, train linear probes

- **Design tradeoffs**: Sparse vs full attention reduces interference but may miss long-range dependencies (ablation shows 17.4% gain on rhythm tasks with sparse); 12 cls tokens capture lead-specific info but increase parameters (incompatible with single cls token models); hierarchical HUG mimics clinical workflow but adds complexity (single-level grouping shows lower performance)

- **Failure signatures**: Reconstruction shows incorrect wave shapes (check view-guided attention); good shape but poor detail (check conduction-guided attention); few-shot performance drops (verify HUG head is being used); missing leads cause crash (handle gracefully, model works with 2 leads but degrades)

- **First 3 experiments**: 1) Reconstruction quality sanity check: Train CLEAR on small subset, visualize reconstructions for each lead. Compare CLEAR vs w/o Ic vs w/o Iv vs baseline MAE; 2) Attention mask validation: Log attention weights during training. Confirm cls token for Lead I attends only to Lead I beat tokens; 3) HUG ablation on single dataset: Run fine-tuning on PTBXL-Rhythm with simple averaging, weighted averaging, single-level grouping, full HUG. Expect progressive improvement.

## Open Questions the Paper Calls Out

- Can the HUG head be effectively adapted for ECG foundation models that utilize a single global `[cls]` token? (HUG is tightly coupled with CLEAR's 12 cls token design)
- Does the CLEAR pretraining framework generalize to regression-based downstream tasks (e.g., QT interval estimation) or signal generation tasks? (Only evaluated on classification tasks)
- Does the strict isolation of heartbeat tokens via sparse attention during pretraining limit the model's ability to capture long-range temporal dependencies characteristic of complex arrhythmias? (Inter-beat attention is explicitly excluded)
- How robust is the fixed hierarchical grouping strategy when applied to non-standard or reduced-lead ECG configurations? (HUG is hard-coded for 12-lead standard grouping)

## Limitations
- HeartBEAT alignment assumption: Model assumes perfect temporal alignment across leads, which may not hold in arrhythmias or noisy clinical data
- Lead-specific representation capacity: 12 cls token design choice lacks ablation comparison to alternatives in main text
- Hierarchical design complexity: 7-group structure adds complexity without sufficient evidence it's justified for all tasks

## Confidence
- **High**: Hierarchical lead grouping follows clinical practice and AHA guidelines; 6.84% overall improvement is statistically significant; few-shot learning advantage (8.25% with 1% data) demonstrates practical utility
- **Medium**: Conduction-view sparse attention effectiveness relies on synchronized heartbeat assumption; 12 cls token design is supported but lacks comparison to alternatives; clinical interpretability shown qualitatively but not quantitatively
- **Low**: Real-world deployment performance with missing leads, noisy signals, or extreme arrhythmias; computational efficiency compared to simpler baselines; long-term stability across different ECG acquisition systems

## Next Checks
1. **HeartBEAT Misalignment Robustness Test**: Introduce temporal misalignment (0-50ms) between leads in MIMIC-IV-ECG validation set and measure reconstruction quality and downstream task performance degradation
2. **Cross-Institutional Generalization Study**: Fine-tune CLEAR-HUG on PTB-XL and evaluate on hospital's proprietary ECG dataset with different acquisition systems and patient populations
3. **Sparse Attention Ablation Across All Tasks**: Implement full-attention variant of CLEAR and compare performance on all six tasks, not just rhythm tasks, to validate universal benefits of sparse attention