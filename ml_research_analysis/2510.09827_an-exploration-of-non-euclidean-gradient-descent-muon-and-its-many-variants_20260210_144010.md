---
ver: rpa2
title: 'An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants'
arxiv_id: '2510.09827'
source_url: https://arxiv.org/abs/2510.09827
tags:
- norm
- loss
- descent
- learning
- steepest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically explores the design space of non-Euclidean
  gradient descent for neural networks by unifying existing optimizers (Muon, Scion)
  under a common framework based on product norms. The authors formalize these methods
  as steepest descent with respect to specific norms on the entire parameter space
  and derive a new variant called MuonMax using a hybrid product norm.
---

# An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants

## Quick Facts
- **arXiv ID:** 2510.09827
- **Source URL:** https://arxiv.org/abs/2510.09827
- **Reference count:** 40
- **Primary result:** MuonMax-Momo is significantly more robust to learning rate tuning than standard optimizers while maintaining competitive performance on GPT models.

## Executive Summary
This paper systematically explores the design space of non-Euclidean gradient descent for neural networks by unifying existing optimizers (Muon, Scion) under a common framework based on product norms. The authors formalize these methods as steepest descent with respect to specific norms on the entire parameter space and derive a new variant called MuonMax using a hybrid product norm. Through extensive experiments on GPT models (124M and 774M parameters) with up to 6B tokens, MuonMax-Momo consistently matches or outperforms existing methods while being significantly more robust to learning rate tuning, achieving competitive performance across several orders of magnitude in learning rates. The paper also introduces stale nuclear norm approximations that reduce computational overhead by ~5% without performance degradation.

## Method Summary
The paper unifies Muon and Scion optimizers as instances of steepest descent methods with respect to product norms on the parameter space. Standard Muon uses constrained steepest descent (CSD) with a product norm where each weight matrix is normalized by its spectral norm, while Scion uses a modular norm that makes the loss Lipschitz continuous. The new MuonMax variant uses regularized steepest descent (RSD) with a hybrid product norm that scales updates by the dual norm of the momentum, providing automatic step size adaptation. The authors extend the model truncation (Momo) technique to general steepest descent methods, adaptively scaling updates based on a lower bound of the loss. A key innovation is the "stale nuclear norm approximation" that reuses previous step's dual norm values to avoid expensive SVD computations, reducing overhead by ~5% with negligible performance impact.

## Key Results
- MuonMax-Momo achieves competitive performance with standard optimizers while being robust across 2-3 orders of magnitude in learning rates
- The stale nuclear norm approximation reduces computational overhead by ~5% without degrading performance
- Model truncation (Momo) combined with steepest descent significantly increases robustness to hyperparameter choices
- MuonMax outperforms standard Muon in terms of learning rate sensitivity while maintaining similar final validation loss

## Why This Works (Mechanism)

### Mechanism 1: Model Truncation Robustness
Momo uses a lower bound on loss (F*) to construct more accurate loss models at each step, adaptively scaling step sizes to prevent destabilizing updates from poorly tuned learning rates. This works when F* is reasonably accurate (e.g., F* >= 0). Evidence shows MuonAdam-Momo and MuonMax-Momo have wider competitive learning rate ranges. Break condition: if F* is set too high, algorithm becomes overly conservative and may stall.

### Mechanism 2: Adaptive Step Sizing via Dual Norms
MuonMax uses regularized steepest descent where update magnitude scales by dual norm of momentum (||m_t||_*), causing automatic step size adaptation—larger steps for large gradients, smaller steps for small gradients. This provides local adaptation absent in fixed-norm CSD. Works when dual norm provides stable signal for scaling. Break condition: if nuclear norm becomes extremely large/small, updates could become unstable or vanish.

### Mechanism 3: Stale Nuclear Norm Approximation
Uses dual norm from previous step instead of recomputing expensive SVD, avoiding memory/time costs of storing/recomputing all LMO calls. Works when momentum changes slowly enough that previous value is good approximation. Evidence shows negligible performance impact while reducing wall-clock time. Break condition: assumption m_t ≈ m_{t-1} fails with very small momentum coefficient β.

## Foundational Learning

- **Steepest Descent**: Generalization of gradient descent where updates minimize linear approximation under specific norm constraints. Needed here because it provides the mathematical framework unifying Muon variants and enables the adaptive properties of MuonMax.

- **Product Norms**: Norms defined as products of individual parameter norms (e.g., ||W||_μ = ∏_ℓ ||W^ℓ||_ℓ). Needed to handle heterogeneous parameter types (weight matrices vs biases) in neural networks with appropriate normalization.

- **Model Truncation (Momo)**: Technique that uses lower bound F* on loss to construct adaptive step sizes. Needed to make steepest descent methods robust to hyperparameter choices by preventing overly aggressive updates.

- **Polar Decomposition**: Matrix factorization W = UP where U is orthogonal and P is positive semi-definite. Needed because Muon variants require polar factors for updates, and PolarExpress algorithm efficiently computes these in bf16.

- **Nuclear Norm**: Dual norm to spectral norm, equal to sum of singular values. Needed for dual norm calculations in MuonMax, though expensive to compute exactly.

## Architecture Onboarding

**Component Map:** GPT-2 model -> PolarExpress (polar decomposition) -> MuonMax-Momo update rule -> Validation loss computation

**Critical Path:** Weight matrices → Polar decomposition → Momentum update → Dual norm calculation (stale) → Parameter update → Loss evaluation

**Design Tradeoffs:** MuonMax trades computational complexity (dual norm calculations) for improved robustness to learning rates; stale approximation trades exactness for ~5% speedup; Momo trades conservative updates for wider hyperparameter basin.

**Failure Signatures:** Training loss divergence indicates poor F* calibration or excessive learning rate; oscillation in validation loss suggests stale approximation breaking down; plateauing performance may indicate overly conservative F* setting.

**First Experiments:**
1. Implement GPT-2 Small (124M) with standard tokenizer and train for 1B tokens using MuonMax-Momo with η_m=η_b=0.01, F*=3.2
2. Compare learning rate sensitivity sweep: scale both LRs by ρ∈{0.1,1,10,100} and plot final validation loss vs η_m
3. Ablation: compare exact vs stale nuclear norm computation on same training run to measure overhead and performance impact

## Open Questions the Paper Calls Out

- **Modular Norm Comparison:** How does steepest descent with respect to the modular norm compare empirically to the product norms proposed? The authors note they're not aware of any existing evaluation of steepest descent with respect to the modular norm.

- **Cross-Domain Generalization:** Does MuonMax-Momo retain its robustness and performance advantages in non-transformer architectures (e.g., CNNs, ViTs)? All experiments focus on GPT-2 architectures with language modeling objectives.

- **Stale Approximation Limits:** Under what conditions (e.g., low momentum β, non-stationary distributions) does the stale nuclear norm approximation fail? The authors justify it based on slow momentum changes but only validate with β=0.95.

## Limitations

- Core claims about MuonMax-Momo's robustness depend heavily on assumed F* value, which could break for different tasks/architectures
- All experiments focus on GPT-2 architectures with language modeling objectives, limiting generalization claims
- Stale nuclear norm approximation lacks theoretical guarantees about error accumulation over long training runs

## Confidence

**High Confidence:** Mathematical formalization of Muon/Sion as steepest descent methods is rigorous and well-supported. Implementation details (PolarExpress, stale approximations) are clearly specified.

**Medium Confidence:** Claims about MuonMax-Momo being "significantly more robust" are empirically supported but rely on specific hyperparameter settings and haven't been tested across domains.

**Low Confidence:** Claim that stale nuclear norm approximations have "negligible effect on performance" is based on a single ablation study without statistical significance testing.

## Next Checks

1. **F* Sensitivity Analysis:** Systematically vary F* across multiple orders of magnitude on both FineWeb and SlimPajama to determine actual basin of robustness and identify failure modes.

2. **Cross-Domain Generalization:** Apply MuonMax-Momo to non-language tasks (e.g., CIFAR-10 classification with CNNs) to test whether robustness claims hold beyond GPT-language modeling setting.

3. **Long-Horizon Stability:** Train MuonMax-Momo for >100B tokens on 774M parameter model to verify that stale nuclear norm approximation errors don't accumulate and cause performance degradation over extended training.