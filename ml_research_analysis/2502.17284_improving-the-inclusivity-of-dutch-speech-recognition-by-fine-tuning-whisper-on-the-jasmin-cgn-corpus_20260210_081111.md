---
ver: rpa2
title: Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning Whisper
  on the JASMIN-CGN Corpus
arxiv_id: '2502.17284'
source_url: https://arxiv.org/abs/2502.17284
tags:
- speech
- whisper
- recognition
- non-native
- children
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated fine-tuned Whisper models for Dutch ASR across
  diverse speaker groups (children, elderly, non-native) using the JASMIN-CGN corpus.
  The primary goal was to assess how age and linguistic background influence ASR performance.
---

# Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning Whisper on the JASMIN-CGN Corpus

## Quick Facts
- arXiv ID: 2502.17284
- Source URL: https://arxiv.org/abs/2502.17284
- Reference count: 7
- Zero-shot Whisper performance was poor across all groups (WER: 26.12-42.07%), but fine-tuning significantly improved accuracy with relative WER reductions of 81% for native children.

## Executive Summary
This study evaluates fine-tuned Whisper models for Dutch ASR across diverse speaker groups (children, elderly, non-native) using the JASMIN-CGN corpus. The primary goal was to assess how age and linguistic background influence ASR performance. Zero-shot Whisper performance was poor across all groups, but fine-tuning significantly improved accuracy, with relative WER reductions of 81% for native children, 72% for non-native children, 67% for non-native adults, and 65% for native elderly. Fine-tuning on one group also moderately improved recognition for other groups, with best results achieved by training on the comprehensive dataset. The findings demonstrate that training ASR models on underrepresented subpopulations substantially enhances inclusivity and accuracy for diverse speech patterns.

## Method Summary
The study fine-tuned Whisper large-v2 on the JASMIN-CGN corpus for Dutch ASR, using 10-fold cross-validation with 80% training, 10% validation, and 10% test splits per demographic group. Five groups were analyzed: native children (7-11y, 12h21m), native teenagers (12-16y, 12h21m), non-native children (7-14y, 12h21m), non-native adults (18-60y, 12h21m), and native elderly (65+y, 9h26m). Learning rate was 3×10⁻⁵ with 5 epochs, checkpointing every 0.1 epoch and selecting models by lowest validation WER. Experiments included four group-specific fine-tunings plus one unified training on all groups.

## Key Results
- Zero-shot Whisper WER ranged from 26.12% (native children) to 42.07% (non-native adults)
- Fine-tuning achieved 81% relative WER reduction for native children (5.45% final WER)
- Unified training on all groups performed at least as well as group-specific training for each demographic
- Cross-demographic transfer showed age variation improves generalizability more than nativeness variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a large pre-trained ASR model on demographically-specific data yields substantial WER reductions for underrepresented subpopulations.
- Mechanism: Whisper's pre-training on 680,000 hours of labeled speech provides a strong acoustic-linguistic foundation. Fine-tuning on JASMIN-CGN's spontaneous, interactive speech adapts the model's encoder-decoder attention patterns to acoustic variations (pitch, tempo, pronunciation) characteristic of children, elderly, and non-native speakers.
- Core assumption: The pre-trained representations are sufficiently general to be adapted with limited domain data (~10-12 hours per group).
- Evidence anchors:
  - [abstract] "Fine-tuned performance is remarkably better than zero-shot performance, achieving a relative reduction in WER of 81% for native children"
  - [section 5.1] "fine-tuning gave the four groups large WER improvements, with the largest improvement for native children (from 26.12% to 5.45%)"
  - [corpus] Neighbor papers on Whisper domain adaptation (BEARD) and accent-specific LoRA support the general finding that fine-tuning improves low-resource ASR, though this specific demographic breakdown is unique to this study.
- Break condition: If the target demographic exhibits acoustic features entirely absent from pre-training data, or if fine-tuning data contains systematic transcription errors, improvements may not replicate.

### Mechanism 2
- Claim: Fine-tuning on one demographic group produces transfer improvements to other groups, with asymmetric benefits.
- Mechanism: JASMIN-CGN's interactive speech style (short sentences, spontaneous patterns) creates a "corpus effect" — adapting to this speech register helps across demographics. Additionally, age-variation in training data appears to improve generalizability more than nativeness-variation.
- Core assumption: Shared speech register characteristics (spontaneous interaction) transfer across demographic boundaries more than acoustic matching alone would predict.
- Evidence anchors:
  - [section 5.3] "fine-tuning on one group tends to improve the recognition of all other groups... the recognition of non-native child and non-native adult speech improves appreciably even by fine-tuning a model on native children or native elderly"
  - [section 5.3] "age variation improves generalizability more than nativeness variation does"
  - [corpus] No direct corpus evidence for cross-demographic transfer in Dutch; related work on accented English (Sullivan et al. 2022, cited in paper) supports L1/L2 training benefits but not the age/nativeness asymmetry.
- Break condition: If the corpus speech style is substantially different (e.g., formal read speech), cross-demographic transfer may weaken.

### Mechanism 3
- Claim: Unified training on all demographic groups achieves performance at least as good as group-specific training for each demographic.
- Mechanism: Combined training exposes the model to greater acoustic and linguistic variation, improving robustness. The model learns shared representations that accommodate multiple speaker characteristics simultaneously.
- Core assumption: The dataset balancing issues (bias toward larger groups) do not critically degrade performance for smaller groups.
- Evidence anchors:
  - [section 5.2] "these results are on average at least as good as those on the diagonal, from which we conclude that training all four groups together does not yield worse results for a single group than training that single group alone"
  - [section 5.4] "the best results are achieved by training on a comprehensive dataset that includes both age variation and nativeness variation"
  - [corpus] Swedish Whispers paper confirms unified fine-tuning on large variable corpora improves mid-resource language ASR, supporting the general principle.
- Break condition: If one demographic is severely underrepresented in the combined dataset, its performance may degrade relative to single-group training.

## Foundational Learning

- Concept: **k-fold cross-validation for ASR evaluation**
  - Why needed here: With limited data (~10-12 hours per group), single splits produce unreliable WER estimates. Ten folds provide variance estimates and more stable performance assessment.
  - Quick check question: Can you explain why reporting mean WER across 10 folds is more trustworthy than a single train/test split for this dataset size?

- Concept: **Word Error Rate (WER) computation**
  - Why needed here: WER (substitutions + deletions + insertions / reference word count) is the primary metric. Understanding its components helps diagnose whether errors stem from mispronunciation, disfluency, or model limitations.
  - Quick check question: If a model transcribes "the cat sat" as "the cat sits," what is the WER and which error type occurs?

- Concept: **Transfer learning in sequence-to-sequence models**
  - Why needed here: Whisper fine-tuning leverages frozen pre-trained representations while updating model weights. Understanding encoder-decoder attention helps explain why acoustic adaptation transfers across demographics.
  - Quick check question: What is the difference between fine-tuning all model weights versus using adapters/LoRA, and why might full fine-tuning be preferred for domain adaptation?

## Architecture Onboarding

- Component map:
  - Raw audio -> Mel spectrogram (30-second windows) -> Log-mel filterbank features -> Whisper encoder (transformer blocks with self-attention) -> Decoder (autoregressive transformer with cross-attention) -> Text output

- Critical path:
  1. Data preparation: Clean transcriptions, segment long audio, verify encoding
  2. Fold creation: 10-fold split per demographic (80% train / 10% val / 10% test)
  3. Fine-tuning: Train 5 epochs, checkpoint every ~0.1 epoch
  4. Checkpoint selection: Choose lowest validation WER per fold
  5. Evaluation: Test-set WER computation with proper language token to avoid hallucinations

- Design tradeoffs:
  - Full fine-tuning vs. LoRA: Full fine-tuning used here for maximum adaptation; LoRA would reduce compute but may limit performance gains
  - Group-specific vs. unified training: Unified training is more practical but risks demographic imbalance; the study found unified training sufficient
  - Read vs. interactive speech: JASMIN's spontaneous speech better matches real-world use but differs from Whisper's LibriSpeech/Common Voice pre-training

- Failure signatures:
  - Hallucinated Unicode replacement character (U+FFFD): Occurs when language token not specified in evaluation pipeline; resolved by explicitly setting Dutch language
  - High zero-shot WER on non-native adults (42.07%): Indicates Whisper's pre-training underrepresents this demographic
  - Lower transfer from non-native to native groups: Suggests nativeness adaptation is more asymmetric than age adaptation

- First 3 experiments:
  1. **Baseline replication**: Run zero-shot Whisper-large-v2 on a held-out JASMIN fold to reproduce reported WERs (26-42% range) before any fine-tuning.
  2. **Single-group fine-tuning**: Fine-tune on one demographic (e.g., native children) with 10-fold cross-validation, checkpointing every 0.1 epoch; verify diagonal performance improvement (~5-14% WER target).
  3. **Cross-demographic evaluation**: Take the model from Experiment 2 and evaluate on all four demographic test sets; confirm off-diagonal transfer pattern (moderate improvement for non-native groups, minimal for other native groups).

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size per demographic group (10-12 hours) limits generalizability despite 10-fold cross-validation
- Native teenager group mentioned but not analyzed separately, creating ambiguity about age-related patterns
- Cross-demographic transfer benefits lack direct corpus-level validation and experimental isolation

## Confidence
- **High confidence**: Fine-tuning Whisper on demographic-specific data substantially improves WER for underrepresented groups (81% reduction for native children)
- **Medium confidence**: Unified training on all groups performs as well as or better than group-specific training
- **Medium confidence**: Asymmetric transfer finding (age variation improves generalizability more than nativeness variation)

## Next Checks
1. **Speaker independence verification**: Audit the 10-fold splits to confirm no speaker overlap between train, validation, and test sets within each demographic group.
2. **Group-specific performance isolation**: Conduct an additional experiment training on native teenagers separately to verify the claimed performance patterns.
3. **Transfer asymmetry validation**: Design a controlled experiment isolating age and nativeness effects by training on matched subsets to directly test the asymmetric transfer hypothesis.