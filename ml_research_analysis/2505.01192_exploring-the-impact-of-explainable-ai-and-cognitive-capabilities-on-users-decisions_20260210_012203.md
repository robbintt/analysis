---
ver: rpa2
title: Exploring the Impact of Explainable AI and Cognitive Capabilities on Users'
  Decisions
arxiv_id: '2505.01192'
source_url: https://arxiv.org/abs/2505.01192
tags:
- https
- explanations
- users
- confidence
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how AI assistance (prediction, confidence,
  accuracy) and explanation styles (example-based, feature-based, rule-based, counterfactual)
  affect user accuracy, reliance, and cognitive load in loan approval decisions. A
  user study (N=288) revealed that high AI confidence increases reliance and reduces
  cognitive load, while counterfactual explanations, despite being less understandable,
  enhance overall accuracy and decrease cognitive load when AI predictions are correct.
---

# Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions

## Quick Facts
- **arXiv ID:** 2505.01192
- **Source URL:** https://arxiv.org/abs/2505.01192
- **Reference count:** 40
- **Primary result:** High AI confidence increases reliance and reduces cognitive load; counterfactual explanations improve accuracy despite being less understandable.

## Executive Summary
This study investigates how AI assistance (prediction, confidence, accuracy) and explanation styles (example-based, feature-based, rule-based, counterfactual) affect user accuracy, reliance, and cognitive load in loan approval decisions. A user study (N=288) revealed that high AI confidence increases reliance and reduces cognitive load, while counterfactual explanations, despite being less understandable, enhance overall accuracy and decrease cognitive load when AI predictions are correct. Surprisingly, feature-based explanations did not improve accuracy compared to other conditions. Additionally, no significant differences in accuracy or cognitive load were found between low and high Need for Cognition individuals, challenging assumptions about personality-based differences in AI-assisted decision-making. These findings highlight the need for personalized, hybrid XAI interfaces combining diverse explanation styles.

## Method Summary
The study used a binary classification loan approval task with a Random Forest model (83% accuracy on test set) trained on the Kaggle "Loan Prediction Problem Dataset" (614 samples, 12 features). User study (N=288) employed a one-stage decision paradigm where participants saw loan attributes, AI information (prediction, confidence, accuracy), and one of four explanation styles (example-based, feature-based, rule-based, counterfactual) before making their decision. Confidence was estimated via Shannon entropy normalized to 0-100 scale (low < 44.3, high > 61.6). Sixteen instances were selected balanced on AI correctness, confidence, and class. Outcomes measured: accuracy, reliance (whether user agreed with AI), and cognitive load via SEQ.

## Key Results
- High AI confidence significantly increases user reliance on AI predictions and reduces cognitive load.
- Counterfactual explanations enhance overall accuracy and decrease cognitive load when AI predictions are correct, despite being rated as less understandable than other styles.
- No significant differences in accuracy or cognitive load were found between low and high Need for Cognition individuals, suggesting task complexity may suppress NFC's predictive power.
- Feature-based explanations (SHAP) did not improve accuracy compared to other explanation styles.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High AI confidence acts as a heuristic cue that increases user reliance and reduces cognitive load.
- Mechanism: Users treat the stated confidence score as a proxy for correctness. When confidence is high, they allocate less effort to independent reasoning and default to the AI suggestion, effectively outsourcing the cognitive work to the system.
- Core assumption: Users interpret confidence scores as calibrated probabilities and dynamically adjust their cognitive engagement based on this signal.
- Evidence anchors:
  - [abstract] "Our findings show that high AI confidence significantly increases reliance on AI while reducing cognitive load."
  - [section 5.2.1] "The results of the analysis showed a significant effect (Log-Odds = 1.22... p < .01) of high AI confidence in increasing users' reliance on AI than low AI confidence."
  - [corpus] Prior work (Zhang et al., Rechkemmer & Yin) consistently links high AI confidence to greater user trust.
- Break condition: If confidence estimates are miscalibrated (e.g., high confidence for wrong predictions), this mechanism leads to systematic over-reliance and error.

### Mechanism 2
- Claim: Counterfactual explanations improve accuracy and reduce cognitive load when the AI is correct, despite being rated as less understandable than other styles.
- Mechanism: Counterfactuals provide actionable, "what-if" scenarios that allow users to verify the AI's logic through contrastive reasoning. This may guide users toward correct outcomes more effectively than feature importance plots, even if the format is less intuitive.
- Core assumption: The actionable nature of counterfactuals enables more effective verification of the AI's decision boundary than feature contribution scores, which can create an "illusion of explanatory depth."
- Evidence anchors:
  - [abstract] "...counterfactual explanations, despite being less understandable, enhance overall accuracy and decrease cognitive load when AI predictions are correct."
  - [section 5.3] "Counterfactual explanation interaction with AI correct predictions leads to an increase in reliance... and a decrease in users' cognitive load."
  - [corpus] Xuan et al. (2025) suggest explanations perceived as "easy to understand" can be both intelligible and misleading.
- Break condition: If a task is highly complex or users lack domain knowledge, counterfactuals may confuse rather than clarify, especially if proposed changes are large or implausible.

### Mechanism 3
- Claim: In complex, high-stakes tasks, the Need for Cognition (NFC) trait fails to predict differences in user accuracy or cognitive load.
- Mechanism: The cognitive demands of a complex, unfamiliar domain (e.g., loan approval with tabular data and monetary stakes) may overwhelm the typical expression of NFC, causing both low- and high-NFC users to adopt similar heuristic strategies (e.g., prioritizing loan attributes and explanations over AI information).
- Core assumption: The study's specific task complexity and domain novelty suppress the predictive power of NFC for this context.
- Evidence anchors:
  - [abstract] "...no significant differences in accuracy or cognitive load were found between low and high Need for Cognition individuals..."
  - [section 6.3] "This suggests that as task complexity increases, NFC may lose its predictive ability to differentiate individual behaviors."
  - [corpus] Studies in simpler domains showed NFC differences (Millecamp et al.), but recent work with LLMs/RL (Buçinca et al. 2024) also found NFC may not consistently predict outcomes.
- Break condition: In simpler tasks or with expert users, NFC differences may re-emerge, with high-NFC users showing greater engagement with explanations.

## Foundational Learning

- Concept: **Calibrated Confidence Estimation**
  - Why needed here: Confidence drives reliance. An engineer must understand that a confidence score is only a reliable signal if the model is calibrated (i.e., 80% confidence ≈ 80% accuracy). Uncalibrated confidence misleads users.
  - Quick check question: If a model outputs 90% confidence but is only correct 60% of the time, will user reliance increase or decrease accuracy?

- Concept: **Cognitive Load in Interface Design**
  - Why needed here: The paper measures cognitive load as a core outcome. Different explanation styles and information densities impose different cognitive costs, directly affecting user performance and experience.
  - Quick check question: Which likely imposes higher cognitive load: a single prediction label or a table of three counterfactual instances with highlighted differences?

- Concept: **Explanation Style Trade-offs**
  - Why needed here: The finding challenges the default use of feature-based explanations. An engineer must weigh understandability against objective performance; "easy to understand" does not mean "improves accuracy."
  - Quick check question: A user finds SHAP plots intuitive. According to the paper, does this guarantee better accuracy than using counterfactuals?

## Architecture Onboarding

- Component map: Random Forest Classifier (model) -> Confidence estimator (entropy-based) -> Explanation generator (SHAP, Anchors, DiCE, k-NN) -> User interface (loan attributes + AI info + explanation) -> Decision capture -> Metrics (accuracy, reliance, cognitive load).
- Critical path: Model training -> Confidence & explanation generation -> Instance selection (balanced for confidence, correctness, class) -> User interaction -> Decision & metric logging.
- Design tradeoffs: Displaying more AI info (prediction + confidence + accuracy) improves transparency but risks over-reliance. Counterfactuals may boost accuracy but reduce perceived understandability. A one-stage decision paradigm (simultaneous AI and user input) limits the ability to isolate user reasoning.
- Failure signatures: High over-reliance on incorrect, high-confidence predictions; high cognitive load with no accuracy gain (indicative of confusing explanations); no performance difference between AI-assisted and non-AI conditions.
- First 3 experiments:
  1. **Confidence Calibration A/B Test:** Provide calibrated confidence to one group, uncalibrated to another. Measure impact on over-reliance for incorrect predictions.
  2. **Hybrid Explanation Interface:** Combine a feature-based summary with a single actionable counterfactual. Compare accuracy and reliance against single-style conditions.
  3. **Two-Stage Decision Protocol:** Force an initial independent user decision before showing AI advice. Test if this elicits NFC-based behavioral differences absent in the one-stage setup.

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses a simulated loan-approval task, which may not fully capture real-world decision-making dynamics and stakes.
- NFC differences were not observed, but this may be due to the specific task complexity or the one-stage decision paradigm rather than a generalizable finding about NFC's irrelevance in AI-assisted decisions.
- Counterfactual explanations improved accuracy but were rated as less understandable, raising questions about long-term user trust and adoption.
- The Random Forest model and entropy-based confidence may not generalize to other ML algorithms or calibration methods.

## Confidence
- **High confidence:** High AI confidence increases reliance and reduces cognitive load; counterfactual explanations improve accuracy when AI is correct.
- **Medium confidence:** Feature-based explanations do not outperform other styles; NFC does not predict differences in this specific task.
- **Low confidence:** These findings generalize to other domains, ML models, or decision contexts.

## Next Checks
1. Replicate the study with a two-stage decision protocol (user decision first, then AI advice) to test if NFC differences emerge when users commit to an initial judgment.
2. Conduct a calibration-focused experiment: compare calibrated vs. uncalibrated confidence scores to measure the impact of miscalibration on over-reliance for incorrect predictions.
3. Test hybrid explanation interfaces (e.g., feature summary + single counterfactual) to determine if combining styles improves both understandability and accuracy.