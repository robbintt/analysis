---
ver: rpa2
title: 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training
  Recipe'
arxiv_id: '2509.18154'
source_url: https://arxiv.org/abs/2509.18154
tags:
- wang
- zhang
- reasoning
- data
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MiniCPM-V 4.5 introduces a unified 3D-Resampler model architecture\
  \ for efficient image and video encoding, achieving up to 96\xD7 compression rate\
  \ compared to existing models. This allows the 8B parameter model to process high-resolution\
  \ images and long videos with significantly reduced GPU memory and inference time."
---

# MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe

## Quick Facts
- arXiv ID: 2509.18154
- Source URL: https://arxiv.org/abs/2509.18154
- Reference count: 40
- Primary result: Achieves state-of-the-art performance among models under 30B parameters while using 46.7% GPU memory and 8.7% inference time of Qwen2.5-VL 7B on VideoMME

## Executive Summary
MiniCPM-V 4.5 introduces a unified 3D-Resampler architecture that compresses video frames up to 96× while maintaining performance, enabling efficient processing of high-resolution images and long videos on an 8B parameter model. The model employs a novel unified learning paradigm for document knowledge and OCR by dynamically corrupting text regions and asking the model to reconstruct them, eliminating reliance on external parsers. A hybrid reinforcement learning strategy with mixed short/long reasoning modes enables flexible control and mutual performance enhancement, achieving state-of-the-art results on OpenCompass benchmarks while significantly reducing computational resources.

## Method Summary
MiniCPM-V 4.5 uses a three-module architecture (visual encoder → unified 3D-Resampler → LLM decoder) trained through a three-stage pre-training pipeline with strategic freezing, followed by supervised fine-tuning and hybrid reinforcement learning. The 3D-Resampler performs joint spatial-temporal compression using cross-attention with learnable query tokens augmented with 2D spatial and temporal positional embeddings. Document learning uses dynamic text corruption at three intensity levels to unify OCR and knowledge acquisition under a single reconstruction objective. The hybrid RL strategy alternates between short and long reasoning modes during training, using GRPO with rule-based, probability-based, and calibrated preference rewards to enable cross-mode generalization.

## Key Results
- Outperforms GPT-4o-latest and significantly larger open-source models (Qwen2.5-VL 72B) on OpenCompass benchmarks
- Achieves state-of-the-art performance among models under 30B parameters
- Uses only 46.7% GPU memory and 8.7% inference time of Qwen2.5-VL 7B on VideoMME
- 96× compression rate for video encoding without proportional performance loss
- Hybrid RL achieves 77.1 OpenCompass score with 3.1B training tokens vs. 77.0 with 4.4B tokens for long-only

## Why This Works (Mechanism)

### Mechanism 1
Joint spatial-temporal compression via 3D-Resampler enables up to 96× reduction in visual token count without proportional performance loss. The 3D-Resampler extends 2D cross-attention by adding temporal positional embeddings to learnable query tokens, allowing the model to jointly compress redundant information across adjacent video frames. Video frames are grouped into "packages" and cross-attention produces fixed-length outputs per package, exploiting both spatial and temporal redundancy. Core assumption: Adjacent video frames contain highly redundant visual information that can be lossily compressed without degrading task-relevant semantics.

### Mechanism 2
Dynamic corruption of text regions in document images unifies OCR and knowledge acquisition under a single reconstruction objective, eliminating reliance on brittle external parsers. For each document, text regions are corrupted at three intensity levels: low corruption forces pure OCR, moderate corruption requires integrating noisy visual cues with context, and high corruption forces pure contextual inference from layout and surrounding content. The model learns to adaptively switch between recognition and inference modes based on visibility. Core assumption: The model has sufficient prior knowledge to infer masked content from multimodal context when visual cues are unavailable.

### Mechanism 3
Hybrid reinforcement learning with mixed short/long reasoning rollouts enables cross-mode generalization and reduces training cost compared to single-mode training. During GRPO-based RL, rollouts randomly alternate between short reasoning mode and long reasoning mode. The shared optimization forces foundational perceptual and cognitive skills to transfer between modes. Reward shaping combines rule-verified rewards, probability-based rewards, and calibrated preference rewards. Core assumption: Short and long reasoning share transferable underlying skills that can mutually reinforce through joint training.

## Foundational Learning

- **Cross-attention and Resampler Architectures**
  - Why needed here: The 3D-Resampler relies on cross-attention between learnable query tokens and visual features to achieve compression. Understanding how query tokens aggregate information is essential for debugging compression quality.
  - Quick check question: Can you explain how a learnable query token in cross-attention differs from standard self-attention, and why fixed query count enables variable-length input compression?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL stage uses GRPO without KL/entropy loss for stability. Understanding relative advantage estimation within groups is critical for tuning reward shaping and rollout strategies.
  - Quick check question: How does GRPO's group-based advantage normalization differ from standard PPO's advantage estimation, and what stability benefits does it provide?

- **Vision Encoder Pre-training and Freezing Strategies**
  - Why needed here: The 3-stage pre-training selectively unfreezes modules (Resampler → Vision Encoder → LLM). Knowing when and why to freeze components prevents catastrophic forgetting and optimizes compute.
  - Quick check question: Why might freezing the LLM decoder during Stage 2 (vision encoder unfreezing) prevent degradation of language modeling capability?

## Architecture Onboarding

- **Component map:**
  Visual Encoder (SigLIP-based) → Image Partitioning/Video Packing → Unified 3D-Resampler → LLM Decoder

- **Critical path:**
  1. Video/image input → partitioning/packing
  2. Visual encoder → per-frame/per-slice features
  3. 3D-Resampler → compressed tokens (this is the efficiency bottleneck mitigation point)
  4. LLM decoder → response generation
  5. RL reward computation → GRPO update (rule-based + probability-based + preference)

- **Design tradeoffs:**
  - Token count vs. fine-grained detail: Fewer query tokens improve efficiency but may lose subtle visual details (ablation shows 3D-Resampler achieves higher performance with 1/3 tokens of 2D baseline)
  - Compression rate vs. temporal precision: Aggressive temporal compression (6×) enables long videos but may blur rapid motion sequences
  - Short vs. long reasoning allocation: 50/50 split during RL provides best efficiency-performance balance; long-only is compute-heavy, short-only sacrifices complex reasoning

- **Failure signatures:**
  - Video understanding degrades on fast-motion content: Likely temporal over-compression → increase package size or frame rate
  - Document OCR hallucinates on degraded inputs: Corruption level too high for available context → reduce masking ratio or add more document pre-training
  - RL training unstable with long reasoning: Preference model misaligned on CoT → apply preference reward only to final answer segment
  - Cross-mode transfer not occurring: Rollout mode mixing insufficient → ensure random alternation during GRPO sampling

- **First 3 experiments:**
  1. Validate 3D-Resampler compression vs. quality tradeoff: Encode sample videos at varying query token counts (32, 64, 128 per package) and measure performance on VideoMME vs. inference time. Expect diminishing returns above 64 tokens.
  2. Test corruption level sensitivity for document learning: Train on document subset with corruption ratios varied (0.3, 0.5, 0.7 masked regions) and evaluate OCRBench vs. MMMU to find optimal balance between recognition and inference.
  3. Probe hybrid RL mode transfer: Run ablation with different short/long reasoning ratios (0/100, 25/75, 50/50, 75/25, 100/0) and plot training tokens vs. OpenCompass score to verify cross-mode generalization hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
Does the 96× compression rate achieved by the unified 3D-Resampler result in irreversible information loss for fine-grained tasks requiring high spatio-temporal fidelity, such as medical video diagnosis or high-speed action recognition? The paper evaluates general video understanding benchmarks but does not assess domains where subtle temporal changes are critical diagnostic features.

### Open Question 2
Does the dynamic visual corruption strategy for document learning generalize effectively to documents with severe geometric distortions (e.g., crumpling, perspective warp) which may not be adequately simulated by noise and masking? The method focuses on text visibility but does not explicitly address structural deformations of the document layout itself.

### Open Question 3
Can the model be trained to autonomously switch between short and long reasoning modes based on problem difficulty, rather than relying on prompt instructions? The current architecture requires external specification of the reasoning mode despite joint optimization improving both modes.

## Limitations

- Architecture details for visual encoder and LLM decoder are unspecified, requiring architectural guesswork for exact reproduction
- Training data specifics including dataset sizes, batch sizes, and step counts per stage are not provided
- Document corruption levels, corruption ratios, and mask types are unspecified
- Reward engineering details including format reward and repetition penalty definitions are missing
- Prompt templates distinguishing short vs long reasoning modes are not provided

## Confidence

**High confidence (⚡):** The 3D-Resampler achieves significant compression (96× for video, 16× for images) while maintaining or improving performance, as validated by ablation showing higher accuracy with fewer tokens than 2D baseline.

**High confidence (⚡):** The unified document learning paradigm enables OCR and knowledge acquisition without external parsers, demonstrated by gains on MMMU, AI2D, and OCRBench.

**Medium confidence (⚡):** The hybrid RL strategy achieves cross-mode generalization, as evidenced by similar OpenCompass scores between hybrid and long-only training despite using 70% fewer training tokens.

**Medium confidence (⚡):** Overall performance superiority over GPT-4o-latest and Qwen2.5-VL 72B is reported, but exact evaluation conditions are not fully specified.

## Next Checks

1. Validate 3D-Resampler compression vs. quality tradeoff: Encode sample videos at varying query token counts (32, 64, 128 per package) and measure performance on VideoMME vs. inference time. Expect diminishing returns above 64 tokens.

2. Test corruption level sensitivity for document learning: Train on document subset with corruption ratios varied (0.3, 0.5, 0.7 masked regions) and evaluate OCRBench vs. MMMU to find optimal balance between recognition and inference.

3. Probe hybrid RL mode transfer: Run ablation with different short/long reasoning ratios (0/100, 25/75, 50/50, 75/25, 100/0) and plot training tokens vs. OpenCompass score to verify cross-mode generalization hypothesis.