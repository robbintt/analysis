---
ver: rpa2
title: 'ActiveCQ: Active Estimation of Causal Quantities'
arxiv_id: '2509.24293'
source_url: https://arxiv.org/abs/2509.24293
tags:
- treatment
- acquired
- causal
- amse
- cate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficiently estimating causal
  quantities (CQs) like CATE, ATE, ATT, and ATEDS, which typically require large datasets
  that can be costly to obtain. The core method idea is a unified Bayesian framework
  that models the regression function with a Gaussian Process and represents the target
  distribution using conditional mean embeddings (CMEs) in a reproducing kernel Hilbert
  space.
---

# ActiveCQ: Active Estimation of Causal Quantities

## Quick Facts
- arXiv ID: 2509.24293
- Source URL: https://arxiv.org/abs/2509.24293
- Authors: Erdun Gao; Dino Sejdinovic
- Reference count: 40
- Key outcome: Unified Bayesian framework with GP priors and CMEs achieves superior sample efficiency for estimating causal quantities across simulations and semi-synthetic datasets

## Executive Summary
ActiveCQ presents a unified Bayesian framework for actively estimating causal quantities (CQs) like CATE, ATE, ATT, and ATEDS. The method combines Gaussian Process regression for the outcome model with conditional mean embeddings (CMEs) to represent target distributions in reproducing kernel Hilbert spaces. This approach enables principled derivation of acquisition strategies that reduce posterior uncertainty in the CQ estimator through information gain and total variance reduction. The framework significantly outperforms traditional baselines in sample efficiency across various causal quantities.

## Method Summary
The ActiveCQ framework models the regression function with a Gaussian Process prior and represents the target distribution using conditional mean embeddings in a reproducing kernel Hilbert space. The key innovation is deriving acquisition strategies that minimize posterior uncertainty in the causal quantity estimator. Two utility functions are proposed: information gain (IG) which selects points that maximally reduce uncertainty about the CQ, and total variance reduction (TVR) which directly minimizes the posterior variance of the estimator. The method handles different CQs through a unified treatment where the causal estimand is expressed as an expectation over a target distribution, with the observational distribution acting as a prior that can be updated through active sampling.

## Key Results
- Demonstrated significant sample efficiency gains over traditional baselines across simulations and semi-synthetic datasets
- The CME-based approach shows particular effectiveness in handling high-dimensional data and adapting to learned feature spaces
- Outperforms existing methods for estimating various causal quantities including CATE, ATE, ATT, and ATEDS
- Shows robust performance across different data-generating mechanisms and causal structures

## Why This Works (Mechanism)
The framework works by explicitly modeling the uncertainty in both the regression function and the target distribution, then strategically reducing this uncertainty through active sampling. The Gaussian Process provides a principled way to quantify uncertainty in the outcome model, while the conditional mean embedding approach allows for flexible representation of the target distribution. By deriving acquisition strategies that target the reduction of uncertainty in the final causal quantity estimator (rather than intermediate quantities), the method ensures that each sample collected maximally contributes to improving the estimate.

## Foundational Learning

**Gaussian Process Regression**: Bayesian non-parametric approach for regression that provides uncertainty quantification through posterior distributions over functions. Why needed: Essential for modeling outcome uncertainty with principled uncertainty quantification. Quick check: Verify GP hyperparameters (lengthscale, variance) are appropriately tuned for the data.

**Reproducing Kernel Hilbert Spaces**: Mathematical framework that allows functions to be represented as inner products in a high-dimensional feature space. Why needed: Provides the mathematical foundation for conditional mean embeddings and kernel methods. Quick check: Ensure the kernel captures relevant similarities in the data.

**Conditional Mean Embeddings**: Technique for representing conditional distributions as elements in a reproducing kernel Hilbert space. Why needed: Enables flexible representation of target distributions without parametric assumptions. Quick check: Validate that the CME accurately captures the conditional structure of the data.

**Causal Quantities (CATE, ATE, ATT, ATEDS)**: Different estimands representing treatment effects under various target populations. Why needed: Framework must handle multiple types of causal questions. Quick check: Confirm correct identification assumptions hold for each CQ being estimated.

## Architecture Onboarding

**Component Map**: GP Regression -> CME Representation -> Uncertainty Quantification -> Acquisition Strategy -> Data Collection -> Updated Model

**Critical Path**: The essential flow is: model the outcome with GP -> represent target distribution with CME -> derive posterior uncertainty for CQ -> select acquisition points -> update model with new data. Each component must function correctly for the next to work.

**Design Tradeoffs**: GP-based approach provides uncertainty quantification but suffers from cubic complexity; CME approach offers flexibility but requires careful kernel selection; acquisition strategies balance exploration and exploitation but may be computationally intensive.

**Failure Signatures**: Poor performance may indicate: kernel mismatch with data structure, insufficient exploration leading to local optima, computational bottlenecks from GP scalability, or identification failures due to violated assumptions.

**First Experiments**: 1) Run on synthetic data with known ground truth to validate basic functionality; 2) Compare different acquisition strategies on moderate-sized datasets; 3) Test scalability on high-dimensional synthetic data with varying sample sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the ActiveCQ framework perform when replacing Gaussian Processes with Bayesian Neural Networks (BNNs) or Scalable Sparse GPs for large-scale datasets?
- **Basis in paper:** [explicit] The Conclusion states, "While our GP-based instantiation inherits cubic complexity, the framework's modularity is a key strength, inviting future work that integrates scalable approximations like Sparse GPs or Nystr√∂m methods."
- **Why unresolved:** The experiments exclusively utilized standard Gaussian Processes, which have cubic complexity limits. The theoretical advantages of CQ-focused utility functions (IG/TVR) remain untested in high-parameter or deep learning settings where posterior approximations differ significantly.
- **What evidence would resolve it:** A comparative study on large-scale datasets evaluating the sample efficiency and convergence rates of ActiveCQ using Variational GPs or BNNs against the current baseline.

### Open Question 2
- **Question:** Can the proposed uncertainty reduction utility functions be effectively adapted to causal inference settings with unmeasured confounders, such as those requiring instrumental variables or proxies?
- **Basis in paper:** [explicit] The Conclusion mentions extending the approach "to handle more complex causal settings involving hidden confounders or instrumental variables."
- **Why unresolved:** The current identification relies on the backdoor criterion and no unmeasured confounders. The utility functions (IG/TVR) currently target the estimator $\hat{\tau}$ derived from standard adjustment. It is unclear if these utilities remain theoretically sound when $\hat{\tau}$ depends on complex identification constraints (e.g., moment conditions in IV regression).
- **What evidence would resolve it:** Derivation of the posterior variance for a causal estimator under an Instrumental Variable setting and empirical verification that the proposed acquisition strategies outperform random sampling in confounded environments.

### Open Question 3
- **Question:** Does the "mismatch" between the observational validation set and the target distribution significantly bias the model selection and hyperparameter optimization in ActiveCQ?
- **Basis in paper:** [explicit] Appendix E.2.1 notes, "This highlights a mismatch between the current validation setup and the optimal validation strategy... one could construct a validation set that mimics the new target distribution."
- **Why unresolved:** The authors used a validation set sampled from the observational distribution to control early stopping and hyperparameter selection. However, since the goal is to minimize error on a shifted target distribution (e.g., CATE or ATE-DS), optimizing for the observational validation set may lead to suboptimal kernel bandwidths or regularization parameters for the specific causal quantity of interest.
- **What evidence would resolve it:** An ablation study comparing model performance when hyperparameters are tuned on a standard validation set versus a synthetic validation set constructed to mimic the target distribution $P(S|Z)$ or $\tilde{P}(S)$.

## Limitations
- The GP-based approach has cubic computational complexity, limiting scalability to large datasets
- Assumes no unmeasured confounders and valid identification strategies, which may not hold in practice
- Requires careful kernel selection and hyperparameter tuning, which can be sensitive to choices
- Theoretical guarantees are primarily asymptotic, with limited finite-sample analysis

## Confidence

1. Unified Bayesian framework for active causal estimation: High confidence
2. Superior sample efficiency compared to baselines: High confidence
3. Generalizability across different causal quantities (CATE, ATE, ATT, ATEDS): Medium confidence (primarily validated on synthetic/semi-synthetic data)
4. Scalability to high-dimensional data: Low confidence (limited empirical validation)

## Next Checks

1. Test the framework on real-world datasets with known causal structures to validate performance on complex, noisy, and potentially non-linear relationships
2. Conduct ablation studies to quantify the contribution of different components (GP priors, CME approach, acquisition strategies) to overall performance
3. Evaluate computational scalability by testing on datasets with increasing feature dimensions and sample sizes, comparing runtime and memory usage against alternative methods