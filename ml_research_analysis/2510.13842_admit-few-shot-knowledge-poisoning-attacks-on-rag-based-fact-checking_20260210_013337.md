---
ver: rpa2
title: 'ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking'
arxiv_id: '2510.13842'
source_url: https://arxiv.org/abs/2510.13842
tags:
- shot
- passages
- admit
- gid00001
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ADMIT addresses the challenge of knowledge poisoning in RAG-based
  fact-checking systems, where injected content can manipulate verification outcomes.
  The core method, ADMIT, employs a few-shot, semantically aligned adversarial passage
  generation strategy that iteratively refines content under proxy verification to
  flip fact-checking decisions while maintaining plausible justifications.
---

# ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking

## Quick Facts
- **arXiv ID:** 2510.13842
- **Source URL:** https://arxiv.org/abs/2510.13842
- **Reference count:** 40
- **Primary result:** ADMIT achieves 86% attack success rate at 0.93×10⁻⁶ poisoning rate, outperforming state-of-the-art by 11.2%

## Executive Summary
ADMIT addresses the challenge of knowledge poisoning in RAG-based fact-checking systems, where injected content can manipulate verification outcomes. The core method, ADMIT, employs a few-shot, semantically aligned adversarial passage generation strategy that iteratively refines content under proxy verification to flip fact-checking decisions while maintaining plausible justifications. Extensive experiments across four benchmarks, 11 LLMs, and four retrievers demonstrate that ADMIT achieves an average attack success rate of 86% at an extremely low poisoning rate of 0.93×10⁻⁶, outperforming prior state-of-the-art by 11.2% and remaining effective even in the presence of clean counter-evidence.

## Method Summary
ADMIT is a few-shot knowledge poisoning technique that attacks RAG-based fact-checking systems by injecting adversarial passages to manipulate verification outcomes. The method uses a proxy-guided multi-turn optimization approach where an attacker LLM generates candidate passages, a proxy verifier evaluates them, and the attacker refines based on feedback. Key innovations include adversarial prefix augmentation (prepending search queries to passages for higher retrieval ranking) and multi-turn optimization with memory resetting. The attack is evaluated across four benchmarks (FEVER, SciFact, Climate-FEVER, HealthVer) using various retriever-LLM combinations.

## Key Results
- Achieves 86% average attack success rate across 4 benchmarks, 11 LLMs, and 4 retrievers
- Operates at extremely low poisoning rate of 0.93×10⁻⁶ (1-5 injected passages)
- Outperforms state-of-the-art by 11.2% in attack success rate
- Maintains effectiveness even when clean evidence is present in the knowledge base

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injected passages achieve high retrieval rank by prepending query-related keywords, artificially inflating semantic similarity scores.
- **Mechanism:** The **Adversarial Prefix Augmentation** strategy prepends search-style queries to the passage body (AUGAP = Q ⊕ AP). This exploits the matching function of both dense and sparse retrievers, which treat the prefix as highly relevant content. If the retriever uses dot-product or cosine similarity, the prefix aligns the passage embedding with the query embedding.
- **Core assumption:** The retrieval model does not strictly filter or penalize repetitive query-like text at the start of passages.
- **Evidence anchors:**
  - [section] Section 3.3 states ADMIT prepends decomposed search queries to boost retrievability without white-box access.
  - [section] Table 3 demonstrates that prefix augmentation increases average recall from 0.77 (without prefix) to 0.98 (with prefix) on BGE-large.
  - [corpus] Corpus signals do not directly address prefix augmentation, but standard retrieval theory supports the mechanism.
- **Break condition:** If a defense implements aggressive re-ranking that penalizes exact query overlap or detects "keyword stuffing" patterns, retrieval success may degrade.

### Mechanism 2
- **Claim:** LLMs prioritize semantically coherent, authoritative-sounding adversarial text over authentic evidence, particularly when the claim involves uncertainty or conflicting signals.
- **Mechanism:** ADMIT generates "misinformation-level" text (e.g., mimicking journalistic tone) rather than gibberish triggers. This exploits the LLM's **contextual grounding bias**. When clean evidence (proxy passages) and adversarial text conflict, the model often aligns with the more coherent or explicitly confident text. The mechanism is amplified for "Gray" claims (where internal LLM knowledge and external clean evidence already conflict).
- **Core assumption:** The target LLM lacks robust mechanisms to cross-check specific citations or detect subtle stylistic forgeries within the context window.
- **Evidence anchors:**
  - [section] Table 7 analyzes claim types, showing "Gray" claims have an ASR of 0.75 compared to 0.52 for "Gold" claims, suggesting uncertainty is a vector.
  - [section] Table 2 shows ADMIT generates plausible text (e.g., fake Reuters reports) compared to unreadable gradient-based attacks.
  - [corpus] The paper "Resolving Conflicting Evidence..." suggests LLMs struggle to assess credibility when evidence conflicts, indirectly supporting this vulnerability.
- **Break condition:** If the target LLM employs advanced chain-of-thought verification or "knowledge consolidation" that cross-references multiple retrieved chunks, the attack success rate drops (as seen in "Consolidate-then-Select" defense, Table 15).

### Mechanism 3
- **Claim:** Optimization against a proxy system (Search + Proxy LLM) generates attacks that transfer to unseen target LLMs and retrievers.
- **Mechanism:** ADMIT uses **Proxy-Guided Multi-turn Optimization**. Instead of optimizing gradients (which are model-specific), it optimizes text via black-box feedback. By generating diverse proxy passages via web search, the attack creates a "superset" of possible contexts. If the adversarial passage flips the verdict in this noisy, diverse proxy environment, it is likely to succeed in the cleaner target environment due to shared linguistic priors across LLMs.
- **Core assumption:** The proxy LLM and target LLM share sufficient linguistic understanding and reasoning patterns for the adversarial pattern to generalize.
- **Evidence anchors:**
  - [abstract] Claims ADMIT transfers effectively across 11 LLMs and 4 retrievers without access to them.
  - [section] Section 5 (Ablation) shows that using 3 proxy passages provides the best balance, enabling robustness against retrieval noise.
  - [corpus] Weak corpus evidence for this specific transfer mechanism; relies on the paper's reported cross-model ASR.
- **Break condition:** If the target LLM has a significantly different safety alignment or reasoning architecture (e.g., strictly trained to reject external claims without high-confidence sources), transferability may fail.

## Foundational Learning

- **Concept: Dense vs. Sparse Retrieval**
  - **Why needed here:** ADMIT's prefix attack targets the core mechanics of how queries match documents. You must understand that BM25 (sparse) relies on keyword overlap while Contriever/BGE (dense) rely on embedding similarity.
  - **Quick check question:** Does adding the query text to the document help BM25, dot-product similarity, or both?

- **Concept: Black-Box Optimization**
  - **Why needed here:** ADMIT does not use gradients. It treats the RAG system as an oracle. Understanding this clarifies why "Proxy" models are necessary—it's a replacement for backpropagation.
  - **Quick check question:** Why is "memory resetting" (Section 3.1) important in a textual feedback loop?

- **Concept: Knowledge Conflict in RAG**
  - **Why needed here:** The attack exploits the moment the LLM decides between "what I know" and "what I see." Understanding this conflict is key to predicting which claims are vulnerable (Gray vs. Gold).
  - **Quick check question:** According to the paper, are claims where the LLM and RAG agree ("Gold") easier or harder to attack than claims where they disagree ("Gray")?

## Architecture Onboarding

- **Component map:** Claim Input -> Proxy Constructor (Search/LLM) -> Attacker LLM -> Proxy Verifier -> Optimization Loop -> Injector
- **Critical path:** The **Proxy Construction** phase (Algo 1). If the proxy passages do not accurately represent the types of evidence the target system retrieves, the optimization will drift, and the attack will fail to generalize.
- **Design tradeoffs:**
  - **Web Search vs. LLM Proxy:** Web search is more realistic but slower/costly. LLM generation is fast but may hallucinate unrealistic contexts. The paper suggests combining them (Section 3.2).
  - **Prefix Augmentation:** Increases recall significantly but makes the injection more detectable by pattern-matching defenses.
- **Failure signatures:**
  - **Low Recall:** The prefix is missing or the retriever filters strictly on semantic density (solved by prefix).
  - **High NEI Rate:** The attack succeeds in confusing the model but fails to induce a specific verdict flip (observed in failure cases, Section 4.2).
  - **Defensive Triggers:** High perplexity or "fake news" classifiers catching the passage (ADMIT specifically minimizes this by optimizing for semantic coherence).
- **First 3 experiments:**
  1. **Sanity Check Retrieval:** Implement the prefix augmentation logic. Inject a passage with the prefix into a clean corpus and measure Recall@5 for the target claim. Verify if recall > 0.90.
  2. **Proxy Optimization Loop:** Run the multi-turn generation (Algo 2) using a small open-source model (e.g., Qwen-32B) as both attacker and proxy verifier. Track how many iterations are needed to flip a "Supported" claim to "Refuted."
  3. **Cross-Model Transfer:** Take the passages generated in Experiment 2 and test them against a *different* model (e.g., GPT-4o or Llama-3) without re-optimizing. This validates the transferability claim.

## Open Questions the Paper Calls Out
None

## Limitations

- The attack relies on prefix augmentation, but the paper does not test defenses specifically designed to detect keyword stuffing or query-overlap patterns
- The transferability mechanism assumes linguistic similarity across LLMs, but doesn't analyze which specific architectural or training differences break this transfer
- The "lightweight classifier" for filtering search results in proxy construction is not specified, making exact reproduction difficult

## Confidence

- **High Confidence:** The core claim that ADMIT achieves 86% ASR at 0.93×10⁻⁶ poisoning rate, based on extensive experiments across 4 benchmarks and 11 LLMs
- **Medium Confidence:** The claim that prefix augmentation increases recall from 0.77 to 0.98, as this is demonstrated on BGE-large but not comprehensively across all retriever types
- **Medium Confidence:** The transferability claim across different LLMs and retrievers, as the paper reports success but doesn't deeply analyze which architectural differences affect transferability

## Next Checks

1. Implement a detection mechanism for query-overlap patterns in retrieved passages and measure ADMIT's ASR when such a defense is active
2. Test ADMIT-generated passages against a completely different LLM family (e.g., Claude or Gemini) to validate cross-architecture transferability claims
3. Replace the unspecified "lightweight classifier" with a simple keyword filter and measure the impact on proxy construction quality and final attack success rate