---
ver: rpa2
title: Optimization with Access to Auxiliary Information
arxiv_id: '2206.00395'
source_url: https://arxiv.org/abs/2206.00395
tags:
- learning
- auxmom
- have
- case
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of optimizing a target function\
  \ whose gradients are expensive or limited, using auxiliary functions with cheaper\
  \ or more available gradients. The authors propose two algorithms\u2014AuxMOM and\
  \ AuxMVR\u2014that reuse gradients from both the target and auxiliary functions\
  \ to accelerate convergence."
---

# Optimization with Access to Auxiliary Information

## Quick Facts
- arXiv ID: 2206.00395
- Source URL: https://arxiv.org/abs/2206.00395
- Reference count: 40
- Primary result: Two algorithms (AuxMOM, AuxMVR) accelerate optimization by leveraging cheap auxiliary gradients under Hessian similarity assumptions

## Executive Summary
This paper addresses the problem of optimizing a target function whose gradients are expensive or limited, using auxiliary functions with cheaper or more available gradients. The authors propose two algorithms—AuxMOM and AuxMVR—that reuse gradients from both the target and auxiliary functions to accelerate convergence. Under the Hessian similarity assumption between target and auxiliary functions, the methods improve upon known optimal rates for non-convex optimization. AuxMOM replaces Lσ² with δσ² in convergence bounds, potentially leading to significant speedups when δ ≪ L. AuxMVR achieves a rate of O((δF₀σ²/T)²/³), matching optimal non-convex MVR rates while benefiting from auxiliary information.

## Method Summary
The framework uses an auxiliary function h with cheap gradients to optimize a target function f with expensive gradients. The core update computes a biased gradient estimate ∇h(y) - ∇h(x) + ∇f(x), which generalizes SVRG's control variate idea. When ∇f is unavailable, a momentum buffer m_{f-h} tracks the bias between functions. AuxMOM uses exponential moving average for momentum, while AuxMVR employs recursive variance reduction. The algorithms perform K local steps on h between periodic target gradient computations, with convergence rates depending on the Hessian similarity δ rather than the target smoothness L when δ ≪ L.

## Key Results
- AuxMOM replaces Lσ² with δσ² in convergence bounds, yielding speedups when δ ≪ L
- AuxMVR achieves O((δF₀σ²/T)²/³) rate, matching optimal non-convex MVR while benefiting from auxiliary information
- Framework applies to federated learning, transfer learning, and semi-supervised learning scenarios
- Experiments on MNIST and CIFAR datasets demonstrate improved accuracy and faster convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing expensive target gradients with cheap auxiliary gradients is possible if the bias is corrected using a control variate-like term.
- **Mechanism:** The algorithm constructs a biased gradient estimator ∇h(y) - ∇h(x) + ∇f(x). This uses the helper h for the local update at y but anchors it to the target f via a snapshot x. The term ∇h(x) - ∇f(x) acts as a control variate that cancels out the drift caused by the difference between f and h.
- **Core assumption:** Hessian Similarity (Assumption 3.3), where the difference in curvature between f and h is bounded by δ.
- **Break condition:** If the gradient bias ∇f(x) - ∇h(x) is large and uncorrected (Naive approach), the mechanism fails to converge.

### Mechanism 2
- **Claim:** Momentum can approximate the exact correction term ∇f(x) - ∇h(x) when target gradients are stochastic or inaccessible.
- **Mechanism:** Since computing the exact difference ∇f(x) - ∇h(x) at every step is expensive, the algorithm maintains a momentum buffer m_{f-h}. In AuxMOM, this is an exponential moving average; in AuxMVR, it uses a recursive variance-reduced update. This buffer tracks the persistent bias between functions while averaging out noise.
- **Core assumption:** The noise in the stochastic gradients is zero-mean, allowing the momentum to converge to the true bias vector.
- **Break condition:** If the correlation between noise in f and h is negative or high variance without sufficient averaging, the momentum estimate lags, causing instability.

### Mechanism 3
- **Claim:** Convergence speed depends on the Hessian similarity δ rather than the target smoothness L.
- **Mechanism:** Standard SGD rates depend on L. By using the helper h to perform K local steps, the proposed algorithms effectively "transfer" the optimization difficulty. If h approximates f well (small δ), the convergence rate scales with δ instead of L, effectively decoupling the cost of expensive gradients from the geometry of the problem.
- **Core assumption:** δ ≪ L (the helper function is geometrically similar to the target).
- **Break condition:** If δ ≈ L (e.g., h is unrelated to f), the theoretical speedup vanishes, though the algorithm still converges.

## Foundational Learning

- **Concept: Stochastic Variance Reduced Gradient (SVRG)**
  - **Why needed here:** The paper explicitly frames its update rule (Eq. 3) as a generalization of SVRG. Understanding how SVRG reduces variance by subtracting a snapshot gradient is the key to understanding the bias correction mechanism here.
  - **Quick check question:** How does subtracting ∇h(x) from ∇h(y) help estimate ∇f(y)?

- **Concept: Hessian Similarity / Dissimilarity (δ)**
  - **Why needed here:** This is the specific metric used to quantify "relatedness" between the target and auxiliary functions. All theoretical gains rely on this parameter being small. It differs from standard smoothness (L) by measuring the difference in curvature.
  - **Quick check question:** If h(x) = f(x), what is δ? (Answer: 0).

- **Concept: Control Variates**
  - **Why needed here:** The method uses ∇h as a control variate for ∇f. This statistical technique reduces the variance (or bias, in this specific formulation) of an estimator using a correlated variable with known properties.
  - **Quick check question:** Why does the algorithm need to compute g_{f-h} (the difference gradient) periodically?

## Architecture Onboarding

- **Component map:** Target Node (f) -> Momentum Buffer (m_{f-h}) -> Helper Node (h) -> Local State (y)
- **Critical path:**
  1. Snapshot: Sample a gradient difference g_{f-h} at the current state x
  2. Update Momentum: Update buffer m_{f-h} using Eq. (5) or (6)
  3. Local Steps: Perform K updates on y using g_h(y) + m_{f-h}
  4. Sync: Set the new x to the final y
- **Design tradeoffs:**
  - K (Local Steps): High K amortizes the cost of f but increases "drift" if δ is large
  - Momentum type: AuxMOM is simpler; AuxMVR offers better theoretical rates but requires stricter assumptions
  - Batch Size: Larger batches for f-h reduce momentum error E_0 but increase per-iteration cost
- **Failure signatures:**
  - Divergence/Lockup: If the "Naive" approach is used (momentum off) and gradients of f and h point in different directions, the model converges to a suboptimal point bounded by the bias ζ²
  - Slow Convergence: If δ is actually large (unrelated helper), the algorithm wastes computation on h steps without the theoretical acceleration
- **First 3 experiments:**
  1. Toy Quadratic (Sanity Check): Replicate Figure 1. Optimize f(x)=x² using h(x) = (1+δ)(x - ζ/(1+δ))². Verify that Naive fails for large ζ while AuxMOM converges.
  2. Mislabeled Data Robustness: Train on MNIST/CIFAR using a clean target set f and a corrupted helper set h (e.g., random labels). Compare AuxMOM vs. Fine-Tuning vs. Naive to see if the mechanism isolates the signal from the noise.
  3. Core-set Acceleration: Train using a small core-set as the helper h and the full dataset as f. Measure if AuxMOM reaches full-dataset accuracy faster than training on the core-set alone.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can we construct practical algorithms for sampling positively correlated stochastic gradients of f and h to achieve variance σ²_{f-h} < σ²_f + σ²_h? (Basis: [explicit] "We showed... we might benefit if we could sample gradients of h that are positively correlated with gradients of f, but we did not mention how this can be done.")

- **Open Question 2:** What similarity measures beyond Hessian similarity can capture practical relationships between target and auxiliary functions in real applications like semi-supervised learning or federated learning? (Basis: [explicit] "In this work, we will mainly consider Hessian similarity... and leave devising more practical similarity measures as a future direction.")

- **Open Question 3:** Can the convergence theory be extended to generalized smoothness settings where Hessian norms grow with gradient norms rather than being uniformly bounded? (Basis: [explicit] "Jingzhao et al. propose a generalized smoothness assumption where the norm of the Hessian can grow with the norm of the gradient. We believe it possible to extend our theory to accommodate such an assumption.")

- **Open Question 4:** Can a hybrid approach reconcile fine-tuning's reusability advantage with AuxMOM's faster convergence for small δ? (Basis: [explicit] "Fine Tuning uses the helper gradients first and then uses the gradients of f... Can we reconcile both worlds?")

## Limitations
- Hessian similarity assumption (δ ≪ L) may not hold in practice when auxiliary functions are poorly chosen or mismatched
- No practical method provided for estimating δ or L from data, making hyperparameter tuning challenging
- Momentum buffer m_{f-h} requires periodic updates with fresh target gradients, which may be expensive or unavailable in some settings

## Confidence

- **High Confidence:** The mechanism of using momentum to approximate the bias correction term and the basic framework of combining cheap auxiliary gradients with infrequent expensive target gradient corrections
- **Medium Confidence:** The theoretical rate improvements (replacing L with δ in convergence bounds) hold under stated assumptions, but practical benefits depend heavily on unknown quantities like δ and σ²
- **Low Confidence:** The practical impact of AuxMVR's improved O(1/T^{2/3}) rate, given the strong assumptions required and lack of implementation details for the recursive variance reduction scheme

## Next Checks
1. **Hessian Similarity Estimation:** Develop and validate a method to estimate δ between target and auxiliary functions from data, then test whether predicted vs. actual convergence rates align across multiple function pairs

2. **Momentum Buffer Accuracy vs. Cost:** Empirically measure the trade-off between momentum buffer accuracy (E_0) and the frequency of expensive g_{f-h} updates, determining the optimal update schedule for different noise regimes

3. **Cross-Domain Transferability:** Test whether the algorithm's performance transfers across substantially different tasks (e.g., from MNIST to CIFAR, or from image to text classification) where δ may be large but the computational constraints remain