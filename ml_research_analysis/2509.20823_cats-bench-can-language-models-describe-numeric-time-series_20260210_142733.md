---
ver: rpa2
title: 'CaTS-Bench: Can Language Models Describe Numeric Time Series?'
arxiv_id: '2509.20823'
source_url: https://arxiv.org/abs/2509.20823
tags:
- series
- time
- numeric
- captions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CaTS-Bench is a multimodal benchmark for time series captioning,
  combining numeric data, metadata, line-plot images, and human-rewritten descriptions.
  It features 11 real-world domains and 1,746 human-rewritten captions as a gold standard,
  along with a synthetic augmentation pipeline for scalable training.
---

# CaTS-Bench: Can Language Models Describe Numeric Time Series?

## Quick Facts
- **arXiv ID:** 2509.20823
- **Source URL:** https://arxiv.org/abs/2509.20823
- **Reference count:** 40
- **Primary result:** VLMs struggle with numeric grounding and multimodal integration on time series captioning; finetuning on synthetic data improves performance substantially.

## Executive Summary
CaTS-Bench introduces a multimodal benchmark for time series captioning that evaluates whether language models can generate accurate natural language descriptions from numeric data, metadata, and line-plot images. The benchmark features 11 real-world domains and 1,746 human-rewritten captions as gold standard, along with a synthetic augmentation pipeline enabling scalable training with ~16k synthetic samples. Evaluation of leading VLMs reveals fundamental limitations in numeric reasoning, with near-zero accuracy on standard deviation inference and minimal benefit from visual plot inputs. Finetuning on synthetic data yields substantial performance gains, establishing CaTS-Bench as a rigorous foundation for future research in grounded language generation over numeric time series.

## Method Summary
The benchmark combines numeric time series data with metadata and line-plot images, generating captions through a synthetic pipeline using oracle LLMs, then refining with human-rewritten test sets. VLMs are evaluated on both linguistic metrics (BLEU, ROUGE-L, DeBERTaScore) and numeric fidelity scores, including statistical inference accuracy with 5% relative error tolerance. Training uses LoRA finetuning with batch size 4, learning rate 2×10⁻⁵, and 3 epochs on 8×A100 GPUs. The evaluation framework includes 910 diagnostic multiple-choice questions probing temporal reasoning, comparative analysis, and multimodal integration capabilities.

## Key Results
- VLMs achieve near-zero accuracy on standard deviation inference despite finetuning, indicating fundamental limitations in higher-order statistical reasoning.
- Visual plots provide minimal performance benefit; ablation studies show comparable results when removing visual inputs.
- Finetuning on synthetic data substantially improves performance on both synthetic and human-rewritten test sets, though gains may reflect style-matching rather than genuine understanding.

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its multimodal design requiring integration of numeric data, visual plots, and metadata to generate accurate captions. The synthetic augmentation pipeline enables scalable training while the human-rewritten gold standard ensures evaluation quality. Diagnostic MCQ tasks provide fine-grained analysis of model capabilities across different reasoning types, revealing specific failure modes in temporal alignment, statistical inference, and multimodal grounding.

## Foundational Learning

- **Time Series Fundamentals (Trends, Seasonality, Stationarity)**
  - Why needed here: Interpreting numeric sequences requires distinguishing genuine patterns from noise; captions must identify and verbalize trends, fluctuations, and anomalies.
  - Quick check question: Given a series [10, 12, 8, 14, 6, 16], would you describe it as trending upward, downward, or fluctuating? What statistic best captures its variability?

- **Statistical Reasoning (Mean, Variance, Extrema, Temporal Alignment)**
  - Why needed here: The benchmark explicitly evaluates whether models can correctly infer and verbalize statistics (mean, min, max, std) and align values with timestamps.
  - Quick check question: If a series has values [3, 7, 2, 9, 5] starting on January 1st at daily frequency, what is the value on January 4th? What's the mean and standard deviation?

- **Vision-Language Multimodal Integration**
  - Why needed here: CaTS-Bench provides both numeric data and line plots; understanding how visual signals should augment or constrain textual generation is central to the task.
  - Quick check question: When you see a line plot with a clear peak at t=50, what additional information do you need to confidently state the peak's exact value and time in a caption?

## Architecture Onboarding

- **Component map:** Input pipeline (numeric series + metadata JSON + line plot) → VLM backbone → evaluation framework (linguistic + numeric metrics) → diagnostic MCQ suite
- **Critical path:** 1) Baseline evaluation using provided prompt template on pretrained VLM, 2) Finetune using synthetic training data with LoRA, 3) Evaluate on HR gold standard with linguistic and numeric metrics, diagnose using Q&A tasks
- **Design tradeoffs:** Synthetic vs. human data (scale vs. quality), metric emphasis (recall-weighted Numeric Fidelity), metadata inclusion (excludes stats during evaluation to force inference)
- **Failure signatures:** Hallucinated statistics, visual grounding failure (minimal plot contribution), temporal indexing failure (degrades with distance), overconfident finetuning (incorrect statistical claims)
- **First 3 experiments:** 1) Baseline assessment on HR gold standard with linguistic and numeric metrics, 2) Ablation study comparing Text+TS vs Text+TS+Plot, 3) Targeted finetuning experiment comparing HR vs synthetic test set performance

## Open Questions the Paper Calls Out

- **Open Question 1:** Does finetuning on synthetic captions generated by a diverse ensemble of oracle LLMs mitigate modeling biases observed with single oracle (Gemini 2.0 Flash)?
- **Open Question 2:** Does incorporating domain-specific expert annotations improve models' ability to capture deep insights compared to generalist human rewrites?
- **Open Question 3:** What architectural modifications are required to compel VLMs to leverage visual plot structures for numeric reasoning rather than relying on textual priors?

## Limitations

- VLMs struggle with higher-order statistical reasoning (near-zero accuracy on standard deviation inference) despite finetuning
- Visual information from line plots contributes minimally to caption quality, raising questions about genuine multimodal reasoning
- Synthetic augmentation pipeline may introduce oracle biases that inflate synthetic test performance while failing to generalize to human-rewritten captions

## Confidence

- **High Confidence:** Core finding that VLMs struggle with numeric grounding and multimodal integration is well-supported by consistent results across evaluation tracks
- **Medium Confidence:** Effectiveness of finetuning on synthetic data is supported but extent of genuine understanding vs. style-matching remains uncertain
- **Low Confidence:** Interpretation that models fail to perform genuine multimodal reasoning may be premature due to evaluation methodology constraints

## Next Checks

1. **Evaluation Pipeline Validation:** Re-run diagnostic MCQ tasks with relaxed evaluation criteria to determine if near-zero STD inference accuracy reflects model limitations or overly strict evaluation
2. **Visual Modality Stress Test:** Design experiments where visual patterns deliberately conflict with textual statistics to test genuine multimodal integration capabilities
3. **Synthetic Data Bias Analysis:** Compare model performance on synthetic vs human-rewritten test sets across different training durations and synthetic generation parameters to identify bias sources