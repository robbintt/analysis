---
ver: rpa2
title: 'Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations
  During Inference in Neural Network Classifiers'
arxiv_id: '2511.11834'
source_url: https://arxiv.org/abs/2511.11834
tags:
- accuracy
- adversarial
- mnist
- training
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Volatility in Certainty (VC), a label-free
  metric designed to detect adversarial perturbations in neural network classifiers
  during inference without requiring ground-truth labels. VC quantifies irregularities
  in model confidence by measuring the dispersion of sorted softmax outputs, capturing
  local fluctuations in model output smoothness through the average squared log-ratio
  of adjacent certainty values.
---

# Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers

## Quick Facts
- arXiv ID: 2511.11834
- Source URL: https://arxiv.org/abs/2511.11834
- Authors: Vahid Hemmati; Ahmad Mohammadi; Abdul-Rauf Nuhu; Reza Ahmari; Parham Kebria; Abdollah Homaifar
- Reference count: 29
- Primary result: Introduces VC metric showing strong negative correlation (rho < -0.90) with accuracy under adversarial attacks, detecting perturbations without ground-truth labels

## Executive Summary
This paper introduces Volatility in Certainty (VC), a label-free metric designed to detect adversarial perturbations in neural network classifiers during inference without requiring ground-truth labels. VC quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs, capturing local fluctuations in model output smoothness through the average squared log-ratio of adjacent certainty values. Experiments on MNIST (ANN, CNN) and CIFAR-10 (regularized VGG) show that log(VC) exhibits a strong negative correlation with classification accuracy (rho < -0.90) under adversarial attack using FGSM, indicating VC effectively reflects performance degradation.

## Method Summary
VC computes certainty values as the difference between top two softmax probabilities, sorts them, and measures volatility through squared log-ratios of adjacent values. The metric averages these values over the central 60% of the sorted distribution to reduce noise. The method was evaluated on MNIST (ANN and CNN architectures) and CIFAR-10 (VGG) against FGSM attacks at varying perturbation strengths. VC was calculated on clean and adversarial test sets, with correlation analysis and t-tests used to validate detection effectiveness.

## Key Results
- log(VC) shows strong negative correlation with accuracy (rho < -0.90) under FGSM attacks
- VC detects adversarial contamination even at 5% contamination levels with statistical significance (p < 0.05)
- The metric is architecture-agnostic and requires no ground-truth labels during inference
- Performance remains stable across different neural network architectures (ANN, CNN, VGG)

## Why This Works (Mechanism)

### Mechanism 1
Adversarial perturbations disrupt the smoothness of sorted confidence profiles, creating detectable irregularities in adjacent certainty values. Well-trained models produce structured softmax outputs where certainty values transition smoothly when sorted. Adversarial inputs inject noise that causes non-uniform confidence degradation, manifesting as "jumps" in the sorted sequence. The squared log-ratio amplifies these discontinuities.

### Mechanism 2
The logit margin (difference between top two softmax probabilities) serves as a label-free proxy for model confidence that degrades under adversarial shift. High margins indicate decisive predictions; low margins indicate ambiguity. Adversarial examples reduce margins non-uniformly across samples, increasing the variance of the certainty distribution and thus VC.

### Mechanism 3
Trimming to the central 60% of sorted certainty values reduces noise from outliers and improves detection stability. Extreme certainty values (very low/high) may arise from outliers or artifacts. The central region represents the "core" distribution where volatility signal is most consistent.

## Foundational Learning

- **Softmax distributions and logit margins**: VC operates entirely on softmax outputs; understanding how margins reflect confidence is essential for interpreting VC values. Quick check: Given softmax output [0.7, 0.2, 0.1], what is the logit margin? (Answer: 0.5)

- **FGSM adversarial attack mechanism**: The paper evaluates VC specifically against FGSM; understanding how perturbations are generated helps interpret why VC detects them. Quick check: What is the formula for FGSM perturbation? (Answer: x̃ = x + ε·sign(∇ₓL(h(x), y)))

- **Correlation and statistical significance testing**: The paper relies on Pearson/Spearman correlation and t-tests to validate VC; understanding these metrics is necessary to assess evidence strength. Quick check: If ρ = -0.95 between log(VC) and accuracy, what does this imply about their relationship? (Answer: Strong inverse linear relationship)

## Architecture Onboarding

- **Component map**: Certainty extraction → sorting → log-ratio computation → central trimming → threshold comparison
- **Critical path**: Certainty extraction → sorting → log-ratio computation → central trimming → threshold comparison
- **Design tradeoffs**: Trimming ratio (60%): Higher = more stable but may discard signal; lower = more sensitive to outliers; ε₀ selection: Too small causes numerical instability; too large dampens signal; Threshold calibration: Must balance false positive rate vs. detection sensitivity
- **Failure signatures**: Consistently high VC on clean data → model may be poorly calibrated; No VC change under known attack → attack may be VC-aware; High variance in VC across batches → sample size too small
- **First 3 experiments**: 
  1. Baseline calibration: Compute VC on clean validation set across multiple batch sizes (100, 500, 1000) to establish stable reference distribution
  2. Sensitivity sweep: Generate FGSM examples at ε ∈ {0.001, 0.005, 0.01, 0.02, 0.05} and plot log(VC) vs. accuracy
  3. Contamination detection: Mix clean/adversarial samples at ratios 1%, 5%, 10%, 20% and run two-sample t-tests

## Open Questions the Paper Calls Out

- Can Volatility in Certainty (VC) be effectively utilized as a regularization term during training to improve out-of-distribution (OOD) generalization?
- Does the strong negative correlation between log(VC) and accuracy persist under stronger, iterative adversarial attacks like Projected Gradient Descent (PGD)?
- Can VC reliably distinguish between adversarial distribution shifts and natural domain shifts (e.g., sensor noise or lighting changes)?

## Limitations
- Limited validation to FGSM attacks only, without testing against adaptive or iterative attacks
- No validation of trimming strategy across different architectures and data distributions
- Uncertainty about VC's performance on natural domain shifts versus adversarial perturbations

## Confidence

- **High**: Mathematical formulation of VC metric, correlation analysis methodology
- **Medium**: Detection effectiveness on MNIST/CIFAR-10 against FGSM attacks
- **Low**: Generalization to other attack types, optimal parameter selection (trimming ratio, ε₀)

## Next Checks

1. Apply VC to ResNet-50 on ImageNet and MobileNet on TinyImageNet to verify architecture-agnostic performance beyond simple CNNs

2. Test VC against PGD (ε=8/255, step_size=2/255, 10 steps), Carlini-Wagner L₂, and black-box attacks to assess vulnerability to adaptive perturbations

3. Systematically vary trimming ratio (40%-80%) and ε₀ (1e-8 to 1e-3) across multiple datasets to identify robust default configurations and quantify sensitivity to hyperparameter choices