---
ver: rpa2
title: Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers
  Evidence Across Models and Ontologies
arxiv_id: '2509.04458'
source_url: https://arxiv.org/abs/2509.04458
tags:
- terms
- ontology
- term
- biomedical
- annotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We analyzed why large language models (LLMs) often fail to correctly
  link biomedical ontology terms to their unique identifiers. Using GPT-4o and LLaMa
  3.1 405B, we evaluated 18,988 Human Phenotype Ontology (HPO) and 4,023 Gene Ontology
  (GO-CC) terms across nine features capturing term familiarity, identifier exposure,
  and ontology structure.
---

# Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers

## Quick Facts
- arXiv ID: 2509.04458
- Source URL: https://arxiv.org/abs/2509.04458
- Reference count: 33
- Primary result: Usage frequency in biomedical literature, not term structure, predicts LLM linking success for ontology terms

## Executive Summary
This study investigates why large language models fail to correctly link biomedical ontology terms to their unique identifiers. Analyzing GPT-4o and LLaMa 3.1 405B across 23,011 ontology terms from Human Phenotype Ontology and Gene Ontology Cellular Component, the researchers found systematic failures driven by limited training data exposure rather than random errors. Over 40% of HPO and 54% of GO-CC terms were "ontology deserts" - unused in PubMed Central and resulting in near-zero linking accuracy. The study identifies usage frequency in biomedical literature and curated annotations as the strongest predictors of linking success, while term morphology and ontology depth show no significant impact.

## Method Summary
The researchers evaluated two frontier LLMs (GPT-4o and LLaMa 3.1 405B) on their ability to link ontology terms to identifiers across 18,988 HPO terms and 4,023 GO-CC terms. They extracted nine features capturing term familiarity (usage frequency, annotations), identifier exposure (ontology depth, formatting), and ontology structure (term length, composition). Univariate and multivariate analyses examined feature relationships with linking success, while cross-validation (k=5) ensured result robustness. The study specifically investigated "ontology deserts" - terms with zero usage frequency - and their impact on model performance.

## Key Results
- Usage frequency in PubMed Central and curated annotations are the strongest predictors of linking success
- Over 40% of HPO and 54% of GO-CC terms are "ontology deserts" with zero linking accuracy
- A novel "leading zero" formatting effect in HPO identifiers improves model performance
- Term morphology and ontology depth show no significant impact on linking success

## Why This Works (Mechanism)
The study demonstrates that LLM failures in ontology linking are systematic rather than random, driven by the models' limited exposure to ontology identifiers in their training data. When terms appear frequently in biomedical literature and curated annotations, models learn to associate them with correct identifiers. Conversely, "ontology desert" terms that never appear in training data cannot be linked correctly, resulting in near-zero accuracy. The leading zero formatting effect suggests that consistent identifier patterns can improve model performance even for less frequent terms.

## Foundational Learning
- **Ontology deserts**: Terms with zero usage frequency in biomedical literature that result in near-zero linking accuracy
  - Why needed: Explains the systematic nature of LLM failures
  - Quick check: Verify term frequency in PubMed Central corpus

- **Identifier exposure hypothesis**: Model performance correlates with identifier frequency in training data
  - Why needed: Provides causal mechanism for linking failures
- **Leading zero formatting**: Consistent identifier patterns improve model performance
  - Why needed: Suggests practical interventions for improving linking accuracy

## Architecture Onboarding
- **Component map**: Term features (frequency, annotations, morphology) -> LLM models (GPT-4o, LLaMa) -> Linking success
- **Critical path**: Training data exposure → identifier recognition → correct linking
- **Design tradeoffs**: Model scale vs. training data curation costs
- **Failure signatures**: Zero usage frequency → zero linking accuracy; inconsistent identifier formatting → reduced performance
- **First experiments**: 1) Test additional ontologies (SNOMED CT, ICD-11) for generalizability; 2) Fine-tune models on curated identifier pairs; 3) Compare different LLM architectures for exposure-success relationship

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Study examines only two ontologies (HPO, GO-CC) and two models (GPT-4o, LLaMa 3.1 405B)
- Cross-validation provides internal consistency but doesn't address domain shift
- Cannot directly observe training data to prove causation vs. correlation
- Leading zero formatting effect represents minor practical improvement

## Confidence
- Core finding (training data exposure predicts success): **High**
- Causal mechanism (limited exposure driving failures): **Medium**
- Quantitative impact of ontology deserts: **Medium**
- Leading zero formatting practical impact: **Low**

## Next Checks
1. Replicate analysis across 5-10 additional biomedical ontologies (SNOMED CT, ICD-11, MeSH) to test generalizability
2. Test newer LLM architectures (GPT-4 Turbo, Claude 3, Gemini Pro) to determine if model scale changes exposure-success relationship
3. Conduct ablation studies by fine-tuning models on curated identifier-annotation pairs to measure performance gains and validate exposure hypothesis