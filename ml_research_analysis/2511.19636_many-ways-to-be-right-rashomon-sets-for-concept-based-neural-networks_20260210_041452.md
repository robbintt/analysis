---
ver: rpa2
title: 'Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks'
arxiv_id: '2511.19636'
source_url: https://arxiv.org/abs/2511.19636
tags:
- concept
- rashomon
- diversity
- cbms
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rashomon CBMs address the challenge of finding multiple deep neural
  networks that achieve similar accuracy but rely on different reasoning pathways.
  By introducing lightweight adapter modules and a diversity-regularized training
  objective, the framework efficiently constructs a diverse set of concept-based models
  without retraining from scratch.
---

# Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks

## Quick Facts
- arXiv ID: 2511.19636
- Source URL: https://arxiv.org/abs/2511.19636
- Reference count: 40
- Key outcome: Rashomon CBMs constructs diverse concept-based model sets using lightweight adapters and diversity regularization while maintaining competitive accuracy

## Executive Summary
Rashomon CBMs addresses the challenge of finding multiple deep neural networks that achieve similar accuracy but rely on different reasoning pathways. The framework introduces lightweight adapter modules and a diversity-regularized training objective to efficiently construct a diverse set of concept-based models without retraining from scratch. By inserting model-specific adapters into a shared backbone and jointly optimizing all models to minimize the worst-performing model's losses while encouraging concept diversity, the method reveals rich diversity in concept utilization and layer-wise reasoning while maintaining competitive accuracy.

## Method Summary
The method constructs a Rashomon set of M concept-based models by inserting LoRA adapters (rank=8, α=16, dropout=0.1) into a frozen backbone (ViT-S/16 or ResNet-18). Each model has per-concept linear predictors and task classifiers, trained jointly with a loss function that minimizes the worst-performing model's losses while encouraging diversity through cosine similarity regularization. The approach uses model-axis gradient checkpointing for memory efficiency and trains with Adam (lr=1e-4, batch=64) for 500 epochs with early stopping.

## Key Results
- Rashomon CBMs maintains competitive task accuracy while significantly increasing concept diversity across four datasets
- Layer-wise analysis shows diversity concentrates in adapter modules of deeper layers, enabling different concept reliances
- The framework achieves memory-efficient scalability through adapter-based architecture and model-axis checkpointing

## Why This Works (Mechanism)
The framework works by creating a shared backbone with model-specific adapters that learn different concept representations while maintaining task performance. The diversity regularization encourages models to use different concepts through cosine similarity penalties, and the max-pooling across model losses ensures all models remain competitive. The adapter architecture allows for efficient parameter sharing while enabling distinct reasoning pathways.

## Foundational Learning
- **Concept-based modeling**: Why needed - enables human-understandable reasoning pathways; Quick check - verify per-concept predictors are properly initialized and trained
- **LoRA adapters**: Why needed - efficient parameter sharing with model-specific adaptation; Quick check - confirm adapter matrices are correctly inserted into attention blocks
- **Diversity regularization**: Why needed - encourages genuinely different reasoning pathways; Quick check - monitor diversity metrics during training to ensure they improve
- **Model-axis checkpointing**: Why needed - reduces memory overhead for joint training; Quick check - verify gradient flow through checkpointed segments
- **Max-pooling loss aggregation**: Why needed - prevents single model dominance; Quick check - monitor per-model losses to ensure balanced training

## Architecture Onboarding
- **Component map**: Frozen backbone -> M adapter sets -> Per-concept predictors -> Task classifiers
- **Critical path**: Backbone forward pass → Adapter transformation → Concept prediction → Task prediction → Joint loss computation
- **Design tradeoffs**: Adapter-based vs full retraining (efficiency vs capacity), diversity regularization strength (exploration vs exploitation), model count (diversity vs complexity)
- **Failure signatures**: All models collapse to identical concepts (high CKA, low SHAP diversity), single model dominance (unbalanced per-model losses), training instability (diverging losses)
- **First experiments**: 1) Train with diversity regularization disabled to establish baseline collapse, 2) Monitor per-model loss evolution to verify max-pooling effectiveness, 3) Validate concept diversity using SHAP similarity before and after training

## Open Questions the Paper Calls Out
None

## Limitations
- Diversity measures (SHAP similarity, CKA) may conflate correlation with causation in concept utilization
- The framework doesn't demonstrate concrete applications of diversity for improving model reliability
- Connection between Rashomon diversity and practical interpretability gains remains largely theoretical

## Confidence
- Core diversity claims: Medium - experimental results show improvements but analysis focuses on correlation-based measures
- Scalability claims: High - well-supported through thorough memory efficiency analysis
- Interpretability benefits: Low-Medium - diversity is demonstrated but practical benefits are theoretical

## Next Checks
1. Implement ablation studies removing the diversity regularization term to quantify its actual contribution versus random initialization
2. Conduct targeted intervention experiments where specific concepts are masked to verify different reasoning pathways
3. Apply the framework to synthetic data with explicit concept rules to test discovery of valid reasoning strategies