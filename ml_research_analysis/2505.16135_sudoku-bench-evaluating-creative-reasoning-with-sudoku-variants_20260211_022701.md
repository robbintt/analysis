---
ver: rpa2
title: 'Sudoku-Bench: Evaluating creative reasoning with Sudoku variants'
arxiv_id: '2505.16135'
source_url: https://arxiv.org/abs/2505.16135
tags:
- reasoning
- sudoku
- puzzle
- puzzles
- variants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sudoku-Bench, a new benchmark designed to
  evaluate creative, multi-step reasoning in large language models using challenging
  Sudoku variants. Unlike traditional benchmarks that reward memorization, Sudoku
  variants require novel logical breakthroughs due to their unique and interacting
  constraints, making them resistant to rote learning.
---

# Sudoku-Bench: Evaluating creative reasoning with Sudoku variants

## Quick Facts
- **arXiv ID:** 2505.16135
- **Source URL:** https://arxiv.org/abs/2505.16135
- **Reference count:** 6
- **Primary result:** Even best current LLMs solve fewer than 15% of challenging Sudoku variants unaided

## Executive Summary
Sudoku-Bench introduces a new benchmark for evaluating creative, multi-step reasoning in large language models using challenging Sudoku variants. Unlike traditional benchmarks that reward memorization, these variants require novel logical breakthroughs due to unique and interacting constraints. Baseline experiments show that even the best current LLMs solve fewer than 15% of the puzzles unaided, highlighting the difficulty of long-horizon, strategic reasoning tasks. The benchmark includes 100 carefully curated puzzles with standardized text-based representation and tools for interaction, plus thousands of expert reasoning traces from the Cracking the Cryptic YouTube channel.

## Method Summary
The benchmark evaluates LLMs on 100 curated Sudoku variants using text-based reasoning. The challenge_100 dataset contains 15 4×4, 15 6×6, and 70 9×9 puzzles with standardized text representation including rules, initial grid, and coordinate-based visual elements. Evaluation uses two modes: single-shot (complete solution in one response) and multi-step (incremental placement with board-state feedback, terminating on incorrect placement). No external tools are used for baselines. The dataset and evaluation framework are available on HuggingFace and GitHub, with expert reasoning traces from Cracking the Cryptic provided for imitation learning research.

## Key Results
- Current LLMs solve fewer than 15% of Sudoku variants unaided
- Performance collapses from 40-73% on 4×4 to near-zero on 9×9 grids
- Most common failure mode is "Incorrect Solution" - confident wrong answers without recognized error
- Models frequently claim puzzles are underspecified or contain contradictions when they cannot find break-in strategies

## Why This Works (Mechanism)

### Mechanism 1
Sudoku variants resist memorization-based solutions by introducing unique constraint interactions that require novel logical insights rather than pattern retrieval. Each puzzle features unique or subtly interacting constraints, making memorization of solution templates infeasible and forcing solvers to discover puzzle-specific logical breakthroughs.

### Mechanism 2
The "break-in" requirement forces models to engage in strategic meta-reasoning about which techniques to apply, rather than incremental deduction from obvious starting points. Many puzzles start with minimal or no given digits, requiring solvers to spend time understanding constraint interactions before any placement is possible—this necessitates deciding upfront which reasoning techniques to employ.

### Mechanism 3
Complex puzzles demand maintaining logical consistency across hundreds of steps with persistent deductions, testing long-horizon memory and context management. Initial deductions remain pertinent throughout the solve, meaning that robustly solving some puzzles over 100s of steps will either require externalized memory or very long context windows.

## Foundational Learning

- **Concept: Constraint Satisfaction Problems (CSPs)**
  - Why needed here: Sudoku variants are CSPs where understanding constraint propagation is fundamental. The benchmark specifically tests whether models can reason about constraint interactions rather than pattern-match solutions.
  - Quick check question: In a standard 9×9 Sudoku, if you place digit 5 in row 1, column 1, which cells become constrained and why?

- **Concept: Memorization vs. Generalization in Sequence Models**
  - Why needed here: The benchmark's core value proposition depends on distinguishing genuine reasoning from pattern retrieval. Understanding when models extract generalizable principles vs. memorize surface patterns is essential for interpreting results.
  - Quick check question: Why might a model achieve 90% accuracy on training-distribution math problems but fail on isomorphic problems with different surface formatting?

- **Concept: Search vs. Insight in Problem-Solving**
  - Why needed here: The paper explicitly contrasts human preference for logical shortcuts with LLM tendency toward brute-force search. Understanding this distinction helps diagnose failure modes.
  - Quick check question: What's the computational difference between solving Sudoku via systematic constraint propagation versus exhaustive tree search with backtracking?

## Architecture Onboarding

- **Component map:**
  Puzzle Dataset (HuggingFace) -> Text extraction and validation -> Model evaluation loop -> Performance metrics and failure analysis

- **Critical path:**
  1. Load puzzle from dataset (text representation: rules + initial grid + visual elements as coordinates)
  2. Model generates reasoning trace and outputs digit placement(s)
  3. Validate placement against constraints (incorrect placement terminates multi-step evaluation)
  4. Iterate until solved or failure; categorize failure mode if applicable

- **Design tradeoffs:**
  - **Text vs. Visual input**: Text isolates reasoning from vision but excludes geometric/symmetry-based break-ins; paper notes vision models struggle with coordinate extraction
  - **Tool-assisted vs. unaided**: Unaided evaluation tests intrinsic reasoning; tools would enable constraint-solver approaches that bypass the targeted "break-in" reasoning
  - **Multi-step vs. single-shot**: Multi-step provides feedback and tests incremental reasoning; single-shot tests complete solution generation but may conflate planning and execution failures

- **Failure signatures:**
  - **Incorrect Solution** (most common): Confident wrong answer without recognized error
  - **Missing Information**: Claims puzzle underspecified (likely due to few given digits—variants often start empty vs. 17 minimum in vanilla Sudoku)
  - **Claimed Contradiction**: Mistakenly identifies logical impossibility in valid rules
  - **Surrender**: Explicitly gives up mid-solve
  - **No Reasoning Trace**: Cannot categorize failure mode from output

- **First 3 experiments:**
  1. Establish baseline: Run target model on full 100-puzzle benchmark in both modes; stratify results by grid size to identify where performance collapses (expect 4×4 >> 6×6 >> 9×9 based on paper findings).
  2. Failure mode analysis: For unsolved puzzles, categorize failures using the taxonomy; if "Missing Information" dominates, investigate whether model struggles with sparse initial grids or novel rule parsing.
  3. Break-in probing: Manually inspect model reasoning on 3-5 puzzles where it failed; determine whether it attempted search-based approaches versus strategic constraint analysis—this diagnoses whether the gap is search efficiency or strategic reasoning capability.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning or imitation learning on the provided expert reasoning traces (Cracking the Cryptic dataset) enable LLMs to adopt "human-like" logical break-ins rather than relying on brute-force search? The authors explicitly ask this and release the CTC dataset to facilitate this research. This remains unresolved because baseline experiments show current models fail to identify novel logical entry points, resorting to inefficient search strategies.

### Open Question 2
Can advanced multimodal models solve Sudoku variants using visual inputs alone, given the failure of current models to parse visual elements accurately? The paper states solving this benchmark using vision would represent a significant improvement over current multimodal LLMs. This remains unresolved because the authors opted for text-based representation due to current models' struggles with visual feature extraction.

### Open Question 3
How effectively can LLMs utilize external constraint solvers as tools to handle puzzles that require translating natural language rules into formal logic? The authors note future work could consider a separate tool-use track for puzzles requiring natural language understanding. This remains unresolved because the current evaluation assesses intrinsic reasoning without tools.

## Limitations

- **Data generation bias**: The 100 puzzles were curated from a YouTube channel with expert solvers, potentially overrepresenting certain puzzle types or difficulty distributions
- **Text vs. visual reasoning**: The text-based representation excludes geometric and symmetry-based break-ins that are crucial to some variants
- **Sampling and temperature effects**: Baseline results don't report temperature or sampling parameters, which could affect performance characterization

## Confidence

- **High confidence**: The benchmark's utility for evaluating long-horizon, multi-step reasoning is well-supported with reproducible evaluation framework
- **Medium confidence**: The claim that variants resist memorization is plausible but lacks systematic evaluation of memorization potential
- **Medium confidence**: The meta-reasoning requirement is logically compelling but evidence is primarily conceptual rather than empirical

## Next Checks

1. **Training data audit**: Analyze whether popular LLMs were trained on Cracking the Cryptic or similar Sudoku variant content to validate memorization-resistance claims
2. **Cross-representation evaluation**: Run a subset of puzzles through vision models using the original visual format to quantify what reasoning capabilities are lost in text abstraction
3. **Memory mechanism ablation**: Test whether models with structured external memory significantly outperform standard models on complex 9×9 variants to validate memory-demand as primary bottleneck