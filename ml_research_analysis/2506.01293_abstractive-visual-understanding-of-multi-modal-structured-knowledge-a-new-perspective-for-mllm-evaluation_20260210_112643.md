---
ver: rpa2
title: 'Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New
  Perspective for MLLM Evaluation'
arxiv_id: '2506.01293'
source_url: https://arxiv.org/abs/2506.01293
tags:
- mllms
- multi-modal
- visual
- task
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3STR, a new benchmark for evaluating multi-modal
  large language models (MLLMs) on abstractive visual understanding of structured
  knowledge. The authors address the gap in existing MLLM evaluation by focusing on
  comprehension of visual representations of structured knowledge like knowledge graphs.
---

# Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation

## Quick Facts
- arXiv ID: 2506.01293
- Source URL: https://arxiv.org/abs/2506.01293
- Reference count: 40
- Introduces M3STR benchmark for evaluating MLLMs on abstractive visual understanding of structured knowledge

## Executive Summary
This paper introduces M3STR, a new benchmark for evaluating multi-modal large language models (MLLMs) on abstractive visual understanding of structured knowledge. The authors address the gap in existing MLLM evaluation by focusing on comprehension of visual representations of structured knowledge like knowledge graphs. M3STR employs multi-modal knowledge graphs as data sources, synthesizing images that encode both entities and relational topologies. The benchmark includes three task types (count, detection, completion) with seven subtasks targeting entity and relation understanding.

## Method Summary
M3STR uses multi-modal knowledge graphs as data sources to synthesize images that encode entities and relational topologies. The benchmark includes three task types (count, detection, completion) with seven subtasks targeting entity and relation understanding. Evaluation was conducted on 26 mainstream MLLMs to assess their performance on these tasks, revealing that current models struggle with abstractive visual content, particularly in tasks requiring fine-grained perception of relational structures.

## Key Results
- Qwen2.5-VL-72B achieved the best performance among evaluated models
- Smaller MLLMs showed limited capability on abstractive visual understanding tasks
- Current MLLMs struggle with fine-grained perception of relational structures in visual data

## Why This Works (Mechanism)
The mechanism underlying M3STR's effectiveness appears to stem from its focus on abstractive visual understanding of structured knowledge. By using multi-modal knowledge graphs as data sources and synthesizing images that encode both entities and relational topologies, the benchmark challenges models to go beyond simple pattern recognition and engage in more complex reasoning about visual representations of knowledge structures. The inclusion of tasks requiring both entity and relation understanding, across count, detection, and completion scenarios, provides a comprehensive assessment of a model's ability to interpret and reason about abstract visual content.

## Foundational Learning
The foundational learning approach employed by M3STR leverages multi-modal knowledge graphs to create a rich training and evaluation environment. This approach assumes that by exposing models to visual representations of structured knowledge, they can develop a deeper understanding of how information is organized and related in abstract visual formats. The use of synthetic image generation to create these visual representations allows for controlled experimentation and the creation of diverse scenarios for model evaluation.

## Architecture Onboarding
For models to effectively engage with M3STR, they likely need to possess certain architectural features. These may include strong visual encoding capabilities to process the synthetic images, robust relational reasoning mechanisms to understand the connections between entities, and the ability to integrate visual and textual information effectively. The benchmark's focus on abstractive visual content suggests that models with transformer-based architectures and multi-modal fusion techniques may be particularly well-suited to handle these tasks.

## Open Questions the Paper Calls Out
- How well do MLLMs generalize from synthetic knowledge graph visualizations to real-world representations?
- What is the impact of synthetic data generation artifacts on model performance?
- How do MLLMs compare in their ability to reason about visual versus linguistic representations of structured knowledge?
- Are there specific architectural features that significantly improve performance on abstractive visual understanding tasks?

## Limitations
- The evaluation focuses on synthetic data rather than real-world knowledge graph visualizations
- Benchmark relies on synthetic image generation which may introduce artifacts affecting model performance
- Only 26 models were evaluated, potentially not representing full diversity of MLLM architectures
- The abstractive nature of the visual content may not fully capture all aspects of real-world knowledge graph representations

## Confidence
- Confidence in claims about MLLMs' struggles with abstractive visual content: Medium
- Confidence in claims about weakness in understanding relational structures: Low
- Confidence in comparison between visual and linguistic reasoning: Medium

## Next Checks
1. Evaluate model performance on real-world knowledge graph visualizations from diverse sources
2. Conduct ablation studies to isolate impact of synthetic data generation artifacts on model performance
3. Compare model performance on equivalent visual and textual representations of same structured knowledge