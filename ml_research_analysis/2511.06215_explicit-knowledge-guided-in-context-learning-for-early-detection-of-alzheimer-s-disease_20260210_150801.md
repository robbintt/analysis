---
ver: rpa2
title: Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's
  Disease
arxiv_id: '2511.06215'
source_url: https://arxiv.org/abs/2511.06215
tags:
- label
- task
- ek-icl
- knowledge
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of detecting Alzheimer's Disease
  (AD) from narrative transcripts using Large Language Models (LLMs), especially under
  out-of-distribution (OOD) and low-data conditions. The core challenge lies in LLMs'
  difficulty with task recognition and demonstration selection when transcripts lack
  semantic contrast and label words are misaligned with the task.
---

# Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease

## Quick Facts
- arXiv ID: 2511.06215
- Source URL: https://arxiv.org/abs/2511.06215
- Authors: Puzhen Su; Yongzhu Miao; Chunxi Guo; Jintao Tang; Shasha Li; Ting Wang
- Reference count: 18
- Primary result: EK-ICL achieves 93.75% accuracy and 93.33% F1-score on AD detection from transcripts

## Executive Summary
This paper addresses the challenge of detecting Alzheimer's Disease from narrative transcripts using Large Language Models (LLMs), particularly under out-of-distribution and low-data conditions. The core problem is that standard in-context learning fails when transcripts lack semantic contrast and label words misalign with LLM priors. The authors propose Explicit Knowledge In-Context Learners (EK-ICL), which integrates structured explicit knowledge into ICL through three components: SLM-derived confidence scores, parsing-based feature scores, and in-distribution label replacement. Experiments on three AD datasets show EK-ICL significantly outperforms both fine-tuning and standard ICL baselines, achieving state-of-the-art performance with up to 93.75% accuracy.

## Method Summary
EK-ICL is a framework that integrates explicit knowledge into in-context learning for AD detection from narrative transcripts. The method uses a three-component approach: (1) confidence scores from a small language model (SLM) to ground predictions in task-relevant patterns, (2) parsing feature scores to capture syntactic differences between AD and control transcripts for better demonstration retrieval, and (3) in-distribution label replacement (Good/Bad) to align with LLM pre-trained knowledge. The framework employs parsing-based retrieval and ensemble prediction to improve reasoning stability. The method is evaluated on three AD datasets (ADReSS, Lu corpora, Pitt corpora) using accuracy, precision, recall, and F1-score as metrics, with a llama3.1-8B-instruct LLM and bert-base-uncased SLM.

## Key Results
- EK-ICL achieves 93.75% accuracy and 93.33% F1-score on ADReSS-Test, significantly outperforming both fine-tuning and standard ICL baselines
- Ablation studies confirm each knowledge component is critical: removing SLM confidence scores causes near-random performance (50% accuracy), and OOD labels cause stagnation even with increased demonstrations
- EK-ICL demonstrates strong generalization across three different AD datasets under low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLM-derived confidence scores ground LLM predictions in task-relevant patterns and stabilize task recognition under OOD conditions
- Mechanism: A lightweight SLM uses noise injector (Gumbel-Softmax perturbation) and contribution judger (linear probe with sigmoid) to compute token-level contribution weights, aggregating into confidence scores via global max-pooling
- Core assumption: SLM logits capture task-relevant patterns that LLMs struggle to recognize directly in OOD settings
- Evidence anchors: [abstract] mentions SLM confidence scores; [section II.A] provides equations for Injector, Judger, and confidence computation; corpus support for this specific mechanism is weak
- Break condition: If SLM is poorly trained or data is too scarce for SLM to learn meaningful patterns, confidence scores may mislead rather than guide

### Mechanism 2
- Claim: Parsing-based feature scores and retrieval capture structural differences between AD and control transcripts that semantic similarity misses
- Mechanism: Transcripts are decomposed into six parsing categories (Subject, Object, Action, Location, Filler, Pronoun), with contribution weights forming feature vectors used for parsing similarity-based retrieval
- Core assumption: AD and control transcripts differ systematically in syntactic structure (e.g., fillers, pronouns) in ways LLMs cannot detect from semantic content alone
- Evidence anchors: [abstract] mentions parsing feature scores; [Fig. 2] shows differential parsing category distributions; partial conceptual overlap with Delta-KNN paper
- Break condition: If parsing categories do not differ between classes in target dataset, or if parsing errors propagate, retrieval quality degrades

### Mechanism 3
- Claim: In-distribution (ID) label word replacement aligns task semantics with LLM pre-trained knowledge, mitigating label-task mismatch
- Mechanism: Replace domain-specific labels ("Alzheimer"/"Control") with semantically universal labels ("Good"/"Bad") to leverage LLM pre-trained associations with universal concepts
- Core assumption: LLMs have stronger pre-trained priors for universal semantic oppositions (Good/Bad) than for clinical labels, and this alignment is critical for ICL in OOD tasks
- Evidence anchors: [abstract] mentions label word replacement; [Fig. 5] shows dramatic performance variation across 30 label pairs; no direct corpus validation of this specific hypothesis
- Break condition: If target task lacks a natural polarity that maps to pre-trained concepts, or if label semantics conflict with task context, replacement fails

## Foundational Learning

- Concept: **In-Context Learning (ICL) limitations**
  - Why needed here: Understanding why standard ICL fails in AD detection (task recognition failure, semantic homogeneity, label misalignment) is prerequisite to appreciating EK-ICL's design
  - Quick check question: Can you explain why semantic similarity retrieval fails when all transcripts describe the same picture?

- Concept: **Syntactic parsing for clinical text**
  - Why needed here: EK-ICL relies on parsing categories (Actions, Fillers, Pronouns) to distinguish AD from control speech; understanding CHAT protocol and parsing extraction is essential
  - Quick check question: What parsing category shows the largest frequency difference between AD and control groups in Fig. 2?

- Concept: **OOD generalization in clinical NLP**
  - Why needed here: The paper explicitly targets out-of-distribution scenarios where training and test distributions differ; this framing motivates all design choices
  - Quick check question: Why would a model trained on ADReSS struggle on the Pitt corpus without explicit knowledge transfer?

## Architecture Onboarding

- Component map: Input Transcript → SLM Assessor → Confidence Scores (S_conf) → Parsing Decomposer → Feature Scores (S_feat) → Parsing Search → Demo Retrieval → Label Replacement (ID labels: Good/Bad) → Ensemble In-Context Learners (3 learners, 1 demo each) → Majority Vote → Prediction

- Critical path: SLM training → Parsing feature extraction → Label configuration → Retrieval → Ensemble inference. The SLM Assessor is trained first on available data; all downstream components depend on its outputs.

- Design tradeoffs:
  - SLM size vs. speed: Heavier SLM may yield better confidence scores but adds latency
  - Ensemble size: Paper uses 3 learners; more learners increase stability but cost
  - Label selection: Good/Bad works best in experiments, but may not generalize to all clinical tasks

- Failure signatures:
  - Accuracy drops to ~50% with random/semantic retrieval (Table I: Vanilla ICL)
  - Removing confidence scores causes near-random performance (Table II: 50% accuracy, undefined F1)
  - OOD labels (Alzheimer/Control) cause stagnation even with increased demos (Fig. 4b)

- First 3 experiments:
  1. **Sanity check**: Run Vanilla ICL (shot=1, random demos) on ADReSS-Test with Alzheimer/Control labels; expect ~65% accuracy per Table I
  2. **Component isolation**: Add only parsing-based retrieval (no SLM confidence, no label replacement); expect ~83-87% accuracy per ablation
  3. **Full EK-ICL**: Enable all three knowledge components with Good/Bad labels; verify ~93% accuracy on ADReSS-Test matches paper claim

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability of label alignment may not hold for many clinical or domain-specific tasks where labels lack natural semantic polarities
- Parsing category validity depends on systematic syntactic differences between AD and control transcripts, which may not generalize across different datasets
- SLM dependency means poor SLM training or scarce data can cause the entire framework to collapse to random guessing

## Confidence
- **High confidence**: Standard ICL struggles with task recognition and demo selection in semantically homogeneous transcripts is well-supported by dramatic performance differences across ablation studies
- **Medium confidence**: Parsing-based retrieval mechanism is plausible given reported category distributions but lacks cross-dataset validation
- **Low confidence**: SLM confidence mechanism has weak corpus support and lacks ablation data showing SLM performance alone

## Next Checks
1. **Ablation on SLM-only performance**: Train and evaluate the SLM assessor independently on held-out validation data to verify it learns meaningful task-relevant patterns before using its outputs for guidance

2. **Cross-dataset parsing validation**: Apply the parsing category extraction to an independent AD dataset (not ADReSS, Lu, or Pitt) to verify that the assumed syntactic differences between AD and control groups hold across different transcript collections and protocols

3. **Label generalization test**: Systematically test EK-ICL with alternative label pairs (e.g., Positive/Negative, Patient/Healthy) on the same datasets to quantify how much performance depends on the specific Good/Bad alignment versus the framework's other components