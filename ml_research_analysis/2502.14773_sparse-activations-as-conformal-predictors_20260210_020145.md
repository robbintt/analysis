---
ver: rpa2
title: Sparse Activations as Conformal Predictors
arxiv_id: '2502.14773'
source_url: https://arxiv.org/abs/2502.14773
tags:
- prediction
- conformal
- entmax
- coverage
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new connection between conformal prediction\
  \ and sparse softmax-like transformations, such as sparsemax and \u03B3-entmax,\
  \ for uncertainty quantification in classification. The authors derive new non-conformity\
  \ scores that make the calibration process equivalent to temperature scaling of\
  \ these sparse transformations."
---

# Sparse Activations as Conformal Predictors

## Quick Facts
- arXiv ID: 2502.14773
- Source URL: https://arxiv.org/abs/2502.14773
- Authors: Margarida M. Campos; João Calém; Sophia Sklaviadis; Mário A. T. Figueiredo; André F. T. Martins
- Reference count: 40
- Primary result: Sparse softmax-like transformations (sparsemax, γ-entmax) can be used as non-conformity scores in conformal prediction, achieving competitive coverage and efficiency

## Executive Summary
This paper establishes a novel theoretical connection between sparse softmax transformations and conformal prediction, demonstrating that temperature-scaled sparsemax and γ-entmax can serve as effective non-conformity scores. The authors show that calibrating these sparse transformations is equivalent to temperature scaling in the conformal prediction framework, providing a principled way to construct prediction sets with guaranteed coverage. Through experiments on computer vision and text classification tasks, the method achieves competitive performance compared to standard softmax-based approaches while offering the interpretability benefits of sparse probability distributions.

## Method Summary
The authors derive a new class of non-conformity scores based on sparse softmax transformations, specifically sparsemax and γ-entmax. They establish that the calibration process for these scores is mathematically equivalent to temperature scaling, allowing the temperature parameter to be learned from calibration data. At test time, applying the sparse transformation with the calibrated temperature produces prediction sets that inherit the coverage guarantees of conformal prediction. This approach provides a theoretically grounded alternative to traditional softmax-based non-conformity scores while maintaining computational efficiency and interpretability through sparse probability distributions.

## Key Results
- Sparsemax and γ-entmax transformations with calibrated temperature achieve competitive coverage rates compared to softmax-based conformal prediction
- The method demonstrates strong adaptiveness, producing prediction sets that better reflect model uncertainty
- Experiments on CIFAR-10, CIFAR-100, and IMDb benchmarks show the approach maintains efficiency while providing guaranteed coverage

## Why This Works (Mechanism)
The mechanism works because sparse softmax transformations naturally produce probability distributions with exact zeros, making them interpretable as prediction sets where only non-zero entries are included. The temperature scaling equivalence ensures that the calibration process maintains the theoretical guarantees of conformal prediction while adapting the sparsity level to the data. This connection allows the method to leverage the mathematical foundations of conformal prediction while benefiting from the interpretability and efficiency of sparse probability distributions.

## Foundational Learning
- **Conformal Prediction**: A framework for uncertainty quantification that provides statistically valid prediction sets; needed to understand the theoretical guarantees and calibration process
  - Quick check: Verify that the calibration procedure maintains marginal coverage probability
- **Sparsemax Transformation**: A softmax variant that produces sparse probability distributions; needed for interpretable prediction sets with exact zeros
  - Quick check: Confirm that sparsemax outputs sum to 1 with some entries exactly zero
- **γ-entmax**: A generalization of sparsemax with tunable sparsity; needed for controlling the degree of sparsity in probability distributions
  - Quick check: Verify that γ-entmax reduces to softmax when γ=1 and sparsemax when γ=2
- **Temperature Scaling**: A calibration technique for adjusting model confidence; needed to understand the equivalence between sparse transformation calibration and conformal prediction
  - Quick check: Ensure temperature scaling preserves the argmax of the distribution
- **Non-conformity Scores**: Measures of how different a new example is from training examples; needed to construct prediction sets in conformal prediction
  - Quick check: Verify that non-conformity scores are permutation invariant to the order of calibration examples
- **Prediction Sets**: Sets of labels that contain the true label with guaranteed probability; needed to understand the final output format and coverage guarantees
  - Quick check: Confirm that prediction sets have the desired coverage on calibration data

## Architecture Onboarding

Component Map:
Model (classification network) -> Sparse transformation (sparsemax/γ-entmax) -> Temperature scaling -> Non-conformity scores -> Prediction sets

Critical Path:
1. Model produces logits for each class
2. Logits passed through sparse transformation with temperature parameter
3. Temperature parameter learned from calibration data
4. Non-conformity scores computed from sparse probabilities
5. Prediction sets constructed at test time using calibrated threshold

Design Tradeoffs:
- Sparsity vs. coverage: More sparse distributions may exclude true labels, reducing coverage
- Temperature calibration: Higher temperatures increase smoothness but may reduce sparsity benefits
- Computational overhead: Sparse transformations add minimal overhead compared to softmax

Failure Signatures:
- Poor coverage: Indicates temperature parameter not properly calibrated or model poorly calibrated
- Overly sparse predictions: May miss true labels, suggests temperature too low
- No sparsity: Temperature too high, reduces to standard softmax behavior

First Experiments:
1. Verify coverage on calibration set matches theoretical guarantees
2. Compare sparsity levels between sparsemax, γ-entmax, and softmax on validation set
3. Test prediction set size distribution across different confidence levels

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical evaluation limited to standard benchmarks without out-of-distribution testing
- Assumes correctness of base model architecture and training procedure
- Computational efficiency and scalability to larger models not thoroughly investigated

## Confidence
- Theoretical claims connecting sparse transformations to conformal prediction: High
- Empirical results on standard benchmarks: Medium
- Practical utility versus standard approaches: Medium
- Scalability to larger models and datasets: Low

## Next Checks
1. Evaluate performance on out-of-distribution datasets (CIFAR-10-C, ImageNet-O) to assess robustness beyond in-distribution calibration
2. Compare computational overhead and memory requirements of sparsemax-based conformal prediction against traditional approaches on larger models (ResNet-50/101, ViT)
3. Test behavior under covariate shift conditions where feature distribution changes but label space remains constant