---
ver: rpa2
title: Design and testing of an agent chatbot supporting decision making with public
  transport data
arxiv_id: '2505.22698'
source_url: https://arxiv.org/abs/2505.22698
tags:
- data
- chatbot
- query
- such
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an agent-based chatbot designed to support
  decision-making with public transport data by allowing users to interact with structured
  datasets using natural language. The chatbot employs an agent architecture where
  a core Large Language Model (LLM) generates SQL queries based on user questions,
  and a suite of tools execute these queries, verify their correctness, and visualize
  the results through maps and charts.
---

# Design and testing of an agent chatbot supporting decision making with public transport data

## Quick Facts
- arXiv ID: 2505.22698
- Source URL: https://arxiv.org/abs/2505.22698
- Reference count: 14
- The chatbot achieved correct answers in approximately 53% of cases for simpler queries while struggling with complex query templates

## Executive Summary
This paper presents an agent-based chatbot system designed to enable natural language interaction with public transport data stored in GTFS format. The chatbot uses a core Large Language Model (LLM) to generate SQL queries from user questions, supported by a suite of tools for query verification, execution, and visualization through maps and charts. The system was tested on transport data from Bologna, Ferrara, and Milan, with performance evaluation involving 146 template-based questions compared against manually created "gold" queries. The study provides both technical insights into LLM-based query generation for structured data and a methodology for quantitative performance assessment.

## Method Summary
The system processes GTFS datasets from Italian cities into a relational database format, then uses an agent architecture where the LLM generates SQL queries based on natural language input. A set of tools executes these queries, verifies their correctness, and provides visualizations. Performance was evaluated through a workflow that repeatedly tested the chatbot with questions derived from templates, storing generated queries and comparing results to manually created gold queries. The evaluation focused on both accuracy rates and identification of common error types including SQL syntax issues and logical mistakes in query generation.

## Key Results
- The chatbot provided correct answers in approximately 53% of cases for two simpler query templates
- Performance significantly degraded for complex queries requiring substantial adaptation of example patterns
- Common error types were identified as SQL syntax issues and logical mistakes in query generation
- The evaluation methodology established a reproducible framework for testing LLM-based query systems

## Why This Works (Mechanism)
The system works by leveraging the LLM's ability to understand natural language and map it to structured SQL queries, while using supporting tools to verify and execute these queries. The agent architecture allows the system to handle the complexity of translating user intent into database operations through a multi-step process. The visualization tools help bridge the gap between raw data and actionable insights for decision-makers.

## Foundational Learning
- **GTFS data format**: Standard format for public transport schedules and associated geographic information; needed to understand the data structure being queried
- **SQL query generation**: Process of converting natural language to structured database queries; quick check: verify generated SQL syntax is valid
- **Agent architecture**: Multi-component system where different tools handle specific tasks; quick check: ensure each tool has clear input/output interfaces
- **Query verification**: Process of checking SQL correctness before execution; quick check: compare generated queries against known valid patterns
- **Performance benchmarking**: Methodology for measuring system accuracy against ground truth; quick check: ensure evaluation dataset is representative

## Architecture Onboarding

**Component Map:**
User Input -> LLM Core -> SQL Query Generator -> Query Verifier -> Database Executor -> Visualization Tools -> Results Output

**Critical Path:**
User question → LLM interpretation → SQL generation → Query verification → Database execution → Result visualization

**Design Tradeoffs:**
The system trades computational efficiency for flexibility, as the agent-based approach with multiple tool interactions is likely slower than direct SQL querying but enables natural language interaction. This design prioritizes accessibility over raw performance.

**Failure Signatures:**
Common failures include SQL syntax errors when the LLM generates invalid queries, logical mistakes where the query structure doesn't match user intent, and visualization errors when the result format doesn't match what the visualization tools expect.

**First 3 Experiments:**
1. Test the system with a simple query like "How many bus routes operate in Bologna?" to verify basic functionality
2. Attempt a more complex query involving multiple tables, such as "Show the most frequent bus stops in Milan"
3. Evaluate the system's response to an ambiguous question to test how well it handles uncertainty

## Open Questions the Paper Calls Out
None

## Limitations
- Results are limited to GTFS datasets from three Italian cities, affecting generalizability
- The evaluation relies on manually created "gold" queries which may not capture all valid approaches
- Computational costs and latency of the agent-based approach were not addressed
- User experience aspects like ease of interaction and error recovery were not evaluated

## Confidence

**High Confidence:**
- The chatbot architecture design and data processing pipeline are well-documented and reproducible

**Medium Confidence:**
- The 53% accuracy rate for simple queries is reliable within the tested dataset and query templates
- The identification of common error types (SQL syntax issues and logical mistakes) is supported by the data

## Next Checks

1. Test the chatbot on GTFS datasets from non-Italian cities with different transport network complexities to assess generalizability
2. Conduct user studies with domain experts to evaluate the practical utility and usability of the system beyond accuracy metrics
3. Perform a cost-benefit analysis comparing the agent-based approach to direct SQL querying for various query complexity levels