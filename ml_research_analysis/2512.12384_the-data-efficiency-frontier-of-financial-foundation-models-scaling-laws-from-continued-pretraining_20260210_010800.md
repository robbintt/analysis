---
ver: rpa2
title: 'The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws
  from Continued Pretraining'
arxiv_id: '2512.12384'
source_url: https://arxiv.org/abs/2512.12384
tags:
- loss
- domain
- pretraining
- financial
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the data efficiency of domain-adaptive
  pretraining (DAPT) on financial language by training Llama-3.2 models on SEC filings.
  Models were trained for one epoch over 400M tokens, with validation checkpoints
  at 50M, 100M, 200M, and 400M tokens.
---

# The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining

## Quick Facts
- arXiv ID: 2512.12384
- Source URL: https://arxiv.org/abs/2512.12384
- Authors: Jesse Ponnock
- Reference count: 9
- Key outcome: Shallow scaling exponents reveal that meaningful financial domain adaptation can be achieved with modest token budgets (400M), with minimal degradation on general-domain text.

## Executive Summary
This study investigates the data efficiency of domain-adaptive pretraining (DAPT) on financial language by training Llama-3.2 models on SEC filings. Models were trained for one epoch over 400M tokens, with validation checkpoints at 50M, 100M, 200M, and 400M tokens. SEC domain validation loss decreased steadily for both 1B and 3B models, with the largest improvements occurring in the first 200M tokens and diminishing returns thereafter. Power-law analysis revealed shallow scaling exponents, indicating that financial text is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remained effectively unchanged, showing no signs of catastrophic forgetting.

## Method Summary
The study performed continued pretraining on Llama-3.2-1B and 3B models using a 400M-token corpus of SEC filings (10-K, 10-Q, DEF 14A) sourced from EDGAR. The corpus was cleaned using whitelist extraction (MD&A, Risk Factors, Business Overview, CD&A, Notes to Financial Statements) and deduplicated with MinHash LSH. Models were trained for one epoch with AdamW optimizer (lr=5×10⁻⁶, batch=8), logging validation loss every ~25M tokens. Dual validation sets (4M-token SEC validation set and 4M-token Wikipedia validation set) were used to construct a data-efficiency frontier measuring specialization vs. forgetting.

## Key Results
- SEC domain validation loss decreased steadily for both 1B and 3B models, with largest gains in first 200M tokens
- Power-law fits revealed shallow scaling exponents (|b|≪1), indicating financial language is highly regular and efficiently learnable
- General-domain validation loss remained effectively unchanged across all token budgets, showing no catastrophic forgetting
- Data-efficiency frontier demonstrated specialization improvement with negligible degradation on mixed-domain text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallow scaling exponents enable efficient domain adaptation with modest token budgets for highly structured corpora
- Mechanism: SEC filings follow regular reporting conventions with consistent structure and recurring terminology. This low-entropy distribution allows models to acquire essential patterns quickly, producing shallow power-law exponents that indicate efficient learnability
- Core assumption: Scaling behavior observed on 1B–3B models extrapolates predictably to larger scales (7B–70B)
- Evidence anchors: Power-law fits reveal shallow exponents, indicating financial language is highly regular; fitted exponents are shallow, characteristic of late stage continued pretraining

### Mechanism 2
- Claim: Narrow domain shifts from DAPT preserve general-domain representations without catastrophic forgetting
- Mechanism: SEC text introduces specialized terminology but does not substantially overwrite broad linguistic representations from initial pretraining. The domain shift is sufficiently narrow that general text validation loss remains within noise margins
- Core assumption: Base model has sufficiently broad pretraining that domain-specific tokens refine rather than replace representations
- Evidence anchors: General-domain validation loss remains effectively unchanged across all token budgets; SEC language represents a relatively narrow domain shift

### Mechanism 3
- Claim: Late-stage continued pretraining yields diminishing marginal returns beyond domain-specific saturation point
- Mechanism: Models extract most learnable signals early (50M–200M tokens), after which additional tokens provide incrementally smaller loss reductions. This reflects saturation of domain patterns rather than training inefficiency
- Core assumption: One epoch is sufficient; multi-epoch training does not substantially change saturation dynamics
- Evidence anchors: Largest gains occurring within first 200M tokens and diminishing returns thereafter; beyond roughly 250M tokens, both curves begin to flatten

## Foundational Learning

- **Power-law scaling in language models**:
  - Why needed here: Central analysis relies on fitting power-law relationships between tokens seen and validation loss to project data requirements
  - Quick check question: Can you explain why log-log plots reveal scaling relationships that appear flat in linear space?

- **Catastrophic forgetting in transfer learning**:
  - Why needed here: Key claim is that DAPT on 400M SEC tokens does not degrade general-domain performance; understanding forgetting mechanisms helps assess when this claim generalizes
  - Quick check question: What factors determine whether fine-tuning on domain A degrades performance on domain B?

- **Decoder-only transformer pretraining objectives**:
  - Why needed here: Paper measures "loss" throughout; understanding what next-token prediction loss represents is essential for interpreting results
  - Quick check question: What does a reduction in validation loss imply about a model's distributional knowledge?

## Architecture Onboarding

- **Component map**: SEC EDGAR → EdgarTools extraction → whitelist filtering (MD&A, Risk Factors, etc.) → MinHash LSH deduplication → Llama-3 tokenizer → 1024-token packing → Llama-3.2 base → full-parameter DAPT → AdamW training → dual validation

- **Critical path**: Corpus construction and deduplication quality directly affects saturation point; checkpoint cadence must capture early gains for meaningful scaling analysis; dual validation sets required to construct data-efficiency frontier

- **Design tradeoffs**: Single-epoch vs. multi-epoch (paper uses one epoch; reuse may help smaller corpora but risks overfitting); whitelist filtering excludes tables/numerical data (conservative) vs. including structured data (higher coverage, noisier); fixed learning rate vs. scheduler (paper uses no scheduler; learning rate decay may extend gains past 400M tokens)

- **Failure signatures**: SEC validation loss plateaus before 100M tokens → corpus may be too small or deduplication too aggressive; general-domain loss increases >0.05 → domain shift too broad; training loss diverges → learning rate too high

- **First 3 experiments**: Replicate with 50M and 100M token budgets to verify early scaling behavior; add mixed-domain validation set containing financial news; test learning rate sensitivity at 1×10⁻⁵ and 2×10⁻⁶

## Open Questions the Paper Calls Out

- Do the observed reductions in SEC-domain validation loss correlate with improved performance on downstream financial tasks?
- Do the extrapolated token requirements for data efficiency hold true for much larger models (7B–70B parameters)?
- Can the diminishing returns seen after 200M tokens be overcome by incorporating more diverse financial text sources?

## Limitations
- Extrapolation of shallow scaling exponents from 1B–3B models to projected 7B–70B architectures lacks empirical validation
- Analysis depends on single epoch of pretraining, which may not capture multi-epoch dynamics or overfitting risks
- Whitelist extraction conservatively excludes structured numerical data and tables, potentially underestimating corpus information density

## Confidence
- **High confidence**: Shallow scaling exponents (|b|≪1) for SEC text adaptation, preservation of general-domain performance within noise margins, early saturation of domain-specific gains within 200M tokens
- **Medium confidence**: Meaningful financial domain adaptation achievable with modest token budgets (400M tokens)
- **Low confidence**: Projection that 7B–70B models remain tractable under same scaling dynamics

## Next Checks
1. Measure performance on financial QA, NER, or classification benchmarks to verify validation loss improvements translate to practical capabilities
2. Run continued pretraining for multiple epochs on 400M-token corpus to identify overfitting thresholds
3. Repeat DAPT pipeline on earnings call transcripts or financial news to test whether shallow scaling exponents persist for less structured financial text