---
ver: rpa2
title: Density Ratio-Free Doubly Robust Proxy Causal Learning
arxiv_id: '2505.19807'
source_url: https://arxiv.org/abs/2505.19807
tags:
- bridge
- function
- kernel
- treatment
- outcome
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses causal effect estimation under unobserved\
  \ confounding in the Proxy Causal Learning (PCL) framework, where proxy variables\
  \ for both treatment and outcome are available. The authors propose two kernel-based\
  \ doubly robust estimators\u2014DRKPV and DRPMMR\u2014that combine outcome bridge\
  \ and treatment bridge functions to identify causal effects without explicit density\
  \ ratio estimation."
---

# Density Ratio-Free Doubly Robust Proxy Causal Learning

## Quick Facts
- arXiv ID: 2505.19807
- Source URL: https://arxiv.org/abs/2505.19807
- Authors: Bariscan Bozkurt; Houssam Zenati; Dimitri Meunier; Liyuan Xu; Arthur Gretton
- Reference count: 40
- Primary result: Proposed kernel-based doubly robust estimators (DRKPV, DRPMMR) achieve state-of-the-art performance in proxy causal learning without explicit density ratio estimation

## Executive Summary
This paper addresses causal effect estimation under unobserved confounding using proxy variables for both treatment and outcome. The authors propose two novel kernel-based doubly robust estimators that combine outcome and treatment bridge functions to identify causal effects without explicit density ratio estimation. Leveraging conditional mean embeddings in reproducing kernel Hilbert spaces, both methods admit closed-form solutions and demonstrate superior performance compared to state-of-the-art baselines across synthetic and real-world datasets. The methods are particularly well-suited for continuous or high-dimensional treatments where traditional density ratio approaches become unstable.

## Method Summary
The proposed approach uses kernel mean embeddings to estimate dose-response curves under proxy confounding. Two estimators are developed: DRKPV (three-stage) and DRPMMR (single-stage), both combining outcome bridge functions (KPV or PMMR) with treatment bridge functions (KAP). The key innovation is reformulating the treatment bridge objective to avoid explicit density ratio estimation, instead using decoupled expectations in RKHS. The doubly robust property ensures consistency if either bridge function is correctly specified. The final estimate is computed as θ̂_DR(a) = θ̂₁(a) + θ̂₂(a) - θ̂₃(a), where θ̂₃ is a slack term that cancels errors when one bridge function is misspecified.

## Key Results
- DRKPV and DRPMMR outperform state-of-the-art PCL baselines, including previous doubly robust approaches
- Methods demonstrate robustness under bridge function misspecification, maintaining consistency when either outcome or treatment bridge is correct
- Successfully handle continuous and high-dimensional treatments without kernel smoothing
- Show superior performance on synthetic data and real-world dSprite dataset with 4096-dimensional treatments

## Why This Works (Mechanism)

### Mechanism 1
The doubly robust property ensures consistency if either bridge function is correctly specified. The identification formula θ^(DR) = E[φ(Y - h) | A=a] + E[h] acts as a bias-cancellation circuit where the slack term cancels errors when one bridge function is correct.

**Core assumption:** The causal structure follows the PCL DAG and either Assumption 2.3 (Outcome completeness) or Assumption 2.5 (Treatment completeness) holds.

**Evidence anchors:**
- Abstract: "The methods also exhibit robustness under misspecification of bridge functions, with the doubly robust property ensuring consistency if either bridge function is correctly specified."
- Section 2.3: Theorem 2.7 proves identification relies on either h solving Eq (1) OR φ solving Eq (2).

**Break condition:** Both bridge functions are misspecified simultaneously, or proxies lack sufficient variation relative to the confounder.

### Mechanism 2
The method enables causal estimation without explicit density ratio estimation by reformulating the treatment bridge objective using decoupled expectations E_W E_A[·], replacing density estimation with regression in RKHS.

**Core assumption:** The treatment bridge function φ_0 exists in the Reproducing Kernel Hilbert Space (RKHS).

**Evidence anchors:**
- Abstract: "identification strategy builds on a recent density ratio-free method... it does not require indicator functions or kernel smoothing over the treatment variable."
- Section 3.2: Equation 4 shows simplification where the ratio vanishes into decoupled expectations.

**Break condition:** The kernel choice fails to approximate required relationships, or treatment proxy provides no information about the confounder.

### Mechanism 3
Closed-form solutions are achievable for continuous and high-dimensional treatments using Conditional Mean Embeddings (CMEs) to compute conditional expectations as inner products in Hilbert space, replacing numerical integration with algebraic solutions.

**Core assumption:** The regularity condition holds such that E[g(Z,W)|A] ∈ H_A for relevant functions g.

**Evidence anchors:**
- Abstract: "By using kernel mean embeddings, we have closed-form solutions... especially well-suited for continuous or high-dimensional treatments."
- Section 3.2: Derivation shows E[φ̂(Z,a)ĥ(W,a)|A=a] ≈ Σ ξ_i(a)φ̂(z_i,a)ĥ(w_i,a).

**Break condition:** Computational cost of kernel matrix inversion becomes prohibitive, or regularization is poorly tuned.

## Foundational Learning

**Concept: Completeness Assumptions (2.3 & 2.5)**
- **Why needed here:** These are the "informativeness" requirements for proxies. You must verify that Z varies enough with U (for outcome bridge) and W varies enough with U (for treatment bridge). Without one of these, the respective bridge function does not exist.
- **Quick check question:** Does the treatment proxy Z capture all confounding modes present in the outcome proxy W, or vice versa?

**Concept: Reproducing Kernel Hilbert Spaces (RKHS) & Representer Theorem**
- **Why needed here:** This allows the infinite-dimensional optimization (finding functions h and φ) to reduce to a finite-dimensional vector optimization (finding coefficients α). It is the engine behind the "closed-form" claims.
- **Quick check question:** Can you express the solution to a regularized least-squares functional problem as a linear combination of kernel evaluations?

**Concept: Conditional Mean Embeddings (CMEs)**
- **Why needed here:** CMEs allow you to treat probability distributions (and conditional expectations) as vectors (points) in Hilbert space. This lets you apply linear algebra (ridge regression) to solve statistical problems.
- **Quick check question:** How do you represent the conditional distribution P(Z|A=a) as a vector for use in a linear regression equation?

## Architecture Onboarding

**Component map:** Stage 1 (CME Estimation) -> Stage 2 (Bridge Estimation) -> Stage 3 (Aggregation)

**Critical path:** The success of the method depends on the accuracy of Stage 1 (estimating conditional mean embeddings). If the embeddings μ_{W|Z,A} are poor, the bridge functions in Stage 2 will be biased, potentially breaking the double robustness property.

**Design tradeoffs:**
- KPV (2-stage) vs. PMMR (1-step): KPV is more modular but requires careful data splitting; PMMR is more direct but sensitive to noise amplification.
- Data Splitting: The paper notes reusing data across stages is possible but theoretically requires distinct sample sizes (n_h, m_h, t) for consistency guarantees.

**Failure signatures:**
- Variance Explosion: If proxies are weak (low completeness), kernel matrices may be ill-conditioned, requiring large regularization which kills signal.
- High-Dim Treatments: If PKDR fails but DRKPV works, it confirms the benefit of avoiding kernel smoothing.

**First 3 experiments:**
1. **Synthetic Low Dimensional (Fig 2a):** Validate basic consistency. Check if MSE decreases as sample size increases from 500 to 2000.
2. **Misspecification Test (Fig 3):** Perturb learned bridge coefficients α with noise N(0, 0.2). Verify DR estimate remains close to ground truth while single-bridge methods diverge.
3. **High-Dimensional Treatment (dSprite):** Test on flattened image data (A ∈ R^4096). This validates the mechanism against baseline PKDR which fails here.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can scalable approximations (e.g., random Fourier features, Nyström methods, or neural network representations) be developed for DRKPV and DRPMMR while preserving their consistency guarantees and double robustness properties?
- **Basis in paper:** "A key limitation our work is the computational cost of kernel methods, motivating future work on scalable approximations."
- **Why unresolved:** The paper provides closed-form kernel-based solutions but notes computational cost as a limitation; no approximation schemes are analyzed.
- **What evidence would resolve it:** Derivation and empirical validation of approximate methods showing consistency rates comparable to full kernel methods with reduced computational complexity.

**Open Question 2**
- **Question:** What are the minimax optimal rates for doubly robust proxy causal learning with continuous treatments, and do DRKPV/DRPMMR achieve these rates?
- **Basis in paper:** The paper provides convergence rates (Theorems E.27, E.28) but does not establish minimax optimality or compare to theoretical lower bounds.
- **Why unresolved:** Consistency results give explicit rates but optimality in the doubly robust PCL setting remains uncharacterized.
- **What evidence would resolve it:** Derivation of minimax lower bounds for the doubly robust PCL problem and matching upper bounds from the proposed estimators.

**Open Question 3**
- **Question:** How can the completeness conditions (Assumptions 2.3 and 2.5) be empirically tested or validated from observed data?
- **Basis in paper:** The paper assumes completeness for identification but states "in practice, it is often difficult to know in advance which scenario applies."
- **Why unresolved:** Completeness assumptions are untestable from observational data alone, yet determine which bridge function approach succeeds.
- **What evidence would resolve it:** Development of diagnostic tests or sensitivity analysis procedures for assessing proxy quality and completeness violations.

## Limitations
- The method's performance under weak proxy informativeness is not extensively explored, with unclear practical thresholds for proxy strength.
- Computational complexity for high-dimensional treatments is acknowledged but not thoroughly benchmarked, particularly regarding kernel matrix inversion costs.
- The completeness assumptions (2.3 and 2.5) are untestable from observational data, creating practical uncertainty about method applicability.

## Confidence

**High:** The double robustness property (Mechanism 1) is theoretically proven and empirically validated across synthetic and real-world settings.

**Medium:** The density ratio-free reformulation (Mechanism 2) is algebraically sound, but its stability relative to traditional density ratio methods requires further empirical comparison.

**Medium:** The closed-form solution claim (Mechanism 3) is technically accurate, though reliance on regularization and kernel choice introduces practical variability.

## Next Checks
1. **Synthetic Stress Test:** Systematically vary proxy informativeness (completeness parameters) and measure breakdown points for each bridge function to identify practical limits.
2. **Computational Scalability:** Benchmark runtime and memory usage for increasing treatment dimensions (beyond dSprite's 4096) to validate scalability claims under realistic computational constraints.
3. **Distribution Shift:** Evaluate method performance under covariate shift between training and test distributions to test robustness beyond stated assumptions.