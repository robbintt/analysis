---
ver: rpa2
title: 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General
  Samples Replay'
arxiv_id: '2508.04676'
source_url: https://arxiv.org/abs/2508.04676
tags:
- tasks
- replay
- samples
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces General Sample Replay (GeRe), a simple yet
  effective method to address catastrophic forgetting in continual learning of large
  language models (LLMs). The core idea is to use a small, fixed set of general pretraining
  texts for replay learning, combined with an activation state constrained optimization
  via threshold-based margin (TM) loss.
---

# GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay

## Quick Facts
- arXiv ID: 2508.04676
- Source URL: https://arxiv.org/abs/2508.04676
- Reference count: 40
- Primary result: GeRe uses fixed general pretraining samples for replay, outperforming traditional task-specific replay in 15 downstream tasks with Llama-3.1-8B.

## Executive Summary
This paper introduces General Sample Replay (GeRe), a simple yet effective method to address catastrophic forgetting in continual learning of large language models (LLMs). The core idea is to use a small, fixed set of general pretraining texts for replay learning, combined with an activation state constrained optimization via threshold-based margin (TM) loss. Unlike traditional replay methods that require laborious collection of task-specific replay samples, GeRe demonstrates that replaying general samples alone can simultaneously preserve general capabilities and enhance performance on sequential downstream tasks. Through extensive experiments on 15 diverse tasks using Llama-3.1-8B, GeRe consistently outperforms standard replay strategies (including logit and feature imitation) in both full-parameter and LoRA settings. It achieves superior MMLU scores (e.g., 60.78% under full-parameter), higher average task performance, and better robustness to learning rate changes and optimization dynamics. This work provides a practical, efficient solution for continual learning of LLMs, eliminating the need for task-specific replay data.

## Method Summary
GeRe consists of two phases: offline and online. In the offline phase, a base LLM processes a fixed set of 1,000 general pretraining samples (from SlimPajama-627B), extracting last-layer hidden states to compute per-dimension activation thresholds (mean±σ). In the online phase, sequential fine-tuning on downstream tasks uses batch insertion to mix replay samples with task data. The model optimizes a joint loss combining task-specific CE loss and TM loss that constrains activation states to match base model thresholds, with dynamic weighting balancing the two objectives. The method is evaluated on 15 classification, NLI, and QA tasks using Llama-3.1-8B in both full-parameter and LoRA settings.

## Key Results
- GeRe achieves 60.78% MMLU score under full-parameter setting, outperforming baseline replay strategies
- Maintains higher average performance (AP) across 15 downstream tasks compared to logit and feature imitation methods
- Shows superior robustness to learning rate changes and optimization dynamics
- Eliminates need for task-specific replay data collection while preserving general capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A fixed set of general pretraining texts is sufficient to preserve general capabilities and promote downstream task performance during continual learning.
- **Mechanism:** General replay samples encode the base model's foundational knowledge distribution; maintaining activation patterns on these samples constrains weight updates to stay within a region that preserves general capabilities, which in turn provides a stronger foundation for learning downstream tasks.
- **Core assumption:** General pretraining texts adequately represent the critical knowledge that enables both general capabilities and downstream task learning.
- **Evidence anchors:**
  - [abstract] "We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns—retaining general capabilities while promoting overall performance across sequential tasks."
  - [Section 4.5.1] "This provides an alternative to the traditional practice of laboriously collecting downstream task replay samples during continual finetuning."
  - [corpus] Limited direct validation in related work; "Replay Can Provably Increase Forgetting" suggests replay effectiveness depends on implementation—contextual tension exists.
- **Break condition:** If general replay samples fail to capture knowledge relevant to downstream tasks, or if the distribution shift between pretraining and downstream domains is too large, the mechanism may not transfer.

### Mechanism 2
- **Claim:** Threshold-based margin (TM) loss on activation states provides more robust anti-forgetting than L1/L2 feature imitation or KL-based logit imitation.
- **Mechanism:** TM loss discretizes continuous activation values into three states (positive activation, negative activation, non-activation) using mean±σ thresholds; the piecewise margin loss only penalizes predictions that violate these state boundaries, providing flexibility within the non-activation band while maintaining critical activation patterns.
- **Core assumption:** Critical neural information is sparsely distributed across activated neurons, and maintaining discrete activation states is sufficient to preserve task-relevant representations.
- **Evidence anchors:**
  - [Section 3.1.2] "values greater than τ+ are considered positively activated, values less than τ− are considered negatively activated, values between τ− and τ+ are considered non-activated"
  - [Section 4.5.1] "BaselineR+TM further alleviate the overly rigid inherence of L1/L2 loss by appropriately constraining optimization from an activation state perspective, achieving the highest performance."
  - [corpus] Weak direct evidence; related work focuses on replay quantity rather than activation-state constraints.
- **Break condition:** If the Gaussian assumption for threshold computation fails (non-normal activation distributions), or if critical information lies within the "non-activated" band that TM loss ignores, performance may degrade.

### Mechanism 3
- **Claim:** Maintaining general capabilities inherently facilitates better learning of sequential downstream tasks.
- **Mechanism:** Strong general capabilities provide a robust representational foundation that reduces inter-task conflicts and overfitting to specific task patterns; the model can leverage broad knowledge rather than memorizing task-specific shortcuts.
- **Core assumption:** Downstream task learning benefits from and depends on the preservation of broad, generalizable knowledge rather than task-specific patterns.
- **Evidence anchors:**
  - [Section 1] "From a cognitive perspective, a learner obtaining superior general capabilities is more likely to achieve better generalization and robustness in downstream tasks."
  - [Section 4.5.4] Shows MMLU scores (general capability) positively correlate with AP (downstream performance) across methods—methods preserving MMLU tend to maintain higher AP.
  - [corpus] Conceptually aligned with continual learning principles, but causal mechanism not directly tested in related work.
- **Break condition:** If downstream tasks require highly specialized knowledge incompatible with general capabilities (e.g., contradictory factual updates), this facilitation may reverse.

## Foundational Learning

### Concept 1: Catastrophic Forgetting in Continual Learning
- **Why needed here:** GeRe specifically targets two forms of forgetting: loss of general capabilities (measured by MMLU) and decline in previously learned tasks (measured by Average Performance). Understanding the stability-plasticity tradeoff is essential for interpreting why TM loss works.
- **Quick check question:** Can you explain why learning task N might degrade performance on tasks 1 through N-1, even without data overlap?

### Concept 2: Knowledge Distillation for Regularization
- **Why needed here:** TM loss is a distillation variant that constrains the student model (current fine-tuned LLM) to match activation states from the teacher (base LLM). Understanding feature-based vs. logit-based distillation helps contextualize why TM outperforms KL and L1/L2.
- **Quick check question:** What information is preserved in hidden-state features that is lost when distilling only from output logits?

### Concept 3: Sparse Neural Coding and Activation States
- **Why needed here:** TM loss is motivated by the hypothesis that "critical information is sparsely distributed across a few activated neurons" (Section 1). The three-state discretization (positive/negative/non-activated) draws from sparse coding theory.
- **Quick check question:** In a layer with 4096 hidden dimensions, if ~68% of activations fall within the "non-activated" band (mean±σ), what fraction of dimensions receive gradient updates from TM loss?

## Architecture Onboarding

### Component Map
Offline Phase: Base LLM → General Replay Samples (1K texts) → Forward Pass → Extract Hidden States → Compute Per-Dimension Mean & Std → Derive Activation Thresholds (τ⁺, τ⁻) → Store Thresholds + (optionally) Pre-computed Hidden States

Online Phase (Per Task): For each training batch: 1. Batch Insertion: Mix ρBI replay samples with downstream samples → 2. Forward pass on mixed batch → 3. Compute: L_CE on downstream samples, L_TM on replay samples (piecewise margin using stored thresholds), Dynamic weight: ω_TM = detach(L_CE / L_TM) → 4. Joint optimization: L = L_CE + ω_TM · L_TM

### Critical Path
1. **Threshold computation accuracy:** If mean/σ are computed incorrectly (wrong aggregation dimension, numerical precision issues), all subsequent TM loss calculations will be misaligned.
2. **Last-layer targeting:** Paper uses only the last layer's hidden states (Section 3.1.1)—modifying this changes the information preserved.
3. **Dynamic weight stability:** Division by zero or extreme ratios if L_TM → 0 or L_CE → 0.

### Design Tradeoffs
- **Fixed vs. dynamic replay set size:** Paper uses 1K samples; larger sets may capture more general knowledge but increase computation and storage.
- **Last-layer vs. multi-layer distillation:** Last layer encodes "majority of semantic information" (Section 3.1.1) but may miss intermediate representations useful for some tasks.
- **Batch Insertion vs. random mixing:** BI guarantees replay in every batch but may reduce effective downstream samples per batch for small datasets.

### Failure Signatures
1. **TM loss = 0 immediately:** Thresholds too wide (e.g., using mean±2σ instead of mean±1σ) or wrong layer extracted—check threshold computation and hidden-state indexing.
2. **MMLU crashes despite low TM loss:** Replay samples may not represent general capabilities (e.g., contaminated with task-specific texts)—inspect replay sample sources.
3. **AP improves but MMLU still degrades:** TM weight too low (ω_TM << 1)—increase base weight or check dynamic weighting implementation.

### First 3 Experiments
1. **Reproduce baseline comparison (Table 2, full-parameter setting):** Implement BaselineR+TM with w=100 and dynamic weighting on a 3-task subset (e.g., yelp→amazon→MNLI); verify F1 average within ±2% of reported values. This validates the core implementation before scaling to 15 tasks.
2. **Ablation on threshold tightness:** Compare mean±1σ (paper default) vs. mean±0.5σ vs. mean±2σ; hypothesize tighter thresholds increase constraint but may over-regularize. Expect sweet spot near 1σ.
3. **Replay set size sensitivity:** Test with 250, 500, 1K (default), and 2K general replay samples; paper claims 1K is sufficient—verify if performance plateaus or continues improving. This determines if the "fixed set" claim is robust to sample quantity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal replay insertion ratio for the Batch Insertion (BI) strategy when scaling to large-scale downstream datasets?
- **Basis in paper:** [explicit] The authors state in Section 4.5.3 that "the adoption of BI should be determined by practical considerations and an optimal replay insertion ratio, which warrants further investigation," specifically noting that simple mixing suffices for small datasets but large-scale datasets require balancing.
- **Why unresolved:** The current experiments used relatively small finetuning datasets where BI showed negligible improvement or mixed results, leaving the scaling behavior unproven.
- **What evidence would resolve it:** Empirical results from experiments using GeRe on large-scale downstream tasks (e.g., datasets with millions of samples) comparing various insertion ratios against the baseline.

### Open Question 2
- **Question:** Can a unified dynamic weight balancing strategy be developed that performs optimally for both full-parameter and LoRA fine-tuning settings?
- **Basis in paper:** [explicit] Section 4.5.3 notes that "different settings should better have their individual weight strategies," with dynamic weighting performing best for full-parameter tuning but fixed weighting performing best for LoRA, concluding that "Unified settings remain for future research."
- **Why unresolved:** The study found that the optimal loss weighting mechanism (dynamic vs. fixed) is dependent on the tuning method, indicating a lack of a generalizable configuration.
- **What evidence would resolve it:** A single adaptive weighting algorithm that achieves peak performance (or statistically equivalent to peak) across both full-parameter and LoRA setups without manual hyperparameter adjustments.

### Open Question 3
- **Question:** Would incorporating activation state constraints from intermediate Transformer layers, rather than solely the last layer, improve the retention of general capabilities?
- **Basis in paper:** [inferred] The method explicitly restricts the computation of activation states to the last layer (Section 3.1.2) based on the assumption that it encodes the "majority of the semantic information."
- **Why unresolved:** While the last layer captures high-level semantics, intermediate layers often encode syntactic or structural knowledge which might also be vulnerable to forgetting; this trade-off was not explored.
- **What evidence would resolve it:** An ablation study applying the Threshold-based Margin (TM) loss to various hidden layers (e.g., the middle layers or all layers) and measuring the resulting MMLU and downstream task performance.

### Open Question 4
- **Question:** Is the statistical definition of activation thresholds (mean $\pm$ 1 standard deviation) optimal for all model architectures, or can learned thresholds improve efficiency?
- **Basis in paper:** [inferred] The paper uses a fixed heuristic based on Gaussian distribution characteristics to define the three activation states (Sec 3.1.2), without exploring if these boundaries are the most informative for the optimization process.
- **Why unresolved:** A fixed statistical boundary might not align perfectly with the actual distribution of semantic information in the neural states, potentially leading to suboptimal gradient constraints.
- **What evidence would resolve it:** A comparative study where the threshold values are treated as learnable parameters or tuned via grid search, compared against the current statistical baseline.

## Limitations
- General replay sufficiency claim lacks validation across different pretraining corpora, model architectures, and domain shifts beyond Llama-3.1-8B and SlimPajama-627B.
- TM loss assumes Gaussian activation distributions; performance may degrade if hidden states exhibit heavy-tailed or multimodal distributions.
- Correlation between general capabilities and downstream performance shown, but causal relationship not established through direct ablation studies.

## Confidence

### High confidence
- TM loss provides more robust anti-forgetting than L1/L2 feature imitation or KL-based logit imitation on the tested 15-task benchmark with Llama-3.1-8B. This is directly supported by comparative results showing consistent AP and MMLU improvements.

### Medium confidence
- A fixed set of general pretraining samples can simultaneously preserve general capabilities and enhance downstream task performance. The claim is well-supported for the tested configuration but lacks validation across diverse model families, task distributions, and general corpus sources.

### Low confidence
- Maintaining general capabilities inherently facilitates better learning of sequential downstream tasks. This mechanism is conceptually plausible and shows correlation in results, but lacks direct causal validation or ablation studies isolating the general capability effect from other confounding factors.

## Next Checks

1. **Cross-corpus generalization:** Repeat the sequential learning experiment using a different general pretraining corpus (e.g., C4 or RedPajama) to verify whether the "fixed general replay set" claim holds across diverse pretraining distributions.

2. **Activation distribution validation:** Analyze the empirical distribution of last-layer hidden states across tasks and replay samples to quantify deviation from Gaussian assumptions; test TM loss performance with percentile-based thresholds as an alternative to mean±σ.

3. **Causal ablation on general capability preservation:** Design an experiment where general capability is explicitly degraded (e.g., by removing TM loss entirely) while keeping other factors constant, to measure the direct impact on downstream task learning curves and AP degradation rates.