---
ver: rpa2
title: Voice Activity Projection Model with Multimodal Encoders
arxiv_id: '2506.03980'
source_url: https://arxiv.org/abs/2506.03980
tags:
- turn-taking
- prediction
- face
- multimodal
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses turn-taking prediction in human-agent interactions
  by incorporating pre-trained facial image encoders into voice activity projection
  (VAP) models. The authors propose three variants of multimodal VAP models that combine
  audio signals with facial images, optionally including head/gaze/body features and
  action units.
---

# Voice Activity Projection Model with Multimodal Encoders

## Quick Facts
- arXiv ID: 2506.03980
- Source URL: https://arxiv.org/abs/2506.03980
- Reference count: 0
- Primary result: Best shift prediction accuracy of 0.794 achieved using multimodal VAP with pre-trained facial encoders

## Executive Summary
This paper addresses turn-taking prediction in human-agent interactions by incorporating pre-trained facial image encoders into voice activity projection (VAP) models. The authors propose three variants of multimodal VAP models that combine audio signals with facial images, optionally including head/gaze/body features and action units. The key innovation is replacing manually extracted facial action units with richer facial embeddings from a pre-trained Dynamic Facial Expression Recognition Transformer (Former-DFER). Experiments on the NoXi dataset show that the proposed models achieve competitive or superior performance on shift prediction and backchannel prediction metrics compared to state-of-the-art multimodal models.

## Method Summary
The proposed approach builds on the Voice Activity Projection framework, using stereo audio (16kHz) and facial video (25fps) from the NoXi dataset. Three multimodal VAP variants are introduced, each combining audio with facial embeddings from a pre-trained Former-DFER encoder, with optional additional signals (head/gaze/body features, action units). The models use per-user multimodal fusion before inter-user fusion, differing from previous approaches. Training uses 20-second input windows with 2-second prediction horizon, employing multi-task learning for four VAP metrics. The best model (Proposed3) achieves shift prediction accuracy of 0.794.

## Key Results
- Best shift prediction (SP) accuracy reaches 0.794 with Proposed3 model
- Multimodal models outperform audio-only baselines on most metrics
- Backchannel prediction remains challenging at ~0.50 accuracy across all models
- Per-user multimodal fusion before inter-user fusion shows promise for capturing participant-specific cue integration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained facial expression encoders capture turn-taking cues that manually-coded action units miss.
- **Mechanism:** The Former-DFER encoder, trained on dynamic facial expression recognition, embeds sequential dynamics and subtle facial movements into rich representations that the VAP model can leverage for predicting turn shifts and backchannels.
- **Core assumption:** Social turn-taking cues manifest in facial expressions that are more granular and temporally coherent than what discrete action unit coding can represent.
- **Evidence anchors:**
  - [abstract] "replacing manually extracted facial action units with richer facial embeddings from a pre-trained Dynamic Facial Expression Recognition Transformer"
  - [section 3] "Since Onishi and colleagues reported that face information contributed the most, we hypothesized that richer representations extracted with a pre-trained encoder would improve performance"
  - [corpus] Related work on visual cues in turn-taking (MM-VAP) supports multimodal enhancement, though direct comparison to this specific encoder is not available
- **Break condition:** If the pre-trained encoder was trained on expressions unrelated to conversational dynamics (e.g., exaggerated emotions), its embeddings may not generalize to subtle turn-taking cues.

### Mechanism 2
- **Claim:** Per-user multimodal fusion before inter-user fusion captures participant-specific cue integration patterns.
- **Mechanism:** By merging audio, face, head/gaze, and body signals for each speaker independently before fusing across participants, the model preserves how each individual's modalities interact, which is then combined for dyadic prediction.
- **Core assumption:** Turn-taking cues emerge from how modalities combine within each participant's behavior, not just from modality-level aggregations across speakers.
- **Evidence anchors:**
  - [section 3] "We first merged multimodal signals for each person separately, followed by the fusion across user embeddings"
  - [section 5] "combining these features with the face encoder achieved the highest score"
  - [corpus] No direct corpus comparison of fusion strategies; this remains an architectural hypothesis
- **Break condition:** If modalities provide independent (non-interacting) information, early per-user fusion adds complexity without benefit over modality-then-user fusion.

### Mechanism 3
- **Claim:** Unified VAP state representation enables joint learning of multiple turn-taking behaviors.
- **Mechanism:** The 2×4 binary bin structure (two speakers, four future time windows) serves as a multi-task target, allowing the model to learn shared representations for shift-hold, short-long, shift prediction, and backchannel prediction simultaneously.
- **Core assumption:** Turn-taking behaviors share underlying predictive features that benefit from joint optimization.
- **Evidence anchors:**
  - [section 2] "VAP state is a discrete representation shaped as 2 × 4 binary bins"
  - [section 1] "Using the unified representation, they successfully predicted both the timings of the turn shift and the back channel"
  - [corpus] VAP-based approaches appear in multiple related works (triadic VAP, noise-robust VAP), suggesting reproducibility
- **Break condition:** If backchannels rely on fundamentally different cues than turn shifts (as the paper speculates regarding backchannel types), unified representation may limit performance on specific subtasks.

## Foundational Learning

- **Concept: Voice Activity Projection (VAP)**
  - **Why needed here:** The entire architecture builds on VAP's predictive framework; understanding the 2×4 bin structure and its relationship to VAD is essential.
  - **Quick check question:** Can you explain why VAP predicts future voice activity in bins of 0.2s, 0.4s, 0.6s, and 0.8s rather than uniform intervals?

- **Concept: Pre-trained encoder transfer**
  - **Why needed here:** The core innovation is substituting handcrafted features with Former-DFER embeddings; understanding what these encoders capture determines deployment feasibility.
  - **Quick check question:** What types of pre-training data might make a facial encoder suitable (or unsuitable) for turn-taking cue extraction?

- **Concept: Multimodal fusion ordering**
  - **Why needed here:** The architectural change from Onishi's approach (modality-first vs. user-first fusion) directly impacts what the model can learn.
  - **Quick check question:** In a three-participant conversation, how would per-user fusion scale compared to modality-first fusion?

## Architecture Onboarding

- **Component map:** Stereo audio (16kHz) → CPC encoder → Audio Transformer → Inter-user Transformer; Face images (112×112) → Former-DFER encoder → Face Transformer; Optional signals: Head/gaze angles, body positions, face AUs → concatenated with face embeddings; Per-user multimodal merge → User encoders → Inter-user Transformer → VAP head + VAD head
- **Critical path:** Face detection accuracy → Former-DFER embedding quality → per-user fusion → VAP state prediction. If face detection fails or crops are poor, the entire visual pathway degrades.
- **Design tradeoffs:** Proposed1 (audio + face embeddings only) vs. Proposed3 (all signals including AU): Proposed3 achieves best SP (0.794) but requires OpenFace/OpenPose infrastructure; Pre-trained encoder reduces implementation complexity vs. handcrafted features, but introduces dependency on encoder's training domain
- **Failure signatures:** S/H and S/L underperforming Baseline2 suggests face embeddings may introduce noise for duration-based predictions; Backchannel accuracy consistently low (~0.50) across all models indicates the BC definition (1s speech + silence windows) may not match functional backchannel types
- **First 3 experiments:** 1) Reproduce Baseline1 (audio-only) on your target dataset to establish a reference; verify VAP bin structure and loss computation match. 2) Ablate the face encoder by comparing Proposed1 vs. Baseline2_1 (AU-only) on identical data; quantify the embedding contribution. 3) Test fusion ordering by implementing Onishi's modality-first fusion as a control; isolate whether per-user fusion is responsible for SP gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating a pre-trained encoder for body posture and movement (similar to the facial encoder used here) improve turn-taking prediction accuracy, particularly for shift prediction?
- **Basis in paper:** [explicit] The authors state that the improvement seen in Proposed3 "implies that further improvement could arise with the inclusion of a body encoder," and list "integrate further pre-trained encoders on other nonverbal modalities" as a future aim.
- **Why unresolved:** While the study tested raw 2D body positions, it did not test pre-trained body embeddings; the authors hypothesize that the interaction between face embeddings and body signals is key to unlocking better performance.
- **What evidence would resolve it:** Ablation studies comparing models using raw body coordinates versus those using features from a pre-trained body action encoder on the NoXi dataset.

### Open Question 2
- **Question:** Would explicitly modeling different functional types of backchannels (e.g., continuers vs. assessments) improve the prediction accuracy compared to the current definition based solely on duration and silence?
- **Basis in paper:** [explicit] The discussion notes that the low accuracy in backchannel prediction might stem from the "imperfect definition" of backchannels in the VAP model and states, "we should predict not only backchannel timings, but also types of backchannels for improvements."
- **Why unresolved:** The current model treats all backchannels as a single class defined by duration, ignoring functional differences cited by the authors (e.g., continuers vs. assessments), which may require different multimodal cues.
- **What evidence would resolve it:** Experiments utilizing a dataset annotated with backchannel functions to train and evaluate a multi-class prediction head against the current binary approach.

### Open Question 3
- **Question:** Does training the proposed multimodal architecture on a mixture of multilingual data enhance the model's resilience and performance compared to training on a single-language subset?
- **Basis in paper:** [explicit] The conclusion explicitly identifies "room to improve resilience by mixing multilingual data to train models," referencing previous work by Inoue.
- **Why unresolved:** The experiments in this paper were restricted to the French subset of the NoXi dataset to simplify the experimental setting, leaving cross-linguistic robustness unverified for this specific encoder combination.
- **What evidence would resolve it:** Benchmarking the model's shift and backchannel prediction performance when trained on the combined multilingual NoXi corpus (German, English, French, Japanese) versus the French-only data.

## Limitations
- Former-DFER encoder's effectiveness depends on whether its training domain (dynamic facial expression recognition) overlaps with subtle conversational turn-taking cues
- Backchannel prediction performance remains significantly below other metrics (~0.50 accuracy), suggesting limitations in the BC definition or modeling approach
- Model does not address missing face detections or synchronization drift between audio (16kHz) and video (25fps) streams

## Confidence

- **High Confidence**: The VAP framework's ability to predict turn shifts (SP accuracy of 0.794) is well-supported by the unified bin representation and consistent with prior VAP literature.
- **Medium Confidence**: The per-user multimodal fusion strategy's superiority over modality-first fusion is plausible but lacks direct comparative evidence in the corpus.
- **Low Confidence**: The claim that pre-trained facial encoders capture turn-taking cues missed by handcrafted AUs assumes the DFEW-trained Former-DFER generalizes to conversational contexts without validation on expression-diverse datasets.

## Next Checks

1. **Test the face encoder's generalization**: Compare the Former-DFER embeddings against handcrafted AUs on a dataset with diverse expressions (e.g., DFEW itself) to verify the encoder captures relevant conversational dynamics rather than just dramatic expressions.

2. **Evaluate missing data handling**: Systematically remove face detections at varying rates (0%, 10%, 30%, 50%) to measure how performance degrades and whether interpolation or fallback strategies maintain viability.

3. **Probe backchannel modeling assumptions**: Re-label the BC task using alternative definitions (e.g., 0.5s speech windows, multi-type BC classification) to determine whether the low accuracy reflects task definition or fundamental limitations in the feature representations.