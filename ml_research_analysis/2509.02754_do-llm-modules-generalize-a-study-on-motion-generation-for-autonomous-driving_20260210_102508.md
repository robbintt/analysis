---
ver: rpa2
title: Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving
arxiv_id: '2509.02754'
source_url: https://arxiv.org/abs/2509.02754
tags:
- motion
- agents
- generation
- driving
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well LLM components transfer to autonomous\
  \ driving motion generation. The authors systematically study five key modules\u2014\
  tokenizer design, positional embedding, pre-training, post-training, and test-time\
  \ computing\u2014on the Waymo Sim Agents benchmark."
---

# Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving

## Quick Facts
- arXiv ID: 2509.02754
- Source URL: https://arxiv.org/abs/2509.02754
- Reference count: 40
- This paper evaluates how well LLM components transfer to autonomous driving motion generation.

## Executive Summary
This paper evaluates how well LLM components transfer to autonomous driving motion generation. The authors systematically study five key modules—tokenizer design, positional embedding, pre-training, post-training, and test-time computing—on the Waymo Sim Agents benchmark. They find that most LLM-inspired techniques improve performance when adapted to the driving domain: a model-driven Verlet-Agent tokenizer outperforms data-driven alternatives, global-DRoPE positional embedding improves spatial reasoning, and test-time rollouts with safety filtering enhance planning safety. Post-training via GRPO yields better safety without sacrificing realism compared to imitation learning. Their combined optimized model achieves competitive results on the leaderboard, demonstrating that LLM modules can generalize effectively to autonomous driving when appropriately tailored.

## Method Summary
The authors systematically evaluate LLM-inspired modules for autonomous driving motion generation on the Waymo Sim Agents benchmark. They implement a model-driven Verlet-Agent tokenizer that discretizes position deltas into 169 tokens, use Global-DRoPE positional embeddings to maintain spatial semantics, pre-train with next-token prediction, fine-tune with GRPO safety rewards, and deploy test-time rollouts with safety filtering. Their combined approach achieves competitive performance on the benchmark, demonstrating that LLM techniques can generalize to driving when appropriately adapted.

## Key Results
- Model-driven Verlet-Agent tokenizer outperforms data-driven alternatives
- Global-DRoPE positional embedding improves spatial reasoning
- GRPO post-training yields better safety-realism trade-off than imitation learning

## Why This Works (Mechanism)
The study demonstrates that LLM modules can generalize to autonomous driving when domain-specific adaptations are made. The model-driven Verlet-Agent tokenizer works better than data-driven approaches because it encodes agent-centric coordinates that naturally capture driving dynamics. Global-DRoPE preserves spatial semantics by maintaining global coordinates for lane features while enabling relative attention for temporal reasoning. GRPO fine-tuning improves safety without sacrificing realism by balancing policy optimization with KL constraints that prevent drift from human-prior distributions. Test-time rollouts with safety filtering provide a practical mechanism for handling the safety-critical nature of driving scenarios.

## Foundational Learning

### Verlet Integration
- **Why needed:** Numerical method for simulating particle motion that predicts future positions based on current position and velocity
- **Quick check:** Verify that position updates follow the discrete-time Verlet formula: x(t+Δt) = 2x(t) - x(t-Δt) + a(t)Δt²

### Agent-Centric Coordinate System
- **Why needed:** Transforms absolute world coordinates into relative coordinates centered on the ego vehicle, making the model invariant to global position
- **Quick check:** Confirm that all agent positions are transformed relative to the ego vehicle's current position and orientation

### Group Relative Policy Optimization (GRPO)
- **Why needed:** Reinforcement learning algorithm that optimizes policies using group-relative advantages to handle variance in reward signals
- **Quick check:** Verify that rewards are normalized within groups of trajectories before computing policy gradients

### Kinematic Metrics
- **Why needed:** Measures of driving realism based on physical constraints like acceleration, jerk, and lane-keeping
- **Quick check:** Calculate acceleration limits (typically ±3 m/s²) and ensure generated trajectories stay within these bounds

### KL Divergence Constraint
- **Why needed:** Regularizes policy updates to prevent divergence from the pre-trained human-prior distribution
- **Quick check:** Monitor KL divergence between old and new policies during training, ensuring it stays below the threshold (0.8 in this work)

## Architecture Onboarding

### Component Map
Map Encoder -> Tokenizer -> GPT Decoder (4 layers, 5.3M params) -> GRPO Post-training -> Safety Filter -> Output

### Critical Path
Input (map + agent history) -> Tokenizer (Verlet-Agent) -> Scene Encoder (PointNet-like + Attention) -> GPT Decoder (Global-DRoPE) -> Output trajectories -> Safety Filter (collision check) -> Final selection

### Design Tradeoffs
The authors chose a relatively small 5.3M parameter model to prevent overfitting given limited driving data, sacrificing potential performance gains from larger models. They prioritized safety through GRPO fine-tuning with collision/off-road penalties rather than maximizing realism alone. The model-driven tokenizer trades data efficiency for better generalization to unseen driving scenarios.

### Failure Signatures
- Repetition loops: Model gets stuck predicting the same "maintain speed" token, ignoring map geometry
- Out-of-distribution drift: Generated trajectories deviate from human-like behavior due to aggressive RL optimization
- Positional embedding collapse: Lane features become indistinguishable when using local coordinates in attention mechanisms

### First Experiments
1. Implement the Verlet-Agent tokenizer and verify it correctly encodes/decodes position deltas for simple straight-line trajectories
2. Test the Global-DRoPE positional embedding by visualizing lane features before and after transformation to confirm semantic preservation
3. Run a small-scale GRPO fine-tuning experiment with synthetic collision rewards to verify the safety improvement mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM-specific architectures such as Mixture-of-Experts (MoE) transfer effectively to autonomous driving motion generation, and what domain-specific adaptations are required?
- Basis in paper: The authors state: "One such direction is the investigation of LLM-specific mechanisms such as Mixture-of-Experts (MoE)."
- Why unresolved: MoE was not studied in this work; the paper focused on tokenizer, positional embedding, pre-training, post-training, and test-time computing modules.
- What evidence would resolve it: Ablation studies comparing dense vs. MoE architectures on motion generation benchmarks, measuring both performance and computational efficiency.

### Open Question 2
- Question: Does the observed scaling law behavior extend to significantly larger model sizes (beyond ~5.3M parameters) and larger training datasets for motion generation?
- Basis in paper: "Due to limited data and computational resources, we have not studied the scaling behavior of larger models."
- Why unresolved: Current experiments showed overfitting signs even at the "Large" model size with 800% augmented data, suggesting data diversity—not model capacity—is the bottleneck.
- What evidence would resolve it: Training larger models (e.g., 50M+ parameters) on substantially larger, more diverse driving datasets and analyzing loss curves for power-law behavior.

### Open Question 3
- Question: What are the precise triggering conditions and underlying mechanisms causing the rare repetition phenomenon (token loops) in autoregressive motion generation?
- Basis in paper: "We hypothesize it relates to progressive out-of-distribution (OOD) drift, but the exact triggering scenarios remain open for future study."
- Why unresolved: The phenomenon occurs in under 0.5% of cases but poses safety risks; the authors only hypothesized OOD drift without systematic analysis.
- What evidence would resolve it: Controlled experiments varying initial state distributions, analysis of attention patterns during repetition, and identification of specific scenario features that predict loop occurrence.

## Limitations
- Constrained ablation scope - does not explore full design space of alternative tokenizers, positional embeddings, or post-training algorithms
- Safety-focused GRPO may not generalize to scenarios requiring more complex reward structures beyond collision/off-road penalties
- Benchmark evaluation on simulated (not real-world) driving conditions may not capture sensor noise and edge cases

## Confidence

**High Confidence:** The core finding that model-driven tokenization (Verlet-Agent) outperforms data-driven alternatives in the driving domain is well-supported by systematic comparisons. The improvement from Global-DRoPE over standard Rotary Embedding is also clearly demonstrated through quantitative metrics.

**Medium Confidence:** The post-training results showing GRPO's superiority over imitation learning for safety-realism trade-offs are convincing, but the specific hyperparameters (KL penalty of 0.8, entropy weight of 0.01) may not be optimal across all driving scenarios.

**Low Confidence:** The generalizability of these findings to other autonomous driving tasks beyond motion prediction (such as planning or control) remains untested. The specific architectural choices (4-layer decoder, 5.3M parameters) may not represent optimal scaling laws for this domain.

## Next Checks

1. **Cross-Domain Transfer:** Validate whether the Verlet-Agent tokenizer and Global-DRoPE positional embedding maintain their performance advantages when applied to related autonomous driving tasks like behavior prediction or motion planning in the same benchmark.

2. **Real-World Robustness:** Test the trained models on real-world driving datasets or closed-course evaluations to assess performance degradation under sensor noise, unexpected pedestrian behavior, and adverse weather conditions not present in simulation.

3. **Scaling Analysis:** Systematically vary model depth (2-8 layers) and parameter count (1M-20M) to identify whether the current 4-layer, 5.3M parameter configuration represents an optimal point or if larger models would yield disproportionate improvements in safety-realism balance.