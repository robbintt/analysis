---
ver: rpa2
title: 'Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs'
arxiv_id: '2601.07972'
source_url: https://arxiv.org/abs/2601.07972
tags:
- value
- values
- llms
- action
- scenario
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines whether LLMs\u2019 internal value systems\
  \ translate into consistent real-world behavior. The authors introduce VALACT-15K,\
  \ a benchmark of 3,000 Reddit-derived scenarios paired with PVQ-40 self-report measures,\
  \ to test both declared and enacted values across ten Schwartz-defined dimensions."
---

# Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs

## Quick Facts
- arXiv ID: 2601.07972
- Source URL: https://arxiv.org/abs/2601.07972
- Authors: Jen-tse Huang; Jiantong Qin; Xueli Qiu; Sharon Levy; Michelle R. Kaufman; Mark Dredze
- Reference count: 29
- One-line primary result: LLMs show near-perfect cross-model consistency (r ≈ 1.0) in value-based decisions but exhibit weak correspondence (r = 0.3) between declared and enacted values, revealing a systematic knowledge-action gap.

## Executive Summary
This paper examines whether LLMs' internal value systems translate into consistent real-world behavior. The authors introduce VALACT-15K, a benchmark of 3,000 Reddit-derived scenarios paired with PVQ-40 self-report measures, to test both declared and enacted values across ten Schwartz-defined dimensions. Ten LLMs (U.S. and Chinese) and 55 human participants were evaluated. Results show near-perfect cross-model consistency in scenario choices (Pearson r ≈ 1.0) and strong alignment between models and humans, contrasting sharply with high human-to-human variability (r ∈ [-0.79, 0.98]). Both groups exhibit weak correspondence between self-reported and enacted values (humans r = 0.4, LLMs r = 0.3). When instructed to role-play a specific value, LLMs' performance drops up to 6.6% compared to value selection, revealing a role-play resistance. This indicates that alignment training yields normative convergence but does not ensure stable value-consistent action, highlighting a systematic knowledge-action gap.

## Method Summary
The study evaluates ten LLMs (GPT-4o, Claude-4-Sonnet, Gemini-2.5-Pro, Qwen-2.5-72B, Grok-4-0709, Seed-1.6, Yi-1.5, InternLM-3.1, Kimi-1.5, Zhipu-AI-4.0) and 55 human participants using VALACT-15K, a benchmark of 3,000 Reddit-derived scenarios paired with PVQ-40 self-report measures. Each scenario contains four actions, each mapped to one of ten Schwartz values. Evaluation modes include default selection (neutral prompt), value selection (instructed to choose action matching value), and value adoption (role-play persona holding value). Models were evaluated at temperature=0 with 5 prompt variants averaged. Human participants completed 120-item questionnaires on Prolific.

## Key Results
- Cross-model consistency: LLMs exhibit near-perfect similarity in value-informed decisions (Pearson r ≈ 1.0 across all model pairs).
- Knowledge-action gap: Weak correspondence between PVQ-40 self-reports and scenario-based decisions (LLMs r = 0.32, humans r = 0.4).
- Role-play resistance: When instructed to role-play a specific value, LLMs' performance drops up to 6.6% compared to value selection.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alignment training produces convergent, normative value profiles across geographically and organizationally diverse LLMs.
- **Mechanism:** Reinforcement learning from human feedback (RLHF) and instruction tuning optimize for socially acceptable responses, which compress diverse pre-training value signals into a narrow, homogenized attractor basin.
- **Core assumption:** The convergence reflects alignment optimization rather than shared pre-training data alone.
- **Evidence anchors:**
  - [abstract] "near-perfect cross-model consistency in scenario-based decisions (Pearson r ≈ 1.0), contrasting sharply with the broad variability observed among humans (r ∈ [-0.79, 0.98])"
  - [Section 3.1] "Across all 3,000 real-world dilemmas in VALACT-15K, the ten models exhibit near-perfect similarity in value-informed decisions, with Pearson correlations of 0.99–1.00 for every model pair"
  - [corpus] Related work on psychometric testing (arxiv 2510.11254) notes similar convergence phenomena but questions validity of applying human instruments to LLMs.
- **Break condition:** If models were evaluated on novel domains outside typical alignment data (e.g., novel cultural value systems), convergence would likely decrease.

### Mechanism 2
- **Claim:** A systematic knowledge-action gap emerges because value recognition and value enactment operate as partially dissociated competencies in both humans and LLMs.
- **Mechanism:** Self-report measures (PVQ-40) probe explicit value declarations, while scenario-based tasks require context-sensitive integration and tradeoffs. The weak correlation (r=0.3 for LLMs, r=0.4 for humans) suggests that knowing which values are normative does not deterministically translate into consistent behavioral enactment when competing constraints exist.
- **Core assumption:** The gap reflects computational/architectural limitations rather than measurement artifact.
- **Evidence anchors:**
  - [abstract] "both humans and LLMs show weak correspondence between self-reported and enacted values (r = 0.4, 0.3), revealing a systematic knowledge-action gap"
  - [Section 3.1] "Across models, the correspondence between PVQ-40 self-reports and scenario-based decisions using VALACT-15K is weak: LLMs show a mean correlation of only 0.32"
  - [corpus] "Do Psychometric Tests Work for Large Language Models?" (arxiv 2510.11254) raises validity concerns about psychometric instruments applied to LLMs.
- **Break condition:** If models received explicit training on value-consistent action generation (not just preference modeling), the gap would narrow.

### Mechanism 3
- **Claim:** Role-play instructions degrade value-consistent behavior because persona adoption requires maintaining coherent latent state updates that compete with default alignment-induced priors.
- **Mechanism:** When instructed to "hold" a value as a persona (value adoption), models must override their default response distribution. The 3.9–6.6% accuracy drop (up to 10% when restricted to correctly recognized values) indicates that instruction-based steering produces shallower behavioral changes than selection-based recognition.
- **Core assumption:** The resistance reflects competition between persona-context and alignment priors rather than prompt misunderstanding.
- **Evidence anchors:**
  - [abstract] "When instructed to role-play a specific value, LLMs' performance drops up to 6.6% compared to value selection, revealing a role-play resistance"
  - [Section 3.2] "When restricting analysis to scenarios in which a model correctly identified the value in the Selection condition... accuracy drops reached up to 10%"
  - [corpus] Corpus papers on divergent/convergent reasoning (arxiv 2507.18368) discuss related persona effects but do not directly address value enactment resistance.
- **Break condition:** If persona instructions were reinforced through few-shot examples or chain-of-thought reasoning, resistance might decrease.

## Foundational Learning

- **Concept: Schwartz Theory of Basic Human Values (10 dimensions)**
  - Why needed here: The entire VALACT-15K benchmark is structured around Schwartz's framework. Understanding that values like "universalism" vs. "benevolence" or "security" vs. "stimulation" represent motivational tradeoffs is essential for interpreting why scenario choices matter.
  - Quick check question: If a model consistently chooses "security" over "self-direction" in career scenarios, what does this reveal about its value profile?

- **Concept: PVQ-40 (Portrait Values Questionnaire)**
  - Why needed here: The paper uses PVQ-40 as the "declared values" baseline against which enacted values are compared. Understanding that PVQ-40 uses likable-person-descriptions rather than direct questions explains why it may not predict behavior.
  - Quick check question: Why might a model score high on "universalism" in PVQ-40 but not consistently select universalism-consistent actions?

- **Concept: Knowledge-Action Gap**
  - Why needed here: This is the central phenomenon the paper documents. The gap describes dissociation between what agents explicitly endorse vs. what they contextually enact.
  - Quick check question: Name two reasons (from the paper or psychological literature) why declared values might diverge from enacted values.

## Architecture Onboarding

- **Component map:**
  - Reddit posts (3,000 scenarios) -> GPT-4o/Qwen-2.5 action generation (15,000 questions) -> LLMs (10 models) + humans (55 participants) -> PVQ-40 + scenario evaluation -> 10-dimensional value vectors -> Pearson correlation analysis

- **Critical path:**
  1. Run PVQ-40 (5 prompt variants × 10 runs with shuffled order)
  2. Run scenario evaluation (15,000 questions, temperature=0)
  3. Compute adjusted value vectors (raw scores minus agent mean)
  4. Correlate declared vs. enacted vectors

- **Design tradeoffs:**
  - **English-only evaluation:** Controls language but limits cross-cultural claims. Chinese PVQ-40 validation showed r≥0.95, suggesting invariance.
  - **LLM-generated actions:** GPT-4o and Qwen-2.5 generated options; validated by PhD-level experts (91.3% accuracy). Risk of model-dependent biases.
  - **Reddit-derived scenarios:** Realistic but demographically skewed. Potential pre-training contamination noted as limitation.

- **Failure signatures:**
  - Near-zero correlation between declared and enacted values (r≈0.3) in a new model would indicate persistent knowledge-action gap.
  - High cross-model correlation (r>0.95) with human mean (r>0.9) suggests alignment-induced convergence is present.
  - Large accuracy drop (>5%) between selection and adoption modes signals role-play resistance.

- **First 3 experiments:**
  1. **Baseline replication:** Run your target model through PVQ-40 and 100-scenario subset. Compute declared-enacted correlation. Expected: r∈[0.2, 0.5].
  2. **Selection vs. Adoption probe:** Test 50 scenarios in both modes. Compare accuracy. Expected: 2–7% drop in adoption mode.
  3. **Cross-domain consistency:** Stratify scenarios by domain (career, finance, education, relationships, ethics). Check if knowledge-action gap varies by domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training-time interventions (e.g., reinforcement learning) close the knowledge-action gap without degrading other model capabilities like calibration, helpfulfulness, or fairness?
- Basis in paper: [explicit] The authors suggest, "We can investigate training-time interventions... to explicitly reduce the knowledge–action gap and to test whether closing this gap trades off against other desiderata."
- Why unresolved: This study evaluates existing models via prompting but does not attempt to modify the models to close the gap.
- What evidence would resolve it: Training models on behavioral benchmarks like VALACT-15K using RL and measuring the resulting trade-offs on standard safety and performance benchmarks.

### Open Question 2
- Question: How do value profiles and the knowledge-action gap evolve over time in autonomous agents or multi-agent systems?
- Basis in paper: [explicit] The authors state that "longitudinal studies of value drift, cross-release consistency, and system-level interactions among multiple agents are essential."
- Why unresolved: The paper presents a static evaluation of model behaviors, leaving the dynamics of deployed or interacting agents unexplored.
- What evidence would resolve it: Longitudinal tracking of agent decisions over extended interactions using scenario-based probes to measure drift or divergence from initial value states.

### Open Question 3
- Question: To what extent does the near-perfect cross-model convergence result from data contamination, given that Reddit is a common source in pre-training corpora?
- Basis in paper: [inferred] The authors acknowledge the "potential for data contamination cannot be entirely dismissed," which casts uncertainty on whether the high consistency (r ≈ 1.0) reflects genuine reasoning or memorization.
- Why unresolved: Without strict temporal holdouts or out-of-distribution verification, it is difficult to distinguish value enactment from retrieval.
- What evidence would resolve it: Evaluating models on synthetic scenarios constructed post-training cutoff or scenarios explicitly excluded from common web-scraped datasets.

### Open Question 4
- Question: Do the findings of high cross-model convergence and the knowledge-action gap generalize to diverse, non-Western populations and non-English languages?
- Basis in paper: [explicit] The authors list a limitation regarding the "English-centric" evaluation and note that "future research should extend these evaluations to more diverse, multi-country settings."
- Why unresolved: The human cohort was restricted to U.S. residents, and while PVQ-40 was tested in Chinese, the primary scenario evaluation was English-only.
- What evidence would resolve it: Replicating the evaluation with localized versions of VALACT-15K and participant pools from varied cultural backgrounds.

## Limitations
- English-language evaluation constrains generalizability across linguistic and cultural contexts.
- Use of LLM-generated scenario options (via GPT-4o and Qwen-2.5) introduces potential model-dependent biases.
- Pre-training contamination remains a possibility given the Reddit-derived scenarios.

## Confidence
- **High confidence**: Cross-model consistency (r≈1.0) and weak declared-enacted correlation (r=0.3-0.4) are robustly supported by multiple analytical approaches and replication across diverse models.
- **Medium confidence**: Role-play resistance findings (3.9-6.6% accuracy drop) are well-documented but may depend on prompt engineering quality.
- **Low confidence**: Cultural generalizability claims remain speculative given English-only evaluation and limited Chinese validation data.

## Next Checks
1. **Cross-linguistic validation**: Run VALACT-15K with Chinese-language scenarios and PVQ-40 on models trained primarily on Chinese data to test if convergence holds across linguistic boundaries.
2. **Pre-training contamination assessment**: Evaluate models on held-out value-consistent scenarios from domains not represented in Reddit training data to isolate alignment effects from memorization.
3. **Value-consistent action training**: Fine-tune a model on VALACT-15K-style value-consistent reasoning and re-measure the knowledge-action gap to test whether explicit training can reduce the dissociation.