---
ver: rpa2
title: Optimal Arm Elimination Algorithms for Combinatorial Bandits
arxiv_id: '2510.23992'
source_url: https://arxiv.org/abs/2510.23992
tags:
- algorithm
- regret
- optimal
- arms
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimal arm elimination in
  combinatorial bandit settings, where the learner selects multiple arms per round
  and observes rewards with general graph feedback. While UCB-based approaches are
  natural extensions, they can fail due to insufficient exploration.
---

# Optimal Arm Elimination Algorithms for Combinatorial Bandits

## Quick Facts
- **arXiv ID**: 2510.23992
- **Source URL**: https://arxiv.org/abs/2510.23992
- **Reference count**: 40
- **Primary result**: Near-optimal elimination algorithms for combinatorial bandits with graph feedback and contextual settings, achieving optimal worst-case and gap-dependent regret bounds

## Executive Summary
This paper addresses the challenge of optimal arm elimination in combinatorial bandit settings where multiple arms are selected per round with general graph feedback. The authors introduce a novel elimination framework that partitions arms into confirmed, active, and eliminated categories, enabling explicit exploration-exploitation trade-offs. For graph feedback settings, their algorithm achieves near-optimal regret bounds simultaneously attaining O(√(αST) + S√T) worst-case regret and O((α+S)log(T)/∆*) gap-dependent regret. The paper also extends this approach to combinatorial linear contextual bandits, achieving optimal O(√(dST)) regret. The work demonstrates that standard UCB methods are provably suboptimal in these settings, while the elimination-based approach effectively balances exploration and exploitation.

## Method Summary
The core approach partitions arms into three sets: confirmed (high-value arms), active (arms under consideration), and eliminated. At each round, confirmed arms are selected for exploitation while the remaining budget explores active arms to gather information. For graph feedback, exploration uses a greedy out-degree-based selection to efficiently propagate information. In the contextual setting, a hierarchical stage partitioning scheme ensures conditional independence of reward observations. The algorithms maintain confidence widths using concentration inequalities and update partitions based on empirical estimates. The key innovation is the explicit exploration budget that differs from UCB's implicit exploration, allowing optimal performance in combinatorial settings where simultaneous selection of multiple arms creates exploration challenges.

## Key Results
- Achieves near-optimal O(√(αST) + S√T) worst-case regret and O((α+S)log(T)/∆*) gap-dependent regret for combinatorial bandits with graph feedback
- Demonstrates UCB methods are provably suboptimal (linear regret possible) in graph feedback settings
- Extends to contextual bandits achieving optimal O(√(dST)) regret using hierarchical stage partitioning
- Proves matching lower bounds showing these rates are tight
- Shows the three-way partition framework effectively balances exploration and exploitation

## Why This Works (Mechanism)

### Mechanism 1: Three-Way Arm Partition with Dynamic Exploration-Exploitation Budget
- **Claim**: Partitioning arms into confirmed, active, and eliminated sets enables near-optimal regret by dynamically allocating budget between exploitation and explicit exploration
- **Mechanism**: At each round, confirmed arms are selected while remaining budget explores active arms; partition updates based on confidence widths relative to S-th empirically best arm
- **Core assumption**: Stochastic rewards with time-invariant means; confidence intervals hold with high probability
- **Evidence anchors**: Abstract mentions "partitions arms into three categories... and incorporates explicit exploration"; Section 2.2 explains the exploration-exploitation trade-off based on confidence width w_t
- **Break condition**: If confidence width estimation fails, optimal arms may be incorrectly eliminated

### Mechanism 2: Explicit Exploration Via Graph-Guided Selection
- **Claim**: For graph feedback, explicit exploration using out-degree-based arm selection achieves optimal minimax regret
- **Mechanism**: Within exploration budget, greedily selects arms with largest out-degree in current subgraph, ensuring efficient information propagation
- **Core assumption**: Feedback graph structure known and contains self-loops; independence number α characterizes exploration difficulty
- **Evidence anchors**: Abstract states "attains O(√(αST) + S√T) worst-case regret"; Section 2.5 explains UCB's suboptimality without forced exploration
- **Break condition**: If graph structure is misspecified or α poorly estimated, exploration becomes inefficient

### Mechanism 3: Hierarchical Stage Partitioning for Conditional Independence
- **Claim**: For linear contextual bandits, partitioning across H stages ensures conditional independence of reward observations
- **Mechanism**: Each stage uses only historical observations from Φ_t^(h) to compute ridge regression estimates; decisions built incrementally across stages
- **Core assumption**: Linear reward model with bounded contexts; rewards in each stage conditionally independent given contexts
- **Evidence anchors**: Section 3.2 states algorithm builds final decision by only looking at independent information; Lemma 3 requires conditional independence for valid estimation
- **Break condition**: If rewards within stages not conditionally independent, confidence bounds become invalid

## Foundational Learning

- **Concept**: Exploration-Exploitation Trade-off in Bandits
  - **Why needed here**: Core contribution is novel way to balance this trade-off when selecting multiple arms; understanding UCB's implicit vs. elimination's explicit exploration is essential
  - **Quick check question**: Can you explain why implicit exploration (UCB) fails when S > 1 arms must be selected simultaneously?

- **Concept**: Confidence Intervals and Concentration Inequalities
  - **Why needed here**: All elimination decisions rely on confidence widths w(n) = √(log(2KT/δ)/n); three-way partition updates depend on these bounds holding
  - **Quick check question**: What happens to Algorithm 1's guarantees if Lemma 1's high-probability event fails?

- **Concept**: Combinatorial Structure and Regret Decomposition
  - **Why needed here**: Regret in combinatorial bandits decomposes differently from standard MAB; benchmark is top-S arms collectively, not single best arm
  - **Quick check question**: Why is the elimination benchmark more complex when S ≥ 2 compared to S = 1?

## Architecture Onboarding

- **Component map**: Confidence width computation → Three-way partition update → Out-degree-based exploration → Feedback collection (Graph feedback); Context observation → H-stage hierarchical loop → Base learner (ridge regression) → Incremental V_t construction (Contextual)

- **Critical path**:
  1. Initialize A_con = ∅, A_act = [K], N = 0
  2. Each round: Compute confidence widths → Update confirmed set → Update active set → Select V_t mixing confirmed + explored arms
  3. Graph feedback: Use greedy out-degree selection for exploration budget
  4. Contextual: Build decision across H stages with dynamic capacity tracking

- **Design tradeoffs**:
  - UCB (simpler, O(√(KST)) regret) vs. Elimination (more complex, optimal O(√(αST) + S√T) regret)
  - Persistent partition (Algorithm 1) vs. per-round re-elimination (Algorithm 3 due to context variability)
  - Computational cost: Hierarchical stages add O(H) overhead per round in contextual setting

- **Failure signatures**:
  - Suboptimal regret if exploration budget S - |A_con| becomes too small too quickly
  - Linear regret if UCB-style implicit exploration is used in graph feedback settings (Theorem 2.4)
  - Invalid confidence bounds in contextual setting if independence assumption violated (Remark 1)

- **First 3 experiments**:
  1. Graph feedback sanity check: Implement Algorithm 1 with clique-structured feedback graph (α = K/S), verify regret scales as O(√(KST))
  2. UCB vs. elimination comparison: On construction from Theorem 2.4, measure regret ratio between Algorithm 2 (UCB) and Algorithm 1 across varying α
  3. Contextual validation: Synthetic data with known θ* and d = 10, K = 50, S = 5; verify Algorithm 3 achieves O(√(dST)) and compare against LinUCB baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: Is there a computationally efficient elimination algorithm for combinatorial bandits with general constrained decision subsets?
  - **Basis in paper**: Appendix C states that for Algorithm 5 (constrained setting), "finding a minimal set of decisions... can be computationally hard in general"
  - **Why unresolved**: Reliance on finding minimal decision sets introduces computational complexity that current approximations don't resolve efficiently for general subsets A_0
  - **What evidence would resolve it**: An algorithm achieving stated regret bounds with polynomial time complexity per round for general constraints, or proof of computational hardness

- **Open Question 2**: Can the poly-logarithmic factors in instance-dependent regret bounds, specifically the log²K term, be removed or tightened?
  - **Basis in paper**: Theorem 2.1 presents upper bound scaling with (α log²K + S), while lower bounds scale more directly with α and S
  - **Why unresolved**: log²K factor arises from graph covering arguments in analysis; unclear if artifact of proof technique or fundamental to problem structure
  - **What evidence would resolve it**: Refined analysis removing log²K factor, or specific graph construction where regret necessarily scales with log²K

- **Open Question 3**: Can the proposed arm elimination framework be adapted to the adversarial setting to achieve optimal regret?
  - **Basis in paper**: Introduction contrasts paper's stochastic results with adversarial results from [Wen25], noting UCB methods fail in stochastic case due to exploration issues
  - **Why unresolved**: Current algorithm relies on validating confidence intervals on mean rewards, which don't exist in adversarial environments
  - **What evidence would resolve it**: Algorithm maintaining confirmed/active/eliminated partition structure using adversarial estimators to achieve matching minimax rates

## Limitations

- **Computational complexity**: Greedy graph domination subroutine may become prohibitive for large K; extension to general combinatorial constraints not computationally efficient
- **Assumption sensitivity**: Performance relies on confidence width concentration and conditional independence assumptions; violations could lead to suboptimal performance
- **Parameter tuning**: Requires careful selection of parameters like λ and β in contextual setting; suboptimal choices could degrade performance

## Confidence

- **Regret bounds optimality (Theorem 2.1, 2.2, 3.1)**: High - Proven through matching lower bounds and clear decomposition arguments
- **Mechanism 1 (three-way partition)**: Medium - Theoretically sound but requires careful parameter tuning in practice
- **Mechanism 2 (graph-guided exploration)**: High - Well-analyzed with explicit out-degree strategy and α-dependent bounds
- **Mechanism 3 (hierarchical contextual)**: Medium - Conditional independence assumption critical but not extensively validated empirically

## Next Checks

1. **Empirical validation of phase transition**: Implement construction from Theorem 2.4 and measure regret gap between Algorithm 2 (UCB) and Algorithm 1 across varying α values to verify predicted suboptimality

2. **Scalability assessment**: Benchmark Algorithm 1's runtime on random graphs with increasing K (e.g., K=100, 500, 1000) to characterize practical limits of greedy out-degree selection

3. **Contextual robustness test**: Design contextual bandit instance where independence assumptions are violated (e.g., correlated rewards across stages) and measure confidence interval coverage to validate conditional independence requirement