---
ver: rpa2
title: 'Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented
  LLMs'
arxiv_id: '2505.17762'
source_url: https://arxiv.org/abs/2505.17762
tags:
- evidence
- source
- media
- credibility
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of fact-checking with conflicting
  evidence from sources of varying credibility. The authors introduce CONFACT, a dataset
  of claims paired with conflicting documents annotated with source credibility, and
  systematically evaluate retrieval-augmented large language models (RAG-LLMs) on
  this task.
---

# Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs

## Quick Facts
- arXiv ID: 2505.17762
- Source URL: https://arxiv.org/abs/2505.17762
- Reference count: 36
- Primary result: Standard RAG models fail on conflicting evidence; source credibility cues at answer generation stage yield up to 10% absolute F1 improvement.

## Executive Summary
This study addresses the challenge of fact-checking when retrieved evidence contains conflicting documents from sources of varying credibility. The authors introduce CONFACT, a dataset of claims paired with conflicting documents annotated with source credibility, and systematically evaluate retrieval-augmented large language models (RAG-LLMs) on this task. They find that standard RAG methods struggle when evidence conflicts, often misattributing reliability to less credible sources. To improve performance, the authors incorporate media background information at different stages of the RAG pipeline. The most effective strategy is augmenting the answer generation stage with source credibility cues, particularly when combined with Chain-of-Thought prompting or explicit instructions to discern unreliable sources. Integrating source credibility during generation yields up to 10% absolute F1 improvement over baselines. Incorporating credibility at retrieval or ranking stages is less effective, likely due to loss of nuanced background information. Human evaluation confirms that source credibility aids in resolving conflicts, but automated credibility estimation remains noisy.

## Method Summary
The study evaluates RAG-LLMs on the CONFACT dataset using a three-stage pipeline: retrieval (BM25 over claim-derived questions), ranking (chunking into ≤256-word paragraphs, top-K selection), and answer generation. Strategies compared include baseline RAG methods (Direct Answer, Majority Vote, Discern and Answer, Chain-of-Thought) and credibility-augmented variants: Source Filtering, Credibility Weighting, and Source Background Augmentation. Credibility sources include ground-truth MBFC annotations (GT-MB) and a hybrid automated approach (Hybrid-MB). Inference uses vLLM with BFloat16 precision on 2x NVIDIA A100-80 GPUs.

## Key Results
- Standard RAG models fail on conflicting evidence, often misattributing reliability to less credible sources.
- Augmenting answer generation with source credibility cues yields up to 10% absolute F1 improvement.
- Credibility integration at retrieval or ranking stages is less effective due to loss of nuanced background information.
- Combining Chain-of-Thought prompting or explicit instructions to discern unreliable sources with source backgrounds significantly improves conflict resolution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting source credibility information at the answer generation stage (SBA) is more effective than filtering or re-ranking at retrieval.
- Mechanism: Providing the full textual source background description alongside each evidence paragraph preserves nuanced credibility signals (e.g., history of misinformation, bias type, failed fact checks). In contrast, earlier-stage methods must compress this into a scalar score or binary filter, discarding context needed by the LLM to reason about *why* a source is unreliable.
- Core assumption: LLMs can perform implicit credibility-weighted reasoning when given structured background text; this ability degrades when the signal is reduced to a numeric score.
- Evidence anchors:
  - [abstract] "The most effective strategy is augmenting the answer generation stage with source credibility cues... Incorporating credibility at retrieval or ranking stages is less effective, likely due to loss of nuanced background information."
  - [section 5.1, RQ 3] "strategies that introduce media backgrounds in earlier stages—such as retrieval or ranking—are less effective. This is likely due to information loss when converting detailed textual source descriptions into a single credibility level or a credibility score."
  - [corpus] Related work on RAG robustness (e.g., "Worse than Zero-shot?", "Rethinking All Evidence") similarly finds that LLM reasoning under knowledge conflict benefits from richer, structured context rather than hard filtering.
- Break condition: If source background descriptions are inaccurate (Hybrid-MB noise) or if the LLM has weak long-context handling (observed with Qwen-2 in Section 5.1), SBA gains diminish.

### Mechanism 2
- Claim: Combining Chain-of-Thought (CoT) prompting or explicit "discern unreliable sources" instructions with source backgrounds significantly improves conflict resolution.
- Mechanism: CoT forces the model to generate intermediate reasoning that references both evidence content and source credibility before concluding. This explicit reasoning step reduces tendency to default to majority-view or surface-level heuristics. Explicit discernment instructions act similarly by directing attention to credibility before answer synthesis.
- Core assumption: The model will follow the reasoning prompt and integrate credibility cues rather than ignore them; prompt adherence is reliable.
- Evidence anchors:
  - [abstract] "particularly when combined with Chain-of-Thought prompting or explicit instructions to discern unreliable sources."
  - [section 5.1, RQ 1] "using GPT-4o in RAG methods showed no clear advantage over open-source models, highlighting the problem's complexity"; yet prompting strategies (DisA., CoT) improved over direct answering.
  - [corpus] Adjacent work on multi-agent fact-checking (RAMA, FinVet) also uses staged reasoning to handle multimodal or domain-specific misinformation.
- Break condition: If prompts are not followed faithfully, or if the model lacks instruction-following capability, CoT benefits may not materialize.

### Mechanism 3
- Claim: Standard RAG models fail on conflicting evidence because they treat all retrieved documents as equally valid and lack credibility-aware reasoning.
- Mechanism: Without explicit source reliability signals, LLMs default to heuristics—frequency of supporting evidence, surface keyword overlap, or majority vote. When low-credibility sources outnumber high-credibility ones, the model amplifies misinformation.
- Core assumption: Conflict is primarily driven by source credibility differences, not by ambiguity in the claim itself.
- Evidence anchors:
  - [abstract] "standard RAG methods struggle when evidence conflicts, often misattributing reliability to less credible sources."
  - [section 5.1, RQ 1] "inability to distinguish misinformation from reliable sources—since vanilla RAG models do not assess source credibility, they treat all retrieved documents as equally valid"
  - [corpus] "ADMIT" and "Worse than Zero-shot?" examine RAG vulnerability to misleading retrievals and knowledge poisoning, reinforcing that naive retrieval is brittle under adversarial or noisy contexts.
- Break condition: If conflicting evidence arises from legitimate ambiguity (multiple valid perspectives) rather than credibility asymmetry, treating all sources equally may actually be appropriate.

## Foundational Learning

- Concept: **RAG pipeline stages (retrieval → ranking → generation)**
  - Why needed here: The paper's interventions are organized by where in the pipeline credibility is introduced. Understanding the baseline flow is prerequisite to diagnosing why early-stage injection underperforms.
  - Quick check question: At which stage does the paper find credibility integration most effective, and why?

- Concept: **Source credibility estimation**
  - Why needed here: The paper relies on MBFC ground-truth and a Hybrid-MB predictor; noise in automated estimation directly impacts method performance.
  - Quick check question: What evidence suggests that human-annotated credibility (GT-MB) is more reliable than automated estimation (Hybrid-MB) in this study?

- Concept: **Chain-of-Thought prompting**
  - Why needed here: CoT is central to the best-performing SBA variants; it is not a drop-in improvement for all models and requires understanding of instruction-following behavior.
  - Quick check question: How does CoT interact with source background information to improve conflict resolution, according to the paper's mechanism explanation?

## Architecture Onboarding

- Component map:
  - Retrieval: BM25 over claim-derived questions → top-N documents (provided fixed for reproducibility)
  - Ranking: Chunking into ≤256-word paragraphs → top-K selection (K=5 default)
  - Media Background Provider: GT-MB (expert annotations) or Hybrid-MB (LLM + search-augmented generation → credibility score via predictor)
  - Generation variants: (1) Baseline prompts (DirA, MajV, DisA, CoT); (2) Credibility-informed (SF, CWsoft, CWhard, SBAdir, SBACoT, SBAexp, SBAens)

- Critical path:
  1. Start with baseline RAG + CoT to establish performance floor
  2. Add GT-MB source backgrounds at generation (SBACoT) → expected highest gains (up to ~10% absolute F1 per abstract)
  3. Compare against SF/CW variants to confirm that early-stage compression underperforms

- Design tradeoffs:
  - GT-MB vs Hybrid-MB: Higher reliability vs broader coverage; Hybrid introduces estimation noise (Section 5.1, Table 2)
  - Paragraph vs sentence chunking: Paragraph preserves context but increases token load; sentence chunking fragments context (ablation Table 3)
  - Top-K size: More documents increase distraction and attention burden; fewer but higher-relevance chunks preferred (Section 5.2)

- Failure signatures:
  - Model defaults to majority view despite contradictory high-credibility evidence → likely not receiving or using source backgrounds
  - Performance collapse on Hybrid-MB compared to GT-MB → noisy credibility predictions misleading the model
  - Qwen-2 fails to benefit from SBA → weak long-context handling; check context length and attention behavior

- First 3 experiments:
  1. Baseline: Run RAG with DirA and CoT (no source backgrounds) on ModC/HumC; record accuracy and macro-F1
  2. Intervention: Add GT-MB backgrounds at generation with SBACoT; expect +5–10% F1 improvement
  3. Ablation: Replace GT-MB with Hybrid-MB; quantify performance drop to assess sensitivity to estimation noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic, context-aware credibility estimation methods outperform static source-level annotations in fact-checking pipelines?
- Basis in paper: [explicit] The authors state in the Limitations section that their approach "treats media source credibility as static and context-independent," and suggest future work could explore "dynamic, context-aware credibility estimation methods."
- Why unresolved: Current implementations rely on fixed credibility scores (e.g., MBFC), which fail to capture variations in reliability across different topics or time periods.
- What evidence would resolve it: A dynamic estimation model that adjusts credibility based on the specific claim domain, demonstrating higher accuracy on the HumC split than static GT-MB baselines.

### Open Question 2
- Question: How can automated credibility prediction be refined to eliminate the performance gap between Hybrid-MB and expert-verified GT-MB baselines?
- Basis in paper: [explicit] The conclusion identifies "Improving the accuracy of LLM-based credibility prediction" as a "key open challenge," noting that automated methods remain limited and noisy compared to curated assessments.
- Why unresolved: The study found that Hybrid-MB (automated) introduces noise that degrades performance, whereas GT-MB (human-curated) consistently yields better results but lacks scalability.
- What evidence would resolve it: An automated credibility estimator that achieves comparable or superior F1 scores to GT-MB on the ModC split without relying on pre-existing human annotations.

### Open Question 3
- Question: Does sentence-level de-contextualization allow for more efficient processing without sacrificing the reasoning capability provided by paragraph-level chunking?
- Basis in paper: [explicit] The ablation study notes that while paragraph-level chunking outperforms sentence-level, "longer paragraph inputs increase computational overhead." The authors suggest "de-contextualization methods" as a potential solution.
- Why unresolved: There is currently a trade-off where sentence-level chunking is computationally cheaper but lacks the necessary context to resolve conflicts effectively.
- What evidence would resolve it: A de-contextualization strategy that enables sentence-level inputs to achieve Macro-F1 scores comparable to the paragraph-level baseline (Table 2) on the CONFACT dataset.

## Limitations
- Reliance on automated source credibility estimation introduces noise that degrades performance compared to ground-truth annotations
- Fixed retrieval and ranking pipeline may not capture all relevant evidence, particularly for complex claims requiring deeper context
- Evaluation focuses on binary classification, potentially oversimplifying nuanced fact-checking scenarios
- Does not address adversarial attacks that could manipulate source credibility signals or retrieval results

## Confidence
- **High confidence**: The mechanism that generation-stage credibility augmentation outperforms early-stage methods is well-supported by ablation studies and human evaluation. The observation that CoT prompting improves conflict resolution is also robust across model families.
- **Medium confidence**: Claims about specific performance improvements (e.g., "up to 10% absolute F1 improvement") are model- and dataset-dependent. The effectiveness of different SBA variants varies significantly across model families, suggesting these results may not generalize to all LLMs.
- **Low confidence**: The assertion that RAG models "fail" on conflicting evidence assumes all conflicts stem from credibility differences rather than legitimate ambiguity. The paper does not thoroughly explore this alternative explanation.

## Next Checks
1. **Ablation study on retrieval depth**: Compare performance when retrieving top-3 vs top-5 vs top-10 paragraphs to quantify the trade-off between evidence coverage and attention burden.
2. **Human evaluation of Hybrid-MB noise**: Systematically sample and annotate a subset of Hybrid-MB predictions to quantify estimation error rates and their correlation with performance degradation.
3. **Cross-dataset generalization**: Evaluate the best-performing methods (SBACoT with GT-MB) on an independent fact-checking dataset with conflicting evidence to assess robustness beyond CONFACT.