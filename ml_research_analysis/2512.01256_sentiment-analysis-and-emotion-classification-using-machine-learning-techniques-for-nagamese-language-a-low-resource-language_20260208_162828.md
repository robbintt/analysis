---
ver: rpa2
title: Sentiment Analysis and Emotion Classification using Machine Learning Techniques
  for Nagamese Language -- A Low-resource Language
arxiv_id: '2512.01256'
source_url: https://arxiv.org/abs/2512.01256
tags:
- sentiment
- nagamese
- language
- emotion
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first sentiment and emotion classification
  system for the low-resource Nagamese language. The authors created a small annotated
  dataset of 594 sentences and a sentiment lexicon of 1,195 words, using these to
  build features for machine learning classifiers.
---

# Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language -- A Low-resource Language

## Quick Facts
- arXiv ID: 2512.01256
- Source URL: https://arxiv.org/abs/2512.01256
- Reference count: 4
- Primary result: First sentiment and emotion classification system for low-resource Nagamese language, achieving 71% accuracy for sentiment polarity and 67% for emotion classification using SVM classifiers

## Executive Summary
This paper presents the first sentiment and emotion classification system for Nagamese, a low-resource creole language derived from Assamese and other Naga languages. The authors created a small annotated dataset of 594 sentences and a manually-built sentiment lexicon of 1,195 words, using these to develop features for machine learning classifiers. They applied Naïve Bayes and SVM classifiers with lexical and structural features to predict both sentiment polarity (positive, negative, neutral) and eight basic emotions. SVM achieved the best performance: 71% accuracy for sentiment polarity and 67% for emotion classification, demonstrating the feasibility of sentiment analysis for low-resource languages using simple supervised learning approaches.

## Method Summary
The authors collected 594 manually annotated Nagamese sentences from news websites and bible passages, with three annotators categorizing each sentence by sentiment polarity and emotion. They built a sentiment lexicon of 1,195 words categorized as positive, negative, or neutral. For classification, they extracted nine features including counts of positive/negative words from the lexicon, intensity words, specific markers ("bisi", "olop"), emoticons, and punctuation. They applied both Gaussian Naïve Bayes and SVM classifiers with RBF and polynomial kernels, using an 80/20 train-test split. The SVM with RBF kernel achieved 71% accuracy for sentiment classification, while the SVM with polynomial kernel achieved 67% accuracy for emotion classification.

## Key Results
- SVM classifier with RBF kernel achieved 71% accuracy for sentiment polarity classification
- SVM classifier with polynomial kernel achieved 67% accuracy for 8-class emotion classification
- Highest per-class F1-scores were 0.76 (positive polarity) and 1.00 (fear, surprise emotions)
- Naïve Bayes performed substantially worse at 59% (polarity) and 21% (emotion)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Handcrafted sentiment lexicons can substitute for large annotated corpora when building initial classification systems for low-resource languages
- Mechanism: Domain experts manually label 1,195 unique Nagamese words as positive, negative, or neutral. These counts become numerical features fed to classifiers, converting sparse text into dense feature vectors without requiring word embeddings or large pretraining data
- Core assumption: Sentiment signals are primarily lexical (word-level) rather than contextual or syntactic
- Evidence anchors:
  - [abstract] "We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning"
  - [section 5.1] Sentiment dictionary contains 162 positive, 162 negative, and 871 neutral words used for feature generation
  - [corpus] Related work on low-resource Arabic (MultiProSE) similarly uses curated lexical resources for sentiment tasks
- Break condition: If sentiment is highly context-dependent (negation, sarcasm, domain shift), pure lexical counts will fail

### Mechanism 2
- Claim: SVM with non-linear kernels outperforms Naïve Bayes on small, manually-featurized datasets for multi-class sentiment tasks
- Mechanism: SVM maximizes margin between classes in transformed kernel space. The RBF kernel (best for polarity at 71%) captures non-linear relationships among lexical count features; polynomial kernel (best for emotion at 67%) handles multi-class boundaries across 8 emotion categories
- Core assumption: Feature space has learnable structure despite only 494 training examples
- Evidence anchors:
  - [section 7] "We obtain the highest accuracy for both sentiment and emotion using SVM classifier, with a score of 71% using rbf kernel and 67% respectively using poly kernel"
  - [section 7] Naïve Bayes achieved only 59% (polarity) and 21% (emotion)—substantially worse
  - [corpus] Weak direct corpus evidence for kernel-specific mechanisms in low-resource settings
- Break condition: If training data drops below ~200-300 samples or features become highly correlated, SVM may overfit or fail to converge

### Mechanism 3
- Claim: Feature selection (removing neutral word count, question mark, sentence length) improves generalization on small datasets
- Mechanism: Starting with 12 features, authors heuristically selected 9 "best features" by likely removing noisy or redundant signals. This reduces dimensionality relative to sample size (494 training samples), lowering overfitting risk
- Core assumption: Some features (e.g., neutral word counts, sentence length) add noise rather than signal for sentiment/emotion tasks
- Evidence anchors:
  - [section 5.2] Lists 12 initial features and 9 "best features set for prediction"—neutral word count, question mark, and sentence length were removed
  - [section 6] Training set of 494 sentences with 9 features yields ~55 samples per feature (acceptable for SVM)
  - [corpus] No direct corpus evidence on feature selection for this specific language
- Break condition: If important domain-specific signals are accidentally removed, performance degrades; systematic feature importance analysis (e.g., SHAP, permutation) would be needed to verify

## Foundational Learning

- **Concept: Sentiment Lexicons**
  - Why needed here: The entire system depends on a manually-built dictionary mapping words to sentiment polarities. Understanding lexicon limitations (coverage, polysemy, negation) is critical
  - Quick check question: If "bhal" means "good" in isolation, how would you handle "bhal nai" (not good)?

- **Concept: SVM Kernels and Margin Classification**
  - Why needed here: SVM is the best-performing model. Knowing why RBF works for polarity and poly for emotions helps diagnose and extend the system
  - Quick check question: What happens to an RBF SVM decision boundary if gamma is set too high on a small dataset?

- **Concept: Feature Engineering for Low-Resource Languages**
  - Why needed here: No pre-trained embeddings exist for Nagamese. All signal comes from handcrafted features (word counts, emoticons, intensity markers)
  - Quick check question: Why might counting emoticons help despite their likely scarcity in formal news text?

## Architecture Onboarding

- **Component map:** Raw Nagamese Text → Preprocessing (tokenization) → Sentiment Dictionary Lookup (1,195 words) → Feature Extraction (9 features: pos/neg counts, intensity counts, emoticons, punctuation) → SVM Classifier (RBF for polarity, Poly for emotion) → Predicted Labels (3 polarities OR 8 emotions)

- **Critical path:** Dictionary quality is the bottleneck. Errors in lexicon annotation propagate directly to all downstream predictions. The lexicon took significant manual effort (1,195 words categorized)

- **Design tradeoffs:**
  - SVM vs. Naïve Bayes: SVM is 12-46 percentage points better but requires kernel tuning and is slower at inference
  - Lexicon features vs. learned embeddings: Lexicons are interpretable and require no GPU, but cannot capture context or rare words outside the dictionary
  - 9 vs. 12 features: Reducing features may help generalization but risks losing signal; no ablation study in paper quantifies this

- **Failure signatures:**
  - Emotions with zero F1-score (anger, trust in Table 7) indicate class imbalance or insufficient distinctive features
  - Confusion between anticipation↔joy (11+8 misclassifications) suggests feature overlap between related emotions
  - Neutral sentiment misclassified as positive (11 instances) may reflect lexical sparsity or annotator subjectivity

- **First 3 experiments:**
  1. **Baseline replication:** Train SVM (RBF) on the 9 features with 80/20 split; verify ~71% polarity accuracy to confirm reproducibility
  2. **Ablation study:** Remove one feature at a time (especially emoticons, "bisi/olop" intensity words) to quantify each feature's contribution; expect emoticons to have minimal impact on news text
  3. **Class balance analysis:** Report per-class F1-scores and confusion matrices; if anger/trust remain at 0.00, explore oversampling or merging rare emotion classes

## Open Questions the Paper Calls Out
- Question: How would deep learning models (e.g., CNNs, LSTMs, transformers) compare to traditional ML approaches for Nagamese sentiment analysis?
  - Basis in paper: [explicit] Authors state in conclusion: "It is intended to create a larger dataset in the future and explore other machine and deep learning models."
  - Why unresolved: This study only evaluated Naïve Bayes and SVM; no neural approaches were tested
  - What evidence would resolve it: Comparative experiments with neural models on the same or expanded Nagamese dataset

## Limitations
- Dataset availability: The annotated Nagamese corpus (594 sentences) and sentiment lexicon (1,195 words) are not publicly released, preventing direct replication and validation of results
- Feature selection methodology: The selection of "best features" from 12 to 9 is not explained; no ablation study quantifies individual feature contributions or risks of information loss
- Class imbalance: Several emotion classes (anger, trust) have zero F1-scores, suggesting severe imbalance that may not be adequately addressed by the current approach

## Confidence
- **High confidence:** SVM outperforms Naïve Bayes on this task (71% vs 59% for polarity, 67% vs 21% for emotion)
- **Medium confidence:** Handcrafted lexical features with SVM are viable for low-resource sentiment analysis (given limited alternatives)
- **Low confidence:** Specific feature engineering choices (9 vs 12 features) are optimal; individual feature contributions are understood

## Next Checks
1. **Dataset release and annotation verification:** Request public release of the annotated corpus and lexicon; examine inter-annotator agreement scores to assess label reliability
2. **Systematic feature ablation:** Remove each of the 9 features individually and measure performance impact; quantify how much accuracy depends on each component (especially emoticons, intensity words)
3. **Class balance intervention:** Stratify train/test splits by emotion class; test oversampling or class-weighted training for minority classes (anger, trust) to assess if zero F1-scores can be improved