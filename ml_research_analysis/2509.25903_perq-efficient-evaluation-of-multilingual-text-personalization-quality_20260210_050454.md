---
ver: rpa2
title: 'PerQ: Efficient Evaluation of Multilingual Text Personalization Quality'
arxiv_id: '2509.25903'
source_url: https://arxiv.org/abs/2509.25903
tags:
- personalization
- quality
- texts
- text
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PerQ, an efficient evaluation metric for multilingual
  text personalization quality. The authors address the problem of using resource-intensive
  large language models (LLMs) for meta-evaluation of personalization quality, which
  is common due to the lack of dedicated metrics.
---

# PerQ: Efficient Evaluation of Multilingual Text Personalization Quality

## Quick Facts
- arXiv ID: 2509.25903
- Source URL: https://arxiv.org/abs/2509.25903
- Authors: Dominik Macko; Andrew Pulver
- Reference count: 14
- Primary result: Introduces PerQ, a fine-tuned classifier for multilingual text personalization quality evaluation

## Executive Summary
PerQ addresses the computational inefficiency of using large language models (LLMs) for meta-evaluation of text personalization quality. The authors develop a fine-tuned classifier that mimics the outputs of three diverse LLMs, achieving strong correlation (Spearman > 0.8) while requiring 96× less computation time and 12× less GPU memory. A case study demonstrates PerQ's practical utility in comparing personalization capabilities across different LLMs, languages, and platforms, confirming findings from expensive LLM-based evaluations at a fraction of the cost.

## Method Summary
The PerQ approach involves training a classifier to predict text personalization quality scores by learning from majority-voted labels generated by three diverse LLMs. The model is trained on multilingual data spanning seven languages and evaluated using Spearman correlation against the LLM ground truth. The key innovation lies in replacing repeated LLM inference with a single forward pass through the fine-tuned PerQ model, dramatically reducing computational requirements while maintaining evaluation fidelity.

## Key Results
- PerQ achieves Spearman correlation > 0.8 with three LLM evaluations across seven languages
- Computational efficiency: 96× faster predictions with 12× less GPU memory than using three LLMs
- Case study confirms PerQ can reliably compare personalization capabilities across different LLMs, languages, and platforms at minimal cost

## Why This Works (Mechanism)
PerQ works by distilling the collective judgment of three diverse LLMs into a single, efficient classifier. The model learns to reproduce the consensus of these LLMs through supervised fine-tuning on their majority-voted outputs. This approach leverages the fact that while individual LLM evaluations are expensive, the space of possible personalization quality judgments is relatively constrained, allowing a smaller model to approximate the ensemble behavior with high fidelity.

## Foundational Learning
- **Multilingual embeddings**: Understanding how language representations work across different linguistic families (needed for cross-lingual generalization; quick check: evaluate on held-out languages)
- **Teacher-student distillation**: The process of training a smaller model to mimic a larger ensemble (needed for efficiency gains; quick check: compare correlation with individual LLM vs. ensemble)
- **Spearman correlation**: Non-parametric measure of rank correlation (needed for evaluating prediction quality; quick check: compute on validation set)
- **Majority voting**: Ensemble method combining multiple model outputs (needed for robust ground truth; quick check: verify consistency across LLMs)
- **Fine-tuning vs. training from scratch**: Transfer learning efficiency considerations (needed for practical deployment; quick check: compare performance with pre-trained vs. random initialization)
- **GPU memory optimization**: Techniques for reducing computational overhead (needed for scalability; quick check: measure memory usage across batch sizes)

## Architecture Onboarding
- **Component map**: Input text → PerQ classifier → Quality score (single forward pass)
- **Critical path**: Text preprocessing → Embedding generation → PerQ prediction → Output score
- **Design tradeoffs**: Speed vs. accuracy (acceptable 0.8+ correlation), generalization vs. specialization (trained on 7 languages but tested zero-shot), computational cost vs. deployment simplicity
- **Failure signatures**: Poor performance on languages outside training set, degraded accuracy with highly domain-specific text, correlation drops when LLM consensus is weak
- **First experiments**: 1) Measure correlation on validation set across all 7 languages, 2) Benchmark computational efficiency against three LLM approach, 3) Test zero-shot performance on a language not in the training set

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does PerQ maintain strong correlation with human judgments in non-English languages, given that validation relied solely on an English proxy dataset?
- Basis in paper: Appendix C validates the approach against human judgment using English-only data, stating "we realize this correlation might be different in multilingual settings."
- Why unresolved: The authors could not conduct a full multilingual human annotation study due to cost and feasibility constraints.
- What evidence would resolve it: A human annotation study on non-English samples comparing PerQ scores directly against human ratings.

### Open Question 2
- Question: Can PerQ generalize to languages from families outside the seven tested (Indo-European/Uralic), such as tonal or right-to-left languages?
- Basis in paper: The Limitations section states the study is restricted to 7 languages and authors are "unsure about generalization of our findings to other languages."
- Why unresolved: The training data covers a specific, limited set of language families, potentially limiting the model's multilingual embeddings.
- What evidence would resolve it: Zero-shot evaluation of PerQ performance on diverse languages not included in the training set (e.g., Chinese, Arabic).

### Open Question 3
- Question: Does the reliance on LLM-based majority voting as ground truth cause PerQ to inherit systematic biases or failure modes from the teacher models?
- Basis in paper: The Limitations section acknowledges that despite using 3 LLMs to limit bias, "there can still be some" internal bias in the meta-evaluation.
- Why unresolved: The metric is trained to mimic the LLMs, creating a potential blind spot for human-perceived quality that LLMs miss.
- What evidence would resolve it: Comparative analysis of error cases where PerQ and the teacher LLMs agree but differ significantly from human judgment.

## Limitations
- PerQ's performance on non-English languages remains uncertain due to validation being conducted only on English human judgments
- The model's generalization to languages outside the seven tested (primarily Indo-European/Uralic) is unknown
- Potential bias inheritance from the three LLM teacher models, which could cause PerQ to miss human-perceived quality issues

## Confidence
- **High confidence**: Computational efficiency improvements relative to multi-LLM approaches
- **Medium confidence**: Correlation strength with LLM evaluations across tested languages
- **Medium confidence**: Case study findings on personalization capabilities across platforms

## Next Checks
1. Test PerQ's generalization performance on languages not included in the original training set, particularly low-resource languages
2. Conduct ablation studies to determine the impact of individual LLM contributions to the majority voting process
3. Validate the stability of efficiency claims across different hardware configurations and scaling scenarios