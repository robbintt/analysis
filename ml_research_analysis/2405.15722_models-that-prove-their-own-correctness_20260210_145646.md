---
ver: rpa2
title: Models That Prove Their Own Correctness
arxiv_id: '2405.15722'
source_url: https://arxiv.org/abs/2405.15722
tags:
- learning
- proof
- theorem
- verifier
- transcript
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Proving models that can formally prove
  the correctness of their outputs to a verification algorithm via an interactive
  proof system. The core idea is to train models that not only generate correct outputs
  but also convince a verifier of their correctness with worst-case soundness guarantees
  - ensuring that incorrect outputs from any model are detected with high probability.
---

# Models That Prove Their Own Correctness

## Quick Facts
- arXiv ID: 2405.15722
- Source URL: https://arxiv.org/abs/2405.15722
- Authors: Noga Amit; Shafi Goldwasser; Orr Paradise; Guy Rothblum
- Reference count: 40
- Key outcome: Introduces Self-Proving models that formally prove correctness of their outputs to a verification algorithm via interactive proof systems, achieving 60.3% verifiability on GCD that increases to 96% with annotated transcripts.

## Executive Summary
This paper introduces Self-Proving models that can formally prove the correctness of their outputs to a verification algorithm via an interactive proof system. The core idea is to train models that not only generate correct outputs but also convince a verifier of their correctness with worst-case soundness guarantees - ensuring that incorrect outputs from any model are detected with high probability. The authors propose two methods for learning Self-Proving models: Transcript Learning (TL) which trains models on transcripts of accepting interactions, and Reinforcement Learning from Verifier Feedback (RLVF) which trains via interaction with the verifier.

## Method Summary
The framework trains a transformer model to act as a prover in an interactive proof system with a fixed verifier algorithm. The model learns to generate both correct outputs and convincing proofs. Transcript Learning uses pre-computed "accepting transcripts" (input, output, and proof) to train the model via supervised learning, while RLVF uses policy gradient methods to learn from sparse verifier feedback. The approach is demonstrated on computing the greatest common divisor (GCD), where the model must provide Bézout coefficients as proof of correctness.

## Key Results
- A 6.3M parameter transformer trained with Transcript Learning proves 60.3% of its GCD answers correct
- Using annotated transcripts (intermediate computation steps) increases verifiability to 96%
- Transcript Learning provides gradient approximation guarantees under convexity assumptions
- The framework enables verification of individual model outputs rather than relying on average-case correctness

## Why This Works (Mechanism)

### Mechanism 1: Transcript Learning (TL) Gradient Estimation
- **Claim:** Training on complete "accepting transcripts" aligns the model's policy with the verification protocol
- **Mechanism:** TL optimizes parameters by maximizing log-likelihood of tokens in honest transcripts, with Theorem 4.2 proving the update step is an unbiased estimator for the gradient of agreement probability
- **Core assumption:** Access to honest transcript generator
- **Break condition:** Fails if input distribution shifts significantly or model capacity is insufficient

### Mechanism 2: Worst-Case Soundness via Interactive Proof
- **Claim:** Provides formal correctness guarantee for individual inputs via sound verification protocol
- **Mechanism:** Verifier is a fixed efficient algorithm with soundness property - incorrect outputs from any prover are detected with high probability
- **Core assumption:** Verifier correctly implements soundness guarantee
- **Break condition:** Fails if problem doesn't admit efficient interactive proof or verifier has implementation bugs

### Mechanism 3: Annotation as Chain-of-Thought Scaffolding
- **Claim:** Prepending intermediate computation steps increases verifiability
- **Mechanism:** Annotations act as Chain-of-Thought, breaking proof generation into simpler steps and reducing learning burden
- **Core assumption:** Intermediate steps provide valid logical trajectory
- **Break condition:** Fails if reasoning depth exceeds context window or annotations introduce noise

## Foundational Learning

- **Concept:** Interactive Proof Systems (IP)
  - **Why needed here:** Theoretical foundation understanding Prover/Verifier roles and Completeness vs Soundness
  - **Quick check question:** If a model generates an incorrect GCD but Verifier accepts it, which property (Completeness or Soundness) was violated?

- **Concept:** Autoregressive Modeling
  - **Why needed here:** Models prover as autoregressive transformer; transcript probability is product of conditional probabilities
  - **Quick check question:** In Equation (3) of Section A.2, how is the probability of a transcript π decomposed?

- **Concept:** Policy Gradient / REINFORCE
  - **Why needed here:** Required to understand RLVF which treats verification outcome as sparse reward signal
  - **Quick check question:** Why does RLVF suffer from an "exploration problem" compared to Transcript Learning?

## Architecture Onboarding

- **Component map:** P_θ (Model) -> V (Verifier) -> T*ᵥ (Generator)
- **Critical path:** 1) Generate transcripts using honest prover, 2) Train model via Transcript Learning on (x, y, π) tuples, 3) At inference, model outputs y and interacts with V
- **Design tradeoffs:** TL is sample-efficient but needs pre-computed transcripts; RLVF needs no transcripts but requires careful initialization and suffers from exploration issues
- **Failure signatures:** Correct but Unverifiable (right answer but no accepted proof), Exploration Collapse (RLVF never receives reward signal)
- **First 3 experiments:** 1) Baseline GPT on (x, y) pairs to confirm high accuracy but zero verifiability, 2) Transcript Learning on (x, y, π) tuples, 3) Annotation ablation varying intermediate steps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what theoretical conditions does RLVF converge to an optimal Self-Proving model?
- **Basis in paper:** [Explicit] Remark 5.2 and Section 8 state that analyzing RLVF convergence is "a challenging and active area of research" and leave full analysis to future work
- **Why unresolved:** RLVF is derived as Policy Gradient, a class of algorithms lacking general convergence guarantees unlike Transcript Learning
- **What evidence would resolve it:** Formal proof establishing convergence bounds for RLVF or empirical analysis identifying specific hyperparameters and conditions that guarantee convergence

### Open Question 2
- **Question:** Can the framework be adapted to train a single "generalist" model that proves correctness across multiple distinct verifiers?
- **Basis in paper:** [Explicit] Section 8 notes limitation that current methods require separate models for each capability and suggests adapting definition for generalist models
- **Why unresolved:** Current theory focuses on single tasks; unclear if single parameter set can satisfy interactive proof requirements for unrelated functionalities
- **What evidence would resolve it:** Successful training and evaluation of unified model maintaining high verifiability across distinct proof systems

### Open Question 3
- **Question:** To what extent does distribution bias in "Backward Data Generation" impair generalization to out-of-distribution inputs?
- **Basis in paper:** [Explicit] Section 8 (Limitations) and Section 4.1.1 discuss inherent limitation of generating transcripts before inputs, warning models may learn construction patterns rather than generalizable reasoning
- **Why unresolved:** While theoretically sound for data generation, authors explicitly highlight risk of training distribution bias but don't quantify impact on generalization
- **What evidence would resolve it:** Comparative study evaluating verifiability of models trained via backward generation versus standard methods on significantly different test distributions

## Limitations
- Framework depends critically on existence of efficient interactive proof systems for target problems
- Requires either pre-computed training transcripts or very careful initialization, limiting scalability
- Experimental evaluation narrowly focused on GCD problem, leaving uncertainty about performance on complex tasks

## Confidence

**High Confidence:** Theoretical foundations regarding interactive proof systems, formal definitions of completeness and soundness, and gradient approximation proofs are mathematically rigorous and well-supported.

**Medium Confidence:** GCD experimental results are well-documented and reproducible, but generalizability to other problems remains uncertain. Claims about annotation improving verifiability are supported but lack detailed ablation studies.

**Low Confidence:** Claims about framework's applicability to "any problem admitting efficient interactive proof system" are speculative without empirical validation on multiple problem domains.

## Next Checks

1. **Domain Transfer Validation:** Apply Self-Proving framework to fundamentally different problem (e.g., graph reachability or sorting verification) to test generalizability beyond arithmetic problems.

2. **Verifier Robustness Testing:** Systematically test verifier implementation for soundness violations by attempting to construct adversarial proofs that exploit potential implementation bugs or edge cases.

3. **Scaling Analysis:** Evaluate how verifiability scales with model size, problem complexity, and annotation depth across multiple problem types to identify practical limits of the approach.