---
ver: rpa2
title: 'ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language
  Models'
arxiv_id: '2503.00564'
source_url: https://arxiv.org/abs/2503.00564
tags:
- user
- call
- dialogue
- system
- inform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToolDial, a multi-turn dialogue dataset for
  tool-augmented language models (TALMs) that addresses the gap in existing benchmarks
  by simulating realistic interactions involving API chaining and complex user-system
  dynamics. The dataset contains 11,111 dialogues averaging 8.95 turns each, constructed
  using an API graph that captures input-output compatibility between 4,474 APIs from
  RapidAPI.
---

# ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language Models

## Quick Facts
- arXiv ID: 2503.00564
- Source URL: https://arxiv.org/abs/2503.00564
- Reference count: 40
- 11,111 dialogues with 8.95 turns average, evaluating multi-turn tool-augmented language models

## Executive Summary
This paper introduces ToolDial, a multi-turn dialogue dataset designed to address evaluation gaps in tool-augmented language models (TALMs). Unlike existing benchmarks that focus on single-turn interactions, ToolDial simulates realistic scenarios involving API chaining, clarification requests, and complex user-system dynamics. The dataset contains 11,111 dialogues with an average of 8.95 turns each, constructed using an API graph that captures input-output compatibility between 4,474 APIs from RapidAPI.

The authors evaluate modern language models on three core tasks: dialogue state tracking, action prediction, and response faithfulness. Results show that while GPT models achieve high faithfulness (>90%), they struggle with action prediction, particularly for "Request" and "Clarify" actions. Fine-tuning smaller Llama models on ToolDial significantly improves performance on these challenging actions, demonstrating the dataset's value for training and assessing TALMs in complex multi-turn interactions.

## Method Summary
ToolDial employs a four-stage generation process: (1) constructing an API graph where edges represent input-output compatibility between APIs using embedding similarity and keyword matching, (2) selecting from 23 predefined action sequences that capture realistic interaction patterns, (3) generating scenario instructions with dialogue states and API parameters using GPT-4o, and (4) converting scenarios to natural dialogue utterances. The dataset incorporates 16 user and system actions including "Request", "Clarify", and "Fail inform" to handle edge cases where systems must ask clarifying questions or call additional APIs when users cannot provide required information.

## Key Results
- Dialogue state tracking accuracy drops below 70% for GPT models, with accuracy decreasing as dialogue length increases
- Action prediction F1 scores around 60% for GPT models, with particularly poor performance on "Request" (14.3%) and "Clarify" actions without ground truth
- Response faithfulness remains above 90% for GPT models but drops to 88.4% for fine-tuned Llama models
- Fine-tuning smaller Llama models on ToolDial significantly improves performance on actions that GPT models struggle with, achieving 78.4% F1 on "Request" action

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The API graph enables systematic selection of tool chains where one API's output can feed another's input, reducing the combinatorial complexity of multi-API dialogue generation.
- Mechanism: The paper constructs an API graph where nodes are 4,474 APIs and edges connect pairs where output/input entity compatibility exists (validated via embedding similarity, keyword matching, and LCS on entity names). This graph structure allows systematic sampling of API pairs for dialogue scenarios involving tool chaining (e.g., when a user cannot provide required parameters).
- Core assumption: API input-output compatibility can be inferred from documentation text embeddings and name similarity, and this automated inference is sufficient for generating realistic tool-chaining dialogues.
- Evidence anchors:
  - [abstract] "we introduce a method for generating an API graph that represents input and output compatibility between APIs"
  - [section 3.1] Edge construction uses emb(do, di) > td ∧ emb(do + ko, di + ki) > tk ∧ LCS(no, ni) > tl, achieving 70.9% precision and 95.0% NPV on human evaluation
  - [corpus] Related work ToolNet also uses API graphs but connects APIs by co-occurrence in dialogues rather than input-output compatibility
- Break condition: If API documentation is incomplete, ambiguous, or uses inconsistent naming conventions across APIs, the edge construction criteria may fail to capture valid chains or create spurious connections.

### Mechanism 2
- Claim: Predefined action sequences (16 actions, 23 sequences) constrain dialogue generation to realistic interaction patterns, ensuring the dataset captures edge cases like clarification requests and failure handling.
- Mechanism: Rather than freely generating dialogues, the method first selects an action sequence as a "skeleton" (e.g., Inform intent clear → Retriever call → Request → Fail inform → Retriever call → Request → Inform → Call → Response). This skeleton is then augmented with concrete dialogue states and API-specific information before utterance generation.
- Core assumption: The 16 defined actions and 23 sequences adequately cover the space of realistic user-system-tool interactions, and constraining generation to these patterns produces more useful training data than unconstrained generation.
- Evidence anchors:
  - [abstract] "ToolDial incorporates 16 user and system actions (e.g., 'Request', 'Clarify', 'Fail inform') and 23 plausible action sequences"
  - [section 3.2] "We compile a taxonomy that covers a wide range of actions occurring in user-system interactions"
  - [corpus] Weak direct corpus evidence on action sequence efficacy; related benchmarks like API-Bank use fewer action types (7 vs. 16)
- Break condition: If real-world tool interactions require action patterns not captured in the 23 predefined sequences, the dataset will systematically miss those interaction types.

### Mechanism 3
- Claim: Fine-tuning on ToolDial's multi-turn, action-diverse dialogues improves smaller models' ability to handle complex action prediction (especially Request, Clarify, Suggest) that large models struggle with.
- Mechanism: The dataset's explicit action annotations and dialogue state labels provide training signal for when to request information vs. immediately respond. GPT models tend to rush to answers (Table 5: GPT-4o achieves only 14.3% F1 on "Request" action without ground truth), while TD-Llama fine-tuned on ToolDial achieves 78.4% F1.
- Core assumption: The action prediction improvements from fine-tuning generalize to real-world tool-use scenarios beyond the RapidAPI domains in the dataset.
- Evidence anchors:
  - [abstract] "Fine-tuning smaller Llama models on ToolDial significantly improves performance, particularly on actions that GPT models struggle with"
  - [section 4.3] Table 5 shows TD-Llama achieves 78.4% F1 on Request action (w/o GT) vs. GPT-4o's 14.3%
  - [corpus] DiaFORGE (corpus neighbor) also addresses disambiguation in tool calling, suggesting action prediction difficulty is a recognized problem
- Break condition: If downstream applications have different action distributions or API retrieval mechanisms than the ToolDial setup, fine-tuning benefits may not transfer.

## Foundational Learning

- **Dialogue State Tracking (DST)**:
  - Why needed here: DST is a core evaluation task (Section 4.1), measuring whether models can extract API names and parameter values from dialogue history. Understanding DST is essential to interpret the paper's finding that accuracy degrades with dialogue length (Appendix A.10).
  - Quick check question: Given dialogue history where user says "I want weather for Seattle" and system has retrieved WeatherAPI requiring 'city' and 'date', what should the dialogue state be?

- **Tool-Augmented Language Models (TALMs)**:
  - Why needed here: The entire paper addresses TALM evaluation gaps. Understanding that TALMs must select tools, extract parameters, and handle multi-turn interactions explains why existing single-turn benchmarks are insufficient.
  - Quick check question: How does a TALM differ from a standard chatbot when a user asks "Book me a flight to Tokyo next Tuesday"?

- **API Chaining**:
  - Why needed here: The API graph mechanism (Section 3.1) exists specifically to enable scenarios where API2's output provides API1's missing input. This is central to the "Fail inform" action sequences.
  - Quick check question: If API A outputs coordinates and API B requires location_name, can they be chained? What if API C converts coordinates to location_name?

## Architecture Onboarding

- **Component map**: API Graph Construction -> Action Sequence Selection -> Scenario Generation -> Utterance Generation
- **Critical path**: API Graph → (API Pair Selection + Action Sequence Selection) → Dialogue State Augmentation → Utterance Generation → Final Dialogue
- **Design tradeoffs**:
  - Automated vs. human validation: Graph edges validated via StableToolBench (70.9% precision) rather than human annotation at scale
  - Constrained vs. free generation: Action sequences constrain diversity but ensure coverage of edge cases like "Fail inform"
  - GPT-4o generation: Creates scalable data but may inherit model biases (addressed via predefined speaking styles in Appendix A.6)
- **Failure signatures**:
  - Dialogue State Tracking accuracy drops with turn count (Appendix A.10: ~28% full-dialogue accuracy for TD-Llama)
  - "Response fail" action poorly predicted by all models (0% F1 for GPT models)
  - API retrieval score thresholds (0.5/0.6) may not generalize to different retrievers
- **First 3 experiments**:
  1. **Reproduce DST degradation analysis**: Evaluate TD-Llama on held-out dialogues, plotting accuracy vs. turn number to confirm the 28.3% full-dialogue accuracy finding
  2. **Ablate action sequence diversity**: Train on subsets of ToolDial using only high-frequency action sequences vs. full 23 sequences, measuring impact on "Request" and "Clarify" action F1
  3. **Validate graph construction**: Sample 100 new API pairs from RapidAPI not in original graph, manually verify edge validity, compare to automated evaluation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dialogue state tracking accuracy be maintained as conversation length increases in multi-turn tool-augmented dialogues?
- Basis in paper: [explicit] The authors observe that "accuracy decreases as the number of turns increases" and that "dialogue state tracking over multiple turns in real-world settings remains a challenging task."
- Why unresolved: Current models accumulate errors across turns, with TD-Llama dropping from ~90% to ~70% accuracy as turns increase from 2 to 10 in the w/o GT setting.
- What evidence would resolve it: Development of methods that maintain consistent DST accuracy across increasing turn counts, evaluated on dialogues with 10+ turns.

### Open Question 2
- Question: What architectural or training interventions can prevent LLMs from prematurely generating responses instead of first requesting necessary information or asking clarifying questions?
- Basis in paper: [explicit] The authors note GPT models "show relatively low scores for predicting actions like 'Request', 'Clarify', and 'Suggest'" and "often rush to provide answers without collecting further information."
- Why unresolved: This tendency causes models to hallucinate or provide incomplete answers rather than systematically gather required API parameters.
- What evidence would resolve it: Models achieving >80% F1 on "Request" and "Clarify" action prediction while maintaining response quality.

### Open Question 3
- Question: How can smaller language models be made more faithful to API call outputs in tool-augmented dialogue settings?
- Basis in paper: [explicit] The authors state that "small language models are vulnerable to hallucination" with TD-Llama achieving only 88.4% faithfulness compared to >96% for GPT models, noting "we need better methods for improving the faithfulness of these models."
- Why unresolved: Smaller models may lack the parametric knowledge or reasoning capacity to ground responses in API outputs consistently.
- What evidence would resolve it: Methods enabling 7B-scale models to achieve >95% faithfulness on API-grounded response generation.

### Open Question 4
- Question: Can models trained on ToolDial's two-API scenarios generalize effectively to dialogues requiring three or more chained API calls?
- Basis in paper: [inferred] The paper constructs action sequences assuming "at most two APIs are called in a dialogue," limiting complexity to pairs from the API graph.
- Why unresolved: Real-world scenarios may require deeper API chains to obtain nested dependencies, but the training data and action sequences do not cover these cases.
- What evidence would resolve it: Evaluation of ToolDial-trained models on dialogues with 3+ API chains, measuring DST and action prediction accuracy.

## Limitations

- Limited evaluation scope: The action prediction evaluation may be inflated by ground-truth conditioning in some settings
- Graph construction reliability: The automated edge construction criteria may not generalize well to APIs with different documentation styles
- Action sequence coverage: The 23 predefined action sequences may not fully capture all realistic tool-interaction patterns

## Confidence

**High confidence**:
- The API graph construction methodology and its validation metrics
- The three-task evaluation framework (DST, action prediction, response faithfulness)
- Fine-tuning benefits for smaller Llama models on action prediction tasks

**Medium confidence**:
- The claim that existing benchmarks cannot handle API chaining scenarios
- The assertion that 23 action sequences provide adequate coverage of realistic interactions
- The generalization of fine-tuning benefits to real-world tool-use scenarios

**Low confidence**:
- The paper's claim about covering "edge cases like clarification requests and failure handling"
- The assertion that the dataset significantly advances TALM evaluation - requires external validation

## Next Checks

1. **Evaluate model performance on held-out dialogues with increasing turn counts**: Replicate the DST degradation analysis by plotting accuracy vs. turn number on a held-out test set to confirm the reported 28.3% full-dialogue accuracy for TD-Llama, and assess whether this degradation pattern holds across different API domains.

2. **Analyze action sequence frequency distribution**: Examine the ToolDial dataset to quantify how frequently each of the 23 action sequences occurs, and compare this distribution to real-world tool-use logs (if available) to assess whether the synthetic data captures realistic interaction patterns.

3. **Test fine-tuning generalization across domains**: Evaluate TD-Llama models fine-tuned on ToolDial against benchmarks from different API domains (e.g., API-Bank, ToolNet) to assess whether the action prediction improvements generalize beyond the RapidAPI domains used in ToolDial.