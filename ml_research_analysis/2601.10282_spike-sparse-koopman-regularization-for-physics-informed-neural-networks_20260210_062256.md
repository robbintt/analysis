---
ver: rpa2
title: 'SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks'
arxiv_id: '2601.10282'
source_url: https://arxiv.org/abs/2601.10282
tags:
- koopman
- e-02
- systems
- training
- pinn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPIKE, a framework that improves Physics-Informed
  Neural Networks (PINNs) for solving differential equations by incorporating continuous-time
  Koopman operators. The key innovation is using Koopman theory to regularize PINNs,
  enforcing linear dynamics in a learned observable space while promoting sparse representations
  through L1 regularization.
---

# SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2601.10282
- Source URL: https://arxiv.org/abs/2601.10282
- Authors: Jose Marie Antonio Miñoza
- Reference count: 40
- Primary result: 2-184x improvements in temporal extrapolation for PINNs across 14 dynamical systems

## Executive Summary
SPIKE introduces a Koopman regularization framework for Physics-Informed Neural Networks (PINNs) that improves temporal extrapolation, spatial generalization, and long-term prediction accuracy for solving differential equations. The method enforces linear dynamics in a learned observable space using continuous-time Koopman operators while promoting sparse representations through L1 regularization. Experimental results demonstrate significant improvements across 14 systems including parabolic, hyperbolic, dispersive, and stiff PDEs, as well as chaotic ODEs, with 2-184x gains in temporal extrapolation and up to 5.7x reduction in non-zero generator entries.

## Method Summary
SPIKE builds on PINNs by adding Koopman regularization through a trainable continuous-time generator matrix A that enforces dz/dt = Az in observable space. The observable embedding g(u) combines a polynomial library (degree 2) with learned MLP features to form a 64-dimensional latent space. The total loss includes physics loss, Koopman consistency loss (computed via matrix exponential integration for stability), and L1 sparsity on A. The continuous-time formulation avoids the identity collapse problem of discrete-time Koopman operators, while the matrix exponential provides unconditional stability for stiff systems. Post-hoc coefficient recovery is enabled through the sparse generator structure.

## Key Results
- 2-184x improvements in temporal extrapolation across 14 dynamical systems
- Up to 5.7x reduction in non-zero generator entries through L1 regularization
- <1% error in post-hoc coefficient recovery for 9 PDEs/ODEs
- Unconditional stability for stiff systems using matrix exponential integration
- Enables long-term prediction for chaotic systems (Lorenz) with 184× longer valid prediction time

## Why This Works (Mechanism)

### Mechanism 1
Koopman regularization improves temporal extrapolation by constraining learned dynamics to evolve linearly in observable space. The Koopman loss enforces consistency between ẋ=Ax and the PINN's learned solution trajectory, preventing the network from fitting spurious high-frequency components that satisfy the PDE locally but extrapolate poorly. The OOD bound shows extrapolation error scales with Koopman consistency error ε_K and spectral radius ρ_0.

### Mechanism 2
Continuous-time generator formulation avoids the "identity collapse" problem inherent in discrete-time Koopman operators. Discrete K = e^{AΔt} approaches identity as Δt→0, making off-diagonal entries vanish. The continuous generator A remains Δt-independent, with A_ij directly encoding observable interaction rates. L1 regularization on A thus promotes meaningful sparsity rather than artifact suppression.

### Mechanism 3
L1 sparsity on the generator matrix yields parsimonious, interpretable dynamics representations. Proximal gradient descent with soft-thresholding sets entries with |A_ij| < λ to zero. For polynomial observables ψ = [1, u, u², ...], non-zero entries A_ij indicate that ψ_j contributes to dψ_i/dt. This enables post-hoc coefficient recovery via regression on PINN derivatives.

## Foundational Learning

- **Koopman Operator Theory**
  - Why needed here: Provides the theoretical foundation for representing nonlinear dynamics as linear operators in observable space
  - Quick check question: Can you explain why the Koopman operator is always linear even when the underlying dynamics are nonlinear?

- **Physics-Informed Neural Networks (PINNs)**
  - Why needed here: SPIKE builds on PINNs as the base solver
  - Quick check question: How does a PINN compute u_xx without explicit finite differences?

- **Matrix Exponential for Stiff Systems**
  - Why needed here: The EXPM integrator provides unconditional A-stability critical for fourth-order PDEs
  - Quick check question: Why does the matrix exponential remain stable when eigenvalues have large negative real parts?

## Architecture Onboarding

- **Component map:** (x,t) → PINN encoder → u_θ → g(u) = z → A → dz/dt = Az → Koopman loss
- **Critical path:** 1) Forward pass: (x,t) → u_θ → g(u_θ) = z 2) Compute physics residual via autograd: N[u_θ] 3) Compute Koopman loss: compare z(t+Δt) with integrated prediction 4) Backpropagate combined loss; L1 regularization on A
- **Design tradeoffs:** Euler vs. RK4 vs. EXPM integrators (speed vs. accuracy vs. stability); λ_koopman vs. λ_sparse balance (tight Koopman consistency vs. physics fitting); polynomial degree limits representation completeness
- **Failure signatures:** Training collapse on stiff PDEs with Euler integrator → switch to EXPM; high OOD error with positive eigenvalues in A → reduce λ_k; poor coefficient recovery with dense A → increase λ_sparse
- **First 3 experiments:** 1) Heat equation (validate 2x OOD-Time MSE improvement) 2) Burgers equation (test spatial generalization, verify latent correlation with u_xx) 3) Lorenz system (confirm 184× valid prediction time improvement)

## Open Questions the Paper Calls Out

### Open Question 1
What are the theoretical approximation error bounds for neural network observables in finite-dimensional Koopman subspaces? General error bounds for neural network observables remain open. The paper provides existence proofs for finite-dimensional approximations but relies on empirical validation rather than theoretical bounds for the specific neural architectures used.

### Open Question 2
How can the Lipschitz constant L_g of the decoder be empirically estimated to validate the proposed generalization bound? Empirical estimation of L_g remains an open direction. The OOD generalization bound depends on this constant, but it is currently treated as an abstract parameter rather than a measurable quantity.

### Open Question 3
Does augmenting the polynomial library with spatial derivatives improve symbolic recovery for convective terms like u · u_x? The paper notes the latent MLP shows low correlation with u · u_x (0.01 for Burgers) and suggests derivative-augmented libraries as a promising extension. The utility of explicit derivative terms remains untested.

## Limitations

- Assumes underlying dynamics admit approximately finite-dimensional linear representations in observable space, which may not hold for systems with discontinuous or highly irregular dynamics
- Continuous-time formulation with matrix exponential integration is computationally expensive, potentially limiting scalability to high-dimensional problems
- Sparsity assumption may incorrectly suppress relevant terms if true dynamics require dense or non-polynomial representations
- Performance on high-dimensional PDEs (beyond 2D) and real-world noisy data remains untested

## Confidence

- **High confidence:** Temporal extrapolation improvements (2-184x) across 14 systems, generator sparsity reductions (up to 5.7x), and post-hoc coefficient recovery (<1% error)
- **Medium confidence:** Mechanism by which Koopman regularization prevents overfitting and improves generalization, as theoretical bounds depend on assumptions about spectral properties
- **Low confidence:** Claims about computational efficiency and scalability, as EXPM integration adds significant overhead and no comparison with alternative sparse discovery methods is provided

## Next Checks

1. **Spectral radius validation:** Verify that learned generators A for all tested systems have spectral radius ρ(A) < 0, confirming the stability claim by computing eigenvalues for the 14 systems.

2. **Robustness to noise:** Test the framework on synthetic data with increasing noise levels (5-20%) for a representative subset of PDEs (Heat, Burgers, Kuramoto-Sivashinsky) to assess sensitivity to measurement noise.

3. **Scalability benchmark:** Implement a 3D PDE (e.g., 3D heat equation or Navier-Stokes) and measure training time, memory usage, and extrapolation performance compared to standard PINNs to validate practical applicability.