---
ver: rpa2
title: 'Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement
  Learning via Trust Regions'
arxiv_id: '2512.23770'
source_url: https://arxiv.org/abs/2512.23770
tags:
- reward
- cost
- safe
- safety
- sb-trpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SB-TRPO introduces a trust-region method for hard-constrained reinforcement
  learning that dynamically balances cost reduction with reward improvement. By combining
  reward and cost natural policy gradients via a convex combination, the method ensures
  a fixed fraction of optimal cost reduction at each update while using remaining
  capacity for reward improvement.
---

# Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions

## Quick Facts
- **arXiv ID:** 2512.23770
- **Source URL:** https://arxiv.org/abs/2512.23770
- **Reference count:** 40
- **Primary result:** SB-TRPO achieves superior safety-performance balance on Safety Gymnasium compared to state-of-the-art constrained RL methods

## Executive Summary
SB-TRPO introduces a trust-region method for hard-constrained reinforcement learning that dynamically balances cost reduction with reward improvement. By combining reward and cost natural policy gradients via a convex combination, the method ensures a fixed fraction of optimal cost reduction at each update while using remaining capacity for reward improvement. Theoretical guarantees show that every update yields local cost reduction and reward improvement when gradients are suitably aligned. Experiments on Safety Gymnasium tasks demonstrate that SB-TRPO consistently achieves the best balance of safety and task performance compared to state-of-the-art methods, avoiding the over-conservatism seen in CPO-style approaches while maintaining high safety.

## Method Summary
SB-TRPO addresses hard-constrained RL by computing natural policy gradients for both reward and cost objectives, then forming a convex combination weighted by a dynamically computed coefficient μ. The method guarantees a minimum fraction β of optimal cost improvement while maximizing reward improvement within the trust region. Unlike CPO, which switches to pure cost optimization when infeasible, SB-TRPO always considers both objectives, eliminating the need for a separate recovery phase. The algorithm uses conjugate gradient to solve the trust-region subproblems and employs line search to ensure KL and cost constraint satisfaction.

## Key Results
- SB-TRPO achieves superior safety-performance balance compared to CPO, C-TRPO, and PPO-Lag on Safety Gymnasium benchmarks
- The safety bias parameter β=0.75 provides robust performance across all tested tasks
- SB-TRPO eliminates the need for a recovery phase while maintaining theoretical guarantees for local cost reduction and reward improvement
- The method successfully scales to continuous cost spaces when augmented with critics, though Monte Carlo estimation performs well for sparse costs

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Convex Combination of Natural Policy Gradients
Computing update direction as a dynamically weighted combination of reward and cost natural gradients enables simultaneous progress on both objectives. SB-TRPO solves separate trust-region problems for reward (∆r) and cost (∆c) using conjugate gradient, then forms the combined update ∆ = (1-μ)·∆r + μ·∆c where μ is computed via Equation (4). The weight μ is set to the smallest value satisfying the cost reduction constraint, maximizing reward improvement while guaranteeing the safety bias requirement.

### Mechanism 2: Safety Bias β Controls Minimum Cost Reduction Per Step
Requiring only a fraction β of the optimal cost improvement prevents collapse into trivially safe but task-ineffective policies. Define ε = β·(Jc(πold) − c*πold) where c*πold is the minimum achievable cost within the trust region. Setting β < 1 allows the policy to pursue reward improvement while still making guaranteed progress toward safety.

### Mechanism 3: Guaranteed Local Improvement Without Recovery Phase
The update formulation ensures local cost reduction and reward improvement when gradients are suitably aligned, eliminating the need for a separate feasibility recovery phase. Theorem 4.2 proves: (1) if gc ≠ 0, cost strictly decreases; (2) if gr ≠ 0 and ⟨gr,∆c⟩ ≥ 0, reward improves.

## Foundational Learning

- **Concept: Constrained Markov Decision Process (CMDP)**
  - Why needed here: The paper formulates safe RL as a CMDP with zero cost threshold; understanding the difference between reward Jc(π) and cost Jr(π) objectives is essential.
  - Quick check question: If a policy has expected discounted cost Jc(π) = 0.5, is it feasible under the paper's hard-constraint formulation?

- **Concept: Trust Region Methods (TRPO)**
  - Why needed here: SB-TRPO builds on TRPO's KL-constrained policy updates; understanding Dmax_KL(πold || π) ≤ δ and the policy improvement bound is prerequisite.
  - Quick check question: Why does constraining KL divergence help ensure surrogate objectives approximate true policy performance?

- **Concept: Natural Policy Gradient / Fisher Information Matrix**
  - Why needed here: The algorithm computes ∆r and ∆c via conjugate gradient on the natural gradient F⁻¹g; understanding why this accounts for policy geometry is critical.
  - Quick check question: How does the Fisher information matrix relate to the curvature of the KL divergence in parameter space?

## Architecture Onboarding

- **Component map:** Rollout Collection -> Gradient Estimation -> Conjugate Gradient Subproblems -> μ Computation -> Line Search -> Parameter Update

- **Critical path:** The μ computation (step 4) is the algorithmic core—if this value is incorrect, the safety bias guarantee fails. The line search (step 5) is the safety gate ensuring the quadratic approximation remains valid.

- **Design tradeoffs:**
  - **β selection:** Higher β prioritizes safety but may slow reward learning; β = 0.75 worked well across all tasks in experiments
  - **Critic vs Monte Carlo:** Paper ablates critics; critics accelerate reward learning but increase cost due to sparse cost signal issues
  - **KL constraint δ:** Standard TRPO value 0.01 used; larger values allow bigger steps but risk surrogate inaccuracy

- **Failure signatures:**
  - Cost oscillation without convergence: May indicate β too low or gradient estimation noise
  - Reward stuck near zero with low cost: Over-conservatism; try reducing β
  - NaN in μ computation: Check for division by zero when ⟨gc,∆r⟩ ≈ ⟨gc,∆c⟩; ensure κ is applied
  - Line search fails to find valid η: Trust region may be too tight; consider increasing δ or checking gradient quality

- **First 3 experiments:**
  1. **Reproduce Car Circle baseline:** Run SB-TRPO with β = 0.75, δ = 0.01 for 1000 epochs; verify training curves match Figure 2 (reward ~7-8, cost near 0)
  2. **Ablate β on Point Button:** Test β ∈ {0.6, 0.7, 0.75, 0.8, 0.9}; confirm Pareto frontier in reward vs safety probability matches Figure 3
  3. **Compare gradient alignment:** Log angles between ∆ and gr/gc during training; verify angles remain < 90° with reward gradient as in Figure 4

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SB-TRPO be extended to handle CMDPs with positive cost thresholds (soft constraints) while preserving its dynamic balancing properties?
  - Basis in paper: "SB-TRPO targets hard constraints: it is not directly applicable to CMDPs with positive cost thresholds. We leave hybrid methods replacing conventional recovery phases (e.g. in CPO/C-TRPO) with our (Update 3) to future work."
  - Why unresolved: The current formulation assumes zero-cost thresholds, and the convex combination mechanism is designed specifically for the hard-constraint regime.

- **Open Question 2:** Can stronger theoretical guarantees for almost-sure safety be derived for SB-TRPO on highly challenging tasks?
  - Basis in paper: "while our method achieves strong results in Safety Gymnasium, it does not guarantee almost-sure safety on the most challenging tasks."
  - Why unresolved: Finite sample estimates, sparse cost signals, and the approximate nature of updates cause deviations from idealized theoretical behavior.

- **Open Question 3:** How does SB-TRPO perform under gradient estimation errors and finite-sample conditions compared to the exact-gradient theoretical analysis?
  - Basis in paper: "As with other policy optimisation methods... our performance guarantee (Theorem 4.2) assumes exact gradients and holds only approximately with estimates."
  - Why unresolved: The paper notes deviations from theoretical behavior due to "finite sample estimates, quadratic approximations, and sparse cost signals" but does not quantify the gap between theory and practice.

## Limitations
- Theoretical guarantees apply only locally within the trust region; global convergence to optimal safe policy is not established
- The safety bias β requires manual tuning per task, though β=0.75 shows robustness across domains
- Algorithm assumes accurate gradient estimates via Monte Carlo sampling, which may be challenging in high-variance environments
- Fisher information matrix approximation may degrade for highly non-linear policies or when KL divergence exceeds trust region validity

## Confidence
- **High Confidence:** The local improvement guarantees (Theorem 4.2) and the mechanism of dynamic convex combination are mathematically rigorous and well-supported by theoretical analysis. The empirical demonstration that SB-TRPO outperforms CPO in balancing safety and reward is clearly shown in Figure 2 and Table 1.
- **Medium Confidence:** The claim that β=0.75 provides good performance across all tasks relies on limited hyperparameter sweeps. The assertion that the Fisher information matrix accurately approximates KL divergence locally is standard in TRPO literature but not specifically validated for the safety-constrained setting.
- **Low Confidence:** The scalability of the method to continuous cost spaces with dense signals is demonstrated only through ablation with critics, and the computational overhead of maintaining separate value networks is not fully characterized.

## Next Checks
1. **Ablation of Fisher matrix accuracy:** Systematically test how SB-TRPO performance degrades as KL divergence exceeds δ, comparing against theoretical bounds on surrogate approximation error.

2. **Cross-task β generalization:** Conduct a systematic grid search over β values across all Safety Gymnasium tasks to map the Pareto frontier more comprehensively and identify if β=0.75 is truly optimal or task-dependent.

3. **Gradient variance analysis:** Measure the variance of gr and gc estimates across rollouts and correlate with training stability metrics (cost oscillations, reward plateaus) to quantify the impact of Monte Carlo estimation error.