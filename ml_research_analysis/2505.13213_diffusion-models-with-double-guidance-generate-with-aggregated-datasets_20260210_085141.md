---
ver: rpa2
title: 'Diffusion Models with Double Guidance: Generate with aggregated datasets'
arxiv_id: '2505.13213'
source_url: https://arxiv.org/abs/2505.13213
tags:
- diffusion
- guidance
- conditional
- dmdg
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses conditional generative modeling on aggregated
  datasets with block-wise missing conditions, where different datasets contain distinct
  sets of attributes. The proposed Diffusion Model with Double Guidance (DMDG) enables
  joint conditional generation without requiring samples containing all conditions
  simultaneously.
---

# Diffusion Models with Double Guidance: Generate with aggregated datasets

## Quick Facts
- arXiv ID: 2505.13213
- Source URL: https://arxiv.org/abs/2505.13213
- Reference count: 40
- Key outcome: DMDG enables joint conditional generation from datasets with block-wise missing conditions, achieving 76% success rate for 6-property molecular generation

## Executive Summary
This paper addresses the challenge of conditional generative modeling when different datasets contain distinct sets of attributes with no samples containing all conditions simultaneously. The proposed Diffusion Model with Double Guidance (DMDG) decomposes the conditional score function using conditional independence assumptions and introduces separate guidance terms for each condition. By leveraging Tweedie projection and carefully designed regressor inputs, DMDG can generate samples satisfying multiple conditions without requiring joint training data. The method is theoretically grounded with error bounds for guidance effectiveness and validated across synthetic data, molecular datasets (GEOM-DRUG, ZINC250k), and image inpainting tasks.

## Method Summary
DMDG addresses conditional generative modeling on aggregated datasets with block-wise missing conditions by decomposing the conditional score function using conditional independence assumptions. The method assumes conditions arise from deterministic functions of X₀ with additive independent Gaussian noise, enabling factorization of the joint conditional density. Separate guidance terms are introduced for each condition, with the second guidance term conditioned on the first condition through Tweedie projection estimates. The method uses pre-trained denoisers and regressors to approximate conditional expectations, with guidance scales theoretically optimal at O(1/σ²). Sampling is performed via reverse diffusion with gradient injection, and the framework can be extended to multiple conditions.

## Key Results
- Synthetic data experiments show DMDG achieves lower 2-Wasserstein distance to true conditional distribution compared to baselines
- Molecular generation on GEOM-DRUG and ZINC250k datasets achieves 76% success rate in generating molecules satisfying six properties simultaneously
- Image inpainting results demonstrate improved perceptual quality and condition alignment compared to existing methods
- Theoretical analysis establishes factors affecting guidance effectiveness including gradient magnitudes, prediction errors, and guidance scale choices

## Why This Works (Mechanism)

### Mechanism 1
The conditional score ∇_X_t log p(X_t|C₁,C₂) can be decomposed into separate guidance terms without joint samples of (C₁,C₂) when conditions are conditionally independent given X₀. By assuming C₁ ⊥⊥ C₂ | X₀ (from the function-mapping model in Eq. 1), the joint conditional density factors as p(C₂|X₀,C₁) ≈ p(C₂|E[X₀|X_t,C₁]) and p(C₁|X₀) ≈ p(C₁|E[X₀|X_t]). These are approximated via Tweedie projection using neural denoisers nn_θ. The core assumption is that conditions arise from deterministic functions of X₀ with additive independent Gaussian noise, enforcing C₁ ⊥⊥ C₂ | X₀. If C₁ and C₂ share unobserved confounders beyond X₀, or if the function-mapping assumption fails, the decomposition introduces systematic bias.

### Mechanism 2
Using f̂₂(X₀|ₜ,C₁) instead of f̂₂(X₀|ₜ) as the regressor input preserves the dependency between C₁ and C₂ during guidance. The second guidance term -λ₂∇_X_t‖C₂ - f̂₂(X₀|ₜ,C₁)‖² conditions on C₁ through the Tweedie estimate X₀|ₜ,C₁ := nn_θ(X_t, t, C₁). This contrasts with prior "independent double guidance" (DMIDG) which uses f̂₂(X₀|ₜ), implicitly assuming C₁ ⊥⊥ C₂ | X_t—which does NOT hold. The pre-trained conditional denoiser nn_θ(X_t, t, C₁) provides a faithful estimate of E[X₀|X_t,C₁]. If nn_θ(X_t, t, C₁) is poorly trained or the conditional distribution is multi-modal, the posterior mean estimate degrades, propagating error to f̂₂.

### Mechanism 3
Guidance scale selection should match the inverse noise variance of the condition mapping: λ ~ O(1/σ²). Theorem 1 bounds the density approximation error with a term U(σ² - 1/2λ)². When the guidance-assigned variance 1/2λ mismatches the true noise variance σ², additional error accumulates. The gradient magnitude terms also contribute but are generally bounded. The regressors f̂₁, f̂₂ are L-layer ReLU networks with bounded weight matrix norms M. If regressor prediction error ‖f - f̂‖² is large (e.g., due to distribution shift between D^(1) and D^(2) training data), Theorem 4 bounds may not suffice.

## Foundational Learning

- Concept: Score functions and Tweedie projection
  - Why needed here: DMDG relies on approximating E[X₀|X_t] via nn_θ(X_t,t,∅) using the Tweedie formula X₀|ₜ = X_t + t²∇ log p(X_t). Understanding this is essential for interpreting the guidance terms.
  - Quick check question: Given noisy X_t = X₀ + tε, how does the Tweedie estimate recover X₀?

- Concept: Conditional independence vs marginal independence
  - Why needed here: The method hinges on C₁ ⊥⊥ C₂ | X₀ being true while C₁ ⊥⊥ C₂ | X_t is FALSE. Confusing these leads to incorrect application of DMIDG.
  - Quick check question: In Fig. 4, why does I(C₁;C₂|X_t) increase with t while I(C₁;C₂|X₀|ₜ,C₁) stays near zero?

- Concept: Classifier Guidance vs Classifier-Free Guidance trade-offs
  - Why needed here: DMDG uses CG-style gradients; DMHG hybridizes CFG for C₁ with CG for C₂. Choosing between them depends on condition complexity and sample quality requirements.
  - Quick check question: When should you prefer DMHG over DMDG for a high-dimensional semantic condition?

## Architecture Onboarding

- Component map:
  - Base diffusion model (EDM) -> nn_θ(X_t, t, C₁) with masking probability p_non
  - Regressor f̂₁ -> Trained on D^(1) = {X₀^(1), C₁^(1)} to predict C₁ from clean X₀
  - Regressor f̂₂ -> Trained on D^(2) = {X₀^(2), C₂^(2)} to predict C₂ from X₀ AND C₁ as auxiliary input
  - Guidance integration -> Per-step gradient injection into reverse ODE

- Critical path:
  1. Verify conditional independence C₁ ⊥⊥ C₂ | X₀ holds for your domain (check via CMI or domain knowledge)
  2. Train base EDM on pooled X₀ data (both datasets), with conditional training on D^(1) for C₁
  3. Train f̂₁ on D^(1) and f̂₂ on D^(2) (note: f̂₂ must accept C₁ as input, so synthesize C₁ labels for D^(2) via f̂₁ if needed, or restructure)
  4. Tune λ₁, λ₂ starting from ~1/σ² estimates

- Design tradeoffs:
  - DMDG vs DMHG: Use DMDG when C₁ is low-dimensional and well-calibrated; use DMHG for complex/semantic C₁ (e.g., class labels, text)
  - Regressor architecture: Deeper networks improve prediction but increase ∇z norm M, amplifying guidance error per Theorem 1
  - Sampling steps: EDM uses 18 steps by default; fewer steps increase Tweedie estimate error

- Failure signatures:
  - Generated samples violate both conditions: Check if f̂₁, f̂₂ generalize across distribution shift (Theorem 4); verify training data overlap
  - High variance in conditional distribution: λ may be too low; increase guidance scale
  - Samples collapse to single mode: λ may be too high; reduce or check for regressor overconfidence
  - NaN during sampling: For multi-condition extensions (DMTG), only activate third guidance when t ≤ σ_data to avoid numerical explosion

- First 3 experiments:
  1. Synthetic validation: Replicate Setting II (Gaussian mixture with correlated C₁,C₂) to verify your implementation matches Table 2 W₂ distances before applying to real data
  2. Ablation on f̂₂ input: Compare f̂₂(X₀|ₜ,C₁) vs f̂₂(X₀|ₜ) on your domain to quantify the dependency-aware advantage
  3. Guidance scale sweep: For a fixed target condition pair, grid search λ₁, λ₂ in [0.5/σ², 5/σ²] and plot condition satisfaction vs sample quality (e.g., FID or domain-specific metric)

## Open Questions the Paper Calls Out

### Open Question 1
How does the magnitude of distribution shift between aggregated datasets (e.g., P(X₀) vs P'(X₀)) affect the accuracy of joint conditional generation, and can the theoretical bounds in Theorem 4 be tightened for practical guidance scale selection? The bound is asymptotic and involves constants not estimated from data, so practical calibration remains unclear. Controlled experiments varying P(X₀) vs P'(X₀) and reporting both W₂ distances and calibrated confidence intervals for the guidance scale would resolve this.

### Open Question 2
Can the conditional independence assumption (C₁ ⊥⊥ C₂ | X₀) be relaxed or tested empirically without requiring joint samples, and how robust is DMDG when this assumption is violated? Violating CI may bias the score decomposition, but the magnitude of this bias is unknown. Synthetic experiments where CI is progressively violated and comparing W₂ distances between DMDG, DMIDG, and oracle joint training would provide evidence.

### Open Question 3
Does DMDG scale effectively to high-dimensional condition spaces (e.g., text or multi-property molecular generation with >6 conditions), and how does computational cost grow with the number of conditions? Triple guidance requires additional regressors and chained posterior estimates, which may compound errors or become computationally prohibitive. Experiments on datasets with >3 condition blocks measuring both generation quality and wall-clock time would resolve this.

### Open Question 4
Can the guidance scale parameters (λ₁, λ₂) be automatically adapted during sampling rather than manually tuned, potentially using the theoretical insights from Theorem 1? No adaptive mechanism is proposed; manual tuning may not generalize across datasets or tasks. Implementing adaptive λ based on runtime estimates of the terms in Theorem 1 and comparing to fixed-λ baselines would provide evidence.

## Limitations

- The method critically relies on the conditional independence assumption C₁ ⊥⊥ C₂ | X₀, which may not hold in many real-world datasets with shared confounders
- Performance depends heavily on the quality of pre-trained denoisers and regressors, which can fail with multi-modal conditional distributions or distribution shifts
- Guidance scale selection remains manual despite theoretical bounds, limiting practical deployment across diverse condition types
- The method's performance on high-dimensional semantic conditions (e.g., text-to-image) is not demonstrated

## Confidence

**High confidence**: The theoretical analysis of guidance error bounds (Theorem 1) and the experimental validation on synthetic data are rigorous and reproducible. The mechanism of using dependency-aware regressors (f̂₂(X₀|ₜ,C₁)) is clearly demonstrated in Table 2's comparison with DMIDG.

**Medium confidence**: The molecular generation results show strong performance, but the evaluation metrics (success rate for 6 properties) depend on the specific property definitions and thresholds. The image inpainting results are promising but use a different guidance mechanism (DMHG) that may not generalize to all condition types.

**Low confidence**: The guidance scale selection theory (λ ~ O(1/σ²)) is supported by Theorem 1 but lacks empirical validation across diverse condition types. The method's performance on high-dimensional semantic conditions (e.g., text-to-image) is not demonstrated.

## Next Checks

1. **Conditional Independence Verification**: Compute I(C₁;C₂|X_t) across different t values for your specific datasets. If this mutual information increases with t, the dependency-aware DMDG is necessary; if it stays near zero, simpler approaches may suffice.

2. **Regressor Generalization Test**: Hold out condition pairs that appear in neither training dataset and evaluate whether DMDG can generate valid samples satisfying both conditions. This tests whether the regressors f̂₁, f̂₂ generalize across the distribution shift.

3. **Guidance Scale Sensitivity Analysis**: Systematically vary λ₁, λ₂ around the theoretical O(1/σ²) estimates and measure the trade-off between condition satisfaction rate and sample quality (FID, domain-specific metrics). Identify the optimal operating point for your specific use case.