---
ver: rpa2
title: 'CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion
  in Text'
arxiv_id: '2507.07539'
source_url: https://arxiv.org/abs/2507.07539
tags:
- subjective
- subjectivity
- objective
- sentence
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a competitive approach to multilingual subjectivity
  detection using large language models (LLMs) with few-shot prompting. We participated
  in Task 1: Subjectivity of the CheckThat!'
---

# CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text

## Quick Facts
- arXiv ID: 2507.07539
- Source URL: https://arxiv.org/abs/2507.07539
- Reference count: 36
- Primary result: LLM few-shot prompting achieved first place in Arabic and Polish subjectivity detection tasks

## Executive Summary
This paper presents a competitive approach to multilingual subjectivity detection using large language models (LLMs) with few-shot prompting. We participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation campaign. We show that LLMs, when paired with carefully designed prompts, can match or outperform fine-tuned smaller language models (SLMs), particularly in noisy or low-quality data settings. Despite experimenting with advanced prompt engineering techniques—such as debating LLMs and various example selection strategies—we found limited benefit beyond well-crafted standard few-shot prompts. Our system achieved top rankings across multiple languages in the CheckThat! 2025 subjectivity detection task, including first place in Arabic and Polish, and top-four finishes in Italian, English, German, and multilingual tracks. Notably, our method proved especially robust on the Arabic dataset, likely due to its resilience to annotation inconsistencies. These findings highlight the effectiveness and adaptability of LLM-based few-shot learning for multilingual sentiment tasks, offering a strong alternative to traditional fine-tuning, particularly when labeled data is scarce or inconsistent.

## Method Summary
The approach relies on LLM few-shot prompting with extended prompts derived from annotation guidelines. For each target sentence, we construct prompts containing: (1) an extended prompt template encoding task-specific linguistic criteria from annotation guidelines, (2) 6 class-balanced in-context examples (3 objective, 3 subjective) randomly selected from training data, and (3) the target sentence to classify. We tested various models including GPT-4o-mini, LLaMA 70B, Qwen 72B, and RoBERTa-Base, with final predictions obtained through ensemble voting. The system was evaluated across 8 languages using macro F1-score as the primary metric.

## Key Results
- First place in Arabic subjectivity detection (macro F1: 0.6884)
- First place in Polish subjectivity detection
- Top-four finishes in Italian, English, German, and multilingual tracks
- Random example selection outperformed similarity-based selection (0.76 vs 0.70 macro F1)
- Extended prompts significantly improved performance over basic prompts (+0.12 macro F1)

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Derived Task Specification
- Claim: Detailed prompts derived from annotation guidelines substantially improve LLM classification performance over generic prompts.
- Mechanism: Extended prompts encode domain-specific decision criteria (handling of quotes, emotions, intensifiers, speculations), constraining the model's interpretation space and aligning its inductive bias with the target annotation scheme.
- Core assumption: LLMs can transfer explicit linguistic criteria from prompt instructions to classification decisions without parameter updates.
- Evidence anchors:
  - [abstract] "LLMs, when paired with carefully designed prompts, can match or outperform fine-tuned smaller language models"
  - [section] Table 3 shows basic prompt yields 0.54 macro F1 with 0.32 subjective precision; extended prompt achieves 0.66 macro F1 (+0.12), subjective precision improves to 0.46 (+0.14)
  - [corpus] XplaiNLP at CheckThat! 2025 similarly reports success combining fine-tuned transformers with prompt-based inference
- Break condition: When annotation guidelines are internally inconsistent or ambiguous, prompt encoding may not resolve classification boundaries.

### Mechanism 2: In-Context Learning as Implicit Regularization
- Claim: Few-shot examples provide task grounding that reduces over-prediction bias without requiring gradient-based learning.
- Mechanism: Labeled examples in the prompt establish class boundaries and label semantics, letting the model infer classification criteria through pattern matching rather than memorizing noisy training labels.
- Core assumption: The model's attention mechanism can extract and apply classification patterns from demonstration examples to new inputs.
- Evidence anchors:
  - [abstract] "The approach relies on carefully designed prompts, optionally supplemented with in-context examples"
  - [section] Table 3: Adding 6-shot random examples to extended prompt improves macro F1 from 0.66 to 0.76; subjective precision rises from 0.46 to 0.69
  - [corpus] Limited direct corpus evidence on ICL mechanisms for subjectivity; this remains an active research area
- Break condition: When examples are unrepresentative or contradict annotation guidelines, few-shot learning can introduce prompt-induced bias.

### Mechanism 3: Noise Resilience Through Prompt-Based Inference
- Claim: LLMs with few-shot prompting are more robust to annotation inconsistencies than fine-tuned models that directly learn from noisy labels.
- Mechanism: Fine-tuned models internalize training label noise through gradient updates; prompt-based approaches condition on examples without weight updates, limiting propagation of annotation errors to model parameters.
- Core assumption: In-context learning depends less on exact label quality than supervised fine-tuning does.
- Evidence anchors:
  - [abstract] "method was particularly resilient on the Arabic dataset, likely due to its ability to handle annotation inconsistencies"
  - [section] Section 6: Arabic dataset showed annotation inconsistencies; authors achieved 0.6884 macro F1 (1st place) vs. baseline 0.5133. Manual reannotation led to F1=0.65 improvement, confirming label quality issues
  - [corpus] "Bias in, Bias out: Annotation Bias in Multilingual Large Language Models" addresses annotation bias challenges in multilingual settings
- Break condition: When noise is systematic rather than random (e.g., consistent mislabeling of specific linguistic patterns), even prompt-based approaches may inherit biases.

## Foundational Learning

- Concept: **Macro F1-score for imbalanced classification**
  - Why needed here: Subjectivity datasets are class-imbalanced (more objective than subjective sentences); macro F1 equally weights both classes, preventing inflated metrics from majority-class accuracy.
  - Quick check question: If a model achieves 90% accuracy by always predicting "objective" on a 90% objective dataset, what would its macro F1 be?

- Concept: **Prompt engineering vs. fine-tuning trade-offs**
  - Why needed here: Understanding when prompt-based approaches are preferable to fine-tuning affects system design choices, especially with limited or noisy data.
  - Quick check question: Why might a model that performs worse on clean data outperform on noisy data?

- Concept: **Example selection strategies for in-context learning**
  - Why needed here: The paper tests similarity-based, dissimilarity-based, and random selection—understanding these helps optimize few-shot prompt construction.
  - Quick check question: Why might semantically similar examples not always be the best choice for few-shot classification?

## Architecture Onboarding

- Component map: Input Sentence → Embedding Layer (text-embedding-3-small) → Example Retriever → Few-Shot Prompt Constructor ← Annotation Guidelines (Extended Prompt Template) → LLM (GPT-4o-mini / LLaMA 70B / Qwen 72B) → Classification Output (OBJ/SUBJ) → Optional: Multi-Agent Debater → Judge LLM → Final Classification → Ensemble Aggregator (voting across models)

- Critical path:
  1. Start with extended prompt template encoding annotation guidelines
  2. Retrieve 6 class-balanced examples (random selection performed best)
  3. Construct prompt: guidelines + examples + target sentence
  4. Query LLM for classification
  5. For ensemble: aggregate predictions from multiple model families

- Design tradeoffs:
  - Extended prompts improve precision but require more tokens; balance detail vs. cost
  - Random example selection outperformed similarity-based; suggests diversity matters more than semantic matching for this task
  - Debate prompting (+0.01 macro F1 over standard few-shot) adds complexity and latency for marginal gains
  - Ensembling heterogeneous models (transformer + LLMs) yields best performance but increases inference cost

- Failure signatures:
  - Low subjective precision with high recall → prompt under-specifies task (over-predicts subjectivity)
  - Performance plateaus across prompting strategies → check dataset annotation quality
  - Cross-lingual performance drops → verify prompt translation quality and embedding model language coverage

- First 3 experiments:
  1. **Baseline comparison**: Test basic prompt vs. extended prompt on development set; expect +0.10-0.15 macro F1 improvement
  2. **Few-shot scaling**: Compare 0-shot, 6-shot, 12-shot with random examples; expect plateau around 6-shot
  3. **Cross-validation on noisy data**: Train fine-tuned RoBERTa on the same data; if LLM outperforms by >0.05 F1, annotation quality may be the bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does random example selection outperform semantic similarity-based selection for few-shot subjectivity detection, contradicting prior work on in-context learning?
- Basis in paper: [explicit] The authors state: "These results contrast with earlier findings highlighting the benefits of semantically similar examplars for in-context learning" and report random sampling achieving 0.76 Macro F1 vs. 0.70 for similarity-based selection.
- Why unresolved: The paper documents the phenomenon but offers no mechanistic explanation for why dissimilar or random examples help more than similar ones in this specific task.
- What evidence would resolve it: Ablation studies varying task type (subjectivity vs. other classification tasks), example diversity metrics, and analysis of whether subjectivity's contextual nature favors diverse demonstrations.

### Open Question 2
- Question: What specific properties make LLMs with few-shot prompting more robust to annotation noise than fine-tuned SLMs?
- Basis in paper: [explicit] The authors attribute their Arabic success to resilience against "annotation inconsistencies" and cite prior work suggesting in-context learning is "relatively independent of the exact label quality," but do not test this mechanism directly.
- Why unresolved: The claim is inferred from performance differences rather than demonstrated through controlled experiments with synthetic noise.
- What evidence would resolve it: Systematic experiments injecting controlled label noise at varying rates into training/few-shot data, comparing LLM and SLM degradation curves.

### Open Question 3
- Question: Under what conditions do multi-agent debating strategies provide meaningful improvements over standard few-shot prompting?
- Basis in paper: [explicit] The authors note debating LLMs "did not lead to substantial improvements over standard few-shot prompting" despite achieving the best single macro F1 (0.77), leaving unclear when the added complexity is justified.
- Why unresolved: Limited experimentation with debate configurations and no analysis of which sentence types benefit from adversarial reasoning.
- What evidence would resolve it: Error analysis comparing debate vs. standard prompting across linguistic features (irony, implicit bias, quoted speech) and cost-benefit analysis accounting for computational overhead.

## Limitations
- Cross-lingual generalizability remains limited due to translation dependencies for prompt templates
- Annotation quality impact is under-quantified without controlled noise experiments
- Prompt construction cost vs benefit trade-off is unclear due to lack of engineering effort quantification

## Confidence
- High confidence - LLM few-shot performance claims, basic vs extended prompt comparisons, random example selection superiority, and overall methodology description
- Medium confidence - The noise-resilience mechanism, debate prompting marginal improvements, and ensemble voting mechanisms
- Low confidence - Cross-lingual prompt transfer quality, exact LLM parameter settings, and the causal relationship between annotation inconsistency and prompt-based resilience

## Next Checks
1. **Controlled annotation noise experiment** - Create three test sets from the same language: (a) original annotations, (b) systematically corrupted labels (10% random flipping), and (c) consistently annotated subset. Compare LLM few-shot performance across all three to directly measure noise resilience.

2. **Prompt engineering ablation study** - Systematically remove components from the extended prompt (guidelines, examples, formatting) to quantify the marginal contribution of each element. This would reveal whether the detailed guidelines justify their token cost versus simpler prompt variations.

3. **Cross-lingual prompt transfer evaluation** - Translate the English-extended prompt into all target languages, then back-translate to English to measure semantic drift. Additionally, test whether a prompt constructed directly from native-language annotation guidelines (when available) outperforms the translated version.