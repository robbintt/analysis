---
ver: rpa2
title: Generating Fair Consensus Statements with Social Choice on Token-Level MDPs
arxiv_id: '2510.14106'
source_url: https://arxiv.org/abs/2510.14106
tags:
- consensus
- agent
- statements
- policy
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal token-level Markov Decision Process
  (MDP) framework for generating consensus statements that ensures provable fairness
  guarantees. The key innovation is modeling each agent's opinion as a personalized
  language model policy, which implicitly defines optimal Q-functions and enables
  stepwise reward computation without explicit value functions.
---

# Generating Fair Consensus Statements with Social Choice on Token-Level MDPs

## Quick Facts
- **arXiv ID:** 2510.14106
- **Source URL:** https://arxiv.org/abs/2510.14106
- **Reference count:** 40
- **Primary result:** Introduces a token-level MDP framework that generates consensus statements with provable fairness guarantees through social choice theory integration

## Executive Summary
This paper presents a formal token-level Markov Decision Process (MDP) framework for generating consensus statements from multiple agents' opinions. The key innovation is modeling each agent's opinion as a personalized language model policy, which implicitly defines optimal Q-functions and enables stepwise reward computation without explicit value functions. The framework integrates social choice theory by developing two approaches: a stochastic policy guaranteed to lie in the ex-ante core (achieving proportional fairness through Nash Welfare maximization) and a deterministic search algorithm that maximizes egalitarian welfare to produce single consensus statements. Empirical results demonstrate that search algorithms guided by egalitarian welfare objectives generate consensus statements with significantly improved worst-case agent alignment compared to baseline methods including the Habermas Machine.

## Method Summary
The method formalizes text generation as a token-level MDP where each prefix is a state and token selection is an action. Agent rewards are computed from log-likelihoods of their personalized language model policies. For stochastic consensus, a Nash Welfare-maximizing lottery is solved via Frank-Wolfe optimization, then converted to a policy guaranteed to lie in the ex-ante core. For deterministic consensus, beam search or finite lookahead algorithms optimize egalitarian welfare. The framework uses token-level rewards derived from agent policies, enabling stepwise reward computation without training explicit value functions. Chunking reduces computational complexity but may void fairness guarantees.

## Key Results
- Search algorithms guided by egalitarian welfare objectives generate consensus statements with significantly improved worst-case agent alignment compared to baseline methods
- Stochastic policy derived from Nash Welfare maximization lies in the ex-ante core, guaranteeing proportional fairness across all coalitions
- Beam search with width=4 achieves lowest Egalitarian Perplexity (2.87 avg), outperforming Best-of-N and Prompted Habermas Machine baselines
- Credit assignment experiments show localized reward signals at semantically relevant tokens when comparing aligned vs. misaligned sequences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token-level rewards can be derived from agent policies without training explicit value functions.
- **Mechanism:** Each agent's policy πᵢ assigns likelihoods πᵢ(s,a) to token choices. Following Rafailov et al. (2024), policies implicitly define optimal Q-functions, enabling rewards r_log(s,a) = βlogπᵢ(s,a) or r_prob(s,a) = πᵢ(s,a) at each generation step.
- **Core assumption:** The implicit Q-function relationship holds for instruction-tuned LLMs prompted with agent opinions (not just DPO-fine-tuned models).
- **Evidence anchors:**
  - [abstract] "...policies implicitly define optimal Q-functions, providing a principled way to quantify rewards at each generation step without a value function"
  - [Section 6.2] Credit assignment experiments show localized reward signals at semantically relevant tokens when comparing aligned vs. misaligned sequences
  - [corpus] Weak direct support; related work on MDPs in RL (e.g., "Generalization in Mon-MDPs") does not address policy-derived rewards.
- **Break condition:** If prompted policies do not exhibit localized credit assignment, token-level rewards become noisy and search guidance degrades.

### Mechanism 2
- **Claim:** A stochastic policy derived from a Nash Welfare-maximizing lottery lies in the ex-ante core, guaranteeing proportional fairness.
- **Mechanism:** First, solve for p* maximizing ∏ᵢ Uᵢᵖʳᵒᵇ(p) over the simplex (convex program). Then derive policy π*(a|s) = P*(C(s,a))/P*(C(s)), the conditional probability of action a given prefix s under p*. By Theorem 1, executing π* reproduces p*, which is in the core.
- **Core assumption:** The set of candidate statements C is finite and tractable (or chunked); agents have non-negative expected utilities.
- **Evidence anchors:**
  - [Section 4] Definition 2 and Corollary 1 formally establish core membership
  - [Section 4.2, Figure 2] Empirical blocking test shows π* maintains α*=1 across polarization levels while uniform and utilitarian policies become blockable
  - [corpus] No direct corpus evidence for core stability in text generation; related social choice work ("Generative Social Choice: The Next Generation") addresses proportional representation but not core guarantees.
- **Break condition:** Chunking can arbitrarily degrade Nash Welfare relative to the unchunked optimum (Theorem 2), breaking fairness guarantees if high-utility statements are pruned.

### Mechanism 3
- **Claim:** Search algorithms optimizing egalitarian welfare produce single consensus statements with improved worst-case agent alignment.
- **Mechanism:** Finite lookahead explores depth-d paths from each state, selecting actions that maximize minᵢ Uˡᵒᵍ(X_prefix ∥ P). Beam search maintains top-w partial paths by EW score, pruning low-scoring hypotheses. Both avoid hedging toward generic text by committing to paths that maximize minimum utility.
- **Core assumption:** EW computed over partial sequences correlates with EW of complete statements; myopic pruning does not systematically exclude optimal paths.
- **Evidence anchors:**
  - [Section 6.3, Figure 4] Beam search achieves lowest Egalitarian Perplexity (2.87 avg), finite lookahead (4.18) outperforms Best-of-N (6.35) and Prompted Habermas Machine (9.67)
  - [Section E.2.3] Best-of-N with large N can match or exceed search methods, suggesting tradeoffs between compute and EW optimization
  - [corpus] Guided decoding methods (PPO-MCTS, VAS) use value networks for steering; this approach differs by using policy-derived rewards and explicit social choice objectives.
- **Break condition:** Beam width too narrow or lookahead too shallow causes premature pruning; width too large increases myopic errors from partial EPPL scores (Section E.2.2 shows width=8 degrades performance).

## Foundational Learning

- **Concept:** Markov Decision Processes (states, actions, transitions, rewards, policies)
  - **Why needed here:** The entire framework formalizes text generation as a token-level MDP where each prefix is a state and token selection is an action.
  - **Quick check question:** Given a prefix s="The event should" and action a=" feature", what is the next state?

- **Concept:** Social choice fundamentals (Nash Welfare, core stability, egalitarian welfare)
  - **Why needed here:** Fairness guarantees derive from social choice theory—Nash Welfare for proportional fairness (ex-ante core), egalitarian welfare for worst-case alignment.
  - **Quick check question:** Why does maximizing Nash Welfare produce a lottery in the core?

- **Concept:** Language model log-likelihood as preference signal
  - **Why needed here:** Agent rewards are computed from logπᵢ(s,a), requiring understanding of how conditional likelihoods reflect preference alignment.
  - **Quick check question:** If πᵢ("spaghetti" | "morning context") < πᵢ("pancakes" | "morning context"), which token receives higher reward for a morning-preferring agent?

## Architecture Onboarding

- **Component map:**
  1. Agent Policy Module: Prompted LLMs (Llama 3.1 8B Instruct) conditioned on each agent's opinion; outputs πᵢ(s,a)
  2. Reward Computation: Computes r_log(s,a) = βlogπᵢ(s,a) or r_prob(s,a) = πᵢ(s,a) for all agents at each state-action pair
  3. Tree Construction: Expands token tree with branching B and max depth L_max (or chunked macro-actions)
  4. Core Policy Solver: Frank-Wolfe optimization over simplex to find Nash-Welfare-maximizing p*, then derive π*
  5. Search Module: Finite lookahead (depth d) or beam search (width w) optimizing egalitarian welfare
  6. Evaluation: Egalitarian Perplexity (EPPL) = maxᵢ PPLᵢ(X)

- **Critical path:**
  1. Validate credit assignment: confirm prompted policies assign highest reward deltas to semantically relevant tokens
  2. Implement tree expansion with configurable branching/chunking
  3. For stochastic policy: implement convex solver for Nash Welfare, derive conditional policy
  4. For deterministic: implement beam search with EW scoring
  5. Evaluate using EPPL and LLM-judge ranking

- **Design tradeoffs:**
  - **Chunking:** Reduces complexity from B^L to B^(L/c) but voids fairness guarantees (Theorem 2); treat as anytime algorithm
  - **Beam width:** Width=4 optimal for EPPL in experiments; larger widths cause myopic pruning errors
  - **Reward type:** Log-likelihood for additive utility (single statements); probability for multiplicative utility (Nash Welfare)
  - **Policy instantiation:** DPO-fine-tuned models preferred (stronger credit assignment) but prompted instruction-tuned models work

- **Failure signatures:**
  - Uniform reward signals across tokens → policies not encoding preferences
  - Beam search produces generic/hedged text → EW objective not differentiating paths
  - Core policy blocked by coalition → chunking pruned critical statements or utilities poorly estimated
  - EPPL correlates poorly with human ratings → policy-opinion alignment weak

- **First 3 experiments:**
  1. **Credit assignment validation:** Replicate Section 6.2—compare log-probability deltas between aligned/misaligned token pairs under user vs. reference policies; expect Z-scores >2 at target tokens
  2. **Scaling study:** Replicate Section E.2.1—vary N for Best-of-N vs. Habermas candidates; confirm Best-of-N improves with N while Habermas plateaus
  3. **Beam width sweep:** Replicate Section E.2.2—test widths {2,4,6,8} on held-out scenarios; expect width=4 optimal for EPPL, confirm degradation at width=8

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the core be approximated efficiently on the unchunked token space without enumerating the full tree?
  - **Basis in paper:** [explicit] "finding a way to approximate the core on the unchunked space without looking at the whole tree is an important step to work towards"
  - **Why unresolved:** Chunking (token groups) makes computation tractable but Theorem 2 proves no constant-factor approximation guarantee exists when pruning leaves.
  - **What evidence would resolve it:** An algorithm with provable approximation bounds for Nash Welfare optimization on the full tree, or evidence that no polynomial-time approximation is possible.

- **Open Question 2:** What theoretical fairness guarantees can be established for single-statement (deterministic) egalitarian welfare maximization?
  - **Basis in paper:** [explicit] "future work should focus on theoretical guarantees for the single statement case, which we did not obtain"
  - **Why unresolved:** The paper proves core membership for stochastic policies but only provides empirical validation for deterministic search algorithms (beam search, finite lookahead).
  - **What evidence would resolve it:** Formal bounds relating the egalitarian welfare of search outputs to optimal egalitarian welfare, or approximation guarantees under specific assumptions.

- **Open Question 3:** How can personalized agent policies be trained to more faithfully represent individual human preferences?
  - **Basis in paper:** [explicit] "finding methods to train more faithful personalized policies for each agent is an important direction as it is upstream of many important challenges"
  - **Why unresolved:** The opinion-length scaling experiment (Figure 3) shows positive but still low correlation (~0.3) between model likelihoods and human Likert ratings.
  - **What evidence would resolve it:** Improved correlation metrics (>0.6) between policy likelihoods and human ratings, or demonstrated gains in downstream consensus quality.

## Limitations

- **Reward Signal Validity:** The core mechanism relies on agent policies providing meaningful token-level rewards through log-likelihoods. While experiments show localized credit assignment, the assumption that prompted instruction-tuned LLMs exhibit the same policy-derived Q-function properties as DPO-fine-tuned models remains empirically unverified.
- **Core Stability Under Chunking:** Theorem 2 establishes that chunking can arbitrarily degrade Nash Welfare relative to the unchunked optimum, but the empirical impact on core membership is not characterized.
- **Search Algorithm Robustness:** Beam search achieves optimal EPPL at width=4, but this result may be dataset-specific. The finite lookahead algorithm's performance depends critically on depth-d selection and branching factor B, with no sensitivity analysis provided for different task domains or agent numbers.

## Confidence

**High Confidence:** The mathematical framework connecting token-level MDPs to social choice theory (Section 4) is formally sound. The EPPL metric definition and its relationship to agent alignment are clearly established. The core theoretical guarantees (Nash Welfare maximization yields ex-ante core policies) are mathematically proven.

**Medium Confidence:** Empirical results showing search algorithms outperforming baselines are robust within tested conditions. The credit assignment experiments demonstrate localized reward signals, though the generalization to diverse opinion domains needs verification. The beam width optimization (w=4) is well-supported for the tested dataset.

**Low Confidence:** The scalability of the approach to larger agent pools or more complex generation tasks is unclear. The impact of different LLM architectures (beyond Llama 3.1 8B) on reward quality and search performance is unknown. The approximation error introduced by chunking and its relationship to core stability requires further characterization.

## Next Checks

1. **Policy Transfer Experiment:** Test whether prompted instruction-tuned models exhibit the same localized credit assignment properties as DPO-fine-tuned models by comparing Z-score distributions for aligned vs. misaligned token pairs across both model types.

2. **Chunking Robustness Analysis:** Systematically vary chunking parameters (chunk size, overlap) on a diverse set of scenarios to quantify the relationship between approximation error and core membership stability, identifying conditions where fairness guarantees break down.

3. **Search Algorithm Sensitivity:** Conduct a comprehensive hyperparameter sweep across beam widths (2-10), lookahead depths (1-6), and branching factors (2-5) on multiple datasets to establish performance boundaries and identify optimal configurations for different task characteristics.