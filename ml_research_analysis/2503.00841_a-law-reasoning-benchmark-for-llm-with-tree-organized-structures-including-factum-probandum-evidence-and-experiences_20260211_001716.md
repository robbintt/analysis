---
ver: rpa2
title: A Law Reasoning Benchmark for LLM with Tree-Organized Structures including
  Factum Probandum, Evidence and Experiences
arxiv_id: '2503.00841'
source_url: https://arxiv.org/abs/2503.00841
tags:
- legal
- task
- evidence
- reasoning
- probandum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a transparent law reasoning benchmark for
  large language models (LLMs), focusing on the task of generating tree-organized
  structures that justify judicial decisions. The proposed schema integrates hierarchical
  factum probandum, evidence, and experiences, making the reasoning process explicit
  and preventing bias.
---

# A Law Reasoning Benchmark for LLM with Tree-Organized Structures including Factum Probandum, Evidence and Experiences

## Quick Facts
- **arXiv ID:** 2503.00841
- **Source URL:** https://arxiv.org/abs/2503.00841
- **Reference count:** 37
- **Primary Result:** Introduces a transparent law reasoning benchmark with tree-organized structures, achieving a comprehensive score of 31.50 on a dataset of 453 cases using a tool-augmented TL Agent.

## Executive Summary
This paper proposes a transparent law reasoning benchmark for large language models (LLMs) that generates hierarchical reasoning structures to justify judicial decisions. The proposed schema explicitly maps evidence to granular "facts to be proved" (Factum Probandum) before reaching a conclusion, preventing bias and enhancing interpretability. A crowdsourced dataset of 453 cases with over 2,600 factum probanda, 14,500 pieces of evidence, and 16,400 experiences is created for evaluation. The authors also introduce a TL Agent that employs a suite of legal analysis tools, outperforming strong baselines on the benchmark tasks.

## Method Summary
The benchmark involves generating tree-organized structures from case descriptions, comprising factum probandum, evidence, experiences, and their inference links. Three sub-tasks are defined: (I) Factum Probandum Generation (interim + ultimate), (II) Evidence Reasoning (extraction + linking), and (III) Experience Generation. The TL Agent uses a ReAct-like strategy with GPT-4o-mini and specialized toolkits, including Fact Finding Head, Knowledge Search (Chroma + bge-large-zh-v1.5), MultiRole Checker, Legal Knowledge, Reflection, and others. The agent decomposes the complex reasoning task into sub-tasks handled by distinct tools, enhancing performance over monolithic prompting.

## Key Results
- The TL Agent achieves a comprehensive score of 31.50 on the dataset, surpassing several strong baselines.
- The MultiRole Checker significantly enhances the accuracy of the generated reasoning structure by simulating a "mock court" with different personas.
- Ablation study shows that removing the MultiRole Checker drops the S_fact-l score from 33.83 to 32.12, highlighting its value in the critique loop.

## Why This Works (Mechanism)
### Mechanism 1: Hierarchical Schema Constraint
The schema forces the model to explicitly map raw evidentiary text to granular "facts to be proved" before reaching a conclusion. This structural constraint acts as a guardrail, preventing the model from jumping directly from evidence to verdict without intermediate justification. Core assumption: Legal reasoning is inherently hierarchical and can be decomposed into discrete inference steps that LLMs can identify and replicate.

### Mechanism 2: Tool-Augmented Agentic Decomposition
The agent uses a "Fact Finding Head" for extraction and a "MultiRole Checker" for verification, splitting the cognitive load and allowing the base model to focus on specific syntactic or semantic operations per step. Core assumption: The errors in one sub-task can be corrected or isolated before they propagate to the next step via the agent's reflection loop.

### Mechanism 3: Multi-Role Critique (Self-Correction)
By prompting the LLM to adopt different personas (Police, Lawyer, Public) and then synthesize their feedback (Judge), the system creates a self-correction loop. This ensemble-like approach identifies logical gaps or biases that a single-pass generation might miss. Core assumption: The base LLM possesses sufficient theory-of-mind and legal domain knowledge to meaningfully simulate distinct expert perspectives.

## Foundational Learning
- **Concept: Factum Probandum vs. Evidence**
  - **Why needed here:** The core of this paper is distinguishing *what* needs to be proven (Factum Probandum) from the raw data supporting it (Evidence). Without this distinction, one cannot build the required tree structure.
  - **Quick check question:** In the sentence "The defendant was seen running from the scene," is this an Ultimate Probandum or Evidence?

- **Concept: ReAct (Reasoning + Acting) Pattern**
  - **Why needed here:** The TL Agent is explicitly built on a "ReAct-like strategy." Understanding the loop of [Thought → Action → Observation] is essential to debugging the agent's workflow.
  - **Quick check question:** If the agent retrieves irrelevant evidence from the vector DB, does the ReAct loop allow it to discard that evidence before generating the final tree?

- **Concept: Implicit Experience in Legal Logic**
  - **Why needed here:** The paper uniquely defines "Experience" (e.g., "women are not inclined to dominate") as a bridge between evidence and facts. You must understand this as a heuristic or "common sense" bridge, distinct from hard logic.
  - **Quick check question:** If an "Experience" node is biased (as shown in the "Rex v. Bywaters" example), does the schema flag it as an error or simply record it?

## Architecture Onboarding
- **Component map:** Input (Case Description) → Controller (LLM-based Agent Brain) → Toolkits (Fact Finding Head, Knowledge Search, MultiRole Checker, Reflection) → Output (Hierarchical JSON structure)
- **Critical path:**
  1. Ingestion: Agent reads the case description.
  2. Extraction: `Fact Finding Head` extracts interim/ultimate probandum and evidence.
  3. Linking: Agent attempts to link Evidence to Probandum.
  4. Experience Bridging: `Experience Generation Tool` infers the implicit logic connecting Evidence to Fact.
  5. Critique: `MultiRole Checker` validates the links and experiences.
  6. Reflection: `Reflection Tool` updates the structure based on critique.

- **Design tradeoffs:**
  - **Transparency vs. Performance:** The tree structure is highly interpretable but achieving high structural accuracy (Comprehensive Score ~31.5) is difficult; simpler prediction tasks might yield higher raw accuracy but less explainability.
  - **Agent Complexity:** Using a suite of tools (Knowledge Search + MultiRole) increases latency and cost compared to a zero-shot LLM call, though it improves precision (Recall in Evidence Reasoning jumped from ~19% to ~40% with the Agent).

- **Failure signatures:**
  - **Redundant Linking:** The paper notes models tend to link evidence to facts even when no substantive relation exists (Low Precision in Task 2).
  - **Hallucinated Experience:** The model might insert stereotypical or biased "experiences" that aren't grounded in the case text or legal statutes.
  - **State Drift:** In long cases, the agent might lose track of the "Ultimate Probandum" while focusing on interim facts.

- **First 3 experiments:**
  1. **Sanity Check (Task 1):** Run the `Fact Finding Head` tool on a single case to verify if the extracted "Interim Probandum" aligns with human-labeled ground truth.
  2. **Ablation (Tool Removal):** Disable the `MultiRole Checker` and measure the drop in the "Comprehensive Score" to quantify the value of the critique loop.
  3. **Stress Test (Experience Gen):** Input a case with ambiguous evidence (like the "Rex v. Bywaters" example) to see if the `Experience Generation` tool captures the divergence in reasoning or hallucinates a single path.

## Open Questions the Paper Calls Out
- **Question:** What specific interaction mechanisms between the agent's toolkits (e.g., MultiRole Checker vs. Knowledge Search) drive the performance improvements over standard LLMs?
  - **Basis in paper:** [explicit] The "Limitations" section states, "the underlying reasons for these outcomes have not been thoroughly investigated."
  - **Why unresolved:** The paper reports the comprehensive score (31.50) but does not provide a detailed ablation analysis or error analysis explaining *why* the tool suite succeeds where baselines fail.
  - **What evidence would resolve it:** A detailed ablation study analyzing the marginal contribution of each tool and a qualitative error analysis of failure cases.

- **Question:** Can the proposed schema and TL Agent generalize effectively to Common Law systems or other jurisdictions outside of the Chinese Civil Law context?
  - **Basis in paper:** [inferred] The dataset is exclusively derived from the "China Judgement Online Website," yet the introduction frames the task as general "Law Reasoning."
  - **Why unresolved:** The dataset and "experiences" are culturally and legally specific to Chinese judicial norms; it is unclear if the tree-organized structure applies to adversarial legal systems.
  - **What evidence would resolve it:** Evaluation of the TL Agent on a dataset of non-Chinese legal cases (e.g., US or UK case law) using the same schema.

- **Question:** How can the generation of "Experiences" be validated to ensure they constitute valid legal commonsense rather than hallucinations or "imperceptible bias"?
  - **Basis in paper:** [explicit] The "Future Work" section explicitly identifies "imperceptible bias" and the need to address challenges in "interpretability" as requiring further investigation.
  - **Why unresolved:** The "Experience" nodes are generated by LLMs based on implicit reasoning, which may inadvertently introduce societal biases not present in the explicit evidence.
  - **What evidence would resolve it:** A standardized method or benchmark for verifying the neutrality and legal validity of the generated "Experience" nodes.

## Limitations
- The benchmark focuses on Chinese civil cases, limiting generalizability to other legal systems.
- The hierarchical tree schema may oversimplify complex legal reasoning involving non-linear inference chains.
- The dataset size (453 cases) is modest for LLM training, and the evaluation metrics heavily penalize structural errors.

## Confidence
- **High Confidence:** The benchmark design (tree-organized schema) effectively forces explicit reasoning chains and reduces opacity in legal judgments. The ablation study demonstrating the MultiRole Checker's value is robust.
- **Medium Confidence:** The TL Agent's superior performance over baselines is well-supported, though the exact contribution of each tool component remains partially unclear due to incomplete implementation details.
- **Low Confidence:** The generalizability of the hierarchical reasoning approach to complex or adversarial legal scenarios (e.g., cases requiring circular reasoning) has not been validated.

## Next Checks
1. **Cross-Jurisdictional Validation:** Test the benchmark schema and TL Agent on legal cases from different jurisdictions (e.g., US or European civil law) to assess generalizability.
2. **Ablation on Tool Components:** Systematically disable individual tools (e.g., Knowledge Search, Reflection) to isolate their specific contributions to performance gains.
3. **Bias Analysis in Experience Generation:** Analyze the "Experience" nodes generated by the model for stereotypical or biased reasoning, particularly in cases involving sensitive attributes (e.g., gender, ethnicity).