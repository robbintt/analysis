---
ver: rpa2
title: Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity
arxiv_id: '2504.18929'
source_url: https://arxiv.org/abs/2504.18929
tags:
- distribution
- entropy
- ptgt
- arxiv
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how transformers compress information and exhibit
  dynamic sparsity through controlled experiments. It finds that transformers prefer
  learning lower-entropy distributions than the target, with larger models showing
  this bias more strongly, driven by the FFN module.
---

# Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity

## Quick Facts
- **arXiv ID**: 2504.18929
- **Source URL**: https://arxiv.org/abs/2504.18929
- **Reference count**: 22
- **Primary result**: Transformers compress information and exhibit dynamic sparsity, with larger models showing stronger low-entropy bias driven by FFN modules

## Executive Summary
This paper explores how transformers compress information and exhibit dynamic sparsity through controlled experiments. It finds that transformers prefer learning lower-entropy distributions than the target, with larger models showing this bias more strongly, driven by the FFN module. It also shows that larger transformers favor bypassing attention computations via residual connections and have fewer active neurons, with dead neuron spikes correlating with training instability. The study uses small, fully controllable settings to reveal these inductive biases and sparsity patterns, which are hard to observe in real-world large-scale scenarios.

## Method Summary
The paper uses a synthetic sequence modeling task with vocabulary size |V|=5 and sequence length n=5, creating a fully controllable environment where all 3125 possible sequences can be enumerated. The target distribution follows a sparsity principle with 0.8/0.2 conditional probabilities. The study compares decoder-only transformers (L=5 or 8, d=8-64, heads=4) against 1-layer RNNs, using Post-LN architecture with ReLU FFN layers. Custom evaluation tracks exact entropy H(pθ) and KL divergence across all possible sequences, while monitoring dead neuron counts and routing weights. Training uses Adam optimizer with batch size 512 for 100 epochs.

## Key Results
- Transformers learn distributions with systematically lower entropy than the target, with larger models showing stronger bias
- The FFN module drives the low-entropy preference, confirmed through ablation studies
- Larger transformers bypass attention via residual connections, with near-zero attention weights in routing analysis
- Dead neuron formation occurs in discrete jumps correlated with loss spikes, amplified by second-order optimizer moments

## Why This Works (Mechanism)

### Mechanism 1
The FFN module drives Transformers to learn lower-entropy distributions than the target, acting as implicit regularization that strengthens with model scale. FFN layers introduce an inductive bias that compresses information beyond the target distribution's entropy floor. The paper formalizes this as an implicit entropy penalty term αH(pθ) added to the standard cross-entropy objective, where α scales with model size. Ablations show "FFN-main" models (retaining FFN while removing attention layers) achieve entropy comparable to full Transformers, while "Attention-only" models remain near or above target entropy.

### Mechanism 2
Larger Transformers preferentially route computation through residual connections rather than attention heads, creating sparse forward paths. With a routing mechanism added to attention layers, larger models assign near-zero weights to attention head paths and near-one weights to residual connections for many inputs. This effectively bypasses attention computation, reducing parameter activation. The paper attributes this to larger models learning shortcuts that preserve gradient flow while discarding redundant transformations.

### Mechanism 3
Dynamic sparsity in FFN modules forms in discrete jumps correlated with loss spikes, and this process is amplified by second-order gradient information in Adam-like optimizers. During training, dead neurons (never activated across all inputs) increase suddenly rather than gradually. These jumps temporally align with loss spikes. Optimizer comparison shows SGD+momentum produces neither loss spikes nor dead neuron accumulation, while RMSprop and Adam variants (leveraging second-order moments) do.

## Foundational Learning

- **Concept: Entropy H(p) and KL divergence**
  - Why needed here: The paper's central claim is that Transformers learn distributions with entropy systematically below target entropy. Understanding entropy as a measure of distribution uncertainty and KL divergence as distribution mismatch is essential to interpret the findings.
  - Quick check question: Given a target distribution with H(ptgt)=3.5 bits, would a learned distribution with H(pθ)=2.8 bits and high KL(ptgt|pθ) indicate over-compression or under-fitting?

- **Concept: Residual connections and gradient highways**
  - Why needed here: The paper's attention sparsity finding hinges on residual connections serving as bypass paths. Without understanding how residuals preserve gradient flow and enable "identity-like" transformations, the preference for residuals over attention heads is opaque.
  - Quick check question: In a 12-layer Transformer, if layer 8's attention output is consistently near-zero and the residual path dominates, what does this imply about information flow through that layer?

- **Concept: FFN as key-value memory**
  - Why needed here: The paper frames FFN neurons as key-value pairs selectively activated by input patterns. This interpretation underpins the dead neuron analysis and the link to dynamic sparsity.
  - Quick check question: If 30% of FFN neurons in layer 6 are never activated for any valid input, does this indicate over-parameterization or learned specialization?

## Architecture Onboarding

- **Component map**: Input embedding → Positional encoding → [Layer × L: LayerNorm → Multi-head attention (with optional routing) → Add residual → LayerNorm → FFN (ReLU-activated, dh=4d) → Add residual] → Output projection → Softmax
- **Critical path**: Generate synthetic target distribution with known entropy via sparse conditional probabilities → Train model with cross-entropy loss while logging H(pθ), KL(ptgt|pθ), and dead neuron counts → Ablate FFN vs. attention components to isolate low-entropy driver → Compare optimizers to identify second-order gradient role in sparsity jumps
- **Design tradeoffs**: Larger model → stronger low-entropy bias (may harm target alignment) + more residual bypass + more dead neurons + higher spike risk; FFN-heavy architectures → lower entropy but potentially over-compressed representations; Adam-family optimizers → faster convergence but correlated with instability spikes and sparsity jumps; SGD+momentum → stable but slower
- **Failure signatures**: Sudden loss spike during training → check dead neuron count; likely jump in sparsity; Learned entropy far below target despite low loss → FFN-driven over-compression; consider reducing model size or FFN ratio; High residual path weights (>0.8) across layers → attention under-utilization; consider reducing depth or adding auxiliary attention losses
- **First 3 experiments**: Replicate the controlled setup with |V|=5, n=5, track H(pθ) vs. H(ptgt) across epochs for d∈{8,16,32,64} to confirm low-entropy scaling trend; Ablate FFN layers (Attention-only vs. FFN-main) at d=64 to verify FFN as entropy driver; log per-layer residual weights and dead neuron ratios; Compare Adam vs. SGD+momentum training at d=64; align loss spike timestamps with dead neuron count deltas to test optimizer-dependent sparsity formation

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the rigorous theoretical mechanism driving Transformers to implicitly regularize towards lower-entropy distributions?
  - The authors state they "leave a more rigorous theoretical analysis for our future work" regarding the intuitive formalization of low-entropy preference as implicit regularization.

- **Open Question 2**: How do Transformers explicitly guide the jump-like enhancement of dynamic sparsity during training?
  - The authors note that while they observe sparsity increasing in jumps, "it remains to be further explored how Transformers guide the gradual enhancement of this dynamic sparsity."

- **Open Question 3**: To what extent is the loss spike phenomenon dependent on the specific Transformer architecture versus the optimizer choice?
  - The authors conclude that "the occurrence of loss spikes is related to not only the second-order information of the optimizer, but also the special model architecture. We leave these aspects for our future work."

## Limitations

- The controlled synthetic setting (|V|=5, n=5) provides strong interpretability but raises questions about generalization to natural language distributions
- The explicit entropy calculation across all possible sequences is only feasible in this toy domain
- While the paper identifies correlation between dead neurons and loss spikes, it does not establish definitive causal mechanisms

## Confidence

**High Confidence:**
- Transformers systematically learn distributions with entropy below target entropy
- FFN modules drive the low-entropy bias
- Larger models show stronger preference for residual over attention computation
- Dead neuron formation correlates with training instability

**Medium Confidence:**
- The low-entropy bias scales with model size in real-world settings
- Residual bypass preference reflects fundamental architectural efficiency rather than routing artifact
- Second-order optimizer moments directly cause dead neuron formation (versus merely correlating)

## Next Checks

1. **Scaling validation**: Replicate entropy analysis on a medium-scale language modeling task (e.g., WikiText-2) where target entropy is known. Track whether FFN-heavy architectures consistently show lower entropy than attention-only or LSTM baselines, and whether this correlates with performance degradation.

2. **Architectural dependency test**: Remove the routing mechanism and directly measure attention head utilization versus residual connection magnitudes in standard Transformers. Compare head activation patterns across depths to determine if residual bypass preference persists without explicit routing.

3. **Optimizer ablation refinement**: Design an experiment where a single FFN neuron's activation threshold is monitored while systematically varying Adam's β₂ parameter. Track whether dead neuron formation timing shifts predictably with second-order moment influence, establishing stronger causal evidence beyond correlation.