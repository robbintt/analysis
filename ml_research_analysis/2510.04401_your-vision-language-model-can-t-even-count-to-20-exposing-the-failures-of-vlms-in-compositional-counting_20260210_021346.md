---
ver: rpa2
title: 'Your Vision-Language Model Can''t Even Count to 20: Exposing the Failures
  of VLMs in Compositional Counting'
arxiv_id: '2510.04401'
source_url: https://arxiv.org/abs/2510.04401
tags:
- counting
- arxiv
- object
- image
- vision-language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether modern vision-language models (VLMs)
  can accurately count objects. The authors introduce VLMCountBench, a minimalist
  benchmark that isolates counting tasks using basic geometric shapes (triangles,
  circles, squares) without semantic complexity.
---

# Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting

## Quick Facts
- arXiv ID: 2510.04401
- Source URL: https://arxiv.org/abs/2510.04401
- Reference count: 27
- Key outcome: VLMs show systematic failure in compositional counting tasks, with accuracy dropping from 0.23-0.60 for single shapes to 0.19-0.45 for three-shape scenarios

## Executive Summary
This paper evaluates whether modern vision-language models can accurately count objects by introducing VLMCountBench, a minimalist benchmark using basic geometric shapes. The authors systematically test eight state-of-the-art VLMs across three compositional levels (single shape, two shapes, three shapes) with 1-20 objects per shape. Results reveal a significant performance gap: while VLMs achieve reasonable accuracy for single-shape counting, their performance substantially degrades for compositional counting tasks, even with simple visual properties. The study demonstrates that current VLMs have fundamental limitations in handling compositional counting, highlighting a critical gap between their perceived capabilities and reliable counting performance.

## Method Summary
The authors introduce VLMCountBench, a controlled benchmark isolating counting tasks using basic geometric shapes (triangles, circles, squares) without semantic complexity. The benchmark systematically varies color, size, and prompt structure across three compositional levels: single shape, two shapes, and three shapes, with 1-20 objects per shape. Eight state-of-the-art VLMs are evaluated, including both open-source and closed-source models. The controlled experimental design allows isolation of specific factors affecting counting performance, such as visual perturbations (color, size changes) and prompt refinements (spatial or type decomposition).

## Key Results
- VLMs achieve 0.23-0.60 accuracy for single-shape counting but drop to 0.19-0.45 for three-shape scenarios
- Relative error increases from single to multi-shape counting, indicating degraded precision
- Visual perturbations generally degrade performance, with color changes having stronger negative impact than size changes
- Prompt refinements like spatial or type decomposition fail to substantially improve compositional counting results

## Why This Works (Mechanism)
VLMs struggle with compositional counting because they lack explicit mechanisms for object localization and enumeration. Unlike specialized counting models or object detection systems that process visual information through dedicated spatial reasoning modules, VLMs treat counting as an implicit language task embedded within their multimodal processing pipeline. This architectural approach fails to provide the structured representation needed for accurate compositional counting.

## Foundational Learning
- **Vision-Language Model Architecture**: Understanding how VLMs process visual and textual inputs together through cross-attention mechanisms
  - *Why needed*: Essential for grasping why VLMs fail at compositional counting tasks
  - *Quick check*: Review transformer-based VLM architectures focusing on visual encoding and cross-modal attention

- **Compositional Reasoning**: The ability to process multiple distinct elements simultaneously and maintain accurate relationships between them
  - *Why needed*: Critical for understanding why counting multiple shapes is more challenging than counting single shapes
  - *Quick check*: Compare single vs. multi-object tracking performance in vision systems

- **Prompt Engineering Techniques**: Methods for structuring inputs to elicit desired responses from language models
  - *Why needed*: Important for understanding why simple prompt refinements failed to improve counting accuracy
  - *Quick check*: Review prompt engineering literature for vision-language tasks

## Architecture Onboarding

**Component Map**: Image Encoder -> Cross-Attention Layer -> Language Decoder -> Output

**Critical Path**: Visual feature extraction → Cross-modal attention → Language generation → Counting response

**Design Tradeoffs**: VLMs prioritize general-purpose multimodal understanding over specialized counting capabilities, trading precision for versatility. This design choice enables broad application but creates systematic failures in tasks requiring exact enumeration.

**Failure Signatures**: Accuracy degradation with increasing compositional complexity, sensitivity to visual perturbations, inability to leverage spatial decomposition prompts effectively.

**First 3 Experiments**:
1. Test VLM performance on single-object counting with varying object sizes and colors
2. Evaluate two-shape compositional counting with identical vs. different colors
3. Assess impact of explicit spatial relationship prompts on three-shape counting accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark uses minimalist geometric shapes that may not represent real-world visual complexity
- Restricted prompt variety and single instruction format may underestimate VLM potential with alternative prompting
- Absence of occlusion or overlapping objects represents significant simplification of practical counting scenarios

## Confidence

**High Confidence**: Systematic degradation of performance from single-shape to multi-shape counting tasks; consistent pattern of relative error increase across models; observed negative impact of visual perturbations on accuracy.

**Medium Confidence**: Conclusion about fundamental limitations in compositional counting, though potentially influenced by experimental setup and limited prompt engineering exploration.

**Low Confidence**: Claims about practical implications for real-world applications requiring additional validation beyond controlled geometric benchmark.

## Next Checks
1. Replicate benchmark with natural images containing real objects in cluttered scenes to assess whether geometric limitations extend to practical counting scenarios.
2. Test broader range of prompt engineering techniques, including chain-of-thought reasoning and multi-step decomposition strategies, to establish whether performance improvements are possible with more sophisticated prompting.
3. Evaluate VLMs' counting performance when combined with object detection models as a hybrid approach to determine whether explicit localization helps overcome compositional counting challenges.