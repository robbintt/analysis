---
ver: rpa2
title: 'Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard
  Security'
arxiv_id: '2510.06525'
source_url: https://arxiv.org/abs/2510.06525
tags:
- diffusion
- prompt
- stable
- prompts
- deanonymization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that text-to-image (T2I) models leave distinctive
  signatures in their generated outputs, enabling deanonymization attacks on voting-based
  leaderboards. The authors show that using CLIP embeddings and a simple centroid-based
  classification method, they can accurately identify which model generated a given
  image from a leaderboard, even without control over the input prompts.
---

# Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security

## Quick Facts
- arXiv ID: 2510.06525
- Source URL: https://arxiv.org/abs/2510.06525
- Reference count: 34
- Text-to-image models can be deanonymized from leaderboard images with 87% top-1 accuracy

## Executive Summary
This paper reveals that text-to-image models leave distinctive signatures in their generated outputs, enabling deanonymization attacks on voting-based leaderboards. Using CLIP embeddings and a simple centroid-based classification method, the authors demonstrate they can accurately identify which model generated a given image from a leaderboard, even without control over input prompts. The attack achieves 87% top-1 accuracy and 95% top-3 accuracy across 19 diverse T2I models and 280 prompts, far exceeding random guessing. These findings expose significant security vulnerabilities in T2I leaderboards, as adversaries can reliably identify models using only publicly displayed images.

## Method Summary
The authors develop a deanonymization attack that leverages CLIP embeddings to identify the source model of text-to-image generations. Their approach creates centroid embeddings for each model by averaging embeddings of generated images, then classifies new images by comparing their embeddings to these centroids. They evaluate this method across 19 different T2I models using 280 prompts, demonstrating high accuracy in model identification. The study also introduces a distinguishability metric to identify prompts that yield near-perfect separability in embedding space, revealing that certain prompts are particularly effective at exposing model signatures.

## Key Results
- 87% top-1 accuracy and 95% top-3 accuracy in identifying T2I model provenance from generated images
- CLIP embeddings provide sufficient separability between different T2I models for reliable identification
- Introduced distinguishability metric successfully identifies prompts with near-perfect embedding space separation
- Leaderboards are highly vulnerable to manipulation through deanonymization attacks

## Why This Works (Mechanism)
Text-to-image models leave identifiable signatures in their outputs due to differences in their training data, architectures, and generation processes. These differences manifest as distinct patterns in the CLIP embedding space, where images generated by the same model cluster together while remaining separable from other models' outputs. The centroid-based classification method exploits these clustering patterns to identify model provenance without requiring knowledge of the input prompts used to generate the images.

## Foundational Learning
- **CLIP embeddings**: Why needed - Provide a unified embedding space for comparing diverse image outputs; Quick check - Verify embeddings capture meaningful semantic and stylistic differences between models
- **Centroid-based classification**: Why needed - Enables model identification without prompt information; Quick check - Test classification accuracy with varying numbers of reference images per model
- **Distinguishability metric**: Why needed - Identifies prompts that maximize separation between models; Quick check - Correlate metric scores with actual classification accuracy
- **Leaderboard security**: Why needed - Context for understanding real-world implications; Quick check - Assess vulnerability across different leaderboard designs
- **Model signature detection**: Why needed - Core capability being evaluated; Quick check - Verify signatures persist across different generations of the same model
- **Embedding space separability**: Why needed - Fundamental property enabling the attack; Quick check - Measure inter-model distances in embedding space

## Architecture Onboarding

Component map: T2I Model -> Image Generation -> CLIP Embedding Extraction -> Centroid Calculation -> Classification

Critical path: The pipeline flows from model generation through embedding extraction to classification. The most critical components are the CLIP embedding extraction (which captures the distinguishing features) and the centroid calculation (which creates the model signatures). Any degradation in embedding quality or centroid accuracy directly impacts classification performance.

Design tradeoffs: The method prioritizes simplicity and generalizability over complexity, using basic centroid-based classification rather than more sophisticated machine learning approaches. This makes the attack more robust to variations in model architecture but may limit maximum achievable accuracy compared to model-specific attacks.

Failure signatures: The attack would fail if CLIP embeddings do not capture sufficient model-specific characteristics, if multiple models produce indistinguishable outputs, or if the number of reference images per model is too small to create reliable centroids. Additionally, models with similar training approaches or architectures may produce overlapping embeddings.

3 first experiments:
1. Test classification accuracy when using different numbers of reference images per model to determine optimal training set size
2. Evaluate performance when mixing prompts from different sources to assess robustness to prompt variation
3. Measure embedding distances between models with similar architectures to identify potential confusion points

## Open Questions the Paper Calls Out
None

## Limitations
- Attack success rate may vary significantly depending on the specific set of models tested
- Evaluation uses a controlled dataset of 280 prompts, which may not reflect real-world adversarial scenarios
- Method relies on publicly available CLIP embeddings, which may not generalize to all embedding spaces or future model architectures
- Temporal stability of signatures when models are updated or fine-tuned is not fully characterized

## Confidence
- **High confidence**: Primary claims about leaderboard vulnerability and model identification appear robust based on experimental results
- **High confidence**: Introduced distinguishability metric and its effectiveness are well-supported
- **Medium confidence**: Generalizability to larger model collections and adversarial scenarios requires further validation

## Next Checks
1. Test the attack methodology against a broader range of T2I models, including fine-tuned variants of the same base models, to assess robustness against model similarity
2. Evaluate performance when models are updated or fine-tuned after the initial embedding analysis to determine temporal stability
3. Assess the attack's effectiveness against adversarial prompt selection strategies designed to minimize distinguishability in embedding space