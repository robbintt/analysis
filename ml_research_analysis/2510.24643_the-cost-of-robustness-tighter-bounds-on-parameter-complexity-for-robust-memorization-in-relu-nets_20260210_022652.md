---
ver: rpa2
title: 'The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust
  Memorization in ReLU Nets'
arxiv_id: '2510.24643'
source_url: https://arxiv.org/abs/2510.24643
tags:
- parameters
- have
- fproj
- network
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the parameter complexity of robust memorization
  in ReLU networks, aiming to determine how many parameters are required to memorize
  any dataset with a certain robustness ratio $\rho = \mu/\epsilon$ between robustness
  radius $\mu$ and data separation $\epsilon$. The authors establish both upper and
  lower bounds on the parameter count across the entire range $\rho \in (0,1)$, showing
  that parameter complexity matches that of non-robust memorization when $\rho$ is
  small, but grows with increasing $\rho$.
---

# The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets

## Quick Facts
- arXiv ID: 2510.24643
- Source URL: https://arxiv.org/abs/2510.24643
- Reference count: 40
- This paper establishes tight bounds on parameter complexity for robust memorization in ReLU networks, showing that parameter count matches non-robust memorization for small robustness radii but grows significantly as robustness increases.

## Executive Summary
This paper investigates the fundamental question of how many parameters are required for ReLU neural networks to memorize any dataset while maintaining a specified level of adversarial robustness. The authors introduce the robustness ratio $\rho = \mu/\epsilon$, which captures the trade-off between robustness radius $\mu$ and data separation $\epsilon$. They establish both necessary conditions (lower bounds) and sufficient conditions (upper bounds) on the parameter count needed for robust memorization across the entire range of robustness ratios $\rho \in (0,1)$.

The key insight is that parameter complexity for robust memorization depends critically on the robustness ratio. When robustness is minimal ($\rho$ small), the parameter count matches that of non-robust memorization, requiring only $\tilde{O}(\sqrt{N})$ parameters. However, as robustness increases, the parameter requirements grow substantially, reaching $\tilde{O}(Nd^2\rho^4)$ for larger robustness ratios. The paper also reveals necessary conditions on the first hidden layer width and total parameter count that must be satisfied for any robust memorization architecture.

## Method Summary
The authors employ a combination of combinatorial and geometric arguments to establish both upper and lower bounds on parameter complexity. For the lower bounds, they use information-theoretic arguments based on the number of possible labelings that can be robustly realized, showing that the network must have sufficient capacity to distinguish between different input regions. The analysis considers the partition of input space induced by the ReLU network and counts the number of linear regions that can be created with different network widths and depths.

For the upper bounds, the authors construct explicit network architectures that achieve robust memorization. The construction involves carefully designed activation patterns and weight assignments that create the necessary decision boundaries while maintaining robustness within specified radii. The analysis considers three different regimes of the robustness ratio $\rho$ and provides network architectures with matching parameter counts for each regime.

## Key Results
- Necessary conditions show the first hidden layer must have width at least $\rho^2\min\{N,d\}$ and the network must have $\Omega(\min\{\sqrt{N/(1-\rho^2)}, \sqrt{d}\}\sqrt{N})$ parameters
- Sufficient conditions demonstrate robust memorization with $\tilde{O}(\sqrt{N})$ parameters when $\rho < 1/(5N\sqrt{d})$
- Parameter complexity scales as $\tilde{O}(Nd^{1/4}\rho^{1/2})$ for intermediate robustness ratios and $\tilde{O}(Nd^2\rho^4)$ for larger robustness ratios
- Results reveal that achieving robustness is relatively inexpensive for small robustness radii but becomes increasingly costly as the radius grows

## Why This Works (Mechanism)
The mechanism behind these results relies on the geometric properties of ReLU networks and their ability to partition input space. When the robustness radius is small relative to data separation, the network can create narrow decision boundaries that separate classes while maintaining robustness. As the robustness radius increases, wider decision boundaries are needed, requiring more parameters to create the necessary partitions. The analysis leverages the relationship between network width, depth, and the number of linear regions that can be created, showing how this geometric capacity translates to parameter complexity requirements.

## Foundational Learning
- **Robust Memorization**: The ability of a network to perfectly fit training data while maintaining classification within a specified perturbation radius. Needed to understand the core problem being analyzed; quick check: verify the definition of $\rho = \mu/\epsilon$ and its interpretation.
- **ReLU Network Geometry**: How ReLU activations partition input space into linear regions. Critical for understanding the geometric arguments; quick check: understand how ReLU depth affects the maximum number of linear regions.
- **Parameter Complexity Bounds**: The relationship between network size and its expressive capacity. Fundamental to the paper's contributions; quick check: verify the asymptotic notation conventions used.
- **Adversarial Robustness**: The property that small input perturbations do not change classification. Central to the paper's motivation; quick check: understand the difference between robust and standard memorization.
- **Combinatorial Arguments**: Counting techniques used to establish lower bounds on network capacity. Key methodological tool; quick check: follow the counting argument for labelings in the proof.
- **Constructive Upper Bounds**: Explicit network constructions that achieve desired properties. Used to establish sufficient conditions; quick check: trace through one of the construction proofs.

## Architecture Onboarding
Component Map: Input -> First Hidden Layer (width $\Omega(\rho^2\min\{N,d\})$) -> Subsequent Layers -> Output

Critical Path: The first hidden layer width is the critical bottleneck, as it determines the network's ability to create sufficient linear regions for robust separation. The total parameter count is constrained by the need to represent all possible robust labelings.

Design Tradeoffs: Wider first layers allow for more linear regions but increase parameter count. Deeper networks can create more regions with fewer parameters per layer but may require careful weight initialization. The optimal architecture depends on the robustness ratio $\rho$ and the relationship between dataset size $N$ and input dimension $d$.

Failure Signatures: Insufficient first layer width leads to inability to robustly separate classes. Too few total parameters result in inability to represent all required robust labelings. Overly wide networks may overfit or become computationally inefficient.

First Experiments:
1. Verify the lower bound on first layer width by attempting robust memorization with progressively narrower first layers.
2. Test the parameter scaling predictions by measuring actual parameter counts needed for robust memorization across different $\rho$ values.
3. Validate the construction algorithms by implementing them and measuring achieved robustness on synthetic datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses on memorization rather than generalization, limiting direct applicability to real-world scenarios
- Results are asymptotic and may not accurately reflect practical parameter requirements for finite datasets
- The bounds show gaps in the intermediate $\rho$ regime, suggesting room for tightening
- Assumes idealized conditions that may not hold in practical settings

## Confidence
- Main theoretical bounds: High
- Tightness of bounds across all $\rho$ values: Medium
- Practical applicability of asymptotic results: Medium
- Generality to other architectures and activation functions: Low

## Next Checks
1. Empirical validation of the theoretical bounds on synthetic and real datasets to verify the predicted parameter scaling behavior
2. Investigation of whether the lower bounds can be tightened further, particularly in the intermediate $\rho$ regime where the current bounds show the largest gaps
3. Extension of the analysis to other network architectures and activation functions to assess the generality of the results