---
ver: rpa2
title: 'OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning'
arxiv_id: '2510.24636'
source_url: https://arxiv.org/abs/2510.24636
tags:
- reward
- openrm
- arxiv
- training
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OpenReward, a tool-augmented reward model
  for evaluating long-form agentic tasks by using external tools to gather evidence.
  The authors address the challenge of knowledge-intensive tasks requiring grounding
  beyond internal model knowledge by training with reinforcement learning on over
  27K synthetic pairwise examples.
---

# OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.24636
- **Source URL**: https://arxiv.org/abs/2510.24636
- **Reference count**: 9
- **Primary result**: OpenReward achieves 91.33% average accuracy across three in-domain tasks and 78.54% on out-of-domain benchmarks

## Executive Summary
OpenReward introduces a tool-augmented reward model that learns to evaluate long-form agentic tasks by using external tools to gather evidence beyond internal model knowledge. The approach addresses the challenge of knowledge-intensive tasks by training with reinforcement learning on synthetic pairwise examples, using Group Relative Policy Optimization (GRPO) to jointly supervise intermediate tool usage and final judgment accuracy. Extensive experiments demonstrate OpenReward outperforms existing reward models, showing consistent gains when integrated into both inference-time response selection and training-time data selection for LLM alignment.

## Method Summary
OpenReward is a tool-augmented reward model designed to evaluate long-form agentic tasks by leveraging external tools for evidence gathering. The system addresses the challenge of knowledge-intensive tasks that exceed the internal knowledge capacity of language models by incorporating external retrieval capabilities. The reward function is a composite of outcome reward (based on final judgment accuracy) and tool-use reward (based on evidence relevance and completeness), balanced by a hyperparameter λ. The model is trained using Group Relative Policy Optimization (GRPO) on synthetic pairwise examples, jointly supervising both intermediate tool usage and final judgment accuracy. This approach enables the model to learn optimal strategies for when and how to use external tools effectively during task evaluation.

## Key Results
- OpenReward achieves 91.33% average accuracy across three in-domain tasks
- OpenReward achieves 78.54% accuracy on out-of-domain benchmarks
- Tool-augmented approach demonstrates consistent gains when integrated into inference-time response selection and training-time data selection for LLM alignment

## Why This Works (Mechanism)
OpenReward's effectiveness stems from its ability to dynamically access external knowledge sources when internal model knowledge is insufficient. The composite reward function encourages both accurate final judgments and effective evidence gathering, preventing the model from either ignoring tools entirely or over-relying on them. The GRPO training approach enables learning from synthetic pairwise comparisons, allowing the model to develop sophisticated strategies for tool utilization. By jointly supervising intermediate tool usage and final judgment accuracy, OpenReward learns to optimize the entire evaluation process rather than just the end result.

## Foundational Learning
- **Reinforcement Learning with GRPO**: Needed for learning from synthetic pairwise comparisons; quick check: verify gradient updates follow policy improvement theorem
- **Tool-augmented evaluation**: Needed to access knowledge beyond internal model capabilities; quick check: validate retrieval relevance scores correlate with task accuracy
- **Composite reward design**: Needed to balance tool usage and outcome accuracy; quick check: test sensitivity to λ hyperparameter across domains
- **Synthetic data generation**: Needed to create training examples at scale; quick check: ensure synthetic examples cover realistic failure modes
- **Pairwise comparison training**: Needed for stable reward learning; quick check: verify relative preferences are consistent across training batches

## Architecture Onboarding

**Component map**: External Tools -> Evidence Retrieval -> Reward Model -> Final Judgment

**Critical path**: Task input → Tool selection → Evidence gathering → Judgment generation → Reward calculation → Model update

**Design tradeoffs**: 
- Composite reward vs. outcome-only: Prevents lazy searching and over-searching
- Tool-augmented vs. knowledge distillation: Better generalization to unseen scenarios
- GRPO vs. supervised learning: Enables learning from synthetic data without explicit labels

**Failure signatures**:
- Lazy searching: Low tool usage, high outcome-only reward leads to incomplete evidence
- Over-searching: High tool usage, decoupled tool rewards lead to excessive retrieval
- Tool reliability issues: Noisy or irrelevant evidence degrades judgment quality
- Latency problems: Slow tool responses impact real-time deployment feasibility

**First experiments**:
1. Ablation study comparing outcome-only vs. composite reward performance
2. Sensitivity analysis of λ hyperparameter across different task domains
3. Evaluation of tool reliability impact by introducing controlled noise in retrieval results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the reliability and latency of external retrieval tools impact OpenReward's judgment accuracy and deployment feasibility in real-time applications?
- **Basis in paper**: The conclusion states: "the effectiveness of OPENRM relies on the availability and reliability of external tools, which may introduce bias or extra latency in evidence retrieval."
- **Why unresolved**: The paper does not quantify tool failure rates, retrieval latency, or how noise in retrieved evidence affects final judgment quality.
- **What evidence would resolve it**: Systematic evaluation across varying retrieval latency conditions and corrupted/noisy retrieval corpora, with accuracy and latency metrics.

### Open Question 2
- **Question**: Can the tool-augmented reward modeling framework generalize to multimodal evaluation involving visual or tabular inputs?
- **Basis in paper**: The conclusion notes: "our current implementation only focuses on text-based evaluation, and has yet to be extended to multimodal settings involving visual or tabular inputs."
- **Why unresolved**: The architecture and reward design assume text-only tool outputs; handling image or table retrieval requires new tool integration and reward formulations.
- **What evidence would resolve it**: Extending OpenReward to image search or table retrieval tools and benchmarking on multimodal evaluation tasks.

### Open Question 3
- **Question**: What is the optimal balance between tool-use supervision and outcome supervision in the composite reward function across different task domains?
- **Basis in paper**: Ablation studies reveal failure modes—lazy searching under outcome-only rewards and over-searching with decoupled tool rewards—but do not establish whether optimal λ varies by domain or task complexity.
- **Why unresolved**: The fixed λ=0.5 may not generalize; different domains (e.g., medical vs. Wikipedia) may require different weighting strategies.
- **What evidence would resolve it**: Domain-specific hyperparameter sweeps for λ with statistical validation across diverse benchmarks.

## Limitations
- The method relies heavily on external tools, introducing potential bias and latency issues
- The approach has not been validated on multimodal evaluation tasks involving visual or tabular inputs
- The fixed λ=0.5 may not be optimal across all task domains and complexities

## Confidence
- **High confidence**: Experimental results showing superior performance on tested benchmarks
- **Medium confidence**: Generalization claims to out-of-domain tasks based on limited benchmarks
- **Medium confidence**: Benefits of tool-augmentation demonstrated but failure mode analysis is limited

## Next Checks
1. Conduct real-world deployment tests with actual users performing long-form agentic tasks to validate performance beyond synthetic benchmarks
2. Perform ablation studies isolating the contribution of different tool types to understand which evidence sources are most valuable
3. Test scalability by evaluating performance on progressively longer tasks (10x the current maximum length) and with expanded tool libraries covering more diverse domains