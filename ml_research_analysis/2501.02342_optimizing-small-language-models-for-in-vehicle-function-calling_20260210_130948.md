---
ver: rpa2
title: Optimizing Small Language Models for In-Vehicle Function-Calling
arxiv_id: '2501.02342'
source_url: https://arxiv.org/abs/2501.02342
tags:
- pruning
- function-calling
- language
- fine-tuning
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Optimizing Small Language Models for In-Vehicle Function-Calling

## Quick Facts
- arXiv ID: 2501.02342
- Source URL: https://arxiv.org/abs/2501.02342
- Reference count: 37
- Primary result: Pruned Phi-3 mini (2.8B params) achieves 0.88 function-calling accuracy at 11 tokens/sec on 7-core ARM CPU

## Executive Summary
This paper presents a systematic approach to optimize Phi-3 mini for in-vehicle function-calling through structured pruning, healing, and quantization. The authors demonstrate that depth-wise pruning can remove approximately 25% of decoder layers with acceptable performance loss, while extended healing on web-scale data recovers lost capabilities. The resulting 2.8B parameter model achieves high function-calling accuracy (0.88) and real-time inference on standard vehicle CPUs without hardware acceleration, making it suitable for automotive edge deployment.

## Method Summary
The authors employ a three-stage pipeline: (1) depth-wise pruning using angular distance to identify and remove redundant layers (layers 21-29), reducing parameters from 3.8B to 2.8B; (2) healing with short QLoRA (5K steps) followed by extended fine-tuning (15B tokens on fineweb-edu) to recover knowledge lost during pruning; and (3) function-calling fine-tuning using LoRA on a synthetic dataset with special MB tokens mapping to vehicle functions. The model is then converted to GGUF format and 4-bit quantized for deployment on ARM CPUs, achieving 11 tokens/sec inference speed.

## Key Results
- Depth-wise pruning removes ~1B parameters (25% of layers) with modest benchmark decline
- Extended healing (15B tokens) recovers knowledge representations lost during pruning
- Pruned model achieves 0.88 function-calling accuracy on vehicle commands
- 4-bit quantized model runs at 11 tokens/sec on 7-core ARM CPU (<2GB RAM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured depth-wise pruning can remove ~25% of decoder layers with acceptable performance loss when followed by healing.
- Mechanism: Angular distance between hidden states at layer i and i+n identifies redundant layer blocks. Layers 21-29 of Phi-3 mini showed maximum similarity (minimum distance) and were pruned as a contiguous 8-layer block, reducing 3.8B to 2.8B parameters.
- Core assumption: Layers with highly similar input/output activations perform redundant transformations and can be collapsed.
- Evidence anchors:
  - [Section 3.1]: "Layers 21-29 were maximally similar, and thus pruned."
  - [Section 4.1]: "Depth-wise pruning led to a modest decline in model performance - considering that approximately 1B parameters were removed."
  - [corpus]: Weak direct evidence; corpus papers focus on in-vehicle networks/intrusion detection, not LLM pruning. Gromov et al. cited within paper supports 30% layer removal threshold.
- Break condition: Pruning >30% of layers causes model collapse; width pruning on top of depth pruning (to 1.8B) causes significant benchmark degradation.

### Mechanism 2
- Claim: Extended healing (15B tokens) recovers more capability than short healing (5000 steps/QLoRA only).
- Mechanism: Post-pruning, models lose coherence and factual knowledge. Short QLoRA healing on MLP weights restores sentence formation but not knowledge. Extended full fine-tuning on fineweb-edu (~15B tokens) recovers knowledge representations.
- Core assumption: Healing requires exposure to diverse web-scale data, not just parameter-efficient adaptation on limited weights.
- Evidence anchors:
  - [Section 3.2]: "While the resulting models were able to form correct and meaningful sentences again, the factual knowledge of the original Phi3-mini was almost entirely lost."
  - [Section 4.1]: "longer healing yields better scores on MMLU, HellaSwag, and ARC for (Phi3-2.8B + hlong) vs (Phi3-2.8B + hshort)."
  - [corpus]: No direct corpus evidence for healing mechanisms.
- Break condition: Width-pruned models (1.8B) cannot fully recover regardless of healing duration.

### Mechanism 3
- Claim: Special functional tokens + synthetic fine-tuning enable function-calling with 0.86-0.88 accuracy even in pruned models.
- Mechanism: Eight MB tokens (e.g., `<MB_1>`, `<MB_2>`) are added to the tokenizer, each mapping to a vehicle function (seat heating, ambient lighting, etc.). Synthetic dataset with 25K positive examples and 500 negative examples trains the model to output token sequences that parse to gRPC calls.
- Core assumption: Function-calling is a constrained schema-matching task that does not require full model capacity; specialized tokens create direct output-to-API mappings.
- Evidence anchors:
  - [Section 3.3]: "set_ambient_light_color_program mapped to <MB_1> and set_seat_heating_intensity mapped to <MB_2>."
  - [Section 4.2]: "Fine-tuning the Phi3-2.8B model with LoRA achieves a function-calling accuracy of 0.88."
  - [corpus]: Octopus v2 (cited in paper, not in corpus neighbors) introduced functional tokens; corpus papers are unrelated to function-calling.
- Break condition: Negative sampling is critical—without irrelevant queries, models may over-trigger functions.

## Foundational Learning

- Concept: **Structured vs. Unstructured Pruning**
  - Why needed here: Structured pruning removes entire layers/neurons, enabling CPU inference; unstructured pruning creates sparse matrices requiring custom hardware.
  - Quick check question: If you remove 50% of individual weights randomly, will inference speed up on a standard CPU? (Answer: No—sparse operations lack hardware support.)

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Used for healing (short) and function-calling fine-tuning. Adds trainable low-rank matrices instead of modifying full weights, enabling modularity and lower compute.
  - Quick check question: Why might LoRA require more epochs and higher learning rate than full fine-tuning for function-calling? (Answer: Fewer trainable parameters need more exposure to converge.)

- Concept: **Quantization (4-bit)**
  - Why needed here: Reduces memory footprint to <2GB RAM and enables real-time inference (11 t/s) on vehicle CPU without hardware acceleration.
  - Quick check question: What is the tradeoff between 2-bit and 8-bit quantization for this use case? (Answer: 2-bit saves more memory but adds perplexity; 4-bit balances throughput and quality.)

## Architecture Onboarding

- Component map:
  - **Base model**: Phi-3 mini (3.8B, 32 layers, 3072 hidden dim)
  - **Pruning pipeline**: Depth-wise (similarity-based layer removal) → optional width-wise (activation magnitude pruning)
  - **Healing stage**: hshort (QLoRA, 5K steps) → hlong (full FT, 15B tokens) → SFT (instruction tuning, OpenHermes-2.5)
  - **Function-calling FT**: LoRA (rank 96, 2 epochs) or FFT (1 epoch, weight decay 0.1)
  - **Deployment**: Convert to GGUF → 4-bit quantize → run via llama.cpp on ARM CPU (7 cores)

- Critical path:
  1. Calibrate pruning on fineweb dataset → compute angular distances → identify layers 21-29
  2. Prune → immediately heal (hlong + SFT) before any task fine-tuning
  3. Fine-tune on synthetic function-calling dataset with MB tokens
  4. Quantize to 4-bit GGUF; validate function-calling accuracy holds

- Design tradeoffs:
  - **2.8B vs 1.8B**: 2.8B retains general benchmarks (MMLU ~34); 1.8B drops significantly (MMLU ~31) but function-calling accuracy similar (~0.85)
  - **LoRA vs FFT**: LoRA modular/switchable; FFT slightly more stable but requires more regularization
  - **Speed vs quality**: Depth pruning doubles token speed vs width pruning; removing layers is more consequential for generation quality

- Failure signatures:
  - Model outputs incoherent text after pruning → insufficient healing
  - Function-calling accuracy drops after quantization → check GGUF conversion; verify tokenizer merges LoRA correctly
  - CPU saturation during inference → expected (400% on 7-core ARM); allocate resources dynamically before inference

- First 3 experiments:
  1. Replicate depth-wise pruning on Phi-3 mini with block size n=8; verify layers 21-29 have minimum angular distance on your calibration data.
  2. Compare hshort vs hlong healing on pruned model; evaluate MMLU/HellaSwag before/after to confirm knowledge recovery requires extended training.
  3. Fine-tune healed model on function-calling dataset; test exact match accuracy on held-out vehicle commands with both LoRA and FFT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal healing strategy (duration, dataset, and fine-tuning method) to recover general language capabilities after structured pruning?
- Basis in paper: [inferred] The authors found that the recommended 5,000-step QLoRA healing from prior work was insufficient—factual knowledge was "almost entirely lost" and required 45,000 additional steps on fineweb-edu. However, they do not systematically explore the optimal healing configuration.
- Why unresolved: Only two healing durations were tested (short vs. long), and the relationship between healing investment and capability recovery remains unquantified.
- What evidence would resolve it: Ablation studies varying healing steps, dataset composition, and fine-tuning methods (LoRA vs. full fine-tuning) with systematic benchmarking across general language tasks.

### Open Question 2
- Question: Can width-wise pruning be made viable for in-vehicle SLM deployment without causing catastrophic capability degradation?
- Basis in paper: [explicit] The paper states that width-wise pruning on top of the depth-pruned 2.8B model "caused significant degradation in model capabilities across all benchmarks regardless of the healing and alignment strategy applied to it."
- Why unresolved: The authors applied one width-pruning approach (MINITRON) with a single healing strategy; it remains unclear whether alternative width-pruning methods or more aggressive healing could preserve performance.
- What evidence would resolve it: Experiments combining depth pruning with alternative width-pruning techniques and extended healing, measuring retention on both general benchmarks and function-calling accuracy.

### Open Question 3
- Question: How can CPU resource spikes during on-device inference be effectively mitigated in automotive head units with shared computational resources?
- Basis in paper: [explicit] The authors note the CPU usage spike "can be further mitigated by dynamically allocating resources to the LLM process before inference" but do not implement or evaluate this.
- Why unresolved: The paper demonstrates feasibility but leaves resource management for production deployment unaddressed; the 400% CPU usage (4 of 7 cores) may conflict with other vehicle systems.
- What evidence would resolve it: Implementation and profiling of dynamic resource allocation strategies on actual vehicle hardware, measuring impact on inference latency and interference with concurrent vehicle processes.

### Open Question 4
- Question: Does the observed 30% parameter removal ceiling generalize across different SLM architectures beyond Phi-3?
- Basis in paper: [inferred] The authors infer "the upper limit of parameter removal is roughly 30% of the original model parameters" based on experiments with Phi-3 and reference to Gromov et al.'s findings on other model families, but do not systematically validate this across architectures.
- Why unresolved: Only Phi-3 mini was optimized in this work; the pruning threshold may be architecture-dependent.
- What evidence would resolve it: Replicating the pruning and healing pipeline on multiple SLM families (Gemma, Mistral, Llama) and identifying whether the 30% threshold holds or varies by architecture.

## Limitations

- Pruning and healing pipeline evaluated only on Phi-3 mini, limiting generalizability to other model architectures
- Synthetic function-calling dataset (25K positive, 500 negative examples) may cause overfitting and lacks real-world validation
- Healing process requires extensive fine-tuning (15B tokens) without understanding which knowledge representations are specifically lost
- Deployment aspects focus on CPU inference without addressing power consumption or thermal constraints

## Confidence

**High Confidence (8/10):**
- Structured depth-wise pruning can remove ~25% of decoder layers with acceptable performance loss when followed by healing
- Extended healing (15B tokens) recovers more capability than short healing (5000 steps/QLoRA only)
- Special functional tokens + synthetic fine-tuning enable function-calling with 0.86-0.88 accuracy even in pruned models
- 4-bit quantization enables real-time inference (11 t/s) on vehicle CPU without hardware acceleration

**Medium Confidence (6/10):**
- Depth pruning is more effective than width pruning for generation quality (supported by results but mechanism not fully explained)
- LoRA vs FFT tradeoffs (modular vs stable) are validated on this specific task but may not generalize
- Knowledge loss during pruning is recoverable with sufficient healing (assumes the 15B tokens capture all lost representations)

**Low Confidence (4/10):**
- Pruning threshold of 30% layers is a hard limit before model collapse (only tested on this specific model and pruning pattern)
- The 8-layer contiguous block (21-29) represents a generalizable pattern for identifying redundant layers (may be Phi-3 mini specific)

## Next Checks

1. **Cross-Model Pruning Validation**: Apply the angular distance-based depth pruning approach to different small language models (e.g., Gemma, Qwen) to verify whether layers 21-29 consistently show maximum similarity across architectures, or if the pattern is Phi-3 mini specific.

2. **Real-World Function-Calling Testing**: Deploy the pruned, healed, and quantized model in actual in-vehicle scenarios with diverse voice commands and user interactions to validate synthetic dataset performance transfers to real-world accuracy and robustness.

3. **Healing Efficiency Analysis**: Conduct ablation studies on healing duration and data diversity by comparing recovery curves using different healing datasets (fineweb-edu vs other web corpora) and token budgets to identify the minimum effective healing requirements.