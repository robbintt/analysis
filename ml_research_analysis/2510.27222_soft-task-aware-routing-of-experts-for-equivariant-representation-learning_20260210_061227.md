---
ver: rpa2
title: Soft Task-Aware Routing of Experts for Equivariant Representation Learning
arxiv_id: '2510.27222'
source_url: https://arxiv.org/abs/2510.27222
tags:
- learning
- equivariant
- experts
- expert
- invariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses redundant feature learning in equivariant
  representation learning, where separate projection heads for invariant and equivariant
  objectives independently capture shared information. The authors propose Soft Task-Aware
  Routing (STAR), which uses a Mixture-of-Experts (MoE) module with task-specific
  routers to dynamically allocate experts between invariant and equivariant learning.
---

# Soft Task-Aware Routing of Experts for Equivariant Representation Learning

## Quick Facts
- **arXiv ID:** 2510.27222
- **Source URL:** https://arxiv.org/abs/2510.27222
- **Reference count:** 40
- **Primary result:** STAR reduces redundant feature learning in equivariant representation learning, achieving state-of-the-art performance on 7/11 transfer tasks (STL10) and 10/11 (ImageNet100).

## Executive Summary
This paper addresses the problem of redundant feature learning in joint invariant and equivariant representation learning. Current approaches use separate projection heads for each task, leading to independent learning of shared information. The authors propose Soft Task-Aware Routing (STAR), which uses a Mixture-of-Experts (MoE) module with task-specific routers to dynamically allocate experts between invariant and equivariant learning. This reduces redundant feature learning and improves specialization. Experimental results show consistent improvements across 11 transfer learning datasets, with STAR achieving state-of-the-art performance in 7/11 tasks when pretrained on STL10 and 10/11 when pretrained on ImageNet100.

## Method Summary
STAR introduces an MMoE projection head during pretraining that uses task-specific routers (R_inv, R_eq) to assign weights to shared experts. The method learns invariant and equivariant representations simultaneously by minimizing a combined loss. During downstream tasks, only the backbone encoder is transferred, eliminating the transferability limitations of conventional MoE. The approach uses soft routing (softmax) rather than sparse routing for stability, and confines the MMoE structure to the projection head to preserve transfer benefits.

## Key Results
- STAR achieves state-of-the-art performance on 7/11 transfer learning datasets when pretrained on STL10
- STAR achieves state-of-the-art performance on 10/11 transfer learning datasets when pretrained on ImageNet100
- Lower canonical correlation between invariant and equivariant embeddings confirms reduced redundant feature learning
- STAR excels in object detection (Pascal VOC) and few-shot classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Soft routing enables specialized experts for shared vs. task-specific information, reducing redundancy.
- **Mechanism:** The MMoE projection uses task-specific routers (R_inv, R_eq) to assign weights (s_i,k) to shared experts (E_k). Soft routing (softmax) allows each expert to contribute proportionally, creating a gradient competition that pushes experts toward specializing in either invariant (semantic), equivariant (transformation-sensitive), or shared information. This dynamic allocation prevents all experts from learning the same features.
- **Core assumption:** Invariant and equivariant learning tasks share some underlying information (interdependence), which a shared expert can capture more efficiently than independent projection heads.
- **Evidence anchors:** Expert routing weight distribution analysis shows Experts 2-6 mainly used for invariant objective, Experts 7-8 mainly used for equivariant objective.

### Mechanism 2
- **Claim:** Lower canonical correlation between invariant and equivariant embeddings indicates successful reduction of redundant feature learning.
- **Mechanism:** By forcing experts to specialize, the model disentangles the feature sets used for the invariant and equivariant objectives. Canonical Correlation Analysis (CCA) measures the linear relationship between the learned embeddings. A lower mean canonical correlation implies the embeddings capture less common information, i.e., less redundancy.
- **Core assumption:** Lower redundancy (correlation) in the projection head space directly translates to more distinct and useful features in the backbone representation, improving downstream task generalization.
- **Evidence anchors:** Increasing number of experts tends to reduce redundant feature learning, as reflected in lower mean canonical correlation.

### Mechanism 3
- **Claim:** Confining MoE to the projection head during pretraining improves backbone gradient quality for faster convergence and better transfer.
- **Mechanism:** The MMoE is used only during pretraining and discarded for downstream tasks. The routing mechanism shapes the gradients backpropagated to the shared backbone. By reducing redundant gradients (cosine similarity 0.59 -> 0.30) from different objectives, the backbone receives higher-quality, task-specific updates.
- **Core assumption:** The projection head is a trainable bottleneck that can shape gradients; optimizing its structure (via MoE) directly impacts backbone learning efficiency.
- **Evidence anchors:** Experts in STAR converge faster with smaller Frobenius norm update compared to baselines; cosine similarity between gradients decreases from 0.59 to 0.30.

## Foundational Learning

### Concept: Canonical Correlation Analysis (CCA)
- **Why needed here:** To quantify the success of the core intervention—redundant feature learning reduction. Without this metric, the claim of reduced redundancy remains unsubstantiated.
- **Quick check question:** Can you explain why high CCA between two representations implies redundancy?

### Concept: Mixture-of-Experts (MoE) / Multi-gate Mixture-of-Experts (MMoE)
- **Why needed here:** This is the architectural core of STAR. Understanding routing, gating, and expert specialization is essential to implement or modify the method.
- **Quick check question:** How does a soft routing gate differ from a hard (top-k) gate, and why did the authors choose soft routing here?

### Concept: Invariant vs. Equivariant Representation Learning
- **Why needed here:** The method is designed specifically for the joint learning of these two types of representations. The problem (redundancy) and the solution (STAR) are defined by their relationship.
- **Quick check question:** For an image rotation task, what would an invariant feature be vs. an equivariant feature?

## Architecture Onboarding

### Component map:
- Augmented views -> Backbone -> Latent Rep -> (Split path) -> (1) Router Inv -> Weighted Sum of Experts -> Inv Embedding -> Inv Loss
- (2) Router Eq -> Weighted Sum of Experts -> Eq Embedding -> Predictor -> Eq Loss

### Critical path:
Batch of 2B augmented views -> Backbone (f) -> Latent representations -> MMoE Projection Module (Experts {E_k} + Routers R_inv, R_eq) -> Invariant/Equivariant embeddings -> Respective losses -> Total Loss = Inv Loss + lambda * Eq Loss

### Design tradeoffs:
- **Number of Experts (N):** Higher N -> More specialization, lower redundancy, higher compute/memory. Paper finds benefits up to 32 experts.
- **Routing Type:** Soft routing (used) vs. Sparse routing. Authors note sparse routing is unstable with batch normalization in SSL. Soft routing is stable but less computationally efficient.
- **Projection Head Depth/Width:** Standard MLP choices. Paper uses 3-layer MLPs for experts.

### Failure signatures:
- **Training Instability:** Check for NaNs in loss, especially if exploring sparse routing or large learning rates.
- **Expert Collapse:** Monitor routing weights. If one expert dominates (receives ~all weight) or all experts get uniform weights, specialization fails.
- **Performance Stagnation:** If downstream performance doesn't improve, check CCA. High CCA means the mechanism is not working.

### First 3 experiments:
1. **Reproduce main result:** Pretrain a ResNet-18 on STL-10 using STAR-MMoE (8 experts) vs. EquiMod baseline. Measure and plot the mean canonical correlation over training epochs to confirm the core claim of reduced redundancy.
2. **Expert specialization check:** Train the model, then pass the test set through. Visualize the average routing weights per expert for both the invariant and equivariant routers to verify experts are specializing.
3. **Downstream transfer evaluation:** Take the pretrained backbone from step 1 and perform linear evaluation on a subset of the 11 transfer datasets (e.g., CIFAR10, Food) to verify the performance gain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can sparse routing strategies (e.g., top-k routing) be effectively integrated into STAR to improve computational efficiency without destabilizing training, potentially by substituting Batch Normalization with Layer Normalization?
- **Basis in paper:** The authors explicitly state in the Limitations section that they use soft routing because sparse routing leads to unstable training due to unreliable batch statistics in Batch Normalization layers when only a subset of experts are active.
- **Why unresolved:** The current implementation forces all experts to be active to maintain stable BN statistics, sacrificing the computational efficiency usually associated with MoE models.
- **What evidence would resolve it:** A training run utilizing top-k routing with Layer Normalization that maintains training stability while demonstrating reduced FLOPs and equivalent accuracy.

### Open Question 2
- **Question:** Does applying the STAR mechanism directly to the backbone encoder, rather than restricting it to the projection heads, improve transfer learning performance?
- **Basis in paper:** The authors explicitly confine the MMoE structure to the projection head to "completely eliminate the transferability limitation of conventional MoE," suggesting that the utility of this routing for the backbone itself remains unexplored.
- **Why unresolved:** The paper focuses on transferring only the encoder; it is unknown if expert routing within the backbone would create overly specialized features that harm transferability or if it would further enhance the "richness" of the representations.
- **What evidence would resolve it:** Comparative transfer learning experiments where the backbone contains STAR-based routing layers (transferred either frozen or fine-tuned) against the baseline projection-only approach.

### Open Question 3
- **Question:** To what extent does the reduction in canonical correlation between expert embeddings causally drive the improvements in generalization, versus being a mere correlate of the training dynamics?
- **Basis in paper:** The authors validate their method by observing "lower canonical correlations" and noting a "positive correlation" between this reduction and improved mean accuracy, relying on empirical correlation rather than a causal proof.
- **Why unresolved:** While the paper establishes a link, it does not isolate whether forcing low correlation is the mechanism for success or if the routing mechanism improves gradients independently of the correlation metric.
- **What evidence would resolve it:** An ablation study that explicitly regularizes for low canonical correlation (without routing) to see if it yields similar gains, or an analysis of gradient conflicts resolved by the router independent of correlation metrics.

### Open Question 4
- **Question:** How does the performance and efficiency of STAR scale when pretraining on significantly larger datasets (e.g., ImageNet-1K) compared to the subsets (STL10, ImageNet100) used in the study?
- **Basis in paper:** The experimental scope is limited to STL10 and ImageNet100; the behavior of MoE-based pretraining often changes distinctively at larger scales compared to dense models.
- **Why unresolved:** It is unclear if the benefits of reducing redundant feature learning scale linearly with data size or if the overhead of the MMoE projection becomes negligible or prohibitive on massive datasets.
- **What evidence would resolve it:** Benchmarks of the proposed method on ImageNet-1K, reporting both linear evaluation accuracy and wall-clock training time relative to standard equivariant baselines.

## Limitations
- The exact ImageNet100 class list is not specified in the paper, requiring reference to an external source
- Specific augmentation parameter normalization constants are not provided, requiring empirical estimation
- The paper does not provide ablation studies on the impact of expert dimension size or the exact routing network architecture

## Confidence

- **High Confidence:** The core claim of reducing redundant feature learning (measured by lower CCA) and the associated performance gains in downstream tasks. This is directly supported by experimental results (Figures 4 and 5) and ablation studies (Table 2).
- **Medium Confidence:** The mechanism by which soft routing induces expert specialization. While the routing weight analysis (Figure 4a) shows experts specialize, the paper does not provide direct evidence that this specialization is the sole cause of the performance improvement, as opposed to other factors like improved gradient flow.
- **Low Confidence:** The claim that the method is "more efficient" than prior work. While STAR reduces parameters in the projection head, the paper does not provide a comprehensive computational efficiency analysis (e.g., FLOPs, wall-clock time) compared to baselines like EquiMod.

## Next Checks

1. **Verify Expert Specialization:** After training, analyze the average routing weights for each expert across the test set to confirm they are specializing into invariant, equivariant, or shared roles as claimed.
2. **Test Representation Collapse:** Systematically vary the temperature hyperparameter (τ) and the loss weight (λ) to identify the boundaries where the method fails (e.g., representation collapse or lack of specialization).
3. **Evaluate on Different Backbones:** Reproduce the main results using a smaller backbone (e.g., ResNet-18) on a smaller dataset (e.g., STL10) to verify the method's effectiveness is not dependent on the specific ResNet-50/ImageNet100 combination used in the paper.