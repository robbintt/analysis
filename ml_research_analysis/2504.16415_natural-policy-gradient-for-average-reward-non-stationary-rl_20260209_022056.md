---
ver: rpa2
title: Natural Policy Gradient for Average Reward Non-Stationary RL
arxiv_id: '2504.16415'
source_url: https://arxiv.org/abs/2504.16415
tags:
- reward
- policy
- lemma
- average
- non-stationary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of non-stationary reinforcement
  learning in the infinite-horizon average-reward setting, where the environment evolves
  over time with varying rewards and transition probabilities. The authors propose
  Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method that incorporates
  restarts for change exploration and interprets learning rates as adapting factors.
---

# Natural Policy Gradient for Average Reward Non-Stationary RL

## Quick Facts
- **arXiv ID:** 2504.16415
- **Source URL:** https://arxiv.org/abs/2504.16415
- **Reference count:** 40
- **Primary result:** Achieves $\tilde{O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ dynamic regret in infinite-horizon average-reward non-stationary RL

## Executive Summary
This paper addresses non-stationary reinforcement learning in the infinite-horizon average-reward setting, where the environment evolves over time with varying rewards and transition probabilities. The authors propose NS-NAC, a policy gradient method incorporating restarts for change exploration and interpreting learning rates as adapting factors. They also introduce BORL-NS-NAC, a parameter-free algorithm based on the bandit-over-RL framework that doesn't require prior knowledge of the variation budget. Both algorithms achieve a dynamic regret of $\tilde{O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$, where $T$ is the time horizon, $\Delta_T$ is the variation budget, and $|S|$, $|A|$ are the sizes of the state and action spaces.

## Method Summary
The NS-NAC algorithm divides the time horizon $T$ into $N = \Delta_T^{5/6}T^{1/6}$ segments of length $H$, periodically resetting the policy to uniform and value estimates to zero to force re-exploration. The algorithm uses natural actor-critic updates with TD learning for the critic and softmax policy updates for the actor. Optimal step sizes are $\alpha^* = \beta^* = (\Delta_T/T)^{1/3}$ for critic updates and $\gamma^* = (\Delta_T/T)^{1/2}$ for actor updates. The BORL-NS-NAC algorithm extends this by using EXP3.P to adaptively tune hyperparameters when the variation budget is unknown.

## Key Results
- Achieves dynamic regret bound of $\tilde{O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ in non-stationary RL
- NS-NAC matches performance of existing baseline algorithms on synthetic MDPs
- BORL-NS-NAC successfully learns optimal hyperparameters without prior knowledge of $\Delta_T$
- Empirical results validate theoretical analysis showing sub-linear regret scaling

## Why This Works (Mechanism)

### Mechanism 1: Restart-Based Exploration for Change Detection
If the environment changes non-stationarily, periodically resetting the policy and value estimates to initialization forces the agent to re-explore, preventing it from getting stuck in policies optimal only for past environments. The algorithm divides the time horizon $T$ into $N$ segments of length $H$. At the start of each segment, the policy $\pi$ is reset to uniform and the critic $Q$ to zero. This "forgetting" mechanism ensures that stale estimates derived from old transition dynamics $P_{old}$ do not permanently bias the learning of new dynamics $P_{new}$.

### Mechanism 2: Learning Rates as Adaptation Factors
Treating learning rates ($\alpha, \beta, \gamma$) as factors dependent on the variation budget $\Delta_T$ allows the agent to balance the bias-variance trade-off specific to non-stationary dynamics. By scaling step-sizes like $\alpha^* \propto (\Delta_T/T)^{1/3}$, the agent effectively increases its "plasticity" (forgetting old data faster) as the environment becomes more volatile.

### Mechanism 3: Bandit-Over-RL (BORL) for Hyperparameter Tuning
If the variation budget $\Delta_T$ is unknown, an outer adversarial bandit loop can adaptively tune the step-sizes and restart frequency by treating specific configurations as "arms." The BORL-NS-NAC algorithm runs EXP3.P over a discrete set of hyperparameter configurations (arms). It splits time into epochs, runs NS-NAC with the selected configuration, and uses the observed cumulative reward as feedback to update the bandit prior.

## Foundational Learning

- **Concept: Natural Actor-Critic (NAC)**
  - Why needed: This is the base optimizer. Standard gradients can be slow or unstable; NAC uses the Fisher Information Matrix (second-order information) to re-scale the gradient, which is critical for stabilizing the policy in the infinite-horizon setting.
  - Quick check: Can you explain why the natural gradient $F^{-1}\nabla J$ might be preferred over the standard gradient $\nabla J$ when the policy parameterization involves softmax? (Hint: Think of the geometry of probability distributions).

- **Concept: Dynamic Regret**
  - Why needed: This is the performance metric. Unlike static regret (comparing to the best fixed policy), dynamic regret compares the agent to the *current optimal policy* $\pi_t^\star$ at every step. Understanding this distinction is key to understanding why the algorithm must "chase" a moving target.
  - Quick check: Why is sub-linear dynamic regret harder to achieve than sub-linear static regret in a non-stationary environment?

- **Concept: Variation Budget ($\Delta_T$)**
  - Why needed: This quantifies the difficulty of the problem. It sums the total variation in rewards and transitions over time. It acts as the "hardness parameter" that dictates the optimal learning rates.
  - Quick check: If $\Delta_T = 0$, what does the optimal step-size strategy imply for the learning rates? (Answer: They should likely shrink to 0 over time, typical of stationary stochastic approximation).

## Architecture Onboarding

- **Component map:**
  1. Environment Wrapper: Handles the non-stationary MDP (time-varying $P_t, r_t$)
  2. Learner (NS-NAC):
      * Critic (Fast Timescale): Updates Q-values and average reward estimate $\eta$ via TD-learning
      * Actor (Slow Timescale): Updates Policy via Natural Gradient (softmax update)
  3. Scheduler:
      * Restart Logic: Maintains counter for segment length $H$. Triggers reset of weights to initial values
      * BORL (Optional): Meta-controller running EXP3.P. Selects $\alpha, \beta, N$ from a discrete grid $\mathcal{T}$

- **Critical path:**
  1. Observe state $s_t$
  2. Restart Check: If $t \pmod H == 0$, reset $\pi \to \text{Uniform}, Q \to 0$
  3. Action: Sample $a_t \sim \pi(\cdot|s_t)$
  4. Transition: Receive $r_t, s_{t+1}$
  5. Critic Update: Update $Q(s_t, a_t)$ and average reward $\eta$
  6. Actor Update: Update $\pi$ using the projected natural gradient step
  7. BORL Update (if applicable): If end of bandit epoch, update probability distribution over hyperparameter arms

- **Design tradeoffs:**
  * Segment Length ($H$) vs. Restart Count ($N$): A large $N$ (short $H$) improves responsiveness to changes but increases the cost of "cold starts" (learning from scratch). The optimal balance is $N^* = \Delta_T^{5/6}T^{1/6}$
  * Step-sizes ($\alpha, \beta$): High step-sizes track changes faster but introduce noise. The paper suggests $\beta^* = (\Delta_T/T)^{1/2}$

- **Failure signatures:**
  * Linear Regret: Check if the restart interval $H$ is too large relative to the frequency of environmental shifts
  * Critic Divergence: The paper uses a projection radius $R_Q$ for the critic. If $Q$-values explode, the projection is likely too tight or the non-stationarity is violating the ergodicity assumptions
  * BORL Stagnation: If the bandit loop fails to converge, ensure the reward scaling for the bandit feedback matches the scale of the RL rewards

- **First 3 experiments:**
  1. Stationary Baseline: Run NS-NAC on a fixed MDP. Verify that it reduces to standard NAC behavior (regret stabilizes/converges)
  2. Sensitivity to $\Delta_T$: Run on synthetic MDPs varying the number of switches. Plot Dynamic Regret vs. $\Delta_T$ on a log-log scale to verify the $\Delta_T^{1/6}$ scaling
  3. BORL Ablation: Run BORL-NS-NAC against NS-NAC with "oracle" knowledge of $\Delta_T$. Measure the gap in performance to assess the cost of not knowing the variation budget

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dynamic regret upper bound of $\tilde{O}(T^{5/6})$ be tightened to match the lower bound of $\Omega(T^{2/3})$ for model-free policy-based methods?
- **Basis in paper:** The authors state, "We conjecture that the gap between the bounds results from a slack in the analysis of the underlying Natural Actor-Critic (NAC) algorithm," specifically citing a mismatch where the actor requires the norm of critic error while the critic establishes bounds on the norm-squared error.
- **Why unresolved:** The current proof technique involves a "squaring trick" that introduces slack, and it remains unproven whether this gap is an artifact of the analysis or a fundamental limitation of single-loop natural policy gradient methods in non-stationary settings.
- **What evidence would resolve it:** A refined analysis of the Natural Actor-Critic that establishes linear convergence for the critic error (rather than squared error) or a modified algorithm that bridges the $T^{5/6}$ and $T^{2/3}$ gap.

### Open Question 2
- **Question:** Can the theoretical guarantees of NS-NAC be extended to general function approximation (e.g., deep neural networks) without relying on compatible function approximation?
- **Basis in paper:** Appendix E extends the algorithm to the function approximation setting but explicitly relies on **Assumption E.3**, which requires "compatible function approximation" where feature vectors exactly match the gradient of the log-policy.
- **Why unresolved:** The theoretical bounds depend on the linearity and compatibility of features to ensure the critic approximates the true value function well. Without this, the approximation error $\epsilon_{app}$ and distribution shift in non-stationary environments become difficult to bound.
- **What evidence would resolve it:** A regret bound for NS-NAC that holds for general smooth function classes (like neural networks) or an empirical demonstration of convergence guarantees under relaxed approximation conditions.

### Open Question 3
- **Question:** Is the assumption of Uniform Ergodicity necessary to achieve sub-linear dynamic regret, or can the analysis be adapted to environments with non-uniform or time-varying mixing times?
- **Basis in paper:** **Assumption 5.1** requires that Markov chains induced by *all* policies in *all* environments are uniformly ergodic. This implies the environment's mixing properties cannot degrade over time.
- **Why unresolved:** The constants derived from the mixing time ($m, \rho$) are fixed globally in the regret analysis (Theorem 5.3). It is unclear if the restart mechanism would suffice if the environment evolved to have significantly slower mixing dynamics later in the horizon.
- **What evidence would resolve it:** A regret analysis that explicitly incorporates time-varying mixing parameters into the bound, or counter-examples showing that non-uniform ergodicity leads to linear regret for the current NS-NAC configuration.

## Limitations
- The theoretical analysis critically depends on the bounded projection radius $R_Q = 2U_R/\lambda$, where $\lambda$ is the maximum eigenvalue of the steady-state distribution matrix, which is environment-specific and not specified how to estimate
- The BORL hyperparameter search grid details are not fully specified, making exact replication challenging
- The analysis assumes uniform ergodicity of all policies across all environments, which may not hold for complex real-world non-stationary systems

## Confidence

- **High Confidence:** The core mechanism of restart-based exploration for change detection is well-established in non-stationary RL literature and the theoretical regret bound derivation follows standard Lyapunov analysis techniques.
- **Medium Confidence:** The interpretation of learning rates as adaptation factors and their optimal scaling with $\Delta_T$ is theoretically sound but requires careful empirical tuning in practice.
- **Medium Confidence:** The BORL-NS-NAC algorithm provides a practical solution for unknown variation budgets, though the bandit feedback mechanism's effectiveness depends on the smoothness of non-stationarity.

## Next Validation Checks

1. **Parameter Sensitivity:** Run ablation studies varying the projection radius $R_Q$ and restart frequency $N$ to identify break points where performance degrades.
2. **Unknown $\Delta_T$ Scenario:** Implement the BORL-NS-NAC algorithm with varying epoch lengths $W$ to test robustness when the variation budget is not known a priori.
3. **Theoretical Assumptions:** Verify the ergodicity and bounded eigenvalue assumptions on synthetic MDPs by computing empirical $\lambda$ values and testing whether the projected TD updates remain stable across different non-stationary patterns.