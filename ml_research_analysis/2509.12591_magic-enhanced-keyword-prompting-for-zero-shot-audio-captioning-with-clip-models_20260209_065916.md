---
ver: rpa2
title: MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models
arxiv_id: '2509.12591'
source_url: https://arxiv.org/abs/2509.12591
tags:
- audio
- keyword
- magic
- captioning
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a zero-shot automated audio captioning system
  that leverages pre-trained audio CLIP models to extract auditory features and generate
  structured prompts, which guide a Large Language Model (LLM) in caption generation.
  Unlike traditional greedy decoding, the method refines token selection through the
  audio CLIP model to ensure alignment with the audio content.
---

# MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models

## Quick Facts
- **arXiv ID:** 2509.12591
- **Source URL:** https://arxiv.org/abs/2509.12591
- **Reference count:** 40
- **Primary result:** 35% improvement in NLG mean score (from 4.7 to 7.3) using MAGIC search with WavCaps model

## Executive Summary
This paper introduces a zero-shot automated audio captioning system that leverages pre-trained audio CLIP models and large language models (LLMs) to generate text descriptions for audio clips without any training. The approach combines keyword-guided prompting with an audio-guided MAGIC search algorithm to refine token selection during decoding. Experimental results demonstrate significant improvements over baseline methods, achieving a 35% increase in NLG mean score while maintaining zero-shot capabilities. The method shows particular effectiveness when using a single keyword prompt and the WavCaps audio CLIP model.

## Method Summary
The proposed method employs a zero-shot keyword selection (ZSKS) approach that uses audio CLIP models to extract auditory features from audio clips and compute cosine similarity against keyword embeddings to select relevant keywords. These keywords are then incorporated into a Socratic-style prompt ("This is a sound of [keyword]") to guide the LLM. The MAGIC search algorithm generates top-k candidate tokens at each decoding step and re-ranks them using a combination of confidence scores, degeneration penalties, and MAGIC scores (cosine similarity between candidate sequence embedding and audio embedding). The system achieves zero-shot performance by relying entirely on pre-trained models without any task-specific training.

## Key Results
- Achieved 35% improvement in NLG mean score (from 4.7 to 7.3) compared to baseline methods
- Optimal performance achieved using a single keyword prompt, with 50% performance drop when no keyword list is used
- WavCaps audio CLIP model outperformed other models, demonstrating the importance of audio-text matching quality
- The approach surpasses baseline methods while remaining behind fully supervised AAC models

## Why This Works (Mechanism)
The method works by aligning audio content with textual descriptions through the audio CLIP model's learned representations. By extracting auditory features and comparing them to keyword embeddings, the system identifies relevant semantic concepts present in the audio. The MAGIC search then ensures that generated tokens remain semantically consistent with the audio content throughout the decoding process. This alignment between audio features and generated text, guided by carefully selected keywords, enables coherent and contextually relevant caption generation without any training data.

## Foundational Learning
- **Audio CLIP Models**: Pre-trained models that learn joint audio-text representations, enabling cross-modal retrieval and similarity computation
- **MAGIC Search Algorithm**: A decoding strategy that re-ranks candidate tokens based on multiple criteria to improve generation quality
- **Zero-Shot Learning**: Ability to perform tasks without task-specific training by leveraging pre-trained models and prompt engineering
- **Cosine Similarity in Embedding Space**: Measures semantic similarity between audio and text representations in a shared vector space
- **Keyword Prompting**: Strategy of incorporating relevant keywords into prompts to guide LLM generation
- **Degeneration Penalty**: Mechanism to prevent repetitive or nonsensical output during decoding

## Architecture Onboarding

**Component Map:** Audio CLIP Model → Keyword Selector → Prompt Generator → LLM → MAGIC Decoder → Caption Output

**Critical Path:** Audio → Audio CLIP → Keyword Selection → Prompt Construction → LLM Generation → MAGIC Search → Final Caption

**Design Tradeoffs:**
- Zero-shot capability vs. performance gap compared to supervised models
- Keyword list comprehensiveness vs. noise and hallucination risk
- MAGIC search computational cost vs. generation quality improvement
- Audio CLIP model selection (WavCaps vs. HTSAT) affecting overall performance

**Failure Signatures:**
- Repetitive or gibberish output (e.g., "Happyasdfasdfchildren") indicating degeneration penalty issues
- Hallucinated content not present in audio, suggesting keyword selection problems
- Performance drops with expanded keyword lists, indicating semantic drift or noise

**First Experiments:**
1. Implement audio CLIP model (HTSAT or WavCaps) to extract features from AudioCaps test set
2. Build keyword selector using cosine similarity between audio and keyword embeddings
3. Implement MAGIC search decoding loop with top-k candidates and re-ranking

## Open Questions the Paper Calls Out

**Cross-Dataset Generalization:** The paper notes that the 10-second preprocessing alignment may discard critical sound events in longer Clotho samples, raising questions about handling variable-length audio without losing important content.

**Keyword List Optimization:** While expanded GPT-3.5 Turbo keyword lists (1987 classes) showed suboptimal performance compared to AudioSet (512 classes), the reasons for this discrepancy remain unclear, potentially due to semantic drift or noise in the expanded list.

**Performance Gap Analysis:** The paper acknowledges that the zero-shot approach remains behind task-specific AAC models, but doesn't identify whether this gap stems primarily from audio encoder limitations or LLM alignment issues.

## Limitations

- **LLM Dependency:** The specific LLM architecture used is not explicitly identified, creating uncertainty about exact replication requirements
- **Keyword Sensitivity:** Performance heavily depends on keyword selection quality, with significant drops when keyword lists are absent or suboptimal
- **Audio Length Constraints:** The 10-second preprocessing requirement may discard critical information from longer audio samples

## Confidence

- **High Confidence:** Core methodology combining audio CLIP features with MAGIC search is well-defined and reproducible
- **Medium Confidence:** Implementation details for MAGIC search and keyword selection are sufficiently specified
- **Low Confidence:** Exact LLM architecture and gamma hyperparameter values are not explicitly provided

## Next Checks

1. **LLM Architecture Verification:** Confirm whether GPT-2 or another LLM was actually used in experiments, as this affects MAGIC search implementation
2. **Hyperparameter Sensitivity Analysis:** Systematically test different gamma and beta values to understand their impact on performance
3. **Cross-Dataset Evaluation:** Test the approach on additional audio datasets beyond AudioCaps and Clotho to assess robustness across domains