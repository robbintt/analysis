---
ver: rpa2
title: 'BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in
  Language Model Pretraining'
arxiv_id: '2510.06048'
source_url: https://arxiv.org/abs/2510.06048
tags:
- data
- training
- proxy
- selection
- bilevel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BLISS, a lightweight bilevel influence scoring
  method for data selection in language model pretraining. The method operates from
  scratch without relying on external pretrained models and explicitly accounts for
  the long-term impact of selected data by training a proxy model to convergence.
---

# BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining

## Quick Facts
- arXiv ID: 2510.06048
- Source URL: https://arxiv.org/abs/2510.06048
- Authors: Jie Hao; Rui Yu; Wei Zhang; Huixia Wang; Jie Xu; Mingrui Liu
- Reference count: 34
- Primary result: 1.7× speedup in pretraining performance using 20% of C4 dataset

## Executive Summary
BLISS introduces a lightweight bilevel optimization framework for data selection in language model pretraining. The method learns importance weights for training samples by training a proxy model to convergence and aligning its preferences with a target LLM via KL divergence. Unlike prior work that evaluates single-step influence, BLISS explicitly captures long-term data impact through nested optimization. Experiments demonstrate that BLISS achieves comparable performance to state-of-the-art methods while using only 20% of the training data and providing 1.7× speedup under the 1B model setting.

## Method Summary
BLISS formulates data selection as a bilevel optimization problem where a score model learns to assign importance weights to training samples based on their influence on downstream performance. The lower-level optimizes a proxy model on weighted training loss plus KL divergence alignment to a frozen target LLM until convergence. The upper-level trains the score model via hypergradient descent to minimize validation loss. The method operates from scratch without external oracle models, using only the pretraining data and validation set. During training, BLISS iterates through warmup, bilevel optimization rounds, data selection, and LLM pretraining phases, ultimately achieving efficient data utilization while maintaining downstream performance.

## Key Results
- 1.7× speedup in reaching target performance compared to state-of-the-art methods under 1B model setting
- Consistently outperforms baselines across 9 downstream tasks including ARC, LogiQA, and BoolQ
- Achieves comparable performance to full-data training using only 20% of C4 dataset
- Single-level optimization degrades accuracy by 0.5% versus bilevel approach

## Why This Works (Mechanism)

### Mechanism 1: Bilevel Optimization for Learning Data Importance
- Claim: Bilevel optimization allows the score model to learn which samples benefit downstream performance when the proxy model is trained to convergence
- Mechanism: Upper-level optimizes score model θs to minimize validation loss; lower-level trains proxy model θp on weighted training loss (weights from score model) until convergence. Hypergradient backpropagates through converged proxy model to update score model.
- Core assumption: Proxy model's data preferences at convergence generalize to full LLM
- Evidence: Bilevel formulation in Section 4.1, Eq. 1; 0.5% accuracy degradation when using single-level update (Section 6.1)

### Mechanism 2: KL Divergence Alignment Between Proxy and LLM
- Claim: Aligning proxy model output logits with LLM via KL divergence ensures proxy's learned data preferences transfer to target model
- Mechanism: Lower-level loss includes γ·D_KL(ℓ(θp)||ℓ(θtr)) term, forcing proxy to mimic LLM's output distribution
- Core assumption: Output distribution alignment correlates with data preference alignment across training trajectories
- Evidence: +9.3% on LogiQA and +1.4% average accuracy when including KL term (Section 6.2)

### Mechanism 3: Long-Term Influence via Convergence Training
- Claim: Training proxy model to convergence captures cumulative effects of data selection, unlike single-step influence methods
- Mechanism: Lower-level optimization runs until θ*p(θs) reaches convergence, then upper-level evaluates this converged state
- Core assumption: Convergence dynamics of small proxy model approximate those of full LLM
- Evidence: Contrast with MATES which "evaluates impact of individual training samples based on single training step" (Section 1)

## Foundational Learning

- **Concept**: Bilevel Optimization
  - Why needed here: Core mathematical framework; requires understanding hypergradients, implicit differentiation, and nested optimization
  - Quick check question: Can you derive why computing ∇θs Φ(θs) requires solving for lower-level optimal solution θ*p(θs) first?

- **Concept**: Influence Functions for Data Selection
  - Why needed here: BLISS is influence-based method; understanding how data affects model behavior underpins scoring mechanism
  - Quick check question: Explain why single-step influence estimation may miss cumulative effects that convergence-based methods capture

- **Concept**: Knowledge Distillation via KL Divergence
  - Why needed here: Aligns proxy model with LLM; requires understanding soft targets and distribution matching
  - Quick check question: Why might minimizing KL divergence to undertrained LLM temporarily hurt proxy performance but still improve data selection?

## Architecture Onboarding

- **Component map**: Score Model (θs) → Proxy Model (θp) → Target LLM (θtr) → Validation Loss → Hypergradient → Score Model

- **Critical path**:
  1. Warmup: Train proxy, score, and LLM on random data (~10k steps)
  2. Bilevel Optimization (per round): Train proxy + score for 3k steps; score model learns to weight samples
  3. Scoring: Infer influence scores on full training shard; select top 20%
  4. Pretrain: Train LLM on selected data (~10k steps)
  5. Repeat: 5 rounds total, one shard per round

- **Design tradeoffs**:
  - Proxy size vs. fidelity: 31M proxy works for 410M LLM; larger (160M) provides marginal gains but higher cost (Table 6)
  - Bilevel steps vs. convergence: 3k steps sufficient for Pythia; 5k yields no additional gain (Figure 8)
  - KL coefficient (γ): Too low → misaligned preferences; too high → proxy overfits to imperfect LLM logits
  - Memory overhead: HVP computation in bilevel optimization increases peak memory 36-50% vs. single-level (Table 10)

- **Failure signatures**:
  - Validation loss increases during bilevel training: Normal; KL term dominates initially (Section 5.3). Should stabilize by round 2
  - Selected data degrades downstream performance: Check proxy-LLM alignment; verify KL divergence is <0.2 (Figure 4)
  - Distributed softmax NaN/Inf: Ensure gradient synchronization across GPUs (Appendix G)
  - Score model collapses to uniform: Check learning rate (should be 1e-5) and verify softmax denominator is computed globally

- **First 3 experiments**:
  1. Proxy fidelity check: Train LLaMA-134M proxy with/without KL alignment to LLaMA-300M; compare domain weight learning curves (replicate Figure 10)
  2. Ablate softmax reparameterization: Run BLISS with raw sigmoid scores vs. softmax-normalized weights on 410M/10B token setting
  3. Single-round pilot: Run 1 round of BLISS on C4 shard with 10k warmup + 3k bilevel; measure time and memory

## Open Questions the Paper Calls Out
- How can the peak memory usage during Hessian-Vector-Product (HVP) calculation in BLISS's bilevel optimization be reduced without compromising data selection quality?
- What is the optimal relationship between proxy model size and target LLM size for maintaining fidelity while minimizing computational overhead?
- How robust is BLISS to the choice of validation dataset, and can a principled method for validation set selection be developed?

## Limitations
- Higher peak memory usage (74.51 GB for 1B target vs 63.52 GB for MATES) due to bilevel optimization overhead
- Limited exploration of proxy-to-target size scaling relationships
- Sensitivity to validation dataset choice without principled selection criteria

## Confidence

- **High Confidence**: Bilevel optimization framework is correctly implemented and empirical speedups (1.7×) and downstream improvements are reproducible
- **Medium Confidence**: KL divergence alignment mechanism provides measurable benefit and convergence-based influence is superior to single-step methods
- **Low Confidence**: Claims about "long-term influence" being fundamentally better than alternatives are supported by relative performance but lack isolation of convergence effect

## Next Checks

1. **Scale Sensitivity Test**: Run BLISS with proxy sizes ranging from 10M to 200M on same 410M target to quantify relationship between proxy fidelity and downstream performance

2. **Alternative Influence Baselines**: Implement single-step influence method (MATES-style) but with KL alignment to LLM, isolating whether convergence or alignment drives performance gains

3. **Cross-Dataset Generalization**: Apply BLISS to non-C4 dataset (e.g., OpenWebText or RedPajama) to test whether proxy-LLM alignment and bilevel optimization generalize beyond original training distribution