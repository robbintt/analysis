---
ver: rpa2
title: 'Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment'
arxiv_id: '2601.00908'
source_url: https://arxiv.org/abs/2601.00908
tags:
- feature
- tasks
- coverage
- shift
- concentration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how conformal prediction degrades under distribution\
  \ shift using COVID-19 as a natural experiment across 8 supply chain tasks. Despite\
  \ identical severe feature turnover (Jaccard \u22480), coverage drops vary from\
  \ 0% to 86.7%, spanning two orders of magnitude."
---

# Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment

## Quick Facts
- arXiv ID: 2601.00908
- Source URL: https://arxiv.org/abs/2601.00908
- Authors: Chorok Lee
- Reference count: 17
- Primary result: Coverage drops vary 0% to 86.7% across tasks with identical feature turnover

## Executive Summary
This paper investigates conformal prediction degradation under distribution shift using COVID-19 as a natural experiment across 8 supply chain tasks. Despite all tasks experiencing identical severe feature turnover (Jaccard ≈0), coverage drops vary dramatically from 0% to 86.7%, spanning two orders of magnitude. The study reveals that catastrophic failures correlate with single-feature dependence, where vulnerable tasks concentrate importance in one feature while robust tasks redistribute importance across many features. Quarterly retraining restores catastrophic task coverage from 22% to 41% but provides no benefit for robust tasks.

## Method Summary
The study analyzes 8 supply chain tasks during COVID-19, measuring feature drift using Jaccard similarity (all ≈0) and coverage degradation across three conformal prediction methods. SHAP values are used to quantify feature importance concentration, revealing that catastrophic failures correlate with single-feature dependence. A quarterly retraining experiment tests whether model updating can restore coverage, showing differential benefits depending on task vulnerability. The analysis is complemented by exploratory evaluation of 4 additional tasks with moderate feature stability.

## Key Results
- Coverage drops vary from 0% to 86.7% across tasks with identical severe feature turnover (Jaccard ≈0)
- Catastrophic failures correlate with single-feature dependence (ρ = 0.714, p = 0.047)
- Vulnerable tasks concentrate importance in one feature (4.5x increase) while robust tasks redistribute across many (10-20x)
- Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04)
- Retraining provides no benefit for robust tasks (99.8% coverage maintained)

## Why This Works (Mechanism)
The paper demonstrates that conformal prediction degradation under distribution shift is not uniform but depends on how model features redistribute importance. When a single feature dominates predictions, any shift in that feature's distribution causes catastrophic coverage loss. In contrast, when importance is distributed across multiple features, the model can maintain coverage even under severe feature turnover. This mechanism explains why identical feature drift can lead to vastly different coverage outcomes.

## Foundational Learning
- **Conformal Prediction**: Provides distribution-free uncertainty quantification for predictions
  - Why needed: Establishes baseline method for measuring coverage degradation
  - Quick check: Verify coverage at expected level under no shift

- **SHAP Values**: Quantify feature importance in model predictions
  - Why needed: Enables measurement of feature concentration across tasks
  - Quick check: Confirm feature importance sums to 1 across all features

- **Jaccard Similarity**: Measures feature overlap between distributions
  - Why needed: Quantifies severity of feature drift across tasks
  - Quick check: Values near 0 indicate severe feature turnover

- **Feature Concentration**: Ratio of top feature importance to total importance
  - Why needed: Captures vulnerability to distribution shift
  - Quick check: Higher values indicate single-feature dependence

## Architecture Onboarding
- **Component Map**: Data → Conformal Prediction → Coverage Metrics → SHAP Analysis → Retraining Decision
- **Critical Path**: Feature drift detection → Coverage monitoring → Feature concentration analysis → Retraining decision
- **Design Tradeoffs**: Quarterly retraining balances computational cost against coverage restoration
- **Failure Signatures**: Single-feature dependence, coverage drops >40%, SHAP concentration >0.4
- **First Experiments**: 1) Measure Jaccard similarity across all features, 2) Calculate SHAP concentration for each task, 3) Test coverage sensitivity to feature perturbation

## Open Questions the Paper Calls Out
None

## Limitations
- COVID-19 supply chain context may not generalize to other domains
- Analysis focuses on aggregate coverage without instance-level prediction quality
- SHAP-based analysis captures only one dimension of model vulnerability
- Quarterly retraining experiment lacks exploration of alternative frequencies

## Confidence
- High: Coverage drop variation (0% to 86.7%) across tasks with identical feature turnover, and statistical relationship between feature concentration and catastrophic failures
- Medium: Decision framework recommendations given single-domain experiment and limited retraining strategy exploration
- Low: Generalization to other distribution shift types and assumption that feature concentration is primary determinant of robustness

## Next Checks
1. Test the feature concentration-coverage relationship across multiple domains and types of distribution shift
2. Evaluate alternative retraining strategies (adaptive frequency, partial retraining, online learning) and their impact on both catastrophic and robust tasks
3. Conduct ablation studies to identify additional failure modes beyond feature concentration, such as prediction variance or error pattern changes