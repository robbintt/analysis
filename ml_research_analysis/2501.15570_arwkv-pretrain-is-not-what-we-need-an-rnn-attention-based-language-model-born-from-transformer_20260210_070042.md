---
ver: rpa2
title: 'ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model
  Born from Transformer'
arxiv_id: '2501.15570'
source_url: https://arxiv.org/abs/2501.15570
tags:
- attention
- arxiv
- stage
- state
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents ARWKV, a novel RNN-attention-based language
  model that achieves competitive performance through knowledge distillation from
  Qwen 2.5. The approach replaces standard transformer self-attention with RWKV-7
  time mixing modules while preserving expressiveness through attention alignment
  during training.
---

# ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer

## Quick Facts
- arXiv ID: 2501.15570
- Source URL: https://arxiv.org/abs/2501.15570
- Authors: Lin Yueyu; Li Zhiyuan; Peter Yue; Liu Xiao
- Reference count: 7
- Primary result: RNN-attention model distilled from Qwen 2.5 achieves MMLU 62.41, GSM8K 39.95 using only 16 MI300X GPUs in 8 hours

## Executive Summary
This work presents ARWKV, a novel RNN-attention-based language model that achieves competitive performance through knowledge distillation from Qwen 2.5. The approach replaces standard transformer self-attention with RWKV-7 time mixing modules while preserving expressiveness through attention alignment during training. The 7B parameter model is distilled from Qwen 2.5-7B-Instruct using a three-stage process: attention alignment, knowledge distillation, and fine-tuning. Notably, the entire knowledge processing pipeline can be completed in just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance characteristics.

## Method Summary
The method distills Qwen 2.5 into ARWKV through three stages: (1) Attention alignment where TimeMixer learns to match teacher attention outputs through hidden state L2 minimization while MLP layers remain frozen, (2) Knowledge distillation using word-level KL divergence with optional gate mechanism and frozen/active MLP variants, and (3) SFT plus DPO fine-tuning for context extension. The RWKV-7 time mixing module replaces self-attention with a state matrix formulation that expands eigenvalue range for stronger state tracking. The entire pipeline processes knowledge from larger models to smaller ones with fewer tokens, making it practical for academic use with minimal GPU requirements.

## Key Results
- MMLU score of 62.41 and GSM8K score of 39.95 achieved by 7B parameter distilled model
- Complete knowledge processing pipeline completed in 8 hours using 16 AMD MI300X GPUs
- Gate-free variant outperforms gated versions, demonstrating convergence advantages
- Frozen MLP strategy works for same-size transfers but shows limitations for 32B→7B capacity mismatches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning RWKV-7 time mixing outputs to teacher attention hidden states enables functional attention transfer without initializing from teacher weights.
- Mechanism: The student TimeMixer is trained to minimize L2 distance to teacher attention outputs (Equation 5), forcing the RNN state matrix to approximate quadratic attention behavior. The residual difference between self-attention and TimeMixer is combined with original self-attention output during training, creating a progressive optimization target.
- Core assumption: The RWKV-7 state matrix (Equation 3) has sufficient representational capacity to approximate the attention patterns learned by group query attention in the teacher model.
- Evidence anchors: [abstract] "replacing Transformer self-attention with RWKV-7's time mixing module, enabling efficient state tracking while preserving attention expressiveness"; [section 3.1] "we align the hidden state output between the student and teacher attention block... we found that initializing state attention from teacher's attention is not necessary"

### Mechanism 2
- Claim: Freezing MLP layers during attention transfer preserves previously learned representations while allowing the time mixing module to specialize.
- Mechanism: By keeping MLP weights fixed and removing group normalization from attention output, the optimization focuses exclusively on learning the mapping from self-attention to time mixing. The gate initialized to 1 allows gradient flow through both paths initially.
- Core assumption: MLP representations learned during pretraining are transferable to the new attention mechanism without further adaptation.
- Evidence anchors: [abstract] "models distilled without gating mechanisms and with frozen MLPs achieve competitive performance"; [section 4] "analysis of Table 1 reveals that knowledge distillation from the 32B parameter model, when performed without gating mechanisms and with frozen MLPs, yields suboptimal results... attributed to the limited capacity of the 7B model's MLP layers"

### Mechanism 3
- Claim: RWKV-7's expanded eigenvalue range in its transition matrix enables stronger state tracking than prior linear RNNs, approaching transformer-level context learning.
- Mechanism: The transition matrix formulation (Equation 3) with in-context learning rate parameter a_t allows dynamic modulation of state updates, unlike RWKV-6's simpler diagonal decay. This enables the model to maintain more precise sequential dependencies.
- Core assumption: The eigenvalue distribution of the learned transition matrices correlates with state tracking benchmarks like passkey retrieval.
- Evidence anchors: [abstract] "demonstrates state tracking ability beyond transformers"; [section 1] "RWKV-7 introduced a promising architecture, where a 0.1B parameter model achieved perfect results in 16k passkey retrieval... With its transition matrix having wider eigenvalues"

## Foundational Learning

- Concept: **Knowledge Distillation (Hidden State Alignment)**
  - Why needed here: Stage 1 requires understanding how to train a student network to match intermediate representations of a teacher, not just output probabilities.
  - Quick check question: Can you explain why minimizing hidden state L2 distance might preserve more structural information than KL divergence on outputs?

- Concept: **Linear RNN State Dynamics**
  - Why needed here: RWKV-7's state update equation differs from standard RNNs; understanding how the transition matrix and in-context learning rate interact is essential for debugging convergence.
  - Quick check question: How does Equation 3 differ from a standard RNN hidden state update, and what does the term (a_t ⊙ κ̂_t) enable?

- Concept: **Attention as Optimal Transport**
  - Why needed here: The paper frames attention transfer as a "compression process" and "map between probability measures" - this theoretical lens helps understand why alignment works.
  - Quick check question: If attention outputs are viewed as probability distributions, what constraints does the RWKV-7 state size impose on this mapping?

## Architecture Onboarding

- Component map: Input → RMSNorm → [AttentionWrapper: Self-Attention || TimeMixer] → Residual Add → SwiGLU MLP → Output
- Critical path:
  1. Stage 1: Train TimeMixer only (frozen MLP), L_special loss, 20M tokens, 2048 context
  2. Stage 2: Full model distillation with/without gate, 40M tokens
  3. Stage 3: SFT + DPO for context extension, 770M tokens
- Design tradeoffs:
  - Gate vs Gate-free: Gate mechanism may hinder convergence; paper recommends gate-free
  - Frozen vs Active MLP: Frozen preserves stability but limits 32B→7B transfer; active MLP helps capacity-limited students
  - Teacher size: 7B→7B easier than 32B→7B due to MLP capacity mismatch
- Failure signatures:
  - Stage 1 loss plateau: Check state dimension sufficiency for target attention patterns
  - 32B→7B underperformance: MLP capacity bottleneck; consider active MLP or intermediate teacher
  - FP16 overflow at inference: Unlike original RWKV, ARWKV uses BF16 training but FP16 inference improves results
- First 3 experiments:
  1. Reproduce Stage 1 with 7B teacher on single GPU: Verify loss convergence matches Figure 5 trajectory (~18 hours on 8×H800 for 4B tokens).
  2. Ablate gate mechanism: Train ARWKV-M vs ARWKV-G-M and compare MMLU/GSM8K deltas to validate gate-free advantage.
  3. Test FP16 vs BF16 inference: Run evaluation with both precisions to confirm the unexpected FP16 improvement reported in Section 4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed distillation methodology effectively replicate advanced reasoning capabilities (similar to DeepSeek-R1) through Stage 3 post-training?
- Basis in paper: [explicit] The authors state in the "Future Work" section: "For our subsequent phase of investigation, we will implement Stage 3 post-training to replicate the reasoning capabilities demonstrated by deepseek-R1 models."
- Why unresolved: The current paper primarily details the architectural alignment (Stages 1-2) and evaluates foundational benchmarks, whereas reasoning capabilities require complex reinforcement learning or specialized SFT not yet detailed.
- What evidence would resolve it: Evaluation results on reasoning-heavy benchmarks (e.g., MATH, GPQA) for the model after completing Stage 3 training.

### Open Question 2
- Question: Does the attention alignment and distillation process transfer successfully to Mixture-of-Experts (MoE) or multimodal architectures?
- Basis in paper: [explicit] The "Future Work" section explicitly proposes generalizing the methodology to "diverse architectural paradigms, encompassing Mixture-of-Experts (MoE) frameworks, multimodal architectures [and] hybrid architectures."
- Why unresolved: The current experimental validation is restricted to dense Transformer-to-RNN transfers (specifically Qwen 2.5), leaving the interaction with MoE routing or multimodal encoders untested.
- What evidence would resolve it: Successful training logs and performance metrics of an MoE-based RWKV variant or a multimodal variant constructed using this distillation pipeline.

### Open Question 3
- Question: How can the MLP capacity bottleneck be mitigated when distilling attention patterns from significantly larger teacher models to smaller students?
- Basis in paper: [inferred] The analysis of Table 1 notes that distilling from 32B to 7B with frozen MLPs yields suboptimal results, likely due to the "limited capacity of the 7B model's MLP layers to effectively accommodate... attention patterns" of the larger model.
- Why unresolved: The paper identifies this architectural mismatch and performance degradation as an observation but does not propose or test a specific mechanism (e.g., adaptive MLP scaling) to solve the capacity gap.
- What evidence would resolve it: An ablation study testing varying MLP widths or unfreezing strategies specifically for the 32B-to-7B transfer to recover lost performance.

## Limitations

- Critical implementation details missing: Exact datasets, learning rate schedules, batch sizes, and optimizer configurations for each distillation stage are not specified
- Capacity transfer assumptions: Frozen MLP strategy shows clear limitations when transferring from 32B to 7B models, but the paper doesn't provide quantitative analysis of this bottleneck
- Theoretical framing gaps: The "optimal transport" and "compression between probability measures" framing lacks rigorous mathematical substantiation

## Confidence

**High Confidence Claims**:
- The three-stage distillation framework (attention alignment → knowledge distillation → fine-tuning) is well-documented and produces measurable results
- The gate-free variant outperforms gated versions, supported by experimental results showing MMLU improvements
- MLP freezing strategy works effectively for same-size teacher-student pairs (7B→7B) but fails for large capacity mismatches (32B→7B)

**Medium Confidence Claims**:
- The RWKV-7 time mixing module can approximate transformer attention patterns sufficiently for competitive performance
- The eigenvalue range expansion in RWKV-7 enables stronger state tracking than prior linear RNNs
- The 8-hour training timeline on 16 MI300X GPUs is achievable with the specified token counts

**Low Confidence Claims**:
- The theoretical framing of attention transfer as "optimal transport" and "compression between probability measures" lacks rigorous mathematical substantiation
- The claim that RWKV-7 demonstrates "state tracking ability beyond transformers" is based on limited passkey retrieval benchmarks without broader empirical validation

## Next Checks

1. **Stage 1 Convergence Verification**: Reproduce the attention alignment stage using a single GPU (H800/A100 80G) with Qwen2.5-7B-Instruct as teacher. Verify that the L_special loss follows the trajectory shown in Figure 5 and converges within the reported 18-hour timeframe on 8×H800 for 4B tokens.

2. **Gate Mechanism Ablation Study**: Train three variants (ARWKV, ARWKV-M, ARWKV-G-M) using the same teacher and token budget. Compare MMLU and GSM8K scores to quantify the gate-free advantage and validate that the gate mechanism indeed hinders convergence as claimed.

3. **FP16 vs BF16 Inference Validation**: Conduct controlled experiments testing both FP16 and BF16 inference precision on the distilled models. Document performance differences across all benchmark tasks to confirm or refute the reported FP16 advantage and investigate the underlying cause of this anomaly.