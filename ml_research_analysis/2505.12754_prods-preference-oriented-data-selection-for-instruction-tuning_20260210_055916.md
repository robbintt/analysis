---
ver: rpa2
title: 'ProDS: Preference-oriented Data Selection for Instruction Tuning'
arxiv_id: '2505.12754'
source_url: https://arxiv.org/abs/2505.12754
tags:
- data
- training
- preference
- response
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses instruction data selection for fine-tuning
  large language models. The key problem is that existing methods focus on instruction-to-response
  mapping and overlook human preferences for diverse responses.
---

# ProDS: Preference-oriented Data Selection for Instruction Tuning

## Quick Facts
- arXiv ID: 2505.12754
- Source URL: https://arxiv.org/abs/2505.12754
- Reference count: 35
- This paper addresses instruction data selection for fine-tuning large language models by incorporating human preference information through direct preference optimization gradients.

## Executive Summary
This paper addresses the challenge of selecting high-quality instruction tuning data that aligns with human preferences for diverse responses. While existing methods focus on instruction-to-response mapping, ProDS explicitly incorporates response preference information into the data selection process. The method uses Direct Preference Optimization (DPO) gradients to estimate human preferences and employs a bidirectional preference synthesis strategy to score training samples based on their consistency with both positive and negative preferences. Experimental results demonstrate that ProDS outperforms existing task-agnostic and targeted data selection methods, achieving superior performance with significantly less data.

## Method Summary
ProDS is a three-stage pipeline that fine-tunes LLMs on selected instruction data. First, it performs SFT warm-up on 5% random data to establish basic instruction-following capability. Second, it constructs DPO pairs from the remaining data where ground-truth responses are preferred over model-generated ones, and runs DPO warm-up. Third, it computes bidirectional preference gradients using DPO gradients with random projection, scores training samples via cosine similarity to preference directions, optimizes synthesis weights via simulated annealing, and selects top-k% samples for target model fine-tuning. The method aims to capture not just instruction-following ability but also alignment with human preferences for diverse responses.

## Key Results
- On Alpaca-related datasets, Llama2-7B fine-tuned on top 10% selected by ProDS achieves an average winning score of 1.06 compared to full Alpaca fine-tuning
- ProDS outperforms task-agnostic baselines (CE, CEB, CEBL) and targeted selection methods (FACS, TACOS, T-Selector)
- Ablation shows bidirectional preference synthesis (WS=1.06) significantly outperforms unified preference construction (WS=0.83)
- Simulated annealing-based weighting (WS=1.06) improves over fixed weighting (WS=0.94)

## Why This Works (Mechanism)

### Mechanism 1: DPO Gradient as Preference Direction Encoding
ProDS uses DPO gradients to represent human preference directions in target tasks, scoring training samples by their alignment with these preferences. After warm-up fine-tuning, it constructs DPO pairs from validation data where preferred responses (rw) are compared against dispreferred responses (rl). The gradient ∇LD captures the parameter update direction that would increase likelihood of preferred responses while decreasing likelihood of dispreferred ones, serving as a vector representation of "what the model should move toward" to align with human preferences.

### Mechanism 2: Bidirectional Preference Synthesis
Separately modeling positive preferences (toward desired responses) and negative preferences (away from undesired responses) provides more comprehensive sample quality assessment. ProDS constructs two separate preference sets: Papp (positive direction, where sc > sb) and Pawy (negative direction, where sc < sb). Training samples are scored by their cosine similarity to both preference directions, then combined using a learned weighting parameter Λ. This dual-direction approach identifies samples that simultaneously align with improvement directions while avoiding degradation directions.

### Mechanism 3: Simulated Annealing for Preference Weight Optimization
Using simulated annealing to optimize the instance-level weighting parameter Λ enables automatic, data-driven synthesis of positive and negative preference alignment scores. Rather than using fixed weights, ProDS treats Λ as a per-sample parameter optimized via simulated annealing with energy function E(Λ) = -Σ(Λ × Γapp - (1-Λ) × Γawy). The algorithm explores the solution space probabilistically, accepting improvements definitively and accepting worse solutions with probability decreasing over time.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: ProDS relies on understanding how DPO transforms preference data into optimization objectives through implicit reward modeling.
  - Quick check question: Can you explain why DPO avoids training an explicit reward model while still optimizing for human preferences?

- **Concept: Gradient-based Influence Estimation**
  - Why needed here: ProDS uses gradient similarity as a proxy for sample influence. Understanding why gradients represent "how a sample would change the model" is essential for interpreting the scoring mechanism.
  - Quick check question: Why might cosine similarity of gradients be a reasonable proxy for "how much does this training sample align with this optimization direction"?

- **Concept: Simulated Annealing Optimization**
  - Why needed here: The annealing algorithm determines how ProDS combines positive and negative preference scores.
  - Quick check question: In simulated annealing, why does the algorithm sometimes accept solutions that increase the energy function, and how does the temperature parameter control this behavior?

## Architecture Onboarding

- **Component map:** SFT Warm-up Module → DPO Warm-up Module → Validation DPO Pair Constructor → Gradient Computation Module → Scoring Module → Data Selection Module

- **Critical path:** Warm-up quality determines gradient representation quality; validation DPO pair construction quality determines preference direction fidelity; gradient projection dimension affects approximation quality; annealing hyperparameters control optimization quality vs. speed tradeoff.

- **Design tradeoffs:**
  - Warm-up data scale (5%): Larger warm-up improves gradient quality but increases upfront cost
  - Gradient projection dimension: d=8192 balances memory efficiency with information preservation
  - Validation set size: 10% random or task-specific shot-based sampling; too small may not capture task preferences
  - GPT-4 as preference judge: Provides automated preference labeling but introduces cost and potential biases

- **Failure signatures:**
  - Low winning scores despite selection: Check if warm-up model fails to follow instructions
  - Γapp and Γawy highly correlated: Suggests bidirectional modeling provides no benefit
  - Annealing converges to extreme Λ values: Suggests one preference direction dominates
  - Selected data distribution diverges from test set: Check Figure 7 visualization

- **First 3 experiments:**
  1. Reproduce Figure 4 baseline comparison: Select 10% of Alpaca using ProDS with Llama2-7B as selection model; fine-tune Llama2-7B on selected data; evaluate on Vicuna, Koala, Wizardlm, Sinstruct, LIMA using GPT-4 pairwise comparison.
  2. Ablate bidirectional vs. unified preference: Compare separate Papp/Pawy with unified preference pair construction on Wizardlm test set.
  3. Validate cross-model selection transfer: Use Llama3.2-1B as selection model, fine-tune Llama2-7B as target model on selected 10% Alpaca.

## Open Questions the Paper Calls Out

- **Question:** Can preference modeling strategies beyond DPO gradients improve data selection quality or reduce computational overhead?
- **Basis:** "Exploring more effective preference modeling strategies remains a valuable direction for future research"
- **Why unresolved:** The current method relies exclusively on DPO gradients; alternative preference representations are not compared.

- **Question:** What semantic preferences beyond response length does ProDS capture, and can they be systematically quantified?
- **Basis:** Appendix C shows that reconstructing data with longer responses causes minimal performance drop, suggesting "other semantic preferences" are modeled but not characterized.
- **Why unresolved:** The paper demonstrates the existence of other preferences but does not identify or quantify them.

- **Question:** How can preference-oriented selection be adapted for tasks with limited response diversity, such as multiple-choice benchmarks like MMLU?
- **Basis:** "Since MMLU contains multiple tasks and the response format is in the form of option numbers, the preference contained among different responses is relatively scarce."
- **Why unresolved:** The method assumes meaningful preference distinctions between responses, which may not hold for constrained output formats.

## Limitations
- Dependence on high-quality preference signals for validation DPO pair construction, with performance sensitive to GPT-4's subjective judgments
- Additional complexity through bidirectional preference synthesis and annealing optimization without theoretical justification for per-sample weight variation
- Computational efficiency gain from Rademacher random projection may come at information loss tradeoff
- Limited analysis of failure modes and sensitivity to warm-up data scales

## Confidence
- **High Confidence:** Outperformance of task-agnostic baselines on Alpaca-related datasets is well-supported with consistent winning scores >1.0
- **Medium Confidence:** Superiority over targeted selection methods is demonstrated but relies on few comparison points and optimal targeted selection assumptions
- **Low Confidence:** Limited analysis of failure modes and sensitivity to validation pair quality; assumption that DPO gradients generalize is plausible but not rigorously validated

## Next Checks
1. **Validation Pair Quality Analysis:** Systematically evaluate how validation DPO pair quality affects ProDS performance by varying validation shots and measuring downstream selection quality.
2. **Alternative Preference Judges:** Replace GPT-4 with other preference judgment mechanisms to assess sensitivity to preference signal source.
3. **Gradient Projection Sensitivity:** Conduct controlled experiments varying random projection dimension and comparing against full gradient similarity to quantify information loss tradeoff.