---
ver: rpa2
title: Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition
arxiv_id: '2509.10729'
source_url: https://arxiv.org/abs/2509.10729
tags:
- audio
- activity
- time
- step
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated using large language models (LLMs) for late
  multimodal sensor fusion to classify activities from audio and motion time series
  data. A curated subset of the Ego4D dataset was used, containing 12 diverse daily
  activities such as cooking, cleaning, and playing sports.
---

# Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition

## Quick Facts
- **arXiv ID**: 2509.10729
- **Source URL**: https://arxiv.org/abs/2509.10729
- **Reference count**: 40
- **Primary result**: LLM-based late fusion of audio and motion data achieved F1-scores significantly above chance in zero- and one-shot activity classification.

## Executive Summary
This paper demonstrates using large language models (LLMs) for late multimodal sensor fusion to classify daily activities from audio and motion time series data. The approach converts sensor data into text-based predictions from modality-specific models (MS CLAP for audio captions, VGGish for audio labels, IMU classifier for motion) and prompts an LLM to reason about activities. Evaluated on a curated subset of the Ego4D dataset with 12 diverse daily activities, the method achieved F1-scores significantly above chance in both zero- and one-shot classification settings, with audio captions providing the most informative modality. The work shows LLM-based reasoning can enable multimodal temporal applications where aligned training data is limited.

## Method Summary
The method involves prompting LLMs to reason about high-level activities based on per-modality predictions from audio and motion models. Audio data is processed by MS CLAP (generating captions) and VGGish (generating labels), while motion data comes from an IMU classifier. These textual predictions across time steps are formatted into structured prompts for the LLM, which performs reasoning to classify activities. The approach uses zero-shot and one-shot prompting strategies, with optional synthetic context (indoor/outdoor setting, heart rate zone) to enhance reasoning. The Ego4D dataset subset contains 12 daily activities including cooking, cleaning, and playing sports.

## Key Results
- Zero- and one-shot classification achieved F1-scores significantly above chance (e.g., 60% accuracy for audio captions with Gemini-2.5-pro)
- Audio captions provided the most informative modality, outperforming audio labels (41% accuracy) and IMU predictions (13% accuracy)
- Adding synthetic context further improved LLM reasoning performance
- Open-ended activity classification also performed above chance, demonstrating the potential of LLM-based reasoning for multimodal fusion without requiring aligned training data

## Why This Works (Mechanism)

### Mechanism 1: Text-Mediated Semantic Bridging
LLMs fuse multimodal sensor data by reasoning over text-based intermediate representations rather than raw signals. Modality-specific models convert time-series signals into semantic tokens, bypassing the need for learning aligned embedding spaces across modalities.

### Mechanism 2: World Knowledge for Contextual Disambiguation
Pre-trained LLMs encode relationships between activities, sounds, movements, and settings that enable reasoning beyond raw prediction accuracy. The LLM uses learned associations to integrate noisy per-modality predictions into coherent activity hypotheses.

### Mechanism 3: Hierarchical Information Weighting
Audio captions provide more discriminative information than audio labels or IMU predictions for this activity taxonomy. The richer semantic content in captions gives the LLM more reasoning material, while IMU predictions contribute primarily for high-motion activity disambiguation.

## Foundational Learning

- **Concept: Late vs. Early Fusion**
  - Why needed: The paper's central contribution is demonstrating late fusion via LLMs as an alternative to early fusion requiring joint embedding training.
  - Quick check: Can you explain why late fusion via text might outperform early fusion when aligned training data is scarce?

- **Concept: Zero-Shot and One-Shot Learning**
  - Why needed: The paper evaluates both settings, showing performance gains from a single exemplar.
  - Quick check: What additional information does a one-shot example provide that changes model behavior compared to zero-shot?

- **Concept: Modality-Specific Encoders (CLAP, VGGish, IMU Classifiers)**
  - Why needed: The LLM's performance is bounded by the quality and granularity of upstream model outputs.
  - Quick check: Why might audio captions outperform audio labels for activity recognition, and what does this suggest about encoder selection?

## Architecture Onboarding

- **Component map**: Audio encoder (MS CLAP) -> Audio encoder (VGGish) -> IMU classifier -> Context generator -> LLM (Gemini-2.5-pro or Qwen-32B) -> Activity prediction
- **Critical path**: Align audio (2s) and IMU (4s with 2s overlap) windows at their centers → Run each window through its modality-specific model → Format all predictions into the prompt template → LLM performs multi-step reasoning and outputs activity prediction
- **Design tradeoffs**: Caption richness vs. label precision; IMU window size affects motion data smoothness; LLM choice impacts accuracy vs. inference cost; synthetic context improves performance but requires additional sensing
- **Failure signatures**: Cascading errors from inaccurate audio labels; low-motion ambiguity when audio is uninformative; temporal inconsistency when modality predictions contradict across time steps
- **First 3 experiments**: 1) Replicate closed-set evaluation on Ego4D segments with different LLMs; 2) Ablate modalities systematically to validate caption informativeness; 3) Substitute IMU classifier with higher-capacity model to assess motion contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted training strategies (e.g., reasoning distillation or reinforcement learning) improve LLM performance on modality integration for sensor fusion beyond zero-shot baselines?
- Basis: Only zero-shot and one-shot prompting were evaluated; no task-specific LLM training was attempted.

### Open Question 2
- Question: How does LLM-based late fusion compare to traditional multimodal fusion methods (e.g., contrastive learning, early feature fusion) on aligned sensor datasets?
- Basis: The paper claims LLM-based fusion enables applications where aligned training data is limited, but doesn't benchmark against supervised multimodal baselines.

### Open Question 3
- Question: Would improving IMU model capacity and output granularity yield meaningful gains in fusion accuracy?
- Basis: The IMU model had limited output space and near-chance standalone performance.

### Open Question 4
- Question: Does using real (vs. synthetically generated) context such as heart rate or location data improve fusion performance?
- Basis: Synthetic context improved performance, but the authors note it was designed to "mimic information that could be captured from sensors" rather than actual sensor data.

## Limitations
- Results are based on a curated subset of Ego4D with 12 specific daily activities, limiting generalizability to different activity sets
- The approach inherits errors from modality-specific models, particularly when audio labels misclassify sounds
- Synthetic context requires additional sensing infrastructure and wasn't evaluated for accuracy impact or computational overhead
- Temporal granularity mismatch between 2-second audio windows and 4-second IMU windows may create alignment challenges

## Confidence

**High Confidence**: The core demonstration that LLMs can perform zero- and one-shot activity classification from text-based modality predictions is well-supported by F1-scores significantly above chance.

**Medium Confidence**: The generalizability of the approach to different activity taxonomies and sensor configurations remains uncertain, as does the extent to which LLM reasoning compensates for systematic prediction failures.

**Low Confidence**: The relative performance advantage of this late fusion approach versus traditional early fusion methods on aligned training data hasn't been quantified.

## Next Checks

1. **Cross-Activity Set Evaluation**: Test the same pipeline on a different HAR dataset (e.g., PAMAP2 or USC-HAD) with distinct activity profiles to determine whether audio captions consistently dominate or if performance patterns shift.

2. **Error Propagation Analysis**: Systematically vary the accuracy of modality-specific predictions to quantify how LLM reasoning performance degrades and whether it consistently outperforms simple majority voting across prediction error rates.

3. **Real-Time Context Validation**: Deploy the synthetic context generation in a realistic setting to measure actual accuracy and latency, then evaluate whether the performance improvement from context justifies the additional sensing and computation costs.