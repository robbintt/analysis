---
ver: rpa2
title: 'KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs'
arxiv_id: '2508.01273'
source_url: https://arxiv.org/abs/2508.01273
tags:
- reasoning
- knowledge
- conflicts
- answer
- conflicting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inter-context knowledge conflicts
  in long-context scenarios, where multiple sources of information contradict each
  other and can confuse large language models. The proposed Knowledge Conflict Reasoning
  (KCR) framework resolves these conflicts by training backbone LLMs to follow reasoning
  paths that exhibit stronger logical consistency.
---

# KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs

## Quick Facts
- **arXiv ID:** 2508.01273
- **Source URL:** https://arxiv.org/abs/2508.01273
- **Reference count:** 30
- **Primary result:** 7B-parameter models enhanced with KCR can outperform their original 32B-parameter counterparts on knowledge conflict resolution tasks

## Executive Summary
This paper addresses the problem of inter-context knowledge conflicts in long-context scenarios, where multiple sources of information contradict each other and can confuse large language models. The proposed Knowledge Conflict Reasoning (KCR) framework resolves these conflicts by training backbone LLMs to follow reasoning paths that exhibit stronger logical consistency. KCR extracts reasoning paths from conflicting contexts using both text and local knowledge graphs, then employs Reinforcement Learning with Verifiable Rewards to encourage the model to imitate correct reasoning logic while maintaining consistency between reasoning process and final answer. Experiments show that KCR significantly improves performance across various backbone models on two benchmark datasets (popQA and strategyQA), with improvements of up to 23.75% in semantic accuracy.

## Method Summary
KCR uses a two-phase approach: first extracting reasoning paths from conflicting contexts via text chains and local knowledge graphs using GPT-4o-mini, then training backbone LLMs with GRPO (RLVR) using discrete logic reward + consistency reward. The framework constructs local knowledge graphs from long contexts, extracts reasoning paths in both textual and graph formats centered on query entities, and employs JS divergence between semantic distributions to reward logical consistency while using Levenshtein similarity to enforce consistency between reasoning process and final answer. Training uses LoRA adapters with DeepSpeed ZeRO-2 for efficiency.

## Key Results
- KCR achieves up to 23.75% improvement in semantic accuracy (ACCL) over baseline models
- 7B-parameter models with KCR can outperform their original 32B-parameter counterparts
- Text-form reasoning paths slightly outperform graph-form paths on popQA (0.7222 vs. 0.7208)
- Logic reward contributes more to performance gains than consistency reward in ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Dual-Format Reasoning Path Extraction
Extracting reasoning paths from both unstructured text and structured knowledge graphs helps models retain key information in long, conflicting contexts. KCR constructs reasoning chains in two parallel formats: textual relational paths from raw text preserving entity-relation sequences, and graph-based paths from local knowledge graphs. These paths capture the underlying logical structure of each conflicting answer's supporting context.

### Mechanism 2: Logic Reward via JS Divergence
Rewarding models for matching the logical consistency of correct reasoning paths while diverging from incorrect ones improves conflict resolution. KCR computes a "logic score" using Jensen-Shannon divergence between semantic distributions of consecutive reasoning chains. The model receives positive reward when its generated reasoning process is closer in logical structure to the correct candidate's paths than to the incorrect candidate's paths.

### Mechanism 3: Consistency Reward Between Process and Answer
Enforcing token-level consistency between the reasoning process and final answer reduces hallucination in conflict scenarios. Uses Levenshtein similarity to compare the generated answer against both candidates and the reasoning process against both reasoning path sets. The model is rewarded when its reasoning-answer pair aligns consistently with one candidate rather than mixing contradictory signals.

## Foundational Learning

- **Concept: Jensen-Shannon Divergence**
  - **Why needed here:** Core metric for quantifying "logical consistency" between reasoning steps
  - **Quick check question:** Given two probability distributions over vocabulary, can you compute their JS divergence and explain what a lower value implies about semantic similarity?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** KCR uses GRPO (a variant) to train backbone models with custom reward signals
  - **Quick check question:** How does RLVR differ from standard RLHF, and why might verifiable rewards be preferable for reasoning tasks?

- **Concept: Knowledge Graph Construction from Text**
  - **Why needed here:** Phase 1 constructs local KGs from long contexts to extract structured reasoning paths
  - **Quick check question:** Given a paragraph, can you identify entity-relation triples and explain how prompt-based KG extraction might fail on ambiguous pronouns?

## Architecture Onboarding

- **Component map:**
  ```
  Input: (query, conflicting_answer_1 + context_1, conflicting_answer_2 + context_2)
     ↓
  Phase 1: Reasoning Path Extraction
     ├─→ Textual Path Extractor (frozen LLM prompt-based)
     └─→ KG Builder (GPT-4o-mini) → Path Extractor
     ↓
  Phase 2: RLVR Training (GRPO)
     ├─→ Logic Reward Model (JS divergence on embeddings)
     ├─→ Consistency Reward Model (Levenshtein similarity)
     └─→ Ground-truth Reward (format + accuracy)
     ↓
  Output: Trained backbone LLM that generates (reasoning_process, final_answer)
  ```

- **Critical path:** The quality of Phase 1 extraction directly determines reward signal quality. If extracted paths are noisy or miss key relations, RLVR optimizes toward incorrect targets. Test extraction quality on held-out examples before training.

- **Design tradeoffs:**
  - Discrete vs. continuous rewards: Discrete preferred for stability (Section 3.2.1), but continuous may provide richer gradients
  - Text vs. graph paths: Table 2 shows text-form slightly outperforms graph-form on popQA (0.7222 vs. 0.7208), but both help. Consider computational cost of KG construction
  - LoRA vs. full fine-tuning: Paper uses LoRA for efficiency; full fine-tuning may yield higher gains but at higher cost

- **Failure signatures:**
  - Multilingual spillover (Section 4.4): Larger Qwen models (14B, 32B) generate non-English outputs on complex queries, artificially depressing English-only metrics
  - "Lost in the middle" persists if reasoning paths don't adequately surface middle-context information
  - Low Levenshtein similarity may indicate model is mixing conflicting information rather than committing to one

- **First 3 experiments:**
  1. Reproduce extraction quality: Run Phase 1 on 50 examples from popQA, manually verify that extracted paths capture the key reasoning steps. Check for missing entities or spurious relations
  2. Ablate reward components: Train Qwen-2.5-7B with (a) logic-only, (b) consistency-only, (c) both rewards. Compare ACCL scores to confirm relative contributions per Section 4.3.2
  3. Test on out-of-domain conflicts: Apply trained model to a held-out domain (e.g., scientific contradictions) to assess generalization beyond popQA/strategyQA. Monitor for hallucination increase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the KCR framework scale to scenarios involving more than two conflicting contexts (multi-way conflicts)?
- Basis in paper: [inferred] Section 2 (Problem Definition) restricts the input space to strictly two candidates ($A_a$ and $A_b$), and Section 3.2.1 formulates rewards based on relative binary comparisons
- Why unresolved: The reward mechanisms (Equations 6–9) rely on calculating the relative distance or similarity between a generated path and exactly two reference sets. This mathematical formulation does not inherently support $n$-way comparisons
- What evidence would resolve it: Experiments on datasets containing queries with 3 or more contradictory contexts, requiring a reformulation of the logic reward to handle multiple divergent reference points

### Open Question 2
- Question: Is the Jensen-Shannon (JS) divergence of semantic embeddings a valid proxy for "logical rigor" in reasoning?
- Basis in paper: [inferred] Section 3.2.1 posits that lower JS divergence scores between consecutive reasoning steps indicate "more rigorous logic," utilizing embeddings to measure distributional similarity
- Why unresolved: Semantic similarity between sentences does not necessarily correlate with logical validity (e.g., a coherent but factually incorrect fallacy could yield a high logic score)
- What evidence would resolve it: A correlation analysis comparing the KCR "logic score" against formal logic verification or human evaluation of reasoning validity on a held-out test set

### Open Question 3
- Question: Can KCR effectively suppress "multilingual spillover" in larger models (14B/32B) to restore their performance advantage?
- Basis in paper: [inferred] Section 4.4 notes that larger Qwen models (14B, 32B) suffer from performance degradation due to generating non-English text, but the primary results focus on fixing 7B models
- Why unresolved: While KCR improves 7B models, it is unclear if the consistency reward is strong enough to override the strong multilingual tendencies of the larger 14B/32B models, or if the 32B+KCR combination would set a new state-of-the-art
- What evidence would resolve it: Experimental results applying KCR specifically to Qwen-14B and Qwen-32B backbones, reporting on English adherence rates and semantic accuracy

## Limitations
- Reliance on high-quality reasoning path extraction in Phase 1 - errors here lead to optimizing toward incorrect targets
- JS divergence conflates semantic smoothness with valid reasoning leaps, potentially penalizing non-linear but correct reasoning
- Levenshtein-based consistency reward doesn't guarantee factual correctness, only internal coherence
- Multilingual spillover observed in larger Qwen models suggests instability in maintaining language consistency

## Confidence
- **High confidence:** Overall two-phase architecture is technically sound and well-specified; experimental results are clearly reported
- **Medium confidence:** Effectiveness of JS divergence as a metric for logical consistency in reasoning paths
- **Medium confidence:** Claim that 7B-parameter models with KCR outperform original 32B-parameter counterparts (could be influenced by multilingual artifact)

## Next Checks
1. Extraction quality validation: Manually audit extracted reasoning paths on 50 held-out examples from popQA to verify that key entities, relations, and logical steps are captured
2. Reward signal ablation study: Train Qwen-2.5-7B with three configurations—(a) logic reward only, (b) consistency reward only, (c) both rewards—and compare ACCL scores
3. Out-of-domain generalization test: Apply trained model to a held-out domain with conflicting information (e.g., scientific facts) not present in popQA/strategyQA, monitoring for increased hallucination rates