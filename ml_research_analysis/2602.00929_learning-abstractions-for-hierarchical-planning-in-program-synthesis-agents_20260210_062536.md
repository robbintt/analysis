---
ver: rpa2
title: Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents
arxiv_id: '2602.00929'
source_url: https://arxiv.org/abs/2602.00929
tags:
- learning
- abstractions
- agent
- state
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TheoryCoder-2 is a theory-based reinforcement learning agent that
  learns reusable abstractions from experience to enable efficient hierarchical planning
  in complex environments. It synthesizes high-level PDDL operators and Python world
  models using LLMs' in-context learning, then reuses these abstractions across tasks.
---

# Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents

## Quick Facts
- arXiv ID: 2602.00929
- Source URL: https://arxiv.org/abs/2602.00929
- Authors: Zergham Ahmed; Kazuki Irie; Joshua B. Tenenbaum; Christopher J. Bates; Samuel J. Gershman
- Reference count: 21
- Primary result: TheoryCoder-2 achieves 0-2000 token costs (vs 8500-33000 for baselines) on complex levels, solving tasks like Boss levels that baselines fail

## Executive Summary
TheoryCoder-2 is a theory-based reinforcement learning agent that learns reusable abstractions from experience to enable efficient hierarchical planning in complex environments. It synthesizes high-level PDDL operators and Python world models using LLMs' in-context learning, then reuses these abstractions across tasks. Experiments on BabyAI, Minihack, and VGDL games show TheoryCoder-2 achieves dramatic cost reductions compared to baselines, successfully solving complex tasks through abstraction transfer. The approach bridges symbolic planning with learned world models, demonstrating state-of-the-art performance on challenging environments while requiring minimal human prompts compared to prior theory-based RL systems.

## Method Summary
TheoryCoder-2 synthesizes high-level PDDL operators and Python world models using LLMs' in-context learning capabilities. The agent learns reusable abstractions from experience in complex environments and applies these across tasks to enable efficient hierarchical planning. The system combines symbolic planning with learned world models, using few-shot examples to guide the LLM in generating appropriate abstractions. These abstractions are then used for hierarchical planning, with the agent demonstrating significant performance improvements on benchmark environments including BabyAI, Minihack, and VGDL games through sample efficiency and abstraction transfer.

## Key Results
- Achieves 0-2000 token costs compared to 8500-33000 for baselines on complex levels
- Successfully solves Boss levels that baselines fail to complete
- Demonstrates sample efficiency through abstraction transfer requiring minimal human prompts

## Why This Works (Mechanism)
TheoryCoder-2 works by leveraging LLMs to synthesize reusable abstractions that capture the essential structure of tasks and environments. The key mechanism is the combination of symbolic PDDL operators with learned world models, allowing the agent to plan at multiple levels of abstraction. By reusing abstractions across tasks, the system avoids the computational overhead of learning from scratch each time. The few-shot prompting approach enables efficient guidance of the LLM synthesis process, while the hierarchical planning structure allows the agent to operate efficiently in complex environments by decomposing tasks into manageable subgoals.

## Foundational Learning
- **PDDL (Planning Domain Definition Language)**: Why needed - provides a standardized symbolic representation for planning operators and domains; Quick check - can the agent correctly parse and use PDDL files for planning
- **Hierarchical Planning**: Why needed - enables decomposition of complex tasks into manageable subgoals; Quick check - does the agent successfully break down tasks into appropriate hierarchical levels
- **In-context Learning**: Why needed - allows efficient guidance of LLM synthesis without fine-tuning; Quick check - can the agent generate correct abstractions from few-shot examples
- **World Models**: Why needed - provides predictive understanding of environment dynamics; Quick check - does the learned world model accurately predict state transitions
- **Abstraction Transfer**: Why needed - enables reuse of learned knowledge across tasks; Quick check - does performance improve when abstractions transfer between similar tasks
- **Reinforcement Learning**: Why needed - provides framework for learning optimal behaviors through interaction; Quick check - does the agent improve performance through repeated trials

## Architecture Onboarding

**Component Map:**
LLM → PDDL Operator Synthesis → Python World Model → Hierarchical Planner → Environment Interaction → Abstraction Repository

**Critical Path:**
Observation → LLM Synthesis (Operators + World Model) → Plan Generation → Execution → Feedback → Abstraction Update

**Design Tradeoffs:**
- Symbolic vs. end-to-end learning: TheoryCoder-2 chooses symbolic PDDL for interpretability and transfer, sacrificing some flexibility of pure neural approaches
- LLM-based synthesis vs. learned modules: Uses LLM for abstraction synthesis to leverage in-context learning, accepting computational overhead for improved sample efficiency
- Discrete abstractions vs. continuous representations: Employs discrete PDDL operators for compatibility with classical planning, limiting direct application to continuous domains

**Failure Signatures:**
- LLM generates incorrect or incomplete abstractions leading to failed planning
- Abstraction transfer fails when task dynamics shift significantly
- Computational overhead of LLM synthesis becomes prohibitive for very large state spaces
- Symbolic representations cannot capture necessary continuous aspects of environment

**3 First Experiments:**
1. Verify basic PDDL operator synthesis works on a simple grid navigation task with known solution
2. Test abstraction transfer by learning operators in one BabyAI level and applying to a similar level
3. Compare planning efficiency with and without learned world models on a Minihack environment

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the system be adapted to learn abstractions directly from raw visual inputs without relying on pre-defined object-oriented text representations?
- Basis in paper: The authors state the approach assumes access to an object-oriented, text-based state representation and note that scaling to complex settings requires robust methods for object discovery and attribute inference.
- Why unresolved: The current architecture relies on a symbolic state dictionary; it does not include a perception module for extracting these symbols from pixels.
- What evidence would resolve it: Demonstrating successful abstraction learning and hierarchical planning in environments where the state must be derived from raw pixels (e.g., using a vision-language model as a frontend).

### Open Question 2
- Question: How can the discrete PDDL-based abstraction layer be extended to handle continuous action spaces and state dynamics?
- Basis in paper: The authors list extending beyond discrete domains to continuous ones as a specific limitation, noting challenges such as predicting continuous trajectories.
- Why unresolved: The current symbolic operators (PDDL) and low-level world model are designed for discrete state transitions and grid-based movements.
- What evidence would resolve it: A variation of the agent successfully synthesizing abstractions that utilize continuous parameters or operators in a robotics simulation or continuous control task.

### Open Question 3
- Question: What mechanisms are required to enable robust, iterative trial-and-error refinement of PDDL abstractions when the initial observation is insufficient?
- Basis in paper: The authors note that while the LLM generated adequate abstractions from initial observations in these tests, "more challenging or less familiar environments will likely require additional mechanisms to support trial-and-error learning for the PDDL representations."
- Why unresolved: The current system primarily synthesizes abstractions zero-shot from the initial observation and few-shot examples, lacking a formal feedback loop for correcting the high-level PDDL logic itself.
- What evidence would resolve it: An experiment in a novel, complex domain showing improved success rates when a mechanism for validating and revising PDDL operators based on execution failure is introduced.

## Limitations
- Evaluation primarily relies on synthetic environments with limited real-world applicability testing
- Performance metrics depend heavily on specific implementation choices for LLM prompting strategies
- Limited analysis of when and why abstraction transfer fails between tasks

## Confidence
- **High confidence**: The core architecture combining symbolic PDDL operators with learned world models is technically sound and the basic functionality works as described
- **Medium confidence**: The reported performance improvements are likely real but may be somewhat implementation-specific; the relative advantage over baselines is convincing but absolute numbers may vary
- **Medium confidence**: Claims about sample efficiency and transfer are supported but could benefit from more rigorous ablation studies

## Next Checks
1. Conduct cross-model validation using different LLM backends (e.g., GPT-4, Claude, open-source alternatives) to verify that performance gains aren't tied to specific prompting strategies or model behaviors
2. Implement systematic ablation studies removing the theory-learning component to quantify exactly how much performance depends on abstraction reuse versus other architectural choices
3. Test transfer capabilities on tasks with deliberately shifted dynamics or objectives to establish failure modes and boundaries of the abstraction transfer mechanism