---
ver: rpa2
title: 'NumCoKE: Ordinal-Aware Numerical Reasoning over Knowledge Graphs with Mixture-of-Experts
  and Contrastive Learning'
arxiv_id: '2411.12950'
source_url: https://arxiv.org/abs/2411.12950
tags:
- numerical
- numcoke
- knowledge
- learning
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NumCoKE, a novel numerical reasoning framework
  for knowledge graphs that combines Mixture-of-Experts and Ordinal Contrastive Embedding.
  The method addresses two key challenges: incomplete semantic integration between
  entities, relations, and numerical attributes, and ordinal indistinguishability
  between close numerical values.'
---

# NumCoKE: Ordinal-Aware Numerical Reasoning over Knowledge Graphs with Mixture-of-Experts and Contrastive Learning

## Quick Facts
- **arXiv ID:** 2411.12950
- **Source URL:** https://arxiv.org/abs/2411.12950
- **Reference count:** 40
- **Primary result:** State-of-the-art performance on three KG benchmarks using MoE and ordinal contrastive learning

## Executive Summary
This paper introduces NumCoKE, a novel numerical reasoning framework for knowledge graphs that combines Mixture-of-Experts and Ordinal Contrastive Embedding. The method addresses two key challenges: incomplete semantic integration between entities, relations, and numerical attributes, and ordinal indistinguishability between close numerical values. NumCoKE uses a Mixture-of-Experts Knowledge-Aware (MoEKA) encoder to dynamically align symbolic and numeric components while routing attribute features to relation-specific experts. It also introduces Ordinal Knowledge Contrastive Learning (OKCL) that constructs ordinal-aware positive and negative samples to capture fine-grained ordinal relationships. Experiments on three public KG benchmarks show that NumCoKE consistently outperforms competitive baselines, achieving state-of-the-art performance across diverse attribute distributions.

## Method Summary
NumCoKE is a numerical reasoning framework for knowledge graphs that integrates symbolic triples with numerical attributes through a Mixture-of-Experts Knowledge-Aware (MoEKA) encoder. The encoder uses relation-specific experts to process entity features and routes numerical attributes through knowledge perceptual attention. An ordinal contrastive learning module generates positive/negative samples based on cosine similarity to preserve ordinal relationships. The model employs a hybrid score function combining TransE translation with order-embedding constraints. The framework is trained with a combined BCE and contrastive loss, using hyperparameters K=3 experts, L=5 attention heads, and λ=0.25 for contrastive loss.

## Key Results
- Achieves state-of-the-art MRR of 0.794 on the Credit dataset, outperforming baselines by 6.4%
- Demonstrates consistent performance improvements across all three datasets (US-Cities, Spotify, Credit)
- Ablation studies show each component (MoE, OKCL, hybrid scoring) contributes to overall performance gains
- Shows robustness to 20% masked numerical values during training

## Why This Works (Mechanism)

### Mechanism 1: Relation-Contextualized Numerical Integration
- **Claim:** Routing numerical attributes through relation-specific expert modules improves semantic alignment between numeric values and their relational context.
- **Mechanism:** The Mixture-of-Experts Knowledge-Aware (MoEKA) encoder maintains K relation-aware experts. Each expert processes entity embeddings, and a learned gating network routes numerical attributes to the most relevant expert based on the current relation context.
- **Core assumption:** Different relations require different numerical attribute weighting patterns, and these patterns are learnable through expert specialization.
- **Evidence anchors:** MoEKA encoder dynamically aligns symbolic and numeric components while routing attribute features to relation-specific experts (abstract).

### Mechanism 2: Ordinal-Aware Contrastive Sampling
- **Claim:** Constructing contrastive samples based on ordinal proximity enables finer-grained numerical discrimination than random negative sampling.
- **Mechanism:** OKCL generates positive/negative samples from available tail entities, then selects top-k samples using cosine similarity to the anchor embedding. These are blended with the head entity using coefficients α, β sampled from [0,1].
- **Core assumption:** Cosine similarity in the learned embedding space correlates with ordinal proximity in the original numerical space.
- **Evidence anchors:** Ordinal Knowledge Contrastive Learning (OKCL) constructs ordinal-aware positive and negative samples to capture fine-grained ordinal relationships (abstract).

### Mechanism 3: Hybrid Score Function with Order Preservation
- **Claim:** Combining translation-based distance with order-embedding constraints enables the model to distinguish both relatedness and numerical ordering simultaneously.
- **Mechanism:** The score function combines a TransE-style translation term with an order-embedding term that enforces ordering constraints in the projected relation space.
- **Core assumption:** The projection matrix W_r can map entity embeddings to a space where numerical ordering aligns with embedding coordinates.
- **Evidence anchors:** To address this issue, we design a rationality score function specifically for the numerical reasoning task (section on training/score function).

## Foundational Learning

- **Mixture-of-Experts (MoE) Gating**
  - Why needed here: The core innovation requires understanding how soft routing allows different attributes to be processed by specialized experts rather than uniform processing.
  - Quick check question: Can you explain why Eq. (2) uses a relation-aware temperature ϵ_r, and what would happen if it were set to a constant?

- **Contrastive Learning with Hard Negatives**
  - Why needed here: OKCL builds on standard contrastive learning but introduces ordinal-aware sample selection, which requires understanding how negative sampling strategies affect embedding quality.
  - Quick check question: What is the difference between randomly sampling negatives from E- vs. selecting top-k by cosine similarity, and why might the latter help with "hard negatives"?

- **Order-Embedding Theory**
  - Why needed here: The second term of the score function is derived from order-embeddings (Vendrov et al. 2016), which encodes partial order relationships in embedding space.
  - Quick check question: In Eq. (14), why does the order term use max(0, W_r*e_h - W_r*e_t) rather than just W_r*e_h - W_r*e_t?

## Architecture Onboarding

- **Component map:**
  Entity → [MoE Experts + Gating] → [Knowledge Perceptual Attention over attributes] → e_att_joint (attribute-enriched embedding) → [Contrastive Learning with OKCL samples] → [Score Function] → Loss

- **Critical path:**
  Entity → [MoE Experts + Gating] → [Knowledge Perceptual Attention over attributes] → e_att_joint (attribute-enriched embedding) → [Contrastive Learning with OKCL samples] → [Score Function] → Loss

- **Design tradeoffs:**
  - K (number of experts): Too few loses specialization; too many causes semantic confusion (Figure 3 shows peak at K=3 on Spotify)
  - L (attention heads): More heads capture finer-grained attribute importance but increase computation
  - λ (contrastive loss coefficient): Paper uses 0.25; setting too high may dominate BCE loss
  - δ_e, δ_r (entity/relation perception weights): 40-90% relation weight optimal (Figure 4); 100% relation-only performs poorly

- **Failure signatures:**
  - Expert collapse: All gating weights F_k converge to similar values (monitor variance across experts)
  - Ordinal structure loss: Top-k neighbors in embedding space don't correlate with numerical neighbors in raw data
  - Score function imbalance: If order term dominates, model may correctly order entities but fail at link prediction

- **First 3 experiments:**
  1. **Ablation of MoE vs. static embeddings:** Remove MoE (set K=1), compare MRR on Credit dataset (expect ~5% drop per Table 2)
  2. **Ordinal sampling ablation:** Replace top-k cosine similarity selection with random sampling from E+, E- (expect ~6.4% drop on Credit)
  3. **Hyperparameter sensitivity sweep:** Vary λ (0, 0.25, 0.5, 0.75) and Δ (0, 0.25, 0.5, 0.75) to identify stability regions (Figure 6 shows results for Credit)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be adapted to effectively handle link prediction tasks involving fine-grained numerical comparisons where value differences are extremely small (e.g., less than 1)?
- **Basis in paper:** In the Breakdown Analysis (Section E.9), the authors state: "In the future, we will explore how to target the improvement of the link prediction tasks involving fine-grained numerical size comparison relations."
- **Why unresolved:** The authors note that in the US-Cities dataset, attribute differences are often less than 1, making relations like "is located in county" difficult to determine, and they have not yet proposed a solution for this precision issue.
- **Evidence:** A modified model architecture or loss function that demonstrates statistically significant improvements on a dataset specifically constructed with micro-scale numerical variations.

### Open Question 2
- **Question:** Can the NumCoKE framework be generalized to non-translation-based knowledge graph embedding backbones (e.g., RotatE) without suffering the severe performance degradation observed in the study?
- **Basis in paper:** Table 11 (Section E.6) shows that replacing TransE with RotatE causes the MRR to crash from 0.794 to 0.115. The authors infer this is due to ordering/boundary problems but do not resolve the incompatibility.
- **Why unresolved:** The current methodology appears heavily reliant on the specific geometric properties of TransE, limiting the choice of backbone models for practitioners who might prefer rotational or other complex embeddings.
- **Evidence:** Successful integration of the MoEKA encoder and OKCL strategy with a rotational model (like RotatE) that achieves comparable performance to the current TransE-based implementation.

### Open Question 3
- **Question:** Does the model maintain robustness and stability when facing high-frequency, high-magnitude extreme values (outliers) in attribute distributions?
- **Basis in paper:** In Section E.9, the authors state: "In addition, we are already working [on] the challenges of extreme values on model robustness."
- **Why unresolved:** While a small experiment with "10% extra extreme samples" (Table 10) showed stability, the authors explicitly identify the general challenge of extreme values as ongoing work.
- **Evidence:** A sensitivity analysis on heavily skewed or heavy-tailed distributions showing that the Mixture-of-Experts routing does not collapse or overfit to outlier attributes.

## Limitations
- Heavy reliance on TransE backbone limits generalization to other embedding models like RotatE
- Assumes ordinal structure can be preserved through contrastive sampling without empirical validation
- Performance depends on careful hyperparameter tuning of K experts, λ contrastive weight, and Δ order term balance

## Confidence
- **High confidence:** The architectural design choices (MoE with K=3 experts, λ=0.25 for contrastive loss) are empirically justified through ablation studies showing clear performance degradation when these values are changed
- **Medium confidence:** The ordinal sampling strategy's effectiveness assumes cosine similarity in embedding space preserves numerical order, but this correlation is not empirically validated on the raw data
- **Low confidence:** The claim that relation-aware expert routing is superior to static attribute weighting lacks comparison to simpler alternatives like learned relation-specific attention weights without expert specialization

## Next Checks
1. **Ordinal Structure Validation:** Compute the correlation between cosine similarity in the learned embedding space and actual numerical proximity in the raw data for top-k samples to verify that ordinal structure is preserved
2. **Expert Specialization Analysis:** Visualize and quantify the learned gating weights F_k across different relations to confirm that experts are actually specializing rather than collapsing to similar weight distributions
3. **Score Function Component Isolation:** Train separate models with only TransE, only Order-Embedding, and the hybrid combination to quantify the marginal contribution of the order-embedding component beyond standard translation-based scoring