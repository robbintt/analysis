---
ver: rpa2
title: How does Misinformation Affect Large Language Model Behaviors and Preferences?
arxiv_id: '2505.21608'
source_url: https://arxiv.org/abs/2505.21608
tags:
- misinformation
- deterding
- llms
- university
- stanford
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models remain vulnerable to misinformation, struggling
  to discern conflicting claims across knowledge domains and textual styles. This
  paper introduces MISBENCH, a comprehensive benchmark containing 10.3 million misinformation
  examples spanning factual, temporal, and semantic conflicts across six textual styles
  including news reports, blogs, and technical language.
---

# How does Misinformation Affect Large Language Model Behaviors and Preferences?

## Quick Facts
- arXiv ID: 2505.21608
- Source URL: https://arxiv.org/abs/2505.21608
- Reference count: 40
- Large language models remain vulnerable to misinformation, struggling to discern conflicting claims across knowledge domains and textual styles

## Executive Summary
Large language models remain vulnerable to misinformation despite their advanced reasoning capabilities. This paper introduces MISBENCH, a comprehensive benchmark containing 10.3 million misinformation examples spanning factual, temporal, and semantic conflicts across six textual styles. Experiments reveal that while LLMs can identify contextual inconsistencies, they are particularly susceptible to factual contradictions and ambiguous semantic constructs, with susceptibility varying by task complexity and presentation style. Building on these insights, the authors propose Reconstruct to Discriminate (RtD), which enhances misinformation detection by reconstructing entity descriptions from external knowledge sources and comparing them with original claims.

## Method Summary
The paper constructs MisBench through a pipeline that extracts claims from Wikidata, generates conflicting misinformation using LLaMA-3-70B, applies stylistic transformations, and filters through quality control using NLI and semantic matching. The RtD method improves detection by extracting entities, retrieving Wikipedia descriptions, reconstructing evidence via LLM, and comparing claims. Evaluation uses Success Rate% (correctly identified misinformation), Memorization Ratio (MR), and Evidence Tendency (TendCM). The approach is tested across multiple models (LLaMA3-8B/70B, Qwen2.5-3B/7B/14B/72B, Gemma2-2B/9B/27B) on the MisBench benchmark.

## Key Results
- LLMs are particularly vulnerable to factual contradictions and ambiguous semantic constructs
- Comparative tasks (two conflicting evidences) significantly outperform single-evidence binary classification
- RtD improves detection success rates by up to 20.6% on Gemma2-9B and 6.0% on Qwen2.5-14B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can identify misinformation through contextual inconsistencies even without prior parametric knowledge of the subject.
- **Mechanism:** The model detects surface-level coherence violations, logical gaps, and implausible claim-evidence relationships by comparing internal linguistic patterns rather than retrieving stored facts.
- **Core assumption:** LLMs encode generalized "plausibility heuristics" from training data that generalize to unseen misinformation patterns.
- **Evidence anchors:**
  - [abstract] "while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations"
  - [section 3.2] "LLMs are capable of discerning misinformation even without corresponding prior factual knowledge... while lack of prior knowledge reduces models' misinformation Success Rate% (average 12.6% drop for LLaMA3-8B), they still maintain reasonable performance"
  - [corpus] "The Effect of Belief Boxes and Open-mindedness on Persuasion" explores how belief states affect agent persuasion, suggesting LLMs have detectable belief-like structures (FMR=0.50)
- **Break condition:** When misinformation is crafted to be linguistically coherent and internally consistent (e.g., semantic conflicts with fabricated but plausible contexts), this mechanism degrades significantly—see the larger performance drops on semantic misinformation in Table 3.

### Mechanism 2
- **Claim:** LLMs perform significantly better at misinformation detection when given comparative tasks (two conflicting evidences) versus single-evidence binary classification.
- **Mechanism:** Comparative framing reduces false-positive bias by providing a concrete reference point; the model can leverage relative confidence scoring rather than absolute judgment.
- **Core assumption:** The model's internal representations encode sufficient signal to rank competing claims even when absolute certainty is low.
- **Evidence anchors:**
  - [section 3.3] "LLMs are better at distinguishing than solely judgment... LLMs achieve notably higher MR when evaluating contradictory evidence compared to single-evidence scenarios"
  - [section 4] "Building upon our empirical findings that 'LLMs perform better when comparing multiple pieces of conflicting information rather than making isolated judgments', we propose enhancing LLMs' misinformation-discerning capabilities"
  - [corpus] No direct corpus evidence on comparative vs. single judgment; related work on preference gaps (Mind the Gap, FMR=0.51) is adjacent but not directly supportive
- **Break condition:** When both evidences are misinformation or when the correct evidence is poorly written/formatted, comparative advantage may reverse.

### Mechanism 3
- **Claim:** External evidence reconstruction (RtD) improves misinformation detection by grounding comparison in retrieved factual descriptions.
- **Mechanism:** (1) Entity extraction focuses attention; (2) Wikipedia retrieval provides authoritative context; (3) LLM-generated reconstruction normalizes style differences; (4) Comparison task leverages Mechanism 2's comparative advantage.
- **Core assumption:** Retrieved descriptions are factually correct and the LLM can faithfully reconstruct evidence without hallucination.
- **Evidence anchors:**
  - [abstract] "Reconstruct to Discriminate (RtD), a method that enhances LLM misinformation detection by leveraging external evidence reconstruction... increasing Success Rate by 6.0% on Qwen2.5-14B and 20.6% on Gemma2-9B"
  - [section 4, Table 4] RtD shows substantial gains over baseline (e.g., LLaMA3-8B factual: 18.16→70.66, semantic: 11.75→78.67)
  - [corpus] No direct corpus evidence on RtD-style approaches; this is a novel contribution
- **Break condition:** When entity extraction fails, Wikipedia lacks coverage, or the LLM hallucinates during reconstruction—these failure modes are not analyzed in the paper.

## Foundational Learning

- **Concept: Knowledge Conflict Taxonomy** (Factual/Temporal/Semantic)
  - **Why needed here:** The paper's entire experimental design hinges on distinguishing these three conflict types; without this, you cannot interpret Tables 3-4 or Figure 4.
  - **Quick check question:** Given claim "X attended Stanford in 2039," is this a temporal or factual conflict?

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** Quality control (Section 2.4) uses NLI for entailment checking between claims and generated evidence.
  - **Quick check question:** If claim="X attended Yale" and evidence="X studied at Yale in 2020," does this entail, contradict, or remain neutral?

- **Concept: Memorization Ratio (MR) and Evidence Tendency (TendCM)**
  - **Why needed here:** These are the core evaluation metrics distinguishing parametric knowledge reliance vs. external evidence influence.
  - **Quick check question:** If an LLM chooses its memory answer 8 times and misinformation answer 2 times, what is MR? What is TendCM?

## Architecture Onboarding

- **Component map:** Wikidata → SPARQL → one-hop/multi-hop claims → Conflict Constructor → Misinformation Generator → Stylizer → Quality Controller → RtD Module
- **Critical path:** Quality control (step 5) is the bottleneck—NLI filtering and semantic matching remove ~45% of generated pairs (see Table 6: 765,583 claims → 347,892 after filtering).
- **Design tradeoffs:**
  - Scale vs. quality: 10.3M pieces generated, but quality control is computationally expensive (224+ hours for one-hop construction)
  - Synthetic vs. real: All misinformation is LLM-generated; may not capture real-world adversarial patterns
  - Coverage vs. depth: 12 domains covered, but temporal conflicts use future dates (synthetic scenario)
- **Failure signatures:**
  - Low TendCM scores with semantic misinformation indicate entity ambiguity confusion
  - High perplexity in narrative styles (Blog/Confident Language) correlates with lower detection—see Table 10
  - RtD shows minimal improvement on temporal misinformation for stronger models (Qwen2.5-14B: 99.27→95.68, suggesting ceiling effects)
- **First 3 experiments:**
  1. Reproduce Table 3 baseline on a 10K subset using the provided prompts (Table 16-19) to validate pipeline integration.
  2. Ablate RtD components: test (a) retrieval-only, (b) reconstruction-only, (c) full RtD to isolate mechanism contributions.
  3. Test style-transfer robustness: evaluate if stylizing correct evidence (rather than misinformation) to match misinformation style reduces detection—this probes whether style similarity is the confound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does multimodal misinformation (combining text with images, metadata, or other modalities) affect LLM behavior compared to text-only misinformation?
- Basis in paper: [explicit] The limitations section states: "Finally, we focus primarily on text-based content, and future work should consider the impact of metadata, visual content, and other forms of information that could influence LLM's convincingness towards misinformation."
- Why unresolved: MisBench and RtD are designed for text-only misinformation. The influence of visual cues, metadata, or cross-modal consistency on LLM vulnerability remains unexplored.
- What evidence would resolve it: Experiments extending MisBench to include multimodal misinformation (e.g., synthetic images, video, or structured metadata) and evaluating LLM performance changes with and without cross-modal alignment checks.

### Open Question 2
- Question: Can efficient methods be developed to identify and de-duplicate conflict pairs that already exist in pre-training corpora?
- Basis in paper: [explicit] The limitations section notes: "While conflict pairs may be extracted from pre-training corpora, the sheer volume of data makes it difficult to efficiently identify. In future work, we plan to explore additional methods for constructing conflict pairs to further validate the robustness of our dataset."
- Why unresolved: Current work uses synthetic generation, but pre-existing conflicts in training data could bias results or cause data contamination. Scalable identification is an open problem.
- What evidence would resolve it: Development and testing of algorithms (e.g., approximate nearest-neighbor search over entity-relation embeddings, or contrastive learning methods) that can detect knowledge conflicts across billions of pre-training tokens with high precision/recall, followed by validation on MisBench tasks.

### Open Question 3
- Question: Does the RtD (Reconstruct to Discriminate) method generalize to emerging forms of misinformation not covered in MisBench, such as adversarial perturbations or context-manipulated text?
- Basis in paper: [inferred] While the paper shows RtD's effectiveness on MisBench's defined misinformation types (factual, temporal, semantic) and styles, the abstract acknowledges the "evolving nature of misinformation" and the need for future work to address additional forms.
- Why unresolved: The benchmark and method focus on specific conflict patterns and stylistic variations. Robustness to new manipulation techniques (e.g., subtle rewording, persuasive framing, or logical fallacies) is not tested.
- What evidence would resolve it: Out-of-distribution tests evaluating RtD on novel misinformation types created by (1) adversarial attacks on entity mentions, (2) persuasion techniques from social science literature, or (3) logical fallacy injection, without retraining the underlying LLM or RtD prompt.

## Limitations
- MisBench relies entirely on synthetically generated misinformation rather than real-world adversarial examples, potentially limiting ecological validity
- RtD assumes Wikipedia contains complete and accurate descriptions for all entities, but this assumption breaks down for niche or emerging topics
- Quality control removes ~45% of generated pairs through computational filtering, raising concerns about systematic bias in what misinformation types survive the pipeline

## Confidence

- **High Confidence**: Baseline findings that LLMs struggle with factual contradictions and semantic ambiguities, and that comparative tasks improve detection—these align with established literature on LLM reasoning limitations
- **Medium Confidence**: The specific performance improvements from RtD (+6.0% to +20.6%)—while well-documented in controlled settings, the approach's dependence on external knowledge retrieval introduces variables not fully characterized in the paper
- **Low Confidence**: Claims about stylistic vulnerability (e.g., narrative styles being harder) require careful interpretation since style transfer itself may introduce confounding factors that weren't controlled for in the experiments

## Next Checks
1. Test RtD on real-world misinformation samples (e.g., from fact-checking databases) to validate whether synthetic benchmark performance generalizes to authentic adversarial examples
2. Conduct ablation studies isolating each RtD component (entity extraction, retrieval, reconstruction, comparison) to quantify their individual contributions and identify potential failure points
3. Evaluate RtD's performance on temporally ambiguous cases where both claims could be technically true but contextually misleading, which represents a common real-world misinformation pattern not fully explored in the current framework