---
ver: rpa2
title: 'TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive
  Density Estimation'
arxiv_id: '2602.01135'
source_url: https://arxiv.org/abs/2602.01135
tags:
- causal
- trace
- discovery
- event
- single
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRACE introduces a scalable framework for causal discovery from
  single, high-dimensional event sequences by repurposing pretrained autoregressive
  models as density estimators. Unlike traditional methods requiring repeated samples
  or multi-stream data, TRACE processes a single sequence using Monte Carlo estimation
  of conditional mutual information (CMI) to infer the summary causal graph between
  event types.
---

# TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation

## Quick Facts
- arXiv ID: 2602.01135
- Source URL: https://arxiv.org/abs/2602.01135
- Authors: Hugo Math; Rainer Lienhart
- Reference count: 40
- Primary result: F1 ≈0.91 on synthetic linear SCMs with vocabularies exceeding 29,100 event types

## Executive Summary
TRACE introduces a scalable framework for causal discovery from single, high-dimensional event sequences by repurposing pretrained autoregressive models as density estimators. Unlike traditional methods requiring repeated samples or multi-stream data, TRACE processes a single sequence using Monte Carlo estimation of conditional mutual information (CMI) to infer the summary causal graph between event types. By parallelizing CMI computation on GPUs and leveraging the learned probabilistic dynamics of autoregressive models, TRACE scales linearly with vocabulary size, bypassing combinatorial explosions. Experiments on synthetic structural causal models show superior performance (F1 ≈0.91) compared to baselines, maintaining stability across vocabularies exceeding 29,100 event types.

## Method Summary
TRACE operates in two phases: first, a standard autoregressive model (e.g., Transformer/LLaMA) is trained on a corpus of sequences to minimize cross-entropy loss. Second, this frozen model acts as a probability engine to estimate CMI via Monte Carlo simulations. The framework constructs intervention tensors where intermediate events are replaced by noise samples to simulate do-interventions, enabling parallel computation of CMI across all event pairs. By projecting a dense instance time causal graph onto a sparse summary causal graph, TRACE achieves linear complexity with vocabulary size while maintaining theoretical identifiability guarantees under $\epsilon$-Strong Faithfulness assumptions.

## Key Results
- Achieves F1 ≈0.91 on synthetic linear SCMs with vocabularies up to 29,100 event types
- Scales linearly with vocabulary size, avoiding combinatorial explosion of traditional DAG learning
- Maintains stable performance across sequence lengths while baselines degrade significantly

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive Density Estimation as a Proxy for Causal Dynamics
TRACE decouples learning from discovery by training an autoregressive model to approximate the true data-generating process, then using this model as an $\epsilon$-Oracle to estimate conditional probabilities required for causal discovery. If the KL divergence between the model and true DGP is bounded by $\epsilon$, the causal identification error is also bounded, enabling amortized structure learning.

### Mechanism 2: Parallelized Conditional Mutual Information via Simulated Interventions
The framework parallelizes traditional CI tests by constructing intervention tensors where intermediate events are replaced by noise samples, simulating do(M~Q) operations to block backdoor paths. This enables GPU-accelerated estimation of CMI by comparing KL divergences of probability distributions output by the frozen autoregressive model under different contexts.

### Mechanism 3: Linear Complexity via Sparse Graph Projection
Instead of searching the super-exponential space of all DAGs, TRACE identifies parents for each event in the observed sequence and aggregates them. The sparse approximation variant bounds memory horizon m, changing memory complexity from quadratic O(L²) to linear O(mL), enabling processing of massive vocabularies that cause combinatorial explosions in constraint-based methods.

## Foundational Learning

- **Conditional Mutual Information (CMI)**: Core metric distinguishing causation from correlation by quantifying information gained about future events given past events and history. Quick check: If I(Xₜ₊₁; Xₜ | X<ₜ) = 0, what does that imply about the causal relationship between Xₜ and Xₜ₊₁?

- **$\epsilon$-Strong Faithfulness**: Concept explaining why TRACE works with imperfect models by requiring true causal signals to exceed approximation noise threshold τε. Quick check: Why is the standard assumption of "Faithfulness" (exact zeros) considered untenable for deep learning models trained on finite data?

- **Autoregressive Factorization**: The framework relies on chain rule probability P(s) = ∏P(xᵢ | x<ᵢ), requiring understanding of how Transformers model next-token probabilities. Quick check: How does the "Teacher Forcing" regime affect the estimation of history distribution X<ₜ compared to sampling from the model itself?

## Architecture Onboarding

- **Component map**: Dataset → Autoregressive Backbone (e.g., LLaMA) → Next-Token Prediction Loss; Single Sequence s → Context Truncation → Parallel CMI Module (Constructs Intervention Tensor Xdo) → Frozen Backbone → Probability Tensor Praw → CMI Estimator ÎN → Thresholding τ → Instance Summary Graph Gs

- **Critical path**: Construction of intervention tensor Xdo with staircase intervention pattern. This tensor must batch observed history with randomized mediators to enable parallel KL-divergence computation.

- **Design tradeoffs**: Full variant captures long-range dependencies but requires O(L²) memory; sparse variant runs in linear time/memory O(mL) but assumes finite memory horizon m. Precision vs. recall controlled by threshold τ.

- **Failure signatures**: High precision, low recall indicates conservative model or aggressive threshold; high SHD with random baselines indicates AR model failed to learn DGP; performance collapse on unseen lengths indicates context truncation discarding vital causal history.

- **First 3 experiments**: 1) Train AR backbone on small synthetic SCM and plot Oracle Score ε̂ until ε̂ ≈ 0.05; 2) Run TRACE on validation sequence and sweep threshold τ to identify τopt where F1 peaks; 3) Vary Monte Carlo particles N to confirm CMI estimation stabilizes as N increases.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does TRACE perform on highly non-linear Structural Causal Models with multiplicative interactions or threshold effects?
- **Basis**: Experimental validation relies exclusively on synthetic linear SCM data while method claims non-parametric nature
- **Why unresolved**: It's unclear if identifiability results hold when causal mechanisms are complex non-linear functions harder for AR models to approximate
- **What evidence would resolve it**: Evaluation on synthetic benchmarks with known non-linear additive or multiplicative noise models to measure degradation in SHD

### Open Question 2
- **Question**: How can TRACE be extended to handle instantaneous causal effects between events at identical time indices?
- **Basis**: Appendix C.1.2 states TRACE assumes Temporal Precedence and doesn't model instantaneous effects
- **Why unresolved**: Real-world event logs often have multiple distinct events sharing timestamps due to batching effects or limited sensor resolution
- **What evidence would resolve it**: Theoretical extension or algorithmic modification allowing recovery of instantaneous edges X→Y when tX = tY

### Open Question 3
- **Question**: How sensitive is CMI estimator to violations of Causal Sufficiency caused by unobserved confounders?
- **Basis**: While assuming Causal Sufficiency, Appendix C.1.1 only tests noise injection and missing intermediaries, not scenarios with unobserved variables creating spurious correlations
- **Why unresolved**: Constraint-based methods generate false positives with latent confounders; it's unverified if TRACE's AR density estimator dampens these spurious signals
- **What evidence would resolve it**: Experiments on synthetic graphs with deliberately omitted variables to measure false positive rates compared to fully observed setting

### Open Question 4
- **Question**: What is the theoretical bias introduced by context truncation required for parallelization?
- **Basis**: Section 5.1 mentions truncation "might break Markovianity" and relies on empirical robustness without theoretical error bounds
- **Why unresolved**: If true causal memory m exceeds truncated context c, CMI calculation may condition on incomplete history potentially failing to block back-door paths
- **What evidence would resolve it**: Derivation of approximation error specifically for truncated conditioning set, showing how error scales with difference between true memory m and context window c

## Limitations
- Requires strong Markov assumption for sparse variant, which may not hold for systems with long-range dependencies
- Performance degrades when autoregressive model fails to accurately approximate true data-generating process
- Assumes causal sufficiency with all relevant confounders observed in event vocabulary

## Confidence
- **High Confidence**: Linear scaling claim and parallelized CMI computation mechanism are well-supported by theoretical analysis and empirical results
- **Medium Confidence**: Theoretical bounds on identifiability under imperfect approximations rely on specific assumptions that may not generalize
- **Low Confidence**: Exact hyperparameter requirements for optimal performance are underspecified and likely dataset-dependent

## Next Checks
1. **Robustness to Model Error**: Systematically vary autoregressive model's training duration and measure degradation in causal discovery performance to identify critical ε threshold beyond which identification fails

2. **Long-Range Dependency Test**: Evaluate TRACE on synthetic SCMs with controlled long-range causal effects (lag > m) to quantify performance collapse when memory assumption is violated

3. **Hidden Confounder Simulation**: Create synthetic datasets with known hidden confounders and measure how often TRACE incorrectly identifies spurious edges or misses true causal relationships