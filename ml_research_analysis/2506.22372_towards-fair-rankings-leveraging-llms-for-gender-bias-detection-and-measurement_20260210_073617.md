---
ver: rpa2
title: 'Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement'
arxiv_id: '2506.22372'
source_url: https://arxiv.org/abs/2506.22372
tags:
- bias
- gender
- metric
- llms
- male
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in passage ranking systems, highlighting
  limitations of existing lexical-based metrics that miss subtle biases. The authors
  propose using large language models (LLMs) to detect gender bias by classifying
  documents as male-biased, female-biased, or neutral, then introduce a novel metric
  called Class-wise Weighted Exposure (CWEx) to measure fairness in rankings.
---

# Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement

## Quick Facts
- **arXiv ID**: 2506.22372
- **Source URL**: https://arxiv.org/abs/2506.22372
- **Reference count**: 40
- **Primary result**: LLM-based gender bias detection achieves 58.77% agreement with human judgments versus 18.51% for lexical metrics

## Executive Summary
This paper addresses the challenge of detecting and measuring gender bias in passage ranking systems, where traditional lexical-based metrics fail to capture subtle biases. The authors propose a novel approach using large language models (LLMs) to classify documents as male-biased, female-biased, or neutral, then introduce a Class-wise Weighted Exposure (CWEx) metric to quantify fairness in rankings. Their method is evaluated on the MS MARCO Passage Ranking dataset, demonstrating significantly better alignment with human judgments compared to existing metrics. The work highlights the potential of LLMs for nuanced bias detection in information retrieval systems.

## Method Summary
The authors propose a two-stage approach: first, they use LLMs to classify passages as male-biased, female-biased, or neutral; second, they introduce the CWEx metric to measure fairness in rankings. They create the MSMGenderBias dataset by annotating a subset of MS MARCO Passage Ranking data with gender bias labels. The LLM-based classification system is then used to evaluate multiple ranking models, measuring how fairly they expose content to different gender groups. The approach leverages the semantic understanding capabilities of LLMs to detect subtle gender bias that lexical metrics miss.

## Key Results
- LLM-based classification achieves 58.77% agreement with human judgments (Cohen's Kappa) versus 18.51% for traditional lexical metrics
- The CWEx metric effectively quantifies gender bias exposure differences across ranking positions
- The method successfully distinguishes between ranking models with varying levels of gender bias
- Results demonstrate the superiority of semantic-based approaches over lexical metrics for bias detection

## Why This Works (Mechanism)
The approach works by leveraging LLMs' semantic understanding capabilities to detect nuanced gender bias patterns that lexical metrics miss. LLMs can recognize contextual cues, implicit associations, and subtle language patterns that indicate bias toward specific gender groups. By classifying documents into gender-biased categories and measuring their exposure in rankings through the CWEx metric, the system captures both the presence of bias and its impact on ranking fairness. This semantic approach overcomes the limitations of surface-level keyword matching used in traditional metrics.

## Foundational Learning

**Gender Bias Detection**: Understanding how to identify and measure bias in text-based systems - needed to ensure fair information access across demographic groups - quick check: verify bias detection methods capture both explicit and implicit gender associations

**Ranking Fairness Metrics**: Principles of measuring fairness in ranked results - needed to evaluate whether ranking systems treat different groups equitably - quick check: confirm metrics account for position bias and exposure differences

**LLM Semantic Understanding**: How large language models interpret context and nuance - needed to leverage LLMs for sophisticated bias detection - quick check: validate LLM classifications against human judgments across diverse examples

**Exposure Measurement**: Quantifying how much content from different groups users see in rankings - needed to assess real-world impact of ranking biases - quick check: ensure exposure calculations account for user attention decay with ranking position

## Architecture Onboarding

**Component Map**: MS MARCO dataset -> LLM bias classifier -> Gender-biased document classification -> CWEx metric calculation -> Ranking model evaluation

**Critical Path**: Document classification → CWEx calculation → Fairness assessment. The LLM must first accurately classify gender bias, then CWEx must correctly compute exposure differences to produce meaningful fairness scores.

**Design Tradeoffs**: LLM-based classification offers superior semantic understanding but requires significant computational resources and lacks full human validation; CWEx provides comprehensive fairness measurement but needs broader validation beyond gender bias.

**Failure Signatures**: Poor LLM classification accuracy leads to incorrect bias labels; inadequate dataset diversity results in biased annotations; CWEx may not capture intersectional biases or other fairness dimensions.

**First Experiments**: 1) Validate LLM classifications with human annotators across diverse demographic perspectives, 2) Test CWEx on multiple ranking datasets beyond MS MARCO, 3) Evaluate detection of intersectional biases combining gender with other attributes.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- LLM-based classification lacks extensive human validation, introducing uncertainty about annotation accuracy
- MS MARCO dataset may not represent diverse gender expressions or non-binary perspectives, limiting generalizability
- CWEx metric has not been validated against broader fairness criteria beyond gender bias in rankings

## Confidence
- **High confidence** in the identification of lexical metrics' limitations for subtle bias detection
- **Medium confidence** in the alignment between LLM-based classification and human judgment (58.77% agreement), given the moderate Cohen's Kappa value
- **Medium confidence** in the practical utility of CWEx as a fairness metric, pending broader validation

## Next Checks
1. Conduct a comprehensive human annotation study to validate LLM-based gender bias classifications across diverse demographic perspectives
2. Test the CWEx metric on multiple ranking datasets beyond MS MARCO to assess generalizability
3. Evaluate whether the proposed approach can detect intersectional biases (e.g., gender combined with ethnicity or age) in ranking systems