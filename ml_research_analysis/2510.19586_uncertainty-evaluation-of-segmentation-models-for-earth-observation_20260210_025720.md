---
ver: rpa2
title: Uncertainty evaluation of segmentation models for Earth observation
arxiv_id: '2510.19586'
source_url: https://arxiv.org/abs/2510.19586
tags:
- uncertainty
- noise
- segmentation
- image
- pixels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates methods for estimating uncertainty in
  semantic segmentation predictions derived from satellite imagery, focusing on per-pixel
  uncertainty estimation for remote sensing and Earth observation applications. The
  authors benchmark existing methods, including Stochastic Segmentation Networks (SSN),
  ensembles, and uncertainty metrics like normalized entropy and maximum probability,
  on two datasets: PASTIS and ForTy, which differ in scale, geographic coverage, and
  label confidence.'
---

# Uncertainty evaluation of segmentation models for Earth observation

## Quick Facts
- **arXiv ID:** 2510.19586
- **Source URL:** https://arxiv.org/abs/2510.19586
- **Reference count:** 18
- **One-line primary result:** Vision Transformer-based models substantially outperform convolutional models for uncertainty estimation in semantic segmentation of satellite imagery, with SSN variants showing improved segmentation and noise robustness.

## Executive Summary
This paper systematically benchmarks uncertainty estimation methods for semantic segmentation of satellite imagery, evaluating both pixel-level error detection and image-level quality assessment across two Earth observation datasets. The authors find that while common uncertainty metrics show limited effectiveness at identifying individual misclassified pixels, they are substantially more effective at identifying poorly segmented images when evaluated at the image level. Vision Transformer architectures demonstrate clear superiority over convolutional models for uncertainty estimation, and Stochastic Segmentation Networks with Gaussian latent layers provide improved segmentation performance and noise robustness compared to deterministic models.

## Method Summary
The study evaluates uncertainty estimation for semantic segmentation on satellite imagery using two datasets: PASTIS (2,433 Sentinel-2 time series) and ForTy (~200,000 samples). Three model architectures are tested: U-TAE (temporal CNN+attention), UNET3D (3D CNN), and TSViT (Transformer). Four uncertainty estimation approaches are compared: deterministic models with maxprob/entropy metrics, ensembles of 5 models, and Stochastic Segmentation Networks (SSNs) with spatially correlated Gaussian latent layers. Evaluation uses Uncertainty-Error PR curves for pixel-wise error detection, Uncertainty-Noise PR for detecting corrupted pixels, and image-level correlation between aggregated uncertainty and segmentation quality (mIoU). Models are trained with standard cross-entropy loss, and noise injection experiments test robustness to corrupted inputs.

## Key Results
- Vision Transformer architectures substantially outperform convolutional models for uncertainty estimation across all evaluated metrics and datasets
- Common uncertainty estimation methods are more effective at image-level quality assessment than pixel-level error identification
- SSNs with Gaussian latent layers improve segmentation performance in both noise-free settings and when training with noisy inputs compared to standard models
- Entropy-based uncertainty metrics better detect noisy inputs than maxprob, while maxprob better identifies misclassified pixels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Vision Transformer (ViT) architectures produce better-calibrated uncertainty estimates than convolutional architectures for semantic segmentation.
- **Mechanism:** ViTs likely maintain better calibration through their global attention mechanism, which may preserve uncertainty signals across spatial contexts rather than diluting them through local convolution operations. The self-attention mechanism allows the model to maintain uncertainty information across the entire image context during prediction.
- **Core assumption:** The architectural inductive biases of Transformers (global attention, patch-based processing) are inherently better suited to preserving uncertainty information than the local receptive fields of CNNs.
- **Evidence anchors:**
  - [abstract] "The choice of model architecture significantly impacts uncertainty estimation, with Vision Transformer-based models substantially outperforming convolutional models in identifying segmentation errors."
  - [section 6.1] "The most prominent pattern in these results is the clear superiority of the Transformer architecture over the other two, for all model/uncertainty metric combinations."
  - [corpus] Limited direct corpus support for this specific mechanism; neighbor papers focus on domain adaptation and segmentation tasks rather than uncertainty calibration comparisons.
- **Break condition:** If convolutional architectures with specific calibration techniques (e.g., temperature scaling, focal loss) match or exceed ViT performance, the architectural advantage would be attributed to calibration method rather than architecture.

### Mechanism 2
- **Claim:** Stochastic Segmentation Networks (SSNs) with spatially correlated latent variables improve both segmentation accuracy and noise robustness compared to deterministic models.
- **Mechanism:** SSNs model the joint distribution of logits across all pixels using a Gaussian latent layer with a low-rank covariance decomposition (Σ = PP^T + D), enabling the model to capture spatial dependencies in uncertainty. During training, the model learns to generate correlated noise patterns that reflect realistic spatial uncertainty structures. At inference, multiple samples from this distribution provide diverse plausible segmentations, and the variance across samples captures uncertainty.
- **Core assumption:** Spatially correlated uncertainty is a more faithful representation of true prediction uncertainty than per-pixel independent uncertainty estimates; the low-rank approximation (R∼10) sufficiently captures the relevant covariance structure.
- **Evidence anchors:**
  - [abstract] "SSNs with Gaussian latent layers improve segmentation performance in both noise-free settings and when training with noisy inputs compared to standard Transformer-based models."
  - [section 4] "To obtain spatially consistent predictions however we need a noise model over the whole segmentation mask where noise across pixels is correlated."
  - [section 7.1] "SSN models consistently benefited more from training with noise than standard models, sometimes even outperforming ensembles."
  - [corpus] No direct corpus validation; neighbor papers do not address SSN mechanisms.
- **Break condition:** If increasing the rank R beyond ~10 yields substantial performance gains, or if independent pixel-wise uncertainty methods match SSN performance on spatially correlated tasks, the covariance modeling benefit would be questionable.

### Mechanism 3
- **Claim:** Uncertainty estimation methods are substantially more effective at image-level quality assessment than pixel-level error identification.
- **Mechanism:** Aggregating pixel-wise uncertainty metrics (via mean or median) to the image level averages out local noise and idiosyncratic errors, producing a more stable signal that correlates with overall segmentation difficulty. Individual pixel uncertainty estimates are noisy because local errors can arise from multiple sources (label noise, boundary ambiguity, feature confusion), but image-level aggregation reveals the overall "difficulty signal."
- **Core assumption:** The correlation between aggregated uncertainty and segmentation quality (mIoU) reflects genuine model uncertainty rather than artifacts of the aggregation method or dataset characteristics.
- **Evidence anchors:**
  - [abstract] "Common uncertainty estimation methods demonstrate limited performance in the task of identifying misclassified pixels at test time. However, these methods are substantially more effective at identifying poorly segmented images when evaluated at the image level."
  - [section 6.1] "Figure 6 reveals a strong relationship, suggesting that uncertainty methods might be better at assessing the overall difficulty of an example than at identifying individual pixel errors."
  - [corpus] Weak corpus support; Kahl et al. (2024) mentioned in paper focuses on image-level evaluation, but not directly in neighbor corpus.
- **Break condition:** If pixel-level uncertainty methods with different aggregation strategies (weighted by class frequency, spatial clustering, superpixel-based) substantially outperform simple mean/median aggregation, the mechanism would involve more sophisticated spatial reasoning.

## Foundational Learning

- **Concept:** Aleatoric vs. Epistemic Uncertainty
  - **Why needed here:** The paper explicitly notes it does NOT separate these uncertainty types, but understanding the distinction is critical for interpreting results. Aleatoric uncertainty (data noise) vs. epistemic uncertainty (model uncertainty) have different implications for error detection and OOD detection.
  - **Quick check question:** If a model trained on clean data encounters a corrupted input region, which uncertainty type primarily increases? (Answer: Primarily epistemic, as this is a distribution shift, but aleatoric may also increase if the corruption adds noise.)

- **Concept:** Probability Calibration
  - **Why needed here:** The maxprob and entropy metrics assume the model's output probabilities reflect true correctness likelihood. Poor calibration (overconfident or underconfident outputs) directly degrades uncertainty estimation quality.
  - **Quick check question:** A model outputs probability 0.9 for a class that is correct only 60% of the time—what type of miscalibration is this? (Answer: Overconfidence; the model is not calibrated.)

- **Concept:** Low-Rank Covariance Approximation
  - **Why needed here:** SSN uses Σ = PP^T + D to make full covariance learning feasible. Understanding this approximation is essential for debugging SSN behavior and determining if rank R is sufficient.
  - **Quick check question:** With R=10 and output dimension SK=128×128×18 (for PASTIS), what is the parameter reduction ratio for the covariance? (Answer: Original would be (SK)^2 ≈ 6.7B parameters; low-rank uses SK×(R+1) ≈ 3M parameters—a 2000× reduction.)

## Architecture Onboarding

- **Component map:**
  1. **Base Architecture**: Choose from TSViT (Transformer), U-TAE (temporal CNN+attention), or UNET3D (3D CNN)
  2. **Uncertainty Head** (for SSN): Extends output layer from (H, W, K) to (H, W, (2+R)K) to output Gaussian parameters (μ, diagonal σ, and low-rank components P)
  3. **Ensemble Wrapper**: Trains N independent models (N=5 in paper) with different seeds
  4. **Uncertainty Metrics Module**: Computes maxprob, entropy, inter-model variance, or sampled categorical variation
  5. **Evaluation Framework**: UE-PR curves (uncertainty-error precision-recall), noise detection PR curves

- **Critical path:**
  1. Start with **TSViT base model** on your dataset—this architecture consistently outperforms others for uncertainty tasks
  2. Implement **maxprob uncertainty metric** first—it outperforms entropy for error detection and is trivial to compute
  3. If you need improved performance, implement **SSN variant** of TSViT (only 1.01× parameter increase)
  4. Use **image-level aggregation** (mean) for quality assessment, not pixel-level error detection
  5. Create a **dedicated uncertainty test set** before deployment

- **Design tradeoffs:**
  - **Ensembles vs. SSN**: Ensembles (5× parameters) provide best segmentation + uncertainty; SSNs (1.01× parameters) provide nearly-as-good performance with minimal overhead
  - **Entropy vs. Maxprob**: Maxprob better for error detection; entropy better for noise/OOD detection
  - **Training with noise**: Improves robustness but reduces ability to detect noise at inference (model adapts to noise distribution)
  - **Rank R in SSN**: Higher R captures more covariance structure but increases compute/memory; R=10 worked well for 128×128 images

- **Failure signatures:**
  - **Pixel-level UE-PR curves near random baseline**: Normal—the paper shows this is expected; switch to image-level evaluation
  - **Ensemble inter-model variance underperforming maxprob**: Normal—paper shows model-specific metrics often underperform
  - **SSN mIoU lower than base model**: Check hyperparameters—SSN needs different learning dynamics (scaling factors for P, D matrices)
  - **High uncertainty on correct predictions, low on errors**: Model is miscalibrated; consider temperature scaling or different loss functions
  - **Unable to detect corrupted regions when trained with noise**: Expected—model adapts to noise; only detectable when test noise >> train noise

- **First 3 experiments:**
  1. **Baseline calibration check**: Train TSViT base model, compute maxprob uncertainty, plot image-level uncertainty vs. per-image mIoU—verify strong correlation as sanity check
  2. **SSN vs. base comparison**: Train TSViT-SSN with R=10 alongside base TSViT, compare mIoU and UE-PR curves—SSN should show slight mIoU improvement and comparable or better uncertainty
  3. **Noise detection sensitivity**: Add structured noise (elliptical or object-level) at varying intensities to test set, evaluate entropy-based detection at different train/test noise level combinations—verify that detection only works when test noise >> train noise

## Open Questions the Paper Calls Out

- **Open Question 1:** Does aggregating uncertainty at intermediate granularities (e.g., superpixels) improve the detection of segmentation errors compared to standard pixel-level analysis?
  - **Basis in paper:** [explicit] The authors suggest exploring uncertainty at "intermediate granularities" because pinpointing pixel-level errors proved difficult, though image-level metrics correlated well with quality.
  - **Why unresolved:** It is currently undefined what constitutes the "optimal scale" for these regions or how to effectively aggregate uncertainty within them to bridge the gap between pixel and image performance.
  - **What evidence would resolve it:** A benchmark demonstrating superior Precision-Recall for identifying segmentation errors when uncertainty is aggregated over superpixels compared to raw pixel-wise maps.

- **Open Question 2:** How does the choice of loss function (e.g., Dice or Focal loss) impact the calibration and quality of uncertainty estimates in deep segmentation models?
  - **Basis in paper:** [explicit] The conclusion explicitly identifies investigating the impact of different loss functions on uncertainty calibration as a necessary step toward improved architecture-loss combinations.
  - **Why unresolved:** The study benchmarked architectures and model types (SSN, ensembles) but did not vary the loss functions to observe their effect on uncertainty metrics like entropy or maxprob.
  - **What evidence would resolve it:** A comparative study measuring calibration error and Uncertainty-Error PR curves on the PASTIS or ForTy datasets using identical architectures trained with different loss functions.

- **Open Question 3:** Can the spatially correlated uncertainty modeled by Stochastic Segmentation Networks (SSNs) be utilized to capture uncertainty information that is inaccessible to models assuming pixel independence?
  - **Basis in paper:** [explicit] The authors state that testing whether SSNs "capture additional uncertainty information" due to modeling spatial dependence is an interesting direction for future work.
  - **Why unresolved:** While SSNs showed improved segmentation performance, it is unclear if the spatial correlation mechanism specifically provides "richer" uncertainty insights distinct from simple performance gains.
  - **What evidence would resolve it:** Experiments showing SSNs outperforming uncorrelated baselines specifically in tasks requiring spatial consistency, such as detecting structured noise artifacts or object-level boundary errors.

## Limitations

- **Pixel-level uncertainty estimation remains fundamentally challenging** - the paper confirms that common uncertainty metrics perform poorly at identifying misclassified pixels, suggesting fundamental limitations in how uncertainty is modeled or aggregated for this task.
- **Architecture-specific performance variability** - uncertainty estimation quality varies substantially across model architectures, making direct comparison difficult without controlling for architectural differences.
- **Limited generalization to other domains** - results are specific to Earth observation semantic segmentation with Sentinel-2 imagery; performance on natural images or different remote sensing modalities may differ substantially.

## Confidence

- **High Confidence:** Transformer architectures significantly outperform convolutional models for uncertainty estimation (strong empirical support across multiple metrics and datasets)
- **Medium Confidence:** SSNs with Gaussian latent layers improve segmentation performance and noise robustness (supported by ablation studies, but architecture-specific results)
- **Medium Confidence:** Image-level uncertainty aggregation correlates strongly with segmentation quality (strong correlation observed, but aggregation method sensitivity not fully explored)
- **Low Confidence:** The specific mechanism by which transformer architectures better preserve uncertainty information (while strongly supported empirically, the underlying theoretical explanation is speculative)

## Next Checks

1. **Architecture-agnostic uncertainty calibration**: Test temperature scaling and focal loss calibration on convolutional architectures to determine if transformer superiority is due to architecture or calibration methods
2. **Spatial uncertainty aggregation methods**: Compare mean/median aggregation against weighted aggregation (by class frequency or spatial clustering) to determine if sophisticated spatial reasoning improves pixel-level uncertainty estimation
3. **Rank sensitivity analysis**: Systematically vary the rank R parameter in SSNs beyond R=10 to determine if the low-rank approximation is sufficient for capturing spatial uncertainty structure