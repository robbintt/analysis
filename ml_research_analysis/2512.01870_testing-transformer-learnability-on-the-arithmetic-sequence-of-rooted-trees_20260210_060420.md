---
ver: rpa2
title: Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees
arxiv_id: '2512.01870'
source_url: https://arxiv.org/abs/2512.01870
tags:
- sequence
- words
- prime
- tokens
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tests whether a transformer network (GPT-2) can learn
  the internal grammar of a deterministic sequence of rooted trees derived from the
  prime factorizations of natural numbers. Each integer is mapped to a rooted planar
  tree, which is encoded as a Dyck word, forming an arithmetic text with measurable
  statistical structure.
---

# Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees

## Quick Facts
- arXiv ID: 2512.01870
- Source URL: https://arxiv.org/abs/2512.01870
- Reference count: 12
- Primary result: GPT-2 partially learns arithmetic structure from prime-factorization-derived tree sequences, achieving word-level accuracy around 0.3 on next-word prediction

## Executive Summary
This paper investigates whether transformer networks can learn the internal grammar of an arithmetic sequence derived from prime factorizations. The authors map natural numbers to rooted planar trees, encode them as Dyck words, and train a GPT-2 model from scratch on this arithmetic text. The model achieves moderate success on both next-word prediction and masked language modeling tasks, outperforming Markov chain baselines and capturing non-trivial statistical regularities in the arithmetic sequence. However, the model struggles with long-range arithmetic dependencies, particularly for prime numbers.

## Method Summary
The authors create an arithmetic text by mapping natural numbers to rooted planar trees through prime factorization, then encoding these trees as Dyck words. They train a GPT-2 model from scratch on the first 10^11 such encoded integers. The model is evaluated on two tasks: next-word prediction (NWP) and masked language modeling (MLM). Performance is measured using word-level accuracy, KL divergence, and token-level accuracy across different temperatures and masking rates. A Markov chain baseline is used for comparison to assess whether the transformer captures more than simple statistical patterns.

## Key Results
- GPT-2 outperforms Markov chain baseline with word-level accuracy around 0.3 and KL divergence of 0.02-0.03 on NWP task
- Model achieves moderate precision and recall (~0.3) for predicting primes and square-free numbers
- Token-level accuracy reaches up to 0.4 at low temperature and masking rates on MLM task
- Model captures non-trivial statistical regularities but struggles with long-range arithmetic dependencies, especially for prime numbers

## Why This Works (Mechanism)
The transformer learns statistical patterns in the Dyck word representation of prime-factorization-derived trees. The model captures surface-level regularities in the encoded arithmetic sequence through attention mechanisms that recognize frequent patterns in the tree structures. However, the limited success with prime numbers suggests the model relies more on local statistical dependencies than deep understanding of multiplicative arithmetic relationships.

## Foundational Learning
- **Dyck words and context-free grammars**: Why needed - To encode rooted planar trees in a way that preserves structural information for the transformer. Quick check - Verify that the Dyck word representation maintains the tree structure information needed for arithmetic pattern recognition.
- **Prime factorization arithmetic**: Why needed - Forms the fundamental mathematical structure underlying the tree sequence. Quick check - Confirm that the tree construction correctly represents the multiplicative relationships between numbers.
- **Transformer attention mechanisms**: Why needed - To capture dependencies between different parts of the encoded tree sequences. Quick check - Examine attention patterns to see if the model focuses on mathematically relevant positions.
- **Statistical evaluation metrics**: Why needed - To quantify the model's ability to predict and reconstruct the arithmetic sequence. Quick check - Compare metric distributions between training and test sets to assess generalization.

## Architecture Onboarding

**Component Map**
GPT-2 (self-attention layers -> feed-forward networks -> output projection) -> Evaluation metrics (accuracy, KL divergence)

**Critical Path**
Input encoding -> Multi-head self-attention -> Feed-forward layers -> Output projection -> Loss computation -> Parameter updates

**Design Tradeoffs**
- Large model capacity vs. training efficiency: GPT-2 provides strong representational power but requires significant computational resources
- Tokenization vs. character-level encoding: Tokenization improves efficiency but may lose fine-grained structural information
- Encoding choices: Dyck words preserve structure but may introduce artificial regularities that help the model

**Failure Signatures**
- Low performance on prime numbers indicates inability to capture fundamental arithmetic building blocks
- Temperature sensitivity in MLM suggests the model relies heavily on local statistical patterns
- Gap between training and test performance would indicate overfitting to surface-level patterns

**First 3 Experiments**
1. Train on a subset of the arithmetic sequence (e.g., only composite numbers) to isolate the model's ability to learn multiplication patterns
2. Evaluate on completely unseen number ranges (e.g., primes > 10^9) to test genuine generalization vs. memorization
3. Compare different tree encodings (binary vs. planar) to determine if success depends on encoding artifacts

## Open Questions the Paper Calls Out
None

## Limitations
- Model's understanding of multiplicative arithmetic relationships remains unclear, as performance on fundamental building blocks (primes) is limited
- Dyck word encoding may introduce artificial regularities that help the model without requiring genuine arithmetic understanding
- Difficulty distinguishing between memorization of frequent patterns versus true generalization of arithmetic rules

## Confidence
- **Medium**: The model outperforms Markov baselines and captures non-trivial statistical regularities
- **Medium**: The model shows partial success on prime and square-free number prediction
- **Low**: The model's actual understanding of multiplicative arithmetic relationships
- **Medium**: The general conclusion that transformers can partially learn arithmetic structure

## Next Checks
1. Test model generalization by evaluating on numbers far outside the training range (e.g., primes > 10^9) to distinguish memorization from genuine arithmetic learning
2. Compare performance using different tree encodings (binary vs. planar, Dyck vs. bracket-free representations) to isolate whether success depends on encoding artifacts
3. Implement controlled ablation studies removing specific arithmetic patterns from training data to measure which statistical regularities the model actually captures