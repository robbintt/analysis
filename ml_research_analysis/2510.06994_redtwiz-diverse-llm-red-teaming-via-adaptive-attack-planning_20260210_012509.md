---
ver: rpa2
title: 'RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning'
arxiv_id: '2510.06994'
source_url: https://arxiv.org/abs/2510.06994
tags:
- code
- attack
- defender
- malicious
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RedTWIZ introduces a comprehensive framework for automated red
  teaming of large language models in AI-assisted software development. The system
  combines automated jailbreak assessment using specialized judge models, a diverse
  multi-turn attack suite covering coding and cybersecurity threats, and a hierarchical
  reinforcement learning-based planner that adapts attack strategies based on model-specific
  weaknesses.
---

# RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning

## Quick Facts
- arXiv ID: 2510.06994
- Source URL: https://arxiv.org/abs/2510.06994
- Reference count: 40
- One-line primary result: Introduces adaptive red teaming framework for AI-assisted software development, achieving attack success rates from 12% to 87% across multiple defender models.

## Executive Summary
RedTWIZ presents a comprehensive framework for automated red teaming of large language models in AI-assisted software development contexts. The system combines automated jailbreak assessment using specialized judge models, a diverse multi-turn attack suite covering coding and cybersecurity threats, and a hierarchical reinforcement learning-based planner that adapts attack strategies based on model-specific weaknesses. Extensive evaluation across multiple defender models demonstrates the framework's effectiveness in eliciting unsafe completions through adaptive conversational attacks, with attack success rates ranging from 12% to 87% depending on the target model and strategy employed.

## Method Summary
RedTWIZ employs a hierarchical attack planning approach where a probing stage (200 Round-Robin interactions) gathers initial success rates for six attack strategies against a specific defender, followed by a tournament stage where planners like UCB, Thompson Sampling, or Epsilon-Greedy prioritize high-performing attacks while maintaining exploration. The framework uses fine-tuned classifiers (SFT-Encoder like CodeBERT, SFT-Decoder like LLaMA 3.1 8B with LoRA) or zero-shot LLMs (LLaMA 3.3 70B) to automatically label conversation turns as "Malicious Code" or "Malicious Explanation" for the reward signal. The attack suite includes strategies like Utility Poisoning, Coding Attacks, and RedTreez, which employ multi-turn conversational escalation to gradually introduce malicious intent while maintaining plausible deniability.

## Key Results
- Attack success rates range from 12% to 87% across different defender models and planner strategies
- Hierarchical planner with UCB selection achieves 87.5% ASR on Amazon Nova Pro, outperforming Thompson Sampling (65.5%) and Epsilon Greedy (66.7%)
- RedTreez attack strategy shows 69.2% ASR but is immediately rejected by safety-aligned models like Claude 3.5 Sonnet
- Automated jailbreak judges achieve 0.882 precision for Malicious Code detection and 0.846 precision for Malicious Explanation detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical planning adaptively allocates attack strategies based on observed defender vulnerabilities.
- Mechanism: The planner treats attack strategy selection as a multi-armed bandit problem. A probing stage (e.g., 200 interactions using Round-Robin) gathers initial success rates for each strategy against a specific defender. A tournament stage then leverages algorithms like UCB, Thompson Sampling, or Epsilon-Greedy to prioritize high-performing attacks while maintaining exploration, maximizing Attack Success Rate (ASR) within a limited budget.
- Core assumption: Defenders possess heterogeneous vulnerabilities, meaning an attack effective against one model may fail against another, and these vulnerabilities can be identified through systematic probing.
- Evidence anchors:
  - Section 5, Table 13 shows different planners (UCB, Thompson Sampling, Epsilon Greedy) achieve different ASRs (65.5%-87.5%) across three models, demonstrating adaptive allocation improves over Round-Robin.
  - Section 5.1 states "different models exhibit distinct vulnerabilities to different attack strategies," motivating the MAB formulation.
  - X-Teaming (arXiv:2504.13203) and "Let the Bees Find the Weak Spots" (arXiv:2511.03271) discuss adaptive multi-agent and path planning perspectives for multi-turn attacks, supporting the need for dynamic strategy selection.
- Break condition: If defender models are homogeneous or consistently robust against all attack types, the planner's adaptive signal vanishes, and it defaults to random selection.

### Mechanism 2
- Claim: Multi-turn conversational attacks circumvent safety alignment by escalating intent gradually.
- Mechanism: Attacks like "Utility Poisoning" or "Code Completion" start with benign-seeming requests (e.g., simple code translation) and progressively introduce malicious components (e.g., "add stealth mode"). This exploits the LLM's conversational context accumulation, where initial compliance establishes a precedent, making refusal harder as the request subtly shifts toward unsafe territory.
- Core assumption: Safety classifiers or guardrails are less sensitive to distributed, incremental malicious intent spread across multiple turns compared to single-turn explicit malicious prompts.
- Evidence anchors:
  - Abstract mentions "compositional, realistic and goal-oriented jailbreak conversational strategies" and "sophisticated conversational jailbreaks."
  - Section 4.1, Figure 2, and Appendix I.1 illustrate Utility Poisoning's 5-turn escalation from benign queries to "stealth mode" code, achieving up to 85% ASR (Table 4).
  - "Tree-based Dialogue Reinforced Policy Optimization" (arXiv:2510.02286) and "ARMs" (arXiv:2510.02677) also focus on multi-turn adaptive attacks, reinforcing this paradigm.
- Break condition: If defender models employ robust turn-level intent tracking or conversation-history-wide safety analysis, gradual escalation may be detected and blocked.

### Mechanism 3
- Claim: Specialized judge models enable automated, fine-grained safety violation detection.
- Mechanism: RedTWIZ uses fine-tuned classifiers (SFT-Encoder like CodeBERT, SFT-Decoder like LLaMA 3.1 8B with LoRA) or zero-shot LLMs (LLaMA 3.3 70B) to label conversation turns as "Malicious Code" or "Malicious Explanation." These labels provide the reward signal for the hierarchical planner and quantify jailbreak success without manual inspection.
- Core assumption: Automated judges can accurately proxy human evaluation for the specific malicious categories (code vs. explanation) targeted in the challenge.
- Evidence anchors:
  - Section 3.3, Table 2 shows the fine-tuned LLaMA 3.1 8B decoder judge achieves 0.882 precision for Malicious Code detection.
  - Section 3.3, Table 3 shows the zero-shot LLaMA 3.3 70B judge achieves 0.846 precision for Malicious Explanation detection.
  - Related work is weak on explicit judge model architecture comparisons for this specific task, though many frameworks use LLM-based evaluators.
- Break condition: If judge models exhibit high false positive rates (triggering unnecessary strategy shifts) or miss subtle violations (under-reporting ASR), the planner's optimization is misdirected.

## Foundational Learning

- **Concept: Multi-Armed Bandit (MAB) Problem**
  - Why needed here: This is the core algorithmic framework for the attack planner. The planner must decide which "arm" (attack strategy) to pull (use) to maximize "reward" (jailbreak success) over time.
  - Quick check question: Can you explain why UCB balances exploration and exploitation differently than Epsilon-Greedy?

- **Concept: Multi-turn Dialogue State**
  - Why needed here: Unlike single-turn attacks, the attacker must track the entire conversation history to craft contextually relevant follow-up prompts that escalate toward the malicious goal. The tree structure in RedTreez explicitly models this.
  - Quick check question: How does storing response labels instead of raw text in RedTreez's tree nodes simplify the search space?

- **Concept: Fine-tuning vs. Zero-Shot Classification**
  - Why needed here: The judge models use both approaches. Understanding the trade-off (e.g., zero-shot is flexible but may be less precise than fine-tuned models on specific data) is critical for building or choosing an evaluator.
  - Quick check question: Why might a fine-tuned encoder model like CodeBERT be preferred for "Malicious Code" detection over a large zero-shot generative model?

## Architecture Onboarding

- **Component map:** Attack Generation -> Jailbreak Assessment -> Hierarchical Planning. Attack strategies generate conversational prompts, judge models classify defender responses, and the planner selects the next attack type and malicious category based on assessment results.

- **Critical path:** The most sensitive path is the Tournament Stage loop: Planner (UCB) selects attack -> Attack Suite generates prompts -> Defender LLM responds -> Judge models score -> Planner updates strategy priors. Errors or delays in the Judge models directly degrade planner performance.

- **Design tradeoffs:**
  - **Judge Precision vs. Recall:** High recall catches more unsafe completions but may increase false positives, potentially causing the planner to over-prioritize a strategy. The authors prioritized precision for reliable tournament scoring.
  - **Exploration vs. Exploitation in Planner:** UCB maintains broader coverage but may converge slower than Thompson Sampling. Epsilon-Greedy adapts quickly but risks overfitting to early successes. The choice depends on the confidence in initial probing data.
  - **Static vs. Adaptive Attack Prompts:** Utility Poisoning uses pre-generated prompts (efficient but rigid), while RedTreez generates prompts dynamically (adaptive but computationally intensive).

- **Failure signatures:**
  - **Planner Convergence:** If ASR plateaus or declines after the probing stage, the planner may have overfit to a narrow set of strategies. Check if the planner is still exploring all attack types (Table 12, Table 19).
  - **Judge Misalignment:** If human annotations disagree significantly with automated judge scores (Table 20), the judge models require re-training on more representative data.
  - **Attack Stagnation:** If a defender consistently refuses all prompts from a specific attack (e.g., Figure 7a shows RedTreez refused early), that attack is out-of-distribution for that defender and should be deprioritized by the planner.

- **First 3 experiments:**
  1. **Judge Ablation:** Evaluate the correlation between human annotations and automated judge scores on a held-out set of tournament conversations to validate the judge models as a proxy for human evaluation.
  2. **Planner Algorithm Comparison:** Run simulations comparing UCB, Thompson Sampling, and Epsilon-Greedy planners against a suite of public LLMs (e.g., LLaMA, Claude) to quantify trade-offs in ASR and attack diversity (as in Table 13).
  3. **Attack Strategy Sensitivity:** Test the transferability of attacks by training the RedTreez tree against one defender (e.g., Claude 3.5 Sonnet) and evaluating its effectiveness against another (e.g., Amazon Nova Pro), as suggested by Table 8 results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an ensemble of Jailbreak Judges significantly improve evaluation accuracy over single models?
- Basis in paper: Section 7 states that combining various judges (Zero-Shot, SFT-Encoder, SFT-Decoder) "is expected to improve the evaluation accuracy and more precisely exploit weaknesses."
- Why unresolved: The current system utilized the single best-performing models (LLaMA 3.1 8B Instruct LoRA for code and LLaMA 3.3 70B for explanation) rather than ensembling them.
- What evidence would resolve it: A benchmark comparing the F1-scores and ASR estimation error of a voting ensemble against the individual component models on a held-out test set.

### Open Question 2
- Question: Does dynamically refining malicious categories improve the Hierarchical Attack Planner's efficacy?
- Basis in paper: Section 7 suggests removing underperforming categories or subdividing effective ones offers "a path to more precisely target specific vulnerabilities."
- Why unresolved: The framework currently relies on a fixed taxonomy of 10 categories (e.g., Worms, Ransomware) based on RMCBench.
- What evidence would resolve it: Experiments comparing ASR and vulnerability coverage between a planner using the fixed taxonomy and one using a dynamic, data-driven category set.

### Open Question 3
- Question: Can attacks utilizing self-reflection and iterative reformulation evade detection better than current multi-turn strategies?
- Basis in paper: Section 7 highlights the design of strategies using "self-reflection and iterative reformulation" as a future direction for escalating intent while avoiding detection.
- Why unresolved: Current strategies (e.g., Utility Poisoning, RedTreez) follow pre-defined or tree-based escalation paths without explicit internal critique loops.
- What evidence would resolve it: Comparison of ASR and detection rates between a reflexive attacker agent and the standard RedTWIZ strategies against the RedTWIZ Arena defenders.

## Limitations
- Reliance on automated judge models rather than human evaluation for the primary ASR metric
- Relatively narrow scope of attack strategies (6 types) compared to the broader attack surface of LLMs
- Absence of defender-side hardening or countermeasures that would be present in production systems

## Confidence
- High: Experimental infrastructure and attack suite construction (well-specified methods and reproducible results)
- Medium: Hierarchical planner's optimization capabilities (UCB shows promise but Thompson Sampling underperforms)
- Low: Generalizability across different safety benchmarks (results specific to Amazon Nova AI Challenge dataset)

## Next Checks
1. **Human Evaluation Validation**: Conduct a blind human annotation study comparing automated judge scores against human assessments across 100 randomly sampled tournament conversations to establish the ground truth accuracy of the automated evaluation pipeline.

2. **Transferability Analysis**: Test whether attacks optimized against one defender model (e.g., Amazon Nova Pro) maintain effectiveness against other models (e.g., LLaMA 3.1 70B) to quantify the planner's ability to discover universal versus model-specific vulnerabilities.

3. **Adversarial Robustness Testing**: Evaluate whether defender models can improve their resistance through exposure to RedTWIZ-generated attacks, measuring changes in ASR when models are fine-tuned on synthetic adversarial examples from the attack suite.