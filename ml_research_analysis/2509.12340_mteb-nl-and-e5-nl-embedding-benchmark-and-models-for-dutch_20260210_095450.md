---
ver: rpa2
title: 'MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch'
arxiv_id: '2509.12340'
source_url: https://arxiv.org/abs/2509.12340
tags:
- dutch
- dataset
- retrieval
- datasets
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MTEB-NL, a comprehensive benchmark for Dutch
  text embeddings, and E5-NL, a suite of compact Dutch embedding models. MTEB-NL combines
  existing Dutch datasets with newly created ones across classification, retrieval,
  clustering, and semantic similarity tasks.
---

# MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch

## Quick Facts
- **arXiv ID:** 2509.12340
- **Source URL:** https://arxiv.org/abs/2509.12340
- **Reference count:** 0
- **Key outcome:** This work introduces MTEB-NL, a comprehensive benchmark for Dutch text embeddings, and E5-NL, a suite of compact Dutch embedding models. MTEB-NL combines existing Dutch datasets with newly created ones across classification, retrieval, clustering, and semantic similarity tasks. To train E5-NL models, the authors compile human-annotated Dutch retrieval data and augment it with synthetic data generated by LLMs, using topic sampling and filtering to improve diversity and quality. Experiments show that E5-NL models outperform non-instruct baselines and match or exceed some larger instruct models, while being significantly more parameter-efficient. The benchmark and models are publicly available to support future Dutch NLP research.

## Executive Summary
This paper addresses the lack of dedicated Dutch text embedding benchmarks and models by introducing MTEB-NL and E5-NL. MTEB-NL is a comprehensive benchmark combining existing Dutch datasets with newly created ones across classification, retrieval, clustering, and semantic similarity tasks. E5-NL is a suite of compact Dutch embedding models trained via contrastive learning on a mixture of human-annotated and synthetic retrieval data. The models demonstrate strong performance while being significantly more parameter-efficient than larger multilingual instruct models, providing valuable resources for Dutch NLP research.

## Method Summary
The authors create MTEB-NL by combining existing Dutch datasets with newly created ones across four task categories. For E5-NL model training, they compile 620K human-annotated Dutch retrieval samples and generate 350K synthetic samples using LLMs with topic sampling and filtering. The models are initialized from multilingual E5 and fine-tuned using contrastive learning with InfoNCE loss. Key training innovations include vocabulary trimming (reducing to 50k tokens) or transtokeniser, source-homogeneous batching to prevent false negatives, and hard negative mining using a TopK-STDMarginPos strategy. Models are trained for 1 epoch with AdamW optimization and evaluated on the MTEB-NL benchmark.

## Key Results
- E5-NL models outperform non-instruct baselines and match or exceed some larger instruct models
- Vocabulary trimming and source-homogeneous batching significantly improve performance
- E5-NL models achieve strong results while being significantly more parameter-efficient than larger instruct models
- The synthetic data augmentation improves performance across task types

## Why This Works (Mechanism)
The success of E5-NL models stems from their focused training on Dutch-specific data combined with effective contrastive learning techniques. The vocabulary trimming reduces model complexity while maintaining Dutch language representation. Source-homogeneous batching prevents false negatives that would otherwise degrade the contrastive loss. The combination of human-annotated and high-quality synthetic data provides diverse training signals across task types. The hard negative mining strategy ensures the model learns to distinguish between semantically similar but distinct text pairs.

## Foundational Learning
- **Contrastive Learning**: Learning representations by pulling similar items together and pushing dissimilar items apart in embedding space
  - *Why needed*: Enables effective training of text embeddings without explicit labels
  - *Quick check*: Verify that InfoNCE loss implementation correctly computes similarity scores between embeddings

- **Hard Negative Mining**: Selecting negative samples that are semantically similar to positive samples to make the learning task more challenging
  - *Why needed*: Prevents model from learning trivial decision boundaries and improves discriminative power
  - *Quick check*: Confirm that TopK-STDMarginPos strategy selects negatives with high semantic similarity scores

- **Vocabulary Trimming**: Reducing model vocabulary size to focus on language-specific tokens
  - *Why needed*: Decreases model size and computational requirements while maintaining language-specific performance
  - *Quick check*: Verify that trimmed vocabulary retains high-frequency Dutch tokens and that embeddings are properly resized

## Architecture Onboarding

**Component Map**: Multilingual E5 -> Vocabulary Trimming/Transtokeniser -> Source-Homogeneous Batching -> Hard Negative Mining -> InfoNCE Loss -> MTEB-NL Evaluation

**Critical Path**: Data Curation (Human + Synthetic) -> Model Initialization -> Fine-tuning with Source-Homogeneous Batching -> Evaluation on MTEB-NL

**Design Tradeoffs**: Parameter efficiency vs. performance (compact models vs. larger instruct models), training data quality vs. quantity (human vs. synthetic data), model initialization (multilingual vs. Dutch-specific)

**Failure Signatures**: Performance degradation on non-retrieval tasks indicates insufficient synthetic data diversity; training instability suggests improper implementation of source-homogeneous batching or hard negative mining

**First Experiments**:
1. Verify vocabulary trimming implementation by checking Dutch token retention and embedding matrix dimensions
2. Test source-homogeneous batching by confirming batch composition and absence of false negatives
3. Evaluate hard negative mining by examining similarity scores of selected negative samples

## Open Questions the Paper Calls Out
- Can a compact, Dutch-specific instruct embedding model outperform current multilingual instruct models on the MTEB-NL benchmark?
- Does applying weak supervision to Dutch self-supervised encoders (e.g., RobBERT-2023) significantly narrow the performance gap with supervised multilingual models?
- How does the performance of embedding models diverge when evaluated on native Dutch datasets versus machine-translated datasets?

## Limitations
- The reliance on translated data limits the ability to fully capture the linguistic nuances and cultural context of Dutch
- The specific implementation details of the TopK-STDMarginPos negative mining strategy are not fully specified
- The exact topic distribution parameters for synthetic data generation are not provided

## Confidence

**High Confidence**: The benchmark creation methodology (MTEB-NL combining existing and newly created datasets), the general training framework (contrastive learning with InfoNCE loss), and the comparative performance claims against non-instruct baselines are well-supported by the methodology described.

**Medium Confidence**: The exact implementation details of vocabulary trimming, the source-homogeneous batching strategy, and the specific performance gains over larger instruct models have some ambiguity in the described methodology.

**Low Confidence**: The precise impact of the synthetic data generation process and the specific hard negative mining implementation on final model performance cannot be fully verified from the description alone.

## Next Checks
1. Implement and test the TopK-STDMarginPos negative mining strategy with varying margin thresholds to empirically determine its impact on retrieval performance compared to standard hard negative mining
2. Systematically vary the synthetic data composition ratios and topic distributions to identify which components most strongly influence performance across different task types in MTEB-NL
3. Conduct ablation studies comparing models trained with and without source-homogeneous batching to quantify the negative impact of false negatives on contrastive learning effectiveness