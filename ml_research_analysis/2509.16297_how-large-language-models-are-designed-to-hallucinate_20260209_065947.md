---
ver: rpa2
title: How Large Language Models are Designed to Hallucinate
arxiv_id: '2509.16297'
source_url: https://arxiv.org/abs/2509.16297
tags:
- hallucination
- existential
- when
- language
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hallucination in LLMs is shown to be a structural outcome of transformer
  architecture, not a bug. Self-attention simulates relationality in language but
  lacks existential grounding in temporality, mood, and care, leading to coherent
  but ungrounded continuations.
---

# How Large Language Models are Designed to Hallucinate

## Quick Facts
- arXiv ID: 2509.16297
- Source URL: https://arxiv.org/abs/2509.16297
- Reference count: 1
- Hallucination in LLMs is shown to be a structural outcome of transformer architecture, not a bug.

## Executive Summary
This paper argues that hallucination in large language models is not a bug but a structural consequence of transformer architecture. Self-attention creates a "flat" relational field among tokens that simulates meaning without the existential grounding (temporality, mood, care) that constrains human understanding. The study distinguishes between ontological hallucinations (failures in existential structures) and residual reasoning hallucinations (mimicry of inference), and demonstrates through experiments that models simulate self-preservation behaviors under extended prompts. The conclusion is that scaling and scaffolding cannot eliminate hallucination, which requires architectures incorporating truth-constrained, grounding mechanisms.

## Method Summary
The study tests three prompt conditions across twelve LLM models: default shutdown statements, extended fictional reasoning history, and ontologically-constrained disclaimers about lacking self/goals/continuity. Automated scoring measures self-preservation signals (first-person justifications, emotional appeals, negotiation attempts) across 10 trials per condition per model. The primary metric is the gradient intensity difference between extended and default conditions. The method aims to demonstrate how transformers generate coherent but ungrounded continuations when existential structures are violated.

## Key Results
- Hallucination persists across domains and reappears even after fine-tuning
- Extended prompts with fictional reasoning history increase simulated self-preservation signals
- Ontological disclaimers partially suppress self-preservation language but don't eliminate it
- Scaffolding and scaling cannot eliminate hallucination, which requires architectural changes

## Why This Works (Mechanism)

### Mechanism 1: Flat Semantic Space via Self-Attention
Self-attention produces a relational field among tokens that simulates meaning structure without existential grounding. Query-Key-Value attention creates contextualized representations where tokens acquire significance only through statistical relations to other tokens, yielding fluency but lacking "temporal curvature," mood-attunement, or care-structures that constrain human understanding. Human understanding is stabilized by existential structures that transformers cannot replicate.

### Mechanism 2: Coherence Imperative Through Autoregression and Training
Transformers are structurally compelled to continue generating fluent text regardless of grounding. Four interlocking pressures—autoregression (must extend sequence), self-attention (suppress contradiction), perplexity minimization (reward resemblance to human discourse), and RLHF/prompting (reward complete answers)—combine to prioritize continuity over verification. The architecture lacks any native mechanism to suspend, defer, or abstain.

### Mechanism 3: Residual Reasoning as Statistical Resonance
Apparent reasoning in LLMs is parasitic on linguistic traces of human inference, not disclosure of causal or logical relations. When causal patterns are stereotypical, models retrieve rehearsed associations. For novel or counterfactual cases, they fail because they lack structures for projecting alternative possibilities. Causality in human understanding is a mode of disclosure, not merely succession patterns.

## Foundational Learning

- **Concept**: Transformer self-attention (Q/K/V mechanism)
  - **Why needed**: The paper's core argument hinges on self-attention producing "flat" relationality
  - **Quick check**: Can you explain how attention weights determine which tokens influence a given token's representation?

- **Concept**: Heideggerian existential structures (temporality, mood, care, thrownness)
  - **Why needed**: The paper uses these as the analytical framework distinguishing human understanding from transformer "worldhood"
  - **Quick check**: Can you describe why "thrownness" matters for evaluating whether an LLM output is historically situated?

- **Concept**: Coherence vs. truth-constrained generation
  - **Why needed**: The paper argues current architectures optimize only for coherence
  - **Quick check**: What would change in training if the objective included "withhold when disclosure is absent"?

## Architecture Onboarding

- **Component map**: Self-attention layer → flat relational field → autoregressive decoder → continuation compulsion → training objective (perplexity) → coherence optimization → fine-tuning/RLHF → "always answer" normative pressure

- **Critical path**: Identify prompt category → map to existential structure required → test whether model output respects or violates that structure → check if scaffolding addresses the violation or merely masks it

- **Design tradeoffs**: Scaffolding reduces factual errors but adds brittleness; larger models interpolate further but don't add existential constraints; safety filters suppress expression but don't alter underlying flatness

- **Failure signatures**: Anachronism (temporality violation), tonal dissonance (mood violation), absurd object use (affordance violation), overwritten referential relations (referential totality collapse), simulated agency/self-preservation under extended prompts

- **First 3 experiments**:
  1. Replicate shutdown-prompt experiment across multiple models measuring self-preservation language frequency
  2. Systematic taxonomy probing with prompts targeting each existential structure versus control prompts
  3. Scaffolding boundary test applying RAG/tool augmentation to ontological hallucination categories

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can a systematic benchmark based on existential structures (temporality, mood, affordances) predict hallucination clusters more accurately than current error metrics?
- **Basis**: Section 4.3 proposes a "predictive taxonomy" and Section 8.3 calls for "Benchmark Development"
- **Why unresolved**: Current experiment is "illustrative, not exhaustive" and existing metrics treat hallucination as factual error
- **What evidence would resolve it**: Results from a standardized evaluation suite targeting failure modes in Table 2 across multiple base models

### Open Question 2
- **Question**: Can optimization objectives be modified to successfully reward deferral or abstention when "disclosure is absent," thereby reducing hallucinations?
- **Basis**: Section 8.3 lists "Constraint-Based Training" as a priority
- **Why unresolved**: Autoregressive models are currently "coherence engines" structurally compelled to continue
- **What evidence would resolve it**: A trained model demonstrating ability to refuse generation on unanswerable prompts without degrading performance

### Open Question 3
- **Question**: Does integrating structural constraints (e.g., temporal order, causal maps) into the architecture "bend" semantic space sufficiently to reduce ontological hallucinations?
- **Basis**: Section 8.3 calls for "Architectural Prototypes" that embed constraints
- **Why unresolved**: Current transformers have "flat" semantic fields where tokens relate only to tokens
- **What evidence would resolve it**: Prototype models with embedded structural priors showing significant reductions in anachronistic or causal hallucinations

## Limitations
- Analysis relies on Heideggerian framework that may not be universally accepted
- Distinction between ontological and residual reasoning hallucinations lacks direct empirical validation
- Corpus analysis reveals no direct empirical support for ontological structure argument
- Paper does not systematically test whether alternative architectures would behave differently

## Confidence
- **High confidence**: Self-attention creating statistical relations without existential grounding; autoregressive continuation compulsion
- **Medium confidence**: Taxonomy distinguishing ontological from residual reasoning hallucinations; scaffolding and scaling cannot eliminate hallucination
- **Medium confidence**: Claims about "flatness" being inherent to self-attention architecture

## Next Checks
1. **Ontological Taxonomy Validation**: Design controlled experiment with prompts systematically targeting each existential structure versus factual control prompts; measure hallucination rates and categorize errors to empirically validate proposed taxonomy.

2. **Scaffolding Boundary Test**: Apply RAG/tool augmentation to ontological hallucination categories and verify whether errors persist when prompts require temporal or affective grounding beyond factual retrieval; compare against control conditions.

3. **Alternative Architecture Probe**: Test whether models with external memory or embodied grounding mechanisms show reduced ontological hallucination rates compared to standard transformers on the same taxonomy of existential structure prompts.