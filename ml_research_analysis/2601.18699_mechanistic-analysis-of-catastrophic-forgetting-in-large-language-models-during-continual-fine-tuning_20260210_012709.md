---
ver: rpa2
title: Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During
  Continual Fine-tuning
arxiv_id: '2601.18699'
source_url: https://arxiv.org/abs/2601.18699
tags:
- forgetting
- gradient
- attention
- task
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study provides the first comprehensive mechanistic analysis
  of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning.
  The authors identify three primary mechanisms: gradient interference in attention
  weights (disrupting 15-23% of heads in lower layers), representational drift in
  intermediate layers (CKA similarity decreases by 0.32-0.47), and loss landscape
  flattening around prior task minima.'
---

# Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning

## Quick Facts
- arXiv ID: 2601.18699
- Source URL: https://arxiv.org/abs/2601.18699
- Reference count: 40
- Primary result: First comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs, identifying gradient interference, representational drift, and loss landscape flattening as primary mechanisms

## Executive Summary
This study presents the first comprehensive mechanistic analysis of catastrophic forgetting in transformer-based large language models during sequential fine-tuning. The authors identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening around prior task minima. Through extensive empirical analysis across multiple benchmark tasks, the research establishes strong correlations between task similarity and forgetting severity, while developing predictive metrics for forgetting based on early gradient alignment. The integrated mechanistic model explains forgetting as coupled processes operating at different timescales, providing foundational insights for developing targeted mitigation strategies in continual learning systems.

## Method Summary
The authors conducted extensive empirical analysis of catastrophic forgetting during sequential fine-tuning of transformer-based LLMs using multiple benchmark tasks. They employed CKA similarity measurements to quantify representational drift, gradient alignment analysis to identify interference patterns, and loss landscape curvature assessment to characterize forgetting dynamics. The study systematically varied task similarity, training schedules, and model architectures to isolate the three identified mechanisms. Statistical analysis established correlations between early training metrics and long-term forgetting outcomes, while ablation studies helped validate the relative contributions of different mechanisms across training epochs.

## Key Results
- Three primary forgetting mechanisms identified: gradient interference in attention weights (disrupting 15-23% of heads in lower layers), representational drift (CKA similarity decreases by 0.32-0.47), and loss landscape flattening around prior task minima
- Strong correlation between task similarity and forgetting severity (Pearson r=0.87)
- Early gradient alignment metrics show predictive power for long-term forgetting (r=-0.79)
- Attention mechanisms drive early forgetting, with lower layers showing greater susceptibility

## Why This Works (Mechanism)
The mechanistic analysis reveals that catastrophic forgetting in LLMs results from three coupled processes operating at different timescales. Gradient interference occurs immediately during sequential fine-tuning when updates for new tasks disrupt attention weight configurations learned for previous tasks, particularly affecting lower layers where 15-23% of heads show disruption. Representational drift accumulates gradually as intermediate layer representations diverge from their original configurations, measurable through CKA similarity decreases of 0.32-0.47. Loss landscape flattening emerges later as the optimization process creates broader, flatter minima for new tasks that fail to preserve the sharp minima of previous tasks. The coupling between these mechanisms creates a cascade effect where early interference amplifies drift, which in turn accelerates landscape changes.

## Foundational Learning

**CKA Similarity Analysis**
- Why needed: Quantifies representational similarity between different neural network layers across training stages
- Quick check: Verify CKA computation using reference implementations and validate with known similarity cases

**Gradient Interference Detection**
- Why needed: Identifies when parameter updates for new tasks disrupt representations important for previous tasks
- Quick check: Compute cosine similarity between gradients for different tasks across training iterations

**Loss Landscape Analysis**
- Why needed: Characterizes the optimization geometry and stability of learned representations
- Quick check: Measure curvature and sharpness of minima using second-order derivatives or random perturbation analysis

## Architecture Onboarding

**Component Map**
Input Data -> Transformer Layers -> Attention Heads -> Feed-Forward Networks -> Output Layer

**Critical Path**
Training Data → Sequential Fine-tuning → Gradient Updates → Parameter Changes → Forgetting Manifestation

**Design Tradeoffs**
Memory Efficiency (single model for multiple tasks) vs. Performance Preservation (maintaining task-specific capabilities); Computational Cost (fine-tuning entire model) vs. Task Interference (forgetting from sequential updates)

**Failure Signatures**
- Early Epochs: Sharp drops in attention head effectiveness, particularly in lower layers (15-23% disruption)
- Middle Epochs: Increasing representational drift as measured by decreasing CKA similarity (0.32-0.47 range)
- Late Epochs: Loss landscape flattening around prior task minima, reducing task-specific precision

**First Experiments**
1. Measure CKA similarity changes across layers during sequential fine-tuning to quantify representational drift
2. Analyze gradient alignment between successive tasks to identify interference patterns
3. Test attention head ablation to determine causal role in early forgetting mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on transformer-based LLMs, limiting generalizability to other architectures
- Correlation coefficients (r=0.87 for task similarity, r=-0.79 for gradient alignment) indicate strong relationships but don't establish definitive causation
- Temporal mechanism analysis remains preliminary, with proposed progression from gradient interference to landscape flattening needing longer-term validation

## Confidence

**High confidence**: Identification of gradient interference in attention weights and representational drift as core mechanisms; quantitative CKA similarity measurements showing 0.32-0.47 decreases

**Medium confidence**: Temporal separation of forgetting mechanisms across training epochs; predictive power of early gradient alignment metrics

**Medium confidence**: Attention mechanisms driving early forgetting with lower layers showing greater susceptibility based on 15-23% head disruption observation

## Next Checks

1. Conduct ablation studies systematically disabling specific attention heads across different layers to directly test the causal role of attention mechanisms in early forgetting

2. Replicate the temporal mechanism analysis using longer training schedules (beyond 20 epochs) to validate the proposed progression from gradient interference to landscape flattening

3. Test the generalizability of findings across diverse model families (RNNs, CNNs, different transformer variants) and non-synthetic task distributions to assess architecture-specific versus universal forgetting mechanisms