---
ver: rpa2
title: How Does Preconditioning Guide Feature Learning in Deep Neural Networks?
arxiv_id: '2509.25637'
source_url: https://arxiv.org/abs/2509.25637
tags:
- train
- test
- learning
- generalization
- preconditioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how preconditioning influences feature
  learning and generalization in deep neural networks. The authors establish that
  preconditioning determines the similarity metric via the Gram matrix, thereby controlling
  the spectral bias in feature learning.
---

# How Does Preconditioning Guide Feature Learning in Deep Neural Networks?

## Quick Facts
- **arXiv ID:** 2509.25637
- **Source URL:** https://arxiv.org/abs/2509.25637
- **Reference count:** 40
- **Primary result:** Preconditioning controls spectral bias in feature learning, with generalization depending on alignment between the preconditioner's emphasis and the teacher's spectral structure.

## Executive Summary
This paper investigates how preconditioning influences feature learning and generalization in deep neural networks. The authors establish that preconditioning determines the similarity metric via the Gram matrix, thereby controlling the spectral bias in feature learning. They show that the exponent $p$ in the preconditioner $\Sigma_X^p$ governs the emphasis on high- or low-variance components, and that generalization depends critically on the alignment between this spectral bias and the teacher's spectral structure. Through synthetic experiments with varying noise levels and alignment scenarios, they demonstrate that models become sensitive to the variance components emphasized by the preconditioner, with optimal performance occurring when $p$ matches the teacher's spectral emphasis.

## Method Summary
The method involves training two-layer ReLU MLPs on synthetic data generated from single-index teacher models with controlled spectral alignment. The key innovation is applying preconditioners of the form $\Sigma_X^p$ to the first-layer gradients, where $\Sigma_X$ is the input covariance matrix. The exponent $p$ controls spectral emphasis, with positive values amplifying high-variance components and negative values emphasizing low-variance components. Experiments sweep across different $p$ values, noise levels, and alignment scenarios to test the teacher-alignment hypothesis. For OOD generalization, class-specific Gaussian noise is added to MNIST, and for transfer learning, the first layer is frozen after pretraining with different $p$ values.

## Key Results
- The exponent $p$ in preconditioner $\Sigma_X^p$ controls spectral bias, determining whether high-variance or low-variance components dominate learning
- Generalization is maximized when the spectral bias induced by $p$ aligns with the spectral location of the teacher's signal
- For forward knowledge transfer, $p=-1$ (uniform treatment across all spectral components) yields the best performance, while specialized $p$ values excel in noise-robust and OOD settings when alignment is known

## Why This Works (Mechanism)

### Mechanism 1: The Preconditioned Gram Matrix as an Information Bottleneck
Preconditioning fundamentally alters the "similarity geometry" of the input space, restricting the model to access input information solely through the Gram matrix defined by the preconditioner's metric. The neuron-wise preconditioned update forces hidden states to evolve based on $G_P = X^\top P X$. Theorems establish that training state and test-time predictions hold zero mutual information with raw input beyond what's contained in this Gram matrix history and labels.

### Mechanism 2: Spectral Emphasis via the Power $p$
The exponent $p$ in preconditioner $\Sigma_X^p$ acts as continuous control for spectral bias, determining whether high-variance or low-variance components dominate. By factorizing preconditioner as $P = \Sigma_X^p$, the induced Gram matrix scales spectral components by $s^{2(p+1)}$. Increasing $p$ amplifies high-variance directions while decreasing $p$ flattens spectrum, emphasizing low-variance directions.

### Mechanism 3: Teacher-Alignment Hypothesis for Generalization
Generalization performance is maximized when spectral bias induced by $p$ aligns with spectral location of teacher's signal. If teacher signal resides in high-variance directions, large $p$ improves generalization by suppressing noisy low-variance directions. Conversely, if signal is in low-variance directions, negative $p$ is required to up-weight these signals and suppress high-variance noise.

## Foundational Learning

- **Concept:** Mahalanobis Distance and Similarity Geometry
  - **Why needed here:** Preconditioning changes the "inner product" used by network; understanding $x^\top P x$ defines distance metric is crucial
  - **Quick check question:** If $P$ is identity matrix, what is Mahalanobis distance equivalent to?

- **Concept:** Spectral Bias (Frequency/High vs. Low Variance)
  - **Why needed here:** Core mechanism involves trading off emphasis between high-variance (often "smooth") and low-variance (often "fine-grained") features
  - **Quick check question:** In context of this paper, does positive power $p$ emphasize high-variance or low-variance input components?

- **Concept:** Information Bottleneck & Mutual Information
  - **Why needed here:** Theoretical proof relies on Mutual Information to prove raw input is "lost" and only Gram matrix history remains
  - **Quick check question:** According to Theorem 1, what two quantities must you condition on for hidden states to have zero mutual information with raw input?

## Architecture Onboarding

- **Component map:** Input Layer $Z = W_1^\top X$ -> Preconditioner $P = \Sigma_X^p$ -> Gram Matrix $G_P = X^\top P X$ -> Teacher generates labels $Y$ with spectral alignment $\alpha$

- **Critical path:**
  1. Estimate $\Sigma_X$ (covariance) or approximate Hessian
  2. Apply power $p$ to eigenvalues
  3. Use modified matrix to precondition first-layer gradients
  4. Verify alignment: Check if target signal correlates with high or low variance directions

- **Design tradeoffs:**
  - **Specialization ($p \neq -1$):** Optimizes for current task's alignment; best for noise robustness and OOD if alignment known; risk: discards information useful for future tasks
  - **Uniformity ($p = -1$):** Treats all spectral components equally (similar to whitening); best for Forward Knowledge Transfer, preserving information for unknown future tasks
  - **Approximation:** Exact covariance ideal for theory; practical systems use AdaHessian (diagonal) or K-FAC (block-diagonal)

- **Failure signatures:**
  - **Robustness Failure:** Model overfits to label noise if $p$ emphasizes variance band where noise resides
  - **OOD Failure:** If invariant features are low-variance and spurious features are high-variance, standard optimizers will fail; requires tuning $p$ negative
  - **Transfer Failure:** Aggressive spectral specialization in pretraining degrades performance on downstream tasks with different spectral structures

- **First 3 experiments:**
  1. **Synthetic Alignment Check:** Replicate "Case High" vs "Case Low" experiment using 2-layer MLP; vary $p \in \{0, -0.5, -1\}$ and observe if optimal $p$ shifts when teacher signal moves from largest to smallest eigenvalue
  2. **Transfer Probe:** Train on source task with $p=0$ and $p=-1$; freeze first layer and train linear probe on target task with inverted spectral alignment; verify if $p=-1$ yields lower loss
  3. **Hessian Power Sweep:** Replace exact covariance with diagonal Hessian approximation; sweep power $p$ applied to diagonal elements to verify theoretical mechanism holds for approximate second-order methods

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework assumes preconditioners depend only on training data and initialization is P-isotropic, but these assumptions aren't empirically validated across all experiments
- Single-index teacher model used for synthetic experiments may not capture complexity of real-world label structures, particularly for multi-modal or hierarchical tasks
- Claim that Gram matrix serves as complete information bottleneck requires preconditioner to be fixed during training, which may not hold for adaptive methods like AdaHessian

## Confidence
- **High Confidence:** Core mechanism linking preconditioner power $p$ to spectral emphasis is well-supported by mathematical derivation and synthetic experiments
- **Medium Confidence:** Teacher-alignment hypothesis demonstrated in controlled synthetic settings but requires additional validation in more complex, real-world scenarios
- **Medium Confidence:** OOD generalization results show promising patterns but specific implementation details for "flipping noise/digits" are unclear

## Next Checks
1. **Multi-index Teacher Validation:** Extend synthetic experiments to two-index teachers where signals exist in both high and low variance components; test whether optimal $p$ depends on relative strength of each signal component

2. **Adaptive Preconditioner Analysis:** Implement adaptive version of preconditioner that updates during training and compare against fixed preconditioner case; measure deviation from theoretical information bottleneck claim

3. **Real-world Task Transfer:** Apply transfer experiment framework to real-world vision task like CIFAR-10 to SVHN transfer; verify whether $p=-1$ consistently outperforms specialized values across different spectral alignments