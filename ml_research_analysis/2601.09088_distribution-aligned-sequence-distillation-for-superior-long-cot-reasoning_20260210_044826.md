---
ver: rpa2
title: Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning
arxiv_id: '2601.09088'
source_url: https://arxiv.org/abs/2601.09088
tags:
- teacher
- student
- training
- distillation
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of improving long chain-of-thought
  (CoT) reasoning in smaller language models through sequence-level distillation from
  larger teacher models. The authors identify three key limitations in current SFT-based
  distillation: inadequate coverage of the teacher''s sequence-level distribution,
  misalignment between teacher and student output distributions, and exposure bias
  from teacher-forced training.'
---

# Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning

## Quick Facts
- arXiv ID: 2601.09088
- Source URL: https://arxiv.org/abs/2601.09088
- Authors: Shaotian Yan; Kaiyuan Liu; Chen Shen; Bing Wang; Sinan Fan; Jun Zhang; Yue Wu; Zheng Wang; Jieping Ye
- Reference count: 5
- Primary result: Achieves 88.5 on AIME24 and 83.3 on AIME25 with only 448K samples, setting state-of-the-art for 4B-scale open-source reasoning models

## Executive Summary
This paper addresses the problem of improving long chain-of-thought (CoT) reasoning in smaller language models through sequence-level distillation from larger teacher models. The authors identify three key limitations in current SFT-based distillation: inadequate coverage of the teacher's sequence-level distribution, misalignment between teacher and student output distributions, and exposure bias from teacher-forced training. To address these issues, they propose three methodological innovations: temperature-scheduled learning (gradually shifting from low to high sampling temperatures to better capture the teacher's full output distribution), divergence-aware sampling (prioritizing training examples where the teacher assigns high confidence but the student assigns low probability), and mixed-policy distillation (combining teacher-forced and student-generated trajectories to mitigate exposure bias). Using these techniques, they develop DASD-4B-Thinking, a 4B parameter model that achieves state-of-the-art performance among open-source models of comparable scale, scoring 88.5 on AIME24, 83.3 on AIME25, 69.3 on LiveCodeBench v5, and 68.4 on GPQA-Diamond. Notably, these results are achieved using only 448K training samples - an order of magnitude fewer than most existing open-source efforts.

## Method Summary
The paper proposes a three-stage sequence-level distillation pipeline to transfer long chain-of-thought reasoning from large teacher models to compact students. Stage 1 uses low-temperature sampling (T=0.6) with divergence-aware sampling to establish stable reasoning patterns over 105K samples. Stage 2 continues with high-temperature sampling (T=1.0) to capture broader distributional coverage across 330K samples. Stage 3 applies mixed-policy distillation, combining student-generated prefixes with teacher completions to mitigate exposure bias from 12.7K samples. The approach addresses three limitations: inadequate teacher distribution coverage (solved by temperature scheduling), teacher-student misalignment (solved by divergence-aware sampling), and exposure bias (solved by mixed-policy training). The final model, DASD-4B-Thinking, achieves state-of-the-art performance on reasoning benchmarks while using only 448K samples total.

## Key Results
- Achieves 88.5 on AIME24 and 83.3 on AIME25, setting new state-of-the-art for 4B-scale open-source reasoning models
- Demonstrates 69.3 on LiveCodeBench v5 and 68.4 on GPQA-Diamond across math, code, and science reasoning tasks
- Uses only 448K training samples - an order of magnitude fewer than most existing open-source efforts
- Ablation studies show each component contributes measurably: temperature scheduling (+8.7 AIME25), DAS (+2.1 AIME24), and mixed-policy (+0.9 LCB v5)

## Why This Works (Mechanism)

### Mechanism 1: Temperature-scheduled Learning
A curriculum that begins with low-temperature teacher samples and progressively incorporates higher-temperature samples improves student learning compared to single-temperature training. Low-temperature samples concentrate probability mass on high-confidence responses with consistent patterns, enabling rapid early-stage convergence. High-temperature samples flatten the distribution, capturing rarer modes and richer latent information that transfer generalization capability. The staged approach reconciles learning stability with distributional coverage. The core assumption is that the student model has limited capacity and cannot efficiently absorb heterogeneous high-temperature samples without first establishing a stable reasoning foundation. Evidence shows cold-starting with 50K samples at T=0.6 followed by T=1.0 training yields +3.5 AIME24 and +5.2 AIME25 gains over T=1.0-only training.

### Mechanism 2: Divergence-aware Sampling (DAS)
Prioritizing training examples where the teacher assigns high probability but the student assigns low probability yields more effective learning than random sampling. These "Teacher Sentences" avoid misleading gradients - SFT on teacher data amplifies probabilities for all target tokens, but when the student already assigns high probability to tokens the teacher considers unlikely, this drives the student away from the teacher's distribution. DAS selects examples where the student can increase probability toward the teacher without gradient conflict. The core assumption is that the probability discrepancy between teacher and student on shared sentences is diagnostic of learnable content. Evidence shows DAS outperforms random sampling across temperatures (e.g., 50K Math + DAS at T=1.0 achieves 85.0 AIME24 vs. 83.1 for RS).

### Mechanism 3: Mixed-policy Distillation
A lightweight fine-tuning stage that combines student-generated prefixes with teacher completions mitigates exposure bias introduced by pure teacher-forced training. During standard SFT, the student conditions on teacher prefixes but must generate autoregressively at inference - this train-test mismatch causes error accumulation. Mixed-policy data exposes the student to its own generation distribution while receiving corrective supervision from the teacher on error-prone continuations. The core assumption is that a small subset of on-policy trajectories (7-15K samples) is sufficient to correct the distributional shift without destabilizing the already-trained student. Evidence shows cut-off rate analysis confirms student outputs diverge increasingly from teacher responses as sequence length grows, validating the exposure bias problem.

## Foundational Learning

- **Concept: Sequence-level Knowledge Distillation**
  - Why needed here: The paper's entire framework treats SFT on teacher-generated data as an approximation to KL-divergence minimization over response distributions. Understanding this formulation is prerequisite to grasping why distribution coverage, alignment, and exposure bias matter.
  - Quick check question: Can you explain why standard SFT on teacher outputs is equivalent to minimizing a point-mass approximation of the sequence-level KL divergence?

- **Concept: Exposure Bias in Autoregressive Models**
  - Why needed here: Mixed-policy distillation directly addresses the train-inference distribution mismatch. Without understanding why teacher-forced training creates compounding errors at test time, the mechanism appears to be mere data augmentation.
  - Quick check question: During inference, the model conditions on its own previous predictions rather than ground-truth prefixes. How does this cause error propagation, and why doesn't this occur during training?

- **Concept: Temperature Sampling and Distribution Coverage**
  - Why needed here: Temperature-scheduled learning relies on the relationship between sampling temperature, probability distribution shape, and mode coverage. The trade-off between learning efficiency and distributional fidelity is central to the design.
  - Quick check question: If you sample from a language model at T=0.6 vs. T=1.2, how does the entropy of the output distribution change, and what does this imply for mode coverage?

## Architecture Onboarding

- **Component map**:
```
[Question Collection]
        ↓
[Teacher Response Sampling] ←── Temperature scheduling (T=0.6 → T=1.0)
        ↓                         Divergence-aware filtering
[Response Quality Filtering] ←── Length/structure/repetition checks
        ↓
[Stage 1: Low-Temp SFT] ←── Qwen3-4B-Instruct-2507 student
        ↓                     6 epochs, LR 5e-5 → 1e-5, 64K context
[Stage 2: High-Temp SFT] ←── Resume from Stage 1 checkpoint
        ↓
[Student Re-generation] ←── Identify truncated/divergent responses
        ↓
[Teacher Revision] ←── Random prefix truncation + teacher completion
        ↓
[Stage 3: Mixed-Policy SFT] ←── 12.7K samples, 1 epoch
        ↓
[DASD-4B-Thinking]
```

- **Critical path**:
  1. Divergence-aware sampling implementation: Requires computing teacher-side probabilities during sampling (often available via API logprobs) and student-side probabilities via forward pass. The ratio filtering logic is the key data curation step.
  2. Two-stage temperature scheduling: Stage 1 establishes stable reasoning patterns; Stage 2 expands distributional coverage. Skipping Stage 1 degrades convergence.
  3. Mixed-policy data generation: Must detect student divergence (truncation), randomly cut prefixes, and obtain teacher completions with quality filtering. This stage is lightweight but sensitive to prefix cut-point selection.

- **Design tradeoffs**:
  - **Data efficiency vs. distributional fidelity**: 448K samples is 6x smaller than competing efforts, but achieving this requires precise curation (DAS). Random sampling at scale may be simpler but less efficient.
  - **Architectural flexibility vs. logit-based methods**: Sequence-level distillation works across different tokenizers and vocabularies (e.g., gpt-oss-120b teacher → Qwen3-4B student), unlike logit distillation which requires aligned output spaces.
  - **Training simplicity vs. exposure bias mitigation**: Pure SFT is simpler but leaves performance on the table; mixed-policy adds complexity for marginal (but consistent) gains.

- **Failure signatures**:
  - **Student generates repetitive/verbose outputs**: Indicates insufficient repetition filtering during data curation.
  - **High training loss that plateaus early**: Likely caused by premature introduction of high-temperature samples without low-temperature cold start.
  - **Performance regression after mixed-policy stage**: May indicate masking student-generated portions or using low-quality teacher completions.
  - **DAS provides no improvement over random sampling**: Check if teacher/student probabilities are computed correctly on identical tokenizations.

- **First 3 experiments**:
  1. **Temperature ablation on small subset**: Sample 10K math questions at T=0.6 and T=1.0. Train student from scratch on each, then train with cold-start (T=0.6 → T=1.0). Compare AIME25 performance to validate curriculum effect locally before full-scale training.
  2. **DAS vs. random sampling diagnostic**: For 1K held-out questions, compute teacher/student probability ratios. Visualize the distribution and verify that high-divergence examples are not simply low-quality responses. Then train two models on 25K examples each (DAS vs. RS) and compare.
  3. **Exposure bias quantification**: After Stage 2 training, regenerate all training responses using the student model. Measure: (a) truncation rate, (b) length distribution shift, (c) answer accuracy degradation vs. teacher responses. This establishes the baseline for mixed-policy stage impact.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does replacing divergence-aware sampling with distribution-aware reweighting (using teacher probabilities) improve distillation fidelity?
  - Basis in paper: The conclusion lists exploring "distribution-aware reweighting during SFT" to more faithfully approximate the target distribution as a primary direction for future work.
  - Why unresolved: The current implementation relies on sampling strategies (DAS) rather than reweighting the loss function based on sequence-level probabilities.
  - What evidence would resolve it: A comparative study measuring downstream task accuracy when training with reweighted SFT loss versus the proposed DAS pipeline on identical datasets.

- **Open Question 2**: Can "Boosted Sentences" (sequences amplified by distillation) be identified a priori to mitigate their potential negative correlation with accuracy?
  - Basis in paper: The authors note it is "impossible to directly identify Boosted Sentences" in the current pipeline, yet analysis suggests they correlate negatively with test-set performance.
  - Why unresolved: The current pipeline only admits the teacher and student models prior to training, preventing the detection of sequences that are boosted post-training.
  - What evidence would resolve it: Developing a proxy metric to identify potential boosted sentences and demonstrating that their removal improves final reasoning benchmarks.

- **Open Question 3**: Does the effectiveness of divergence-aware sampling (DAS) diminish as the capacity gap between teacher and student models narrows?
  - Basis in paper: DAS prioritizes examples where the teacher assigns high confidence but the student assigns low probability; this divergence may disappear if the student model is sufficiently large.
  - Why unresolved: The paper primarily validates the method using large capacity gaps (e.g., 120B to 4B parameters), leaving the behavior under smaller gaps unexplored.
  - What evidence would resolve it: Ablation experiments applying DAS to teacher-student pairs with similar parameter counts (e.g., 8B to 7B) to observe if the performance gain persists.

## Limitations

- The temperature-scheduled learning mechanism relies on unstated thresholds for the transition between low and high temperatures, with the paper not specifying how the 50K sample cutoff was determined.
- Divergence-aware sampling's effectiveness depends on accurate probability estimation from both teacher and student models, but the paper doesn't address potential calibration issues or tokenizer mismatch robustness.
- The mixed-policy distillation stage uses a relatively small number of samples (12.7K) and the selection criteria for quality filtering remain unspecified, making it difficult to assess whether gains are robust or data-dependent.

## Confidence

**High Confidence**: The core observation that sequence-level distillation suffers from inadequate distribution coverage, misalignment, and exposure bias is well-supported by empirical evidence. The AIME and GPQA results demonstrating state-of-the-art performance among open-source models of similar scale are convincing given the controlled evaluation setup.

**Medium Confidence**: The specific mechanisms proposed (temperature scheduling, DAS, mixed-policy) show consistent improvements across multiple datasets, but the magnitude of gains may be partially dataset-dependent. The claim that these improvements stem primarily from the proposed mechanisms rather than from careful data curation or model initialization requires further validation.

**Low Confidence**: The assertion that 448K samples is "an order of magnitude fewer" than existing efforts is difficult to verify without comprehensive sampling of the open-source distillation literature, and the paper doesn't provide systematic comparison of training efficiency across different approaches.

## Next Checks

1. **Temperature transition sensitivity analysis**: Systematically vary the transition point between low and high temperature stages (e.g., 25K, 50K, 75K samples) to determine whether the current 50K threshold is optimal or merely sufficient. This would validate whether the curriculum effect is robust or fragile.

2. **DAS threshold calibration study**: Conduct ablation studies varying the probability ratio threshold for divergence-aware sampling (e.g., 2x, 5x, 10x) and analyze how different thresholds affect the distribution of selected examples and downstream performance. This would reveal whether DAS's effectiveness depends on specific hyperparameter choices.

3. **Teacher-student tokenizer alignment stress test**: Train DAS-filtered models using teacher and student pairs with deliberately mismatched tokenizers to assess whether the method's effectiveness degrades when exact probability computation across models becomes unreliable. This would validate the claimed robustness to architectural differences.