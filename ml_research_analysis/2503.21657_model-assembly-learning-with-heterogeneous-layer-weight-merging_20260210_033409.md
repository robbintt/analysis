---
ver: rpa2
title: Model Assembly Learning with Heterogeneous Layer Weight Merging
arxiv_id: '2503.21657'
source_url: https://arxiv.org/abs/2503.21657
tags:
- merging
- parameters
- learning
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Assembly Learning (MAL), a novel paradigm
  for merging heterogeneous neural network models with different architectures and
  training data. The core innovation is a generalized permutation transformation that
  addresses layer-width mismatches through zero-padding and bidirectional alignment,
  allowing selective parameter integration across layers.
---

# Model Assembly Learning with Heterogeneous Layer Weight Merging

## Quick Facts
- arXiv ID: 2503.21657
- Source URL: https://arxiv.org/abs/2503.21657
- Authors: Yi-Kai Zhang; Jin Wang; Xu-Xiang Zhong; De-Chuan Zhan; Han-Jia Ye
- Reference count: 5
- One-line primary result: Introduces Model Assembly Learning (MAL) for merging heterogeneous neural network models with different architectures and training data while preserving linear mode connectivity.

## Executive Summary
This paper presents Model Assembly Learning (MAL), a novel paradigm for merging heterogeneous neural network models with different architectures and training data. The core innovation is a generalized permutation transformation that addresses layer-width mismatches through zero-padding and bidirectional alignment, allowing selective parameter integration across layers. Unlike previous homogeneous model merging approaches, MAL enables iterative integration of parameters from diverse pre-trained models into a base model, guided by layer output invariance rather than full model output invariance. The authors establish key laws and practical guidelines for effective heterogeneous parameter merging.

## Method Summary
MAL operates by aligning and merging layer parameters from heterogeneous models through generalized permutation transformations. For size-compatible layers, it solves a Linear Assignment Problem to find optimal neuron alignment. For size-incompatible layers, it employs bidirectional alternating optimization with zero-padding. The method merges parameters layer-by-layer using convex combinations, maintaining linear mode connectivity when the merging factor stays below a threshold. Training uses learning rates of 1e-4 and 1e-3, and the approach is evaluated across 30 MLP architectures spanning 5 categories with widths from 16-128 on CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.

## Key Results
- MAL maintains linear mode connectivity and preserves original-domain performance within a merging factor threshold
- The approach enables knowledge transfer across 30 different architectures spanning 5 categories trained on 4 datasets
- Experimental results demonstrate effectiveness of zero-padding and bidirectional alignment for handling size mismatches
- The method provides practical guidelines for selective layer merging based on performance considerations

## Why This Works (Mechanism)
MAL works by exploiting permutation invariance in neural networks, where swapping neurons within a layer (and corresponding neurons in adjacent layers) produces functionally identical networks. This symmetry allows the algorithm to find optimal alignments between heterogeneous layers through Linear Assignment Problem solving. The bidirectional alternating optimization ensures both directions of alignment (base→target and target→base) are optimized, creating a more stable merging pathway. By focusing on layer output invariance rather than full model invariance, MAL can selectively transfer knowledge while preserving the base model's core capabilities.

## Foundational Learning
- **Concept: Linear Mode Connectivity (LMC)**
  - Why needed here: Provides theoretical guarantee that convexly combining aligned weights won't destroy model performance
  - Quick check question: Can you explain why two models with identical architectures but different random initializations typically fail to satisfy LMC, and how permutation invariance helps?

- **Concept: Permutation Invariance in Neural Networks**
  - Why needed here: Core technical operation is aligning neurons by exploiting functional symmetry
  - Quick check question: For a simple 2-layer MLP y = W2 σ(W1 x), if you permute the hidden units, what is the precise mathematical operation on W1 and W2 to keep the function f(x) identical?

- **Concept: Linear Assignment Problem (LAP)**
  - Why needed here: Frames alignment problem as computationally tractable optimization to find optimal permutation matrix P
  - Quick check question: How is the cost matrix for the LAP constructed in this context, and what does an optimal solution represent in terms of layer features?

## Architecture Onboarding
- **Component map:** Model zoo -> Alignment Engine (LAP or alternating optimization) -> Merger (convex combination) -> Controller (merging factor selection)
- **Critical path:** Success hinges on the Alignment Engine; suboptimal permutation P causes high loss barriers and LMC failure
- **Design tradeoffs:**
  - Selective vs. Full Merging: Determining which layers to merge; merging deep layers into shallow ones can hurt performance
  - Alignment Fidelity vs. Compute: Bidirectional alignment is more complex; restricting to size-compatible layers for speed
  - Merging Factor λ: Critical threshold; pushing too far breaks LMC even with good alignment
- **Failure signatures:**
  - High Loss Barrier: Loss spikes significantly above parent models during λ variation
  - Semantic Drift: Merged model loses base model's original capabilities catastrophically
  - Non-Convergence of Alternating Optimization: Solver fails to converge, indicating fundamental incompatibility
- **First 3 experiments:**
  1. Validate LAP Solver: Replicate size-compatible merging with two MLPs on CIFAR-10, plot loss barrier vs. λ, compare to naive merge
  2. Test Bidirectional Alignment: Create size-incompatible scenario (base width 32, target width 48), implement alternating optimization, compare to zero-padding + standard LAP
  3. Probe "Bag of Laws": Merge only final layer, only first layer, and all layers from target model; measure performance on original and target tasks

## Open Questions the Paper Calls Out
- Can MAL effectively scale to Large Language Models (LLMs) with billions of parameters?
- How does MAL perform in multi-task scenarios where base and target models have conflicting objective functions?
- Does the alternating optimization strategy for "size-incompatible" layers guarantee convergence to a superior local optimum compared to naive merging?

## Limitations
- Scalability to extremely large models (billions of parameters) remains unproven
- Theoretical justification for selective layer merging is empirical rather than derived from first principles
- Performance threshold where MAL breaks down under extreme width mismatches remains unclear

## Confidence
- **High confidence**: Fundamental claim that LMC can be preserved through generalized permutation alignment is well-supported
- **Medium confidence**: Effectiveness of zero-padding and bidirectional alignment across all dataset pairs
- **Low confidence**: Scalability of alternating optimization solver for bidirectional alignment in very deep networks or with extreme width mismatches

## Next Checks
1. **Architectural generalization test**: Apply MAL to merge heterogeneous CNNs (varying channel widths and spatial dimensions) and measure LMC preservation across ResNet, VGG, and EfficientNet variants
2. **Extreme mismatch analysis**: Systematically evaluate MAL performance when base and target models differ by 2×, 4×, and 8× in layer width, measuring the threshold where LMC breaks down
3. **Cross-task knowledge transfer**: Test whether MAL can transfer knowledge from one task to another (e.g., merging a CIFAR-10 model into a CIFAR-100 base model) while preserving both original capabilities and gaining new ones