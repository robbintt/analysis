---
ver: rpa2
title: 'Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences
  in Post-Hoc Feature Attribution'
arxiv_id: '2512.11108'
source_url: https://arxiv.org/abs/2512.11108
tags:
- bias-agg
- bias-attr
- bias
- position
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework to evaluate lexical and position\
  \ biases in feature attribution methods for language models. The authors propose\
  \ three metrics\u2014Bias-cons (inter-seed consistency), Bias-agg (inter-model comparison),\
  \ and Bias-attr (inter-method comparison)\u2014to quantify different sources of\
  \ bias."
---

# Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution

## Quick Facts
- **arXiv ID:** 2512.11108
- **Source URL:** https://arxiv.org/abs/2512.11108
- **Reference count:** 40
- **Primary result:** Introduces a framework to evaluate lexical and position biases in post-hoc feature attribution, revealing that bias sources differ across seeds, models, and methods, and that explanation disagreement correlates with higher bias.

## Executive Summary
This paper introduces a framework to systematically evaluate lexical and position biases in post-hoc feature attribution methods for language models. The authors propose three novel metrics—Bias-cons (inter-seed consistency), Bias-agg (inter-model comparison), and Bias-attr (inter-method comparison)—to quantify different sources of bias in attribution explanations. Experiments on artificial and semi-natural datasets reveal that lexical and position biases are not necessarily correlated, that more divergent explanations are associated with higher bias, and that classical faithfulness metrics fail to detect bias in models that do not learn the task. The work underscores the importance of bias-aware evaluation for trustworthy model interpretability in NLP.

## Method Summary
The authors introduce a framework to evaluate lexical and position biases in post-hoc feature attribution methods for NLP models. They propose three metrics: Bias-cons (measuring consistency of bias across training seeds), Bias-agg (comparing bias across different models), and Bias-attr (assessing bias differences between attribution methods). Experiments are conducted on two BERT-based models using artificial datasets with controlled lexical/position information and a semi-controlled natural language task for causal relation detection. The framework quantifies bias via lexical bias (preference for specific words) and position bias (preference for specific token positions), and compares attribution methods by their disagreement and bias levels. The approach is validated through controlled experiments and cross-model comparisons.

## Key Results
- Lexical and position biases are not necessarily correlated—models high in one type may be low in the other.
- ModernBERT does not consistently exhibit lower bias than traditional BERT.
- Methods producing more divergent explanations (higher disagreement) tend to be more biased overall.
- Classical faithfulness metrics (sufficiency and comprehensiveness) fail to detect bias in models that do not learn the task, suggesting limitations in their applicability.

## Why This Works (Mechanism)
The framework works by disentangling bias sources via three metrics that isolate effects of random seeds, model architectures, and attribution methods. Controlled datasets enable precise measurement of lexical and position preferences. The metrics quantify bias as deviations from random baseline and compare explanation stability and disagreement across conditions. This design reveals that bias is not monolithic and that faithfulness metrics alone are insufficient for bias detection.

## Foundational Learning
- **Lexical bias**: Model preference for specific words; needed to detect over-attribution to certain terms. Quick check: measure word frequency in attributions vs. true importance.
- **Position bias**: Model preference for certain token positions; needed to detect positional artifacts in explanations. Quick check: vary token positions and measure attribution stability.
- **Explanation disagreement**: Variability in attributions across methods; needed to assess robustness of explanations. Quick check: compute pairwise attribution differences.
- **Bias-consistency**: Stability of bias across training seeds; needed to ensure bias is not due to random initialization. Quick check: repeat training with different seeds and compare bias.
- **Faithfulness metrics**: Sufficiency and comprehensiveness; needed as baselines for comparison. Quick check: verify these metrics detect actual task performance.
- **Controlled datasets**: Artificial tasks with known lexical/position structure; needed for ground truth bias measurement. Quick check: confirm dataset preserves intended biases.

## Architecture Onboarding
- **Component map:** Artificial dataset generation -> Bias-cons, Bias-agg, Bias-attr metrics -> Model and method comparisons -> Analysis of faithfulness metrics
- **Critical path:** Generate controlled dataset -> Compute bias metrics across seeds, models, methods -> Analyze bias patterns and explanation disagreement -> Validate with faithfulness metrics
- **Design tradeoffs:** Controlled datasets enable ground truth but may not generalize; bias metrics are sensitive but depend on dataset design; faithfulness metrics are standard but miss certain bias types
- **Failure signatures:** Bias metrics inconsistent across seeds; explanation disagreement low but bias high; faithfulness metrics fail to detect non-learning models
- **First experiments:** 1) Replicate bias measurement on a new model (e.g., RoBERTa); 2) Compare bias metrics on a real-world dataset; 3) Test alternative attribution methods for bias detection

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Reliability of bias measurements depends on metric stability and generalizability across architectures and tasks.
- Generalizability to non-BERT architectures (e.g., GPT-style or multilingual models) is unclear.
- Classical faithfulness metrics fail in non-learning models, suggesting broader applicability issues.

## Confidence
- **High confidence:** bias dissociations, bias-aggregation metric results, bias-attr metric results, and faithfulness metrics' failure in non-learning models
- **Medium confidence:** lexical vs. position bias independence across methods, model bias comparisons, and explanation disagreement–bias relationship
- **Low confidence:** generalizability to other architectures and tasks, stability of metrics across diverse settings

## Next Checks
1. Replicate bias measurements across a wider range of architectures (e.g., RoBERTa, DeBERTa, GPT-style models) and languages to test metric robustness.
2. Conduct ablation studies on dataset complexity and task difficulty to assess whether observed bias patterns persist under varied conditions.
3. Compare proposed metrics against alternative bias detection approaches (e.g., SHAP, LIME, gradient-based methods) to evaluate their relative sensitivity and specificity.