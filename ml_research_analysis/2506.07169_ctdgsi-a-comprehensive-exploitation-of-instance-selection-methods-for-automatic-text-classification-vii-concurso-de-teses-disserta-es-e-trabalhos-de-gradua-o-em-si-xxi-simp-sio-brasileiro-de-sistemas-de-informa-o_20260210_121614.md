---
ver: rpa2
title: "CTDGSI: A comprehensive exploitation of instance selection methods for automatic\
  \ text classification. VII Concurso de Teses, Disserta\xE7\xF5es e Trabalhos de\
  \ Gradua\xE7\xE3o em SI -- XXI Simp\xF3sio Brasileiro de Sistemas de Informa\xE7\
  \xE3o"
arxiv_id: '2506.07169'
source_url: https://arxiv.org/abs/2506.07169
tags:
- methods
- effectiveness
- datasets
- selection
- cunha
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This Ph.D. dissertation investigates Instance Selection (IS) methods
  for reducing training data in Automatic Text Classification (ATC) while maintaining
  model effectiveness.
---

# CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação

## Quick Facts
- arXiv ID: 2506.07169
- Source URL: https://arxiv.org/abs/2506.07169
- Reference count: 0
- Achieves 41% average reduction in training data while maintaining model effectiveness for automatic text classification

## Executive Summary
This Ph.D. dissertation addresses the computational challenges of training transformer-based models for automatic text classification by developing novel instance selection methods. Traditional IS techniques designed for small tabular datasets have been largely unexplored in NLP contexts despite the massive computational requirements of modern transformer architectures. The research introduces E2SC, a two-step framework that estimates optimal reduction rates using calibrated weak classifiers, and extends this to biO-IS, which simultaneously removes both redundant and noisy instances. Experimental results demonstrate that ATC models can be effectively trained with substantially less data while achieving significant computational speedups.

## Method Summary
The research proposes a novel approach to instance selection for automatic text classification by introducing E2SC (Estimation of Effective Sampling Criteria), a two-step framework that first estimates optimal reduction rates using calibrated weak classifiers before removing redundant instances. Building on E2SC, the biO-IS framework extends this approach to simultaneously address both redundancy and noise removal through an iterative process. These methods leverage the redundancy inherent in text classification datasets while maintaining model effectiveness, representing a significant departure from traditional IS methods that were primarily designed for small tabular datasets. The frameworks are specifically designed to work with transformer-based architectures, addressing the substantial computational resources required for training these models on large text datasets.

## Key Results
- Achieves an average 41% reduction in training sets across all tested datasets
- Maintains model effectiveness while significantly reducing computational requirements
- Delivers speedup improvements of 1.67x (up to 2.46x) in training time
- Demonstrates that transformer-based ATC models can be effectively trained with substantially less data

## Why This Works (Mechanism)
The effectiveness stems from the redundancy inherent in text classification datasets combined with the robustness of transformer architectures. By using calibrated weak classifiers to estimate optimal reduction rates, the framework identifies instances that contribute minimal additional information while preserving the core semantic patterns necessary for accurate classification. The iterative approach in biO-IS allows for simultaneous identification and removal of both redundant and noisy instances, addressing two critical data quality issues that typically degrade model performance.

## Foundational Learning

**Instance Selection (IS) Methods**: Techniques for reducing training data size while maintaining model performance - needed for computational efficiency; quick check: compare training time and resource usage with and without IS

**Transformer-based Architectures**: Deep learning models using self-attention mechanisms for NLP tasks - needed for modern ATC applications; quick check: verify model architecture specifications and parameter counts

**Calibrated Weak Classifiers**: Simple models adjusted to provide reliable probability estimates - needed for accurate reduction rate estimation; quick check: validate calibration metrics like Brier score

## Architecture Onboarding

**Component Map**: Raw Text Data -> Weak Classifier Calibration -> Redundancy Estimation -> Instance Filtering -> Reduced Training Set -> Transformer Model Training -> Performance Evaluation

**Critical Path**: The sequence from weak classifier calibration through redundancy estimation to instance filtering represents the most time-sensitive path, as delays in early stages compound throughout the pipeline.

**Design Tradeoffs**: The framework balances between aggressive data reduction (maximizing computational savings) and maintaining model effectiveness (preserving classification accuracy). The use of weak classifiers trades some precision in reduction estimation for computational efficiency in the selection process.

**Failure Signatures**: If model performance degrades significantly after reduction, it likely indicates over-aggressive filtering or poor calibration of weak classifiers. If speedup gains are minimal despite substantial data reduction, it may suggest inefficient implementation or suboptimal selection criteria.

**First Experiments**:
1. Run E2SC on a small benchmark dataset to validate reduction rate estimation accuracy
2. Compare biO-IS against traditional random sampling methods on a medium-sized dataset
3. Perform ablation studies to quantify the individual contributions of redundancy versus noise removal

## Open Questions the Paper Calls Out
None

## Limitations

- Limited generalizability beyond tested datasets and transformer architectures
- Uncertainty about scalability to extremely large datasets or complex language tasks
- Lack of extensive ablation studies to quantify individual component contributions

## Confidence

- Effectiveness claims: Medium-High (comprehensive experimental design but limited scope)
- Energy and cost savings claims: Low-Medium (based on computational metrics rather than direct measurements)

## Next Checks

1. Conduct cross-domain validation testing the frameworks on diverse NLP tasks including sentiment analysis, named entity recognition, and document summarization to assess generalizability
2. Perform ablation studies on biO-IS to isolate the impact of redundancy removal versus noise removal on model performance
3. Implement real-world deployment testing with direct energy consumption measurements to verify claimed cost savings and environmental benefits