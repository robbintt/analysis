---
ver: rpa2
title: Reinforcement Learning on Pre-Training Data
arxiv_id: '2509.19249'
source_url: https://arxiv.org/abs/2509.19249
tags:
- uni00000015
- uni00000011
- reasoning
- rlpt
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new training-time scaling paradigm for large
  language models called Reinforcement Learning on Pre-Training data (RLPT). The method
  addresses the challenge of finite high-quality text data constraining conventional
  scaling approaches.
---

# Reinforcement Learning on Pre-Training Data

## Quick Facts
- arXiv ID: 2509.19249
- Source URL: https://arxiv.org/abs/2509.19249
- Reference count: 9
- Key outcome: RLPT achieves absolute improvements of 3.0-8.1 on multiple benchmarks when applied to Qwen3-4B-Base

## Executive Summary
This paper introduces Reinforcement Learning on Pre-Training data (RLPT), a novel training-time scaling paradigm for large language models that addresses the limitation of finite high-quality text data. Unlike conventional approaches that require human annotation, RLPT enables autonomous exploration of meaningful reasoning trajectories through reinforcement learning directly on pre-training data. The method employs a next-segment reasoning objective that rewards accurate prediction of subsequent text segments, eliminating the annotation bottleneck.

The approach demonstrates significant performance gains across multiple benchmarks including MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, and AIME competitions. The results show favorable scaling behavior with training compute and establish RLPT as a strong foundation that extends reasoning boundaries while enhancing subsequent reinforcement learning from human feedback (RLVR) performance.

## Method Summary
RLPT addresses the finite high-quality data constraint by performing reinforcement learning directly on pre-training data rather than requiring additional human-annotated datasets. The method uses a next-segment reasoning objective where the model is rewarded for accurately predicting subsequent text segments, enabling autonomous exploration of reasoning trajectories. This self-supervised approach eliminates the need for human annotation while maintaining the benefits of reinforcement learning. The training process involves exploring meaningful reasoning paths through the pre-training corpus and reinforcing trajectories that lead to accurate predictions of following segments.

## Key Results
- Applied to Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1, 6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25 respectively
- Demonstrates favorable scaling behavior with training compute
- Provides a strong foundation that extends reasoning boundaries and enhances subsequent RLVR performance

## Why This Works (Mechanism)
RLPT works by leveraging the vast amount of pre-training data to enable autonomous reasoning trajectory exploration without human intervention. The next-segment reasoning objective creates a self-supervised reward signal that guides the model toward discovering meaningful reasoning patterns. By performing reinforcement learning directly on the pre-training corpus, the method avoids the data bottleneck associated with human annotation while still benefiting from the exploration-exploitation dynamics of RL. The approach effectively transforms the pre-training data into a playground for autonomous reasoning skill development.

## Foundational Learning
- Reinforcement Learning: Why needed - Enables autonomous exploration and skill acquisition beyond supervised learning; Quick check - Verify policy gradients are properly computed and applied
- Trajectory-based reasoning: Why needed - Captures sequential decision-making in problem-solving; Quick check - Confirm trajectory diversity and quality metrics
- Self-supervised reward signals: Why needed - Eliminates dependency on human annotation; Quick check - Validate reward consistency across similar reasoning paths
- Text segmentation strategies: Why needed - Enables meaningful next-segment prediction tasks; Quick check - Test segmentation stability across different text types
- Compute-efficient scaling: Why needed - Maintains performance gains as model size increases; Quick check - Measure training efficiency across different compute budgets

## Architecture Onboarding

Component map: Pre-training corpus -> Text segmenter -> RL agent -> Next-segment predictor -> Reward calculator -> Policy optimizer

Critical path: The core loop involves segmenting pre-training text, having the RL agent predict next segments, calculating rewards based on prediction accuracy, and updating the policy through optimization.

Design tradeoffs: The method trades off between exploration (discovering new reasoning paths) and exploitation (reinforcing known successful trajectories). The self-supervised nature eliminates annotation costs but may introduce biases from the pre-training data distribution.

Failure signatures: Potential issues include mode collapse where the model converges to limited reasoning patterns, reward hacking through superficial text patterns rather than genuine reasoning, and overfitting to specific text segment structures.

First experiments:
1. Validate that next-segment prediction accuracy correlates with downstream task performance
2. Test trajectory diversity across different reasoning domains
3. Compare performance against baseline models trained with standard RL from human feedback

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Uncertainty whether performance gains stem from genuine reasoning improvements versus memorization of text patterns
- Lack of ablation studies to isolate contributions of individual components (reasoning objective, trajectory exploration, reinforcement learning)
- Limited discussion of potential overfitting to specific benchmark datasets and generalization to real-world applications
- Scaling behavior claims based only on Qwen3-4B model size, limiting conclusions about effectiveness across different scales

## Confidence

High confidence: The RLPT methodology is clearly described and reproducible

Medium confidence: The reported benchmark improvements are valid but may not generalize beyond tested domains

Medium confidence: The scaling behavior claims are preliminary due to limited model size testing

## Next Checks

1. Conduct ablation studies to determine which components of RLPT contribute most to performance gains

2. Test RLPT on out-of-distribution tasks and real-world applications to assess generalization

3. Evaluate performance across multiple model sizes to validate scaling behavior claims