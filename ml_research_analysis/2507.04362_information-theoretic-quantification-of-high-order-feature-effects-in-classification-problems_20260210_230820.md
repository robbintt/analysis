---
ver: rpa2
title: Information-theoretic Quantification of High-order Feature Effects in Classification
  Problems
arxiv_id: '2507.04362'
source_url: https://arxiv.org/abs/2507.04362
tags:
- feature
- features
- class
- information
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the High-order interactions for Feature importance
  (Hi-Fi) method by embedding it within an information-theoretic framework based on
  Conditional Mutual Information (CMI). The approach decomposes feature contributions
  into unique, synergistic, and redundant components, offering richer, model-independent
  interpretability.
---

# Information-theoretic Quantification of High-order Feature Effects in Classification Problems

## Quick Facts
- **arXiv ID:** 2507.04362
- **Source URL:** https://arxiv.org/abs/2507.04362
- **Reference count:** 0
- **Primary result:** A kNN-based CMI estimator accurately recovers feature importance decompositions into unique, redundant, and synergistic components in synthetic and real data.

## Executive Summary
This work extends the Hi-Fi method by embedding it within an information-theoretic framework using Conditional Mutual Information (CMI). The approach decomposes feature contributions into unique, synergistic, and redundant components, offering richer, model-independent interpretability. A k-Nearest Neighbor (kNN) estimator is proposed to handle mixed discrete and continuous variables, enabling robust computation of CMI even in high-dimensional settings. Validation on synthetic datasets with known Gaussian structures demonstrates accurate recovery of theoretical values, while experiments on non-Gaussian and real-world breast cancer gene expression data from TCGA-BRCA show consistent identification of expected interaction patterns.

## Method Summary
The method quantifies feature importance by decomposing CMI into unique (U), redundant (R), and synergistic (S) components. It uses a mixed-variable kNN estimator to compute CMI between continuous features and discrete targets, with bias reduction via distance projection. A greedy forward search identifies conditioning subsets that minimize (for redundancy) or maximize (for synergy) CMI, with surrogate testing to ensure statistical significance. The decomposition framework builds on the Hi-Fi method, providing a model-agnostic, information-theoretic interpretation of feature interactions in classification problems.

## Key Results
- Accurate recovery of theoretical CMI values on synthetic Gaussian datasets with known interaction structures
- Correct identification of synergistic interactions in non-Gaussian XOR-like synthetic setups where standard correlation fails
- Consistent detection of biologically plausible interaction patterns in TCGA-BRCA breast cancer gene expression data

## Why This Works (Mechanism)

### Mechanism 1
The method decomposes feature importance into unique, redundant, and synergistic components by finding specific conditioning subsets of features that minimize or maximize Conditional Mutual Information (CMI). The framework searches for two subsets: $Z_{min}$, which minimizes CMI $I(Y;X|Z)$ to isolate unique information ($U$), and $Z_{max}$, which maximizes CMI to capture synergistic interactions ($S$). Redundancy ($R$) is derived as the difference between the marginal mutual information and the minimized CMI. This assumes unique contribution is strictly the information remaining when all redundant confounders are conditioned away, and synergy arises specifically when conditioning on variables that alter the correlation structure with the target.

### Mechanism 2
A mixed k-Nearest Neighbor (kNN) estimator allows for the robust computation of CMI across continuous features and discrete class labels without parametric assumptions. The estimator adapts the Kraskov kNN method to mixed variables by performing neighbor searches in the joint space of the source feature $X$, the class $Y$, and the conditioning set $Z$. It projects distances from the highest-dimensional space to lower ones to reduce estimation bias. This relies on the local uniformity assumption inherent to kNN density estimation holding for the mixed discrete-continuous joint distributions in the dataset.

### Mechanism 3
Statistical significance testing via surrogate data prevents the inclusion of spurious features into the redundant or synergistic subsets. Before adding a candidate variable $V_j$ to a conditioning set $Z$, the method shuffles $V_j$ to generate a null distribution of the CMI change. The variable is retained only if the observed CMI difference exceeds the 95th percentile of this surrogate distribution. This assumes the surrogate generation process (shuffling) accurately simulates the null hypothesis of independence without distorting the underlying marginal distributions.

## Foundational Learning

- **Concept: Conditional Mutual Information (CMI)**
  - **Why needed here:** This is the fundamental metric used to quantify the information a feature provides about a target, given that we already know other features. Without this, the definitions of "unique" and "synergistic" in the paper are unintelligible.
  - **Quick check question:** If $I(Y;X|Z) > I(Y;X)$, what does this imply about the relationship between $X$ and $Z$ regarding $Y$? (Answer: Synergy/Interaction)

- **Concept: Partial Information Decomposition (PID)**
  - **Why needed here:** The paper explicitly contrasts its approach against PID and uses PID terminology (Unique, Redundant, Synergistic). Understanding PID helps clarify that this paper offers an alternative "observation-driven" decomposition rather than a strict lattice-based PID.
  - **Quick check question:** Why is defining "redundancy" considered a hard problem in PID frameworks? (Answer: Defining shared information without double-counting or requiring intersection definitions)

- **Concept: Bias-Variance Tradeoff in kNN Estimation**
  - **Why needed here:** The choice of $k$ (neighbors) and the projection method are attempts to manage estimation error. Understanding this helps in diagnosing why the estimator might fail on small or high-dimensional data.
  - **Quick check question:** In kNN estimation, what generally happens to the bias if $k$ is set too small?

## Architecture Onboarding

- **Component map:** Input -> Mixed kNN Estimator -> Search Engine -> Significance Filter -> Decomposition Layer
- **Critical path:** The Search Engine is the computational bottleneck. It requires repeated calls to the CMI Estimator for every candidate feature at every iteration $j$.
- **Design tradeoffs:**
  - Greedy vs. Exhaustive Search: The paper uses a greedy approach to manage complexity, potentially missing global optima for interaction sets
  - Fixed $k$ vs. Adaptive $k$: Using a fixed $k=10$ simplifies implementation but assumes scale invariance or uniform density which may not hold
- **Failure signatures:**
  - Empty Sets: If data is highly noisy or $N$ is small, the Significance Filter may reject all candidates, resulting in $Z_{min} = Z_{max} = \emptyset$
  - Inconsistent Decomposition: Due to estimation variance, theoretically impossible combinations might appear if the search converges to local minima
- **First 3 experiments:**
  1. Validation on Gaussian Data: Replicate the "Unique," "Redundant," and "Synergistic" simulations to verify the estimator recovers theoretical CMI values
  2. Sensitivity Analysis: Test the "Multiple Interactions" setup with reduced sample sizes to observe bias introduction
  3. Non-Gaussian Logic Check: Implement the XOR-like synthetic setup to confirm the method detects synergy where standard correlation metrics fail

## Open Questions the Paper Calls Out
1. How robust is the kNN-based CMI estimator in high-dimensional systems where the curse of dimensionality significantly impacts distance metrics?
2. How does the proposed CMI-based decomposition empirically and theoretically differ from standard Partial Information Decomposition (PID) frameworks?
3. To what extent does the greedy search algorithm fail to identify the true interaction subsets ($Z_{min}$, $Z_{max}$) compared to an exhaustive search?

## Limitations
- Computational complexity of greedy search scales poorly with feature dimensionality
- Reliance on local uniformity assumptions in kNN estimation may break down in high-dimensional spaces
- Surrogate testing may be overly conservative for small sample sizes, potentially missing genuine interactions

## Confidence

- **High Confidence:** The core mathematical framework (CMI decomposition into U, R, S components) and the kNN estimation methodology are well-established in the literature and correctly implemented
- **Medium Confidence:** The greedy search algorithm's ability to find globally optimal conditioning sets is assumed but not rigorously proven; local minima could lead to misattribution of feature effects
- **Low Confidence:** The paper's claims about performance on real-world TCGA-BRCA data are not independently verified, and the biological relevance of identified interactions is not established

## Next Checks
1. **Scalability Test:** Run the method on synthetic datasets with increasing numbers of features (e.g., 10, 20, 50) to quantify the exponential growth in computation time and identify practical limits
2. **Robustness to Noise:** Introduce varying levels of Gaussian noise to the synthetic Gaussian datasets and measure how estimation variance affects the recovery of theoretical CMI values
3. **Cross-Validation on Real Data:** Apply the method to a different real-world dataset (e.g., MNIST or a different gene expression dataset) to verify that the identified interaction patterns are consistent and not specific to the TCGA-BRCA data structure