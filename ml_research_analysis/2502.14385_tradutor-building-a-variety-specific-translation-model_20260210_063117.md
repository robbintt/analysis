---
ver: rpa2
title: 'Tradutor: Building a Variety Specific Translation Model'
arxiv_id: '2502.14385'
source_url: https://arxiv.org/abs/2502.14385
tags:
- portuguese
- language
- translation
- european
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to create high-quality translation
  models for underrepresented language varieties, using European Portuguese as a case
  study. The core approach involves translating monolingual European Portuguese text
  into English using a high-quality translation system, creating a parallel corpus,
  and then fine-tuning a pre-trained language model on this data.
---

# Tradutor: Building a Variety Specific Translation Model

## Quick Facts
- **arXiv ID**: 2502.14385
- **Source URL**: https://arxiv.org/abs/2502.14385
- **Reference count**: 19
- **Primary result**: Fine-tuned translation models outperform open-source systems and achieve performance close to leading closed-source systems like Google Translate and DeepL on European Portuguese-English translation.

## Executive Summary
This paper presents a method to create high-quality translation models for underrepresented language varieties, using European Portuguese as a case study. The authors developed the largest European Portuguese-English parallel corpus to date, named PTradutor, comprising 1.7 million documents. By translating monolingual European Portuguese text into English using Google Translate and fine-tuning pre-trained language models on this data, they demonstrate that their models outperform all open-source translation systems and achieve performance close to leading closed-source systems like Google Translate and DeepL. The models also maintain strong fidelity to European Portuguese, as measured by a language variety classifier.

## Method Summary
The authors create PTradutor by translating monolingual European Portuguese texts into English using Google Translate, then applying a multi-stage filtering pipeline to ensure data quality. They fine-tune Gemma-2, Phi-3, and LLaMA-3 models using both full fine-tuning and LoRA approaches. Training uses batch sizes of 256-512, learning rate of 2e-5, weight decay of 0.1, and early stopping with patience of 3,000 steps. Models are evaluated on FRMT and NTrex benchmarks using BLEU, ROUGE-L, COMET, and a novel VID (Variety ID) score that measures preservation of European Portuguese variety.

## Key Results
- The best fine-tuned model outperforms all open-source translation systems on European Portuguese benchmarks
- Models achieve performance close to leading closed-source systems like Google Translate and DeepL
- Full fine-tuning produces better translation quality than LoRA, while LoRA better preserves European Portuguese variety identity
- VID scores show the models maintain strong fidelity to European Portuguese

## Why This Works (Mechanism)

### Mechanism 1: Retro-Translation for Parallel Corpus Synthesis
Translating monolingual texts from the low-resource variety to a resource-rich language produces higher-quality parallel training data than the reverse direction. Industry translation systems achieve better quality when translating INTO resource-rich languages, so collecting authentic European Portuguese texts and translating them TO English keeps the low-resource side human-written with genuine variety-specific features intact.

### Mechanism 2: Quality Filtering Pipeline
Aggressive filtering on data quality yields better models than maximizing raw corpus size. Multi-stage filtering removes ~2.2M of 4M documents through boilerplate detection, deduplication, invalid character removal, over-represented template patterns, and token length limits. Social media content is most heavily pruned.

### Mechanism 3: Full Fine-Tuning vs. LoRA Trade-off
Full fine-tuning achieves translation quality; LoRA preserves variety identity but sacrifices coherence. Full fine-tuning updates all parameters to learn translation patterns, while LoRA's limited adapter capacity causes repetition loops and early training plateau—translation is too complex for low-rank adaptation alone.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (LoRA)**: Understanding LoRA's limitations for translation tasks prevents wasted experiments with under-capacity adapters. *Quick check*: What failure pattern indicates LoRA adapter capacity is insufficient for the task?

- **Language Variety vs. Dialect**: European and Brazilian Portuguese differ in grammar, lexicon, and cultural context—not just accent. This matters for healthcare and judiciary applications. *Quick check*: Why does the paper argue that treating Portuguese as a single language harms Portuguese-speaking countries?

- **VID (Variety ID) Score**: Novel metric introduced to quantify variety preservation, computed as the ratio of classifier-labeled model outputs to classifier-labeled reference translations. *Quick check*: Why compare against reference translations rather than using raw classification percentage directly?

## Architecture Onboarding

- **Component map**: PT-PT Monolingual Sources → Google Translate (PT-PT → EN) → Filtering → PTradutor → Fine-tune Gemma-2/Phi-3/LLaMA-3 → Evaluate on FRMT/NTrex

- **Critical path**: 1) Filtering quality determines model ceiling—most data loss occurs at jusText stage 2) Full fine-tuning required for acceptable translation quality; LoRA insufficient 3) VID score must compare to reference classification to control for classifier bias

- **Design tradeoffs**: VID vs. BLEU (optimizing variety preservation may reduce translation fluency), Full fine-tuning vs. LoRA (quality vs. compute/storage cost), 900-token limit (faster training with larger batches vs. inability to handle long documents)

- **Failure signatures**: Repetition loops in generation → LoRA adapter capacity too low, High BLEU, low VID → Model defaulting to dominant variety (Brazilian Portuguese), Training loss plateaus while validation improves → LoRA early stopping triggered

- **First 3 experiments**: 1) Run filtering pipeline on a sample corpus to understand which stages remove the most data and characterize false positives 2) Test LoRA with increased rank (512→1024) and alpha to validate authors' hypothesis that capacity is the bottleneck 3) Compare zero-shot vs. fine-tuned VID scores to quantify variety alignment improvement from training data

## Open Questions the Paper Calls Out
- How do different generation configurations beyond greedy decoding (e.g., beam search) impact translation quality and European Portuguese language variety fidelity?
- Can prompt optimization before and after training enhance translation performance for European Portuguese?
- Would increasing LoRA adapter capacity (alpha and rank parameters) resolve the observed trade-off between translation quality and language variety preservation?
- How does the 900-token input limit affect translation quality for documents exceeding this threshold, and what sentence splitting strategies best preserve coherence?

## Limitations
- The quality of the retro-translation pipeline from Google Translate is assumed rather than independently verified
- The VID metric lacks validation against human judgment and may be sensitive to classifier bias
- The LoRA failure finding is based on limited experimental variation and may not represent an inherent limitation
- The 900-token input limit requires document splitting strategies that weren't evaluated

## Confidence
- **High Confidence**: Models outperform open-source systems on European Portuguese benchmarks
- **Medium Confidence**: Full fine-tuning is necessary while LoRA is insufficient (based on limited parameter variation)
- **Low Confidence**: Retro-translation produces higher-quality parallel data than other low-resource methods (lacks comparative validation)

## Next Checks
1. Conduct human evaluation of translation quality comparing PT-PT→EN translations against reference translations on a subset of documents
2. Systematically test LoRA configurations with alpha/rank values ranging from 128→1024 to determine whether failure patterns are capacity-limited
3. Compare VID scores against human judgments of variety preservation on a sample of model outputs to validate the metric