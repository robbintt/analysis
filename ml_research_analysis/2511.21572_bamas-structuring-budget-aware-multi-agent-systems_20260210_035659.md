---
ver: rpa2
title: 'BAMAS: Structuring Budget-Aware Multi-Agent Systems'
arxiv_id: '2511.21572'
source_url: https://arxiv.org/abs/2511.21572
tags:
- cost
- bamas
- budget
- topology
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BAMAS, a framework for constructing budget-aware
  multi-agent systems. BAMAS jointly optimizes LLM selection and agent collaboration
  topology through Integer Linear Programming and reinforcement learning, respectively,
  to maximize task performance within a fixed cost budget.
---

# BAMAS: Structuring Budget-Aware Multi-Agent Systems

## Quick Facts
- arXiv ID: 2511.21572
- Source URL: https://arxiv.org/abs/2511.21572
- Reference count: 25
- Key outcome: Reduces LLM costs by up to 86% while maintaining accuracy

## Executive Summary
This paper introduces BAMAS, a framework for constructing budget-aware multi-agent systems. BAMAS jointly optimizes LLM selection and agent collaboration topology through Integer Linear Programming and reinforcement learning, respectively, to maximize task performance within a fixed cost budget. Evaluated on three widely-used datasets (GSM8K, MBPP, MATH), BAMAS achieves comparable accuracy to state-of-the-art approaches while reducing costs by up to 86%, demonstrating an effective cost-performance trade-off. The approach also shows adaptability by selecting collaboration topologies based on task requirements and budget constraints.

## Method Summary
BAMAS employs a two-stage optimization framework for budget-aware multi-agent systems. First, it uses Integer Linear Programming (ILP) to select the optimal combination of LLMs that maximizes task performance within a given budget constraint. Second, it applies reinforcement learning (RL) to optimize the collaboration topology among agents, determining which agents should collaborate on which subtasks. The framework jointly considers LLM selection and agent topology optimization, allowing for adaptive responses to varying task requirements and budget constraints.

## Key Results
- Achieves comparable accuracy to state-of-the-art approaches while reducing costs by up to 86%
- Demonstrates effective cost-performance trade-off on three widely-used datasets (GSM8K, MBPP, MATH)
- Shows adaptability by selecting collaboration topologies based on task requirements and budget constraints

## Why This Works (Mechanism)
The effectiveness of BAMAS stems from its systematic approach to balancing computational costs with performance. By using ILP for LLM selection, it ensures optimal resource allocation within budget constraints. The RL-based topology optimization allows agents to learn efficient collaboration patterns that minimize redundant computations while maximizing information flow. This dual optimization approach addresses both the "what" (which LLMs to use) and "how" (how agents should collaborate) aspects of multi-agent system design.

## Foundational Learning
- Integer Linear Programming (ILP): Why needed - for optimal LLM selection under budget constraints; Quick check - verify ILP constraints properly model cost and performance trade-offs
- Reinforcement Learning (RL): Why needed - for learning optimal agent collaboration topologies; Quick check - ensure reward function captures both performance and efficiency
- Multi-Agent Systems: Why needed - to decompose complex tasks into manageable subtasks; Quick check - validate task decomposition strategy works across different problem domains

## Architecture Onboarding

**Component Map:**
ILP Optimizer -> RL Topology Optimizer -> Task Execution -> Performance Evaluation

**Critical Path:**
Budget constraint input → ILP optimization → LLM selection → RL-based topology optimization → Task execution → Performance evaluation → Feedback to optimization loop

**Design Tradeoffs:**
The framework trades off computational overhead during the planning phase (ILP and RL optimization) for reduced runtime costs and improved performance. This approach may be less suitable for scenarios requiring real-time optimization or where task requirements cannot be predicted in advance.

**Failure Signatures:**
- Suboptimal LLM selection leading to budget overruns
- Ineffective agent collaboration topologies causing redundant computations
- Poor task decomposition resulting in inefficient agent coordination
- RL training instability leading to suboptimal topology configurations

**First 3 Experiments:**
1. Validate ILP-based LLM selection on a simple benchmark with known optimal solutions
2. Test RL-based topology optimization on a small-scale multi-agent task with clear performance metrics
3. Evaluate end-to-end performance on a single dataset (e.g., GSM8K) to verify the complete BAMAS pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead during planning phase due to ILP and RL optimization
- Performance unverified on extremely complex reasoning tasks beyond tested datasets
- Assumption that task requirements can be accurately predicted beforehand may not hold in dynamic scenarios

## Confidence
- High: Cost reduction claims (up to 86%) and comparable accuracy are well-supported by experimental results
- Medium: Generalizability across diverse task domains beyond mathematical and coding problems requires further validation
- Medium: Adaptability claims supported by experimental results but may face challenges in real-world deployment scenarios

## Next Checks
1. Test BAMAS on open-domain multi-step reasoning tasks from diverse domains (e.g., legal reasoning, medical diagnosis) to assess cross-domain generalization
2. Evaluate the framework's performance in dynamic environments where task requirements evolve during execution, rather than being known upfront
3. Conduct a thorough ablation study comparing BAMAS against simpler heuristic-based LLM selection and agent topology approaches to quantify the value of the ILP and RL components