---
ver: rpa2
title: Sparse Nonparametric Contextual Bandits
arxiv_id: '2503.16382'
source_url: https://arxiv.org/abs/2503.16382
tags:
- regret
- sparsity
- bandits
- contextual
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies sparse nonparametric contextual bandits, where
  the reward function is a sparse linear combination of features selected from a countable
  or uncountable infinite set. The authors provide lower bounds on the minimax regret,
  showing polynomial dependence on the number of actions K and the sparsity s.
---

# Sparse Nonparametric Contextual Bandits

## Quick Facts
- arXiv ID: 2503.16382
- Source URL: https://arxiv.org/abs/2503.16382
- Reference count: 40
- One-line primary result: Achieves near-optimal regret bounds for sparse nonparametric contextual bandits by combining Feel-Good Thompson Sampling with sparsity-inducing priors

## Executive Summary
This paper establishes fundamental limits and provides algorithms for sparse nonparametric contextual bandits where rewards are sparse linear combinations of features from infinite sets. The authors prove polynomial dependence on the number of actions K is unavoidable in the worst case, then propose a Feel-Good Thompson Sampling variant that achieves regret bounds matching these lower bounds up to logarithmic factors. For countable sparsity, regret is O(∥w∗∥0√(Ksn log(d_eff n))), and for uncountable sparsity, O(∥w∗∥0√(Ksdn log(n))), where ∥w∗∥0 is the true sparsity.

## Method Summary
The method employs Feel-Good Thompson Sampling with sparsity-inducing priors. For countable sparsity, it restricts features to an effective dimension d_eff and uses a subset selection prior over [d_eff]. For uncountable sparsity, it uses a hierarchical prior with geometric distribution over feature counts. The "feel-good" modification encourages optimism by subtracting λf_ν(x) from the negative log-likelihood. The algorithm samples parameters from the posterior and selects actions to maximize the sampled reward function.

## Key Results
- Proves polynomial dependence on K is unavoidable in worst-case sparse nonparametric contextual bandits
- Achieves regret O(∥w∗∥0√(Ksn log(d_eff n))) for countable sparsity matching lower bounds
- Achieves regret O(∥w∗∥0√(Ksdn log(n))) for uncountable sparsity
- Shows sparsity enables better regret bounds than previous nonparametric methods when horizon is large relative to sparsity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricting the prior distribution to sparse subsets allows the algorithm to ignore irrelevant features in an infinite feature space.
- **Mechanism:** The algorithm employs a subset selection prior (factorised into distribution over subset sizes and uniform distribution over subsets). For countable sparsity, it restricts support to the effective dimension d_eff. For uncountable sparsity, it uses a discrete distribution over the number of features m. This penalizes large models and focuses the posterior on low-dimensional subspaces.
- **Core assumption:** The true reward function is s-sparse (∥w∗∥0 = s) and the feature maps satisfy uniform decay or Lipschitz conditions.
- **Break condition:** If the true reward function is dense, the prior assigns near-zero probability to the correct model, causing regret bounds to fail.

### Mechanism 2
- **Claim:** The "Feel-Good" modification to Thompson Sampling reduces frequentist regret by encouraging optimism.
- **Mechanism:** FGTS modifies the negative log-likelihood L by subtracting λf_ν(x), favoring parameters that predict high maximum reward for current context, effectively biasing exploration toward optimistic outcomes.
- **Core assumption:** The decoupling technique (Zhang 2022) holds, allowing regret to be bounded by a log-partition function Z_n.
- **Break condition:** If λ is not tuned correctly, optimism might be too aggressive or too conservative, breaking the √n regret rate.

### Mechanism 3
- **Claim:** Polynomial dependence on the number of actions K is unavoidable in the worst-case sparse setting.
- **Mechanism:** The paper constructs a theoretical "hard instance" by reducing the sparse bandit problem to a sequence of K-armed bandits. Because the agent must identify correct features and correct action, difficulty scales with K.
- **Core assumption:** Contexts can be chosen by adaptive adversary, and noise is Gaussian.
- **Break condition:** This lower bound (Ω(√Ksn)) holds for worst-case instances but may not hold under additional assumptions like well-conditioned context distributions.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS) & Mercer's Theorem**
  - **Why needed here:** The countable sparsity model relies on expressing reward function as linear combination of eigenfunctions of a kernel (Mercer's theorem).
  - **Quick check question:** Can you explain how a function in an RKHS can be represented as an infinite linear combination of features with decaying coefficients?

- **Concept: Minimax Regret & Lower Bounds**
  - **Why needed here:** Understanding how lower bounds are constructed via reduction to simpler problems is essential to grasp the paper's theoretical weight.
  - **Quick check question:** Why does reducing a contextual bandit problem to a sequence of multi-armed bandits provide a lower bound on the regret of any possible policy?

- **Concept: PAC-Bayes Analysis**
  - **Why needed here:** The analysis of FGTS algorithm and bounds on Z_n utilize PAC-Bayesian style bounds.
  - **Quick check question:** How does a PAC-Bayes bound differ from a standard concentration inequality when analyzing the regret of a randomized algorithm like Thompson Sampling?

## Architecture Onboarding

- **Component map:** Context Input (X_t) -> Feature Map (φ) -> Sparsity Prior (p_1) -> Feel-Good Likelihood (L) -> Posterior (p_t) -> Action Selection
- **Critical path:** The construction of the prior and the sampling from the posterior. The theory guarantees low regret if you can sample from the specific posterior defined.
- **Design tradeoffs:**
  - Known vs. Unknown Sparsity: If s is known, you can tune λ optimally. If unknown, you must use a looser bound or aggregation techniques.
  - Countable vs. Uncountable: Countable requires decay conditions on features; Uncountable requires Lipschitz conditions. The architecture changes significantly between Eq 5 and Eq 7.
  - Oracle Efficiency vs. Runtime: The algorithm is "oracle-efficient" but practically intractable without approximations.
- **Failure signatures:**
  - Linear Regret: If K is very large and horizon n is small, algorithm fails to learn.
  - Approximation Error: If feature map doesn't satisfy decay/Lipschitz conditions, effective dimension d_eff explodes.
  - Slow Mixing: If using MCMC for sampling, sample complexity might degrade if Markov chain doesn't mix well.
- **First 3 experiments:**
  1. Oracle Simulation: Implement the "sequence of K-armed bandits" construction from Section 3. Verify any policy suffers regret ≈ √Ksn.
  2. Uncountable Sparsity on Synthetic Data: Set up neural bandit setting with single-layer network. Use Metropolis-Hastings sampler to implement posterior. Plot regret against n to check for √n scaling.
  3. Sensitivity Analysis (λ): Vary the "feel-good" parameter λ. Plot regret for fixed horizon. Demonstrate trade-off where too little λ explores insufficiently and too much λ destabilizes likelihood.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does a computationally efficient (polynomial time) algorithm exist that achieves the minimax optimal regret bounds for this setting?
  - **Basis in paper:** Section 5.1 asks whether there exists a computationally efficient algorithm with (nearly) matching regret bound.
  - **Why unresolved:** The statistically optimal FGTS algorithm requires sampling from posteriors for which no polynomial-time sampler is known.
  - **What evidence would resolve it:** A polynomial-time algorithm achieving regret ˜O(√Ksn) for the countable sparsity setting.

- **Open Question 2:** Can individual recovery conditions (e.g., parameter separation) improve the dependence on the number of actions K from polynomial to logarithmic?
  - **Basis in paper:** Section 5.2 suggests investigating if individual recovery conditions enable regret bounds with logarithmic (or better) dependence on the number of actions.
  - **Why unresolved:** The paper's lower bounds establish polynomial dependence on K is unavoidable in the general case, and standard compatibility conditions fail for infinite feature sets.
  - **What evidence would resolve it:** A regret analysis showing O(polylog(K)) dependence when true parameters θ^*_i are well-separated.

- **Open Question 3:** Does the margin condition enable logarithmic regret bounds for sparse nonparametric contextual bandits?
  - **Basis in paper:** Section 5.3 states it would be interesting to investigate whether the margin condition enables logarithmic regret bounds.
  - **Why unresolved:** Current results focus on minimax rates of √n. It is unknown if large gaps between optimal and sub-optimal action rewards can be exploited to achieve faster rates.
  - **What evidence would resolve it:** An algorithm achieving O(log n) regret rates under a margin condition.

## Limitations

- Computational intractability: No polynomial-time sampler exists for the specific nonparametric posterior, creating a gap between theory and practice
- Strong assumptions required: Exact sparsity, Gaussian noise, and uniform decay conditions on features may not hold in practice
- Worst-case focus: Lower bounds hold for adversarial contexts but may not reflect typical real-world settings with i.i.d. or well-conditioned contexts

## Confidence

- **High**: Lower bounds are tight and correctly derived (Sections 3 and Appendix C)
- **Medium**: The regret bounds for countable sparsity (Theorem 2) - depends on eigenvalue decay conditions that may be hard to verify
- **Medium**: The FGTS regret bounds (Theorem 1) - relies on decoupling assumption and optimal λ tuning
- **Low**: Practical applicability without approximation methods for posterior sampling

## Next Checks

1. Implement the adversarial lower bound construction (Section 3) to verify the Ω(√Ksn) regret is achievable by any algorithm
2. Test the algorithm on synthetic countable sparsity data with known eigenvalue decay to empirically verify the O(√Ksn log(d_eff n)) scaling
3. Conduct sensitivity analysis on λ parameter to identify optimal tuning ranges and verify robustness to misspecification