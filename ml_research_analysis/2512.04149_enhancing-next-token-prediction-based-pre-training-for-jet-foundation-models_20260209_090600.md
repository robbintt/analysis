---
ver: rpa2
title: Enhancing next token prediction based pre-training for jet foundation models
arxiv_id: '2512.04149'
source_url: https://arxiv.org/abs/2512.04149
tags:
- jetclass
- pre-training
- tokenized
- training
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper enhances next-token prediction pre-training for jet\
  \ foundation models by addressing two key limitations of the original OmniJet-\u03B1\
  \ approach. First, it adopts a hybrid setup where continuous feature vectors serve\
  \ as model input while token-IDs remain as pre-training targets, improving downstream\
  \ classification performance without sacrificing generative capabilities."
---

# Enhancing next token prediction based pre-training for jet foundation models

## Quick Facts
- **arXiv ID:** 2512.04149
- **Source URL:** https://arxiv.org/abs/2512.04149
- **Reference count:** 0
- **Primary result:** Hybrid continuous input with discrete targets and joint NTP+MPM pre-training significantly improves jet classification accuracy (up to 15%) while maintaining generative fidelity.

## Executive Summary
This paper addresses two key limitations in foundation models for particle physics jets: the use of discrete token-IDs as inputs for downstream classification and reliance on next-token prediction (NTP) alone for pre-training. The authors introduce a hybrid setup where continuous feature vectors serve as model input while token-IDs remain as pre-training targets, and combine NTP with masked particle modeling (MPM) in a joint pre-training strategy. These modifications yield substantially better classification performance on jet tagging tasks while preserving the model's ability to generate realistic jets across different datasets.

## Method Summary
The method involves three key innovations: (1) a hybrid pre-training setup using continuous feature vectors as input while maintaining token-IDs as NTP targets, (2) joint pre-training with both NTP and MPM objectives, and (3) using a VQ-VAE for tokenization. The backbone is an 8-layer transformer with bi-directional attention for classification and causal attention for NTP. Models are pre-trained on 100M jets from the JetClass dataset and fine-tuned on top-tagging and jet-classification tasks. The VQ-VAE encodes continuous jet features into discrete tokens, which serve as both input embeddings (via decoder) and prediction targets.

## Key Results
- Continuous feature input improves classification accuracy by up to 15% compared to token-ID input across in-distribution and out-of-distribution settings
- Joint NTP+MPM pre-training significantly outperforms pure NTP pre-training on classification tasks while maintaining strong generative capabilities
- Bi-directional attention in MPM backbone is crucial for classification performance, with MPM-Causal performing similarly to NTP
- The hybrid setup achieves better performance with fewer fine-tuning samples, particularly beneficial in small-data regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous input features for downstream classification improve performance by avoiding information loss inherent in discrete tokenization.
- Mechanism: Using continuous feature vectors directly as input preserves full resolution and detail of original data. Tokenization via VQ-VAE is lossy compression, limiting classifier's access to fine-grained information. The hybrid setup decouples input representation (continuous) from pre-training target (discrete token-IDs), allowing classifier to operate on richer data.
- Core assumption: Information discarded during tokenization is relevant and valuable for downstream classification task.
- Evidence anchors: [abstract] shows hybrid setup uses continuous feature vectors as input; [Section III A, Figure 4] shows models with continuous input significantly outperform those with token-ID input.

### Mechanism 2
- Claim: Joint pre-training with MPM and NTP creates representations better suited for classification than NTP alone.
- Mechanism: NTP with causal attention excels at generation but may learn representations too strongly aligned with sequential prediction. MPM, particularly with bi-directional attention, forces model to learn contextual embeddings by predicting masked tokens based on full jet context. Combining both objectives encourages learning richer, more versatile representation capturing both sequential dependencies and global contextual features.
- Core assumption: Downstream classification benefits from representations capturing global context in bi-directional manner, not just sequential dependencies.
- Evidence anchors: [abstract] states joint NTP+MPM significantly improves classification accuracy while maintaining generative fidelity; [Section III B, Figure 5] shows joint NTP+MPM clearly outperforms standard NTP.

### Mechanism 3
- Claim: Bi-directional attention is key factor for MPM's superior classification performance over NTP, beyond masked prediction objective.
- Mechanism: Comparison between MPM (bi-directional attention) and MPM-Causal (causal attention) isolates effect of attention mechanism. Bi-directional attention allows each particle to attend to all other particles in jet, enabling more complete context-aware representation. Superior performance of MPM over MPM-Causal suggests full context is critical for classification.
- Core assumption: Jet classification is more naturally point cloud or set-based task than purely sequential one.
- Evidence anchors: [Section III B, p. 6] indicates classification performance gains seen in MPM are not solely due to masked prediction objective itself; [Section IV] concludes bi-directional attention of MPM plays major role in aligning learned representation for classification task.

## Foundational Learning

- **Vector Quantized Variational Autoencoder (VQ-VAE)**
  - Why needed here: Hybrid setup relies on VQ-VAE to tokenize continuous jet constituent data into discrete token-IDs, which serve as prediction targets for NTP and MPM. Crucial for bridging continuous detector data with discrete, language-model-style pre-training objectives.
  - Quick check question: Can you explain how a VQ-VAE maps a continuous input vector to a discrete codebook entry, and what role the decoder plays?

- **Attention Masking (Causal vs. Bi-directional)**
  - Why needed here: Paper directly compares effect of these two attention schemes. Causal attention essential for NTP objective, ensuring model can only attend to previous tokens. Bi-directional attention used in MPM backbone, which this work finds is key for learning representations for classification.
  - Quick check question: If you have sequence of tokens A, B, C, and D, which tokens can token C attend to under causal mask? Under bi-directional mask?

- **Pre-training Objectives: NTP vs. Masked Modeling**
  - Why needed here: Central experiment compares NTP (predicting next item in sequence, good for generation) and Masked Modeling (predicting missing items from context, good for representation learning). Paper argues NTP pre-training alone produces representations sub-optimally aligned for classification.
  - Quick check question: What is fundamental input-target relationship for model trained with Next Token Prediction objective?

## Architecture Onboarding

- **Component map:**
  1. Tokenizer (VQ-VAE): Pre-trained model. Encoder generates token-IDs from continuous jet features during pre-training only. Decoder converts predicted token-IDs back to continuous features during generative fine-tuning and inference.
  2. Backbone: Transformer-based encoder. Takes sequence of feature vectors as input. Operates with causal attention (for NTP) or bi-directional attention (for MPM and downstream classification). Conditioned on number of particles.
  3. Model Heads (Pre-training): NTP Head consists of transformer blocks followed by linear layer to project to vocabulary logits. MPM Head takes backbone output, reinserts mask embeddings at masked positions, uses transformer blocks + linear layer to predict token-IDs of masked particles.
  4. Classification Head: Used for downstream tasks. Takes backbone's output and uses class-attention blocks to produce jet-level class prediction.

- **Critical path:**
  1. Pre-train VQ-VAE on continuous jet data to learn discrete codebook and encoder/decoder mappings.
  2. Pre-train backbone on large dataset using joint NTP+MPM loss. Input is pseudo-continuous features from VQ-VAE decoder; targets are token-IDs.
  3. Fine-tune for Classification: Initialize backbone with pre-trained weights. Replace pre-training heads with classification head. Use full-resolution continuous features as input. Use bi-directional attention in backbone. Train on labeled dataset.

- **Design tradeoffs:**
  - Continuous vs. Token-ID Input: Continuous input preserves information and improves classification but requires more complex hybrid setup for generation. Token-ID input is simpler but loses information and harms classification performance.
  - Causal vs. Bi-directional Attention: Causal required for NTP and generation. Bi-directional better for representation learning and classification. Joint pre-training strategy allows using both.
  - Joint vs. Single-Objective Pre-training: Joint pre-training improves classification performance over NTP alone but doubles forward pass count per training step, increasing computational cost.

- **Failure signatures:**
  - Poor Generative Performance: If VQ-VAE reconstruction is poor or if NTP loss plateaus at high value.
  - No Classification Gain from Pre-training: If fine-tuned models perform similarly or worse than training from scratch. Paper attributes this to NTP representations being poorly aligned with classification tasks.
  - Fixed Backbone Fails: If attempting to fine-tune with frozen backbone yields poor results, indicating pre-trained representations are not transferable.

- **First 3 experiments:**
  1. Reproduce Input Type Ablation: Train two small classifiers on jet tagging task—one using token-IDs as input and one using continuous features. Verify that continuous-input model achieves higher accuracy, confirming core problem paper addresses.
  2. Compare Pre-training Objectives: Pre-train three small backbone models: (a) NTP only, (b) MPM only, (c) joint NTP+MPM. Fine-tune each on downstream classification task and compare accuracy curves to reproduce Figure 5a.
  3. Verify Attention Mechanism Role: Implement MPM-Causal (causal attention instead of bi-directional) and test on classification task to confirm attention mechanism, not just masked prediction objective, drives performance gains.

## Open Questions the Paper Calls Out

- **Can alternative tokenization schemes outperform the VQ-VAE approach used in this study, particularly as the number of input features increases?**
  - Basis in paper: Authors state future work could investigate whether other tokenization schemes could yield performances surpassing the VQ-VAE — especially as the number of features is further increased.
  - Why unresolved: Current study relied exclusively on VQ-VAE for tokenization, leaving comparative efficiency of other quantization methods unexplored.
  - What evidence would resolve it: Comparative benchmark showing downstream classification accuracy and generative fidelity for different tokenizers on extended feature sets.

- **Is it possible to achieve high-fidelity jet generation while fully avoiding discrete tokens?**
  - Basis in paper: Conclusion suggests it would be valuable to explore whether generation could even be achieved while fully avoiding discrete tokens.
  - Why unresolved: Current architecture uses hybrid setup where targets remain discrete token-IDs to ensure generative performance; continuous-only generation was not implemented or tested.
  - What evidence would resolve it: Model trained with continuous targets that generates realistic jet distributions comparable to token-based model.

- **How does integration of explicit physical priors affect performance of proposed hybrid NTP/MPM foundation model?**
  - Basis in paper: Authors propose hybrid ideas presented could also point towards approaches that include physical priors more explicitly.
  - Why unresolved: Current implementation utilizes plain transformer architecture without physics-inspired interaction features or Lorentz-equivariance.
  - What evidence would resolve it: Performance metrics from model utilizing hybrid pre-training strategy augmented with symmetry-preserving layers.

## Limitations

- **Data processing details unspecified:** Paper omits explicit specifications for jet clustering parameters beyond R=0.8, particle feature normalization ranges, and handling of empty jet constituents, which materially affect tokenization quality and downstream classification accuracy.

- **Implementation fidelity challenges:** Hybrid continuous-input/token-ID-target setup requires careful orchestration between VQ-VAE encoder/decoder, backbone, and different attention mechanisms. Without access to codebase, reproducing exact interaction between components presents significant challenges.

- **Scaling regime validity uncertain:** Results demonstrated primarily in small-data regime (≤1.2M jets). Claimed advantages of hybrid setup and joint pre-training may not scale linearly or could exhibit diminishing returns with larger datasets typical of industrial applications.

## Confidence

- **High confidence** in core empirical finding that continuous feature inputs outperform token-ID inputs for classification tasks, demonstrated through controlled ablation studies with statistically significant accuracy improvements (up to 15%) across multiple datasets and training sizes.

- **Medium confidence** in specific architectural choices (LayerScale initialization=1.0, 8 transformer blocks, 128-dim embedding) as optimal, since paper provides limited ablation studies on these hyperparameters and doesn't explore design space comprehensively.

- **Low confidence** in claimed computational efficiency of hybrid approach, as paper doesn't provide detailed timing or memory usage comparisons between pure NTP and joint pre-training setup, despite acknowledging doubled forward pass count.

## Next Checks

1. **Verify attention mechanism impact:** Implement and test MPM-Causal (bi-directional attention removed from MPM backbone) on classification task to confirm attention mechanism, not just masked prediction objective, drives performance gains.

2. **Stress test hybrid setup:** Systematically vary VQ-VAE codebook size (8192→32768) and masking rate (40%→20%/60%) to identify breaking points where hybrid approach loses classification advantage.

3. **Out-of-distribution robustness:** Evaluate pre-trained models on jet datasets with significantly different pT ranges or η distributions than JetClass to assess whether improvements generalize beyond training distribution.