---
ver: rpa2
title: "Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse:\
  \ Insights from Spanish-English and Spanish-Guaran\xED"
arxiv_id: '2512.03334'
source_url: https://arxiv.org/abs/2512.03334
tags:
- language
- topic
- bilingual
- discourse
- code-switching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents a large language model (LLM)-assisted annotation\
  \ pipeline for topic and sociolinguistic analysis of code-switched bilingual discourse.\
  \ It applies the approach to Spanish-English and Spanish-Guaran\xED datasets, automatically\
  \ labeling topics, genres, and discourse functions across 3,691 code-switched sentences."
---

# Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní

## Quick Facts
- arXiv ID: 2512.03334
- Source URL: https://arxiv.org/abs/2512.03334
- Reference count: 0
- LLM-assisted annotation pipeline for code-switched discourse achieves up to 100% accuracy on primary labels

## Executive Summary
This paper presents an LLM-assisted annotation pipeline for analyzing topics and sociolinguistic variation in code-switched bilingual discourse. The method applies GPT-4.1 to automatically label topics, genres, and discourse-pragmatic functions across 3,691 code-switched sentences from Spanish-English and Spanish-Guaraní corpora. By integrating demographic metadata and employing structured prompting with explicit category schemas, the pipeline achieves high annotation accuracy while revealing systematic links between sociolinguistic variables and discourse patterns. The results demonstrate that LLMs can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, with the Miami corpus showing clear associations between gender, language dominance, and discourse function, and the Paraguayan data revealing diglossic patterns.

## Method Summary
The pipeline uses GPT-4.1 via OpenAI API with deterministic parameters (temp=0, max_tokens=200) to annotate code-switched sentences. Structured prompts contain three coordinated components: a system prompt defining the annotator role and JSON-only output constraint, a base prompt specifying input fields and labeling rules, and instruction lists enumerating valid labels with explanatory notes. The approach processes sentences in batches of 50-100, parses JSON outputs, and normalizes labels to canonical strings. Annotation schemas are iteratively refined through exploration of 30-sample subsets followed by validation on additional sentences. The method integrates demographic metadata (speaker, age, gender, language dominance) to enable sociolinguistic analysis, with accuracy evaluated by bilingual linguists on 30 randomly sampled sentences per corpus.

## Key Results
- Pipeline achieves 100% accuracy for primary labels on Miami corpus and 94.17% accuracy across four fields on Spanish-Guaraní corpus
- Systematic gender associations found: female speakers contribute more to "Office_Logistics" and "Education_YouthOrganizations" in Miami data
- Clear diglossic division identified between formal Guaraní and informal Spanish in Paraguayan texts
- Secondary function/topic labels show lower accuracy (60% for Miami secondary_function) but capture additional discourse complexity

## Why This Works (Mechanism)

### Mechanism 1
Structured prompting with explicit category schemas enables reliable LLM annotation of code-switched discourse. The pipeline constrains the LLM's output space through coordinated prompt components while providing sufficient context for discourse-level interpretation. This approach assumes GPT-4.1 can semantically parse mixed-language input well enough to map utterances to predefined functional categories when given explicit constraints.

### Mechanism 2
Iterative schema refinement with bilingual annotators improves category coverage and label consistency. Initial categories are derived from 30-sample exploration, then refined by reviewing additional sentences to merge overlapping categories and add clarifying notes. This human-in-the-loop calibration grounds the taxonomy in actual discourse patterns, assuming a fixed taxonomy of 10-30 categories can capture the majority of discourse functions in code-switched speech.

### Mechanism 3
Integrating demographic metadata enables recovery of sociolinguistic patterns from corpus-scale annotations. Speaker attributes are passed as context in prompts and used to segment annotated outputs post-hoc, revealing systematic associations between variables like gender and discourse function. This relies on the assumption that metadata accurately reflects speaker identities and that LLM outputs are not biased toward spurious correlations with these variables.

## Foundational Learning

- **Code-switching typology** (intra-sentential vs. inter-sentential; matrix language frame)
  - Why needed: The pipeline specifically filters for intra-sentential switches; understanding switch boundaries is prerequisite to interpreting why switches occur
  - Quick check: Can you distinguish between "I went to the tienda y compré pan" (intra-sentential) and "I went to the store. Y luego fui a casa" (inter-sentential)?

- **Sociolinguistic variables** (gender, language dominance, diglossia)
  - Why needed: The paper's core contribution links these variables to discourse function; without this grounding, results are uninterpretable
  - Quick check: In a diglossic community, which language typically dominates formal/institutional contexts vs. informal/home contexts?

- **Structured prompting for LLMs** (system/base/instruction decomposition)
  - Why needed: The pipeline's reliability hinges on prompt engineering; understanding constraint mechanisms enables reproduction and debugging
  - Quick check: Why would setting `temp=0` improve annotation consistency compared to `temp=0.7`?

## Architecture Onboarding

- **Component map**: Sentence text + metadata (speaker, age, gender, situation, lang_tag) -> Prompt constructor (system + base + instruction lists + exemplars) -> LLM inference (GPT-4.1 API) -> Output parser (JSON extraction, normalization) -> Metadata integrator (join with demographic data) -> Analysis layer (cross-tabulations, distributions)

- **Critical path**: Extract intra-sentential code-switched sentences -> Construct per-sentence prompts with metadata context -> Batch inference (50-100 sentences) -> Parse and validate JSON outputs -> Merge with metadata and compute segmented distributions

- **Design tradeoffs**: Fixed taxonomy vs. dynamic topic generation (comparability vs. flexibility); single-label vs. multi-label (precision vs. coverage); high-resource vs. low-resource focus (rich metadata vs. limited sociolinguistic depth)

- **Failure signatures**: Low JSON parse rate (prompt constraint failure); high UNKNOWN_FUNCTION/Other rates (schema coverage gaps); systematic label bias toward one language (treating code-switched text as noise); secondary function accuracy far below primary (multi-label task exceeds model's discourse resolution)

- **First 3 experiments**: 1) Ablate metadata context: Run annotation with and without speaker metadata; measure change in label distributions and accuracy. 2) Schema perturbation: Add noise categories or merge adjacent topics; evaluate whether accuracy degrades predictably. 3) Language-pair transfer: Apply Miami-derived schema to new code-switched pair (e.g., Hindi-English) with minimal adaptation; identify coverage gaps and accuracy drops.

## Open Questions the Paper Calls Out

1. How do discourse roles interact with structural switch points when integrated into a multilevel model of bilingual production? (The study isolates discourse/pragmatic functions without explicitly linking them to syntactic constraints)

2. Can incorporating measures of lexical overlap and semantic similarity successfully link discourse-level switching patterns to cognitive processes of bilingual word retrieval? (While results align with psycholinguistic theories, the study does not computationally operationalize this connection)

3. Do dynamic topic generation methods offer superior semantic coherence for code-switched discourse compared to fixed taxonomies? (The current pipeline relies on fixed schemas which may constrain discovery of fluid thematic structures)

4. To what extent can adaptive prompt optimization mitigate potential contextual biases in LLM-generated sociolinguistic labels? (The study acknowledges potential cultural bias but relies on manual verification rather than algorithmic mitigation)

## Limitations

- Annotation accuracy relies heavily on few-shot exemplars that are only partially specified, making full replication challenging
- The Spanish-Guaraní corpus has limited demographic metadata, restricting sociolinguistic analysis depth
- The study does not address potential LLM biases toward specific languages or discourse patterns that could skew interpretations
- Accuracy evaluation on 30 randomly sampled sentences per corpus may not capture systematic errors across larger datasets

## Confidence

**High Confidence**: The core methodological approach of using structured prompting with explicit category schemas is well-established and internally consistent accuracy rates support the approach.

**Medium Confidence**: Recovered sociolinguistic patterns are plausible given the corpora's contexts but require independent validation to rule out annotation artifacts.

**Low Confidence**: Generalizability of the fixed taxonomy approach across different code-switching contexts and language pairs remains uncertain, particularly for typologically distant languages.

## Next Checks

1. **Cross-validation with human annotations**: Apply the pipeline to a held-out test set of 100+ sentences per corpus with gold-standard human annotations to assess accuracy and identify systematic error patterns.

2. **Schema transferability test**: Apply the Miami-derived annotation schema to a different code-switched language pair (e.g., Hindi-English or Mandarin-English) with minimal adaptation to quantify generalizability and identify required revisions.

3. **Metadata ablation study**: Systematically evaluate metadata contribution by running the pipeline with and without metadata context, measuring changes in label distributions and sociolinguistic pattern recovery to isolate metadata's true impact.