---
ver: rpa2
title: Learning Joint Interventional Effects from Single-Variable Interventions in
  Additive Models
arxiv_id: '2506.04945'
source_url: https://arxiv.org/abs/2506.04945
tags:
- joint
- interventional
- data
- causal
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating joint interventional
  effects when only single-variable interventions and observational data are available.
  The authors present an identifiability result showing that for causal models with
  nonlinear additive outcome mechanisms, joint effects can be recovered without joint
  interventional data.
---

# Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models

## Quick Facts
- arXiv ID: 2506.04945
- Source URL: https://arxiv.org/abs/2506.04945
- Reference count: 40
- Primary result: Joint interventional effects can be recovered from single-variable interventions under nonlinear additive outcome mechanisms without requiring joint interventional data.

## Executive Summary
This paper addresses the problem of estimating joint interventional effects when only single-variable interventions and observational data are available. The authors present an identifiability result showing that for causal models with nonlinear additive outcome mechanisms, joint effects can be recovered without joint interventional data. They propose a practical estimator that decomposes causal effects into confounded and unconfounded contributions for each intervention variable. Experiments on synthetic data demonstrate that their method achieves performance comparable to models trained directly on joint interventional data, outperforming purely observational estimators.

## Method Summary
The method learns K estimator functions f̂k(a1,...,aK, Rk) where Rk indicates intervention on variable Ak. These functions are trained on observational data (all Rk=0) and single-intervention data (one Rj=1, others 0) to match their respective expectations. The combined estimator f̂ = Σk f̂k then predicts joint intervention effects by setting all Rk=1. The key insight is that under additive outcome mechanisms Y = Σfk(Ak, Ck) + U, each action's confounding bias can be identified from single-intervention data and corrected for joint predictions.

## Key Results
- Joint interventional effects are identifiable from single-variable interventions under nonlinear additive outcome mechanisms
- The proposed estimator achieves RMSE performance comparable to models trained on joint interventional data
- The method requires approximately 10× more samples than direct joint-intervention training to reach similar accuracy

## Why This Works (Mechanism)

### Mechanism 1
Joint interventional effects can be recovered from single-variable interventions under nonlinear additive outcome mechanisms. When the outcome Y = Σfk(Ak, Ck) + U, each action's contribution is separable. Intervening on Ak breaks its confounding with Ck, allowing E[fk(ak, Ck)] to be estimated at the marginal distribution p(Ck) rather than the confounded p(Ck|a1,...,aK). The estimator learns both confounded and unconfounded versions of each contribution, then selects the appropriate one based on intervention status.

### Mechanism 2
The confounded vs. unconfounded contribution difference is identifiable from single-intervention data. For each action Aj, comparing E[Y|a1,...,do(aj),...,aK] to E[Y|a1,...,aK] isolates the difference E_{p(Cj)}[fj(aj,Cj)] - E_{p(Cj|a1,...,aK)}[fj(aj,Cj)]. This difference represents the confounding bias for that action, which can then be corrected when predicting joint interventions.

### Mechanism 3
A unified estimator with intervention indicators can simultaneously fit all available data regimes and generalize to joint interventions. Define estimator functions f̂k(a1,...,aK, Rk) where Rk∈{0,1} indicates intervention on Ak. Fit the overall estimator f̂ = Σk f̂k to match observational (all Rk=0) and single-intervention (one Rj=1, others 0) expectations. The fitted estimator then predicts joint intervention (all Rk=1) by construction.

## Foundational Learning

- **Structural Causal Models (SCMs) and do-operator**: The paper uses SCMs to define interventions formally. The do(Ak) operator represents cutting incoming edges to Ak and setting its value exogenously, which is how confounding is broken. Quick check: Can you explain why E[Y|do(a)] ≠ E[Y|a] when there's unblocked confounding?

- **Identifiability in causal inference**: The main theoretical contribution is proving that joint effects are identifiable from single-intervention data under additivity. Understanding identifiability means knowing when a quantity can be uniquely determined from available data. Quick check: If two different SCMs produce identical observational and single-intervention distributions but different joint-intervention distributions, is the joint effect identifiable?

- **Additive models and GAMs**: The additivity assumption is central. Understanding that Y = Σfk(Ak, Ck) means no interaction terms exist between different action-confounder pairs. Quick check: In a marketing context, if email campaigns amplify the effect of social media ads, would the additive assumption hold?

## Architecture Onboarding

- **Component map**: Dobs + {Dk_int} -> f̂k(a1,...,aK, Rk) -> f̂ = Σk f̂k -> E[Y|do(a1,...,aK)]
- **Critical path**: 1) Collect observational data, 2) Collect single-intervention data for each action, 3) Fit combined estimator to all regimes, 4) Predict joint intervention by setting all Rk=1
- **Design tradeoffs**: Additivity vs. expressiveness (stricter additivity enables identifiability but may misrepresent synergistic interactions); Sample efficiency vs. experimental cost (method requires ~10x more samples but avoids exponentially many joint experiments); Confounder structure (pairwise confounding assumption limits applicability)
- **Failure signatures**: Systematic bias in joint predictions when true mechanism has interaction effects; High variance with limited single-intervention data; Support mismatch between observational and interventional action distributions
- **First 3 experiments**: 1) Synthetic validation: Generate SCMs with known additive structure, verify RMSE approaches topline as sample size increases; 2) Additivity violation test: Introduce controlled interaction terms, quantify resulting bias as function of interaction strength; 3) Sample ratio sweep: Fix total data budget, vary Nsint/Nobs to find optimal allocation for specific problem dimensionality

## Open Questions the Paper Calls Out

### Open Question 1
What is the minimal set of assumptions required for identifiability of joint interventional effects beyond the additive outcome mechanism constraint? The paper notes this as a direction for future work, having established identifiability under additivity but not characterizing whether weaker or alternative structural constraints could suffice.

### Open Question 2
Can Post-Nonlinear Models or Generalized Additive Models serve as intermediate function classes that preserve identifiability while allowing more expressive outcome mechanisms? These function classes are proposed as potential generalizations but their identifiability properties remain unanalyzed.

### Open Question 3
What finite sample complexity guarantees can be established for the Intervention Generalization estimator? The identifiability result only addresses the infinite data regime, leaving practical sample bounds unknown.

## Limitations

- The method relies on strict additive outcome mechanisms, which may not hold in real-world systems with interaction effects
- The identifiability result requires pairwise confounding structure, limiting applicability to cases where confounders affect only one action variable
- The method requires approximately 10× more samples than direct joint-intervention training to achieve comparable performance

## Confidence

- **High confidence**: The theoretical identifiability result under stated assumptions (additivity, pairwise confounding, support conditions)
- **Medium confidence**: The empirical validation showing comparable performance to joint-intervention training
- **Low confidence**: The method's behavior under partial violations of additivity

## Next Checks

1. **Interaction effect stress test**: Generate synthetic data with varying levels of interaction terms and measure how RMSE scales with interaction strength
2. **Real-world pilot study**: Apply the method to a semi-synthetic dataset from a real domain where ground truth joint effects are known through simulation, testing both additive and interactive scenarios
3. **Sample efficiency comparison**: Systematically vary total sample budget while keeping Nobs + Nsint constant, measuring the optimal Nsint/Nobs ratio for different K values to validate Figure 2b's implications