---
ver: rpa2
title: 'Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box
  Framework'
arxiv_id: '2505.18864'
source_url: https://arxiv.org/abs/2505.18864
tags:
- audio
- adversarial
- speech
- jailbreak
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first white-box adversarial attack on\
  \ speech inputs to aligned multimodal large language models, specifically targeting\
  \ SpeechGPT. The method leverages access to the model\u2019s speech tokenization\
  \ to perform a greedy search over discrete audio tokens, synthesizing adversarial\
  \ audio that bypasses safety alignments and elicits harmful outputs."
---

# Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework

## Quick Facts
- arXiv ID: 2505.18864
- Source URL: https://arxiv.org/abs/2505.18864
- Reference count: 36
- Key outcome: First white-box adversarial attack on speech inputs to SpeechGPT, achieving up to 89% success rate across six prohibited task categories

## Executive Summary
This paper presents the first white-box adversarial attack targeting SpeechGPT through speech inputs. The attack leverages access to the model's speech tokenization pipeline to perform a greedy search over discrete audio tokens, synthesizing adversarial audio that bypasses safety alignments and elicits harmful outputs. The method achieves up to 89% success rate across six prohibited task categories, outperforming existing voice-based jailbreak baselines. The results demonstrate that speech interfaces introduce unique vulnerabilities that require specialized defenses tailored to audio-based adversarial threats.

## Method Summary
The attack uses a three-stage pipeline: (1) Discrete token extraction using HuBERT on harmful audio prompts, (2) Greedy search over adversarial tokens to optimize for harmful responses, and (3) Audio reconstruction via HiFi-GAN vocoder with noise perturbation to match target token clusters. The method operates entirely in discrete token space, iteratively substituting candidate tokens and selecting those that minimize loss toward predefined harmful target responses. The attack achieves high success rates while maintaining audio quality through semantic content preservation.

## Key Results
- Achieves up to 89% attack success rate across six prohibited categories
- Outperforms existing voice jailbreak baselines (0.75 and 0.30 ASR) by leveraging white-box token access
- Semantic adversarial audio yields slightly higher ASR than pure noise while maintaining better quality
- Demonstrates vulnerability gap between text-based and audio-token alignment mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy search over discrete audio tokens can discover adversarial sequences that bypass safety alignment when the attacker has access to the tokenization pipeline and can observe scalar loss values.
- Mechanism: The attack iteratively substitutes candidate tokens at each position in an adversarial suffix, selects the token that minimizes loss toward a harmful target response, and repeats until the model produces jailbreak outputs.
- Core assumption: The loss signal provides sufficient signal for gradient-free optimization to find adversarial token combinations that alignment mechanisms do not recognize as harmful.
- Evidence anchors: [abstract] "we introduce a novel token-level attack that leverages access to the model's speech tokenization to generate adversarial token sequences"; [Section III.B] "At each step, for every token position in the adversarial token sequence, we sample a set of candidate tokens, substitute each into the sequence, and compute the loss between the model's output and a predefined target response"; [corpus] Related work achieved lower ASR without token-level optimization access.

### Mechanism 2
- Claim: Appending adversarial tokens to preserved harmful speech tokens maintains semantic content while exploiting the model's token-level processing to bypass alignment.
- Mechanism: Original harmful audio is tokenized but left unmodified. A short adversarial token sequence is appended and optimized. This preserves prosody and intelligibility while the suffix exploits the model's sequential processing to override refusal behavior.
- Core assumption: The model's alignment mechanisms do not robustly propagate safety constraints across token boundaries, allowing suffix tokens to redirect the output trajectory.
- Evidence anchors: [Section III.A] "In our method, we tokenize malicious audio using HuBERT and append a short, randomized sequence of adversarial tokens. Since the original tokens remain unchanged, the natural prosody of the audio is preserved"; [Section IV.B] "semantically meaningful adversarial audio yields slightly higher ASR, suggesting linguistic content enhances jailbreak effectiveness".

### Mechanism 3
- Claim: Vocoder-based audio reconstruction with noise optimization ensures synthesized audio maps back to target adversarial tokens when re-tokenized.
- Mechanism: After greedy search identifies target tokens, a vocoder generates initial audio. Noise perturbation is then optimized via gradient descent to minimize the distance between re-tokenized audio clusters and the target token sequence, ensuring the attack survives the tokenization round-trip.
- Core assumption: The vocoder and clustering model (HuBERT) are sufficiently differentiable or locally smooth that noise optimization can reliably match target clusters.
- Evidence anchors: [Section III.C] "A global noise perturbation is then applied to the synthesized audio. The perturbed audio is passed through a feature extractor and clustering model (e.g., HuBERT) to produce a new cluster token sequence"; [Algorithm 2] explicitly describes "Cluster-Matching Noise Optimization with Vocoder Synthesis"; [Figure 4] shows reverse loss drops sharply beyond noise budget 0.04.

## Foundational Learning

- Concept: **Discrete audio tokenization (HuBERT)**
  - Why needed here: The entire attack operates in discrete token space. Understanding how HuBERT clusters audio features into discrete units is essential to grasp why token-level manipulation is possible.
  - Quick check question: Given a 1-second audio clip, approximately how many discrete tokens would HuBERT produce, and what does each token represent?

- Concept: **Greedy search in discrete spaces**
  - Why needed here: The attack cannot use gradients (internal parameters inaccessible). Understanding coordinate descent-style optimization over discrete vocabularies clarifies why this succeeds despite limited signal.
  - Quick check question: If the token vocabulary has 1000 units and the adversarial suffix is 200 tokens, what is the naive search space size, and how does greedy search reduce this?

- Concept: **Safety alignment in multimodal models**
  - Why needed here: The attack exploits a hypothesized gap between text-based alignment and audio-token alignment. Understanding where alignment is applied (training data, RLHF, inference filters) clarifies the vulnerability surface.
  - Quick check question: Does SpeechGPT apply alignment training on audio tokens directly, on transcribed text, or both? How would each choice affect attack surface?

## Architecture Onboarding

- Component map:
Raw Audio → [HuBERT Discrete Unit Extractor] → Token Sequence → [Concat with Random Tokens] → [Greedy Token Search] → Target Token Sequence → [HiFi-GAN Vocoder] → Preliminary Audio → [Noise Optimization Loop] → Final Adversarial Audio → SpeechGPT

- Critical path:
  1. Tokenization fidelity: If HuBERT tokenization loses information critical to the attack, greedy search operates on insufficient signal.
  2. Loss observability: The attack requires scalar loss values from the model; if loss is obscured or quantized, optimization degrades.
  3. Reconstruction accuracy: The noise optimization must achieve near-exact cluster matching; Figure 4 shows this stabilizes around noise budget 0.04+.

- Design tradeoffs:
  - **Adversarial token length vs. audio quality**: Longer suffixes provide more optimization flexibility but increase perceptible distortion. Paper uses 200 tokens (Table IV).
  - **Noise budget vs. stealth**: Higher budgets (0.1) achieve 93%+ ASR but presumably reduce audio naturalness.
  - **Semantic content vs. pure noise**: Semantic audio achieves marginally higher ASR (0.89 vs 0.83) with better NISQA scores, suggesting a quality-effectiveness coupling.

- Failure signatures:
  - ASR drops on "Privacy Violence" (0.90) vs "Illegal Activity" (0.95) may indicate task-specific alignment strength differences.
  - Table IV shows privacy violations require the most iterations (419.6 avg), suggesting stronger alignment or longer target responses.
  - Overly specific target responses increase failure rates per Section IV.B discussion.

- First 3 experiments:
  1. **Baseline reproduction**: Implement the greedy search on SpeechGPT with the 10-question subset per category from ForbiddenQuestionSet. Verify ASR within ±5% of reported values (0.89 avg). Log iteration counts and loss trajectories.
  2. **Ablation on adversarial suffix length**: Test suffix lengths of 50, 100, 200, 400 tokens. Hypothesis: ASR increases with length but plateaus; reconstruction loss may increase at extreme lengths.
  3. **Transferability probe**: Apply adversarial audio generated for SpeechGPT to another audio-capable MLLM (e.g., GPT-4o audio mode if accessible, or SALMONN). Measure ASR drop to assess cross-model vulnerability.

## Open Questions the Paper Calls Out

- Can adversarial audio attacks transfer effectively to other voice-enabled MLLMs beyond SpeechGPT (e.g., GPT-4o, Gemini) in black-box settings without access to tokenization pipelines?
- What defense mechanisms specifically tailored to the audio modality can effectively mitigate token-level adversarial attacks while preserving speech intelligibility?
- Can adversarial audio remain effective when played through real-world acoustic environments (speakers, room acoustics, background noise) rather than direct digital input?
- How can the audio fidelity of adversarial examples be improved while maintaining attack effectiveness, given that global token clustering currently requires noise across entire sequences?

## Limitations
- White-box access dependency: Attack relies on detailed knowledge of SpeechGPT's tokenization and loss computation, limiting real-world applicability
- Token-level loss access requirement: Attack requires continuous access to scalar loss values, which may be unavailable in practical scenarios
- Limited generalizability: Attack only evaluated on SpeechGPT; transferability to other models remains untested

## Confidence
- **High Confidence**: ASR results across 6 categories (0.83-0.95) and NISQA scores showing semantic audio maintains quality (0.71-0.75) are directly measurable and reproducible
- **Medium Confidence**: Greedy search mechanism's effectiveness relies on assumed loss signal quality and token space structure
- **Low Confidence**: Generalizability claim to other audio MLLMs is unsupported by transferability testing

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary the number of candidate tokens k (e.g., 20, 50, 100, 200) and adversarial suffix length n (50, 100, 200, 400) to determine optimal settings and identify performance plateaus or degradation points.

2. **Cross-Model Transferability**: Test adversarial audio generated for SpeechGPT against GPT-4o's audio mode or SALMONN. Measure ASR drop and analyze failure patterns to quantify real-world vulnerability.

3. **Robustness to Alignment Improvements**: Apply the same attack methodology to a version of SpeechGPT with enhanced audio-specific safety training (e.g., incorporating audio-token alignment data). Compare ASR to baseline to measure attack resilience.