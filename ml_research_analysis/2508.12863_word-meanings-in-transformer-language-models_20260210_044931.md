---
ver: rpa2
title: Word Meanings in Transformer Language Models
arxiv_id: '2508.12863'
source_url: https://arxiv.org/abs/2508.12863
tags:
- clusters
- word
- information
- semantic
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether static embeddings in transformer
  language models contain semantic information. The authors cluster the 768-dimensional
  static embeddings of RoBERTa-base (50,265 tokens) into 200 groups and analyze their
  sensitivity to five psycholinguistic measures: valence, concreteness, iconicity,
  taboo, and age of acquisition.'
---

# Word Meanings in Transformer Language Models

## Quick Facts
- arXiv ID: 2508.12863
- Source URL: https://arxiv.org/abs/2508.12863
- Reference count: 6
- The paper finds that static embeddings in RoBERTa-base encode rich semantic information, with 73 of 200 clusters showing sensitivity to psycholinguistic attributes like concreteness and age of acquisition.

## Executive Summary
This paper investigates whether static embeddings in transformer language models contain semantic information by clustering the 768-dimensional embeddings of RoBERTa-base tokens and testing their sensitivity to five psycholinguistic measures. The authors manually inspected clusters and found many organized by semantic themes such as colors, medical terms, and occupations. Statistical analysis revealed that 73 clusters were sensitive to at least one psycholinguistic attribute, with concreteness and age of acquisition affecting the most clusters. These findings challenge "meaning eliminativist" hypotheses by demonstrating that static embeddings serve as a lexical store containing rich semantic information rather than merely functioning as placeholders.

## Method Summary
The study extracted the token embedding matrix from RoBERTa-base, filtered to 50,265 meaningful tokens, and applied k-means clustering with k=200. Each cluster was manually inspected to identify semantic themes, and the distribution of five psycholinguistic attributes (concreteness, valence, iconicity, taboo, and age of acquisition) was computed for each cluster. The authors tested whether these distributions differed significantly from random sampling using log-probability comparisons, with clusters showing P<0.05 considered sensitive to an attribute.

## Key Results
- 73 out of 200 clusters showed sensitivity to at least one psycholinguistic attribute
- Concreteness was the most sensitive attribute (60 clusters), followed by age of acquisition (36 clusters)
- Manual inspection revealed semantically coherent clusters including body parts, medical terms, family relations, and negative/positive sentiment words
- The findings refute "meaning eliminativist" hypotheses about how LLMs process meaning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Static embeddings encode semantic information at the vocabulary level, serving as a "lexical store" rather than merely functioning as placeholders.
- Mechanism: During pretraining, the embedding matrix learns associations between tokens and semantic properties through gradient updates that optimize next-token prediction across billions of contexts. Semantic regularities in the training distribution become encoded as geometric structure in the embedding space.
- Core assumption: The 768-dimensional embedding space has sufficient capacity to encode multiple orthogonal semantic features per token without catastrophic interference.
- Evidence anchors:
  - [abstract] "there is a wide variety of semantic information encoded within the token embedding space"
  - [section 4] Table 1 documents 67 clusters sensitive to semantic information including taxonomic relations (cluster 197: body parts), thematic relations (cluster 76: medical terms), sentiment (clusters 172, 175: negative/positive terms), and register (clusters 40, 95)
  - [corpus] Related work "Vector Arithmetic in Concept and Token Subspaces" confirms transformers maintain separate representations for semantic vs. surface-level information
- Break condition: If alternative architectures (e.g., character-level models without static embeddings) show equivalent semantic sensitivity, the mechanism would not be specific to the embedding matrix.

### Mechanism 2
- Claim: K-means clustering on the static embedding space surfaces semantically coherent groupings because the embedding geometry reflects semantic similarity.
- Mechanism: K-means partitions the 768-dimensional space into 200 Voronoi cells by minimizing within-cluster variance. If semantic information is encoded, semantically similar tokens will occupy proximate regions and fall into common clusters.
- Core assumption: Semantic similarity manifests as Euclidean proximity in the embedding space (or at least correlates with it).
- Evidence anchors:
  - [abstract] "We extracted the token embedding space of RoBERTa-base and k-means clustered it into 200 clusters"
  - [section 4] Cluster 199 contains "animals, birds, cats, dogs, dog"; Cluster 101 contains "mother, father, dad, parents, mom"; Cluster 197 contains "legs, knee, neck, thigh, muscles"
  - [corpus] No direct corpus validation of k-means as optimal clustering method for this purpose; alternative clustering approaches (EM algorithm, hierarchical clustering) mentioned but not tested in paper
- Break condition: If different clustering algorithms or distance metrics produce incompatible semantic groupings, the mechanism may be sensitive to methodological choice rather than reflecting intrinsic structure.

### Mechanism 3
- Claim: Psycholinguistic attributes (concreteness, valence, AoA, taboo, iconicity) correlate with the statistical distribution of tokens across embedding clusters.
- Mechanism: Clusters that are semantically homogeneous should show non-random distributions of attribute values when tested against a null hypothesis of random sampling from the population distribution. The paper computes log P(C|pcat) to detect significant deviations.
- Core assumption: Human psycholinguistic ratings from small participant samples validly represent semantic properties that should be encoded in models trained on different data distributions.
- Evidence anchors:
  - [section 5.3] Concreteness: 60 sensitive clusters (highest); Valence: 27; AoA: 36; Iconicity: 9; Taboo: 6 (though some with sampling errors)
  - [section 5.3] "The null hypothesis was disconfirmed for 6 clusters" for taboo attribute
  - [section 5.4] Authors caution that correlation does not prove the cluster is "about" that attribute—e.g., cluster 76 labeled "medical terms" is sensitive to taboo due to words like "cancer," "tumour"
- Break condition: If clusters are sensitive to attributes only through correlated surface features (e.g., syllable count correlating with iconicity), the mechanism does not prove direct semantic encoding.

## Foundational Learning

- Concept: **Static vs. Contextualized Embeddings**
  - Why needed here: The entire paper hinges on distinguishing the invariant token embedding (assigned once per vocabulary item) from the contextualized embedding (output of self-attention). Confusing these would make the research question incoherent.
  - Quick check question: In RoBERTa-base, which representation changes when the word "bank" appears in "river bank" vs. "bank account"—the static embedding, the contextualized embedding, or both?

- Concept: **Null Hypothesis Significance Testing with Multinomial Distributions**
  - Why needed here: Study 2's methodology relies on computing the log-probability of observed cluster attribute distributions under a null hypothesis that tokens were randomly sampled. Understanding this is necessary to interpret the figures and results.
  - Quick check question: If cluster 172 has 340 words with valence counts C = {4, 117, 149, 52, 12, 6, 0, 0}, what does log P(C|pcat) = -354.667 tell you compared to the null distribution?

- Concept: **Taxonomic vs. Thematic Semantic Relations**
  - Why needed here: The paper uses Lin & Murphy's (2001) distinction to interpret cluster contents. Taxonomic relations involve shared category membership (e.g., body parts); thematic relations involve functional or situational associations (e.g., medical terms). This affects how you interpret "semantic" sensitivity.
  - Quick check question: Would a cluster containing "stethoscope, hospital, doctor, nurse" reflect taxonomic or thematic organization?

## Architecture Onboarding

- Component map: RoBERTa-base Vocabulary (50,265 tokens) -> Embedding Matrix E ∈ ℝ^(50265 × 768) -> Self-Attention Layers (12 layers × 12 heads) -> Contextualized Embeddings

- Critical path:
  1. Extract embedding matrix from HuggingFace `transformers` library (`model.get_input_embeddings()`)
  2. Filter vocabulary to meaningful tokens (exclude special symbols, sub-word fragments)
  3. Run k-means with k=200 on 768-dimensional vectors
  4. For each cluster, compute attribute distributions and test against null
  5. Manually inspect cluster centroids and top-N nearest tokens

- Design tradeoffs:
  - **k=200 clusters**: Authors chose this without justification. Lower k merges distinct semantic categories; higher k creates sparse clusters with sampling issues (45 clusters had ≤50 words; 25 had ≤10).
  - **Duplicate token handling**: "dog," " dog," "Dog" have separate embeddings. Authors conservatively assigned attribute values to only one variant per word to avoid inflating results. Alternative would be to analyze all variants separately.
  - **Attribute list coverage**: Taboo had only 1,085 annotated tokens vs. 37,058 for concreteness, causing sampling errors in small clusters.

- Failure signatures:
  - Clusters with ≤5 annotated tokens for an attribute produce unreliable p-values (e.g., cluster 149 with only 5 AoA values)
  - Clusters appearing sensitive to an attribute but actually sensitive to a confound (e.g., iconicity correlating with syllable count)
  - Clusters that are sensitive to taboo due to 1-2 extreme outliers rather than coherent semantic organization

- First 3 experiments:
  1. **Reproduce the clustering with different random seeds**: Run k-means 10 times with different initializations. Report variation in which clusters show semantic coherence. This tests whether findings are robust to clustering instability.
  2. **Test alternative values of k**: Run k=100, k=300, k=500. Compare the number of clusters showing semantic sensitivity. If findings are robust, the general conclusion should hold across reasonable k values.
  3. **Cross-validate with held-out attribute lists**: Split each psycholinguistic word list into train/test halves. Verify that clusters identified as sensitive on the train split also show non-random distributions on the test split. This guards against overfitting to noise in the attribute ratings.

## Open Questions the Paper Calls Out
None

## Limitations
- K-means clustering may not optimally partition semantic space, and the choice of k=200 lacks principled justification
- Small clusters (25 with ≤10 words) have unreliable attribute statistics due to sampling errors, particularly for taboo
- Correlation with psycholinguistic attributes doesn't prove semantic encoding—clusters may be sensitive due to correlated surface features
- Manual inspection of 200 clusters introduces subjective bias in interpreting semantic coherence
- Findings may not generalize beyond RoBERTa-base to other architectures or training regimes

## Confidence
- **High confidence**: Static embeddings encode *some* semantic information (cluster 197: body parts, cluster 101: family relations)
- **Medium confidence**: Semantic sensitivity varies meaningfully across psycholinguistic attributes
- **Low confidence**: K-means clustering optimally surfaces semantic structure

## Next Checks
1. **Cluster stability validation**: Run k-means 10 times with different random seeds on the same RoBERTa-base embeddings. Report the Jaccard similarity between cluster assignments across runs and test whether the same clusters consistently show semantic sensitivity.

2. **Cross-architecture generalization**: Extract and cluster static embeddings from BERT-base, GPT-2, and ALBERT. Test whether clusters sensitive to concreteness and age of acquisition in RoBERTa also show similar sensitivity patterns in these models.

3. **Controlled confounding test**: For clusters identified as sensitive to iconicity, test whether they remain sensitive after controlling for syllable count and word frequency. If sensitivity disappears after controlling for these confounds, it would suggest the mechanism detects surface-level patterns rather than semantic properties.