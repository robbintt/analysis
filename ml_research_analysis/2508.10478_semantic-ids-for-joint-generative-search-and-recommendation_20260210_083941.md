---
ver: rpa2
title: Semantic IDs for Joint Generative Search and Recommendation
arxiv_id: '2508.10478'
source_url: https://arxiv.org/abs/2508.10478
tags:
- search
- recommendation
- semantic
- generative
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how to construct Semantic IDs that perform
  well for both search and recommendation in a unified generative model. Traditional
  task-specific embeddings yield strong results in isolation but underperform in a
  joint setting.
---

# Semantic IDs for Joint Generative Search and Recommendation

## Quick Facts
- arXiv ID: 2508.10478
- Source URL: https://arxiv.org/abs/2508.10478
- Reference count: 30
- Primary result: Jointly fine-tuned bi-encoders with RQ-KMeans quantization achieve balanced search (R@30=0.046) and recommendation (R@30=0.049) performance in a unified generative model.

## Executive Summary
This paper addresses the challenge of building unified generative models for both search and recommendation tasks using Semantic IDs. Traditional approaches use task-specific embeddings that perform well in isolation but underperform when combined. The authors propose using a bi-encoder model jointly fine-tuned on both search and recommendation tasks to generate item embeddings, which are then discretized into Semantic IDs using RQ-KMeans quantization. This cross-task approach provides a better balance between search and recommendation effectiveness compared to task-specific methods, with the Multi-task variant achieving R@30 scores of 0.046 for search and 0.049 for recommendation. The findings suggest that shared, generalizable representations can streamline generative model design without sacrificing quality.

## Method Summary
The method involves training a bi-encoder model jointly on search (query-item pairs) and recommendation (item-item co-occurrence pairs) data using contrastive losses. The resulting embeddings are discretized into Semantic IDs using RQ-KMeans quantization with 2 codebooks of size 256 each. These IDs are added to the vocabulary of a flan-t5-base generative model, which is fine-tuned to generate Semantic IDs in response to task-specific prompts. The system uses diversified beam search decoding with beam size 60 and 30 groups to generate diverse results.

## Key Results
- Multi-task bi-encoder with RQ-KMeans achieves balanced performance: Search R@30=0.046, Rec R@30=0.049
- Task-specific approaches show inverse relationship: Search-based yields 0.072/0.026 while Rec-based yields 0.004/0.062
- RQ-KMeans outperforms RQ-VAE (0.002/0.024) and standard K-Means for Semantic ID construction
- Separate task-specific IDs (1024 tokens) underperform shared IDs (512 tokens), suggesting regularization benefits

## Why This Works (Mechanism)

### Mechanism 1
Jointly fine-tuning the embedding model on both search and recommendation signals creates a shared latent space that mitigates the task-specific trade-off. A bi-encoder is trained using the sum of two contrastive losses: one on query-item pairs (search) and one on co-occurring item pairs (recommendation). This forces the model to learn item representations that satisfy both content-based relevance and collaborative similarity simultaneously. The assumption is that gradients from the two distinct tasks do not conflict destructively but converge to a useful intersection of features.

### Mechanism 2
Residual Quantization (RQ-KMeans) effectively preserves the semantic nuance required for generative retrieval better than VAE-based approaches. RQ-KMeans discretizes the continuous embeddings by iteratively quantizing the residuals (error) from the previous step. This hierarchical approach allows the resulting Semantic IDs to represent fine-grained details necessary for distinguishing similar items in a joint space. The assumption is that the residual errors capture meaningful semantic variance rather than just noise.

### Mechanism 3
A unified Semantic ID space enables better generalization (regularization) in a joint generative model than maintaining separate ID tokens for each task. By sharing the vocabulary of Semantic IDs, the generative model (LLM) is forced to use the same item representations regardless of the task prompt. This parameter sharing acts as a regularizer, preventing the model from overfitting to task-specific artifacts and leveraging shared semantic knowledge. The assumption is that the input prompt (e.g., "Search:" vs "Rec:") is sufficient to disambiguate the task while the output tokens (IDs) remain shared.

## Foundational Learning

- **Concept: Bi-Encoders and Contrastive Learning**
  - **Why needed here:** This is the method used to generate the embeddings. Understanding how query-item pairs and item-item pairs are mapped to the same vector space is crucial to grasping how "Semantic IDs" are derived.
  - **Quick check question:** How does the loss function change when we switch from a search-only bi-encoder to the multi-task bi-encoder described in the paper?

- **Concept: Generative Retrieval (Retrieval-as-Generation)**
  - **Why needed here:** The paper moves from "retrieving an ID" (matching) to "generating an ID" (decoding). You must understand that the LLM predicts tokens that *constitute* the item identifier rather than a text summary.
  - **Quick check question:** In this architecture, does the model output the item's title or a specific sequence of discrete semantic tokens?

- **Concept: Residual Quantization (RQ)**
  - **Why needed here:** The Semantic IDs are created via RQ-KMeans. Without this, one might assume standard clustering (K-Means) or hashing, which are less precise for the high-dimensional spaces discussed.
  - **Quick check question:** Why does the paper prefer RQ-KMeans over standard K-Means or RQ-VAE for creating these specific IDs?

## Architecture Onboarding

- **Component map:** MovieLens25M + Synthetic Queries (Gemini) -> Bi-Encoder (Sentence Transformer all-mpnet-base-v2, jointly fine-tuned) -> RQ-KMeans Quantization (2 codebooks, size 256) -> Semantic IDs (512 tokens) -> flan-t5-base (3 epochs, LR 0.002) -> Diversified Beam Search (beam 60, groups 30)

- **Critical path:** The **Multi-task Bi-encoder training** (Section 3.2) is the most critical step. If this embedding space is biased toward one task, no amount of quantization optimization fixes the joint performance. You must verify the loss balance here first.

- **Design tradeoffs:**
  - **Separate vs. Shared IDs:** "Separate" IDs (task-specific tokens) doubled the vocabulary (1024 tokens) but failed to outperform "Shared" IDs (512 tokens), suggesting efficiency and regularization are preferred over task-specific parameter expansion.
  - **Fusion Strategy:** "Multi-task" (training one model on mixed data) outperformed "Fused" (combining independent model outputs), likely due to better alignment in the latent space.

- **Failure signatures:**
  - **Task Collapse:** High Search R@30 (e.g., 0.07) but near-zero Rec R@30 (e.g., 0.004). This indicates the embeddings are "Search-based" (Table 1, row 2) and have not learned collaborative signals.
  - **Cold Start Failure:** If the model performs well on "Head" items but fails on "Torso" items (Table 2), the semantic ID construction may be relying too heavily on ID frequency rather than content.

- **First 3 experiments:**
  1. **Reproduce the Trade-off:** Run the "Search-based" vs. "Rec-based" baselines to confirm the inverse relationship between Search R@30 and Rec R@30 on your specific data slice.
  2. **Validate Multi-task Alignment:** Train the proposed Multi-task Bi-encoder. Plot the distribution of embeddings for a sample of itemsâ€”check if search-relevant and rec-relevant items cluster closer together compared to the task-specific baselines.
  3. **Tokenizer Ablation:** Swap RQ-KMeans for a simpler method (like standard K-Means) on the Multi-task embeddings to verify the authors' claim that the quantization method is a decisive factor (specifically testing the drop in performance).

## Open Questions the Paper Calls Out

- **Open Question 1:** Why does RQ-KMeans consistently outperform learned auto-encoder approaches (RQ-VAE, ResidualLFQ) for Semantic ID tokenization in joint search and recommendation tasks?
- **Open Question 2:** How robust are cross-task Semantic ID construction methods when search and recommendation data exhibit different popularity distributions?
- **Open Question 3:** Can methods like Matryoshka representation learning create equal-dimensionality embedding spaces that improve fusion strategies over SVD-based dimension reduction?
- **Open Question 4:** How do joint Semantic ID schemes perform in cold-start scenarios with truly unseen items?

## Limitations
- The multi-task bi-encoder training details are underspecified, particularly batch composition ratios and epoch counts, which are critical for reproducing the exact performance balance
- The synthetic search dataset uses uniform item sampling that may not reflect real-world query distributions where popularity correlates with recommendation interactions
- Cold-start scenarios with truly unseen items were not evaluated despite being mentioned as a potential application of Semantic IDs

## Confidence
- **High Confidence:** The fundamental trade-off between search and recommendation performance when using task-specific embeddings
- **Medium Confidence:** The superiority of RQ-KMeans over RQ-VAE for Semantic ID construction
- **Medium Confidence:** The claim that joint fine-tuning creates better-balanced representations than task-specific approaches

## Next Checks
1. **Validate Multi-task Training Configuration:** Recreate the joint bi-encoder training with controlled batch composition ratios (e.g., 1:1, 2:1, 1:2 search:rec pairs) and verify that the resulting embeddings show balanced search/rec performance rather than collapsing to one task's objective.
2. **Quantization Method Ablation:** Train the multi-task embeddings and systematically compare RQ-KMeans against standard K-Means and RQ-VAE using identical embedding spaces to isolate the quantization method's contribution to the observed performance gains.
3. **Task Prompt Disambiguation Test:** Evaluate whether the model truly uses shared Semantic IDs by testing with ambiguous prompts (e.g., no "Search:" or "Rec:" prefix) and measuring performance degradation, which would validate or challenge the assumption that task prompts sufficiently disambiguate the shared representation.