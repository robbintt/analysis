---
ver: rpa2
title: Analyzing the Role of Context in Forecasting with Large Language Models
arxiv_id: '2501.06496'
source_url: https://arxiv.org/abs/2501.06496
tags:
- question
- news
- forecasting
- prompt
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates LLM forecasting performance on binary questions
  using a novel dataset of 614 recent Metaculus questions, augmented with news articles
  and summaries. It tests GPT-3.5-turbo, Alpaca-7B, and Llama2-13B-chat with five
  prompt variants ranging from question-only to question-plus-background-plus-news-plus-resolution-criteria-plus-few-shot-examples.
---

# Analyzing the Role of Context in Forecasting with Large Language Models

## Quick Facts
- arXiv ID: 2501.06496
- Source URL: https://arxiv.org/abs/2501.06496
- Reference count: 13
- Primary result: Incorporating news articles improves binary forecasting accuracy; few-shot examples degrade performance

## Executive Summary
This study evaluates LLM forecasting performance on binary questions using a novel dataset of 614 recent Metaculus questions, augmented with news articles and summaries. The authors test GPT-3.5-turbo, Alpaca-7B, and Llama2-13B-chat with five prompt variants ranging from question-only to question-plus-background-plus-news-plus-resolution-criteria-plus-few-shot-examples. Results show that incorporating news articles improves accuracy (68% for GPT-3.5-turbo), while adding few-shot examples reduces performance (67%). Larger models consistently outperform smaller ones. Input prompts without context lead to near-uniform "no" predictions, highlighting the importance of contextual information for effective LLM-based forecasting.

## Method Summary
The authors created a dataset of 614 binary forecasting questions from Metaculus, filtered to ensure questions were resolved after model training cutoffs. For each question, they retrieved at least three relevant news articles published at least five days before resolution, then summarized each article using GPT-3.5-turbo. They tested three LLMs (GPT-3.5-turbo, Alpaca-7B, Llama2-13B-chat) with five prompt variants: question-only, question plus background, question plus background plus news articles, question plus background plus news plus resolution criteria, and question plus background plus news plus resolution plus few-shot examples. Model outputs were parsed using delimiters to extract yes/no predictions and rationales, then evaluated using accuracy, precision, recall, and F1 metrics.

## Key Results
- GPT-3.5-turbo achieved 68% accuracy when provided news articles, outperforming question-only conditions
- Adding few-shot examples reduced accuracy across all models to 67%, attributed to prompt length
- Larger models (GPT-3.5-turbo, Llama2-13B-chat) consistently outperformed smaller Alpaca-7B
- Context-free prompts resulted in near-uniform "no" predictions, demonstrating the critical need for task-relevant information

## Why This Works (Mechanism)

### Mechanism 1: External Context Injection via News Summaries
- Claim: Adding summarized news articles to forecasting prompts improves binary prediction accuracy.
- Mechanism: News articles provide temporally relevant, question-specific evidence that extends beyond the model's training data cutoff. Summaries condense this into a format that fits context windows while preserving task-relevant signals.
- Core assumption: The retrieved news articles contain predictive signal relevant to the question outcome.
- Evidence anchors:
  - [abstract]: "incorporating news articles significantly improves performance"
  - [section 5]: "The third prompt (which includes background information and news article) achieves the best score, comparable to the forecasts made with prompt four"
  - [corpus]: Limited direct replication; PROPHET (arxiv 2504.01509) addresses LLM forecasting with causal estimation but does not specifically validate news augmentation effects.
- Break condition: News articles are irrelevant to the question, contain systematic bias, or are published too close to resolution date (information leakage).

### Mechanism 2: Context Overload from Few-Shot Examples
- Claim: Adding few-shot examples to forecasting prompts reduces accuracy across all tested models.
- Mechanism: Including complete examples (question + context + outcome) substantially increases prompt length. This may exceed the model's effective context utilization capacity, diluting attention to the target question.
- Core assumption: Performance degradation is driven by prompt length/complexity rather than example quality.
- Evidence anchors:
  - [abstract]: "using few-shot examples leads to a decline in accuracy"
  - [section 5]: "We observe a decreasing success rate across all three LLMs when adding few-shot examples... This decrease may result from the input prompts becoming quite large"
  - [corpus]: No direct corpus evidence found for this specific degradation pattern in forecasting.
- Break condition: Models with larger effective context windows, or carefully curated minimal examples, may avoid this penalty.

### Mechanism 3: Default Negation Bias Under Uncertainty
- Claim: Without contextual information, LLMs produce near-uniform "no" predictions regardless of question content.
- Mechanism: Absence of task-relevant context signals high uncertainty. Models may default to negative predictions as a conservative response, reflecting either learned priors or an inability to engage with the forecasting task meaningfully.
- Core assumption: The bias stems from lack of context rather than inherent question structure.
- Evidence anchors:
  - [abstract]: "Input prompts without context lead to near-uniform 'no' predictions"
  - [section 5, Figure 1]: "when provided with only the question as input, GPT-3.5-turbo and Llama2-13B-chat forecast almost all questions as 'no'. As the context in the input prompt increases, the ratio of 'no' forecasts decreases"
  - [corpus]: "Future Is Unevenly Distributed" (arxiv 2511.18394) confirms forecasting ability varies with prompt framing and question structure.
- Break condition: Models fine-tuned specifically for forecasting, or those with different training distributions, may exhibit different default behaviors.

## Foundational Learning

- **Binary Classification Evaluation Metrics**
  - Why needed: The paper reports accuracy, precision, recall, and F1 across conditions. Understanding how these metrics interact—especially with imbalanced classes (223 "yes" vs. 391 "no" in the dataset)—is essential for interpreting results.
  - Quick check question: If a model predicts "no" for all 614 questions, what would its accuracy be? (Answer: ~64%, which explains the high baseline for context-free prompts.)

- **Training Data Cutoff and Temporal Contamination**
  - Why needed: Valid forecasting experiments require that questions reference events after the model's knowledge cutoff. The paper explicitly filters questions starting after October 2021 for GPT-3.5-turbo.
  - Quick check question: Why would testing GPT-3.5-turbo on a question resolved in March 2022 still be valid even though its cutoff was September 2021?

- **Context Window Management**
  - Why needed: The paper summarizes news articles specifically to prevent "LLMs from being overwhelmed by too much context." Understanding token budgets and effective context utilization is critical for prompt design.
  - Quick check question: Given average context lengths (Background: 155 words, News: 97 words each, Resolution: 86 words), estimate the total tokens for the Q,B,NA,R prompt.

## Architecture Onboarding

- **Component map:**
  - Metaculus questions -> Background info/Resolution criteria extraction
  - Google News query -> Newspaper3k scraping -> ≥3 articles per question
  - GPT-3.5-turbo summarization -> Question-targeted summaries
  - Progressive prompt assembly: Q -> Q,B -> Q,B,NA -> Q,B,NA,R -> Q,B,NA,R,FS
  - LLM inference -> Structured response parsing
  - Resolved outcomes -> Accuracy/precision/recall/F1 evaluation

- **Critical path:**
  1. Validate question dates are post-cutoff for each model
  2. Ensure news article publish date ≥5 days before resolution (no leakage)
  3. Confirm at least 3 articles per question (dataset inclusion criterion)
  4. Parse structured LLM output correctly

- **Design tradeoffs:**
  - More news articles -> more signal but longer prompts
  - Detailed summaries -> better context but token budget pressure
  - Few-shot examples -> format guidance but observed accuracy penalty
  - Larger models -> better performance but higher inference cost

- **Failure signatures:**
  - >95% "no" predictions -> missing or malformed context in prompt
  - Accuracy drops with additional context -> potential context window overflow
  - High variance across question categories -> domain-specific knowledge gaps
  - Inconsistent parsing failures -> delimiter issues in prompt template

- **First 3 experiments:**
  1. **Reproduce baseline bias:** Run Q-only prompts on 50 questions to confirm near-uniform "no" predictions.
  2. **Quantify context contribution:** Compare Q vs. Q,B vs. Q,B,NA to isolate marginal gains from background and news.
  3. **Validate few-shot penalty:** Test Q,B,NA,R with and without few-shot examples to confirm the observed degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does expanding the dataset to incorporate sequences of future events affect LLM capabilities in constructing multiple future timelines?
- Basis in paper: [explicit] The conclusion states the plan to "expand our dataset to incorporate sequences of future events... to extend the task to the construction of multiple future timelines."
- Why unresolved: The current study is restricted to forecasting single, isolated binary events rather than generating coherent chains or timelines of related events.
- What evidence would resolve it: A follow-up study evaluating LLM performance on temporal sequence generation tasks using the proposed expanded dataset structure.

### Open Question 2
- Question: To what extent can alternative prompt engineering strategies improve upon the forecasting accuracy achieved by the context-variant prompts tested?
- Basis in paper: [explicit] The conclusion suggests that "exploring different prompts and prompt engineering methods could improve our results."
- Why unresolved: The authors only tested five specific prompt variants based on context availability (e.g., adding news or few-shot examples) and did not explore techniques like Chain-of-Thought or specific formatting optimizations.
- What evidence would resolve it: Comparative benchmarks of different prompting methodologies (e.g., CoT, Self-Consistency) applied to the same Metaculus dataset used in the paper.

### Open Question 3
- Question: How does the presence of diverse biases in automatically retrieved news articles influence the calibration and accuracy of LLM forecasts?
- Basis in paper: [inferred] The limitations section notes that news articles were "automatically searched... without manual analysis" and might "contain diverse kinds of biases."
- Why unresolved: While the study measures the aggregate improvement from adding news, it does not isolate or quantify how specific biases in the source text (e.g., political leaning, recency bias) impact the model's reasoning.
- What evidence would resolve it: An analysis correlating the bias metrics of the source articles with the error rates or "yes/no" distribution of the model's forecasts.

## Limitations

- Dataset accessibility: The custom dataset of 614 Metaculus questions with news article summaries requires direct author contact for reproduction
- Implementation specificity: News article retrieval methodology lacks precise query construction and summarization details
- Evaluation scope: Focuses solely on binary classification accuracy without exploring confidence calibration or probabilistic outputs

## Confidence

**High Confidence:** The observed default "no" bias when context is absent, and the consistent performance improvements from adding news articles. These findings align with established LLM behavior patterns and are supported by direct evidence in the results.

**Medium Confidence:** The specific accuracy numbers and model comparisons, as they depend on the exact dataset composition and prompt implementations. The few-shot example degradation effect is plausible but requires careful verification given the limited corpus evidence.

**Low Confidence:** The generalizability of findings beyond binary questions to multi-class or continuous forecasting tasks, as the study's scope is deliberately restricted to binary outcomes.

## Next Checks

1. **Validate context dependency:** Reproduce the baseline context-free condition on a subset of questions to confirm the near-uniform "no" prediction pattern. Compare against GPT-4 or Claude to test whether this bias is model-specific.

2. **Isolate news article contribution:** Run controlled experiments comparing Q,B vs. Q,B,NA prompts to quantify the marginal gain from news summaries versus background information alone. Test with varying numbers of articles (1, 3, 5) to identify optimal context length.

3. **Test few-shot example boundary:** Systematically vary the number and content of few-shot examples in prompts to determine if the performance degradation is due to prompt length, example quality, or both. Compare against alternative prompt engineering approaches like chain-of-thought reasoning.