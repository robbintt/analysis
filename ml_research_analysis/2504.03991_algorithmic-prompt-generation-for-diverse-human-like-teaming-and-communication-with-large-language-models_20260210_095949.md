---
ver: rpa2
title: Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication
  with Large Language Models
arxiv_id: '2504.03991'
source_url: https://arxiv.org/abs/2504.03991
tags:
- agents
- prompt
- communication
- human
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating diverse, human-like
  teaming behaviors in collaborative multi-agent environments without requiring large-scale
  human data collection. The proposed PLAN-QD framework combines Quality Diversity
  (QD) optimization with LLM-powered agents to algorithmically generate prompts that
  elicit diverse team behaviors in a cooking game domain (Steakhouse).
---

# Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models

## Quick Facts
- arXiv ID: 2504.03991
- Source URL: https://arxiv.org/abs/2504.03991
- Reference count: 40
- Key outcome: PLAN-QD framework generates diverse human-like teaming behaviors without human data, achieving 15-38% higher coverage than random generation

## Executive Summary
This paper addresses the challenge of generating diverse, human-like teaming behaviors in collaborative multi-agent environments without requiring large-scale human data collection. The proposed PLAN-QD framework combines Quality Diversity (QD) optimization with LLM-powered agents to algorithmically generate prompts that elicit diverse team behaviors in a cooking game domain (Steakhouse). PLAN-QD uses a mutator LLM to iteratively improve prompts based on measure functions, creating an archive of diverse high-performing agents.

The approach was validated through a human-subjects study (n=54 participants) showing that humans exhibit diverse coordination and communication behaviors. PLAN-QD successfully replicated these behavioral trends, with agents matching human communication effects in 12/16 layout-metric combinations. The framework achieved significantly higher coverage (15-38% improvement) compared to random prompt generation, capturing both explicitly targeted behaviors and additional diverse behaviors. These results demonstrate PLAN-QD's effectiveness in generating diverse LLM agents that can model human teaming dynamics and serve as a tool for studying human-AI collaboration.

## Method Summary
PLAN-QD combines Quality Diversity optimization with LLM-powered agents to generate diverse human-like teaming behaviors. The framework iteratively mutates and evaluates prompts using a mutator LLM, guided by measure functions that capture different behavioral dimensions. It maintains an archive of diverse high-performing prompts, exploring the space of possible team behaviors. The approach was validated in a cooking game environment (Steakhouse) where agents must collaborate to prepare dishes, comparing generated behaviors against human study data from 54 participants.

## Key Results
- PLAN-QD achieved 15-38% higher coverage of diverse behaviors compared to random prompt generation
- Agents matched human communication effects in 12 out of 16 layout-metric combinations
- The framework successfully captured both explicitly targeted behaviors and additional diverse behaviors not directly optimized for

## Why This Works (Mechanism)
PLAN-QD leverages Quality Diversity optimization to systematically explore the space of possible LLM behaviors while maintaining performance standards. By using measure functions to guide diversity and a mutator LLM to iteratively improve prompts, the framework can discover novel behavioral combinations that human designers might not anticipate. The iterative refinement process allows the system to progressively build an archive of diverse, high-performing prompts that capture the richness of human teaming dynamics.

## Foundational Learning
- **Quality Diversity (QD) optimization**: Needed to systematically explore diverse behaviors while maintaining performance standards; quick check: verify archive contains both diverse and high-performing solutions
- **Prompt engineering for LLMs**: Essential for controlling agent behavior; quick check: ensure prompts effectively guide desired actions
- **Measure functions for behavior characterization**: Required to quantify and guide diversity; quick check: validate measures capture meaningful behavioral differences
- **Multi-agent coordination**: Critical for realistic teaming scenarios; quick check: verify agents can effectively collaborate
- **Human behavior analysis**: Needed as ground truth for diversity; quick check: ensure human study captures representative behaviors

## Architecture Onboarding

Component map: Human study data -> Measure functions -> PLAN-QD framework (mutator LLM + archive) -> Diverse agents -> Performance evaluation

Critical path: Prompt generation -> Agent execution -> Behavior measurement -> Archive update -> Prompt mutation

Design tradeoffs: 
- Balance between diversity and performance (quality diversity optimization)
- Manual vs. automated measure function design
- Exploration vs. exploitation in prompt space

Failure signatures:
- Archive dominated by similar behaviors (insufficient diversity)
- Low-performing agents in archive (poor quality maintenance)
- Failure to match human behavioral patterns (inadequate measure functions)

First experiments:
1. Validate basic agent performance in cooking game without diversity optimization
2. Test measure functions' ability to distinguish between different behaviors
3. Run PLAN-QD with minimal iterations to verify prompt mutation works

## Open Questions the Paper Calls Out
None

## Limitations
- Validated only in a single cooking game domain (Steakhouse), limiting generalizability
- Human study sample size of 54 participants may not capture full spectrum of teaming diversity
- Manually designed measure functions could introduce bias and may miss important behavioral dimensions

## Confidence
- **High Confidence**: Framework's ability to generate diverse behaviors beyond random prompt generation (15-38% coverage improvement)
- **Medium Confidence**: Replication of human communication effects in 12/16 layout-metric combinations
- **Medium Confidence**: Claim that PLAN-QD can serve as a tool for studying human-AI collaboration

## Next Checks
1. Validate PLAN-QD's performance across multiple collaborative domains beyond cooking games to assess generalizability
2. Conduct ablation studies to determine which measure functions most effectively capture diverse human behaviors
3. Test the framework's ability to generate novel behaviors not observed in the original human study to evaluate its creative potential