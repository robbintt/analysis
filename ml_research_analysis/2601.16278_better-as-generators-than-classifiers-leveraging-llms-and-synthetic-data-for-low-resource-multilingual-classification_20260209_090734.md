---
ver: rpa2
title: 'Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data
  for Low-Resource Multilingual Classification'
arxiv_id: '2601.16278'
source_url: https://arxiv.org/abs/2601.16278
tags:
- samples
- synthetic
- language
- languages
- smaller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) are
  more effective as generators of synthetic data rather than classifiers in low-resource
  multilingual settings. The authors use a state-of-the-art multilingual LLM to generate
  synthetic datasets across 11 languages and 4 classification tasks, then train smaller
  models using these samples through fine-tuning, instruction tuning, or as in-context
  examples.
---

# Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification

## Quick Facts
- arXiv ID: 2601.16278
- Source URL: https://arxiv.org/abs/2601.16278
- Reference count: 31
- Primary result: Small models trained on synthetic data outperform large generator LLMs on low-resource multilingual classification tasks

## Executive Summary
This paper investigates whether large language models are more effective as generators of synthetic data rather than as direct classifiers in low-resource multilingual settings. Using LLaMA-3 70B to generate synthetic datasets across 11 languages and 4 classification tasks, the authors train smaller models via fine-tuning, instruction tuning, or in-context learning. Their experiments demonstrate that small amounts of synthetic data (as few as 20-100 samples depending on language group) enable smaller models to outperform the large generator itself, particularly for low-resource languages and underrepresented tasks.

## Method Summary
The study uses LLaMA-3 70B-Instruct to generate 200 synthetic samples per class per language, guided by 10 human examples per label in the target language. The synthetic data is filtered through self-revision and used to train smaller models: XLM-RoBERTa Large for fine-tuning, LLaMA-3.1-8B/Gemma-3-4B/Qwen-2.5-7B for in-context learning or LoRA instruction tuning. Each configuration is evaluated over 20 random subsampling runs (10-2000 samples) measuring accuracy against held-out test sets, compared to the generator's zero-shot performance.

## Key Results
- Small models trained on as few as 50 synthetic samples outperform the large generator LLM across all language groups
- Performance gains are largest for low-resource languages (13-18% improvement) versus high-resource (5-11% improvement)
- Fine-tuning offers best sample efficiency while instruction tuning provides highest peak performance
- Synthetic data benefits stagnate compared to human data beyond 150-200 samples due to lower diversity

## Why This Works (Mechanism)

### Mechanism 1
Few-shot guided synthetic sample generation produces training data that captures task-relevant patterns even for low-resource languages. The generator LLM receives 10 human examples per label in the target language, which grounds generation in task semantics while leveraging the model's multilingual capabilities. Self-revision filtering removes low-quality outputs (e.g., wrong-language samples). Core assumption: The generator LLM has sufficient multilingual knowledge to produce coherent, label-appropriate samples in low-resource languages given minimal exemplars. Break condition: When target language is severely underrepresented in generator's pretraining data, sample quality degrades.

### Mechanism 2
Data-driven distillation via synthetic samples transfers task knowledge from large generator to smaller specialized models more efficiently than direct prompting. Synthetic samples act as an intermediate representation—the large model externalizes its task understanding into labeled examples, which smaller models then learn from through gradient updates (fine-tuning) or instruction-tuning, rather than relying on frozen in-context learning. Core assumption: The synthetic samples preserve sufficient signal about decision boundaries that gradient-based learning can exploit better than the generator's own inference-time behavior. Break condition: For overrepresented tasks (e.g., sentiment classification in English), synthetic samples provide limited benefit because fine-tuning requires more data to outperform well-calibrated prompting.

### Mechanism 3
Performance gains scale inversely with resource level—low-resource languages and underrepresented tasks show largest improvements. High-resource languages are overrepresented in pretraining, so generator LLMs already perform well via zero-shot. Low-resource settings have more room for improvement, and synthetic data compensates for missing training signal. Core assumption: The generator's multilingual representations generalize sufficiently to produce useful synthetic samples even when its own classification performance is suboptimal. Break condition: Hyperparameter sensitivity increases in low-resource settings; the paper notes "tendency of the fine-tuning models to quickly overfit on the synthetic samples."

## Foundational Learning

- **Knowledge Distillation vs. Direct Transfer**
  - Why needed here: The paper frames synthetic data generation as a form of distillation—understanding how this differs from traditional logit-based distillation clarifies why it works for classification.
  - Quick check question: Can you explain why generating labeled samples and training a student model differs from having the student match the teacher's output probabilities?

- **In-Context Learning vs. Fine-Tuning Trade-offs**
  - Why needed here: The paper compares three approaches (fine-tuning, ICL, instruction-tuning) with different computational costs and data efficiency profiles.
  - Quick check question: Given 50 labeled samples, would you expect ICL or fine-tuning to perform better, and why does the paper suggest fine-tuning often wins at this scale?

- **Multilingual Representation Quality Gradient**
  - Why needed here: Performance varies systematically across language resource levels; understanding why helps predict when synthetic data will help most.
  - Quick check question: Why might Azerbaijani benefit more from synthetic data generation than German, even when using the same generator model?

## Architecture Onboarding

- **Component map**: Generator LLM (LLaMA-3 70B) -> Student models (XLM-RoBERTa Large, LLaMA-3.1-8B/Gemma-3-4B/Qwen-2.5-7B) -> Evaluation pipeline

- **Critical path**:
  1. Collect 10 human examples per label (minimum viable seed)
  2. Generate 200 samples per label with temperature=0.7, self-revision filtering
  3. Subsample to target size (10-2000 samples)
  4. Train student model (fine-tuning or instruction-tuning) OR use for ICL
  5. Evaluate against held-out test set; compare to generator's zero-shot performance

- **Design tradeoffs**:
  - Fine-tuning: Lowest compute cost, highest hyperparameter sensitivity, best sample efficiency
  - Instruction-tuning: Highest compute (~10x fine-tuning), best peak performance, more stable
  - ICL with synthetic examples: No training cost, but limited improvement ceiling especially for smaller LLMs

- **Failure signatures**:
  - Overfitting on synthetic data: Performance plateaus or degrades beyond 150-200 samples
  - Generator-student mismatch: Gemma/Qwen benefit less from LLaMA-generated samples than LLaMA-3.1-8B does
  - Task overrepresentation: Sentiment classification shows minimal synthetic data benefit for high-resource languages

- **First 3 experiments**:
  1. Replicate single-language pilot: Generate 200 synthetic samples for one low-resource language (e.g., Slovenian) on intent classification; fine-tune XLM-RoBERTa with 50 samples; compare to LLaMA-70B zero-shot.
  2. Ablate seed example count: Test generation quality with 0, 5, 10, 20 human examples to validate the 10-example design choice.
  3. Measure diversity gap: Compare intra-class embedding variance between synthetic and human samples to quantify the "lower diversity" observation from Section 4.3.

## Open Questions the Paper Calls Out

### Open Question 1
Can sophisticated generation strategies focused on diversity prevent the performance stagnation observed when training with larger volumes of synthetic data? The authors observed that synthetic data benefits stagnate compared to human data as sample size increases, attributing this to the "lower diversity and informativeness of the generated samples," but they did not implement specific diversity incentives.

### Open Question 2
Does architectural alignment between the generator and student models improve the efficacy of synthetic data distillation? The authors note that smaller models (Gemma, Qwen) benefited less than LLaMA-3-8B, speculating that "samples generated by the large LLaMA model may represent a different distribution for the Gemma and Qwen models."

### Open Question 3
Can a specific hyperparameter or regularization protocol be established to mitigate the overfitting tendency of models trained on synthetic data? While the authors identified overfitting as a limiting factor, they relied on a general hyperparameter optimization search rather than proposing a specific regime for synthetic data.

## Limitations
- Synthetic data quality ceiling: samples are only comparable to human data in severely low-resource settings
- Hyperparameter sensitivity: fine-tuning shows high sensitivity with multiple failed runs
- Task representation bias: findings may not generalize beyond the four studied classification tasks

## Confidence
- **High Confidence**: LLMs perform better as synthetic data generators than as direct classifiers for low-resource languages; small models trained on synthetic data can outperform the generator LLM itself in low-resource settings; performance gains scale inversely with resource level
- **Medium Confidence**: Synthetic samples are comparable to human data only in severely resource-constrained settings; fine-tuning provides best sample efficiency while instruction-tuning offers highest peak performance; generator-student model compatibility affects synthetic data effectiveness
- **Low Confidence**: Specific mechanisms driving quality differences between synthetic and human samples; optimal hyperparameters for fine-tuning across all sample sizes and language groups; generalization of findings to task types beyond the four studied classification problems

## Next Checks
1. **Quantify Synthetic Data Quality Gaps**: Systematically measure and compare embedding variance, perplexity, and lexical diversity between synthetic and human samples across resource levels.
2. **Ablation Study on Human Seed Examples**: Test generation quality with varying numbers of human examples (0, 5, 10, 20) to precisely quantify the minimum viable seed requirement.
3. **Cross-Generator Compatibility Test**: Generate synthetic data using multiple different LLMs and test whether the observed pattern of smaller models outperforming their generators holds consistently across generator-student pairs.