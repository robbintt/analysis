---
ver: rpa2
title: A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards
  in Large Language Model Reasoning
arxiv_id: '2602.01523'
source_url: https://arxiv.org/abs/2602.01523
tags:
- budget
- relative
- learning
- reasoning
- regime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the sample complexity of reinforcement learning\
  \ with verifiable rewards (RLVR) for large language models, unifying compute constraints\
  \ and task difficulty into a relative budget framework. The key insight is that\
  \ the ratio of token budget H to expected time-to-solution \xB5x (relative budget\
  \ \u03BE=H/\xB5x) determines three distinct learning regimes: deficient (\u03BE\u2192\
  0, sample complexity explodes), balanced (\u03BE=\u0398(1), maximal sample efficiency),\
  \ and ample (\u03BE\u2192\u221E, diminishing returns)."
---

# A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2602.01523
- Source URL: https://arxiv.org/abs/2602.01523
- Reference count: 40
- Key outcome: This paper analyzes the sample complexity of reinforcement learning with verifiable rewards (RLVR) for large language models, unifying compute constraints and task difficulty into a relative budget framework. The key insight is that the ratio of token budget H to expected time-to-solution μx (relative budget ξ=H/μx) determines three distinct learning regimes: deficient (ξ→0, sample complexity explodes), balanced (ξ=Θ(1), maximal sample efficiency), and ample (ξ→∞, diminishing returns). The authors prove that in the balanced regime, anti-concentration remains bounded away from zero, enabling efficient learning, while in the deficient regime, the probability of discovering high-reward trajectories vanishes. For online RL, they show that the relative budget grows linearly over iterations under a gamma distribution model, requiring increasingly many samples. Empirically, they validate these predictions across models and tasks, finding that the optimal learning occurs at ξ∈[1.5,2.0], coinciding with peak reasoning performance. The work identifies the relative budget as a key factor for RLVR efficacy and provides guidance for optimal compute allocation.

## Executive Summary
This paper introduces a unified framework for analyzing reinforcement learning with verifiable rewards (RLVR) in large language models by examining the relationship between token budget and task difficulty. The authors prove that the ratio of token budget H to expected time-to-solution μx (relative budget ξ=H/μx) determines three distinct learning regimes: deficient (ξ→0), balanced (ξ=Θ(1)), and ample (ξ→∞). In the balanced regime, anti-concentration remains bounded away from zero, enabling efficient learning, while in the deficient regime, the probability of discovering high-reward trajectories vanishes. The work provides both theoretical guarantees on sample complexity and empirical validation across multiple model families and reasoning datasets, showing that optimal learning occurs at ξ∈[1.5,2.0].

## Method Summary
The authors analyze RLVR through a relative-budget framework that unifies compute constraints (maximum generation length H) and task difficulty (expected time-to-solution μx). They prove that the ratio ξ=H/μx determines three learning regimes based on anti-concentration properties. The theoretical analysis uses shaped rewards R(τ)=max{0,H-T(τ)} to enable tractable variance analysis, proving sample complexity bounds for monotonic policy improvement under χ²-divergence trust regions. Empirically, they validate predictions by generating traces across different token budgets to simulate ξ values, measuring normalized reward variance and anti-concentration coefficients. For online RL validation, they fine-tune models using GRPO with LoRA across varying maximum completion lengths to control ξ, measuring test accuracy improvements.

## Key Results
- The relative budget ξ=H/μx determines three distinct learning regimes: deficient (ξ→0), balanced (ξ=Θ(1)), and ample (ξ→∞)
- In balanced regime (ξ∈[1.5,2.0]), anti-concentration remains bounded away from zero, enabling efficient learning with linear sample complexity
- In deficient regime (ξ≪1), informative trajectories are rare and sample complexity explodes exponentially
- In ample regime (ξ≫2), variance diminishes as most traces succeed, yielding diminishing returns per sample
- Under gamma-distributed time-to-solution, the relative budget grows linearly with online RL iterations, requiring increasingly many samples for continued improvement

## Why This Works (Mechanism)

### Mechanism 1: Relative Budget Governs Anti-Concentration
- Claim: The ratio of token budget to expected solution time determines whether RL can discover informative trajectories.
- Mechanism: When ξ = H/μₓ is too small (deficient regime), the probability of sampling high-reward traces vanishes because the budget truncates most successful solutions. When ξ ≈ 1–2 (balanced regime), anti-concentration remains bounded away from zero, enabling efficient credit assignment. When ξ → ∞ (ample regime), variance diminishes as most traces succeed, yielding diminishing returns per sample.
- Core assumption: The left-tail distribution of time-to-solution follows a non-vanishing function f(z) for z > 0 (Assumption 4.3), ruling out exponential decay that would make early solutions impossible.
- Evidence anchors:
  - [abstract]: "The key insight is that the ratio of token budget H to expected time-to-solution μₓ determines three distinct learning regimes"
  - [Theorem 5.2]: "In the deficient regime (ξ→0), informative trajectories are rare and the sample complexity explodes; in the balanced regime (ξ=Θ(1)), informative trajectories occur with non-negligible probability"
  - [corpus]: Weak direct support; related work on difficulty filtering (Bae et al., Cui et al.) empirically shows optimal learning at intermediate difficulty but doesn't formalize the anti-concentration mechanism.
- Break condition: If the time-to-solution distribution has heavy tails or bimodal structure (e.g., Qwen3-4B in Appendix A), the gamma model assumptions break and ξ predictions become unreliable.

### Mechanism 2: Regime-Dependent Sample Complexity Scaling
- Claim: Sample complexity for monotonic policy improvement follows distinct scaling laws across regimes, with the balanced regime being most efficient.
- Mechanism: The sub-optimality gap scales as O(H / c₀(ξ)·n), where c₀(ξ) is the anti-concentration coefficient. In balanced regime, c₀(ξ) = Θ(1), giving linear sample efficiency. In deficient regime, c₀(ξ) = Θ(f(ξ)) → 0, causing exponential sample cost. In ample regime, variance decays as O(H/ξ), requiring more samples for same improvement signal.
- Core assumption: Homogeneity within relative budget slices—variance is approximately constant across instances with similar ξ (Assumption 6.1).
- Evidence anchors:
  - [Theorem 6.3]: Explicit sample complexity bounds: deficient = e^Ω((f(ξ))^(-3/2)), balanced = e^Ω(1/√κ), ample = e^Ω(ξ²)
  - [Section 7.2]: Empirical phase transition at ξ≈1 with accuracy gains peaking at ξ∈[1.5,2.0]
  - [corpus]: Related work on adaptive rollout allocation (arXiv:2602.01601) addresses similar efficiency concerns but through dynamic sample allocation rather than budget theory.
- Break condition: If tasks have heterogeneous structure within same ξ-slice, aggregate statistics may not predict per-task behavior.

### Mechanism 3: Linear Relative Budget Growth Under Gamma Model
- Claim: During online RL, the relative budget grows linearly with iterations, progressively shifting tasks from balanced to ample regime.
- Mechanism: As policy improves, expected time-to-solution μₓ decreases. Under gamma-distributed T(τ), Theorem 6.4 proves ξᵢ - ξ₀ ≥ i/(2K) asymptotically. This creates a trade-off: improvement per iteration decays while sample cost grows quadratically (nᵢ = e^Θ(Kξᵢ²)).
- Core assumption: Time-to-solution follows Gamma(K, p) distribution, continuous approximation valid for large token counts (Section 5.2).
- Evidence anchors:
  - [Theorem 6.4]: "ξᵢ - ξ₀ ≥ i·(2K)⁻¹" with sample complexity "nᵢ ≥ e^Θ(Kξᵢ²)"
  - [Appendix A]: Empirical fit of gamma distributions to token counts from Llama and Phi models
  - [corpus]: No direct external validation of linear growth prediction; related work focuses on static difficulty filtering rather than dynamics.
- Break condition: If token distributions deviate significantly from gamma (e.g., bimodal from data contamination), linear growth guarantees fail.

## Foundational Learning

- Concept: **Anti-concentration coefficient**
  - Why needed here: The paper's core theoretical contribution uses anti-concentration to quantify the probability of discovering reward-improving trajectories. Without this concept, the regime analysis is opaque.
  - Quick check question: Given a distribution with mean μ and variance σ², what is P(X ≥ μ + √ε·σ)? If this probability vanishes as ε→0, can gradient-based RL learn efficiently?

- Concept: **Sub-optimality gap and sample complexity**
  - Why needed here: The paper bounds how many samples RL needs to achieve near-optimal performance within a trust region. Understanding J(π*) - J(π̂) scaling is essential for interpreting theoretical results.
  - Quick check question: If sub-optimality gap scales as O(1/n), how many samples are needed to halve the gap?

- Concept: **χ²-divergence trust regions**
  - Why needed here: The paper's RL guarantees operate within χ²-constrained policy neighborhoods (Definition 3.1). This differs from KL-based trust regions common in RLHF.
  - Quick check question: How does χ²-divergence differ from KL-divergence in penalizing low-probability events?

## Architecture Onboarding

- Component map:
  - **Token budget H**: Maximum generation length per sample
  - **Expected time-to-solution μₓ**: Estimated tokens until first correct solution under base policy
  - **Relative budget ξ = H/μₓ**: Normalize budget by task difficulty
  - **Anti-concentration c₀(ξ)**: Probability of sampling trajectories with reward > mean + √κ·σ

- Critical path:
  1. Estimate μₓ per task from base policy rollouts (requires solving traces)
  2. Set H such that ξ ∈ [1.5, 2.0] for target task distribution
  3. Monitor c₀ during training—if it drops, tasks may have shifted to deficient regime
  4. Adjust H or filter tasks to maintain balanced regime

- Design tradeoffs:
  - **Higher H**: Moves tasks from deficient→balanced→ample; increases compute cost per sample; eventually yields diminishing returns
  - **Task filtering**: Removing tasks with ξ too low or too high concentrates learning signal but reduces coverage
  - **Budget forcing** (Muennighoff et al.): Appending "Wait" tokens artificially increases ξ for marginally solvable tasks

- Failure signatures:
  - Training loss plateaus despite low success rate → likely deficient regime (ξ ≪ 1)
  - High success rate but minimal accuracy improvement → likely ample regime (ξ ≫ 2)
  - Bimodal token distributions → gamma model violated, regime predictions unreliable

- First 3 experiments:
  1. **Estimate task-specific μₓ**: Sample 50–100 traces per task with large H; compute mean solution length for correct traces only. Validate gamma fit via QQ-plot.
  2. **Sweep H to find ξ range**: Train identical models with varying max_token_length; plot final accuracy vs estimated ξ. Confirm phase transition near ξ≈1 and peak at ξ∈[1.5,2.0].
  3. **Validate anti-concentration prediction**: For each ξ-slice, estimate c₀ empirically from rollout rewards; verify c₀ ≈ 0 in deficient regime and c₀ bounded away from 0 in balanced/ample regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the relative budget theory be extended to binary-reward settings (i.e., terminal 1/0 outcome supervision) rather than the continuous proxy reward used in the current analysis?
- Basis in paper: [explicit] "Extending the theory to the binary-reward setting (i.e., terminal 1/0 outcome supervision) remains a key open problem for characterizing standard reasoning pipelines and for developing a more general theory of reasoning."
- Why unresolved: The continuous proxy reward R(τ) = max{0, H−T(τ)} enables tractable variance analysis; binary rewards make the variance computation significantly more complex.
- What evidence would resolve it: Derivation of anti-concentration bounds and sample complexity for binary rewards, showing whether the three-regime structure persists.

### Open Question 2
- Question: How should the theory be modified to handle models with bimodal token distributions caused by data contamination or memorization?
- Basis in paper: [inferred] The paper notes Qwen3-4B-Instruct "exhibits a bimodal distribution...attributed to data contamination, where the model oscillates between retrieving memorized solutions and engaging in genuine reasoning. While our gamma-based theory models the latter, these exceptions would require a mixture model approach."
- Why unresolved: Single gamma distribution cannot capture the mixture of memorized retrieval and genuine reasoning behaviors.
- What evidence would resolve it: A mixture-model extension showing how bimodal distributions affect anti-concentration and optimal ξ values.

### Open Question 3
- Question: Does the three-regime structure hold under heavy-tailed time-to-solution distributions that violate the sub-Weibull assumption?
- Basis in paper: [inferred] Assumption 4.2 explicitly "rules out heavy-tailed behaviors, ensuring the standard deviation of the time-to-solution scales with its mean" via sub-Weibull tails.
- Why unresolved: Real-world reasoning may exhibit heavier tails than sub-Weibull, potentially changing how variance scales with budget.
- What evidence would resolve it: Analysis of anti-concentration under Pareto or log-normal distributions, identifying whether the balanced regime still maximizes efficiency.

## Limitations

- The theory relies critically on the gamma distribution assumption for time-to-solution, which may not hold for all model-task combinations, particularly when token distributions are bimodal or heavy-tailed.
- The empirical validation is limited to two reasoning datasets (GSM8K, MATH-500) and three model families (Llama-3.2, Phi-3, Qwen3), leaving generalization to other domains uncertain.
- The anti-concentration analysis assumes task homogeneity within ξ-slices, but real datasets likely contain heterogeneous problem structures that violate this assumption.
- The online RL analysis assumes a specific gamma-K model for time-to-solution evolution that requires further empirical validation across longer training horizons.

## Confidence

- **High confidence**: The relative budget framework itself (ξ = H/μₓ) and its definition of three distinct regimes (deficient, balanced, ample) - this is clearly defined and empirically validated across multiple experiments
- **Medium confidence**: The specific numerical predictions (optimal ξ ∈ [1.5, 2.0], linear growth rate of ξᵢ in online RL) - supported by experiments but dependent on model assumptions that could break
- **Low confidence**: The theoretical guarantees for online RL sample complexity under the gamma-K model - these require assumptions that weren't fully validated empirically

## Next Checks

1. **Validate gamma distribution assumptions**: For each model-task combination used in experiments, fit gamma distributions to solution time data and test goodness-of-fit using QQ-plots and Kolmogorov-Smirnov tests. Report cases where the gamma assumption breaks and quantify the impact on theoretical predictions.

2. **Test anti-concentration predictions across diverse tasks**: Sample problems from multiple domains (not just GSM8K/MATH) and measure empirical anti-concentration coefficients across ξ values. Verify whether c₀(ξ) remains bounded away from zero in balanced regime and vanishes in deficient regime as predicted.

3. **Validate online RL dynamics**: Track ξᵢ and sample complexity nᵢ across multiple training iterations in the online RL setup. Compare empirical growth rates against the theoretical prediction ξᵢ - ξ₀ ≥ i/(2K) and verify the quadratic scaling of sample complexity with ξ.