---
ver: rpa2
title: 'NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search'
arxiv_id: '2505.14680'
source_url: https://arxiv.org/abs/2505.14680
tags:
- user
- search
- feedback
- users
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of rebuilding the user feedback
  ecosystem in generative AI search, which currently lacks the fine-grained feedback
  mechanisms that powered traditional web search. The authors propose NExT-Search,
  a paradigm that introduces two complementary feedback modes: User Debug Mode allowing
  users to intervene at various pipeline stages (query decomposition, retrieval, generation),
  and Shadow User Mode employing a personalized agent to simulate user preferences
  when explicit feedback is unavailable.'
---

# NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search

## Quick Facts
- arXiv ID: 2505.14680
- Source URL: https://arxiv.org/abs/2505.14680
- Authors: Sunhao Dai; Wenjie Wang; Liang Pang; Jun Xu; See-Kiong Ng; Ji-Rong Wen; Tat-Seng Chua
- Reference count: 40
- Primary result: Proposes a dual-mode user feedback system for generative AI search to address the lack of fine-grained feedback mechanisms that powered traditional web search ranking improvements

## Executive Summary
This paper addresses the critical gap in generative AI search systems where the rich feedback ecosystem that powered traditional web search has been lost. Current generative AI search lacks the ability to collect fine-grained, pipeline-level feedback that would enable continuous improvement. The authors propose NExT-Search, a paradigm that introduces two complementary feedback modes to rebuild this ecosystem. The system aims to collect both explicit user interventions and simulated preferences to drive both real-time refinements and long-term model improvements, while also introducing a monetization mechanism to incentivize user participation.

## Method Summary
NExT-Search proposes a dual-mode feedback system for generative AI search. The first mode, User Debug Mode, allows users to intervene at three pipeline stages: query decomposition (adding/removing/reordering sub-queries), retrieval (annotating relevance, re-ranking, filtering passages), and generation (correcting errors, editing content, adjusting style). The second mode, Shadow User Mode, employs a personalized agent that learns user preferences and simulates feedback when explicit input is unavailable. The system leverages feedback through online adaptation for real-time refinements and offline updates for long-term model improvements via instruction fine-tuning, DPO, or RLHF. A feedback store mechanism is proposed to incentivize user participation by enabling monetization of debugging efforts.

## Key Results
- Introduces User Debug Mode allowing manual intervention at query decomposition, retrieval, and generation stages
- Proposes Shadow User Mode with personalized agents to simulate user preferences when explicit feedback is unavailable
- Presents a feedback store mechanism to incentivize user participation through monetization of debugging efforts

## Why This Works (Mechanism)
The system works by capturing fine-grained user interventions at multiple pipeline stages rather than just final output judgments. By allowing users to debug at the query, retrieval, and generation levels, the system can attribute feedback to specific failure points and make targeted improvements. The Shadow User Mode extends the feedback ecosystem by generating synthetic preferences based on learned user behavior, ensuring continuous data flow even when users don't provide explicit feedback. The dual-mode approach balances high-quality sparse feedback with abundant simulated feedback.

## Foundational Learning
- **Query decomposition**: Breaking complex queries into sub-queries; needed for handling multi-faceted questions and improving retrieval precision; quick check: can the system handle queries requiring multiple information needs?
- **Dense retrieval**: Vector-based passage retrieval using learned embeddings; needed for finding relevant documents beyond keyword matching; quick check: does retrieval return passages semantically related to query intent?
- **LLM-based generation**: Using large language models to synthesize answers from retrieved passages; needed for creating coherent, contextual responses; quick check: does generated output accurately cite retrieved sources?
- **User preference learning**: Building personalized models of user behavior and feedback patterns; needed for Shadow User Mode to simulate realistic feedback; quick check: can the system predict user corrections with reasonable accuracy?
- **Online adaptation**: Real-time re-execution of pipeline stages based on feedback; needed for immediate system improvements; quick check: does the system respond to user interventions within acceptable latency?
- **Offline model updates**: Periodic retraining using accumulated feedback; needed for long-term capability enhancement; quick check: does model performance improve over time with more feedback data?

## Architecture Onboarding

Component Map: User Query -> Query Decomposition -> Retrieval -> Generation -> User Feedback -> Online Adaptation/Offline Update

Critical Path: The most critical path is User Query → Query Decomposition → Retrieval → Generation → User Feedback, as this represents the core generative search pipeline where user feedback is collected and applied.

Design Tradeoffs: The system trades off user interaction burden against feedback quality. User Debug Mode provides high-quality feedback but requires significant user effort, while Shadow User Mode reduces burden but may produce noisier feedback. The dual-mode approach attempts to balance these competing concerns.

Failure Signatures: Low intervention rates in User Debug Mode suggest the interface is too complex or the cognitive burden is too high. Poor alignment between Shadow User Mode feedback and explicit user corrections indicates preference learning failure. Lack of improvement in retrieval or generation metrics despite feedback collection suggests the feedback integration mechanism is ineffective.

First Experiments:
1. Implement baseline RAG pipeline with query decomposition, dense retrieval, and LLM generation to establish performance baseline
2. Build User Debug Mode interfaces for all three pipeline stages and measure intervention rates and user satisfaction in a small user study
3. Implement basic Shadow User Mode using in-context learning and evaluate feedback alignment against explicit user corrections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can personalized user agents reliably simulate fine-grained user preferences from limited interaction data while preserving user privacy?
- Basis in paper: Section 5 identifies "building personalized user agents capable of reliably simulating user behavior" as a critical challenge, noting the need to balance personalization with privacy concerns given sensitive signals like profiles and engagement logs.
- Why unresolved: The paper describes what the Shadow User Mode should do but provides no implementation or evaluation of how accurately such agents can simulate preferences, especially with sparse user data.
- What evidence would resolve it: Empirical studies measuring the accuracy of AI-simulated feedback against ground-truth user feedback across varying levels of available user data, combined with privacy-preserving techniques like federated learning.

### Open Question 2
- Question: How can systems effectively integrate heterogeneous feedback sources—sparse high-quality human feedback and abundant but potentially noisy AI-assisted feedback?
- Basis in paper: Section 5 states that "integrating heterogeneous feedback sources" is a core challenge: User Debug Mode offers high-quality but sparse feedback while Shadow User Mode supplies abundant but potentially noisy signals.
- Why unresolved: The paper proposes the dual-mode framework but does not specify algorithms or training procedures for weighting, validating, or combining these qualitatively different feedback types.
- What evidence would resolve it: Comparative experiments using different fusion strategies (e.g., multi-task learning, curriculum learning, or adaptive weighting) showing improved model performance on retrieval and generation metrics.

### Open Question 3
- Question: What interface designs and interaction mechanisms can encourage users to provide fine-grained pipeline-level feedback without imposing excessive cognitive burden?
- Basis in paper: Section 5 asks: "How should intermediate steps be presented to encourage actionable feedback? What level of user intervention is appropriate across different users and tasks?"
- Why unresolved: The paper proposes rollback and debug capabilities across query decomposition, retrieval, and generation stages, but provides no user studies or interface prototypes validating usability or adoption rates.
- What evidence would resolve it: User studies with functional prototypes measuring task completion rates, subjective satisfaction, and frequency of feedback provision across different interface configurations.

## Limitations
- Lacks specific implementation details, datasets, and evaluation metrics necessary for reproducibility
- Claims about user engagement and feedback quality are speculative without empirical validation
- Monetization feedback store mechanism is vague with no details on incentive structures or technical implementation
- User preference learning approach is not specified, and the paper does not address privacy concerns or scalability challenges

## Confidence

Conceptual framework and problem identification: Medium
Technical implementation details: Low
User engagement and feedback quality claims: Low
Monetization mechanism: Low

## Next Checks
1. Develop a minimal prototype of the User Debug Mode interface and conduct a small-scale user study to measure intervention rates and user satisfaction
2. Create a synthetic dataset with query-retrieval-answer triples and simulate user feedback to test the online adaptation mechanism
3. Implement a basic Shadow User Mode using in-context learning and evaluate its ability to generate feedback aligned with explicit user corrections on a held-out test set