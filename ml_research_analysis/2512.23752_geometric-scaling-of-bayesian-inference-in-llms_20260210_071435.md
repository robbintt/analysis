---
ver: rpa2
title: Geometric Scaling of Bayesian Inference in LLMs
arxiv_id: '2512.23752'
source_url: https://arxiv.org/abs/2512.23752
tags:
- bayesian
- geometric
- value
- attention
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether the geometric substrate for Bayesian\
  \ inference\u2014low-dimensional value manifolds, orthogonal key frames, and progressive\
  \ attention focusing\u2014persists in production-scale language models. Across Pythia,\
  \ Phi-2, Llama-3, and Mistral families, the authors find that last-layer value representations\
  \ organize along a dominant entropy-aligned axis, with domain restriction collapsing\
  \ this structure into the same low-dimensional manifolds observed in synthetic settings."
---

# Geometric Scaling of Bayesian Inference in LLMs

## Quick Facts
- **arXiv ID**: 2512.23752
- **Source URL**: https://arxiv.org/abs/2512.23752
- **Reference count**: 15
- **Primary result**: Production-scale transformers preserve geometric substrates for Bayesian inference (low-dimensional value manifolds, orthogonal key frames, attention focusing) across Pythia, Phi-2, Llama-3, and Mistral families

## Executive Summary
This paper investigates whether the geometric substrate for Bayesian inference—low-dimensional value manifolds, orthogonal key frames, and progressive attention focusing—persists in production-scale language models. Across Pythia, Phi-2, Llama-3, and Mistral families, the authors find that last-layer value representations organize along a dominant entropy-aligned axis, with domain restriction collapsing this structure into the same low-dimensional manifolds observed in synthetic settings. Targeted interventions on this axis in Pythia-410M disrupt local uncertainty geometry while leaving Bayesian-like calibration largely intact, indicating the geometry is a privileged uncertainty readout rather than a singular computational bottleneck. These results show that modern transformers preserve the geometric substrate underlying Bayesian inference primitives and organize their approximate updates along this substrate, even when ground-truth posteriors are unavailable.

## Method Summary
The paper extracts final-token value vectors from the last layer of transformer models, concatenates them across attention heads, and performs PCA to analyze geometric structure. They measure PC1/PC1+PC2 variance explained (thresholds: PC1 >30% or PC1+PC2 >30%), key orthogonality (mean off-diagonal cosine <0.20), and attention entropy reduction (>30%). Domain-restricted prompts are used to test manifold collapse, and SULA task with 250 prompts evaluates calibration. The methodology involves forward-pass extraction of value vectors, key matrices, and attention distributions for stratified-entropy prompts, followed by standardization and PCA analysis.

## Key Results
- All tested models (Pythia, Phi-2, Llama-3, Mistral) show value manifolds with PC1 explaining 30-80% variance, organized along entropy-aligned axis
- Key projection matrices achieve orthogonality 2-10× better than random baselines (mean off-diagonal cosine 0.034-0.18)
- Domain restriction collapses mixed-domain manifolds toward 1D structure (PC1+PC2 increases from ~51% to ~74% in Llama-3.2-1B)
- Targeted PC1-axis ablation disrupts local uncertainty geometry but preserves SULA calibration in Pythia-410M

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Last-layer value representations form low-dimensional manifolds parameterized by predictive entropy
- Mechanism: Transformers encode posterior uncertainty geometrically—value vectors cluster along a dominant principal component (PC1) whose coordinate correlates with next-token entropy
- Core assumption: Cross-entropy loss during training sculpts value geometry to encode uncertainty structure
- Evidence anchors: PC1 coordinates correlate strongly with analytical Bayesian entropy (|ρ| = 0.65-0.80); last-layer value representations form low-dimensional manifolds parameterized by predictive entropy
- Break condition: If value vectors were high-dimensional random embeddings, PC1 would be ~5% (random baseline) rather than observed 30-80% variance explained

### Mechanism 2
- Claim: Key projection matrices develop orthogonal "hypothesis frame" directions during training
- Mechanism: Keys define separable directions in representation space corresponding to distinct hypotheses; orthogonality enables efficient hypothesis discrimination through attention
- Core assumption: Orthogonal key structure emerges from gradient descent dynamics, not architectural initialization
- Evidence anchors: All models achieve key orthogonality 2-10× better than random baselines; mean off-diagonal cosine between 0.034 and 0.18 across most layers
- Break condition: If keys were uncorrelated with hypothesis structure, mean off-diagonal cosine would match initialization baseline (~0.35-0.45)

### Mechanism 3
- Claim: Domain-restricted prompts collapse value manifolds toward one-dimensional structure
- Mechanism: When prompts span a coherent domain, the model operates in a single inference mode, isolating the Bayesian geometric substrate
- Core assumption: Domain restriction isolates a single "inference mode" rather than merely reducing lexical diversity
- Evidence anchors: Llama-3.2-1B shows PC1+PC2 increasing from 51.4% (mixed) to 73.6% (math-only); domain restriction collapses structure to same 1D regime observed in controlled settings
- Break condition: If geometric structure were task-independent, domain restriction would have no effect

## Foundational Learning

- **Predictive entropy as uncertainty measure**: The geometric framework assumes entropy is the target variable being encoded; without this, manifold structure is uninterpretable
  - Quick check: If a model's next-token distribution has entropy 2.5 bits, what does this tell you about its uncertainty over the next token?

- **PCA and explained variance**: The paper quantifies manifold collapse using PC1+PC2 variance explained; understanding dimensionality reduction is essential for interpreting geometric signatures
  - Quick check: If PC1 explains 80% of variance in value vectors, what does this imply about the effective dimensionality of the representation?

- **Attention mechanism (K, Q, V decomposition)**: Key orthogonality and value manifolds are properties of specific attention components; without understanding K/Q/V factorization, the mechanisms are opaque
  - Quick check: In a transformer attention head, what roles do keys and values play differently in computing the output?

## Architecture Onboarding

- **Component map**: Value manifolds (last-layer value projection outputs) -> Key frames (key projection matrices define hypothesis directions) -> Attention focusing (layerwise entropy reduction reflects posterior refinement) -> Entropy axis (first principal component oriented to correlate with predictive entropy)

- **Critical path**: Extract final-token value vectors from target layer → Compute PCA on standardized value batch → Measure PC1/PC2 variance explained and correlation with model entropy → Compute key matrix orthogonality (mean off-diagonal cosine) → Track attention entropy across layers for focusing signature

- **Design tradeoffs**:
  - Standard MHA (Pythia, Phi-2): Clearest geometric signatures, highest interpretability
  - GQA (Llama): 4× KV cache reduction, but 50% weaker orthogonality and 62% weaker focusing
  - Sliding-window/MoE (Mistral): Efficiency gains, but static geometry persists while dynamic focusing is disrupted
  - Training data: Curated data (Phi-2) → sharper geometry; web-scale (Llama) → reduced clarity

- **Failure signatures**:
  - No manifold structure: PC1 ≈ 5% (random baseline) indicates absent Bayesian geometry
  - No orthogonality: Mean off-diagonal cosine ≈ 0.40 indicates untrained hypothesis frames
  - Non-monotone attention entropy: Architecture constraints preventing global evidence aggregation
  - Domain-insensitive manifolds: Model operates in fixed inference mode (like Pythia-410M)

- **First 3 experiments**:
  1. Extract value manifold from your model using stratified-entropy prompts (5 domains, 15 prompts per entropy quintile). Compute PC1+PC2 and compare to 30% threshold.
  2. Test domain restriction: Compare manifold dimensionality on mixed-domain vs. single-domain prompts. Expect collapse toward 1D in restricted setting.
  3. Measure key orthogonality: Compute mean off-diagonal cosine for W_K matrices across layers. Values < 0.20 indicate learned hypothesis structure; values > 0.35 suggest initialization-level structure.

## Open Questions the Paper Calls Out

- **Does the entropy-ordered value manifold causally enable Bayesian inference, or is it merely a correlated readout of distributed uncertainty computations?**
  - Basis: Single-layer ablations selectively disrupted geometry but left SULA calibration intact, suggesting distributed representation rather than a bottleneck
  - Why unresolved: Multi-layer interventions were not performed
  - What evidence would resolve it: Multi-layer axis ablations, activation patching, or training-time interventions that selectively degrade Bayesian calibration when geometry is disrupted

- **Do new geometric phenomena emerge at frontier scale (70B-400B parameters), or do multi-lobed manifolds remain the dominant pattern under mixed-domain prompts?**
  - Basis: Study only evaluated models up to 12B dense and 8×7B MoE; scaling behavior beyond this remains untested
  - Why unresolved: Largest dense model is 12B parameters
  - What evidence would resolve it: Applying the same geometric extraction protocol to frontier-scale checkpoints and comparing manifold dimensionality and structure

- **What computational mechanisms underlie the 2D or multi-lobed manifolds observed in deeper models?**
  - Basis: Current theory predicts 1D entropy-ordered manifolds; the origin of higher-dimensional structure is not explained
  - Why unresolved: The emergence of 2D or multi-lobed manifolds in deeper or larger models is not yet theoretically understood
  - What evidence would resolve it: Controlled experiments varying training data heterogeneity, task mixture, or model depth to identify which factors induce multi-dimensional manifolds

## Limitations

- The mechanism linking cross-entropy loss to value geometry is inferred from companion work rather than directly validated
- Domain restriction results lack theoretical grounding and may simply reflect lexical narrowing rather than true inference-mode isolation
- SULA intervention shows local uncertainty disruption but leaves overall calibration intact, suggesting geometry is "privileged" rather than computationally essential

## Confidence

- **High confidence**: The empirical observations of low-dimensional value manifolds (PC1 >30% variance) and key orthogonality (mean off-diagonal cosine <0.20) across multiple model families
- **Medium confidence**: The interpretation that these geometric structures represent a "Bayesian substrate" for inference
- **Low confidence**: The domain restriction mechanism's claimed role in isolating inference modes

## Next Checks

1. **Causal intervention on manifold structure**: Systematically ablate PC1/PC2 components in value representations and measure downstream effects on predictive accuracy and uncertainty calibration across diverse tasks, not just local entropy perturbations
2. **Architectural perturbation study**: Compare geometric signatures across controlled architectural variations (attention head count, embedding dimension, layer depth) to isolate which design choices most strongly influence manifold dimensionality and key orthogonality
3. **Cross-entropy gradient analysis**: Directly measure gradients of cross-entropy loss with respect to value projections during training to verify the claimed sculpting mechanism, rather than relying on companion paper results