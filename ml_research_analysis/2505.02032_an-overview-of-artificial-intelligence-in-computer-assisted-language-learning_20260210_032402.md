---
ver: rpa2
title: An overview of artificial intelligence in computer-assisted language learning
arxiv_id: '2505.02032'
source_url: https://arxiv.org/abs/2505.02032
tags:
- language
- learning
- proceedings
- call
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys AI methods applicable to computer-assisted language
  learning (CALL), focusing on intelligent tutoring systems (ITS) structure and functionality.
  It reviews automatic exercise generation (gap-filling, multiple-choice, reading/listening
  comprehension), assessment methods (automated essay scoring, speech evaluation),
  and feedback generation.
---

# An overview of artificial intelligence in computer-assisted language learning

## Quick Facts
- arXiv ID: 2505.02032
- Source URL: https://arxiv.org/abs/2505.02032
- Reference count: 40
- Primary result: Surveys AI methods for CALL systems, focusing on ITS structure, exercise generation, assessment, and feedback mechanisms

## Executive Summary
This paper provides a comprehensive survey of artificial intelligence methods applicable to computer-assisted language learning (CALL), with particular emphasis on intelligent tutoring systems (ITS) architecture and functionality. The review covers automatic exercise generation (gap-filling, multiple-choice, reading/listening comprehension), assessment methods (automated essay scoring, speech evaluation), and feedback generation. The paper highlights how recent advances in large language models (LLMs) show promise for generating exercises and assessments, while also identifying persistent challenges regarding factual accuracy, personalization, and ethical concerns. The survey aims to guide CALL system developers and connect interdisciplinary research efforts in AI-driven language education.

## Method Summary
The paper conducts a literature survey of AI methods in CALL, drawing on 40 references across various domains including exercise generation, assessment, and feedback mechanisms. The review synthesizes approaches ranging from traditional rule-based systems to modern neural and LLM-based methods. While specific datasets and implementation details are not provided (as this is a survey), the paper references multiple learner corpora, ESL corpora, and specific datasets like Speechocean762. The methodological scope spans from simple word embedding similarity measures for distractor generation to complex neural architectures for automated essay scoring and speech assessment. The survey emphasizes the interconnection between ITS components and their role in personalizing language learning experiences.

## Key Results
- Intelligent tutoring systems personalize language learning through dynamic adaptation based on Domain, Student, Instruction, and Interface Models
- Large language models show promise for generating exercises and assessments but face challenges with factual accuracy and hallucinations
- Automated essay scoring has progressed from handcrafted features to neural networks and LLM-based approaches with high agreement to human markers
- Integration of assessment systems into existing CALL architectures remains limited despite advances in individual components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intelligent tutoring systems (ITS) can personalize language learning by decoupling the representation of domain knowledge from the learner's proficiency state.
- **Mechanism:** The system maintains a **Domain Model** (facts, rules, constructs) and a **Student Model** (dynamic proficiency state). The **Instruction Model** queries these to select tasks within the learner's Zone of Proximal Development (ZPD), theoretically maximizing learning efficiency by avoiding tasks that are too simple or too difficult.
- **Core assumption:** Learners improve faster when instruction is dynamically adapted to their current mastery level rather than following a fixed sequence.
- **Evidence anchors:**
  - [section] Section 2 explicitly defines the Domain, Student, and Instruction models and their role in simulating a "good teacher" by determining suitable tasks.
  - [abstract] Notes the "interconnection between ITS components (Domain, Student, Instruction, and Interface Models) and their role in personalizing language learning."
  - [corpus] Corpus relevance is weak; neighbors focus on AI in microbiology and quantum software engineering, not ITS architecture.
- **Break condition:** Personalization fails if the Student Model updates are inaccurate (e.g., guessing vs. mastery) or if the Domain Model lacks granular connectivity between concepts.

### Mechanism 2
- **Claim:** Large Language Models (LLMs) can function as "force multipliers" for content creation, generating reading comprehension questions and distractors for exercises.
- **Mechanism:** LLMs use zero-shot or few-shot learning to generate question-answer pairs or plausible incorrect options (distractors) based on input text. The "Personalized Cloze Test Generation" (PCGL) framework demonstrates adjusting difficulty via prompt engineering.
- **Core assumption:** The quality of LLM-generated exercises is sufficient to replace or augment human-curated content without disrupting pedagogical integrity.
- **Evidence anchors:**
  - [section] Section 3.1 references Wang et al. [73] and Shen et al. [74] using GPT-3.5/LLMs for cloze tests and the PCGL framework.
  - [abstract] States that "Recent advances in large language models (LLMs) show promise for generating exercises... but challenges remain regarding factual accuracy."
  - [corpus] Weak relevance; corpus does not cover LLMs in education specifically.
- **Break condition:** This mechanism degrades if the LLM hallucinates factual errors, produces biassed content, or generates distractors that are obviously incorrect, undermining the assessment's validity.

### Mechanism 3
- **Claim:** Automated Essay Scoring (AES) can approximate human evaluation by mapping textual features to quality scores via neural networks or LLMs.
- **Mechanism:** Deep learning models (e.g., LSTMs, Transformers, BERT) analyze linguistic features—syntax, coherence, vocabulary usage—to predict a score. Recent methods like **Rank-Then-Score** use LLMs to first order essays by quality before assigning a score.
- **Core assumption:** High correlation exists between the extracted linguistic features/semantic embeddings and the human perception of "essay quality."
- **Evidence anchors:**
  - [section] Section 3.2 details the shift from handcrafted features to deep learning and LLMs, specifically citing the "Rank-Then-Score" (RTS) framework [168] and LLM substantial agreement with human markers [167].
  - [corpus] Weak relevance; corpus papers discuss AI in other domains (e.g., microbiome research) rather than NLP assessment.
- **Break condition:** Scoring validity breaks down if the model relies on spurious correlations (e.g., essay length) or fails to detect subtle semantic errors that a human would catch.

## Foundational Learning

- **Concept: Zone of Proximal Development (ZPD)**
  - **Why needed here:** This educational theory underpins the **Instruction Model**. It defines the algorithmic goal: selecting tasks that are neither too hard (frustration) nor too easy (boredom).
  - **Quick check question:** Can you explain how an ITS determines the boundaries of a specific learner's ZPD using the Student Model?

- **Concept: Gap-filling (Cloze) vs. Multiple Choice (MCQ)**
  - **Why needed here:** These are the primary atomic units of exercise generation discussed. Understanding the difference is crucial for designing the **Domain Model** and **Assessment** logic.
  - **Quick check question:** What is the specific technical challenge in generating "distractors" for MCQs compared to simply masking a word for a gap-fill exercise?

- **Concept: Grammatical Error Detection (GED)**
  - **Why needed here:** GED is a prerequisite for **Assessment** and **Feedback**. Without accurate error detection, the system cannot update the Student Model or provide corrective feedback.
  - **Quick check question:** Why might GED be particularly difficult in spontaneous speech compared to written text (as hinted in the speech assessment section)?

## Architecture Onboarding

- **Component map:** Domain Model -> Student Model -> Instruction Model -> Interface Model
- **Critical path:**
  1. **Define Domain:** Establish the scope of linguistic constructs (e.g., "German irregular verbs")
  2. **Implement Assessment:** Build the evaluation logic (e.g., GED, speech recognition) to ground the Student Model
  3. **Loop Instruction:** Connect Assessment results to Instruction logic to serve the next Domain item

- **Design tradeoffs:**
  - **Rules vs. LLMs:** Rule-based systems (e.g., GramEx) are accurate but brittle; LLMs are flexible but prone to hallucination
  - **Implicit vs. Explicit Feedback:** Explicit correction is more effective for learning but harder to generate accurately automatically
  - **Granularity:** A highly detailed Student Model offers better personalization but requires significantly more data to calibrate

- **Failure signatures:**
  - **"Drill & Kill":** System repeats the same construct despite learner mastery (Instruction Model failing to update ZPD)
  - **Nonsense Generation:** LLM produces grammatically correct but factually wrong or nonsensical distractors
  - **Feedback Frustration:** System provides indirect feedback ("Try again") when the learner lacks the metalinguistic knowledge to self-correct

- **First 3 experiments:**
  1. **Distractor Validity Test:** Generate MCQ distractors using a base LLM vs. a WordNet-based approach; have human annotators rate "plausibility" and "distracting power"
  2. **ZPD Calibration:** Implement a simple Student Model (e.g., based on correct/incorrect streaks) and measure learner engagement (time on task) vs. a random sequencing baseline
  3. **Rank-Then-Score AES Evaluation:** Fine-tune a small LLM to rank a small corpus of essays and correlate the ranking order with human teacher scores to verify the paper's suggested "Rank-Then-Score" efficacy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can automated essay scoring (AES) systems be effectively integrated into existing CALL systems to provide real-time, formative feedback within a unified intelligent tutoring architecture?
- **Basis in paper:** [explicit] The authors state: "Despite these advancements, to the best of our knowledge, no AES system has been integrated into an existing CALL system."
- **Why unresolved:** AES research has progressed independently from CALL system development; integration requires aligning AES outputs with the Student Model updates and Instruction Model decision-making in ITS architectures.
- **What evidence would resolve it:** A working prototype demonstrating AES feedback feeding into Student Model updates and influencing subsequent exercise selection, with evaluation of learning outcomes compared to non-integrated approaches.

### Open Question 2
- **Question:** What mechanisms can mitigate factual inaccuracies and hallucinations in LLM-generated language learning exercises while preserving their ability to produce contextually rich, pedagogically appropriate content?
- **Basis in paper:** [explicit] The paper notes "concerns remain regarding the factual accuracy of generated responses" and "other ethical issues" with LLMs for exercise generation.
- **Why unresolved:** LLMs optimize for plausible text generation rather than factual correctness; current guardrails (RLHF, prompting strategies) reduce but do not eliminate hallucinations in educational content.
- **What evidence would resolve it:** Comparative studies measuring factual error rates across LLM exercise generation methods, combined with error analysis identifying systematic failure modes and successful mitigation strategies.

### Open Question 3
- **Question:** How can CALL systems dynamically adapt exercise generation and feedback strategies across the full proficiency spectrum from beginner to advanced learners?
- **Basis in paper:** [explicit] "In future work, we plan to address further facets, such as the level of learner proficiency, and contrast the approaches for beginners vs. advanced learners."
- **Why unresolved:** Most systems target narrow proficiency ranges; strategies effective for beginners (e.g., controlled vocabulary, explicit feedback) may differ substantially from those appropriate for advanced learners (e.g., nuanced error detection, implicit correction).
- **What evidence would resolve it:** Empirical studies comparing learning outcomes when the same CALL components use proficiency-specific adaptations versus uniform approaches across learner levels.

### Open Question 4
- **Question:** What approaches can effectively personalize feedback in CALL systems by accounting for individual learner differences (L1 background, educational history, learning preferences) in automated speech and writing assessment?
- **Basis in paper:** [inferred] The paper states: "most existing tools do not account for individual differences among users for feedback generation, though L2 proficiency and educational background can significantly affect how learners interpret feedback."
- **Why unresolved:** Personalization requires modeling complex learner characteristics, but current systems rely primarily on performance history rather than static learner attributes or learning style preferences.
- **What evidence would resolve it:** Ablation studies showing whether incorporating learner background variables into feedback generation improves comprehension, engagement, or learning gains compared to generic feedback.

## Limitations
- Evidence base relies heavily on cited works rather than direct empirical validation, with many claims supported only through references
- Corpus analysis shows weak topical relevance (average neighbor FMR of 0.36), suggesting the paper operates somewhat independently from broader AI research
- Survey format means methodological details are often incomplete or absent, particularly for neural approaches where training data and hyperparameters are not specified
- Acknowledges but does not fully address ethical concerns around LLM-generated content and factual accuracy issues

## Confidence
- **High:** The theoretical framework of ITS components (Domain, Student, Instruction, Interface Models) and their interconnection for personalization
- **Medium:** Claims about LLM potential for exercise generation, given documented successes but acknowledged hallucination risks
- **Low:** Specific technical implementation details for neural approaches, as these are only referenced rather than demonstrated

## Next Checks
1. **Distractor Generation Validation:** Implement word embedding-based distractor generation using fastText and compare distractor quality against human expert ratings to verify the paper's claims about embedding-based methods.

2. **ZPD Algorithm Testing:** Build a simple Student Model based on performance streaks and measure learner engagement metrics (time on task) when using ZPD-based task selection versus random sequencing.

3. **AES Correlation Study:** Replicate the Rank-Then-Score framework by fine-tuning a small LLM on a corpus of essays, ranking them, and calculating correlation with human teacher scores to validate the claimed efficacy.