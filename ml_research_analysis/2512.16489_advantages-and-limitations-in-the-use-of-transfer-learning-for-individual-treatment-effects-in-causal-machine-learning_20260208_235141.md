---
ver: rpa2
title: Advantages and limitations in the use of transfer learning for individual treatment
  effects in causal machine learning
arxiv_id: '2512.16489'
source_url: https://arxiv.org/abs/2512.16489
tags:
- target
- source
- learning
- transfer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how transfer learning improves the estimation
  of individual treatment effects (ITEs) in small-sample settings. It proposes using
  Treatment-Agnostic Representation Networks (TARNet) trained on a large source dataset,
  then fine-tuning the model on a smaller target dataset.
---

# Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning

## Quick Facts
- arXiv ID: 2512.16489
- Source URL: https://arxiv.org/abs/2512.16489
- Reference count: 9
- Primary result: Transfer learning with TARNet reduces bias and improves ITE estimation precision in small target samples when source and target datasets are sufficiently similar

## Executive Summary
This paper examines how transfer learning improves the estimation of individual treatment effects (ITEs) in small-sample settings. It proposes using Treatment-Agnostic Representation Networks (TARNet) trained on a large source dataset, then fine-tuning the model on a smaller target dataset. The approach is evaluated through simulations and an empirical study using IHDS-II data. Results show that transfer learning reduces bias and improves estimation precision in target datasets, particularly when the source and target datasets are sufficiently similar and the target sample is small.

## Method Summary
The method trains TARNet on large source data (N≥5000), freezes encoder layers, and fine-tunes only final layers on target data. This provides regularized initialization that reduces variance without requiring the target sample to learn representations from scratch. The integral probability metric (IPM), specifically 1-Wasserstein distance, measures distributional distance between treated and control representations to align them for counterfactual prediction. CITA (Causal Inference Task Affinity) is introduced to assess dataset compatibility before transfer by computing Fisher information matrix similarity between source and target tasks.

## Key Results
- Transfer learning reduces bias and improves estimation precision in target datasets, particularly when the source and target datasets are sufficiently similar and the target sample is small
- For TL-TARNet, target sample sizes do not affect precision once the source sample size exceeds 5000
- CITA scores effectively predict transfer success, with scores near 0 indicating compatible transfer and scores near 1 indicating dissimilar tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning improves ITE estimation in small target samples by providing a pretrained representation that constrains the hypothesis space before fine-tuning
- Mechanism: Train TARNet on large source data (N≥5000), freeze encoder layers, fine-tune only final layers on target. The source model provides regularized initialization that reduces variance without requiring the target sample to learn representations from scratch
- Core assumption: Source and target datasets share similar causal structure; the treatment effect relationship generalizes across domains
- Evidence anchors: [abstract] "transfer learning reduces bias and improves estimation precision in target datasets, particularly when the source and target datasets are sufficiently similar and the target sample is small"; [section] Page 24: "For TL-TARNet target sample sizes do not affect the precision. Only the source sample size indicates that N_S > 5000 do not substantially improve the precision."
- Break condition: Target sample becomes large enough (N≥500) that standalone TARNet achieves comparable precision; source has <5000 samples; or source-target CITA score >0.3 indicating dissimilar causal tasks

### Mechanism 2
- Claim: IPM-based regularization aligns treatment and control representations, enabling counterfactual prediction without observing counterfactuals
- Mechanism: The integral probability metric (specifically 1-Wasserstein distance) measures distributional distance between Φ(x)_treated and Φ(x)_control in representation space. Minimizing this distance alongside prediction loss creates balanced representations where treated/control groups are exchangeable
- Core assumption: Strong ignorability holds conditional on observed covariates; the representation Φ is injective
- Evidence anchors: [section] Page 8-9: "If both factual losses within treatment and control groups...and the IPM distance go toward zero, the counterfactual error will be small, too."; [section] Equation 6 shows the loss function combining empirical loss with α·IPM_G term
- Break condition: Treatment assignment depends on unobserved confounders; IPM cannot converge because fundamental distribution overlap is missing; α hyperparameter set to 0 (disables balancing)

### Mechanism 3
- Claim: CITA (Causal Inference Task Affinity) predicts transfer success by measuring Fisher information matrix similarity between source and target tasks using a frozen source model
- Mechanism: Compute Fisher matrices F_S,S (source model gradients on source data) and F_S,T (source model gradients on target data). The Frobenius norm of their difference indicates task similarity; scores near 0 suggest compatible transfer, near 1 suggests dissimilar tasks
- Core assumption: Source model is well-trained on its own task; Fisher matrix sensitivity correlates with counterfactual error
- Evidence anchors: [section] Page 16-17: Equations 13-15 define CITA computation; [section] Page 26: "Uttar Pradesh's subsamples have a lower CITA [0.09-0.11] than Punjab [0.32], which is plausible given they originate from the same population."
- Break condition: Source model poorly fitted (produces meaningless Fisher matrices); target has different covariate definitions (not just different distributions); computational budget doesn't allow Fisher matrix computation

## Foundational Learning

- Concept: **Potential Outcomes Framework (Rubin Causal Model)**
  - Why needed here: ITE estimation requires understanding Y(1) and Y(0) as counterfactual quantities that cannot both be observed. The entire TARNet architecture is designed to estimate these unobservables
  - Quick check question: Can you explain why we never observe τ_i = Y_i(1) - Y_i(0) directly for any individual?

- Concept: **Distributional Distance Metrics (Wasserstein/IPM)**
  - Why needed here: The IPM term in TARNet loss quantifies how different treated/control distributions are in representation space. Understanding optimal transport intuition helps debug why IPM isn't converging
  - Quick check question: If two distributions have the same mean but different variances, would Wasserstein distance be zero or positive?

- Concept: **Transfer Learning Paradigm (Fine-tuning Strategy)**
  - Why needed here: TL-TARNet relies on freezing layers strategically. Understanding which layers capture "general features" vs. "task-specific features" determines transfer success
  - Quick check question: If you freeze all layers, what happens to the model's ability to adapt to target-specific distribution shifts?

## Architecture Onboarding

- Component map:
  - Encoder (Φ): 3 fully-connected layers with ReLU, maps covariates to balanced representation
  - Treatment head (h_1): Predicts Y(1) from Φ(x)
  - Control head (h_0): Predicts Y(0) from Φ(x)
  - IPM calculator: Computes Wasserstein distance between Φ(x)_treated and Φ(x)_control
  - Transfer adapter: Freezes source encoder, reinitializes final layer(s) for target fine-tuning

- Critical path:
  1. Train source TARNet until factual loss + IPM converges (monitor both terms)
  2. Compute CITA score to validate target compatibility
  3. Copy source weights, freeze encoder, reinitialize final layer(s)
  4. Fine-tune in two phases: (a) minimize source-target IPM, (b) minimize target factual loss
  5. Evaluate ITE as h_1(Φ(x)) - h_0(Φ(x))

- Design tradeoffs:
  - Number of unfrozen layers: More unfrozen → better target adaptation but requires larger N_T. Paper used 1 unfrozen layer; suggests increasing only if target sample >500
  - α (IPM weight): Too small neglects balancing; too large sacrifices predictive accuracy. Must tune
  - Learning rate schedule: Transfer requires small learning rates (10^-5 range) with more epochs vs. source training (10^-3 range)
  - Batch size: Must include both treated/control units per batch for valid IPM; small datasets need small batches but risk single-group batches

- Failure signatures:
  - IPM term not decreasing: Treatment/control groups have insufficient covariate overlap; check propensity score distribution
  - CITA score >0.4: Source-target dissimilarity too high; transfer will likely cause negative transfer
  - Target ITE variance collapses to near-zero: Over-regularization; unfrozen layer count too low for target complexity
  - Source model factual loss doesn't converge: Source dataset too small or architecture overparameterized; check parameter count vs. N_S

- First 3 experiments:
  1. Baseline convergence check: Train TARNet on source data alone with varying α (0, 0.1, 1.0). Verify factual loss and IPM both converge. Plot learning curves to confirm stable optimization before any transfer
  2. Synthetic transfer validation: Create target dataset by subsampling source (N_T=100, 250, 500). Compute CITA scores. Transfer should achieve near-zero ITE error since source=target population. Verify CITA correlates with transfer error
  3. Selection bias stress test: Create biased target by selecting treatment assignment based on Y(0) as in paper's simulation. Compare TARNet vs. TL-TARNet mean ITE bias. Expected: TARNet shows severe bias (~2x true effect), TL-TARNet recovers estimates closer to source (~1x)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can similarity between source and target datasets be evaluated when covariates measure the same underlying construct but use different instruments or measurement scales?
- Basis in paper: [explicit] "Future research should further investigate how to evaluate the similarity between source and target data sets when covariates reflect the same underlying construct but are measured in different ways."
- Why unresolved: Different instruments (e.g., two depression scales) assess the same construct but produce non-interchangeable scores, making direct comparison infeasible
- What evidence would resolve it: Development and validation of a cross-instrument mapping or harmonization procedure that preserves causal signal in the representation space

### Open Question 2
- Question: How can multiple heterogeneous datasets be merged to construct a robust source dataset when some subgroups or treatment conditions are absent or underrepresented in individual sources?
- Basis in paper: [explicit] "Another direction for future work concerns the merging of multiple data sets to create a big source dataset, particularly when some subgroups or conditions are absent or underrepresented in certain data sources."
- Why unresolved: Simple pooling can bias estimates if covariate distributions differ or treatment effects vary across contexts; larger datasets may dominate and overwhelm signal
- What evidence would resolve it: A principled weighting or selection strategy for multi-source merging that maintains ITE accuracy across subgroups in simulation studies

### Open Question 3
- Question: How does the optimal number of unfrozen layers during fine-tuning depend on target sample size, covariate overlap, and CITA distance?
- Basis in paper: [inferred] Practical considerations section notes researchers "can decide how many layers are needed to be frozen and trainable" but provides no guidance; empirical application used one unfrozen layer arbitrarily
- Why unresolved: The trade-off between preserving stable source features and adapting to target idiosyncrasies lacks theoretical characterization
- What evidence would resolve it: Systematic simulation varying unfrozen layer count, target sample size, and source-target similarity to identify decision rules

## Limitations

- Theoretical guarantees for CITA correlation with transfer success are absent, relying on empirical validation rather than formal proofs
- Empirical validation uses only one real-world dataset (IHDS-II), limiting generalizability claims across different causal domains
- The definition of "sufficiently similar" datasets lacks formal thresholds, with the observed CITA cutoff of 0.3 appearing empirical rather than theoretically derived

## Confidence

- High Confidence: Mechanism 2 (IPM regularization for counterfactual prediction) - well-established in causal ML literature with strong theoretical foundations from Shajarisales et al. (2015)
- Medium Confidence: Mechanism 1 (transfer benefits for small samples) - supported by extensive simulations but limited to linear DGP
- Medium Confidence: CITA metric effectiveness - empirical correlation demonstrated but theoretical guarantees absent
- Low Confidence: Universal applicability across causal domains - based on single empirical dataset

## Next Checks

1. **Non-linear DGP Stress Test**: Generate target data with quadratic and interaction treatment effects. Evaluate whether TL-TARNet maintains advantage over TARNet as treatment heterogeneity complexity increases

2. **CITA Threshold Validation**: Create a grid of source-target pairs with systematically varying CITA scores (0.1, 0.2, 0.3, 0.4, 0.5). Measure transfer error vs. CITA to empirically establish performance thresholds and negative transfer points

3. **High-Dimensional Covariates Test**: Increase the number of covariates beyond 5 (e.g., 20-50) while maintaining the same sample sizes. Assess whether transfer benefits scale with dimensionality and whether CITA remains predictive of success in high-dimensional settings