---
ver: rpa2
title: 'CAFe: Unifying Representation and Generation with Contrastive-Autoregressive
  Finetuning'
arxiv_id: '2503.19900'
source_url: https://arxiv.org/abs/2503.19900
tags:
- multimodal
- arxiv
- language
- zhang
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAFe is a fine-tuning framework that enables large vision-language
  models (LVLMs) to perform both multimodal retrieval and text generation without
  compromising either capability. It introduces a contrastive-autoregressive objective
  that combines InfoNCE contrastive loss for representation learning with autoregressive
  language modeling loss, trained jointly on paired image-text data.
---

# CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning

## Quick Facts
- arXiv ID: 2503.19900
- Source URL: https://arxiv.org/abs/2503.19900
- Reference count: 14
- CAFe-7B achieves 87.5% R@1 for image→text and 75.3% R@1 for text→image on Flickr30K zero-shot retrieval

## Executive Summary
CAFe is a fine-tuning framework that enables large vision-language models (LVLMs) to perform both multimodal retrieval and text generation without compromising either capability. It introduces a contrastive-autoregressive objective that combines InfoNCE contrastive loss for representation learning with autoregressive language modeling loss, trained jointly on paired image-text data. The model uses a specialized embedding instruction to generate aligned image and text embeddings, while also maintaining its ability to generate coherent free-form text. CAFe removes the modality gap in representation space, where different modalities were previously embedded far apart.

## Method Summary
CAFe combines contrastive learning with autoregressive generation through a dual-objective fine-tuning approach. The framework trains on paired image-text data using InfoNCE contrastive loss to align multimodal representations in a shared embedding space, while simultaneously optimizing autoregressive language modeling loss for text generation. A specialized embedding instruction generates aligned image and text embeddings, addressing the modality gap where different modalities were previously embedded far apart. This unified approach enables the model to perform both tasks effectively without requiring separate specialized models.

## Key Results
- CAFe-7B achieves 87.5% R@1 for image→text and 75.3% R@1 for text→image on Flickr30K zero-shot retrieval
- On MMEB multimodal retrieval benchmark, CAFe-7B improves upon state-of-the-art by 10.2 percentage points in overall score
- CAFe demonstrates hallucination robustness with 89.1% accuracy on POPE and 88.9% F1 on THRONE, reducing hallucinated object generation by 1.5 percentage points

## Why This Works (Mechanism)
The framework works by unifying two previously separate objectives: representation learning through contrastive loss and generation through autoregressive modeling. By training these objectives jointly on paired image-text data, CAFe creates a shared embedding space where images and text are properly aligned while maintaining the model's generative capabilities. The specialized embedding instruction ensures that both modalities can be mapped to the same representational space without losing the ability to generate coherent text sequences.

## Foundational Learning
- **InfoNCE contrastive loss**: Needed to align multimodal representations in shared embedding space; quick check: measure similarity between matched and mismatched image-text pairs
- **Autoregressive language modeling**: Required for text generation capability; quick check: evaluate perplexity on text-only datasets
- **Multimodal embedding space**: Essential for unified representation; quick check: visualize embedding distances between matched and mismatched pairs
- **Fine-tuning objectives**: Critical for adapting pre-trained models; quick check: monitor training loss curves for both objectives
- **Paired image-text data**: Foundation for joint training; quick check: verify data quality and alignment
- **Modality gap**: Problem being solved; quick check: measure embedding distances between different modalities

## Architecture Onboarding
- **Component map**: Image encoder -> Shared embedding space <- Text encoder -> Autoregressive decoder
- **Critical path**: Image/text input → Contrastive alignment → Joint embedding → Text generation
- **Design tradeoffs**: Joint training vs separate models, embedding size vs computational cost, contrastive vs generative emphasis
- **Failure signatures**: Poor retrieval performance indicates contrastive loss issues, incoherent text indicates generation problems, modality gap persists if embeddings don't align
- **First experiments**: 1) Test contrastive retrieval on Flickr30K, 2) Evaluate text generation quality on standard benchmarks, 3) Measure modality gap reduction through embedding visualization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on English-language datasets, leaving multilingual capabilities untested
- Performance improvements demonstrated on specific benchmark datasets that may not generalize to all scenarios
- Ablation studies limited to 3B and 7B parameter models, leaving uncertainty about scalability to larger models

## Confidence
- **High Confidence**: Core technical contribution of unifying contrastive and autoregressive objectives is well-supported by experimental results
- **Medium Confidence**: Hallucination reduction claims are supported but based on relatively small-scale evaluation datasets
- **Medium Confidence**: Maintenance of multimodal understanding capabilities demonstrated through limited comparisons

## Next Checks
1. Test CAFe's retrieval and generation performance on non-English datasets to assess multilingual capabilities
2. Evaluate the framework on diverse hallucination benchmarks beyond POPE and THRONE
3. Conduct experiments with larger model sizes (13B, 30B) to verify scalability of the contrastive-autoregressive objective