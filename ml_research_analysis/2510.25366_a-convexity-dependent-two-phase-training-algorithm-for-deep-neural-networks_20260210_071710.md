---
ver: rpa2
title: A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks
arxiv_id: '2510.25366'
source_url: https://arxiv.org/abs/2510.25366
tags:
- adam
- loss
- gradient
- convex
- non-convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient optimization in deep
  neural networks by leveraging convexity properties of the loss function. The authors
  propose a two-phase training algorithm that detects the transition from non-convex
  to convex regions of the loss function and switches between Adam and Conjugate Gradient
  (CG) optimization methods accordingly.
---

# A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks

## Quick Facts
- arXiv ID: 2510.25366
- Source URL: https://arxiv.org/abs/2510.25366
- Reference count: 4
- Primary result: Two-phase training algorithm achieves better final loss and accuracy than pure Adam optimization

## Executive Summary
This paper presents a novel two-phase training algorithm for deep neural networks that exploits the convexity properties of loss functions near local minima. The key insight is that while neural network loss functions are typically non-convex, they become convex in neighborhoods around local minima, enabling the use of optimization methods with superlinear convergence guarantees. The algorithm detects this transition from non-convex to convex regions and switches from Adam to Conjugate Gradient optimization accordingly. Experimental results across Vision Transformer architectures and VGG5 on standard vision datasets demonstrate consistent superiority over pure Adam optimization, achieving better final loss and accuracy.

## Method Summary
The two-phase training algorithm monitors the gradient norm behavior during optimization to detect when the loss function transitions from non-convex to convex regions. In the initial phase, Adam optimization is used to navigate the non-convex landscape. Once the algorithm detects that the loss function has become convex near a local minimum (identified by specific gradient norm patterns), it switches to Conjugate Gradient optimization to exploit the convex structure and achieve faster, more stable convergence with superlinear guarantees. The method requires no architectural modifications and works as a drop-in optimization strategy.

## Key Results
- Consistent improvement in final loss and accuracy across multiple Vision Transformer variants (ViT-mlp, ViT-nomlp, ViT-nomlp-wkewq, ViT-nomlp-wkewq-wvwo1) and VGG5
- Sustained performance advantages across all tested datasets (MNIST, CIFAR-10, CIFAR-100)
- The convexity structure enabling this approach appears common enough in real-world tasks to be practically exploitable
- Only requires monitoring of gradient norm behavior for convexity transition detection

## Why This Works (Mechanism)
The algorithm leverages the theoretical property that neural network loss functions, while globally non-convex, exhibit local convexity in neighborhoods around local minima. This allows for combining the exploration capabilities of Adam in non-convex regions with the superlinear convergence guarantees of Conjugate Gradient in convex regions. The gradient norm monitoring serves as a practical proxy for detecting the convexity transition, enabling automatic switching between optimization methods without manual intervention or hyperparameter tuning specific to this transition point.

## Foundational Learning

**Convexity in optimization**: Why needed - Understanding the fundamental difference between convex and non-convex optimization landscapes and their implications for convergence guarantees. Quick check - Verify that a function is convex if the line segment between any two points on the graph lies above or on the graph.

**Gradient norm monitoring**: Why needed - The method relies on gradient norm behavior as the primary signal for detecting convexity transitions. Quick check - Plot gradient norms during training to identify the characteristic decrease pattern that indicates convexity emergence.

**Conjugate Gradient optimization**: Why needed - This method provides superlinear convergence in convex regions but requires the loss function to be convex. Quick check - Confirm that CG updates are orthogonal to previous search directions in the transformed space.

## Architecture Onboarding

**Component map**: Loss function landscape -> Gradient norm monitoring -> Adam optimizer (Phase 1) -> CG optimizer (Phase 2) -> Model parameters

**Critical path**: The most critical component is the convexity transition detection mechanism, as incorrect detection timing would either miss the optimization benefits or switch too early before true convexity emerges.

**Design tradeoffs**: The algorithm trades implementation simplicity (only requires monitoring gradient norms) for theoretical rigor (relies on empirical observation of convexity emergence rather than proven guarantees for all architectures).

**Failure signatures**: Premature switching to CG before true convexity emerges would lead to oscillations or divergence; delayed switching would miss optimization benefits; gradient norm monitoring failure could cause the algorithm to remain in Adam mode indefinitely.

**First experiments**: 1) Run gradient norm monitoring on a simple convex problem to establish baseline behavior; 2) Test on a known non-convex problem with a clear convexity transition point; 3) Apply to a small neural network on a simple dataset to verify the two-phase behavior.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification for why convexity emerges near minima in deep networks lacks rigorous mathematical proof for general cases
- Experimental validation limited to Vision Transformer architectures and VGG5 on standard vision datasets, raising questions about cross-domain applicability
- Convexity transition detection relies on gradient norm behavior, which may not be universally reliable across all architectures and loss landscapes

## Confidence

**High**: The two-phase algorithm framework is well-defined and the experimental methodology is sound

**Medium**: The empirical demonstration of convexity detection and its practical benefits

**Low**: The theoretical justification for why convexity emerges near minima in deep networks

## Next Checks

1. Test the algorithm on recurrent neural networks and transformer-based language models to assess cross-domain applicability

2. Conduct ablation studies with different convexity detection thresholds and alternative transition criteria

3. Compare against other optimization method combinations (e.g., SGD + L-BFGS) to isolate the benefit of the Adam+CG pairing specifically