---
ver: rpa2
title: 'Five Years of SciCap: What We Learned and Future Directions for Scientific
  Figure Captioning'
arxiv_id: '2512.21789'
source_url: https://arxiv.org/abs/2512.21789
tags:
- figure
- captions
- scientific
- captioning
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SciCap project, spanning 2021-2025, explored scientific figure
  captioning by curating large datasets from arXiv papers, developing generation models,
  and conducting human evaluations. Initial results showed poor performance from vision-to-language
  models, but treating captioning as text summarization yielded strong results.
---

# Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning

## Quick Facts
- arXiv ID: 2512.21789
- Source URL: https://arxiv.org/abs/2512.21789
- Reference count: 8
- Five-year project (2021-2025) on scientific figure captioning using arXiv data, finding that treating it as text summarization works better than vision-to-language models

## Executive Summary
The SciCap project explored scientific figure captioning by curating large datasets from arXiv papers, developing generation models, and conducting human evaluations over five years. Initial results showed poor performance from vision-to-language models, but treating captioning as text summarization yielded strong results. The project evolved to leverage large language models and launched annual challenges. Human evaluations revealed that LLM-generated captions were often preferred over author-written ones, particularly for long captions. The work identified key challenges: handling incomplete context, personalizing for diverse readers and writer styles, studying real writing processes, and bridging captioning with figure understanding.

## Method Summary
The project collected 295,028 arXiv papers (2010-2020) to create a dataset of 2.17M figures, filtering to 416,804 graph plots and 133,543 single-panel figures. Figure-mentioning paragraphs were extracted via regex patterns, and OCR was run on figure images. The approach treated captioning as text summarization, fine-tuning PEGASUS on figure-mentioning paragraphs with original captions as targets. Evaluation used ROUGE-2-Normalized for automatic metrics and human evaluation by PhD students/editors. The dataset expanded to 476,389 figures across 8 domains and 5 figure types for the Challenge dataset.

## Key Results
- Vision-to-language models performed poorly on scientific figure captioning
- Text summarization approach (paragraphs as input) yielded strong ROUGE-2 scores (0.4574)
- ~75% of generated caption words traceable to paragraphs/OCR text
- LLM-generated captions often preferred over author-written ones in human evaluations
- 18.81% of figures lacked identified figure-mentioning paragraphs

## Why This Works (Mechanism)
The success of treating figure captioning as text summarization works because scientific writing typically follows a pattern where figures are explicitly described in surrounding text. The figure-mentioning paragraphs contain most of the information needed for captions, making the visual content partially redundant. This approach bypasses the challenging computer vision problems of interpreting complex scientific figures by leveraging the existing textual descriptions that authors naturally include.

## Foundational Learning
- **Scientific figure-mentioning patterns**: Why needed - to identify where captions should come from; Quick check - test regex patterns on 100 random papers
- **Text summarization for captioning**: Why needed - provides alternative to vision-only approaches; Quick check - compare PEGASUS performance against vision models
- **Human evaluation methodology**: Why needed - automated metrics don't capture caption quality; Quick check - run pilot study with 10 captions and 3 evaluators
- **Context dependency**: Why needed - reveals why pure vision approaches fail; Quick check - analyze captions for figures without mentions
- **Personalization tradeoffs**: Why needed - shows limitations of one-size-fits-all generation; Quick check - compare expert vs novice preferences on sample captions

## Architecture Onboarding

**Component Map**
PDF parsing -> Figure extraction -> OCR -> Text extraction -> PEGASUS fine-tuning -> Evaluation

**Critical Path**
Figure extraction → OCR → Text extraction → PEGASUS fine-tuning → Human evaluation

**Design Tradeoffs**
- Text-only vs vision-input: Simpler, leverages existing descriptions but misses visual details
- PEGASUS vs other models: Proven text summarization performance vs potential for better domain adaptation
- Human vs automated evaluation: More accurate quality assessment vs scalability and consistency

**Failure Signatures**
- Generic captions ignoring figure-specific details → Check figure-mentioning paragraph extraction
- Low ROUGE scores → Verify text extraction and OCR quality
- Human preference mismatch → Re-examine evaluation methodology and sampling

**First Experiments**
1. Extract figures and OCR text from 50 arXiv papers, verify extraction quality
2. Fine-tune PEGASUS on extracted text for 10 epochs, check training loss
3. Generate captions for test set, calculate initial ROUGE-2 scores

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Is personalized text generation necessary for effective AI writing assistance, or is generic high-quality generation sufficient for authors?
- Basis in paper: [explicit] The authors explicitly ask, "to support writing, do we actually need personalized text generation, or is generic generation sufficient for most authors to build on?"
- Why unresolved: Challenge results showed a trade-off where models mimicking author style (personalized) were often perceived as lower quality by human judges compared to vanilla LLM outputs.
- What evidence would resolve it: A controlled study measuring final caption quality and author satisfaction when using generic versus personalized tools, specifically analyzing if stylistic fidelity impedes factual accuracy.

**Open Question 2**
- Question: How can systems generate meaningful captions for scientific figures when surrounding document context is missing or incomplete?
- Basis in paper: [explicit] The authors state that "Generating meaningful captions when only the image is available remains one of the field's most practical yet unsolved problems."
- Why unresolved: 18.81% of figures in the SciCap dataset lacked explicit text mentions, and the project found that treating captioning as text summarization (requiring context) yielded better results than vision-only approaches.
- What evidence would resolve it: The development of vision-language models that can extract implicit context directly from the image (e.g., reading axis labels, interpreting trends) to generate high-faithfulness captions without external text.

**Open Question 3**
- Question: How can captioning models implicitly adapt to different reader expertise levels without requiring explicit user profiling?
- Basis in paper: [explicit] The paper notes that "Building audience-aware captioning systems without relying on extensive user profiling remains an open challenge in personalization."
- Why unresolved: Evaluations revealed a disconnect between novice (undergraduates) and expert (PhDs) preferences regarding detail versus takeaways, making a "one-size-fits-all" model insufficient.
- What evidence would resolve it: A controllable generation framework that can dynamically adjust technical depth based on inferred audience intent, validated by diverging satisfaction scores from distinct user groups.

## Limitations
- Text-only approach ignores visual content, leaving 25% of caption words unexplained
- Human evaluation methodology needs more rigorous sampling validation
- Domain coverage limited to CS/ML, limiting generalizability to other scientific fields
- Reported performance improvements may reflect dataset quality rather than model capability

## Confidence

**High Confidence**: The core finding that vision-to-language models perform poorly on scientific figure captioning is well-supported. The identification of scientific figure captioning as a highly contextual task requiring specialized approaches is robust.

**Medium Confidence**: The claim that treating captioning as text summarization yields strong results is supported but limited by the visual-blind nature of the approach. The preference for LLM-generated over author captions needs more rigorous validation.

**Low Confidence**: The generalizability of findings to non-CS/ML domains and non-graph figure types. The practical utility of 75% trace agreement for real-world scientific writing.

## Next Checks

1. **Visual Content Verification**: Select 50 randomly sampled figures from the dataset and manually verify whether generated captions accurately describe the actual visual content versus what the text mentions. This would test the 25% unexplained content and reveal the practical limitations of the text-only approach.

2. **Cross-Domain Generalization**: Apply the PEGASUS fine-tuning pipeline to a sample of figures from biology, chemistry, or physics papers to assess performance degradation. Compare ROUGE scores and human preference ratings against the CS/ML baseline to quantify domain specificity.

3. **End-to-End Writing Process Study**: Conduct a small-scale observational study where scientists write figure captions from scratch while thinking aloud, measuring time, revision patterns, and information sources. Compare against LLM-generated captions on the same figures to assess practical utility beyond automated metrics.