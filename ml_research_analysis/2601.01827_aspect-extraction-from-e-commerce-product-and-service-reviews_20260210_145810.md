---
ver: rpa2
title: Aspect Extraction from E-Commerce Product and Service Reviews
arxiv_id: '2601.01827'
source_url: https://arxiv.org/abs/2601.01827
tags:
- aspect
- aspects
- reviews
- dataset
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles aspect extraction from Taglish (Tagalog-English)
  product reviews, a low-resource, code-switched context. A multi-method pipeline
  combining rule-based systems, LLM annotation, and fine-tuning was built.
---

# Aspect Extraction from E-Commerce Product and Service Reviews

## Quick Facts
- arXiv ID: 2601.01827
- Source URL: https://arxiv.org/abs/2601.01827
- Reference count: 2
- Primary result: LLM outperforms rule-based and fine-tuned models for Taglish aspect extraction, especially implicit aspects

## Executive Summary
This work addresses aspect extraction from Taglish (Tagalog-English) e-commerce reviews, a low-resource, code-switched context. A multi-method pipeline combining rule-based systems, LLM annotation, and fine-tuning was built. The study introduces a Hierarchical Aspect Framework (HAF) using LDA, BERTopic, and LLM synthesis, and a dual-mode scheme handling explicit and implicit aspects. Four models—Rule-Based, Gemini 2.0 Flash, and two Gemma-3 1B fine-tuned variants—were evaluated. Gemini 2.0 Flash achieved the highest performance (Macro F1 0.91), excelling at implicit aspects, while the Gemma models underperformed due to dataset imbalance and limited capacity. Results indicate that LLMs are best suited for nuanced, code-switched AE tasks, while fine-tuning smaller models on imbalanced data is ineffective.

## Method Summary
The study uses the SentiTaglishProductsAndServices dataset (10,510 Taglish reviews) with a manually calibrated subset (67 reviews, Fleiss' Kappa: 0.691). A Hierarchical Aspect Framework (HAF) was developed using LDA and BERTopic topic modeling, then refined with LLM synthesis. Annotation was performed via manual calibration and Gemini 2.0 few-shot labeling (validated at 92.31% accuracy). Four models were evaluated: Rule-Based (regex + lexicon + 10 disambiguation rules), Gemini 2.0 Flash (few-shot inference), and two Gemma-3 1B fine-tuned variants (flat and hierarchical) trained on RB-annotated or LLM-annotated data using LoRA with inverse frequency class weights. Evaluation metrics included Exact Match, Hamming Loss, and Macro/Micro F1.

## Key Results
- Gemini 2.0 Flash achieved the highest performance (Macro F1 0.91), excelling at implicit aspect detection
- Gemma-3 1B fine-tuned models underperformed due to dataset imbalance and limited capacity
- Rule-Based system achieved ~52% exact match, limited by explicit keyword matching
- Price aspect category showed F1=0.0, indicating severe training data imbalance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generative LLMs outperform rule-based and fine-tuned small models for implicit aspect detection in code-switched text.
- **Mechanism:** Large pretrained models encode broad semantic patterns from multilingual pretraining, enabling inference of unstated aspects from context (e.g., inferring "Delivery-Timeliness" from "took a week to arrive" without the keyword "late").
- **Core assumption:** The LLM's pretraining corpus included sufficient code-switched or multilingual data to transfer semantic reasoning to Taglish.
- **Evidence anchors:**
  - [abstract] "Gemini 2.0 Flash achieved the highest performance (Macro F1 0.91), demonstrating superior capability in handling implicit aspects."
  - [section IV.A] "The LLM excels here, correctly inferring the aspect from the context of time duration" while rule-based systems fail without explicit keywords.
  - [corpus] Related work on cross-domain ABSA (arXiv:2501.08974) similarly finds LLMs effective for aspect extraction, but does not specifically validate Taglish performance.
- **Break condition:** If implicit aspect frequency in test data significantly exceeds pretraining distribution, or if inference cost constraints prohibit LLM deployment, performance advantage may not translate to production viability.

### Mechanism 2
- **Claim:** Hierarchical classification with conditional activation improves generalization by constraining specific-aspect predictions to valid parent categories.
- **Mechanism:** General models learn broader patterns first; specific models only activate when parent category is predicted, reducing cross-aspect contamination (e.g., preventing "Product-Correctness" from firing on fulfillment errors that should be "Delivery-Correctness").
- **Core assumption:** Aspects naturally decompose into stable parent-child relationships that transfer across reviews.
- **Evidence anchors:**
  - [section II.D.3] Describes two-stage training: general model first, then domain-specific models conditioned on general predictions.
  - [Table IV] Hierarchical (H) variants show different precision/recall tradeoffs than flat (F) variants, though both fine-tuned models struggled.
  - [corpus] No direct corpus validation of hierarchical ABSA architectures; this appears to be a domain-specific design choice.
- **Break condition:** If parent-aspect predictions are unreliable (as occurred with fine-tuned Gemma models), cascading errors will propagate to specific aspects.

### Mechanism 3
- **Claim:** Fine-tuning sub-billion-parameter models on complex, imbalanced code-switched data fails due to capacity mismatch, not data quality.
- **Mechanism:** The 1B parameter architecture lacks sufficient representational capacity to simultaneously encode noisy code-switched patterns and distinguish 25 labels across a multi-label space, causing over-prediction (high recall, near-zero precision).
- **Core assumption:** Model failure stems from architecture constraints rather than training hyperparameters or data preprocessing.
- **Evidence anchors:**
  - [section IV.C] "The 1B parameter architecture likely lacked the capacity to encode the noisy, code-switched patterns... while simultaneously handling a multi-label classification task."
  - [Table II] FT-LLM-DS (trained on superior data) achieved lower exact match (8.7%) than FT-RB-DS (13.04%), suggesting better data did not compensate for capacity limits.
  - [corpus] Related Bangla ABSA work (arXiv:2511.21381) uses ensemble deep learning rather than small-model fine-tuning for low-resource settings, consistent with capacity concerns but not directly comparable.
- **Break condition:** Assumption: different training configurations (higher rank LoRA, curriculum learning, or data augmentation) might partially overcome capacity limits; this was not tested.

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - Why needed here: The entire pipeline assumes understanding that AE is the first subtask of ABSA, identifying *what* is being reviewed before assigning sentiment.
  - Quick check question: Given "The food was delicious but service was slow," can you identify two distinct aspects and explain why document-level sentiment would lose this granularity?

- **Concept: Code-Switching / Code-Mixing**
  - Why needed here: Taglish reviews blend Tagalog and English within sentences; standard NLP pipelines that translate to a single language risk losing pragmatic markers and cultural context.
  - Quick check question: Why would translating "Ang ganda ng quality, sulit na sulit!" to pure English potentially lose information relevant to aspect detection?

- **Concept: Implicit vs. Explicit Aspects**
  - Why needed here: The paper's central finding is that LLM success derives from implicit aspect inference; understanding this distinction is essential for interpreting results.
  - Quick check question: In "Took forever to get here," what is the implicit aspect, and why would a keyword-matching system fail?

## Architecture Onboarding

- **Component map:**
  Raw Reviews → HAF Definition (LDA + BERTopic + LLM synthesis) → Annotation Pipeline (Manual calibration → Gemini 2.0 few-shot labeling) → Training Data (LLM-DS or RB-DS) → Model Branches (evaluated separately): 1. Rule-Based (Regex + Lexicon + Disambiguation Rules), 2. Gemini 2.0 Flash (Few-shot inference, no training), 3. Gemma-3 1B + LoRA (Flat or Hierarchical) → Evaluation (Exact Match, Hamming Loss, F1)

- **Critical path:** HAF taxonomy quality → Annotation reliability → Model selection (LLM vs. fine-tuned) → Deployment decision. The HAF structure is foundational; errors here propagate through all downstream models.

- **Design tradeoffs:**
  - LLM (Gemini): Highest accuracy, handles implicit aspects, but requires API calls per review (cost, latency, data privacy concerns).
  - Rule-Based: Fast, interpretable, no inference cost, but recall limited to explicit keywords; achieved 52% exact match vs. LLM's 78%.
  - Fine-tuned Small Model: Intended for low-cost local deployment, but proved ineffective for this task complexity.

- **Failure signatures:**
  - Cross-aspect contamination: Model predicts Product-Correctness when review describes delivery error → indicates insufficient context modeling.
  - Over-prediction spam: High recall with near-zero precision → suggests capacity mismatch or class imbalance issues.
  - Price category F1 = 0.0: Complete failure on a category with explicit numerical signals → indicates severe training data imbalance or learning rate issues.

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Implement the rule-based tagger with the described lexicon and 10 disambiguation rules on a held-out sample; verify ~52% exact match baseline before investing in LLM integration.
  2. **Test LLM prompt stability:** Run Gemini 2.0 Flash on 50 reviews with 3 different prompt phrasings (same few-shot examples, different instruction wording); measure variance in aspect predictions to assess prompt sensitivity.
  3. **Pilot fine-tuning with larger backbone:** If local deployment is required, test Gemma-3 4B or 12B (instead of 1B) with identical LoRA config to determine if capacity was the limiting factor vs. dataset quality.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a hybrid pipeline combining rule-based precision with LLM semantic reasoning optimize the trade-off between accuracy and computational cost?
  - Basis in paper: [explicit] The authors state future work should "explore hybrid pipelines that combine the high precision of rules with the semantic reasoning of LLMs to optimize the trade-off between accuracy and computational cost."
  - Why unresolved: The current study evaluated these methods independently; the potential synergies or conflicts in a combined architecture were not tested.
  - What evidence would resolve it: Benchmarking a hybrid system (e.g., rules for explicit extraction, LLM for implicit) against latency and F1 scores.

- **Open Question 2:** Does addressing dataset imbalance, specifically for underrepresented "Service" aspects, significantly improve the performance of fine-tuned smaller models?
  - Basis in paper: [explicit] The conclusion notes that "Future work should aim to expand the dataset to address imbalance, particularly for underrepresented Service aspects."
  - Why unresolved: The current fine-tuned models failed partly due to class imbalance, but it is unconfirmed if balancing the data alone would fix the "architectural failure" observed in the 1B parameter models.
  - What evidence would resolve it: Re-training the Gemma-1B models on a balanced, augmented dataset and comparing F1 scores on the "Service" category.

- **Open Question 3:** Would increasing the parameter capacity beyond 1B allow fine-tuned models to effectively handle the complexity of code-switched Taglish data?
  - Basis in paper: [inferred] The authors attribute the failure of fine-tuned models to a "capacity mismatch," suggesting the 1B architecture "lacked the capacity to encode the noisy, code-switched patterns."
  - Why unresolved: It remains unclear if the failure was purely due to size constraints or if the architectural approach (fine-tuning) is fundamentally ill-suited for this specific low-resource task compared to generative prompting.
  - What evidence would resolve it: Fine-tuning larger variants (e.g., 7B or 12B parameters) on the same LLM-annotated dataset to see if performance approaches that of Gemini 2.0 Flash.

## Limitations

- **Limited fine-tuning success:** Gemma-3 1B fine-tuning underperformed significantly, achieving up to 23% exact match and near-zero precision on several aspects (e.g., Price F1=0.0).
- **Single-language code-switching:** Results apply only to Taglish (Tagalog-English). Generalization to other code-switched languages or monolingual low-resource settings is unproven.
- **No runtime or cost analysis:** While LLM inference outperformed fine-tuning, the paper does not quantify latency, API cost, or data privacy implications, which are critical for deployment decisions.

## Confidence

- **High confidence:** LLM superiority for implicit aspect detection in code-switched text. Multiple evidence points (abstract, section IV.A, Table IV) consistently show Gemini 2.0 Flash achieving 91% F1, with explicit failure modes for rule-based and fine-tuned models on implicit aspects.
- **Medium confidence:** Hierarchical classification improves generalization by constraining specific-aspect predictions to valid parent categories. Design is plausible and logically sound, but performance data shows mixed results—fine-tuned models failed regardless of hierarchy, so the benefit is theoretical rather than empirically validated in this work.
- **Low confidence:** 1B parameter architecture is the limiting factor for fine-tuning. While authors claim capacity mismatch, they did not test larger models (e.g., Gemma-3 4B/12B) or alternative training strategies, so this remains an untested hypothesis.

## Next Checks

1. **Test prompt stability:** Run Gemini 2.0 Flash on 50 reviews with 3 different prompt phrasings (same few-shot examples, different instruction wording); measure variance in aspect predictions to assess reliability and identify sensitivity to prompt engineering.
2. **Pilot fine-tuning with larger backbone:** If local deployment is required, test Gemma-3 4B or 12B (instead of 1B) with identical LoRA config and training data to determine if capacity was the limiting factor versus dataset quality or training hyperparameters.
3. **Validate hierarchical benefit:** Train a flat classifier (no parent-child constraints) with the same Gemma-3 1B architecture and compare cross-aspect contamination rates to hierarchical variants to isolate whether hierarchy helps when capacity constraints are relaxed.