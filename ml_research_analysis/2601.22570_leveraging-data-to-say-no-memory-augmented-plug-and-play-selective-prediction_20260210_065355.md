---
ver: rpa2
title: 'Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction'
arxiv_id: '2601.22570'
source_url: https://arxiv.org/abs/2601.22570
tags:
- ma-papsp
- papsp
- score
- selective
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a memory-augmented approach for selective prediction
  in vision-language foundation models, addressing the instability and poor calibration
  of similarity scores. The method, MA-PaPSP, uses a retrieval dataset to reduce embedding
  variance by averaging nearest-neighbor representations and employs contrastive normalization
  to improve score calibration.
---

# Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction

## Quick Facts
- arXiv ID: 2601.22570
- Source URL: https://arxiv.org/abs/2601.22570
- Reference count: 40
- Primary result: MA-PaPSP reduces AURC by up to 34 points on MS-COCO captioning compared to non-memory baselines.

## Executive Summary
This paper introduces a memory-augmented approach for selective prediction in vision-language foundation models, addressing the instability and poor calibration of similarity scores. The method, MA-PaPSP, uses a retrieval dataset to reduce embedding variance by averaging nearest-neighbor representations and employs contrastive normalization to improve score calibration. Evaluated across classification, image-text matching, and captioning tasks, MA-PaPSP outperforms baselines including VQAScore, SeeTRUE, and LVLM-as-a-judge, achieving significant gains in AURC across multiple datasets.

## Method Summary
MA-PaPSP augments the Plug-and-Play Selective Prediction (PaPSP) framework with a retrieval dataset to stabilize proxy embeddings and improve calibration. For a given image, the method retrieves K nearest-neighbor text embeddings from a precomputed retrieval set, computes a weighted average to form a stable proxy embedding, and generates a contrastive similarity score using hard-negative captions. This score is normalized against hard negatives to produce calibrated [0,1] values for accept/reject decisions. The approach operates without training, relying on pretrained VLM encoders and precomputed retrieval embeddings.

## Key Results
- MA-PaPSP achieves significant AURC reductions across classification, ITM, and captioning tasks compared to PaPSP and other baselines.
- Using a mixed retrieval set (CC12M + CC3M + SBU-1M) performs best, but large generic datasets can match or exceed small in-domain sets.
- Rule-based hard-negative generation is much faster (4.72ms) than SLM-based (42-92ms) with only small AURC differences.
- K=15 nearest neighbors provides optimal proxy embedding stability without excessive noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving and averaging nearest-neighbor embeddings reduces representation variance, producing more stable proxy embeddings.
- Mechanism: Query embedding ϕ(q) retrieves K neighbors from dataset R; proxy ̃ϕ(q) = weighted average of neighbor embeddings per Eq. 6. This smooths single-embedding noise toward a "ground-truth" concept centroid.
- Core assumption: Neighbors in embedding space share semantic content with the query; variance is noise, not signal.
- Evidence anchors:
  - [abstract] "...reduce embedding variance by averaging retrieved nearest-neighbor pairs..."
  - [section 4, Figure 2c] Visualization of proxy embedding moving closer to ground-truth concept.
  - [corpus] Weak direct evidence; neighboring papers discuss memory/RAG but not this specific variance-reduction mechanism.
- Break condition: If retrieval set lacks domain coverage, neighbors may be semantically irrelevant, degrading proxy quality.

### Mechanism 2
- Claim: Contrastive normalization improves calibration by normalizing the similarity score against hard-negative alternatives.
- Mechanism: Score s_tc = exp(s_tp/τ) / Σ exp(s_tp(x,y_k)/τ) over hard negatives E(f(x)) (Eq. 8). This produces [0,1] scores with more uniform magnitude across embedding space.
- Core assumption: Hard negatives (noun swaps via rule-based or SLM) form a meaningful contrast set; the embedding geometry's local variations are multiplicative.
- Evidence anchors:
  - [abstract] "...contrastive normalization to improve score calibration."
  - [section 4, Figure 2d] Visualization showing contrastive scores producing consistent similarity magnitudes.
  - [corpus] No direct calibration-mechanism evidence found.
- Break condition: If hard negatives are too similar or incoherent, normalization may not improve discrimination; score may collapse.

### Mechanism 3
- Claim: A retrieval dataset enables both mechanisms without training, by providing a reference structure in embedding space.
- Mechanism: Dataset R = {(x_R, y_R)} provides precomputed embeddings for nearest-neighbor lookup (proxy) and anchors for calibration context.
- Core assumption: The retrieval set's distribution reasonably overlaps with target domain; size and diversity matter more than exact match.
- Evidence anchors:
  - [section 5, Table 3] Out-of-domain retrieval (15M samples) often matches or exceeds in-domain (110K) performance; mixed is best.
  - [abstract] "...augments the PaPSP with a retrieval dataset..."
  - [corpus] Related RAG/memory work supports retrieval utility but not calibration/instability claims specifically.
- Break condition: If retrieval set is tiny or semantically mismatched (random dataset), performance degrades (Table 3, "Random" column).

## Foundational Learning

- Concept: CLIP-style dual-encoder embeddings and cosine similarity
  - Why needed here: The entire method operates in a pretrained VLRM embedding space; understanding alignment and similarity is prerequisite.
  - Quick check question: Can you explain why cosine similarity is used instead of Euclidean distance for CLIP embeddings?

- Concept: Selective prediction and the risk-coverage tradeoff
  - Why needed here: The paper's objective is minimizing risk at each coverage level (AURC metric).
  - Quick check question: If coverage drops to 50%, what must happen to risk for a better AURC?

- Concept: Nearest-neighbor retrieval and weighting schemes
  - Why needed here: Proxy embeddings depend on K-NN retrieval and proximity-weighted averaging (Eq. 6).
  - Quick check question: Why would K=1 (hard nearest neighbor) underperform K=15 (soft averaging)?

## Architecture Onboarding

- Component map: Image → SP-VLM image embedding → K-NN retrieval from R → proxy embedding → similarity with predicted caption embedding → contrastive normalization → score vs. threshold
- Critical path: Image → SP-VLM image embedding → K-NN retrieval from R → proxy embedding → similarity with predicted caption embedding → contrastive normalization → score vs. threshold
- Design tradeoffs:
  - Retrieval set: Larger generic (CC12M) vs. smaller in-domain; mixed best but increases storage.
  - Hard negatives: Rule-based (fast, 4.72ms) vs. LLM-based (better quality, 42–92ms); Table C.4 shows small AURC gap.
  - K value: K≈15 optimal; too small underuses neighbors, too large adds noise (Table C.1).
- Failure signatures:
  - High AURC on classification but low on captioning → likely retrieval set lacks linguistic diversity.
  - Score distribution compressed near 0.5 → contrastive set may be poorly constructed.
  - MA-PAPSP worse than PAPSP → check retrieval set overlap; may be random/mismatched (Table 3 "Random").
- First 3 experiments:
  1. Reproduce Table 2: Run PAPSP vs. MA-PAPSP (SigLIP B/16) on MS-COCO captioning; verify AURC gap >10%.
  2. Ablate K: Sweep K ∈ {1, 5, 15, 20} on one captioning dataset; confirm K≈15 is optimal (Table C.1).
  3. Retrieval set sensitivity: Compare in-domain vs. out-of-domain vs. random retrieval on Flowers classification (Table 3); observe degradation with mismatched data.

## Open Questions the Paper Calls Out
The paper identifies achieving optimal retrieval set selection for specific domains as requiring "some trial and error by practitioners" (Section 6) and notes that replacing contrastive score generation with sentence manipulations is an "avenue for future investigation" (Appendix B).

## Limitations
- The method's performance depends on having a retrieval set with reasonable domain overlap, which constitutes a form of implicit domain adaptation.
- The specific effectiveness of rule-based vs. SLM hard negatives is supported by timing and AURC numbers, but the qualitative difference in negative quality is not shown.
- Performance gains on out-of-domain retrieval datasets are substantial but not compared against a learned memory-augmented selective prediction method.

## Confidence
- **High Confidence:** The core mechanisms of proxy embedding via K-NN averaging and contrastive normalization are well-defined and mathematically sound. The retrieval dataset utility is empirically demonstrated across multiple tasks.
- **Medium Confidence:** The specific effectiveness of rule-based vs. SLM hard negatives is supported by timing and AURC numbers, but the qualitative difference in negative quality is not shown. The mixed retrieval set performance claim is based on a single dataset combination.
- **Low Confidence:** The claim that the method works without "domain-specific fine-tuning" is true for the retrieval set, but the performance still depends on having a retrieval set with reasonable domain overlap—this is a form of implicit domain adaptation.

## Next Checks
1. **Hyperparameter sensitivity:** Sweep the contrastive temperature τ ∈ {0.01, 0.05, 0.1, 1.0} on MS-COCO captioning to quantify its impact on AURC and calibration curves.
2. **Hard-negative quality test:** Manually annotate 100 hard negatives generated by rule-based vs. SLM methods; compute semantic similarity to the positive to quantify "hardness" and check if SLM truly generates harder negatives.
3. **Retrieval set ablation:** Replace the CC12M+CC3M+SBU-1M mix with a random image-text pair set of equal size; confirm AURC degrades significantly, validating the importance of retrieval set quality.