---
ver: rpa2
title: Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable
  Global Prototypes
arxiv_id: '2508.19009'
source_url: https://arxiv.org/abs/2508.19009
tags:
- server
- client
- prototype
- learning
- prototypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the prototype margin shrinking problem in heterogeneous
  federated learning, where model and data heterogeneity cause prototype decision
  boundaries to collapse during weighted averaging. FedProtoKD introduces an adaptive
  class-wise margin-based trainable prototype (ACTP) framework using contrastive learning
  to synthesize global prototypes that preserve inter-class separability.
---

# Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes

## Quick Facts
- **arXiv ID:** 2508.19009
- **Source URL:** https://arxiv.org/abs/2508.19009
- **Reference count:** 26
- **Key outcome:** FedProtoKD improves server accuracy by 1.13% on average and up to 34.13% over state-of-the-art methods in heterogeneous federated learning

## Executive Summary
FedProtoKD addresses prototype margin shrinking in heterogeneous federated learning by introducing an adaptive class-wise margin-based trainable prototype (ACTP) framework. The system uses contrastive learning to synthesize global prototypes that preserve inter-class separability across heterogeneous client models and non-IID data. A dual-knowledge distillation mechanism combines client logits and prototype representations, while sample prioritization assigns importance weights based on proximity to class prototypes. Experiments show significant accuracy improvements across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.

## Method Summary
The framework trains heterogeneous client models with learnable projection layers that map diverse features to a unified 512-dimensional space. Clients compute class-wise prototypes and logits on unlabeled public proxy data. The server employs an ACTP module with a contrastive generator to synthesize global prototypes with adaptive margins, preventing decision boundary collapse. Dual knowledge distillation combines KL divergence on aggregated logits with MSE between server features and generated prototypes. Sample prioritization weights public samples by inverse L2 distance to class centroids. The system uses 10 clients, 100 rounds, 5 local epochs, and ResNet34 server model.

## Key Results
- Server accuracy improves by 1.13% on average and up to 34.13% over state-of-the-art methods
- Optimal public proxy data size is 2,500 samples (Figure 8)
- Local epochs beyond 5 degrade server performance due to client drift (Table X)
- ACTP successfully maintains inter-class margins while preventing boundary collapse

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Prototype Generation
- **Claim:** Contrastive learning-based generator prevents inter-class decision boundary collapse in heterogeneous environments
- **Mechanism:** ACTP module initializes latent class anchors and passes them through an MLP generator optimized via contrastive loss to produce global prototypes maintaining specific distance margins from other classes
- **Core assumption:** Non-linear generator can synthesize more separable feature space geometry than linear averaging of misaligned client features
- **Evidence anchors:** Abstract mentions "contrastive learning-based trainable server prototype," Section III.B details generator architecture and contrastive loss
- **Break condition:** Adaptive margin threshold ζ set too high forces unnatural separation, too low allows margin shrinkage

### Mechanism 2: Dual Knowledge Distillation
- **Claim:** Fusing classification knowledge (logits) with feature alignment (prototypes) creates more robust global model than either signal alone
- **Mechanism:** Server loss combines KL divergence on aggregated logits with MSE between server feature maps and generated global prototypes
- **Core assumption:** Public proxy data is available and representative enough to transfer both feature-level and output-level knowledge
- **Evidence anchors:** Abstract mentions "dual-knowledge distillation mechanism leverages both client logits and prototype representations," Section III.E provides loss equations
- **Break condition:** Domain-shifted public dataset fails to generalize to client tasks

### Mechanism 3: Sample Prioritization
- **Claim:** Prioritizing public samples based on geometric proximity to class centroids reduces noise during knowledge distillation
- **Mechanism:** Importance weights assigned using inverse L2 distance between sample features and pseudo-label's global prototype
- **Core assumption:** Samples closer to class centroid are high-quality representations, outliers are noisy
- **Evidence anchors:** Abstract mentions "sample prioritization strategy assigns importance weights based on proximity to class prototypes," Section III.D defines importance score
- **Break condition:** High intra-class variance or multi-modal distributions cause valid diverse features to be suppressed

## Foundational Learning

- **Concept:** Prototype-based Federated Learning
  - **Why needed here:** Core unit of communication is prototype (class feature centroid) not model weights; essential to understand margin shrinking problem
  - **Quick check question:** Can you explain why averaging two vectors from different feature spaces might reduce distance between different classes?

- **Concept:** Knowledge Distillation (KD)
  - **Why needed here:** System transfers "soft" knowledge (logits) and feature representations rather than sharing parameters
  - **Quick check question:** How does KL divergence measure difference between student model's predictions and teacher model's soft labels?

- **Concept:** Contrastive Learning
  - **Why needed here:** ACTP module uses contrastive loss to explicitly pull positive pairs closer and push negative pairs apart to enforce margins
  - **Quick check question:** What constitutes "positive" vs "negative" pair for server prototype generator in this context?

## Architecture Onboarding

- **Component map:** Private Data -> Local Train -> Projections -> Prototypes/Logits -> (Upload) -> ACTP/Dual KD -> (Download) -> Local Distillation

- **Critical path:** Learnable Projection Layer ($h_{\phi k}$) is integration point for heterogeneous models; if it fails to map diverse architectures to unified dimension, ACTP generator receives misaligned inputs

- **Design tradeoffs:**
  - Public Data Size: 2,500 samples optimal; more data increases overhead without accuracy gains
  - Local Epochs: Stick to 5 local epochs; excessive training degrades server performance

- **Failure signatures:**
  - Margin Collapse: t-SNE plots showing overlapping class clusters indicates ineffective Adaptive Margin
  - Low Client Accuracy: Local models overfit to global prototypes without balancing local loss

- **First 3 experiments:**
  1. **Sanity Check (Projection Alignment):** Train 2-3 clients with different architectures on IID data; verify projection outputs are dimensionally aligned
  2. **ACTP Validation (Margin Visuals):** Run ACTP on Non-IID subset; plot t-SNE of generated vs averaged prototypes to confirm increased inter-class distance
  3. **Ablation (Dual KD):** Disable prototype-loss or logit-loss separately to measure contribution of each distillation branch

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Effectiveness of adaptive margin mechanism difficult to verify without detailed gradient flow analysis
- Sample prioritization assumes centroid proximity equals sample quality, which may fail for multi-modal distributions
- Projection layer architecture and optimization details are underspecified, creating implementation ambiguity

## Confidence
- **High confidence:** Overall dual-distillation framework and contribution to margin preservation
- **Medium confidence:** ACTP generator's contrastive learning approach, pending ablation studies
- **Low confidence:** Sample prioritization strategy's robustness across diverse data distributions

## Next Checks
1. **Gradient stability audit:** Monitor ACTP generator gradients and margin values (ξc(t)) across training rounds to detect collapse or explosion scenarios
2. **Multi-modal stress test:** Evaluate FedProtoKD on datasets with known multi-modal classes to test sample prioritization robustness
3. **Projection layer ablation:** Systematically vary projection layer architectures and dimensions to quantify impact on prototype quality and final accuracy