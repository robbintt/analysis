---
ver: rpa2
title: Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs
arxiv_id: '2508.17400'
source_url: https://arxiv.org/abs/2508.17400
tags:
- retrieval
- arxiv
- tokens
- performance
- beir
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks retrieval performance across LLM model sizes
  from 125M to 7B parameters, trained on datasets ranging from 1B to 2T tokens. The
  researchers finetune these models on 500k MS MARCO samples and evaluate zero-shot
  retrieval on BEIR tasks.
---

# Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs

## Quick Facts
- arXiv ID: 2508.17400
- Source URL: https://arxiv.org/abs/2508.17400
- Authors: Jacob Portes; Connor Jennings; Erica Ji Yuen; Sasha Doubov; Michael Carbin
- Reference count: 27
- Primary result: Retrieval performance scales predictably with model size, training duration, and FLOPs; for fixed FLOPs, smaller models trained longer perform similarly to larger models trained less.

## Executive Summary
This study benchmarks retrieval performance across LLM model sizes from 125M to 7B parameters, trained on datasets ranging from 1B to 2T tokens. The researchers finetune these models on 500k MS MARCO samples and evaluate zero-shot retrieval on BEIR tasks. They find that retrieval performance scales predictably with model size, training duration, and FLOPs. For fixed FLOPs, small models trained longer perform similarly to larger models trained less. In-Context Learning scores strongly correlate with retrieval performance across tasks. These results demonstrate that decoder-style LLMs are effective retrieval models, especially those trained on trillions of tokens with strong ICL capabilities, and suggest retrieval performance may continue scaling beyond 7B parameters.

## Method Summary
The study finetunes decoder-only transformer models (MPT) ranging from 125M to 7B parameters on a 500k sample subset of MS MARCO passage retrieval dataset. Models are trained with InfoNCE contrastive loss using cosine similarity, with average pooling of final hidden representations. Evaluation is performed zero-shot on the BEIR benchmark using nDCG@10 as the primary metric. The study systematically varies model size and training tokens to analyze scaling trends with FLOPs (model size × training tokens), while also measuring correlation with In-Context Learning capabilities.

## Key Results
- Retrieval performance on zero-shot BEIR tasks scales predictably with LLM size, training duration, and estimated FLOPs
- For fixed FLOPs, small models trained longer perform similarly to larger models trained less
- In-Context Learning scores are strongly correlated with retrieval scores across retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1: Representation Generalization via Compute Scaling
- Claim: Increased pretraining compute (FLOPs) improves the model's ability to generalize representations for retrieval tasks.
- Mechanism: Training on more tokens, or with a larger model, forces the network to learn more robust, transferable token-level representations. When finetuned with contrastive loss (InfoNCE), these representations become better separated in embedding space for query-passage matching.
- Evidence anchors:
  - [abstract] "We find that retrieval performance on zero-shot BEIR tasks predictably scales with LLM size, training duration, and estimated FLOPs."
  - [section] Figure 1A and 1B show that nDCG@10 on BEIR improves with both model size and training duration.
- Break condition: The trend may break if pretraining data quality degrades significantly at scale, or if finetuning data is mismatched to the target retrieval domain.

### Mechanism 2: In-Context Learning (ICL) Correlation with Retrieval
- Claim: Retrieval performance is strongly correlated with a model's in-context learning (ICL) ability.
- Mechanism: ICL requires the model to rapidly adapt to new tasks using only context. This ability is likely underpinned by the same high-quality, generalizable token representations required for effective query-passage similarity scoring in retrieval.
- Evidence anchors:
  - [abstract] "We also show that In-Context Learning scores are strongly correlated with retrieval scores across retrieval tasks."
  - [section] Figure 3 shows a strong positive correlation between ICL scores and BEIR nDCG@10.
- Break condition: The correlation may weaken for retrieval tasks that require specialized knowledge (e.g., scientific or legal domains) not well-covered by the ICL benchmark.

### Mechanism 3: IsoFLOP Performance Equivalence
- Claim: For a fixed compute budget, a smaller model trained longer can achieve similar retrieval performance to a larger model trained shorter.
- Mechanism: Increasing training tokens on a smaller model allows it to see more data patterns, compensating for its lower parameter count. The total FLOPs, which combines model size and training tokens, is the primary driver of the quality of learned representations.
- Evidence anchors:
  - [abstract] "For fixed FLOPs, small models trained longer perform similarly to larger models trained less."
  - [section] Section 3.5 states, "MPT-125M trained for 1.5T tokens... has same average BEIR performance as a MPT-1B model trained for 25.2B tokens."
- Break condition: The equivalence likely breaks at extreme token-to-parameter ratios where the small model becomes data-saturated or cannot memorize necessary patterns.

## Foundational Learning

### Concept: Contrastive Learning (InfoNCE Loss)
- Why needed here: This is the core finetuning method used to convert a general-purpose LLM into a retrieval model by learning to distinguish positive query-passage pairs from negative ones.
- Quick check question: How does the InfoNCE loss function penalize the model for retrieving hard negative passages?

### Concept: Zero-Shot Evaluation
- Why needed here: The study evaluates on BEIR to measure generalization. Understanding this is key to interpreting why the models were only finetuned on MS MARCO and not the BEIR datasets themselves.
- Quick check question: Why is zero-shot evaluation on BEIR a more rigorous test of generalization than reporting performance on MS MARCO?

### Concept: FLOPs as a Scaling Metric
- Why needed here: The paper's central thesis is built on FLOPs as the unifying metric. One must understand how FLOPs combine model size and data volume to follow the scaling arguments.
- Quick check question: If Model A has 2x parameters of Model B, how many more training tokens would Model B need to reach a comparable FLOP count?

## Architecture Onboarding

### Component map
Pretrained MPT checkpoint -> (MS MARCO Finetuning with InfoNCE Loss) -> Pooling -> Embedding Generation -> (BEIR Evaluation via Cosine Similarity)

### Critical path
Pretrained MPT checkpoint → (MS MARCO Finetuning with InfoNCE Loss) → Pooling → Embedding Generation → (BEIR Evaluation via Cosine Similarity)

### Design tradeoffs
- Model Size vs. Inference Latency: The paper shows 7B models perform best, but for production, smaller models (e.g., MPT-125M with extreme pretraining) may offer a better latency-quality tradeoff.
- Finetuning Data vs. Generalization: Finetuning only on MS MARCO provides a clean measure of scaling but likely underperforms SOTA models trained on massive, multi-domain contrastive datasets.
- Sequence Length: The study used a short 128-token limit due to compute, which is a significant handicap for real-world retrieval tasks requiring longer context.

### Failure signatures
- Plateauing Performance: If retrieval performance stops improving despite increasing FLOPs, it may indicate data saturation or a need for architectural changes.
- Low Correlation with ICL: A drop in correlation between ICL and retrieval scores could suggest the pretraining data is becoming less diverse or representative.

### First 3 experiments
1. Establish Baseline: Finetune a standard BERT-base and MPT-125M on the provided 500k MS MARCO subset. Compare their zero-shot BEIR performance to validate the experimental setup.
2. IsoFLOP Replication: Select two model sizes (e.g., 350M and 1B). Train/fine-tune each to two different FLOP targets to reproduce the isoFLOP curve (e.g., 350M with more data vs 1B with less).
3. ICL Correlation Check: Evaluate the pretrained checkpoints on a subset of the MosaicML Evaluation Gauntlet tasks. Plot the ICL scores against the BEIR nDCG@10 scores to verify the claimed correlation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM scaling properties for retrieval hold on newer benchmarks like AIR Bench, LongEmbed, and BRIGHT?
- Basis in paper: [explicit] The authors state in Section 2.2 that they "save the analysis of LLM scaling properties on these benchmarks for future work" after listing these recent datasets.
- Why unresolved: The study exclusively benchmarks performance on the BEIR dataset to establish baseline scaling trends.
- What evidence would resolve it: Replicating the experimental setup (varying model size and pretraining tokens) and evaluating the resulting checkpoints on AIR Bench, LongEmbed, and BRIGHT.

### Open Question 2
- Question: Can precise phenomenological scaling laws be derived for retrieval performance?
- Basis in paper: [explicit] The Discussion notes, "In this study, we don't derive phenomenological scaling laws; rather we report a strong trend. We save formal retrieval scaling laws for future work."
- Why unresolved: The paper visually demonstrates that performance scales with FLOPs but does not offer a parametric equation to predict retrieval scores.
- What evidence would resolve it: Fitting a parametric loss function to the retrieval scores (similar to the Kaplan or Chinchilla laws for language modeling loss) to predict performance based on model size and training tokens.

### Open Question 3
- Question: Does retrieval performance continue to improve significantly for decoder models larger than 7B parameters?
- Basis in paper: [explicit] The Discussion explicitly asks, "Does retrieval performance continue to increase beyond 7B models?" while noting mixed reports on models like RepLlama 13B.
- Why unresolved: The study limits its compute analysis to models ranging from 125M to 7B parameters.
- What evidence would resolve it: Benchmarking 13B and larger models (e.g., 30B, 70B) that have been pretrained on tokens-per-parameter ratios similar to the smaller models in the study.

## Limitations

- The study does not specify the exact composition or quality of the pretraining corpora used for the various MPT checkpoints, which could affect the isoFLOP equivalence claim.
- Generalization beyond BEIR is uncertain, as the benchmark may not be a reliable proxy for all retrieval scenarios, particularly specialized domains.
- Hyperparameter sensitivity is not fully explored, as the paper does not report specific training hyperparameters like learning rate, batch size, or weight decay values.

## Confidence

- **High Confidence:** The core finding that retrieval performance scales predictably with model size and training duration (i.e., FLOPs) is well-supported by the empirical data presented in Figures 1A and 1B. The correlation between ICL and retrieval performance is also clearly demonstrated.
- **Medium Confidence:** The isoFLOP performance equivalence claim is supported by a single comparison example (MPT-125M vs. MPT-1B) and is theoretically sound, but would benefit from more extensive validation across the model size spectrum.
- **Low Confidence:** The prediction that retrieval performance will continue to scale beyond 7B parameters is speculative and extrapolates from a limited range of model sizes without accounting for potential saturation effects or changes in pretraining data dynamics at larger scales.

## Next Checks

1. **IsoFLOP Scaling Curve:** Conduct a more comprehensive study by training multiple model sizes (e.g., 350M, 1B, 3B) to several different FLOP targets. Plot the isoFLOP curves to confirm that the performance equivalence holds across a wider range of compute budgets.

2. **Pretraining Data Ablation:** Replicate the study using models pretrained on corpora of verified, consistent quality. For instance, train two models of the same size on different datasets (e.g., one on a high-quality web corpus and one on a smaller, curated dataset) to the same FLOP count and compare their retrieval performance.

3. **Domain-Specific Generalization:** Evaluate the best-performing model (e.g., 7B trained on 2T tokens) on a retrieval benchmark from a specialized domain (e.g., scientific papers from arXiv or legal documents) not represented in BEIR. This will test the limits of the model's generalization capability and the validity of using BEIR as a universal proxy.