---
ver: rpa2
title: Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due
  to Data Contamination
arxiv_id: '2507.10532'
source_url: https://arxiv.org/abs/2507.10532
tags:
- qwen2
- math-500
- template
- aime2024
- livemathbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of reinforcement learning
  (RL) in enhancing mathematical reasoning in large language models (LLMs), focusing
  on the Qwen2.5 series. The authors hypothesize that Qwen2.5's performance gains
  with spurious rewards may stem from data contamination in training data, rather
  than genuine reasoning ability.
---

# Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination

## Quick Facts
- **arXiv ID**: 2507.10532
- **Source URL**: https://arxiv.org/abs/2507.10532
- **Reference count**: 40
- **Key outcome**: Data contamination causes spurious RL performance gains in Qwen2.5 on MATH-500, while only correct rewards improve reasoning on clean benchmarks.

## Executive Summary
This paper investigates why Qwen2.5 models show performance gains with spurious rewards during reinforcement learning (RL), revealing that data contamination in training data drives these improvements through memorization rather than genuine reasoning. The authors conduct a systematic leakage audit using partial-prompt completion rate and accuracy, finding strong evidence of memorization in Qwen2.5 on benchmarks like MATH-500. To obtain trustworthy evaluations, they introduce a clean benchmark, RandomCalculation, with arbitrary-length arithmetic problems. RL experiments demonstrate that only correct rewards yield steady improvements, while random or incorrect rewards do not. The authors recommend using uncontaminated benchmarks and testing diverse model series for trustworthy conclusions about RL methods.

## Method Summary
The authors systematically audit data contamination in Qwen2.5 models by testing partial-prompt completion rates on MATH-500, then validate RL effectiveness on a synthetically generated clean dataset called RandomCalculation. They implement a leakage audit using truncated prompts (40%, 60%, 80%) and measure memorization via ROUGE-L and exact match scores. For RL validation, they generate nested arithmetic expressions (1-20 steps) and train models using GRPO with correct versus random rewards. The clean dataset contains 700 training and 300 validation examples split into 5-step and 10-step difficulty levels. Performance is measured through accuracy and continuous reward functions sensitive to numerical errors.

## Key Results
- Qwen2.5 models show strong memorization on MATH-500 (high partial-prompt completion rates) indicating data contamination
- Random rewards during RLVR training on clean RandomCalculation data fail to produce stable performance improvements
- Only correct reward signals yield steady improvements on uncontaminated benchmarks
- Spurious rewards trigger memory retrieval rather than genuine reasoning in contaminated models

## Why This Works (Mechanism)
None

## Foundational Learning
- **Data Contamination Detection**: Identifying memorized training data through partial-prompt completion
  - Why needed: To distinguish genuine reasoning from memorized answers
  - Quick check: High ROUGE-L scores on truncated prompts indicate contamination

- **GRPO Training Dynamics**: Understanding how reward signals affect learning trajectories
  - Why needed: To evaluate whether spurious rewards can genuinely improve reasoning
  - Quick check: Random rewards should show unstable or no improvement on clean data

- **Continuous Reward Functions**: Designing sensitive metrics for numerical accuracy
  - Why needed: To provide meaningful feedback during arithmetic reasoning training
  - Quick check: Reward should decrease smoothly with numerical error magnitude

## Architecture Onboarding
- **Component Map**: Qwen2.5 (Base) -> GRPO Optimizer -> Reward Function -> Performance Metrics
- **Critical Path**: Prompt truncation → Model completion → ROUGE/EM calculation → Contamination assessment
- **Design Tradeoffs**: Synthetic clean data vs. real-world complexity; sensitivity of continuous rewards vs. computational cost
- **Failure Signatures**: Memorization manifests as high partial-prompt completion; spurious rewards cause unstable training
- **First Experiments**:
  1. Run partial-prompt completion audit on Qwen2.5 and Llama3.1 using 60% truncated MATH-500 questions
  2. Generate RandomCalculation dataset using recursive expression construction
  3. Train GRPO with correct vs. random rewards and compare stability curves

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Do the findings regarding spurious rewards and memorization hold for complex reasoning domains beyond synthetic arithmetic?
- Basis in paper: [explicit] Appendix A (Discussion and Limitation) states the intent to focus future efforts on "comprehensive evaluation on more diverse benchmarks."
- Why unresolved: The paper establishes the failure of spurious rewards using a clean, synthetic arithmetic dataset (RandomCalculation), but it remains unclear if the interaction between data contamination and RL is identical for domains requiring complex logic or symbolic reasoning rather than calculation.
- Evidence would resolve it: Conducting equivalent RLVR experiments with spurious rewards on novel, uncontaminated benchmarks for logical deduction or code generation.

### Open Question 2
- Question: Is the susceptibility to data contamination and spurious reward gains a universal phenomenon or specific to the Qwen2.5 architecture?
- Basis in paper: [explicit] The Conclusion recommends that future studies "test various model series," and Appendix A explicitly lists "model families" as a dimension for future work.
- Why unresolved: While the paper demonstrates that Llama3.1 behaves differently from Qwen2.5, the study acknowledges it was restricted to a subset of models due to computational resources, leaving the behavior of other architectures unknown.
- Evidence would resolve it: Applying the leakage audit (partial-prompt completion) and the spurious-reward RL protocol to other major model families (e.g., DeepSeek, Gemma, Mistral).

### Open Question 3
- Question: Do other reinforcement learning algorithms exhibit the same "exploitation bias" that triggers memory retrieval with spurious rewards?
- Basis in paper: [explicit] Appendix A notes that given the "rapid development of various RL algorithms," it is infeasible to evaluate them all and lists "reinforcement learning methods" as a focus for future work.
- Why unresolved: The paper's theoretical analysis of "exploitation bias" is specific to the GRPO algorithm (clipped objective); whether PPO, DPO, or other optimizers would similarly trigger memorization in the presence of contaminated data is not tested.
- Evidence would resolve it: Replicating the MATH-500 and RandomCalculation experiments using alternative RL algorithms (e.g., PPO, ReMax) to observe if random rewards yield similar performance surges on contaminated data.

## Limitations
- Findings are primarily demonstrated on arithmetic reasoning tasks and may not generalize to complex reasoning domains
- Study focuses on Qwen2.5 and Llama3.1 models, leaving behavior of other architectures unknown
- RandomCalculation benchmark, while clean, may not capture full complexity of real-world mathematical problem-solving

## Confidence
- **High confidence**: Core claims about data contamination affecting Qwen2.5 models on MATH-500 are robustly demonstrated through strong memorization signals and failed random reward experiments
- **Medium confidence**: Generalization to other model families and RL methods requires further validation
- **Low confidence**: RandomCalculation's effectiveness as a universal clean benchmark for all reasoning tasks

## Next Checks
1. Cross-model verification: Replicate the contamination audit on additional model series (Mistral, DeepSeek) to test whether Qwen2.5's contamination is unique or indicative of broader industry practices
2. Reward signal robustness: Test alternative reward functions on RandomCalculation, including sparse rewards and reward shaping variations, to confirm that continuous reward is optimal and that random rewards consistently fail to improve performance
3. Benchmark transferability: Apply the partial-prompt completion methodology to non-mathematical benchmarks (code generation, reasoning tasks) to determine if contamination detection generalizes beyond arithmetic problems