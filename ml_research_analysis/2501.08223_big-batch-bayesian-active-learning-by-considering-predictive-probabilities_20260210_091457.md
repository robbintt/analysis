---
ver: rpa2
title: Big Batch Bayesian Active Learning by Considering Predictive Probabilities
arxiv_id: '2501.08223'
source_url: https://arxiv.org/abs/2501.08223
tags:
- batch
- learning
- bayesian
- batchbald
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in BatchBALD, a popular batch acquisition
  function for Bayesian active learning in classification. BatchBALD suffers from
  conflating epistemic and aleatoric uncertainty, leading to suboptimal performance
  and computational inefficiency.
---

# Big Batch Bayesian Active Learning by Considering Predictive Probabilities

## Quick Facts
- arXiv ID: 2501.08223
- Source URL: https://arxiv.org/abs/2501.08223
- Reference count: 18
- Outperforms BatchBALD in accuracy and runtime for larger batch sizes

## Executive Summary
This paper addresses limitations in BatchBALD, a popular batch acquisition function for Bayesian active learning in classification. BatchBALD suffers from conflating epistemic and aleatoric uncertainty, leading to suboptimal performance and computational inefficiency. The authors propose a new acquisition function, BBB-AL (Big Batch Bayesian Active Learning), that focuses on predictive probabilities to target only epistemic uncertainty. This is achieved by maximizing the joint differential entropy of predictive probabilities over the batch. The method avoids the combinatorial explosion of BatchBALD by operating in the continuous space of probabilities.

## Method Summary
The BBB-AL acquisition function maximizes the joint differential entropy of predictive probabilities over the batch: a_BBB = Σ_{c=0}^{C-1} log det(C^c_p), where C^c_p is the B×B covariance matrix of probabilities for class c. This formulation targets only epistemic uncertainty by focusing on the predictive probabilities themselves rather than their distribution. For general models, a sampling-based approach is used where S posterior samples are drawn to estimate covariances, with Ledoit-Wolf shrinkage applied to ensure positive definiteness. The greedy optimization has O(|Dpool| × B²) complexity, avoiding BatchBALD's combinatorial scaling.

## Key Results
- BBB-AL outperforms BatchBALD in both accuracy and runtime, especially for larger batch sizes (B=10, 50)
- Achieves slightly better results for single-point acquisition (B=1)
- Enables larger batch sizes with faster acquisition times, making it more practical for real-world applications
- Experiments conducted on CIFAR-10 with ResNet-8 architecture

## Why This Works (Mechanism)
The core insight is that BatchBALD's entropy formulation conflates epistemic uncertainty (what we don't know about the model) with aleatoric uncertainty (inherent noise in the data). By focusing directly on the predictive probabilities through their joint differential entropy, BBB-AL isolates the epistemic component. This is achieved by computing covariances between predictive probabilities across the batch, which captures how uncertain we are about the model's predictions rather than the inherent variability in the data.

## Foundational Learning
- Bayesian active learning: Needed for understanding the problem space; quick check: can you explain the difference between aleatoric and epistemic uncertainty?
- Joint differential entropy: Core mathematical concept; quick check: can you derive the entropy formula for multivariate Gaussian distributions?
- Covariance estimation: Critical for BBB-AL; quick check: can you explain why positive definiteness is important for covariance matrices?
- Ledoit-Wolf shrinkage: Numerical stabilization technique; quick check: can you explain when shrinkage estimators are necessary?

## Architecture Onboarding

**Component Map**
Data pool → Model posterior samples → Predictive probability covariances → Log-determinant computation → Batch acquisition

**Critical Path**
1. Sample model posteriors to obtain predictive probabilities
2. Compute class-wise covariance matrices C^c_p
3. Apply Ledoit-Wolf shrinkage if necessary
4. Calculate joint differential entropy via log det sum
5. Greedy optimization for batch selection

**Design Tradeoffs**
- Sampling-based vs. exact computation: Sampling enables application to any model but introduces variance
- Greedy optimization vs. global search: Greedy provides tractable O(|Dpool| × B²) complexity vs. exponential scaling
- Covariance estimation: Ledoit-Wolf shrinkage trades bias for variance reduction and positive definiteness

**Failure Signatures**
- Degenerate covariance matrix when B ≥ S (batch size exceeds sample count)
- BatchBALD selects similar points despite batch acquisition (diagonal penalty issue)
- Computational bottleneck at acquisition time for very large pool sizes

**First Experiments**
1. Verify that BBB-AL selects diverse points by checking the acquisition landscape
2. Compare acquisition times for B=10 vs. B=50 to confirm runtime improvements
3. Test that covariance matrices are positive definite after shrinkage application

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Performance comparison limited to BatchBALD baseline only
- Experiments conducted only on CIFAR-10 dataset with ResNet-8 architecture
- Batch size limited to 50 points maximum in experiments

## Confidence
High: Method is mathematically sound, results are clearly presented, and implementation details are sufficient for reproduction.
Medium: Limited comparison to other acquisition functions and datasets.
Low: None identified.

## Next Checks
1. Verify that BBB-AL consistently outperforms BatchBALD across different random seeds
2. Test BBB-AL on additional datasets beyond CIFAR-10 to confirm generalization
3. Measure the impact of varying the number of posterior samples S on acquisition quality and runtime