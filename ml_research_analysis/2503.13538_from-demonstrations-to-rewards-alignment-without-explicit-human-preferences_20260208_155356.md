---
ver: rpa2
title: 'From Demonstrations to Rewards: Alignment Without Explicit Human Preferences'
arxiv_id: '2503.13538'
source_url: https://arxiv.org/abs/2503.13538
tags:
- reward
- policy
- learning
- data
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an inverse reinforcement learning (IRL) framework
  for aligning language models using only demonstration data, without explicit human
  preference labels. The method learns a reward model from demonstrations such that
  the corresponding optimal policy maximizes the likelihood of the demonstration data.
---

# From Demonstrations to Rewards: Alignment Without Explicit Human Preferences

## Quick Facts
- arXiv ID: 2503.13538
- Source URL: https://arxiv.org/abs/2503.13538
- Reference count: 40
- Primary result: Demonstration-only IRL framework achieves better alignment than SFT and SPIN baselines

## Executive Summary
This paper introduces an inverse reinforcement learning (IRL) framework for aligning language models using only demonstration data, without explicit human preference labels. The method learns a reward model from demonstrations such that the corresponding optimal policy maximizes the likelihood of the demonstration data. The authors formulate this as a bi-level optimization problem, alternating between policy alignment (via PPO or similar) and reward alignment (via gradient-based updates). Experiments on the TL;DR and UltraChat datasets show that the proposed approach outperforms standard supervised fine-tuning and the demonstration-only baseline SPIN, achieving higher reward accuracy, better win rates on human evaluations, and stronger performance on the Open LLM Leaderboard and MT-Bench.

## Method Summary
The method uses a bi-level optimization framework where a reward model is learned from demonstration data, and then a policy is optimized to maximize expected reward under KL regularization. The process alternates between policy alignment (updating the policy using PPO or Online-DPO) and reward alignment (updating the reward model using synthetic preference pairs constructed from demonstrations and current model outputs). The reward model is trained to assign higher scores to demonstration responses compared to model-generated responses, creating a synthetic preference signal. The policy is updated to maximize the expected reward while staying close to a reference model via KL regularization.

## Key Results
- IRL framework outperforms standard supervised fine-tuning and SPIN baseline on TL;DR and UltraChat datasets
- Achieves higher reward accuracy on held-out preference data compared to demonstration-only methods
- Better win rates against demonstrations on human evaluations using GPT-4o as judge
- Strong performance on Open LLM Leaderboard and MT-Bench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The system extracts implicit preference signals from demonstration data by treating alignment as a bi-level optimization problem.
- **Mechanism:** The framework formulates Inverse Reinforcement Learning (IRL) to find a reward model $r_\theta$ such that its corresponding optimal policy $\pi^*$ maximizes the likelihood of the demonstration dataset $\mathcal{D}$. This relies on the assumption that the expert policy generating the demonstrations is optimal under some latent reward function.
- **Core assumption:** Demonstration data contains sufficient information to approximate the expert's latent reward function via maximum likelihood estimation (Assumption 1, Lemma 1).
- **Evidence anchors:**
  - [abstract] ("we directly learn the reward model from demonstration data")
  - [section 4.1] (Equation 5 defines the bi-level formulation)
  - [corpus] (Neighbor paper "Learning from Failures" supports Inverse RL as a viable method for extracting latent incentives from behavior)
- **Break condition:** Fails if the demonstration dataset is too small or low quality to statistically approximate the expert distribution, as the surrogate objective $\hat{L}$ would diverge from the true likelihood $L$.

### Mechanism 2
- **Claim:** Reward learning is driven by the contrast between expert demonstrations and current model rollouts.
- **Mechanism:** The gradient of the surrogate objective (Eq. 10) updates the reward model by pushing the reward of demonstration samples up while pushing the reward of model-generated samples down. This creates a synthetic preference signal where demonstrations are "preferred" over current model outputs.
- **Core assumption:** The current policy $\pi_k$ provides a meaningful baseline (negative sample) to contrast against the static demonstration data.
- **Evidence anchors:**
  - [section 5] ("intuitively... the algorithm will find the reward update direction that increases the gap")
  - [algorithm 1] (Reward Alignment step minimizes loss based on $y \sim \mathcal{D}$ vs $y' \sim \pi$)
  - [corpus] (Weak direct link; neighbor papers focus on explicit preference aggregation rather than synthetic contrast)
- **Break condition:** If the policy collapses or mode-collapses early, the generated samples $y'$ will not provide a useful gradient signal to distinguish rewards, stalling learning.

### Mechanism 3
- **Claim:** Alternating policy and reward optimization stabilizes the alignment process without explicit human labels.
- **Mechanism:** The architecture iterates between a "Policy Alignment" step (fixing reward, updating policy via PPO/DPO to maximize Eq. 3) and a "Reward Alignment" step (fixing policy, updating reward via synthetic pairs). This allows the model to bootstrap performance using its own improving generation quality as a discriminator.
- **Core assumption:** KL-regularization (Eq. 3) effectively prevents the policy from drifting too far from the reference model $\pi_{ref}$ during these updates.
- **Evidence anchors:**
  - [section 5] (Describes the two-step training loop)
  - [section 3] (Defines the KL constraint in Eq. 3)
  - [corpus] (Neighbor "Towards Reward Fairness" discusses reward biases in RLHF, relevant to the risks of iterative reward updates)
- **Break condition:** If the KL penalty $\beta$ is too low, the policy may over-optimize the learned reward (reward hacking), or if too high, the policy may not move enough to generate informative negative samples for the reward model.

## Foundational Learning

- **Concept:** Inverse Reinforcement Learning (IRL)
  - **Why needed here:** Unlike standard RLHF which learns a reward from *rankings*, this method learns a reward from *behavior* (demonstrations). You must understand that IRL reverses the standard RL flow: instead of Policy <- Reward, it is Reward <- Expert Policy.
  - **Quick check question:** How does the gradient in Equation 10 differ from the standard reward modeling loss in Equation 2?

- **Concept:** Bi-level Optimization
  - **Why needed here:** The core contribution is treating alignment as a nested problem (Eq 5a and 5b). The upper level optimizes the reward; the lower level optimizes the policy based on that reward. Understanding this dependency is crucial for debugging convergence.
  - **Quick check question:** In the proposed algorithm, does the reward model update depend on the policy's current state, or is it purely offline?

- **Concept:** KL-Divergence Regularization
  - **Why needed here:** The paper relies on a "reference model" (usually the SFT checkpoint) to keep the policy grounded. This constraint is mathematically essential for the soft policy iteration formula (Eq 9) used in the theoretical derivation.
  - **Quick check question:** What happens to the optimal policy $\pi^*$ in Equation 9 if the reward $r(x,y)$ is uniformly zero?

## Architecture Onboarding

- **Component map:** Dataset $\mathcal{D}$ -> Reference Model $\pi_{ref}$ -> Policy Model $\pi$ -> Reward Model $r_\theta$

- **Critical path:**
  1. Initialize Policy and Reward models from SFT checkpoint.
  2. **Rollout:** Generate synthetic responses $y'$ using current Policy.
  3. **Synthetic Pairing:** Pair demonstrations $y$ (as "chosen") with rollouts $y'$ (as "rejected").
  4. **Reward Update:** Train Reward Model on these synthetic pairs (Binary Cross Entropy).
  5. **Policy Update:** Use Reward Model scores to update Policy (PPO/REINFORCE/DPO).
  6. Repeat for $K$ iterations.

- **Design tradeoffs:**
  - **PPO vs. DPO for Policy Step:** The paper notes PPO is standard but memory-heavy; Online DPO is presented as a more stable, memory-efficient alternative for the UltraChat experiments.
  - **Explicit vs. Implicit Reward:** The authors argue explicit parameterization (separate reward model) outperforms implicit methods like SPIN (which uses the policy logits as rewards) by allowing more flexible reward shaping.

- **Failure signatures:**
  - **Reward Overfitting:** High accuracy on synthetic training pairs but low win-rates against ground truth (GPT-4o evaluations). Monitor Figure 2a vs 2c.
  - **Distribution Shift:** If the policy improves rapidly, the synthetic "rejected" samples ($y'$) become too different from the demonstrations, potentially destabilizing the reward model gradient.

- **First 3 experiments:**
  1. **SFT Baseline:** Train $\pi_{ref}$ on $\mathcal{D}$ using standard behavior cloning (Eq 1) to establish a baseline win rate.
  2. **Single-Step IRL:** Run one iteration of Algorithm 1. Compare the Reward Model's accuracy on a *held-out* preference dataset (not synthetic) to verify the "demonstration-to-preference" transfer hypothesis.
  3. **Iterative Evaluation:** Run for 5-7 iterations (as in Figure 2) and plot "Reward Score" vs. "GPT-4o Win Rate" to ensure the proxy reward is actually aligning with external quality metrics.

## Open Questions the Paper Calls Out
- **Question:** How can this IRL-based method be integrated with the current RLHF pipeline to achieve better flexibility and performance?
  - **Basis in paper:** [explicit] The conclusion explicitly states, "As a future direction, we aim to integrate our proposed IRL-based methods with the current RLHF pipeline to achieve better flexibility and performance."
  - **Why unresolved:** The current work isolates the method to demonstration-only settings to prove it can match or beat baselines like SPIN, leaving hybrid approaches unexplored.
  - **What evidence would resolve it:** A study showing that combining this IRL framework with explicit preference data outperforms standard RLHF or demonstration-only IRL on standard benchmarks.

## Limitations
- Limited scalability due to small dataset sizes (10K demonstrations) and few iterations (K=2 for UltraChat)
- Synthetic preference generation mechanism untested against human-annotated preferences
- Diminishing returns observed after 4-5 iterations suggest potential hard limits on effectiveness
- Comparison with SPIN focuses on code rather than directly comparable implicit-reward methods

## Confidence
- **High Confidence:** The mathematical formulation of the bi-level IRL optimization problem (Section 4.1) and the KL-regularized policy objective (Section 3) are clearly derived and internally consistent.
- **Medium Confidence:** The experimental results showing IRL outperforming SFT and SPIN on TL;DR and UltraChat are methodologically sound, but the transfer to larger models and different domains remains unproven.
- **Low Confidence:** The claim that demonstration data contains "sufficient" preference information for alignment is overstated given the limited dataset sizes and the fact that performance gains over SFT are incremental rather than transformative.

## Next Checks
1. **Scaling Test:** Apply the IRL framework to a larger, more diverse dataset (e.g., 100K+ demonstrations from multiple domains) and evaluate whether the bi-level optimization still converges and maintains performance gains.
2. **Human Preference Validation:** Compare the IRL-learned reward model's predictions against a held-out set of human-annotated preferences to measure the gap between synthetic and real preference signals.
3. **Implicit vs. Explicit Reward Ablation:** Implement a direct comparison between IRL (explicit reward) and a method like SPIN but with the full PPO optimization loop (implicit reward) on the same codebase and datasets to isolate the benefit of explicit reward modeling.