---
ver: rpa2
title: Is RL fine-tuning harder than regression? A PDE learning approach for diffusion
  models
arxiv_id: '2509.02528'
source_url: https://arxiv.org/abs/2509.02528
tags:
- have
- lemma
- page
- function
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of fine-tuning diffusion models
  using reinforcement learning, addressing the gap between theoretical guarantees
  in supervised learning versus reinforcement learning. The core method introduces
  a variational inequality framework based on the Hamilton-Jacobi-Bellman equations,
  enabling learning through supervised regression instead of traditional RL approaches.
---

# Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models

## Quick Facts
- arXiv ID: 2509.02528
- Source URL: https://arxiv.org/abs/2509.02528
- Reference count: 40
- Primary result: Introduces variational inequality framework enabling RL fine-tuning through supervised regression, achieving faster convergence than standard regression via self-mitigating error phenomenon

## Executive Summary
This paper bridges the gap between supervised learning and reinforcement learning by showing that fine-tuning diffusion models via RL can achieve statistical guarantees comparable to regression. The key insight is that the optimal control problem inherent in RL fine-tuning can be linearized via a Cole-Hopf transformation, converting the non-linear Hamilton-Jacobi-Bellman equation into a linear parabolic PDE. This enables solving the RL problem through supervised regression rather than traditional RL approaches, with theoretical guarantees showing faster convergence than standard regression due to a self-mitigating error phenomenon.

## Method Summary
The method transforms the RL fine-tuning problem into a supervised regression task through three key steps: (1) Linearizing the HJB equation using Cole-Hopf transform to convert the control problem into a linear PDE, (2) Solving a variational inequality in Sobolev spaces that provides a weak formulation of the PDE, and (3) Iteratively solving regression problems at each step to converge to the optimal value function. The approach works by learning the transformed value function f through a convex optimization problem that enforces the PDE constraints in a variational sense, then extracting the policy via the gradient of the log-transformed function. The empirical bilinear form is computed using sampled trajectories from the uncontrolled diffusion, with automatic differentiation handling the necessary derivatives.

## Key Results
- Sharp statistical rates for value function and policy estimation that improve upon standard regression rates
- Self-mitigating error phenomenon where effective noise scales with approximation error, creating positive feedback
- Exponential convergence of the iterative algorithm to the statistical neighborhood
- Oracle inequalities comparable to supervised learning while maintaining sample efficiency

## Why This Works (Mechanism)

### Mechanism 1: Linearization of the Hamilton-Jacobi-Bellman (HJB) Equation via Cole-Hopf Transform
The paper claims that the non-linear optimal control problem inherent in RL fine-tuning can be reduced to a linear supervised regression problem. A Cole-Hopf transformation $f_t(x) = \exp(v_t(x)/\alpha)$ is applied to the HJB equation, converting the non-linear term $\max_a \{\langle a, \nabla v \rangle - \frac{1}{2\alpha}a^\top\Lambda^{-1}a\}$ into a linear term $\frac{\alpha}{2}(\nabla v)^\top\Lambda\nabla v$ after transformation, resulting in a linear parabolic PDE. This relies on the specific structure of control-affine systems with quadratic regularization and the assumption that $\alpha > 0$.

### Mechanism 2: Variational Inequality (VI) Formulation in Sobolev Spaces
The paper proposes learning the value function by solving a variational inequality defined by a bilinear form, rather than solving the PDE directly or using Bellman residuals. Instead of enforcing the PDE pointwise, the method enforces a weak formulation integrated over the trajectory distribution, defining an inner product structure that captures both spatial and temporal regularity. The learning objective becomes finding $\hat{f} \in F$ such that the error is orthogonal to the function class. This relies on Uniform Ellipticity to ensure the bilinear form is positive-definite, providing stable geometry for convergence.

### Mechanism 3: Self-Mitigating Statistical Error
The theoretical convergence rate is faster than standard regression because the effective noise level in the critical radius scales with the approximation error. In Theorem 1, the "noise level" term depends on the approximation error itself. As the function class approximation improves (approximation error decreases), the effective noise decreases, creating a positive feedback loop that accelerates convergence beyond standard $O(1/\sqrt{n})$ rates in specific regimes. This requires the function class $F$ to be convex and sufficiently rich.

## Foundational Learning

- **Stochastic Optimal Control & HJB Equations**: Understanding how value functions satisfy PDEs (HJB) is prerequisite to understanding the solution method. Can you explain why the max operator in the HJB equation creates non-linearity that standard regression cannot handle directly?

- **Weak Formulations & Sobolev Spaces**: The algorithm relies on minimizing a "variational inequality" (weak form) in a Hilbert space defined by Sobolev norms. You must understand $H^1$ norms (function + gradient) to implement the loss function. How does a weak formulation (integral form) differ from a strong formulation (point-wise PDE), and why does it handle discrete samples better?

- **Empirical Process Theory (Covering Numbers)**: The theoretical guarantees rely on "critical radii" defined by metric entropy (covering numbers). Understanding these concepts is necessary to interpret the convergence bounds. What does the "local Rademacher complexity" or "covering number" tell us about the capacity of the function class $F$ relative to the sample size $n$?

## Architecture Onboarding

- **Component map**: Inputs (trajectories, rewards, terminal values) -> Cole-Hopf Transform -> Bilinear Form Estimator -> Iterative Solver -> Policy Extraction

- **Critical path**: 1. Construct the empirical bilinear form $\hat{B}_n$ (Eq 17). 2. Initialize $\hat{f}^{(0)}$. 3. Run iterative loop: Update $\hat{f}$ by minimizing $\hat{E}_n(f - f^{(m)}, f - f^{(m)}) - 2\gamma \hat{B}_n(\dots)$ (Algorithm 2, Eq 2). 4. Extract policy: $\hat{\pi}_t(x) = \Lambda_t(x) \nabla \log \hat{f}_t(x)$.

- **Design tradeoffs**: ODE vs. SDE requires Uniform Ellipticity (Assumption UE), implying stochastic diffusion; pure ODE samplers break this assumption. Convexity analysis assumes convex function class $F$; using non-convex NNs practically works but loses theoretical guarantees. Discretization uses discrete observations $K$; as $K$ decreases, sampling error $\rho_{smpl}$ dominates.

- **Failure signatures**: Exploding Gradients from Cole-Hopf transform exponentials if rewards aren't scaled correctly. Non-convergence if diffusion matrix $\Lambda$ is degenerate or network capacity is too low, causing Algorithm 2 to oscillate.

- **First 3 experiments**: 1. Validation on Linear Quadratic Regulator (LQR) where ground truth HJB solution is known. 2. Sensitivity to $K$ (Time Discretization) to verify $\rho_{smpl}$ scales as predicted. 3. Ablation on Uniform Ellipticity comparing deterministic sampler (ODE) vs. SDE to confirm performance degradation.

## Open Questions the Paper Calls Out

1. Can the variational inequality framework and its theoretical guarantees be extended to non-convex function classes such as neural networks? [explicit] Page 12 states: "It is an interesting direction of future work to extend the variational problem and our analysis to non-convex classes, such as neural networks."

2. Can the self-mitigating error phenomenon and oracle inequalities be extended to general controlled Markov diffusions beyond the fine-tuning setting? [explicit] Page 35 states: "We conjecture that the insights gained from our analysis could be extended to general controlled Markov diffusions beyond the fine-tuning setting."

3. How can the techniques be adapted to fine-tuning of auto-regressive language models? [explicit] Page 36 states: "it is important to investigate how the techniques developed in this paper can be adapted to the fine-tuning of auto-regressive language models."

4. Can the uniform ellipticity assumption be relaxed to cover ODE-based diffusion models? [inferred] Page 10 states the uniform ellipticity assumption is "satisfied by most SDE models, but not by ODE models," yet all theoretical results require this assumption.

## Limitations

- The Cole-Hopf transformation requires careful reward scaling to prevent exploding gradients from exponential operations, which could be problematic in high-dimensional settings.
- The theoretical guarantees rely heavily on convexity of the function class, which may not hold in realistic neural network implementations, potentially limiting the self-mitigating error phenomenon's effectiveness.
- The statistical efficiency claims assume the function class is sufficiently rich to capture the optimal value function while maintaining bounded complexity, which may be difficult to achieve in high-dimensional settings.

## Confidence

**High Confidence**: The theoretical framework connecting HJB equations to variational inequalities through Cole-Hopf transformation is mathematically sound. The linear PDE structure and weak formulation are well-established in the PDE control literature.

**Medium Confidence**: The self-mitigating error phenomenon and its resulting statistical rates are theoretically rigorous but may be sensitive to practical implementation details. The convexity assumptions may not hold in realistic neural network implementations.

**Low Confidence**: The practical implementation details for handling the Cole-Hopf transformation with high-dimensional data, particularly the numerical stability of exponential operations and the effective approximation of the time derivative, are not fully specified.

## Next Checks

1. **LQR Benchmark Validation**: Implement the algorithm on a Linear Quadratic Regulator problem where the ground truth HJB solution is known analytically. Verify that the learned value function converges to the theoretical optimum and that the self-mitigating error phenomenon manifests as predicted.

2. **High-Dimensional Scaling Study**: Test the algorithm on a controlled diffusion problem with increasing dimensionality (d=2, 5, 10, 20) to empirically validate the statistical rates and identify at which dimensionality the theoretical advantages diminish due to curse-of-dimensionality effects.

3. **ODE vs SDE Comparison**: Systematically compare performance when using deterministic ODE-based diffusion models versus stochastic SDE-based models to quantify the impact of the Uniform Ellipticity assumption on both convergence speed and final policy quality.