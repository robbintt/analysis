---
ver: rpa2
title: 'Demo: Generative AI helps Radiotherapy Planning with User Preference'
arxiv_id: '2512.08996'
source_url: https://arxiv.org/abs/2512.08996
tags:
- dose
- mean
- planning
- prediction
- rapidplan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel two-stage training framework for radiotherapy
  dose prediction that incorporates user preferences through interactive sliders.
  The approach uses a foundational dose decoder pretraining stage followed by a flexible
  dose prediction stage with multi-conditional inputs including user-defined trade-offs
  between organs-at-risk (OARs) and planning target volumes (PTVs).
---

# Demo: Generative AI helps Radiotherapy Planning with User Preference

## Quick Facts
- arXiv ID: 2512.08996
- Source URL: https://arxiv.org/abs/2512.08996
- Reference count: 40
- This paper presents a novel two-stage training framework for radiotherapy dose prediction that incorporates user preferences through interactive sliders.

## Executive Summary
This paper introduces a two-stage training framework for radiotherapy dose prediction that incorporates user preferences through interactive sliders. The approach uses a foundational dose decoder pretraining stage followed by a flexible dose prediction stage with multi-conditional inputs including user-defined trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs). The method was validated on head-and-neck cancer cases and compared against Varian RapidPlan. Results showed superior performance in both intra-patient and inter-patient scenarios with lower standard deviations in dose-volume histogram (DVH) differences. The interactive slider interface enabled real-time customization of dose predictions, and the predicted doses were successfully translated into optimization objectives for clinical treatment planning systems.

## Method Summary
The method employs a two-stage training framework. Stage I pretrains a VQ-VAE on 31K dose distributions to learn a realistic latent dose space, which is then frozen and reused in Stage II. Stage II trains an image encoder that maps patient anatomy and user preferences into this pre-regularized latent space, with user preferences for OAR sparing vs. PTV homogeneity injected via adaptive instance normalization (AdaIN). The predicted 3D dose distributions are converted to DVH-derived optimization objectives for Eclipse TPS. The framework was trained on 820 head-and-neck cases and evaluated on 114 test cases.

## Key Results
- Superior performance in intra-patient and inter-patient scenarios with lower standard deviations in DVH differences compared to Varian RapidPlan
- Better OAR sparing (14 better vs 0 worse cases) and PTV homogeneity/conformity (1 better vs 0 worse cases)
- Real-time dose prediction with ~30ms inference time and interactive slider customization

## Why This Works (Mechanism)

### Mechanism 1
A two-stage training framework with foundational dose decoder pretraining stabilizes dose prediction under complex multi-conditional inputs. Stage I trains a VQ-VAE on 31K dose distributions to learn a realistic latent dose space. This pretrained decoder is frozen and reused in Stage II, where an image encoder learns to map patient anatomy and user preferences into this pre-regularized latent space. The uniformity loss in Stage I encourages well-distributed latent representations. The latent space learned from large-scale dose distributions contains realistic dose patterns that transfer to patient-specific prediction.

### Mechanism 2
User preferences for OAR sparing vs. PTV homogeneity can be injected via adaptive instance normalization (AdaIN) and learned through objective-specific loss terms. Sliding bar values $\tilde{h}$ (homogeneity preference) and $\tilde{w}$ (OAR sparing preference) are randomly sampled during training and encoded via AdaIN into the image encoder. The $L_{obj}$ loss penalizes mismatches between user-specified preferences and predicted dose metrics (HI, PTV mean dose, OAR mean doses). This creates a conditional mapping from preference space to dose space. The preference space can be meaningfully disentangled into orthogonal axes (OAR sparing vs. PTV homogeneity) that map linearly to dose metrics.

### Mechanism 3
Predicted 3D dose distributions can be translated into deliverable plans via DVH-derived optimization objectives. The 3D dose prediction is converted to DVH point-objectives (sampled at specific volume percentages) and mean dose objectives for each OAR, with rule-based margins added. These objectives are fed into Eclipse TPS for inverse planning optimization. DVH-derived objectives with margins sufficiently capture the spatial information in the 3D prediction for practical optimization.

## Foundational Learning

- **VQ-VAE (Vector Quantized Variational Autoencoder):**
  - Why needed here: Stage I uses VQ-VAE to learn a discrete latent codebook of dose distributions, providing regularization and realistic dose reconstruction.
  - Quick check question: Can you explain how the codebook matching in VQ-VAE differs from continuous latent spaces in standard VAEs?

- **Adaptive Instance Normalization (AdaIN):**
  - Why needed here: AdaIN injects user preference embeddings into the encoder's feature maps, enabling conditional generation without modifying network architecture.
  - Quick check question: How does AdaIN differ from standard conditional normalization (e.g., FiLM) in terms of style-content separation?

- **Dose-Volume Histograms (DVHs):**
  - Why needed here: DVHs are the primary evaluation and optimization metric; understanding their cumulative nature and clinical interpretation is essential.
  - Quick check question: If an OAR's DVH curve shifts leftward after replanning, what does this indicate about mean dose and maximum dose?

## Architecture Onboarding

- **Component map:**
  Stage I (Pretrain Only): CT + Structures → Dose Encoder → VQ Codebook → Dose Decoder → Reconstructed Dose → [Frozen for Stage II]
  Stage II (Flexible Prediction): CT + Structures → Image Encoder (MedNext-style) → Latent → Pretrained Decoder → Predicted Dose; User Sliders + Beam Plates → AdaIN conditioning

- **Critical path:**
  1. Pretrain Stage I VQ-VAE on 31K dose distributions to learn realistic dose patterns
  2. Freeze decoder; train Stage II encoder with multi-channel input (CT + PTV masks + OAR masks)
  3. During Stage II training, randomly sample slider values and compute $L_{obj}$ against reference plan metrics
  4. At inference, user adjusts sliders; model forward pass takes ~30ms, visualization ~1.5s

- **Design tradeoffs:**
  - GAN-based one-step generation vs. diffusion: Chose GAN for speed (30ms inference) over diffusion's iterative sampling
  - DVH-derived objectives vs. direct 3D optimization: Chose DVH conversion for Eclipse TPS compatibility; loses some spatial detail
  - Head-and-neck only in evaluation: Chose most challenging site; generalization to other sites assumed straightforward but not yet validated

- **Failure signatures:**
  - Boundary artifacts at PTV/OAR edges (indicates Stage I pretraining was skipped or undertrained)
  - Slider adjustments produce no DVH change (indicates $L_{obj}$ loss weight too low or AdaIN conditioning failed)
  - Achieved DVH differs significantly from predicted DVH (indicates margin rules in objective conversion are insufficient)

- **First 3 experiments:**
  1. Ablation of Stage I: Train Stage II from scratch (no pretrained decoder); compare MAE and visual artifacts on held-out test set
  2. Preference sensitivity test: For a fixed patient, sweep slider values and plot resulting DVH curves; verify monotonic response to preference changes
  3. Objective conversion validation: For 10 patients, compare AI-predicted DVH vs. Eclipse-achieved DVH; identify structures with largest deviations and adjust margin rules

## Open Questions the Paper Calls Out
- How well does the flexible dose proposer generalize to treatment sites beyond head-and-neck cancer? The authors note their current evaluation focuses on head-and-neck cancer site and extending to other treatment sites is expected to be straightforward but is part of future work.
- Can the rule-based translation from 3D dose predictions to optimization objectives be improved through learned approaches? The dose-to-objectives conversion uses "rule-based margins" and heuristics, raising questions about whether a learned mapper could better preserve prediction fidelity.
- How robust are the interactive slider preferences across diverse institutional planning styles and clinical protocols? The authors note clinical quality comparison is "inherently complex and subjective" and plan "more rigorous comparisons... in diverse clinical scenarios."

## Limitations
- Generalization beyond head-and-neck cases is assumed but not empirically tested; performance on other anatomical sites remains unknown
- DVH-derived objectives may lose spatial dose information compared to direct 3D optimization, potentially limiting plan quality
- Preference disentanglement into orthogonal OAR/PTV axes may not hold in practice; correlated objectives could lead to unrealistic dose compromises

## Confidence
- High confidence: Stage I pretraining improves dose prediction quality (ablation shows clear performance degradation without pretraining)
- Medium confidence: User preference injection via AdaIN and L_obj loss effectively controls dose predictions (novel mechanism, no direct comparisons available)
- Low confidence: Predicted doses translate seamlessly to clinically deliverable plans (DVH conversion is heuristic, not theoretically grounded)

## Next Checks
1. Test model generalization on non-head-and-neck sites (lung, prostate) to validate anatomical transferability
2. Compare DVH-to-objective conversion with direct 3D dose optimization (if available) to quantify spatial information loss
3. Conduct user study with multiple planners adjusting preferences to evaluate consistency and practical utility of the slider interface