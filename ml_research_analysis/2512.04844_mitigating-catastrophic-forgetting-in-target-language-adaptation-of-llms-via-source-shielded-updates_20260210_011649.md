---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via
  Source-Shielded Updates
arxiv_id: '2512.04844'
source_url: https://arxiv.org/abs/2512.04844
tags:
- language
- source
- target
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in adapting instruct
  LLMs to new languages using only unlabeled target language data. The core method,
  Source-Shielded Updates (SSU), proactively identifies and freezes source-critical
  parameters before adaptation, using importance scores derived from a small set of
  source data.
---

# Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates

## Quick Facts
- **arXiv ID:** 2512.04844
- **Source URL:** https://arxiv.org/abs/2512.04844
- **Reference count:** 40
- **Primary result:** SSU significantly reduces source language performance drops to 3.4% (7B) and 2.8% (13B), compared to 20.3% and 22.3% for full fine-tuning, while maintaining strong target-language performance.

## Executive Summary
This paper addresses catastrophic forgetting when adapting instruction-tuned LLMs to new languages using only unlabeled target language data. The proposed Source-Shielded Updates (SSU) method proactively identifies and freezes source-critical parameters before adaptation, using importance scores derived from a small set of source data. This column-wise freezing strategy preserves the model's foundational capabilities while enabling strong target-language gains. SSU significantly outperforms full fine-tuning, reducing source language performance drops to 3.4% (7B) and 2.8% (13B), compared to 20.3% and 22.3%, while achieving target-language performance highly competitive with full fine-tuning.

## Method Summary
The SSU method consists of three main steps. First, it calculates parameter importance scores using Wanda (Weight * Activation Norm) on a small calibration set of source data. Second, it aggregates these scores column-wise and freezes the top 50% of columns per weight matrix. Third, it performs standard CPT on target data with frozen weights masked out of gradient updates. The approach enables effective target language adaptation while preserving source language capabilities through proactive parameter protection.

## Key Results
- SSU reduces source language performance drops to 3.4% (7B) and 2.8% (13B), compared to 20.3% and 22.3% for full fine-tuning
- Target language performance matches or exceeds full fine-tuning across all evaluated tasks
- Ablation studies confirm column-wise freezing is critical for success

## Why This Works (Mechanism)
SSU works by proactively identifying and protecting parameters that are most important for maintaining source language capabilities before adaptation begins. By using Wanda scores to rank parameter importance and freezing the most critical columns, the method creates a "shield" around source knowledge. During target language adaptation, these shielded parameters cannot be updated, effectively preventing catastrophic forgetting while allowing the remaining parameters to learn target language patterns.

## Foundational Learning
- **Catastrophic Forgetting**: The phenomenon where neural networks lose previously learned knowledge when trained on new tasks - needed to understand the core problem being solved; quick check: verify source task performance degrades significantly with standard fine-tuning
- **Parameter Importance Scoring**: Methods to identify which model parameters are most critical for specific capabilities - needed to determine what to protect; quick check: confirm importance scores correlate with actual parameter impact on source performance
- **Column-wise vs. Element-wise Freezing**: Different granularities of parameter freezing - needed to understand why SSU's approach is effective; quick check: compare source retention across different freezing granularities
- **CPT (Continuation Training)**: Fine-tuning on unlabeled data - needed to understand the adaptation mechanism; quick check: verify standard CPT achieves strong target performance but causes catastrophic forgetting
- **Wanda Score**: A parameter importance metric combining weight magnitude with activation norms - needed to understand the scoring mechanism; quick check: confirm Wanda scores effectively identify important parameters

## Architecture Onboarding

**Component Map:** OLMo-2-Instruct -> Wanda Scoring -> Column-wise Aggregation -> Binary Mask -> Masked CPT Training -> Evaluation

**Critical Path:** The critical path is OLMo-2-Instruct → Wanda Scoring → Column-wise Aggregation → Binary Mask → Masked CPT Training. Any error in the scoring or masking steps will directly impact the model's ability to retain source knowledge while adapting.

**Design Tradeoffs:** The method trades some adaptation capacity (by freezing 50% of columns) for source knowledge preservation. Alternative granularities were tested, but column-wise freezing at 50% provided the best balance. The tradeoff is justified by the significant gains in source retention without sacrificing target performance.

**Failure Signatures:** 
- Source task performance drops >10% relative indicates incorrect masking granularity or application
- Poor target language performance suggests insufficient adaptation capacity or data issues
- Code-mixed or malformed outputs indicate chat template leakage or improper data preprocessing

**3 First Experiments:**
1. Verify column-wise vs. element-wise masking impact on source retention using a small calibration set
2. Test different freezing percentages (25%, 50%, 75%) to find optimal balance
3. Validate Wanda scoring on calibration data by ablating high/low scoring parameters

## Open Questions the Paper Calls Out
None

## Limitations
- The exact preprocessing of the MADLAD-400 "clean subset" is unspecified, which could impact adaptation performance
- Random selection of calibration samples introduces variability in the specific mask generated
- Column-wise freezing may not capture all parameter interactions that contribute to source knowledge retention

## Confidence

**High Confidence:** The core SSU methodology (Wanda scoring + column-wise freezing) is well-specified and reproducible. The relative performance gains over baseline methods (FT/Adapter) are robust across different configurations.

**Medium Confidence:** The absolute performance numbers are likely reproducible given the same data and implementation details, but the exact MADLAD-400 preprocessing step is a critical dependency.

**Low Confidence:** The specific choice of freezing 50% of columns is based on ablation but may not be optimal for all target languages or model sizes.

## Next Checks

1. **Data Preprocessing Validation:** Implement and test multiple MADLAD-400 cleaning/filtering strategies to establish a range of expected performance and verify robustness to preprocessing choices.

2. **Calibration Sample Sensitivity:** Reproduce the SSU mask using different random seeds for the 500 calibration samples and compare resulting masks and performance to quantify variability.

3. **Granularity Ablation:** Systematically test element-wise, row-wise, and column-wise masking on a smaller scale to empirically validate that column-wise is the critical factor for source retention.