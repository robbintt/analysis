---
ver: rpa2
title: Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based
  Retrieval
arxiv_id: '2506.00363'
source_url: https://arxiv.org/abs/2506.00363
tags:
- embedding
- retrieval
- bmembed
- query
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BMEmbed, a framework that adapts general-purpose
  text embedding models to private, domain-specific datasets using keyword-based retrieval
  signals. It leverages BM25 to generate ranking-based supervisory signals, which
  are then used in a listwise fine-tuning process to improve embedding quality for
  domain-specific retrieval tasks.
---

# Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval

## Quick Facts
- **arXiv ID:** 2506.00363
- **Source URL:** https://arxiv.org/abs/2506.00363
- **Reference count:** 40
- **Primary result:** BMEmbed adapts general-purpose embedding models to private domain datasets using BM25-based listwise fine-tuning, achieving consistent retrieval accuracy gains across financial, legal, and multi-hop QA domains.

## Executive Summary
This paper introduces BMEmbed, a framework that adapts general-purpose text embedding models to private, domain-specific datasets using keyword-based retrieval signals. It leverages BM25 to generate ranking-based supervisory signals, which are then used in a listwise fine-tuning process to improve embedding quality for domain-specific retrieval tasks. Experiments across three datasets (financial, legal, and multi-hop QA) show consistent performance gains, with improvements in both retrieval accuracy and embedding uniformity. BMEmbed also balances lexical sensitivity with semantic generalization, making it well-suited for proprietary datasets with specialized terminology.

## Method Summary
BMEmbed adapts embedding models through a three-stage process: (1) LLM-based domain query generation using event extraction and synthetic query synthesis, (2) BM25 retrieval with fine-to-coarse partitioning to create ranked lists for supervision, and (3) listwise fine-tuning using ListNet loss with LoRA optimization. The method uses temperature-controlled sampling from BM25 rankings to create training pairs, optimizing for both retrieval accuracy and embedding space quality metrics (alignment and uniformity).

## Key Results
- BMEmbed consistently outperforms base embedding models across all three datasets (Finance-RAG, LegalBench-RAG, Multihop-RAG) with MAP@10 improvements of 3-5 points
- Listwise fine-tuning with BMEmbed outperforms contrastive learning baselines, particularly for smaller embedding models
- The method achieves better balance between lexical sensitivity and semantic generalization compared to both base models and hybrid approaches like RRF
- BMEmbed maintains competitive performance even when compared against models specifically trained on the target domain data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BM25 lexical ranking provides effective weak supervision for domain adaptation
- Mechanism: BM25 term-frequency signals create structured pseudo-labels (ranked lists) that transfer domain-specific lexical patterns to neural embeddings through gradient alignment during listwise optimization
- Core assumption: BM25's lexical relevance correlates with semantic relevance for domain-specific terminology, enabling signal transfer without ground-truth labels
- Evidence anchors:
  - [abstract]: "By leveraging the well-established keyword-based retrieval technique (BM25), we construct supervisory signals from the ranking of keyword-based retrieval results to facilitate model adaptation."
  - [section 4.2]: "BMEmbed consistently outperform BM25 across models and datasets, despite being trained with supervisory signals derived from BM25. This demonstrates that BMEmbed is not merely mimicking BM25."
  - [corpus]: Limited corpus evidence; related work ChEmbed (2508.01647) similarly shows domain-specific embedding adaptation improves retrieval over general-purpose models

### Mechanism 2
- Claim: Listwise ranking loss preserves ranking structure better than contrastive learning
- Mechanism: The ListNet objective matches predicted similarity distributions to BM25 score distributions via KL-divergence minimization, explicitly encoding relative ordering rather than binary positive/negative distinctions
- Core assumption: Ranking information contains richer signal than binary relevance labels for embedding optimization
- Evidence anchors:
  - [section 3.3]: "Since BM25 retrieval results produce a ranked list, we hypothesize that this ranking contains valuable information that can be better utilized through a listwise training objective"
  - [section 4.2]: "Surprisingly, we find that applying CL to base models do not always improve performance... This indicates that contrastive learning is sensitive to the quality of positive and negative samples"
  - [corpus]: No corpus papers directly compare listwise vs contrastive learning for embedding adaptation

### Mechanism 3
- Claim: Fine-to-coarse sampling with low temperature optimizes alignment-uniformity tradeoff
- Mechanism: Fine-grained partitioning near top ranks provides clean positive signals while coarse-grained negative sampling from lower ranks improves embedding distribution uniformity; low temperature sharpens target distribution to reduce label noise propagation
- Core assumption: Optimal embeddings require balancing semantic clustering (alignment) with representation diversity (uniformity)
- Evidence anchors:
  - [section 5.4]: "Embedding models fine-tuned with BMEmbed achieve better retrieval results due to increased uniformity compared to the base model, while maintaining relatively low alignment"
  - [section 5.2]: "the fine-to-coarse strategy achieves better retrieval performance and superior alignment compared to the uniform strategy... [which] leads to a loss of alignment"
  - [corpus]: No corpus evidence on alignment-uniformity in this context

## Foundational Learning

- Concept: BM25 ranking function
  - Why needed here: Core signal source; understanding TF-IDF weighting explains why BM25 excels at lexical matching but fails on semantic synonyms
  - Quick check question: Given query "fiscal hawks" and documents containing "fiscal" vs "budgetary conservative," which would BM25 rank higher and why?

- Concept: ListNet vs InfoNCE loss functions
  - Why needed here: BMEmbed uses ListNet; understanding why listwise objectives preserve ranking structure vs contrastive objectives' binary distinction is critical for debugging training
  - Quick check question: If you have a ranked list [A, B, C, D] with scores [10, 8, 5, 1], how would InfoNCE vs ListNet treat the (query, B) pair differently?

- Concept: Alignment and uniformity in embedding space
  - Why needed here: Paper explicitly uses these metrics to explain BMEmbed's improvements; they provide interpretable diagnostics for embedding quality
  - Quick check question: A model has perfect alignment (all positive pairs have identical embeddings) but poor uniformity. What retrieval failure mode would you expect?

## Architecture Onboarding

- Component map: Private Corpus → [1. Domain Query Gen] → Synthetic Queries → [2. Relevant Sampling] → BM25 Ranked Lists → Partition & Sample → [3. Listwise Fine-Tuning] → Adapted Embedding Model

- Critical path:
  1. Event extraction quality directly impacts synthetic query relevance
  2. BM25 chunk size must match domain document structure (256 for multi-hop, 1024 for long-context)
  3. Temperature α (1.0-5.0 range) and partition count m (6-9) are coupled—test together
  4. LoRA rank-16 for 1000 steps is sufficient; longer training showed no clear gains in ablations

- Design tradeoffs:
  - Fine-to-coarse vs uniform partitioning: Fine-to-coarse preserves alignment better; use uniform only if domain has distributed relevance
  - Higher m (more partitions) increases uniformity but may introduce noise from lower-ranked samples
  - Lower temperature sharpens distribution but risks overfitting to BM25 noise

- Failure signatures:
  - CL baseline outperforms BMEmbed → Check synthetic query quality; LLM may generate off-domain questions
  - RRF outperforms BMEmbed → Domain has high synonym/paraphrase usage where lexical matching fundamentally fails
  - Adapted model degrades on out-of-domain eval → Overfitting to domain; reduce training steps or increase temperature

- First 3 experiments:
  1. Baseline reproduction on Finance-RAG: Run BMEmbed with reported hyperparameters (m=6, k=1000, α=1.0-5.0), verify ~3-5 point MAP@10 improvement over base model
  2. Ablation on partition strategy: Compare fine-to-coarse vs uniform with m=6,8,10; plot alignment-uniformity curves to identify optimal operating point for your domain
  3. Query perturbation test: Apply keyword masking/substitution protocol from Section 6 to quantify lexical sensitivity vs semantic generalization tradeoff; adjust α if lexical sensitivity is insufficient

## Open Questions the Paper Calls Out

- **Question:** Can supervisory signals derived from keyword-based retrieval (BM25) effectively adapt embedding models for non-retrieval tasks such as clustering and semantic textual similarity (STS)?
- **Basis:** Section 8 explicitly states: "An interesting direction for future research is exploring task-specific supervisory signals... for applications beyond retrieval, including clustering and STS."
- **Why unresolved:** BMEmbed optimizes embeddings using listwise ranking signals specifically for retrieval, but it is unproven whether these ranking-based improvements transfer to tasks requiring semantic similarity or grouping without explicit ranking contexts.

- **Question:** How does BMEmbed perform when applied to real-world proprietary datasets compared to the public "private" simulations used in the study?
- **Basis:** Section 8 notes: "However, applying this method to proprietary datasets in real-world RAG scenarios remains an important next step."
- **Why unresolved:** The current results rely on public datasets released after the base models' training cutoffs to simulate privacy. Real corporate data may possess different noise profiles, scale, or specialized jargon that synthetic query generation or BM25 signals cannot handle effectively.

- **Question:** Is there a theoretical or automated method to determine the optimal partitioning strategy (Uniform vs. Fine-to-Coarse) and interval count ($m$) for different domains?
- **Basis:** Section 5.2 empirically tests varying values of $m$ and sampling strategies, noting that "further increasing m causes performance fluctuations" and "fine-to-coarse achieves better... performance," but provides no theoretical grounding for selecting these hyperparameters for a new dataset.
- **Why unresolved:** The selection appears heuristic and tuned for the specific datasets in the paper. It is unclear if there is a generalizable rule connecting corpus statistics (e.g., term frequency distribution) to the optimal sampling strategy.

## Limitations

- **Synthetic Query Quality Dependency**: BMEmbed's performance critically depends on the quality of domain-specific queries generated by the LLM pipeline, creating a potential fragility point for real-world deployment.
- **Hyperparameter Sensitivity**: Optimal temperature α, partition count m, and sampling parameters appear dataset-dependent, suggesting potential performance variability across different private corpora.
- **BM25 Assumption Validity**: The method assumes BM25 lexical relevance correlates with semantic relevance for domain-specific terminology, which may break down in domains with heavy synonym usage or specialized abbreviations.

## Confidence

**High Confidence**: BMEmbed consistently improves retrieval accuracy over base models across all three datasets; Listwise fine-tuning outperforms contrastive learning for this adaptation task; BMEmbed provides better balance between lexical sensitivity and semantic generalization than alternatives.

**Medium Confidence**: Fine-to-coarse sampling strategy improves alignment-uniformity tradeoff compared to uniform sampling; Temperature parameter effectively controls label smoothing vs noise amplification; Performance gains are robust to model scale (1.5B vs 7B parameters).

**Low Confidence**: BMEmbed will generalize to arbitrary private domains without hyperparameter tuning; The framework maintains performance when applied to non-retrieval embedding tasks; BM25-generated signals remain effective for highly specialized terminology not present in general corpora.

## Next Checks

1. **Query Synthesis Robustness Test**: Systematically corrupt the query synthesis pipeline (add noise, use weaker LLMs, corrupt event extraction) and measure degradation in retrieval performance to quantify dependency on synthetic query quality.

2. **Cross-Domain Transferability**: Evaluate adapted models on out-of-domain retrieval tasks to measure overfitting by running Finance-RAG adapted model on LegalBench-RAG and vice versa, comparing to in-domain adapted models.

3. **Non-Retrieval Embedding Quality**: Test adapted embeddings on downstream tasks beyond retrieval including document clustering (PMI coherence), semantic similarity benchmarks (STS), and classification tasks to validate general embedding quality improvements.