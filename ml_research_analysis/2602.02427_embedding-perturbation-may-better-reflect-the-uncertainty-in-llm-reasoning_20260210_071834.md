---
ver: rpa2
title: Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning
arxiv_id: '2602.02427'
source_url: https://arxiv.org/abs/2602.02427
tags:
- reasoning
- top3
- top5
- uncertainty
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel uncertainty quantification (UQ)\
  \ approach for large language models (LLMs) that uses embedding perturbations to\
  \ identify unreliable reasoning steps. Unlike prior methods relying on token probabilities,\
  \ entropy, or multiple sampling\u2014which struggle to capture long-term context\
  \ dependencies\u2014the proposed method measures how sensitive each generated token\
  \ is to small perturbations in its preceding embeddings."
---

# Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning

## Quick Facts
- arXiv ID: 2602.02427
- Source URL: https://arxiv.org/abs/2602.02427
- Reference count: 11
- Primary result: Embedding perturbation achieves up to 66% detection rate for erroneous reasoning steps, outperforming baseline uncertainty metrics.

## Executive Summary
This paper introduces a novel uncertainty quantification (UQ) approach for large language models (LLMs) that uses embedding perturbations to identify unreliable reasoning steps. Unlike prior methods relying on token probabilities, entropy, or multiple sampling—which struggle to capture long-term context dependencies—the proposed method measures how sensitive each generated token is to small perturbations in its preceding embeddings. Higher sensitivity suggests weaker grounding in the preceding context, indicating potential errors. Evaluated on two reasoning benchmarks—pure logical reasoning and mathematical reasoning—the method achieves up to 66% detection rates for erroneous steps, outperforming baseline UQ metrics. It is also computationally efficient, requiring only a single model run per token. Limitations include reduced effectiveness on factual hallucinations and tasks with high reasoning variability. Overall, embedding perturbation provides a more effective and efficient way to detect intermediate uncertainty in LLM reasoning.

## Method Summary
The method applies random Gaussian noise or adversarial gradients to token embeddings and measures how much token probabilities fluctuate as a result. Three metrics are introduced: (1) Random Perturbation - adds Gaussian noise (σ=0.001) to embeddings, samples r=20 times, computes variance of token probabilities; (2) l∞-Adversarial Perturbation - subtracts scaled gradient of total log-probability from embeddings; (3) l2-Adversarial Perturbation - same but without sign operator. The method generates responses at T=0.2, extracts embeddings, applies perturbations, and ranks tokens by their sensitivity scores to identify potentially erroneous reasoning steps.

## Key Results
- Up to 66% successful detection rate for erroneous reasoning steps (top-5 tokens) on Llama-3.1-8B
- Outperforms baseline metrics (NLL, entropy) which achieve only 45% detection rate
- Adversarial perturbation (l∞-Adv.) is computationally efficient (0.04s per case) compared to random perturbation (0.85s per case)
- Performance degrades to near-chance levels on factual hallucination detection tasks

## Why This Works (Mechanism)

### Mechanism 1: Contextual Grounding Instability
- Claim: Tokens in incorrect reasoning steps exhibit higher sensitivity to perturbations in their preceding context embeddings.
- Mechanism: Incorrectly generated tokens are weakly grounded in their preceding reasoning context. When small noise is added to the embeddings of earlier tokens, the generation probability of these "uncertain" tokens fluctuates more than well-grounded tokens.
- Core assumption: Reasoning errors arise from shaky contextual dependencies rather than solely from low-frequency word choices.
- Evidence anchors: [abstract] "LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings."
- Break condition: If tokens are low-probability due to word frequency (not reasoning uncertainty), perturbation sensitivity may not correlate with errors.

### Mechanism 2: Variance Under Random Perturbation
- Claim: The variance of a token's generation probability across multiple random embedding perturbations serves as an uncertainty signal.
- Mechanism: Sample Gaussian noise r times, add to all embeddings, compute probability variance per token (Eq. 4). Higher variance implies the token's probability is unstable given its context.
- Core assumption: Context-grounded tokens have stable probabilities; uncertain tokens have flatter or unstable probability landscapes.
- Evidence anchors: [results] Random perturbation achieves up to 61% detection (top-5, Llama, Web of Lies) vs. 45% for NLL baseline
- Break condition: If the number of noise samples r is too low, variance estimates may be noisy; ablation shows r=20 is sufficient.

### Mechanism 3: Adversarial Gradient Sensitivity
- Claim: Perturbing embeddings along the gradient direction that decreases total log-probability exposes tokens most vulnerable to context disruption.
- Mechanism: Compute gradient of total log-prob w.r.t. embeddings, subtract (scaled) gradient from embeddings, measure per-token probability drop (Eq. 7). Largest drops indicate uncertainty.
- Core assumption: Gradient directions toward lower total probability also reveal which individual tokens are least robust.
- Evidence anchors: [results] l∞-Adv. Pert. achieves up to 66% detection (top-5, Llama, Web of Lies)
- Break condition: For models with large gradient magnitudes (e.g., Mistral), fixed α=0.0001 may over-perturb, degrading performance.

## Foundational Learning

- **Autoregressive Token Generation**
  - Why needed here: Each token's probability depends only on preceding tokens (not future ones), enabling perturbation of the full sequence for per-token sensitivity analysis.
  - Quick check question: Given tokens [A, B, C, D], which tokens influence P(D)?

- **Token Embedding Space**
  - Why needed here: Perturbations are applied directly to embedding vectors; understanding how embeddings represent meaning is essential for interpreting sensitivity.
  - Quick check question: What is the dimension d of the embedding vectors in the paper's notation?

- **Uncertainty Quantification (Entropy vs. Probability)**
  - Why needed here: Baseline methods (NLL, entropy) are compared against perturbation-based metrics; understanding their failure modes motivates the new approach.
  - Quick check question: Why might a low-probability token (e.g., "we" starting a sentence) not indicate a reasoning error?

## Architecture Onboarding

- **Component map**: Generate response -> Extract embeddings -> Apply perturbation -> Re-compute token probabilities -> Score and rank
- **Critical path**: Generate response → Extract embeddings → Apply perturbation → Re-compute token probabilities → Score and rank
- **Design tradeoffs**:
  - Random vs. Adversarial: Random requires r=20 inferences (~0.85s/case); Adversarial requires single gradient pass (~0.04s/case). Adversarial is faster but sensitive to α and gradient scale.
  - Token-level vs. Sentence-level: Paper focuses on token-level; aggregating to sentences dilutes signal for intermediate step detection.
  - Detection vs. Forecasting: Perturbation metrics excel at locating intermediate errors but do not consistently outperform entropy/NLL for final-answer correctness forecasting.
- **Failure signatures**:
  - Factual hallucinations: Weak context dependency means perturbations don't expose errors (Section 5.1 shows AUROC drops to ~0.5 for Qwen/Mistral).
  - High pathway variability: Models with multiple valid reasoning paths (e.g., Qwen) show elevated uncertainty even for correct tokens.
  - Gradient magnitude issues: Mistral's large gradients degrade l2-Adv. Pert. performance.
- **First 3 experiments**:
  1. Reproduce baseline comparison on GSM8K: Use Llama-3.1-8B, T=0.2, compare top-5 detection rate for NLL, Entropy, Rand. Pert. (r=20, σ=0.001), l∞-Adv. Pert. (α=0.0001).
  2. Ablate perturbation scale: Vary σ ∈ {0.0001, 0.001, 0.01} for Rand. Pert. and α ∈ {0.00001, 0.0001, 0.001} for l∞-Adv. Pert. on a held-out subset.
  3. Test on hallucination task: Run perturbation metrics on FactScore (celebrity biographies) to confirm the stated limitation that these metrics do not generalize to factual error detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can uncertainty metrics be refined to distinguish between uncertainty regarding reasoning correctness and uncertainty regarding the selection among multiple valid reasoning pathways?
- Basis in paper: [explicit] The authors acknowledge the method "may not reliably disentangle uncertainty about the 'correctness of each reasoning step' from uncertainty arising from the 'selection among multiple plausible reasoning pathways'."
- Why unresolved: The current method flags tokens with high variability (e.g., in the Qwen model) as uncertain, even when those tokens represent valid alternative reasoning steps, leading to false positives.
- What evidence would resolve it: A metric that maintains high detection rates for logical/math errors while suppressing uncertainty scores for tokens that initiate semantically equivalent alternative reasoning paths.

### Open Question 2
- Question: Can perturbation-based approaches be adapted to effectively detect factual hallucinations that stem from internal model knowledge rather than contextual deduction errors?
- Basis in paper: [explicit] The paper lists as a key limitation that "they may not exhibit strong UQ performance to detect LLM's factual hallucination" because such errors originate from internal knowledge rather than context.
- Why unresolved: The method relies on the sensitivity of a token to its preceding context; factual errors often lack strong dependencies on preceding tokens, rendering context perturbation insensitive to these hallucinations.
- What evidence would resolve it: Successful application of a modified perturbation strategy or a hybrid metric on factual benchmarks like FactScore, where the current method underperforms compared to baselines.

### Open Question 3
- Question: Can the token-level uncertainty signals provided by perturbation metrics effectively guide automated self-correction or backtracking mechanisms during generation?
- Basis in paper: [inferred] The introduction motivates the work by stating intermediate uncertainty enables "targeted interventions, such as... self-correction mechanisms," but the experiments only validate the detection of errors, not the utility of the signal for repair.
- Why unresolved: While the paper proves high-sensitivity tokens correlate with errors, it does not demonstrate if halting or altering generation at these points leads to successful reasoning recovery.
- What evidence would resolve it: Experiments showing that using high perturbation sensitivity as a trigger for self-correction yields a higher rate of correct final answers compared to standard correction triggers.

## Limitations

- **Reasoning vs. Knowledge Distinction**: The method excels at detecting intermediate reasoning errors but shows near-chance performance (AUROC ≈ 0.5) on factual hallucination detection tasks.
- **Model Variability Sensitivity**: Effectiveness varies significantly across models, with Qwen showing high false positives due to multiple valid reasoning pathways and Mistral experiencing degraded performance due to large gradient magnitudes.
- **Step Boundary Ambiguity**: Evaluation assumes a "first wrong step" exists in incorrect responses, but doesn't address cases where errors are distributed across multiple steps or where final answer errors don't correspond to clear intermediate errors.

## Confidence

- **High Confidence**: The core mechanism of using embedding perturbation to detect contextual grounding instability in reasoning tokens is well-supported by empirical results (up to 66% detection rate vs. 45% for NLL baseline).
- **Medium Confidence**: The claim that embedding perturbation is more efficient than sampling-based methods (0.04s vs. 0.85s per case) is supported, but the computational comparison assumes specific hardware and doesn't account for implementation overhead.
- **Medium Confidence**: The superiority over entropy and NLL baselines is demonstrated, but the evaluation is limited to specific reasoning tasks. The method's generalizability to broader reasoning domains remains untested.

## Next Checks

1. **Cross-Domain Robustness Test**: Apply the perturbation metrics to a diverse set of reasoning tasks beyond mathematical and logical domains, including commonsense reasoning, planning, and causal inference. Compare detection rates across domains to establish the method's breadth of applicability.

2. **Human Annotation Validation**: Conduct a human study where annotators independently identify erroneous reasoning steps in a subset of responses, then compare against the perturbation metrics' detections. This would validate the assumption that the "first wrong step" identification is accurate.

3. **Knowledge vs. Reasoning Error Disentanglement**: Design a controlled experiment where responses contain either knowledge-based hallucinations or contextual reasoning errors. Apply both perturbation metrics and knowledge-retrieval methods to each error type, quantifying the complementary nature of these approaches.