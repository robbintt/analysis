---
ver: rpa2
title: 'FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning
  on Streaming Dataflow'
arxiv_id: '2511.04768'
source_url: https://arxiv.org/abs/2511.04768
tags:
- fusion
- sparse
- dataflow
- fuseflow
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FuseFlow is a compiler that converts sparse machine learning models
  to fused sparse dataflow graphs for reconfigurable dataflow architectures. It supports
  general cross-expression fusion across sparse operations, enabling performance optimization
  beyond prior frameworks.
---

# FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow

## Quick Facts
- arXiv ID: 2511.04768
- Source URL: https://arxiv.org/abs/2511.04768
- Reference count: 40
- Primary result: Achieves 2.7x speedup for GPT-3 with BigBird attention via sparse ML fusion

## Executive Summary
FuseFlow is a compiler framework that transforms sparse machine learning models into fused sparse dataflow graphs optimized for streaming dataflow architectures. The framework introduces cross-expression fusion, enabling kernel fusion across sparse operations that previously required intermediate tensor materialization. By supporting general fusion of sparse operations and introducing fusion tables to manage intermediate streams, FuseFlow achieves significant performance improvements over unfused baselines, particularly for models with nested sparse operations like GPT-3 with BigBird attention.

## Method Summary
FuseFlow compiles PyTorch sparse models through a multi-stage pipeline: PyTorch → Torch-MLIR/MPACT → MLIR Linalg+SparseTensor dialects → Fused Einsum expressions → Cross-expression fusion using partial order graphs → Fusion tables → SAMML dataflow graph → Comal simulator or Vitis HLS for FPGA synthesis. The framework supports user-controlled fusion granularity via `Fuse{}` annotations and provides optimizations including parallelization, dataflow ordering, and sparsity blocking. Compilation overhead remains under 750ms, and evaluation uses five real-world datasets across graph, image, and text domains with sparsity ranging from 50-99.9%.

## Key Results
- Achieves 2.7x speedup for GPT-3 with BigBird attention compared to unfused baseline
- Demonstrates that partial fusion often outperforms full fusion for models like GCN and GraphSAGE
- Shows fusion granularity significantly impacts performance, with optimal configurations model-dependent

## Why This Works (Mechanism)

### Mechanism 1
Cross-expression kernel fusion eliminates intermediate tensor materialization by inlining producer results into consumers while building a partial order graph that tracks index ordering constraints from both user-specified dataflow orders and tensor storage formats. The POG enables topological sorting to find valid global iteration orders.

### Mechanism 2
Factored iteration spaces outperform globally fused iteration spaces by deferring node creation and memoizing intermediate streams through fusion tables. This interleaves input-iteration and computation rather than deferring all computation to innermost loops, reducing coordinate explosion from n-dimensional iteration.

### Mechanism 3
Optimal fusion granularity is model-dependent, with partial fusion often outperforming full fusion. A heuristic estimates FLOPs and memory transfers from tensor dimensions, sparsity percentages, and intersection rates to prune suboptimal schedules before simulation.

## Foundational Learning

**Concordant vs. discordant sparse tensor traversal**
- Why needed: The fusion algorithm must preserve format-specific ordering constraints; traversing against storage order causes asymptotic performance degradation
- Quick check: Given a CSR matrix stored row-major, would iterating j→i be concordant or discordant?

**Einsum notation and index variables**
- Why needed: FuseFlow represents operations as Einsum expressions; understanding contraction, reduction, and broadcast semantics is essential for tracing the fusion algorithm
- Quick check: In `T_ij = A_ik * B_kj`, which index is reduced?

**Streaming dataflow primitives**
- Why needed: SAMML graphs compose these primitives; understanding their data token types (coordinate, reference, value) is required to debug generated graphs
- Quick check: What primitive joins coordinate streams from two sparse tensors?

## Architecture Onboarding

**Component map:**
PyTorch -> Torch-MLIR/MPACT -> MLIR Linalg+SparseTensor dialects -> Cross-expression fusion algorithm -> Fused Einsum + Partial Order Graph -> Fusion table construction -> SAMML dataflow graph -> Comal simulator or Vitis HLS

**Critical path:**
PyTorch model → Einsum expressions (Section 4) → Cross-expression fusion (Section 5, Algorithm 1) → Fusion table construction (Section 6, Algorithm 2) → SAMML codegen → Comal simulation

**Design tradeoffs:**
- Global vs. factored iteration: FuseFlow chooses factored to avoid coordinate explosion; trades off potential global optimization opportunities
- Full vs. partial fusion: User-controlled via schedules; partial fusion often better for models with large sparse matmuls followed by smaller ops
- Heuristic vs. exhaustive search: Heuristic prunes search space but may miss optimal configurations

**Failure signatures:**
- POG cycles: "Conflicting ordering constraints" → check if tensor is used with incompatible mode orders; compiler will materialize permuted copy or error
- Recomputation blowup: FLOPs increase significantly under full fusion → check if nested matmuls create exponential coordinate growth
- Memory bandwidth saturation: High byte transfer with low utilization → consider sparsity blocking or parallelization

**First 3 experiments:**
1. Compile GCN on OGB-Collab with unfused, partially fused, and fully fused configurations; verify partial fusion (~2.63×) outperforms full fusion
2. Run fusion heuristic on GPT-3 BigBird with block sizes 16, 32, 64; compare estimated FLOPs/bytes against simulated results to validate error bounds (~1.8-5.7% per Table 3)
3. Sweep dataflow orders for nested matmul (Section 8.8); identify worst vs. best order (~29× difference) and verify constrained search space reduction (Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
Can an automated autoscheduler be developed to determine optimal fusion granularity and dataflow orders for arbitrary sparse ML models without user intervention? The current implementation relies on explicit user schedules or manual tuning, and the design space of valid dataflow orders is extremely large (up to $\sim10^{15}$ for models like GCN).

### Open Question 2
How does FuseFlow's generated code perform when deployed on the Opal coarse-grained reconfigurable array (CGRA) compared to the cycle-accurate simulation? While current evaluation uses simulation and FPGA synthesis, "we plan to target [Opal] as future work."

### Open Question 3
Can the compilation strategy be extended to handle dynamic sparsity where tensor storage formats may change during execution? The compiler depends on static format information to construct the fusion table and lower operations; runtime format changes would invalidate the generated dataflow graph topology.

## Limitations
- Comal simulator's cycle-accuracy versus real hardware behavior is not validated
- Framework currently targets a specific FPGA-based RDA (RC-1000), limiting generalizability
- While the approach handles CSR/CSC formats, extension to more complex sparse formats (like DIA or ELLPACK) is not discussed

## Confidence
- **High Confidence**: Cross-expression fusion mechanism (Section 5), SAMML dataflow graph generation, fusion tables (Section 6)
- **Medium Confidence**: Fusion heuristic effectiveness, optimal fusion granularity claims, parallelization optimization results
- **Low Confidence**: Hardware simulation accuracy, generalizability beyond RC-1000, real-world deployment overhead

## Next Checks
1. Implement a smaller-scale benchmark suite (e.g., GCN on Cora only) to verify compilation pipeline correctness and measure actual FPGA synthesis time versus simulated cycles
2. Profile the fusion heuristic by comparing its analytical estimates against detailed cycle-accurate measurements across diverse model configurations
3. Test the POG cycle detection mechanism with synthetic conflicting ordering constraints to verify the compiler's fallback behavior and error reporting