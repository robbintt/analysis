---
ver: rpa2
title: Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation
arxiv_id: '2510.09599'
source_url: https://arxiv.org/abs/2510.09599
tags:
- reasoning
- arxiv
- accuracy
- star
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce Prompting Test-Time Scaling (P-TTS), a method
  that uses instruction reframing to generate diverse reasoning prompts from a small
  pool of 90 high-quality math problems. By applying principle-guided wrappers such
  as reward, penalty, correctness, and step-by-step cues, they create augmented training
  data that improves supervised fine-tuning of Qwen-2.5 models.
---

# Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation
## Quick Facts
- arXiv ID: 2510.09599
- Source URL: https://arxiv.org/abs/2510.09599
- Reference count: 40
- One-line result: P-TTS-32B trained on 900 examples matches or exceeds baselines trained on datasets up to two orders of magnitude larger

## Executive Summary
Prompting Test-Time Scaling (P-TTS) is a novel data augmentation method that leverages instruction reframing to generate diverse reasoning prompts from a small pool of high-quality math problems. By applying principle-guided wrappers such as reward, penalty, correctness, and step-by-step cues, P-TTS creates augmented training data that significantly improves supervised fine-tuning of Qwen-2.5 models. The method demonstrates that test-time scaling can be effectively harnessed during training, leading to state-of-the-art performance on mathematical reasoning benchmarks with minimal data requirements.

## Method Summary
P-TTS introduces a data augmentation approach that reframes a small set of high-quality math problems using four principle-guided wrappers: reward, penalty, correctness, and step-by-step. These reframings generate diverse training prompts that guide the model's reasoning process during both training and inference. The method is applied to fine-tune Qwen-2.5 models, resulting in substantial performance improvements on mathematical reasoning tasks. P-TTS-32B, trained on only 900 examples, achieves competitive results compared to baselines trained on much larger datasets, demonstrating the efficiency and effectiveness of the approach.

## Key Results
- P-TTS-32B achieves 73.3% accuracy on AIME-2024 benchmark
- P-TTS-32B achieves 94.2% accuracy on MATH-500 benchmark
- Outperforms open-weight baselines trained on datasets up to two orders of magnitude larger

## Why This Works (Mechanism)
P-TTS works by leveraging instruction reframing to generate diverse reasoning prompts from a small set of high-quality problems. The method applies four principle-guided wrappers—reward, penalty, correctness, and step-by-step—to create augmented training data that guides the model's reasoning process. During inference, the model generates multiple reframed prompts and uses majority voting to select the final answer, effectively harnessing test-time scaling. This approach enhances the model's ability to reason through complex problems by exposing it to various reasoning paths and problem-solving strategies during training, leading to improved performance on mathematical reasoning tasks.

## Foundational Learning
- **Supervised Fine-Tuning**: Why needed - To adapt pre-trained models to specific reasoning tasks. Quick check - Model performance on downstream benchmarks improves after fine-tuning.
- **Data Augmentation**: Why needed - To increase the diversity and quantity of training data without manual labeling. Quick check - Augmented data leads to improved model generalization.
- **Instruction Reframing**: Why needed - To expose models to diverse problem-solving approaches and reasoning paths. Quick check - Reframed prompts generate varied and relevant solutions.
- **Test-Time Scaling**: Why needed - To leverage multiple inference paths for improved decision-making. Quick check - Majority voting across reframed prompts improves accuracy.
- **Majority Voting**: Why needed - To aggregate multiple inference results and select the most reliable answer. Quick check - Voting mechanism consistently improves final accuracy.

## Architecture Onboarding
- **Component Map**: Seed Problems -> Instruction Reframing -> Augmented Dataset -> Supervised Fine-Tuning -> Qwen-2.5 Model -> Inference with Reframing
- **Critical Path**: High-quality seed problems are reframed using principle-guided wrappers to create augmented training data, which is then used to fine-tune the Qwen-2.5 model. During inference, the model generates multiple reframed prompts and uses majority voting to select the final answer.
- **Design Tradeoffs**: P-TTS trades computational overhead during inference (generating multiple reframed prompts) for improved accuracy and generalization. The method relies on a small, high-quality seed dataset rather than massive-scale data collection, prioritizing efficiency over raw data volume.
- **Failure Signatures**: Poor performance may occur if the seed problem pool lacks diversity or if the reframing principles are not well-suited to the target domain. Cross-architecture generalization may be limited, and the method may struggle with non-mathematical reasoning tasks.
- **First Experiments**: 1) Evaluate P-TTS on non-mathematical reasoning tasks to assess domain generalization. 2) Conduct ablation studies to determine the relative contribution of each reframing principle. 3) Measure computational overhead during inference and compare it to performance benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a curated seed dataset of 90 high-quality math problems may limit generalization to non-mathematical domains
- Effectiveness depends heavily on the quality of the initial problem pool and appropriateness of reframing principles
- Focuses on Qwen-2.5 models, raising questions about cross-architecture generalization

## Confidence
- **High Confidence**: P-TTS performance improvements on MATH-500 and AIME-2024 benchmarks are well-supported by experimental results
- **Medium Confidence**: Claims about matching or exceeding baselines trained on larger datasets depend on representative comparison data and consistent evaluation methodology
- **Medium Confidence**: Transfer performance to out-of-domain and multilingual benchmarks is demonstrated but limited in scope

## Next Checks
1. Test P-TTS performance on non-mathematical reasoning tasks (e.g., logical puzzles, coding problems) to assess domain generalization
2. Conduct ablation studies to determine the relative contribution of each reframing principle (reward, penalty, correctness, step-by-step) to overall performance gains
3. Evaluate computational overhead of generating multiple reframed prompts during inference and compare it to performance benefits to establish practical efficiency trade-offs