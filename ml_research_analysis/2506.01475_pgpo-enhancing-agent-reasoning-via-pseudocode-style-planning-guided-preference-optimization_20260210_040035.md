---
ver: rpa2
title: 'PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference
  Optimization'
arxiv_id: '2506.01475'
source_url: https://arxiv.org/abs/2506.01475
tags:
- plan
- agent
- black
- p-code
- pgpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using pseudocode-style plans (P-code Plan)
  to guide reasoning in large language model (LLM) agents. P-code Plans are more structured
  and concise than natural language plans, capturing the logical steps needed to complete
  a task while allowing better generalization to unseen scenarios.
---

# PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization

## Quick Facts
- arXiv ID: 2506.01475
- Source URL: https://arxiv.org/abs/2506.01475
- Reference count: 37
- Primary result: PGPO improves average rewards by up to 11.6% over baselines across ALFWorld, WebShop, and TextCraft benchmarks

## Executive Summary
This paper introduces PGPO (Planning Guided Preference Optimization), a method that enhances LLM agent reasoning by incorporating pseudocode-style plans (P-code Plans) into training. P-code Plans provide structured, concise representations of reasoning steps that improve generalization across tasks. The authors propose a dual-reward system that evaluates both plan quality and plan-following behavior, combined with iterative DPO training with SFT regularization. Experiments with four open-source LLMs show PGPO achieves state-of-the-art performance, reducing action errors and interaction steps needed to complete tasks.

## Method Summary
PGPO works by first converting expert ReAct trajectories into structured P-code Plans using a GPT-4o-based distillation pipeline. These plans are then prepended to trajectories during SFT training. The method iteratively refines the model using two planning-oriented rewards: plan-driven reward (environment outcome) and plan-following reward (Monte Carlo sampling of N=5 trajectories). DPO optimization combines these with an SFT regularization loss to prevent degradation. The process runs for up to 4 iterations, with early stopping based on validation performance.

## Key Results
- PGPO achieves 11.6% average reward improvement over leading baselines across three agent benchmarks
- P-code Plans consistently outperform natural language plans on unseen ALFWorld tasks, reducing interaction turns by 8.2%
- The method reduces invalid action rates from 32-34% to 23-27% compared to baselines
- PGPO shows particular advantage on complex tasks like WebShop, achieving 85.4% average reward on seen tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: P-code Plans improve generalization by capturing abstract structural logic rather than task-specific details
- Mechanism: The structured format separates abstract planning steps from task-specific entities, allowing models to learn reusable reasoning patterns
- Core assumption: Abstract planning patterns transfer across tasks sharing similar subtask structures
- Evidence anchors: P-code Plans reduce interaction turns on unseen ALFWorld tasks; SOP-Agent paper confirms domain-specific structured procedures enhance capabilities

### Mechanism 2
- Claim: Dual planning-oriented rewards create richer preference signals than outcome-only rewards
- Mechanism: Plan-driven reward evaluates plan quality via trajectory outcome; plan-following reward uses Monte Carlo sampling to assess adherence
- Core assumption: Accurate plan-following correlates with task success when the plan itself is high-quality
- Evidence anchors: Removing plan-following reward causes 2.1% performance drop; PilotRL paper uses similar global planning-guided rewards

### Mechanism 3
- Claim: Iterative DPO with SFT regularization prevents preference optimization degradation while refining planning ability
- Mechanism: Combined loss (L_p + L_f + L_s) addresses DPO's tendency to decrease log probability of chosen trajectories
- Core assumption: SFT loss on preferred trajectories prevents capability forgetting during preference optimization
- Evidence anchors: Removing SFT loss causes largest performance drop (7.1% on ALFWorld-Seen); performance peaks within 4 iterations then degrades

## Foundational Learning

- Concept: **ReAct-style agent reasoning** (Thought-Action-Observation tuples)
  - Why needed here: PGPO builds on ReAct trajectories as the base format for agent interaction
  - Quick check question: Can you explain how a Thought-Action-Observation loop differs from standard chain-of-thought prompting?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: PGPO uses DPO loss to optimize the agent using contrastive trajectory pairs
  - Quick check question: What does the reference model π_ref represent in DPO, and how does β control deviation from it?

- Concept: **Monte Carlo Tree Search concepts (rollout sampling for reward estimation)**
  - Why needed here: Plan-following reward uses N=5 sampled trajectories to estimate expected outcome
  - Quick check question: Why might averaging rewards over N samples be preferable to a single deterministic evaluation for estimating plan-following quality?

## Architecture Onboarding

- Component map: P-code Plan Generator -> SFT Base Agent -> Exploration Module -> Reward Estimator -> Contrastive Dataset Builder -> DPO Optimizer
- Critical path:
  1. Generate P-code Plans from expert trajectories (GPT-4o with human verification)
  2. SFT training on plan-augmented trajectories (3 epochs, lr=2e-5)
  3. Iterative loop (max 4 iterations): explore trajectories, compute rewards, build contrastive datasets, DPO optimization
  4. Select best iteration based on validation performance
- Design tradeoffs:
  - P-code vs NL plans: P-code generalizes better but NL may perform slightly better on seen tasks
  - Monte Carlo N value: N=5 balances accuracy with compute cost
  - Iteration count: Performance peaks then declines; early stopping critical
  - Step-wise vs trajectory-level rewards: Step-wise rewards degraded performance due to ambiguous preference pairs
- Failure signatures:
  - Over-iteration: Performance degradation after iteration 3-4
  - High invalid action rate: Indicates plan-entity mismatch
  - Reward hacking: Ambiguous preference pairs in step-wise rewards
  - Plan-observation mismatch: Plans don't account for environment dynamics
- First 3 experiments:
  1. P-code format ablation: Compare SFT agents with no plan / NL plan / P-code Plan on ALFWorld-seen and unseen splits
  2. Single vs dual reward: Compare PGPO with only r_d vs r_d + r_f on WebShop dense reward task
  3. Iteration sweep: Run PGPO for 5 iterations on WebShop, logging validation reward and trajectory win count per iteration

## Open Questions the Paper Calls Out

- Can automated plan verification methods replace human verification while maintaining quality?
  - Currently 15% of generated P-code Plans require manual refinement
  - An automated method would need to check both format adherence and logical consistency

- Can rule-based rewards derived from P-code Plan structure improve upon Monte Carlo sampling?
  - Monte Carlo sampling incurs additional inference costs (N=5 samples per trajectory)
  - Rule-based rewards could be more efficient if designed to leverage structured pseudocode format

- Why does adding step-wise process supervision (PGPO+) degrade performance compared to PGPO?
  - The authors hypothesize that "some constructed preference pairs may be ambiguous, which poses a potential risk of reward hacking"
  - The mechanism causing performance degradation needs more rigorous analysis

## Limitations

- P-code Plan generation relies on GPT-4o distillation, which may not scale well to different task domains
- Monte Carlo reward estimation for plan-following introduces variance that isn't fully characterized
- The optimal iteration stopping point varies by task and isn't systematically determined

## Confidence

- Medium: Claims about P-code Plan generalization advantages rely on limited domain shifts within ALFWorld tasks
- Medium: Monte Carlo reward estimation may introduce variance affecting plan-following assessment
- Medium: SFT regularization prevents degradation but optimal iteration stopping varies by task

## Next Checks

1. Systematically vary N in Monte Carlo reward estimation to determine sensitivity and optimal sample size
2. Test PGPO with environment stochasticity increased to stress-test plan-following reward reliability
3. Apply the method to a substantially different domain (e.g., robotics or multi-agent collaboration) to validate generalization claims beyond current benchmarks