---
ver: rpa2
title: Scalable Best-of-N Selection for Large Language Models via Self-Certainty
arxiv_id: '2502.18581'
source_url: https://arxiv.org/abs/2502.18581
tags:
- self-certainty
- reasoning
- confidence
- borda
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes self-certainty, a novel metric for selecting
  the best response from multiple LLM-generated candidates in Best-of-N selection
  tasks. Self-certainty measures the divergence of the predicted token distribution
  from a uniform distribution, quantifying the model's confidence in its output.
---

# Scalable Best-of-N Selection for Large Language Models via Self-Certainty

## Quick Facts
- arXiv ID: 2502.18581
- Source URL: https://arxiv.org/abs/2502.18581
- Reference count: 38
- Primary result: Self-certainty metric + Borda voting achieves 88.06% accuracy on MATH at N=64, outperforming self-consistency (87.62%) and greedy decoding (77.54%)

## Executive Summary
This paper introduces self-certainty, a novel metric for selecting the best response from multiple LLM-generated candidates in Best-of-N selection tasks. Self-certainty measures the divergence of the predicted token distribution from a uniform distribution, providing a confidence signal that correlates with response accuracy. The method demonstrates strong performance on diverse reasoning benchmarks, scaling effectively with sample size and complementing chain-of-thought reasoning. When combined with Borda voting, self-certainty-based selection consistently outperforms self-consistency and greedy decoding while maintaining computational efficiency by leveraging inherent LLM probability distributions.

## Method Summary
Self-certainty computes the average KL-divergence between the model's predicted next-token distribution and a uniform distribution across all token positions in a response. The method generates N samples per question using temperature=0.6 and top_p=0.9 sampling, computes self-certainty scores for each sample, and applies Borda voting with power-law weighting (v(r) = (N-r+1)^p) to aggregate answers. The recommended parameter settings are p=0.3 for N≤16 and p=1.2 for N≥32. The approach is tested on mathematical reasoning (GSM8K, MATH, LiveBench-Math), code reasoning (CRUXEval-O), and open-ended code generation (LiveCodeBench) benchmarks using Llama-3.1-8B-Instruct as the primary model.

## Key Results
- Self-certainty achieves 88.06% accuracy on MATH test set at N=64, outperforming self-consistency (87.62%) and greedy decoding (77.54%)
- On GSM8K, self-certainty + Borda voting reaches 85.06% accuracy at N=64, improving upon self-consistency (84.53%)
- The method maintains computational efficiency by leveraging inherent LLM probability distributions without requiring external reward models
- Self-certainty demonstrates length-invariance, avoiding the verbosity bias inherent in perplexity-based metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A token distribution's divergence from a uniform distribution provides a more robust confidence signal than the probability of sampled tokens alone.
- **Mechanism:** Self-certainty computes token-wise KL-divergence between predicted next-token distribution and uniform distribution, capturing the "peakedness" of model belief across entire vocabulary.
- **Core assumption:** An LLM's probability distribution encodes its internal certainty, where concentrated distribution reflects higher confidence.
- **Evidence anchors:** Figure 1 shows correct/incorrect response distributions centered around distinct values; related work like ModeX supports evaluator-free selection trends.
- **Break condition:** Mechanism relies on alignment between model certainty and correctness; systematic overconfidence in errors could weaken correlation.

### Mechanism 2
- **Claim:** Self-certainty is largely invariant to response length, avoiding verbosity bias inherent in perplexity-based metrics.
- **Mechanism:** Unlike perplexity which decreases artificially with longer sequences, self-certainty's distributional measure remains stable as reasoning length increases.
- **Core assumption:** Longer responses do not inherently imply higher quality or correctness.
- **Evidence anchors:** Section 6.1 shows self-certainty remains invariant while other metrics correlate positively with response length; Figure 5 demonstrates minimal correlation.
- **Break condition:** Future model training schemes could incentivize "uncertain" patterns in correct long-form responses, diminishing perceived invariance.

### Mechanism 3
- **Claim:** Combining self-certainty with Borda voting scheme improves upon standard self-consistency for tasks with definitive answers.
- **Mechanism:** Ranks N outputs by self-certainty score, assigns votes based on rank using v(r) = (N-r+1)^p, giving more weight to answers from high-certainty reasoning paths.
- **Core assumption:** Both frequency of answer and confidence of reasoning path are useful signals for correctness.
- **Evidence anchors:** Table 1 and 2 show Borda voting consistently outperforms standard self-consistency across GSM8K, MATH, and CRUXEval datasets.
- **Break condition:** Misconfigured p parameter can degrade performance; very high p reverts to pure self-certainty selection which underperforms for some math problems.

## Foundational Learning

**KL-Divergence**
- Why needed here: Self-certainty is defined as average KL-divergence between predicted token distribution and uniform distribution.
- Quick check question: If predicted distribution is identical to uniform distribution, what is their KL-divergence? (Answer: 0, representing maximum uncertainty/minimum self-certainty).

**Best-of-N Selection**
- Why needed here: Entire paper framed as method to improve this paradigm of generating N candidates and selecting best one.
- Quick check question: How does self-certainty make Best-of-N selection more efficient compared to external reward model? (Answer: Requires no extra training or external models, incurring almost no computational overhead).

**Chain-of-Thought (CoT) Prompting**
- Why needed here: Paper explicitly tests method on CoT reasoning and finds self-certainty provides "orthogonal enhancement."
- Quick check question: Why would metric biased towards longer responses be problematic for evaluating CoT reasoning? (Answer: Might incorrectly select long, rambling but incorrect chain of thought).

## Architecture Onboarding

**Component map:** LLM Inference -> Self-Certainty Scorer -> Ranking & Voting Engine

**Critical path:**
1. Modify inference code to capture p(j | x, y_{<=i}) for all tokens j in vocabulary at each step i
2. Implement Self-Certainty calculation (Equation 9) as vectorized operation on distributions
3. Implement BordaVote aggregation, requiring function to extract final answers and group them

**Design tradeoffs:**
- Self-Certainty vs. Perplexity: Self-certainty more robust to length but slightly more complex to compute; perplexity simpler but noisier
- Pure SC vs. Borda Voting: Pure self-certainty faster but underperforms self-consistency on math tasks; Borda voting superior but requires tuning p
- Choice of p: Heuristic is p=0.3 for N≤16 and p=1.2 for N≥32

**Failure signatures:**
- Overconfident errors: Model may generate incorrect answer with high certainty, mitigated but not eliminated by Borda voting
- Misconfigured p: Setting p=0 makes it equivalent to self-consistency; setting too high reverts to pure self-certainty, suboptimal for some tasks
- No extractable answer: Borda voting requires grouping samples by final answer; if extraction fails, method cannot aggregate

**First 3 experiments:**
1. Reproduce Figure 1: Generate 64 samples per question for small GSM8K set, compute and plot histograms of self-certainty scores for correct vs incorrect answers
2. Ablate p parameter: On MATH Level 1-3 validation set, run Best-of-N selection with Borda voting for N=16 and N=64, sweep p∈[0.0, 2.0] to find optimal value
3. Compare to baselines: Run full evaluation comparing greedy decoding, self-consistency, and self-certainty + Borda voting on LiveBench-Math or CRUXEval, report accuracy vs N for N=4,8,16,32,64

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can learned, data-driven weighting schemes for aggregating token-level self-certainty outperform current simple averaging and power-law Borda voting strategies?
- Basis in paper: Section 7 states "Future work should explore more sophisticated aggregation functions or data-driven approaches for learning optimal vote weighting schemes."
- Why unresolved: Current implementation relies on heuristic averaging and fixed power function which may not capture nuanced importance of different reasoning steps across diverse tasks.
- What evidence would resolve it: Comparative study showing trained aggregation function (e.g., small neural network) yields statistically significant accuracy improvements over heuristic methods on MATH and LiveBench benchmarks.

**Open Question 2**
- Question: Does incorporating self-certainty as auxiliary signal in reward model training improve robustness and mitigate "reward hacking"?
- Basis in paper: Section 7 suggests self-certainty "could enhance reward model design by shifting from token-level scoring to distribution-aware confidence estimation" to create more stable objectives.
- Why unresolved: While self-certainty correlates with accuracy, unknown if properties effectively regularize training of external reward models prone to distribution shifts.
- What evidence would resolve it: Experiments training reward models with self-certainty regularization term, demonstrating reduced over-optimization and higher correlation with ground-truth rewards compared to standard RM training.

**Open Question 3**
- Question: Is it possible to develop universal, adaptive mechanism for setting Borda parameter p that eliminates need for task-specific validation sets?
- Basis in paper: Section 6.2 notes optimal parameter p varies with sample size N and model type, currently requiring grid search on validation set.
- Why unresolved: Dependence on validation set reduces "plug-and-play" efficiency for new tasks where labeled data is scarce.
- What evidence would resolve it: Formulation of theoretical or algorithmic rule that dynamically sets p based on distribution of self-certainty scores in real-time, achieving performance comparable to oracle best-p baseline without manual tuning.

## Limitations
- Prompt sensitivity: Method's effectiveness varies with prompt engineering, robustness across diverse real-world prompts remains unclear
- Limited task diversity: Strong performance on mathematical/code reasoning but limited evaluation on other domains like summarization or creative writing
- Model scale dependencies: Relationship between model scale and self-certainty's effectiveness not systematically explored

## Confidence
- **High Confidence:** Self-certainty provides better separation than perplexity (Figure 1), method is computationally efficient, self-certainty is largely length-invariant (Figure 5)
- **Medium Confidence:** Self-certainty scales effectively with sample size N, Borda voting outperforms standard self-consistency (Table 1 and 2), orthogonal enhancement to chain-of-thought reasoning (GSM8K results)
- **Low Confidence:** Generalizes to open-ended tasks (limited evaluation without baseline comparisons), optimal p parameter heuristic is universally applicable, works equally well across all reasoning domains (primarily mathematical reasoning tested)

## Next Checks
- **Check 1: Cross-Domain Robustness:** Evaluate self-certainty on diverse task families including summarization (CNN/DM), question answering (SQuAD), and creative writing (StoryCommonsense), compare against perplexity and self-consistency baselines
- **Check 2: Parameter Sensitivity Analysis:** Systematically sweep p parameter across wider range (0.0 to 2.0) and multiple N values (4, 8, 16, 32, 64) on GSM8K and MATH validation sets, generate heatmaps showing accuracy vs p and N
- **Check 3: Model Scale Scaling:** Test self-certainty on model family spanning different scales (Llama-3.1-8B, 70B, 405B), analyze whether correlation between self-certainty and correctness strengthens or weakens with model size, whether optimal p parameter shifts with model capacity