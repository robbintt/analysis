---
ver: rpa2
title: 'Hallucination as a Computational Boundary: A Hierarchy of Inevitability and
  the Oracle Escape'
arxiv_id: '2508.07334'
source_url: https://arxiv.org/abs/2508.07334
tags:
- escape
- learning
- hallucination
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a theoretical framework that explains why\
  \ hallucination in LLMs is inevitable at three computational boundaries\u2014diagonalization,\
  \ uncomputability, and information theory\u2014using a \"learner pump lemma.\" It\
  \ formalizes LLMs as probabilistic Turing machines and introduces two escape paths:\
  \ (1) absolute escape via oracle-augmented machines like RAG, and (2) adaptive escape\
  \ via continual learning framed as a neuro-game-theoretic process. The work culminates\
  \ in the Computational Class Alignment (CCA) principle, which requires matching\
  \ task complexity to system capability."
---

# Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape

## Quick Facts
- **arXiv ID:** 2508.07334
- **Source URL:** https://arxiv.org/abs/2508.07334
- **Reference count:** 2
- **Primary result:** Introduces a theoretical framework explaining LLM hallucination inevitability and validates a RAG-CL hybrid strategy achieving 96.5% accuracy with 1.1% forgetting.

## Executive Summary
This paper introduces a theoretical framework that explains why hallucination in LLMs is inevitable at three computational boundaries—diagonalization, uncomputability, and information theory—using a "learner pump lemma." It formalizes LLMs as probabilistic Turing machines and introduces two escape paths: (1) absolute escape via oracle-augmented machines like RAG, and (2) adaptive escape via continual learning framed as a neuro-game-theoretic process. The work culminates in the Computational Class Alignment (CCA) principle, which requires matching task complexity to system capability. Experiments validate that a RAG-CL hybrid strategy achieves high accuracy (96.5%) with low forgetting (1.1%) and superior robustness to noise compared to pure RAG or CLM approaches. The framework provides both theoretical insight and actionable guidelines for building more reliable and secure AI systems.

## Method Summary
The paper compares three hallucination mitigation strategies on a synthetic corpus of novel scientific facts and TriviaQA benchmark. Pure RAG uses FAISS retrieval with Mistral-7B; Pure CLM uses LoRA-based fine-tuning; RAG-CL hybrid combines both with a frequency-based trigger for internalization. The system models learning as a hierarchical game between fast-encoding (hippocampal) and slow-generalizing (cortical) agents. Evaluation measures accuracy, forgetting rate, robustness to 15% corpus noise, and amortized per-query inference cost, with a theoretical crossover point at ~287 queries.

## Key Results
- RAG-CL hybrid achieves 96.5% accuracy, 1.1% forgetting, and 92.3% robustness
- Theoretical crossover point where CL becomes cheaper than RAG is ~287 queries
- Demonstrates attention shift from external context to internal beliefs after consolidation
- Validates Computational Class Alignment principle matching task complexity to system capability

## Why This Works (Mechanism)

### Mechanism 1: The Computational Necessity Hierarchy
The paper argues that if LLMs are formalized as Probabilistic Turing Machines with finite capacity $K(h)$, hallucination is inevitable when the truth function $f$ has complexity $K(f) > K(h)$. The Learner Pump Lemma proves that models cannot perfectly reproduce information exceeding their static capacity, forcing "hallucination" (low-probability outputs) when queries require inaccessible information. This creates three boundaries: diagonalization (self-reference), uncomputability (halting problems), and information capacity (Kolmogorov complexity).

### Mechanism 2: Oracle-Augmented Absolute Escape
RAG functions as an "Oracle Machine" that provides absolute escape from hallucination by moving computation outside the model. Theorem 3.1 proves that while standard PLMs fail on adversarial self-referential inputs, oracle-augmented models $h_O$ can bypass internal computation and output $y_O = O(s)$ with probability 1, achieving $H_{Stray} = 0$ when the oracle contains ground truth.

### Mechanism 3: Hybrid Neuro-Game Theoretic Adaptation
The RAG-CL system creates an "Internalized Oracle" by modeling learning as a hierarchical game. A fast "Hippocampal" agent rapidly encodes new data while a slow "Cortical" agent generalizes it. When RAG retrieves a fact, CL can internalize it, shifting attention from noisy external context to robust internal beliefs. This optimizes the trade-off between RAG's high accuracy and CL's efficiency.

## Foundational Learning

- **Concept: Kolmogorov Complexity ($K(x)$)**
  - **Why needed here:** Defines the minimum description length of information, serving as the theoretical limit of model capacity for the information-theoretic boundary
  - **Quick check question:** Why does a random string have higher Kolmogorov complexity than a repetitive string, and how does this relate to an LLM's inability to memorize arbitrary random data?

- **Concept: Diagonalization**
  - **Why needed here:** Required to understand the diagonalization boundary through construction of "nemesis" functions that prove no single model can solve all problems
  - **Quick check question:** How does constructing a "nemesis" function $f_R(s_i) \neq h_i(s_i)$ prove that no single model can solve all problems?

- **Concept: Complementary Learning Systems (CLS)**
  - **Why needed here:** Provides biological inspiration for the neural game theory architecture, explaining why learning splits into fast "hippocampal" and slow "cortical" streams
  - **Quick check question:** In CLS theory, which system is responsible for rapid encoding of specific episodes, and which for slow generalization?

## Architecture Onboarding

- **Component map:** Mistral-7B (PLM) -> FAISS vector index (Oracle) -> LoRA fine-tuning (CL update function) -> Hierarchical game controller
- **Critical path:** Query enters -> Check recurrence frequency -> Oracle path (RAG) if not internalized -> Retrieve context -> Generate answer -> Adaptive path (CL) if frequent -> Trigger LoRA update -> Internalize knowledge
- **Design tradeoffs:**
  - Pure RAG: High accuracy (98.6%), 0% forgetting, but high latency and 76.5% noise robustness
  - Pure CLM: Efficient inference, but 12.4% forgetting and fact blending
  - RAG-CL Hybrid: 96.5% accuracy, 92.3% robustness, 1.1% forgetting, crossover at ~287 queries
- **Failure signatures:**
  - Fact Blending: Attribute merging during CL parameter updates
  - Noise Amplification: RAG blindly trusting corrupted retrieval context
  - Forgetting: Accuracy drops on old facts if consolidation fails
- **First 3 experiments:**
  1. Cost Crossover Analysis: Plot latency vs. query count (1-1000) for Pure RAG vs. RAG-CL
  2. Noise Robustness Test: Inject 15% errors into FAISS index and measure accuracy drop
  3. Attention Shift Probe: Compare cross-attention weights before/after CL updates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the theoretical information capacity $K(h)$ be quantified for specific neural architectures to predict hallucination thresholds?
- **Basis in paper:** [explicit] The conclusion explicitly lists the need to "quantify the information capacity 'K(h)' for specific neural architectures."
- **Why unresolved:** The paper uses Kolmogorov Complexity conceptually but lacks a practical metric for measuring finite capacity in deployed models like Mistral-7B.
- **What evidence would resolve it:** An empirical method to calculate $K(h)$ for a specific model and a demonstrated correlation between this capacity limit and failure rates on complex tasks.

### Open Question 2
- **Question:** Does the collective computational class of a multi-agent system transcend individual limitations, or does it introduce new failure modes?
- **Basis in paper:** [explicit] The authors call for future work to "analyze the collective computational class of multi-agent systems."
- **Why unresolved:** The current formalism applies to standalone machines; it is unclear if inter-agent communication acts as a reliable oracle or causes recursive error propagation.
- **What evidence would resolve it:** Formal proofs regarding the computational class of agent swarms and empirical studies comparing hallucination boundaries of isolated versus collaborative systems.

### Open Question 3
- **Question:** How do the trade-offs between RAG and Continual Learning strategies shift when the external feedback is noisy or bounded?
- **Basis in paper:** [explicit] The conclusion suggests studying "trade-offs... especially in the presence of noisy or bounded feedback."
- **Why unresolved:** While the paper tests static data noise, the dynamic impact of imperfect feedback signals on the stability of the "Adaptive Escape" path remains unquantified.
- **What evidence would resolve it:** Comparative experiments evaluating the stability of CL mechanisms under adversarial or low-fidelity gradient signals relative to Oracle-based methods.

## Limitations
- Theoretical claims rely on idealized assumptions about oracle "perfect" knowledge and stable consolidation protocols
- Practical applicability of PLM formalization to real transformers requires further validation
- Specific CL module hyperparameters (LoRA rank, VLA architecture) are not fully specified
- Impressive forgetting rate (1.1%) depends on detailed consolidation process not fully documented

## Confidence

**High Confidence:** Empirical results showing RAG-CL hybrid's superior accuracy (96.5%) and robustness (92.3%) are well-supported by stated metrics and attention shift mechanism. Cost-crossover analysis at ~287 queries is a concrete, testable prediction.

**Medium Confidence:** Theoretical framework of computational necessity hierarchy is logically constructed and supported by cited lemmas, but leap from abstract theory to transformer behavior requires validation. Oracle Escape mechanism is well-founded in computability theory but depends on corpus quality.

**Low Confidence:** Specific CL module hyperparameters and architectural details are not fully specified, making exact reproduction difficult. The 1.1% forgetting rate claim hinges on consolidation protocol details not fully provided.

## Next Checks

1. **Theoretical-Experimental Alignment:** Design experiment with synthetic truth function $f$ of known Kolmogorov complexity $K(f)$ embedded in queries. Test whether LLM with known capacity $K(h)$ fails when $K(f) > K(h)$, providing direct empirical support for information-theoretic boundary.

2. **Oracle Noise Tolerance:** Systematically vary FAISS corpus noise (0% to 30%) and measure accuracy of all three strategies. Verify RAG-CL's 92.3% robustness holds across noise conditions and analyze "internalized oracle" denoising effect.

3. **Dynamic Capacity Test:** Implement adaptive RAG-CL variant where LoRA update threshold adjusts based on fact frequency and model accuracy. Test whether this improves forgetting rate below 1.1% and enhances performance on shifting data distributions.