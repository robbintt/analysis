---
ver: rpa2
title: 'Human-Centered Explainability in AI-Enhanced UI Security Interfaces: Designing
  Trustworthy Copilots for Cybersecurity Analysts'
arxiv_id: '2601.22653'
source_url: https://arxiv.org/abs/2601.22653
tags:
- explanation
- security
- explanations
- trust
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how different explanation UI strategies
  affect user trust, decision accuracy, and cognitive load in AI-assisted cybersecurity
  dashboards. A controlled user study with 24 participants compared four explanation
  styles: confidence visualizations, natural language rationales, counterfactual explanations,
  and hybrid approaches.'
---

# Human-Centered Explainability in AI-Enhanced UI Security Interfaces: Designing Trustworthy Copilots for Cybersecurity Analysts

## Quick Facts
- arXiv ID: 2601.22653
- Source URL: https://arxiv.org/abs/2601.22653
- Authors: Mona Rajhans
- Reference count: 0
- A controlled study of four explanation UI strategies (confidence, natural language, counterfactual, hybrid) found hybrid and counterfactual approaches improve accuracy up to 82% while managing trust calibration.

## Executive Summary
This paper investigates how different explanation UI strategies affect user trust, decision accuracy, and cognitive load in AI-assisted cybersecurity dashboards. A controlled user study with 24 participants compared four explanation styles: confidence visualizations, natural language rationales, counterfactual explanations, and hybrid approaches. Results showed that hybrid and counterfactual strategies significantly improved decision accuracy (up to 82%) and trust calibration, but counterfactuals increased cognitive workload. Natural language explanations were fastest to process but promoted over-trust in novice users. The study provides empirical evidence and design guidelines for aligning explanation strategies with analyst expertise and task criticality, advancing human-centered AI explainability in high-stakes security contexts.

## Method Summary
The study employed a within-subjects design with 24 participants (12 security analysts, 12 graduate students with IT security background) who completed 16 triage tasks each, four per explanation type. Participants interacted with a web-based prototype dashboard modeled after SIEM workflows, processing alerts containing metadata fields like timestamp, source/destination address, correlated events, host risk score, and MITRE ATT&CK mapping. AI recommendations were generated via fixed GPT-based rule templates. The study measured decision accuracy, task completion time, NASA-TLX cognitive load, trust calibration via Schaffer & Barlow Trust-in-Automation scale, and decision reversal rate, with results analyzed using repeated-measures ANOVA and Holm-corrected post-hoc comparisons.

## Key Results
- Hybrid and counterfactual strategies improved decision accuracy up to 82% while maintaining trust calibration
- Natural language explanations processed fastest (16.7s) but promoted over-trust in novice users
- Counterfactual explanations drove highest analytical engagement (7.2s inspection, 19% reversal rate) but increased cognitive workload (NASA-TLX 58.3)
- Progressive disclosure in hybrid interfaces reduced cognitive load (49.6) compared to counterfactual-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid explanation interfaces combining confidence indicators with optional rationale expansion improve decision accuracy while managing cognitive load.
- Mechanism: Progressive disclosure allows users to control information depth—defaulting to glanceable confidence cues, then escalating to detailed rationale or counterfactual views only when task complexity warrants. This reduces extraneous cognitive processing while preserving access to deeper reasoning when needed.
- Core assumption: Users can accurately self-assess when they need more explanatory depth; time pressure does not systematically cause users to skip expansion when it would improve decisions.
- Evidence anchors:
  - [abstract] "hybrid and counterfactual strategies significantly improved decision accuracy (up to 82%) and trust calibration"
  - [section IV] "Hybrid > Confidence (p < .001), Hybrid > Natural Language (p = .002)" for decision accuracy; NASA-TLX for Hybrid (49.6) significantly lower than Counterfactual (58.3, p = .04)
  - [corpus] "Explanation User Interfaces: A Systematic Literature Review" notes progressive disclosure as a common pattern but does not provide comparative efficacy data.
- Break condition: If users under time pressure consistently skip expansion even when accuracy would benefit, hybrid advantages erode.

### Mechanism 2
- Claim: Counterfactual explanations promote analytical engagement and higher decision reversal rates, at the cost of increased cognitive workload.
- Mechanism: "What would change if..." logic exposes decision boundaries, enabling users to test whether AI reasoning matches their mental model. This triggers deeper inspection and more frequent justified decision reversals, but requires sustained attention and parameter manipulation.
- Core assumption: Users have sufficient domain knowledge to evaluate counterfactual scenarios meaningfully; cognitive overhead does not lead to disengagement.
- Evidence anchors:
  - [abstract] "counterfactuals increased cognitive workload"
  - [section IV] Counterfactual UIs had highest inspection time (M = 7.2 sec per alert); NASA-TLX 58.3 significantly higher than Confidence-only (37.5, p < .001) and Natural Language (42.1, p = .01); decision reversal rate 19%
  - [corpus] Related papers mention counterfactual explanations but lack empirical workload comparisons in security contexts.
- Break condition: If users lack domain expertise to interpret counterfactuals, increased workload yields frustration rather than accuracy gains.

### Mechanism 3
- Claim: Natural language rationales enable fastest task completion but promote over-trust in novice users due to reduced critical scrutiny.
- Mechanism: Readable prose explanations are processed quickly and feel intuitive, lowering friction. However, this ease of processing can bypass analytical evaluation, leading novices to accept recommendations uncritically—especially when the rationale appears coherent but is incomplete or misleading.
- Core assumption: Over-trust manifests primarily in novices; experienced practitioners apply domain skepticism regardless of explanation format.
- Evidence anchors:
  - [abstract] "Natural language explanations were fastest to process but promoted over-trust in novice users"
  - [section IV] Natural Language yielded fastest triage time (16.7s); "Natural Language increased perceived trust but exhibited greater variance, reflecting over-trust tendencies among novice users"; participants "read quickly but sometimes accepted uncritically"
  - [corpus] "Human-Centered Explainability in Interactive Information Systems: A Survey" identifies user expertise as a key moderating variable but does not quantify over-trust by explanation type.
- Break condition: If natural language rationales include structured uncertainty markers or skepticism cues, over-trust effects may diminish.

## Foundational Learning

- Concept: **Trust Calibration vs. Trust Maximization**
  - Why needed here: The paper explicitly argues that appropriate trust—not maximum trust—is the design goal. Over-trust leads to automation bias; under-trust leads to dismissal of valid AI recommendations. Understanding this distinction is essential for interpreting the results and applying the guidelines.
  - Quick check question: Can you explain why a higher trust score is not always a better outcome in high-stakes AI interfaces?

- Concept: **Progressive Disclosure**
  - Why needed here: The hybrid strategy's effectiveness depends on this interaction design pattern—presenting minimal information by default with optional expansion. Without this concept, the architecture tradeoffs between explanation styles will be unclear.
  - Quick check question: How would you design an alert triage UI that defaults to a single confidence bar but reveals structured rationale on demand?

- Concept: **NASA-TLX Workload Assessment**
  - Why needed here: The paper uses NASA-TLX as the primary cognitive load measure. Understanding what this scale captures (mental demand, temporal demand, effort, frustration, etc.) is necessary to interpret why counterfactuals scored highest.
  - Quick check question: What dimensions of cognitive workload might explain why counterfactual interfaces scored 58.3 vs. 37.5 for confidence-only interfaces?

## Architecture Onboarding

- Component map:
  - Explanation Layer -> Progressive Disclosure Controller -> Interaction Logger -> Alert Triage Engine -> Explanation Generation Module

- Critical path:
  1. Alert ingestion → metadata enrichment (MITRE mapping, risk scoring)
  2. AI recommendation generation + explanation artifact creation
  3. UI renders explanation based on selected strategy + user preferences
  4. User reviews, optionally expands explanation, records triage decision
  5. Interaction log captures explanation engagement + decision for audit trail

- Design tradeoffs:
  - **Accuracy vs. Speed**: Counterfactual/Hybrid improve accuracy (78-82%) but add 2-5 seconds vs. Natural Language (16.7s)
  - **Workload vs. Engagement**: Counterfactuals drive analytical engagement (7.2s inspection, 19% reversal) but highest NASA-TLX (58.3)
  - **Novice Safety vs. Expert Efficiency**: Natural Language is novice-friendly but over-trust prone; structured/interactive explanations suit experts but may overwhelm juniors
  - **Transparency vs. Information Overload**: Monolithic explanations combining all elements caused confusion; separated "What/Why/What If" performed better

- Failure signatures:
  - **Over-trust loop**: High trust scores + low decision reversal rate + high error rate on adversarial/misleading alerts (signals natural language over-reliance)
  - **Cognitive overload abandonment**: High dwell time on counterfactual panels + high NASA-TLX + increased "skip" or default-accept behaviors
  - **Under-trust dismissal**: Low trust scores + high reversal rate away from AI recommendations + low accuracy improvement from AI assistance
  - **Audit gap**: Missing interaction logs for which explanations were viewed before high-impact decisions

- First 3 experiments:
  1. **A/B test Hybrid vs. Natural Language with time-pressure constraint**: Measure whether hybrid's accuracy advantage persists when users have <15 seconds per alert. Hypothesis: Under extreme time pressure, expansion toggles go unused, narrowing accuracy gap.
  2. **Expertise-stratified counterfactual evaluation**: Separate novice (junior analysts, students) from experienced (Tier-2+) cohorts. Measure whether counterfactual workload penalty is higher for novices and whether accuracy gains differ by group.
  3. **Uncertainty annotation impact on over-trust**: Add explicit uncertainty markers ("Model prediction unstable due to sparse data") to natural language rationales. Measure whether this reduces novice over-trust variance without sacrificing speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adaptive explanation interfaces that dynamically escalate detail based on user hesitation signals (e.g., cursor latency, inactivity) impact cognitive load and decision accuracy?
- Basis in paper: [explicit] The authors state that "future work will explore adaptive explanation systems where the interface dynamically escalates explanation depth based on anomaly severity or user hesitation signals."
- Why unresolved: The current study used static explanation strategies (hybrid, counterfactual, etc.) that required manual toggling; it did not test systems that automatically adjust to user behavior in real-time.
- What evidence would resolve it: Results from a user study comparing static UIs against adaptive UIs that trigger progressive disclosure based on interaction telemetry.

### Open Question 2
- Question: How does the trust calibration of security analysts evolve over time when using AI copilots in live Security Operations Center (SOC) workflows?
- Basis in paper: [explicit] The authors note the need to "integrate longitudinal field deployment into real SOC workflows to observe trust trajectory over time rather than snapshot decision accuracy."
- Why unresolved: The current findings rely on a controlled study with simulated tasks, capturing immediate reactions rather than the long-term formation of trust or reliance patterns.
- What evidence would resolve it: A longitudinal field study tracking the same analysts' trust metrics and interaction behaviors over weeks or months of actual operational use.

### Open Question 3
- Question: How can explanation interfaces effectively convey dynamic, time-dependent reasoning (such as that from Social LSTM models) rather than static classification logic?
- Basis in paper: [explicit] The paper argues that as AI moves to "proactive threat anticipation," future mechanisms "must account not only for static classification logic but also for dynamic, time-dependent reasoning."
- Why unresolved: The study focused on explaining static triage decisions, and the authors acknowledge that explaining temporal predictions introduces new complexity not covered by current UI strategies.
- What evidence would resolve it: Design proposals and user validation for interface patterns capable of visualizing temporal trajectories and sequential logic in threat prediction.

## Limitations
- Small sample size (N=24) may limit generalizability to larger analyst populations
- Controlled laboratory setting lacks real-world time pressure and workflow complexity of operational SOCs
- Alert scenarios may not fully capture ambiguity and context-switching demands of live threat triage

## Confidence
- **High confidence**: Hybrid and counterfactual strategies improving decision accuracy; natural language explanations enabling faster processing but promoting novice over-trust; cognitive workload differences between explanation types
- **Medium confidence**: Trust calibration patterns and over-trust mechanisms in novices; generalization of interaction patterns to other high-stakes AI domains
- **Low confidence**: Long-term behavioral adaptation to explanation interfaces; impact of explanation fatigue across extended work sessions

## Next Checks
1. Conduct field study with Tier-1 analysts performing live alert triage over 2-week period, measuring real-time accuracy and over-trust indicators in production SOC environment
2. Test explanation interfaces with heterogeneous alert queues containing mixed urgency levels to evaluate whether progressive disclosure patterns hold under variable time constraints
3. Implement longitudinal monitoring of explanation engagement patterns to identify fatigue thresholds where interaction frequency declines despite persistent decision difficulty