---
ver: rpa2
title: Online inductive learning from answer sets for efficient reinforcement learning
  exploration
arxiv_id: '2501.07445'
source_url: https://arxiv.org/abs/2501.07445
tags:
- learning
- agent
- policy
- dist
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel neurosymbolic approach that combines
  Inductive Logic Programming (ILP) with reinforcement learning to improve training
  efficiency and policy explainability. The method learns ASP-based policy heuristics
  online during RL training by converting high-reward experience batches into weighted
  examples for ILP.
---

# Online inductive learning from answer sets for efficient reinforcement learning exploration

## Quick Facts
- arXiv ID: 2501.07445
- Source URL: https://arxiv.org/abs/2501.07445
- Reference count: 31
- Combines ILP with reinforcement learning to improve training efficiency and policy explainability

## Executive Summary
This paper presents a novel neurosymbolic approach that combines Inductive Logic Programming (ILP) with reinforcement learning to improve training efficiency and policy explainability. The method learns ASP-based policy heuristics online during RL training by converting high-reward experience batches into weighted examples for ILP. These heuristics are then used to guide exploration through probabilistic soft bias, preserving RL optimality while avoiding inefficient reward shaping. The approach was evaluated on the Pac-Man domain with two map sizes, achieving significantly higher discounted returns (almost double in the larger map) compared to standard approximate Q-learning.

## Method Summary
The approach interleaves reinforcement learning with online inductive learning of ASP-based policy heuristics. During RL training, high-return episodes are collected and ranked, with the top σ episodes converted into Weighted Context-Dependent Partial Interpretations (WCDPIs) for ILP learning via FastLAS. The learned rules are then used to guide exploration through a soft probabilistic bias mechanism that preserves optimality while focusing on promising actions. The entire procedure runs online during RL execution, with rules converging within ~70 batches and adding only ~25% computational overhead.

## Key Results
- Achieved nearly doubled discounted returns on the larger Pac-Man map compared to baseline Q-learning
- ASP reasoning component added only ~25% computational overhead
- Learned policy heuristics converged within ~70/200 training batches
- Produced interpretable ASP rules explaining the black-box policy behavior

## Why This Works (Mechanism)

### Mechanism 1
Converting high-reward experience batches into symbolic rules concentrates learning signal and provides structured priors. The system ranks RL episodes by cumulative return, selecting the top σ examples (5 in the small map, 3 in the large map). These high-reward trajectories are translated into WCDPIs that capture state-action pairs. FastLAS then induces compact rules (e.g., `move(Dir) :- food_dist_leq(Dir, Dist, 1).`) that generalize successful behaviors, creating a distilled symbolic representation of what actions led to success.

### Mechanism 2
Soft probabilistic bias preserves RL convergence guarantees while focusing exploration. Instead of hard constraints or reward shaping, the system uses ASP-derived suggested actions Ah to bias exploration probabilistically. During exploration (probability ε), the agent selects from Ah with probability ρ (normalized average return of best episodes) and from A \ Ah with probability 1-ρ. This guides the agent toward promising regions without eliminating alternative paths.

### Mechanism 3
Online interleaved learning allows symbolic and neural components to co-adapt as policy improves. At each batch end (every 100 episodes), the system updates rules based on recent best experiences. As RL policy improves, examples fed to ILP become better, inducing more accurate heuristics which improve exploration—a virtuous feedback loop where rules converge alongside the policy (~70/200 batches).

## Foundational Learning

- **Q-learning and action-value functions**: The base learner is Approximate Q-learning; understanding Q(s,a), TD updates, and ε-greedy exploration is essential. Quick check: Can you explain how `Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]` propagates reward information?

- **Inductive Logic Programming (ILP) from examples**: FastLAS learns ASP rules from weighted examples (WCDPIs); understanding hypothesis search is required. Quick check: Given positive examples `p(a), p(b)` and background `q(a), q(b), q(c)`, what minimal rule hypothesis covers `p(X)`?

- **Answer Set Programming semantics**: ASP provides the logical formalism; understanding normal rules, grounding, and answer sets is necessary to interpret learned policies. Quick check: For `p :- not q. q :- not p.`, how many answer sets exist and what are they?

## Architecture Onboarding

- **Component map**: RL Core -> Experience Buffer -> Batch Ranker -> Symbolic Translator -> ILP Learner (FastLAS) -> ASP Reasoner -> Soft Bias Selector

- **Critical path**:
  1. RL agent explores → collects episode
  2. Episode stored in batch buffer
  3. At batch end: rank → translate top σ to WCDPIs → FastLAS learns H
  4. Next batch: ASP reasons over H at each exploration step to produce Ah
  5. Action sampled via soft bias
  6. Loop

- **Design tradeoffs**:
  - **Batch size (Sb)**: Smaller → frequent updates but noisier; larger → stable but slower adaptation
  - **Top episodes (σ)**: Higher → richer ILP signal but slower; lower → faster but risks missing patterns
  - **Search space (SM)**: Larger → expressive but slower ILP; constrain via mode declarations

- **Failure signatures**:
  - **Ah = ∅ consistently**: Rules never trigger; check FF or rule body conditions
  - **Rules oscillate batch-to-batch**: Policy unstable; increase σ or batch size
  - **Performance worse than baseline**: ρ too high or feature representation insufficient
  - **ILP timeout**: SM too large or too many examples; prune mode declarations

- **First 3 experiments**:
  1. **Ablate symbolic guidance**: Run NeuroQ with ILP disabled (ρ = 0) vs. full system on the same map.
  2. **Vary σ and Sb**: Test σ ∈ {1, 3, 5, 10} and Sb ∈ {50, 100, 200} on small map to find compute/performance sweet spot.
  3. **Inspect rule convergence**: Log H after each batch; plot Hamming distance to final rule set, verify ~70-batch convergence.

## Open Questions the Paper Calls Out
- Can the proposed online ILP-RL integration maintain computational efficiency and convergence speed in environments with significantly higher state complexity or continuous action spaces compared to the discrete Pac-Man benchmark?
- How effectively does the soft bias mechanism transfer to modern Deep Reinforcement Learning algorithms (e.g., PPO, DQN, or SAC) compared to the Approximate Q-learning implementation used in the study?
- Is the system robust to noise or sparsity in the manually defined ASP background knowledge, or does the reliance on hand-crafted feature maps create a bottleneck for generalization?

## Limitations
- Computational overhead quantification relies on single comparison point (25% vs baseline); scaling behavior on larger domains unknown
- Rule convergence timeline (~70 batches) may be environment-specific; no cross-domain validation
- Ghost behavior specification incomplete ("close" distance undefined)

## Confidence
- **High Confidence**: Performance improvement claim (nearly doubled returns on large map), rule convergence observation, soft bias preserving optimality
- **Medium Confidence**: Computational overhead claim (25%), ILP rule quality and interpretability, mechanism of soft bias improving exploration efficiency
- **Low Confidence**: Generalization to other RL domains, scalability to larger state spaces, robustness to reward function changes

## Next Checks
1. Cross-domain transfer: Apply methodology to gridworld navigation with different reward structures and measure performance degradation/gain
2. Sensitivity analysis: Systematically vary ρ, σ, and batch size to map performance/compute tradeoff surface
3. Feature ablation: Remove individual feature types (e.g., ghost proximity) and measure impact on rule quality and RL performance