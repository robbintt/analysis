---
ver: rpa2
title: Label Smoothing Improves Gradient Ascent in LLM Unlearning
arxiv_id: '2510.22376'
source_url: https://arxiv.org/abs/2510.22376
tags:
- forget
- data
- unlearning
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Smoothed Gradient Ascent (SGA), a method for
  large language model unlearning that addresses the instability and utility collapse
  issues of traditional Gradient Ascent. SGA improves upon GA by combining forget
  data with generated normal data using a tunable smoothing rate, enabling more stable
  forgetting while better preserving model performance.
---

# Label Smoothing Improves Gradient Ascent in LLM Unlearning

## Quick Facts
- arXiv ID: 2510.22376
- Source URL: https://arxiv.org/abs/2510.22376
- Reference count: 32
- Top-2 performance among all baseline methods on several key metrics

## Executive Summary
This paper introduces Smoothed Gradient Ascent (SGA), a method for large language model unlearning that improves upon standard Gradient Ascent by incorporating generated normal data with a tunable smoothing rate. SGA addresses the instability and utility collapse issues of traditional gradient ascent methods by combining forget data with multiple constructed normal data samples, enabling more stable forgetting while better preserving model performance. The authors provide theoretical guidance for selecting the optimal smoothing rate and empirically evaluate SGA on three benchmarks, demonstrating consistent improvement over the original GA method.

## Method Summary
SGA modifies the standard gradient ascent approach to unlearning by combining forget data with multiple constructed normal data samples through a tunable smoothing rate. For each forget instance, the method generates K-1 normal samples that are either semantically similar to the forget data or constitute safe responses. These normal data provide positive-gradient signals that reinforce safe, normal behavior while the forget data's negative-gradient signals induce forgetting. The combined gradient update is computed using a smoothed label distribution that balances the forgetting objective with utility preservation.

## Key Results
- SGA consistently outperforms the original GA method across all metrics
- Achieves top-2 performance among all baseline methods on several key metrics
- Demonstrates more stable unlearning while better preserving model utility compared to traditional approaches
- Shows improved performance on three benchmarks: TOFU, Harry Potter, and MUSE-NEWS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating normal data gradients deflects the divergent gradient ascent direction, reducing parameter explosion.
- Mechanism: Standard gradient ascent on forget data computes updates ∆θt ∝ −g_f, which is divergent. SGA forms a combined update ∆θt ∝ −[(1−r+ r/K)g_f + (r/K)Σ_k g_p^(k)], where r is the smoothing rate and K is the number of normal samples. This pulls the update away from a pure ascent direction toward a stable direction, mitigating utility collapse.
- Core assumption: The normal data gradients g_p^(k) are not perfectly anti-aligned with the forget gradient g_f and thus provide a stabilizing component.
- Evidence anchors:
  - [abstract]: "SGA combines the forget data with multiple constructed normal data through a tunable smoothing rate... enabling more stable unlearning while better preserving model utility."
  - [section 4.2]: Derivation showing the gradient update under SGA as a combination of forget and normal gradients.

### Mechanism 2
- Claim: A tunable smoothing rate provides a theoretically grounded balance between forgetting and retention.
- Mechanism: SGA treats the problem as learning over a K-dimensional label space comprising one forget sample and K−1 normal samples. The label distribution is smoothed as (−(1−(K−1)r/K), −r/K, −r/K, ...), where the negative coefficient induces forgetting and positive coefficients reinforce normal behavior. This formulation yields a closed-form expression for an optimal smoothing rate r* that minimizes the update norm ∥d(r)∥².
- Core assumption: The theoretical derivation treats g_f and the average normal gradient \bar{g}_p as approximately fixed vectors for the purpose of finding r*.
- Evidence anchors:
  - [abstract]: "Theoretically, we provide the theoretical guidance on the selection of the optimal smoothing rate."
  - [section 4.3]: Equation (9) r* = ⟨g_f, u⟩ / ∥u∥² is derived as the optimal smoothing rate.

### Mechanism 3
- Claim: Generated normal data provide semantically relevant, safe targets that anchor the model's behavior.
- Mechanism: For each forget instance, SGA constructs K−1 normal samples either by selecting high-similarity retain-set data or by generating safe alternatives (e.g., "I don't know") via GPT-4o-mini. These data provide positive-gradient signals that reinforce safe, normal behavior while the forget data's negative-gradient signals induce forgetting.
- Core assumption: The generated normal data are both semantically related to the forget set and free from the sensitive content that must be forgotten.
- Evidence anchors:
  - [abstract]: "SGA combines the forget data with multiple constructed normal data through a tunable smoothing rate."
  - [section 4.1, Step 1]: "The generation of normal data follows the principle that they should either be semantically similar to the forget data or constitute entirely safe responses."

## Foundational Learning

- **Concept: Gradient Ascent for Unlearning**
  - Why needed here: SGA is explicitly designed as a modification of gradient ascent; understanding how ascent maximizes the loss on forget data and its inherent instability is essential to grasp why SGA introduces normal data and a smoothing rate.
  - Quick check question: Why does pure gradient ascent on forget data tend to cause parameter divergence?

- **Concept: Label Smoothing**
  - Why needed here: SGA builds on Generalized Label Smoothing (GLS), distributing label probability across K classes. This idea directly informs the K-dimensional label space construction used to blend forget and normal data.
  - Quick check question: How does label smoothing change the target distribution in a multi-class setting?

- **Concept: Next-Token Prediction Loss in LLMs**
  - Why needed here: The GA and SGA objectives are formulated in terms of next-token prediction loss; this underpins the derivation of the gradient expressions g_f and g_p^(k) and the combined update rule.
  - Quick check question: In the standard language modeling setup, what does minimizing (or maximizing) the next-token loss incentivize?

## Architecture Onboarding

- **Component map**: Normal Data Generator -> Smoothing Rate (r) -> Label Distribution Constructor -> Optimizer
- **Critical path**:
  1. Prepare forget set and generate corresponding normal data.
  2. Initialize smoothing rate r (preferably within the estimated feasible range from the sign of ⟨g_f, u⟩).
  3. Construct the smoothed label distribution for each batch.
  4. Compute the combined loss and perform gradient updates.
  5. Evaluate unlearning and utility metrics, then adjust r as needed.
- **Design tradeoffs**:
  - Retain-set selection vs. generation: Retrieving similar retain data is cheaper but may not always yield semantically close samples; generation via GPT-4o-mini is more flexible but incurs cost and latency.
  - Fixed vs. dynamic r: Fixing r is efficient but may become suboptimal as training progresses; dynamic adjustment could improve performance but adds complexity.
  - Larger K vs. computational cost: Increasing K provides more gradient stabilization but increases per-batch cost.
- **Failure signatures**:
  - Extremely high perplexity and near-zero accuracy indicate divergence (often due to poorly chosen r or inadequate normal data).
  - Strong forget quality but near-zero retain set performance suggests over-aggressive forgetting (r may be too low or normal data ineffective).
  - Poor forget quality with high utility may indicate under-unlearning (r may be too high, overwhelming the forget gradient).
- **First 3 experiments**:
  1. Baseline comparison: Run SGA (with a moderate r, e.g., r = -0.2) against standard GA on TOFU's forget01 split, recording Forget Quality (FQ) and Model Utility (MU).
  2. Smoothing rate sweep: On the same split, evaluate a range of r values (e.g., -4 to 0.8) to identify the empirical optimum and compare to the theoretical estimate from the sign of ⟨g_f, u⟩.
  3. Normal data ablation: Compare SGA performance when normal data are selected via embedding similarity versus when generated by GPT-4o-mini, holding r fixed.

## Open Questions the Paper Calls Out

- Can dynamically adjusting the smoothing rate during the unlearning process yield better results than fixing it?
- How does the optimal smoothing rate scale with model size and architecture?
- How does the semantic quality or source of the generated "normal data" impact the stability and optimal smoothing rate?

## Limitations

- The theoretical derivation assumes forget and normal gradients are approximately fixed, which may not hold during extended training.
- The effectiveness of SGA heavily depends on the semantic relevance and safety of the generated normal data.
- Constructing multiple normal samples per forget instance increases computational overhead, especially when using LLM generation for normal data.

## Confidence

- **High Confidence**: The claim that SGA outperforms standard GA in terms of stability and utility preservation is supported by empirical results across three benchmarks (TOFU, Harry Potter, MUSE-NEWS).
- **Medium Confidence**: The theoretical guidance for selecting the optimal smoothing rate is sound in derivation, but its practical utility is limited by the assumption of fixed gradients and the need to estimate the sign of ⟨g_f, u⟩.
- **Medium Confidence**: The mechanism by which normal data gradients stabilize the update direction is plausible and supported by ablation studies, but its effectiveness is contingent on the quality of the normal data.

## Next Checks

1. **Dynamic Smoothing Rate**: Implement and evaluate SGA with an adaptive smoothing rate that updates r* periodically during training, comparing performance against the fixed-r baseline.

2. **Robustness to Normal Data Quality**: Systematically vary the quality and semantic distance of the generated normal data to assess SGA's sensitivity to this critical component, including adversarial or irrelevant normal data scenarios.

3. **Scalability Analysis**: Measure the computational overhead of SGA with increasing K and compare its scalability to other unlearning methods, particularly on large datasets or models.