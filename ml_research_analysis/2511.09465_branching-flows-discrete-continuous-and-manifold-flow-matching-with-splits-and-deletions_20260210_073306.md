---
ver: rpa2
title: 'Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits
  and Deletions'
arxiv_id: '2511.09465'
source_url: https://arxiv.org/abs/2511.09465
tags:
- flows
- branching
- distribution
- which
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Branching Flows is a generative modeling framework that extends
  diffusion and flow matching to variable-length sequences with elements in continuous,
  discrete, manifold, or multimodal spaces. It achieves this by augmenting a base
  Markov generator with a branching and deletion process, where elements evolve along
  binary trees with learned split and deletion rates.
---

# Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions

## Quick Facts
- **arXiv ID**: 2511.09465
- **Source URL**: https://arxiv.org/abs/2511.09465
- **Reference count**: 22
- **Primary result**: Extends flow matching to variable-length sequences with elements in continuous, discrete, manifold, or multimodal spaces via learned split and deletion processes.

## Executive Summary
Branching Flows is a generative modeling framework that enables variable-length sequence generation across continuous, discrete, manifold, and multimodal spaces. The method extends diffusion and flow matching by augmenting a base Markov generator with a branching and deletion process, where elements evolve along binary trees with learned split and deletion rates. This allows control over sequence length during generation while maintaining the theoretical guarantees of flow matching.

The framework composes with any flow matching base process and was demonstrated on three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal). For small molecules, Branching Flows showed superior distribution matching compared to transdimensional jump diffusion models, with Kolmogorov-Smirnov statistics ranging from 0.91-0.99 versus 0.87-0.97. For antibody sequences, it achieved similar distribution matching to an oracle-length model that has access to true sequence lengths.

## Method Summary
Branching Flows generates variable-length sequences by sampling a latent variable Z containing a forest of binary trees that define how elements evolve from an initial state to a target state. Each tree root corresponds to an initial element, and each leaf corresponds to a target element (surviving or to be deleted). Elements evolve independently along branches but duplicate at bifurcations. The model learns split and deletion rates using time-dependent hazard functions that ensure completion by t=1. During training, the model predicts the expected number of remaining splits and deletion probability, trained via Bregman divergences against conditional targets. The framework supports auxiliary generator matching that marginalizes branch indicators, simplifying the architecture while preserving correctness.

## Key Results
- Small molecule generation: Superior distribution matching to transdimensional jump diffusion (KS statistics 0.91-0.99 vs 0.87-0.97)
- Antibody sequence generation: Achieved similar distribution matching to oracle-length models (1-KS D scores of 0.99 for length matching)
- Protein backbone generation: Generated variable-length chains with geometry consistent with state-of-the-art folding models (scTM scores near 1.0)
- Variable-length generation: Successfully demonstrated across continuous, discrete, and multimodal domains

## Why This Works (Mechanism)

### Mechanism 1: Tree-Structured Conditional Coupling
Variable-length generation is enabled by pre-sampling binary tree structures that couple initial elements to target elements via paths with deterministic split/deletion schedules. A latent variable Z = (X_1^ø, X_0, T, A) is sampled per training example, where T is a forest of labeled binary trees. Each tree root corresponds to an initial element; each leaf corresponds to a target element (surviving or "to be deleted"). Elements evolve independently along branches but duplicate at bifurcations. The conditional path terminates at X_1 with probability 1 because hazard rates explode as t→1.

### Mechanism 2: Split and Deletion as Time-Inhomogeneous Counting Processes
The model learns split and deletion rates by predicting the expected number of remaining splits and deletion probability by t=1, trained via Bregman divergences against conditional targets. Split intensity is h_split(t)·R_θ(t,i)(X_t) where R_θ predicts remaining splits. Deletion intensity is h_del(t)·ρ_θ(t,i)(X_t) where ρ_θ predicts deletion probability. Both use hazard distributions supported on [0,1] with exploding rates at t=1 to force completion.

### Mechanism 3: Auxiliary Generator Matching Marginalizes Branch Indicators
The model need not observe branch indicators G_t during training; marginalizing over them preserves correctness while simplifying architecture. During training, G_t steers the conditional process (linking elements to correct anchors for the loss), but the model only receives X_t as input. Theorem D.1 shows that averaging conditional generators weighted by p(G_t|X_t) recovers the marginal generator that generates p_t(X_t).

## Foundational Learning

- **Generator Matching / Flow Matching**: The theoretical foundation enabling conditional path constructions and Bregman divergence losses. Quick check: Can you explain why training against conditional generators L_t^z recovers the marginal generator L_t that generates p_t?
- **Continuous-Time Markov Chains (CTMCs) and Infinitesimal Generators**: Understanding jump processes, rate matrices, and generators is needed to implement sampling and loss computation. Quick check: Given a rate λ(t) for a jump event, what is the probability of exactly one jump in interval Δt?
- **Ornstein-Uhlenbeck Bridges**: The base process for continuous components is an OU-like process with mean-reversion toward anchors. Quick check: For an OU process dX_t = θ(μ - X_t)dt + √v_t dW_t conditioned on X_1 = x_1, what is the distribution of X_t given X_0?

## Architecture Onboarding

- **Component map**: Input X_t, t -> Transformer (positional encoding, spatial pair features) -> Heads (base process, splits, deletions) -> Loss (Bregman divergences)
- **Critical path**: 1) Implement Z-sampling: data → X_1^ø → trees T → anchors A 2) Implement conditional path sampler: given Z, sample X_t with correct branch structure 3) Implement loss computation: extract R, ρ, F targets from Z and G_t 4) Implement marginal sampler: Euler steps with stochastic split/delete events
- **Design tradeoffs**: Augmented vs. Auxiliary GM (higher capacity vs. simpler), Tree-sampling strategy (uniform adjacent merge vs. domain-informed), Hazard distributions (Beta(1, 3/2) for splits vs. alternatives)
- **Failure signatures**: Length distribution mismatch (check hazard distributions), Mode collapse in structure (check tree-sampling), Chain breaks in protein generation (check post-hoc filtering)
- **First 3 experiments**: 1) Discrete-only validation on antibody sequences 2) Continuous manifold test on 2D point clouds 3) Ablation on tree-sampling (uniform vs. domain-informed)

## Open Questions the Paper Calls Out

### Open Question 1
How do branching-only, deletion-only, and combined branching-deletion processes compare in terms of convergence speed and distribution matching for different data modalities? The paper identifies the need for systematic comparison of these components but hasn't performed comprehensive ablation studies.

### Open Question 2
Does introducing stochastic noise upon element splitting ("duplicate and add noise") provide better performance or stability compared to the current deterministic duplication strategy? The authors note potential "tie breaking" effects but haven't implemented noise-injected variants.

### Open Question 3
Can constraining the latent forest structure to mirror domain-specific hierarchies (such as syntax trees for code or secondary structure for proteins) significantly improve learning? The paper relies on uniform adjacent merge policy but suggests structured trees could act as beneficial inductive bias.

### Open Question 4
To what extent is the dependence on sequence ordering a fundamental constraint of Branching Flows versus a limitation of the transformer architecture used? The paper notes performance drops for unordered data but attributes it to "model inadequacies" rather than the flow method itself.

## Limitations

- Tree-sampling strategy may not capture domain-specific structure for complex sequence types
- Marginalization of branch indicators may limit capacity for highly structured branching patterns
- Protein backbone experiments rely on pretrained ChainStorm weights without detailed finetuning protocol
- Hydrogen reordering heuristic for QM9 not fully specified

## Confidence

- **High**: Theoretical framework of generator matching with time-dependent Bregman divergences
- **High**: Basic architecture implementation for discrete-only case
- **Medium**: Performance claims on QM9 (depends on exact implementation details)
- **Medium**: Distribution matching claims for antibody sequences (depends on tree-sampling effectiveness)
- **Low**: Protein backbone generation claims (limited evaluation scope, dependence on pretrained model)

## Next Checks

1. Implement and validate discrete-only Branching Flows on antibody sequences; verify length distribution matching and perplexity against oracle-length baseline
2. Test tree-sampling ablations: compare uniform adjacent merge vs. domain-informed schemes on simple sequence tasks
3. Verify training stability: monitor split/deletion event distributions over time to ensure hazards drive completion by t=1