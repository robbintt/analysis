---
ver: rpa2
title: How Many Heads Make an SSM? A Unified Framework for Attention and State Space
  Models
arxiv_id: '2512.15115'
source_url: https://arxiv.org/abs/2512.15115
tags:
- attention
- interaction
- linear
- factorized
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a unified framework for sequence modeling
  architectures, representing them through an input-dependent effective interaction
  operator Wij(X). It identifies two core construction patterns: (1) the Unified Factorized
  Framework (explicit), where Wij(X) varies through scalar coefficients applied to
  shared value maps (e.g., attention), and (2) Structured Dynamics (implicit), where
  Wij is induced by a latent dynamical system (e.g., state space models).'
---

# How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models

## Quick Facts
- arXiv ID: 2512.15115
- Source URL: https://arxiv.org/abs/2512.15115
- Authors: Ali Ghodsi
- Reference count: 15
- Primary result: Multi-head attention is necessary (H ≥ k) to represent linear SSMs with k-dimensional interaction subspaces.

## Executive Summary
This paper establishes a unified framework for analyzing sequence modeling architectures through an input-dependent effective interaction operator W_ij(X). It identifies two fundamental construction patterns: Unified Factorized Framework (explicit) where W_ij varies through scalar coefficients on shared value maps, and Structured Dynamics (implicit) where W_ij is induced by latent dynamical systems. The framework enables rigorous theoretical analysis revealing fundamental trade-offs between expressivity and trainability in modern sequence models.

## Method Summary
The theoretical framework represents sequence models through W_ij(X) ∈ R^{p×d}, the operator mapping input at position j to contribution at output position i. The unified approach characterizes models as either factorized (explicit weighting functions) or structured dynamics (implicit state evolution). Synthetic experiments validate theoretical predictions: (1) system identification task matching multi-head attention to teacher SSMs with varying interaction rank k, and (2) gradient flow comparison measuring ||∇_{x_0} y_T|| across sequence lengths T for SSM vs attention architectures.

## Key Results
- Interaction Rank Gap theorem shows single-head factorized models cannot represent certain structured dynamical maps due to rank limitations
- Equivalence (Head-Count) Theorem proves H=k heads are both necessary and sufficient to represent linear SSMs with k-dimensional interaction subspaces
- Gradient Highway Result demonstrates attention maintains distance-independent gradient paths while stable linear SSMs exhibit exponential attenuation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-head factorized models cannot uniformly approximate certain linear SSMs with non-collinear impulse response operators.
- Mechanism: A single shared value matrix V constrains all interaction operators W_ij to lie in a 1-dimensional subspace (scalar multiples of V). Linear SSMs with rotation-like dynamics produce impulse response matrices W^(τ) spanning a k-dimensional subspace. When k > 1, no single V can span this space.
- Core assumption: The target SSM has impulse-response operators that are not all collinear (interaction rank k ≥ 2).
- Evidence anchors:
  - [abstract] "Interaction Rank Gap showing single-head attention-style models are algebraically rank-limited and cannot represent certain structured dynamical maps."
  - [Section 4.1, Theorem 4.2] Proves inf_{f∈M_Fact} ||F−f||_∞ ≥ 1 for a rotation-based SSM, independent of n.
  - [corpus] "The Effect of Attention Head Count on Transformer Approximation" studies head count effects but does not contradict this rank constraint.
- Break condition: If the target dynamics have interaction rank k=1 (all W^(τ) collinear), the gap closes and single-head suffices.

### Mechanism 2
- Claim: Multi-head attention with H heads can exactly represent any linear SSM whose lag operators span a k-dimensional subspace if and only if H ≥ k.
- Mechanism: Each head h provides an independent value matrix V^(h). The model's interaction subspace is span{V^(1), ..., V(H)} with dimension ≤ H. To match a target SSM with interaction rank k, we need H ≥ k to span the target subspace. The sufficiency construction uses positional feature maps to encode lag-dependent coefficients.
- Core assumption: Causal setting; sequence length n is large enough that the first n impulse responses span the full interaction subspace (Eq. 7).
- Evidence anchors:
  - [abstract] "Equivalence (Head-Count) Theorem proving that representing a linear SSM with k-dimensional lag operators requires H = k heads."
  - [Section 4.2, Theorem 4.4] Proves necessity (H ≥ k) and sufficiency (constructive with r ≤ n feature dimension).
  - [Section 6.1] Empirical verification: test MSE drops sharply as H approaches k, consistent with theory.
  - [corpus] Weak/indirect—neighbor papers discuss head count effects but don't address this specific equivalence.
- Break condition: If input-dependent dynamics (e.g., Mamba) where W^(τ) varies per sequence, the fixed-head equivalence may not hold without input-conditioned features.

### Mechanism 3
- Claim: Attention layers admit input sequences with distance-independent gradient paths, while stable linear SSMs exhibit exponentially decaying gradients with distance.
- Mechanism: In attention, the Jacobian ∂y_i/∂x_j can remain bounded below by ||V||₂ − ε via careful input construction (concentrating softmax mass on position j). In linear SSMs, J_{i,j} = C·Ā^{i−j}·B̄, and if ||Ā||₂ < 1, the norm decays as ||Ā||₂^{i−j}. This creates a "gradient highway" in attention but a sequential bottleneck in SSMs.
- Core assumption: For attention, W_K^⊤W_Q has non-zero action on some direction u (Eq. 10). For SSMs, spectral norm ||Ā||₂ < 1 (stable dynamics).
- Evidence anchors:
  - [abstract] "Gradient Highway Result showing attention layers admit inputs with distance-independent gradient paths while stable linear dynamics exhibit distance-dependent gradient attenuation."
  - [Section 5, Theorem 5.1] Part (1) bounds SSM Jacobian decay; Part (2) constructs inputs preserving ||J_{i,j}||₂ ≥ ||V||₂ − ε.
  - [Section 6.2, Figure 4] Empirical: linear SSM gradient norms decay exponentially with T; attention decays slowly.
  - [corpus] "Rethinking the long-range dependency in Mamba/SSM and transformer models" discusses long-range dependency but doesn't directly address this gradient mechanism.
- Break condition: If ||Ā||₂ ≥ 1 (unstable SSM) or attention has degenerate Q/K projections, the claimed asymmetry weakens or reverses.

## Foundational Learning

- Concept: Matrix rank and subspace span
  - Why needed here: The Interaction Rank Gap and Head-Count Theorem are fundamentally about whether a set of matrices {W^(τ)} can be expressed within the span of {V^(h)}.
  - Quick check question: Given matrices A, B ∈ R^{2×2}, how do you determine if they lie in a 1-dimensional subspace?

- Concept: State-space model impulse response
  - Why needed here: SSM expressivity is characterized through W^(τ) = C·Ā^τ·B̄, and the paper's proofs rely on understanding how Ā^τ evolves.
  - Quick check question: For Ā being a 90° rotation matrix, what are Ā^0, Ā^1, Ā^2, Ā^3, Ā^4?

- Concept: Gradient flow through sequential composition
  - Why needed here: The Gradient Highway analysis requires understanding how Jacobians compose through matrix multiplication and why ||Ā||₂ < 1 causes exponential decay.
  - Quick check question: If J = A·A·A·...·A (n times), what is ||J||₂ in terms of ||A||₂?

## Architecture Onboarding

- Component map:
  - **Unified Factorized (Explicit):** Scalar weight function f_θ(x_i, x_j) × shared V matrix. Includes MLP, CNN (static), Attention, Linear Attention, KAN (dynamic).
  - **Structured Dynamics (Implicit):** W_ij defined by latent state evolution. Includes RNNs, SSMs (S4), selective SSMs (Mamba).
  - **Interaction operator:** W_ij ∈ R^{p×d} maps input at position j to contribution at output position i.

- Critical path:
  1. Identify target task's interaction rank requirements (how many independent lag operators are needed?).
  2. Choose H ≥ k heads if using factorized architecture.
  3. If long-range gradient flow is critical, consider attention or hybrid (attention + SSM) designs.
  4. If efficiency is paramount and k is small, structured dynamics may suffice with proper initialization.

- Design tradeoffs:
  - **Expressivity vs. Efficiency:** SSMs provide high interaction rank with O(n) compute but require careful initialization. Attention provides gradient highways but O(n²) compute (O(n) for linear variants).
  - **Head count vs. Feature dimension:** Per Theorem 4.4, H heads can represent k-dimensional interaction family. Positional feature maps may require r = O(n) dimensions in the worst case, but r = O(m·J²) under spectral assumptions (Appendix A).
  - **Static vs. Dynamic weights:** Static (CNN, S4) are faster but content-independent. Dynamic (Attention, Mamba) adapt to input but add complexity.

- Failure signatures:
  - **Under-capacity (H < k):** Model cannot fit target dynamics; test MSE remains high regardless of training. See Figure 3 (left) for empirical signature.
  - **Gradient vanishing in SSMs:** Long-range sensitivities decay exponentially (Figure 4). May appear as inability to learn dependencies beyond certain distance.
  - **Attention on short sequences with k > H:** Even with full attention, single-head cannot represent rank-k dynamics. Multi-head is necessary, not just beneficial.

- First 3 experiments:
  1. **Head-count sweep on synthetic SSM:** Train multi-head linear attention to fit a teacher SSM with known interaction rank k ∈ {2, 4, 8}. Vary H ∈ {1, ..., 9}. Expect MSE cliff at H = k. Reproduces Figure 3.
  2. **Gradient norm vs. sequence length:** Compare ||∇_{x_0} y_T|| for linear SSM vs. attention across T ∈ {32, 64, 128, 256, 512}. Expect exponential decay for SSM, near-constant for attention. Reproduces Figure 4.
  3. **Singular value analysis:** For a trained H-head model targeting rank-k SSM, compute singular values of learned interaction operator. Expect ≈ k dominant components. Reproduces Figure 3 (right).

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical results rely on idealized linear settings and random input assumptions
- The equivalence theorem requires feature maps with sufficient dimension (r ≤ n worst case, r = O(mJ²) under spectral assumptions)
- The gradient analysis assumes stable SSMs (||Ā||₂ < 1) and attention with non-degenerate Q/K projections

## Confidence
- **High Confidence:** The algebraic rank constraints (Mechanism 1 and 2) and their consequences for multi-head necessity are mathematically rigorous with formal proofs in the appendix.
- **Medium Confidence:** The gradient highway result (Mechanism 3) follows from the theoretical bounds but requires careful experimental validation across different initialization schemes and sequence patterns.
- **Medium Confidence:** The practical implications for architecture design are logically sound but depend on empirical verification that synthetic insights transfer to real sequence modeling tasks.

## Next Checks
1. **Rank sensitivity analysis:** Systematically vary the interaction rank k of target SSMs (including rank-1 cases) to confirm the theoretical head-count threshold where MSE plateaus. Measure singular value spectra of learned interaction operators to verify subspace alignment.
2. **Gradient path ablation:** Compare gradient flow in attention vs SSM under different conditions: unstable SSMs (||Ā||₂ ≥ 1), attention with learned vs random Q/K projections, and hybrid architectures. Test whether attention's gradient highway persists with linear attention variants.
3. **Feature dimension efficiency:** Implement and evaluate practical feature map constructions that achieve r = O(mJ²) rather than r = O(n). Measure the trade-off between feature dimension and approximation quality across different k values.