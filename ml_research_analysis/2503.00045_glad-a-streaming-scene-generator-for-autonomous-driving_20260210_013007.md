---
ver: rpa2
title: 'Glad: A Streaming Scene Generator for Autonomous Driving'
arxiv_id: '2503.00045'
source_url: https://arxiv.org/abs/2503.00045
tags:
- video
- glad
- data
- generation
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Glad, a streaming scene generator for autonomous
  driving that generates video data in a frame-by-frame manner. The key innovation
  is a latent variable propagation module that maintains temporal consistency by using
  denoised latent features from the previous frame as noise prior for the current
  frame.
---

# Glad: A Streaming Scene Generator for Autonomous Driving

## Quick Facts
- **arXiv ID:** 2503.00045
- **Source URL:** https://arxiv.org/abs/2503.00045
- **Reference count:** 25
- **Primary result:** Achieves FID 11.18 and FVD 188 on nuScenes for frame-by-frame video generation

## Executive Summary
Glad is a streaming scene generator for autonomous driving that generates video data frame-by-frame, enabling videos of arbitrary length. It introduces a Latent Variable Propagation (LVP) module that uses denoised latent features from the previous frame as a noise prior for the current frame, ensuring temporal consistency. A Streaming Data Sampler is designed for efficient training by caching and reusing latent features. The model is evaluated on the nuScenes dataset, achieving strong performance in both video generation quality (FID 11.18, FVD 188) and downstream 3D object detection tasks (28.3 mAP, 41.3 NDS), outperforming offline approaches.

## Method Summary
Glad is built upon Stable Diffusion 2.1 with a ControlNet for spatial conditioning. It generates videos frame-by-frame using Latent Variable Propagation (LVP), which injects the denoised latent features from the previous frame as a noise prior into the current frame's generation process. This is implemented by replacing the standard Gaussian noise initialization with a LayerNorm-normalized version of the previous frame's latent. A Streaming Data Sampler manages training by loading consecutive frames and caching latents, enabling efficient learning without the computational cost of generating prior frames from scratch. The model is trained in two stages: image-level pre-training (1.25M iterations) followed by video-level fine-tuning on nuScenes (48 epochs, 8 A100 GPUs).

## Key Results
- Achieves FID 11.18 and FVD 188 on nuScenes for frame-by-frame video generation
- Improves downstream 3D object detection: 28.3 mAP and 41.3 NDS using StreamPETR
- LVP improves temporal consistency: NDS +1.3, mAP +1.5, FID from 20.85 to 12.57 compared to baseline
- Streaming Data Sampler maintains constant 31GB+7GB memory usage regardless of video length, avoiding OOM errors

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Propagation (LVP) for Temporal Consistency
- **Claim:** Using denoised latent features from the previous frame as a noise prior improves temporal consistency in frame-by-frame video generation compared to standard Gaussian noise initialization.
- **Mechanism:** For frame $n$, the forward diffusion process is conditioned on the LayerNorm-normalized denoised latent features $z_{n-1}^0$ of the previous frame: $q(z_n^t | z_n^{t-1}) = \mathcal{N}(z_n^t; \sqrt{1 - \beta_t} z_n^{t-1}, \beta_t \phi(z_{n-1}^0))$.
- **Core assumption:** Denoised latent features contain sufficient structural and semantic information about the scene to guide the generation of the subsequent frame.
- **Evidence anchors:** LVP shows 1.3 NDS and 1.5 mAP improvements; FID reduces from 20.85 to 12.57.
- **Break condition:** Removing Layer Normalization causes training loss to become NAN.

### Mechanism 2: Streaming Data Sampler (SDS) for Efficient Training
- **Claim:** A streaming data sampler enables efficient training by caching and reusing latent features, avoiding the computational cost of generating prior frames from scratch.
- **Mechanism:** During training, frames are sampled sequentially. The denoised latent features $z_{n-1}^0$ generated at iteration $t$ are saved in a cache and retrieved at iteration $t+1$ for frame $n$.
- **Core assumption:** Accessing a cache is significantly faster than regenerating the previous frame's latent features via the full diffusion process.
- **Evidence anchors:** SDS memory usage remains constant (31GB +7GB) regardless of video length, whereas sliding window approaches lead to OOM errors.
- **Break condition:** Fails if cache is not managed correctly or GPU memory cannot hold cached latents.

### Mechanism 3: BEV Layout and ControlNet for Fine-Grained Control
- **Claim:** Injecting Bird's Eye View (BEV) layouts via a ControlNet allows for precise, frame-by-frame control over scene composition.
- **Mechanism:** BEV layouts (object categories, map elements) are converted to camera view perspective controls (bounding boxes, depth maps, road maps) and fed into a ControlNet operating in parallel with the main U-Net.
- **Core assumption:** BEV layout provides a disentangled representation of the scene's spatial structure that can be effectively projected into the latent space to guide generation.
- **Evidence anchors:** Generated data improves downstream MapTR model mAP by 4.9 points.
- **Break condition:** Fails if BEV-to-camera projection is inaccurate or ControlNet is not properly trained.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDM)**
  - **Why needed here:** Glad is built upon Stable Diffusion, an LDM. Understanding the forward (adding noise) and reverse (denoising) processes in a lower-dimensional latent space is essential.
  - **Quick check question:** Can you explain why LDMs are more computationally efficient than pixel-space diffusion models?

- **Concept: Bird's Eye View (BEV) Representation**
  - **Why needed here:** The model takes BEV layouts as conditional inputs. Understanding what a BEV map represents (a top-down view of the scene) is critical for understanding the control mechanism.
  - **Quick check question:** How does a BEV representation simplify the problem of representing object positions compared to a standard front-facing camera view?

- **Concept: ControlNet**
  - **Why needed here:** Glad uses a ControlNet to inject spatial conditions. Understanding how ControlNet adds an extra, trainable copy of the U-Net's encoder to process conditional inputs helps explain how fine-grained control is achieved.
  - **Quick check question:** What is the primary advantage of using a ControlNet architecture over simply concatenating a condition map with the input noise?

## Architecture Onboarding

- **Component map:** Input (Gaussian Noise or Reference Frame Latent) -> VAE Encoder (if ref) -> LVP Module (receives prior latent) -> Add Noise (conditioned by LVP output) -> Denoising U-Net (guided by ControlNet features from BEV/Text) -> VAE Decoder -> Output Image. The output latent is cached and fed back into the LVP module for the next frame.
- **Critical path:** Input -> VAE Encoder -> LVP Module -> Add Noise -> Denoising U-Net (ControlNet) -> VAE Decoder -> Output. The LVP module is the key innovation for temporal consistency.
- **Design tradeoffs:**
  - **Online vs. Offline Generation:** Glad generates frames one by one (online), allowing arbitrary video length and lower per-frame memory, but can suffer from error accumulation over very long sequences.
  - **Chunking Strategy:** Splitting training videos into chunks (e.g., 2 chunks) balances training efficiency with data diversity. Longer chunks may improve temporal consistency but increase memory/complexity.
- **Failure signatures:**
  - **Loss becomes NAN:** Indicates that the Layer Normalization in the LVP module has been removed or is malfunctioning.
  - **Temporal Inconsistency:** Flickering or jumping objects between frames suggests the LVP mechanism is not working correctly.
  - **Omitted Objects/Incorrect Layout:** The ControlNet may be under-trained or the BEV-to-camera projection has errors.
- **First 3 experiments:**
  1. **Ablation on LVP:** Train two models—one with LVP and one without (baseline). Compare FID/FVD scores to quantify the impact on temporal consistency.
  2. **Memory Profiling:** Measure GPU memory consumption during training with the Streaming Data Sampler versus a standard sliding window approach across different video lengths.
  3. **Downstream Task Evaluation:** Generate a synthetic validation set and run a pre-trained 3D object detector (e.g., StreamPETR) on it. Compare the mAP/NDS scores against the same model evaluated on real data.

## Open Questions the Paper Calls Out
- **Question:** How can the Latent Variable Propagation (LVP) module be modified to ensure temporal consistency in high-dynamic driving scenes?
  - **Basis in paper:** [explicit] The Conclusion states the method "struggles to generate high-quality video data under high dynamic scenes" and that "temporal consistency of objects still needs to be improved."
  - **Why unresolved:** The current LVP mechanism may fail to capture large frame-to-frame displacements or rapid appearance changes characteristic of high-dynamic scenes.
  - **What evidence would resolve it:** Quantitative results (FVD, NDS) showing improved performance on a curated subset of high-motion scenarios compared to the baseline LVP.

- **Question:** To what extent does error accumulation in the frame-by-frame generation process degrade downstream task performance when generating videos of lengths significantly greater than the evaluated 16 frames?
  - **Basis in paper:** [inferred] Tab. 5(d) shows a drop in NDS (41.3 → 40.5) when video length increases from 8 to 16 frames, and Fig. 4 shows detection performance stabilizing at ~85% of real data.
  - **Why unresolved:** While the model supports "arbitrary lengths" structurally, the ablation studies only validate up to 16 frames; the cumulative effect of small denoising errors over very long sequences remains unquantified.
  - **What evidence would resolve it:** Evaluation of downstream detection accuracy (mAP/NDS) on generated video sequences exceeding 50 frames compared against ground truth.

- **Question:** Can the efficiency of the Latent Variable Propagation (LVP) be effectively combined with the superior temporal modeling of Temporal Attention Layers (TL) to mitigate the trade-off between speed and FVD performance?
  - **Basis in paper:** [inferred] Appendix Tab. 11 highlights a trade-off: Temporal Layers achieve a better FVD (183 vs 207) but require significantly more memory and training time (5 Days vs 1 Day).
  - **Why unresolved:** The paper presents LVP and TL as distinct architectural choices; it does not explore if a hybrid approach could capture complex temporal dependencies without the full computational burden of TL.
  - **What evidence would resolve it:** A hybrid model implementation that achieves an FVD closer to 183 while maintaining the training/inference speed of the LVP-only model.

## Limitations
- Struggles to generate high-quality video data under high dynamic scenes, where temporal consistency of objects still needs improvement.
- Ablation studies only validate performance up to 16 frames; the cumulative effect of error accumulation over very long sequences remains unquantified.
- Relies on external work (MagicDrive/Panacea) for BEV control encoding without providing the conversion code.

## Confidence

- **High confidence:** The LVP mechanism's core idea (using previous frame's denoised latent as noise prior) is well-supported by ablation results showing 1.3 NDS and 1.5 mAP improvements.
- **Medium confidence:** The SDS efficiency claims are supported by memory profiling, but the sequential sampling approach's impact on data diversity and generalization is not thoroughly explored.
- **Medium confidence:** Downstream task improvements (28.3 mAP, 41.3 NDS) demonstrate practical utility, but the evaluation uses a specific detector (StreamPETR) without comparison to alternatives.

## Next Checks

1. **Ablation on Layer Normalization:** Remove the LN function from the LVP module and verify that training loss becomes NaN, confirming the paper's explicit warning about this failure mode.
2. **Memory Scaling Test:** Implement both the Streaming Data Sampler and a sliding window approach, measuring GPU memory usage across video lengths from 8 to 64 frames to confirm the constant memory overhead claim.
3. **Temporal Consistency Analysis:** Generate videos with and without LVP, then compute frame-to-frame feature distance metrics (e.g., LPIPS) to quantitatively assess temporal consistency improvements beyond the aggregate FID/FVD scores.